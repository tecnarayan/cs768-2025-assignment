@article{Henderson2020,
title = {Towards the systematic reporting of the energy and carbon footprints of machine learning},
author = {P. Henderson and J. Hu and J. Romoff and E. Brunskill and D. Jurafsky and J. Pineau},
journal = {JMLR},
volume = {21},
number = {248},
pages = {1-43},
year = {2020}
}

@article{HowardRuder2018,
author = {Howard, J. and Ruder, S},
title = {Universal language model fine-tuning for text classification},
journal = {ACL},
year = {2018}
}

@article{DaiLe2015,
title = {Semi-supervised Sequence Learning},
author = {A.M. Dai and Q.V. Le},
journal = {NIPS},
year = {2015}
}

@article{Neyshabur2020,
title = {What is being transferred in transfer learning?},
author = {B. Neyshabur and H. Sedghi and C. Zhang},
journal = {NeurIPS},
year = {2020}
}

@article{Raghu2019,
title = {Transfusion: Understanding Transfer Learning for Medical Imaging},
author = {M. Raghu and C. Zhang and J. Kleinberg and S. Bengio},
journal = {NeurIPS},
year = {2019}
}



@article{Hao2019,
title = {Visualizing and Understanding the Effectiveness of {BERT}},
author = {Y. Hao and L. Dong and F. Wei and K. Xu},
journal = {EMNLP},
year = {2019}
}

@article{He2019,
title = {Rethinking {I}mage{N}et pre-training},
author = {K. He and R. Girshick and P. Dollár},
journal = {ICCV},
year = {2019}
}

@article{Radford2018,
author = {Radford, A. and Narasimhan, K. and Salimans, T. and Sutskever, I.},
title = {Improving language understanding by generative pre-training},
journal ={OpenAI},
year = {2018}
}

@article{Yosinski2014,
title = {How transferable are features in deep neural networks?},
author = {J. Yosinski and J. Clune and Y. Bengio and H. Lipson},
journal = {NIPS},
year = {2014}
}

@article{Huh2016,
title = {What makes {I}mage{N}et good for transfer learning?},
author = {M. Huh and P. Agrawal and A. A. Efros},
journal = {arXiv:1608.08614},
year = {2016}
}

@article{BERT,
author = {Devlin, J. and Chang, M.-W. and Lee, K. and Toutanova, K.},
title = {{BERT}: {P}re-training of deep bidirectional transformers for language understanding},
journal = {arXiv preprint arXiv:1810.04805},
year = {2018}
}

@article{Wikitext2,
	title     = {Pointer Sentinel Mixture Models},
	author = {Merity, S. and Xiong, C. and Bradbury, J. and Socher, R.},
	journal = {ICLR},
	year      = {2017},
	unusedURL       = {https://openreview.net/forum?id=Byj72udxe}
}

@inproceedings{Transformer,
	title={Attention is all you need},
	author = {A. Vaswani and N. Shazeer and N. Parmar and J. Uszkoreit and L. Jones and A. N. Gomez and L. Kaiser and I. Polosukhin},
	booktitle={Advances in Neural Information Processing Systems},
	pages={5998--6008},
	year={2017},
	unusedURL={http://papers.nips.cc/paper/7181-attention-is-all-you-need}
}

@inproceedings{resnet,
	title={Deep residual learning for image recognition},
	author = {He, K. and Zhang, X. and Ren, S. and Sun, J.}, 
	booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages={770--778},
	year={2016},
	unusedURL={http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html}
}

@article{Pytorch,
	author = {A. Paszke and S. Gross and S. Chintala and G. Chanan and E. Yang and Z. DeVito and Z. Lin and A. Desmaison and L. Antiga and A. Lerer},
	title = {Automatic differentiation in {P}y{T}orch},
	year = {2017},
	unusedURL={https://openreview.net/pdf?id=BJJsrmfCZ}
}

@article{cifar10,
  title={Learning multiple layers of features from tiny images},
  author={A. Krizhevsky and G. Hinton},
  year={2009},
  publisher={Technical Report, University of Toronto}
}

@article{Li2019,
title = {Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks},
author = {Y. Li and C. Wei and T. Ma},
journal = {NeurIPS},
year = {2019}
}

@article{Krizhevsky2012,
author = {A. Krizhevsky and I. Sutskever and G. E. Hinton},
title = {Image{N}et classification with deep convolutional neural networks},
journal = {NIPS},
year = {2012}
}

@article{wideresnet,
author = {S. Zagoruyko and N. Komodakis},
title = {Wide residual networks},
journal = {CoRR}, 
year = {2016}
}

@article{Jastrzebski2020,
author = {S. Jastrzębski and M. Szymczak and S. Fort and D. Arpit and J. Tabor and K. Cho and K. Geras},
title = {The break-even point on optimization trajectories of deep neural networks},
journal = {ICLR},
year = {2020}
}

@article{Frankle2020,
author = {J. Frankle and D.J. Schwab and A.S. Morcos},
title = {The early phase of neural network training},
journal = {ICLR},
year = {2020}
}

@article{Hoffer2017,
author = {E. Hoffer and I. Hubara and D. Soudry},
title = {Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
journal = {NIPS},
year = {2017}
}

@article{Tuckerman1992,
author = {M. E. Tuckerman and B. J. Berne and G. J. Martyna},
title = {Reversible multiple time scale molecular dynamics},
journal = {The Journal of Chemical Physics},
volume = {97},
number = {3},
pages = {1990-2001},
year = {1992}
}

@article{Tuckerman1991,
author = {M. E. Tuckerman and B. J. Berne and G. J. Martyna},
title = {Molecular dynamics algorithm for multiple time scales: Systems with long range forces},
journal = {The Journal of Chemical Physics}, 
volume = {94},
number = {10},
pages = {6811-6815},
year = {1991}
}

@article{Biesiadecki1993,
author = {J. J. Biesiadecki and R. D. Skeel},
title = {Dangers of multiple time step methods},
journal = {Journal of Computational Physics},
volume = {109},
pages = {318-328},
year = {1993}
}

@book{LeimkuhlerReich2005,
author = {B. Leimkuhler and S. Reich},
title = {Simulating Hamiltonian Dynamics},
publisher = {Cambridge Monographs on Applied and Computational Mathematics. Cambridge University Press},
year = {2005}
}

@article{Ma2003,
author = {Q. Ma and J. Izaguirre and R. Skeel}, 
title = {Verlet-{I}/r-{RESPA}/{I}mpulse is limited by nonlinear instabilities},
journal = {SIAM Journal on Scientific Computing},
volume = {24},
number = {6},
pages = {1951-1973},
year = {2003}
}
@article{Leimkuhler2013,
author = {B. Leimkuhler and C. Matthews},
title = {Rational construction of stochastic numerical methods for molecular sampling},
journal = {Applied Mathematics Research eXpress}, 
volume = {2013},
number = {1},
pages = {34-56},
year = {2013}
}


@book{LeimkuhlerMatthews2015,
author = {B. Leimkuhler and C. Matthews},
title = {Molecular Dynamics: With Deterministic and Stochastic Numerical Methods},
publisher = {Interdisciplinary Applied Mathematics. Springer},
year = {2015}
}

@article{Leimkuhler2001,
author = {B. Leimkuhler and S. Reich},
title = {A reversible averaging integrator for multiple time- scale dynamics},
journal = {Journal of Computational Physics},
volume = {171},
number = {1},
pages = {95-114},
year = {2001}
}

@article{SST,
author = {R. Socher and A. Perelygin and J. Wu and J. Chuang and C. Manning and A. Ng and C. Potts},
title = {Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank},
journal = {Conference on Empirical Methods in Natural Language Processing},
year = {2013}
}

@article{molly1,
author = {B. Garc{\'i}a-Archilla and J.M. Sanz-Serna and R. Skeel},
title = {Long-time-step methods for oscillatory differential equations},
journal = {SIAM J. Sci. Comput.},
volume = {20},
number = {930-963},
year = {1998}
}

@article{molly2,
author = {J. A. Izaguirre and Q. Ma and T. Matthey and J. Willcock and T. Slabach and B. Moore and G. Viamontes},
title = {Overcoming instabilities in {V}erlet-{I}/r-{RESPA} with the mollified impulse method},
book = {Computational Methods for Macromolecules: Challenges and Applications, volume 24 of Lecture Notes in Computational Science and Engineering},
pages = {146–174},
publisher = {Springer Berlin Heidelberg},
year = {2002}
}

@article{HintonPlautfastweights,
author = {G. E. Hinton and D. C. Plaut},
title = {Using fast weights to deblur old memories},
journal = {In Proceedings of the ninth annual conference of the Cognitive Science Society},
pages = {177-186},
year = {1987}
}

@article{feldmanfastweights,
author = {J. A. Feldman},
title = {Dynamic connections in neural networks},
journal = {Biological Cybernetics}, volume = {46},
number = {1},
pages = {27-39},
year = {1982}
}

@article{Ba2016,
title = {Using Fast Weights to Attend to the Recent Past},
author = {J. Ba and G. Hinton and V. Mnih and J. Z. Leibo and C. Ionescu},
journal = {NeurIPS},
year = {2016}
}

@article{Zhang2019,
title = {Are All Layers Created Equal?},
author = {C. Zhang and S. Bengio and Y. Singer},
journal = {arXiv:1902.01996},
year = {2019}
}

@article{LARS,
author = { Y. You and I. Gitman and B. Ginsburg}, 
title = {Large batch training of convolutional networks},
journal = {arXiv preprint arXiv:1708.03888},
year = {2017}
}
@article{LAMB,
author= {Y. You and J. Li and S. Reddi and J. Hseu and S. Kumar and S. Bhojanapalli and X. Song and J. Demmel and K. Keutzer and C. Hsieh},
title = {Large batch optimization for deep learning: training {BERT} in 76 minutes}, 
journal = {ICLR},
year = {2020}
}

@article{LMV,
	author = {B. Leimkuhler and C. Matthews and T. Vlaar},
	title = {{P}artitioned integrators for thermodynamic parameterization of neural networks}, 
	journal = {Foundations of Data Science},
	volume = {1},
	number = {4},
	pages = {457--489}, 
	year = {2019},
	unusedURL = {http://doi.org/10.3934/fods.2019019}
}

@article{dropout,
author = {N. Srivastava and G. Hinton and A. Krizhevsky and I. Sutskever and R. Salakhutdinov},
title = {Dropout: a simple way to prevent neural networks from overfitting},
journal = {JMLR},
volume = {15}, 
pages = {1929-1958},
year = {2014}
}

@article{dropconnect,
author = {L. Wan and M. Zeiler and S. Zhang and Y. Le Cun and R. Fergus},
title = {Regularization of neural networks using {D}rop{C}onnect},
journal = {ICML},
year = {2013}
}

@article{Jouppi2017,
author = {N. P. Jouppi and C. Young and N. Patil and D. Patterson and G. Agrawal and R. Bajwa and S. Bates and S. Bhatia and N. Boden and A. Borchers and et al.},
title = {In-datacenter performance analysis of a tensor processing unit},
journal = {44th International Symposium on Computer Architecture (ISCA)},
year = {2017}
}

@article{paleo,
author = {H. Qi and E. R. Sparks and A. Talwalkar},
title = {{PALEO}: A performance model for deep neural networks},
journal = {ICLR},
year = {2017}
}

@article{justus2018,
author = {D. Justus and J. Brennan and S. Bonner and A.S. McGough},
title = {Predicting the computational cost of deep learning models},
journal = {CoRR},
year = {2018}
}

@article{batchnorm,
author = {S. Ioffe and C. Szegedy},
title = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
journal = {ICML},
year = {2015}
}

@article{understandingBN,
author = {P. Luo and X. Wang and W. Shao and Z. Pen},
title = {Towards understanding regularization in batch normalization}, 
journal = {ICLR},
year = {2019}
}

@article{BNdrop1,
title = {Understanding the Disharmony between Dropout and Batch Normalization by Variance Shift},
author = {X. Li and S. Chen and X. Hu and J. Yang},
journal = {IEEE/CVF Conference on Computer Vision and Pattern Recognition},
year = {2018}
}

@article{BNdrop2,
title = {Rethinking the Usage of Batch Normalization and Dropout in the Training of Deep Neural Networks},
author = {G. Chen and P. Chen and Y. Shi and C. Hsieh and B. Liao and S. Zhang},
journal = {arXiv:1905.05928},
year = {2019}
}

@article{SWA,
title = {Averaging weights leads to wider optima and better generalization},
author = {P. Izmailov and D. Podoprikhin and T. Garipov and D. Vetrov and A. G. Wilson}, 
journal = {Uncertainty in Artificial Intelligence},
year = {2018}
}

@article{Leimkuhler1998,
title = {Timestep Acceleration of Waveform Relaxation},
author = {B. Leimkuhler},
journal = {SIAM Journal on Numerical Analysis},
volume = {35},
number = {1},
pages = {31-50},
year = {1998}
} 

@article{Gear1974,
title = {Multirate methods for ordinary differential equations},
author = {C. W. Gear},
journal = {Technical Report, University of Illinois},
year = {1974}
}

@article{Rice1960,
author = {J. R. Rice},
title = {Split runge-kutta methods for simultaneous equations},
journal = {Journal of Research of the National Institute of Standards and Technology},
volume = {60},
year = {1960}
}

@book{gardiner,
Author = {C. Gardiner},
Publisher = {3rd edn. Springer, New York},
Title = {Handbook of Stochastic Methods for Physics, Chemistry, and the Natural Sciences},
year = {2004}
}

@article{WeT11,
	title={{B}ayesian learning via stochastic gradient {L}angevin dynamics},
	author={Welling, M. and Teh, Y. W.},
	journal = {ICML},
	pages={681--688},
	year={2011}
}

@article{MaStHi2002,
	Author = {J.C. Mattingly and A.M.Stuart and D.J. Higham},
	Date-Added = {2017-11-14 20:27:48 +0000},
	Date-Modified = {2017-12-05 02:49:08 +0000},
	Journal = {Stochastic Processes and their Applications},
	Number = {2},
	Pages = {185--232},
	Publisher = {Elsevier},
	Title = {Ergodicity for {SDEs} and approximations: locally {L}ipschitz vector fields and degenerate noise},
	Volume = {101},
	Year = {2002}}
	
	
@article{coldposterior,
	author = {F.  Wenzel and K. Roth and B. S. Veeling and J. Swiatkowski and L. Tran and S. Mandt and J. Snoek and T. Salimans and R. Jenatton and S. Nowozin},
	title = {How good is the {B}ayes posterior in deep neural networks really?},
	journal={arXiv:2002.02405},
	year = {2020},
	unusedURL={https://arxiv.org/abs/2002.02405}
}

@article{AdamW,
title = {Decoupled weight decay regularization},
author = {I. Loshchilov and F. Hutter},
journal = {ICLR},
year = {2019}
}


@article{Engstler1997,
author = {Ch. Engstler and Ch. Lubich},
title = {Multirate extrapolation methods for differential equations with different time scales},
journal = {Computing},
volume = {58},
pages = {173-185},
year = {1997} 
}

@article{Gunther1993,
author = {M. G{\"u}nther and P. Rentrop},
title = {Multirate ROW methods and latency of electric circuits},
journal = {Appl. Numer. Math.},
volume = {13},
pages = {83-102},
year = {1993}
}

@article{GearWells1984,
author = {C.W. Gear and D.R. Wells},
title = {Multirate linear multistep methods},
journal = {BIT},
volume ={24}, 
pages = {484-502},
year = {1984}
}

@article{Constantinescu2013,
title = {Extrapolated Multirate Methods for Differential Equations with Multiple Time Scales},
author = {E.M. Constantinescu and A. Sandu},
journal = {Journal of Scientific Computing},
volume ={56},
number = {1},
pages = {28-44},
year = {2013}
}

@article{VF,
title = {What can linear interpolation of neural network loss landscapes tell us?},
author = {T. Vlaar and J. Frankle},
journal = {ICML},
year = {2022}
}

@article{GDwithmom,
author = {B. T. Polyak},
title = {Some methods of speeding up the convergence of iteration methods},
journal = {USSR Computational Mathematics and Mathematical Physics},
volume = {4},
number = {5},
pages = {1-17}, 
year = {1964}
}

@article{nesterov,
author = {Y. Nesterov},
title = {A method of solving a convex programming problem with convergence rate {O}(1/k2)},
year = {1983}, 
journal = {Soviet Mathematics Doklady},
Volume = {27},
pages = {372-376}
}

@article{sutskever,
title = {On the importance of initialization and momentum in deep learning},
author = {I. Sutskever and J. Martens and G. Dahl and G. Hinton},
journal = {ICML},
year = {2013}
}

@article{kristiadi2020,
title = {Being {B}ayesian, Even Just a Bit, Fixes Overconfidence in {R}e{LU} Networks},
author = {A. Kristiadi and M. Hein and P. Hennig},
journal = {ICML},
year = {2020}
}

@article{murfet2020,
title = {Deep Learning is Singular, and That’s Good},
author = {D. Murfet and S. Wei and M. Gong and H. Li and J. Gell-Redman and T. Quella},
journal = {arXiv:2010.11560},
year = {2020}
}

@article{Li2020,
author = {H. Li and P. Chaudhari and H. Yang and M. Lam and A. Ravichandran and R. Bhotika and S. Soatto},
title = {Rethinking the hyperparameters for fine-tuning},
journal = {ICLR},
year = {2020}
}

@article{PennTreebank,
	title = {Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank},
	author = {M. P. Marcus and B. Santorini and M. A. Marcinkiewicz},
	journal = {Computational Linguistics},
	volume = {19},
	number = {2},
	year = {1993},
	pages = {313--330},
	unusedURL = {https://www.aclweb.org/anthology/J93-2004}
}

@article{glue,
title = {{GLUE}: {A} multi-task benchmark and analysis platform for natural language understanding},
author = {A. Wang and A. Singh and J. Michael and F. Hill and O. Levy and S. R. Bowman},
journal = {ICLR},
year = {2019}
}

@article{roberta,
title = {Ro{BERT}a: {A} Robustly Optimized {BERT} Pretraining Approach}, 
author = {Y. Liu and M. Ott and N. Goyal and J. Du and M. Joshi and D. Chen and O. Levy and M. Lewis and L. Zettlemoyer and V. Stoyanov},
year = {2019},
journal = {arXiv:1907.11692}
}

@article{AdamvsSGD,
author = {J. Zhang and S. Karimireddy and A. Veit and S. Kim and S. Reddi and S. Kumar and S. Sra},
title = {Why are Adaptive Methods Good for Attention Models?},
journal = {NeurIPS},
year = {2020}

}

@article{distilbert,
title = {Distil{BERT}, a distilled version of {BERT}: {S}maller, faster, cheaper and lighter},
author = {V. Sanh and L. Debut and J. Chaumond and T. Wolf
},
journal = {Energy Efficient Machine Learning and Cognitive Computing Workshop, Advances in Neural Information Processing Systems},
year = {2019}
}

@article{revisitresnet,
title = {Revisiting {R}es{N}ets: {I}mproved Training and Scaling Strategies},
author = {I. Bello and W. Fedus and X. Du and E. D. Cubuk and A. Srinivas and T.-Y. Lin and J. Shlens and B. Zoph},
journal = {NeurIPS},
year = {2021}
}