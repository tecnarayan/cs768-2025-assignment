\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2014)Agarwal, Anandkumar, Jain, Netrapalli, and
  Tandon]{agarwal2014learning}
Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and
  Rashish Tandon.
\newblock Learning sparsely used overcomplete dictionaries.
\newblock In \emph{COLT}, pages 123--137, 2014.

\bibitem[Agarwal et~al.(2016)Agarwal, Allen-Zhu, Bullins, Hazan, and
  Ma]{agarwal2016linear}
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma.
\newblock Finding approximate local minima for nonconvex optimization in linear
  time.
\newblock \emph{arXiv preprint arXiv:1611.01146}, 2016.

\bibitem[Alistarh et~al.(2018)Alistarh, Allen-Zhu, and Li]{alistarh2018sgd}
Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li.
\newblock Byzantine stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1803.08917}, 2018.

\bibitem[Allen-Zhu(2017)]{allen2017natasha}
Zeyuan Allen-Zhu.
\newblock Natasha 2: Faster non-convex optimization than {SGD}.
\newblock \emph{arXiv preprint arXiv:1708.08694}, 2017.

\bibitem[Allen-Zhu and Li(2017)]{allen2017neon2}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Neon2: Finding local minima via first-order oracles.
\newblock \emph{arXiv preprint arXiv:1711.06673}, 2017.

\bibitem[Balakrishnan et~al.(2014)Balakrishnan, Wainwright, and
  Yu]{balakrishnan2014EM}
Sivaraman Balakrishnan, Martin~J. Wainwright, and Bin Yu.
\newblock Statistical guarantees for the {EM} algorithm: From population to
  sample-based analysis.
\newblock \emph{arXiv preprint arXiv:1408.2156}, 2014.

\bibitem[Bertsekas and Tsitsiklis(2000)]{bertsekas2000errors}
Dimitri~P. Bertsekas and John~N. Tsitsiklis.
\newblock Gradient convergence in gradient methods with errors.
\newblock \emph{SIAM Journal on Optimization}, 10\penalty0 (3):\penalty0
  627--642, 2000.

\bibitem[Bhatia et~al.(2015)Bhatia, Jain, and Kar]{bhatia2015robust}
Kush Bhatia, Prateek Jain, and Purushottam Kar.
\newblock Robust regression via hard thresholding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  721--729, 2015.

\bibitem[Bhatia et~al.(2017)Bhatia, Jain, Kamalaruban, and
  Kar]{bhatia2017consistent}
Kush Bhatia, Prateek Jain, Parameswaran Kamalaruban, and Purushottam Kar.
\newblock Consistent robust regression.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2107--2116, 2017.

\bibitem[Blanchard et~al.(2017)Blanchard, Mhamdi, Guerraoui, and
  Stainer]{blanchard2017byzantine}
Peva Blanchard, El~Mahdi~El Mhamdi, Rachid Guerraoui, and Julien Stainer.
\newblock Byzantine-tolerant machine learning.
\newblock \emph{arXiv preprint arXiv:1703.02757}, 2017.

\bibitem[Candes et~al.(2015)Candes, Li, and
  Soltanolkotabi]{candes2014wirtinger}
Emmanuel~J Candes, Xiaodong Li, and Mahdi Soltanolkotabi.
\newblock Phase retrieval via {W}irtinger flow: Theory and algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (4):\penalty0 1985--2007, 2015.

\bibitem[Carmon et~al.(2016)Carmon, Duchi, Hinder, and
  Sidford]{carmon2016accelerated}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Accelerated methods for non-convex optimization.
\newblock \emph{arXiv preprint arXiv:1611.00756}, 2016.

\bibitem[Charikar et~al.(2017)Charikar, Steinhardt, and
  Valiant]{charikar2017learning}
Moses Charikar, Jacob Steinhardt, and Gregory Valiant.
\newblock Learning from untrusted data.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 47--60. ACM, 2017.

\bibitem[Chatterji and Bartlett(2017)]{chatterji2017alternating}
Niladri Chatterji and Peter~L Bartlett.
\newblock Alternating minimization for dictionary learning with random
  initialization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1994--2003, 2017.

\bibitem[Chen et~al.(2018{\natexlab{a}})Chen, Charles, Papailiopoulos,
  et~al.]{chen2018draco}
Lingjiao Chen, Zachary Charles, Dimitris Papailiopoulos, et~al.
\newblock {DRACO}: Robust distributed training via redundant gradients.
\newblock \emph{arXiv preprint arXiv:1803.09877}, 2018{\natexlab{a}}.

\bibitem[Chen and Wainwright(2015)]{chen2015fast}
Yudong Chen and Martin~J Wainwright.
\newblock Fast low-rank estimation by projected gradient descent: General
  statistical and algorithmic guarantees.
\newblock \emph{arXiv preprint arXiv:1509.03025}, 2015.

\bibitem[Chen et~al.(2017)Chen, Su, and Xu]{chen2017distributed}
Yudong Chen, Lili Su, and Jiaming Xu.
\newblock Distributed statistical machine learning in adversarial settings:
  {Byzantine} gradient descent.
\newblock \emph{arXiv preprint arXiv:1705.05491}, 2017.

\bibitem[Chen et~al.(2018{\natexlab{b}})Chen, Chi, Fan, and
  Ma]{chen2018gradient}
Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma.
\newblock Gradient descent with random initialization: Fast global convergence
  for nonconvex phase retrieval.
\newblock \emph{arXiv preprint arXiv:1803.07726}, 2018{\natexlab{b}}.

\bibitem[Curtis et~al.(2017)Curtis, Robinson, and Samadi]{curtis2017trust}
Frank~E Curtis, Daniel~P Robinson, and Mohammadreza Samadi.
\newblock A trust region algorithm with a worst-case iteration complexity of
  $\epsilon^{-3/2}$ for nonconvex optimization.
\newblock \emph{Mathematical Programming}, 162\penalty0 (1-2):\penalty0 1--32,
  2017.

\bibitem[Damaskinos et~al.(2018)Damaskinos, Mhamdi, Guerraoui, Patra, and
  Taziki]{damaskinos2018asynchronous}
Georgios Damaskinos, El~Mahdi~El Mhamdi, Rachid Guerraoui, Rhicheek Patra, and
  Mahsa Taziki.
\newblock Asynchronous {Byzantine} machine learning.
\newblock \emph{arXiv preprint arXiv:1802.07928}, 2018.

\bibitem[Devolder et~al.(2014)Devolder, Glineur, and
  Nesterov]{devolder2014inexact}
Olivier Devolder, Fran{\c{c}}ois Glineur, and Yurii Nesterov.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock \emph{Mathematical Programming}, 146\penalty0 (1-2):\penalty0 37--75,
  2014.

\bibitem[Diakonikolas et~al.(2016)Diakonikolas, Kamath, Kane, Li, Moitra, and
  Stewart]{diakonikolas2016robust}
Ilias Diakonikolas, Gautam Kamath, Daniel~M Kane, Jerry Li, Ankur Moitra, and
  Alistair Stewart.
\newblock Robust estimators in high dimensions without the computational
  intractability.
\newblock In \emph{Foundations of Computer Science (FOCS), 2016 IEEE 57th
  Annual Symposium on}, pages 655--664. IEEE, 2016.

\bibitem[Diakonikolas et~al.(2017)Diakonikolas, Kamath, Kane, Li, Moitra, and
  Stewart]{diakonikolas2017being}
Ilias Diakonikolas, Gautam Kamath, Daniel~M Kane, Jerry Li, Ankur Moitra, and
  Alistair Stewart.
\newblock Being robust (in high dimensions) can be practical.
\newblock \emph{arXiv preprint arXiv:1703.00893}, 2017.

\bibitem[Diakonikolas et~al.(2018)Diakonikolas, Kamath, Kane, Li, Steinhardt,
  and Stewart]{diakonikolas2018sever}
Ilias Diakonikolas, Gautam Kamath, Daniel~M Kane, Jerry Li, Jacob Steinhardt,
  and Alistair Stewart.
\newblock Sever: A robust meta-algorithm for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1803.02815}, 2018.

\bibitem[Du et~al.(2017)Du, Jin, Lee, Jordan, Singh, and
  Poczos]{du2017gradient}
Simon~S Du, Chi Jin, Jason~D Lee, Michael~I Jordan, Aarti Singh, and Barnabas
  Poczos.
\newblock Gradient descent can take exponential time to escape saddle points.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1067--1077, 2017.

\bibitem[Feng et~al.(2014)Feng, Xu, and Mannor]{feng2014distributed}
Jiashi Feng, Huan Xu, and Shie Mannor.
\newblock Distributed robust learning.
\newblock \emph{arXiv preprint arXiv:1409.5937}, 2014.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{ge2015escaping}
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.
\newblock Escaping from saddle pointsâ€”online stochastic gradient for tensor
  decomposition.
\newblock In \emph{COLT}, pages 797--842, 2015.

\bibitem[Ge et~al.(2016)Ge, Lee, and Ma]{ge2016matrix}
Rong Ge, Jason~D Lee, and Tengyu Ma.
\newblock Matrix completion has no spurious local minimum.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2973--2981, 2016.

\bibitem[Ge et~al.(2017)Ge, Jin, and Zheng]{ge2017no}
Rong Ge, Chi Jin, and Yi~Zheng.
\newblock No spurious local minima in nonconvex low rank problems: A unified
  geometric analysis.
\newblock \emph{arXiv preprint arXiv:1704.00708}, 2017.

\bibitem[Huber(2011)]{huber2011robust}
Peter~J Huber.
\newblock Robust statistics.
\newblock In \emph{International Encyclopedia of Statistical Science}, pages
  1248--1251. Springer, 2011.

\bibitem[Jerrum et~al.(1986)Jerrum, Valiant, and Vazirani]{jerrum1986random}
Mark~R Jerrum, Leslie~G Valiant, and Vijay~V Vazirani.
\newblock Random generation of combinatorial structures from a uniform
  distribution.
\newblock \emph{Theoretical Computer Science}, 43:\penalty0 169--188, 1986.

\bibitem[Jin et~al.(2017{\natexlab{a}})Jin, Ge, Netrapalli, Kakade, and
  Jordan]{jin2017escape}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M Kakade, and Michael~I Jordan.
\newblock How to escape saddle points efficiently.
\newblock \emph{arXiv preprint arXiv:1703.00887}, 2017{\natexlab{a}}.

\bibitem[Jin et~al.(2017{\natexlab{b}})Jin, Netrapalli, and
  Jordan]{jin2017accelerated}
Chi Jin, Praneeth Netrapalli, and Michael~I Jordan.
\newblock Accelerated gradient descent escapes saddle points faster than
  gradient descent.
\newblock \emph{arXiv preprint arXiv:1711.10456}, 2017{\natexlab{b}}.

\bibitem[Jin et~al.(2018)Jin, Liu, Ge, and Jordan]{jin2018minimizing}
Chi Jin, Lydia~T Liu, Rong Ge, and Michael~I Jordan.
\newblock Minimizing nonconvex population risk from rough empirical risk.
\newblock \emph{arXiv preprint arXiv:1803.09357}, 2018.

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  586--594, 2016.

\bibitem[Klivans et~al.(2018)Klivans, Kothari, and Meka]{klivans2018efficient}
Adam Klivans, Pravesh~K Kothari, and Raghu Meka.
\newblock Efficient algorithms for outlier-robust regression.
\newblock \emph{arXiv preprint arXiv:1803.03241}, 2018.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2015)Kone{\v{c}}n{\`y}, McMahan, and
  Ramage]{konevcny2015federated}
Jakub Kone{\v{c}}n{\`y}, Brendan McMahan, and Daniel Ramage.
\newblock Federated optimization: Distributed optimization beyond the
  datacenter.
\newblock \emph{arXiv preprint arXiv:1511.03575}, 2015.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Ramage, and
  Richt{\'a}rik]{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Daniel Ramage, and Peter
  Richt{\'a}rik.
\newblock Federated optimization: distributed machine learning for on-device
  intelligence.
\newblock \emph{arXiv preprint arXiv:1610.02527}, 2016.

\bibitem[Lai et~al.(2016)Lai, Rao, and Vempala]{lai2016agnostic}
Kevin~A Lai, Anup~B Rao, and Santosh Vempala.
\newblock Agnostic estimation of mean and covariance.
\newblock In \emph{Foundations of Computer Science (FOCS), 2016 IEEE 57th
  Annual Symposium on}, pages 665--674. IEEE, 2016.

\bibitem[Lamport et~al.(1982)Lamport, Shostak, and Pease]{lamport1982byzantine}
Leslie Lamport, Robert Shostak, and Marshall Pease.
\newblock The {Byzantine} generals problem.
\newblock \emph{ACM Transactions on Programming Languages and Systems
  (TOPLAS)}, 4\penalty0 (3):\penalty0 382--401, 1982.

\bibitem[Lee et~al.(2015)Lee, Lin, Ma, and Yang]{lee2015distributed}
Jason~D Lee, Qihang Lin, Tengyu Ma, and Tianbao Yang.
\newblock Distributed stochastic variance reduced gradient methods and a lower
  bound for communication complexity.
\newblock \emph{arXiv preprint arXiv:1507.07595}, 2015.

\bibitem[Lee et~al.(2016)Lee, Simchowitz, Jordan, and Recht]{lee2016converge}
Jason~D Lee, Max Simchowitz, Michael~I Jordan, and Benjamin Recht.
\newblock Gradient descent converges to minimizers.
\newblock \emph{arXiv preprint arXiv:1602.04915}, 2016.

\bibitem[Lee et~al.(2017)Lee, Panageas, Piliouras, Simchowitz, Jordan, and
  Recht]{lee2017first}
Jason~D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz, Michael~I
  Jordan, and Benjamin Recht.
\newblock First-order methods almost always avoid saddle points.
\newblock \emph{arXiv preprint arXiv:1710.07406}, 2017.

\bibitem[Levy(2016)]{levy2016power}
Kfir~Y Levy.
\newblock The power of normalization: Faster evasion of saddle points.
\newblock \emph{arXiv preprint arXiv:1611.04831}, 2016.

\bibitem[Li(2017)]{li2017robust}
Jerry Li.
\newblock Robust sparse estimation tasks in high dimensions.
\newblock \emph{arXiv preprint arXiv:1702.05860}, 2017.

\bibitem[Liu et~al.(2018)Liu, Shen, Li, and Caramanis]{liu2018regression}
Liu Liu, Yanyao Shen, Tianyang Li, and Constantine Caramanis.
\newblock High dimensional robust sparse regression.
\newblock \emph{arXiv preprint arXiv:1805.11643}, 2018.

\bibitem[Lugosi and Mendelson(2016)]{lugosi2016risk}
Gabor Lugosi and Shahar Mendelson.
\newblock Risk minimization by median-of-means tournaments.
\newblock \emph{arXiv preprint arXiv:1608.00757}, 2016.

\bibitem[Lynch(1996)]{lynch1996distributed}
Nancy~A. Lynch.
\newblock \emph{Distributed Algorithms}.
\newblock Elsevier, 1996.

\bibitem[McMahan and Ramage(2017)]{mcmahan2017federated}
Brendan McMahan and Daniel Ramage.
\newblock Federated learning: Collaborative machine learning without
  centralized training data.
\newblock
  \url{https://research.googleblog.com/2017/04/federated-learning-collaborative.html},
  2017.

\bibitem[Minsker and Strawn(2017)]{minsker2017distributed}
Stanislav Minsker and Nate Strawn.
\newblock Distributed statistical estimation and rates of convergence in normal
  approximation.
\newblock \emph{arXiv preprint arXiv:1704.02658}, 2017.

\bibitem[Minsker et~al.(2015)]{minsker2015geometric}
Stanislav Minsker et~al.
\newblock Geometric median and robust estimation in banach spaces.
\newblock \emph{Bernoulli}, 21\penalty0 (4):\penalty0 2308--2335, 2015.

\bibitem[Nemirovskii et~al.(1983)Nemirovskii, Yudin, and
  Dawson]{nemirovskii1983problem}
Arkadii Nemirovskii, David~Borisovich Yudin, and Edgar~Ronald Dawson.
\newblock \emph{Problem complexity and method efficiency in optimization}.
\newblock Wiley, 1983.

\bibitem[Nesterov(1998)]{nesterov1998introductory}
Yurii Nesterov.
\newblock Introductory lectures on convex programming volume i: Basic course.
\newblock \emph{Lecture notes}, 1998.

\bibitem[Nesterov and Polyak(2006)]{nesterov2006cubic}
Yurii Nesterov and Boris~T Polyak.
\newblock Cubic regularization of newton method and its global performance.
\newblock \emph{Mathematical Programming}, 108\penalty0 (1):\penalty0 177--205,
  2006.

\bibitem[Prasad et~al.(2018)Prasad, Suggala, Balakrishnan, and
  Ravikumar]{prasad2018robust}
Adarsh Prasad, Arun~Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar.
\newblock Robust estimation via robust gradient estimation.
\newblock \emph{arXiv preprint arXiv:1802.06485}, 2018.

\bibitem[Royer and Wright(2018)]{royer2018complexity}
Cl{\'e}ment~W Royer and Stephen~J Wright.
\newblock Complexity analysis of second-order line-search algorithms for smooth
  nonconvex optimization.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (2):\penalty0
  1448--1477, 2018.

\bibitem[Royer et~al.(2018)Royer, O'Neill, and Wright]{royer2018newton}
Cl{\'e}ment~W Royer, Michael O'Neill, and Stephen~J Wright.
\newblock A newton-cg algorithm with complexity guarantees for smooth
  unconstrained optimization.
\newblock \emph{arXiv preprint arXiv:1803.02924}, 2018.

\bibitem[Shamir et~al.(2014)Shamir, Srebro, and Zhang]{shamir2014communication}
Ohad Shamir, Nati Srebro, and Tong Zhang.
\newblock Communication-efficient distributed optimization using an approximate
  newton-type method.
\newblock In \emph{International Conference on Machine Learning}, pages
  1000--1008, 2014.

\bibitem[Soudry and Carmon(2016)]{soudry2016no}
Daniel Soudry and Yair Carmon.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock \emph{arXiv preprint arXiv:1605.08361}, 2016.

\bibitem[Steinhardt et~al.(2017)Steinhardt, Charikar, and
  Valiant]{steinhardt2017resilience}
Jacob Steinhardt, Moses Charikar, and Gregory Valiant.
\newblock Resilience: A criterion for learning in the presence of arbitrary
  outliers.
\newblock \emph{arXiv preprint arXiv:1703.04940}, 2017.

\bibitem[Su and Vaidya(2016{\natexlab{a}})]{su2016fault}
Lili Su and Nitin~H Vaidya.
\newblock Fault-tolerant multi-agent optimization: optimal iterative
  distributed algorithms.
\newblock In \emph{Proceedings of the 2016 ACM Symposium on Principles of
  Distributed Computing}, pages 425--434. ACM, 2016{\natexlab{a}}.

\bibitem[Su and Vaidya(2016{\natexlab{b}})]{su2016non}
Lili Su and Nitin~H Vaidya.
\newblock Non-{Bayesian} learning in the presence of {Byzantine} agents.
\newblock In \emph{International Symposium on Distributed Computing}, pages
  414--427. Springer, 2016{\natexlab{b}}.

\bibitem[Su and Xu(2018)]{su2018securing}
Lili Su and Jiaming Xu.
\newblock Securing distributed machine learning in high dimensions.
\newblock \emph{arXiv preprint arXiv:1804.10140}, 2018.

\bibitem[Sun et~al.(2015)Sun, Qu, and Wright]{sun2015complete}
J.~Sun, Q.~Qu, and J.~Wright.
\newblock Complete dictionary recovery using nonconvex optimization.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, pages 2351--2360, 2015.

\bibitem[Tu et~al.(2015)Tu, Boczar, Simchowitz, Soltanolkotabi, and
  Recht]{tu2015low}
Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Benjamin
  Recht.
\newblock Low-rank solutions of linear matrix equations via procrustes flow.
\newblock \emph{arXiv preprint arXiv:1507.03566}, 2015.

\bibitem[Vershynin(2010)]{vershynin2010introduction}
Roman Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint arXiv:1011.3027}, 2010.

\bibitem[Xie et~al.(2018)Xie, Koyejo, and Gupta]{xie2018generalized}
Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta.
\newblock Generalized {B}yzantine-tolerant {SGD}.
\newblock \emph{arXiv preprint arXiv:1802.10116}, 2018.

\bibitem[Xu and Yang(2017)]{xu2017first}
Yi~Xu and Tianbao Yang.
\newblock First-order stochastic algorithms for escaping from saddle points in
  almost linear time.
\newblock \emph{arXiv preprint arXiv:1711.01944}, 2017.

\bibitem[Yin et~al.(2018{\natexlab{a}})Yin, Chen, Kannan, and
  Bartlett]{yin2018byzantine}
Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett.
\newblock {B}yzantine-robust distributed learning: Towards optimal statistical
  rates.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pages 5650--5659, 2018{\natexlab{a}}.

\bibitem[Yin et~al.(2018{\natexlab{b}})Yin, Pananjady, Lam, Papailiopoulos,
  Ramchandran, and Bartlett]{yin2017gradient}
Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan
  Ramchandran, and Peter Bartlett.
\newblock Gradient diversity: a key ingredient for scalable distributed
  learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1998--2007, 2018{\natexlab{b}}.

\bibitem[Zhang et~al.(2016)Zhang, Chi, and Liang]{zhang2016provable}
Huishuai Zhang, Yuejie Chi, and Yingbin Liang.
\newblock Provable non-convex phase retrieval with outliers: Median-truncated
  wirtinger flow.
\newblock In \emph{International Conference on Machine Learning}, pages
  1022--1031, 2016.

\bibitem[Zhao et~al.(2015)Zhao, Wang, and Liu]{zhao2015nonconvex}
Tuo Zhao, Zhaoran Wang, and Han Liu.
\newblock A nonconvex optimization framework for low rank matrix estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  559--567, 2015.

\end{thebibliography}
