\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, and Yang]{agarwal2020model}
Alekh Agarwal, Sham Kakade, and Lin~F Yang.
\newblock Model-based reinforcement learning with a generative model is minimax
  optimal.
\newblock In \emph{Conference on Learning Theory}, 2020.

\bibitem[Ahn et~al.(2020)Ahn, Zhu, Hartikainen, Ponte, Gupta, Levine, and
  Kumar]{ahn2020robel}
Michael Ahn, Henry Zhu, Kristian Hartikainen, Hugo Ponte, Abhishek Gupta,
  Sergey Levine, and Vikash Kumar.
\newblock Robel: Robotics benchmarks for learning with low-cost robots.
\newblock In \emph{Conference on robot learning}, 2020.

\bibitem[Amos et~al.(2018)Amos, Jimenez, Sacks, Boots, and
  Kolter]{amos2018differentiable}
Brandon Amos, Ivan Jimenez, Jacob Sacks, Byron Boots, and J~Zico Kolter.
\newblock Differentiable mpc for end-to-end planning and control.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Asadi et~al.(2018)Asadi, Misra, and Littman]{asadi2018lipschitz}
Kavosh Asadi, Dipendra Misra, and Michael Littman.
\newblock Lipschitz continuity in model-based reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Azar et~al.(2013)Azar, Munos, and Kappen]{azar2013minimax}
Mohammad~Gheshlaghi Azar, R{\'e}mi Munos, and Hilbert Kappen.
\newblock Minimax pac bounds on the sample complexity of reinforcement learning
  with a generative model.
\newblock \emph{Machine Learning}, 91\penalty0 (3):\penalty0 325--349, 2013.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Buckman et~al.()Buckman, Hafner, Tucker, Brevdo, and
  Lee]{buckman2018sample}
Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee.
\newblock Sample-efficient reinforcement learning with stochastic ensemble
  value expansion.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[Chua et~al.()Chua, Calandra, McAllister, and Levine]{chua2018deep}
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[Deisenroth et~al.(2011)Deisenroth, Rasmussen, and
  Fox]{deisenroth2011learning}
Marc~Peter Deisenroth, Carl~Edward Rasmussen, and Dieter Fox.
\newblock Learning to control a low-cost manipulator using data-efficient
  reinforcement learning.
\newblock \emph{Robotics: Science and Systems VII}, 7:\penalty0 57--64, 2011.

\bibitem[Efroni et~al.()Efroni, Merlis, Ghavamzadeh, and
  Mannor]{efroni2019tight}
Yonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, and Shie Mannor.
\newblock Tight regret bounds for model-based reinforcement learning with
  greedy policies.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[Fan and Ming(2021)]{fan2021model}
Ying Fan and Yifei Ming.
\newblock Model-based reinforcement learning for continuous control with
  posterior sampling.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Farahmand et~al.(2017)Farahmand, Barreto, and
  Nikovski]{farahmand2017value}
Amir-massoud Farahmand, Andre Barreto, and Daniel Nikovski.
\newblock Value-aware loss function for model-based reinforcement learning.
\newblock In \emph{Artificial Intelligence and Statistics}, 2017.

\bibitem[Feinberg et~al.(2018)Feinberg, Wan, Stoica, Jordan, Gonzalez, and
  Levine]{feinberg2018model}
Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael~I Jordan, Joseph~E Gonzalez,
  and Sergey Levine.
\newblock Model-based value estimation for efficient model-free reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1803.00101}, 2018.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{fujimoto2018addressing}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Gal et~al.(2016)Gal, McAllister, and Rasmussen]{gal2016improving}
Yarin Gal, Rowan McAllister, and Carl~Edward Rasmussen.
\newblock Improving pilco with bayesian neural network dynamics models.
\newblock In \emph{ICML Workshop on Data-EfÔ¨Åcient Machine Learning Workshop},
  2016.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Heemels et~al.(2012)Heemels, Johansson, and
  Tabuada]{heemels2012introduction}
Wilhelmus~PMH Heemels, Karl~Henrik Johansson, and Paulo Tabuada.
\newblock An introduction to event-triggered and self-triggered control.
\newblock In \emph{2012 ieee 51st ieee conference on decision and control
  (cdc)}, pages 3270--3285. IEEE, 2012.

\bibitem[Hester et~al.(2012)Hester, Quinlan, and Stone]{hester2012rtmba}
Todd Hester, Michael Quinlan, and Peter Stone.
\newblock Rtmba: A real-time model-based reinforcement learning architecture
  for robot control.
\newblock In \emph{International Conference on Robotics and Automation}, 2012.

\bibitem[James et~al.(2019)James, Freese, and Davison]{james2019pyrep}
Stephen James, Marc Freese, and Andrew~J. Davison.
\newblock Pyrep: Bringing v-rep to deep robot learning.
\newblock \emph{arXiv preprint arXiv:1906.11176}, 2019.

\bibitem[Janner et~al.(2019)Janner, Fu, Zhang, and Levine]{janner2019trust}
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.
\newblock When to trust your model: Model-based policy optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Kaelbling et~al.(1996)Kaelbling, Littman, and
  Moore]{kaelbling1996reinforcement}
Leslie~Pack Kaelbling, Michael~L Littman, and Andrew~W Moore.
\newblock Reinforcement learning: A survey.
\newblock \emph{Journal of artificial intelligence research}, 4:\penalty0
  237--285, 1996.

\bibitem[Kakade and Langford(2002)]{kakade2002approximately}
Sham Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2002.

\bibitem[Kakade et~al.(2020)Kakade, Krishnamurthy, Lowrey, Ohnishi, and
  Sun]{kakade2020information}
Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun.
\newblock Information theoretic regret bounds for online nonlinear control.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Kalweit and Boedecker(2017)]{kalweit2017uncertainty}
Gabriel Kalweit and Joschka Boedecker.
\newblock Uncertainty-driven imagination for continuous deep reinforcement
  learning.
\newblock In \emph{Conference on Robot Learning}, 2017.

\bibitem[Kearns et~al.(2002)Kearns, Mansour, and Ng]{kearns2002sparse}
Michael Kearns, Yishay Mansour, and Andrew~Y Ng.
\newblock A sparse sampling algorithm for near-optimal planning in large markov
  decision processes.
\newblock \emph{Machine learning}, 49\penalty0 (2):\penalty0 193--208, 2002.

\bibitem[Ko and Fox(2009)]{ko2009gp}
Jonathan Ko and Dieter Fox.
\newblock Gp-bayesfilters: Bayesian filtering using gaussian process prediction
  and observation models.
\newblock \emph{Autonomous Robots}, 27\penalty0 (1):\penalty0 75--90, 2009.

\bibitem[Kurutach et~al.(2018)Kurutach, Clavera, Duan, Tamar, and
  Abbeel]{kurutach2018model}
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel.
\newblock Model-ensemble trust-region policy optimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lai et~al.(2021)Lai, Shen, Zhang, Huang, Zhang, Tang, Yu, and
  Li]{lai2021effective}
Hang Lai, Jian Shen, Weinan Zhang, Yimin Huang, Xing Zhang, Ruiming Tang, Yong
  Yu, and Zhenguo Li.
\newblock On effective scheduling of model-based reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Levine and Koltun(2013)]{levine2013guided}
Sergey Levine and Vladlen Koltun.
\newblock Guided policy search.
\newblock In \emph{International Conference on Machine Learning}, 2013.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Li et~al.(2020)Li, Wei, Chi, Gu, and Chen]{li2020breaking}
Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen.
\newblock Breaking the sample size barrier in model-based reinforcement
  learning with a generative model.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Li and Shi(2014)]{li2014event}
Huiping Li and Yang Shi.
\newblock Event-triggered robust model predictive control of continuous-time
  nonlinear systems.
\newblock \emph{Automatica}, 50\penalty0 (5):\penalty0 1507--1513, 2014.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2016continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Luo et~al.(2018)Luo, Xu, Li, Tian, Darrell, and
  Ma]{luo2018algorithmic}
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu
  Ma.
\newblock Algorithmic framework for model-based deep reinforcement learning
  with theoretical guarantees.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{atari2013}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2013.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Morimoto and Atkeson(2002)]{morimoto2002minimax}
Jun Morimoto and Christopher Atkeson.
\newblock Minimax differential dynamic programming: An application to robust
  biped walking.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2002.

\bibitem[Nagabandi et~al.(2018)Nagabandi, Kahn, Fearing, and
  Levine]{nagabandi2018neural}
Anusha Nagabandi, Gregory Kahn, Ronald~S Fearing, and Sergey Levine.
\newblock Neural network dynamics for model-based deep reinforcement learning
  with model-free fine-tuning.
\newblock In \emph{International Conference on Robotics and Automation}, 2018.

\bibitem[Pan et~al.()Pan, He, Tu, and He]{pan2020trust}
Feiyang Pan, Jia He, Dandan Tu, and Qing He.
\newblock Trust the model when it is confident: Masked model-based
  actor-critic.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[Pineda et~al.(2021)Pineda, Amos, Zhang, Lambert, and
  Calandra]{Pineda2021MBRL}
Luis Pineda, Brandon Amos, Amy Zhang, Nathan~O. Lambert, and Roberto Calandra.
\newblock Mbrl-lib: A modular library for model-based reinforcement learning.
\newblock \emph{Arxiv}, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.10159}.

\bibitem[Polydoros and Nalpantidis(2017)]{polydoros2017survey}
Athanasios~S Polydoros and Lazaros Nalpantidis.
\newblock Survey of model-based reinforcement learning: Applications on
  robotics.
\newblock \emph{Journal of Intelligent \& Robotic Systems}, 86\penalty0
  (2):\penalty0 153--173, 2017.

\bibitem[pranz24(2018)]{saclib}
pranz24.
\newblock pytorch-soft-actor-critic.
\newblock \url{https://github.com/pranz24/pytorch-soft-actor-critic}, 2018.

\bibitem[Puterman(2014)]{puterman2014markov}
Martin~L Puterman.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Schulman et~al.(2015{\natexlab{a}})Schulman, Levine, Abbeel, Jordan,
  and Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning},
  2015{\natexlab{a}}.

\bibitem[Schulman et~al.(2015{\natexlab{b}})Schulman, Moritz, Levine, Jordan,
  and Abbeel]{schulman2015high}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter
  Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{arXiv preprint arXiv:1506.02438}, 2015{\natexlab{b}}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sidford et~al.(2018)Sidford, Wang, Wu, Yang, and Ye]{sidford2018near}
Aaron Sidford, Mengdi Wang, Xian Wu, Lin~F Yang, and Yinyu Ye.
\newblock Near-optimal time and sample complexities for solving discounted
  markov decision process with a generative model.
\newblock \emph{arXiv preprint arXiv:1806.01492}, 2018.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Sun et~al.()Sun, Gordon, Boots, and Bagnell]{sun2018dual}
Wen Sun, Geoffrey~J Gordon, Byron Boots, and J~Bagnell.
\newblock Dual policy iteration.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[Sutton(1990)]{sutton1990integrated}
Richard~S Sutton.
\newblock Integrated architecture for learning, planning, and reacting based on
  approximating dynamic programming.
\newblock In \emph{International Conference on Machine Learning}, 1990.

\bibitem[Sutton(1991{\natexlab{a}})]{sutton1991dyna}
Richard~S Sutton.
\newblock Dyna, an integrated architecture for learning, planning, and
  reacting.
\newblock \emph{ACM Sigart Bulletin}, 2\penalty0 (4):\penalty0 160--163,
  1991{\natexlab{a}}.

\bibitem[Sutton(1991{\natexlab{b}})]{sutton1991planning}
Richard~S Sutton.
\newblock Planning by incremental dynamic programming.
\newblock In \emph{International Conference on Machine Learning},
  1991{\natexlab{b}}.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, Cambridge, MA, 2018.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{International Conference on Intelligent Robots and Systems},
  2012.

\bibitem[Wang et~al.(2019)Wang, Bao, Clavera, Hoang, Wen, Langlois, Zhang,
  Zhang, Abbeel, and Ba]{wang2019benchmarking}
Tingwu Wang, Xuchan Bao, Ignasi Clavera, Jerrick Hoang, Yeming Wen, Eric
  Langlois, Shunshi Zhang, Guodong Zhang, Pieter Abbeel, and Jimmy Ba.
\newblock Benchmarking model-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1907.02057}, 2019.

\bibitem[Weissman et~al.(2003)Weissman, Ordentlich, Seroussi, Verdu, and
  Weinberger]{weissman2003inequalities}
Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo~J
  Weinberger.
\newblock Inequalities for the l1 deviation of the empirical distribution.
\newblock \emph{Hewlett-Packard Labs, Tech. Rep}, 2003.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James~Y Zou, Sergey
  Levine, Chelsea Finn, and Tengyu Ma.
\newblock Mopo: Model-based offline policy optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\end{thebibliography}
