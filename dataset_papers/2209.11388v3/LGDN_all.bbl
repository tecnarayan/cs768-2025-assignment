\begin{thebibliography}{10}

\bibitem{AmraniBRB21}
Elad Amrani, Rami Ben{-}Ari, Daniel Rotman, and Alex~M. Bronstein.
\newblock Noise estimation using density estimation for self-supervised
  multimodal learning.
\newblock In {\em AAAI}, pages 6644--6652, 2021.

\bibitem{bain2021frozen}
Max Bain, Arsha Nagrani, G{\"u}l Varol, and Andrew Zisserman.
\newblock Frozen in time: A joint video and image encoder for end-to-end
  retrieval.
\newblock In {\em ICCV}, pages 1728--1738, 2021.

\bibitem{changpinyo2021conceptual}
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
\newblock {Conceptual 12M}: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In {\em CVPR}, pages 3558--3568, 2021.

\bibitem{chen2011collecting}
David Chen and William~B Dolan.
\newblock Collecting highly parallel data for paraphrase evaluation.
\newblock In {\em ACL}, pages 190--200, 2011.

\bibitem{chen2020fine}
Shizhe Chen, Yida Zhao, Qin Jin, and Qi~Wu.
\newblock Fine-grained video-text retrieval with hierarchical graph reasoning.
\newblock In {\em CVPR}, pages 10635--10644, 2020.

\bibitem{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em ICML}, pages 1597--1607, 2020.

\bibitem{jacob2019bert}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL-HLT}, pages 4171--4186, 2019.

\bibitem{dong2018predicting}
Jianfeng Dong, Xirong Li, and Cees~GM Snoek.
\newblock Predicting visual features from text for image and video caption
  retrieval.
\newblock {\em IEEE Transactions on Multimedia}, 20(12):3377--3388, 2018.

\bibitem{alexey2021vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em ICLR}, 2021.

\bibitem{faghri2017vse++}
Fartash Faghri, David~J. Fleet, Jamie~Ryan Kiros, and Sanja Fidler.
\newblock {VSE++:} improving visual-semantic embeddings with hard negatives.
\newblock In {\em BMVC}, page~12, 2018.

\bibitem{FanZZW0H19}
Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, and Heng
  Huang.
\newblock Heterogeneous memory enhanced multimodal attention model for video
  question answering.
\newblock In {\em CVPR}, pages 1999--2007, 2019.

\bibitem{gabeur2020multi}
Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid.
\newblock Multi-modal transformer for video retrieval.
\newblock In {\em ECCV}, pages 214--229, 2020.

\bibitem{he2021improving}
Feng He, Qi~Wang, Zhifan Feng, Wenbin Jiang, Yajuan Lu, Yong Zhu, and Xiao Tan.
\newblock Improving video retrieval by adaptive margin.
\newblock In {\em SIGIR}, pages 1359--1368, 2021.

\bibitem{he2020moco}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross~B. Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em CVPR}, pages 9726--9735, 2020.

\bibitem{anne2017localizing}
Anne~Lisa Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell,
  and Bryan Russell.
\newblock Localizing moments in video with natural language.
\newblock In {\em ICCV}, pages 5804--5813, 2017.

\bibitem{hermans2017defense}
Alexander Hermans, Lucas Beyer, and Bastian Leibe.
\newblock In defense of the triplet loss for person re-identification.
\newblock {\em arXiv preprint arXiv:1703.07737}, 2017.

\bibitem{huo2021wenlan}
Yuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu, Yizhao Gao, Guoxing Yang,
  Jingyuan Wen, Heng Zhang, Baogui Xu, Weihao Zheng, et~al.
\newblock {WenLan}: Bridging vision and language by large-scale multi-modal
  pre-training.
\newblock {\em arXiv preprint arXiv:2103.06561}, 2021.

\bibitem{jia2021scaling}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc~V
  Le, Yunhsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In {\em ICML}, pages 4904--4916, 2021.

\bibitem{jin2021hierarchical}
Weike Jin, Zhou Zhao, Pengcheng Zhang, Jieming Zhu, Xiuqiang He, and Yueting
  Zhuang.
\newblock Hierarchical cross-modal graph consistency learning for video-text
  retrieval.
\newblock In {\em SIGIR}, pages 1114--1124, 2021.

\bibitem{kim2021vilt}
Wonjae Kim, Bokyung Son, and Ildoo Kim.
\newblock {ViLT}: Vision-and-language transformer without convolution or region
  supervision.
\newblock In {\em ICML}, pages 5583--5594, 2021.

\bibitem{kiros2014unifying}
Ryan Kiros, Ruslan Salakhutdinov, and Richard~S. Zemel.
\newblock Unifying visual-semantic embeddings with multimodal neural language
  models.
\newblock {\em arXiv preprint arXiv:1411.2539}, 2014.

\bibitem{krishna2017visual}
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
  Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David~A Shamma, et~al.
\newblock Visual genome: Connecting language and vision using crowdsourced
  dense image annotations.
\newblock {\em IJCV}, 123(1):32--73, 2017.

\bibitem{LeLV020}
Thao~Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran.
\newblock Hierarchical conditional relation networks for video question
  answering.
\newblock In {\em CVPR}, pages 9972--9981, 2020.

\bibitem{lei2021less}
Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara~L Berg, Mohit Bansal, and
  Jingjing Liu.
\newblock Less is more: {ClipBERT} for video-and-language learning via sparse
  sampling.
\newblock In {\em CVPR}, pages 7331--7341, 2021.

\bibitem{li2021albef}
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong,
  and Steven Chu~Hong Hoi.
\newblock Align before fuse: Vision and language representation learning with
  momentum distillation.
\newblock In {\em NeurIPS}, pages 9694--9705, 2021.

\bibitem{li2020hero}
Linjie Li, Yen-Chun Chen, Yu~Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu.
\newblock {HERO}: Hierarchical encoder for video+ language omni-representation
  pre-training.
\newblock {\em EMNLP}, pages 2046--2065, 2020.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C.~Lawrence Zitnick.
\newblock Microsoft {COCO}: Common objects in context.
\newblock In {\em ECCV}, pages 740--755, 2014.

\bibitem{liu2021hit}
Song Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui Ding, and Zhongyuan
  Wang.
\newblock {HiT}: Hierarchical transformer with momentum contrast for video-text
  retrieval.
\newblock In {\em ICCV}, pages 11915--11925, 2021.

\bibitem{liu2019use}
Yang Liu, Samuel Albanie, Arsha Nagrani, and Andrew Zisserman.
\newblock Use what you have: Video retrieval using representations from
  collaborative experts.
\newblock In {\em BMVC}, page 279, 2019.

\bibitem{LoshchilovH19}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em ICLR}, 2019.

\bibitem{lu2022cots}
Haoyu Lu, Nanyi Fei, Yuqi Huo, Yizhao Gao, Zhiwu Lu, and Ji-Rong Wen.
\newblock {COTS}: Collaborative two-stream vision-language pre-training model
  for cross-modal retrieval.
\newblock In {\em CVPR}, pages 15692--15701, 2022.

\bibitem{luo2020univilm}
Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Xilin
  Chen, and Ming Zhou.
\newblock {UniVL}: A unified video and language pre-training model for
  multimodal understanding and generation.
\newblock {\em arXiv preprint arXiv:2002.06353}, 2020.

\bibitem{luo2021clip4clip}
Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li.
\newblock {CLIP4Clip}: An empirical study of clip for end to end video clip
  retrieval.
\newblock {\em arXiv preprint arXiv:2104.08860}, 2021.

\bibitem{miech2020end}
Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic,
  and Andrew Zisserman.
\newblock End-to-end learning of visual representations from uncurated
  instructional videos.
\newblock In {\em CVPR}, pages 9879--9889, 2020.

\bibitem{miech2019howto100m}
Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan
  Laptev, and Josef Sivic.
\newblock {HowTo100M}: Learning a text-video embedding by watching hundred
  million narrated video clips.
\newblock In {\em ICCV}, pages 2630--2640, 2019.

\bibitem{mithun2018learning}
Niluthpol~Chowdhury Mithun, Juncheng Li, Florian Metze, and Amit~K
  Roy-Chowdhury.
\newblock Learning joint embedding with multimodal cues for cross-modal
  video-text retrieval.
\newblock In {\em ICMR}, pages 19--27, 2018.

\bibitem{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv preprint arXiv:1807.03748}, 2018.

\bibitem{ordonez2011im2text}
Vicente Ordonez, Girish Kulkarni, and Tamara Berg.
\newblock {Im2Text}: Describing images using 1 million captioned photographs.
\newblock In {\em NeurIPS}, pages 1143--1151, 2011.

\bibitem{patrick2020support}
Mandela Patrick, Po{-}Yao Huang, Yuki~Markus Asano, Florian Metze, Alexander~G.
  Hauptmann, Jo{\~{a}}o~F. Henriques, and Andrea Vedaldi.
\newblock Support-set bottlenecks for video-text representation learning.
\newblock In {\em ICLR}, 2021.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em ICML}, pages 8748--8763, 2021.

\bibitem{sharma2018conceptual}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In {\em ACL}, pages 2556--2565, 2018.

\bibitem{TanB19}
Hao Tan and Mohit Bansal.
\newblock {LXMERT:} learning cross-modality encoder representations from
  transformers.
\newblock In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors,
  {\em EMNLP-IJCNLP}, pages 5099--5110, 2019.

\bibitem{vaswani2017transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NeurIPS}, pages 5998--6008, 2017.

\bibitem{venugopalan2014translating}
Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond~J.
  Mooney, and Kate Saenko.
\newblock Translating videos to natural language using deep recurrent neural
  networks.
\newblock In {\em NAACL-HLT}, pages 1494--1504, 2015.

\bibitem{wang2021t2vlad}
Xiaohan Wang, Linchao Zhu, and Yi~Yang.
\newblock {T}2{VLAD}: global-local sequence alignment for text-video retrieval.
\newblock In {\em CVPR}, pages 5079--5088, 2021.

\bibitem{wang2021vatex}
Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan{-}Fang Wang, and William~Yang
  Wang.
\newblock {VaTeX}: {A} large-scale, high-quality multilingual dataset for
  video-and-language research.
\newblock In {\em ICCV}, pages 4580--4590, 2019.

\bibitem{wu2018unsupervised}
Zhirong Wu, Yuanjun Xiong, Stella~X Yu, and Dahua Lin.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In {\em CVPR}, pages 3733--3742, 2018.

\bibitem{xie2018rethinking}
Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy.
\newblock Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs
  in video classification.
\newblock In {\em ECCV}, pages 318--335, 2018.

\bibitem{XuZX0Z0Z17}
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting
  Zhuang.
\newblock Video question answering via gradually refined attention over
  appearance and motion.
\newblock In {\em ACMMM}, pages 1645--1653, 2017.

\bibitem{xu2016msr}
Jun Xu, Tao Mei, Ting Yao, and Yong Rui.
\newblock {MSR-VTT}: A large video description dataset for bridging video and
  language.
\newblock In {\em CVPR}, pages 5288--5296, 2016.

\bibitem{YangMSLS21}
Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.
\newblock Just ask: Learning to answer questions from millions of narrated
  videos.
\newblock In {\em ICCV}, pages 1666--1677, 2021.

\bibitem{yang2021taco}
Jianwei Yang, Yonatan Bisk, and Jianfeng Gao.
\newblock {TACo}: Token-aware cascade contrastive learning for video-text
  alignment.
\newblock In {\em ICCV}, pages 11562--11572, 2021.

\bibitem{zhang2018cross}
Bowen Zhang, Hexiang Hu, and Fei Sha.
\newblock Cross-modal and hierarchical modeling of video and text.
\newblock In {\em ECCV}, pages 385--401, 2018.

\end{thebibliography}
