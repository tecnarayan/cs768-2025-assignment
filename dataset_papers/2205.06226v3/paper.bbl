\begin{thebibliography}{93}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu and Li(2019)]{allen2019can}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Allen-Zhu and Li(2020{\natexlab{a}})]{allen2020backward}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Backward feature correction: How deep learning performs deep
  learning.
\newblock \emph{arXiv preprint arXiv:2001.04413}, 2020{\natexlab{a}}.

\bibitem[Allen-Zhu and Li(2020{\natexlab{b}})]{allen2020towards}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Towards understanding ensemble, knowledge distillation and
  self-distillation in deep learning.
\newblock \emph{arXiv preprint arXiv:2012.09816}, 2020{\natexlab{b}}.

\bibitem[Allen-Zhu and Li(2021)]{allen2021forward}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Forward super-resolution: How can gans learn hierarchical generative
  models for real-world distributions.
\newblock \emph{arXiv preprint arXiv:2106.02619}, 2021.

\bibitem[Allen{-}Zhu and Li(2021)]{allenzhu2021feature}
Zeyuan Allen{-}Zhu and Yuanzhi Li.
\newblock Feature purification: How adversarial training performs robust deep
  learning.
\newblock In \emph{62nd {IEEE} Annual Symposium on Foundations of Computer
  Science, {FOCS} 2021, Denver, CO, USA, February 7-10, 2022}, pages 977--988.
  {IEEE}, 2021.
\newblock URL \url{https://doi.org/10.1109/FOCS52979.2021.00098}.

\bibitem[{Allen-Zhu} et~al.(2019{\natexlab{a}}){Allen-Zhu}, {Li}, and
  {Liang}]{allen-zhu2019learning}
Zeyuan {Allen-Zhu}, Yuanzhi {Li}, and Yingyu {Liang}.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \emph{NeurIPS 2019 : Thirty-third Conference on Neural Information
  Processing Systems}, pages 6158--6169, 2019{\natexlab{a}}.

\bibitem[{Allen-Zhu} et~al.(2019{\natexlab{b}}){Allen-Zhu}, {Li}, and
  {Song}]{allen-zhu2019a}
Zeyuan {Allen-Zhu}, Yuanzhi {Li}, and Zhao {Song}.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{ICML 2019 : Thirty-sixth International Conference on Machine
  Learning}, pages 242--252, 2019{\natexlab{b}}.

\bibitem[{Allen-Zhu} et~al.(2019{\natexlab{c}}){Allen-Zhu}, {Li}, and
  {Song}]{allen-zhu2019on}
Zeyuan {Allen-Zhu}, Yuanzhi {Li}, and Zhao {Song}.
\newblock On the convergence rate of training recurrent neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6676--6688, 2019{\natexlab{c}}.

\bibitem[Andoni et~al.(2014)Andoni, Panigrahy, Valiant, and
  Zhang]{andoni2014learning}
Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li~Zhang.
\newblock Learning polynomials with neural networks.
\newblock In \emph{International conference on machine learning}, pages
  1908--1916. PMLR, 2014.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Cohen, Hu, and
  Luo]{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32,
  2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, and
  Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  322--332. PMLR, 2019{\natexlab{b}}.

\bibitem[Arora et~al.(2019{\natexlab{c}})Arora, Khandeparkar, Khodak,
  Plevrakis, and Saunshi]{arora2019theoretical}
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and
  Nikunj Saunshi.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock \emph{arXiv preprint arXiv:1902.09229}, 2019{\natexlab{c}}.

\bibitem[Ash et~al.(2021)Ash, Goel, Krishnamurthy, and
  Misra]{ash2021investigating}
Jordan~T Ash, Surbhi Goel, Akshay Krishnamurthy, and Dipendra Misra.
\newblock Investigating the role of negatives in contrastive representation
  learning.
\newblock \emph{arXiv preprint arXiv:2106.09943}, 2021.

\bibitem[Bansal et~al.(2020)Bansal, Kaplun, and Barak]{bansal2020self}
Yamini Bansal, Gal Kaplun, and Boaz Barak.
\newblock For self-supervised learning, rationality implies generalization,
  provably.
\newblock \emph{arXiv preprint arXiv:2010.08508}, 2020.

\bibitem[Bao et~al.(2021{\natexlab{a}})Bao, Nagano, and Nozawa]{bao2021sharp}
Han Bao, Yoshihiro Nagano, and Kento Nozawa.
\newblock Sharp learning bounds for contrastive unsupervised representation
  learning.
\newblock \emph{arXiv preprint arXiv:2110.02501}, 2021{\natexlab{a}}.

\bibitem[Bao et~al.(2021{\natexlab{b}})Bao, Dong, and Wei]{bao2021beit}
Hangbo Bao, Li~Dong, and Furu Wei.
\newblock Beit: Bert pre-training of image transformers.
\newblock \emph{arXiv preprint arXiv:2106.08254}, 2021{\natexlab{b}}.

\bibitem[Bardes et~al.(2021)Bardes, Ponce, and LeCun]{bardes2021vicreg}
Adrien Bardes, Jean Ponce, and Yann LeCun.
\newblock Vicreg: Variance-invariance-covariance regularization for
  self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2105.04906}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Brutzkus and Globerson(2017)]{brutzkus2017globally}
Alon Brutzkus and Amir Globerson.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock In \emph{Proceedings of the 34\textsuperscript{th} International
  Conference on Machine Learning-Volume 70}, pages 605--614. JMLR. org, 2017.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020unsupervised}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9912--9924, 2020.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal,
  Bojanowski, and Joulin]{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv{\'e} J{\'e}gou, Julien Mairal,
  Piotr Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9650--9660, 2021.

\bibitem[{Chen} et~al.(2020{\natexlab{a}}){Chen}, {Dobriban}, and
  {Lee}]{chen2020a}
Shuxiao {Chen}, Edgar {Dobriban}, and Jane~H. {Lee}.
\newblock A group-theoretic framework for data augmentation.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (245):\penalty0 1--71, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2022)Chen, Li, Li, and Zhang]{chen2022learning}
Sitan Chen, Jerry Li, Yuanzhi Li, and Anru~R Zhang.
\newblock Learning polynomial transformations.
\newblock \emph{arXiv preprint arXiv:2204.04209}, 2022.

\bibitem[{Chen} et~al.(2020{\natexlab{b}}){Chen}, {Kornblith}, {Norouzi}, and
  {Hinton}]{Chen2020}
Ting {Chen}, Simon {Kornblith}, Mohammad {Norouzi}, and Geoffrey {Hinton}.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{ICML 2020: 37th International Conference on Machine
  Learning}, volume~1, pages 1597--1607, 2020{\natexlab{b}}.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Luo, and Li]{chen2021intriguing}
Ting Chen, Calvin Luo, and Lala Li.
\newblock Intriguing properties of contrastive losses.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Chen and He(2021)]{chen2021exploring}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 15750--15758, 2021.

\bibitem[Chen et~al.(2020)Chen, Fan, Girshick, and He]{chen2020improved}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock \emph{arXiv preprint arXiv:2003.04297}, 2020.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Xie, and He]{chen2021empirical}
Xinlei Chen, Saining Xie, and Kaiming He.
\newblock An empirical study of training self-supervised vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9640--9649, 2021{\natexlab{b}}.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory}, pages 1305--1338. PMLR,
  2020.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of NAACL-HLT}, pages 4171--4186, 2019.

\bibitem[Du et~al.(2018)Du, Lee, and Tian]{du2018convolutional}
Simon~S Du, Jason~D Lee, and Yuandong Tian.
\newblock When is a convolutional filter easy to learn?
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[{Du} et~al.(2019){Du}, {Lee}, {Li}, {Wang}, and
  {Zhai}]{du2019gradient}
Simon~S. {Du}, Jason~D. {Lee}, Haochuan {Li}, Liwei {Wang}, and Xiyu {Zhai}.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{36\textsuperscript{th} International Conference on Machine
  Learning, ICML 2019}, pages 1675--1685, 2019.

\bibitem[Ermolov et~al.(2021)Ermolov, Siarohin, Sangineto, and
  Sebe]{ermolov2021whitening}
Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe.
\newblock Whitening for self-supervised representation learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3015--3024. PMLR, 2021.

\bibitem[Gao et~al.(2021)Gao, Yao, and Chen]{gao2021simcse}
Tianyu Gao, Xingcheng Yao, and Danqi Chen.
\newblock Simcse: Simple contrastive learning of sentence embeddings.
\newblock \emph{arXiv preprint arXiv:2104.08821}, 2021.

\bibitem[Gautier et~al.(2016)Gautier, Nguyen, and Hein]{gautier2016globally}
Antoine Gautier, Quynh~N Nguyen, and Matthias Hein.
\newblock Globally optimal training of generalized polynomial neural networks
  with nonlinear spectral methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{GhorbaniMMM19}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Limitations of lazy training of two-layers neural network.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pages 9108--9118, 2019.

\bibitem[Ghorbani et~al.(2020)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{GhorbaniMMM20}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock When do neural networks outperform kernel methods?
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Gowal et~al.(2020)Gowal, Huang, van~den Oord, Mann, and
  Kohli]{gowal2020self}
Sven Gowal, Po-Sen Huang, Aaron van~den Oord, Timothy Mann, and Pushmeet Kohli.
\newblock Self-supervised adversarial robustness for the low-label, high-data
  regime.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar,
  et~al.]{grill2020bootstrap}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec, Pierre
  Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila~Pires, Zhaohan
  Guo, Mohammad Gheshlaghi~Azar, et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 21271--21284, 2020.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
Suriya Gunasekar, Jason~D Lee, Daniel Soudry, and Nati Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and Ma]{haochen2021provable}
Jeff~Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock \emph{arXiv preprint arXiv:2106.04156}, 2021.

\bibitem[{He} et~al.(2020){He}, {Fan}, {Wu}, {Xie}, and
  {Girshick}]{he2020momentum}
Kaiming {He}, Haoqi {Fan}, Yuxin {Wu}, Saining {Xie}, and Ross {Girshick}.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 9729--9738, 2020.

\bibitem[He et~al.(2021)He, Chen, Xie, Li, Doll{\'a}r, and
  Girshick]{he2021masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock \emph{arXiv preprint arXiv:2111.06377}, 2021.

\bibitem[Hjelm et~al.(2019)Hjelm, Fedorov, Lavoie-Marchildon, Grewal, Bachman,
  Trischler, and Bengio]{hjelm2018learning}
R~Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil
  Bachman, Adam Trischler, and Yoshua Bengio.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Hua et~al.(2021)Hua, Wang, Xue, Ren, Wang, and Zhao]{hua2021feature}
Tianyu Hua, Wenxiao Wang, Zihui Xue, Sucheng Ren, Yue Wang, and Hang Zhao.
\newblock On feature decorrelation in self-supervised learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9598--9608, 2021.

\bibitem[Huang et~al.(2021)Huang, Yi, and Zhao]{huang2021towards}
Weiran Huang, Mingyang Yi, and Xuyang Zhao.
\newblock Towards the generalization of contrastive self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2111.00743}, 2021.

\bibitem[Jelassi and Li(2022)]{jelassi2022towards}
Samy Jelassi and Yuanzhi Li.
\newblock Towards understanding how momentum improves generalization in deep
  learning, 2022.
\newblock URL \url{https://openreview.net/forum?id=lf0W6tcWmh-}.

\bibitem[Jelassi et~al.(2022)Jelassi, Mensch, Gidel, and Li]{jelassi2022adam}
Samy Jelassi, Arthur Mensch, Gauthier Gidel, and Yuanzhi Li.
\newblock Adam is no better than normalized {SGD}: Dissecting how adaptivity
  improves {GAN} performance, 2022.
\newblock URL \url{https://openreview.net/forum?id=D9SuLzhgK9}.

\bibitem[Ji et~al.(2021)Ji, Deng, Nakada, Zou, and Zhang]{ji2021power}
Wenlong Ji, Zhun Deng, Ryumei Nakada, James Zou, and Linjun Zhang.
\newblock The power of contrast for feature learning: A theoretical analysis.
\newblock \emph{arXiv preprint arXiv:2110.02473}, 2021.

\bibitem[Ji and Telgarsky(2019)]{ji2019implicit}
Ziwei Ji and Matus Telgarsky.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In \emph{Conference on Learning Theory}, pages 1772--1798. PMLR,
  2019.

\bibitem[Jing et~al.(2021)Jing, Vincent, LeCun, and
  Tian]{jing2021understanding}
Li~Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian.
\newblock Understanding dimensional collapse in contrastive self-supervised
  learning.
\newblock \emph{arXiv preprint arXiv:2110.09348}, 2021.

\bibitem[Karp et~al.(2021)Karp, Winston, Li, and Singh]{karp2021local}
Stefani Karp, Ezra Winston, Yuanzhi Li, and Aarti Singh.
\newblock Local signal adaptivity: Provable feature learning in neural networks
  beyond kernels.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Kileel et~al.(2019)Kileel, Trager, and Bruna]{kileel2019expressive}
Joe Kileel, Matthew Trager, and Joan Bruna.
\newblock On the expressive power of deep polynomial neural networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Lee et~al.(2021)Lee, Lei, Saunshi, and Zhuo]{lee2021predicting}
Jason~D Lee, Qi~Lei, Nikunj Saunshi, and Jiacheng Zhuo.
\newblock Predicting what you already know helps: Provable self-supervised
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Li and Dou(2020)]{li2020making}
Yuanzhi Li and Zehao Dou.
\newblock Making method of moments great again?--how can gans learn
  distributions.
\newblock \emph{arXiv preprint arXiv:2003.04033}, 2020.

\bibitem[Li and Yuan(2017)]{li2017convergence}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock In \emph{Advances in neural information processing systems}, pages
  597--607, 2017.

\bibitem[{Li} et~al.(2018){Li}, {Ma}, and {Zhang}]{li2018algorithmic}
Yuanzhi {Li}, Tengyu {Ma}, and Hongyang {Zhang}.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In \emph{COLT 2018: 31\textsuperscript{st} Annual Conference on
  Learning Theory}, pages 2--47, 2018.

\bibitem[{Li} et~al.(2019){Li}, {Wei}, and {Ma}]{li2019towards}
Yuanzhi {Li}, Colin {Wei}, and Tengyu {Ma}.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock In \emph{NeurIPS 2019 : Thirty-third Conference on Neural Information
  Processing Systems}, pages 11674--11685, 2019.

\bibitem[{Li} et~al.(2020){Li}, {Ma}, and {Zhang}]{li2020learning}
Yuanzhi {Li}, Tengyu {Ma}, and Hongyang~R. {Zhang}.
\newblock Learning over-parametrized two-layer relu neural networks beyond ntk.
\newblock In \emph{COLT}, pages 2613--2682, 2020.

\bibitem[Liu et~al.(2021)Liu, Rosenfeld, Ravikumar, and
  Risteski]{liu2021analyzing}
Bingbin Liu, Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski.
\newblock Analyzing and improving the optimization landscape of
  noise-contrastive estimation.
\newblock \emph{arXiv preprint arXiv:2110.11271}, 2021.

\bibitem[Liu et~al.(2022)Liu, Hsu, Ravikumar, and Risteski]{liu2022masked}
Bingbin Liu, Daniel Hsu, Pradeep Ravikumar, and Andrej Risteski.
\newblock Masked prediction tasks: a parameter identifiability view.
\newblock \emph{arXiv preprint arXiv:2202.09305}, 2022.

\bibitem[Luo et~al.(2022)Luo, Weng, Wu, Zhou, and Ge]{luo2022one}
Zeping Luo, Cindy Weng, Shiyou Wu, Mo~Zhou, and Rong Ge.
\newblock One objective for all models--self-supervised learning for topic
  models.
\newblock \emph{arXiv preprint arXiv:2203.03539}, 2022.

\bibitem[Lyu and Li(2019)]{lyu2019gradient}
Kaifeng Lyu and Jian Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock \emph{arXiv preprint arXiv:1906.05890}, 2019.

\bibitem[Niizumi et~al.(2021)Niizumi, Takeuchi, Ohishi, Harada, and
  Kashino]{niizumi2021byol}
Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, and Kunio
  Kashino.
\newblock Byol for audio: Self-supervised learning for general-purpose audio
  representation.
\newblock In \emph{2021 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--8. IEEE, 2021.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Pokle et~al.(2022)Pokle, Tian, Li, and Risteski]{pokle2022contrasting}
Ashwini Pokle, Jinjin Tian, Yuchen Li, and Andrej Risteski.
\newblock Contrasting the landscape of contrastive and non-contrastive
  learning.
\newblock \emph{arXiv preprint arXiv:2203.15702}, 2022.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{International Conference on Machine Learning}, pages
  8821--8831. PMLR, 2021.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and
  Chen]{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{OpenAI blog}, 2022.
\newblock URL \url{https://cdn.openai.com/papers/dall-e-2.pdf}.

\bibitem[Razin and Cohen(2020)]{razin2020implicit}
Noam Razin and Nadav Cohen.
\newblock Implicit regularization in deep learning may not be explainable by
  norms.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 21174--21187, 2020.

\bibitem[Richemond et~al.(2020)Richemond, Grill, Altch{\'{e}}, Tallec, Strub,
  Brock, Smith, De, Pascanu, Piot, and Valko]{richemond_byol_2020}
Pierre~H. Richemond, Jean-Bastien Grill, Florent Altch{\'{e}}, Corentin Tallec,
  Florian Strub, Andrew Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal
  Piot, and Michal Valko.
\newblock {BYOL} works even without batch statistics.
\newblock \emph{arXiv:2010.10241 [cs, stat]}, October 2020.

\bibitem[Robinson et~al.(2021)Robinson, Sun, Yu, Batmanghelich, Jegelka, and
  Sra]{robinson2021can}
Joshua Robinson, Li~Sun, Ke~Yu, Kayhan Batmanghelich, Stefanie Jegelka, and
  Suvrit Sra.
\newblock Can contrastive learning avoid shortcut solutions?
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Saunshi et~al.(2020)Saunshi, Malladi, and
  Arora]{saunshi2020mathematical}
Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora.
\newblock A mathematical exploration of why language models help solve
  downstream tasks.
\newblock \emph{arXiv preprint arXiv:2010.03648}, 2020.

\bibitem[Saunshi et~al.(2022)Saunshi, Ash, Goel, Misra, Zhang, Arora, Kakade,
  and Krishnamurthy]{saunshi2022understanding}
Nikunj Saunshi, Jordan Ash, Surbhi Goel, Dipendra Misra, Cyril Zhang, Sanjeev
  Arora, Sham Kakade, and Akshay Krishnamurthy.
\newblock Understanding contrastive learning requires incorporating inductive
  biases.
\newblock \emph{arXiv preprint arXiv:2202.14037}, 2022.

\bibitem[{Soltanolkotabi}(2017)]{soltanolkotabi2017learning}
Mahdi {Soltanolkotabi}.
\newblock Learning relus via gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30, pages 2007--2017, 2017.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Teng et~al.(2021)Teng, Huang, and He]{teng2021can}
Jiaye Teng, Weiran Huang, and Haowei He.
\newblock Can pretext-based self-supervised learning be boosted by downstream
  data? a theoretical analysis.
\newblock \emph{arXiv preprint arXiv:2103.03568}, 2021.

\bibitem[Tian et~al.(2020)Tian, Krishnan, and Isola]{tian2020contrastive}
Yonglong Tian, Dilip Krishnan, and Phillip Isola.
\newblock Contrastive multiview coding.
\newblock In \emph{European conference on computer vision}, pages 776--794.
  Springer, 2020.

\bibitem[{Tian} et~al.(2020){Tian}, {Sun}, {Poole}, {Krishnan}, {Schmid}, and
  {Isola}]{tian2020what}
Yonglong {Tian}, Chen {Sun}, Ben {Poole}, Dilip {Krishnan}, Cordelia {Schmid},
  and Phillip {Isola}.
\newblock What makes for good views for contrastive learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[Tian(2017)]{tian2017analytical}
Yuandong Tian.
\newblock An analytical formula of population gradient for two-layered relu
  network and its applications in convergence and critical point analysis.
\newblock In \emph{Proceedings of the 34\textsuperscript{th} International
  Conference on Machine Learning-Volume 70}, pages 3404--3413. JMLR. org, 2017.

\bibitem[Tian et~al.(2021)Tian, Chen, and Ganguli]{tian2021understanding}
Yuandong Tian, Xinlei Chen, and Surya Ganguli.
\newblock Understanding self-supervised learning dynamics without contrastive
  pairs.
\newblock In \emph{International Conference on Machine Learning}, pages
  10268--10278. PMLR, 2021.

\bibitem[Tosh et~al.(2020)Tosh, Krishnamurthy, and Hsu]{tosh2020contrastive}
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu.
\newblock Contrastive estimation reveals topic posterior information to linear
  models.
\newblock \emph{arXiv preprint arXiv:2003.02234}, 2020.

\bibitem[Tosh et~al.(2021)Tosh, Krishnamurthy, and Hsu]{tosh2021contrastive}
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu.
\newblock Contrastive learning, multi-view redundancy, and linear models.
\newblock In \emph{Algorithmic Learning Theory}, pages 1179--1206. PMLR, 2021.

\bibitem[Tsai et~al.(2020)Tsai, Wu, Salakhutdinov, and Morency]{tsai2020self}
Yao-Hung~Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency.
\newblock Self-supervised learning from a multi-view perspective.
\newblock \emph{arXiv preprint arXiv:2006.05576}, 2020.

\bibitem[Von~K{\"u}gelgen et~al.(2021)Von~K{\"u}gelgen, Sharma, Gresele,
  Brendel, Sch{\"o}lkopf, Besserve, and Locatello]{von2021self}
Julius Von~K{\"u}gelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard
  Sch{\"o}lkopf, Michel Besserve, and Francesco Locatello.
\newblock Self-supervised learning with data augmentations provably isolates
  content from style.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Wang et~al.(2021)Wang, Chen, Du, and Tian]{wang2021towards}
Xiang Wang, Xinlei Chen, Simon~S Du, and Yuandong Tian.
\newblock Towards demystifying representation learning with non-contrastive
  self-supervision.
\newblock \emph{arXiv preprint arXiv:2110.04947}, 2021.

\bibitem[Wei et~al.(2021)Wei, Xie, and Ma]{wei2021pretrained}
Colin Wei, Sang~Michael Xie, and Tengyu Ma.
\newblock Why do pretrained language models help in downstream tasks? an
  analysis of head and prompt tuning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Wen and Li(2021)]{wen2021toward}
Zixin Wen and Yuanzhi Li.
\newblock Toward understanding the feature learning process of self-supervised
  contrastive learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  11112--11122. PMLR, 2021.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov,
  and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock In \emph{Advances in neural information processing systems}, pages
  5754--5764, 2019.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar2021barlow}
Jure Zbontar, Li~Jing, Ishan Misra, Yann LeCun, and St{\'e}phane Deny.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In \emph{International Conference on Machine Learning}, pages
  12310--12320. PMLR, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Zhang, Pham, Yoo, and
  Kweon]{zhang2022does}
Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Trung~X Pham, Chang~D Yoo, and
  In~So Kweon.
\newblock How does simsiam avoid collapse without negative samples? a unified
  understanding with self-supervised contrastive learning.
\newblock \emph{arXiv preprint arXiv:2203.16262}, 2022.

\bibitem[Zhong et~al.(2017)Zhong, Song, Jain, Bartlett, and
  Dhillon]{zhong2017recovery}
Kai Zhong, Zhao Song, Prateek Jain, Peter~L Bartlett, and Inderjit~S Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In \emph{Proceedings of the 34\textsuperscript{th} International
  Conference on Machine Learning-Volume 70}, pages 4140--4149. JMLR. org, 2017.

\bibitem[Zou et~al.(2021)Zou, Cao, Li, and Gu]{zou2021understanding}
Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu.
\newblock Understanding the generalization of adam in learning neural networks
  with proper regularization.
\newblock \emph{arXiv preprint arXiv:2108.11371}, 2021.

\end{thebibliography}
