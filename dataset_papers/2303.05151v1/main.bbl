\begin{thebibliography}{74}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[ope(2023)]{opencode}
Open source code for all the algorithms presented in this paper, 2023.
\newblock
  \href{https://github.com/muradtuk/Provable-Data-Subset-Selection-For-Efficient-Neural-Network-Training}{Link
  for open-source code.}

\bibitem[Babu \& Suresh(2012)Babu and Suresh]{babu2012sequential}
Babu, G.~S. and Suresh, S.
\newblock Sequential projection-based metacognitive learning in a radial basis
  function network for classification problems.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  24\penalty0 (2):\penalty0 194--206, 2012.

\bibitem[Baykal et~al.(2022)Baykal, Liebenwein, Gilitschenski, Feldman, and
  Rus]{BaykalLGFR22}
Baykal, C., Liebenwein, L., Gilitschenski, I., Feldman, D., and Rus, D.
\newblock Sensitivity-informed provable pruning of neural networks.
\newblock \emph{{SIAM} J. Math. Data Sci.}, 4\penalty0 (1):\penalty0 26--45,
  2022.

\bibitem[Bohdal et~al.(2020)Bohdal, Yang, and Hospedales]{BohdalYH20}
Bohdal, O., Yang, Y., and Hospedales, T.~M.
\newblock Flexible dataset distillation: Learn labels instead of images.
\newblock \emph{CoRR}, abs/2006.08572, 2020.

\bibitem[Braverman et~al.(2016)Braverman, Feldman, and Lang]{braverman2016new}
Braverman, V., Feldman, D., and Lang, H.
\newblock New frameworks for offline and streaming coreset constructions.
\newblock \emph{arXiv preprint arXiv:1612.00889}, 2016.

\bibitem[Braverman et~al.(2020)Braverman, Drineas, Musco, Musco, Upadhyay,
  Woodruff, and Zhou]{BravermanDMMUWZ20}
Braverman, V., Drineas, P., Musco, C., Musco, C., Upadhyay, J., Woodruff,
  D.~P., and Zhou, S.
\newblock Near optimal linear algebra in the online and sliding window models.
\newblock In \emph{61st {IEEE} Annual Symposium on Foundations of Computer
  Science, {FOCS}}, pp.\  517--528, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{BrownMRSKDNSSAA20}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert{-}Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A.,
  Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin,
  M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A.,
  Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems, NeurIPS}, 2020.

\bibitem[Campbell \& Broderick(2018)Campbell and Broderick]{CampbellB18}
Campbell, T. and Broderick, T.
\newblock Bayesian coreset construction via greedy iterative geodesic ascent.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML}}, pp.\  697--705, 2018.

\bibitem[Chen(2009)]{Chen09}
Chen, K.
\newblock On coresets for k-median and k-means clustering in metric and
  euclidean spaces and their applications.
\newblock \emph{{SIAM} J. Comput.}, 39\penalty0 (3):\penalty0 923--947, 2009.

\bibitem[Chhaya et~al.(2020)Chhaya, Dasgupta, and Shit]{Chhaya0S20}
Chhaya, R., Dasgupta, A., and Shit, S.
\newblock On coresets for regularized regression.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML}}, 2020.

\bibitem[Claici \& Solomon(2018)Claici and Solomon]{claici2018wasserstein}
Claici, S. and Solomon, J.
\newblock Wasserstein coresets for lipschitz costs.
\newblock \emph{stat}, 1050:\penalty0 18, 2018.

\bibitem[Claici et~al.(2018)Claici, Genevay, and
  Solomon]{claici2018wasserstein2}
Claici, S., Genevay, A., and Solomon, J.
\newblock Wasserstein measure coresets.
\newblock \emph{arXiv preprint arXiv:1805.07412}, 2018.

\bibitem[Clarkson(2010)]{Clarkson10}
Clarkson, K.~L.
\newblock Coresets, sparse greedy approximation, and the frank-wolfe algorithm.
\newblock \emph{{ACM} Trans. Algorithms}, 6\penalty0 (4):\penalty0 63:1--63:30,
  2010.

\bibitem[Clarkson \& Woodruff(2017)Clarkson and Woodruff]{clarkson2017low}
Clarkson, K.~L. and Woodruff, D.~P.
\newblock Low-rank approximation and regression in input sparsity time.
\newblock \emph{Journal of the ACM (JACM)}, 63\penalty0 (6):\penalty0 1--45,
  2017.

\bibitem[Cohen et~al.(2017)Cohen, Musco, and Musco]{CohenMM17}
Cohen, M.~B., Musco, C., and Musco, C.
\newblock Input sparsity time low-rank approximation via ridge leverage score
  sampling.
\newblock In \emph{Proceedings of the Twenty-Eighth Annual {ACM-SIAM} Symposium
  on Discrete Algorithms, {SODA}}, pp.\  1758--1777, 2017.

\bibitem[Cohen{-}Addad et~al.(2022)Cohen{-}Addad, Larsen, Saulpic, and
  Schwiegelshohn]{Cohen-AddadLSS22}
Cohen{-}Addad, V., Larsen, K.~G., Saulpic, D., and Schwiegelshohn, C.
\newblock Towards optimal lower bounds for k-median and k-means coresets.
\newblock In \emph{{STOC} '22: 54th Annual {ACM} {SIGACT} Symposium on Theory
  of Computing}, pp.\  1038--1051, 2022.

\bibitem[Coleman et~al.(2019)Coleman, Yeh, Mussmann, Mirzasoleiman, Bailis,
  Liang, Leskovec, and Zaharia]{coleman2019selection}
Coleman, C., Yeh, C., Mussmann, S., Mirzasoleiman, B., Bailis, P., Liang, P.,
  Leskovec, J., and Zaharia, M.
\newblock Selection via proxy: Efficient data selection for deep learning.
\newblock \emph{arXiv preprint arXiv:1906.11829}, 2019.

\bibitem[Dasgupta et~al.(2008)Dasgupta, Drineas, Harb, Kumar, and
  Mahoney]{DasguptaDHKM08}
Dasgupta, A., Drineas, P., Harb, B., Kumar, R., and Mahoney, M.~W.
\newblock Sampling algorithms and coresets for $l_p$ regression.
\newblock In \emph{Proceedings of the Nineteenth Annual {ACM-SIAM} Symposium on
  Discrete Algorithms, {SODA}}, pp.\  932--941, 2008.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{DevlinCLT19}
Devlin, J., Chang, M., Lee, K., and Toutanova, K.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, {NAACL-HLT}}, pp.\  4171--4186, 2019.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{DosovitskiyB0WZ21}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR}}, 2021.

\bibitem[Dua et~al.(2017)Dua, Graff, et~al.]{dua2017uci}
Dua, D., Graff, C., et~al.
\newblock Uci machine learning repository, 2017.

\bibitem[Feldman(2020)]{Feldman20}
Feldman, D.
\newblock Core-sets: An updated survey.
\newblock \emph{WIREs Data Mining Knowl. Discov.}, 10\penalty0 (1), 2020.

\bibitem[Feldman et~al.(2020)Feldman, Schmidt, and Sohler]{FeldmanSS20}
Feldman, D., Schmidt, M., and Sohler, C.
\newblock Turning big data into tiny data: Constant-size coresets for k-means,
  pca, and projective clustering.
\newblock \emph{{SIAM} J. Comput.}, 49\penalty0 (3):\penalty0 601--657, 2020.

\bibitem[Har{-}Peled \& Mazumdar(2004)Har{-}Peled and Mazumdar]{Har-PeledM04}
Har{-}Peled, S. and Mazumdar, S.
\newblock On coresets for k-means and k-median clustering.
\newblock In \emph{Proceedings of the 36th Annual {ACM} Symposium on Theory of
  Computing}, pp.\  291--300, 2004.

\bibitem[Harpham \& Dawson(2006)Harpham and Dawson]{harpham2006effect}
Harpham, C. and Dawson, C.~W.
\newblock The effect of different basis functions on a radial basis function
  network for time series prediction: A comparative study.
\newblock \emph{Neurocomputing}, 69\penalty0 (16-18):\penalty0 2161--2170,
  2006.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Huang \& Vishnoi(2020)Huang and Vishnoi]{HuangV20}
Huang, L. and Vishnoi, N.~K.
\newblock Coresets for clustering in euclidean spaces: importance sampling is
  nearly optimal.
\newblock In \emph{Proccedings of the 52nd Annual {ACM} {SIGACT} Symposium on
  Theory of Computing, {STOC}}, pp.\  1416--1429, 2020.

\bibitem[Indyk et~al.(2020)Indyk, Mahabadi, Gharan, and Rezaei]{IndykMGR20}
Indyk, P., Mahabadi, S., Gharan, S.~O., and Rezaei, A.
\newblock Composable core-sets for determinant maximization problems via
  spectral spanners.
\newblock In \emph{Proceedings of the 2020 {ACM-SIAM} Symposium on Discrete
  Algorithms, {SODA}}, pp.\  1675--1694, 2020.

\bibitem[Jubran et~al.(2020)Jubran, Tukan, Maalouf, and
  Feldman]{jubran2020sets}
Jubran, I., Tukan, M., Maalouf, A., and Feldman, D.
\newblock Sets clustering.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4994--5005. PMLR, 2020.

\bibitem[Jubran et~al.(2021)Jubran, Maalouf, and Feldman]{jubran2021overview}
Jubran, I., Maalouf, A., and Feldman, D.
\newblock Overview of accurate coresets.
\newblock \emph{Wiley Interdisciplinary Reviews: Data Mining and Knowledge
  Discovery}, pp.\  e1429, 2021.

\bibitem[Kaushal et~al.(2019)Kaushal, Iyer, Kothawade, Mahadev, Doctor, and
  Ramakrishnan]{kaushal2019learning}
Kaushal, V., Iyer, R., Kothawade, S., Mahadev, R., Doctor, K., and
  Ramakrishnan, G.
\newblock Learning from less data: A unified data subset selection and active
  learning framework for computer vision.
\newblock In \emph{2019 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pp.\  1289--1299. IEEE Computer Society, 2019.

\bibitem[Killamsetty et~al.(2021{\natexlab{a}})Killamsetty, Sivasubramanian,
  Ramakrishnan, De, and Iyer]{killamsetty2021grad}
Killamsetty, K., Sivasubramanian, D., Ramakrishnan, G., De, A., and Iyer, R.~K.
\newblock {GRAD-MATCH:} gradient matching based data subset selection for
  efficient deep model training.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning, {ICML}}, pp.\  5464--5474, 2021{\natexlab{a}}.

\bibitem[Killamsetty et~al.(2021{\natexlab{b}})Killamsetty, Sivasubramanian,
  Ramakrishnan, and Iyer]{killamsetty2021glister}
Killamsetty, K., Sivasubramanian, D., Ramakrishnan, G., and Iyer, R.~K.
\newblock {GLISTER:} generalization based data subset selection for efficient
  and robust learning.
\newblock In \emph{Thirty-Fifth {AAAI} Conference on Artificial Intelligence,
  {AAAI}}, 2021{\natexlab{b}}.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem[Krizhevsky et~al.(2017)Krizhevsky, Sutskever, and
  Hinton]{KrizhevskySH17}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Commun. {ACM}}, 60\penalty0 (6):\penalty0 84--90, 2017.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Leonard \& Kramer(1991)Leonard and Kramer]{leonard1991radial}
Leonard, J.~A. and Kramer, M.~A.
\newblock Radial basis function networks for classifying process faults.
\newblock \emph{IEEE Control Systems Magazine}, 11\penalty0 (3):\penalty0
  31--38, 1991.

\bibitem[Leung et~al.(2001)Leung, Lo, and Wang]{LeungLW01}
Leung, H., Lo, T. K.~Y., and Wang, S.
\newblock Prediction of noisy chaotic time series using an optimal radial basis
  function neural network.
\newblock \emph{{IEEE} Trans. Neural Networks}, 12\penalty0 (5):\penalty0
  1163--1172, 2001.

\bibitem[Liebenwein et~al.(2019)Liebenwein, Baykal, Lang, Feldman, and
  Rus]{liebenwein2019provable}
Liebenwein, L., Baykal, C., Lang, H., Feldman, D., and Rus, D.
\newblock Provable filter pruning for efficient neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Liu(2013)]{liu2013radial}
Liu, J.
\newblock \emph{Radial Basis Function (RBF) neural network control for
  mechanical systems: design, analysis and Matlab simulation}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{loshchilov2016sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Lu et~al.(1997)Lu, Sundararajan, and Saratchandran]{LuSS97}
Lu, Y., Sundararajan, N., and Saratchandran, P.
\newblock A sequential learning scheme for function approximation using minimal
  radial basis function neural networks.
\newblock \emph{Neural Comput.}, 9\penalty0 (2):\penalty0 461--478, 1997.

\bibitem[Maalouf et~al.(2019)Maalouf, Jubran, and Feldman]{maalouf2019fast}
Maalouf, A., Jubran, I., and Feldman, D.
\newblock Fast and accurate least-mean-squares solvers.
\newblock In \emph{Proceedings of the 33rd International Conference on Neural
  Information Processing Systems}, pp.\  8307--8318, 2019.

\bibitem[Maalouf et~al.(2020)Maalouf, Statman, and Feldman]{maalouf2020tight}
Maalouf, A., Statman, A., and Feldman, D.
\newblock Tight sensitivity bounds for smaller coresets.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  2051--2061, 2020.

\bibitem[Maalouf et~al.(2021)Maalouf, Jubran, Tukan, and
  Feldman]{maalouf2021coresets}
Maalouf, A., Jubran, I., Tukan, M., and Feldman, D.
\newblock Coresets for the average case error for finite query sets.
\newblock \emph{Sensors}, 21\penalty0 (19):\penalty0 6689, 2021.

\bibitem[Maalouf et~al.(2022{\natexlab{a}})Maalouf, Eini, Mussay, Feldman, and
  Osadchy]{maalouf2022unified}
Maalouf, A., Eini, G., Mussay, B., Feldman, D., and Osadchy, M.
\newblock A unified approach to coreset learning.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2022{\natexlab{a}}.

\bibitem[Maalouf et~al.(2022{\natexlab{b}})Maalouf, Jubran, and
  Feldman]{maalouf2022fast}
Maalouf, A., Jubran, I., and Feldman, D.
\newblock Fast and accurate least-mean-squares solvers for high dimensional
  data.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2022{\natexlab{b}}.

\bibitem[Maalouf et~al.(2022{\natexlab{c}})Maalouf, Tukan, Price, Kane, and
  Feldman]{Maalouf2022Coresets}
Maalouf, A., Tukan, M., Price, E., Kane, D.~G., and Feldman, D.
\newblock Coresets for data discretization and sine wave fitting.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}. PMLR, 2022{\natexlab{c}}.

\bibitem[Mahabadi et~al.(2020)Mahabadi, Razenshteyn, Woodruff, and
  Zhou]{MahabadiRWZ20}
Mahabadi, S., Razenshteyn, I.~P., Woodruff, D.~P., and Zhou, S.
\newblock Non-adaptive adaptive sampling on turnstile streams.
\newblock In \emph{Proccedings of the 52nd Annual {ACM} {SIGACT} Symposium on
  Theory of Computing, {STOC}}, pp.\  1251--1264, 2020.

\bibitem[Meyer et~al.(2022)Meyer, Musco, Musco, Woodruff, and
  Zhou]{MeyerMMWZ22}
Meyer, R.~A., Musco, C., Musco, C., Woodruff, D.~P., and Zhou, S.
\newblock Fast regression for structured inputs.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR}}, 2022.

\bibitem[Mirzasoleiman et~al.(2020{\natexlab{a}})Mirzasoleiman, Bilmes, and
  Leskovec]{mirzasoleiman2020coresets}
Mirzasoleiman, B., Bilmes, J.~A., and Leskovec, J.
\newblock Coresets for data-efficient training of machine learning models.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML}}, pp.\  6950--6960, 2020{\natexlab{a}}.

\bibitem[Mirzasoleiman et~al.(2020{\natexlab{b}})Mirzasoleiman, Cao, and
  Leskovec]{MirzasoleimanCL20}
Mirzasoleiman, B., Cao, K., and Leskovec, J.
\newblock Coresets for robust training of deep neural networks against noisy
  labels.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems, NeurIPS},
  2020{\natexlab{b}}.

\bibitem[Mussay et~al.(2020)Mussay, Osadchy, Braverman, Zhou, and
  Feldman]{MussayOBZF20}
Mussay, B., Osadchy, M., Braverman, V., Zhou, S., and Feldman, D.
\newblock Data-independent neural pruning via coresets.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR}}, 2020.

\bibitem[Nguyen et~al.(2021)Nguyen, Novak, Xiao, and Lee]{NguyenNXL21}
Nguyen, T., Novak, R., Xiao, L., and Lee, J.
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual
  Conference on Neural Information Processing Systems, NeurIPS}, pp.\
  5186--5198, 2021.

\bibitem[Ozkara et~al.(2021)Ozkara, Singh, Data, and Diggavi]{OzkaraSDD21}
Ozkara, K., Singh, N., Data, D., and Diggavi, S.~N.
\newblock Quped: Quantized personalization via distillation with applications
  to federated learning.
\newblock In \emph{Advances in Neural Information Processing Systems 34: Annual
  Conference on Neural Information Processing Systems, NeurIPS}, pp.\
  3622--3634, 2021.

\bibitem[Padmavati(2011)]{padmavati2011comparative}
Padmavati, J.
\newblock A comparative study on breast cancer prediction using rbf and mlp.
\newblock \emph{International Journal of Scientific \& Engineering Research},
  2\penalty0 (1):\penalty0 1--5, 2011.

\bibitem[Park \& Sandberg(1991)Park and Sandberg]{ParkS91}
Park, J. and Sandberg, I.~W.
\newblock Universal approximation using radial-basis-function networks.
\newblock \emph{Neural Comput.}, 3\penalty0 (2):\penalty0 246--257, 1991.

\bibitem[Park \& Sandberg(1993)Park and Sandberg]{ParkS93}
Park, J. and Sandberg, I.~W.
\newblock Approximation and radial-basis-function networks.
\newblock \emph{Neural Comput.}, 5\penalty0 (2):\penalty0 305--316, 1993.

\bibitem[Schwartz et~al.(2020)Schwartz, Dodge, Smith, and
  Etzioni]{SchwartzDSE20}
Schwartz, R., Dodge, J., Smith, N.~A., and Etzioni, O.
\newblock Green {AI}.
\newblock \emph{Commun. {ACM}}, 63\penalty0 (12):\penalty0 54--63, 2020.

\bibitem[Sharir et~al.(2020)Sharir, Peleg, and Shoham]{SharirPS20}
Sharir, O., Peleg, B., and Shoham, Y.
\newblock The cost of training {NLP} models: {A} concise overview.
\newblock \emph{CoRR}, abs/2004.08900, 2020.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and McCallum]{StrubellGM19}
Strubell, E., Ganesh, A., and McCallum, A.
\newblock Energy and policy considerations for deep learning in {NLP}.
\newblock In \emph{Proceedings of the 57th Conference of the Association for
  Computational Linguistics, {ACL}}, pp.\  3645--3650, 2019.

\bibitem[Tolochinsky et~al.(2022)Tolochinsky, Jubran, and
  Feldman]{TolochinskyJF22}
Tolochinsky, E., Jubran, I., and Feldman, D.
\newblock Generic coreset for scalable learning of monotonic kernels: Logistic
  regression, sigmoid and more.
\newblock In \emph{International Conference on Machine Learning, {ICML}}, 2022.

\bibitem[Tukan et~al.(2020)Tukan, Maalouf, and Feldman]{tukan2020coresets}
Tukan, M., Maalouf, A., and Feldman, D.
\newblock Coresets for near-convex functions.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Tukan et~al.(2021)Tukan, Baykal, Feldman, and Rus]{TukanBFR21}
Tukan, M., Baykal, C., Feldman, D., and Rus, D.
\newblock On coresets for support vector machines.
\newblock \emph{Theor. Comput. Sci.}, 890:\penalty0 171--191, 2021.

\bibitem[Tukan et~al.(2022{\natexlab{a}})Tukan, Mualem, and
  Maalouf]{Tukan2022provable}
Tukan, M., Mualem, L., and Maalouf, A.
\newblock Pruning neural networks via coresets and convex geometry: Towards no
  assumptions.
\newblock In \emph{Proceedings of the 36th International Conference on Neural
  Information Processing Systems}, 2022{\natexlab{a}}.

\bibitem[Tukan et~al.(2022{\natexlab{b}})Tukan, Wu, Zhou, Braverman, and
  Feldman]{Tukan0ZBF22}
Tukan, M., Wu, X., Zhou, S., Braverman, V., and Feldman, D.
\newblock New coresets for projective clustering and applications.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics, {AISTATS}}, pp.\  5391--5415, 2022{\natexlab{b}}.

\bibitem[Wei et~al.(2014)Wei, Iyer, and Bilmes]{wei2014fast}
Wei, K., Iyer, R., and Bilmes, J.
\newblock Fast multi-stage submodular maximization.
\newblock In \emph{International conference on machine learning}, pp.\
  1494--1502. PMLR, 2014.

\bibitem[Whitehead \& Choate(1996)Whitehead and Choate]{WhiteheadC96}
Whitehead, B.~A. and Choate, T.~D.
\newblock Cooperative-competitive genetic evolution of radial basis function
  centers and widths for time series prediction.
\newblock \emph{{IEEE} Trans. Neural Networks}, 7\penalty0 (4):\penalty0
  869--880, 1996.

\bibitem[Woodruff \& Yasuda(2022)Woodruff and Yasuda]{WoodruffY22}
Woodruff, D.~P. and Yasuda, T.
\newblock High-dimensional geometric streaming in polynomial space.
\newblock \emph{CoRR}, abs/2204.03790, 2022.

\bibitem[Wu et~al.(2012)Wu, Wang, Zhang, and Du]{wu2012using}
Wu, Y., Wang, H., Zhang, B., and Du, K.-L.
\newblock Using radial basis function networks for function approximation and
  classification.
\newblock \emph{International Scholarly Research Notices}, 2012, 2012.

\bibitem[Wuxing et~al.(2004)Wuxing, Peter, Guicai, and
  Tielin]{wuxing2004classification}
Wuxing, L., Peter, W.~T., Guicai, Z., and Tielin, S.
\newblock Classification of gear faults using cumulants and the radial basis
  function network.
\newblock \emph{mechanical systems and signal processing}, 18\penalty0
  (2):\penalty0 381--389, 2004.

\bibitem[Yu et~al.(2011)Yu, Xie, Paszczy{\~n}ski, and
  Wilamowski]{yu2011advantages}
Yu, H., Xie, T., Paszczy{\~n}ski, S., and Wilamowski, B.~M.
\newblock Advantages of radial basis function networks for dynamic system
  design.
\newblock \emph{IEEE Transactions on Industrial Electronics}, 58\penalty0
  (12):\penalty0 5438--5450, 2011.

\bibitem[Zhu et~al.(2021)Zhu, Hong, and Zhou]{ZhuHZ21}
Zhu, Z., Hong, J., and Zhou, J.
\newblock Data-free knowledge distillation for heterogeneous federated
  learning.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning, {ICML}}, pp.\  12878--12889, 2021.

\end{thebibliography}
