\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ambrosio et~al.(2005)Ambrosio, Gigli, and
  Savar{\'e}]{ambrosio2005gradient}
L.~Ambrosio, N.~Gigli, and G.~Savar{\'e}.
\newblock \emph{Gradient flows: in metric spaces and in the space of
  probability measures}.
\newblock Springer Science \& Business Media, 2005.

\bibitem[Balasubramanian et~al.(2022)Balasubramanian, Chewi, Erdogdu, Salim,
  and Zhang]{sinho-non-log-concave-lmc}
K.~Balasubramanian, S.~Chewi, M.~A. Erdogdu, A.~Salim, and S.~Zhang.
\newblock Towards a theory of non-log-concave sampling:first-order stationarity
  guarantees for langevin monte carlo.
\newblock In P.-L. Loh and M.~Raginsky, editors, \emph{Proceedings of Thirty
  Fifth Conference on Learning Theory}, volume 178 of \emph{Proceedings of
  Machine Learning Research}, pages 2896--2923. PMLR, 02--05 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v178/balasubramanian22a.html}.

\bibitem[Bernton(2018)]{bernton2018-lmc-jko}
E.~Bernton.
\newblock Langevin monte carlo and jko splitting.
\newblock In \emph{Conference On Learning Theory}, pages 1777--1798. PMLR,
  2018.

\bibitem[Bobkov and Ledoux(1997)]{bobkov1997poincare}
S.~Bobkov and M.~Ledoux.
\newblock Poincar{\'e}’s inequalities and talagrand’s concentration
  phenomenon for the exponential distribution.
\newblock \emph{Probability Theory and Related Fields}, 107:\penalty0 383--400,
  1997.

\bibitem[Carmeli et~al.(2010)Carmeli, De~Vito, Toigo, and
  Umanit{\'a}]{carmeli2010vector}
C.~Carmeli, E.~De~Vito, A.~Toigo, and V.~Umanit{\'a}.
\newblock Vector valued reproducing kernel hilbert spaces and universality.
\newblock \emph{Analysis and Applications}, 8\penalty0 (01):\penalty0 19--61,
  2010.

\bibitem[Chewi et~al.(2020)Chewi, Le~Gouic, Lu, Maunu, and
  Rigollet]{chewi2020svgd}
S.~Chewi, T.~Le~Gouic, C.~Lu, T.~Maunu, and P.~Rigollet.
\newblock Svgd as a kernelized wasserstein gradient flow of the chi-squared
  divergence.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2098--2109, 2020.

\bibitem[Chewi et~al.(2022)Chewi, Erdogdu, Li, Shen, and
  Zhang]{sinho-lmc-poincare-lsi}
S.~Chewi, M.~A. Erdogdu, M.~Li, R.~Shen, and S.~Zhang.
\newblock Analysis of langevin monte carlo from poincare to log-sobolev.
\newblock In P.-L. Loh and M.~Raginsky, editors, \emph{Proceedings of Thirty
  Fifth Conference on Learning Theory}, volume 178 of \emph{Proceedings of
  Machine Learning Research}, pages 1--2. PMLR, 02--05 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v178/chewi22a.html}.

\bibitem[Chwialkowski et~al.(2016)Chwialkowski, Strathmann, and
  Gretton]{chwialkowski2016kernel}
K.~Chwialkowski, H.~Strathmann, and A.~Gretton.
\newblock A kernel test of goodness of fit.
\newblock In \emph{International conference on machine learning}, pages
  2606--2615. PMLR, 2016.

\bibitem[Conway(2019)]{conway2019functional}
J.~B. Conway.
\newblock \emph{A course in functional analysis}, volume~96.
\newblock Springer, 2019.

\bibitem[Das et~al.(2022)Das, Sch{\"o}lkopf, and Muehlebach]{das2022sampling}
A.~Das, B.~Sch{\"o}lkopf, and M.~Muehlebach.
\newblock Sampling without replacement leads to faster rates in finite-sum
  minimax optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Das et~al.(2023)Das, Nagaraj, and Raj]{das2022utilising}
A.~Das, D.~Nagaraj, and A.~Raj.
\newblock Utilising the clt structure in stochastic gradient based sampling:
  Improved analysis and faster algorithms.
\newblock In \emph{Conference on Learning Theory}, 2023.

\bibitem[Dudley(2018)]{dudley2018real}
R.~M. Dudley.
\newblock \emph{Real analysis and probability}.
\newblock CRC Press, 2018.

\bibitem[Duncan et~al.(2019)Duncan, N{\"u}sken, and
  Szpruch]{duncan2019geometry}
A.~Duncan, N.~N{\"u}sken, and L.~Szpruch.
\newblock On the geometry of stein variational gradient descent.
\newblock \emph{arXiv preprint arXiv:1912.00894}, 2019.

\bibitem[El~Alaoui et~al.(2022)El~Alaoui, Montanari, and
  Sellke]{el2022sampling}
A.~El~Alaoui, A.~Montanari, and M.~Sellke.
\newblock Sampling from the sherrington-kirkpatrick gibbs measure via
  algorithmic stochastic localization.
\newblock In \emph{2022 IEEE 63rd Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 323--334. IEEE, 2022.

\bibitem[Gershman et~al.(2012)Gershman, Hoffman, and
  Blei]{gershman2012nonparametric}
S.~Gershman, M.~D. Hoffman, and D.~M. Blei.
\newblock Nonparametric variational inference.
\newblock In \emph{Proceedings of the 29th International Conference on
  International Conference on Machine Learning}, 2012.

\bibitem[Gopi et~al.(2022)Gopi, Lee, and Liu]{gopi2022private}
S.~Gopi, Y.~T. Lee, and D.~Liu.
\newblock Private convex optimization via exponential mechanism.
\newblock In \emph{Conference on Learning Theory}, pages 1948--1989. PMLR,
  2022.

\bibitem[Gorham et~al.(2020)Gorham, Raj, and Mackey]{gorham2020stochastic}
J.~Gorham, A.~Raj, and L.~Mackey.
\newblock Stochastic stein discrepancies.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17931--17942, 2020.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
T.~Haarnoja, H.~Tang, P.~Abbeel, and S.~Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International conference on machine learning}, pages
  1352--1361. PMLR, 2017.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
J.~Ho, A.~Jain, and P.~Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6840--6851, 2020.

\bibitem[Jain et~al.(2021)Jain, Nagaraj, and Netrapalli]{jain2021making}
P.~Jain, D.~M. Nagaraj, and P.~Netrapalli.
\newblock Making the last iterate of sgd information theoretically optimal.
\newblock \emph{SIAM Journal on Optimization}, 31\penalty0 (2):\penalty0
  1108--1130, 2021.

\bibitem[Jaini et~al.(2021)Jaini, Holdijk, and Welling]{jaini2021learning}
P.~Jaini, L.~Holdijk, and M.~Welling.
\newblock Learning equivariant energy based models with equivariant stein
  variational gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 16727--16737, 2021.

\bibitem[Jin et~al.(2020)Jin, Li, and Liu]{jin2020random}
S.~Jin, L.~Li, and J.-G. Liu.
\newblock Random batch methods (rbm) for interacting particle systems.
\newblock \emph{Journal of Computational Physics}, 400:\penalty0 108877, 2020.

\bibitem[Khaled and Richt{\'a}rik(2020)]{khaled2020better}
A.~Khaled and P.~Richt{\'a}rik.
\newblock Better theory for sgd in the nonconvex world.
\newblock \emph{arXiv preprint arXiv:2002.03329}, 2020.

\bibitem[Kinoshita and Suzuki(2022)]{kinoshita2022improved}
Y.~Kinoshita and T.~Suzuki.
\newblock Improved convergence rate of stochastic gradient langevin dynamics
  with variance reduction and its application to optimization.
\newblock \emph{arXiv preprint arXiv:2203.16217}, 2022.

\bibitem[Korba et~al.(2020)Korba, Salim, Arbel, Luise, and
  Gretton]{korba2020non}
A.~Korba, A.~Salim, M.~Arbel, G.~Luise, and A.~Gretton.
\newblock A non-asymptotic analysis for stein variational gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4672--4682, 2020.

\bibitem[Kushner and Clark(2012)]{kushner2012stochastic}
H.~J. Kushner and D.~S. Clark.
\newblock \emph{Stochastic approximation methods for constrained and
  unconstrained systems}, volume~26.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Lee and Vempala(2022)]{lee2022manifold}
Y.~T. Lee and S.~S. Vempala.
\newblock The manifold joys of sampling (invited talk).
\newblock In \emph{49th International Colloquium on Automata, Languages, and
  Programming (ICALP 2022)}. Schloss Dagstuhl-Leibniz-Zentrum f{\"u}r
  Informatik, 2022.

\bibitem[Li et~al.(2020)Li, Li, Liu, Liu, and Lu]{li2020stochastic}
L.~Li, Y.~Li, J.-G. Liu, Z.~Liu, and J.~Lu.
\newblock A stochastic version of stein variational gradient descent for
  efficient sampling.
\newblock \emph{Communications in Applied Mathematics and Computational
  Science}, 15\penalty0 (1):\penalty0 37--63, 2020.

\bibitem[Liu(2017)]{liu2017stein}
Q.~Liu.
\newblock Stein variational gradient descent as gradient flow.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Liu and Wang(2016)]{liu2016stein}
Q.~Liu and D.~Wang.
\newblock Stein variational gradient descent: A general purpose bayesian
  inference algorithm.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Liu et~al.(2016)Liu, Lee, and Jordan]{liu2016kernelized}
Q.~Liu, J.~Lee, and M.~Jordan.
\newblock A kernelized stein discrepancy for goodness-of-fit tests.
\newblock In \emph{International conference on machine learning}, pages
  276--284. PMLR, 2016.

\bibitem[Liu et~al.(2023)Liu, Ghosal, Balasubramanian, and
  Pillai]{liu2023understanding}
T.~Liu, P.~Ghosal, K.~Balasubramanian, and N.~S. Pillai.
\newblock Towards understanding the dynamics of gaussian-stein variational
  gradient descent, 2023.

\bibitem[Liu et~al.(2017)Liu, Ramachandran, Liu, and Peng]{liu2017steinpg}
Y.~Liu, P.~Ramachandran, Q.~Liu, and J.~Peng.
\newblock Stein variational policy gradient.
\newblock \emph{arXiv preprint arXiv:1704.02399}, 2017.

\bibitem[Lu et~al.(2019)Lu, Lu, and Nolen]{lu2019scaling}
J.~Lu, Y.~Lu, and J.~Nolen.
\newblock Scaling limit of the stein variational gradient descent: The mean
  field regime.
\newblock \emph{SIAM Journal on Mathematical Analysis}, 51\penalty0
  (2):\penalty0 648--671, 2019.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
R.~M. Neal et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock \emph{Handbook of markov chain monte carlo}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Nesterov(1998)]{nesterov1998introductory}
Y.~Nesterov.
\newblock Introductory lectures on convex programming volume i: Basic course.
\newblock \emph{Lecture notes}, 3\penalty0 (4):\penalty0 5, 1998.

\bibitem[N{\"u}sken and Renger(2021)]{nusken2021stein}
N.~N{\"u}sken and D.~Renger.
\newblock Stein variational gradient descent: many-particle and long-time
  asymptotics.
\newblock \emph{arXiv preprint arXiv:2102.12956}, 2021.

\bibitem[Parisi(1981)]{parisi1981correlation}
G.~Parisi.
\newblock Correlation functions and computer simulations.
\newblock \emph{Nuclear Physics B}, 180\penalty0 (3):\penalty0 378--384, 1981.

\bibitem[Raginsky et~al.(2017)Raginsky, Rakhlin, and
  Telgarsky]{raginsky2017non}
M.~Raginsky, A.~Rakhlin, and M.~Telgarsky.
\newblock Non-convex learning via stochastic gradient langevin dynamics: a
  nonasymptotic analysis.
\newblock In \emph{Conference on Learning Theory}, pages 1674--1703. PMLR,
  2017.

\bibitem[Roberts and Tweedie(1996)]{roberts1996exponential}
G.~O. Roberts and R.~L. Tweedie.
\newblock Exponential convergence of langevin distributions and their discrete
  approximations.
\newblock \emph{Bernoulli}, pages 341--363, 1996.

\bibitem[Salim et~al.(2022)Salim, Sun, and Richtarik]{salim2022convergence}
A.~Salim, L.~Sun, and P.~Richtarik.
\newblock A convergence theory for svgd in the population limit under
  talagrand’s inequality t1.
\newblock In \emph{International Conference on Machine Learning}, pages
  19139--19152. PMLR, 2022.

\bibitem[Shi and Mackey(2022)]{shi2022finite}
J.~Shi and L.~Mackey.
\newblock A finite-particle convergence rate for stein variational gradient
  descent.
\newblock \emph{arXiv preprint arXiv:2211.09721}, 2022.

\bibitem[Steinwart and Christmann(2008)]{steinwart2008support}
I.~Steinwart and A.~Christmann.
\newblock \emph{Support vector machines}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Sun et~al.(2023)Sun, Karagulyan, and Richtarik]{sun2023convergence}
L.~Sun, A.~Karagulyan, and P.~Richtarik.
\newblock Convergence of stein variational gradient descent under a weaker
  smoothness condition.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3693--3717. PMLR, 2023.

\bibitem[Talagrand(1984)]{talagrand1984pettis}
M.~Talagrand.
\newblock \emph{Pettis integral and measure theory}.
\newblock American Mathematical Soc., 1984.

\bibitem[Vempala and Wibisono(2019)]{vempala2019rapid}
S.~Vempala and A.~Wibisono.
\newblock Rapid convergence of the unadjusted langevin algorithm: Isoperimetry
  suffices.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Vershynin(2018)]{vershynin2018high}
R.~Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Villani et~al.(2009)]{villani2009optimal}
C.~Villani et~al.
\newblock \emph{Optimal transport: old and new}, volume 338.
\newblock Springer, 2009.

\bibitem[Wang and Liu(2016)]{wang2016learning}
D.~Wang and Q.~Liu.
\newblock Learning to draw samples: With application to amortized mle for
  generative adversarial learning.
\newblock \emph{arXiv preprint arXiv:1611.01722}, 2016.

\bibitem[Wang et~al.(2018)Wang, Zeng, and Liu]{wang2018stein}
D.~Wang, Z.~Zeng, and Q.~Liu.
\newblock Stein variational message passing for continuous graphical models.
\newblock In \emph{International Conference on Machine Learning}, pages
  5219--5227. PMLR, 2018.

\bibitem[Welling and Teh(2011)]{welling2011bayesian}
M.~Welling and Y.~W. Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688, 2011.

\bibitem[Wibisono(2018)]{wibisono2018-sampling-as-opt}
A.~Wibisono.
\newblock Sampling as optimization in the space of measures: The langevin
  dynamics as a composite optimization problem.
\newblock In \emph{Conference on Learning Theory}, pages 2093--3027. PMLR,
  2018.

\bibitem[Zhuo et~al.(2018)Zhuo, Liu, Shi, Zhu, Chen, and
  Zhang]{zhuo2018message}
J.~Zhuo, C.~Liu, J.~Shi, J.~Zhu, N.~Chen, and B.~Zhang.
\newblock Message passing stein variational gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  6018--6027. PMLR, 2018.

\bibitem[Zou et~al.(2021)Zou, Xu, and Gu]{zou2021faster}
D.~Zou, P.~Xu, and Q.~Gu.
\newblock Faster convergence of stochastic gradient langevin dynamics for
  non-log-concave sampling.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 1152--1162.
  PMLR, 2021.

\end{thebibliography}
