\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Bossard et~al.(2014)Bossard, Guillaumin, and Van~Gool]{bossard2014food}
Lukas Bossard, Matthieu Guillaumin, and Luc Van~Gool.
\newblock Food-101--mining discriminative components with random forests.
\newblock In \emph{Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13}, pages 446--461. Springer, 2014.

\bibitem[Cimpoi et~al.(2014)Cimpoi, Maji, Kokkinos, Mohamed, and Vedaldi]{cimpoi2014describing}
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.
\newblock Describing textures in the wild.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages 3606--3613, 2014.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE Conference on Computer Vision and Pattern Recognition}, pages 248--255. IEEE, 2009.

\bibitem[Fei-Fei et~al.(2004)Fei-Fei, Fergus, and Perona]{fei2004learning}
Li~Fei-Fei, Rob Fergus, and Pietro Perona.
\newblock Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories.
\newblock In \emph{2004 Conference on Computer Vision and Pattern Recognition Workshop}, pages 178--178. IEEE, 2004.

\bibitem[Feng et~al.(2023)Feng, Yu, Liu, Khan, and Zuo]{feng2023diverse}
Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and Wangmeng Zuo.
\newblock Diverse data augmentation with diffusions for effective test-time prompt tuning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2704--2714, 2023.

\bibitem[Gao et~al.(2024{\natexlab{a}})Gao, Gu, Bai, Xia, Torr, Liu, and Li]{gao2024energy}
Kuofeng Gao, Jindong Gu, Yang Bai, Shu-Tao Xia, Philip Torr, Wei Liu, and Zhifeng Li.
\newblock Energy-latency manipulation of multi-modal large language models via verbose samples.
\newblock \emph{arXiv preprint arXiv:2404.16557}, 2024{\natexlab{a}}.

\bibitem[Gao et~al.(2024{\natexlab{b}})Gao, Geng, Zhang, Ma, Fang, Zhang, Li, and Qiao]{gao2024clip}
Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu~Qiao.
\newblock Clip-adapter: Better vision-language models with feature adapters.
\newblock \emph{International Journal of Computer Vision}, 132\penalty0 (2):\penalty0 581--595, 2024{\natexlab{b}}.

\bibitem[Guo et~al.(2024)Guo, Dai, Ouyang, Zhang, Zha, Chen, and Xia]{guo2024refir}
Hang Guo, Tao Dai, Zhihao Ouyang, Taolin Zhang, Yaohua Zha, Bin Chen, and Shu-tao Xia.
\newblock Refir: Grounding large restoration models with retrieval augmentation.
\newblock \emph{arXiv preprint arXiv:2410.05601}, 2024.

\bibitem[Guo et~al.(2023)Guo, Zhang, Qiu, Ma, Miao, He, and Cui]{guo2023calip}
Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng Miao, Xuming He, and Bin Cui.
\newblock Calip: Zero-shot enhancement of clip with parameter-free attention.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 746--754, 2023.

\bibitem[Helber et~al.(2019)Helber, Bischke, Dengel, and Borth]{helber2019eurosat}
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth.
\newblock Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.
\newblock \emph{IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 12\penalty0 (7):\penalty0 2217--2226, 2019.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and perturbations.
\newblock \emph{arXiv preprint arXiv:1903.12261}, 2019.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo, Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2021many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of out-of-distribution generalization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 8340--8349, 2021{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Zhao, Basart, Steinhardt, and Song]{hendrycks2021natural}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 15262--15271, 2021{\natexlab{b}}.

\bibitem[Iwasawa and Matsuo(2021)]{iwasawa2021test}
Yusuke Iwasawa and Yutaka Matsuo.
\newblock Test-time classifier adjustment module for model-agnostic domain generalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 2427--2440, 2021.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and Duerig]{jia2021scaling}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with noisy text supervision.
\newblock In \emph{ICML}, pages 4904--4916. PMLR, 2021.

\bibitem[Karmanov et~al.(2024)Karmanov, Guan, Lu, Saddik, and Xing]{karmanov2024efficient}
Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb~El Saddik, and Eric Xing.
\newblock Efficient test-time adaptation of vision-language models.
\newblock \emph{arXiv preprint arXiv:2403.18293}, 2024.

\bibitem[Khattak et~al.(2023)Khattak, Rasheed, Maaz, Khan, and Khan]{khattakMaPLe}
Muhammad~Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad~Shahbaz Khan.
\newblock Maple: Multi-modal prompt learning.
\newblock In \emph{The IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2023.

\bibitem[Krause et~al.(2013)Krause, Stark, Deng, and Fei-Fei]{krause20133d}
Jonathan Krause, Michael Stark, Jia Deng, and Li~Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer Vision Workshops}, pages 554--561, 2013.

\bibitem[Kumari et~al.(2023)Kumari, Zhang, Zhang, Shechtman, and Zhu]{kumari2023multi}
Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu.
\newblock Multi-concept customization of text-to-image diffusion.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 1931--1941, 2023.

\bibitem[Lee et~al.(2024)Lee, Jung, Lee, Park, Shin, Hwang, and Yoon]{lee2024entropy}
Jonghyun Lee, Dahuin Jung, Saehyung Lee, Junsung Park, Juhyeon Shin, Uiwon Hwang, and Sungroh Yoon.
\newblock Entropy is not enough for test-time adaptation: From the perspective of disentangled factors.
\newblock \emph{arXiv preprint arXiv:2403.07366}, 2024.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock \emph{arXiv preprint arXiv:2104.08691}, 2021.

\bibitem[Li et~al.(2021)Li, Selvaraju, Gotmare, Joty, Xiong, and Hoi]{li2021align}
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu~Hong Hoi.
\newblock Align before fuse: Vision and language representation learning with momentum distillation.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 9694--9705, 2021.

\bibitem[Li et~al.(2022)Li, Li, Xiong, and Hoi]{li2022blip}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
\newblock In \emph{International conference on machine learning}, pages 12888--12900. PMLR, 2022.

\bibitem[Li et~al.(2023)Li, Li, Savarese, and Hoi]{li2023blip}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In \emph{International conference on machine learning}, pages 19730--19742. PMLR, 2023.

\bibitem[Li et~al.(2024)Li, Lian, Lu, Bai, Chen, and Wang]{li2024graphadapter}
Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, and Xinchao Wang.
\newblock Graphadapter: Tuning vision-language models with dual knowledge graph.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Liang et~al.(2017)Liang, Li, and Srikant]{liang2017enhancing}
Shiyu Liang, Yixuan Li, and Rayadurgam Srikant.
\newblock Enhancing the reliability of out-of-distribution image detection in neural networks.
\newblock \emph{arXiv preprint arXiv:1706.02690}, 2017.

\bibitem[Liu et~al.(2021)Liu, Ji, Fu, Tam, Du, Yang, and Tang]{liu2021p}
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng~Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.
\newblock P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.
\newblock \emph{arXiv preprint arXiv:2110.07602}, 2021.

\bibitem[Lu et~al.(2022)Lu, Liu, Zhang, Liu, and Tian]{lu2022prompt}
Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian.
\newblock Prompt distribution learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 5206--5215, 2022.

\bibitem[Lu et~al.(2023)Lu, Bai, Li, Xiao, and Wang]{lu2023beyond}
Zhihe Lu, Jiawang Bai, Xin Li, Zeyu Xiao, and Xinchao Wang.
\newblock Beyond sole strength: Customized ensembles for generalized vision-language models.
\newblock \emph{arXiv preprint arXiv:2311.17091}, 2023.

\bibitem[Maji et~al.(2013)Maji, Rahtu, Kannala, Blaschko, and Vedaldi]{maji2013fine}
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.
\newblock Fine-grained visual classification of aircraft.
\newblock \emph{arXiv preprint arXiv:1306.5151}, 2013.

\bibitem[Nilsback and Zisserman(2008)]{nilsback2008automated}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{2008 Sixth Indian Conference on Computer Vision, Graphics \& Image Processing}, pages 722--729. IEEE, 2008.

\bibitem[Niu et~al.(2023)Niu, Wu, Zhang, Wen, Chen, Zhao, and Tan]{niu2023towards}
Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan.
\newblock Towards stable test-time adaptation in dynamic wild world.
\newblock \emph{arXiv preprint arXiv:2302.12400}, 2023.

\bibitem[Parkhi et~al.(2012)Parkhi, Vedaldi, Zisserman, and Jawahar]{parkhi2012cats}
Omkar~M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV~Jawahar.
\newblock Cats and dogs.
\newblock In \emph{2012 IEEE Conference on Computer Vision and Pattern Recognition}, pages 3498--3505. IEEE, 2012.

\bibitem[Pratt et~al.(2023)Pratt, Covert, Liu, and Farhadi]{pratt2023does}
Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi.
\newblock What does a platypus look like? generating customized prompts for zero-shot image classification.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 15691--15701, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International Conference on Machine Learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and Shankar]{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International Conference on Machine Learning}, pages 5389--5400. PMLR, 2019.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem[Samadh et~al.(2023)Samadh, Gani, Hussein, Khattak, Naseer, Khan, and Khan]{samadh2023align}
Jameel Hassan~Abdul Samadh, Hanan Gani, Noor~Hazim Hussein, Muhammad~Uzair Khattak, Muzammal Naseer, Fahad Khan, and Salman Khan.
\newblock Align your prompts: Test-time prompting with distribution alignment for zero-shot generalization.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Shen et~al.(2018)Shen, Qu, Zhang, and Yu]{shen2018wasserstein}
Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu.
\newblock Wasserstein distance guided representation learning for domain adaptation.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~32, 2018.

\bibitem[Shu et~al.(2022)Shu, Nie, Huang, Yu, Goldstein, Anandkumar, and Xiao]{shu2022test}
Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao.
\newblock Test-time prompt tuning for zero-shot generalization in vision-language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 14274--14289, 2022.

\bibitem[Soomro et~al.(2012)Soomro, Zamir, and Shah]{soomro2012dataset}
Khurram Soomro, Amir~Roshan Zamir, and Mubarak Shah.
\newblock A dataset of 101 human action classes from videos in the wild.
\newblock \emph{Center for Research in Computer Vision}, 2\penalty0 (11), 2012.

\bibitem[Wang et~al.(2020)Wang, Shelhamer, Liu, Olshausen, and Darrell]{wang2020tent}
Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell.
\newblock Tent: Fully test-time adaptation by entropy minimization.
\newblock \emph{arXiv preprint arXiv:2006.10726}, 2020.

\bibitem[Wang et~al.(2019)Wang, Ge, Lipton, and Xing]{wang2019learning}
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric~P Xing.
\newblock Learning robust global representations by penalizing local predictive power.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Zhang, Yan, Zhang, and Li]{wang2023feature}
Shuai Wang, Daoan Zhang, Zipei Yan, Jianguo Zhang, and Rui Li.
\newblock Feature alignment and uniformity for test time adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 20050--20060, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Zhang, Cen, Gao, Zhang, Zhao, and Sang]{wang2023clip}
Xiang Wang, Shiwei Zhang, Jun Cen, Changxin Gao, Yingya Zhang, Deli Zhao, and Nong Sang.
\newblock Clip-guided prototype modulating for few-shot action recognition.
\newblock \emph{International Journal of Computer Vision}, pages 1--14, 2023{\natexlab{b}}.

\bibitem[Wasim et~al.(2023)Wasim, Naseer, Khan, Khan, and Shah]{wasim2023vita}
Syed~Talal Wasim, Muzammal Naseer, Salman Khan, Fahad~Shahbaz Khan, and Mubarak Shah.
\newblock Vita-clip: Video and text adaptive clip via multimodal prompting.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 23034--23044, 2023.

\bibitem[Xiao et~al.(2010)Xiao, Hays, Ehinger, Oliva, and Torralba]{xiao2010sun}
Jianxiong Xiao, James Hays, Krista~A Ehinger, Aude Oliva, and Antonio Torralba.
\newblock Sun database: Large-scale scene recognition from abbey to zoo.
\newblock In \emph{2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition}, pages 3485--3492. IEEE, 2010.

\bibitem[Yang et~al.(2022)Yang, Duan, Tran, Xu, Chanda, Chen, Zeng, Chilimbi, and Huang]{yang2022vision}
Jinyu Yang, Jiali Duan, Son Tran, Yi~Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi, and Junzhou Huang.
\newblock Vision-language pre-training with triple contrastive learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 15671--15680, 2022.

\bibitem[Yang et~al.(2024)Yang, Bai, Gao, Yang, Li, and Xia]{yang2024not}
Sheng Yang, Jiawang Bai, Kuofeng Gao, Yong Yang, Yiming Li, and Shu-Tao Xia.
\newblock Not all prompts are secure: A switchable backdoor attack against pre-trained vision transfomers.
\newblock In \emph{CVPR}, 2024.

\bibitem[Zha et~al.(2023)Zha, Wang, Dai, Chen, Wang, and Xia]{zha2023instance}
Yaohua Zha, Jinpeng Wang, Tao Dai, Bin Chen, Zhi Wang, and Shu-Tao Xia.
\newblock Instance-aware dynamic prompt tuning for pre-trained point cloud models.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 14161--14170, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Fang, Gao, Li, Dai, Qiao, and Li]{zhang2022tip}
Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu~Qiao, and Hongsheng Li.
\newblock Tip-adapter: Training-free adaption of clip for few-shot classification.
\newblock In \emph{European conference on computer vision}, pages 493--510. Springer, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Wang, Jin, Yuan, Zhang, Wang, Jin, and Tan]{zhang2023adanpc}
Yifan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, and Tieniu Tan.
\newblock Adanpc: Exploring non-parametric classifier for test-time adaptation.
\newblock In \emph{International Conference on Machine Learning}, pages 41647--41676. PMLR, 2023.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Yang, Loy, and Liu]{zhou2022cocoop}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Conditional prompt learning for vision-language models.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022{\natexlab{a}}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, Yang, Loy, and Liu]{zhou2022coop}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Learning to prompt for vision-language models.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 2022{\natexlab{b}}.

\bibitem[Zhu et~al.(2023)Zhu, Zhang, He, Zhou, Wang, Zhao, and Gao]{zhu2023not}
Xiangyang Zhu, Renrui Zhang, Bowei He, Aojun Zhou, Dong Wang, Bin Zhao, and Peng Gao.
\newblock Not all features matter: Enhancing few-shot clip with adaptive prior refinement.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2605--2615, 2023.

\end{thebibliography}
