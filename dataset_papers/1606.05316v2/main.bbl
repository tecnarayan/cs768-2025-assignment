\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bach(2014)]{bach2014breaking}
Bach, Francis.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{arXiv preprint arXiv:1412.8690}, 2014.

\bibitem[Bach(2015)]{bach2015equivalence}
Bach, Francis.
\newblock On the equivalence between kernel quadrature rules and random feature
  expansions.
\newblock \emph{arXiv preprint arXiv:1502.06800}, 2015.

\bibitem[Bo et~al.(2010)Bo, Ren, and Fox]{bo2010kernel}
Bo, Liefeng, Ren, Xiaofeng, and Fox, Dieter.
\newblock Kernel descriptors for visual recognition.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  244--252, 2010.

\bibitem[Bo et~al.(2011)Bo, Lai, Ren, and Fox]{bo2011object}
Bo, Liefeng, Lai, Kevin, Ren, Xiaofeng, and Fox, Dieter.
\newblock Object recognition with hierarchical kernel descriptors.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR), 2011 IEEE
  Conference on}, pp.\  1729--1736. IEEE, 2011.

\bibitem[Bouvrie et~al.(2009)Bouvrie, Rosasco, and
  Poggio]{bouvrie2009invariance}
Bouvrie, Jake, Rosasco, Lorenzo, and Poggio, Tomaso.
\newblock On invariance in hierarchical models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  162--170, 2009.

\bibitem[Cesa-Bianchi et~al.(2011{\natexlab{a}})Cesa-Bianchi, Shalev-Shwartz,
  and Shamir]{cesa2011efficient}
Cesa-Bianchi, Nicolo, Shalev-Shwartz, Shai, and Shamir, Ohad.
\newblock Efficient learning with partially observed attributes.
\newblock \emph{The Journal of Machine Learning Research}, 12:\penalty0
  2857--2878, 2011{\natexlab{a}}.

\bibitem[Cesa-Bianchi et~al.(2011{\natexlab{b}})Cesa-Bianchi, Shalev-Shwartz,
  and Shamir]{cesa2011online}
Cesa-Bianchi, Nicolo, Shalev-Shwartz, Shai, and Shamir, Ohad.
\newblock Online learning of noisy data.
\newblock \emph{Information Theory, IEEE Transactions on}, 57\penalty0
  (12):\penalty0 7907--7931, 2011{\natexlab{b}}.

\bibitem[Cesa-Bianchi et~al.(2015)Cesa-Bianchi, Mansour, and
  Shamir]{cesa2015complexity}
Cesa-Bianchi, Nicol{\`o}, Mansour, Yishay, and Shamir, Ohad.
\newblock On the complexity of learning with kernels.
\newblock In \emph{Proceedings of The 28th Conference on Learning Theory}, pp.\
   297--325, 2015.

\bibitem[Cho \& Saul(2009)Cho and Saul]{cho2009kernel}
Cho, Youngmin and Saul, Lawrence~K.
\newblock Kernel methods for deep learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  342--350, 2009.

\bibitem[Cortes \& Vapnik(1995)Cortes and Vapnik]{cortes1995support}
Cortes, Corinna and Vapnik, Vladimir.
\newblock Support-vector networks.
\newblock \emph{Machine learning}, 20\penalty0 (3):\penalty0 273--297, 1995.

\bibitem[Dai et~al.(2014)Dai, Xie, He, Liang, Raj, Balcan, and
  Song]{dai2014scalable}
Dai, Bo, Xie, Bo, He, Niao, Liang, Yingyu, Raj, Anant, Balcan, Maria-Florina~F,
  and Song, Le.
\newblock Scalable kernel methods via doubly stochastic gradients.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3041--3049, 2014.

\bibitem[Daniely et~al.(2014)Daniely, Linial, and
  Shalev-Shwartz]{daniely2014average}
Daniely, Amit, Linial, Nati, and Shalev-Shwartz, Shai.
\newblock From average case complexity to improper learning complexity.
\newblock In \emph{Proceedings of the 46th Annual ACM Symposium on Theory of
  Computing}, pp.\  441--448. ACM, 2014.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{daniely2016toward}
Daniely, Amit, Frostig, Roy, and Singer, Yoram.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In Lee, D.~D., Sugiyama, M., Luxburg, U.~V., Guyon, I., and Garnett,
  R. (eds.), \emph{Advances in Neural Information Processing Systems 29}, pp.\
  2253--2261. Curran Associates, Inc., 2016.

\bibitem[Deng et~al.(2012)Deng, Tur, He, and Hakkani-Tur]{deng2012use}
Deng, Li, Tur, Gokhan, He, Xiaodong, and Hakkani-Tur, Dilek.
\newblock Use of kernel deep convex networks and end-to-end learning for spoken
  language understanding.
\newblock In \emph{Spoken Language Technology Workshop (SLT), 2012 IEEE}, pp.\
  210--215. IEEE, 2012.

\bibitem[Hazan \& Koren(2012)Hazan and Koren]{hazan2012linear}
Hazan, Elad and Koren, Tomer.
\newblock Linear regression with limited observation.
\newblock In \emph{Proceedings of the 29th International Conference on Machine
  Learning (ICML-12)}, pp.\  807--814, 2012.

\bibitem[Hazan \& Jaakkola(2015)Hazan and Jaakkola]{hazan2015steps}
Hazan, Tamir and Jaakkola, Tommi.
\newblock Steps toward deep kernel methods from infinite neural networks.
\newblock \emph{arXiv preprint arXiv:1508.05133}, 2015.

\bibitem[Heinemann et~al.(2016)Heinemann, Livni, Eban, Elidan, and
  Globerson]{heinemann2016improper}
Heinemann, Uri, Livni, Roi, Eban, Elad, Elidan, Gal, and Globerson, Amir.
\newblock Improper deep kernels.
\newblock In \emph{Proceedings of the 19th International Conference on
  Artificial Intelligence and Statistics}, pp.\  1159--1167, 2016.

\bibitem[Hornik(1993)]{hornik1993some}
Hornik, Kurt.
\newblock Some new results on neural network approximation.
\newblock \emph{Neural Networks}, 6\penalty0 (8):\penalty0 1069--1072, 1993.

\bibitem[Kar \& Karnick(2012)Kar and Karnick]{kar2012random}
Kar, Purushottam and Karnick, Harish.
\newblock Random feature maps for dot product kernels.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  583--591, 2012.

\bibitem[Kivinen et~al.(2004)Kivinen, Smola, and Williamson]{kivinen2004online}
Kivinen, Jyrki, Smola, Alexander~J, and Williamson, Robert~C.
\newblock Online learning with kernels.
\newblock \emph{IEEE transactions on signal processing}, 52\penalty0
  (8):\penalty0 2165--2176, 2004.

\bibitem[Klivans \& Sherstov(2006)Klivans and
  Sherstov]{klivans2006cryptographic}
Klivans, Adam~R and Sherstov, Alexander~A.
\newblock Cryptographic hardness for learning intersections of halfspaces.
\newblock In \emph{Foundations of Computer Science, 2006. FOCS'06. 47th Annual
  IEEE Symposium on}, pp.\  553--562. IEEE, 2006.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[Livni et~al.(2014)Livni, Shalev-Shwartz, and
  Shamir]{livni2014computational}
Livni, Roi, Shalev-Shwartz, Shai, and Shamir, Ohad.
\newblock On the computational efficiency of training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  855--863, 2014.

\bibitem[Mairal et~al.(2014)Mairal, Koniusz, Harchaoui, and
  Schmid]{mairal2014convolutional}
Mairal, Julien, Koniusz, Piotr, Harchaoui, Zaid, and Schmid, Cordelia.
\newblock Convolutional kernel networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2627--2635, 2014.

\bibitem[Minsky \& Papert(1988)Minsky and Papert]{minsky1988perceptrons}
Minsky, Marvin and Papert, Seymour.
\newblock Perceptrons: an introduction to computational geometry (expanded
  edition), 1988.

\bibitem[Rahimi \& Recht(2007)Rahimi and Recht]{rahimi2007random}
Rahimi, Ali and Recht, Benjamin.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1177--1184, 2007.

\bibitem[Rahimi \& Recht(2009)Rahimi and Recht]{rahimi2009weighted}
Rahimi, Ali and Recht, Benjamin.
\newblock Weighted sums of random kitchen sinks: Replacing minimization with
  randomization in learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1313--1320, 2009.

\bibitem[Shalev-Shwartz(2011)]{shalev2011online}
Shalev-Shwartz, Shai.
\newblock Online learning and online convex optimization.
\newblock \emph{Foundations and Trends in Machine Learning}, 4\penalty0
  (2):\penalty0 107--194, 2011.

\bibitem[Shamir(2015)]{shamir2014sample}
Shamir, Ohad.
\newblock The sample complexity of learning linear predictors with the squared
  loss.
\newblock \emph{Journal of Machine Learning Research}, 16(Dec):\penalty0
  3475--3486, 2015.

\bibitem[Srebro et~al.(2010)Srebro, Sridharan, and
  Tewari]{srebro2010smoothness}
Srebro, Nathan, Sridharan, Karthik, and Tewari, Ambuj.
\newblock Smoothness, low noise and fast rates.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2199--2207, 2010.

\bibitem[Sridharan et~al.(2009)Sridharan, Shalev-Shwartz, and
  Srebro]{sridharan2009fast}
Sridharan, Karthik, Shalev-Shwartz, Shai, and Srebro, Nathan.
\newblock Fast rates for regularized objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1545--1552, 2009.

\bibitem[Suykens \& Vandewalle(1999)Suykens and Vandewalle]{suykens1999least}
Suykens, Johan~AK and Vandewalle, Joos.
\newblock Least squares support vector machine classifiers.
\newblock \emph{Neural processing letters}, 9\penalty0 (3):\penalty0 293--300,
  1999.

\bibitem[Suykens et~al.(2002)Suykens, Van~Gestel, and
  De~Brabanter]{suykens2002weighted}
Suykens, Johan~AK, Van~Gestel, Tony, and De~Brabanter, Jos.
\newblock \emph{Least squares support vector machines}.
\newblock World Scientific, 2002.

\bibitem[Williams(1997)]{williams1997computing}
Williams, Christopher.
\newblock Computing with infinite networks.
\newblock \emph{Advances in neural information processing systems}, pp.\
  295--301, 1997.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Zhang, Chiyuan, Bengio, Samy, Hardt, Moritz, Recht, Benjamin, and Vinyals,
  Oriol.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Zhang(2004)]{zhang2004solving}
Zhang, Tong.
\newblock Solving large scale linear prediction problems using stochastic
  gradient descent algorithms.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, pp.\  116. ACM, 2004.

\bibitem[Zinkevich(2003)]{Zinkevich03}
Zinkevich, Martin.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Machine Learning, Proceedings of the Twentieth International
  Conference}, pp.\  928--936, 2003.

\end{thebibliography}
