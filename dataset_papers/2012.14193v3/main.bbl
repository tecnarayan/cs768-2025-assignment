\begin{thebibliography}{77}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille et~al.(2019)Achille, Rovere, and
  Soatto]{achille_critical_2017}
Alessandro Achille, Matteo Rovere, and Stefano Soatto.
\newblock Critical learning periods in deep networks.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}, 2019.

\bibitem[Alizadeh et~al.(2020)Alizadeh, Behboodi, van Baalen, Louizos,
  Blankevoort, and Welling]{Alizadeh2020Gradient}
Milad Alizadeh, Arash Behboodi, Mart van Baalen, Christos Louizos, Tijmen
  Blankevoort, and Max Welling.
\newblock Gradient $\ell_1$ regularization for quantization robustness.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Amari(1998)]{amari1998}
Shun-Ichi Amari.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural Computation}, 1998.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, 2019.

\bibitem[Arpit et~al.(2017)Arpit, Jastrzebski, Ballas, Krueger, Bengio, Kanwal,
  Maharaj, Fischer, Courville, Bengio, and Lacoste{-}Julien]{arpit_closer_2017}
Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel
  Bengio, Maxinder~S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron~C. Courville,
  Yoshua Bengio, and Simon Lacoste{-}Julien.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, Proceedings
  of Machine Learning Research, 2017.

\bibitem[Arpit et~al.(2019)Arpit, Campos, and Bengio]{arpit2019initialize}
Devansh Arpit, V{\'{\i}}ctor Campos, and Yoshua Bengio.
\newblock How to initialize your network? robust initialization for weightnorm
  {\&} resnets.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, 2019.

\bibitem[Barrett \& Dherin(2021)Barrett and Dherin]{barrett2020implicit}
David Barrett and Benoit Dherin.
\newblock Implicit gradient regularization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Bjorck et~al.(2018)Bjorck, Gomes, Selman, and
  Weinberger]{bjorck_understanding_2018}
Johan Bjorck, Carla~P. Gomes, Bart Selman, and Kilian~Q. Weinberger.
\newblock Understanding batch normalization.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
  December 3-8, 2018, Montr{\'{e}}al, Canada}, 2018.

\bibitem[Chan et~al.(2020)Chan, Tay, Ong, and Fu]{chan2020}
Alvin Chan, Yi~Tay, Yew{-}Soon Ong, and Jie Fu.
\newblock Jacobian adversarially regularized networks for robustness.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Chatterjee(2020)]{Chatterjee2020Coherent}
Satrajit Chatterjee.
\newblock Coherent gradients: An approach to understanding generalization in
  gradient descent-based optimization.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Chaudhari \& Soatto(2018)Chaudhari and
  Soatto]{chaudhari2018stochastic}
Pratik Chaudhari and Stefano Soatto.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem[Chaudhari et~al.(2017)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{chaudhari2019entropy}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
  Christian Borgs, Jennifer~T. Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}, 2017.

\bibitem[Chollet \& {others}(2015)Chollet and {others}]{chollet_keras_2015}
François Chollet and {others}.
\newblock \emph{Keras}.
\newblock 2015.

\bibitem[Cohen et~al.(2021)Cohen, Kaur, Li, Kolter, and
  Talwalkar]{cohen2021gradient}
Jeremy Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[De \& Smith(2020)De and Smith]{de2020batch}
Soham De and Samuel~L. Smith.
\newblock Batch normalization biases residual blocks towards the identity
  function in deep networks.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Li]{deng_imagenet_2009}
Jia Deng, Wei Dong, Richard Socher, Li{-}Jia Li, Kai Li, and Fei{-}Fei Li.
\newblock Imagenet: {A} large-scale hierarchical image database.
\newblock In \emph{2009 {IEEE} Computer Society Conference on Computer Vision
  and Pattern Recognition {(CVPR} 2009), 20-25 June 2009, Miami, Florida,
  {USA}}, 2009.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh_sharp_2017}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, Proceedings
  of Machine Learning Research, 2017.

\bibitem[{Drucker} \& {Le Cun}(1992){Drucker} and {Le Cun}]{Drucker1992}
H.~{Drucker} and Y.~{Le Cun}.
\newblock Improving generalization performance using double backpropagation.
\newblock \emph{IEEE Transactionsf on Neural Networks}, 1992.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2021sharpnessaware}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Fort et~al.(2020{\natexlab{a}})Fort, Dziugaite, Paul, Kharaghani, Roy,
  and Ganguli]{fort_deep_learning}
Stanislav Fort, Gintare~Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani,
  Daniel~M. Roy, and Surya Ganguli.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020{\natexlab{a}}.

\bibitem[Fort et~al.(2020{\natexlab{b}})Fort, Nowak, Jastrzebski, and
  Narayanan]{fort_stiffness_2019}
Stanislav Fort, Paweł~Krzysztof Nowak, Stanislaw Jastrzebski, and Srini
  Narayanan.
\newblock Stiffness: A new perspective on generalization in neural networks,
  2020{\natexlab{b}}.

\bibitem[Frankle et~al.(2020)Frankle, Schwab, and Morcos]{Frankle2020The}
Jonathan Frankle, David~J. Schwab, and Ari~S. Morcos.
\newblock The early phase of neural network training.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Geirhos et~al.(2020)Geirhos, Jacobsen, Michaelis, Zemel, Brendel,
  Bethge, and Wichmann]{Geirhos_2020}
Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel,
  Wieland Brendel, Matthias Bethge, and Felix~A. Wichmann.
\newblock Shortcut learning in deep neural networks.
\newblock \emph{Nature Machine Intelligence}, 2020.

\bibitem[Golatkar et~al.(2019)Golatkar, Achille, and Soatto]{golatkar2019}
Aditya Golatkar, Alessandro Achille, and Stefano Soatto.
\newblock Time matters in regularizing deep networks: Weight decay and data
  augmentation affect early learning dynamics, matter little near convergence.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, 2019.

\bibitem[Gotmare et~al.(2019)Gotmare, Keskar, Xiong, and Socher]{gotmare2018a}
Akhilesh Gotmare, Nitish~Shirish Keskar, Caiming Xiong, and Richard Socher.
\newblock A closer look at deep learning heuristics: Learning rate restarts,
  warmup and distillation.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}, 2019.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{Gulrajani2017}
Ishaan Gulrajani, Faruk Ahmed, Mart{\'{\i}}n Arjovsky, Vincent Dumoulin, and
  Aaron~C. Courville.
\newblock Improved training of wasserstein gans.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
  Long Beach, CA, {USA}}, 2017.

\bibitem[Gur-Ari et~al.(2018)Gur-Ari, Roberts, and Dyer]{gur-ari_gradient_2018}
Guy Gur-Ari, Daniel~A. Roberts, and Ethan Dyer.
\newblock Gradient descent happens in a tiny subspace, 2018.

\bibitem[He et~al.(2019)He, Huang, and Yuan]{He2019}
Haowei He, Gao Huang, and Yang Yuan.
\newblock Asymmetric valleys: Beyond sharp and flat local minima.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he_deep_2015}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016}, 2016.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997flat}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Flat minima.
\newblock \emph{Neural Computation}, 1997.

\bibitem[Hoffman et~al.(2019)Hoffman, Roberts, and Yaida]{hoffman2019}
Judy Hoffman, Daniel~A. Roberts, and Sho Yaida.
\newblock Robust learning with jacobian regularization.
\newblock 2019.

\bibitem[Hu et~al.(2020)Hu, Xiao, Adlam, and Pennington]{hu2020surprising}
Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington.
\newblock The surprising simplicity of the early-time learning dynamics of
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Huang et~al.(2017)Huang, Liu, van~der Maaten, and
  Weinberger]{huang_densely_2016}
Gao Huang, Zhuang Liu, Laurens van~der Maaten, and Kilian~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017}, 2017.

\bibitem[Hutchinson(1990)]{hutchinson1990stochastic}
Michael~F Hutchinson.
\newblock A stochastic estimator of the trace of the influence matrix for
  laplacian smoothing splines.
\newblock \emph{Communications in Statistics-Simulation and Computation}, 1990.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averaging}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry~P. Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In \emph{Proceedings of the Thirty-Fourth Conference on Uncertainty
  in Artificial Intelligence, {UAI} 2018, Monterey, California, USA, August
  6-10, 2018}, 2018.

\bibitem[Jacot et~al.(2018)Jacot, Hongler, and Gabriel]{jacot2018}
Arthur Jacot, Cl{\'{e}}ment Hongler, and Franck Gabriel.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
  December 3-8, 2018, Montr{\'{e}}al, Canada}, 2018.

\bibitem[Jastrzebski et~al.(2017)Jastrzebski, Kenton, Arpit, Ballas, Fischer,
  Bengio, and Storkey]{jastrzebski_three_2017}
Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja
  Fischer, Yoshua Bengio, and Amos~J. Storkey.
\newblock Three {Factors} {Influencing} {Minima} in {SGD}.
\newblock 2017.

\bibitem[Jastrzebski et~al.(2019)Jastrzebski, Kenton, Ballas, Fischer, Bengio,
  and Storkey]{jastrzebski_relation_2018}
Stanislaw Jastrzebski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua
  Bengio, and Amos~J. Storkey.
\newblock On the relation between the sharpest directions of {DNN} loss and the
  {SGD} step length.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}, 2019.

\bibitem[Jastrzebski et~al.(2020)Jastrzebski, Szymczak, Fort, Arpit, Tabor,
  Cho, and Geras]{Jastrzebski2020The}
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek
  Tabor, Kyunghyun Cho, and Krzysztof Geras.
\newblock The break-even point on optimization trajectories of deep neural
  networks.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Jiang et~al.(2020{\natexlab{a}})Jiang, Huang, Liu, and
  Yang]{jiang2020}
Lu~Jiang, Di~Huang, Mason Liu, and Weilong Yang.
\newblock Beyond synthetic noise: Deep learning on controlled noisy labels.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, Proceedings of
  Machine Learning Research, 2020{\natexlab{a}}.

\bibitem[Jiang et~al.(2020{\natexlab{b}})Jiang, Neyshabur, Mobahi, Krishnan,
  and Bengio]{jiang_fantastic_2020}
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy
  Bengio.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020{\natexlab{b}}.

\bibitem[Karakida et~al.(2019)Karakida, Akaho, and Amari]{karakida2019}
Ryo Karakida, Shotaro Akaho, and Shun{-}ichi Amari.
\newblock Universal statistics of fisher information in deep neural networks:
  Mean field approach.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics, {AISTATS} 2019, 16-18 April 2019, Naha, Okinawa, Japan},
  Proceedings of Machine Learning Research, 2019.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar_large-batch_2017}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}, 2017.

\bibitem[Krizhevsky(2009)]{krizhevsky_learning_2009}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Kunstner et~al.(2019)Kunstner, Hennig, and
  Balles]{kunstner2019limitations}
Frederik Kunstner, Philipp Hennig, and Lukas Balles.
\newblock Limitations of the empirical fisher approximation for natural
  gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, 2019.

\bibitem[Le \& Yang(2015)Le and Yang]{Le2015TinyIV}
Y.~Le and X.~Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock 2015.

\bibitem[Leclerc \& Madry(2020)Leclerc and Madry]{leclerc2020regimes}
Guillaume Leclerc and Aleksander Madry.
\newblock The two regimes of deep network training, 2020.

\bibitem[LeCun et~al.(2012)LeCun, Bottou, Orr, and
  Müller]{lecun_efficient_2012}
Yann LeCun, Léon Bottou, Genevieve~B. Orr, and Klaus-Robert Müller.
\newblock Efficient {BackProp}.
\newblock In \emph{Neural {Networks}: {Tricks} of the {Trade} (2nd ed.)}. 2012.

\bibitem[Lewkowycz et~al.(2020)Lewkowycz, Bahri, Dyer, Sohl-Dickstein, and
  Gur-Ari]{lewkowycz2020large}
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy
  Gur-Ari.
\newblock The large learning rate phase of deep learning: the catapult
  mechanism, 2020.

\bibitem[Li et~al.(2020)Li, Socher, and Hoi]{Li2020DivideMix}
Junnan Li, Richard Socher, and Steven C.~H. Hoi.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.

\bibitem[Li et~al.(2017)Li, Tai, and E]{li_stochastic_2017}
Qianxiao Li, Cheng Tai, and Weinan E.
\newblock Stochastic modified equations and adaptive stochastic gradient
  algorithms.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, Proceedings
  of Machine Learning Research, 2017.

\bibitem[Li et~al.(2019)Li, Wei, and Ma]{li2019}
Yuanzhi Li, Colin Wei, and Tengyu Ma.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, 2019.

\bibitem[Liang et~al.(2019)Liang, Poggio, Rakhlin, and Stokes]{liang2019}
Tengyuan Liang, Tomaso~A. Poggio, Alexander Rakhlin, and James Stokes.
\newblock Fisher-rao metric, geometry, and complexity of neural networks.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics, {AISTATS} 2019, 16-18 April 2019, Naha, Okinawa, Japan},
  Proceedings of Machine Learning Research, 2019.

\bibitem[Liu et~al.(2020)Liu, Niles-Weed, Razavian, and
  Fernandez-Granda]{liu2020early}
Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda.
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Martens(2020)]{martens2014}
James Martens.
\newblock New insights and perspectives on the natural gradient method, 2020.

\bibitem[Moosavi{-}Dezfooli et~al.(2019)Moosavi{-}Dezfooli, Fawzi, Uesato, and
  Frossard]{moosavi2019robustness}
Seyed{-}Mohsen Moosavi{-}Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal
  Frossard.
\newblock Robustness via curvature regularization, and vice versa.
\newblock In \emph{{IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2019, Long Beach, CA, USA, June 16-20, 2019}, 2019.

\bibitem[Nam et~al.(2020)Nam, Cha, Ahn, Lee, and Shin]{nam2020learning}
Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin.
\newblock Learning from failure: Training debiased classifier from biased
  classifier.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Neyshabur(2017)]{neyshabur2017}
Behnam Neyshabur.
\newblock Implicit regularization in deep learning.
\newblock 2017.

\bibitem[Poggio et~al.(2018)Poggio, Kawaguchi, Liao, Miranda, Rosasco, Boix,
  Hidary, and Mhaskar]{poggio2017theory}
Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco,
  Xavier Boix, Jack Hidary, and Hrushikesh Mhaskar.
\newblock Theory of deep learning iii: explaining the non-overfitting puzzle,
  2018.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{Rahaman2018OnTS}
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred~A.
  Hamprecht, Yoshua Bengio, and Aaron~C. Courville.
\newblock On the spectral bias of neural networks.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  Proceedings of Machine Learning Research, 2019.

\bibitem[Rifai et~al.(2011)Rifai, Vincent, Muller, Glorot, and
  Bengio]{Rifai2011}
Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio.
\newblock Contractive auto-encoders: Explicit invariance during feature
  extraction.
\newblock In \emph{Proceedings of the 28th International Conference on Machine
  Learning, {ICML} 2011, Bellevue, Washington, USA, June 28 - July 2, 2011},
  2011.

\bibitem[Salakhutdinov(2014)]{goodfellow_deep_2016}
Ruslan Salakhutdinov.
\newblock Deep learning.
\newblock In \emph{The 20th {ACM} {SIGKDD} International Conference on
  Knowledge Discovery and Data Mining, {KDD} '14, New York, NY, {USA} - August
  24 - 27, 2014}, 2014.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{Simonyan15}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{3rd International Conference on Learning Representations,
  {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
  Proceedings}, 2015.

\bibitem[Smith \& Le(2018)Smith and Le]{smith2017understanding}
Samuel~L. Smith and Quoc~V. Le.
\newblock A bayesian perspective on generalization and stochastic gradient
  descent.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem[Smith et~al.(2021)Smith, Dherin, Barrett, and De]{smith2021on}
Samuel~L Smith, Benoit Dherin, David Barrett, and Soham De.
\newblock On the origin of implicit regularization in stochastic gradient
  descent.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Song et~al.(2020)Song, Dauphin, Auli, and Ma]{song2020robust}
Jiaming Song, Yann Dauphin, Michael Auli, and Tengyu Ma.
\newblock Robust and on-the-fly dataset denoising for image classification.
\newblock In \emph{Computer Vision -- ECCV 2020}, 2020.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, and Nathan Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem[Thomas et~al.(2020)Thomas, Pedregosa, van Merri{\"{e}}nboer, Manzagol,
  Bengio, and Roux]{thomas2020interplay}
Valentin Thomas, Fabian Pedregosa, Bart van Merri{\"{e}}nboer, Pierre{-}Antoine
  Manzagol, Yoshua Bengio, and Nicolas~Le Roux.
\newblock On the interplay between noise and curvature and its effect on
  optimization and generalization.
\newblock In \emph{The 23rd International Conference on Artificial Intelligence
  and Statistics, {AISTATS} 2020, 26-28 August 2020, Online [Palermo, Sicily,
  Italy]}, Proceedings of Machine Learning Research, 2020.

\bibitem[Tsuzuku et~al.(2020)Tsuzuku, Sato, and
  Sugiyama]{tsuzuku2019normalized}
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama.
\newblock Normalized flat minima: Exploring scale invariant definition of flat
  minima for neural networks using pac-bayesian analysis.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, Proceedings of
  Machine Learning Research, 2020.

\bibitem[Varga et~al.(2018)Varga, Csiszárik, and Zombori]{varga2018gradient}
Dániel Varga, Adrián Csiszárik, and Zsolt Zombori.
\newblock Gradient regularization improves accuracy of discriminative models,
  2018.

\bibitem[Wen et~al.(2018)Wen, Wang, Yan, Xu, Wu, Chen, and
  Li]{wen2018smoothout}
Wei Wen, Yandan Wang, Feng Yan, Cong Xu, Chunpeng Wu, Yiran Chen, and Hai Li.
\newblock Smoothout: Smoothing out sharp minima to improve generalization in
  deep learning, 2018.

\bibitem[Xu(2018)]{xu2018understanding}
Zhiqin~John Xu.
\newblock Understanding training and generalization in deep learning by fourier
  analysis, 2018.

\bibitem[Yoshida \& Miyato(2017)Yoshida and Miyato]{yoshida2017spectral}
Yuichi Yoshida and Takeru Miyato.
\newblock Spectral norm regularization for improving the generalizability of
  deep learning, 2017.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and Komodakis]{zagoruyko2016}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In \emph{Proceedings of the British Machine Vision Conference 2016,
  {BMVC} 2016, York, UK, September 19-22, 2016}, 2016.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang_understanding_2016}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Ciss{\'{e}}, Dauphin, and
  Lopez{-}Paz]{zhang2018mixup}
Hongyi Zhang, Moustapha Ciss{\'{e}}, Yann~N. Dauphin, and David Lopez{-}Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Dauphin, and Ma]{zhang2019fixup}
Hongyi Zhang, Yann~N. Dauphin, and Tengyu Ma.
\newblock Residual learning without normalization via better initialization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\end{thebibliography}
