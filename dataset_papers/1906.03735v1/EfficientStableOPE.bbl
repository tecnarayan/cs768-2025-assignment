\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bowsher and Swain(2012)]{CliveG.Bowsher2012Isov}
C.~G. Bowsher and P.~S. Swain.
\newblock Identifying sources of variation and the flow of information in
  biochemical networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 109, 2012.

\bibitem[Brockman et~al.()Brockman, Cheung, Pettersson, Schneider, Schulman,
  Tang, , and Zaremba]{gym}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, ,
  and W.~Zaremba.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540, 2016}.

\bibitem[Cao et~al.(2009)Cao, Tsiatis, and Davidian]{CaoWeihua2009Iear}
W.~Cao, A.~A. Tsiatis, and M.~Davidian.
\newblock Improving efficiency and robustness of the doubly robust estimator
  for a population mean with incomplete data.
\newblock \emph{Biometrika}, 96:\penalty0 723--734, 2009.

\bibitem[Dudík et~al.(2014)Dudík, Erhan, Langford, and
  Li]{DudikMiroslav2014DRPE}
M.~Dudík, D.~Erhan, J.~Langford, and L.~Li.
\newblock Doubly robust policy evaluation and optimization.
\newblock \emph{Statistical Science}, 29:\penalty0 485--511, 2014.

\bibitem[Farajtabar et~al.(2018)Farajtabar, Chow, and Ghavamzadeh]{Chow2018}
M.~Farajtabar, Y.~Chow, and M.~Ghavamzadeh.
\newblock More robust doubly robust off-policy evaluation.
\newblock \emph{In Proceedings of the 35th International Conference on Machine
  Learning}, pages 1447--1456, 2018.

\bibitem[Jiang and Li(2016)]{jiang}
N.~Jiang and L.~Li.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock \emph{In Proceedings of the 33rd International Conference on
  International Conference on Machine Learning-Volume}, pages 652--661, 2016.

\bibitem[Kang and Schafer(2007)]{KangJosephD.Y.2007DDRA}
J.~D.~Y. Kang and J.~L. Schafer.
\newblock Demystifying double robustness: A comparison of alternative
  strategies for estimating a population mean from incomplete data.
\newblock \emph{Statistical Science}, 22:\penalty0 523--539, 2007.

\bibitem[Li et~al.(2015)Li, Munos, and Szepesvari]{Li2015}
L.~Li, R.~Munos, and C.~Szepesvari.
\newblock Toward minimax off-policy value estimation.
\newblock \emph{In Proceedings of the 18th International Conference on
  Artificial Intelligence and Statistics}, pages 608--616, 2015.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{Liu2018}
Q.~Liu, L.~Li, Z.~Tang, and D.~Zhou.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pages
  5356--5366. 2018.

\bibitem[Mahmood et~al.(2014)Mahmood, van Hasselt, and Sutton]{Mahmood20014}
A.~R. Mahmood, H.~P. van Hasselt, and R.~S. Sutton.
\newblock Weighted importance sampling for off-policy learning with linear
  function approximation.
\newblock In \emph{Advances in Neural Information Processing Systems 27}, pages
  3014--3022. 2014.

\bibitem[Mandel et~al.(2014)Mandel, Liu, Levine, Brunskill, and
  Popovic]{Mandel2014}
T.~Mandel, Y.~Liu, S.~Levine, E.~Brunskill, and Z.~Popovic.
\newblock Off-policy evaluation across representations with applications to
  educational games.
\newblock \emph{In Proceedings of the 13th International Conference on
  Autonomous Agentsand Multi-agent Systems}, page 1077–1084, 2014.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{Munos2016}
R.~Munos, T.~Stepleton, A.~Harutyunyan, and M.~Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 29}, pages
  1054--1062. 2016.

\bibitem[Murphy(2003)]{MurphyS.A.2003Odtr}
S.~A. Murphy.
\newblock Optimal dynamic treatment regimes.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 65:\penalty0 331--355, 2003.

\bibitem[Narita et~al.(2019)Narita, Yasui, and Yata]{narita2018}
Y.~Narita, S.~Yasui, and K.~Yata.
\newblock Efficient counterfactual learning from bandit feedback.
\newblock \emph{AAAI}, 2019.

\bibitem[Newey and Mcfadden(1994)]{newey94}
W.~K. Newey and D.~L. Mcfadden.
\newblock Large sample estimation and hypothesis testing.
\newblock \emph{Handbook of Econometrics}, IV:\penalty0 2113--2245, 1994.

\bibitem[Owen(2001)]{OwenArt.ArtB.2001El}
A.~Owen.
\newblock \emph{Empirical likelihood}.
\newblock Monographs on statistics and applied probability (Series); 92.
  Chapman \& Hall/CRC, 2001.

\bibitem[Precup et~al.(2000)Precup, Sutton, and Singh]{Precup2000}
D.~Precup, R.~Sutton, and S.~Singh.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock \emph{In Proceedings of the 17th International Conference on Machine
  Learning}, pages 759--766, 2000.

\bibitem[Robins et~al.(2007)Robins, Sued, Lei-Gomez, and
  Rotnitzky]{RobinsJames2007CPoD}
J.~Robins, M.~Sued, Q.~Lei-Gomez, and A.~Rotnitzky.
\newblock Comment: Performance of double-robust estimators when "inverse
  probability" weights are highly variable.
\newblock \emph{Statistical Science}, 22:\penalty0 544--559, 2007.

\bibitem[Robins et~al.(1994)Robins, Rotnitzky, and Zhao]{RobinsJamesM.1994EoRC}
J.~M. Robins, A.~Rotnitzky, and L.~P. Zhao.
\newblock Estimation of regression coefficients when some regressors are not
  always observed.
\newblock \emph{Journal of the American Statistical Association}, 89:\penalty0
  846--866, 1994.

\bibitem[Rubin and der Laan(2008)]{rubin08}
D.~B. Rubin and M.~J.~V. der Laan.
\newblock Empirical efficiency maximization: Improved locally efficient
  covariate adjustment in randmized experiments and survival analysis.
\newblock \emph{International Journal of Biostatistics}, 4:\penalty0 Article 5,
  2008.

\bibitem[Sutton(2018)]{SuttonRichardS1998Rl:a}
R.~S. Sutton.
\newblock \emph{Reinforcement learning : an introduction}.
\newblock MIT Press, Cambridge, Mass., 2018.

\bibitem[Swaminathan and Joachims(2015)]{Swaminathan2015b}
A.~Swaminathan and T.~Joachims.
\newblock The self-normalized estimator for counterfactual learning.
\newblock In \emph{Advances in Neural Information Processing Systems 28}, pages
  3231--3239. 2015.

\bibitem[Tan(2004)]{TanZhiqiang2004OLAf}
Z.~Tan.
\newblock On likelihood approach for monte carlo integration.
\newblock \emph{Journal of the American Statistical Association}, 99:\penalty0
  1027--1036, 2004.

\bibitem[Tan(2006)]{TanZhiqiang2006ADAf}
Z.~Tan.
\newblock A distributional approach for causal inference using propensity
  scores.
\newblock \emph{Journal of the American Statistical Association}, 101:\penalty0
  1619--1637, 2006.

\bibitem[Tan(2010)]{TanZhiqiang2010Bead}
Z.~Tan.
\newblock Bounded, efficient and doubly robust estimation with inverse
  weighting.
\newblock \emph{Biometrika}, 97:\penalty0 661--682, 2010.

\bibitem[Thomas and Brunskill(2016)]{thomas2016}
P.~Thomas and E.~Brunskill.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock \emph{In Proceedings of the 33rd International Conference on Machine
  Learning}, pages 2139--2148, 2016.

\bibitem[Tsiatis(2006)]{tsi}
A.~Tsiatis.
\newblock \emph{Semiparametric Theory and Missing Data}.
\newblock Springer, New York, 2006.

\bibitem[van~der Vaart(1998)]{VaartA.W.vander1998As}
A.~W. van~der Vaart.
\newblock \emph{Asymptotic statistics}.
\newblock Cambridge University Press, Cambridge, UK, 1998.

\bibitem[Wang et~al.(2017)Wang, Agarwal, and Dudik]{wang2017optimal}
Y.-X. Wang, A.~Agarwal, and M.~Dudik.
\newblock Optimal and adaptive off-policy evaluation in contextual bandits.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pages 3589--3597, 2017.

\end{thebibliography}
