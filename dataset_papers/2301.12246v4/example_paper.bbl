\begin{thebibliography}{92}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abnar et~al.(2022)Abnar, Dehghani, Neyshabur, and
  Sedghi]{abnar2021exploring}
Abnar, S., Dehghani, M., Neyshabur, B., and Sedghi, H.
\newblock Exploring the limits of large scale pre-training.
\newblock In \emph{ICLR}, 2022.

\bibitem[Baik et~al.(2020)Baik, Choi, Choi, Kim, and Lee]{baik2020meta}
Baik, S., Choi, M., Choi, J., Kim, H., and Lee, K.~M.
\newblock Meta-learning with adaptive hyperparameters.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Bateni et~al.(2020)Bateni, Goyal, Masrani, Wood, and
  Sigal]{simpe_CNAPS}
Bateni, P., Goyal, R., Masrani, V., Wood, F., and Sigal, L.
\newblock Improved few-shot visual classification.
\newblock In \emph{CVPR}, 2020.

\bibitem[Bronskill et~al.(2020)Bronskill, Gordon, Requeima, Nowozin, and
  Turner]{metaBN}
Bronskill, J., Gordon, J., Requeima, J., Nowozin, S., and Turner, R.
\newblock Tasknorm: Rethinking batch normalization for meta-learning.
\newblock In \emph{ICML}, 2020.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{swav}
Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal,
  Bojanowski, and Joulin]{DINO}
Caron, M., Touvron, H., Misra, I., J{\'e}gou, H., Mairal, J., Bojanowski, P.,
  and Joulin, A.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{ICCV}, 2021.

\bibitem[Chen et~al.(2019)Chen, Liu, Kira, Wang, and Huang]{closerlook}
Chen, W., Liu, Y., Kira, Z., Wang, Y.~F., and Huang, J.
\newblock A closer look at few-shot classification.
\newblock In \emph{ICLR}, 2019.

\bibitem[Chen \& He(2021)Chen and He]{SimSiam}
Chen, X. and He, K.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{CVPR}, 2021.

\bibitem[Chen et~al.(2020)Chen, Fan, Girshick, and He]{moco-v2}
Chen, X., Fan, H., Girshick, R., and He, K.
\newblock Improved baselines with momentum contrastive learning.
\newblock \emph{arXiv preprint arXiv:2003.04297}, 2020.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Xie, and He]{MoCo-v3}
Chen, X., Xie, S., and He, K.
\newblock An empirical study of training self-supervised vision transformers.
\newblock In \emph{ICCV}, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Liu, Xu, Darrell, and
  Wang]{metabaseline}
Chen, Y., Liu, Z., Xu, H., Darrell, T., and Wang, X.
\newblock Meta-baseline: Exploring simple meta-learning for few-shot learning.
\newblock In \emph{ICCV}, 2021{\natexlab{b}}.

\bibitem[Das et~al.(2022)Das, Yun, and Porikli]{Confess}
Das, D., Yun, S., and Porikli, F.
\newblock Confess: {A} framework for single source cross-domain few-shot
  learning.
\newblock In \emph{ICLR}, 2022.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Dhillon et~al.(2020)Dhillon, Chaudhari, Ravichandran, and
  Soatto]{baseline}
Dhillon, G.~S., Chaudhari, P., Ravichandran, A., and Soatto, S.
\newblock A baseline for few-shot image classification.
\newblock In \emph{ICLR}, 2020.

\bibitem[Doersch et~al.(2020)Doersch, Gupta, and Zisserman]{crosstransformers}
Doersch, C., Gupta, A., and Zisserman, A.
\newblock Crosstransformers: spatially-aware few-shot transfer.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{ViT}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Dumoulin et~al.(2021)Dumoulin, Houlsby, Evci, Zhai, Goroshin, Gelly,
  and Larochelle]{md+vtab}
Dumoulin, V., Houlsby, N., Evci, U., Zhai, X., Goroshin, R., Gelly, S., and
  Larochelle, H.
\newblock A unified few-shot classification benchmark to compare transfer and
  meta learning approaches.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 1)}, 2021.

\bibitem[Entezari et~al.(2023)Entezari, Wortsman, Saukh, Shariatnia, Sedghi,
  and Schmidt]{entezari2023role}
Entezari, R., Wortsman, M., Saukh, O., Shariatnia, M.~M., Sedghi, H., and
  Schmidt, L.
\newblock The role of pre-training data in transfer learning.
\newblock \emph{arXiv preprint arXiv:2302.13602}, 2023.

\bibitem[Fei{-}Fei et~al.(2006)Fei{-}Fei, Fergus, and Perona]{fei2006one}
Fei{-}Fei, L., Fergus, R., and Perona, P.
\newblock One-shot learning of object categories.
\newblock \emph{IEEE TPAMI}, 2006.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{MAML}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Garnelo et~al.(2018)Garnelo, Rosenbaum, Maddison, Ramalho, Saxton,
  Shanahan, Teh, Rezende, and Eslami]{CNP}
Garnelo, M., Rosenbaum, D., Maddison, C., Ramalho, T., Saxton, D., Shanahan,
  M., Teh, Y.~W., Rezende, D., and Eslami, S.~A.
\newblock Conditional neural processes.
\newblock In \emph{ICML}, 2018.

\bibitem[Gidaris et~al.(2021)Gidaris, Bursuc, Puy, Komodakis, Cord, and
  Perez]{obow}
Gidaris, S., Bursuc, A., Puy, G., Komodakis, N., Cord, M., and Perez, P.
\newblock Obow: Online bag-of-visual-words generation for self-supervised
  learning.
\newblock In \emph{CVPR}, 2021.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar, et~al.]{BYOL}
Grill, J.-B., Strub, F., Altch{\'e}, F., Tallec, C., Richemond, P.,
  Buchatskaya, E., Doersch, C., Avila~Pires, B., Guo, Z., Gheshlaghi~Azar, M.,
  et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Guo et~al.(2020)Guo, Codella, Karlinsky, Codella, Smith, Saenko,
  Rosing, and Feris]{guo2020broader}
Guo, Y., Codella, N.~C., Karlinsky, L., Codella, J.~V., Smith, J.~R., Saenko,
  K., Rosing, T., and Feris, R.
\newblock A broader study of cross-domain few-shot learning.
\newblock In \emph{ECCV}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{ResNet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{MoCo}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{CVPR}, 2020.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and Girshick]{MAE}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'a}r, P., and Girshick, R.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{CVPR}, 2022.

\bibitem[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun,
  Kianinejad, Patwary, Ali, Yang, and Zhou]{scaling-law-1}
Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H.,
  Patwary, M., Ali, M., Yang, Y., and Zhou, Y.
\newblock Deep learning scaling is predictable, empirically.
\newblock \emph{arXiv preprint arXiv:1712.00409}, 2017.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{MobileNet}
Howard, A.~G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
  Andreetto, M., and Adam, H.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem[Hu et~al.(2022)Hu, Li, St{\"u}hmer, Kim, and Hospedales]{pushing}
Hu, S.~X., Li, D., St{\"u}hmer, J., Kim, M., and Hospedales, T.~M.
\newblock Pushing the limits of simple pipelines for few-shot learning:
  External data and fine-tuning make a difference.
\newblock In \emph{CVPR}, 2022.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{DenseNet}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Jamal \& Qi(2019)Jamal and Qi]{jamal2019task}
Jamal, M.~A. and Qi, G.-J.
\newblock Task agnostic meta-learning for few-shot learning.
\newblock In \emph{CVPR}, 2019.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{scaling-law-3}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kolesnikov et~al.(2020)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{bit}
Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and
  Houlsby, N.
\newblock Big transfer (bit): General visual representation learning.
\newblock In \emph{ECCV}. Springer, 2020.

\bibitem[Kornblith et~al.(2019)Kornblith, Shlens, and Le]{transfer_better}
Kornblith, S., Shlens, J., and Le, Q.~V.
\newblock Do better imagenet models transfer better?
\newblock In \emph{CVPR}, 2019.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{AlexNet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{NeurIPS}, 2012.

\bibitem[Lake et~al.(2017)Lake, Ullman, Tenenbaum, and
  Gershman]{lake2017building}
Lake, B.~M., Ullman, T.~D., Tenenbaum, J.~B., and Gershman, S.~J.
\newblock Building machines that learn and think like people.
\newblock \emph{Behavioral and brain sciences}, 2017.

\bibitem[Lee et~al.(2019)Lee, Maji, Ravichandran, and Soatto]{metaopt}
Lee, K., Maji, S., Ravichandran, A., and Soatto, S.
\newblock Meta-learning with differentiable convex optimization.
\newblock In \emph{CVPR}, 2019.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Yang, Zhang, Gao, Xiao, Dai, Yuan,
  and Gao]{EsViT}
Li, C., Yang, J., Zhang, P., Gao, M., Xiao, B., Dai, X., Yuan, L., and Gao, J.
\newblock Efficient self-supervised vision transformers for representation
  learning.
\newblock In \emph{ICLR}, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2021)Li, Liu, and Bilen]{URL}
Li, W., Liu, X., and Bilen, H.
\newblock Universal representation learning from multiple domains for few-shot
  classification.
\newblock In \emph{ICCV}, 2021.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Liu, and Bilen]{TSA}
Li, W.-H., Liu, X., and Bilen, H.
\newblock Cross-domain few-shot learning with task-specific adapters.
\newblock In \emph{CVPR}, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2017)Li, Zhou, Chen, and Li]{metaSGD}
Li, Z., Zhou, F., Chen, F., and Li, H.
\newblock Meta-sgd: Learning to learn quickly for few-shot learning.
\newblock \emph{arXiv preprint arXiv:1707.09835}, 2017.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Lee, Zhu, Chen, Shi, and
  Yang]{liu2021multi}
Liu, Y., Lee, J., Zhu, L., Chen, L., Shi, H., and Yang, Y.
\newblock A multi-mode modulator for multi-domain few-shot classification.
\newblock In \emph{ICCV}, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{ICCV}, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2022)Liu, Mao, Wu, Feichtenhofer, Darrell, and
  Xie]{ConvNext}
Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S.
\newblock A convnet for the 2020s.
\newblock In \emph{CVPR}, 2022.

\bibitem[Luo et~al.(2021)Luo, Wei, Wen, Yang, Xie, Xu, and Tian]{COSOC}
Luo, X., Wei, L., Wen, L., Yang, J., Xie, L., Xu, Z., and Tian, Q.
\newblock Rectifying the shortcut learning of background for few-shot learning.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Luo et~al.(2022)Luo, Xu, and Xu]{luo2022channel}
Luo, X., Xu, J., and Xu, Z.
\newblock Channel importance matters in few-shot image classification.
\newblock In \emph{ICML}, 2022.

\bibitem[Mangla et~al.(2020)Mangla, Kumari, Sinha, Singh, Krishnamurthy, and
  Balasubramanian]{mangla2020charting}
Mangla, P., Kumari, N., Sinha, A., Singh, M., Krishnamurthy, B., and
  Balasubramanian, V.~N.
\newblock Charting the right manifold: Manifold mixup for few-shot learning.
\newblock In \emph{WACV}, 2020.

\bibitem[Mishra et~al.(2018)Mishra, Rohaninejad, Chen, and Abbeel]{SNAIL}
Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P.
\newblock A simple neural attentive meta-learner.
\newblock In \emph{ICLR}, 2018.

\bibitem[Munkhdalai \& Yu(2017)Munkhdalai and Yu]{metanetwork}
Munkhdalai, T. and Yu, H.
\newblock Meta networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Naik \& Mammone(1992)Naik and Mammone]{naik1992meta}
Naik, D.~K. and Mammone, R.~J.
\newblock Meta-neural networks that learn by learning.
\newblock In \emph{IJCNN}, 1992.

\bibitem[Oreshkin et~al.(2018)Oreshkin, Rodr{\'\i}guez~L{\'o}pez, and
  Lacoste]{tadam}
Oreshkin, B., Rodr{\'\i}guez~L{\'o}pez, P., and Lacoste, A.
\newblock Tadam: Task dependent adaptive metric for improved few-shot learning.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Park \& Oliva(2019)Park and Oliva]{metacurvature}
Park, E. and Oliva, J.~B.
\newblock Meta-curvature.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Patacchiola et~al.(2022)Patacchiola, Bronskill, Shysheya, Hofmann,
  Nowozin, and Turner]{uppercase}
Patacchiola, M., Bronskill, J., Shysheya, A., Hofmann, K., Nowozin, S., and
  Turner, R.~E.
\newblock Contextual squeeze-and-excitation for efficient few-shot image
  classification.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{CLIP}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{ICML}, 2021.

\bibitem[Radosavovic et~al.(2020)Radosavovic, Kosaraju, Girshick, He, and
  Doll{\'a}r]{RegNet}
Radosavovic, I., Kosaraju, R.~P., Girshick, R., He, K., and Doll{\'a}r, P.
\newblock Designing network design spaces.
\newblock In \emph{CVPR}, 2020.

\bibitem[Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and
  Levine]{rajeswaran2019meta}
Rajeswaran, A., Finn, C., Kakade, S.~M., and Levine, S.
\newblock Meta-learning with implicit gradients.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Ravi \& Larochelle(2017)Ravi and Larochelle]{op-as-model}
Ravi, S. and Larochelle, H.
\newblock Optimization as a model for few-shot learning.
\newblock In \emph{ICLR}, 2017.

\bibitem[Ren et~al.(2018)Ren, Triantafillou, Ravi, Snell, Swersky, Tenenbaum,
  Larochelle, and Zemel]{tieredImageNet}
Ren, M., Triantafillou, E., Ravi, S., Snell, J., Swersky, K., Tenenbaum, J.~B.,
  Larochelle, H., and Zemel, R.~S.
\newblock Meta-learning for semi-supervised few-shot classification.
\newblock In \emph{ICLR}, 2018.

\bibitem[Requeima et~al.(2019)Requeima, Gordon, Bronskill, Nowozin, and
  Turner]{CNAPS}
Requeima, J., Gordon, J., Bronskill, J., Nowozin, S., and Turner, R.~E.
\newblock Fast and flexible multi-task classification using conditional neural
  adaptive processes.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Rizve et~al.(2021)Rizve, Khan, Khan, and Shah]{exploring}
Rizve, M.~N., Khan, S.~H., Khan, F.~S., and Shah, M.
\newblock Exploring complementary strengths of invariant and equivariant
  representations for few-shot learning.
\newblock In \emph{CVPR}, 2021.

\bibitem[Rusu et~al.(2019)Rusu, Rao, Sygnowski, Vinyals, Pascanu, Osindero, and
  Hadsell]{LEO}
Rusu, A.~A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S.,
  and Hadsell, R.
\newblock Meta-learning with latent embedding optimization.
\newblock In \emph{ICLR}, 2019.

\bibitem[Santoro et~al.(2016)Santoro, Bartunov, Botvinick, Wierstra, and
  Lillicrap]{MANN}
Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In \emph{ICML}, 2016.

\bibitem[Sbai et~al.(2020)Sbai, Couprie, and Aubry]{sbai2020impact}
Sbai, O., Couprie, C., and Aubry, M.
\newblock Impact of base dataset design on few-shot image classification.
\newblock In \emph{ECCV}, 2020.

\bibitem[Schmidhuber(1987)]{schmidhuber1987evolutionary}
Schmidhuber, J.
\newblock \emph{Evolutionary principles in self-referential learning, or on
  learning how to learn: the meta-meta-... hook}.
\newblock PhD thesis, Technische Universit{\"a}t M{\"u}nchen, 1987.

\bibitem[Shysheya et~al.(2023)Shysheya, Bronskill, Patacchiola, Nowozin, and
  Turner]{FiT}
Shysheya, A., Bronskill, J.~F., Patacchiola, M., Nowozin, S., and Turner, R.~E.
\newblock Fit: Parameter efficient few-shot transfer learning for personalized
  and federated image classification.
\newblock In \emph{ICLR}, 2023.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{VGG}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{ICLR}, 2015.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{PN}
Snell, J., Swersky, K., and Zemel, R.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Sun et~al.(2017)Sun, Shrivastava, Singh, and Gupta]{sun2017revisiting}
Sun, C., Shrivastava, A., Singh, S., and Gupta, A.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In \emph{ICCV}, 2017.

\bibitem[Sung et~al.(2018)Sung, Yang, Zhang, Xiang, Torr, and
  Hospedales]{relationnet}
Sung, F., Yang, Y., Zhang, L., Xiang, T., Torr, P.~H., and Hospedales, T.~M.
\newblock Learning to compare: Relation network for few-shot learning.
\newblock In \emph{CVPR}, 2018.

\bibitem[Taori et~al.(2020)Taori, Dave, Shankar, Carlini, Recht, and
  Schmidt]{natural-distribution-shifts}
Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L.
\newblock Measuring robustness to natural distribution shifts in image
  classification.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Thrun \& Pratt(1998)Thrun and Pratt]{thrun1998learning}
Thrun, S. and Pratt, L.
\newblock Learning to learn: Introduction and overview.
\newblock In \emph{Learning to learn}. Springer, 1998.

\bibitem[Tian et~al.(2020)Tian, Wang, Krishnan, Tenenbaum, and
  Isola]{rethinking}
Tian, Y., Wang, Y., Krishnan, D., Tenenbaum, J.~B., and Isola, P.
\newblock Rethinking few-shot image classification: {A} good embedding is all
  you need?
\newblock In \emph{ECCV}, 2020.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{DeiT}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou,
  H.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{ICML}, 2021.

\bibitem[Triantafillou et~al.(2020)Triantafillou, Zhu, Dumoulin, Lamblin, Evci,
  Xu, Goroshin, Gelada, Swersky, Manzagol, and Larochelle]{metadataset}
Triantafillou, E., Zhu, T., Dumoulin, V., Lamblin, P., Evci, U., Xu, K.,
  Goroshin, R., Gelada, C., Swersky, K., Manzagol, P., and Larochelle, H.
\newblock Meta-dataset: {A} dataset of datasets for learning to learn from few
  examples.
\newblock In \emph{ICLR}, 2020.

\bibitem[Triantafillou et~al.(2021)Triantafillou, Larochelle, Zemel, and
  Dumoulin]{FLUTE}
Triantafillou, E., Larochelle, H., Zemel, R.~S., and Dumoulin, V.
\newblock Learning a universal template for few-shot dataset generalization.
\newblock In \emph{ICML}, 2021.

\bibitem[Vaze et~al.(2022)Vaze, Han, Vedaldi, and Zisserman]{closed-set}
Vaze, S., Han, K., Vedaldi, A., and Zisserman, A.
\newblock Open-set recognition: {A} good closed-set classifier is all you need.
\newblock In \emph{ICLR}, 2022.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra,
  et~al.]{MatchingNet}
Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et~al.
\newblock Matching networks for one shot learning.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Wu et~al.(2018)Wu, Xiong, Yu, and Lin]{InscDisc}
Wu, Z., Xiong, Y., Yu, S.~X., and Lin, D.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In \emph{CVPR}, 2018.

\bibitem[Xu et~al.(2022{\natexlab{a}})Xu, Yang, Wang, Wang, Fu, and Xue]{eTT}
Xu, C., Yang, S., Wang, Y., Wang, Z., Fu, Y., and Xue, X.
\newblock Exploring efficient few-shot adaptation for vision transformers.
\newblock \emph{Transactions on Machine Learning Research}, 2022{\natexlab{a}}.

\bibitem[Xu et~al.(2020)Xu, Ton, Kim, Kosiorek, and Teh]{metafun}
Xu, J., Ton, J.-F., Kim, H., Kosiorek, A., and Teh, Y.~W.
\newblock Metafun: Meta-learning with iterative functional updates.
\newblock In \emph{ICML}, 2020.

\bibitem[Xu et~al.(2022{\natexlab{b}})Xu, Luo, Pan, Pei, Li, and
  Xu]{xu2022alleviating}
Xu, J., Luo, X., Pan, X., Pei, W., Li, Y., and Xu, Z.
\newblock Alleviating the sample selection bias in few-shot learning by
  removing projection to the centroid.
\newblock In \emph{NeurIPS}, 2022{\natexlab{b}}.

\bibitem[Ye et~al.(2020)Ye, Hu, Zhan, and Sha]{ye2020few}
Ye, H.-J., Hu, H., Zhan, D.-C., and Sha, F.
\newblock Few-shot learning via embedding adaptation with set-to-set functions.
\newblock In \emph{CVPR}, 2020.

\bibitem[Yoon et~al.(2019)Yoon, Seo, and Moon]{tapnet}
Yoon, S.~W., Seo, J., and Moon, J.
\newblock Tapnet: Neural network augmented with task-adaptive projection for
  few-shot learning.
\newblock In \emph{ICML}, 2019.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and Deny]{Barlow}
Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In \emph{ICML}, 2021.

\bibitem[Zhai et~al.(2019)Zhai, Puigcerver, Kolesnikov, Ruyssen, Riquelme,
  Lucic, Djolonga, Pinto, Neumann, Dosovitskiy, et~al.]{VTAB}
Zhai, X., Puigcerver, J., Kolesnikov, A., Ruyssen, P., Riquelme, C., Lucic, M.,
  Djolonga, J., Pinto, A.~S., Neumann, M., Dosovitskiy, A., et~al.
\newblock A large-scale study of representation learning with the visual task
  adaptation benchmark.
\newblock \emph{arXiv preprint arXiv:1910.04867}, 2019.

\bibitem[Zhai et~al.(2022)Zhai, Kolesnikov, Houlsby, and Beyer]{scaling-law-2}
Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L.
\newblock Scaling vision transformers.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zhang et~al.(2020)Zhang, Cai, Lin, and Shen]{deepemd}
Zhang, C., Cai, Y., Lin, G., and Shen, C.
\newblock Deepemd: Few-shot image classification with differentiable earth
  mover's distance and structured classifiers.
\newblock In \emph{CVPR}, 2020.

\bibitem[Zhao et~al.(2021)Zhao, Wu, Lau, and Lin]{Exemplar}
Zhao, N., Wu, Z., Lau, R. W.~H., and Lin, S.
\newblock What makes instance discrimination good for transfer learning?
\newblock In \emph{ICLR}, 2021.

\bibitem[Zhou et~al.(2022)Zhou, Wei, Wang, Shen, Xie, Yuille, and Kong]{ibot}
Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., and Kong, T.
\newblock ibot: Image bert pre-training with online tokenizer.
\newblock In \emph{ICLR}, 2022.

\bibitem[Zintgraf et~al.(2019)Zintgraf, Shiarli, Kurin, Hofmann, and
  Whiteson]{zintgraf2019fast}
Zintgraf, L., Shiarli, K., Kurin, V., Hofmann, K., and Whiteson, S.
\newblock Fast context adaptation via meta-learning.
\newblock In \emph{ICML}, 2019.

\end{thebibliography}
