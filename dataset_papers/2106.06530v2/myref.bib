@article{guo2018numerical,
  title     = {Numerical analysis near singularities in RBF networks},
  author    = {Guo, Weili and Wei, Haikun and Onflg, Yew-Soon and Hervas, Jaime Rubio and Zhao, Junsheng and Wang, Hai and Zhang, Kanjian},
  journal   = {The Journal of Machine Learning Research},
  volume    = {19},
  number    = {1},
  pages     = {1--39},
  year      = {2018},
  publisher = {JMLR. org}
}

@misc{hendrycks2020gaussian,
  title         = {Gaussian Error Linear Units (GELUs)},
  author        = {Dan Hendrycks and Kevin Gimpel},
  year          = {2020},
  eprint        = {1606.08415},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@misc{du2019gradient,
      title={Gradient Descent Finds Global Minima of Deep Neural Networks}, 
      author={Simon S. Du and Jason D. Lee and Haochuan Li and Liwei Wang and Xiyu Zhai},
      year={2019},
      eprint={1811.03804},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gao2019convergence,
      title={Convergence of Adversarial Training in Overparametrized Neural Networks}, 
      author={Ruiqi Gao and Tianle Cai and Haochuan Li and Liwei Wang and Cho-Jui Hsieh and Jason D. Lee},
      year={2019},
      eprint={1906.07916},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{meng2020dynamic,
  title   = {Dynamic of Stochastic Gradient Descent with State-Dependent Noise},
  author  = {Meng, Qi and Gong, Shiqi and Chen, Wei and Ma, Zhi-Ming and Liu, Tie-Yan},
  journal = {arXiv preprint arXiv:2006.13719},
  year    = {2020}
}
@article{arora2019exact,
  title   = {On exact computation with an infinitely wide neural net},
  author  = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
  journal = {arXiv preprint arXiv:1904.11955},
  year    = {2019}
}
@article{xie2020diffusion,
  title   = {A Diffusion Theory for Deep Learning Dynamics: Stochastic Gradient Descent Escapes From Sharp Minima Exponentially Fast},
  author  = {Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  journal = {arXiv preprint arXiv:2002.03495},
  year    = {2020}
}

@article{lee2016gradient,
  title   = {Gradient descent converges to minimizers},
  author  = {Lee, Jason D and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  journal = {arXiv preprint arXiv:1602.04915},
  year    = {2016}
}

@article{cai2020neural,
  title         = {Neural Temporal-Difference and Q-Learning Provably Converge to Global Optima},
  author        = {Qi Cai and Zhuoran Yang and Jason D. Lee and Zhaoran Wang},
  year          = {2020},
  eprint        = {1905.10027},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{cheng2019stochastic,
  title   = {Stochastic Gradient and Langevin Processes},
  author  = {Cheng, Xiang and Yin, Dong and Bartlett, Peter L and Jordan, Michael I},
  journal = {arXiv preprint arXiv:1907.03215},
  year    = {2019}
}
@article{wu2019implicit,
  title   = {Implicit Regularization of Normalization Methods},
  author  = {Wu, Xiaoxia and Dobriban, Edgar and Ren, Tongzheng and Wu, Shanshan and Li, Zhiyuan and Gunasekar, Suriya and Ward, Rachel and Liu, Qiang},
  journal = {arXiv preprint arXiv:1911.07956},
  year    = {2019}
}
@article{attouch2010proximal,
  title     = {Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the Kurdyka-{\L}ojasiewicz inequality},
  author    = {Attouch, H{\'e}dy and Bolte, J{\'e}r{\^o}me and Redont, Patrick and Soubeyran, Antoine},
  journal   = {Mathematics of operations research},
  volume    = {35},
  number    = {2},
  pages     = {438--457},
  year      = {2010},
  publisher = {INFORMS}
}
@inproceedings{mou2018generalization,
  title     = {Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints},
  author    = {Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  booktitle = {Conference on Learning Theory},
  pages     = {605--638},
  year      = {2018}
}
@article{arora2018theoretical,
  title   = {Theoretical analysis of auto rate-tuning by batch normalization},
  author  = {Arora, Sanjeev and Li, Zhiyuan and Lyu, Kaifeng},
  journal = {arXiv preprint arXiv:1812.03981},
  year    = {2018}
}
@article{arora2020dropout,
  title   = {Dropout: Explicit Forms and Capacity Control},
  author  = {Arora, Raman and Bartlett, Peter and Mianjy, Poorya and Srebro, Nathan},
  journal = {arXiv preprint arXiv:2003.03397},
  year    = {2020}
}

@article{mianjy2019dropout,
  title   = {On dropout and nuclear norm regularization},
  author  = {Mianjy, Poorya and Arora, Raman},
  journal = {arXiv preprint arXiv:1905.11887},
  year    = {2019}
}

@article{wei2020implicit,
  title   = {The Implicit and Explicit Regularization Effects of Dropout},
  author  = {Wei, Colin and Kakade, Sham and Ma, Tengyu},
  journal = {arXiv preprint arXiv:2002.12915},
  year    = {2020}
}
@article{martin2018implicit,
  title   = {Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning},
  author  = {Martin, Charles H and Mahoney, Michael W},
  journal = {arXiv preprint arXiv:1810.01075},
  year    = {2018}
}
@article{chung2006concentration,
  title     = {Concentration inequalities and martingale inequalities: a survey},
  author    = {Chung, Fan and Lu, Linyuan},
  journal   = {Internet Mathematics},
  volume    = {3},
  number    = {1},
  pages     = {79--127},
  year      = {2006},
  publisher = {Taylor \& Francis}
}

@article{oymak2020towards,
  title     = {Towards moderate overparameterization: global convergence guarantees for training shallow neural networks},
  author    = {Oymak, Samet and Soltanolkotabi, Mahdi},
  journal   = {IEEE Journal on Selected Areas in Information Theory},
  year      = {2020},
  publisher = {IEEE}
}

@article{soltanolkotabi2018theoretical,
  title     = {Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author    = {Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D},
  journal   = {IEEE Transactions on Information Theory},
  volume    = {65},
  number    = {2},
  pages     = {742--769},
  year      = {2018},
  publisher = {IEEE}
}

@article{du2018gradient,
  title   = {Gradient descent finds global minima of deep neural networks},
  author  = {Du, Simon S and Lee, Jason D and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  journal = {arXiv preprint arXiv:1811.03804},
  year    = {2018}
}

@article{arora2019fine,
  title   = {Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author  = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  journal = {arXiv preprint arXiv:1901.08584},
  year    = {2019}
}

@inproceedings{hoffer2018norm,
  title     = {Norm matters: efficient and accurate normalization schemes in deep networks},
  author    = {Hoffer, Elad and Banner, Ron and Golan, Itay and Soudry, Daniel},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {2160--2170},
  year      = {2018}
}

@article{poggio2017theory,
  title   = {Theory of deep learning III: explaining the non-overfitting puzzle},
  author  = {Poggio, Tomaso and Kawaguchi, Kenji and Liao, Qianli and Miranda, Brando and Rosasco, Lorenzo and Boix, Xavier and Hidary, Jack and Mhaskar, Hrushikesh},
  journal = {arXiv preprint arXiv:1801.00173},
  year    = {2017}
}

@article{ma2019implicit,
  title     = {Implicit regularization in nonconvex statistical estimation: Gradient descent converges linearly for phase retrieval, matrix completion, and blind deconvolution},
  author    = {Ma, Cong and Wang, Kaizheng and Chi, Yuejie and Chen, Yuxin},
  journal   = {Foundations of Computational Mathematics},
  pages     = {1--182},
  year      = {2019},
  publisher = {Springer}
}

@article{nacson2019lexicographic,
  title   = {Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models},
  author  = {Nacson, Mor Shpigel and Gunasekar, Suriya and Lee, Jason D and Srebro, Nathan and Soudry, Daniel},
  journal = {arXiv preprint arXiv:1905.07325},
  year    = {2019}
}

@inproceedings{chaudhari2018stochastic,
  title        = {Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks},
  author       = {Chaudhari, Pratik and Soatto, Stefano},
  booktitle    = {2018 Information Theory and Applications Workshop (ITA)},
  pages        = {1--10},
  year         = {2018},
  organization = {IEEE}
}


@article{mianjy2018implicit,
  title   = {On the implicit bias of dropout},
  author  = {Mianjy, Poorya and Arora, Raman and Vidal, Rene},
  journal = {arXiv preprint arXiv:1806.09777},
  year    = {2018}
}

@article{chi2019nonconvex,
  title     = {Nonconvex optimization meets low-rank matrix factorization: An overview},
  author    = {Chi, Yuejie and Lu, Yue M and Chen, Yuxin},
  journal   = {IEEE Transactions on Signal Processing},
  volume    = {67},
  number    = {20},
  pages     = {5239--5269},
  year      = {2019},
  publisher = {IEEE}
}

@inproceedings{wei2019data,
  title     = {Data-dependent sample complexity of deep neural networks via lipschitz augmentation},
  author    = {Wei, Colin and Ma, Tengyu},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {9722--9733},
  year      = {2019}
}


@article{nakkiran2020optimal,
  title   = {Optimal regularization can mitigate double descent},
  author  = {Nakkiran, Preetum and Venkat, Prayaag and Kakade, Sham and Ma, Tengyu},
  journal = {arXiv preprint arXiv:2003.01897},
  year    = {2020}
}

@inproceedings{cao2019learning,
  title     = {Learning imbalanced datasets with label-distribution-aware margin loss},
  author    = {Cao, Kaidi and Wei, Colin and Gaidon, Adrien and Arechiga, Nikos and Ma, Tengyu},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {1565--1576},
  year      = {2019}
}

@article{raskutti2012minimax,
  title   = {Minimax-optimal rates for sparse additive models over kernel classes via convex programming},
  author  = {Raskutti, Garvesh and Wainwright, Martin J and Yu, Bin},
  journal = {Journal of Machine Learning Research},
  volume  = {13},
  number  = {Feb},
  pages   = {389--427},
  year    = {2012}
}
@inproceedings{vaswani2019fast,
  title     = {Fast and Faster Convergence of SGD for Over-Parameterized Models and an Accelerated Perceptron},
  author    = {Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics},
  pages     = {1195--1204},
  year      = {2019}
}
@article{chung2006concentration,
  title     = {Concentration inequalities and martingale inequalities: a survey},
  author    = {Chung, Fan and Lu, Linyuan},
  journal   = {Internet Mathematics},
  volume    = {3},
  number    = {1},
  pages     = {79--127},
  year      = {2006},
  publisher = {Taylor \& Francis}
}
@article{hardt2015train,
  title   = {Train faster, generalize better: Stability of stochastic gradient descent},
  author  = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  journal = {arXiv preprint arXiv:1509.01240},
  year    = {2015}
}

@inproceedings{negrea2019information,
  title     = {Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates},
  author    = {Negrea, Jeffrey and Haghifam, Mahdi and Dziugaite, Gintare Karolina and Khisti, Ashish and Roy, Daniel M},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {11013--11023},
  year      = {2019}
}
@inproceedings{ge2015escaping,
  title     = {Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author    = {Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle = {Conference on Learning Theory},
  pages     = {797--842},
  year      = {2015}
}
@article{jin2019stochastic,
  title   = {Stochastic gradient descent escapes saddle points efficiently},
  author  = {Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M and Jordan, Michael I},
  journal = {arXiv preprint arXiv:1902.04811},
  year    = {2019}
}
@article{roberts1996exponential,
  title     = {Exponential convergence of Langevin distributions and their discrete approximations},
  author    = {Roberts, Gareth O and Tweedie, Richard L and others},
  journal   = {Bernoulli},
  volume    = {2},
  number    = {4},
  pages     = {341--363},
  year      = {1996},
  publisher = {Bernoulli Society for Mathematical Statistics and Probability}
}
@article{neelakantan2015adding,
  title   = {Adding gradient noise improves learning for very deep networks},
  author  = {Neelakantan, Arvind and Vilnis, Luke and Le, Quoc V and Sutskever, Ilya and Kaiser, Lukasz and Kurach, Karol and Martens, James},
  journal = {arXiv preprint arXiv:1511.06807},
  year    = {2015}
}
@article{mou2017generalization,
  title   = {Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints},
  author  = {Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  journal = {arXiv preprint arXiv:1707.05947},
  year    = {2017}
}
@article{zhang2017hitting,
  title   = {A hitting time analysis of stochastic gradient langevin dynamics},
  author  = {Zhang, Yuchen and Liang, Percy and Charikar, Moses},
  journal = {arXiv preprint arXiv:1702.05575},
  year    = {2017}
}
@article{raginsky2017non,
  title   = {Non-convex learning via stochastic gradient Langevin dynamics: a nonasymptotic analysis},
  author  = {Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
  journal = {arXiv preprint arXiv:1702.03849},
  year    = {2017}
}

@article{bubeck2018sampling,
  title     = {Sampling from a log-concave distribution with projected langevin monte carlo},
  author    = {Bubeck, S{\'e}bastien and Eldan, Ronen and Lehec, Joseph},
  journal   = {Discrete \& Computational Geometry},
  volume    = {59},
  number    = {4},
  pages     = {757--783},
  year      = {2018},
  publisher = {Springer}
}

@inproceedings{raginsky2017non,
  title     = {Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis},
  author    = {Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
  booktitle = {Conference on Learning Theory},
  pages     = {1674--1703},
  year      = {2017}
}

@article{dalalyan2017theoretical,
  title     = {Theoretical guarantees for approximate sampling from smooth and log-concave densities},
  author    = {Dalalyan, Arnak S},
  journal   = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume    = {79},
  number    = {3},
  pages     = {651--676},
  year      = {2017},
  publisher = {Wiley Online Library}
}
@inproceedings{welling2011bayesian,
  title     = {Bayesian learning via stochastic gradient Langevin dynamics},
  author    = {Welling, Max and Teh, Yee W},
  booktitle = {Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages     = {681--688},
  year      = {2011}
}
@article{teh2016consistency,
  title     = {Consistency and fluctuations for stochastic gradient Langevin dynamics},
  author    = {Teh, Yee Whye and Thiery, Alexandre H and Vollmer, Sebastian J},
  journal   = {The Journal of Machine Learning Research},
  volume    = {17},
  number    = {1},
  pages     = {193--225},
  year      = {2016},
  publisher = {JMLR. org}
}
@inproceedings{ge2016matrix,
  title     = {Matrix completion has no spurious local minimum},
  author    = {Ge, Rong and Lee, Jason D and Ma, Tengyu},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {2973--2981},
  year      = {2016}
}
@article{hochreiter1997flat,
  title     = {Flat minima},
  author    = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal   = {Neural Computation},
  volume    = {9},
  number    = {1},
  pages     = {1--42},
  year      = {1997},
  publisher = {MIT Press}
}

@article{chaudhari2019entropy,
  title     = {Entropy-sgd: Biasing gradient descent into wide valleys},
  author    = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal   = {Journal of Statistical Mechanics: Theory and Experiment},
  volume    = {2019},
  number    = {12},
  pages     = {124018},
  year      = {2019},
  publisher = {IOP Publishing}
}

@book{goodfellow2016deep,
  title     = {Deep learning},
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year      = {2016},
  publisher = {MIT press}
}
@inproceedings{dinh2017sharp,
  title        = {Sharp minima can generalize for deep nets},
  author       = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle    = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages        = {1019--1028},
  year         = {2017},
  organization = {JMLR. org}
}
@incollection{lecun2012efficient,
  title     = {Efficient backprop},
  author    = {LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle = {Neural networks: Tricks of the trade},
  pages     = {9--48},
  year      = {2012},
  publisher = {Springer}
}
@article{tibshirani1996regression,
  title     = {Regression shrinkage and selection via the lasso},
  author    = {Tibshirani, Robert},
  journal   = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume    = {58},
  number    = {1},
  pages     = {267--288},
  year      = {1996},
  publisher = {Wiley Online Library}
}

@article{blanc2019implicit,
  title   = {Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck like process},
  author  = {Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  journal = {arXiv preprint arXiv:1904.09080},
  year    = {2019}
}
@article{li2017algorithmic,
  title   = {Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
  author  = {Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  journal = {arXiv preprint arXiv:1712.09203},
  year    = {2017}
}

@article{agarwal2020disentangling,
  title   = {Disentangling Adaptive Gradient Methods from Learning Rates},
  author  = {Agarwal, Naman and Anil, Rohan and Hazan, Elad and Koren, Tomer and Zhang, Cyril},
  journal = {arXiv preprint arXiv:2002.11803},
  year    = {2020}
}
@article{woodworth2020kernel,
  title   = {Kernel and rich regimes in overparametrized models},
  author  = {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  journal = {arXiv preprint arXiv:2002.09277},
  year    = {2020}
}
@inproceedings{vaskevicius2019implicit,
  title     = {Implicit Regularization for Optimal Sparse Recovery},
  author    = {Vaskevicius, Tomas and Kanade, Varun and Rebeschini, Patrick},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {2968--2979},
  year      = {2019}
}
@inproceedings{szegedy2016rethinking,
  title     = {Rethinking the inception architecture for computer vision},
  author    = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {2818--2826},
  year      = {2016}
}
@article{wen2019interplay,
  title   = {Interplay between optimization and generalization of stochastic gradient descent with covariance noise},
  author  = {Wen, Yeming and Luk, Kevin and Gazeau, Maxime and Zhang, Guodong and Chan, Harris and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1902.08234},
  year    = {2019}
}
@inproceedings{hoffer2017train,
  title     = {Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author    = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {1731--1741},
  year      = {2017}
}
@article{shallue2018measuring,
  title   = {Measuring the effects of data parallelism on neural network training},
  author  = {Shallue, Christopher J and Lee, Jaehoon and Antognini, Joseph and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E},
  journal = {arXiv preprint arXiv:1811.03600},
  year    = {2018}
}

@article{srivastava2014dropout,
  title     = {Dropout: a simple way to prevent neural networks from overfitting},
  author    = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal   = {The journal of machine learning research},
  volume    = {15},
  number    = {1},
  pages     = {1929--1958},
  year      = {2014},
  publisher = {JMLR. org}
}
@article{ioffe2015batch,
  title   = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author  = {Ioffe, Sergey and Szegedy, Christian},
  journal = {arXiv preprint arXiv:1502.03167},
  year    = {2015}
}
@article{chaudhari2019entropy,
  title     = {Entropy-sgd: Biasing gradient descent into wide valleys},
  author    = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal   = {Journal of Statistical Mechanics: Theory and Experiment},
  volume    = {2019},
  number    = {12},
  pages     = {124018},
  year      = {2019},
  publisher = {IOP Publishing}
}

@article{goyal2017accurate,
  title   = {Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author  = {Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal = {arXiv preprint arXiv:1706.02677},
  year    = {2017}
}
@article{smith2017don,
  title   = {Don't decay the learning rate, increase the batch size},
  author  = {Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  journal = {arXiv preprint arXiv:1711.00489},
  year    = {2017}
}
@inproceedings{wilson2017marginal,
  title     = {The marginal value of adaptive gradient methods in machine learning},
  author    = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {4148--4158},
  year      = {2017}
}

@inproceedings{li2019towards,
  title     = {Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author    = {Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {11669--11680},
  year      = {2019}
}
@inproceedings{neyshabur2015path,
  title     = {Path-sgd: Path-normalized optimization in deep neural networks},
  author    = {Neyshabur, Behnam and Salakhutdinov, Russ R and Srebro, Nati},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {2422--2430},
  year      = {2015}
}
@article{kingma2014adam,
  title   = {Adam: A method for stochastic optimization},
  author  = {Kingma, Diederik P and Ba, Jimmy},
  journal = {arXiv preprint arXiv:1412.6980},
  year    = {2014}
}
@inproceedings{sutskever2013importance,
  title     = {On the importance of initialization and momentum in deep learning},
  author    = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle = {International conference on machine learning},
  pages     = {1139--1147},
  year      = {2013}
}
@article{keskar2016large,
  title   = {On large-batch training for deep learning: Generalization gap and sharp minima},
  author  = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal = {arXiv preprint arXiv:1609.04836},
  year    = {2016}
}
@article{zhang2016understanding,
  title   = {Understanding deep learning requires rethinking generalization},
  author  = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal = {arXiv preprint arXiv:1611.03530},
  year    = {2016}
}

@inproceedings{gunasekar2017implicit,
  title     = {Implicit regularization in matrix factorization},
  author    = {Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {6151--6159},
  year      = {2017}
}
@inproceedings{gunasekar2018implicit,
  title     = {Implicit bias of gradient descent on linear convolutional networks},
  author    = {Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {9461--9471},
  year      = {2018}
}

@book{nocedal2006numerical,
  title     = {Numerical optimization},
  author    = {Nocedal, Jorge and Wright, Stephen},
  year      = {2006},
  publisher = {Springer Science \& Business Media}
}


@article{gunasekar2018characterizing,
  title   = {Characterizing implicit bias in terms of optimization geometry},
  author  = {Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  journal = {arXiv preprint arXiv:1802.08246},
  year    = {2018}
}

@article{soudry2018implicit,
  title     = {The implicit bias of gradient descent on separable data},
  author    = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal   = {The Journal of Machine Learning Research},
  volume    = {19},
  number    = {1},
  pages     = {2822--2878},
  year      = {2018},
  publisher = {JMLR. org}
}


@inproceedings{arora2019implicit,
  title     = {Implicit regularization in deep matrix factorization},
  author    = {Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {7411--7422},
  year      = {2019}
}


@article{roberts1996exponential,
  title     = {Exponential convergence of Langevin distributions and their discrete approximations},
  author    = {Roberts, Gareth O and Tweedie, Richard L and others},
  journal   = {Bernoulli},
  volume    = {2},
  number    = {4},
  pages     = {341--363},
  year      = {1996},
  publisher = {Bernoulli Society for Mathematical Statistics and Probability}
}
@article{neal2011mcmc,
  title   = {MCMC using Hamiltonian dynamics},
  author  = {Neal, Radford M and others},
  journal = {Handbook of markov chain monte carlo},
  volume  = {2},
  number  = {11},
  pages   = {2},
  year    = {2011}
}

@inproceedings{welling2011bayesian,
  title     = {Bayesian learning via stochastic gradient Langevin dynamics},
  author    = {Welling, Max and Teh, Yee W},
  booktitle = {Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages     = {681--688},
  year      = {2011}
}
@article{amelunxen2014living,
  title     = {Living on the edge: Phase transitions in convex programs with random data},
  author    = {Amelunxen, Dennis and Lotz, Martin and McCoy, Michael B and Tropp, Joel A},
  journal   = {Information and Inference: A Journal of the IMA},
  volume    = {3},
  number    = {3},
  pages     = {224--294},
  year      = {2014},
  publisher = {OUP}
}

@article{chizat2018note,
  title   = {A note on lazy training in supervised differentiable programming},
  author  = {Chizat, Lenaic and Bach, Francis},
  journal = {arXiv preprint arXiv:1812.07956},
  volume  = {8},
  year    = {2018}
}

@inproceedings{du2018algorithmic,
  title     = {Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},
  author    = {Du, Simon S and Hu, Wei and Lee, Jason D},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {384--395},
  year      = {2018}
}

@article{ji2018risk,
  title   = {Risk and parameter convergence of logistic regression},
  author  = {Ji, Ziwei and Telgarsky, Matus},
  journal = {arXiv preprint arXiv:1803.07300},
  year    = {2018}
}

@article{lyu2019gradient,
  title   = {Gradient descent maximizes the margin of homogeneous neural networks},
  author  = {Lyu, Kaifeng and Li, Jian},
  journal = {arXiv preprint arXiv:1906.05890},
  year    = {2019}
}

@article{nacson2018convergence,
  title   = {Convergence of gradient descent on separable data},
  author  = {Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Savarese, Pedro HP and Srebro, Nathan and Soudry, Daniel},
  journal = {arXiv preprint arXiv:1803.01905},
  year    = {2018}
}

@article{ji2018gradient,
  title   = {Gradient descent aligns the layers of deep linear networks},
  author  = {Ji, Ziwei and Telgarsky, Matus},
  journal = {arXiv preprint arXiv:1810.02032},
  year    = {2018}
}

@inproceedings{zhu2019anisotropic,
  title  = {The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author = {Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  year   = {2019}
}

@inproceedings{jacot2018neural,
  title     = {Neural tangent kernel: Convergence and generalization in neural networks},
  author    = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle = {Advances in neural information processing systems},
  pages     = {8571--8580},
  year      = {2018}
}

@article{du2018gradient,
  title   = {Gradient descent provably optimizes over-parameterized neural networks},
  author  = {Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal = {arXiv preprint arXiv:1810.02054},
  year    = {2018}
}

@inproceedings{wei2019regularization,
  title     = {Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author    = {Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {9709--9721},
  year      = {2019}
}

@inproceedings{ghorbani2019limitations,
  title     = {Limitations of Lazy Training of Two-layers Neural Network},
  author    = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {9108--9118},
  year      = {2019}
}

@article{jastrzkebski2017three,
  title   = {Three factors influencing minima in sgd},
  author  = {Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal = {arXiv preprint arXiv:1711.04623},
  year    = {2017}
}

@article{yaida2018fluctuation,
  title   = {Fluctuation-dissipation relations for stochastic gradient descent},
  author  = {Yaida, Sho},
  journal = {arXiv preprint arXiv:1810.00004},
  year    = {2018}
}

@article{wei2019noise,
  title   = {How noise affects the Hessian spectrum in overparameterized neural networks},
  author  = {Wei, Mingwei and Schwab, David J},
  journal = {arXiv preprint arXiv:1910.00195},
  year    = {2019}
}

@article{agarwal2020disentangling,
  title   = {Disentangling Adaptive Gradient Methods from Learning Rates},
  author  = {Agarwal, Naman and Anil, Rohan and Hazan, Elad and Koren, Tomer and Zhang, Cyril},
  journal = {arXiv preprint arXiv:2002.11803},
  year    = {2020}
}

@article{li2019generalization,
  title   = {On generalization error bounds of noisy gradient methods for non-convex learning},
  author  = {Li, Jian and Luo, Xuanyuan and Qiao, Mingda},
  journal = {arXiv preprint arXiv:1902.00621},
  year    = {2019}
}

@article{gissin2019implicit,
  title   = {The Implicit Bias of Depth: How Incremental Learning Drives Generalization},
  author  = {Gissin, Daniel and Shalev-Shwartz, Shai and Daniely, Amit},
  journal = {arXiv preprint arXiv:1909.12051},
  year    = {2019}
}	

@article{razin2020implicit,
  title   = {Implicit Regularization in Deep Learning May Not Be Explainable by Norms},
  author  = {Razin, Noam and Cohen, Nadav},
  journal = {arXiv preprint arXiv:2005.06398},
  year    = {2020}
}

@article{hardt2015train,
  title   = {Train faster, generalize better: Stability of stochastic gradient descent},
  author  = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  journal = {arXiv preprint arXiv:1509.01240},
  year    = {2015}
}

@inproceedings{li2018learning,
  title     = {Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author    = {Li, Yuanzhi and Liang, Yingyu},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {8157--8166},
  year      = {2018}
}

@article{simonyan2014very,
  title   = {Very deep convolutional networks for large-scale image recognition},
  author  = {Simonyan, Karen and Zisserman, Andrew},
  journal = {arXiv preprint arXiv:1409.1556},
  year    = {2014}
}

@article{haochen2020shape,
  title   = {Shape matters: Understanding the implicit bias of the noise covariance},
  author  = {HaoChen, Jeff Z and Wei, Colin and Lee, Jason D and Ma, Tengyu},
  journal = {arXiv preprint arXiv:2006.08680},
  year    = {2020}
}

@article{liu2019bad,
  title   = {Bad global minima exist and sgd can reach them},
  author  = {Liu, Shengchao and Papailiopoulos, Dimitris and Achlioptas, Dimitris},
  journal = {arXiv preprint arXiv:1906.02613},
  year    = {2019}
}

@article{wei2019improved,
  title   = {Improved sample complexities for deep networks and robust classification via an all-layer margin},
  author  = {Wei, Colin and Ma, Tengyu},
  journal = {arXiv preprint arXiv:1910.04284},
  year    = {2019}
}
@article{pinelis1994optimum,
  title     = {Optimum bounds for the distributions of martingales in Banach spaces},
  author    = {Pinelis, Iosif},
  journal   = {The Annals of Probability},
  pages     = {1679--1706},
  year      = {1994},
  publisher = {JSTOR}
}

@article{li2020towards,
  title   = {Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning},
  author  = {Li, Zhiyuan and Luo, Yuping and Lyu, Kaifeng},
  journal = {arXiv preprint arXiv:2012.09839},
  year    = {2020}
}


@article{razin2020implicit,
  title   = {Implicit regularization in deep learning may not be explainable by norms},
  author  = {Razin, Noam and Cohen, Nadav},
  journal = {arXiv preprint arXiv:2005.06398},
  year    = {2020}
}

@article{nacson2018convergence,
  title   = {Convergence of Gradient Descent on Separable Data},
  author  = {Nacson, Mor Shpigel and Lee, Jason D. and Gunasekar, Suriya and Srebro, Nathan and Soudry, Daniel},
  journal = {Artificial Intelligence and Statistics (AISTATS) },
  year    = {2019}
}


@article{gunasekar2018implicit,
  title   = {Implicit Bias of Gradient Descent on Linear Convolutional Networks},
  author  = {Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  journal = {Neural Information Processing Systems (NIPS)},
  year    = {2018}
}


@article{du2018algorithmic,
  title   = {Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced},
  author  = {Du, Simon S and Hu, Wei and Lee, Jason D},
  journal = {Neural Information Processing Systems (NIPS)},
  year    = {2018}
}


@article{du2018gradient,
  title   = {Gradient Descent Finds Global Minima of Deep Neural Networks},
  author  = {Du, Simon S and Lee, Jason D and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  journal = {International Conference on Machine Learning (ICML)},
  year    = {2019}
}

@article{nacson2019lexicographic,
  title   = {
             Lexicographic and Depth-Sensitive Margins in Homogeneous and Non-Homogeneous Training},
  author  = {Nacson, Mor Sphigel and Gunasekar, Suriya and Lee, Jason D and Srebro, Nathan and Soudry Daniel},
  journal = {International Conference on Machine Learning (ICML)},
  year    = {2019}
}

@article{wei2018margin,
  title   = {Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel.},
  author  = {Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  journal = {Neural Information Processing Systems (NeurIPS)},
  year    = {2019}
}


@article{bai2019beyond,
  title   = {Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks},
  author  = {Bai, Yu and Lee, Jason D},
  journal = {International Conference on Learning Representations (ICLR)},
  year    = {2020}
}


@article{gao2019convergence,
  title   = {Convergence of Adversarial Training in Overparametrized Networks},
  author  = {Gao, Ruiqi and Cai, Tianle and Li, Haochuan and Wang, Liwei and Hsieh, Cho-Jui and Lee, Jason D},
  journal = {Neural Information Processing Systems (NeurIPS)},
  year    = {2019}
}


@article{du2020few,
  title   = {Few-shot learning via learning the representation, provably},
  author  = {Du, Simon S and Hu, Wei and Kakade, Sham M and Lee, Jason D and Lei, Qi},
  journal = {arXiv preprint arXiv:2002.09434},
  year    = {2020}
}

@article{moroshko2020implicit,
  title   = {Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy},
  author  = {Moroshko, Edward and Gunasekar, Suriya and Woodworth, Blake and Lee, Jason D and Srebro, Nathan and Soudry, Daniel},
  journal = {Neural Information Processing Systems (NeurIPS)},
  year    = {2020}
}

@article{chen2020towards,
  title   = {Towards Understanding Hierarchical Learning: Benefits of Neural Representations},
  author  = {Chen, Minshuo and Bai, Yu and Lee, Jason D and Zhao, Tuo and Wang, Huan and Xiong, Caiming and Socher, Richard},
  journal = {Neural Information Processing Systems (NeurIPS)},
  year    = {2020}
}


@article{fang2020modeling,
  title   = {Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks},
  author  = {Fang, Cong and Lee, Jason D and Yang, Pengkun and Zhang, Tong},
  journal = {arXiv preprint arXiv:2007.01452},
  year    = {2020}
}

@article{lee2020predicting,
  title   = {Predicting what you already know helps: Provable self-supervised learning},
  author  = {Lee, Jason D and Lei, Qi and Saunshi, Nikunj and Zhuo, Jiacheng},
  journal = {arXiv preprint arXiv:2008.01064},
  year    = {2020}
}

@article{wang2020beyond,
  title   = {Beyond Lazy Training for Over-parameterized Tensor Decomposition},
  author  = {Wang, Xiang and Wu, Chenwei and Lee, Jason D and Ma, Tengyu and Ge, Rong},
  journal = {Neural Information Processing Systems (NeurIPS)},
  year    = {2020}
}

@article{pemantle1990nonconvergence,
  title     = {Nonconvergence to unstable points in urn models and stochastic approximations},
  author    = {Pemantle, Robin and others},
  journal   = {The Annals of Probability},
  volume    = {18},
  number    = {2},
  pages     = {698--712},
  year      = {1990},
  publisher = {Institute of Mathematical Statistics}
}

@article{foret2020sharpness,
  title   = {Sharpness-Aware Minimization for Efficiently Improving Generalization},
  author  = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal = {arXiv preprint arXiv:2010.01412},
  year    = {2020}
}

@inproceedings{li2020learning,
  title        = {Learning over-parametrized two-layer neural networks beyond ntk},
  author       = {Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang R},
  booktitle    = {Conference on Learning Theory},
  pages        = {2613--2682},
  year         = {2020},
  organization = {PMLR}
}

@article{allen2019can,
  title   = {What can resnet learn efficiently, going beyond kernels?},
  author  = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal = {arXiv preprint arXiv:1905.10337},
  year    = {2019}
}

@misc{cohen2021gradient,
  title         = {Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability},
  author        = {Jeremy M. Cohen and Simran Kaur and Yuanzhi Li and J. Zico Kolter and Ameet Talwalkar},
  year          = {2021},
  eprint        = {2103.00065},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}
@techreport{Krizhevsky09learningmultiple,
  author      = {Alex Krizhevsky},
  title       = {Learning multiple layers of features from tiny images},
  institution = {},
  year        = {2009}
}

@misc{wandb,
  title  = {Experiment Tracking with Weights and Biases},
  year   = {2020},
  note   = {Software available from wandb.com},
  url    = {https://www.wandb.com/},
  author = {Biewald, Lukas}
}
@incollection{pytorch2019,
  title     = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author    = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8024--8035},
  year      = {2019},
  publisher = {Curran Associates, Inc.},
  url       = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}
@article{pytorchlightning2019,
  title   = {PyTorch Lightning},
  author  = {William {Falcon et al.}},
  journal = {GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning},
  volume  = {3},
  year    = {2019}
}