\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aharon et~al.(2006)Aharon, Elad, and Bruckstein]{aharon2006k}
Michal Aharon, Michael Elad, and Alfred Bruckstein.
\newblock {K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation}.
\newblock \emph{IEEE Transactions on signal processing}, 54\penalty0 (11):\penalty0 4311--4322, 2006.

\bibitem[Arora et~al.(2015)Arora, Ge, Ma, and Moitra]{arora2015simple}
Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra.
\newblock Simple, efficient, and neural algorithms for sparse coding.
\newblock In \emph{Conference on learning theory}, pages 113--149. PMLR, 2015.

\bibitem[Arora et~al.(2018)Arora, Li, Liang, Ma, and Risteski]{arora2018linear}
Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski.
\newblock Linear algebraic structure of word senses, with applications to polysemy.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 6:\penalty0 483--495, 2018.
\newblock \doi{10.1162/tacl_a_00034}.
\newblock URL \url{https://aclanthology.org/Q18-1034}.

\bibitem[Arpit et~al.(2016)Arpit, Zhou, Ngo, and Govindaraju]{arpit2016regularized}
Devansh Arpit, Yingbo Zhou, Hung Ngo, and Venu Govindaraju.
\newblock Why regularized auto-encoders learn sparse representation?
\newblock In \emph{International Conference on Machine Learning}, pages 136--144. PMLR, 2016.

\bibitem[Bao et~al.(2015)Bao, Ji, Quan, and Shen]{bao2015dictionary}
Chenglong Bao, Hui Ji, Yuhui Quan, and Zuowei Shen.
\newblock Dictionary learning for sparse coding: Algorithms and convergence analysis.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 38\penalty0 (7):\penalty0 1356--1369, 2015.

\bibitem[Bills et~al.(2023)Bills, Cammarata, Mossing, Tillman, Gao, Goh, Sutskever, Leike, Wu, and Saunders]{bills2023language}
Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders.
\newblock Language models can explain neurons in language models.
\newblock \emph{URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023)}, 2023.

\bibitem[Blasiok and Nelson(2016)]{blasiok2016improved}
Jaroslaw Blasiok and Jelani Nelson.
\newblock An improved analysis of the er-spud dictionary learning algorithm.
\newblock In \emph{43rd International Colloquium on Automata, Languages, and Programming (ICALP 2016)}. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.

\bibitem[Bloom and Chanin(2024)]{SAELens}
Joseph Bloom and David Chanin.
\newblock Sae lens.
\newblock \url{https://github.com/jbloomAus/SAELens}, 2024.

\bibitem[Bricken et~al.(2023)Bricken, Templeton, Batson, Chen, Jermyn, Conerly, Turner, Anil, Denison, Askell, et~al.]{bricken2023towards}
Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, et~al.
\newblock Towards monosemanticity: Decomposing language models with dictionary learning.
\newblock \emph{Transformer Circuits Thread}, page~2, 2023.

\bibitem[Chalk et~al.(2018)Chalk, Marre, and Tka{\v{c}}ik]{chalk2018toward}
Matthew Chalk, Olivier Marre, and Ga{\v{s}}per Tka{\v{c}}ik.
\newblock Toward a unified theory of efficient, predictive, and sparse coding.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0 (1):\penalty0 186--191, 2018.

\bibitem[Chartrand(2007)]{chartrand2007exact}
Rick Chartrand.
\newblock Exact reconstruction of sparse signals via nonconvex minimization.
\newblock \emph{IEEE Signal Processing Letters}, 14\penalty0 (10):\penalty0 707--710, 2007.

\bibitem[Christensen et~al.(2003)]{christensen2003introduction}
Ole Christensen et~al.
\newblock \emph{An introduction to frames and Riesz bases}, volume~7.
\newblock Springer, 2003.

\bibitem[Coates and Ng(2011)]{coates2011importance}
Adam Coates and Andrew~Y Ng.
\newblock The importance of encoding versus training with sparse coding and vector quantization.
\newblock In \emph{Proceedings of the 28th international conference on machine learning (ICML-11)}, pages 921--928, 2011.

\bibitem[Coates et~al.(2011)Coates, Ng, and Lee]{coates2011analysis}
Adam Coates, Andrew Ng, and Honglak Lee.
\newblock An analysis of single-layer networks in unsupervised feature learning.
\newblock In \emph{Proceedings of the fourteenth international conference on artificial intelligence and statistics}, pages 215--223. JMLR Workshop and Conference Proceedings, 2011.

\bibitem[Cooney(2023)]{cooney2023SparseAutoencoder}
Alan Cooney.
\newblock Sparse autoencoder library.
\newblock \url{https://github.com/ai-safety-foundation/sparse_autoencoder}, 2023.

\bibitem[Cunningham et~al.(2023)Cunningham, Ewart, Smith, Huben, and Sharkey]{cunningham2023sparse}
Hoagy Cunningham, Aidan Ewart, Logan~Riggs Smith, Robert Huben, and Lee Sharkey.
\newblock Sparse autoencoders find highly interpretable features in language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Daubechies et~al.(2004)Daubechies, Defrise, and De~Mol]{daubechies2004iterative}
Ingrid Daubechies, Michel Defrise, and Christine De~Mol.
\newblock An iterative thresholding algorithm for linear inverse problems with a sparsity constraint.
\newblock \emph{Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences}, 57\penalty0 (11):\penalty0 1413--1457, 2004.

\bibitem[Davis et~al.(1997)Davis, Mallat, and Avellaneda]{davis1997adaptive}
Geoff Davis, Stephane Mallat, and Marco Avellaneda.
\newblock Adaptive greedy approximations.
\newblock \emph{Constructive approximation}, 13:\penalty0 57--98, 1997.

\bibitem[Donoho(1992)]{donoho1992superresolution}
David~L Donoho.
\newblock Superresolution via sparsity constraints.
\newblock \emph{SIAM journal on mathematical analysis}, 23\penalty0 (5):\penalty0 1309--1331, 1992.

\bibitem[Dumitrescu and Irofti(2018)]{dumitrescu2018dictionary}
Bogdan Dumitrescu and Paul Irofti.
\newblock \emph{Dictionary learning algorithms and applications}.
\newblock Springer, 2018.

\bibitem[Eggert and Korner(2004)]{eggert2004sparse}
Julian Eggert and Edgar Korner.
\newblock Sparse coding and nmf.
\newblock In \emph{2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No. 04CH37541)}, volume~4, pages 2529--2533. IEEE, 2004.

\bibitem[Elad(2010)]{elad2010sparse}
Michael Elad.
\newblock \emph{Sparse and redundant representations: from theory to applications in signal and image processing}.
\newblock Springer Science \& Business Media, 2010.

\bibitem[Elad and Bruckstein(2002)]{elad2002generalized}
Michael Elad and Alfred~M Bruckstein.
\newblock A generalized uncertainty principle and sparse representation in pairs of bases.
\newblock \emph{IEEE Transactions on Information Theory}, 48\penalty0 (9):\penalty0 2558--2567, 2002.

\bibitem[Elhage et~al.(2022)Elhage, Hume, Olsson, Schiefer, Henighan, Kravec, Hatfield-Dodds, Lasenby, Drain, Chen, et~al.]{elhage2022superposition}
Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et~al.
\newblock Toy models of superposition.
\newblock \emph{arXiv preprint arXiv:2209.10652}, 2022.

\bibitem[Ferrando et~al.(2024)Ferrando, Sarti, Bisazza, and Costa-juss{\`a}]{ferrando2024primer}
Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta~R Costa-juss{\`a}.
\newblock A primer on the inner workings of transformer-based language models.
\newblock \emph{arXiv preprint arXiv:2405.00208}, 2024.

\bibitem[Foucart and Rauhut(2013)]{Foucart.2013}
Simon Foucart and Holger Rauhut.
\newblock \emph{{A Mathematical Introduction to Compressive Sensing}}.
\newblock {Springer New York}, New York, NY, 2013.

\bibitem[Gao et~al.(2024)Gao, la~Tour, Tillman, Goh, Troll, Radford, Sutskever, Leike, and Wu]{gao2024scaling}
Leo Gao, Tom~Dupr{\'e} la~Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu.
\newblock Scaling and evaluating sparse autoencoders.
\newblock \emph{arXiv preprint arXiv:2406.04093}, 2024.

\bibitem[Gregor and LeCun(2010)]{gregor2010learning}
Karol Gregor and Yann LeCun.
\newblock Learning fast approximations of sparse coding.
\newblock In \emph{Proceedings of the 27th international conference on international conference on machine learning}, pages 399--406, 2010.

\bibitem[Hinton and Zemel(1993)]{hinton1993autoencoders}
Geoffrey~E Hinton and Richard Zemel.
\newblock Autoencoders, minimum description length and helmholtz free energy.
\newblock \emph{Advances in neural information processing systems}, 6, 1993.

\bibitem[Hoyer(2002)]{hoyer2002non}
Patrik~O Hoyer.
\newblock Non-negative sparse coding.
\newblock In \emph{Proceedings of the 12th IEEE workshop on neural networks for signal processing}, pages 557--565. IEEE, 2002.

\bibitem[Jermyn et~al.(2024)Jermyn, Templeton, Batson, and Bricken]{jermyn2024tanh}
Adam Jermyn, Adly Templeton, Joshua Batson, and Trenton Bricken.
\newblock Tanh penalty in dictionary learning, 2024.
\newblock URL \url{https://transformer-circuits.pub/2024/feb-update/index.html}.
\newblock Accessed: 2024-05-20.

\bibitem[Jung et~al.(2014)Jung, Eldar, and G{\"o}rtz]{jung2014performance}
Alexander Jung, Yonina~C Eldar, and Norbert G{\"o}rtz.
\newblock Performance limits of dictionary learning for sparse coding.
\newblock In \emph{2014 22nd European Signal Processing Conference (EUSIPCO)}, pages 765--769. IEEE, 2014.

\bibitem[Karvonen(2024)]{karvonen2024emergent}
Adam Karvonen.
\newblock Emergent world models and latent variable estimation in chess-playing language models, 2024.

\bibitem[Kissane et~al.(2024)Kissane, Krzyzanowski, Bloom, Conmy, and Nanda]{kissane2024interpreting}
Connor Kissane, Robert Krzyzanowski, Joseph~Isaac Bloom, Arthur Conmy, and Neel Nanda.
\newblock Interpreting attention layer outputs with sparse autoencoders.
\newblock In \emph{ICML 2024 Workshop on Mechanistic Interpretability}, 2024.

\bibitem[Li et~al.(2016)Li, Zhang, Luo, Yang, Yuan, and Zhang]{li2016sparseness}
Jun Li, Tong Zhang, Wei Luo, Jian Yang, Xiao-Tong Yuan, and Jian Zhang.
\newblock Sparseness analysis in the pretraining of deep neural networks.
\newblock \emph{IEEE transactions on neural networks and learning systems}, 28\penalty0 (6):\penalty0 1425--1438, 2016.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Hopkins, Bau, Vi{\'e}gas, Pfister, and Wattenberg]{li2023emergent}
Kenneth Li, Aspen~K Hopkins, David Bau, Fernanda Vi{\'e}gas, Hanspeter Pfister, and Martin Wattenberg.
\newblock Emergent world representations: Exploring a sequence model trained on a synthetic task.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Pei, and Li]{li2023comprehensive}
Pengzhi Li, Yan Pei, and Jianqiang Li.
\newblock A comprehensive survey on design and application of autoencoder in deep learning.
\newblock \emph{Applied Soft Computing}, 138:\penalty0 110176, 2023{\natexlab{b}}.

\bibitem[Lichess(2024)]{lichess}
Lichess.
\newblock lichess.org open database, 2024.
\newblock URL \url{https://database.lichess.org}.

\bibitem[Luo et~al.(2017)Luo, Li, Yang, Xu, and Zhang]{luo2017convolutional}
Wei Luo, Jun Li, Jian Yang, Wei Xu, and Jian Zhang.
\newblock Convolutional sparse autoencoders for image classification.
\newblock \emph{IEEE transactions on neural networks and learning systems}, 29\penalty0 (7):\penalty0 3289--3294, 2017.

\bibitem[Makelov et~al.(2024)Makelov, Lange, and Nanda]{makelov2024towards}
Aleksandar Makelov, George Lange, and Neel Nanda.
\newblock Towards principled evaluations of sparse autoencoders for interpretability and control.
\newblock \emph{arXiv preprint arXiv:2405.08366}, 2024.

\bibitem[Makhzani and Frey(2013)]{makhzani2013k}
Alireza Makhzani and Brendan Frey.
\newblock K-sparse autoencoders.
\newblock \emph{arXiv preprint arXiv:1312.5663}, 2013.

\bibitem[Mallat and Zhang(1993)]{mallat1993matching}
St{\'e}phane~G Mallat and Zhifeng Zhang.
\newblock Matching pursuits with time-frequency dictionaries.
\newblock \emph{IEEE Transactions on signal processing}, 41\penalty0 (12):\penalty0 3397--3415, 1993.

\bibitem[Marks and Mueller(2024)]{dictionarylearning}
Samuel Marks and Aaron Mueller.
\newblock dictionary\_learning.
\newblock \url{https://github.com/saprmarks/dictionary_learning}, 2024.

\bibitem[Marks et~al.(2024)Marks, Rager, Michaud, Belinkov, Bau, and Mueller]{marks2024sparse}
Samuel Marks, Can Rager, Eric~J Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller.
\newblock Sparse feature circuits: Discovering and editing interpretable causal graphs in language models.
\newblock \emph{arXiv preprint arXiv:2403.19647}, 2024.

\bibitem[McGrath et~al.(2022)McGrath, Kapishnikov, Tomašev, Pearce, Wattenberg, Hassabis, Kim, Paquet, and Kramnik]{McGrath2022Acquisition}
Thomas McGrath, Andrei Kapishnikov, Nenad Tomašev, Adam Pearce, Martin Wattenberg, Demis Hassabis, Been Kim, Ulrich Paquet, and Vladimir Kramnik.
\newblock Acquisition of chess knowledge in alphazero.
\newblock \emph{Proceedings of the National Academy of Sciences}, 119\penalty0 (47), November 2022.
\newblock ISSN 1091-6490.
\newblock \doi{10.1073/pnas.2206625119}.
\newblock URL \url{http://dx.doi.org/10.1073/pnas.2206625119}.

\bibitem[Mikolov et~al.(2013)Mikolov, Sutskever, Chen, Corrado, and Dean]{mikolov2013distributed}
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg~S Corrado, and Jeff Dean.
\newblock Distributed representations of words and phrases and their compositionality.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Nanda et~al.(2023)Nanda, Lee, and Wattenberg]{nanda2023emergent}
Neel Nanda, Andrew Lee, and Martin Wattenberg.
\newblock Emergent linear representations in world models of self-supervised sequence models.
\newblock In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, \emph{Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP}, pages 16--30, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.blackboxnlp-1.2}.
\newblock URL \url{https://aclanthology.org/2023.blackboxnlp-1.2}.

\bibitem[Natarajan(1995)]{natarajan1995sparse}
Balas~Kausik Natarajan.
\newblock Sparse approximate solutions to linear systems.
\newblock \emph{SIAM journal on computing}, 24\penalty0 (2):\penalty0 227--234, 1995.

\bibitem[Ng et~al.(2011)]{ng2011sparse}
Andrew Ng et~al.
\newblock Sparse autoencoder.
\newblock \emph{CS294A Lecture notes}, 72\penalty0 (2011):\penalty0 1--19, 2011.

\bibitem[Nguyen et~al.(2019)Nguyen, Wong, and Hegde]{nguyen2019dynamics}
Thanh~V Nguyen, Raymond~KW Wong, and Chinmay Hegde.
\newblock On the dynamics of gradient descent for autoencoders.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence and Statistics}, pages 2858--2867. PMLR, 2019.

\bibitem[Olshausen and Field(1996)]{olshausen1996emergence}
Bruno~A Olshausen and David~J Field.
\newblock Emergence of simple-cell receptive field properties by learning a sparse code for natural images.
\newblock \emph{Nature}, 381\penalty0 (6583):\penalty0 607--609, 1996.

\bibitem[Olshausen and Field(2004)]{olshausen2004sparse}
Bruno~A Olshausen and David~J Field.
\newblock Sparse coding of sensory inputs.
\newblock \emph{Current opinion in neurobiology}, 14\penalty0 (4):\penalty0 481--487, 2004.

\bibitem[Park et~al.(2023)Park, Choe, and Veitch]{park2023linear}
Kiho Park, Yo~Joong Choe, and Victor Veitch.
\newblock The linear representation hypothesis and the geometry of large language models.
\newblock \emph{arXiv preprint arXiv:2311.03658}, 2023.

\bibitem[Rajamanoharan et~al.(2024)Rajamanoharan, Conmy, Smith, Lieberum, Varma, Kram{\'a}r, Shah, and Nanda]{rajamanoharan2024GatedSAE}
Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma, J{\'a}nos Kram{\'a}r, Rohin Shah, and Neel Nanda.
\newblock Improving dictionary learning with gated sparse autoencoders.
\newblock \emph{arXiv preprint arXiv:2404.16014}, 2024.

\bibitem[Rangamani et~al.(2018)Rangamani, Mukherjee, Basu, Arora, Ganapathi, Chin, and Tran]{rangamani2018sparse}
Akshay Rangamani, Anirbit Mukherjee, Amitabh Basu, Ashish Arora, Tejaswini Ganapathi, Sang Chin, and Trac~D Tran.
\newblock Sparse coding and autoencoders.
\newblock In \emph{2018 IEEE International Symposium on Information Theory (ISIT)}, pages 36--40. IEEE, 2018.

\bibitem[Riggs and Brinkmann(2024)]{riggs2024sqrt}
Logan Riggs and Jannik Brinkmann.
\newblock Improving sparse autoencoders by square-rooting l1 and removing lowest activation features, 2024.
\newblock URL \url{https://www.lesswrong.com/posts/YiGs8qJ8aNBgwt2YN/improving-sae-s-by-sqrt-ing-l1-and-removing-lowest}.
\newblock Accessed: 2024-05-20.

\bibitem[Sharkey et~al.(2023)Sharkey, Braun, and Millidge]{sharkey2023technical}
Lee Sharkey, Dan Braun, and Beren Millidge.
\newblock Taking features out of superposition with sparse autoencoders, 2023.
\newblock URL \url{https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition}.
\newblock Accessed: 2023-05-10.

\bibitem[Templeton et~al.(2024)Templeton, Conerly, Marcus, Lindsey, Bricken, Chen, Pearce, Citro, Ameisen, Jones, Cunningham, Turner, McDougall, MacDiarmid, Freeman, Sumers, Rees, Batson, Jermyn, Carter, Olah, and Henighan]{templeton2024scaling}
Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas~L Turner, Callum McDougall, Monte MacDiarmid, C.~Daniel Freeman, Theodore~R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan.
\newblock Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet.
\newblock \emph{Transformer Circuits Thread}, 2024.
\newblock URL \url{https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}.

\bibitem[Tillmann(2014)]{tillmann2014computational}
Andreas~M Tillmann.
\newblock On the computational intractability of exact and approximate dictionary learning.
\newblock \emph{IEEE Signal Processing Letters}, 22\penalty0 (1):\penalty0 45--49, 2014.

\bibitem[Wang et~al.(2011)Wang, Xu, and Tang]{wang2011performance}
Meng Wang, Weiyu Xu, and Ao~Tang.
\newblock On the performance of sparse recovery via $\ell_p$-minimization $(0 \leq p \leq 1)$.
\newblock \emph{IEEE Transactions on Information Theory}, 57\penalty0 (11):\penalty0 7255--7278, 2011.

\bibitem[Wen et~al.(2015)Wen, Li, and Zhu]{wen2015stable}
Jinming Wen, Dongfang Li, and Fumin Zhu.
\newblock Stable recovery of sparse signals via lp-minimization.
\newblock \emph{Applied and Computational Harmonic Analysis}, 38\penalty0 (1):\penalty0 161--176, 2015.

\bibitem[Wright and Sharkey(2024)]{wright2024suppression}
Benjamin Wright and Lee Sharkey.
\newblock Addressing feature suppression in sparse autoencoders, 2024.
\newblock URL \url{https://www.lesswrong.com/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes}.
\newblock Accessed: 2024-05-20.

\bibitem[Wright and Ma(2022)]{wright2022high}
John Wright and Yi~Ma.
\newblock \emph{High-dimensional data analysis with low-dimensional models: Principles, computation, and applications}.
\newblock Cambridge University Press, 2022.

\bibitem[Yang et~al.(2018)Yang, Shen, Ma, Gu, and So]{yang2018sparse}
Chengzhu Yang, Xinyue Shen, Hongbing Ma, Yuantao Gu, and Hing~Cheung So.
\newblock Sparse recovery conditions and performance bounds for $\ell_p$-minimization.
\newblock \emph{IEEE Transactions on Signal Processing}, 66\penalty0 (19):\penalty0 5014--5028, 2018.

\bibitem[Yang et~al.(2009)Yang, Yu, Gong, and Huang]{yang2009linear}
Jianchao Yang, Kai Yu, Yihong Gong, and Thomas Huang.
\newblock Linear spatial pyramid matching using sparse coding for image classification.
\newblock In \emph{2009 IEEE Conference on computer vision and pattern recognition}, pages 1794--1801. IEEE, 2009.

\bibitem[Zheng et~al.(2017)Zheng, Maleki, Weng, Wang, and Long]{zheng2017does}
Le~Zheng, Arian Maleki, Haolei Weng, Xiaodong Wang, and Teng Long.
\newblock Does $\ell_p$-minimization outperform $\ell_1$-minimization?
\newblock \emph{IEEE Transactions on Information Theory}, 63\penalty0 (11):\penalty0 6896--6935, 2017.

\end{thebibliography}
