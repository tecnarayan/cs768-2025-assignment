\begin{thebibliography}{10}

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em NeurIPS}, 33:6840--6851, 2020.

\bibitem{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em CVPR}, pages 10684--10695, 2022.

\bibitem{peebles2023scalable}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 4195--4205, 2023.

\bibitem{zhang2023adding}
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.
\newblock Adding conditional control to text-to-image diffusion models.
\newblock In {\em ICCV}, pages 3836--3847, 2023.

\bibitem{jia2024ssmg}
Chengyou Jia, Minnan Luo, Zhuohang Dang, Guang Dai, Xiaojun Chang, Mengmeng Wang, and Jingdong Wang.
\newblock Ssmg: Spatial-semantic map guided diffusion model for free-form layout-to-image generation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 2480--2488, 2024.

\bibitem{chen2023anydoor}
Xi~Chen, Lianghua Huang, Yu~Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao.
\newblock Anydoor: Zero-shot object-level image customization.
\newblock {\em arXiv preprint arXiv:2307.09481}, 2023.

\bibitem{kawar2023imagic}
Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani.
\newblock Imagic: Text-based real image editing with diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 6007--6017, 2023.

\bibitem{ruiz2023dreambooth}
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.
\newblock Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 22500--22510, 2023.

\bibitem{kumari2023multi}
Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu.
\newblock Multi-concept customization of text-to-image diffusion.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 1931--1941, 2023.

\bibitem{li2023drivingdiffusion}
Xiaofan Li, Yifu Zhang, and Xiaoqing Ye.
\newblock Drivingdiffusion: Layout-guided multi-view driving scene video generation with latent diffusion model.
\newblock {\em arXiv preprint arXiv:2310.07771}, 2023.

\bibitem{gao2023magicdrive}
Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu.
\newblock Magicdrive: Street view generation with diverse 3d geometry control.
\newblock {\em arXiv preprint arXiv:2310.02601}, 2023.

\bibitem{chen2023integrating}
Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, Zhenguo Li, and Dit-Yan Yeung.
\newblock Integrating geometric control into text-to-image diffusion models for high-quality detection data generation via text prompt.
\newblock {\em arXiv: 2306.04607}, 2023.

\bibitem{Chen2023PolyDiffuse}
Jiacheng Chen, Ruizhi Deng, and Yasutaka Furukawa.
\newblock Polydiffuse: Polygonal shape reconstruction via guided set diffusion models.
\newblock {\em ArXiv}, abs/2306.01461, 2023.

\bibitem{wang2023dire}
Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li.
\newblock Dire for diffusion-generated image detection.
\newblock {\em arXiv preprint arXiv:2303.09295}, 2023.

\bibitem{song2020denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock {\em arXiv preprint arXiv:2010.02502}, 2020.

\bibitem{kim2023bksdm}
Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi.
\newblock Bk-sdm: Architecturally compressed stable diffusion for efficient text-to-image generation.
\newblock {\em ICML Workshop on Efficient Systems for Foundation Models (ES-FoMo)}, 2023.

\bibitem{Lee2023koala}
Youngwan Lee, Kwanyong Park, Yoohrim Cho, Yong~Ju Lee, and Sung~Ju Hwang.
\newblock Koala: Self-attention matters in knowledge distillation of latent diffusion models for memory-efficient and fast image synthesis.
\newblock {\em arXiv preprint arXiv:2312.04005}, 2023.

\bibitem{luo2023latent}
Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao.
\newblock Latent consistency models: Synthesizing high-resolution images with few-step inference, 2023.

\bibitem{luo2023lcmlora}
Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolin√°rio Passos, Longbo Huang, Jian Li, and Hang Zhao.
\newblock Lcm-lora: A universal stable-diffusion acceleration module, 2023.

\bibitem{fang2023structural}
Gongfan Fang, Xinyin Ma, and Xinchao Wang.
\newblock Structural pruning for diffusion models.
\newblock In {\em Advances in Neural Information Processing Systems}, 2023.

\bibitem{zhang2024laptopdiff}
Dingkun Zhang, Sijia Li, Chen Chen, Qingsong Xie, and Haonan Lu.
\newblock Laptop-diff: Layer pruning and normalized distillation for compressing diffusion models, 2024.

\bibitem{ma2023deepcache}
Xinyin Ma, Gongfan Fang, and Xinchao Wang.
\newblock Deepcache: Accelerating diffusion models for free.
\newblock In {\em The IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2024.

\bibitem{chen2016training}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock {\em arXiv preprint arXiv:1604.06174}, 2016.

\bibitem{lu2022dpm}
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
\newblock Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.
\newblock {\em Advances in Neural Information Processing Systems}, 35:5775--5787, 2022.

\bibitem{lu2023dpmsolver}
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
\newblock Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models, 2023.

\bibitem{song2023consistency}
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.
\newblock Consistency models.
\newblock {\em arXiv preprint arXiv:2303.01469}, 2023.

\bibitem{liu2022pseudo}
Luping Liu, Yi~Ren, Zhijie Lin, and Zhou Zhao.
\newblock Pseudo numerical methods for diffusion models on manifolds.
\newblock {\em arXiv preprint arXiv:2202.09778}, 2022.

\bibitem{sauer2023adversarial}
Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach.
\newblock Adversarial diffusion distillation.
\newblock {\em arXiv preprint arXiv:2311.17042}, 2023.

\bibitem{tgate}
Wentian Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike~Zheng Shou, and J{\"u}rgen Schmidhuber.
\newblock Cross-attention makes inference cumbersome in text-to-image diffusion models.
\newblock {\em arXiv preprint arXiv:2404.02747}, 2024.

\bibitem{salimans2022progressive}
Tim Salimans and Jonathan Ho.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock {\em arXiv preprint arXiv:2202.00512}, 2022.

\bibitem{berthelot2023tract}
David Berthelot, Arnaud Autef, Jierui Lin, Dian~Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, and Eric Gu.
\newblock Tract: Denoising diffusion models with transitive closure time-distillation.
\newblock {\em arXiv preprint arXiv:2303.04248}, 2023.

\bibitem{lyu2022accelerating}
Zhaoyang Lyu, Xudong Xu, Ceyuan Yang, Dahua Lin, and Bo~Dai.
\newblock Accelerating diffusion models via early stop of the diffusion process.
\newblock {\em arXiv preprint arXiv:2205.12524}, 2022.

\bibitem{he2023ptqd}
Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang.
\newblock Ptqd: Accurate post-training quantization for diffusion models, 2023.

\bibitem{park2020lookahead}
Sejun Park, Jaeho Lee, Sangwoo Mo, and Jinwoo Shin.
\newblock Lookahead: A far-sighted alternative of magnitude-based pruning.
\newblock {\em arXiv preprint arXiv:2002.04809}, 2020.

\bibitem{dong2017learning}
Xin Dong, Shangyu Chen, and Sinno Pan.
\newblock Learning to prune deep neural networks via layer-wise optimal brain surgeon.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{sanh2020movement}
Victor Sanh, Thomas Wolf, and Alexander Rush.
\newblock Movement pruning: Adaptive sparsity by fine-tuning.
\newblock {\em Advances in neural information processing systems}, 33:20378--20389, 2020.

\bibitem{li2017pruning}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets, 2017.

\bibitem{elkerdawy2020filter}
Sara Elkerdawy, Mostafa Elhoushi, Abhineet Singh, Hong Zhang, and Nilanjan Ray.
\newblock To filter prune, or to layer prune, that is the question.
\newblock In {\em Proceedings of the Asian Conference on Computer Vision}, 2020.

\bibitem{ding2019centripetal}
Xiaohan Ding, Guiguang Ding, Yuchen Guo, and Jungong Han.
\newblock Centripetal sgd for pruning very deep convolutional networks with complicated structure.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 4943--4953, 2019.

\bibitem{liu2021group}
Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jing-Hao Xue, Xinjiang Wang, Yimin Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang.
\newblock Group fisher pruning for practical network compression.
\newblock In {\em International Conference on Machine Learning}, pages 7021--7032. PMLR, 2021.

\bibitem{liu2024updp}
Ji~Liu, Dehua Tang, Yuanxian Huang, Li~Zhang, Xiaocheng Zeng, Dong Li, Mingjie Lu, Jinzhang Peng, Yu~Wang, Fan Jiang, Lu~Tian, and Ashish Sirasao.
\newblock Updp: A unified progressive depth pruner for cnn and vision transformer, 2024.

\bibitem{zhang2020accelerating}
Minjia Zhang and Yuxiong He.
\newblock Accelerating training of transformer-based language models with progressive layer dropping.
\newblock {\em Advances in Neural Information Processing Systems}, 33:14011--14023, 2020.

\bibitem{dong2021attention}
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
\newblock Attention is not all you need: Pure attention loses rank doubly exponentially with depth.
\newblock In {\em International Conference on Machine Learning}, pages 2793--2803. PMLR, 2021.

\bibitem{hou2020dynabert}
Lu~Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu.
\newblock Dynabert: Dynamic bert with adaptive width and depth.
\newblock {\em Advances in Neural Information Processing Systems}, 33:9782--9793, 2020.

\bibitem{carion2020end}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In {\em European conference on computer vision}, pages 213--229. Springer, 2020.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural networks.
\newblock {\em arXiv preprint arXiv:1803.03635}, 2018.

\bibitem{holland1992genetic}
John~H Holland.
\newblock Genetic algorithms.
\newblock {\em Scientific american}, 267(1):66--73, 1992.

\bibitem{jang2016categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock {\em arXiv preprint arXiv:1611.01144}, 2016.

\bibitem{wang2004image}
Zhou Wang, Alan~C Bovik, Hamid~R Sheikh, and Eero~P Simoncelli.
\newblock Image quality assessment: from error visibility to structural similarity.
\newblock {\em IEEE transactions on image processing}, 13(4):600--612, 2004.

\bibitem{podell2023sdxl}
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M{\"u}ller, Joe Penna, and Robin Rombach.
\newblock Sdxl: Improving latent diffusion models for high-resolution image synthesis.
\newblock {\em arXiv preprint arXiv:2307.01952}, 2023.

\bibitem{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In {\em Medical image computing and computer-assisted intervention--MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18}, pages 234--241. Springer, 2015.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wangDiffusionDBLargescalePrompt2022}
Zijie~J. Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen~Horng Chau.
\newblock {{DiffusionDB}}: {{A}} large-scale prompt gallery dataset for text-to-image generative models.
\newblock {\em arXiv:2210.14896 [cs]}, 2022.

\bibitem{yu2022scaling}
Jiahui Yu, Yuanzhong Xu, Jing~Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu~Karagol Ayan, et~al.
\newblock Scaling autoregressive models for content-rich text-to-image generation.
\newblock {\em arXiv preprint arXiv:2206.10789}, 2(3):5, 2022.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages 740--755. Springer, 2014.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern recognition}, pages 248--255. Ieee, 2009.

\bibitem{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash equilibrium.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{hessel2021clipscore}
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan~Le Bras, and Yejin Choi.
\newblock Clipscore: A reference-free evaluation metric for image captioning.
\newblock {\em arXiv preprint arXiv:2104.08718}, 2021.

\bibitem{diffusers}
Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, Steven Liu, William Berman, Yiyi Xu, and Thomas Wolf.
\newblock Diffusers: State-of-the-art diffusion models, 2023.

\bibitem{chen2023pixart}
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et~al.
\newblock Pixart-a: Fast training of diffusion transformer for photorealistic text-to-image synthesis.
\newblock {\em arXiv preprint arXiv:2310.00426}, 2023.

\bibitem{cho2023dall}
Jaemin Cho, Abhay Zala, and Mohit Bansal.
\newblock Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 3043--3054, 2023.

\bibitem{naik2023social}
Ranjita Naik and Besmira Nushi.
\newblock Social biases through the text-to-image generation lens.
\newblock In {\em Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society}, pages 786--808, 2023.

\bibitem{ross2020measuring}
Candace Ross, Boris Katz, and Andrei Barbu.
\newblock Measuring social biases in grounded vision and language embeddings.
\newblock {\em arXiv preprint arXiv:2002.08911}, 2020.

\bibitem{lyu2020deepfake}
Siwei Lyu.
\newblock Deepfake detection: Current challenges and next steps.
\newblock pages 1--6, 2020.

\end{thebibliography}
