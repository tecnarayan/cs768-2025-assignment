\begin{thebibliography}{10}

\bibitem{aharoni2019massively}
Roee Aharoni, Melvin Johnson, and Orhan Firat.
\newblock Massively multilingual neural machine translation.
\newblock In {\em Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 3874--3884, 2019.

\bibitem{arivazhagan2019massively}
Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson,
  Maxim Krikun, Mia~Xu Chen, Yuan Cao, George Foster, Colin Cherry, et~al.
\newblock Massively multilingual neural machine translation in the wild:
  Findings and challenges.
\newblock {\em arXiv preprint arXiv:1907.05019}, 2019.

\bibitem{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{bertsekas2014constrained}
Dimitri~P Bertsekas.
\newblock {\em Constrained optimization and Lagrange multiplier methods}.
\newblock Academic press, 2014.

\bibitem{caruana1997multitask}
Rich Caruana.
\newblock Multitask learning.
\newblock {\em Machine learning}, 28(1):41--75, 1997.

\bibitem{chen2018gradnorm}
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich.
\newblock Gradnorm: Gradient normalization for adaptive loss balancing in deep
  multitask networks.
\newblock In {\em International Conference on Machine Learning}, pages
  794--803. PMLR, 2018.

\bibitem{choromanska2015loss}
Anna Choromanska, Mikael Henaff, Michael Mathieu, G{\'e}rard~Ben Arous, and
  Yann LeCun.
\newblock The loss surfaces of multilayer networks.
\newblock In {\em Artificial intelligence and statistics}, pages 192--204.
  PMLR, 2015.

\bibitem{cohen2021gradient}
Jeremy~M Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock {\em arXiv preprint arXiv:2103.00065}, 2021.

\bibitem{collobert2008unified}
Ronan Collobert and Jason Weston.
\newblock A unified architecture for natural language processing: Deep neural
  networks with multitask learning.
\newblock In {\em Proceedings of the 25th international conference on Machine
  learning}, pages 160--167, 2008.

\bibitem{conneau2019unsupervised}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
  Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
  Veselin Stoyanov.
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock {\em arXiv preprint arXiv:1911.02116}, 2019.

\bibitem{doersch2017multi}
Carl Doersch and Andrew Zisserman.
\newblock Multi-task self-supervised visual learning.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 2051--2060, 2017.

\bibitem{fan2020beyond}
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky,
  Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav
  Chaudhary, et~al.
\newblock Beyond english-centric multilingual machine translation.
\newblock {\em arXiv preprint arXiv:2010.11125}, 2020.

\bibitem{fedus2021switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock {\em arXiv preprint arXiv:2101.03961}, 2021.

\bibitem{foret2020sharpness}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock {\em arXiv preprint arXiv:2010.01412}, 2020.

\bibitem{fort2019stiffness}
Stanislav Fort, Pawe{\l}~Krzysztof Nowak, Stanislaw Jastrzebski, and Srini
  Narayanan.
\newblock Stiffness: A new perspective on generalization in neural networks.
\newblock {\em arXiv preprint arXiv:1901.09491}, 2019.

\bibitem{goodfellow2014qualitatively}
Ian~J Goodfellow, Oriol Vinyals, and Andrew~M Saxe.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock {\em arXiv preprint arXiv:1412.6544}, 2014.

\bibitem{grave2018learning}
Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas
  Mikolov.
\newblock Learning word vectors for 157 languages.
\newblock {\em arXiv preprint arXiv:1802.06893}, 2018.

\bibitem{hoffer2017train}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock {\em arXiv preprint arXiv:1705.08741}, 2017.

\bibitem{hu2020xtreme}
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and
  Melvin Johnson.
\newblock Xtreme: A massively multilingual multi-task benchmark for evaluating
  cross-lingual generalisation.
\newblock In {\em International Conference on Machine Learning}, pages
  4411--4421. PMLR, 2020.

\bibitem{jastrzebski2020catastrophic}
Stanislaw Jastrzebski, Devansh Arpit, Oliver Astrand, Giancarlo Kerg, Huan
  Wang, Caiming Xiong, Richard Socher, Kyunghyun Cho, and Krzysztof Geras.
\newblock Catastrophic fisher explosion: Early phase fisher matrix impacts
  generalization.
\newblock {\em arXiv preprint arXiv:2012.14193}, 2020.

\bibitem{jastrzebski2019break}
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek
  Tabor, Kyunghyun Cho, and Krzysztof Geras.
\newblock The break-even point on optimization trajectories of deep neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{jean2019adaptive}
S{\'e}bastien Jean, Orhan Firat, and Melvin Johnson.
\newblock Adaptive scheduling for multi-task learning.
\newblock {\em arXiv preprint arXiv:1909.06434}, 2019.

\bibitem{johnson2017google}
Melvin Johnson, Mike Schuster, Quoc~V Le, Maxim Krikun, Yonghui Wu, Zhifeng
  Chen, Nikhil Thorat, Fernanda Vi{\'e}gas, Martin Wattenberg, Greg Corrado,
  et~al.
\newblock Googleâ€™s multilingual neural machine translation system: Enabling
  zero-shot translation.
\newblock {\em Transactions of the Association for Computational Linguistics},
  5:339--351, 2017.

\bibitem{joshi2020state}
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit
  Choudhury.
\newblock The state and fate of linguistic diversity and inclusion in the nlp
  world.
\newblock {\em arXiv preprint arXiv:2004.09095}, 2020.

\bibitem{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}, 2020.

\bibitem{lample2019cross}
Guillaume Lample and Alexis Conneau.
\newblock Cross-lingual language model pretraining.
\newblock {\em arXiv preprint arXiv:1901.07291}, 2019.

\bibitem{lepikhin2020gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
  Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock Gshard: Scaling giant models with conditional computation and
  automatic sharding.
\newblock {\em arXiv preprint arXiv:2006.16668}, 2020.

\bibitem{li2020deep}
Xian Li, Asa Cooper~Stickland, Yuqing Tang, and Xiang Kong.
\newblock Deep transformers with latent depth.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{li2020train}
Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and
  Joey Gonzalez.
\newblock Train big, then compress: Rethinking model size for efficient
  training and inference of transformers.
\newblock In {\em International Conference on Machine Learning}, pages
  5958--5968. PMLR, 2020.

\bibitem{lin2019pareto}
Xi~Lin, Hui-Ling Zhen, Zhenhua Li, Qingfu Zhang, and Sam Kwong.
\newblock Pareto multi-task learning.
\newblock {\em arXiv preprint arXiv:1912.12854}, 2019.

\bibitem{lin2021learning}
Zehui Lin, Liwei Wu, Mingxuan Wang, and Lei Li.
\newblock Learning language specific sub-network for multilingual machine
  translation.
\newblock {\em arXiv preprint arXiv:2105.09259}, 2021.

\bibitem{liu2019understanding}
Jinlong Liu, Yunzhi Bai, Guoqing Jiang, Ting Chen, and Huayan Wang.
\newblock Understanding why neural networks generalize well through gsnr of
  parameters.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{liu2019end}
Shikun Liu, Edward Johns, and Andrew~J Davison.
\newblock End-to-end multi-task learning with attention.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1871--1880, 2019.

\bibitem{liu2020multilingual}
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan
  Ghazvininejad, Mike Lewis, and Luke Zettlemoyer.
\newblock Multilingual denoising pre-training for neural machine translation.
\newblock {\em Transactions of the Association for Computational Linguistics},
  8:726--742, 2020.

\bibitem{lu2021pretrained}
Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch.
\newblock Pretrained transformers as universal computation engines.
\newblock {\em arXiv preprint arXiv:2103.05247}, 2021.

\bibitem{luong2015multi}
Minh-Thang Luong, Quoc~V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.
\newblock Multi-task sequence to sequence learning.
\newblock {\em arXiv preprint arXiv:1511.06114}, 2015.

\bibitem{mccandlish2018empirical}
Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI~Dota Team.
\newblock An empirical model of large-batch training.
\newblock {\em arXiv preprint arXiv:1812.06162}, 2018.

\bibitem{mirzadeh2020understanding}
Seyed~Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, and Hassan
  Ghasemzadeh.
\newblock Understanding the role of training regimes in continual learning.
\newblock {\em arXiv preprint arXiv:2006.06958}, 2020.

\bibitem{nakkiran2019deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock {\em arXiv preprint arXiv:1912.02292}, 2019.

\bibitem{ott2019fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock {\em arXiv preprint arXiv:1904.01038}, 2019.

\bibitem{pires2019multilingual}
Telmo Pires, Eva Schlinger, and Dan Garrette.
\newblock How multilingual is multilingual bert?
\newblock {\em arXiv preprint arXiv:1906.01502}, 2019.

\bibitem{post2018call}
Matt Post.
\newblock A call for clarity in reporting bleu scores.
\newblock {\em arXiv preprint arXiv:1804.08771}, 2018.

\bibitem{qi2018and}
Ye~Qi, Devendra~Singh Sachan, Matthieu Felix, Sarguna~Janani Padmanabhan, and
  Graham Neubig.
\newblock When and why are pre-trained word embeddings useful for neural
  machine translation?
\newblock {\em arXiv preprint arXiv:1804.06323}, 2018.

\bibitem{ravanelli2020multi}
Mirco Ravanelli, Jianyuan Zhong, Santiago Pascual, Pawel Swietojanski, Joao
  Monteiro, Jan Trmal, and Yoshua Bengio.
\newblock Multi-task self-supervised learning for robust speech recognition.
\newblock In {\em ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 6989--6993. IEEE, 2020.

\bibitem{ruder2017overview}
Sebastian Ruder.
\newblock An overview of multi-task learning in deep neural networks.
\newblock {\em arXiv preprint arXiv:1706.05098}, 2017.

\bibitem{sachan2018parameter}
Devendra~Singh Sachan and Graham Neubig.
\newblock Parameter sharing methods for multilingual self-attentional
  translation models.
\newblock {\em arXiv preprint arXiv:1809.00252}, 2018.

\bibitem{sen2019multilingual}
Sukanta Sen, Kamal~Kumar Gupta, Asif Ekbal, and Pushpak Bhattacharyya.
\newblock Multilingual unsupervised nmt using shared encoder and
  language-specific decoders.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 3083--3089, 2019.

\bibitem{sener2018multi}
Ozan Sener and Vladlen Koltun.
\newblock Multi-task learning as multi-objective optimization.
\newblock {\em arXiv preprint arXiv:1810.04650}, 2018.

\bibitem{smith2017don}
Samuel~L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc~V Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock {\em arXiv preprint arXiv:1711.00489}, 2017.

\bibitem{sun2019ernie}
Yu~Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian,
  Danxiang Zhu, Hao Tian, and Hua Wu.
\newblock Ernie: Enhanced representation through knowledge integration.
\newblock {\em arXiv preprint arXiv:1904.09223}, 2019.

\bibitem{tang2020multilingual}
Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary,
  Jiatao Gu, and Angela Fan.
\newblock Multilingual translation with extensible multilingual pretraining and
  finetuning.
\newblock {\em arXiv preprint arXiv:2008.00401}, 2020.

\bibitem{thomas2020interplay}
Valentin Thomas, Fabian Pedregosa, Bart Merri{\"e}nboer, Pierre-Antoine
  Manzagol, Yoshua Bengio, and Nicolas Le~Roux.
\newblock On the interplay between noise and curvature and its effect on
  optimization and generalization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3503--3513. PMLR, 2020.

\bibitem{wang2020balancing}
Xinyi Wang, Yulia Tsvetkov, and Graham Neubig.
\newblock Balancing training for multilingual neural machine translation.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 8526--8537, 2020.

\bibitem{wang2020gradient}
Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao.
\newblock Gradient vaccine: Investigating and improving multi-task optimization
  in massively multilingual models.
\newblock {\em arXiv preprint arXiv:2010.05874}, 2020.

\bibitem{wu2020all}
Shijie Wu and Mark Dredze.
\newblock Are all languages created equal in multilingual bert?
\newblock {\em arXiv preprint arXiv:2005.09093}, 2020.

\bibitem{xiong2020layer}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing,
  Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In {\em International Conference on Machine Learning}, pages
  10524--10533. PMLR, 2020.

\bibitem{xu2019understanding}
Jingjing Xu, Xu~Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin.
\newblock Understanding and improving layer normalization.
\newblock {\em arXiv preprint arXiv:1911.07013}, 2019.

\bibitem{xue2020mt5}
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
  Siddhant, Aditya Barua, and Colin Raffel.
\newblock mt5: A massively multilingual pre-trained text-to-text transformer.
\newblock {\em arXiv preprint arXiv:2010.11934}, 2020.

\bibitem{yao2018hessian}
Zhewei Yao, Amir Gholami, Qi~Lei, Kurt Keutzer, and Michael~W Mahoney.
\newblock Hessian-based analysis of large batch training and robustness to
  adversaries.
\newblock {\em arXiv preprint arXiv:1802.08241}, 2018.

\bibitem{yu2020gradient}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and
  Chelsea Finn.
\newblock Gradient surgery for multi-task learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{zhang2021share}
Biao Zhang, Ankur Bapna, Rico Sennrich, and Orhan Firat.
\newblock Share or not? learning to schedule language-specific ca-pacity for
  multilingual translation.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{zhang2020improving}
Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich.
\newblock Improving massively multilingual neural machine translation and
  zero-shot translation.
\newblock {\em arXiv preprint arXiv:2004.11867}, 2020.

\end{thebibliography}
