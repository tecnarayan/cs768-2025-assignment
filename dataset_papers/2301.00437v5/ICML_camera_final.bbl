\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{arora18}
Arora, S., Cohen, N., and Hazan, E.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  244--253. PMLR, 2018.

\bibitem[Baldi \& Hornik(1989)Baldi and Hornik]{Baldi89}
Baldi, P. and Hornik, K.
\newblock Neural networks and principal component analysis: Learning from
  examples without local minima.
\newblock \emph{Neural Networks}, 2\penalty0 (1):\penalty0 53--58, 1989.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/0893-6080(89)90014-2}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/0893608089900142}.

\bibitem[Belkin et~al.(2019{\natexlab{a}})Belkin, Hsu, Ma, and
  Mandal]{Belkin19}
Belkin, M., Hsu, D., Ma, S., and Mandal, S.
\newblock Reconciling modern machine-learning practice and the classical
  bias{\textendash}variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, jul 2019{\natexlab{a}}.
\newblock \doi{10.1073/pnas.1903070116}.
\newblock URL \url{https://doi.org/10.1073%2Fpnas.1903070116}.

\bibitem[Belkin et~al.(2019{\natexlab{b}})Belkin, Rakhlin, and
  Tsybakov]{Belkin18}
Belkin, M., Rakhlin, A., and Tsybakov, A.~B.
\newblock Does data interpolation contradict statistical optimality?
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  1611--1619. PMLR, 2019{\natexlab{b}}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{Brown20}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Cao et~al.(2019)Cao, Wei, Gaidon, Arechiga, and Ma]{Cao19}
Cao, K., Wei, C., Gaidon, A., Arechiga, N., and Ma, T.
\newblock Learning imbalanced datasets with label-distribution-aware margin
  loss.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Cohen et~al.(2017)Cohen, Afshar, Tapson, and
  Van~Schaik]{cohen2017emnist}
Cohen, G., Afshar, S., Tapson, J., and Van~Schaik, A.
\newblock Emnist: Extending mnist to handwritten letters.
\newblock In \emph{2017 international joint conference on neural networks
  (IJCNN)}, pp.\  2921--2926. IEEE, 2017.

\bibitem[Demirkaya et~al.(2020)Demirkaya, Chen, and Oymak]{Demirkaya20}
Demirkaya, A., Chen, J., and Oymak, S.
\newblock Exploring the role of loss functions in multiclass classification.
\newblock In \emph{2020 54th Annual Conference on Information Sciences and
  Systems (CISS)}, pp.\  1--5, 2020.
\newblock \doi{10.1109/CISS48834.2020.1570627167}.

\bibitem[Ergen \& Pilanci(2021)Ergen and Pilanci]{Ergen20}
Ergen, T. and Pilanci, M.
\newblock Revealing the structure of deep neural networks via convex duality.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3004--3014. PMLR, 2021.

\bibitem[Fang et~al.(2021)Fang, He, Long, and Su]{Fang21}
Fang, C., He, H., Long, Q., and Su, W.~J.
\newblock Exploring deep neural networks via layer-peeled model: Minority
  collapse in imbalanced training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0
  (43), oct 2021.
\newblock \doi{10.1073/pnas.2103091118}.
\newblock URL \url{https://doi.org/10.1073%2Fpnas.2103091118}.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and Courville]{Good16}
Goodfellow, I.~J., Bengio, Y., and Courville, A.
\newblock \emph{Deep Learning}.
\newblock MIT Press, Cambridge, MA, USA, 2016.
\newblock \url{http://www.deeplearningbook.org}.

\bibitem[Graf et~al.(2021)Graf, Hofer, Niethammer, and Kwitt]{graf23}
Graf, F., Hofer, C., Niethammer, M., and Kwitt, R.
\newblock Dissecting supervised contrastive learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3821--3830. PMLR, 2021.

\bibitem[Guo et~al.(2020)Guo, Alvarez, and Salzmann]{guo21}
Guo, S., Alvarez, J.~M., and Salzmann, M.
\newblock Expandnets: Linear over-parameterization to train compact
  convolutional networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1298--1310, 2020.

\bibitem[Han et~al.(2022)Han, Papyan, and Donoho]{Han21}
Han, X., Papyan, V., and Donoho, D.~L.
\newblock Neural collapse under {MSE} loss: Proximity to and dynamics on the
  central path.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=w1UbdvWH_R3}.

\bibitem[Hardt \& Ma(2017)Hardt and Ma]{Hardt18}
Hardt, M. and Ma, T.
\newblock Identity matters in deep learning.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=ryxB0Rtxx}.

\bibitem[Hastie et~al.(2022)Hastie, Montanari, Rosset, and
  Tibshirani]{Hastie20}
Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R.~J.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{The Annals of Statistics}, 50\penalty0 (2):\penalty0 949--986,
  2022.

\bibitem[He \& Su(2022)He and Su]{He22}
He, H. and Su, W.~J.
\newblock A law of data separation in deep learning.
\newblock \emph{arXiv preprint arXiv:2210.17020}, 2022.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and
  Sun]{DBLP:conf/cvpr/HeZRS16}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016}, pp.\
  770--778. {IEEE} Computer Society, 2016{\natexlab{a}}.
\newblock \doi{10.1109/CVPR.2016.90}.
\newblock URL \url{https://doi.org/10.1109/CVPR.2016.90}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{He15}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016{\natexlab{b}}.

\bibitem[Hornik(1991)]{Hornik91}
Hornik, K.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural Networks}, 4\penalty0 (2):\penalty0 251--257, 1991.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/0893-6080(91)90009-T}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/089360809190009T}.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and White]{Hornik89}
Hornik, K., Stinchcombe, M., and White, H.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural Networks}, 2\penalty0 (5):\penalty0 359--366, 1989.
\newblock ISSN 0893-6080.
\newblock \doi{https://doi.org/10.1016/0893-6080(89)90020-8}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/0893608089900208}.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and Weinberger]{Huang17}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{2017 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  2261--2269, 2017.
\newblock \doi{10.1109/CVPR.2017.243}.

\bibitem[Huh et~al.(2021)Huh, Mobahi, Zhang, Cheung, Agrawal, and Isola]{huh23}
Huh, M., Mobahi, H., Zhang, R., Cheung, B., Agrawal, P., and Isola, P.
\newblock The low-rank simplicity bias in deep networks.
\newblock \emph{CoRR}, abs/2103.10427, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.10427}.

\bibitem[Hui \& Belkin(2021)Hui and Belkin]{Hui20}
Hui, L. and Belkin, M.
\newblock Evaluation of neural architectures trained with square loss vs
  cross-entropy in classification tasks.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=hsFN92eQEla}.

\bibitem[Kang et~al.(2020)Kang, Xie, Rohrbach, Yan, Gordo, Feng, and
  Kalantidis]{Kang19}
Kang, B., Xie, S., Rohrbach, M., Yan, Z., Gordo, A., Feng, J., and Kalantidis,
  Y.
\newblock Decoupling representation and classifier for long-tailed recognition.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=r1gRTCVFvB}.

\bibitem[Kawaguchi(2016)]{Kawaguchi16}
Kawaguchi, K.
\newblock Deep learning without poor local minima.
\newblock In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2016/file/f2fc990265c712c49d51a18a32b39f0c-Paper.pdf}.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{Kingma2014}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization, 2014.
\newblock URL \url{https://arxiv.org/abs/1412.6980}.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{Krizhevsky09learningmultiple}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{Krizhevsky12}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Proceedings of the 25th International Conference on Neural
  Information Processing Systems - Volume 1}, NIPS'12, pp.\  1097â€“1105, Red
  Hook, NY, USA, 2012. Curran Associates Inc.

\bibitem[Laurent \& Brecht(2018)Laurent and Brecht]{Laurent17}
Laurent, T. and Brecht, J.
\newblock Deep linear networks with arbitrary loss: All local minima are
  global.
\newblock In \emph{International conference on machine learning}, pp.\
  2902--2907. PMLR, 2018.

\bibitem[Lu \& Steinerberger(2020)Lu and Steinerberger]{Lu20}
Lu, J. and Steinerberger, S.
\newblock Neural collapse with cross-entropy loss, 2020.
\newblock URL \url{https://arxiv.org/abs/2012.08465}.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{Ma17}
Ma, S., Bassily, R., and Belkin, M.
\newblock The power of interpolation: Understanding the effectiveness of sgd in
  modern over-parametrized learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3325--3334. PMLR, 2018.

\bibitem[Mixon et~al.(2022)Mixon, Parshall, and Pi]{Mixon20}
Mixon, D., Parshall, H., and Pi, J.
\newblock Neural collapse with unconstrained features.
\newblock \emph{Sampling Theory, Signal Processing, and Data Analysis}, 20, 07
  2022.
\newblock \doi{10.1007/s43670-022-00027-5}.

\bibitem[Nakkiran et~al.(2021)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{Nakkiran19}
Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2021\penalty0 (12):\penalty0 124003, 2021.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{Papyan20}
Papyan, V., Han, X., and Donoho, D.~L.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, 2020.

\bibitem[Rangamani \& Banburski-Fahey(2022)Rangamani and
  Banburski-Fahey]{Rangamani22}
Rangamani, A. and Banburski-Fahey, A.
\newblock Neural collapse in deep homogeneous classifiers and the role of
  weight decay.
\newblock In \emph{ICASSP 2022 - 2022 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  4243--4247, 2022.
\newblock \doi{10.1109/ICASSP43922.2022.9746778}.

\bibitem[Ruder(2016)]{Ruder16}
Ruder, S.
\newblock An overview of gradient descent optimization algorithms, 2016.
\newblock URL \url{https://arxiv.org/abs/1609.04747}.

\bibitem[Safran \& Shamir(2018)Safran and Shamir]{Safran17}
Safran, I. and Shamir, O.
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock In \emph{International conference on machine learning}, pp.\
  4433--4441. PMLR, 2018.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{Saxe14}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6120}, 2013.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{Simonyan14}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1409.1556}.

\bibitem[Thrampoulidis et~al.(2022)Thrampoulidis, Kini, Vakilian, and
  Behnia]{Christos22}
Thrampoulidis, C., Kini, G.~R., Vakilian, V., and Behnia, T.
\newblock Imbalance trouble: Revisiting neural-collapse geometry.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27225--27238, 2022.

\bibitem[Tirer \& Bruna(2022)Tirer and Bruna]{Tirer22}
Tirer, T. and Bruna, J.
\newblock Extended unconstrained features model for exploring deep neural
  collapse.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  21478--21505. PMLR, 2022.

\bibitem[Xie et~al.(2023)Xie, Yang, Cai, and He]{Xie22}
Xie, L., Yang, Y., Cai, D., and He, X.
\newblock Neural collapse inspired attraction-repulsion-balanced loss for
  imbalanced learning.
\newblock \emph{Neurocomputing}, 2023.

\bibitem[Yang et~al.(2022)Yang, Chen, Li, Xie, Lin, and Tao]{Yang22}
Yang, Y., Chen, S., Li, X., Xie, L., Lin, Z., and Tao, D.
\newblock Inducing neural collapse in imbalanced learning: Do we really need a
  learnable classifier at the end of deep neural network?
\newblock In \emph{Neural Information Processing Systems}, 2022.

\bibitem[Yaras et~al.(2022)Yaras, Wang, Zhu, Balzano, and Qu]{yaras23}
Yaras, C., Wang, P., Zhu, Z., Balzano, L., and Qu, Q.
\newblock Neural collapse with normalized features: A geometric analysis over
  the riemannian manifold.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Zvh6lF5b26N}.

\bibitem[Yarotsky(2022)]{Yarotsky18}
Yarotsky, D.
\newblock Universal approximations of invariant maps by neural networks.
\newblock \emph{Constructive Approximation}, 55\penalty0 (1):\penalty0
  407--474, 2022.

\bibitem[Yun et~al.(2018)Yun, Sra, and Jadbabaie]{Yun17}
Yun, C., Sra, S., and Jadbabaie, A.
\newblock Global optimality conditions for deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=BJk7Gf-CZ}.

\bibitem[Yun et~al.(2019)Yun, Sra, and Jadbabaie]{Yun18}
Yun, C., Sra, S., and Jadbabaie, A.
\newblock Small nonlinearities in activation functions create bad local minima
  in neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rke_YiRct7}.

\bibitem[Zhou(2020)]{Zhou18}
Zhou, D.-X.
\newblock Universality of deep convolutional neural networks.
\newblock \emph{Applied and computational harmonic analysis}, 48\penalty0
  (2):\penalty0 787--794, 2020.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Li, Ding, You, Qu, and
  Zhu]{Zhou22a}
Zhou, J., Li, X., Ding, T., You, C., Qu, Q., and Zhu, Z.
\newblock On the optimization landscape of neural collapse under mse loss:
  Global optimality with unconstrained features.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  27179--27202. PMLR, 2022{\natexlab{a}}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, You, Li, Liu, Liu, Qu, and
  Zhu]{Zhou22b}
Zhou, J., You, C., Li, X., Liu, K., Liu, S., Qu, Q., and Zhu, Z.
\newblock Are all losses created equal: A neural collapse perspective.
\newblock \emph{arXiv preprint arXiv:2210.02192}, 2022{\natexlab{b}}.

\bibitem[Zhu et~al.(2020)Zhu, Soudry, Eldar, and Wakin]{Zhu18}
Zhu, Z., Soudry, D., Eldar, Y.~C., and Wakin, M.~B.
\newblock The global optimization geometry of shallow linear neural networks.
\newblock \emph{Journal of Mathematical Imaging and Vision}, 62:\penalty0
  279--292, 2020.

\bibitem[Zhu et~al.(2021)Zhu, Ding, Zhou, Li, You, Sulam, and Qu]{Zhu21}
Zhu, Z., Ding, T., Zhou, J., Li, X., You, C., Sulam, J., and Qu, Q.
\newblock A geometric analysis of neural collapse with unconstrained features.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 29820--29834, 2021.

\end{thebibliography}
