\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bai et~al.(2018)Bai, Kolter, and Koltun]{TCN1}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.
\newblock An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.
\newblock \emph{arXiv preprint arXiv:1803.01271}, 2018.

\bibitem[Bergsma et~al.(2023)Bergsma, Zeyl, and Guo]{sutranets}
Shane Bergsma, Tim Zeyl, and Lei Guo.
\newblock Sutranets: Sub-series autoregressive networks for long-sequence, probabilistic forecasting.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 30518--30533, 2023.

\bibitem[Cao et~al.(2023)Cao, Jia, Arik, Pfister, Zheng, Ye, and Liu]{tempo}
Defu Cao, Furong Jia, Sercan~O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu.
\newblock Tempo: Prompt-based generative pre-trained transformer for time series forecasting.
\newblock \emph{arXiv preprint arXiv:2310.04948}, 2023.

\bibitem[Challu et~al.(2023)Challu, Olivares, Oreshkin, Ramirez, Canseco, and Dubrawski]{Nhit}
Cristian Challu, Kin~G Olivares, Boris~N Oreshkin, Federico~Garza Ramirez, Max~Mergenthaler Canseco, and Artur Dubrawski.
\newblock Nhits: Neural hierarchical interpolation for time series forecasting.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 6989--6997, 2023.

\bibitem[Das et~al.(2023)Das, Kong, Leach, Mathur, Sen, and Yu]{TiDE}
Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, and Rose Yu.
\newblock Long-term forecasting with tide: Time-series dense encoder.
\newblock \emph{arXiv preprint arXiv:2304.08424}, 2023.

\bibitem[Deng et~al.(2024)Deng, Ye, Yin, Song, Tsang, and Xiong]{sscnn}
Jinliang Deng, Feiyang Ye, Du~Yin, Xuan Song, Ivor Wai-Hung Tsang, and Hui Xiong.
\newblock Parsimony or capability? decomposition delivers both in long-term time series forecasting.
\newblock 2024.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:267068391}.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{Vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Fan et~al.(2022)Fan, Zheng, Yi, Cao, Fu, Bian, and Liu]{depts}
Wei Fan, Shun Zheng, Xiaohan Yi, Wei Cao, Yanjie Fu, Jiang Bian, and Tie-Yan Liu.
\newblock Depts: Deep expansion learning for periodic time series forecasting.
\newblock \emph{arXiv preprint arXiv:2203.07681}, 2022.

\bibitem[Franceschi et~al.(2019)Franceschi, Dieuleveut, and Jaggi]{TCN2}
Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi.
\newblock Unsupervised scalable representation learning for multivariate time series.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/53c6de78244e9f528eb3e1cda69699bb-Paper.pdf}.

\bibitem[Gong et~al.(2023)Gong, Tang, and Liang]{patchmixer}
Zeying Gong, Yujin Tang, and Junwei Liang.
\newblock Patchmixer: A patch-mixing architecture for long-term time series forecasting.
\newblock \emph{arXiv preprint arXiv:2310.00655}, 2023.

\bibitem[Gruver et~al.(2024)Gruver, Finzi, Qiu, and Wilson]{LLMTime}
Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew~G Wilson.
\newblock Large language models are zero-shot time series forecasters.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Han et~al.(2024{\natexlab{a}})Han, Chen, Ye, and Zhan]{softs}
Lu~Han, Xu-Yang Chen, Han-Jia Ye, and De-Chuan Zhan.
\newblock Softs: Efficient multivariate time series forecasting with series-core fusion.
\newblock \emph{arXiv preprint arXiv:2404.14197}, 2024{\natexlab{a}}.

\bibitem[Han et~al.(2024{\natexlab{b}})Han, Ye, and Zhan]{CI}
Lu~Han, Han-Jia Ye, and De-Chuan Zhan.
\newblock The capacity and robustness trade-off: Revisiting the channel independent strategy for multivariate time series forecasting.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 2024{\natexlab{b}}.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and Girshick]{MAE}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 16000--16009, 2022.

\bibitem[Hou and Yu(2024)]{RWKV-TS}
Haowen Hou and F~Richard Yu.
\newblock Rwkv-ts: Beyond traditional recurrent neural network for time series tasks.
\newblock \emph{arXiv preprint arXiv:2401.09093}, 2024.

\bibitem[Huang et~al.(2024{\natexlab{a}})Huang, Shen, Zhang, Cheng, Ding, Zhou, and Wang]{hdmixer}
Qihe Huang, Lei Shen, Ruixin Zhang, Jiahuan Cheng, Shouhong Ding, Zhengyang Zhou, and Yang Wang.
\newblock Hdmixer: Hierarchical dependency with extendable patch for multivariate time series forecasting.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 12608--12616, 2024{\natexlab{a}}.

\bibitem[Huang et~al.(2024{\natexlab{b}})Huang, Shen, Zhang, Ding, Wang, Zhou, and Wang]{crossgnn}
Qihe Huang, Lei Shen, Ruixin Zhang, Shouhong Ding, Binwu Wang, Zhengyang Zhou, and Yang Wang.
\newblock Crossgnn: Confronting noisy multivariate time series via cross interaction refinement.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{b}}.

\bibitem[Huang et~al.(2024{\natexlab{c}})Huang, Zhou, Yang, Lin, Yi, and Wang]{lenet}
Qihe Huang, Zhengyang Zhou, Kuo Yang, Gengyu Lin, Zhongchao Yi, and Yang Wang.
\newblock Leret: Language-empowered retentive network for time series forecasting.
\newblock In \emph{Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, {IJCAI-24}}, 2024{\natexlab{c}}.

\bibitem[Jia et~al.(2024)Jia, Lin, Hao, Lin, Guo, and Wan]{witran}
Yuxin Jia, Youfang Lin, Xinyan Hao, Yan Lin, Shengnan Guo, and Huaiyu Wan.
\newblock Witran: Water-wave information transmission and recurrent acceleration network for long-range time series forecasting.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Jin et~al.(2023)Jin, Wang, Ma, Chu, Zhang, Shi, Chen, Liang, Li, Pan, et~al.]{Time-llm}
Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James~Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et~al.
\newblock Time-llm: Time series forecasting by reprogramming large language models.
\newblock \emph{arXiv preprint arXiv:2310.01728}, 2023.

\bibitem[Jin et~al.(2024)Jin, Zhang, Chen, Zhang, Liang, Yang, Wang, Pan, and Wen]{jin2024position}
Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jindong Wang, Shirui Pan, and Qingsong Wen.
\newblock Position: What can large language models tell us about time series analysis.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Kim et~al.(2021)Kim, Kim, Tae, Park, Choi, and Choo]{revin}
Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo.
\newblock Reversible instance normalization for accurate time-series forecasting against distribution shift.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Kingma and Ba(2014)]{adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lai et~al.(2018)Lai, Chang, Yang, and Liu]{LSTNet}
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu.
\newblock Modeling long-and short-term temporal patterns with deep neural networks.
\newblock In \emph{The 41st international ACM SIGIR conference on research \& development in information retrieval}, pages 95--104, 2018.

\bibitem[Li et~al.(2019)Li, Jin, Xuan, Zhou, Chen, Wang, and Yan]{LogTrans}
Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan.
\newblock Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Qi, Li, and Xu]{RLinear}
Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu.
\newblock Revisiting long-term time series forecasting: An investigation on linear mapping.
\newblock \emph{arXiv preprint arXiv:2305.10721}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Rao, Pan, Wang, and Xu]{Ti-mae}
Zhe Li, Zhongwen Rao, Lujia Pan, Pengyun Wang, and Zenglin Xu.
\newblock Ti-mae: Self-supervised masked time series autoencoders.
\newblock \emph{arXiv preprint arXiv:2301.08871}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, Rao, Pan, and Xu]{MTS-Mixers}
Zhe Li, Zhongwen Rao, Lujia Pan, and Zenglin Xu.
\newblock Mts-mixers: Multivariate time series forecasting via factorized temporal and channel mixing.
\newblock \emph{arXiv preprint arXiv:2302.04501}, 2023{\natexlab{c}}.

\bibitem[Lim et~al.(2021)Lim, Ar{\i}k, Loeff, and Pfister]{TFT}
Bryan Lim, Sercan~{\"O} Ar{\i}k, Nicolas Loeff, and Tomas Pfister.
\newblock Temporal fusion transformers for interpretable multi-horizon time series forecasting.
\newblock \emph{International Journal of Forecasting}, 37\penalty0 (4):\penalty0 1748--1764, 2021.

\bibitem[Lin et~al.(2023{\natexlab{a}})Lin, Lin, Wu, Wang, and Wang]{petformer}
Shengsheng Lin, Weiwei Lin, Wentai Wu, Songbo Wang, and Yongxiang Wang.
\newblock Petformer: Long-term time series forecasting via placeholder-enhanced transformer.
\newblock \emph{arXiv preprint arXiv:2308.04791}, 2023{\natexlab{a}}.

\bibitem[Lin et~al.(2023{\natexlab{b}})Lin, Lin, Wu, Zhao, Mo, and Zhang]{segrnn}
Shengsheng Lin, Weiwei Lin, Wentai Wu, Feiyu Zhao, Ruichao Mo, and Haotong Zhang.
\newblock Segrnn: Segment recurrent neural network for long-term time series forecasting.
\newblock \emph{arXiv preprint arXiv:2308.11200}, 2023{\natexlab{b}}.

\bibitem[Lin et~al.(2024)Lin, Lin, Wu, Chen, and Yang]{sparsetsf}
Shengsheng Lin, Weiwei Lin, Wentai Wu, Haojun Chen, and Junjie Yang.
\newblock Sparsetsf: Modeling long-term time series forecasting with 1k parameters.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Zhao, Wang, Kamarthi, and Prakash]{LSTPrompt}
Haoxin Liu, Zhiyuan Zhao, Jindong Wang, Harshavardhan Kamarthi, and B~Aditya Prakash.
\newblock Lstprompt: Large language models as zero-shot time series forecasters by long-short-term prompting.
\newblock \emph{arXiv preprint arXiv:2402.16132}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Zeng, Chen, Xu, Lai, Ma, and Xu]{scinet}
Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu.
\newblock Scinet: Time series modeling and forecasting with sample convolution and interaction.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 5816--5828, 2022{\natexlab{a}}.

\bibitem[Liu et~al.(2021)Liu, Yu, Liao, Li, Lin, Liu, and Dustdar]{Pyraformer}
Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex~X Liu, and Schahram Dustdar.
\newblock Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting.
\newblock In \emph{International conference on learning representations}, 2021.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Wu, Wang, and Long]{NSTransformers}
Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long.
\newblock Non-stationary transformers: Exploring the stationarity in time series forecasting.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 9881--9893, 2022{\natexlab{b}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Hu, Zhang, Wu, Wang, Ma, and Long]{liu2024itransformer}
Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long.
\newblock itransformer: Inverted transformers are effective for time series forecasting.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{b}}.

\bibitem[Luo and Wang(2024)]{moderntcn}
Donghao Luo and Xue Wang.
\newblock Moderntcn: A modern pure convolution structure for general time series analysis.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Madsen(2007)]{acf}
Henrik Madsen.
\newblock \emph{Time series analysis}.
\newblock CRC Press, 2007.

\bibitem[Nie et~al.(2023)Nie, H.~Nguyen, Sinthong, and Kalagnanam]{PatchTST}
Yuqi Nie, Nam H.~Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.
\newblock A time series is worth 64 words: Long-term forecasting with transformers.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Qiu et~al.(2024)Qiu, Hu, Zhou, Wu, Du, Zhang, Guo, Zhou, Jensen, Sheng, and Yang]{tfb}
Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang Zhang, Chenjuan Guo, Aoying Zhou, Christian~S. Jensen, Zhenli Sheng, and Bin Yang.
\newblock Tfb: Towards comprehensive and fair benchmarking of time series forecasting methods.
\newblock \emph{Proc. {VLDB} Endow.}, 17\penalty0 (9):\penalty0 2363--2377, 2024.

\bibitem[Salinas et~al.(2020)Salinas, Flunkert, Gasthaus, and Januschowski]{DeepAR}
David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski.
\newblock Deepar: Probabilistic forecasting with autoregressive recurrent networks.
\newblock \emph{International journal of forecasting}, 36\penalty0 (3):\penalty0 1181--1191, 2020.

\bibitem[Toner and Darlow(2024)]{linear-analysis}
William Toner and Luke~Nicholas Darlow.
\newblock An analysis of linear time series forecasting models.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Ulyanov et~al.(2016)Ulyanov, Vedaldi, and Lempitsky]{instance}
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
\newblock Instance normalization: The missing ingredient for fast stylization.
\newblock \emph{arXiv preprint arXiv:1607.08022}, 2016.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{Transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2022)Wang, Peng, Huang, Wang, Chen, and Xiao]{micn}
Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao.
\newblock Micn: Multi-scale local and global context modeling for long-term series forecasting.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Wang et~al.(2024)Wang, Wu, Shi, Hu, Luo, Ma, Zhang, and ZHOU]{timemixer}
Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James~Y Zhang, and JUN ZHOU.
\newblock Timemixer: Decomposable multiscale mixing for time series forecasting.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Wen et~al.(2022)Wen, Zhou, Zhang, Chen, Ma, Yan, and Sun]{TransformerforTS}
Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun.
\newblock Transformers in time series: A survey.
\newblock \emph{arXiv preprint arXiv:2202.07125}, 2022.

\bibitem[Woo et~al.(2022)Woo, Liu, Sahoo, Kumar, and Hoi]{ETSformer}
Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi.
\newblock Etsformer: Exponential smoothing transformers for time-series forecasting.
\newblock \emph{arXiv preprint arXiv:2202.01381}, 2022.

\bibitem[Wu et~al.(2021)Wu, Xu, Wang, and Long]{Autoformer}
Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long.
\newblock Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 22419--22430, 2021.

\bibitem[Wu et~al.(2023)Wu, Hu, Liu, Zhou, Wang, and Long]{timesnet}
Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long.
\newblock Timesnet: Temporal 2d-variation modeling for general time series analysis.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Xu et~al.(2024)Xu, Zeng, and Xu]{fits}
Zhijian Xu, Ailing Zeng, and Qiang Xu.
\newblock Fits: Modeling time series with $10 k $ parameters.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Xue and Salim(2023)]{promptcast}
Hao Xue and Flora~D Salim.
\newblock Promptcast: A new prompt-based learning paradigm for time series forecasting.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 2023.

\bibitem[Yu et~al.(2024)Yu, Zou, Hu, Aviles-Rivero, Qin, and Wang]{LD}
Guoqi Yu, Jing Zou, Xiaowei Hu, Angelica~I Aviles-Rivero, Jing Qin, and Shujun Wang.
\newblock Revitalizing multivariate time series forecasting: Learnable decomposition with inter-series dependencies and intra-series variations modeling.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Zeng et~al.(2023)Zeng, Chen, Zhang, and Xu]{DLinear}
Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu.
\newblock Are transformers effective for time series forecasting?
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~37, pages 11121--11128, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Cao, Bian, Yi, Zheng, and Li]{LightTS}
Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, and Jian Li.
\newblock Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures.
\newblock \emph{arXiv preprint arXiv:2207.01186}, 2022.

\bibitem[Zhang and Yan(2023)]{Crossformer}
Yunhao Zhang and Junchi Yan.
\newblock Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Peng, Zhang, Li, Xiong, and Zhang]{Informer}
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
\newblock Informer: Beyond efficient transformer for long sequence time-series forecasting.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~35, pages 11106--11115, 2021.

\bibitem[Zhou et~al.(2022)Zhou, Ma, Wen, Wang, Sun, and Jin]{Fedformer}
Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.
\newblock Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting.
\newblock In \emph{International conference on machine learning}, pages 27268--27286. PMLR, 2022.

\bibitem[Zhou et~al.(2024)Zhou, Niu, Sun, Jin, et~al.]{OFA}
Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et~al.
\newblock One fits all: Power general time series analysis by pretrained lm.
\newblock \emph{Advances in neural information processing systems}, 36, 2024.

\end{thebibliography}
