\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Al-Rfou et~al.(2018)Al-Rfou, Choe, Constant, Guo, and Jones]{al_rfou}
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones.
\newblock {Character-Level} language modeling with deeper {Self-Attention}.
\newblock \emph{arXiv preprint arXiv:1808.04444}, August 2018.

\bibitem[Bengio et~al.(2013)Bengio, Yao, Alain, and Vincent]{Bengio_2013}
Yoshua Bengio, Li~Yao, Guillaume Alain, and Pascal Vincent.
\newblock Generalized denoising {Auto-Encoders} as generative models.
\newblock \emph{arXiv preprint arXiv:1305.6663}, May 2013.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brock et~al.(2019)Brock, Donahue, and Simonyan]{brock2018large}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale {GAN} training for high fidelity natural image synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Chan et~al.(2020)Chan, Saharia, Hinton, Norouzi, and
  Jaitly]{chan2020imputer}
William Chan, Chitwan Saharia, Geoffrey Hinton, Mohammad Norouzi, and Navdeep
  Jaitly.
\newblock Imputer: Sequence modelling via imputation and dynamic programming.
\newblock In \emph{International Conference on Machine Learning}, pages
  1403--1413. PMLR, 2020.

\bibitem[Chelba et~al.(2013)Chelba, Mikolov, Schuster, Ge, Brants, Koehn, and
  Robinson]{lm1b}
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi~Ge, Thorsten Brants, Phillipp
  Koehn, and Tony Robinson.
\newblock One billion word benchmark for measuring progress in statistical
  language modeling.
\newblock \emph{arXiv preprint arXiv:1312.3005}, December 2013.

\bibitem[Chen et~al.(2020)Chen, Zhang, Zen, Weiss, Norouzi, and Chan]{wavegrad}
Nanxin Chen, Yu~Zhang, Heiga Zen, Ron~J Weiss, Mohammad Norouzi, and William
  Chan.
\newblock {WaveGrad}: Estimating gradients for waveform generation.
\newblock \emph{arXiv preprint arXiv:2009.00713}, September 2020.

\bibitem[Chen et~al.(2018)Chen, Mishra, Rohaninejad, and
  Abbeel]{chen2018pixelsnail}
Xi~Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel.
\newblock Pixel{SNAIL}: An improved autoregressive generative model.
\newblock In \emph{International Conference on Machine Learning}, pages
  863--871, 2018.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{transformer_xl}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V Le, and Ruslan
  Salakhutdinov.
\newblock {Transformer-XL}: Attentive language models beyond a {Fixed-Length}
  context.
\newblock \emph{arXiv preprint arXiv:1901.02860}, January 2019.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, October 2018.

\bibitem[Dinh et~al.(2016)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio.
\newblock Density estimation using {R}eal {NVP}.
\newblock \emph{arXiv preprint arXiv:1605.08803}, 2016.

\bibitem[Feller(1949)]{feller1949theory}
W~Feller.
\newblock On the theory of stochastic processes, with particular reference to
  applications.
\newblock In \emph{Proceedings of the [First] Berkeley Symposium on
  Mathematical Statistics and Probability}. The Regents of the University of
  California, 1949.

\bibitem[Ghazvininejad et~al.(2019)Ghazvininejad, Levy, Liu, and
  Zettlemoyer]{mask_predict}
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer.
\newblock {Mask-Predict}: Parallel decoding of conditional masked language
  models.
\newblock \emph{arXiv preprint arXiv:1904.09324}, April 2019.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2672--2680, 2014.

\bibitem[Gu et~al.(2019)Gu, Wang, and Zhao]{levenstein}
Jiatao Gu, Changhan Wang, and Jake Zhao.
\newblock Levenshtein transformer.
\newblock \emph{arXiv preprint arXiv:1905.11006}, May 2019.

\bibitem[Heek et~al.(2020)Heek, Levskaya, Oliver, Ritter, Rondepierre, Steiner,
  and van {Z}ee]{flax2020github}
Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand
  Rondepierre, Andreas Steiner, and Marc van {Z}ee.
\newblock {F}lax: A neural network library and ecosystem for {JAX}, 2020.
\newblock URL \url{http://github.com/google/flax}.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock {GANs} trained by a two time-scale update rule converge to a local
  {Nash} equilibrium.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6626--6637, 2017.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6840--6851, 2020.

\bibitem[Hoogeboom et~al.(2021)Hoogeboom, Nielsen, Jaini, Forr{\'e}, and
  Welling]{hoogeboom2021argmax}
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr{\'e}, and Max
  Welling.
\newblock Argmax flows and multinomial diffusion: Towards non-autoregressive
  language models.
\newblock \emph{arXiv preprint arXiv:2102.05379}, 2021.

\bibitem[Hyv{\"a}rinen et~al.(2004)Hyv{\"a}rinen, Karhunen, and
  Oja]{hyvarinen2004independent}
Aapo Hyv{\"a}rinen, Juha Karhunen, and Erkki Oja.
\newblock \emph{Independent component analysis}, volume~46.
\newblock John Wiley \& Sons, 2004.

\bibitem[Karras et~al.(2020)Karras, Aittala, Hellsten, Laine, Lehtinen, and
  Aila]{karras2020training}
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and
  Timo Aila.
\newblock Training generative adversarial networks with limited data.
\newblock \emph{arXiv preprint arXiv:2006.06676v1}, 2020.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kingma and Dhariwal(2018)]{kingma2018glow}
Diederik~P Kingma and Prafulla Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10215--10224, 2018.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational {B}ayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kong et~al.(2020)Kong, Ping, Huang, Zhao, and
  Catanzaro]{kong2020diffwave}
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro.
\newblock Diffwave: A versatile diffusion model for audio synthesis.
\newblock \emph{arXiv preprint arXiv:2009.09761}, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Mahoney(2011)]{text8}
Matt Mahoney.
\newblock Text8 dataset.
\newblock \url{http://mattmahoney.net/dc/textdata}, 2011.
\newblock Accessed: 2021-5-24.

\bibitem[Mittal et~al.(2021)Mittal, Engel, Hawthorne, and Simon]{music}
Gautam Mittal, Jesse Engel, Curtis Hawthorne, and Ian Simon.
\newblock Symbolic music generation with diffusion models.
\newblock \emph{arXiv preprint arXiv:2103.16091}, March 2021.

\bibitem[Nichol and Dhariwal(2021)]{nichol2021improved}
Alex Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock \emph{arXiv preprint arXiv:2102.09672}, 2021.

\bibitem[Niu et~al.(2020)Niu, Song, Song, Zhao, Grover, and Ermon]{permutation}
Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano
  Ermon.
\newblock Permutation invariant graph generation via score-based generative
  modeling.
\newblock \emph{arXiv preprint arXiv:2003.00638}, March 2020.

\bibitem[Papamakarios et~al.(2019)Papamakarios, Nalisnick, Rezende, Mohamed,
  and Lakshminarayanan]{papamakarios2019normalizing}
George Papamakarios, Eric Nalisnick, Danilo~Jimenez Rezende, Shakir Mohamed,
  and Balaji Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock \emph{arXiv preprint arXiv:1912.02762}, 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{T5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2020.

\bibitem[Rezende and Mohamed(2015)]{rezende2015variational}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In \emph{International Conference on Machine Learning}, pages
  1530--1538, 2015.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{International Conference on Machine Learning}, pages
  1278--1286, 2014.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015unet}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock {U-Net}: Convolutional networks for biomedical image segmentation.
\newblock In \emph{International Conference on Medical Image Computing and
  Computer-Assisted Intervention}, pages 234--241. Springer, 2015.

\bibitem[Ruis et~al.(2020)Ruis, Stern, Proskurnia, and Chan]{ruis2020insertion}
Laura Ruis, Mitchell Stern, Julia Proskurnia, and William Chan.
\newblock Insertion-deletion transformer.
\newblock \emph{arXiv preprint arXiv:2001.05540}, 2020.

\bibitem[Saharia et~al.(2020)Saharia, Chan, Saxena, and
  Norouzi]{saharia2020latentalignments}
Chitwan Saharia, William Chan, Saurabh Saxena, and Mohammad Norouzi.
\newblock Non-autoregressive machine translation with latent alignments.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 1098--1108, 2020.

\bibitem[Salimans and Kingma(2016)]{salimans2016weight}
Tim Salimans and Durk~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  901--909, 2016.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training gans.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2234--2242, 2016.

\bibitem[Salimans et~al.(2017)Salimans, Karpathy, Chen, and
  Kingma]{salimans2017pixelcnn++}
Tim Salimans, Andrej Karpathy, Xi~Chen, and Diederik~P Kingma.
\newblock Pixel{CNN}++: Improving the {PixelCNN} with discretized logistic
  mixture likelihood and other modifications.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Seff et~al.(2019)Seff, Zhou, Damani, Doyle, and Adams]{princeton_chem}
Ari Seff, Wenda Zhou, Farhan Damani, Abigail Doyle, and Ryan~P Adams.
\newblock Discrete object generation with reversible inductive construction.
\newblock \emph{arXiv preprint arXiv:1907.08268}, July 2019.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{sohl2015deep}
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{International Conference on Machine Learning}, pages
  2256--2265, 2015.

\bibitem[Song et~al.(2021)Song, Meng, and Ermon]{song2021implicit}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Song and Ermon(2019)]{song_2019}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11895--11907, 2019.

\bibitem[Song and Ermon(2020)]{song2020improved}
Yang Song and Stefano Ermon.
\newblock Improved techniques for training score-based generative models.
\newblock \emph{arXiv preprint arXiv:2006.09011}, 2020.

\bibitem[Song et~al.(2020)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and
  Poole]{song2020_sde}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock \emph{arXiv preprint arXiv:2011.13456}, November 2020.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{Szegedy_2016_CVPR}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2016.

\bibitem[Tran et~al.(2019)Tran, Vafa, Agrawal, Dinh, and
  Poole]{tran2019discrete}
Dustin Tran, Keyon Vafa, Kumar Agrawal, Laurent Dinh, and Ben Poole.
\newblock Discrete flows: Invertible generative models of discrete data.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[van~den Oord et~al.(2016{\natexlab{a}})van~den Oord, Dieleman, Zen,
  Simonyan, Vinyals, Graves, Kalchbrenner, Senior, and
  Kavukcuoglu]{oord2016wavenet}
Aaron van~den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals,
  Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.
\newblock {WaveNet}: A generative model for raw audio.
\newblock \emph{arXiv preprint arXiv:1609.03499}, 2016{\natexlab{a}}.

\bibitem[van~den Oord et~al.(2016{\natexlab{b}})van~den Oord, Kalchbrenner, and
  Kavukcuoglu]{oord2016pixel}
Aaron van~den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.
\newblock Pixel recurrent neural networks.
\newblock \emph{International Conference on Machine Learning},
  2016{\natexlab{b}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5998--6008, 2017.

\bibitem[Vincent(2011)]{vincent2011connection}
Pascal Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural Computation}, 23\penalty0 (7):\penalty0 1661--1674,
  2011.

\bibitem[Wang and Cho(2019)]{bert_speak}
Alex Wang and Kyunghyun Cho.
\newblock {BERT} has a mouth, and it must speak: {BERT} as a markov random
  field language model.
\newblock \emph{arXiv preprint arXiv:1902.04094}, February 2019.

\bibitem[Wu and He(2018)]{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 3--19, 2018.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Ziegler and Rush(2019)]{Ziegler2019}
Zachary~M Ziegler and Alexander~M Rush.
\newblock Latent normalizing flows for discrete sequences.
\newblock \emph{arXiv preprint arXiv:1901.10548}, January 2019.

\end{thebibliography}
