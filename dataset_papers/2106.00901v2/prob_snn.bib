@article{Wozniak2020,
abstract = {Spiking neural networks (SNNs) incorporating biologically plausible neurons hold great promise because of their unique temporal dynamics and energy efficiency. However, SNNs have developed separately from artificial neural networks (ANNs), limiting the impact of deep learning advances for SNNs. Here, we present an alternative perspective of the spiking neuron that incorporates its neural dynamics into a recurrent ANN unit called a spiking neural unit (SNU). SNUs may operate as SNNs, using a step function activation, or as ANNs, using continuous activations. We demonstrate the advantages of SNU dynamics through simulations on multiple tasks and obtain accuracies comparable to, or better than, those of ANNs. The SNU concept enables an efficient implementation with in-memory acceleration for both training and inference. We experimentally demonstrate its efficacy for a music-prediction task in an in-memory-based SNN accelerator prototype using 52,800 phase-change memory devices. Our results open up an avenue for broad adoption of biologically inspired neural dynamics in challenging applications and acceleration with neuromorphic hardware.},
author = {Wo{\'{z}}niak, Stanis{\l}aw and Pantazi, Angeliki and Bohnstingl, Thomas and Eleftheriou, Evangelos},
doi = {10.1038/s42256-020-0187-0},
issn = {2522-5839},
journal = {Nature Machine Intelligence},
number = {6},
pages = {325--336},
title = {{Deep learning incorporating biologically inspired neural dynamics and in-memory computing}},
volume = {2},
year = {2020}
}
@inproceedings{NEURIPS2018_82f2b308,
author = {Shrestha, Sumit Bam and Orchard, Garrick},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Bengio, S and Wallach, H and Larochelle, H and Grauman, K and Cesa-Bianchi, N and Garnett, R},
publisher = {Curran Associates, Inc.},
title = {{SLAYER: Spike Layer Error Reassignment in Time}},
volume = {31},
year = {2018}
}
@article{8891809,
author = {Neftci, Emre O and Mostafa, Hesham and Zenke, Friedemann},
doi = {10.1109/MSP.2019.2931595},
journal = {IEEE Signal Processing Magazine},
number = {6},
pages = {51--63},
title = {{Surrogate Gradient Learning in Spiking Neural Networks: Bringing the Power of Gradient-Based Optimization to Spiking Neural Networks}},
volume = {36},
year = {2019}
}
@inproceedings{NEURIPS2020_00ac8ed3,
author = {Shchur, Oleksandr and Gao, Nicholas and Bilo{\v{s}}, Marin and G{\"{u}}nnemann, Stephan},
booktitle = {Advances in Neural Information Processing Systems},
editor = {Larochelle, H and Ranzato, M and Hadsell, R and Balcan, M F and Lin, H},
pages = {73--84},
publisher = {Curran Associates, Inc.},
title = {{Fast and Flexible Temporal Point Processes with Triangular Maps}},
volume = {33},
year = {2020}
}
@article{Bothe2004,
abstract = {This paper surveys recent findings in neuroscience regarding the behavioral relevancy of the precise timing with which real spiking neurons emit spikes. The literature suggests that in almost any system where the processing-speed of a neural (sub)-system is required to be high, the timing of single spikes can be very precise and reliable. Additionally, new, more refined methods are finding precisely timed spikes where previously none where found. This line of evidence thus provides additional motivation for researching the computational properties of networks of artificial spiking neurons that compute with more precisely timed spikes. },
author = {Bothe, S M},
file = {::},
journal = {Natural Computing},
keywords = {neural coding,precise spike timing,spiking neural networks,synchrony coding},
pages = {195--206},
title = {{The evidence for neural information processing with precise spike-times: A survey}},
volume = {2},
year = {2004}
}
@article{Moiseff1981,
abstract = {We demonstrated that ongoing time disparity (OTD) was a sufficient cue for the azimuthal component of receptive fields of auditory neurons in the owl (Tyto alba) midbrain and that OTDs were sufficient to mediate meaningful behavioral responses. We devised a technique which enabled us to change easily between free field and dichotic stimuli while recording from single auditory neurons in the owl mesencephalicus lateralis pars dorsalis (MLD). MLD neurons with restricted spatial receptive fields ('space-mapped neurons') showed marked sensitivity to specific ongoing time disparities. The magnitudes of these disparities were in the behaviorally significant range of tens of microseconds. The ongoing time disparities were correlated significantly with the azimuthal center of receptive fields. Space-mapped neurons were insensitive to transient disparities. MLD neurons which were not space-mapped, i.e., were omnidirectional, did not show any sensitivity to specific OTDs. We confirmed the behavioral relevance of OTD as a cue for localizing a sound in azimuth by presenting OTD differences to tame owls. Using head turning as an assay, we showed that OTD was a sufficient cue for the azimuth of a sound. The relationship between azimuth and OTD obtained from our neurophysiological experiments matched closely the relationship obtained from our behavioral experiments.},
author = {Moiseff, A. and Konishi, M.},
doi = {10.1523/jneurosci.01-01-00040.1981},
file = {::},
issn = {02706474},
journal = {Journal of Neuroscience},
number = {1},
pages = {40--48},
pmid = {7346557},
title = {{Neuronal and behavioral sensitivity to binaural time differences in the owl}},
volume = {1},
year = {1981}
}
@article{Williams1989,
abstract = {The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length.},
author = {Williams, Ronald J. and Zipser, David},
doi = {10.1162/neco.1989.1.2.270},
file = {::},
issn = {0899-7667},
journal = {Neural Computation},
number = {2},
pages = {270--280},
title = {{A Learning Algorithm for Continually Running Fully Recurrent Neural Networks}},
volume = {1},
year = {1989}
}
@techreport{Machler2012,
author = {M{\"{a}}chler, Martin},
file = {::},
title = {{Accurately computing $\log(1-\exp(-|a|))$ assessed by the Rmpfr package}},
year = {2012}
}
@inproceedings{Shchur2020,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6830v3},
author = {Shchur, Oleksandr and Bilo{\v{s}}, Marin and G{\"{u}}nnemann, Stephan},
booktitle = {International Conference on Learning Representations},
eprint = {arXiv:1412.6830v3},
file = {::},
title = {{Intensity-free learning of temporal point processes}},
year = {2020}
}
@article{10.3389/fncom.2014.00038,
abstract = {The ability to learn and perform statistical inference with biologically plausible recurrent networks of spiking neurons is an important step toward understanding perception and reasoning. Here we derive and investigate a new learning rule for recurrent spiking networks with hidden neurons, combining principles from variational learning and reinforcement learning. Our network defines a generative model over spike train histories and the derived learning rule has the form of a local Spike Timing Dependent Plasticity rule modulated by global factors (neuromodulators) conveying information about “novelty” on a statistically rigorous ground. Simulations show that our model is able to learn both stationary and non-stationary patterns of spike trains. We also propose one experiment that could potentially be performed with animals in order to test the dynamics of the predicted novelty signal.},
author = {{Jimenez Rezende}, Danilo and Gerstner, Wulfram},
doi = {10.3389/fncom.2014.00038},
issn = {1662-5188},
journal = {Frontiers in Computational Neuroscience},
pages = {38},
title = {{Stochastic variational learning in recurrent spiking networks}},
volume = {8},
year = {2014}
}
@incollection{NIPS2019_8485,
author = {Omi, Takahiro and naonori Ueda and Aihara, Kazuyuki},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {Wallach, H and Larochelle, H and Beygelzimer, A and d\textquotesingle Alch{\'{e}}-Buc, F and Fox, E and Garnett, R},
pages = {2122--2132},
publisher = {Curran Associates, Inc.},
title = {{Fully Neural Network based Model for General Temporal Point Processes}},
year = {2019}
}
@inproceedings{maddison2017,
author = {Maddison, Chris J and Mnih, Andriy and Teh, Yee Whye},
booktitle = {Proceedings of the Fifth International Conference on Learning Representations},
title = {{The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}},
year = {2017}
}
@article{rasmussen2018,
author = {Rasmussen, Jakob Gulddahl},
journal = {arXiv preprint arXiv:1806.00221},
title = {{Lecture notes: Temporal point processes and the conditional intensity function}},
year = {2018}
}
@article{doi:10.1002/nav.3800260304,
abstract = {Abstract A simple and relatively efficient method for simulating one-dimensional and two-dimensional nonhomogeneous Poisson processes is presented The method is applicable for any rate function and is based on controlled deletion of points in a Poisson process whose rate function dominates the given rate function In its simplest implementation, the method obviates the need for numerical integration of the rate function, for ordering of points, and for generation of Poisson variates.},
author = {Lewis, P A W and Shedler, G S},
doi = {10.1002/nav.3800260304},
journal = {Naval Research Logistics Quarterly},
number = {3},
pages = {403--413},
title = {{Simulation of nonhomogeneous poisson processes by thinning}},
volume = {26},
year = {1979}
}
@article{salimans2013,
author = {Salimans, Tim and Knowles, David A},
journal = {Bayesian Analysis},
number = {4},
pages = {837--882},
title = {{Fixed-form variational posterior approximation through stochastic linear regression}},
volume = {8},
year = {2013}
}
@article{ogata1981,
abstract = {A simple and efficient method of simulation is discussed for point processes that are specified by their conditional intensities. The method is based on the thinning algorithm which was introduced recently by Lewis and Shedler for the simulation of nonhomogeneous Poisson processes. Algorithms are given for past dependent point processes containing multivariate processes. The simulations are performed for some parametric conditional intensity functions, and the accuracy of the simulated data is demonstrated by the likelihood ratio test and the minimum Akaike information criterion (AIC) procedure.},
author = {Ogata, Y},
doi = {10.1109/TIT.1981.1056305},
issn = {1557-9654},
journal = {IEEE Transactions on Information Theory},
keywords = {Point processes},
month = {jan},
number = {1},
pages = {23--31},
title = {{On Lewis' simulation method for point processes}},
volume = {27},
year = {1981}
}
@book{daley2003,
author = {Daley, D J and Vere-Jones, D},
publisher = {Springer-Verlag New York},
title = {{An Introduction to the Theory of Point Processes: Volume I: Elementary Theory and Methods}},
year = {2003}
}
@phdthesis{State2019,
author = {State, Laura},
doi = {10.1007/978-3-030-30487-4_54},
file = {::},
isbn = {9783030304867},
issn = {16113349},
school = {Universit{\"{a}}t T{\"{u}}bingen},
title = {{Training Delays in Spiking Neural Networks}},
year = {2019}
}
@inproceedings{jang2017,
author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
booktitle = {Proceedings of the Fifth International Conference on Learning Representations},
title = {{Categorical Reparameterization with Gumbel-Softmax}},
year = {2017}
}
@article{Wright2012,
abstract = {Transmission delays are an inherent component of spiking neural networks (SNNs) but relatively little is known about how delays are adapted in biological systems and studies on computational learning mechanisms have focused on spike-timing-dependent plasticity (STDP) which adjusts synaptic weights rather than synaptic delays. We propose a novel algorithm for learning temporal delays in SNNs with Gaussian synapses, which we call spike-delay-variance learning (SDVL). A key feature of the algorithm is adaptation of the shape (mean and variance) of the postsynaptic release profiles only, rather than the conventional STDP approach of adapting the network's synaptic weights. The algorithm's ability to learn temporal input sequences was tested in three studies using supervised and unsupervised learning within feed-forward networks. SDVL was able to successfully classify forty spatiotemporal patterns without supervision by providing robust, effective adaption of the postsynaptic release profiles. The results demonstrate how delay learning can contribute to the stability of spiking sequences, and that there is a potential role for adaption of variance as well as mean values in learning algorithms for spiking neural networks. {\textcopyright} 2012 IEEE.},
author = {Wright, Paul W. and Wiles, Janet},
doi = {10.1109/IJCNN.2012.6252371},
file = {::},
isbn = {9781467314909},
journal = {Proceedings of the International Joint Conference on Neural Networks},
keywords = {STDP,delay learning,sequence learning,spike-delay-variance learning,spiking neural networks,transmission delays},
pages = {10--15},
publisher = {IEEE},
title = {{Learning transmission delays in spiking neural networks: A novel approach to sequence learning based on spike delay variance}},
year = {2012}
}
@article{Kim2019,
abstract = {Cortical microcircuits exhibit complex recurrent architectures that possess dynamically rich properties. The neurons that make up these microcircuits communicate mainly via discrete spikes, and it is not clear how spikes give rise to dynamics that can be used to perform computationally challenging tasks. In contrast, continuous models of rate-coding neurons can be trained to perform complex tasks. Here, we present a simple framework to construct biologically realistic spiking recurrent neural networks (RNNs) capable of learning a wide range of tasks. Our framework involves training a continuous-variable rate RNN with important biophysical constraints and transferring the learned dynamics and constraints to a spiking RNN in a one-to-one manner. The proposed framework introduces only 1 additional parameter to establish the equivalence between rate and spiking RNN models. We also study other model parameters related to the rate and spiking networks to optimize the one-to-one mapping. By establishing a close relationship between rate and spiking models, we demonstrate that spiking RNNs could be constructed to achieve similar performance as their counterpart continuous rate networks.},
author = {Kim, Robert and Li, Yinghao and Sejnowski, Terrence J.},
doi = {10.1073/pnas.1905926116},
file = {::},
issn = {10916490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Rate neural networks,Recurrent neural networks,Spiking neural networks},
number = {45},
pages = {22811--22820},
pmid = {31636215},
title = {{Simple framework for constructing functional spiking recurrent neural networks}},
volume = {116},
year = {2019}
}
@inproceedings{kingma2014,
author = {Kingma, Diederik P and Welling, Max},
booktitle = {Proceedings of the Second International Conference on Learning Representations},
title = {{Auto-encoding variational Bayes}},
year = {2014}
}
@misc{mohamed2019monte,
archivePrefix = {arXiv},
arxivId = {stat.ML/1906.10652},
author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
eprint = {1906.10652},
primaryClass = {stat.ML},
title = {{Monte Carlo Gradient Estimation in Machine Learning}},
year = {2019}
}
@incollection{daley2003chapter7,
author = {Daley, D J and Vere-Jones, D},
chapter = {7},
publisher = {Springer-Verlag New York},
title = {{An Introduction to the Theory of Point Processes: Volume I: Elementary Theory and Methods}},
year = {2003}
}
@book{gerstner2014,
author = {Gerstner, Wulfram and Kistler, Werner M and Naud, Richard and Paninski, Liam},
publisher = {Cambridge University Press},
title = {{Neuronal Dynamics}},
year = {2014}
}
@inproceedings{Bohte2000,
author = {Bohte, Sander M. and Kok, Joost N. and Poutr{\'{e}}, Han La},
booktitle = {Proceedings of the 8th European Symposium on Artificial Neural Networks (ESANN 2000)},
pages = {419--424},
title = {{SpikeProp: Backpropagation for Networks of Spiking Neurons}},
year = {2000}
}
@incollection{NIPS2004_2643,
author = {Rao, Rajesh P N},
booktitle = {Advances in Neural Information Processing Systems 17},
editor = {Saul, L K and Weiss, Y and Bottou, L},
pages = {1113--1120},
publisher = {MIT Press},
title = {{Hierarchical Bayesian Inference in Networks of Spiking Neurons}},
year = {2005}
}
@incollection{NIPS2011_4387,
author = {Rezende, Danilo J and Wierstra, Daan and Gerstner, Wulfram},
booktitle = {Advances in Neural Information Processing Systems 24},
editor = {Shawe-Taylor, J and Zemel, R S and Bartlett, P L and Pereira, F and Weinberger, K Q},
pages = {136--144},
publisher = {Curran Associates, Inc.},
title = {{Variational Learning for Recurrent Spiking Networks}},
year = {2011}
}
@incollection{NIPS2014_5273,
author = {Huang, Yanping and Rao, Rajesh P N},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N D and Weinberger, K Q},
pages = {1943--1951},
publisher = {Curran Associates, Inc.},
title = {{Neurons as Monte Carlo Samplers: Bayesian Inference and Learning in Spiking Networks}},
year = {2014}
}
@article{doi:10.1162/neco.2006.18.6.1318,
abstract = { In timing-based neural codes, neurons have to emit action potentials at precise moments in time. We use a supervised learning paradigm to derive a synaptic update rule that optimizes by gradient ascent the likelihood of postsynaptic firing at one or several desired firing times. We find that the optimal strategy of up- and downregulating synaptic efficacies depends on the relative timing between presynaptic spike arrival and desired postsynaptic firing. If the presynaptic spike arrives before the desired postsynaptic spike timing, our optimal learning rule predicts that the synapse should become potentiated. The dependence of the potentiation on spike timing directly reflects the time course of an excitatory postsynaptic potential. However, our approach gives no unique reason for synaptic depression under reversed spike timing. In fact, the presence and amplitude of depression of synaptic efficacies for reversed spike timing depend on how constraints are implemented in the optimization problem. Two different constraints, control of postsynaptic rates and control of temporal locality, are studied. The relation of our results to spike-timing-dependent plasticity and reinforcement learning is discussed. },
author = {Pfister, Jean-Pascal and Toyoizumi, Taro and Barber, David and Gerstner, Wulfram},
doi = {10.1162/neco.2006.18.6.1318},
journal = {Neural Computation},
number = {6},
pages = {1318--1348},
title = {{Optimal Spike-Timing-Dependent Plasticity for Precise Action Potential Firing in Supervised Learning}},
volume = {18},
year = {2006}
}
@article{Williams1992,
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
author = {Williams, Ronald J},
doi = {10.1007/BF00992696},
file = {::},
issn = {1573-0565},
journal = {Machine Learning},
number = {3},
pages = {229--256},
title = {{Simple statistical gradient-following algorithms for connectionist reinforcement learning}},
volume = {8},
year = {1992}
}
@article{doi:10.1162/neco.2008.20.1.91,
abstract = { We show that the dynamics of spiking neurons can be interpreted as a form of Bayesian inference in time. Neurons that optimally integrate evidence about events in the external world exhibit properties similar to leaky integrate-and-fire neurons with spike-dependent adaptation and maximally respond to fluctuations of their input. Spikes signal the occurrence of new information—what cannot be predicted from the past activity. As a result, firing statistics are close to Poisson, albeit providing a deterministic representation of probabilities. },
author = {Deneve, Sophie},
doi = {10.1162/neco.2008.20.1.91},
journal = {Neural Computation},
number = {1},
pages = {91--117},
title = {{Bayesian Spiking Neurons I: Inference}},
volume = {20},
year = {2008}
}
@incollection{NIPS2018_7417,
author = {Huh, Dongsung and Sejnowski, Terrence J},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {Bengio, S and Wallach, H and Larochelle, H and Grauman, K and Cesa-Bianchi, N and Garnett, R},
pages = {1433--1443},
publisher = {Curran Associates, Inc.},
title = {{Gradient Descent for Spiking Neural Networks}},
year = {2018}
}
@article{doi:10.1162/neco.2008.20.1.118,
abstract = { In the companion letter in this issue (“Bayesian Spiking Neurons I: Inference”), we showed that the dynamics of spiking neurons can be interpreted as a form of Bayesian integration, accumulating evidence over time about events in the external world or the body. We proceed to develop a theory of Bayesian learning in spiking neural networks, where the neurons learn to recognize temporal dynamics of their synaptic inputs. Meanwhile, successive layers of neurons learn hierarchical causal models for the sensory input. The corresponding learning rule is local, spike-time dependent, and highly nonlinear. This approach provides a principled description of spiking and plasticity rules maximizing information transfer, while limiting the number of costly spikes, between successive layers of neurons. },
author = {Deneve, Sophie},
doi = {10.1162/neco.2008.20.1.118},
journal = {Neural Computation},
number = {1},
pages = {118--145},
title = {{Bayesian Spiking Neurons II: Learning}},
volume = {20},
year = {2008}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
eprint = {arXiv:1103.4296v1},
file = {::},
isbn = {9780982252925},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
title = {{Adaptive subgradient methods for online learning and stochastic optimization}},
volume = {12},
year = {2011}
}
@online{DIffSNNGithub,
author = {Hiroshi Kajino},
title = {diffsnn},
year = 2021,
url = {https://github.com/ibm-research-tokyo/diffsnn},
urldate = {2021-05-30}
}
