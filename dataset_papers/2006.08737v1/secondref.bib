@article{devlin2018bert,
  title={{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@inproceedings{giant,
  title={GIANT: Globally improved approximate Newton method for distributed optimization},
  author={Wang, Shusen and Roosta, Fred and Xu, Peng and Mahoney, Michael W},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2332--2342},
  year={2018}
}
@inproceedings{dingo,
  title={DINGO: Distributed Newton-Type Method for Gradient-Norm Optimization},
  author={Crane, Rixon and Roosta, Fred},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9494--9504},
  year={2019}
}
@article{newtonmr,
  title={Newton-MR: Newton's Method Without Smoothness or Convexity},
  author={Roosta, Fred and Liu, Yang and Xu, Peng and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1810.00303},
  year={2018}
}

@article{libsvm,
  title={LIBSVM: A library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  volume={2},
  number={3},
  pages={27},
  year={2011},
  publisher={Acm}
}


@inproceedings{dane,
  title={Communication-efficient distributed optimization using an approximate newton-type method},
  author={Shamir, Ohad and Srebro, Nati and Zhang, Tong},
  booktitle={International conference on machine learning},
  pages={1000--1008},
  year={2014}
}



@article{aide,
  title={Aide: Fast and communication efficient distributed optimization},
  author={Reddi, Sashank J and Kone{\v{c}}n{\`y}, Jakub and Richt{\'a}rik, Peter and P{\'o}cz{\'o}s, Barnab{\'a}s and Smola, Alex},
  journal={arXiv preprint arXiv:1608.06879},
  year={2016}
}


@inproceedings{disco,
  title={Disco: Distributed optimization for self-concordant empirical loss},
  author={Zhang, Yuchen and Lin, Xiao},
  booktitle={International conference on machine learning},
  pages={362--370},
  year={2015}
}


@article{oversketched,
  title={Oversketched newton: Fast convex optimization for serverless systems},
  author={Gupta, Vipul and Kadhe, Swanand and Courtade, Thomas and Mahoney, Michael W and Ramchandran, Kannan},
  journal={arXiv preprint arXiv:1903.08857},
  year={2019}
}

@article{dunner2018distributed,
  title={A distributed second-order algorithm you can trust},
  author={D{\"u}nner, Celestine and Lucchi, Aurelien and Gargiani, Matilde and Bian, An and Hofmann, Thomas and Jaggi, Martin},
  journal={arXiv preprint arXiv:1806.07569},
  year={2018}
}
@article{newtonsketch,
  title={Newton sketch: A near linear-time optimization algorithm with linear-quadratic convergence},
  author={Pilanci, Mert and Wainwright, Martin J},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={1},
  pages={205--245},
  year={2017},
  publisher={SIAM}
}
@inproceedings{deter,
  title={Distributed estimation of the inverse Hessian by determinantal averaging},
  author={Derezinski, Michal and Mahoney, Michael W},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11401--11411},
  year={2019}
}

@article{subnewton1,
  title={Sub-sampled Newton methods I: globally convergent algorithms},
  author={Roosta-Khorasani, Farbod and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1601.04737},
  year={2016}
}
@article{subnewton2,
  title={Sub-sampled Newton methods II: Local convergence rates},
  author={Roosta-Khorasani, Farbod and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1601.04738},
  year={2016}
}

@article{exact_inexact,
  title={Exact and inexact subsampled Newton methods for optimization},
  author={Bollapragada, Raghu and Byrd, Richard H and Nocedal, Jorge},
  journal={IMA Journal of Numerical Analysis},
  volume={39},
  number={2},
  pages={545--578},
  year={2019},
  publisher={Oxford University Press}
}
@article{investigation,
  title={An investigation of Newton-sketch and subsampled Newton methods},
  author={Berahas, Albert S and Bollapragada, Raghu and Nocedal, Jorge},
  journal={Optimization Methods and Software},
  pages={1--20},
  year={2020},
  publisher={Taylor \& Francis}
}

@article{wangsketched,
  title={Sketched ridge regression: Optimization perspective, statistical perspective, and model averaging},
  author={Wang, Shusen and Gittens, Alex and Mahoney, Michael W},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8039--8088},
  year={2017},
  publisher={JMLR. org}
}

@article{wangspsd,
  title={SPSD matrix approximation vis column selection: Theories, algorithms, and extensions},
  author={Wang, Shusen and Luo, Luo and Zhang, Zhihua},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1697--1745},
  year={2016},
  publisher={JMLR. org}
}

@article{clarkson2017low,
  title={Low-rank approximation and regression in input sparsity time},
  author={Clarkson, Kenneth L and Woodruff, David P},
  journal={Journal of the ACM (JACM)},
  volume={63},
  number={6},
  pages={1--45},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@article{drineas2012fast,
  title={Fast approximation of matrix coherence and statistical leverage},
  author={Drineas, Petros and Magdon-Ismail, Malik and Mahoney, Michael W and Woodruff, David P},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={Dec},
  pages={3475--3506},
  year={2012}
}

@article{randnla,
  title={RandNLA: randomized numerical linear algebra},
  author={Drineas, Petros and Mahoney, Michael W},
  journal={Communications of the ACM},
  volume={59},
  number={6},
  pages={80--90},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@inproceedings{drineas2006sampling,
  title={Sampling algorithms for l 2 regression and applications},
  author={Drineas, Petros and Mahoney, Michael W and Muthukrishnan, Shan},
  booktitle={Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm},
  pages={1127--1136},
  year={2006},
  organization={Society for Industrial and Applied Mathematics}
}

@article{mahoneyrandomized,
  title={Randomized algorithms for matrices and data},
  author={Mahoney, Michael W},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={3},
  number={2},
  pages={123--224},
  year={2011},
  publisher={Now Publishers Inc.}
}



@article{federated,
  title={Federated optimization: Distributed machine learning for on-device intelligence},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Ramage, Daniel and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1610.02527},
  year={2016}
}



@article{lamport,
 author = {Lamport, Leslie and Shostak, Robert and Pease, Marshall},
 title = {The Byzantine Generals Problem},
 journal = {ACM Trans. Program. Lang. Syst.},
 issue_date = {July 1982},
 volume = {4},
 number = {3},
 month = jul,
 year = {1982},
 issn = {0164-0925},
 pages = {382--401},
 numpages = {20},
 url = {http://doi.acm.org/10.1145/357172.357176},
 doi = {10.1145/357172.357176},
 acmid = {357176},
 publisher = {ACM},
 address = {New York, NY, USA},
} 



@InProceedings{dong,
  title = 	 {{B}yzantine-Robust Distributed Learning: Towards Optimal Statistical Rates},
  author = 	 {Yin, Dong and Chen, Yudong and Kannan, Ramchandran and Bartlett, Peter},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5650--5659},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {StockholmsmÃ€ssan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/yin18a/yin18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/yin18a.html},
  abstract = 	 {In this paper, we develop distributed optimization algorithms that are provably robust against Byzantine failuresâarbitrary and potentially adversarial behavior, in distributed computing systems, with a focus on achieving optimal statistical performance. A main result of this work is a sharp analysis of two robust distributed gradient descent algorithms based on median and trimmed mean operations, respectively. We prove statistical error rates for all of strongly convex, non-strongly convex, and smooth non-convex population loss functions. In particular, these algorithms are shown to achieve order-optimal statistical error rates for strongly convex losses. To achieve better communication efficiency, we further propose a median-based distributed algorithm that is provably robust, and uses only one communication round. For strongly convex quadratic loss, we show that this algorithm achieves the same optimal error rate as the robust distributed gradient descent algorithms.}
}



@article{chen,
  title={Distributed statistical machine learning in adversarial settings: Byzantine gradient descent},
  author={Chen, Yudong and Su, Lili and Xu, Jiaming},
  journal={Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume={1},
  number={2},
  pages={44},
  year={2017},
  publisher={ACM}
}
 @article{anima,
  title={signsgd with majority vote is communication efficient and Byzantine fault tolerant},
  author={Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1810.05291},
  year={2018}
}



@article{blanchard2017byzantine,
  title={Byzantine-tolerant machine learning},
  author={Blanchard, Peva and Mhamdi, El Mahdi El and Guerraoui, Rachid and Stainer, Julien},
  journal={arXiv preprint arXiv:1703.02757},
  year={2017}
}

@InProceedings{dong1,
  title = 	 {Defending Against Saddle Point Attack in {B}yzantine-Robust Distributed Learning},
  author = 	 {Yin, Dong and Chen, Yudong and Kannan, Ramchandran and Bartlett, Peter},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7074--7084},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/yin19a/yin19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/yin19a.html},
  abstract = 	 {We study robust distributed learning that involves minimizing a non-convex loss function with saddle points. We consider the Byzantine setting where some worker machines have abnormal or even arbitrary and adversarial behavior, and in this setting, the Byzantine machines may create fake local minima near a saddle point that is far away from any true local minimum, even when robust gradient estimators are used. We develop ByzantinePGD, a robust first-order algorithm that can provably escape saddle points and fake local minima, and converge to an approximate true local minimizer with low iteration complexity. As a by-product, we give a simpler algorithm and analysis for escaping saddle points in the usual non-Byzantine setting. We further discuss three robust gradient estimators that can be used in ByzantinePGD, including median, trimmed mean, and iterative filtering. We characterize their performance in concrete statistical settings, and argue for their near-optimality in low and high dimensional regimes.}
}
@inproceedings{su,
  title={Fault-tolerant multi-agent optimization: optimal iterative distributed algorithms},
  author={Su, Lili and Vaidya, Nitin H},
  booktitle={Proceedings of the 2016 ACM symposium on principles of distributed computing},
  pages={425--434},
  year={2016},
  organization={ACM}
}

@article{mhamdi2018hidden,
  title={The hidden vulnerability of distributed learning in byzantium},
  author={Mhamdi, El Mahdi El and Guerraoui, Rachid and Rouault, S{\'e}bastien},
  journal={arXiv preprint arXiv:1802.07927},
  year={2018}
}


@article{Damaskinos,
      title = {AGGREGATHOR: Byzantine Machine Learning via Robust  Gradient Aggregation},
      author = {Damaskinos, Georgios and El Mhamdi, El Mahdi and  Guerraoui, Rachid and Guirguis, Arsany Hany Abdelmessih and  Rouault, SÃ©bastien Louis Alexandre},
      pages = {19},
      year = {2019},
      note = {Published in the Conference on Systems and Machine  Learning (SysML) 2019, Stanford, CA, USA.},
      url = {http://infoscience.epfl.ch/record/265684}
}
 
@article{feng,
  title={Distributed robust learning},
  author={Feng, Jiashi and Xu, Huan and Mannor, Shie},
  journal={arXiv preprint arXiv:1409.5937},
  year={2014}
}  

@article{chen,
  title={Distributed statistical machine learning in adversarial settings: Byzantine gradient descent},
  author={Chen, Yudong and Su, Lili and Xu, Jiaming},
  journal={Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume={1},
  number={2},
  pages={44},
  year={2017},
  publisher={ACM}
}
 
@inproceedings{bhatia,
  title={Robust regression via hard thresholding},
  author={Bhatia, Kush and Jain, Prateek and Kar, Purushottam},
  booktitle={Advances in Neural Information Processing Systems},
  pages={721--729},
  year={2015}
}
@article{anima,
  title={signsgd with majority vote is communication efficient and Byzantine fault tolerant},
  author={Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1810.05291},
  year={2018}
}

@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}

@article{ghosh2019robust,
  title={Robust Federated Learning in a Heterogeneous Environment},
  author={Ghosh, Avishek and Hong, Justin and Yin, Dong and Ramchandran, Kannan},
  journal={arXiv preprint arXiv:1906.06629},
  year={2019}
}

@article{blanchard2017byzantine,
  title={Byzantine-tolerant machine learning},
  author={Blanchard, Peva and Mhamdi, El Mahdi El and Guerraoui, Rachid and Stainer, Julien},
  journal={arXiv preprint arXiv:1703.02757},
  year={2017}
}
@inproceedings{su,
  title={Fault-tolerant multi-agent optimization: optimal iterative distributed algorithms},
  author={Su, Lili and Vaidya, Nitin H},
  booktitle={Proceedings of the 2016 ACM symposium on principles of distributed computing},
  pages={425--434},
  year={2016},
  organization={ACM}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Fifteenth Annual Conference of the International Speech Communication Association},
  year={2014}
}


@inproceedings{dme,
  title={Distributed mean estimation with limited communication},
  author={Suresh, Ananda Theertha and Yu, Felix X and Kumar, Sanjiv and McMahan, H Brendan},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3329--3337},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{atomo,
  title={Atomo: Communication-efficient learning via atomic sparsification},
  author={Wang, Hongyi and Sievert, Scott and Liu, Shengchao and Charles, Zachary and Papailiopoulos, Dimitris and Wright, Stephen},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9850--9861},
  year={2018}
}
@article{alistarh2017communication,
  title={Communication-efficient stochastic gradient descent, with applications to neural networks},
  author={Alistarh, Dan and Grubic, Demjan and Liu, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  year={2017},
  publisher={Curran Associates, Inc.}
}

@inproceedings{terngrad,
  title={Terngrad: Ternary gradients to reduce communication in distributed deep learning},
  author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle={Advances in neural information processing systems},
  pages={1509--1519},
  year={2017}
}
@inproceedings{qsgd,
  title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1709--1720},
  year={2017}
}
@article{signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1802.04434},
  year={2018}
}
@article{errorfeed,
  title={Error Feedback Fixes SignSGD and other Gradient Compression Schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian U and Jaggi, Martin},
  journal={arXiv preprint arXiv:1901.09847},
  year={2019}
}

@article{anima_signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1802.04434},
  year={2018}
}

@article{vqsgd,
    title={vqSGD: Vector Quantized Stochastic Gradient Descent},
    author={Venkata Gandikota and Raj Kumar Maity and Arya Mazumdar},
    journal={arXiv preprint arXiv:1911.07971},
	year={2019}
}

@inproceedings{stich2018sparsified,
  title={Sparsified SGD with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4447--4458},
  year={2018}
}
@article{sparsedingo,
    title={Communication Efficient Distributed Approximate Newton Method},
    author={Avishek Ghosh and Raj Kumar Maity and Arya Mazumdar and Kannan Ramchandran},
    journal={ISIT 2020 (accepted), https://tinyurl.com/ujnpt4c},
	year={2020}
}


@misc{swarm2,
  title = {Swarm User Documentation},
  author={Swarm2},
  howpublished = {\url{https://people.cs.umass.edu/~swarm/index.php?n=Main.NewSwarmDoc}},
  year={2018},
  note = {Accessed: 2018-01-05}
}
@article{ghosh2019communication,
  title={Communication-Efficient and Byzantine-Robust Distributed Learning},
  author={Ghosh, Avishek and Maity, Raj Kumar and Kadhe, Swanand and Mazumdar, Arya and Ramchandran, Kannan},
  journal={arXiv preprint arXiv:1911.09721},
  year={2019}
}
