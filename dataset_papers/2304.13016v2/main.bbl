\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adlam \& Pennington(2020{\natexlab{a}})Adlam and
  Pennington]{adlam2020understanding}
Adlam, B. and Pennington, J.
\newblock Understanding double descent requires a fine-grained bias-variance
  decomposition.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 11022--11032, 2020{\natexlab{a}}.

\bibitem[Adlam \& Pennington(2020{\natexlab{b}})Adlam and
  Pennington]{adlam_pennington_2020neural}
Adlam, B. and Pennington, J.
\newblock The neural tangent kernel in high dimensions: Triple descent and a
  multi-scale theory of generalization.
\newblock In \emph{International Conference on Machine Learning}. PMLR,
  2020{\natexlab{b}}.

\bibitem[Allen(1974)]{allen_1974}
Allen, D.~M.
\newblock The relationship between variable selection and data augmentation and
  a method for prediction.
\newblock \emph{Technometrics}, 16\penalty0 (1):\penalty0 125--127, 1974.

\bibitem[Arlot \& Celisse(2010)Arlot and Celisse]{arlot_celisse_2010}
Arlot, S. and Celisse, A.
\newblock A survey of cross-validation procedures for model selection.
\newblock \emph{Statistics surveys}, 4:\penalty0 40--79, 2010.

\bibitem[Bai \& Silverstein(2010)Bai and Silverstein]{bai2010spectral}
Bai, Z. and Silverstein, J.~W.
\newblock \emph{Spectral Analysis of Large Dimensional Random Matrices}.
\newblock Springer, 2010.
\newblock Second edition.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett_long_lugosi_tsigler_2020}
Bartlett, P.~L., Long, P.~M., Lugosi, G., and Tsigler, A.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30063--30070, 2020.

\bibitem[Bartlett et~al.(2021)Bartlett, Montanari, and
  Rakhlin]{bartlett_montanari_rakhlin_2021}
Bartlett, P.~L., Montanari, A., and Rakhlin, A.
\newblock Deep learning: a statistical viewpoint.
\newblock \emph{Acta numerica}, 30:\penalty0 87--201, 2021.

\bibitem[Belkin et~al.(2020)Belkin, Hsu, and Xu]{belkin_hsu_xu_2020}
Belkin, M., Hsu, D., and Xu, J.
\newblock Two models of double descent for weak features.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (4):\penalty0 1167--1180, 2020.

\bibitem[Bloemendal et~al.(2016)Bloemendal, Knowles, Yau, and
  Yin]{bloemendal2016principal}
Bloemendal, A., Knowles, A., Yau, H.-T., and Yin, J.
\newblock On the principal components of sample covariance matrices.
\newblock \emph{Probability theory and Related Fields}, 164\penalty0
  (1):\penalty0 459--552, 2016.

\bibitem[Breiman(1996)]{breiman_1996}
Breiman, L.
\newblock Bagging predictors.
\newblock \emph{Machine Learning}, 24\penalty0 (2):\penalty0 123--140, 1996.

\bibitem[B{\"u}hlmann \& Yu(2002)B{\"u}hlmann and Yu]{buhlmann2002analyzing}
B{\"u}hlmann, P. and Yu, B.
\newblock Analyzing bagging.
\newblock \emph{The Annals of Statistics}, 30\penalty0 (4):\penalty0 927--961,
  2002.

\bibitem[Buja \& Stuetzle(2006)Buja and Stuetzle]{buja2006observations}
Buja, A. and Stuetzle, W.
\newblock Observations on bagging.
\newblock \emph{Statistica Sinica}, pp.\  323--351, 2006.

\bibitem[Craven \& Wahba(1979)Craven and Wahba]{craven_wahba_1979}
Craven, P. and Wahba, G.
\newblock Estimating the correct degree of smoothing by the method of
  generalized cross-validation.
\newblock \emph{Numerische Mathematik}, 31:\penalty0 377--403, 1979.

\bibitem[Dobriban \& Sheng(2021)Dobriban and Sheng]{dobriban_sheng_2021}
Dobriban, E. and Sheng, Y.
\newblock Distributed linear regression by averaging.
\newblock \emph{The Annals of Statistics}, 49\penalty0 (2):\penalty0 918--943,
  2021.

\bibitem[Dobriban \& Wager(2018)Dobriban and Wager]{dobriban_wager_2018}
Dobriban, E. and Wager, S.
\newblock High-dimensional asymptotics of prediction: Ridge regression and
  classification.
\newblock \emph{The Annals of Statistics}, 46\penalty0 (1):\penalty0 247--279,
  2018.

\bibitem[Du et~al.(2022)Du, Cai, and Roeder]{du2022robust}
Du, J.-H., Cai, Z., and Roeder, K.
\newblock Robust probabilistic modeling for single-cell multimodal mosaic
  integration and imputation via scvaeit.
\newblock \emph{Proceedings of the National Academy of Sciences}, 119\penalty0
  (49):\penalty0 e2214414119, 2022.

\bibitem[d’Ascoli et~al.(2020)d’Ascoli, Refinetti, Biroli, and
  Krzakala]{d2020double}
d’Ascoli, S., Refinetti, M., Biroli, G., and Krzakala, F.
\newblock Double trouble in double descent: Bias and variance (s) in the lazy
  regime.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2280--2290. PMLR, 2020.

\bibitem[El~Karoui(2013)]{karoui_2013}
El~Karoui, N.
\newblock Asymptotic behavior of unregularized and ridge-regularized
  high-dimensional robust regression estimators: rigorous results.
\newblock \emph{arXiv preprint arXiv:1311.2445}, 2013.

\bibitem[El~Karoui(2018)]{elkaroui_2018}
El~Karoui, N.
\newblock On the impact of predictor geometry on the performance on
  high-dimensional ridge-regularized generalized robust regression estimators.
\newblock \emph{Probability Theory and Related Fields}, 170\penalty0
  (1):\penalty0 95--175, 2018.

\bibitem[Erd{\H{o}}s \& Yau(2017)Erd{\H{o}}s and Yau]{erdos_yau_2017}
Erd{\H{o}}s, L. and Yau, H.-T.
\newblock \emph{A Dynamical Approach to Random Matrix Theory}.
\newblock American Mathematical Society, 2017.

\bibitem[Friedman et~al.(2010)Friedman, Hastie, and
  Tibshirani]{friedman2010regularization}
Friedman, J., Hastie, T., and Tibshirani, R.
\newblock Regularization paths for generalized linear models via coordinate
  descent.
\newblock \emph{Journal of statistical software}, 33\penalty0 (1):\penalty0 1,
  2010.

\bibitem[Friedman \& Hall(2007)Friedman and Hall]{friedman_hall_2007}
Friedman, J.~H. and Hall, P.
\newblock On bagging and nonlinear estimation.
\newblock \emph{Journal of Statistical Planning and Inference}, 137\penalty0
  (3):\penalty0 669--683, 2007.

\bibitem[Geisser(1975)]{geisser_1975}
Geisser, S.
\newblock The predictive sample reuse method with applications.
\newblock \emph{Journal of the American statistical Association}, 70\penalty0
  (350):\penalty0 320--328, 1975.

\bibitem[Golub et~al.(1979)Golub, Heath, and Wahba]{golub_heath_wabha_1979}
Golub, G.~H., Heath, M., and Wahba, G.
\newblock Generalized cross-validation as a method for choosing a good ridge
  parameter.
\newblock \emph{Technometrics}, 21\penalty0 (2):\penalty0 215--223, 1979.

\bibitem[Greene \& Wellner(2017)Greene and Wellner]{greene2017exponential}
Greene, E. and Wellner, J.~A.
\newblock Exponential bounds for the hypergeometric distribution.
\newblock \emph{Bernoulli}, 23\penalty0 (3):\penalty0 1911, 2017.

\bibitem[Grenander \& Szeg{\"o}(1958)Grenander and
  Szeg{\"o}]{grenander1958toeplitz}
Grenander, U. and Szeg{\"o}, G.
\newblock \emph{Toeplitz Forms and Their Applications}.
\newblock University of California Press, 1958.
\newblock First edition.

\bibitem[Gut(2005)]{gut_2005}
Gut, A.
\newblock \emph{Probability: A Graduate Course}.
\newblock Springer, New York, 2005.

\bibitem[Gy{\"o}rfi et~al.(2006)Gy{\"o}rfi, Kohler, Krzyzak, and
  Walk]{gyorfi_kohler_krzyzak_walk_2006}
Gy{\"o}rfi, L., Kohler, M., Krzyzak, A., and Walk, H.
\newblock \emph{A Distribution-free Theory of Nonparametric Regression}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Hall \& Samworth(2005)Hall and Samworth]{hall_samworth_2005}
Hall, P. and Samworth, R.~J.
\newblock Properties of bagged nearest neighbour classifiers.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 67\penalty0 (3):\penalty0 363--379, 2005.

\bibitem[Hao et~al.(2021)Hao, Hao, Andersen-Nissen, Mauck~III, Zheng, Butler,
  Lee, Wilk, Darby, Zager, et~al.]{hao2021integrated}
Hao, Y., Hao, S., Andersen-Nissen, E., Mauck~III, W.~M., Zheng, S., Butler, A.,
  Lee, M.~J., Wilk, A.~J., Darby, C., Zager, M., et~al.
\newblock Integrated analysis of multimodal single-cell data.
\newblock \emph{Cell}, 2021.

\bibitem[Hastie(2020)]{hastie2020ridge}
Hastie, T.
\newblock Ridge regularization: An essential concept in data science.
\newblock \emph{Technometrics}, 62\penalty0 (4):\penalty0 426--433, 2020.

\bibitem[Hastie et~al.(2009)Hastie, Tibshirani, and
  Friedman]{friedman_hastie_tibshirani_2009}
Hastie, T., Tibshirani, R., and Friedman, J.
\newblock \emph{The Elements of Statistical Learning}.
\newblock Springer Series in Statistics, 2009.
\newblock Second edition.

\bibitem[Hastie et~al.(2022)Hastie, Montanari, Rosset, and
  Tibshirani]{hastie2022surprises}
Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R.~J.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{The Annals of Statistics}, 50\penalty0 (2):\penalty0 949--986,
  2022.

\bibitem[Hoerl \& Kennard(1970{\natexlab{a}})Hoerl and
  Kennard]{hoerl_kennard_1970_1}
Hoerl, A.~E. and Kennard, R.~W.
\newblock Ridge regression: Biased estimation for nonorthogonal problems.
\newblock \emph{Technometrics}, 12\penalty0 (1):\penalty0 55--67,
  1970{\natexlab{a}}.

\bibitem[Hoerl \& Kennard(1970{\natexlab{b}})Hoerl and
  Kennard]{hoerl_kennard_1970_2}
Hoerl, A.~E. and Kennard, R.~W.
\newblock Ridge regression: applications to nonorthogonal problems.
\newblock \emph{Technometrics}, 12\penalty0 (1):\penalty0 69--82,
  1970{\natexlab{b}}.

\bibitem[Kale et~al.(2011)Kale, Kumar, and
  Vassilvitskii]{kale_kumar_vassilvitskii_2011}
Kale, S., Kumar, R., and Vassilvitskii, S.
\newblock Cross-validation and mean-square stability.
\newblock In \emph{In Proceedings of the Second Symposium on Innovations in
  Computer Science}, 2011.

\bibitem[Krogh \& Sollich(1997)Krogh and Sollich]{krogh1997statistical}
Krogh, A. and Sollich, P.
\newblock Statistical mechanics of ensemble learning.
\newblock \emph{Physical Review E}, 55\penalty0 (1):\penalty0 811, 1997.

\bibitem[Kumar et~al.(2013)Kumar, Lokshtanov, Vassilvitskii, and
  Vattani]{kumar_lokshtanov_vassilviskii_vattani_2013}
Kumar, R., Lokshtanov, D., Vassilvitskii, S., and Vattani, A.
\newblock Near-optimal bounds for cross-validation via loss stability.
\newblock In \emph{International Conference on Machine Learning}, 2013.

\bibitem[LeJeune et~al.(2020)LeJeune, Javadi, and
  Baraniuk]{lejeune2020implicit}
LeJeune, D., Javadi, H., and Baraniuk, R.
\newblock The implicit regularization of ordinary least squares ensembles.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2020.

\bibitem[Li(1985)]{li_1985}
Li, K.-C.
\newblock From {Stein's} unbiased risk estimates to the method of generalized
  cross validation.
\newblock \emph{The Annals of Statistics}, pp.\  1352--1377, 1985.

\bibitem[Li(1986)]{li_1986}
Li, K.-C.
\newblock Asymptotic optimality of $ c_l $ and generalized cross-validation in
  ridge regression with application to spline smoothing.
\newblock \emph{The Annals of Statistics}, 14\penalty0 (3):\penalty0
  1101--1112, 1986.

\bibitem[Li(1987)]{li_1987}
Li, K.-C.
\newblock Asymptotic optimality for $ c_p, c_l $, cross-validation and
  generalized cross-validation: Discrete index set.
\newblock \emph{The Annals of Statistics}, 15\penalty0 (3):\penalty0 958--975,
  1987.

\bibitem[Loureiro et~al.(2022)Loureiro, Gerbelot, Refinetti, Sicuro, and
  Krzakala]{loureiro2022fluctuations}
Loureiro, B., Gerbelot, C., Refinetti, M., Sicuro, G., and Krzakala, F.
\newblock Fluctuations, bias, variance \& ensemble of learners: Exact
  asymptotics for convex losses in high-dimension.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  14283--14314. PMLR, 2022.

\bibitem[Mei \& Montanari(2022)Mei and Montanari]{mei_montanari_2022}
Mei, S. and Montanari, A.
\newblock The generalization error of random features regression: Precise
  asymptotics and the double descent curve.
\newblock \emph{Communications on Pure and Applied Mathematics}, 75\penalty0
  (4):\penalty0 667--766, 2022.

\bibitem[Miolane \& Montanari(2021)Miolane and
  Montanari]{miolane_montanari_2021}
Miolane, L. and Montanari, A.
\newblock The distribution of the lasso: Uniform control over sparse balls and
  adaptive parameter tuning.
\newblock \emph{The Annals of Statistics}, 49\penalty0 (4):\penalty0
  2313--2335, 2021.

\bibitem[Muthukumar et~al.(2020)Muthukumar, Vodrahalli, Subramanian, and
  Sahai]{muthukumar_vodrahalli_subramanian_sahai_2020}
Muthukumar, V., Vodrahalli, K., Subramanian, V., and Sahai, A.
\newblock Harmless interpolation of noisy data in regression.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 67--83, 2020.

\bibitem[Obuchi \& Kabashima(2016)Obuchi and Kabashima]{obuchi_kabashima_2016}
Obuchi, T. and Kabashima, Y.
\newblock Cross validation in {LASSO} and its acceleration.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2016.

\bibitem[Patil et~al.(2021)Patil, Wei, Rinaldo, and
  Tibshirani]{patil2021uniform}
Patil, P., Wei, Y., Rinaldo, A., and Tibshirani, R.
\newblock Uniform consistency of cross-validation estimators for
  high-dimensional ridge regression.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}. PMLR, 2021.

\bibitem[Patil et~al.(2022{\natexlab{a}})Patil, Du, and
  Kuchibhotla]{patil2022bagging}
Patil, P., Du, J.-H., and Kuchibhotla, A.~K.
\newblock Bagging in overparameterized learning: Risk characterization and risk
  monotonization.
\newblock \emph{arXiv preprint arXiv:2210.11445}, 2022{\natexlab{a}}.

\bibitem[Patil et~al.(2022{\natexlab{b}})Patil, Kuchibhotla, Wei, and
  Rinaldo]{patil2022mitigating}
Patil, P., Kuchibhotla, A.~K., Wei, Y., and Rinaldo, A.
\newblock Mitigating multiple descents: A model-agnostic framework for risk
  monotonization.
\newblock \emph{arXiv preprint arXiv:2205.12937}, 2022{\natexlab{b}}.

\bibitem[Patil et~al.(2022{\natexlab{c}})Patil, Rinaldo, and
  Tibshirani]{patil2022estimating}
Patil, P., Rinaldo, A., and Tibshirani, R.
\newblock Estimating functionals of the out-of-sample error distribution in
  high-dimensional ridge regression.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}. PMLR, 2022{\natexlab{c}}.

\bibitem[Rad \& Maleki(2020)Rad and Maleki]{rad_maleki_2020}
Rad, K.~R. and Maleki, A.
\newblock A scalable estimate of the out-of-sample prediction error via
  approximate leave-one-out cross-validation.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 82\penalty0 (4):\penalty0 965--996, 2020.

\bibitem[Rad et~al.(2020)Rad, Zhou, and Maleki]{rad_zhou_maleki_2020}
Rad, K.~R., Zhou, W., and Maleki, A.
\newblock Error bounds in estimating the out-of-sample prediction error using
  leave-one-out cross validation in high-dimensions.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}. PMLR, 2020.

\bibitem[Rubio \& Mestre(2011)Rubio and Mestre]{rubio_mestre_2011}
Rubio, F. and Mestre, X.
\newblock Spectral convergence for a general class of random matrices.
\newblock \emph{Statistics \& probability letters}, 81\penalty0 (5):\penalty0
  592--602, 2011.

\bibitem[Rudin(1976)]{rudin_1976}
Rudin, W.
\newblock \emph{Principles of Mathematical Analysis}.
\newblock McGraw-Hill New York, 1976.

\bibitem[Samworth(2012)]{samworth2012optimal}
Samworth, R.~J.
\newblock Optimal weighted nearest neighbour classifiers.
\newblock \emph{The Annals of Statistics}, 40\penalty0 (5):\penalty0
  2733--2763, 2012.

\bibitem[Sollich \& Krogh(1995)Sollich and Krogh]{sollich1995learning}
Sollich, P. and Krogh, A.
\newblock Learning with ensembles: How overfitting can be useful.
\newblock \emph{Advances in neural information processing systems}, 8, 1995.

\bibitem[Stone(1974)]{stone_1974}
Stone, M.
\newblock Cross-validatory choice and assessment of statistical predictions.
\newblock \emph{Journal of the Royal Statistical Society: Series B},
  36\penalty0 (2):\penalty0 111--133, 1974.

\bibitem[Stone(1977)]{stone_1977}
Stone, M.
\newblock Asymptotics for and against cross-validation.
\newblock \emph{Biometrika}, 64\penalty0 (1):\penalty0 29--35, 1977.

\bibitem[Sur et~al.(2019)Sur, Chen, and Cand{\`e}s]{sur_chen_candes_2019}
Sur, P., Chen, Y., and Cand{\`e}s, E.~J.
\newblock The likelihood ratio test in high-dimensional logistic regression is
  asymptotically a rescaled chi-square.
\newblock \emph{Probability Theory and Related Fields}, 175\penalty0
  (1):\penalty0 487--558, 2019.

\bibitem[Thrampoulidis et~al.(2015)Thrampoulidis, Oymak, and
  Hassibi]{thrampoulidis_oymak_hassibi_2015}
Thrampoulidis, C., Oymak, S., and Hassibi, B.
\newblock Regularized linear regression: A precise analysis of the estimation
  error.
\newblock In \emph{Conference on Learning Theory}, pp.\  1683--1709. PMLR,
  2015.

\bibitem[Thrampoulidis et~al.(2018)Thrampoulidis, Abbasi, and
  Hassibi]{thrampoulidis_abbasi_hassibi_2018}
Thrampoulidis, C., Abbasi, E., and Hassibi, B.
\newblock Precise error analysis of regularized {$M$}-estimators in high
  dimensions.
\newblock \emph{IEEE Transactions on Information Theory}, 64\penalty0
  (8):\penalty0 5592--5628, 2018.

\bibitem[Wang et~al.(2018)Wang, Zhou, Lu, Maleki, and
  Mirrokni]{wang_zhou_lu_maleki_mirrokni_2018}
Wang, S., Zhou, W., Lu, H., Maleki, A., and Mirrokni, V.
\newblock Approximate leave-one-out for fast parameter tuning in high
  dimensions.
\newblock \emph{arXiv preprint arXiv:1807.02694}, 2018.

\bibitem[Wasserman(2006)]{wasserman2006all}
Wasserman, L.
\newblock \emph{Olive Nonparametric Statistics}.
\newblock Springer, 2006.

\bibitem[Wei et~al.(2022)Wei, Hu, and Steinhardt]{wei_hu_steinhardt}
Wei, A., Hu, W., and Steinhardt, J.
\newblock More than a toy: Random matrix models predict how real-world neural
  representations generalize.
\newblock \emph{arXiv preprint arXiv:2203.06176}, 2022.

\bibitem[Xu et~al.(2019)Xu, Maleki, and Rad]{xu_maleki_rad_2019}
Xu, J., Maleki, A., and Rad, K.~R.
\newblock Consistent risk estimation in high-dimensional linear regression.
\newblock \emph{arXiv preprint arXiv:1902.01753}, 2019.

\bibitem[Zhang \& Yang(2015)Zhang and Yang]{zhang_yang_2015}
Zhang, Y. and Yang, Y.
\newblock Cross-validation for selecting a model selection procedure.
\newblock \emph{Journal of Econometrics}, 187\penalty0 (1):\penalty0 95--112,
  2015.

\end{thebibliography}
