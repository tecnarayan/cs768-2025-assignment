\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bardes et~al.(2021)Bardes, Ponce, and LeCun]{bardes2021VICReg}
Adrien Bardes, Jean Ponce, and Yann LeCun.
\newblock {VICR}eg: Variance-invariance-covariance regularization for
  self-supervised learning.
\newblock \emph{preprint arXiv:2105.04906}, 2021.

\bibitem[Beery et~al.(2018)Beery, Van~Horn, and Perona]{beery2018recognition}
Sara Beery, Grant Van~Horn, and Pietro Perona.
\newblock Recognition in terra incognita.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 456--473, 2018.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020unsupervised}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 9912--9924, 2020.

\bibitem[Chen and Li(2020)]{chen2020intriguing}
Ting Chen and Lala Li.
\newblock Intriguing properties of contrastive losses.
\newblock \emph{preprint arXiv:2011.02803}, 2020.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{Int. Conference on Machine Learning (ICML)}, pages
  10709--10719, 2020{\natexlab{a}}.

\bibitem[Chen and He(2021)]{chen2020exploring}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2021.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Fan, Girshick, and
  He]{chen2020mocov2}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock \emph{preprint arXiv:2003.04297}, 2020{\natexlab{b}}.

\bibitem[Chizat and Bach(2020)]{chizat2020implicit}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory (COLT)}, pages 1305--1338,
  2020.

\bibitem[Chuang et~al.(2020)Chuang, Robinson, Yen-Chen, Torralba, and
  Jegelka]{chuang2020debiased}
Ching-Yao Chuang, Joshua Robinson, Lin Yen-Chen, Antonio Torralba, and Stefanie
  Jegelka.
\newblock Debiased contrastive learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 8765--8775, 2020.

\bibitem[Geirhos et~al.(2019)Geirhos, Rubisch, Michaelis, Bethge, Wichmann, and
  Brendel]{geirhos2018imagenet}
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix~A
  Wichmann, and Wieland Brendel.
\newblock Image{N}et-trained {CNN}s are biased towards texture; increasing
  shape bias improves accuracy and robustness.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, 2019.

\bibitem[Geirhos et~al.(2020)Geirhos, Jacobsen, Michaelis, Zemel, Brendel,
  Bethge, and Wichmann]{geirhos2020shortcut}
Robert Geirhos, J{\"o}rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel,
  Wieland Brendel, Matthias Bethge, and Felix~A Wichmann.
\newblock Shortcut learning in deep neural networks.
\newblock \emph{Nature Machine Intelligence}, 2\penalty0 (11):\penalty0
  665--673, 2020.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, 2015.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Pires, Guo, Azar, et~al.]{grill2020bootstrap}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec,
  Pierre~H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo~Avila Pires,
  Zhaohan~Daniel Guo, Mohammad~Gheshlaghi Azar, et~al.
\newblock Bootstrap your own latent: A new approach to self-supervised
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 21271--21284, 2020.

\bibitem[Gutmann and Hyv{\"a}rinen(2010)]{gutmann2010noise}
Michael Gutmann and Aapo Hyv{\"a}rinen.
\newblock Noise-contrastive estimation: A new estimation principle for
  unnormalized statistical models.
\newblock In \emph{Proc. Int. Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pages 297--304, 2010.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 9729--9738, 2020.

\bibitem[Hermann and Lampinen(2020)]{hermann2020shapes}
Katherine~L Hermann and Andrew~K Lampinen.
\newblock What shapes feature representations? {E}xploring datasets,
  architectures, and training.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 9995--10006, 2020.

\bibitem[Hermann et~al.(2019)Hermann, Chen, and Kornblith]{hermann2019origins}
Katherine~L Hermann, Ting Chen, and Simon Kornblith.
\newblock The origins and prevalence of texture bias in convolutional neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 19000--19015, 2019.

\bibitem[Hjelm et~al.(2019)Hjelm, Fedorov, Lavoie-Marchildon, Grewal, Bachman,
  Trischler, and Bengio]{hjelm2018learning}
R~Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil
  Bachman, Adam Trischler, and Yoshua Bengio.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, 2019.

\bibitem[Ho and Nvasconcelos(2020)]{ho2020contrastive}
Chih-Hui Ho and Nuno Nvasconcelos.
\newblock Contrastive learning with adversarial examples.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 17081--17093, 2020.

\bibitem[Hu et~al.(2020)Hu, Wang, Hu, and Qi]{hu2020adco}
Qianjiang Hu, Xiao Wang, Wei Hu, and Guo-Jun Qi.
\newblock Adco: Adversarial contrast for efficient learning of unsupervised
  representations from self-trained negative adversaries.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2020.

\bibitem[Huh et~al.(2021)Huh, Mobahi, Zhang, Cheung, Agrawal, and
  Isola]{huh2021low}
Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and
  Phillip Isola.
\newblock The low-rank simplicity bias in deep networks.
\newblock \emph{preprint arXiv:2103.10427}, 2021.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Tsipras, Engstrom, Tran, and
  Madry]{ilyas2019adversarial}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 125--136, 2019.

\bibitem[Jacobsen et~al.(2018)Jacobsen, Behrmann, Zemel, and
  Bethge]{jacobsen2018excessive}
J{\"o}rn-Henrik Jacobsen, Jens Behrmann, Richard Zemel, and Matthias Bethge.
\newblock Excessive invariance causes adversarial vulnerability.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, 2018.

\bibitem[Jiang et~al.(2020)Jiang, Chen, Chen, and Wang]{jiang2020robust}
Ziyu Jiang, Tianlong Chen, Ting Chen, and Zhangyang Wang.
\newblock Robust pre-training by adversarial contrastive learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 16199--16210, 2020.

\bibitem[Kalantidis et~al.(2020)Kalantidis, Sariyildiz, Pion, Weinzaepfel, and
  Larlus]{kalantidis2020hard}
Yannis Kalantidis, Mert~Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and
  Diane Larlus.
\newblock Hard negative mixing for contrastive learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 21798--21809, 2020.

\bibitem[Kim et~al.(2020)Kim, Tack, and Hwang]{kim2020adversarial}
Minseon Kim, Jihoon Tack, and Sung~Ju Hwang.
\newblock Adversarial self-supervised contrastive learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Lee et~al.(2020)Lee, Lei, Saunshi, and Zhuo]{lee2020predicting}
Jason~D Lee, Qi~Lei, Nikunj Saunshi, and Jiacheng Zhuo.
\newblock Predicting what you already know helps: Provable self-supervised
  learning.
\newblock \emph{preprint arXiv:2008.01064}, 2020.

\bibitem[Li et~al.(2020)Li, Fan, Yuan, He, Tian, and Katabi]{li2020information}
Tianhong Li, Lijie Fan, Yuan Yuan, Hao He, Yonglong Tian, and Dina Katabi.
\newblock Information-preserving contrastive learning for self-supervised
  representations.
\newblock \emph{preprint arXiv:2012.09962}, 2020.

\bibitem[Lyu and Li(2020)]{lyu2019gradient}
Kaifeng Lyu and Jian Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, 2020.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, 2018.

\bibitem[Minderer et~al.(2020)Minderer, Bachem, Houlsby, and
  Tschannen]{minderer2020automatic}
Matthias Minderer, Olivier Bachem, Neil Houlsby, and Michael Tschannen.
\newblock Automatic shortcut removal for self-supervised representation
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  6927--6937, 2020.

\bibitem[Nguyen et~al.(2021)Nguyen, Raghu, and Kornblith]{nguyen2020wide}
Thao Nguyen, Maithra Raghu, and Simon Kornblith.
\newblock Do wide and deep networks learn the same things? uncovering how
  neural network representations vary with width and depth.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, 2021.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{preprint arXiv:1807.03748}, 2018.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Pathak et~al.(2016)Pathak, Krahenbuhl, Donahue, Darrell, and
  Efros]{pathak2016context}
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei~A
  Efros.
\newblock Context encoders: Feature learning by inpainting.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 2536--2544, 2016.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, et~al.]{pedregosa2011scikit}
Fabian Pedregosa, Ga{\"e}l Varoquaux, Alexandre Gramfort, Vincent Michel,
  Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
  Weiss, Vincent Dubourg, et~al.
\newblock Scikit-learn: Machine learning in python.
\newblock \emph{Journal of machine learning research}, pages 2825--2830, 2011.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and
  Shankar]{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do {I}mage{N}et classifiers generalize to {I}mage{N}et?
\newblock In \emph{Int. Conference on Machine Learning (ICML)}, pages
  5389--5400, 2019.

\bibitem[Regan et~al.(2011)Regan, Hokanson, Murphy, Make, Lynch, Beaty,
  Curran-Everett, Silverman, and Crapo]{regan2011genetic}
Elizabeth~A Regan, John~E Hokanson, James~R Murphy, Barry Make, David~A Lynch,
  Terri~H Beaty, Douglas Curran-Everett, Edwin~K Silverman, and James~D Crapo.
\newblock Genetic epidemiology of {COPD} ({COPDGene}) study design.
\newblock \emph{{COPD}: Journal of Chronic Obstructive Pulmonary Disease},
  7\penalty0 (1):\penalty0 32--43, 2011.

\bibitem[Robinson et~al.(2020)Robinson, Jegelka, and Sra]{robinson2020strength}
Joshua Robinson, Stefanie Jegelka, and Suvrit Sra.
\newblock Strength from weakness: Fast learning using weak supervision.
\newblock In \emph{Int. Conference on Machine Learning (ICML)}, pages
  8127--8136, 2020.

\bibitem[Robinson et~al.(2021)Robinson, Chuang, Sra, and
  Jegelka]{robinson2020contrastive}
Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka.
\newblock Contrastive learning with hard negative samples.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, 2021.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Sun et~al.(2021)Sun, Yu, and Batmanghelich]{sun2020context}
Li~Sun, Ke~Yu, and Kayhan Batmanghelich.
\newblock Context matters: Graph-based self-supervised representation learning
  for medical images.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 4874--4882, 2021.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, 2014.

\bibitem[Tian et~al.(2020{\natexlab{a}})Tian, Krishnan, and
  Isola]{tian2019contrastive}
Yonglong Tian, Dilip Krishnan, and Phillip Isola.
\newblock Contrastive multiview coding.
\newblock In \emph{Europ. Conference on Computer Vision (ECCV)},
  2020{\natexlab{a}}.

\bibitem[Tian et~al.(2020{\natexlab{b}})Tian, Sun, Poole, Krishnan, Schmid, and
  Isola]{tian2020makes}
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and
  Phillip Isola.
\newblock What makes for good views for contrastive learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 6827--6839, 2020{\natexlab{b}}.

\bibitem[Tsipras et~al.(2018)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, 2018.

\bibitem[Wang and Liu(2021)]{wang2020understandingbehaviour}
Feng Wang and Huaping Liu.
\newblock Understanding the behaviour of contrastive loss.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2021.

\bibitem[Wang et~al.(2020)Wang, Liu, Guo, and Sun]{wang2020unsupervised}
Feng Wang, Huaping Liu, Di~Guo, and Fuchun Sun.
\newblock Unsupervised representation learning by invariance propagation.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 3510--3520, 2020.

\bibitem[Wang and Isola(2020)]{wang2020understanding}
Tongzhou Wang and Phillip Isola.
\newblock Understanding contrastive representation learning through alignment
  and uniformity on the hypersphere.
\newblock In \emph{Int. Conference on Machine Learning (ICML)}, pages
  9574--9584, 2020.

\bibitem[Wang and Qi(2021)]{wang2021augmentations}
Xiao Wang and Guo-Jun Qi.
\newblock Contrastive learning with stronger augmentations.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2021.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar2021barlow}
Jure Zbontar, Li~Jing, Ishan Misra, Yann LeCun, and St{\'e}phane Deny.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In \emph{Int. Conference on Machine Learning (ICML)}, 2021.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, Xing, El~Ghaoui, and
  Jordan]{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El~Ghaoui, and
  Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In \emph{Int. Conference on Machine Learning (ICML)}, pages
  7472--7482, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, 2018.

\bibitem[Zhang et~al.(2016)Zhang, Isola, and Efros]{zhang2016colorful}
Richard Zhang, Phillip Isola, and Alexei~A Efros.
\newblock Colorful image colorization.
\newblock In \emph{Europ. Conference on Computer Vision (ECCV)}, pages
  649--666, 2016.

\bibitem[Zhao et~al.(2021)Zhao, Wu, Lau, and Lin]{zhao2020makes}
Nanxuan Zhao, Zhirong Wu, Rynson~WH Lau, and Stephen Lin.
\newblock What makes instance discrimination good for transfer learning?
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, 2021.

\bibitem[Zimmermann et~al.(2021)Zimmermann, Sharma, Schneider, Bethge, and
  Brendel]{zimmermann2021contrastive}
Roland~S Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and
  Wieland Brendel.
\newblock Contrastive learning inverts the data generating process.
\newblock In \emph{Int. Conference on Machine Learning (ICML)}, 2021.

\end{thebibliography}
