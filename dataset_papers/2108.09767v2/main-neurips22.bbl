\begin{thebibliography}{10}

\bibitem{agarwal2020pc}
Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun.
\newblock Pc-pg: Policy cover directed exploration for provable policy gradient
  learning.
\newblock {\em arXiv preprint arXiv:2007.08459}, 2020.

\bibitem{agarwal2019theory}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift.
\newblock {\em arXiv preprint arXiv:1908.00261}, 2019.

\bibitem{agarwal2020boosting}
Naman Agarwal, Nataly Brukhim, Elad Hazan, and Zhou Lu.
\newblock Boosting for control of dynamical systems.
\newblock In {\em International Conference on Machine Learning}, pages 96--103.
  PMLR, 2020.

\bibitem{agarwal2022variance}
Naman Agarwal, Brian Bullins, and Karan Singh.
\newblock Variance-reduced conservative policy iteration.
\newblock {\em arXiv preprint arXiv:2212.06283}, 2022.

\bibitem{alon2020boosting}
Noga Alon, Alon Gonen, Elad Hazan, and Shay Moran.
\newblock Boosting simple learners.
\newblock {\em arXiv preprint arXiv:2001.11704}, 2020.

\bibitem{bagnell2003policy}
J~Andrew Bagnell, Sham Kakade, Andrew~Y Ng, and Jeff~G Schneider.
\newblock Policy search by dynamic programming.
\newblock In {\em Advances in Neural Information Processing Systems}, 2003.

\bibitem{beygelzimer2015optimal}
Alina Beygelzimer, Satyen Kale, and Haipeng Luo.
\newblock Optimal and adaptive algorithms for online boosting.
\newblock In {\em International Conference on Machine Learning}, pages
  2323--2331, 2015.

\bibitem{beygelzimer2011contextual}
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire.
\newblock Contextual bandit algorithms with supervised learning guarantees.
\newblock In {\em Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pages 19--26. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem{brukhim2020online}
Nataly Brukhim, Xinyi Chen, Elad Hazan, and Shay Moran.
\newblock Online agnostic boosting via regret minimization.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{brukhim2021online}
Nataly Brukhim and Elad Hazan.
\newblock Online boosting with bandit feedback.
\newblock In {\em Algorithmic Learning Theory}, pages 397--420. PMLR, 2021.

\bibitem{chen2018projection}
Lin Chen, Christopher Harshaw, Hamed Hassani, and Amin Karbasi.
\newblock Projection-free online optimization with stochastic gradient: From
  convexity to submodularity.
\newblock In {\em International Conference on Machine Learning}, pages
  814--823, 2018.

\bibitem{chen2012online}
Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu.
\newblock An online boosting algorithm with theoretical justifications.
\newblock In {\em Proceedings of the 29th International Coference on
  International Conference on Machine Learning}, pages 1873--1880, 2012.

\bibitem{chen2014boosting}
Shang-Tse Chen, Hsuan-Tien Lin, and Chi-Jen Lu.
\newblock Boosting with online binary learners for the multiclass bandit
  problem.
\newblock In {\em International Conference on Machine Learning}, pages
  342--350, 2014.

\bibitem{duchi2008efficient}
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.
\newblock Efficient projections onto the l 1-ball for learning in high
  dimensions.
\newblock In {\em Proceedings of the 25th international conference on Machine
  learning}, pages 272--279, 2008.

\bibitem{frank1956algorithm}
Marguerite Frank and Philip Wolfe.
\newblock An algorithm for quadratic programming.
\newblock {\em Naval research logistics quarterly}, 3(1-2):95--110, 1956.

\bibitem{hassani2017gradient}
Hamed Hassani, Mahdi Soltanolkotabi, and Amin Karbasi.
\newblock Gradient methods for submodular maximization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5841--5851, 2017.

\bibitem{hazan2019introduction}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock {\em arXiv preprint arXiv:1909.05207}, 2019.

\bibitem{hazan2019provably}
Elad Hazan, Sham Kakade, Karan Singh, and Abby Van~Soest.
\newblock Provably efficient maximum entropy exploration.
\newblock In {\em International Conference on Machine Learning}, pages
  2681--2691. PMLR, 2019.

\bibitem{hazan2021boosting}
Elad Hazan and Karan Singh.
\newblock Boosting for online convex optimization.
\newblock {\em arXiv preprint arXiv:2102.09305}, 2021.

\bibitem{jaggi2013revisiting}
Martin Jaggi.
\newblock Revisiting frank-wolfe: Projection-free sparse convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  427--435. PMLR, 2013.

\bibitem{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143. PMLR, 2020.

\bibitem{jung2017online}
Young~Hun Jung, Jack Goetz, and Ambuj Tewari.
\newblock Online multiclass boosting.
\newblock In {\em Advances in neural information processing systems}, pages
  919--928, 2017.

\bibitem{jung2018online}
Young~Hun Jung and Ambuj Tewari.
\newblock Online boosting algorithms for multi-label ranking.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 279--287, 2018.

\bibitem{kakade2002approximately}
Sham Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In {\em In Proc. 19th International Conference on Machine Learning}.
  Citeseer, 2002.

\bibitem{kanade2009potential}
Varun Kanade and Adam Kalai.
\newblock Potential-based agnostic boosting.
\newblock In {\em Advances in neural information processing systems}, pages
  880--888, 2009.

\bibitem{lazaric2009hybrid}
Alessandro Lazaric and R{\'e}mi Munos.
\newblock Hybrid stochastic-adversarial on-line learning.
\newblock In {\em Conference on Learning Theory}, 2009.

\bibitem{leistner2009robustness}
Christian Leistner, Amir Saffari, Peter~M Roth, and Horst Bischof.
\newblock On robustness of on-line boosting-a competitive study.
\newblock In {\em IEEE 12th International Conference on Computer Vision
  Workshops, ICCV Workshops}, pages 1362--1369. IEEE, 2009.

\bibitem{london2022boosted}
Ben London, Levi Lu, Ted Sandler, and Thorsten Joachims.
\newblock Boosted off-policy learning.
\newblock {\em arXiv preprint arXiv:2208.01148}, 2022.

\bibitem{mokhtari2018stochastic}
Aryan Mokhtari, Hamed Hassani, and Amin Karbasi.
\newblock Stochastic conditional gradient methods: From convex minimization to
  submodular maximization.
\newblock {\em arXiv preprint arXiv:1804.09554}, 2018.

\bibitem{munos2008finite}
R{\'e}mi Munos and Csaba Szepesv{\'a}ri.
\newblock Finite-time bounds for fitted value iteration.
\newblock {\em Journal of Machine Learning Research}, 9(5), 2008.

\bibitem{pedregosa2011scikit}
Fabian Pedregosa, Ga{\"e}l Varoquaux, Alexandre Gramfort, Vincent Michel,
  Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
  Weiss, Vincent Dubourg, et~al.
\newblock Scikit-learn: Machine learning in python.
\newblock {\em the Journal of machine Learning research}, 12:2825--2830, 2011.

\bibitem{rakhlin2011online}
Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari.
\newblock Online learning: Stochastic and constrained adversaries.
\newblock {\em arXiv preprint arXiv:1104.5070}, 2011.

\bibitem{schapire2012boosting}
Robert~E Schapire and Yoav Freund.
\newblock {\em Boosting: Foundations and Algorithms}.
\newblock MIT Press, 2012.

\bibitem{scherrer2014local}
Bruno Scherrer and Matthieu Geist.
\newblock Local policy search in a convex space and conservative policy
  iteration as boosted policy search.
\newblock In {\em Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 35--50. Springer, 2014.

\bibitem{sun2019model}
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Model-based rl in contextual decision processes: Pac bounds and
  exponential improvements over model-free approaches.
\newblock In {\em Conference on learning theory}, pages 2898--2933. PMLR, 2019.

\bibitem{Sutton1999}
Richard~S Sutton, David McAllester, Satinder Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In S.~Solla, T.~Leen, and K.~M\"{u}ller, editors, {\em Advances in
  Neural Information Processing Systems}, volume~12. MIT Press, 2000.

\bibitem{wang2021exponential}
Yuanhao Wang, Ruosong Wang, and Sham~M Kakade.
\newblock An exponential lower bound for linearly-realizable mdps with constant
  suboptimality gap.
\newblock {\em arXiv preprint arXiv:2103.12690}, 2021.

\bibitem{weisz2021query}
Gellert Weisz, Philip Amortila, Barnab{\'a}s Janzer, Yasin Abbasi-Yadkori, Nan
  Jiang, and Csaba Szepesv{\'a}ri.
\newblock On query-efficient planning in mdps under linear realizability of the
  optimal state-value function.
\newblock {\em arXiv preprint arXiv:2102.02049}, 2021.

\bibitem{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 8(3-4):229--256, 1992.

\bibitem{xie2019stochastic}
Jiahao Xie, Zebang Shen, Chao Zhang, Hui Qian, and Boyu Wang.
\newblock Stochastic recursive gradient-based methods for projection-free
  online learning.
\newblock {\em arXiv preprint arXiv:1910.09396}, 2019.

\end{thebibliography}
