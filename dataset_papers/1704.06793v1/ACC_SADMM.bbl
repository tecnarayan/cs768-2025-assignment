\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu(2017)]{Katyusha}
Allen-Zhu, Zeyuan.
\newblock Katyusha: The first truly accelerated stochastic gradient method.
\newblock In \emph{Annual Symposium on the Theory of Computing}, 2017.

\bibitem[Argyriou et~al.(2007)Argyriou, Evgeniou, and Pontil]{multitask}
Argyriou, Andreas, Evgeniou, Theodoros, and Pontil, Massimiliano.
\newblock Multi-task feature learning.
\newblock \emph{Proc. Conf. Advances in Neural Information Processing Systems},
  2007.

\bibitem[AzadiSra \& Sra(2014)AzadiSra and Sra]{OPT-SADMM}
AzadiSra, Samaneh and Sra, Suvrit.
\newblock Towards an optimal stochastic alternating direction method of
  multipliers.
\newblock In \emph{Proc. Int'l. Conf. on Machine Learning}, 2014.

\bibitem[Beck \& Teboulle(2009)Beck and Teboulle]{FISTA}
Beck, Amir and Teboulle, Marc.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock \emph{SIAM Journal on Imaging Sciences}, 2\penalty0 (1):\penalty0
  183--202, 2009.

\bibitem[Bottou(2004)]{bottou2004stochastic}
Bottou, L{\'e}on.
\newblock Stochastic learning.
\newblock In \emph{Advanced lectures on machine learning}, pp.\  146--168.
  2004.

\bibitem[Boyd et~al.(2011)Boyd, Parikh, Chu, Peleato, and
  Eckstein]{boyd2011distributed}
Boyd, Stephen, Parikh, Neal, Chu, Eric, Peleato, Borja, and Eckstein, Jonathan.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  3\penalty0 (1):\penalty0 1--122, 2011.

\bibitem[Chen et~al.(2015)Chen, Chan, Ma, and Yang]{chen2015inertial}
Chen, Caihua, Chan, Raymond~H, Ma, Shiqian, and Yang, Junfeng.
\newblock Inertial proximal {ADMM} for linearly constrained separable convex
  optimization.
\newblock \emph{SIAM Journal on Imaging Sciences}, 8\penalty0 (4):\penalty0
  2239--2267, 2015.

\bibitem[Davis \& Yin(2016)Davis and Yin]{ADMM}
Davis, Damek and Yin, Wotao.
\newblock Convergence rate analysis of several splitting schemes.
\newblock In \emph{Splitting Methods in Communication, Imaging, Science, and
  Engineering}, pp.\  115--163. 2016.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{SAGA}
Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Proc. Conf. Advances in Neural Information Processing
  Systems}, 2014.

\bibitem[Friedman et~al.(2008)Friedman, Hastie, and
  Tibshirani]{friedman2008sparse}
Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
\newblock Sparse inverse covariance estimation with the graphical lasso.
\newblock \emph{Biostatistics}, 9\penalty0 (3):\penalty0 432--441, 2008.

\bibitem[Frostig et~al.(2015)Frostig, Ge, Kakade, and
  Sidford]{frostig2015regularizing}
Frostig, Roy, Ge, Rong, Kakade, Sham, and Sidford, Aaron.
\newblock Un-regularizing: approximate proximal point and faster stochastic
  algorithms for empirical risk minimization.
\newblock In \emph{Proc. Int'l. Conf. on Machine Learning}, 2015.

\bibitem[He \& Yuan(2012)He and Yuan]{he20121}
He, Bingsheng and Yuan, Xiaoming.
\newblock On the ${O}(1/n)$ convergence rate of the {Douglas--Rachford}
  alternating direction method.
\newblock \emph{SIAM Journal on Numerical Analysis}, 50\penalty0 (2):\penalty0
  700--709, 2012.

\bibitem[Hien et~al.(2016)Hien, Lu, Xu, and Feng]{hien2016accelerated}
Hien, Le Thi~Khanh, Lu, Canyi, Xu, Huan, and Feng, Jiashi.
\newblock Accelerated stochastic mirror descent algorithms for composite
  non-strongly convex optimization.
\newblock \emph{arXiv preprint arXiv:1605.06892}, 2016.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{SVRG}
Johnson, Rie and Zhang, Tong.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Proc. Conf. Advances in Neural Information Processing
  Systems}, 2013.

\bibitem[Kim et~al.(2009)Kim, Sohn, and Xing]{lasso}
Kim, Seyoung, Sohn, Kyung-Ah, and Xing, Eric~P.
\newblock A multivariate regression approach to association analysis of a
  quantitative trait network.
\newblock \emph{Bioinformatics}, 25\penalty0 (12):\penalty0 i204--i212, 2009.

\bibitem[Li \& Lin(2016)Li and Lin]{LADM-NE}
Li, Huan and Lin, Zhouchen.
\newblock Optimal nonergodic ${O}(1/k)$ convergence rate: When linearized {ADM}
  meets nesterov's extrapolation.
\newblock \emph{arXiv preprint arXiv:1608.06366}, 2016.

\bibitem[Lin et~al.(2015{\natexlab{a}})Lin, Mairal, and
  Harchaoui]{lin2015universal}
Lin, Hongzhou, Mairal, Julien, and Harchaoui, Zaid.
\newblock A universal catalyst for first-order optimization.
\newblock In \emph{Proc. Conf. Advances in Neural Information Processing
  Systems}, 2015{\natexlab{a}}.

\bibitem[Lin et~al.(2011)Lin, Liu, and Su]{lin2011linearized}
Lin, Zhouchen, Liu, Risheng, and Su, Zhixun.
\newblock Linearized alternating direction method with adaptive penalty for
  low-rank representation.
\newblock In \emph{Proc. Conf. Advances in Neural Information Processing
  Systems}, 2011.

\bibitem[Lin et~al.(2015{\natexlab{b}})Lin, Liu, and Li]{LADMPSAP}
Lin, Zhouchen, Liu, Risheng, and Li, Huan.
\newblock Linearized alternating direction method with parallel splitting and
  adaptive penalty for separable convex programs in machine learning.
\newblock \emph{Machine Learning}, 99\penalty0 (2):\penalty0 287--325,
  2015{\natexlab{b}}.

\bibitem[Lu et~al.(2016)Lu, Li, Lin, and Yan]{lu2015fast}
Lu, Canyi, Li, Huan, Lin, Zhouchen, and Yan, Shuicheng.
\newblock Fast proximal linearized alternating direction method of multiplier
  with parallel splitting.
\newblock In \emph{Proc. AAAI Conf. on Artificial Intelligence}, 2016.

\bibitem[Nesterov(1983)]{nesterov1983method}
Nesterov, Yurii.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence ${O}(1/k^2)$.
\newblock In \emph{Doklady an SSSR}, volume 269, pp.\  543--547, 1983.

\bibitem[Nesterov(1988)]{nesterov1988approach}
Nesterov, Yurii.
\newblock On an approach to the construction of optimal methods of minimization
  of smooth convex functions.
\newblock \emph{Ekonomika i Mateaticheskie Metody}, 24\penalty0 (3):\penalty0
  509--517, 1988.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Nesterov, Yurii.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock 2013.

\bibitem[Nitanda(2014)]{nitanda2014stochastic}
Nitanda, Atsushi.
\newblock Stochastic proximal gradient descent with acceleration techniques.
\newblock In \emph{Proc. Conf. Advances in Neural Information Processing
  Systems}, 2014.

\bibitem[Ouyang et~al.(2013)Ouyang, He, Tran, and Gray]{STOC-ADMM}
Ouyang, Hua, He, Niao, Tran, Long, and Gray, Alexander~G.
\newblock Stochastic alternating direction method of multipliers.
\newblock \emph{Proc. Int'l. Conf. on Machine Learning}, 2013.

\bibitem[Ouyang et~al.(2015)Ouyang, Chen, Lan, and
  Pasiliao~Jr]{ouyang2015accelerated}
Ouyang, Yuyuan, Chen, Yunmei, Lan, Guanghui, and Pasiliao~Jr, Eduardo.
\newblock An accelerated linearized alternating direction method of
  multipliers.
\newblock \emph{SIAM Journal on Imaging Sciences}, 8\penalty0 (1):\penalty0
  644--681, 2015.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma,
  Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael,
  et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{Int'l. Journal of Computer Vision}, 115\penalty0 (3):\penalty0
  211--252, 2015.

\bibitem[Schmidt et~al.(2013)Schmidt, Le~Roux, and Bach]{SAG}
Schmidt, Mark, Le~Roux, Nicolas, and Bach, Francis.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, pp.\  1--30, 2013.

\bibitem[Shen et~al.(2015)Shen, Sun, Lin, Huang, and Wu]{shenli}
Shen, Li, Sun, Gang, Lin, Zhouchen, Huang, Qingming, and Wu, Enhua.
\newblock Adaptive sharing for image classification.
\newblock In \emph{Proc. Int'l. Joint Conf. on Artificial Intelligence}, 2015.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, Karen and Zisserman, Andrew.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Suzuki(2013)]{OPG-ADMM}
Suzuki, Taiji.
\newblock Dual averaging and proximal gradient descent for online alternating
  direction multiplier method.
\newblock In \emph{Proc. Int'l. Conf. on Machine Learning}, 2013.

\bibitem[Suzuki(2014)]{SDCA-ADMM}
Suzuki, Taiji.
\newblock Stochastic dual coordinate ascent with alternating direction method
  of multipliers.
\newblock In \emph{Proc. Int'l. Conf. on Machine Learning}, 2014.

\bibitem[Tseng(2008)]{Tseng}
Tseng, Paul.
\newblock On accelerated proximal gradient methods for convex-concave
  optimization.
\newblock In \emph{Technical report}, 2008.

\bibitem[Wang et~al.(2016)Wang, He, Wang, Wang, and Tan]{Wang2016Joint}
Wang, Kaiye, He, Ran, Wang, Liang, Wang, Wei, and Tan, Tieniu.
\newblock Joint feature selection and subspace learning for cross-modal
  retrieval.
\newblock \emph{IEEE Trans. on Pattern Analysis and Machine Intelligence},
  38\penalty0 (10):\penalty0 1--1, 2016.

\bibitem[Zhang et~al.(2011)Zhang, Burger, and Osher]{zhang2011unified}
Zhang, Xiaoqun, Burger, Martin, and Osher, Stanley.
\newblock A unified primal-dual algorithm framework based on bregman iteration.
\newblock \emph{Journal of Scientific Computing}, 46:\penalty0 20--46, 2011.

\bibitem[Zheng \& Kwok(2016)Zheng and Kwok]{SVRG-ADMM}
Zheng, Shuai and Kwok, James~T.
\newblock Fast-and-light stochastic admm.
\newblock In \emph{Proc. Int'l. Joint Conf. on Artificial Intelligence}, 2016.

\bibitem[Zhong \& Kwok(2014)Zhong and Kwok]{SAG-ADMM}
Zhong, Wenliang and Kwok, James Tin-Yau.
\newblock Fast stochastic alternating direction method of multipliers.
\newblock In \emph{Proc. Int'l. Conf. on Machine Learning}, 2014.

\bibitem[Zuo \& Lin(2011)Zuo and Lin]{Zuo2011A}
Zuo, Wangmeng and Lin, Zhouchen.
\newblock A generalized accelerated proximal gradient approach for total
  variation-based image restoration.
\newblock \emph{IEEE Trans. on Image Processing}, 20\penalty0 (10):\penalty0
  2748, 2011.

\end{thebibliography}
