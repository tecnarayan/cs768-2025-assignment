\begin{thebibliography}{97}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2010)Agarwal, Negahban, and
  Wainwright]{agarwal2010fast}
A.~Agarwal, S.~Negahban, and M.~J. Wainwright.
\newblock Fast global convergence rates of gradient methods for
  high-dimensional statistical recovery.
\newblock \emph{Advances in Neural Information Processing Systems}, 23, 2010.

\bibitem[Aky{\"u}rek et~al.(2022)Aky{\"u}rek, Schuurmans, Andreas, Ma, and
  Zhou]{akyurek2022learning}
E.~Aky{\"u}rek, D.~Schuurmans, J.~Andreas, T.~Ma, and D.~Zhou.
\newblock What learning algorithm is in-context learning? investigations with
  linear models.
\newblock \emph{arXiv preprint arXiv:2211.15661}, 2022.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
J.~L. Ba, J.~R. Kiros, and G.~E. Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bach(2017)]{bach2017breaking}
F.~Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 629--681, 2017.

\bibitem[Bai et~al.(2021{\natexlab{a}})Bai, Chen, Zhou, Zhao, Lee, Kakade,
  Wang, and Xiong]{bai2021important}
Y.~Bai, M.~Chen, P.~Zhou, T.~Zhao, J.~Lee, S.~Kakade, H.~Wang, and C.~Xiong.
\newblock How important is the train-validation split in meta-learning?
\newblock In \emph{International Conference on Machine Learning}, pages
  543--553. PMLR, 2021{\natexlab{a}}.

\bibitem[Bai et~al.(2021{\natexlab{b}})Bai, Mei, Wang, and Xiong]{bai2021don}
Y.~Bai, S.~Mei, H.~Wang, and C.~Xiong.
\newblock Donâ€™t just blame over-parametrization for over-confidence:
  Theoretical analysis of calibration in binary classification.
\newblock In \emph{International Conference on Machine Learning}, pages
  566--576. PMLR, 2021{\natexlab{b}}.

\bibitem[Baxter(2000)]{baxter2000model}
J.~Baxter.
\newblock A model of inductive bias learning.
\newblock \emph{Journal of artificial intelligence research}, 12:\penalty0
  149--198, 2000.

\bibitem[Beck and Teboulle(2009)]{beck2009gradient}
A.~Beck and M.~Teboulle.
\newblock Gradient-based algorithms with applications to signal recovery.
\newblock \emph{Convex optimization in signal processing and communications},
  pages 42--88, 2009.

\bibitem[Bengio et~al.(2013)Bengio, Bengio, Cloutier, and
  Gescei]{bengio2013optimization}
S.~Bengio, Y.~Bengio, J.~Cloutier, and J.~Gescei.
\newblock On the optimization of a synaptic learning rule.
\newblock In \emph{Optimality in Biological and Artificial Networks?}, pages
  281--303. Routledge, 2013.

\bibitem[Bhattamishra et~al.(2020{\natexlab{a}})Bhattamishra, Ahuja, and
  Goyal]{bhattamishra2020ability}
S.~Bhattamishra, K.~Ahuja, and N.~Goyal.
\newblock On the ability and limitations of transformers to recognize formal
  languages.
\newblock \emph{arXiv preprint arXiv:2009.11264}, 2020{\natexlab{a}}.

\bibitem[Bhattamishra et~al.(2020{\natexlab{b}})Bhattamishra, Patel, and
  Goyal]{bhattamishra2020computational}
S.~Bhattamishra, A.~Patel, and N.~Goyal.
\newblock On the computational power of transformers and its implications in
  sequence modeling.
\newblock \emph{arXiv preprint arXiv:2006.09286}, 2020{\natexlab{b}}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Bubeck(2015)]{bubeck2015convex}
S.~Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,
  Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
S.~Bubeck, V.~Chandrasekaran, R.~Eldan, J.~Gehrke, E.~Horvitz, E.~Kamar,
  P.~Lee, Y.~T. Lee, Y.~Li, S.~Lundberg, et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Chan et~al.(2022)Chan, Santoro, Lampinen, Wang, Singh, Richemond,
  McClelland, and Hill]{chan2022data}
S.~Chan, A.~Santoro, A.~Lampinen, J.~Wang, A.~Singh, P.~Richemond,
  J.~McClelland, and F.~Hill.
\newblock Data distributional properties drive emergent in-context learning in
  transformers.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 18878--18891, 2022.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{chen2021decision}
L.~Chen, K.~Lu, A.~Rajeswaran, K.~Lee, A.~Grover, M.~Laskin, P.~Abbeel,
  A.~Srinivas, and I.~Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 15084--15097, 2021.

\bibitem[Chua et~al.(2021)Chua, Lei, and Lee]{chua2021fine}
K.~Chua, Q.~Lei, and J.~D. Lee.
\newblock How fine-tuning allows for effective meta-learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 8871--8884, 2021.

\bibitem[Dai et~al.(2022)Dai, Sun, Dong, Hao, Sui, and Wei]{dai2022can}
D.~Dai, Y.~Sun, L.~Dong, Y.~Hao, Z.~Sui, and F.~Wei.
\newblock Why can gpt learn in-context? language models secretly perform
  gradient descent as meta optimizers.
\newblock \emph{arXiv preprint arXiv:2212.10559}, 2022.

\bibitem[Denevi et~al.(2018{\natexlab{a}})Denevi, Ciliberto, Stamos, and
  Pontil]{denevi2018incremental}
G.~Denevi, C.~Ciliberto, D.~Stamos, and M.~Pontil.
\newblock Incremental learning-to-learn with statistical guarantees.
\newblock \emph{arXiv preprint arXiv:1803.08089}, 2018{\natexlab{a}}.

\bibitem[Denevi et~al.(2018{\natexlab{b}})Denevi, Ciliberto, Stamos, and
  Pontil]{denevi2018learning}
G.~Denevi, C.~Ciliberto, D.~Stamos, and M.~Pontil.
\newblock Learning to learn around a common mean.
\newblock \emph{Advances in Neural Information Processing Systems}, 31,
  2018{\natexlab{b}}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dobriban and Wager(2018)]{dobriban2018high}
E.~Dobriban and S.~Wager.
\newblock High-dimensional asymptotics of prediction: Ridge regression and
  classification.
\newblock \emph{The Annals of Statistics}, 46\penalty0 (1):\penalty0 247--279,
  2018.

\bibitem[Dong et~al.(2018)Dong, Xu, and Xu]{dong2018speech}
L.~Dong, S.~Xu, and B.~Xu.
\newblock Speech-transformer: a no-recurrence sequence-to-sequence model for
  speech recognition.
\newblock In \emph{2018 IEEE international conference on acoustics, speech and
  signal processing (ICASSP)}, pages 5884--5888. IEEE, 2018.

\bibitem[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and
  Sui]{dong2022survey}
Q.~Dong, L.~Li, D.~Dai, C.~Zheng, Z.~Wu, B.~Chang, X.~Sun, J.~Xu, and Z.~Sui.
\newblock A survey for in-context learning.
\newblock \emph{arXiv preprint arXiv:2301.00234}, 2022.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Du et~al.(2020)Du, Hu, Kakade, Lee, and Lei]{du2020few}
S.~S. Du, W.~Hu, S.~M. Kakade, J.~D. Lee, and Q.~Lei.
\newblock Few-shot learning via learning the representation, provably.
\newblock \emph{arXiv preprint arXiv:2002.09434}, 2020.

\bibitem[Edelman et~al.(2022)Edelman, Goel, Kakade, and
  Zhang]{edelman2022inductive}
B.~L. Edelman, S.~Goel, S.~Kakade, and C.~Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In \emph{International Conference on Machine Learning}, pages
  5793--5831. PMLR, 2022.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann,
  Askell, Bai, Chen, Conerly, et~al.]{elhage2021mathematical}
N.~Elhage, N.~Nanda, C.~Olsson, T.~Henighan, N.~Joseph, B.~Mann, A.~Askell,
  Y.~Bai, A.~Chen, T.~Conerly, et~al.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
C.~Finn, P.~Abbeel, and S.~Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International conference on machine learning}, pages
  1126--1135. PMLR, 2017.

\bibitem[Finn et~al.(2019)Finn, Rajeswaran, Kakade, and Levine]{finn2019online}
C.~Finn, A.~Rajeswaran, S.~Kakade, and S.~Levine.
\newblock Online meta-learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1920--1930. PMLR, 2019.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022can}
S.~Garg, D.~Tsipras, P.~S. Liang, and G.~Valiant.
\newblock What can transformers learn in-context? a case study of simple
  function classes.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 30583--30598, 2022.

\bibitem[Giannou et~al.(2023)Giannou, Rajput, Sohn, Lee, Lee, and
  Papailiopoulos]{giannou2023looped}
A.~Giannou, S.~Rajput, J.-y. Sohn, K.~Lee, J.~D. Lee, and D.~Papailiopoulos.
\newblock Looped transformers as programmable computers.
\newblock \emph{arXiv preprint arXiv:2301.13196}, 2023.

\bibitem[Hahn(2020)]{hahn2020theoretical}
M.~Hahn.
\newblock Theoretical limitations of self-attention in neural sequence models.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 156--171, 2020.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and
  Conwell]{hochreiter2001learning}
S.~Hochreiter, A.~S. Younger, and P.~R. Conwell.
\newblock Learning to learn using gradient descent.
\newblock In \emph{Artificial Neural Networksâ€”ICANN 2001: International
  Conference Vienna, Austria, August 21--25, 2001 Proceedings 11}, pages
  87--94. Springer, 2001.

\bibitem[Hospedales et~al.(2021)Hospedales, Antoniou, Micaelli, and
  Storkey]{hospedales2021meta}
T.~Hospedales, A.~Antoniou, P.~Micaelli, and A.~Storkey.
\newblock Meta-learning in neural networks: A survey.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 44\penalty0 (9):\penalty0 5149--5169, 2021.

\bibitem[Hron et~al.(2020)Hron, Bahri, Sohl-Dickstein, and
  Novak]{hron2020infinite}
J.~Hron, Y.~Bahri, J.~Sohl-Dickstein, and R.~Novak.
\newblock Infinite attention: Nngp and ntk for deep attention networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  4376--4386. PMLR, 2020.

\bibitem[Hsu et~al.(2012)Hsu, Kakade, and Zhang]{hsu2012random}
D.~Hsu, S.~M. Kakade, and T.~Zhang.
\newblock Random design analysis of ridge regression.
\newblock In \emph{Conference on learning theory}, pages 9--1. JMLR Workshop
  and Conference Proceedings, 2012.

\bibitem[Jelassi et~al.(2022)Jelassi, Sander, and Li]{jelassi2022vision}
S.~Jelassi, M.~E. Sander, and Y.~Li.
\newblock Vision transformers provably learn spatial structure.
\newblock \emph{arXiv preprint arXiv:2210.09221}, 2022.

\bibitem[Ji et~al.(2020)Ji, Lee, Liang, and Poor]{ji2020convergence}
K.~Ji, J.~D. Lee, Y.~Liang, and H.~V. Poor.
\newblock Convergence of meta-learning with task-specific adaptation over
  partial parameters.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 11490--11500, 2020.

\bibitem[Jumper et~al.(2021)Jumper, Evans, Pritzel, Green, Figurnov,
  Ronneberger, Tunyasuvunakool, Bates, {\v{Z}}{\'\i}dek, Potapenko,
  et~al.]{jumper2021highly}
J.~Jumper, R.~Evans, A.~Pritzel, T.~Green, M.~Figurnov, O.~Ronneberger,
  K.~Tunyasuvunakool, R.~Bates, A.~{\v{Z}}{\'\i}dek, A.~Potapenko, et~al.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock \emph{Nature}, 596\penalty0 (7873):\penalty0 583--589, 2021.

\bibitem[Kakade et~al.(2011)Kakade, Kanade, Shamir, and
  Kalai]{kakade2011efficient}
S.~M. Kakade, V.~Kanade, O.~Shamir, and A.~Kalai.
\newblock Efficient learning of generalized linear and single index models with
  isotonic regression.
\newblock \emph{Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem[Khodak et~al.(2019)Khodak, Balcan, and Talwalkar]{khodak2019adaptive}
M.~Khodak, M.-F.~F. Balcan, and A.~S. Talwalkar.
\newblock Adaptive gradient-based meta-learning methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Kirsch and Schmidhuber(2021)]{kirsch2021meta}
L.~Kirsch and J.~Schmidhuber.
\newblock Meta learning backpropagation and improving it.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 14122--14134, 2021.

\bibitem[Kirsch et~al.(2022)Kirsch, Harrison, Sohl-Dickstein, and
  Metz]{kirsch2022general}
L.~Kirsch, J.~Harrison, J.~Sohl-Dickstein, and L.~Metz.
\newblock General-purpose in-context learning by meta-learning transformers.
\newblock \emph{arXiv preprint arXiv:2212.04458}, 2022.

\bibitem[Li and Malik(2016)]{li2016learning}
K.~Li and J.~Malik.
\newblock Learning to optimize.
\newblock \emph{arXiv preprint arXiv:1606.01885}, 2016.

\bibitem[Li et~al.(2023)Li, Ildiz, Papailiopoulos, and
  Oymak]{li2023transformers}
Y.~Li, M.~E. Ildiz, D.~Papailiopoulos, and S.~Oymak.
\newblock Transformers as algorithms: Generalization and implicit model
  selection in in-context learning.
\newblock \emph{arXiv preprint arXiv:2301.07067}, 2023.

\bibitem[Liu et~al.(2022)Liu, Ash, Goel, Krishnamurthy, and
  Zhang]{liu2022transformers}
B.~Liu, J.~T. Ash, S.~Goel, A.~Krishnamurthy, and C.~Zhang.
\newblock Transformers learn shortcuts to automata.
\newblock \emph{arXiv preprint arXiv:2210.10749}, 2022.

\bibitem[Liu et~al.(2021)Liu, Shen, Zhang, Dolan, Carin, and
  Chen]{liu2021makes}
J.~Liu, D.~Shen, Y.~Zhang, B.~Dolan, L.~Carin, and W.~Chen.
\newblock What makes good in-context examples for gpt-$3 $?
\newblock \emph{arXiv preprint arXiv:2101.06804}, 2021.

\bibitem[Lu et~al.(2021)Lu, Bartolo, Moore, Riedel, and
  Stenetorp]{lu2021fantastically}
Y.~Lu, M.~Bartolo, A.~Moore, S.~Riedel, and P.~Stenetorp.
\newblock Fantastically ordered prompts and where to find them: Overcoming
  few-shot prompt order sensitivity.
\newblock \emph{arXiv preprint arXiv:2104.08786}, 2021.

\bibitem[Madani et~al.(2020)Madani, McCann, Naik, Keskar, Anand, Eguchi, Huang,
  and Socher]{madani2020progen}
A.~Madani, B.~McCann, N.~Naik, N.~S. Keskar, N.~Anand, R.~R. Eguchi, P.-S.
  Huang, and R.~Socher.
\newblock Progen: Language modeling for protein generation.
\newblock \emph{arXiv preprint arXiv:2004.03497}, 2020.

\bibitem[Maurer et~al.(2016)Maurer, Pontil, and
  Romera-Paredes]{maurer2016benefit}
A.~Maurer, M.~Pontil, and B.~Romera-Paredes.
\newblock The benefit of multitask representation learning.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (81):\penalty0 1--32, 2016.

\bibitem[McCullagh(2019)]{mccullagh2019generalized}
P.~McCullagh.
\newblock \emph{Generalized linear models}.
\newblock Routledge, 2019.

\bibitem[Mei et~al.(2018)Mei, Bai, and Montanari]{mei2018landscape}
S.~Mei, Y.~Bai, and A.~Montanari.
\newblock The landscape of empirical risk for nonconvex losses.
\newblock \emph{The Annals of Statistics}, 46\penalty0 (6A):\penalty0
  2747--2774, 2018.

\bibitem[Min et~al.(2021{\natexlab{a}})Min, Lewis, Hajishirzi, and
  Zettlemoyer]{min2021noisy}
S.~Min, M.~Lewis, H.~Hajishirzi, and L.~Zettlemoyer.
\newblock Noisy channel language model prompting for few-shot text
  classification.
\newblock \emph{arXiv preprint arXiv:2108.04106}, 2021{\natexlab{a}}.

\bibitem[Min et~al.(2021{\natexlab{b}})Min, Lewis, Zettlemoyer, and
  Hajishirzi]{min2021metaicl}
S.~Min, M.~Lewis, L.~Zettlemoyer, and H.~Hajishirzi.
\newblock Metaicl: Learning to learn in context.
\newblock \emph{arXiv preprint arXiv:2110.15943}, 2021{\natexlab{b}}.

\bibitem[Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and
  Zettlemoyer]{min2022rethinking}
S.~Min, X.~Lyu, A.~Holtzman, M.~Artetxe, M.~Lewis, H.~Hajishirzi, and
  L.~Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning
  work?
\newblock \emph{arXiv preprint arXiv:2202.12837}, 2022.

\bibitem[Mishra et~al.(2017)Mishra, Rohaninejad, Chen, and
  Abbeel]{mishra2017simple}
N.~Mishra, M.~Rohaninejad, X.~Chen, and P.~Abbeel.
\newblock A simple neural attentive meta-learner.
\newblock \emph{arXiv preprint arXiv:1707.03141}, 2017.

\bibitem[Naik and Mammone(1992)]{naik1992meta}
D.~K. Naik and R.~J. Mammone.
\newblock Meta-neural networks that learn by learning.
\newblock In \emph{[Proceedings 1992] IJCNN International Joint Conference on
  Neural Networks}, volume~1, pages 437--442. IEEE, 1992.

\bibitem[Negahban et~al.(2012)Negahban, Ravikumar, Wainwright, and
  Yu]{negahban2012unified}
S.~N. Negahban, P.~Ravikumar, M.~J. Wainwright, and B.~Yu.
\newblock A unified framework for high-dimensional analysis of m-estimators
  with decomposable regularizers.
\newblock 2012.

\bibitem[Nesterov(2018)]{nesterov2018lectures}
Y.~Nesterov.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan,
  Mann, Askell, Bai, Chen, et~al.]{olsson2022context}
C.~Olsson, N.~Elhage, N.~Nanda, N.~Joseph, N.~DasSarma, T.~Henighan, B.~Mann,
  A.~Askell, Y.~Bai, A.~Chen, et~al.
\newblock In-context learning and induction heads.
\newblock \emph{arXiv preprint arXiv:2209.11895}, 2022.

\bibitem[OpenAI(2023)]{openai2023gpt}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Parikh et~al.(2014)Parikh, Boyd, et~al.]{parikh2014proximal}
N.~Parikh, S.~Boyd, et~al.
\newblock Proximal algorithms.
\newblock \emph{Foundations and trends{\textregistered} in Optimization},
  1\penalty0 (3):\penalty0 127--239, 2014.

\bibitem[P{\'e}rez et~al.(2019)P{\'e}rez, Marinkovi{\'c}, and
  Barcel{\'o}]{perez2019turing}
J.~P{\'e}rez, J.~Marinkovi{\'c}, and P.~Barcel{\'o}.
\newblock On the turing completeness of modern neural network architectures.
\newblock \emph{arXiv preprint arXiv:1901.03429}, 2019.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever,
  et~al.]{radford2018improving}
A.~Radford, K.~Narasimhan, T.~Salimans, I.~Sutskever, et~al.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International conference on machine learning}, pages
  8748--8763. PMLR, 2021.

\bibitem[Raventos et~al.(2023)Raventos, Paul, Chen, and
  Ganguli]{raventos2023effects}
A.~Raventos, M.~Paul, F.~Chen, and S.~Ganguli.
\newblock The effects of pretraining task diversity on in-context learning of
  ridge regression.
\newblock In \emph{ICLR 2023 Workshop on Mathematical and Empirical
  Understanding of Foundation Models}, 2023.

\bibitem[Ravi and Larochelle(2017)]{ravi2017optimization}
S.~Ravi and H.~Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock In \emph{International conference on learning representations}, 2017.

\bibitem[Razeghi et~al.(2022)Razeghi, Logan~IV, Gardner, and
  Singh]{razeghi2022impact}
Y.~Razeghi, R.~L. Logan~IV, M.~Gardner, and S.~Singh.
\newblock Impact of pretraining term frequencies on few-shot reasoning.
\newblock \emph{arXiv preprint arXiv:2202.07206}, 2022.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov,
  Barth-Maron, Gimenez, Sulsky, Kay, Springenberg, et~al.]{reed2022generalist}
S.~Reed, K.~Zolna, E.~Parisotto, S.~G. Colmenarejo, A.~Novikov, G.~Barth-Maron,
  M.~Gimenez, Y.~Sulsky, J.~Kay, J.~T. Springenberg, et~al.
\newblock A generalist agent.
\newblock \emph{arXiv preprint arXiv:2205.06175}, 2022.

\bibitem[Rubin et~al.(2021)Rubin, Herzig, and Berant]{rubin2021learning}
O.~Rubin, J.~Herzig, and J.~Berant.
\newblock Learning to retrieve prompts for in-context learning.
\newblock \emph{arXiv preprint arXiv:2112.08633}, 2021.

\bibitem[Santoro et~al.(2016)Santoro, Bartunov, Botvinick, Wierstra, and
  Lillicrap]{santoro2016meta}
A.~Santoro, S.~Bartunov, M.~Botvinick, D.~Wierstra, and T.~Lillicrap.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In \emph{International conference on machine learning}, pages
  1842--1850. PMLR, 2016.

\bibitem[Saunshi et~al.(2021)Saunshi, Gupta, and Hu]{saunshi2021representation}
N.~Saunshi, A.~Gupta, and W.~Hu.
\newblock A representation learning perspective on the importance of
  train-validation splitting in meta-learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  9333--9343. PMLR, 2021.

\bibitem[Schmidhuber(1987)]{schmidhuber1987evolutionary}
J.~Schmidhuber.
\newblock \emph{Evolutionary principles in self-referential learning, or on
  learning how to learn: the meta-meta-... hook}.
\newblock PhD thesis, Technische Universit{\"a}t M{\"u}nchen, 1987.

\bibitem[Shen et~al.(2023)Shen, Guo, Tan, Tang, Wang, and Bian]{shen2023study}
K.~Shen, J.~Guo, X.~Tan, S.~Tang, R.~Wang, and J.~Bian.
\newblock A study on relu and softmax in transformer.
\newblock \emph{arXiv preprint arXiv:2302.06461}, 2023.

\bibitem[Snell et~al.(2021)Snell, Zhong, Klein, and
  Steinhardt]{snell2021approximating}
C.~Snell, R.~Zhong, D.~Klein, and J.~Steinhardt.
\newblock Approximating how single head attention learns.
\newblock \emph{arXiv preprint arXiv:2103.07601}, 2021.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
J.~Snell, K.~Swersky, and R.~Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Thrun and Pratt(2012)]{thrun2012learning}
S.~Thrun and L.~Pratt.
\newblock \emph{Learning to learn}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Tibshirani(1996)]{tibshirani1996regression}
R.~Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 58\penalty0 (1):\penalty0 267--288, 1996.

\bibitem[Tripuraneni et~al.(2020)Tripuraneni, Jordan, and
  Jin]{tripuraneni2020theory}
N.~Tripuraneni, M.~Jordan, and C.~Jin.
\newblock On the theory of transfer learning: The importance of task diversity.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 7852--7862, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Vershynin(2018)]{vershynin2018high}
R.~Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[von Oswald et~al.(2022)von Oswald, Niklasson, Randazzo, Sacramento,
  Mordvintsev, Zhmoginov, and Vladymyrov]{von2022transformers}
J.~von Oswald, E.~Niklasson, E.~Randazzo, J.~Sacramento, A.~Mordvintsev,
  A.~Zhmoginov, and M.~Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock \emph{arXiv preprint arXiv:2212.07677}, 2022.

\bibitem[Wainwright(2019)]{wainwright2019high}
M.~J. Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge university press, 2019.

\bibitem[Wang et~al.(2021)Wang, Yuan, Wu, and Ge]{wang2021guarantees}
X.~Wang, S.~Yuan, C.~Wu, and R.~Ge.
\newblock Guarantees for tuning the step size using a learning-to-learn
  approach.
\newblock In \emph{International Conference on Machine Learning}, pages
  10981--10990. PMLR, 2021.

\bibitem[Wei et~al.(2021)Wei, Chen, and Ma]{wei2021statistically}
C.~Wei, Y.~Chen, and T.~Ma.
\newblock Statistically meaningful approximation: a case study on approximating
  turing machines with transformers.
\newblock \emph{arXiv preprint arXiv:2107.13163}, 2021.

\bibitem[Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama,
  Bosma, Zhou, Metzler, et~al.]{wei2022emergent}
J.~Wei, Y.~Tay, R.~Bommasani, C.~Raffel, B.~Zoph, S.~Borgeaud, D.~Yogatama,
  M.~Bosma, D.~Zhou, D.~Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock \emph{arXiv preprint arXiv:2206.07682}, 2022.

\bibitem[Wei et~al.(2023)Wei, Wei, Tay, Tran, Webson, Lu, Chen, Liu, Huang,
  Zhou, et~al.]{wei2023larger}
J.~Wei, J.~Wei, Y.~Tay, D.~Tran, A.~Webson, Y.~Lu, X.~Chen, H.~Liu, D.~Huang,
  D.~Zhou, et~al.
\newblock Larger language models do in-context learning differently.
\newblock \emph{arXiv preprint arXiv:2303.03846}, 2023.

\bibitem[Weiss et~al.(2021)Weiss, Goldberg, and Yahav]{weiss2021thinking}
G.~Weiss, Y.~Goldberg, and E.~Yahav.
\newblock Thinking like transformers.
\newblock In \emph{International Conference on Machine Learning}, pages
  11080--11090. PMLR, 2021.

\bibitem[Xie et~al.(2021)Xie, Raghunathan, Liang, and Ma]{xie2021explanation}
S.~M. Xie, A.~Raghunathan, P.~Liang, and T.~Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock \emph{arXiv preprint arXiv:2111.02080}, 2021.

\bibitem[Yao et~al.(2021)Yao, Peng, Papadimitriou, and Narasimhan]{yao2021self}
S.~Yao, B.~Peng, C.~Papadimitriou, and K.~Narasimhan.
\newblock Self-attention networks can process bounded hierarchical languages.
\newblock \emph{arXiv preprint arXiv:2105.11115}, 2021.

\bibitem[Ying et~al.(2021)Ying, Cai, Luo, Zheng, Ke, He, Shen, and
  Liu]{ying2021transformers}
C.~Ying, T.~Cai, S.~Luo, S.~Zheng, G.~Ke, D.~He, Y.~Shen, and T.-Y. Liu.
\newblock Do transformers really perform badly for graph representation?
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 28877--28888, 2021.

\bibitem[Yun et~al.(2019)Yun, Bhojanapalli, Rawat, Reddi, and
  Kumar]{yun2019transformers}
C.~Yun, S.~Bhojanapalli, A.~S. Rawat, S.~J. Reddi, and S.~Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock \emph{arXiv preprint arXiv:1912.10077}, 2019.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Backurs, Bubeck, Eldan,
  Gunasekar, and Wagner]{zhang2022unveiling}
Y.~Zhang, A.~Backurs, S.~Bubeck, R.~Eldan, S.~Gunasekar, and T.~Wagner.
\newblock Unveiling transformers with lego: a synthetic reasoning task.
\newblock \emph{arXiv preprint arXiv:2206.04301}, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Liu, Cai, Wang, and
  Wang]{zhang2022analysis}
Y.~Zhang, B.~Liu, Q.~Cai, L.~Wang, and Z.~Wang.
\newblock An analysis of attention via the lens of exchangeability and latent
  variable models.
\newblock \emph{arXiv preprint arXiv:2212.14852}, 2022{\natexlab{b}}.

\bibitem[Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and
  Singh]{zhao2021calibrate}
Z.~Zhao, E.~Wallace, S.~Feng, D.~Klein, and S.~Singh.
\newblock Calibrate before use: Improving few-shot performance of language
  models.
\newblock In \emph{International Conference on Machine Learning}, pages
  12697--12706. PMLR, 2021.

\bibitem[Zuo et~al.(2023)Zuo, Chen, Yao, Cao, and Gu]{zuo2023understanding}
X.~Zuo, Z.~Chen, H.~Yao, Y.~Cao, and Q.~Gu.
\newblock Understanding train-validation split in meta-learning with neural
  networks.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=JVlyfHEEm0k}.

\end{thebibliography}
