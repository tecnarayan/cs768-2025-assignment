\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Afsar et~al.(2021)Afsar, Crump, and Far]{afsar2021reinforcement}
M~Mehdi Afsar, Trafford Crump, and Behrouz Far.
\newblock Reinforcement learning based recommender systems: A survey.
\newblock \emph{arXiv preprint arXiv:2101.06286}, 2021.

\bibitem[Agrawal and Jia(2017)]{agrawal2017posterior}
Shipra Agrawal and Randy Jia.
\newblock Posterior sampling for reinforcement learning: worst-case regret
  bounds.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1184--1194, 2017.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{auer2002finite}
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine learning}, 47\penalty0 (2):\penalty0 235--256, 2002.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 263--272. JMLR. org, 2017.

\bibitem[Bai et~al.(2019)Bai, Xie, Jiang, and Wang]{bai2019provably}
Yu~Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang.
\newblock Provably efficient q-learning with low switching cost.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Brafman and Tennenholtz(2002)]{brafman2002r}
Ronen~I Brafman and Moshe Tennenholtz.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Oct):\penalty0 213--231, 2002.

\bibitem[Cesa-Bianchi et~al.(2013)Cesa-Bianchi, Dekel, and
  Shamir]{cesa2013online}
Nicolo Cesa-Bianchi, Ofer Dekel, and Ohad Shamir.
\newblock Online learning with switching costs and other adaptive adversaries.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1160--1168, 2013.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5713--5723, 2017.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1507--1516. PMLR, 2019.

\bibitem[Esfandiari et~al.(2021)Esfandiari, Karbasi, Mehrabian, and
  Mirrokni]{esfandiari2021regret}
Hossein Esfandiari, Amin Karbasi, Abbas Mehrabian, and Vahab Mirrokni.
\newblock Regret bounds for batched bandits.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 7340--7348, 2021.

\bibitem[Gao et~al.(2021)Gao, Xie, Du, and Yang]{gao2021provably}
Minbo Gao, Tianle Xie, Simon~S Du, and Lin~F Yang.
\newblock A provably efficient algorithm for linear markov decision process
  with low switching cost.
\newblock \emph{arXiv preprint arXiv:2101.00494}, 2021.

\bibitem[Gao et~al.(2019)Gao, Han, Ren, and Zhou]{gao2019batched}
Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou.
\newblock Batched multi-armed bandits problem.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Huang et~al.(2022)Huang, Chen, Zhao, Qin, Jiang, and
  Liu]{huang2022towards}
Jiawei Huang, Jinglin Chen, Li~Zhao, Tao Qin, Nan Jiang, and Tie-Yan Liu.
\newblock Towards deployment-efficient reinforcement learning: Lower bound and
  optimality.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=ccWaPGl9Hq}.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (4), 2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Krishnamurthy, Simchowitz, and
  Yu]{jin2020reward}
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu.
\newblock Reward-free exploration for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4870--4879. PMLR, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, Wang, and
  Jordan]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pages 2137--2143. PMLR,
  2020{\natexlab{b}}.

\bibitem[Kaufmann et~al.(2021)Kaufmann, M{\'e}nard, Domingues, Jonsson,
  Leurent, and Valko]{kaufmann2021adaptive}
Emilie Kaufmann, Pierre M{\'e}nard, Omar~Darwiche Domingues, Anders Jonsson,
  Edouard Leurent, and Michal Valko.
\newblock Adaptive reward-free exploration.
\newblock In \emph{Algorithmic Learning Theory}, pages 865--891. PMLR, 2021.

\bibitem[Kearns and Singh(2002)]{kearns2002near}
Michael Kearns and Satinder Singh.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine learning}, 49\penalty0 (2-3):\penalty0 209--232, 2002.

\bibitem[Krishnan et~al.(2018)Krishnan, Yang, Goldberg, Hellerstein, and
  Stoica]{krishnan2018learning}
Sanjay Krishnan, Zongheng Yang, Ken Goldberg, Joseph Hellerstein, and Ion
  Stoica.
\newblock Learning to optimize join queries with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1808.03196}, 2018.

\bibitem[Matsushima et~al.(2020)Matsushima, Furuta, Matsuo, Nachum, and
  Gu]{matsushima2020deployment}
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu.
\newblock Deployment-efficient reinforcement learning via model-based offline
  optimization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Maurer and Pontil(2009)]{maurer2009empirical}
Andreas Maurer and Massimiliano Pontil.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock \emph{arXiv preprint arXiv:0907.3740}, 2009.

\bibitem[M{\'e}nard et~al.(2021)M{\'e}nard, Domingues, Jonsson, Kaufmann,
  Leurent, and Valko]{menard2021fast}
Pierre M{\'e}nard, Omar~Darwiche Domingues, Anders Jonsson, Emilie Kaufmann,
  Edouard Leurent, and Michal Valko.
\newblock Fast active learning for pure exploration in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  7599--7608. PMLR, 2021.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Ian Osband, Daniel Russo, and Benjamin Van~Roy.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Perchet et~al.(2016)Perchet, Rigollet, Chassang, and
  Snowberg]{perchet2016batched}
Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg.
\newblock Batched bandit problems.
\newblock \emph{The Annals of Statistics}, 44\penalty0 (2):\penalty0 660--681,
  2016.

\bibitem[Raccuglia et~al.(2016)Raccuglia, Elbert, Adler, Falk, Wenny, Mollo,
  Zeller, Friedler, Schrier, and Norquist]{raccuglia2016machine}
Paul Raccuglia, Katherine~C Elbert, Philip~DF Adler, Casey Falk, Malia~B Wenny,
  Aurelio Mollo, Matthias Zeller, Sorelle~A Friedler, Joshua Schrier, and
  Alexander~J Norquist.
\newblock Machine-learning-assisted materials discovery using failed
  experiments.
\newblock \emph{Nature}, 533\penalty0 (7601):\penalty0 73--76, 2016.

\bibitem[Simchi-Levi and Xu(2019)]{simchi2019phase}
David Simchi-Levi and Yunzong Xu.
\newblock Phase transitions and cyclic phenomena in bandits with switching
  constraints.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Sutton and Barto(1998)]{sutton1998reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}, volume~1.
\newblock MIT press Cambridge, 1998.

\bibitem[Wang et~al.(2020)Wang, Du, Yang, and Salakhutdinov]{wang2020reward}
Ruosong Wang, Simon~S Du, Lin Yang, and Russ~R Salakhutdinov.
\newblock On reward-free reinforcement learning with linear function
  approximation.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 17816--17826, 2020.

\bibitem[Wang et~al.(2021)Wang, Zhou, and Gu]{wang2021provably}
Tianhao Wang, Dongruo Zhou, and Quanquan Gu.
\newblock Provably efficient reinforcement learning with linear function
  approximation under adaptivity constraints.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Xu et~al.(2018)Xu, Tang, Meng, Zhang, Wang, Liu, and
  Yang]{xu2018experience}
Zhiyuan Xu, Jian Tang, Jingsong Meng, Weiyi Zhang, Yanzhi Wang, Chi~Harold Liu,
  and Dejun Yang.
\newblock Experience-driven networking: A deep reinforcement learning based
  approach.
\newblock In \emph{IEEE INFOCOM 2018-IEEE Conference on Computer
  Communications}, pages 1871--1879. IEEE, 2018.

\bibitem[Yu et~al.(2021)Yu, Liu, Nemati, and Yin]{yu2021reinforcement}
Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin.
\newblock Reinforcement learning in healthcare: A survey.
\newblock \emph{ACM Computing Surveys (CSUR)}, 55\penalty0 (1):\penalty0 1--36,
  2021.

\bibitem[Zanette and Brunskill(2019)]{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pages
  7304--7312. PMLR, 2019.

\bibitem[Zanette et~al.(2020)Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020provably}
Andrea Zanette, Alessandro Lazaric, Mykel~J Kochenderfer, and Emma Brunskill.
\newblock Provably efficient reward-agnostic navigation with linear value
  iteration.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 11756--11766, 2020.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Singla, et~al.]{zhang2020task}
Xuezhou Zhang, Adish Singla, et~al.
\newblock Task-agnostic exploration in reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Du, and Ji]{zhang2020nearly}
Zihan Zhang, Simon~S Du, and Xiangyang Ji.
\newblock Nearly minimax optimal reward-free reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.05901}, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2020{\natexlab{c}})Zhang, Zhou, and Ji]{zhang2020almost}
Zihan Zhang, Yuan Zhou, and Xiangyang Ji.
\newblock Almost optimal model-free reinforcement learningvia
  reference-advantage decomposition.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15198--15207, 2020{\natexlab{c}}.

\end{thebibliography}
