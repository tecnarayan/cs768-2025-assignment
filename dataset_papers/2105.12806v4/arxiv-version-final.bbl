\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{BHMM19}

\bibitem[ASB16]{arjovsky2016unitary}
Martin Arjovsky, Amar Shah, and Yoshua Bengio.
\newblock Unitary evolution recurrent neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1120--1128. PMLR, 2016.

\bibitem[BELM20]{BELM20}
Sebastien Bubeck, Ronen Eldan, Yin~Tat Lee, and Dan Mikulincer.
\newblock Network size and size of the weights in memorization with two-layers
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 4977--4986, 2020.

\bibitem[BFT17]{bartlett2017spectrally}
Peter~L Bartlett, Dylan~J Foster, and Matus~J Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[BHMM19]{Belkin15849}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias{\textendash}variance trade-off.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(32):15849--15854, 2019.

\bibitem[BL97]{bobkov1997poincare}
Sergey Bobkov and Michel Ledoux.
\newblock Poincar{\'e}’s inequalities and talagrand’s concentration
  phenomenon for the exponential distribution.
\newblock {\em Probability Theory and Related Fields}, 107(3):383--400, 1997.

\bibitem[BL00]{bobkov2000brunn}
Sergey~G Bobkov and Michel Ledoux.
\newblock From brunn-minkowski to brascamp-lieb and to logarithmic sobolev
  inequalities.
\newblock {\em Geometric \& Functional Analysis GAFA}, 10(5):1028--1052, 2000.

\bibitem[BLLT20]{Bartlett30063}
Peter~L. Bartlett, Philip~M. Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock {\em Proceedings of the National Academy of Sciences},
  117(48):30063--30070, 2020.

\bibitem[BLN21]{bubeck2020law}
S{\'e}bastien Bubeck, Yuanzhi Li, and Dheeraj~M Nagaraj.
\newblock A law of robustness for two-layers neural networks.
\newblock In {\em Conference on Learning Theory}, pages 804--820. PMLR, 2021.

\bibitem[CBG{\etalchar{+}}17]{cisse2017parseval}
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas
  Usunier.
\newblock {Parseval Networks: Improving Robustness to Adversarial Examples}.
\newblock In {\em International Conference on Machine Learning}, pages
  854--863. PMLR, 2017.

\bibitem[CCNW21]{chen2021dimension}
Hong-Bin Chen, Sinho Chewi, and Jonathan Niles-Weed.
\newblock Dimension-free log-sobolev inequalities for mixture distributions.
\newblock {\em Journal of Functional Analysis}, 281(11):109236, 2021.

\bibitem[FdRL17]{facco2017estimating}
Elena Facco, Maria d’Errico, Alex Rodriguez, and Alessandro Laio.
\newblock Estimating the intrinsic dimension of datasets by a minimal
  neighborhood information.
\newblock {\em Scientific reports}, 7(1):1--8, 2017.

\bibitem[FMN16]{fefferman2016testing}
Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan.
\newblock Testing the manifold hypothesis.
\newblock {\em Journal of the American Mathematical Society}, 29(4):983--1049,
  2016.

\bibitem[GCL{\etalchar{+}}19]{gao2019convergence}
Ruiqi Gao, Tianle Cai, Haochuan Li, Cho-Jui Hsieh, Liwei Wang, and Jason~D Lee.
\newblock Convergence of adversarial training in overparametrized neural
  networks.
\newblock {\em Advances in Neural Information Processing Systems},
  32:13029--13040, 2019.

\bibitem[GMF{\etalchar{+}}18]{gilmer2018adversarial}
Justin Gilmer, Luke Metz, Fartash Faghri, Samuel~S Schoenholz, Maithra Raghu,
  Martin Wattenberg, and Ian Goodfellow.
\newblock Adversarial spheres.
\newblock {\em arXiv preprint arXiv:1801.02774}, 2018.

\bibitem[GQU{\etalchar{+}}20]{gowal2021uncovering}
Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli.
\newblock Uncovering the limits of adversarial training against norm-bounded
  adversarial examples.
\newblock {\em arXiv preprint arXiv:2010.03593}, 2020.

\bibitem[Gro86]{gromov1986isoperimetric}
Mikhael Gromov.
\newblock {Isoperimetric Inequalities in Riemannian Manifolds}.
\newblock In {\em Asymptotic Theory of Finite Dimensional Spaces}, volume 1200,
  pages 114--129. Springer Berlin, 1986.

\bibitem[HS89]{hastie1989principal}
Trevor Hastie and Werner Stuetzle.
\newblock Principal curves.
\newblock {\em Journal of the American Statistical Association},
  84(406):502--516, 1989.

\bibitem[HVD15]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[JCC{\etalchar{+}}19]{jiang2018computation}
Haoming Jiang, Zhehui Chen, Minshuo Chen, Feng Liu, Dingding Wang, and Tuo
  Zhao.
\newblock On computation and generalization of gans with spectrum control.
\newblock {\em Proc. of International Conference on Learning Representation
  (ICLR)}, 2019.

\bibitem[KL93]{kambhatla1993fast}
Nanda Kambhatla and Todd~K Leen.
\newblock Fast nonlinear dimension reduction.
\newblock In {\em IEEE International Conference on Neural Networks}, pages
  1213--1218. IEEE, 1993.

\bibitem[Led99]{ledoux1999concentration}
Michel Ledoux.
\newblock Concentration of measure and logarithmic sobolev inequalities.
\newblock In {\em Seminaire de probabilites XXXIII}, pages 120--216. Springer,
  1999.

\bibitem[Led01]{Ledoux}
M.~Ledoux.
\newblock {The concentration of measure phenomenon}.
\newblock In {\em Mathematical Surveys and Monographs}, volume~89. {American
  Mathematical Society, Providence, RI}, 2001.

\bibitem[MHRB17]{mhammedi2017efficient}
Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey.
\newblock Efficient orthogonal parametrisation of recurrent neural networks
  using householder reflections.
\newblock In {\em International Conference on Machine Learning}, pages
  2401--2409. PMLR, 2017.

\bibitem[MKKY18]{miyato2018spectral}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock {\em Proc. of International Conference on Learning Representation
  (ICLR)}, 2018.

\bibitem[MM19]{mei2020generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and the double descent curve.
\newblock {\em Communications on Pure and Applied Mathematics}, 2019.

\bibitem[MMS{\etalchar{+}}18]{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock {\em Proc. of International Conference on Learning Representation
  (ICLR)}, 2018.

\bibitem[MRT18]{mohri2018foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock {\em Foundations of Machine Learning}.
\newblock MIT press, 2018.

\bibitem[NBA{\etalchar{+}}18]{novak2018sensitivity}
Roman Novak, Yasaman Bahri, Daniel~A. Abolafia, Jeffrey Pennington, and Jascha
  Sohl-Dickstein.
\newblock Sensitivity and generalization in neural networks: an empirical
  study.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem[NKB{\etalchar{+}}20]{Nakkiran2020Deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya
  Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem[NM10]{narayanan2010sample}
Hariharan Narayanan and Sanjoy Mitter.
\newblock Sample complexity of testing the manifold hypothesis.
\newblock In {\em Proceedings of the 23rd International Conference on Neural
  Information Processing Systems-Volume 2}, pages 1786--1794, 2010.

\bibitem[PZA{\etalchar{+}}21]{pope2021intrinsic}
Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein.
\newblock The intrinsic dimension of images and its impact on learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem[RS00]{roweis2000nonlinear}
Sam~T Roweis and Lawrence~K Saul.
\newblock Nonlinear dimensionality reduction by locally linear embedding.
\newblock {\em Science}, 290(5500):2323--2326, 2000.

\bibitem[RWK20]{pmlr-v119-rice20a}
Leslie Rice, Eric Wong, and Zico Kolter.
\newblock Overfitting in adversarially robust deep learning.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, volume 119 of {\em Proceedings of Machine Learning Research},
  pages 8093--8104. PMLR, 2020.

\bibitem[SSBD14]{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[TDSL00]{tenenbaum2000global}
Joshua~B Tenenbaum, Vin De~Silva, and John~C Langford.
\newblock A global geometric framework for nonlinear dimensionality reduction.
\newblock {\em Science}, 290(5500):2319--2323, 2000.

\bibitem[Ver18]{vershynin2018high}
Roman Vershynin.
\newblock {\em High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[vH14]{van2014probability}
Ramon van Handel.
\newblock Probability in high dimension.
\newblock Technical report, Princeton University, 2014.

\bibitem[XY20]{Xie2020Intriguing}
Cihang Xie and Alan Yuille.
\newblock Intriguing properties of adversarial training at scale.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem[YKB19]{yin2019rademacher}
Dong Yin, Ramchandran Kannan, and Peter Bartlett.
\newblock Rademacher complexity for adversarially robust generalization.
\newblock In {\em International conference on machine learning}, pages
  7085--7094. PMLR, 2019.

\bibitem[YM17]{yoshida2017spectral}
Yuichi Yoshida and Takeru Miyato.
\newblock Spectral norm regularization for improving the generalizability of
  deep learning.
\newblock {\em arXiv preprint arXiv:1705.10941}, 2017.

\bibitem[YSJ19]{yun2019small}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock {Small ReLU networks are powerful memorizers: a tight analysis of
  memorization capacity}.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  15532--15543, 2019.

\end{thebibliography}
