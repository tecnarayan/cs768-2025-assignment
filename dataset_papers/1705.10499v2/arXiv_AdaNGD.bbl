\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bullen et~al.(2013)Bullen, Mitrinovic, and Vasic]{bullen2013means}
Bullen, Peter~S, Mitrinovic, Dragoslav~S, and Vasic, M.
\newblock \emph{Means and their Inequalities}, volume~31.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Cesa-Bianchi et~al.(2004)Cesa-Bianchi, Conconi, and
  Gentile]{cesa2004generalization}
Cesa-Bianchi, Nicolo, Conconi, Alex, and Gentile, Claudio.
\newblock On the generalization ability of on-line learning algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 50\penalty0
  (9):\penalty0 2050--2057, 2004.

\bibitem[Clarkson et~al.(2012)Clarkson, Hazan, and
  Woodruff]{clarkson2012sublinear}
Clarkson, Kenneth~L, Hazan, Elad, and Woodruff, David~P.
\newblock Sublinear optimization for machine learning.
\newblock \emph{Journal of the ACM (JACM)}, 59\penalty0 (5):\penalty0 23, 2012.

\bibitem[Cotter et~al.(2011)Cotter, Shamir, Srebro, and
  Sridharan]{cotter2011better}
Cotter, Andrew, Shamir, Ohad, Srebro, Nati, and Sridharan, Karthik.
\newblock Better mini-batch algorithms via accelerated gradient methods.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1647--1655, 2011.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{dekel2012optimal}
Dekel, Ofer, Gilad-Bachrach, Ran, Shamir, Ohad, and Xiao, Lin.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Jan):\penalty0 165--202, 2012.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, John, Hazan, Elad, and Singer, Yoram.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Hazan \& Koren(2012)Hazan and Koren]{hazan2012linear}
Hazan, Elad and Koren, Tomer.
\newblock Linear regression with limited observation.
\newblock In \emph{Proceedings of the 29th International Conference on Machine
  Learning (ICML-12)}, pp.\  807--814, 2012.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{hazan2007logarithmic}
Hazan, Elad, Agarwal, Amit, and Kale, Satyen.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 169--192, 2007.

\bibitem[Hazan et~al.(2015)Hazan, Levy, and Shalev-Shwartz]{hazan2015beyond}
Hazan, Elad, Levy, Kfir, and Shalev-Shwartz, Shai.
\newblock Beyond convexity: Stochastic quasi-convex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1594--1602, 2015.

\bibitem[Jain et~al.(2016)Jain, Kakade, Kidambi, Netrapalli, and
  Sidford]{jain2016parallelizing}
Jain, Prateek, Kakade, Sham~M, Kidambi, Rahul, Netrapalli, Praneeth, and
  Sidford, Aaron.
\newblock Parallelizing stochastic approximation through mini-batching and
  tail-averaging.
\newblock \emph{arXiv preprint arXiv:1610.03774}, 2016.

\bibitem[Juditsky \& Nemirovski(2008)Juditsky and
  Nemirovski]{juditsky2008large}
Juditsky, Anatoli~B and Nemirovski, Arkadi~S.
\newblock Large deviations of vector-valued martingales in 2-smooth normed
  spaces.
\newblock \emph{arXiv preprint arXiv:0809.0813}, 2008.

\bibitem[Kakade(2010)]{Kakade13}
Kakade, Sham.
\newblock Lecture notes in multivariate analysis, dimensionality reduction, and
  spectral methods.
\newblock
  \url{http://stat.wharton.upenn.edu/~skakade/courses/stat991_mult/lectures/MatrixConcen.pdf},
  April 2010.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, Diederik and Ba, Jimmy.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Levin et~al.(2009)Levin, Peres, and Wilmer]{levin2009markov}
Levin, David~Asher, Peres, Yuval, and Wilmer, Elizabeth~Lee.
\newblock \emph{Markov chains and mixing times}.
\newblock American Mathematical Soc., 2009.

\bibitem[Levy(2016)]{levy2016power}
Levy, Kfir~Y.
\newblock The power of normalization: Faster evasion of saddle points.
\newblock \emph{arXiv preprint arXiv:1611.04831}, 2016.

\bibitem[Li et~al.(2014)Li, Zhang, Chen, and Smola]{li2014efficient}
Li, Mu, Zhang, Tong, Chen, Yuqiang, and Smola, Alexander~J.
\newblock Efficient mini-batch training for stochastic optimization.
\newblock In \emph{Proceedings of the 20th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pp.\  661--670. ACM, 2014.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{lin2015universal}
Lin, Hongzhou, Mairal, Julien, and Harchaoui, Zaid.
\newblock A universal catalyst for first-order optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3384--3392, 2015.

\bibitem[McMahan \& Streeter(2010)McMahan and Streeter]{mcmahan2010adaptive}
McMahan, H~Brendan and Streeter, Matthew.
\newblock Adaptive bound optimization for online convex optimization.
\newblock \emph{COLT 2010}, pp.\  244, 2010.

\bibitem[Nemirovskii et~al.(1983)Nemirovskii, Yudin, and
  Dawson]{nemirovskii1983problem}
Nemirovskii, Arkadii, Yudin, David~Borisovich, and Dawson, ER.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[Nesterov(2013)]{composite}
Nesterov, Yu.
\newblock Gradient methods for minimizing composite functions.
\newblock \emph{Mathematical Programming}, 140\penalty0 (1):\penalty0 125--161,
  2013.

\bibitem[Nesterov(2015)]{nesterov2015universal}
Nesterov, Yu.
\newblock Universal gradient methods for convex optimization problems.
\newblock \emph{Mathematical Programming}, 152\penalty0 (1-2):\penalty0
  381--404, 2015.

\bibitem[Nesterov(1984)]{nesterov1984minimization}
Nesterov, Yu~E.
\newblock Minimization methods for nonsmooth convex and quasiconvex functions.
\newblock \emph{Matekon}, 29:\penalty0 519--531, 1984.

\bibitem[Nesterov(1983)]{nesterov1983method}
Nesterov, Yurii.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence o (1/k2).
\newblock In \emph{Doklady an SSSR}, volume 269, pp.\  543--547, 1983.

\bibitem[Shalev-Shwartz \& Zhang(2013)Shalev-Shwartz and
  Zhang]{shalev2013accelerated}
Shalev-Shwartz, Shai and Zhang, Tong.
\newblock Accelerated mini-batch stochastic dual coordinate ascent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  378--385, 2013.

\bibitem[Tak{\'a}{\v{c}} et~al.(2015)Tak{\'a}{\v{c}}, Richt{\'a}rik, and
  Srebro]{takavc2015distributed}
Tak{\'a}{\v{c}}, Martin, Richt{\'a}rik, Peter, and Srebro, Nathan.
\newblock Distributed mini-batch sdca.
\newblock \emph{arXiv preprint arXiv:1507.08322}, 2015.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman2012lecture}
Tieleman, Tijmen and Hinton, Geoffrey.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0
  (2), 2012.

\bibitem[Wright \& Nocedal(1999)Wright and Nocedal]{nocedal}
Wright, Stephen and Nocedal, Jorge.
\newblock Numerical optimization.
\newblock \emph{Springer Science}, 35:\penalty0 67--68, 1999.

\bibitem[Zeiler(2012)]{zeiler2012adadelta}
Zeiler, Matthew~D.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\end{thebibliography}
