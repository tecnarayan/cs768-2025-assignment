@inproceedings{Precup00ET,
  author =        {Doina Precup and Richard S. Sutton and Satinder P. Singh},
  booktitle =     {Proceedings of the 17th International Conference on
                   Machine Learning},
  pages =         {759-766},
  title =         {Eligibility Traces for Off-Policy Policy Evaluation},
  year =          {2000},
}

@article{Murphy01MM,
  author =        {S. Murphy and M. van der Laan and J. Robins},
  journal =       {Journal of American Statistical Association},
  number =        {456},
  pages =         {1410-1423},
  title =         {Marginal Mean Models for Dynamic Regimes},
  volume =        {96},
  year =          {2001},
}

@inproceedings{Thomas15HCPE,
  author =        {P. Thomas and G. Theocharous and M. Ghavamzadeh},
  booktitle =     {Proceedings of the 29th Conference on Artificial
                   Intelligence},
  title =         {High Confidence Off-Policy Evaluation},
  year =          {2015},
}

@inproceedings{NacChoDaiLi19,
  author =        {Ofir Nachum and Yinlam Chow and Bo Dai and Lihong Li},
  booktitle =     {Advances in Neural Information Processing Systems},
  pages =         {2315--2325},
  title =         {{DualDICE}: Behavior-Agnostic Estimation of Discounted
                   Stationary Distribution Corrections},
  year =          {2019},
}

@article{uehara2019minimax,
  author =        {Uehara, Masatoshi and Jiang, Nan},
  journal =       {arXiv preprint arXiv:1910.12809},
  title =         {Minimax Weight and {Q}-Function Learning for Off-Policy
                   Evaluation},
  year =          {2019},
}

@inproceedings{zhang2020gendice,
  author =        {Ruiyi Zhang and Bo Dai and Lihong Li and
                   Dale Schuurmans},
  booktitle =     {International Conference on Learning Representations},
  title =         {{GenDICE}: Generalized Offline Estimation of
                   Stationary Values},
  year =          {2020},
}

@article{zhang2020gradientdice,
  author =        {Zhang, Shangtong and Liu, Bo and Whiteson, Shimon},
  journal =       {arXiv preprint arXiv:2001.11113},
  title =         {{GradientDICE}: Rethinking generalized offline
                   estimation of stationary values},
  year =          {2020},
}

@misc{algae,
  author =        {Ofir Nachum and Bo Dai and Ilya Kostrikov and
                   Yinlam Chow and Lihong Li and Dale Schuurmans},
  title =         {{AlgaeDICE}: Policy Gradient from Arbitrary Experience},
  year =          {2019},
}

@book{puterman1994markov,
  author =        {Puterman, Martin L},
  publisher =     {John Wiley \& Sons, Inc.},
  title =         {Markov Decision Processes: Discrete Stochastic
                   Dynamic Programming},
  year =          {1994},
}

@book{meyn2012markov,
  author =        {Meyn, Sean P and Tweedie, Richard L},
  publisher =     {Springer Science \& Business Media},
  title =         {Markov Chains and Stochastic Stability},
  year =          {2012},
}

@article{rockafellar1974augmented,
  author =        {Rockafellar, R Tyrrell},
  journal =       {SIAM Journal on Control},
  number =        {2},
  pages =         {268--285},
  publisher =     {SIAM},
  title =         {Augmented {Lagrange} multiplier functions and duality
                   in nonconvex programming},
  volume =        {12},
  year =          {1974},
}

@inproceedings{Baird95residualalgorithms,
  author =        {Leemon Baird},
  booktitle =     {In Proceedings of the Twelfth International
                   Conference on Machine Learning},
  pages =         {30--37},
  publisher =     {Morgan Kaufmann},
  title =         {Residual Algorithms: Reinforcement Learning with
                   Function Approximation},
  year =          {1995},
}

@article{dai2016learning,
  author =        {Dai, Bo and He, Niao and Pan, Yunpeng and
                   Boots, Byron and Song, Le},
  journal =       {arXiv preprint arXiv:1607.04579},
  title =         {Learning from conditional distributions via dual
                   embeddings},
  year =          {2016},
}

@article{jiang2015doubly,
  author =        {Jiang, Nan and Li, Lihong},
  journal =       {arXiv preprint arXiv:1511.03722},
  title =         {Doubly robust off-policy value evaluation for
                   reinforcement learning},
  year =          {2015},
}

@article{lagoudakis2003least,
  author =        {Lagoudakis, Michail G and Parr, Ronald},
  journal =       {Journal of machine learning research},
  number =        {Dec},
  pages =         {1107--1149},
  title =         {Least-squares policy iteration},
  volume =        {4},
  year =          {2003},
}

@article{farajtabar2018more,
  author =        {Farajtabar, Mehrdad and Chow, Yinlam and
                   Ghavamzadeh, Mohammad},
  journal =       {arXiv preprint arXiv:1802.03493},
  title =         {More robust doubly robust off-policy evaluation},
  year =          {2018},
}

@article{kallus2019double,
  author =        {Kallus, Nathan and Uehara, Masatoshi},
  journal =       {arXiv preprint arXiv:1908.08526},
  title =         {Double reinforcement learning for efficient
                   off-policy evaluation in {Markov} decision processes},
  year =          {2019},
}

@inproceedings{munos2016safe,
  author =        {R. Munos and T. Stepleton and A. Harutyunyan and
                   M. Bellemare},
  booktitle =     {Advances in Neural Information Processing Systems},
  pages =         {1054--1062},
  title =         {Safe and efficient off-policy reinforcement learning},
  year =          {2016},
}

@article{fonteneau13batch,
  author =        {Raphael Fonteneau and Susan A. Murphy and
                   Louis Wehenkel and Damien Ernst},
  journal =       {Annals of Operations Research},
  number =        {1},
  pages =         {383--416},
  title =         {Batch Mode Reinforcement Learning based on the
                   Synthesis of Artificial Trajectories},
  volume =        {208},
  year =          {2013},
}

@misc{duan2020minimaxoptimal,
  author =        {Yaqi Duan and Mengdi Wang},
  note =          {arXiv:2002.09516},
  title =         {Minimax-Optimal Off-Policy Evaluation with Linear
                   Function Approximation},
  year =          {2020},
}

@article{kallus2019efficiently,
  author =        {Kallus, Nathan and Uehara, Masatoshi},
  journal =       {arXiv preprint arXiv:1909.05850},
  title =         {Efficiently breaking the curse of horizon: Double
                   reinforcement learning in infinite-horizon processes},
  year =          {2019},
}

@inproceedings{liu2018breaking,
  author =        {Liu, Qiang and Li, Lihong and Tang, Ziyang and
                   Zhou, Dengyong},
  booktitle =     {Advances in Neural Information Processing Systems},
  pages =         {5356--5366},
  title =         {Breaking the curse of horizon: Infinite-horizon
                   off-policy estimation},
  year =          {2018},
}

@inproceedings{tang20doubly,
  author =        {Ziyang Tang and Yihao Feng and Lihong Li and
                   Dengyong Zhou and Qiang Liu},
  booktitle =     {Proceedings of the 8th International Conference on
                   Learning Representations},
  pages =         {},
  title =         {Doubly Robust Bias Reduction in Infinite Horizon
                   Off-Policy Estimation},
  year =          {2020},
}

@inproceedings{dai18sbeed,
  author =        {Bo Dai and Albert Shaw and Lihong Li and Lin Xiao and
                   Niao He and Zhen Liu and Jianshu Chen and Le Song},
  booktitle =     {Proceedings of the 35th International Conference on
                   Machine Learning},
  pages =         {1133--1142},
  title =         {{SBEED}: Convergent Reinforcement Learning with
                   Nonlinear Function Approximation},
  year =          {2018},
}

@inproceedings{du2017stochastic,
  author =        {Du, Simon S and Chen, Jianshu and Li, Lihong and
                   Xiao, Lin and Zhou, Dengyong},
  booktitle =     {Proceedings of the 34th International Conference on
                   Machine Learning-Volume 70},
  organization =  {JMLR. org},
  pages =         {1049--1058},
  title =         {Stochastic variance reduction methods for policy
                   evaluation},
  year =          {2017},
}

@inproceedings{LiuLiuGhaMahetal15,
  author =        {Liu, Bo and Liu, Ji and Ghavamzadeh, Mohammad and
                   Mahadevan, Sridhar and Petrik, Marek},
  booktitle =     {Proc. The 31st Conf. Uncertainty in Artificial
                   Intelligence, Amsterdam, Netherlands},
  title =         {Finite-Sample Analysis of Proximal Gradient {TD}
                   Algorithms},
  year =          {2015},
}

@misc{basserrano2019faster,
  author =        {Joan Bas-Serrano and Gergely Neu},
  note =          {arXiv:1909.10904},
  title =         {Faster saddle-point optimization for solving
                   large-scale {Markov} decision processes},
  year =          {2019},
}

@article{chen2018scalable,
  author =        {Chen, Yichen and Li, Lihong and Wang, Mengdi},
  journal =       {arXiv preprint arXiv:1804.10328},
  title =         {Scalable Bilinear $\pi$ Learning Using State and
                   Action Features},
  year =          {2018},
}

@article{wang2017randomized,
  author =        {Wang, Mengdi},
  journal =       {arXiv preprint arXiv:1704.01869},
  title =         {Randomized linear programming solves the discounted
                   {Markov} decision problem in nearly-linear (sometimes
                   sublinear) running time},
  year =          {2017},
}

@article{brockman2016openai,
  author =        {Brockman, Greg and Cheung, Vicki and
                   Pettersson, Ludwig and Schneider, Jonas and
                   Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal =       {arXiv preprint arXiv:1606.01540},
  title =         {{OpenAI Gym}},
  year =          {2016},
}

@inproceedings{van2016deep,
  author =        {H. Van Hasselt and A. Guez and D. Silver},
  booktitle =     {AAAI},
  pages =         {2094--2100},
  title =         {Deep Reinforcement Learning with Double
                   {Q}-Learning.},
  volume =        {16},
  year =          {2016},
}
