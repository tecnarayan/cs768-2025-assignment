@article{Maass1998,
abstract = {We introduce a model for analog computation with discrete time in the presence of analog noise that is flexible enough to cover the most important concrete cases, such as noisy analog neural nets and networks of spiking neurons. This model subsumes the classical model for digital computation in the presence of noise. We show that the presence of arbitrarily small amounts of analog noise reduces the power of analog computational models to that of finite automata, and we also prove a new type of upper bound for the VC-dimension of computational models with analog noise.},
author = {Maass, Wolfgang and Orponen, Pekka},
doi = {10.1162/089976698300017359},
issn = {0899-7667},
journal = {Neural Computation},
month = {jul},
number = {5},
pages = {1071--1095},
publisher = { MIT Press  238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu  },
title = {{On the Effect of Analog Noise in Discrete-Time Analog Computations}},
url = {http://www.mitpressjournals.org/doi/10.1162/089976698300017359},
volume = {10},
year = {1998}
}
@article{Siegelmann1991,
abstract = {This paper shows the existence of a finite neural network, made up of sigmoidal neurons, which simulates a universal Turing machine. It is composed of less than 105 synchronously evolving processors, interconnected linearly. High-order connections are not required. {\textcopyright} 1991.},
author = {Siegelmann, Hava T. and Sontag, Eduardo D.},
doi = {10.1016/0893-9659(91)90080-F},
file = {:C$\backslash$:/Users/danie/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Siegelmann, Sontag - 1991 - Turing computability with neural nets.pdf:pdf},
issn = {08939659},
journal = {Applied Mathematics Letters},
month = {jan},
number = {6},
pages = {77--80},
publisher = {Pergamon},
title = {{Turing computability with neural nets}},
url = {https://www.sciencedirect.com/science/article/pii/089396599190080F},
volume = {4},
year = {1991}
}

@article{lee2019wide,
	title={Wide neural networks of any depth evolve as linear models under gradient descent},
	author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
	journal={arXiv preprint arXiv:1902.06720},
	year={2019}
}

@article{chizat2018note,
	title={A note on lazy training in supervised differentiable programming},
	author={Chizat, Lenaic and Bach, Francis},
	journal={arXiv preprint arXiv:1812.07956},
	year={2018}
}

@article{yehudai2019power,
	title={On the Power and Limitations of Random Features for Understanding Neural Networks},
	author={Yehudai, Gilad and Shamir, Ohad},
	journal={arXiv preprint arXiv:1904.00687},
	year={2019}
}

@inproceedings{cho2009kernel,
	title={Kernel methods for deep learning},
	author={Cho, Youngmin and Saul, Lawrence K},
	booktitle={Advances in neural information processing systems},
	pages={342--350},
	year={2009}
}

% NTK
@ARTICLE{Jacot2018-dv,
  title         = "Neural Tangent Kernel: Convergence and Generalization in
                   Neural Networks",
  author        = "Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment",
  month         =  jun,
  year          =  2018,
  url           = "http://arxiv.org/abs/1806.07572",
  archivePrefix = "arXiv",
  eprint        = "1806.07572",
  primaryClass  = "cs.LG",
  arxivid       = "1806.07572"
}

@article{arora2019exact,
  title={On Exact Computation with an Infinitely Wide Neural Net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
  journal={arXiv preprint arXiv:1904.11955},
  year={2019}
}

%extra meanfield

@article{zhang2019fixup,
  title={Fixup initialization: Residual learning without normalization},
  author={Zhang, Hongyi and Dauphin, Yann N and Ma, Tengyu},
  journal={arXiv preprint arXiv:1901.09321},
  year={2019}
}

@article{yang2019mean,
  title={A mean field theory of batch normalization},
  author={Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S},
  journal={arXiv preprint arXiv:1902.08129},
  year={2019}
}

%

% SP

@article{schoenholz2016deep,
  title={Deep information propagation},
  author={Schoenholz, Samuel S and Gilmer, Justin and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1611.01232},
  year={2016}
}

@inproceedings{pennington2017resurrecting,
  title={Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice},
  author={Pennington, Jeffrey and Schoenholz, Samuel and Ganguli, Surya},
  booktitle={Advances in neural information processing systems},
  pages={4785--4795},
  year={2017}
}

@article{chen2018dynamical,
  title={Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks},
  author={Chen, Minmin and Pennington, Jeffrey and Schoenholz, Samuel S},
  journal={arXiv preprint arXiv:1806.05394},
  year={2018}
}

@article{xiao2018dynamical,
  title={Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks},
  author={Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel S and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:1806.05393},
  year={2018}
}

@article{pennington2018emergence,
  title={The emergence of spectral universality in deep networks},
  author={Pennington, Jeffrey and Schoenholz, Samuel S and Ganguli, Surya},
  journal={arXiv preprint arXiv:1802.09979},
  year={2018}
}

@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances in neural information processing systems},
  pages={3360--3368},
  year={2016}
}

@inproceedings{yang2017mean,
	title={Mean Field Residual Networks: On the Edge of Chaos},
	author={Yang, Greg and Schoenholz, Samuel},
	booktitle={Advances in neural information processing systems},
	pages={7103--7114},
	year={2017}
}

@article{gilboa2019dynamical,
  title={Dynamical Isometry and a Mean Field Theory of LSTMs and GRUs},
  author={Gilboa, Dar and Chang, Bo and Chen, Minmin and Yang, Greg and Schoenholz, Samuel S and Chi, Ed H and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:1901.08987},
  year={2019}
}

@article{matthews2018gaussian,
  title={Gaussian process behaviour in wide deep neural networks},
  author={Matthews, Alexander G de G and Rowland, Mark and Hron, Jiri and Turner, Richard E and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1804.11271},
  year={2018}
}
%

% Quantization

@article{hubara2017quantized,
  title={Quantized neural networks: Training neural networks with low precision weights and activations},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6869--6898},
  year={2017},
  publisher={JMLR. org}
}

@misc{hinton2012neural,
  title={Neural networks for machine learning. Coursera,[video lectures]},
  author={Hinton, G},
  year={2012}
}

%low precision

@article{das2018mixed,
  title={Mixed precision training of convolutional neural networks using integer operations},
  author={Das, Dipankar and Mellempudi, Naveen and Mudigere, Dheevatsa and Kalamkar, Dhiraj and Avancha, Sasikanth and Banerjee, Kunal and Sridharan, Srinivas and Vaidyanathan, Karthik and Kaul, Bharat and Georganas, Evangelos and others},
  journal={arXiv preprint arXiv:1802.00930},
  year={2018}
}

@inproceedings{gupta2015deep,
  title={Deep learning with limited numerical precision},
  author={Gupta, Suyog and Agrawal, Ankur and Gopalakrishnan, Kailash and Narayanan, Pritish},
  booktitle={International Conference on Machine Learning},
  pages={1737--1746},
  year={2015}
}

@inproceedings{lin2017towards,
  title={Towards accurate binary convolutional neural network},
  author={Lin, Xiaofan and Zhao, Cong and Pan, Wei},
  booktitle={Advances in Neural Information Processing Systems},
  pages={345--353},
  year={2017}
}

@article{miyashita2016convolutional,
  title={Convolutional neural networks using logarithmic data representation},
  author={Miyashita, Daisuke and Lee, Edward H and Murmann, Boris},
  journal={arXiv preprint arXiv:1603.01025},
  year={2016}
}

@inproceedings{rastegari2016xnor,
  title={Xnor-net: Imagenet classification using binary convolutional neural networks},
  author={Rastegari, Mohammad and Ordonez, Vicente and Redmon, Joseph and Farhadi, Ali},
  booktitle={European Conference on Computer Vision},
  pages={525--542},
  year={2016},
  organization={Springer}
}

@article{Yin2019,
author = {Yin, Penghang and Lyu, Jiancheng and Zhang, Shuai and Osher, Stanley and Qi, Yingyong and Xin, Jack},
file = {:C$\backslash$:/Users/Daniel/Downloads/51dbe28e82cb0dc01f37fc52fffd58a12cb9da5f.pdf:pdf},
journal = {ICLR},
pages = {1--30},
title = {{Understanding straight-through estimator in training activation quantized neural nets}},
year = {2019}
}

@article{Anderson2017,
abstract = {Recent research has shown that one can train a neural network with binary weights and activations at train time by augmenting the weights with a high-precision continuous latent variable that accumulates small changes from stochastic gradient descent. However, there is a dearth of theoretical analysis to explain why we can effectively capture the features in our data with binary weights and activations. Our main result is that the neural networks with binary weights and activations trained using the method of Courbariaux, Hubara et al. (2016) work because of the high-dimensional geometry of binary vectors. In particular, the ideal continuous vectors that extract out features in the intermediate representations of these BNNs are well-approximated by binary vectors in the sense that dot products are approximately preserved. Compared to previous research that demonstrated the viability of such BNNs, our work explains why these BNNs work in terms of the HD geometry. Our theory serves as a foundation for understanding not only BNNs but a variety of methods that seek to compress traditional neural networks. Furthermore, a better understanding of multilayer binary neural networks serves as a starting point for generalizing BNNs to other neural network architectures such as recurrent neural networks.},
archivePrefix = {arXiv},
arxivId = {1705.07199},
author = {Anderson, Alexander G. and Berg, Cory P.},
eprint = {1705.07199},
file = {:C$\backslash$:/Users/Daniel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Anderson, Berg - 2017 - The High-Dimensional Geometry of Binary Neural Networks.pdf:pdf},
journal = {ICLR},
number = {2014},
pages = {1--13},
title = {{The High-Dimensional Geometry of Binary Neural Networks}},
url = {http://arxiv.org/abs/1705.07199},
year = {2018}
}


@article{Li2017b,
abstract = {Currently, deep neural networks are deployed on low-power embedded devices by first training a full-precision model using powerful computing hardware, and then deriving a corresponding low-precision model for efficient inference on such systems. However, training models directly with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. Numerous recent publications have studied methods for training quantized network, but these studies have mostly been empirical. In this work, we investigate training methods for quantized neural networks from a theoretical viewpoint. We first explore accuracy guarantees for training methods under convexity assumptions. We then look at the behavior of algorithms for non-convex problems, and we show that training algorithms that exploit high-precision representations have an important annealing property that purely quantized training methods lack, which explains many of the observed empirical differences between these types of algorithms.},
archivePrefix = {arXiv},
arxivId = {1706.02379},
author = {Li, Hao and De, Soham and Xu, Zheng and Studer, Christoph and Samet, Hanan and Goldstein, Tom},
eprint = {1706.02379},
file = {:C$\backslash$:/Users/Daniel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2017 - Training Quantized Nets A Deeper Understanding.pdf:pdf},
journal = {NIPS},
month = {jun},
title = {{Training Quantized Nets: A Deeper Understanding}},
url = {http://arxiv.org/abs/1706.02379},
year = {2017}
}


@article{courbariaux2016binarized,
  title={Binarized neural networks},
  author={Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={Advances in Neural Information Processing Systems},
  year={2016}
}

@inproceedings{banner2018scalable,
  title={Scalable methods for 8-bit training of neural networks},
  author={Banner, Ron and Hubara, Itay and Hoffer, Elad and Soudry, Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5145--5153},
  year={2018}
}

@inproceedings{wang2018training,
  title={Training deep neural networks with 8-bit floating point numbers},
  author={Wang, Naigang and Choi, Jungwook and Brand, Daniel and Chen, Chia-Yu and Gopalakrishnan, Kailash},
  booktitle={Advances in neural information processing systems},
  pages={7675--7684},
  year={2018}
}

@article{mishra2017wrpn,
  title={WRPN: wide reduced-precision networks},
  author={Mishra, Asit and Nurvitadhi, Eriko and Cook, Jeffrey J and Marr, Debbie},
  journal={arXiv preprint arXiv:1709.01134},
  year={2017}
}

@inproceedings{zhou2018adaptive,
  title={Adaptive quantization for deep neural network},
  author={Zhou, Yiren and Moosavi-Dezfooli, Seyed-Mohsen and Cheung, Ngai-Man and Frossard, Pascal},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


% math
@inproceedings{
wu2018deterministic,
title={Deterministic Variational Inference for Robust Bayesian Neural Networks},
author={Anqi Wu and Sebastian Nowozin and Edward Meeds and Richard E. Turner and Jose Miguel Hernandez-Lobato and Alexander L. Gaunt},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1l08oAct7},
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}

% misc

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}}
  
@article{howard2017mobilenets,
  title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal={arXiv preprint arXiv:1704.04861},
  year={2017}
}

@inproceedings{chen2015compressing,
  title={Compressing neural networks with the hashing trick},
  author={Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian and Chen, Yixin},
  booktitle={International Conference on Machine Learning},
  pages={2285--2294},
  year={2015}
}

@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}