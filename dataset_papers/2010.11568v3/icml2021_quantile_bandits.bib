@comment{}}

@comment{%-------------------------------
% Reference for Best Arm Identification literature}

@comment{%-------------------------------}

@article{ahmadi-javid_entropic_2012,
 abstract = {This paper introduces the concept of entropic value-at-risk (EVaR), a new coherent risk measure that corresponds to the tightest possible upper bound obtained from the Chernoff inequality for the value-at-risk (VaR) as well as the conditional value-at-risk (CVaR). We show that a broad class of stochastic optimization problems that are computationally intractable with the CVaR is efficiently solvable when the EVaR is incorporated. We also prove that if two distributions have the same EVaR at all confidence levels, then they are identical at all points. The dual representation of the EVaR is closely related to the Kullback-Leibler divergence, also known as the relative entropy. Inspired by this dual representation, we define a large class of coherent risk measures, called g-entropic risk measures. The new class includes both the CVaR and the EVaR.},
 author = {Ahmadi-Javid, A.},
 doi = {10.1007/s10957-011-9968-2},
 file = {Springer Full Text PDF:/home/admin-u6015325/Zotero/storage/RQK77BND/Ahmadi-Javid - 2012 - Entropic Value-at-Risk A New Coherent Risk Measur.pdf:application/pdf},
 issn = {1573-2878},
 journal = {Journal of Optimization Theory and Applications},
 language = {en},
 number = {3},
 pages = {1105--1123},
 shorttitle = {Entropic {Value}-at-{Risk}},
 title = {Entropic {Value}-at-{Risk}: {A} {New} {Coherent} {Risk} {Measure}},
 urldate = {2020-08-28},
 volume = {155},
 year = {2012}
}

@article{AlphaGo2016,
 author = {David Silver and Aja Huang and Christopher J. Maddison and Arthur Guez and Laurent Sifre and George van den Driessche and Julian Schrittwieser and Ioannis Antonoglou and Veda Panneershelvam and Marc Lanctot and Sander Dieleman and Dominik Grewe and John Nham and Nal Kalchbrenner and Ilya Sutskever and Timothy Lillicrap and Madeleine Leach and Koray Kavukcuoglu and Thore Graepel and Demis Hassabis},
 journal = {Nature},
 pages = {484--503},
 title = {Mastering the game of Go with deep neural networks and tree search},
 volume = {529},
 year = {2016}
}

@article{AlphaZero2018,
 author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
 journal = {Science},
 number = {6419},
 pages = {1140--1144},
 publisher = {American Association for the Advancement of Science},
 title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
 volume = {362},
 year = {2018}
}

@article{altschuler2019best,
 author = {Altschuler, Jason and Brunel, Victor-Emmanuel and Malek, Alan},
 journal = {Journal of Machine Learning Research},
 number = {91},
 pages = {1--39},
 title = {Best arm identification for contaminated bandits},
 volume = {20},
 year = {2019}
}

@misc{anonymous,
 author = {Author, N. N.},
 title = {Suppressed for Anonymity},
 year = {2020}
}

@article{artzner_coherent_nodate,
 abstract = {In this paper we study both market risks and non-market risks, without complete markets assumption, and discuss methods of measurement of these risks. We present and justify a set of four desirable properties for measures of risk, and call the measures satisfying these properties “coherent”. We examine the measures of risk provided and the related actions required by SPAN, by the SEC/NASD rules and by quantile based methods. We demonstrate the universality of scenario-based methods for providing coherent measures. We oﬀer suggestions concerning the SEC method. We also suggest a method to repair the failure of subadditivity of quantile-based methods.},
 author = {Artzner, Philippe and Delbaen, Freddy and Eber, Jean-Marc and Heath, David},
 file = {Artzner et al. - COHERENT MEASURES OF RISK.pdf:/home/admin-u6015325/Zotero/storage/AE93LC9W/Artzner et al. - COHERENT MEASURES OF RISK.pdf:application/pdf},
 language = {en},
 pages = {24},
 title = {{COHERENT} {MEASURES} {OF} {RISK}}
}

@article{audibert2009exploration,
 author = {Audibert, Jean-Yves and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
 journal = {Theoretical Computer Science},
 number = {19},
 pages = {1876--1902},
 publisher = {Elsevier},
 title = {Exploration--exploitation tradeoff using variance estimates in multi-armed bandits},
 volume = {410},
 year = {2009}
}

@inproceedings{audibert2010best,
 author = {Jean{-}Yves Audibert and
S{\'{e}}bastien Bubeck and
R{\'{e}}mi Munos},
 booktitle = {{COLT} 2010 - The 23rd Conference on Learning Theory, Haifa, Israel,
June 27-29, 2010},
 pages = {41--53},
 publisher = {Omnipress},
 title = {Best Arm Identification in Multi-Armed Bandits},
 year = {2010}
}

@article{Auer2002,
 abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
 author = {Auer, Peter
and Cesa-Bianchi, Nicol{\`o}
and Fischer, Paul},
 day = {01},
 journal = {Machine Learning},
 number = {2},
 pages = {235--256},
 title = {Finite-time Analysis of the Multiarmed Bandit Problem},
 volume = {47},
 year = {2002}
}

@article{Auer2003adv,
 acmid = {589365},
 author = {Auer, Peter and Cesa-Bianchi, Nicol\`{o} and Freund, Yoav and Schapire, Robert E.},
 issue_date = {2003},
 journal = {SIAM J. Comput.},
 keywords = {adversarial bandit problem, unknown matrix games},
 number = {1},
 numpages = {30},
 pages = {48--77},
 publisher = {Society for Industrial and Applied Mathematics},
 title = {The Nonstochastic Multiarmed Bandit Problem},
 volume = {32},
 year = {2003}
}

@article{babaioff2015dynamic,
 author = {Babaioff, Moshe and Dughmi, Shaddin and Kleinberg, Robert and Slivkins, Aleksandrs},
 journal = {ACM Transactions on Economics and Computation (TEAC)},
 number = {1},
 pages = {4},
 publisher = {ACM},
 title = {Dynamic pricing with limited supply},
 volume = {3},
 year = {2015}
}

@article{bagnoli_log-concave_2005,
 abstract = {In many applications, assumptions about the log-concavity of a probability distribution allow just enough special structure to yield a workable theory. This paper catalogs a series of theorems relating log-concavity and/or log-convexity of probability density functions, distribution functions, reliability functions, and their integrals. We list a large number of commonly-used probability distributions and report the log-concavity or log-convexity of their density functions and their integrals. We also discuss a variety of applications of log-concavity that have appeared in the literature.},
 author = {Bagnoli, Mark and Bergstrom, Ted},
 issn = {0938-2259, 1432-0479},
 journal = {Economic Theory},
 language = {en},
 number = {2},
 pages = {445--469},
 title = {Log-concave probability and its applications},
 urldate = {2020-09-27},
 volume = {26},
 year = {2005}
}

@article{bernstein1924modification,
 author = {Bernstein, Sergei},
 journal = {Ann. Sci. Inst. Sav. Ukraine, Sect. Math},
 number = {4},
 pages = {38--49},
 title = {On a modification of Chebyshev’s inequality and of the error formula of Laplace},
 volume = {1},
 year = {1924}
}

@article{bertsimas_tight_bound_order_stat_2006,
 author = {Bertsimas, Dimitris and Natarajan, Karthik and Teo, Chung-Piaw},
 doi = {10.1017/S0269964806060414},
 issn = {0269-9648, 1469-8951},
 journal = {Probability in the Engineering and Informational Sciences},
 language = {en},
 number = {4},
 pages = {667--686},
 title = {{TIGHT} {BOUNDS} {ON} {EXPECTED} {ORDER} {STATISTICS}},
 urldate = {2020-09-26},
 volume = {20},
 year = {2006}
}

@article{boucheron2012,
 author = {Boucheron, Stéphane and Thomas, Maud},
 fjournal = {Electronic Communications in Probability},
 journal = {Electron. Commun. Probab.},
 pages = {12 pp.},
 pno = {51},
 publisher = {The Institute of Mathematical Statistics and the Bernoulli Society},
 title = {Concentration inequalities for order statistics},
 volume = {17},
 year = {2012}
}

@book{boucheron2013,
 author = {Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Massart, Pascal},
 publisher = {Oxford university press},
 title = {Concentration inequalities: A nonasymptotic theory of independence},
 year = {2013}
}

@inproceedings{boursier_sic-mmab_2019,
 author = {Etienne Boursier and
Vianney Perchet},
 booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada},
 pages = {12048--12057},
 title = {{SIC-MMAB:} Synchronisation Involves Communication in Multiplayer
Multi-Armed Bandits},
 year = {2019}
}

@article{bubeck2013bandits,
 author = {Bubeck, S{\'e}bastien and Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
 journal = {IEEE Transactions on Information Theory},
 number = {11},
 pages = {7711--7717},
 publisher = {IEEE},
 title = {Bandits with heavy tail},
 volume = {59},
 year = {2013}
}

@inproceedings{bubeck2013multiple,
 author = {S{\'{e}}bastien Bubeck and
Tengyao Wang and
Nitin Viswanathan},
 booktitle = {Proceedings of the 30th International Conference on Machine Learning,
{ICML} 2013, Atlanta, GA, USA, 16-21 June 2013},
 pages = {258--265},
 publisher = {JMLR.org},
 series = {{JMLR} Workshop and Conference Proceedings},
 title = {Multiple Identifications in Multi-Armed Bandits},
 volume = {28},
 year = {2013}
}

@inproceedings{bubeck_pure_2009,
 abstract = {We consider the framework of stochastic multi-armed bandit problems and study the possibilities and limitations of strategies that perform an online exploration of the arms. The strategies are assessed in terms of their simple regret, a regret notion that captures the fact that exploration is only constrained by the number of available rounds (not necessarily known in advance), in contrast to the case when the cumulative regret is considered and when exploitation needs to be performed at the same time. We believe that this performance criterion is suited to situations when the cost of pulling an arm is expressed in terms of resources rather than rewards. We discuss the links between the simple and the cumulative regret. The main result is that the required exploration–exploitation trade-offs are qualitatively different, in view of a general lower bound on the simple regret in terms of the cumulative regret.},
 author = {Bubeck, Sébastien and Munos, Rémi and Stoltz, Gilles},
 booktitle = {Algorithmic {Learning} {Theory}},
 file = {Submitted Version:/Users/zhangmengyan/Zotero/storage/B9VDIEN4/Bubeck et al. - 2009 - Pure Exploration in Multi-armed Bandits Problems.pdf:application/pdf},
 language = {en},
 pages = {23--37},
 publisher = {Springer},
 series = {Lecture {Notes} in {Computer} {Science}},
 title = {Pure {Exploration} in {Multi}-armed {Bandits} {Problems}},
 year = {2009}
}

@article{bubeck_regret_2012,
 abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration–exploitation trade-oﬀ. This is the balance between staying with the option that gave highest payoﬀs in the past and exploring new options that might give higher payoﬀs in the future. Although the study of bandit problems dates back to the 1930s, exploration–exploitation trade-oﬀs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is deﬁned by the payoﬀ process associated with each option. In this monograph, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoﬀs and adversarial payoﬀs. Besides the basic setting of ﬁnitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.},
 author = {Bubeck, Sébastien},
 file = {Bubeck - 2012 - Regret Analysis of Stochastic and Nonstochastic Mu.pdf:/home/admin-u6015325/Zotero/storage/4LEA5ZB8/Bubeck - 2012 - Regret Analysis of Stochastic and Nonstochastic Mu.pdf:application/pdf},
 journal = {Foundations and Trends® in Machine Learning},
 language = {en},
 number = {1},
 pages = {1--122},
 title = {Regret {Analysis} of {Stochastic} and {Nonstochastic} {Multi}-armed {Bandit} {Problems}},
 urldate = {2019-08-28},
 volume = {5},
 year = {2012}
}

@article{burtini2015survey,
 author = {Burtini, Giuseppe and Loeppky, Jason and Lawrence, Ramon},
 journal = {arXiv preprint arXiv:1510.00757},
 title = {A survey of online experiment design with the stochastic multi-armed bandit},
 year = {2015}
}

@inproceedings{cassel_general_2018,
 author = {Asaf Cassel and
Shie Mannor and
Assaf Zeevi},
 booktitle = {Conference On Learning Theory, {COLT} 2018, Stockholm, Sweden, 6-9
July 2018},
 pages = {1295--1306},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 title = {A General Approach to Multi-Armed Bandits Under Risk Criteria},
 volume = {75},
 year = {2018}
}

@article{clark2003survival,
 author = {Clark, Taane G and Bradburn, Michael J and Love, Sharon B and Altman, Douglas G},
 journal = {British journal of cancer},
 number = {2},
 pages = {232--238},
 publisher = {Nature Publishing Group},
 title = {Survival analysis part I: basic concepts and first analyses},
 volume = {89},
 year = {2003}
}

@article{coquelin2007bandit,
 author = {Coquelin, Pierre-Arnaud and Munos, R{\'e}mi},
 journal = {arXiv preprint cs/0703062},
 title = {Bandit algorithms for tree search},
 year = {2007}
}

@book{cox2018analysis,
 author = {Cox, David Roxbee},
 publisher = {Chapman and Hall/CRC},
 title = {Analysis of survival data},
 year = {2018}
}

@article{cunningham_vaccine_2016,
 abstract = {In the 21st century, an array of microbiological and molecular allow antigens for new vaccines to be specifically identified, designed, produced and delivered with the aim of optimising the induction of a protective immune response against a well-defined immunogen. New knowledge about the functioning of the immune system and host pathogen interactions has stimulated the rational design of vaccines. The design toolbox includes vaccines made from whole pathogens, protein subunits, polysaccharides, pathogen-like particles, use of viral/bacterial vectors, plus adjuvants and conjugation technology to increase and broaden the immune response. Processes such as recombinant DNA technology can simplify the complexity of manufacturing and facilitate consistent production of large quantities of antigen. Any new vaccine development is greatly enhanced by, and requires integration of information concerning: 1. Pathogen life-cycle \& epidemiology. Knowledge of pathogen structure, route of entry, interaction with cellular receptors, subsequent replication sites and disease-causing mechanisms are all important to identify antigens suitable for disease prevention. The demographics of infection, specific risk groups and age-specific infection rates determine which population to immunise, and at what age. 2. Immune control \& escape. Interactions between the host and pathogen are explored, with determination of the relative importance of antibodies, T-cells of different types and innate immunity, immune escape strategies during infection, and possible immune correlates of protection. This information guides identification and selection of antigen and the specific immune response required for protection. 3. Antigen selection \& vaccine formulation. The selected antigen is formulated to remain suitably immunogenic and stable over time, induce an immune response that is likely to be protective, plus be amenable to eventual scale-up to commercial production. 4. Vaccine preclinical \& clinical testing. The candidate vaccine must be tested for immunogenicity, safety and efficacy in preclinical and appropriately designed clinical trials. This review considers these processes using examples of differing pathogenic challenges, including human papillomavirus, malaria, and ebola.},
 author = {Cunningham, Anthony L. and Garçon, Nathalie and Leo, Oberdan and Friedland, Leonard R. and Strugnell, Richard and Laupèze, Béatrice and Doherty, Mark and Stern, Peter},
 doi = {10.1016/j.vaccine.2016.10.016},
 file = {ScienceDirect Full Text PDF:/home/admin-u6015325/Zotero/storage/ZJJRRD4E/Cunningham et al. - 2016 - Vaccine development From concept to early clinica.pdf:application/pdf;ScienceDirect Snapshot:/home/admin-u6015325/Zotero/storage/38EUPHKV/S0264410X16309173.html:text/html},
 issn = {0264-410X},
 journal = {Vaccine},
 keywords = {Adaptive immunity, Adjuvants, Antigen, Clinical development, Innate immunity, Vaccine},
 language = {en},
 number = {52},
 pages = {6655--6664},
 series = {The {Changing} {Face} of {Vaccines} and {Vaccination}},
 shorttitle = {Vaccine development},
 title = {Vaccine development: {From} concept to early clinical testing},
 urldate = {2020-10-14},
 volume = {34},
 year = {2016}
}

@inproceedings{CVaRICML2020,
 author = {Prashanth L. A. and
Krishna P. Jagannathan and
Ravi Kumar Kolla},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning,
{ICML} 2020, 13-18 July 2020, Virtual Event},
 pages = {5577--5586},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 title = {Concentration bounds for CVaR estimation: The cases of light-tailed
and heavy-tailed distributions},
 volume = {119},
 year = {2020}
}

@article{darling_confidence_1967,
 author = {Darling, D. A. and Robbins, Herbert},
 file = {PubMed Central Full Text PDF:/home/admin-u6015325/Zotero/storage/IN56PRJU/Darling and Robbins - 1967 - CONFIDENCE SEQUENCES FOR MEAN, VARIANCE, AND MEDIA.pdf:application/pdf},
 issn = {0027-8424},
 journal = {Proceedings of the National Academy of Sciences of the United States of America},
 number = {1},
 pages = {66--68},
 pmcid = {PMC335597},
 pmid = {16578652},
 title = {{CONFIDENCE} {SEQUENCES} {FOR} {MEAN}, {VARIANCE}, {AND} {MEDIAN}},
 urldate = {2020-09-03},
 volume = {58},
 year = {1967}
}

@misc{david_max,
 archiveprefix = {arXiv},
 author = {Yahel David and Nahum Shimkin},
 eprint = {1512.07650},
 primaryclass = {stat.ML},
 title = {The Max $K$-Armed Bandit: {PAC} Lower Bounds and Efficient Algorithms},
 year = {2015}
}

@book{david_order_1981,
 author = {David, Herbert Aaron},
 isbn = {978-0-471-02723-2},
 language = {English},
 note = {OCLC: 301102741},
 publisher = {J. Wiley},
 chapter        = {4.6},
 title = {Order statistics.},
 year = {2003}
}

@incollection{david_order_2004,
 abstract = {The basic distribution theory of order statistics (ordered random variables) is developed for both finite random samples and asymptotically. The use of order statistics in the estimation of parameters is described, with special emphasis on suitably chosen linear functions of these variables. Order statistics provide simple robust estimators of location, such as the median or trimmed means. They are also useful, as outlined below, in the treatment of outliers, simultaneous inference, data compression, probability plotting, ranked-set sampling, and ranking and selection.},
 author = {David, H. A. and Nagaraja, H. N.},
 booktitle = {Encyclopedia of {Statistical} {Sciences}},
 copyright = {Copyright © 2004 by John Wiley \& Sons, Inc. All rights reserved.},
 doi = {10.1002/0471667196.ess6023},
 file = {Snapshot:/home/admin-u6015325/Zotero/storage/Q23VMJXT/0471667196.html:text/html},
 isbn = {978-0-471-66719-3},
 keywords = {applications of order statistics, distribution of order statistics, extreme-value theory, linear functions of order statistics},
 language = {en},
 note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/0471667196.ess6023},
 publisher = {American Cancer Society},
 title = {Order {Statistics}},
 urldate = {2020-09-30},
 year = {2004}
}

@inproceedings{david_pac_nodate,
 author = {David, Yahel and Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Ghavamzadeh, Mohammad and Mannor, Shie and Shimkin, Nahum},
 booktitle = {ISAIM},
 title = {{PAC} Bandits with Risk Constraints.},
 year = {2018}
}

@incollection{david_pure_2016,
 abstract = {We consider a variant of the pure exploration problem in Multi-Armed Bandits, where the goal is to ﬁnd the arm for which the λ-quantile is maximal. Within the PAC framework, we provide a lower bound on the sample complexity of any ( , δ)-correct algorithm, and propose algorithms with matching upper bounds. Our bounds sharpen existing ones by explicitly incorporating the quantile factor λ. We further provide experiments that compare the sample complexity of our algorithms with that of previous works.},
 author = {David, Yahel and Shimkin, Nahum},
 booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}},
 file = {David and Shimkin - 2016 - Pure Exploration for Max-Quantile Bandits.pdf:/Users/zhangmengyan/Zotero/storage/ZPFFLBAI/David and Shimkin - 2016 - Pure Exploration for Max-Quantile Bandits.pdf:application/pdf},
 language = {en},
 pages = {556--571},
 publisher = {Springer International Publishing},
 title = {Pure {Exploration} for {Max}-{Quantile} {Bandits}},
 urldate = {2020-01-12},
 volume = {9851},
 year = {2016}
}

@inproceedings{degenne_bridging_2019,
 author = {R{\'{e}}my Degenne and
Thomas Nedelec and
Cl{\'{e}}ment Calauz{\`{e}}nes and
Vianney Perchet},
 booktitle = {The 22nd International Conference on Artificial Intelligence and Statistics,
{AISTATS} 2019, 16-18 April 2019, Naha, Okinawa, Japan},
 pages = {1988--1996},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 title = {Bridging the gap between regret minimization and best arm identification,
with application to {A/B} tests},
 volume = {89},
 year = {2019}
}

@book{DudaHart2nd,
 author = {R. O. Duda and P. E. Hart and D. G. Stork},
 edition = {2nd},
 publisher = {John Wiley and Sons},
 title = {Pattern Classification},
 year = {2000}
}

@article{dvoretzky1956,
 author = {Dvoretzky, A. and Kiefer, J. and Wolfowitz, J.},
 doi = {10.1214/aoms/1177728174},
 fjournal = {Annals of Mathematical Statistics},
 journal = {Ann. Math. Statist.},
 number = {3},
 pages = {642--669},
 publisher = {The Institute of Mathematical Statistics},
 title = {Asymptotic Minimax Character of the Sample Distribution Function and of the Classical Multinomial Estimator},
 volume = {27},
 year = {1956}
}

@article{even2006action,
 author = {Even-Dar, Eyal and Mannor, Shie and Mansour, Yishay},
 journal = {Journal of machine learning research},
 number = {Jun},
 pages = {1079--1105},
 title = {Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems},
 volume = {7},
 year = {2006}
}

@book{foss2011introduction,
 author = {Foss, Sergey and Korshunov, Dmitry and Zachary, Stan and others},
 publisher = {Springer},
 title = {An introduction to heavy-tailed and subexponential distributions},
 volume = {6},
 year = {2011}
}

@inproceedings{gabillon2011multi,
 author = {Victor Gabillon and
Mohammad Ghavamzadeh and
Alessandro Lazaric and
S{\'{e}}bastien Bubeck},
 booktitle = {Advances in Neural Information Processing Systems 24: 25th Annual
Conference on Neural Information Processing Systems 2011. Proceedings
of a meeting held 12-14 December 2011, Granada, Spain},
 pages = {2222--2230},
 title = {Multi-Bandit Best Arm Identification},
 year = {2011}
}

@inproceedings{gabillon_best_2012,
 author = {Victor Gabillon and
Mohammad Ghavamzadeh and
Alessandro Lazaric},
 booktitle = {Advances in Neural Information Processing Systems 25: 26th Annual
Conference on Neural Information Processing Systems 2012. Proceedings
of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States},
 pages = {3221--3229},
 title = {Best Arm Identification: {A} Unified Approach to Fixed Budget and
Fixed Confidence},
 year = {2012}
}

@inproceedings{garivier2011kl,
 author = {Garivier, Aur{\'e}lien and Capp{\'e}, Olivier},
 booktitle = {Proceedings of the 24th annual conference on learning theory},
 pages = {359--376},
 title = {The KL-UCB algorithm for bounded stochastic bandits and beyond},
 year = {2011}
}

@article{gine2002_concen_kernel_density,
 author = {Gin\'e, Evarist and Guillou, Armelle},
 journal = {Annales de l'I.H.P. Probabilit\'es et statistiques},
 language = {en},
 mrnumber = {1955344},
 number = {6},
 pages = {907-921},
 publisher = {Elsevier},
 title = {Rates of strong uniform consistency for multivariate kernel density estimators},
 volume = {38},
 year = {2002},
 zbl = {1011.62034}
}

@inproceedings{gupta_successive_2011,
 abstract = {The relevance of the multi-armed bandit problem has risen in the past few years with the need for online optimization techniques in Internet systems, such as online advertisement and news article recommendation. At the same time, these applications reveal that state-of-the-art solution schemes do not scale well with the number of bandit arms. In this paper, we present two types of Successive Reduction (SR) strategies - 1) Successive Reduction Hoeffding (SRH) and 2) Successive Reduction Order Statistics (SRO). Both use an Order Statistics based Thompson Sampling method for arm selection, and then successively eliminate bandit arms from consideration based on a confidence threshold. While SRH uses Hoeffding Bounds for elimination, SRO uses the probability of an arm being superior to the currently selected arm to measure confidence. A computationally efficient scheme for pairwise calculation of the latter probability is also presented in this paper. Using SR strategies, sampling resources and arm pulls are not wasted on arms that are unlikely to be the optimal one. To demonstrate the scalability of our proposed schemes, we compare them with two state-of-the-art approaches, namely pure Thompson Sampling and UCB-Tuned. The empirical results are truly conclusive, with the performance advantage of proposed SRO scheme increasing persistently with the number of bandit arms while the SRH scheme shows similar performance as pure Thompson Sampling. We thus believe that SR algorithms will open up for improved performance in Internet based on-line optimization, and tackling of larger problems.},
 author = {Gupta, Neha and Granmo, Ole-Christoffer and Agrawala, Ashok},
 booktitle = {Research and {Development} in {Intelligent} {Systems} {XXVIII}},
 doi = {10.1007/978-1-4471-2318-7_13},
 isbn = {978-1-4471-2318-7},
 language = {en},
 pages = {181--194},
 publisher = {Springer},
 title = {Successive {Reduction} of {Arms} in {Multi}-{Armed} {Bandits}},
 year = {2011}
}

@inproceedings{gupta_thompson_2011,
 abstract = {The importance of multi-armed bandit (MAB) problems is on the rise due to their recent application in a large variety of areas such as online advertising, news article selection, wireless networks, and medicinal trials, to name a few. The most common assumption made when solving such MAB problems is that the unknown reward probability θk of each bandit arm k is ﬁxed. However, this assumption rarely holds in practice simply because real-life problems often involve underlying processes that are dynamically evolving. In this paper, we model problems where reward probabilities θk are drifting, and introduce a new method called Dynamic Thompson Sampling (DTS) that facilitates Order Statistics based Thompson Sampling for these dynamically evolving MABs. The DTS algorithm adapts its success probability estimates, θˆk, faster than traditional Thompson Sampling schemes and thus leads to improved performance in terms of lower regret. Extensive experiments demonstrate that DTS outperforms current state-of-the-art approaches, namely pure Thompson Sampling, UCB-Normal and UCBf , for the case of dynamic reward probabilities. Furthermore, this performance advantage increases persistently with the number of bandit arms.},
 author = {Gupta, Neha and Granmo, Ole-Christoffer and Agrawala, Ashok},
 booktitle = {2011 10th {International} {Conference} on {Machine} {Learning} and {Applications} and {Workshops}},
 doi = {10.1109/ICMLA.2011.144},
 file = {Gupta et al. - 2011 - Thompson Sampling for Dynamic Multi-armed Bandits.pdf:/home/admin-u6015325/Zotero/storage/VN32WRWD/Gupta et al. - 2011 - Thompson Sampling for Dynamic Multi-armed Bandits.pdf:application/pdf},
 isbn = {978-1-4577-2134-2 978-0-7695-4607-0},
 language = {en},
 pages = {484--489},
 publisher = {IEEE},
 title = {Thompson {Sampling} for {Dynamic} {Multi}-armed {Bandits}},
 urldate = {2020-09-30},
 year = {2011}
}

@incollection{hoeffding1994probability,
 author = {Hoeffding, Wassily},
 booktitle = {The Collected Works of Wassily Hoeffding},
 pages = {409--426},
 publisher = {Springer},
 title = {Probability inequalities for sums of bounded random variables},
 year = {1994}
}

@inproceedings{honda2014optimality,
 author = {Junya Honda and
Akimichi Takemura},
 booktitle = {Proceedings of the Seventeenth International Conference on Artificial
Intelligence and Statistics, {AISTATS} 2014, Reykjavik, Iceland, April
22-25, 2014},
 pages = {375--383},
 publisher = {JMLR.org},
 series = {{JMLR} Workshop and Conference Proceedings},
 title = {Optimality of Thompson Sampling for Gaussian Bandits Depends on Priors},
 volume = {33},
 year = {2014}
}

@article{howard_sequential_2019,
 abstract = {Consider the problem of sequentially estimating quantiles of any distribution over a complete, fully-ordered set, based on a stream of i.i.d. observations. We propose new, theoretically sound and practically tight confidence sequences for quantiles, that is, sequences of confidence intervals which are valid uniformly over time. We give two methods for tracking a fixed quantile and two methods for tracking all quantiles simultaneously. Specifically, we provide explicit expressions with small constants for intervals whose widths shrink at the fastest possible \${\textbackslash}sqrt\{t{\textasciicircum}\{-1\} {\textbackslash}log{\textbackslash}log t\}\$ rate, as determined by the law of the iterated logarithm (LIL). As a byproduct, we give a non-asymptotic concentration inequality for the empirical distribution function which holds uniformly over time with the LIL rate, thus strengthening Smirnov's asymptotic empirical process LIL, and extending the famed Dvoretzky-Kiefer-Wolfowitz (DKW) inequality to hold uniformly over all sample sizes while only being about twice as wide in practice. This inequality directly yields sequential analogues of the one- and two-sample Kolmogorov-Smirnov tests, and a test of stochastic dominance. We apply our results to the problem of selecting an arm with an approximately best quantile in a multi-armed bandit framework, proving a state-of-the-art sample complexity bound for a novel allocation strategy. Simulations demonstrate that our method stops with fewer samples than existing methods by a factor of five to fifty. Finally, we show how to compute confidence sequences for the difference between quantiles of two arms in an A/B test, along with corresponding always-valid \$p\$-values.},
 annote = {Comment: 29 pages, 8 figures},
 author = {Howard, Steven R. and Ramdas, Aaditya},
 journal = {arXiv:1906.09712},
 keywords = {Statistics - Machine Learning, Mathematics - Statistics Theory, Mathematics - Probability, quantile bai, Statistics - Methodology},
 title = {Sequential estimation of quantiles with applications to {A}/{B}-testing and best-arm identification},
 urldate = {2020-01-13},
 year = {2019}
}

@article{huber1964,
 author = {Huber, Peter J.},
 fjournal = {The Annals of Mathematical Statistics},
 journal = {Ann. Math. Statist.},
 number = {1},
 pages = {73--101},
 publisher = {The Institute of Mathematical Statistics},
 title = {Robust Estimation of a Location Parameter},
 volume = {35},
 year = {1964}
}

@inproceedings{jamieson_lil_nodate,
 author = {Kevin G. Jamieson and
Matthew Malloy and
Robert D. Nowak and
S{\'{e}}bastien Bubeck},
 booktitle = {Proceedings of The 27th Conference on Learning Theory, {COLT} 2014,
Barcelona, Spain, June 13-15, 2014},
 pages = {423--439},
 publisher = {JMLR.org},
 series = {{JMLR} Workshop and Conference Proceedings},
 title = {lil' {UCB} : An Optimal Exploration Algorithm for Multi-Armed Bandits},
 volume = {35},
 year = {2014}
}

@inproceedings{kagrecha2019distribution,
 author = {Anmol Kagrecha and
Jayakrishnan Nair and
Krishna P. Jagannathan},
 booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada},
 pages = {11269--11278},
 title = {Distribution oblivious, risk-aware algorithms for multi-armed bandits
with unbounded rewards},
 year = {2019}
}

@inproceedings{kagrecha_distribution_2019,
 author = {Anmol Kagrecha and
Jayakrishnan Nair and
Krishna P. Jagannathan},
 booktitle = {Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December
8-14, 2019, Vancouver, BC, Canada},
 pages = {11269--11278},
 title = {Distribution oblivious, risk-aware algorithms for multi-armed bandits
with unbounded rewards},
 year = {2019}
}

@inproceedings{kalyanakrishnan2012pac,
 author = {Shivaram Kalyanakrishnan and
Ambuj Tewari and
Peter Auer and
Peter Stone},
 booktitle = {Proceedings of the 29th International Conference on Machine Learning,
{ICML} 2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012},
 publisher = {icml.cc / Omnipress},
 title = {{PAC} Subset Selection in Stochastic Multi-armed Bandits},
 year = {2012}
}

@inproceedings{kandasamy_parallelised_ts,
 author = {Kirthevasan Kandasamy and
Akshay Krishnamurthy and
Jeff Schneider and
Barnab{\'{a}}s P{\'{o}}czos},
 booktitle = {International Conference on Artificial Intelligence and Statistics,
{AISTATS} 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands,
Spain},
 pages = {133--142},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 title = {Parallelised Bayesian Optimisation via Thompson Sampling},
 volume = {84},
 year = {2018}
}

@article{kapoor_corruption-tolerant_2019,
 abstract = {We present algorithms for solving multi-armed and linear-contextual bandit tasks in the face of adversarial corruptions in the arm responses. Traditional algorithms for solving these problems assume that nothing but mild, e.g., i.i.d. sub-Gaussian, noise disrupts an otherwise clean estimate of the utility of the arm. This assumption and the resulting approaches can fail catastrophically if there is an observant adversary that corrupts even a small fraction of the responses generated when arms are pulled. To rectify this, we propose algorithms that use recent advances in robust statistical estimation to perform arm selection in polynomial time. Our algorithms are easy to implement and vastly outperform several existing UCB and EXP-style algorithms for stochastic and adversarial multi-armed and linear-contextual bandit problems in wide variety of experimental settings. Our algorithms enjoy minimax-optimal regret bounds, as well as can tolerate an adversary that is allowed to corrupt upto a universally constant fraction of the arms pulled by the algorithm.},
 author = {Kapoor, Sayash and Patel, Kumar Kshitij and Kar, Purushottam},
 doi = {10.1007/s10994-018-5758-5},
 file = {Springer Full Text PDF:/home/admin-u6015325/Zotero/storage/2KH5X8KY/Kapoor et al. - 2019 - Corruption-tolerant bandit learning.pdf:application/pdf},
 issn = {1573-0565},
 journal = {Machine Learning},
 language = {en},
 number = {4},
 pages = {687--715},
 title = {Corruption-tolerant bandit learning},
 urldate = {2020-03-13},
 volume = {108},
 year = {2019}
}

@phdthesis{kearns89,
 author = {M. J. Kearns},
 school = {Department of Computer Science, Harvard University},
 title = {Computational Complexity of Machine Learning},
 year = {1989}
}

@article{kolla_concentration_2019,
 abstract = {Conditional Value-at-Risk (CVaR) is a popular risk measure for modelling losses in the case of a rare but extreme event. We consider the problem of estimating CVaR from i.i.d. samples of an unbounded random variable, which is either sub-Gaussian or sub-exponential. We derive a novel one-sided concentration bound for a natural sample-based CVaR estimator in this setting. Our bound relies on a concentration result for a quantile-based estimator for Value-at-Risk (VaR), which may be of independent interest.},
 author = {Kolla, Ravi Kumar and L.a., Prashanth and P. Bhat, Sanjay and Jagannathan, Krishna},
 file = {ScienceDirect Full Text PDF:/home/admin-u6015325/Zotero/storage/73S4NQ52/Kolla et al. - 2019 - Concentration bounds for empirical conditional val.pdf:application/pdf;ScienceDirect Snapshot:/home/admin-u6015325/Zotero/storage/UFDXXEUW/S0167637718303869.html:text/html},
 journal = {Operations Research Letters},
 keywords = {Concentration bounds, Conditional value-at-risk, Sub-exponential distributions, Sub-Gaussian distributions, Value-at-risk},
 number = {1},
 pages = {16--20},
 shorttitle = {Concentration bounds for empirical conditional value-at-risk},
 title = {Concentration bounds for empirical conditional value-at-risk: {The} unbounded case},
 urldate = {2019-09-23},
 volume = {47},
 year = {2019}
}

@inproceedings{la2019concentration,
 author = {Prashanth L. A. and
Krishna P. Jagannathan and
Ravi Kumar Kolla},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning,
{ICML} 2020, 13-18 July 2020, Virtual Event},
 pages = {5577--5586},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 title = {Concentration bounds for CVaR estimation: The cases of light-tailed
and heavy-tailed distributions},
 volume = {119},
 year = {2020}
}

@inproceedings{la_concentration_nodate,
 author = {Prashanth L. A. and
Krishna P. Jagannathan and
Ravi Kumar Kolla},
 booktitle = {Proceedings of the 37th International Conference on Machine Learning,
{ICML} 2020, 13-18 July 2020, Virtual Event},
 pages = {5577--5586},
 publisher = {{PMLR}},
 series = {Proceedings of Machine Learning Research},
 title = {Concentration bounds for CVaR estimation: The cases of light-tailed
and heavy-tailed distributions},
 volume = {119},
 year = {2020}
}

@article{lai1985asymptotically,
 author = {Lai, Tze Leung and Robbins, Herbert},
 journal = {Advances in applied mathematics},
 number = {1},
 pages = {4--22},
 publisher = {Academic Press},
 title = {Asymptotically efficient adaptive allocation rules},
 volume = {6},
 year = {1985}
}

@inproceedings{langley00,
 author = {Pat Langley},
 booktitle = {Proceedings of the Seventeenth International Conference on Machine
Learning {(ICML} 2000), Stanford University, Stanford, CA, USA, June
29 - July 2, 2000},
 pages = {1207--1216},
 publisher = {Morgan Kaufmann},
 title = {Crafting Papers on Machine Learning},
 year = {2000}
}

@book{lattimore2018bandit,
 author = {Lattimore, Tor and Szepesvári, Csaba},
 doi = {10.1017/9781108571401},
 place = {Cambridge},
 publisher = {Cambridge University Press},
 title = {Bandit Algorithms},
 year = {2020}
}

@article{lecue_robust_2017,
 annote = {Comment: 48 pages, 6 figures},
 author = {Lecué, Guillaume and Lerasle, Matthieu},
 file = {arXiv\:1711.10306 PDF:/home/admin-u6015325/Zotero/storage/ZPRK87L7/Lecué and Lerasle - 2017 - Robust machine learning by median-of-means  theor.pdf:application/pdf;arXiv.org Snapshot:/home/admin-u6015325/Zotero/storage/G7AMGZTS/1711.html:text/html},
 journal = {arXiv:1711.10306 [math, stat]},
 keywords = {Mathematics - Statistics Theory},
 note = {arXiv: 1711.10306},
 shorttitle = {Robust machine learning by median-of-means},
 title = {Robust machine learning by median-of-means : theory and practice},
 year = {2017}
}

@book{ledoux2001concentration,
 author = {Ledoux, Michel},
 publisher = {American Mathematical Soc.},
 title = {The concentration of measure phenomenon},
 year = {2001}
}

@inproceedings{li2010contextual,
 author = {Lihong Li and
Wei Chu and
John Langford and
Robert E. Schapire},
 booktitle = {Proceedings of the 19th International Conference on World Wide Web,
{WWW} 2010, Raleigh, North Carolina, USA, April 26-30, 2010},
 doi = {10.1145/1772690.1772758},
 pages = {661--670},
 publisher = {{ACM}},
 title = {A contextual-bandit approach to personalized news article recommendation},
 year = {2010}
}

@article{li2018convergence,
 author = {Li, Yongming and Zhou, Yong and Liu, Chao},
 journal = {Journal of inequalities and applications},
 number = {1},
 pages = {1--10},
 publisher = {SpringerOpen},
 title = {On the convergence rates of kernel estimator and hazard estimator for widely dependent samples},
 volume = {2018},
 year = {2018}
}

@article{LibinBAI:2017,
 archiveprefix = {arXiv},
 author = {Pieter Libin and
Timothy Verstraeten and
Diederik M. Roijers and
Jelena Grujic and
Kristof Theys and
Philippe Lemey and
Ann Now{\'{e}}},
 eprint = {1711.06299},
 journal = {CoRR},
 title = {Bayesian Best-Arm Identification for Selecting Influenza Mitigation
Strategies},
 volume = {abs/1711.06299},
 year = {2017}
}

@inproceedings{LibinRegret:2017,
 author = {Libin, Pieter and Verstraeten, Timothy and Theys, Kristof and Roijers, Diederik M and Vrancx, Peter and Now{\'e}, Ann},
 booktitle = {International Conference on Autonomous Agents and Multiagent Systems},
 organization = {Springer},
 pages = {67--85},
 title = {Efficient evaluation of influenza mitigation strategies using preventive bandits},
 year = {2017}
}

@book{MachineLearningI,
 publisher = {Tioga},
 title = {Machine Learning: An Artificial Intelligence
Approach, Vol. I},
 year = {1983}
}

@inproceedings{maillard_robust_2013,
 abstract = {We study a variant of the standard stochastic multi-armed bandit problem when one is not interested in the arm with the best mean, but instead in the arm maximizing some coherent risk measure criterion. Further, we are studying the deviations of the regret instead of the less informative expected regret. We provide an algorithm, called RA-UCB to solve this problem, together with a high probability bound on its regret.},
 author = {Maillard, Odalric-Ambrym},
 booktitle = {Algorithmic {Learning} {Theory}},
 file = {Springer Full Text PDF:/home/admin-u6015325/Zotero/storage/XV9DAGF9/Maillard - 2013 - Robust Risk-Averse Stochastic Multi-armed Bandits.pdf:application/pdf},
 keywords = {Multi-armed bandits, coherent risk measure, concentration of measure, cumulant generative function},
 language = {en},
 pages = {218--233},
 publisher = {Springer Berlin Heidelberg},
 series = {Lecture {Notes} in {Computer} {Science}},
 title = {Robust {Risk}-{Averse} {Stochastic} {Multi}-armed {Bandits}},
 year = {2013}
}

@inproceedings{massart2000some,
 author = {Massart, Pascal},
 booktitle = {Annales de la Facult{\'e} des sciences de Toulouse: Math{\'e}matiques},
 title = {Some applications of concentration inequalities to statistics},
 year = {2000}
}

@techreport{mitchell80,
 author = {T. M. Mitchell},
 institution = {Computer Science Department, Rutgers University},
 title = {The Need for Biases in Learning Generalizations},
 year = {1980}
}

@article{muller2000possible,
 author = {M{\"u}ller, J{\"o}rg W},
 journal = {Journal of research of the National Institute of Standards and Technology},
 number = {4},
 pages = {551},
 publisher = {National Institute of Standards and Technology},
 title = {Possible advantages of a robust evaluation of comparisons},
 volume = {105},
 year = {2000}
}

@techreport{munos_bandits_2014,
 abstract = {This work covers several aspects of the optimism in the face of uncertainty principle applied to large scale optimization problems under finite numerical budget. The initial motivation for the research reported here originated from the empirical success of the so-called Monte-Carlo Tree Search method popularized in computer-go and further extended to many other games as well as optimization and planning problems. Our objective is to contribute to the development of theoretical foundations of the field by characterizing the complexity of the underlying optimization problems and designing efficient algorithms with performance guarantees. The main idea presented here is that it is possible to decompose a complex decision making problem (such as an optimization problem in a large search space) into a sequence of elementary decisions, where each decision of the sequence is solved using a (stochastic) multi-armed bandit (simple mathematical model for decision making in stochastic environments). This so-called hierarchical bandit approach (where the reward observed by a bandit in the hierarchy is itself the return of another bandit at a deeper level) possesses the nice feature of starting the exploration by a quasi-uniform sampling of the space and then focusing progressively on the most promising area, at different scales, according to the evaluations observed so far, and eventually performing a local search around the global optima of the function. The performance of the method is assessed in terms of the optimality of the returned solution as a function of the number of function evaluations. Our main contribution to the field of function optimization is a class of hierarchical optimistic algorithms designed for general search spaces (such as metric spaces, trees, graphs, Euclidean spaces, ...) with different algorithmic instantiations depending on whether the evaluations are noisy or noiseless and whether some measure of the ''smoothness'' of the function is known or unknown. The performance of the algorithms depend on the local behavior of the function around its global optima expressed in terms of the quantity of near-optimal states measured with some metric. If this local smoothness of the function is known then one can design very efficient optimization algorithms (with convergence rate independent of the space dimension), and when it is not known, we can build adaptive techniques that can, in some cases, perform almost as well as when it is known.},
 annote = {130 pages},
 author = {Munos, Rémi},
 file = {HAL PDF Full Text:/home/admin-u6015325/Zotero/storage/JYRSNVF5/Munos - 2014 - From Bandits to Monte-Carlo Tree Search The Optim.pdf:application/pdf},
 keywords = {Bandit theory, Monte-Carlo Tree Search, Optimism in the face of uncertainty, Upper Confidence Bounds},
 shorttitle = {From {Bandits} to {Monte}-{Carlo} {Tree} {Search}},
 title = {From {Bandits} to {Monte}-{Carlo} {Tree} {Search}: {The} {Optimistic} {Principle} {Applied} to {Optimization} and {Planning}},
 urldate = {2020-09-04},
 year = {2014}
}

@incollection{Newell81,
 author = {A. Newell and P. S. Rosenbloom},
 booktitle = {Cognitive Skills and Their Acquisition},
 chapter = {1},
 pages = {1--51},
 publisher = {Lawrence Erlbaum Associates, Inc.},
 title = {Mechanisms of Skill Acquisition and the Law of
Practice},
 year = {1981}
}

@article{okolewski_sharp_bound_order_stat_2001,
 abstract = {Sharp distribution-free lower and upper bounds on the bias in estimating quantiles by the sample counterparts are obtained by the use of Moriguti's greatest convex minorant approach.},
 author = {Okolewski, Andrzej and Rychlik, Tomasz},
 doi = {10.1016/S0167-7152(00)00242-X},
 file = {ScienceDirect Full Text PDF:/home/admin-u6015325/Zotero/storage/4DLFVANX/Okolewski and Rychlik - 2001 - Sharp distribution-free bounds on the bias in esti.pdf:application/pdf;ScienceDirect Snapshot:/home/admin-u6015325/Zotero/storage/6N79TQZ8/S016771520000242X.html:text/html},
 issn = {0167-7152},
 journal = {Statistics \& Probability Letters},
 keywords = {Bernstein polynomial, Bias, Order statistic, Quantile, Sample quantile, The greatest convex minorant, Variation diminishing property},
 language = {en},
 number = {2},
 pages = {207--213},
 title = {Sharp distribution-free bounds on the bias in estimating quantiles via order statistics},
 urldate = {2020-09-27},
 volume = {52},
 year = {2001}
}

@article{panaphut_ceftriaxone_2003,
 abstract = {Abstract.  A prospective, open-label, randomized trial at Khon Kaen Hospital (Thailand) was conducted from July 2000 through December 2001 to compare the clinic},
 author = {Panaphut, Thanachai and Domrongkitchaiporn, Somnuek and Vibhagool, Asda and Thinkamrop, Bandit and Susaengrat, Wattanachai},
 file = {Full Text PDF:/home/admin-u6015325/Zotero/storage/Y2WW473K/Panaphut et al. - 2003 - Ceftriaxone Compared with Sodium Penicillin G for .pdf:application/pdf;Snapshot:/home/admin-u6015325/Zotero/storage/Q2AQPU4B/297044.html:text/html},
 journal = {Clinical Infectious Diseases},
 language = {en},
 number = {12},
 pages = {1507--1513},
 title = {Ceftriaxone {Compared} with {Sodium} {Penicillin} {G} for {Treatment} of {Severe} {Leptospirosis}},
 volume = {36},
 year = {2003}
}

@article{panaput_dialysis_2014,
 author = {Panaput, Thanachai and Thinkhamrop, Bandit and Domrongkitchaiporn, Somnuek and Sirivongs, Dhavee and Praderm, Laksamon and Anukulanantachai, Jirasak and Kanokkantapong, Chavasak and Tungkasereerak, Pakorn and Pongskul, Cholatip and Anutrakulchai, Sirirat and Keobounma, Thathsalang and Narenpitak, Surapong and Intarawongchot, Pisith and Suwattanasin, Ammrit and Tatiyanupanwong, Sajja and Niwattayakul, Kannika},
 file = {Panaput et al. - 2014 - Dialysis Dose and Risk Factors for Death Among ESR.pdf:/home/admin-u6015325/Zotero/storage/W6AHJNHJ/Panaput et al. - 2014 - Dialysis Dose and Risk Factors for Death Among ESR.pdf:application/pdf},
 journal = {Blood Purification},
 language = {en},
 number = {3-4},
 pages = {253--262},
 shorttitle = {Dialysis {Dose} and {Risk} {Factors} for {Death} {Among} {ESRD} {Patients} {Treated} with {Twice}-{Weekly} {Hemodialysis}},
 title = {Dialysis {Dose} and {Risk} {Factors} for {Death} {Among} {ESRD} {Patients} {Treated} with {Twice}-{Weekly} {Hemodialysis}: {A} {Prospective} {Cohort} {Study}},
 volume = {38},
 year = {2014}
}

@article{rinne_hazard_nodate,
 author = {Rinne, Horst},
 file = {Rinne - The Hazard Rate – Theory and Inference.pdf:/home/admin-u6015325/Zotero/storage/D5QPQF7H/Rinne - The Hazard Rate – Theory and Inference.pdf:application/pdf},
 language = {en},
 pages = {296},
 title = {The {Hazard} {Rate} – {Theory} and {Inference}}
}

@article{rockafellar2000optimization,
 author = {Rockafellar, R Tyrrell and Uryasev, Stanislav and others},
 journal = {Journal of risk},
 pages = {21--42},
 title = {Optimization of conditional value-at-risk},
 volume = {2},
 year = {2000}
}

@article{Rousseeuw2011,
 abstract = {Abstract When analyzing data, outlying observations cause problems because they may strongly influence the result. Robust statistics aims at detecting the outliers by searching for the model fitted by the majority of the data. We present an overview of several robust methods and outlier detection tools. We discuss robust procedures for univariate, low-dimensional, and high-dimensional data such as estimation of location and scatter, linear regression, principal component analysis, and classification. © 2011 John Wiley \& Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 73-79 DOI: 10.1002/widm.2 This article is categorized under: Algorithmic Development > Biological Data Mining Algorithmic Development > Spatial and Temporal Data Mining Application Areas > Health Care Technologies > Structure Discovery and Clustering},
 author = {Rousseeuw, Peter J. and Hubert, Mia},
 journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
 number = {1},
 pages = {73-79},
 title = {Robust statistics for outlier detection},
 volume = {1},
 year = {2011}
}

@incollection{rychlik_bounds+expectation_L_estimates_1998,
 author = {Rychlik, Tomasz},
 booktitle = {Handbook of {Statistics}},
 doi = {10.1016/S0169-7161(98)16008-X},
 file = {ScienceDirect Snapshot:/home/admin-u6015325/Zotero/storage/S8KJSRM8/S016971619816008X.html:text/html},
 language = {en},
 pages = {105--145},
 publisher = {Elsevier},
 series = {Order {Statistics}: {Theory} \& {Methods}},
 title = {6 {Bounds} for expectations of {L}-estimates},
 urldate = {2020-09-30},
 volume = {16},
 year = {1998}
}

@article{RYZHOV20111363,
 abstract = {We consider a class of multi-armed bandit problems where the reward obtained by pulling an arm is drawn from an exponential distribution whose parameter is unknown. A Bayesian model with independent gamma priors is used to represent our beliefs and uncertainty about the exponential parameters. We derive a precise expression for the marginal value of information in this problem, which allows us to create a new knowledge gradient (KG) policy for making decisions. The policy is practical and easy to implement, making a case for value of information as a general approach to optimal learning problems with many different types of learning models.},
 author = {Ilya O. Ryzhov and Warren B. Powell},
 journal = {Procedia Computer Science},
 keywords = {multi-armed bandit, knowledge gradient, optimal learning, exponential rewards},
 note = {Proceedings of the International Conference on Computational Science, ICCS 2011},
 pages = {1363 - 1372},
 title = {The value of information in multi-armed bandits with exponentially distributed rewards},
 volume = {4},
 year = {2011}
}

@article{Samuel59,
 author = {A. L. Samuel},
 journal = {IBM Journal of Research and Development},
 number = {3},
 pages = {211--229},
 title = {Some Studies in Machine Learning Using the Game of
Checkers},
 volume = {3},
 year = {1959}
}

@inproceedings{sani_risk-aversion_2012,
 author = {Amir Sani and
Alessandro Lazaric and
R{\'{e}}mi Munos},
 booktitle = {Advances in Neural Information Processing Systems 25: 26th Annual
Conference on Neural Information Processing Systems 2012. Proceedings
of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States},
 pages = {3284--3292},
 title = {Risk-Aversion in Multi-armed Bandits},
 year = {2012}
}

@article{schwartz2017customer,
 author = {Schwartz, Eric M and Bradlow, Eric T and Fader, Peter S},
 journal = {Marketing Science},
 number = {4},
 pages = {500--522},
 publisher = {INFORMS},
 title = {Customer acquisition via display advertising using multi-armed bandit experiments},
 volume = {36},
 year = {2017}
}

@inproceedings{simchowitz_best--k_2016,
 author = {Max Simchowitz and
Kevin G. Jamieson and
Benjamin Recht},
 booktitle = {Proceedings of the 29th Conference on Learning Theory, {COLT} 2016,
New York, USA, June 23-26, 2016},
 pages = {1440--1489},
 publisher = {JMLR.org},
 series = {{JMLR} Workshop and Conference Proceedings},
 title = {Best-of-K-bandits},
 volume = {49},
 year = {2016}
}

@article{smirnov_approximate_nodate1944,
 author = {Smirnov, N V},
 file = {Smirnov - Approximate laws of distribution of random variabl.pdf:/home/admin-u6015325/Zotero/storage/B5ZDGJUQ/Smirnov - Approximate laws of distribution of random variabl.pdf:application/pdf},
 language = {ru},
 pages = {29},
 title = {Approximate laws of distribution of random variables from empirical data}
}

@misc{SpectralRisk2019estimation,
 archiveprefix = {arXiv},
 author = {Ajay Kumar Pandey and Prashanth L. A. and Sanjay P. Bhat},
 eprint = {1912.10398},
 primaryclass = {cs.LG},
 title = {Estimation of Spectral Risk Measures},
 year = {2019}
}

@article{srinivas_information-theoretic_2012,
 abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multiarmed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low norm in a reproducing kernel Hilbert space. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze an intuitive Gaussian process upper confidence bound (GP-UCB) algorithm, and bound its cumulative regret in terms of maximal in- formation gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
 author = {Srinivas, N. and Krause, A. and Kakade, S. M. and Seeger, M. W.},
 doi = {10.1109/TIT.2011.2182033},
 file = {IEEE Xplore Abstract Record:/home/admin-u6015325/Zotero/storage/GSPAATKC/6138914.html:text/html;IEEE Xplore Full Text PDF:/home/admin-u6015325/Zotero/storage/S5ZUXN4U/Srinivas et al. - 2012 - Information-Theoretic Regret Bounds for Gaussian P.pdf:application/pdf},
 issn = {0018-9448},
 journal = {IEEE Transactions on Information Theory},
 keywords = {Gaussian processes, Bandit problems, bandit setting, Bayesian methods, Bayesian prediction, Convergence, cumulative regret, experimental design, Gaussian process (GP), Gaussian process optimization, Hilbert spaces, information gain, information theory, information-theoretic regret bounds, intuitive Gaussian process upper confidence bound algorithm, Kernel, multiarmed bandit problem, Noise, nonparametric statistics, online learning, Optimization, payoff function, regret bound, reproducing kernel Hilbert space, statistical learning, sublinear regret bounds, Temperature sensors},
 number = {5},
 pages = {3250--3265},
 title = {Information-{Theoretic} {Regret} {Bounds} for {Gaussian} {Process} {Optimization} in the {Bandit} {Setting}},
 volume = {58},
 year = {2012}
}

@inproceedings{szorenyi2015qualitative,
 author = {Bal{\'{a}}zs Sz{\"{o}}r{\'{e}}nyi and
R{\'{o}}bert Busa{-}Fekete and
Paul Weng and
Eyke H{\"{u}}llermeier},
 booktitle = {Proceedings of the 32nd International Conference on Machine Learning,
{ICML} 2015, Lille, France, 6-11 July 2015},
 pages = {1660--1668},
 publisher = {JMLR.org},
 series = {{JMLR} Workshop and Conference Proceedings},
 title = {Qualitative Multi-Armed Bandits: {A} Quantile-Based Approach},
 volume = {37},
 year = {2015}
}

@article{tamkind2019,
 author = {Tamkin, Alex and Keramati, Ramtin and Dann, Christoph and Brunskill, Emma},
 journal = {NeurIPS},
 title = {Distributionally-Aware Exploration for CVaR Bandits},
 year = {2019}
}

@article{thompson1933likelihood,
 author = {Thompson, William R},
 journal = {Biometrika},
 number = {3/4},
 pages = {285--294},
 publisher = {JSTOR},
 title = {On the likelihood that one unknown probability exceeds another in view of the evidence of two samples},
 volume = {25},
 year = {1933}
}

@article{torossian_bayesian_2020,
 abstract = {Bayesian optimisation is widely used to optimise stochastic black box functions. While most strategies are focused on optimising conditional expectations, a large variety of applications require risk-averse decisions and alternative criteria accounting for the distribution tails need to be considered. In this paper, we propose new variational models for Bayesian quantile and expectile regression that are well-suited for heteroscedastic settings. Our models consist of two latent Gaussian processes accounting respectively for the conditional quantile (or expectile) and variance that are chained through asymmetric likelihood functions. Furthermore, we propose two Bayesian optimisation strategies, either derived from a GP-UCB or Thompson sampling, that are tailored to such models and that can accommodate large batches of points. As illustrated in the experimental section, the proposed approach clearly outperforms the state of the art.},
 author = {Torossian, Léonard and Picheny, Victor and Durrande, Nicolas},
 file = {arXiv Fulltext PDF:/home/admin-u6015325/Zotero/storage/K6V5RTPD/Torossian et al. - 2020 - Bayesian Quantile and Expectile Optimisation.pdf:application/pdf},
 journal = {arXiv:2001.04833 [cs, stat]},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
 note = {arXiv: 2001.04833},
 title = {Bayesian {Quantile} and {Expectile} {Optimisation}},
 urldate = {2020-09-04},
 year = {2020}
}

@inproceedings{torossian_x-armed_2019,
 author = {Torossian, L{\'e}onard and Garivier, Aur{\'e}lien and Picheny, Victor},
 booktitle = {Asian Conference on Machine Learning},
 organization = {PMLR},
 pages = {252--267},
 title = {X-Armed Bandits: Optimizing Quantiles, CVaR and Other Risks},
 year = {2019}
}

@article{tran-thanh_functional_2014,
 abstract = {We introduce the functional bandit problem, where the objective is to find an arm that optimises a known functional of the unknown arm-reward distributions. These problems arise in many settings such as maximum entropy methods in natural language processing, and risk-averse decision-making, but current best-arm identification techniques fail in these domains. We propose a new approach, that combines functional estimation and arm elimination, to tackle this problem. This method achieves provably efficient performance guarantees. In addition, we illustrate this method on a number of important functionals in risk management and information theory, and refine our generic theoretical results in those cases.},
 author = {Tran-Thanh, Long and Yu, Jia Yuan},
 journal = {arXiv:1405.2432 [cs, stat]},
 keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
 note = {arXiv: 1405.2432},
 title = {Functional {Bandits}},
 year = {2014}
}

@article{wald1944,
 author = {Wald, Abraham},
 journal = {The Annals of Mathematical Statistics},
 title = {On cumulative sums of random variables},
 year = {1944}
}

@book{wasserman2013all,
 author = {Wasserman, Larry},
 publisher = {Springer Science \& Business Media},
 title = {All of statistics: a concise course in statistical inference},
 year = {2013}
}

@inproceedings{yu2013sample,
 author = {Jia Yuan Yu and
Evdokia Nikolova},
 booktitle = {{IJCAI} 2013, Proceedings of the 23rd International Joint Conference
on Artificial Intelligence, Beijing, China, August 3-9, 2013},
 pages = {2576--2582},
 publisher = {{IJCAI/AAAI}},
 title = {Sample Complexity of Risk-Averse Bandit-Arm Selection},
 year = {2013}
}

@inproceedings{yu2018pure,
 author = {Xiaotian Yu and
Han Shao and
Michael R. Lyu and
Irwin King},
 booktitle = {Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial
Intelligence, {UAI} 2018, Monterey, California, USA, August 6-10,
2018},
 pages = {937--946},
 publisher = {{AUAI} Press},
 title = {Pure Exploration of Multi-Armed Bandits with Heavy-Tailed Payoffs},
 year = {2018}
}

@book{zielinski_quantile_est_survey,
  title={Optimal Quantile Estimators Small Sample Approach},
  author={Zielinski, Ryszard},
  year={2004},
  publisher={Polish Academy of Sciences. Institute of Mathematics}
}


@phdthesis{Blom1958,
   author = {Blom, Gunnar},
   institution = {Stockholm College},
   pages = {174},
   school = {, Stockholm College},
   title = {Statistical estimates and transformed beta-variables},
   URL = {https://pubs.sub.su.se/1850.pdf},
   year = {1958}
}

