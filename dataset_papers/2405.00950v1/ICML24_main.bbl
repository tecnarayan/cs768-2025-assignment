\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Altman(1999)]{altman1999constrained}
Altman, E.
\newblock \emph{Constrained Markov decision processes}, volume~7.
\newblock CRC Press, 1999.

\bibitem[Avrachenkov \& Borkar(2022)Avrachenkov and
  Borkar]{avrachenkov2022whittle}
Avrachenkov, K.~E. and Borkar, V.~S.
\newblock Whittle index based q-learning for restless bandits with average
  reward.
\newblock \emph{Automatica}, 139:\penalty0 110186, 2022.

\bibitem[Bercu et~al.(2015)Bercu, Delyon, Rio, et~al.]{bercu2015concentration}
Bercu, B., Delyon, B., Rio, E., et~al.
\newblock \emph{Concentration inequalities for sums and martingales}.
\newblock Springer, 2015.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{boyd2004convex}
Boyd, S.~P. and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Cohen et~al.(2014)Cohen, Zhao, and Scaglione]{cohen2014restless}
Cohen, K., Zhao, Q., and Scaglione, A.
\newblock Restless multi-armed bandits under time-varying activation
  constraints for dynamic spectrum access.
\newblock In \emph{2014 48th Asilomar Conference on Signals, Systems and
  Computers}, pp.\  1575--1578. IEEE, 2014.

\bibitem[Even-Dar et~al.(2009)Even-Dar, Kakade, and Mansour]{even2009online}
Even-Dar, E., Kakade, S.~M., and Mansour, Y.
\newblock Online markov decision processes.
\newblock \emph{Mathematics of Operations Research}, 34\penalty0 (3):\penalty0
  726--736, 2009.

\bibitem[Germano et~al.(2023)Germano, Stradi, Genalti, Castiglioni, Marchesi,
  and Gatti]{germano2023best}
Germano, J., Stradi, F.~E., Genalti, G., Castiglioni, M., Marchesi, A., and
  Gatti, N.
\newblock A best-of-both-worlds algorithm for constrained mdps with long-term
  constraints.
\newblock \emph{arXiv preprint arXiv:2304.14326}, 2023.

\bibitem[Glazebrook et~al.(2011)Glazebrook, Hodge, and
  Kirkbride]{glazebrook2011general}
Glazebrook, K.~D., Hodge, D.~J., and Kirkbride, C.
\newblock General notions of indexability for queueing control and asset
  management.
\newblock \emph{The Annals of Applied Probability}, 21\penalty0 (3):\penalty0
  876--907, 2011.

\bibitem[Herlihy et~al.(2023)Herlihy, Prins, Srinivasan, and
  Dickerson]{herlihy2023planning}
Herlihy, C., Prins, A., Srinivasan, A., and Dickerson, J.~P.
\newblock Planning to fairly allocate: Probabilistic fairness in the restless
  bandit setting.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, pp.\  732--740, 2023.

\bibitem[Hoeffding(1994)]{hoeffding1994probability}
Hoeffding, W.
\newblock Probability inequalities for sums of bounded random variables.
\newblock \emph{The collected works of Wassily Hoeffding}, pp.\  409--426,
  1994.

\bibitem[Jin et~al.(2020)Jin, Jin, Luo, Sra, and Yu]{jin2020learning}
Jin, C., Jin, T., Luo, H., Sra, S., and Yu, T.
\newblock Learning adversarial markov decision processes with bandit feedback
  and unknown transition.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4860--4869. PMLR, 2020.

\bibitem[Jin et~al.(2021)Jin, Huang, and Luo]{jin2021best}
Jin, T., Huang, L., and Luo, H.
\newblock The best of both worlds: stochastic and adversarial episodic mdps
  with unknown transition.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 20491--20502, 2021.

\bibitem[Jin et~al.(2022)Jin, Lancewicki, Luo, Mansour, and
  Rosenberg]{jin2022near}
Jin, T., Lancewicki, T., Luo, H., Mansour, Y., and Rosenberg, A.
\newblock Near-optimal regret for adversarial mdp with delayed bandit feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 33469--33481, 2022.

\bibitem[Jin et~al.(2023)Jin, Liu, Rouyer, Chan, We, and Luo]{jin2023no}
Jin, T., Liu, J., Rouyer, C., Chan, W., We, C.-Y., and Luo, H.
\newblock No-regret online reinforcement learning with adversarial losses and
  transitions.
\newblock \emph{arXiv preprint arXiv:2305.17380}, 2023.

\bibitem[Kang et~al.(2013)Kang, Prabhu, Sawyer, and Griffin]{kang2013markov}
Kang, Y., Prabhu, V.~V., Sawyer, A.~M., and Griffin, P.~M.
\newblock Markov models for treatment adherence in obstructive sleep apnea.
\newblock In \emph{IIE Annual Conference. Proceedings}, pp.\  1592. Institute
  of Industrial and Systems Engineers (IISE), 2013.

\bibitem[Killian et~al.(2021)Killian, Perrault, and Tambe]{killian2021beyond}
Killian, J.~A., Perrault, A., and Tambe, M.
\newblock {Beyond" To Act or Not to Act": Fast Lagrangian Approaches to General
  Multi-Action Restless Bandits}.
\newblock In \emph{Proc.of AAMAS}, 2021.

\bibitem[Larra{\~n}aga et~al.(2014)Larra{\~n}aga, Ayesta, and
  Verloop]{larranaga2014index}
Larra{\~n}aga, M., Ayesta, U., and Verloop, I.~M.
\newblock {Index Policies for A Multi-Class Queue with Convex Holding Cost and
  Abandonments}.
\newblock In \emph{Proc. of ACM Sigmetrics}, 2014.

\bibitem[Lee \& Lee(2023)Lee and Lee]{lee2023online}
Lee, D. and Lee, D.
\newblock Online resource allocation in episodic markov decision processes.
\newblock \emph{arXiv preprint arXiv:2305.10744}, 2023.

\bibitem[Li \& Varakantham(2022)Li and Varakantham]{li2022towards}
Li, D. and Varakantham, P.
\newblock Towards soft fairness in restless multi-armed bandits.
\newblock \emph{arXiv preprint arXiv:2207.13343}, 2022.

\bibitem[Luo et~al.(2021)Luo, Wei, and Lee]{luo2021policy}
Luo, H., Wei, C.-Y., and Lee, C.-W.
\newblock Policy optimization in adversarial mdps: Improved exploration via
  dilated bonuses.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 22931--22942, 2021.

\bibitem[Maurer \& Pontil(2009)Maurer and Pontil]{maurer2009empirical}
Maurer, A. and Pontil, M.
\newblock {Empirical Bernstein Bounds and Sample Variance Penalization}.
\newblock \emph{arXiv preprint arXiv:0907.3740}, 2009.

\bibitem[Neu(2015)]{neu2015explore}
Neu, G.
\newblock Explore no more: Improved high-probability regret bounds for
  non-stochastic bandits.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Neu \& Olkhovskaya(2021)Neu and Olkhovskaya]{neu2021online}
Neu, G. and Olkhovskaya, J.
\newblock Online learning in mdps with linear function approximation and bandit
  feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 10407--10417, 2021.

\bibitem[Neu et~al.(2012)Neu, Gyorgy, and Szepesv{\'a}ri]{neu2012adversarial}
Neu, G., Gyorgy, A., and Szepesv{\'a}ri, C.
\newblock The adversarial stochastic shortest path problem with unknown
  transition probabilities.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  805--813.
  PMLR, 2012.

\bibitem[Ni{\~n}o-Mora(2023)]{nino2023markovian}
Ni{\~n}o-Mora, J.
\newblock Markovian restless bandits and index policies: A review.
\newblock \emph{Mathematics}, 11\penalty0 (7):\penalty0 1639, 2023.

\bibitem[Papadimitriou \& Tsitsiklis(1994)Papadimitriou and
  Tsitsiklis]{papadimitriou1994complexity}
Papadimitriou, C.~H. and Tsitsiklis, J.~N.
\newblock The complexity of optimal queueing network control.
\newblock In \emph{Proceedings of IEEE 9th annual conference on structure in
  complexity Theory}, pp.\  318--322. IEEE, 1994.

\bibitem[Puterman(1994)]{puterman1994markov}
Puterman, M.~L.
\newblock \emph{{Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}}.
\newblock John Wiley \& Sons, 1994.

\bibitem[Qiu et~al.(2020)Qiu, Wei, Yang, Ye, and Wang]{qiu2020upper}
Qiu, S., Wei, X., Yang, Z., Ye, J., and Wang, Z.
\newblock Upper confidence primal-dual reinforcement learning for cmdp with
  adversarial loss.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15277--15287, 2020.

\bibitem[Rosenberg \& Mansour(2019{\natexlab{a}})Rosenberg and
  Mansour]{rosenberg2019b}
Rosenberg, A. and Mansour, Y.
\newblock Online stochastic shortest path with bandit feedback and unknown
  transition function.
\newblock volume~32, 2019{\natexlab{a}}.

\bibitem[Rosenberg \& Mansour(2019{\natexlab{b}})Rosenberg and
  Mansour]{rosenberg2019online}
Rosenberg, A. and Mansour, Y.
\newblock {Online Convex Optimization in Adversarial Markov Decision
  Processes}.
\newblock In \emph{Proc. of ICML}, 2019{\natexlab{b}}.

\bibitem[Sheng et~al.(2014)Sheng, Liu, and Saigal]{sheng2014data}
Sheng, S.-P., Liu, M., and Saigal, R.
\newblock {Data-Driven Channel Modeling Using Spectrum Measurement}.
\newblock \emph{IEEE Transactions on Mobile Computing}, 14\penalty0
  (9):\penalty0 1794--1805, 2014.

\bibitem[Verloop(2016)]{verloop2016asymptotically}
Verloop, I.~M.
\newblock Asymptotically optimal priority policies for indexable and
  nonindexable restless bandits.
\newblock 2016.

\bibitem[Wang et~al.(2020)Wang, Huang, and Lui]{wang2020restless}
Wang, S., Huang, L., and Lui, J.
\newblock Restless-ucb, an efficient and low-complexity algorithm for online
  restless bandits.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 11878--11889, 2020.

\bibitem[Wang et~al.(2024)Wang, Xiong, and Li]{wang2024online}
Wang, S., Xiong, G., and Li, J.
\newblock Online restless multi-armed bandits with long-term fairness
  constraints.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~38, pp.\  15616--15624, 2024.

\bibitem[Weber \& Weiss(1990)Weber and Weiss]{weber1990index}
Weber, R.~R. and Weiss, G.
\newblock {On An Index Policy for Restless Bandits}.
\newblock \emph{Journal of applied probability}, pp.\  637--648, 1990.

\bibitem[Whittle(1988)]{whittle1988restless}
Whittle, P.
\newblock {Restless Bandits: Activity Allocation in A Changing World}.
\newblock \emph{Journal of applied probability}, pp.\  287--298, 1988.

\bibitem[Xiong \& Li(2023)Xiong and Li]{xiong2023finite}
Xiong, G. and Li, J.
\newblock Finite-time analysis of whittle index based q-learning for restless
  multi-armed bandits with neural network function approximation.
\newblock \emph{Advances in Neural Information Processing Systems},
  36:\penalty0 29048--29073, 2023.

\bibitem[Xiong et~al.(2022{\natexlab{a}})Xiong, Li, and Singh]{xiong2022AAAI}
Xiong, G., Li, J., and Singh, R.
\newblock Reinforcement learning augmented asymptotically optimal index policy
  for finite-horizon restless bandits.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pp.\  8726--8734, 2022{\natexlab{a}}.

\bibitem[Xiong et~al.(2022{\natexlab{b}})Xiong, Qin, Li, Singh, and
  Li]{xiong2022index}
Xiong, G., Qin, X., Li, B., Singh, R., and Li, J.
\newblock Index-aware reinforcement learning for adaptive video streaming at
  the wireless edge.
\newblock In \emph{Proceedings of the Twenty-Third International Symposium on
  Theory, Algorithmic Foundations, and Protocol Design for Mobile Networks and
  Mobile Computing}, pp.\  81--90, 2022{\natexlab{b}}.

\bibitem[Xiong et~al.(2022{\natexlab{c}})Xiong, Wang, and Li]{xiong2022Neurips}
Xiong, G., Wang, S., and Li, J.
\newblock Learning infinite-horizon average-reward restless multi-action
  bandits via index awareness.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 17911--17925, 2022{\natexlab{c}}.

\bibitem[Xiong et~al.(2022{\natexlab{d}})Xiong, Wang, Li, and
  Singh]{xiong2022whittle}
Xiong, G., Wang, S., Li, J., and Singh, R.
\newblock Whittle index based q-learning for wireless edge caching with linear
  function approximation.
\newblock \emph{arXiv preprint arXiv:2202.13187}, 2022{\natexlab{d}}.

\bibitem[Xiong et~al.(2023)Xiong, Wang, Yan, and Li]{xiong2023reinforcement}
Xiong, G., Wang, S., Yan, G., and Li, J.
\newblock Reinforcement learning for dynamic dimensioning of cloud caches: A
  restless bandit approach.
\newblock \emph{IEEE/ACM Transactions on Networking}, 2023.

\end{thebibliography}
