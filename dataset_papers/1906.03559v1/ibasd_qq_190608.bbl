\begin{thebibliography}{14}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Salakhutdinov and Srebro(2015)]{neyshabur15a}
B.~Neyshaburand R.~R. Salakhutdinov and N.~Srebro.
\newblock Path-sgd: Path-normalized optimization in deep neural networks.
\newblock \emph{In Advances in Neural Information Processing Systems}, page
  2422–2430, 2015.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and Srebro]{neyshabur15b}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{In International Conference on Learning Representations}, 2015.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar16}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{ICLR}, 2016.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Tomioka, Salakhutdinov, and
  Srebro]{neyshabur17}
B.~Neyshabur, R.~Tomioka, R.~Salakhutdinov, and N.~Srebro.
\newblock Geometry of optimization and implicit regularization in deep
  learning, 2017.
\newblock URL \url{https://arxiv.org/pdf/1705.03071.pdf}.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and Vinyals]{zhang17}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{In International Conference on Learning Representations}, 2017.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry18}
D.~Soudry, E.~Hoffer, M.~S. Nacson, S.~Gunasekar, and N.~Srebro.
\newblock The implicit bias of gradient descent on separable data, 2018.

\bibitem[Telgarsky(2013)]{telgarsky13}
M.~Telgarsky.
\newblock Margins, shrinkage and boosting.
\newblock \emph{Proceedings of the 30th International Conference on Machine
  Learning, PMLR}, 28\penalty0 (2):\penalty0 307--315, 2013.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar17}
Suriya Gunasekar, Blake~E. Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization, 2017.

\bibitem[Gunasekar et~al.(2018{\natexlab{a}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar18a}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock \emph{In Proceedings of the 35th International Conference on Machine
  Learning}, 2018{\natexlab{a}}.

\bibitem[Gunasekar et~al.(2018{\natexlab{b}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar18b}
S.~Gunasekar, J.~Lee, D.~Soudry, and N.~Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock \emph{In Proceedings of the 35th International Conference on Machine
  Learning}, 2018{\natexlab{b}}.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{wilson17}
Ashia~C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin
  Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock \emph{arXiv}, pages 1--14, 2017.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry.]{hoffer17}
E.~Hoffer, I.~Hubara, and D.~Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock \emph{In Advances in Neural Information Processing Systems}, page
  1–13, 2017.

\bibitem[Duchi et~al.(2010)Duchi, Hazan, and Singer]{duchi10}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121 --
  2159, 2010.

\bibitem[Kingma and Ba(2015)]{kingma15}
Diederik~P. Kingma and Jimmy~Lei Ba.
\newblock Adam: a method for stochastic optimization.
\newblock \emph{International Conference on Learning Representations}, pages
  1--13, 2015.

\end{thebibliography}
