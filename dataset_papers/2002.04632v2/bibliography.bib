@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@ARTICLE{cranmer_gold,
       author = {{Brehmer}, Johann and {Louppe}, Gilles and {Pavez}, Juan and
         {Cranmer}, Kyle},
        title = "{Mining gold from implicit models to improve likelihood-free inference}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, High Energy Physics - Phenomenology, Physics - Data Analysis, Statistics and Probability},
         year = "2018",
        month = "May",
          eid = {arXiv:1805.12244},
        pages = {arXiv:1805.12244},
archivePrefix = {arXiv},
       eprint = {1805.12244},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2018arXiv180512244B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{
ruiz2018learning,
title={Learning To Simulate},
author={Nataniel Ruiz and Samuel Schulter and Manmohan Chandraker},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HJgkx2Aqt7},
}


@inproceedings{
grathwohl2018backpropagation,
title={Backpropagation through the Void: Optimizing control variates for black-box gradient estimation},
author={Will Grathwohl and Dami Choi and Yuhuai Wu and Geoff Roeder and David Duvenaud},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SyzKd1bCW},
}

@article{Arjovsky2017WassersteinG,
  title={Wasserstein GAN},
  author={Mart{\'i}n Arjovsky and Soumith Chintala and L{\'e}on Bottou},
  journal={ArXiv},
  year={2017},
  volume={abs/1701.07875}
}

@article{Louppe2017AdversarialVO,
  title={Adversarial Variational Optimization of Non-Differentiable Simulators},
  author={Gilles Louppe and Joeri Hermans and Kyle Cranmer},
  journal={ArXiv},
  year={2017},
  volume={abs/1707.07113}
}

@inproceedings{moving_ass,
  title={The method of moving asymptotes—a new method for structural optimization},
  author={Krister Svanberg},
  year={1987}
}

@INPROCEEDINGS{gaussian_proc,
    author = {Carl Edward Rasmussen},
    title = {Gaussian processes for machine learning},
    booktitle = {},
    year = {2006},
    publisher = {MIT Press}
}

@InProceedings{gp_kernel_selection,
author="Gorbach, Nico S.
and Bian, Andrew An
and Fischer, Benjamin
and Bauer, Stefan
and Buhmann, Joachim M.",
editor="Roth, Volker
and Vetter, Thomas",
title="Model Selection for Gaussian Process Regression",
booktitle="Pattern Recognition",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="306--318",
abstract="Gaussian processes are powerful tools since they can model non-linear dependencies between inputs, while remaining analytically tractable. A Gaussian process is characterized by a mean function and a covariance function (kernel), which are determined by a model selection criterion. The functions to be compared do not just differ in their parametrization but in their fundamental structure. It is often not clear which function structure to choose, for instance to decide between a squared exponential and a rational quadratic kernel. Based on the principle of posterior agreement, we develop a general framework for model selection to rank kernels for Gaussian process regression and compare it with maximum evidence (also called marginal likelihood) and leave-one-out cross-validation. Given the disagreement between current state-of-the-art methods in our experiments, we show the difficulty of model selection and the need for an information-theoretic approach.",
isbn="978-3-319-66709-6"
}

@article{Williams_reinforce,
 author = {Williams, Ronald J.},
 title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
 journal = {Mach. Learn.},
 issue_date = {May 1992},
 volume = {8},
 number = {3-4},
 month = may,
 year = {1992},
 issn = {0885-6125},
 pages = {229--256},
 numpages = {28},
 url = {https://doi.org/10.1007/BF00992696},
 doi = {10.1007/BF00992696},
 acmid = {139614},
 publisher = {Kluwer Academic Publishers},
 address = {Norwell, MA, USA},
 keywords = {Reinforcement learning, connectionist networks, gradient descent, mathematical analysis},
} 


@InProceedings{pmlr-v97-maheswaranathan19a,
  title = 	 {Guided evolutionary strategies: augmenting random search with surrogate gradients},
  author = 	 {Maheswaranathan, Niru and Metz, Luke and Tucker, George and Choi, Dami and Sohl-Dickstein, Jascha},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4264--4273},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/maheswaranathan19a/maheswaranathan19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/maheswaranathan19a.html},
  abstract = 	 {Many applications in machine learning require optimizing a function whose true gradient is unknown or computationally expensive, but where surrogate gradient information, directions that may be correlated with the true gradient, is cheaply available. For example, this occurs when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in reinforcement learning or training networks with discrete variables). We propose Guided Evolutionary Strategies (GES), a method for optimally using surrogate gradient directions to accelerate random search. GES defines a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients and estimates a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace and use this to derive a setting of the hyperparameters that works well across problems. We evaluate GES on several example problems, demonstrating an improvement over both standard evolutionary strategies and first-order methods that directly follow the surrogate gradient.}
}

@article{Rosenbrock,
    author = {Rosenbrock, H. H.},
    title = "{An Automatic Method for Finding the Greatest or Least Value of a Function}",
    journal = {The Computer Journal},
    volume = {3},
    number = {3},
    pages = {175-184},
    year = {1960},
    month = {01},
    abstract = "{The greatest or least value of a function of several variables is to be found when the variables are restricted to a given region. A method is developed for dealing with this problem and is compared with possible alternatives. The method can be used on a digital computer, and is incorporated in a program for Mercury.}",
    issn = {0010-4620},
    doi = {10.1093/comjnl/3.3.175},
    url = {https://doi.org/10.1093/comjnl/3.3.175},
    eprint = {http://oup.prod.sis.lan/comjnl/article-pdf/3/3/175/988633/030175.pdf},
}

@article{opt_design, title={Principles of Optimal Design: Modeling and Computation P. Y. Papalambros and D. J. Wilde Second edition. Cambridge University Press, The Edinburgh Building, Cambridge CB2 2RU, UK. 2000. 390pp. Illustrated. £27.95. ISBN 0-521-62727-3.}, volume={105}, DOI={10.1017/S0001924000012458}, number={1050}, journal={The Aeronautical Journal (1968)}, publisher={Cambridge University Press}, author={Bhaskar, Atul}, year={2001}, pages={458–459}}

@book{genetic_book,
  title={Genetic Programming: An Introduction},
  author={Banzhaf, W. and Nordin, P. and Keller, R.E. and Francone, F.D.},
  isbn={9781558605107},
  lccn={97051603},
  series={Morgan Kaufmann Series in Arti},
  url={https://books.google.ch/books?id=1697qefFdtIC},
  year={1998},
  publisher={Elsevier Science}
}
@book{FEM,
  title={An Introduction to the Finite Element Method},
  author={J N Reddy, D.},
  isbn={9780072466850},
  lccn={2004058177},
  url={https://books.google.ch/books?id=8gqnRwAACAAJ},
  year={2005},
  publisher={McGraw-Hill Education}
}

@article{design_opt_overview,
author = {Lei, Gang and Zhu, Jianguo and Guo, Youguang and Liu, Chengcheng and Ma, Bo},
year = {2017},
month = {11},
pages = {1962},
title = {A Review of Design Optimization Methods for Electrical Machines},
volume = {10},
journal = {Energies},
doi = {10.3390/en10121962}
}

@ARTICLE{gan_paper,
       author = {{Goodfellow}, Ian J. and {Pouget-Abadie}, Jean and {Mirza}, Mehdi and
         {Xu}, Bing and {Warde-Farley}, David and {Ozair}, Sherjil and
         {Courville}, Aaron and {Bengio}, Yoshua},
        title = "{Generative Adversarial Networks}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
         year = "2014",
        month = "Jun",
          eid = {arXiv:1406.2661},
        pages = {arXiv:1406.2661},
archivePrefix = {arXiv},
       eprint = {1406.2661},
 primaryClass = {stat.ML},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2014arXiv1406.2661G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{ffjord_paper,
  author    = {Will Grathwohl and
               Ricky T. Q. Chen and
               Jesse Bettencourt and
               Ilya Sutskever and
               David Duvenaud},
  title     = {{FFJORD:} Free-form Continuous Dynamics for Scalable Reversible Generative
               Models},
  journal   = {CoRR},
  volume    = {abs/1810.01367},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.01367},
  archivePrefix = {arXiv},
  eprint    = {1810.01367},
  timestamp = {Mon, 22 Jul 2019 14:09:24 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-01367},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{vae_rezende,
author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
title = {Stochastic Backpropagation and Approximate Inference in Deep Generative Models},
year = {2014},
publisher = {JMLR.org},
booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
pages = {II–1278–II–1286},
numpages = {9},
location = {Beijing, China},
series = {ICML’14}
}

@article{rezende_nf,
  title={Variational inference with normalizing flows},
  author={Rezende, Danilo Jimenez and Mohamed, Shakir},
  journal={arXiv preprint arXiv:1505.05770},
  year={2015}
}

@inproceedings{wgangp,
  title={Improved training of wasserstein gans},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  booktitle={Advances in neural information processing systems},
  pages={5767--5777},
  year={2017}
}
@article{cramer_gan,
  title={The cramer distance as a solution to biased wasserstein gradients},
  author={Bellemare, Marc G and Danihelka, Ivo and Dabney, Will and Mohamed, Shakir and Lakshminarayanan, Balaji and Hoyer, Stephan and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:1705.10743},
  year={2017}
}

@book{curse_of_dim,
  title={Dynamic Programming},
  author={Bellman, R. and Bellman, R.E. and Rand Corporation},
  lccn={lc57005444},
  series={Rand Corporation research study},
  url={https://books.google.fr/books?id=rZW4ugAACAAJ},
  year={1957},
  publisher={Princeton University Press}
}

@book{convex_opt_book,
  title={Convex Optimization},
  author={Boyd, S. and Boyd, S.P. and Vandenberghe, L. and Cambridge University Press},
  isbn={9780521833783},
  lccn={03063284},
  series={Berichte {\"u}ber verteilte messysteme},
  url={https://books.google.fr/books?id=mYm0bLd3fcoC},
  year={2004},
  publisher={Cambridge University Press}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@article{trustRegion,
  title={Newton’s Method with a Model Trust Region Modification},
  author={Sorensen, D. C.},
  journal={SIAM J. Numer. Anal.},
  volume={19},
  number={2},
  pages={409--426},
  year={1982},
  publisher={SIAM}
}

, 19(2), 409–426. (18 pages)

@article{cond_gan,
  title={Conditional generative adversarial nets},
  author={Mirza, Mehdi and Osindero, Simon},
  journal={arXiv preprint arXiv:1411.1784},
  year={2014}
}

@article{GEANT4,
      author         = "Agostinelli, S. and others",
      title          = "{GEANT4: A Simulation toolkit}",
      collaboration  = "GEANT4",
      journal        = "Nucl. Instrum. Meth.",
      volume         = "A506",
      year           = "2003",
      pages          = "250-303",
      doi            = "10.1016/S0168-9002(03)01368-8",
      reportNumber   = "SLAC-PUB-9350, FERMILAB-PUB-03-339",
      SLACcitation   = "%%CITATION = NUIMA,A506,250;%%"
}
@article{FairRoot,
	doi = "10.1088/1742-6596/396/2/022001",
	year = "2012",
	volume = "396",
	number = "2",
	pages = "022001",
	author = "M Al-Turany and D Bertini and R Karabowicz and D Kresan and P Malzacher and T Stockmanns and F Uhlig",
	title = "The FairRoot framework",
	journal = "Journal of Physics: Conference Series"}
	
	
@article{Chen2016LearningTL,
  title={Learning to Learn for Global Optimization of Black Box Functions},
  author={Yutian Chen and Matthew W. Hoffman and Sergio Gomez Colmenarejo and Misha Denil and Timothy P. Lillicrap and Nando de Freitas},
  journal={ArXiv},
  year={2016},
  volume={abs/1611.03824}
}

@inproceedings{Stulp2013PolicyI,
  title={Policy Improvement : Between Black-Box Optimization and Episodic Reinforcement Learning},
  author={Freek Stulp and Olivier Sigaud},
  year={2013}
}

@misc{gan_batch_size,
    title={Don't Decay the Learning Rate, Increase the Batch Size},
    author={Samuel L. Smith and Pieter-Jan Kindermans and Chris Ying and Quoc V. Le},
    year={2017},
    eprint={1711.00489},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@book{lhc_sampling,
author = {Iman, Ronald and Davenport, James and Zeigler, D.},
year = {1980},
month = {01},
pages = {},
title = {Latin hypercube sampling (program user's guide). [LHC, in FORTRAN]},
isbn = {SAND-79-1473 United StatesThu Feb 07 00:00:56 EST 2008Dep. NTIS, PC A05/MF A01.SNL; ERA-05-017293; EDB-80-043435English}
}

@misc{adam_opt,
    title={Adam: A Method for Stochastic Optimization},
    author={Diederik P. Kingma and Jimmy Ba},
    year={2014},
    eprint={1412.6980},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article {ABC_lik,
	author = {Beaumont, Mark A. and Zhang, Wenyang and Balding, David J.},
	title = {Approximate Bayesian Computation in Population Genetics},
	volume = {162},
	number = {4},
	pages = {2025--2035},
	year = {2002},
	publisher = {Genetics},
	issn = {0016-6731},
	URL = {https://www.genetics.org/content/162/4/2025},
	eprint = {https://www.genetics.org/content/162/4/2025.full.pdf},
	journal = {Genetics}
}

@article{synth_lik,
title = {Statistical inference for noisy nonlinear ecological dynamic systems},
author = {Wood, Simon N},
year = {2010},
month = {8},
day = {26},
doi = {10.1038/nature09319},
volume = {466},
pages = {1102--1104},
journal = {Nature},
issn = {0028-0836},
publisher = {Nature Publishing Grou},
number = {7310},
}

@misc{vae_well,
    title={Auto-Encoding Variational Bayes},
    author={Diederik P Kingma and Max Welling},
    year={2013},
    eprint={1312.6114},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{Keskar2017ImprovingGP,
  title={Improving Generalization Performance by Switching from Adam to SGD},
  author={Nitish Shirish Keskar and Richard Socher},
  journal={ArXiv},
  year={2017},
  volume={abs/1712.07628}
}


@INPROCEEDINGS{ChainQueen, 
author={Y. {Hu} and J. {Liu} and A. {Spielberg} and J. B. {Tenenbaum} and W. T. {Freeman} and J. {Wu} and D. {Rus} and W. {Matusik}}, 
booktitle={2019 International Conference on Robotics and Automation (ICRA)}, 
title={ChainQueen: A Real-Time Differentiable Physical Simulator for Soft Robotics}, 
year={2019}, 
volume={}, 
number={}, 
pages={6265-6271}, 
keywords={deformation;elasticity;gradient methods;inverse problems;least squares approximations;manipulator dynamics;mobile robots;multi-robot systems;optimal control;optimisation;path planning;ChainQueen;real-time differentiable physical simulator;robot planning;gradient-based optimization algorithms;inverse problems;optimal control;motion planning;rigid body simulators;deformable objects;rigid body dynamics;Lagrangian-Eulerian physical simulator;MLS-MPM;soft robotic systems;forward simulation;backward gradient computation;moving least squares material point method;Graphics processing units;Computational modeling;Soft robotics;Three-dimensional displays;Planning;Inverse problems}, 
doi={10.1109/ICRA.2019.8794333}, 
ISSN={}, 
month={May},}

@article{chang2016compositional,
    title={A Compositional Object-Based Approach to Learning Physical Dynamics},
    author={Chang, Michael B and Ullman, Tomer and Torralba, Antonio and Tenenbaum, Joshua B},
    journal={arXiv preprint arXiv:1612.00341},
    year={2016}
}

@inproceedings{BelbutePeres2018EndtoEndDP,
  title={End-to-End Differentiable Physics for Learning and Control},
  author={Filipe de Avila Belbute-Peres and Kevin A. Smith and Kelsey R. Allen and Joshua B. Tenenbaum and J. Zico Kolter},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{Degrave2017ADP,
  title={A DIFFERENTIABLE PHYSICS ENGINE FOR DEEP LEARNING IN ROBOTICS},
  author={Jonas Degrave and Michiel Hermans and Joni Dambre and Francis Wyffels},
  booktitle={Front. Neurorobot.},
  year={2017}
}

@article{SVM,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@article{Papamakarios2018SequentialNL,
  title={Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows},
  author={George Papamakarios and David C. Sterratt and Iain Murray},
  journal={ArXiv},
  year={2018},
  volume={abs/1805.07226}
}

@inproceedings{Lueckmann2018LikelihoodfreeIW,
  title={Likelihood-free inference with emulator networks},
  author={Jan-Matthis Lueckmann and Giacomo Bassetto and Theofanis Karaletsos and Jakob H. Macke},
  booktitle={AABI},
  year={2018}
}

@inproceedings{Greenberg2019AutomaticPT,
  title={Automatic Posterior Transformation for Likelihood-Free Inference},
  author={David S. Greenberg and Marcel Nonnenmacher and Jakob H. Macke},
  booktitle={ICML},
  year={2019}
}

@article{ConcreteDistribution,
author = {Maddison, Christopher and Mnih, Andriy and Teh, Yee},
year = {2017},
booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
pages = {},
title = {The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables}
}

@inproceedings{balog2017lost,
  title={Lost relatives of the Gumbel trick},
  author={Balog, Matej and Tripuraneni, Nilesh and Ghahramani, Zoubin and Weller, Adrian},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={371--379},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{hoos2014efficient,
  title={An efficient approach for assessing hyperparameter importance},
  author={Hoos, Holger and Leyton-Brown, Kevin},
  booktitle={International conference on machine learning},
  pages={754--762},
  year={2014}
}

@inproceedings{wang2013bayesian,
  title={Bayesian optimization in high dimensions via random embeddings},
  author={Wang, Ziyu and Zoghi, Masrour and Hutter, Frank and Matheson, David and De Freitas, Nando},
  booktitle={Twenty-Third International Joint Conference on Artificial Intelligence},
  year={2013}
}

@inproceedings{djolonga2013high,
  title={High-dimensional gaussian process bandits},
  author={Djolonga, Josip and Krause, Andreas and Cevher, Volkan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1025--1033},
  year={2013}
}

@inproceedings{10.5555/3020751.3020776, 
author = {Garnett, Roman and Osborne, Michael A. and Hennig, Philipp}, title = {Active Learning of Linear Embeddings for Gaussian Processes}, year = {2014}, isbn = {9780974903910}, publisher = {AUAI Press}, address = {Arlington, Virginia, USA}, booktitle = {Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence}, pages = {230–239}, numpages = {10}, location = {Quebec City, Quebec, Canada}, series = {UAI’14} 
}

@inproceedings{Zhang2019HighDB,
  title={High Dimensional Bayesian Optimization via Supervised Dimension Reduction},
  author={Miao Zhang and Huiqi Li and Steven W. Su},
  booktitle={IJCAI},
  year={2019}
}

@inproceedings{Oh2018BOCKB,
  title={BOCK : Bayesian Optimization with Cylindrical Kernels},
  author={ChangYong Oh and Efstratios Gavves and Max Welling},
  booktitle={ICML},
  year={2018}
}

@article{Gupta2019FeedbackGF,
  title={Feedback GAN for DNA optimizes protein functions},
  author={Anvita Gupta and James Zou},
  journal={Nature Machine Intelligence},
  year={2019},
  volume={1},
  pages={105-111}
}

@inproceedings{GmezBombarelli2018AutomaticCD,
  title={Automatic Chemical Design Using a Data-Driven Continuous
Representation of Molecules},
  author={Rafael G{\'o}mez-Bombarelli and David Duvenaud and Jos{\'e} Miguel Hern{\'a}ndez-Lobato and Jorge Aguilera-Iparraguirre and Timothy D. Hirzel and Ryan P. Adams and Al{\'a}n Aspuru-Guzik},
  booktitle={ACS central science},
  year={2018}
}

@inproceedings{Brookes2019ConditioningBA,
  title={Conditioning by adaptive sampling for robust design},
  author={David H. Brookes and Hahnbeom Park and Jennifer Listgarten},
  booktitle={ICML},
  year={2019}
}

@article{Suzuki2019DeepLI,
  title={Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic Besov space},
  author={Taiji Suzuki and Atsushi Nitanda},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.12799}
}

@article{Nakada2019AdaptiveAA,
  title={Adaptive Approximation and Estimation of Deep Neural Network to Intrinsic Dimensionality},
  author={Ryumei Nakada and Masaaki Imaizumi},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.02177}
}

@article{SchmidtHieber2019DeepRN,
  title={Deep ReLU network approximation of functions on a manifold},
  author={Johannes Schmidt-Hieber},
  journal={ArXiv},
  year={2019},
  volume={abs/1908.00695}
}

@article{shield_opt_ship,
	doi = {10.1088/1742-6596/934/1/012050},
	url = {https://doi.org/10.1088%2F1742-6596%2F934%2F1%2F012050},
	year = 2017,
	month = {dec},
	publisher = {{IOP} Publishing},
	volume = {934},
	pages = {012050},
	author = {A Baranov and E Burnaev and D Derkach and A Filatov and N Klyuchnikov and O Lantwin and F Ratnikov and A Ustyuzhanin and A Zaitsev},
	title = {Optimising the Active Muon Shield for the {SHiP} Experiment at {CERN}},
	journal = {Journal of Physics: Conference Series},
	abstract = {The SHiP experiment is designed to search for very weakly interacting particles beyond the Standard Model which are produced in a 400 GeV/c proton beam dump at the CERN SPS. The critical challenge for this experiment is to keep the Standard Model background level negligible. In the beam dump, around 1011 muons will be produced per second. The muon rate in the spectrometer has to be reduced by at least four orders of magnitude to avoid muoninduced backgrounds. It is demonstrated that new improved active muon shield may be used to magnetically deflect the muons out of the acceptance of the spectrometer.}
}

@inproceedings{Wang2016ParallelBG,
  title={Parallel Bayesian Global Optimization of Expensive Functions},
  author={Jialei Wang and Scott C. Clark and Eric Liu and Peter I. Frazier},
  year={2016}
}

@article{Li2018MeasuringTI,
  title={Measuring the Intrinsic Dimension of Objective Landscapes},
  author={Chunyuan Li and Heerad Farkhoor and Rosanne Liu and Jason Yosinski},
  journal={ArXiv},
  year={2018},
  volume={abs/1804.08838}
}

@article{bostonD,
author = {Harrison, David and Rubinfeld, Daniel},
year = {1978},
month = {03},
pages = {81-102},
title = {Hedonic housing prices and the demand for clean air},
volume = {5},
journal = {Journal of Environmental Economics and Management},
doi = {10.1016/0095-0696(78)90006-2}
}

@article{Pyro,
  author    = {Eli Bingham and
               Jonathan P. Chen and
               Martin Jankowiak and
               Fritz Obermeyer and
               Neeraj Pradhan and
               Theofanis Karaletsos and
               Rohit Singh and
               Paul A. Szerlip and
               Paul Horsfall and
               Noah D. Goodman},
  title     = {Pyro: Deep Universal Probabilistic Programming},
  journal   = {CoRR},
  volume    = {abs/1810.09538},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.09538},
  archivePrefix = {arXiv},
  eprint    = {1810.09538},
  timestamp = {Wed, 31 Oct 2018 14:24:29 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-09538},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{Maheswaranathan2018GuidedES,
  title={Guided evolutionary strategies: augmenting random search with surrogate gradients},
  author={Niru Maheswaranathan and Luke Metz and George Tucker and Dami Choi and Jascha Sohl-Dickstein},
  booktitle={ICML},
  year={2018}
}

@article{cranmer2019frontier,
  title={The frontier of simulation-based inference},
  author={Cranmer, Kyle and Brehmer, Johann and Louppe, Gilles},
  journal={arXiv preprint arXiv:1911.01429},
  year={2019}
}

@inproceedings{snoek2012practical,
  title={Practical bayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  booktitle={Advances in neural information processing systems},
  pages={2951--2959},
  year={2012}
}

@article{shahriari2015taking,
  title={Taking the human out of the loop: A review of Bayesian optimization},
  author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P and De Freitas, Nando},
  journal={Proceedings of the IEEE},
  volume={104},
  number={1},
  pages={148--175},
  year={2015},
  publisher={IEEE}
}

@incollection{NIPS2019_8788,
title = {Scalable Global Optimization via Local Bayesian Optimization},
author = {Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D and Poloczek, Matthias},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {5496--5507},
year = {2019},
publisher = {Curran Associates, Inc.}
}

@article{Amadio_2017,
	doi = {10.1088/1742-6596/898/4/042026},
	url = {https://doi.org/10.1088%2F1742-6596%2F898%2F4%2F042026},
	year = 2017,
	month = {oct},
	publisher = {{IOP} Publishing},
	volume = {898},
	pages = {042026},
	author = {G. Amadio and J. Apostolakis and M. Bandieramonte and S.P. Behera and R. Brun and P. Canal and F. Carminati and G. Cosmo and L. Duhem and D. Elvira and G. Folger and A. Gheata and M. Gheata and I. Goulas and F. Hariri and S.Y. Jun and D. Konstantinov and H. Kumawat and V. Ivantchenko and G. Lima and T. Nikitina and M. Novak and W. Pokorski and A. Ribon and R. Seghal and O. Shadura and S. Vallecorsa and S. Wenzel},
	title = {Stochastic optimization of {GeantV} code by use of genetic algorithms},
	journal = {Journal of Physics: Conference Series},
	abstract = {GeantV is a complex system based on the interaction of different modules needed for detector simulation, which include transport of particles in fields, physics models simulating their interactions with matter and a geometrical modeler library for describing the detector and locating the particles and computing the path length to the current volume boundary. The GeantV project is recasting the classical simulation approach to get maximum benefit from SIMD/MIMD computational architectures and highly massive parallel systems. This involves finding the appropriate balance between several aspects influencing computational performance (floating-point performance, usage of off-chip memory bandwidth, specification of cache hierarchy, etc.) and handling a large number of program parameters that have to be optimized to achieve the best simulation throughput. This optimization task can be treated as a black-box optimization problem, which requires searching the optimum set of parameters using only point-wise function evaluations. The goal of this study is to provide a mechanism for optimizing complex systems (high energy physics particle transport simulations) with the help of genetic algorithms and evolution strategies as tuning procedures for massive parallel simulations. One of the described approaches is based on introducing a specific multivariate analysis operator that could be used in case of resource expensive or time consuming evaluations of fitness functions, in order to speed-up the convergence of the black-box optimization problem.}
}

@article{sjostrand2008brief,
  title={A brief introduction to PYTHIA 8.1},
  author={Sj{\"o}strand, Torbj{\"o}rn and Mrenna, Stephen and Skands, Peter},
  journal={Computer Physics Communications},
  volume={178},
  number={11},
  pages={852--867},
  year={2008},
  publisher={Elsevier}
}

@article{louppe2017adversarial,
  title={Adversarial variational optimization of non-differentiable simulators},
  author={Louppe, Gilles and Hermans, Joeri and Cranmer, Kyle},
  journal={arXiv preprint arXiv:1707.07113},
  year={2017}
}

@article{abraham2011optimization,
  title={Optimization of parameters for molecular dynamics simulation using smooth particle-mesh Ewald in GROMACS 4.5},
  author={Abraham, Mark J and Gready, Jill E},
  journal={Journal of computational chemistry},
  volume={32},
  number={9},
  pages={2031--2040},
  year={2011},
  publisher={Wiley Online Library}
}

@book{dunn2011exploring,
  title={Exploring Monte Carlo Methods},
  author={Dunn, W.L. and Shultis, J.K.},
  isbn={9780080930619},
  url={https://books.google.fr/books?id=ACTrrQk1UgoC},
  year={2011},
  publisher={Elsevier Science}
}

@book{papalambros_wilde_2000,
    title={Principles of Optimal Design: Modeling and Computation},
    author={Papalambros, Panos Y. and Wilde, Douglass J.},
    isbn={9780511626418},
    publisher={Cambridge University Press}, 
    year={2000}
}

@article{Jamil2013ALS,
  title={A literature survey of benchmark functions for global optimisation problems},
  author={Momin Jamil and Xin-She Yang},
  journal={IJMNO},
  year={2013},
  volume={4},
  pages={150-194}
}

@article{mohamed2019monte,
  title={Monte {Carlo} gradient estimation in machine learning},
  author={Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  journal={arXiv preprint arXiv:1906.10652},
  year={2019}
}

@InProceedings{pmlr-v51-shahriari16,
  title = 	 {Unbounded Bayesian Optimization via Regularization},
  author = 	 {Bobak Shahriari and Alexandre Bouchard-Cote and Nando Freitas},
  booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1168--1176},
  year = 	 {2016},
  editor = 	 {Arthur Gretton and Christian C. Robert},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Cadiz, Spain},
  month = 	 {09--11 May},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v51/shahriari16.pdf},
  url = 	 {http://proceedings.mlr.press/v51/shahriari16.html},
  abstract = 	 {Bayesian optimization has recently emerged as a powerful and flexible tool in machine learning for hyperparameter tuning and more generally for the efficient global optimization of expensive black box functions. The established practice requires a user-defined bounded domain, which is assumed to contain the global optimizer. However, when little is known about the probed objective function, it can be difficult to prescribe such a domain. In this work, we modify the standard Bayesian optimization framework in a principled way to allow for unconstrained exploration of the search space. We introduce a new alternative method and compare it to a volume doubling baseline on two common synthetic benchmarking test functions. Finally, we apply our proposed methods on the task of tuning the stochastic gradient descent optimizer for both a multi-layered perceptron and a convolutional neural network on the MNIST dataset.}
}

@incollection{NIPS2019_9350,
title = {Bayesian Optimization with Unknown Search Space},
author = {Ha, Huong and Rana, Santu and Gupta, Sunil and Nguyen, Thanh and Tran-The, Hung and Venkatesh, Svetha},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. dAlch\'{e}-Buc and E. Fox and R. Garnett},
pages = {11795--11804},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9350-bayesian-optimization-with-unknown-search-space.pdf}
}

@inproceedings{richter2016playing,
  title={Playing for data: Ground truth from computer games},
  author={Richter, Stephan R and Vineet, Vibhav and Roth, Stefan and Koltun, Vladlen},
  booktitle={European Conference on Computer Vision},
  pages={102--118},
  year={2016},
  organization={Springer}
}

@inproceedings{ros2016synthia,
  title={The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes},
  author={Ros, German and Sellart, Laura and Materzynska, Joanna and Vazquez, David and Lopez, Antonio M},
  booktitle={Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
  pages={3234--3243},
  year={2016}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. dAlch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}