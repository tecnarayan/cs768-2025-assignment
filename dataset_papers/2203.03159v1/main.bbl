\begin{thebibliography}{38}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Ahn et~al.(2020)Ahn, Yun and Sra}]{ahn2020sgd}
\textsc{Ahn, K.}, \textsc{Yun, C.} and \textsc{Sra, S.} (2020).
\newblock Sgd with shuffling: optimal rates without component convexity and
  large epoch requirements.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{33} 17526--17535.

\bibitem[{Ali et~al.(2019)Ali, Kolter and Tibshirani}]{ali2019continuous}
\textsc{Ali, A.}, \textsc{Kolter, J.~Z.} and \textsc{Tibshirani, R.~J.} (2019).
\newblock A continuous-time view of early stopping for least squares
  regression.
\newblock In \textit{The 22nd international conference on artificial
  intelligence and statistics}. PMLR.

\bibitem[{Bach and Moulines(2013)}]{bach2013non}
\textsc{Bach, F.} and \textsc{Moulines, E.} (2013).
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate $o(1/n)$.
\newblock \textit{Advances in neural information processing systems}
  \textbf{26} 773--781.

\bibitem[{Bartlett et~al.(2020)Bartlett, Long, Lugosi and
  Tsigler}]{bartlett2020benign}
\textsc{Bartlett, P.~L.}, \textsc{Long, P.~M.}, \textsc{Lugosi, G.} and
  \textsc{Tsigler, A.} (2020).
\newblock Benign overfitting in linear regression.
\newblock \textit{Proceedings of the National Academy of Sciences} .

\bibitem[{Bassily et~al.(2018)Bassily, Belkin and Ma}]{bassily2018exponential}
\textsc{Bassily, R.}, \textsc{Belkin, M.} and \textsc{Ma, S.} (2018).
\newblock On exponential convergence of sgd in non-convex over-parametrized
  learning.
\newblock \textit{arXiv preprint arXiv:1811.02564} .

\bibitem[{Bassily et~al.(2020)Bassily, Feldman, Guzm{\'a}n and
  Talwar}]{bassily2020stability}
\textsc{Bassily, R.}, \textsc{Feldman, V.}, \textsc{Guzm{\'a}n, C.} and
  \textsc{Talwar, K.} (2020).
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{33} 4381--4391.

\bibitem[{Bottou and Bousquet(2007)}]{bottou2007tradeoffs}
\textsc{Bottou, L.} and \textsc{Bousquet, O.} (2007).
\newblock The tradeoffs of large scale learning.
\newblock \textit{Advances in neural information processing systems}
  \textbf{20}.

\bibitem[{Bottou et~al.(2018)Bottou, Curtis and
  Nocedal}]{bottou2018optimization}
\textsc{Bottou, L.}, \textsc{Curtis, F.~E.} and \textsc{Nocedal, J.} (2018).
\newblock Optimization methods for large-scale machine learning.
\newblock \textit{Siam Review} \textbf{60} 223--311.

\bibitem[{Chen et~al.(2018)Chen, Jin and Yu}]{chen2018stability}
\textsc{Chen, Y.}, \textsc{Jin, C.} and \textsc{Yu, B.} (2018).
\newblock Stability and convergence trade-off of iterative optimization
  algorithms.
\newblock \textit{arXiv preprint arXiv:1804.01619} .

\bibitem[{Dieuleveut et~al.(2017)Dieuleveut, Flammarion and
  Bach}]{dieuleveut2017harder}
\textsc{Dieuleveut, A.}, \textsc{Flammarion, N.} and \textsc{Bach, F.} (2017).
\newblock Harder, better, faster, stronger convergence rates for least-squares
  regression.
\newblock \textit{The Journal of Machine Learning Research} \textbf{18}
  3520--3570.

\bibitem[{Dobriban et~al.(2018)Dobriban, Wager et~al.}]{dobriban2018high}
\textsc{Dobriban, E.}, \textsc{Wager, S.} \textsc{et~al.} (2018).
\newblock High-dimensional asymptotics of prediction: Ridge regression and
  classification.
\newblock \textit{The Annals of Statistics} \textbf{46} 247--279.

\bibitem[{Elisseeff et~al.(2005)Elisseeff, Evgeniou, Pontil and
  Kaelbing}]{elisseeff2005stability}
\textsc{Elisseeff, A.}, \textsc{Evgeniou, T.}, \textsc{Pontil, M.} and
  \textsc{Kaelbing, L.~P.} (2005).
\newblock Stability of randomized learning algorithms.
\newblock \textit{Journal of Machine Learning Research} \textbf{6}.

\bibitem[{Ge et~al.(2019)Ge, Kakade, Kidambi and Netrapalli}]{ge2019step}
\textsc{Ge, R.}, \textsc{Kakade, S.~M.}, \textsc{Kidambi, R.} and
  \textsc{Netrapalli, P.} (2019).
\newblock The step decay schedule: A near optimal, geometrically decaying
  learning rate procedure for least squares.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{32}.

\bibitem[{Gunasekar et~al.(2018)Gunasekar, Lee, Soudry and
  Srebro}]{gunasekar2018characterizing}
\textsc{Gunasekar, S.}, \textsc{Lee, J.}, \textsc{Soudry, D.} and
  \textsc{Srebro, N.} (2018).
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Haochen and Sra(2019)}]{haochen2019random}
\textsc{Haochen, J.} and \textsc{Sra, S.} (2019).
\newblock Random shuffling beats sgd after finite epochs.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Hardt et~al.(2016)Hardt, Recht and Singer}]{hardt2016train}
\textsc{Hardt, M.}, \textsc{Recht, B.} and \textsc{Singer, Y.} (2016).
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \textit{International conference on machine learning}. PMLR.

\bibitem[{Jain et~al.(2017{\natexlab{a}})Jain, Kakade, Kidambi, Netrapalli,
  Pillutla and Sidford}]{jain2017markov}
\textsc{Jain, P.}, \textsc{Kakade, S.~M.}, \textsc{Kidambi, R.},
  \textsc{Netrapalli, P.}, \textsc{Pillutla, V.~K.} and \textsc{Sidford, A.}
  (2017{\natexlab{a}}).
\newblock A markov chain theory approach to characterizing the minimax
  optimality of stochastic gradient descent (for least squares).
\newblock \textit{arXiv preprint arXiv:1710.09430} .

\bibitem[{Jain et~al.(2017{\natexlab{b}})Jain, Netrapalli, Kakade, Kidambi and
  Sidford}]{jain2017parallelizing}
\textsc{Jain, P.}, \textsc{Netrapalli, P.}, \textsc{Kakade, S.~M.},
  \textsc{Kidambi, R.} and \textsc{Sidford, A.} (2017{\natexlab{b}}).
\newblock Parallelizing stochastic gradient descent for least squares
  regression: mini-batching, averaging, and model misspecification.
\newblock \textit{The Journal of Machine Learning Research} \textbf{18}
  8258--8299.

\bibitem[{Kuzborskij and Lampert(2018)}]{kuzborskij2018data}
\textsc{Kuzborskij, I.} and \textsc{Lampert, C.} (2018).
\newblock Data-dependent stability of stochastic gradient descent.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Lei et~al.(2021)Lei, Hu and Tang}]{lei2021generalization}
\textsc{Lei, Y.}, \textsc{Hu, T.} and \textsc{Tang, K.} (2021).
\newblock Generalization performance of multi-pass stochastic gradient descent
  with convex loss functions.
\newblock \textit{J. Mach. Learn. Res.} \textbf{22} 25--1.

\bibitem[{Lin and Rosasco(2017)}]{lin2017optimal}
\textsc{Lin, J.} and \textsc{Rosasco, L.} (2017).
\newblock Optimal rates for multi-pass stochastic gradient methods.
\newblock \textit{The Journal of Machine Learning Research} \textbf{18}
  3375--3421.

\bibitem[{Ma et~al.(2018)Ma, Bassily and Belkin}]{ma2018power}
\textsc{Ma, S.}, \textsc{Bassily, R.} and \textsc{Belkin, M.} (2018).
\newblock The power of interpolation: Understanding the effectiveness of sgd in
  modern over-parametrized learning.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{M{\"u}cke et~al.(2019)M{\"u}cke, Neu and Rosasco}]{mucke2019beating}
\textsc{M{\"u}cke, N.}, \textsc{Neu, G.} and \textsc{Rosasco, L.} (2019).
\newblock Beating sgd saturation with tail-averaging and minibatching.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{32}.

\bibitem[{Neu and Rosasco(2018)}]{neu2018iterate}
\textsc{Neu, G.} and \textsc{Rosasco, L.} (2018).
\newblock Iterate averaging as regularization for stochastic gradient descent.
\newblock In \textit{Conference On Learning Theory}. PMLR.

\bibitem[{Pillaud-Vivien et~al.(2018)Pillaud-Vivien, Rudi and
  Bach}]{pillaud2018statistical}
\textsc{Pillaud-Vivien, L.}, \textsc{Rudi, A.} and \textsc{Bach, F.} (2018).
\newblock Statistical optimality of stochastic gradient descent on hard
  learning problems through multiple passes.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{31}.

\bibitem[{Raskutti et~al.(2014)Raskutti, Wainwright and Yu}]{raskutti2014early}
\textsc{Raskutti, G.}, \textsc{Wainwright, M.~J.} and \textsc{Yu, B.} (2014).
\newblock Early stopping and non-parametric regression: an optimal
  data-dependent stopping rule.
\newblock \textit{The Journal of Machine Learning Research} \textbf{15}
  335--366.

\bibitem[{Rosasco and Villa(2015)}]{rosasco2015learning}
\textsc{Rosasco, L.} and \textsc{Villa, S.} (2015).
\newblock Learning with incremental iterative regularization.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{28}.

\bibitem[{Safran and Shamir(2020)}]{safran2020good}
\textsc{Safran, I.} and \textsc{Shamir, O.} (2020).
\newblock How good is sgd with random shuffling?
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Tsigler and Bartlett(2020)}]{tsigler2020benign}
\textsc{Tsigler, A.} and \textsc{Bartlett, P.~L.} (2020).
\newblock Benign overfitting in ridge regression.
\newblock \textit{arXiv preprint arXiv:2009.14286} .

\bibitem[{Vaswani et~al.(2019{\natexlab{a}})Vaswani, Bach and
  Schmidt}]{vaswani2019fast}
\textsc{Vaswani, S.}, \textsc{Bach, F.} and \textsc{Schmidt, M.}
  (2019{\natexlab{a}}).
\newblock Fast and faster convergence of sgd for over-parameterized models and
  an accelerated perceptron.
\newblock In \textit{The 22nd International Conference on Artificial
  Intelligence and Statistics}. PMLR.

\bibitem[{Vaswani et~al.(2019{\natexlab{b}})Vaswani, Mishkin, Laradji, Schmidt,
  Gidel and Lacoste-Julien}]{vaswani2019painless}
\textsc{Vaswani, S.}, \textsc{Mishkin, A.}, \textsc{Laradji, I.},
  \textsc{Schmidt, M.}, \textsc{Gidel, G.} and \textsc{Lacoste-Julien, S.}
  (2019{\natexlab{b}}).
\newblock Painless stochastic gradient: Interpolation, line-search, and
  convergence rates.
\newblock \textit{Advances in neural information processing systems}
  \textbf{32}.

\bibitem[{Wei et~al.(2017)Wei, Yang and Wainwright}]{wei2017early}
\textsc{Wei, Y.}, \textsc{Yang, F.} and \textsc{Wainwright, M.~J.} (2017).
\newblock Early stopping for kernel boosting algorithms: A general analysis
  with localized complexities.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{30}.

\bibitem[{Wu et~al.(2021)Wu, Zou, Braverman, Gu and Kakade}]{wu2021last}
\textsc{Wu, J.}, \textsc{Zou, D.}, \textsc{Braverman, V.}, \textsc{Gu, Q.} and
  \textsc{Kakade, S.~M.} (2021).
\newblock Last iterate risk bounds of sgd with decaying stepsize for
  overparameterized linear regression.
\newblock \textit{arXiv preprint arXiv:2110.06198} .

\bibitem[{Xu and Hsu(2019)}]{xu2019number}
\textsc{Xu, J.} and \textsc{Hsu, D.~J.} (2019).
\newblock On the number of variables to use in principal component regression.
\newblock \textit{Advances in neural information processing systems}
  \textbf{32}.

\bibitem[{Yao et~al.(2007)Yao, Rosasco and Caponnetto}]{yao2007early}
\textsc{Yao, Y.}, \textsc{Rosasco, L.} and \textsc{Caponnetto, A.} (2007).
\newblock On early stopping in gradient descent learning.
\newblock \textit{Constructive Approximation} \textbf{26} 289--315.

\bibitem[{Zhang et~al.(2021)Zhang, Zhang, Bald, Pingali, Chen and
  Goswami}]{zhang2021stability}
\textsc{Zhang, Y.}, \textsc{Zhang, W.}, \textsc{Bald, S.}, \textsc{Pingali,
  V.}, \textsc{Chen, C.} and \textsc{Goswami, M.} (2021).
\newblock Stability of sgd: Tightness analysis and improved bounds.
\newblock \textit{arXiv preprint arXiv:2102.05274} .

\bibitem[{Zou et~al.(2021{\natexlab{a}})Zou, Wu, Braverman, Gu and
  Kakade}]{zou2021benign}
\textsc{Zou, D.}, \textsc{Wu, J.}, \textsc{Braverman, V.}, \textsc{Gu, Q.} and
  \textsc{Kakade, S.} (2021{\natexlab{a}}).
\newblock Benign overfitting of constant-stepsize sgd for linear regression.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Zou et~al.(2021{\natexlab{b}})Zou, Wu, Gu, Foster, Kakade
  et~al.}]{zou2021benefits}
\textsc{Zou, D.}, \textsc{Wu, J.}, \textsc{Gu, Q.}, \textsc{Foster, D.~P.},
  \textsc{Kakade, S.} \textsc{et~al.} (2021{\natexlab{b}}).
\newblock The benefits of implicit regularization from sgd in least squares
  problems.
\newblock \textit{Advances in Neural Information Processing Systems}
  \textbf{34}.

\end{thebibliography}
