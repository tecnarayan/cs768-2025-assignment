\begin{thebibliography}{110}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2022)Agarwal, D'souza, and Hooker]{agarwal2022estimating}
Chirag Agarwal, Daniel D'souza, and Sara Hooker.
\newblock Estimating example difficulty using variance of gradients.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022.

\bibitem[Ajtai(1994)]{ajtai1994complexity}
Mikl{\'o}s Ajtai.
\newblock The complexity of the pigeonhole principle.
\newblock \emph{Combinatorica}, 14:\penalty0 417--433, 1994.

\bibitem[Anonymous(2024)]{anonymous2024democratizing}
Anonymous.
\newblock Democratizing evaluation with infinity-benchmarks: Sample-level heterogeneous testing over arbitrary capabilities.
\newblock In \emph{Submitted to The Thirteenth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=Dj1PVLU8fK}.
\newblock under review.

\bibitem[Baker(2001)]{baker2001basics}
Frank~B Baker.
\newblock \emph{The basics of item response theory}.
\newblock ERIC, 2001.

\bibitem[Bakr et~al.(2023)Bakr, Sun, Shen, Khan, Li, and Elhoseiny]{bakr2023hrs}
Eslam~Mohamed Bakr, Pengzhan Sun, Xiaogian Shen, Faizan~Farooq Khan, Li~Erran Li, and Mohamed Elhoseiny.
\newblock Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models.
\newblock In \emph{International Conference on Computer Vision (ICCV)}, 2023.

\bibitem[Baldock et~al.(2021)Baldock, Maennel, and Neyshabur]{baldock2021deep}
Robert Baldock, Hartmut Maennel, and Behnam Neyshabur.
\newblock Deep learning through the lens of example difficulty.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Bansal and Grover(2023)]{bansal2023leaving}
Hritik Bansal and Aditya Grover.
\newblock Leaving reality to imagination: Robust classification via generated datasets.
\newblock \emph{International Conference on Learning Representations Workshop (ICLR-W)}, 2023.

\bibitem[Barbu et~al.(2019)Barbu, Mayo, Alverio, Luo, Wang, Gutfreund, Tenenbaum, and Katz]{barbu2019objectnet}
Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz.
\newblock Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Bender et~al.(2021)Bender, Gebru, McMillan-Major, and Shmitchell]{bender2021dangers}
Emily~M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In \emph{Conference on Fairness, Accountability, and Transparency (FAccT)}, 2021.

\bibitem[Beyer et~al.(2021)Beyer, H{\'e}naff, Kolesnikov, Zhai, and Oord]{beyer2020we}
Lucas Beyer, Olivier~J H{\'e}naff, Alexander Kolesnikov, Xiaohua Zhai, and A{\"a}ron van~den Oord.
\newblock Are we done with imagenet?
\newblock In \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Bi et~al.(2020)Bi, Ma, Huang, Yin, Liu, Chen, Su, and Wang]{bi2020quality}
Haoyang Bi, Haiping Ma, Zhenya Huang, Yu~Yin, Qi~Liu, Enhong Chen, Yu~Su, and Shijin Wang.
\newblock Quality meets diversity: A model-agnostic framework for computerized adaptive testing.
\newblock In \emph{International Conference on Data Mining (ICDM)}, 2020.

\bibitem[Bitton et~al.(2023)Bitton, Bansal, Hessel, Shao, Zhu, Awadalla, Gardner, Taori, and Schimdt]{bitton2023visit}
Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schimdt.
\newblock Visit-bench: A benchmark for vision-language instruction following inspired by real-world use.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Bitton-Guetta et~al.(2023)Bitton-Guetta, Bitton, Hessel, Schmidt, Elovici, Stanovsky, and Schwartz]{bitton2023breaking}
Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, and Roy Schwartz.
\newblock Breaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2616--2627, 2023.

\bibitem[Blum and Hardt(2015)]{blum2015ladder}
Avrim Blum and Moritz Hardt.
\newblock The ladder: A reliable leaderboard for machine learning competitions.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2015.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Bordes et~al.(2023)Bordes, Shekhar, Ibrahim, Bouchacourt, Vincent, and Morcos]{bordes2023pug}
Florian Bordes, Shashank Shekhar, Mark Ibrahim, Diane Bouchacourt, Pascal Vincent, and Ari~S Morcos.
\newblock Pug: Photorealistic and semantically controllable synthetic data for representation learning.
\newblock \emph{arXiv preprint arXiv:2308.03977}, 2023.

\bibitem[Bowman and Dahl(2021)]{bowman2021will}
Samuel~R Bowman and George~E Dahl.
\newblock What will it take to fix benchmarking in natural language understanding?
\newblock In \emph{North American Chapter of the Association for Computational Linguistics (NAACL)}, 2021.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, et~al.
\newblock Sparks of artificial general intelligence: Early experiments with gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Chen et~al.(2023)Chen, Li, and Xu]{chen2023hibug}
Muxi Chen, Yu~Li, and Qiang Xu.
\newblock Hibug: On human-interpretable model debug.
\newblock In \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Corneanu et~al.(2020)Corneanu, Escalera, and Martinez]{corneanu2020computing}
Ciprian~A Corneanu, Sergio Escalera, and Aleix~M Martinez.
\newblock Computing the testing error without a testing set.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition (CVPR)}, 2020.

\bibitem[Darlow et~al.(2018)Darlow, Crowley, Antoniou, and Storkey]{darlow2018cinic}
Luke~N Darlow, Elliot~J Crowley, Antreas Antoniou, and Amos~J Storkey.
\newblock Cinic-10 is not imagenet or cifar-10.
\newblock \emph{arXiv preprint arXiv:1810.03505}, 2018.

\bibitem[Dehghani et~al.(2021)Dehghani, Arnab, Beyer, Vaswani, and Tay]{dehghani2021efficiency}
Mostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, and Yi~Tay.
\newblock The efficiency misnomer.
\newblock \emph{arXiv preprint arXiv:2110.12894}, 2021.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition (CVPR)}, 2009.

\bibitem[d'Eon et~al.(2022)d'Eon, d'Eon, Wright, and Leyton-Brown]{d2022spotlight}
Greg d'Eon, Jason d'Eon, James~R Wright, and Kevin Leyton-Brown.
\newblock The spotlight: A general method for discovering systematic errors in deep learning models.
\newblock In \emph{Conference on Fairness, Accountability, and Transparency (FAccT)}, 2022.

\bibitem[Dong et~al.(2021)Dong, Liu, Musial, and Gabrys]{dong2021nats}
Xuanyi Dong, Lu~Liu, Katarzyna Musial, and Bogdan Gabrys.
\newblock Nats-bench: Benchmarking nas algorithms for architecture topology and size.
\newblock \emph{Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}, 2021.

\bibitem[Ethayarajh et~al.(2022)Ethayarajh, Choi, and Swayamdipta]{ethayarajh2022understanding}
Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta.
\newblock Understanding dataset difficulty with v-usable information.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Eyuboglu et~al.(2022)Eyuboglu, Varma, Saab, Delbrouck, Lee-Messer, Dunnmon, Zou, and R{\'e}]{eyuboglu2022domino}
Sabri Eyuboglu, Maya Varma, Khaled Saab, Jean-Benoit Delbrouck, Christopher Lee-Messer, Jared Dunnmon, James Zou, and Christopher R{\'e}.
\newblock Domino: Discovering systematic errors with cross-modal embeddings.
\newblock \emph{International Conference on Learning Representations (ICLR)}, 2022.

\bibitem[Fang et~al.(2023)Fang, Kornblith, and Schmidt]{fang2023does}
Alex Fang, Simon Kornblith, and Ludwig Schmidt.
\newblock Does progress on imagenet transfer to real-world datasets?
\newblock In \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Feng et~al.(2023)Feng, Ghosh, Sireci, and Lan]{feng2023balancing}
Wanyong Feng, Aritra Ghosh, Stephen Sireci, and Andrew~S Lan.
\newblock Balancing test accuracy and security in computerized adaptive testing.
\newblock \emph{International Conference on Artificial Intelligence in Education (AIED)}, 2023.

\bibitem[Gadre et~al.(2023)Gadre, Ilharco, Fang, Hayase, Smyrnis, Nguyen, Marten, Wortsman, Ghosh, Zhang, et~al.]{gadre2023datacomp}
Samir~Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et~al.
\newblock Datacomp: In search of the next generation of multimodal datasets.
\newblock In \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Ganguli et~al.(2022)Ganguli, Lovitt, Kernion, Askell, Bai, Kadavath, Mann, Perez, Schiefer, Ndousse, et~al.]{ganguli2022red}
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et~al.
\newblock Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.
\newblock \emph{arXiv preprint arXiv:2209.07858}, 2022.

\bibitem[Gao et~al.(2023)Gao, Ilharco, Lundberg, and Ribeiro]{gao2023adaptive}
Irena Gao, Gabriel Ilharco, Scott Lundberg, and Marco~Tulio Ribeiro.
\newblock Adaptive testing of computer vision models.
\newblock In \emph{International Conference on Computer Vision (ICCV)}, 2023.

\bibitem[Gardner et~al.(2020)Gardner, Artzi, Basmova, Berant, Bogin, Chen, Dasigi, Dua, Elazar, Gottumukkala, et~al.]{gardner2020evaluating}
Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et~al.
\newblock Evaluating models' local decision boundaries via contrast sets.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2020.

\bibitem[Garrido et~al.(2023)Garrido, Balestriero, Najman, and Lecun]{garrido2023rankme}
Quentin Garrido, Randall Balestriero, Laurent Najman, and Yann Lecun.
\newblock Rankme: Assessing the downstream performance of pretrained self-supervised representations by their rank.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2023.

\bibitem[Geirhos et~al.(2018)Geirhos, Rubisch, Michaelis, Bethge, Wichmann, and Brendel]{geirhos2018imagenet}
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix~A Wichmann, and Wieland Brendel.
\newblock Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2018.

\bibitem[Geirhos et~al.(2020)Geirhos, Meding, and Wichmann]{geirhos2020beyond}
Robert Geirhos, Kristof Meding, and Felix~A Wichmann.
\newblock Beyond accuracy: quantifying trial-by-trial behaviour of cnns and humans by measuring error consistency.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Ghosh and Lan(2021)]{ghosh2021bobcat}
Aritra Ghosh and Andrew Lan.
\newblock Bobcat: Bilevel optimization-based computerized adaptive testing.
\newblock \emph{International Joint Conference on Artificial Intelligence (IJCAI)}, 2021.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and perturbations.
\newblock \emph{International Conference on Learning Representations (ICLR)}, 2019.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo, Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2021many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of out-of-distribution generalization.
\newblock In \emph{International Conference on Computer Vision (ICCV)}, 2021{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{International Conference on Learning Representations (ICLR)}, 2021{\natexlab{b}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{c}})Hendrycks, Zhao, Basart, Steinhardt, and Song]{hendrycks2021natural}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition (CVPR)}, 2021{\natexlab{c}}.

\bibitem[Hsieh et~al.(2023)Hsieh, Zhang, Ma, Kembhavi, and Krishna]{hsieh2023sugarcrepe}
Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna.
\newblock Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality.
\newblock \emph{arXiv preprint arXiv:2306.14610}, 2023.

\bibitem[Huang et~al.(2019)Huang, Liu, Zhai, Yin, Chen, Gao, and Hu]{huang2019exploring}
Zhenya Huang, Qi~Liu, Chengxiang Zhai, Yu~Yin, Enhong Chen, Weibo Gao, and Guoping Hu.
\newblock Exploring multi-objective exercise recommendations in online education systems.
\newblock In \emph{International Conference on Information and Knowledge Management (CIKM)}, 2019.

\bibitem[Hutchinson et~al.(2022)Hutchinson, Rostamzadeh, Greer, Heller, and Prabhakaran]{hutchinson2022evaluation}
Ben Hutchinson, Negar Rostamzadeh, Christina Greer, Katherine Heller, and Vinodkumar Prabhakaran.
\newblock Evaluation gaps in machine learning practice.
\newblock In \emph{Conference on Fairness, Accountability, and Transparency (FAccT)}, 2022.

\bibitem[Ilyas~Moutawwakil(2023)]{llm-perf-leaderboard}
RÃ©gis~Pierrard Ilyas~Moutawwakil.
\newblock Llm-perf leaderboard.
\newblock \url{https://huggingface.co/spaces/optimum/llm-perf-leaderboard}, 2023.

\bibitem[Jain et~al.(2023)Jain, Saifullah, Wen, Kirchenbauer, Shu, Saha, Goldblum, Geiping, and Goldstein]{jain2023bring}
Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein.
\newblock Bring your own data! self-supervised evaluation for large language models.
\newblock \emph{arXiv preprint arXiv:2306.13651}, 2023.

\bibitem[Ji et~al.(2021)Ji, Logan, Smyth, and Steyvers]{ji2021active}
Disi Ji, Robert~L Logan, Padhraic Smyth, and Mark Steyvers.
\newblock Active bayesian assessment of black-box classifiers.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pages 7935--7944, 2021.

\bibitem[Kamath et~al.(2023)Kamath, Hessel, and Chang]{kamath2023text}
Amita Kamath, Jack Hessel, and Kai-Wei Chang.
\newblock Text encoders are performance bottlenecks in contrastive vision-language models.
\newblock \emph{arXiv preprint arXiv:2305.14897}, 2023.

\bibitem[Kaplun et~al.(2023)Kaplun, Ghosh, Garg, Barak, and Nakkiran]{kaplun2022deconstructing}
Gal Kaplun, Nikhil Ghosh, Saurabh Garg, Boaz Barak, and Preetum Nakkiran.
\newblock Deconstructing distributions: A pointwise framework of learning.
\newblock \emph{International Conference on Learning Representations (ICLR)}, 2023.

\bibitem[Khan et~al.(2011)Khan, Mutlu, and Zhu]{khan2011humans}
Faisal Khan, Bilge Mutlu, and Jerry Zhu.
\newblock How do humans teach: On curriculum learning and teaching dimension.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Kiela et~al.(2021)Kiela, Bartolo, Nie, Kaushik, Geiger, Wu, Vidgen, Prasad, Singh, Ringshia, et~al.]{kiela2021dynabench}
Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et~al.
\newblock Dynabench: Rethinking benchmarking in nlp.
\newblock \emph{North American Chapter of the Association for Computational Linguistics (NAACL)}, 2021.

\bibitem[Kossen et~al.(2021)Kossen, Farquhar, Gal, and Rainforth]{kossen2021active}
Jannik Kossen, Sebastian Farquhar, Yarin Gal, and Tom Rainforth.
\newblock Active testing: Sample-efficient model evaluation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Kossen et~al.(2022)Kossen, Farquhar, Gal, and Rainforth]{kossen2022active}
Jannik Kossen, Sebastian Farquhar, Yarin Gal, and Thomas Rainforth.
\newblock Active surrogate estimators: An active learning approach to label-efficient model evaluation.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kuznetsova et~al.(2020)Kuznetsova, Rom, Alldrin, Uijlings, Krasin, Pont-Tuset, Kamali, Popov, Malloci, Kolesnikov, et~al.]{kuznetsova2020open}
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et~al.
\newblock The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 128\penalty0 (7):\penalty0 1956--1981, 2020.

\bibitem[Lee et~al.(2023)Lee, Yasunaga, Meng, Mai, Park, Gupta, Zhang, Narayanan, Teufel, Bellagente, et~al.]{lee2023holistic}
Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon~Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah~Benita Teufel, Marco Bellagente, et~al.
\newblock Holistic evaluation of text-to-image models.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga, Zhang, Narayanan, Wu, Kumar, et~al.]{liang2022holistic}
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et~al.
\newblock Holistic evaluation of language models.
\newblock \emph{arXiv preprint arXiv:2211.09110}, 2022.

\bibitem[Liao et~al.(2021)Liao, Taori, Raji, and Schmidt]{liao2021we}
Thomas Liao, Rohan Taori, Inioluwa~Deborah Raji, and Ludwig Schmidt.
\newblock Are we learning yet? a meta review of evaluation failures across machine learning.
\newblock In \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Lu et~al.(2020)Lu, Nott, Olson, Todeschini, Vahabi, Carmon, and Schmidt]{lu2020harder}
Shangyun Lu, Bradley Nott, Aaron Olson, Alberto Todeschini, Hossein Vahabi, Yair Carmon, and Ludwig Schmidt.
\newblock Harder or different? a closer look at distribution shift in dataset reproduction.
\newblock In \emph{International Conference on Machine Learning Workshops (ICML-W)}, 2020.

\bibitem[Magar and Schwartz(2022)]{magar2022data}
Inbal Magar and Roy Schwartz.
\newblock Data contamination: From memorization to exploitation.
\newblock \emph{arXiv preprint arXiv:2203.08242}, 2022.

\bibitem[Mania et~al.(2019)Mania, Miller, Schmidt, Hardt, and Recht]{mania2019model}
Horia Mania, John Miller, Ludwig Schmidt, Moritz Hardt, and Benjamin Recht.
\newblock Model similarity mitigates test set overuse.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 32, 2019.

\bibitem[Mujtaba and Mahapatra(2021)]{mujtaba2021multi}
Dena~F Mujtaba and Nihar~R Mahapatra.
\newblock Multi-objective optimization of item selection in computerized adaptive testing.
\newblock In \emph{Proceedings of the Genetic and Evolutionary Computation Conference}, pages 1018--1026, 2021.

\bibitem[Nie et~al.(2020)Nie, Williams, Dinan, Bansal, Weston, and Kiela]{nie2019adversarial}
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela.
\newblock Adversarial nli: A new benchmark for natural language understanding.
\newblock \emph{Annual Meeting of the Association for Computational Linguistics (ACL)}, 2020.

\bibitem[Ott et~al.(2022)Ott, Barbosa-Silva, Blagec, Brauner, and Samwald]{ott2022mapping}
Simon Ott, Adriano Barbosa-Silva, Kathrin Blagec, Jan Brauner, and Matthias Samwald.
\newblock Mapping global dynamics of benchmark creation and saturation in artificial intelligence.
\newblock \emph{Nature Communications}, 13\penalty0 (1):\penalty0 6793, 2022.

\bibitem[Parcalabescu et~al.(2021)Parcalabescu, Cafagna, Muradjan, Frank, Calixto, and Gatt]{parcalabescu2021valse}
Letitia Parcalabescu, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, and Albert Gatt.
\newblock Valse: A task-independent benchmark for vision and language models centered on linguistic phenomena.
\newblock \emph{arXiv preprint arXiv:2112.07566}, 2021.

\bibitem[Perez et~al.(2022)Perez, Huang, Song, Cai, Ring, Aslanides, Glaese, McAleese, and Irving]{perez2022red}
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.
\newblock Red teaming language models with language models.
\newblock \emph{Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2022.

\bibitem[Perlitz et~al.(2023)Perlitz, Bandel, Gera, Arviv, Ein-Dor, Shnarch, Slonim, Shmueli-Scheuer, and Choshen]{perlitz2023efficient}
Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen.
\newblock Efficient benchmarking (of language models).
\newblock \emph{arXiv preprint arXiv:2308.11696}, 2023.

\bibitem[Peychev et~al.(2023)Peychev, M{\"u}ller, Fischer, and Vechev]{peychev2023automated}
Momchil Peychev, Mark~Niklas M{\"u}ller, Marc Fischer, and Martin Vechev.
\newblock Automated classification of model errors on imagenet.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Polo et~al.(2024)Polo, Weber, Choshen, Sun, Xu, and Yurochkin]{polo2024tinybenchmarks}
Felipe~Maia Polo, Lucas Weber, Leshem Choshen, Yuekai Sun, Gongjun Xu, and Mikhail Yurochkin.
\newblock tinybenchmarks: evaluating llms with fewer examples.
\newblock \emph{arXiv preprint arXiv:2402.14992}, 2024.

\bibitem[Potts et~al.(2021)Potts, Wu, Geiger, and Kiela]{potts2020dynasent}
Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela.
\newblock Dynasent: A dynamic benchmark for sentiment analysis.
\newblock \emph{Dynasent: A dynamic benchmark for sentiment analysis}, 2021.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Raji et~al.(2021)Raji, Bender, Paullada, Denton, and Hanna]{raji2021ai}
Inioluwa~Deborah Raji, Emily~M Bender, Amandalynne Paullada, Emily Denton, and Alex Hanna.
\newblock Ai and the everything in the whole wide world benchmark.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Recht et~al.(2018)Recht, Roelofs, Schmidt, and Shankar]{recht2018cifar}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do cifar-10 classifiers generalize to cifar-10?
\newblock \emph{arXiv preprint arXiv:1806.00451}, 2018.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and Shankar]{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Rodriguez et~al.(2021)Rodriguez, Barrow, Hoyle, Lalor, Jia, and Boyd-Graber]{rodriguez2021evaluation}
Pedro Rodriguez, Joe Barrow, Alexander~Miserlis Hoyle, John~P Lalor, Robin Jia, and Jordan Boyd-Graber.
\newblock Evaluation examples are not equally informative: How should that change nlp leaderboards?
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics (ACL)}, 2021.

\bibitem[Roelofs et~al.(2019)Roelofs, Shankar, Recht, Fridovich-Keil, Hardt, Miller, and Schmidt]{roelofs2019meta}
Rebecca Roelofs, Vaishaal Shankar, Benjamin Recht, Sara Fridovich-Keil, Moritz Hardt, John Miller, and Ludwig Schmidt.
\newblock A meta-analysis of overfitting in machine learning.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Rofin et~al.(2022)Rofin, Mikhailov, Florinskiy, Kravchenko, Tutubalina, Shavrina, Karabekyan, and Artemova]{rofin2022vote}
Mark Rofin, Vladislav Mikhailov, Mikhail Florinskiy, Andrey Kravchenko, Elena Tutubalina, Tatiana Shavrina, Daniel Karabekyan, and Ekaterina Artemova.
\newblock Vote'n'rank: Revision of benchmarking with social choice theory.
\newblock \emph{Annual Meeting of the Association for Computational Linguistics (EACL)}, 2022.

\bibitem[Sardana and Frankle(2023)]{sardana2023beyond}
Nikhil Sardana and Jonathan Frankle.
\newblock Beyond chinchilla-optimal: Accounting for inference in language model scaling laws.
\newblock \emph{arXiv preprint arXiv:2401.00448}, 2023.

\bibitem[Shi et~al.(2023)Shi, Wang, Fan, Yin, Sheng, Qiao, and Shao]{shi2023chef}
Zhelun Shi, Zhipin Wang, Hongxing Fan, Zhenfei Yin, Lu~Sheng, Yu~Qiao, and Jing Shao.
\newblock Chef: A comprehensive evaluation framework for standardized assessment of multimodal large language models.
\newblock \emph{arXiv preprint arXiv:2311.02692}, 2023.

\bibitem[Shirali and Hardt(2023)]{shirali2023makes}
Ali Shirali and Moritz Hardt.
\newblock What makes imagenet look unlike laion.
\newblock \emph{arXiv preprint arXiv:2306.15769}, 2023.

\bibitem[Shirali et~al.(2022)Shirali, Abebe, and Hardt]{shirali2022theory}
Ali Shirali, Rediet Abebe, and Moritz Hardt.
\newblock A theory of dynamic benchmarks.
\newblock \emph{arXiv preprint arXiv:2210.03165}, 2022.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[Sun et~al.(2023)Sun, Leng, Wang, Yang, Huang, and Zheng]{sun2023cifar}
Xiaoxiao Sun, Xingjian Leng, Zijian Wang, Yang Yang, Zi~Huang, and Liang Zheng.
\newblock Cifar-10-warehouse: Broad and more realistic testbeds in model generalization analysis.
\newblock \emph{arXiv preprint arXiv:2310.04414}, 2023.

\bibitem[Taori et~al.(2020)Taori, Dave, Shankar, Carlini, Recht, and Schmidt]{taori2020measuring}
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt.
\newblock Measuring robustness to natural distribution shifts in image classification.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Thrush et~al.(2022)Thrush, Jiang, Bartolo, Singh, Williams, Kiela, and Ross]{thrush2022winoground}
Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross.
\newblock Winoground: Probing vision and language models for visio-linguistic compositionality.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 5238--5248, 2022.

\bibitem[Tian et~al.(2023)Tian, Fan, Chen, Katabi, Krishnan, and Isola]{tian2023learning}
Yonglong Tian, Lijie Fan, Kaifeng Chen, Dina Katabi, Dilip Krishnan, and Phillip Isola.
\newblock Learning vision from models rivals learning vision from data.
\newblock \emph{arXiv preprint arXiv:2312.17742}, 2023.

\bibitem[Torralba and Efros(2011)]{torralba2011unbiased}
Antonio Torralba and Alexei~A Efros.
\newblock Unbiased look at dataset bias.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition (CVPR)}, 2011.

\bibitem[Udandarao et~al.(2023)Udandarao, Burg, Albanie, and Bethge]{udandarao2023visual}
Vishaal Udandarao, Max~F Burg, Samuel Albanie, and Matthias Bethge.
\newblock Visual data-type understanding does not emerge from scaling vision-language models.
\newblock \emph{arXiv preprint arXiv:2310.08577}, 2023.

\bibitem[Van~der Linden and Glas(2000)]{van2000computerized}
Wim~J Van~der Linden and Cees~AW Glas.
\newblock \emph{Computerized adaptive testing: Theory and practice}.
\newblock Springer, 2000.

\bibitem[Vishniakov et~al.(2023)Vishniakov, Shen, and Liu]{vishniakov2023convnet}
Kirill Vishniakov, Zhiqiang Shen, and Zhuang Liu.
\newblock Convnet vs transformer, supervised vs clip: Beyond imagenet accuracy.
\newblock 2023.

\bibitem[Vivek et~al.(2023)Vivek, Ethayarajh, Yang, and Kiela]{vivek2023anchor}
Rajan Vivek, Kawin Ethayarajh, Diyi Yang, and Douwe Kiela.
\newblock Anchor points: Benchmarking models with much fewer examples.
\newblock \emph{arXiv preprint arXiv:2309.08638}, 2023.

\bibitem[Wallace et~al.(2022)Wallace, Williams, Jia, and Kiela]{wallace2022analyzing}
Eric Wallace, Adina Williams, Robin Jia, and Douwe Kiela.
\newblock Analyzing dynamic adversarial training data in the limit.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics (ACL)}, pages 202--217, 2022.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}, 2018.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Pruksachatkun, Nangia, Singh, Michael, Hill, Levy, and Bowman]{wang2019superglue}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
\newblock Superglue: A stickier benchmark for general-purpose language understanding systems.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2019{\natexlab{a}}.

\bibitem[Wang et~al.(2023)Wang, Long, Yin, Zhang, Xia, Hong, Xia, Tang, and Yu]{wang2023gmocat}
Hangyu Wang, Ting Long, Liang Yin, Weinan Zhang, Wei Xia, Qichen Hong, Dingyin Xia, Ruiming Tang, and Yong Yu.
\newblock Gmocat: A graph-enhanced multi-objective method for computerized adaptive testing.
\newblock In \emph{Conference on Knowledge Discovery and Data Mining (KDD)}, 2023.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Ge, Lipton, and Xing]{wang2019learning}
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric~P Xing.
\newblock Learning robust global representations by penalizing local predictive power.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)}, 2019{\natexlab{b}}.

\bibitem[Wang et~al.(2021)Wang, You, Chen, Zhang, Dong, and Zhang]{wang2021prioritizing}
Zan Wang, Hanmo You, Junjie Chen, Yingyi Zhang, Xuyuan Dong, and Wenbin Zhang.
\newblock Prioritizing test inputs for deep neural networks via mutation analysis.
\newblock In \emph{2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)}, pages 397--409. IEEE, 2021.

\bibitem[Wightman(2019)]{rw2019timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Wiles et~al.(2022)Wiles, Albuquerque, and Gowal]{wiles2022discovering}
Olivia Wiles, Isabela Albuquerque, and Sven Gowal.
\newblock Discovering bugs in vision models using off-the-shelf image generation and captioning.
\newblock \emph{arXiv preprint arXiv:2208.08831}, 2022.

\bibitem[Yu et~al.(2023)Yu, Zhenyu, Lei, Yin, Xia, Yu, and Long]{yu2023sacat}
Jingwei Yu, Mu~Zhenyu, Jiayi Lei, Li'Ang Yin, Wei Xia, Yong Yu, and Ting Long.
\newblock Sacat: Student-adaptive computerized adaptive testing.
\newblock In \emph{The Fifth International Conference on Distributed Artificial Intelligence}, 2023.

\bibitem[Yuan and Ghanem(2016)]{yuan2016binary}
Ganzhao Yuan and Bernard Ghanem.
\newblock Binary optimization via mathematical programming with equilibrium constraints.
\newblock \emph{arXiv preprint arXiv:1608.04425}, 2016.

\bibitem[Yue et~al.(2023)Yue, Ni, Zhang, Zheng, Liu, Zhang, Stevens, Jiang, Ren, Sun, et~al.]{yue2023mmmu}
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge~Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et~al.
\newblock Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi.
\newblock \emph{arXiv preprint arXiv:2311.16502}, 2023.

\bibitem[Yuksekgonul et~al.(2022)Yuksekgonul, Bianchi, Kalluri, Jurafsky, and Zou]{yuksekgonul2022and}
Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou.
\newblock When and why vision-language models behave like bags-of-words, and what to do about it?
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Zhai et~al.(2019)Zhai, Puigcerver, Kolesnikov, Ruyssen, Riquelme, Lucic, Djolonga, Pinto, Neumann, Dosovitskiy, et~al.]{zhai2019visual}
Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre~Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et~al.
\newblock The visual task adaptation benchmark.
\newblock 2019.

\bibitem[Zhang et~al.(2023)Zhang, Huang, Ding, Zhan, and Ye]{zhang2023model}
Yi-Kai Zhang, Ting-Ji Huang, Yao-Xiang Ding, De-Chuan Zhan, and Han-Jia Ye.
\newblock Model spider: Learning to rank pre-trained models efficiently.
\newblock \emph{arXiv preprint arXiv:2306.03900}, 2023.

\bibitem[Zhao et~al.(2024)Zhao, Zhao, Lin, Ning, Dai, Yang, and Wang]{zhao2024flasheval}
Lin Zhao, Tianchen Zhao, Zinan Lin, Xuefei Ning, Guohao Dai, Huazhong Yang, and Yu~Wang.
\newblock Flasheval: Towards fast and accurate evaluation of text-to-image diffusion generative models.
\newblock \emph{arXiv preprint arXiv:2403.16379}, 2024.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, Zhang, Gonzalez, and Stoica]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric.~P Xing, Hao Zhang, Joseph~E. Gonzalez, and Ion Stoica.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

\bibitem[Zhou et~al.(2022)Zhou, Zeng, Diao, and Zhang]{zhou2022vlue}
Wangchunshu Zhou, Yan Zeng, Shizhe Diao, and Xinsong Zhang.
\newblock Vlue: A multi-task multi-dimension benchmark for evaluating vision-language pre-training.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Zhuang et~al.(2022)Zhuang, Liu, Huang, Li, Shen, and Ma]{zhuang2022fully}
Yan Zhuang, Qi~Liu, Zhenya Huang, Zhi Li, Shuanghong Shen, and Haiping Ma.
\newblock Fully adaptive framework: Neural computerized adaptive testing for online education.
\newblock In \emph{Conference on Artificial Intelligence (AAAI)}, 2022.

\bibitem[Zohar et~al.(2023)Zohar, Huang, Wang, and Yeung]{zohar2023lovm}
Orr Zohar, Shih-Cheng Huang, Kuan-Chieh Wang, and Serena Yeung.
\newblock Lovm: Language-only vision model selection.
\newblock \emph{arXiv preprint arXiv:2306.08893}, 2023.

\end{thebibliography}
