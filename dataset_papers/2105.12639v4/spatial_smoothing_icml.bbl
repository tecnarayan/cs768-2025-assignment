\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Antoran et~al.(2020)Antoran, Allingham, and
  Hern{\'a}ndez-Lobato]{antoran2020depth}
Antoran, J., Allingham, J., and Hern{\'a}ndez-Lobato, J.~M.
\newblock Depth uncertainty in neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Azulay \& Weiss(2019)Azulay and Weiss]{azulay2018deep}
Azulay, A. and Weiss, Y.
\newblock Why do deep convolutional networks generalize so poorly to small
  image transformations?
\newblock \emph{Journal of Machine Learning Research}, 2019.

\bibitem[Brostow et~al.(2008)Brostow, Shotton, Fauqueur, and
  Cipolla]{brostow2008segmentation}
Brostow, G.~J., Shotton, J., Fauqueur, J., and Cipolla, R.
\newblock Segmentation and recognition using structure from motion point
  clouds.
\newblock In \emph{European Conference on Computer Vision}, 2008.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Dusenberry et~al.(2020)Dusenberry, Jerfel, Wen, Ma, Snoek, Heller,
  Lakshminarayanan, and Tran]{dusenberry2020efficient}
Dusenberry, M., Jerfel, G., Wen, Y., Ma, Y., Snoek, J., Heller, K.,
  Lakshminarayanan, B., and Tran, D.
\newblock Efficient and scalable bayesian neural nets with rank-1 factors.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Engstrom et~al.(2019)Engstrom, Tran, Tsipras, Schmidt, and
  Madry]{engstrom2019exploring}
Engstrom, L., Tran, B., Tsipras, D., Schmidt, L., and Madry, A.
\newblock Exploring the landscape of spatial robustness.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2020sharpness}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Fort et~al.(2019)Fort, Hu, and Lakshminarayanan]{fort2019deep}
Fort, S., Hu, H., and Lakshminarayanan, B.
\newblock Deep ensembles: A loss landscape perspective.
\newblock \emph{arXiv preprint arXiv:1912.02757}, 2019.

\bibitem[Frankle et~al.(2021)Frankle, Schwab, and Morcos]{frankle2020training}
Frankle, J., Schwab, D.~J., and Morcos, A.~S.
\newblock Training batchnorm and only batchnorm: On the expressive power of
  random features in cnns.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016dropout}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Krishnan, and
  Xiao]{ghorbani2019investigation}
Ghorbani, B., Krishnan, S., and Xiao, Y.
\newblock An investigation into neural net optimization via hessian eigenvalue
  density.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Goyal, P., Doll{\'a}r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{he2016identity}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European Conference on Computer Vision}, 2016{\natexlab{b}}.

\bibitem[Hendrycks \& Dietterich(2019)Hendrycks and
  Dietterich]{hendrycks2019benchmarking}
Hendrycks, D. and Dietterich, T.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Hern{\'a}ndez-Lobato \& Adams(2015)Hern{\'a}ndez-Lobato and
  Adams]{hernandez2015probabilistic}
Hern{\'a}ndez-Lobato, J.~M. and Adams, R.
\newblock Probabilistic backpropagation for scalable learning of bayesian
  neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2015.

\bibitem[Hoffer et~al.(2020)Hoffer, Ben-Nun, Hubara, Giladi, Hoefler, and
  Soudry]{hoffer2020augment}
Hoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoefler, T., and Soudry, D.
\newblock Augment your batch: Improving generalization through instance
  repetition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2020.

\bibitem[Kendall \& Gal(2017)Kendall and Gal]{kendall2017uncertainties}
Kendall, A. and Gal, Y.
\newblock What uncertainties do we need in bayesian deep learning for computer
  vision?
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Kendall et~al.(2017)Kendall, Badrinarayanan, and
  Cipolla]{kendall2017bayesian}
Kendall, A., Badrinarayanan, V., and Cipolla, R.
\newblock Bayesian segnet: Model uncertainty in deep convolutional
  encoder-decoder architectures for scene understanding.
\newblock In \emph{BMVC}, 2017.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Krizhevsky \& Hinton(2010)Krizhevsky and
  Hinton]{krizhevsky2010convolutional}
Krizhevsky, A. and Hinton, G.
\newblock Convolutional deep belief networks on cifar-10.
\newblock \emph{Unpublished manuscript}, 2010.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Lakshminarayanan, B., Pritzel, A., and Blundell, C.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2017visualizing}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Lin et~al.(2014)Lin, Chen, and Yan]{lin2013network}
Lin, M., Chen, Q., and Yan, S.
\newblock Network in network.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Loquercio et~al.(2020)Loquercio, Segu, and
  Scaramuzza]{loquercio2020general}
Loquercio, A., Segu, M., and Scaramuzza, D.
\newblock A general framework for uncertainty estimation in deep learning.
\newblock \emph{IEEE Robotics and Automation Letters}, 2020.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Malinin \& Gales(2018)Malinin and Gales]{malinin2018predictive}
Malinin, A. and Gales, M.
\newblock Predictive uncertainty estimation via prior networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019can}
Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon,
  J., Lakshminarayanan, B., and Snoek, J.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Park \& Kim(2022)Park and Kim]{park2021vision}
Park, N. and Kim, S.
\newblock How do vision transformers work?
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Park et~al.(2021)Park, Lee, and Kim]{park2019vqbnn}
Park, N., Lee, T., and Kim, S.
\newblock Vector quantized bayesian neural network inference for data streams.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Postels et~al.(2019)Postels, Ferroni, Coskun, Navab, and
  Tombari]{postels2019sampling}
Postels, J., Ferroni, F., Coskun, H., Navab, N., and Tombari, F.
\newblock Sampling-free epistemic uncertainty estimation using approximated
  variance propagation.
\newblock In \emph{International Conference on Computer Vision}, 2019.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
Ronneberger, O., Fischer, P., and Brox, T.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{International Conference on Medical image computing and
  computer-assisted intervention}, 2015.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision}, 2015.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2018.

\bibitem[Santurkar et~al.(2018)Santurkar, Tsipras, Ilyas, and
  Madry]{santurkar2018does}
Santurkar, S., Tsipras, D., Ilyas, A., and Madry, A.
\newblock How does batch normalization help optimization?
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Shao et~al.(2021)Shao, Shi, Yi, Chen, and Hsieh]{shao2021adversarial}
Shao, R., Shi, Z., Yi, J., Chen, P.-Y., and Hsieh, C.-J.
\newblock On the adversarial robustness of visual transformers.
\newblock \emph{arXiv preprint arXiv:2103.15670}, 2021.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Sinha et~al.(2020)Sinha, Garg, and Larochelle]{sinha2020curriculum}
Sinha, S., Garg, A., and Larochelle, H.
\newblock Curriculum by smoothing.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Vasconcelos et~al.(2020)Vasconcelos, Larochelle, Dumoulin, Roux, and
  Goroshin]{vasconcelos2020effective}
Vasconcelos, C., Larochelle, H., Dumoulin, V., Roux, N.~L., and Goroshin, R.
\newblock An effective anti-aliasing approach for residual networks.
\newblock \emph{arXiv preprint arXiv:2011.10675}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Veit et~al.(2016)Veit, Wilber, and Belongie]{veit2016residual}
Veit, A., Wilber, M.~J., and Belongie, S.
\newblock Residual networks behave like ensembles of relatively shallow
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Wang et~al.(2016)Wang, Shi, and Yeung]{wang2016natural}
Wang, H., Shi, X., and Yeung, D.-Y.
\newblock Natural-parameter networks: A class of probabilistic neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Wang et~al.(2022)Wang, Zheng, Chen, and Wang]{wang2021scaling}
Wang, P., Zheng, W., Chen, T., and Wang, Z.
\newblock Scaling the depth of vision transformers via the fourier domain
  analysis.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Wang et~al.(2018)Wang, Girshick, Gupta, and He]{wang2018non}
Wang, X., Girshick, R., Gupta, A., and He, K.
\newblock Non-local neural networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2018.

\bibitem[Wen et~al.(2020)Wen, Tran, and Ba]{wen2020batchensemble}
Wen, Y., Tran, D., and Ba, J.
\newblock Batchensemble: an alternative approach to efficient ensemble and
  lifelong learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Wilson \& Izmailov(2020)Wilson and Izmailov]{wilson2020bayesian}
Wilson, A.~G. and Izmailov, P.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Wu et~al.(2019)Wu, Nowozin, Meeds, Turner, Hern{\'a}ndez-Lobato, and
  Gaunt]{wu2018deterministic}
Wu, A., Nowozin, S., Meeds, E., Turner, R.~E., Hern{\'a}ndez-Lobato, J.~M., and
  Gaunt, A.~L.
\newblock Deterministic variational inference for robust bayesian neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2017aggregated}
Xie, S., Girshick, R., Doll{\'a}r, P., Tu, Z., and He, K.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2017.

\bibitem[Yao et~al.(2020)Yao, Gholami, Keutzer, and Mahoney]{yao2020pyhessian}
Yao, Z., Gholami, A., Keutzer, K., and Mahoney, M.~W.
\newblock Pyhessian: Neural networks through the lens of the hessian.
\newblock In \emph{2020 IEEE International Conference on Big Data (Big Data)},
  2020.

\bibitem[Yoon et~al.(2018)Yoon, Kim, Dia, Kim, Bengio, and
  Ahn]{yoon2018bayesian}
Yoon, J., Kim, T., Dia, O., Kim, S., Bengio, Y., and Ahn, S.
\newblock Bayesian model-agnostic meta-learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{BMVC}, 2016.

\bibitem[Zhang(2019)]{zhang2019making}
Zhang, R.
\newblock Making convolutional networks shift-invariant again.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Zhou et~al.(2016)Zhou, Khosla, Lapedriza, Oliva, and
  Torralba]{zhou2016learning}
Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba, A.
\newblock Learning deep features for discriminative localization.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2016.

\bibitem[Zou et~al.(2020)Zou, Xiao, Yu, and Lee]{zou2020delving}
Zou, X., Xiao, F., Yu, Z., and Lee, Y.~J.
\newblock Delving deeper into anti-aliasing in convnets.
\newblock In \emph{BMVC}, 2020.

\end{thebibliography}
