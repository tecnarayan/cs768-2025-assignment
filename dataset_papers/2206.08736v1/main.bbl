\begin{thebibliography}{90}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Munos,
  Heess, and Riedmiller]{abdolmaleki2018maximum}
Abdolmaleki, A., Springenberg, J.~T., Tassa, Y., Munos, R., Heess, N., and
  Riedmiller, M.
\newblock Maximum a posteriori policy optimisation.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2018.

\bibitem[Agrawal \& Jia(2017)Agrawal and Jia]{agrawal2017optimistic}
Agrawal, S. and Jia, R.
\newblock Optimistic posterior sampling for reinforcement learning: worst-case
  regret bounds.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Alver \& Precup(2022)Alver and Precup]{alver2022constructing}
Alver, S. and Precup, D.
\newblock Constructing a good behavior basis for transfer using generalized
  policy updates.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2022.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv}, 2016.

\bibitem[Babuschkin et~al.(2020)Babuschkin, Baumli, Bell, Bhupatiraju, Bruce,
  Buchlovsky, Budden, Cai, Clark, Danihelka, Fantacci, Godwin, Jones, Hennigan,
  Hessel, Kapturowski, Keck, Kemaev, King, Martens, Mikulik, Norman, Quan,
  Papamakarios, Ring, Ruiz, Sanchez, Schneider, Sezener, Spencer, Srinivasan,
  Stokowiec, and Viola]{deepmind2020jax}
Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J., Buchlovsky,
  P., Budden, D., Cai, T., Clark, A., Danihelka, I., Fantacci, C., Godwin, J.,
  Jones, C., Hennigan, T., Hessel, M., Kapturowski, S., Keck, T., Kemaev, I.,
  King, M., Martens, L., Mikulik, V., Norman, T., Quan, J., Papamakarios, G.,
  Ring, R., Ruiz, F., Sanchez, A., Schneider, R., Sezener, E., Spencer, S.,
  Srinivasan, S., Stokowiec, W., and Viola, F.
\newblock The {D}eep{M}ind {JAX} {E}cosystem, 2020.

\bibitem[Barreto et~al.(2017)Barreto, Dabney, Munos, Hunt, Schaul, Van~Hasselt,
  and Silver]{barreto2016successor}
Barreto, A., Dabney, W., Munos, R., Hunt, J.~J., Schaul, T., Van~Hasselt, H.,
  and Silver, D.
\newblock Successor features for transfer in reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Barreto et~al.(2018)Barreto, Borsa, Quan, Schaul, Silver, Hessel,
  Mankowitz, Zidek, and Munos]{pmlr-v80-barreto18a}
Barreto, A., Borsa, D., Quan, J., Schaul, T., Silver, D., Hessel, M.,
  Mankowitz, D., Zidek, A., and Munos, R.
\newblock Transfer in deep reinforcement learning using successor features and
  generalised policy improvement.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2018.

\bibitem[Barreto et~al.(2019)Barreto, Borsa, Hou, Comanici, Ayg{\"{u}}n, Hamel,
  Toyama, Hunt, Mourad, Silver, and Precup]{barreto2019optionkeyboard}
Barreto, A., Borsa, D., Hou, S., Comanici, G., Ayg{\"{u}}n, E., Hamel, P.,
  Toyama, D., Hunt, J.~J., Mourad, S., Silver, D., and Precup, D.
\newblock The option keyboard: Combining skills in reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Barreto et~al.(2020)Barreto, Hou, Borsa, Silver, and
  Precup]{barreto2020fast}
Barreto, A., Hou, S., Borsa, D., Silver, D., and Precup, D.
\newblock Fast reinforcement learning with generalized policy updates.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30079--30087, 2020.
\newblock ISSN 0027-8424.

\bibitem[Bertsekas \& Tsitsiklis(1996)Bertsekas and
  Tsitsiklis]{bertsekas1996neuro}
Bertsekas, D.~P. and Tsitsiklis, J.~N.
\newblock \emph{Neuro-Dynamic Programming}.
\newblock Athena Scientific, 1996.

\bibitem[Blier et~al.(2021)Blier, Tallec, and Ollivier]{blier2021learning}
Blier, L., Tallec, C., and Ollivier, Y.
\newblock Learning successor states and goal-dependent values: A mathematical
  viewpoint.
\newblock \emph{arXiv}, 2021.

\bibitem[Borsa et~al.(2019)Borsa, Barreto, Quan, Mankowitz, van Hasselt, Munos,
  Silver, and Schaul]{borsa2019universal}
Borsa, D., Barreto, A., Quan, J., Mankowitz, D.~J., van Hasselt, H., Munos, R.,
  Silver, D., and Schaul, T.
\newblock Universal successor features approximators.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2019.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.

\bibitem[Brunskill \& Li(2014)Brunskill and Li]{brunskill2014pac}
Brunskill, E. and Li, L.
\newblock {PAC}-inspired option discovery in lifelong reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2014.

\bibitem[Bu{\c{s}}oniu \& Munos(2012)Bu{\c{s}}oniu and
  Munos]{busoniu2012optimistic}
Bu{\c{s}}oniu, L. and Munos, R.
\newblock Optimistic planning for {M}arkov decision processes.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, 2012.

\bibitem[Bu{\c{s}}oniu et~al.(2012)Bu{\c{s}}oniu, Munos, and
  Babu{\v{s}}ka]{busoniu2012survey}
Bu{\c{s}}oniu, L., Munos, R., and Babu{\v{s}}ka, R.
\newblock A survey of optimistic planning in {M}arkov decision processes.
\newblock In Lewis, F.~L. and Liu, D. (eds.), \emph{Reinforcement Learning and
  Adaptive Dynamic Programming for Feedback Control}, chapter~22, pp.\
  494--516. John Wiley \& Sons, 2012.

\bibitem[Dabney et~al.(2021)Dabney, Ostrovski, and
  Barreto]{dabney2020temporally}
Dabney, W., Ostrovski, G., and Barreto, A.
\newblock Temporally-extended $\epsilon$-greedy exploration.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2021.

\bibitem[Dalal et~al.(2021)Dalal, Hallak, Dalton, Frosio, Mannor, and
  Chechik]{dalal2021improve}
Dalal, G., Hallak, A., Dalton, S., Frosio, I., Mannor, S., and Chechik, G.
\newblock Improve agents without retraining: Parallel tree search with
  off-policy correction.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Dayan(1993)]{dayan1993improving}
Dayan, P.
\newblock Improving generalization for temporal difference learning: The
  successor representation.
\newblock \emph{Neural Computation}, 5\penalty0 (4):\penalty0 613--624, 1993.

\bibitem[Dinh et~al.(2017)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
Dinh, L., Sohl-Dickstein, J., and Bengio, S.
\newblock Density estimation using real {NVP}.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2017.

\bibitem[Durkan et~al.(2019)Durkan, Bekasov, Murray, and
  Papamakarios]{durkan2019neural}
Durkan, C., Bekasov, A., Murray, I., and Papamakarios, G.
\newblock Neural spline flows.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Efroni et~al.(2018{\natexlab{a}})Efroni, Dalal, Scherrer, and
  Mannor]{efroni2018beyond}
Efroni, Y., Dalal, G., Scherrer, B., and Mannor, S.
\newblock Beyond the one step greedy approach in reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2018{\natexlab{a}}.

\bibitem[Efroni et~al.(2018{\natexlab{b}})Efroni, Dalal, Scherrer, and
  Mannor]{efroni2018multiple}
Efroni, Y., Dalal, G., Scherrer, B., and Mannor, S.
\newblock Multiple-step greedy policies in online and approximate reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2018{\natexlab{b}}.

\bibitem[Efroni et~al.(2019)Efroni, Dalal, Scherrer, and
  Mannor]{efroni2019combine}
Efroni, Y., Dalal, G., Scherrer, B., and Mannor, S.
\newblock How to combine tree-search methods in reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2019.

\bibitem[Eysenbach et~al.(2021)Eysenbach, Salakhutdinov, and
  Levine]{eysenbach2021c}
Eysenbach, B., Salakhutdinov, R., and Levine, S.
\newblock C-learning: Learning to achieve goals via recursive classification.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2021.

\bibitem[Feldman \& Domshlak(2013)Feldman and Domshlak]{feldman2013monte}
Feldman, Z. and Domshlak, C.
\newblock {M}onte-{C}arlo planning: {T}heoretically fast convergence meets
  practical efficiency.
\newblock In \emph{Proceedings of the Conference on Uncertainty in Artificial
  Intelligence}, 2013.

\bibitem[Feldman \& Domshlak(2014{\natexlab{a}})Feldman and
  Domshlak]{feldman2014mabs}
Feldman, Z. and Domshlak, C.
\newblock On {MAB}s and separation of concerns in {M}onte-{C}arlo planning for
  {MDP}s.
\newblock In \emph{Proceedings of the International Conference on Automated
  Planning and Scheduling}, 2014{\natexlab{a}}.

\bibitem[Feldman \& Domshlak(2014{\natexlab{b}})Feldman and
  Domshlak]{feldman2014simple}
Feldman, Z. and Domshlak, C.
\newblock Simple regret optimization in online planning for {M}arkov decision
  processes.
\newblock \emph{Journal of Artificial Intelligence Research}, 51:\penalty0
  165--205, 2014{\natexlab{b}}.

\bibitem[Frobenius(1912)]{frobenius1912ueber}
Frobenius, G.
\newblock {\"U}ber {M}atrizen aus nicht negativen {E}lementen.
\newblock \emph{Sitzungsberichte Akad. Wiss. Berlin}, 1912.

\bibitem[Fujimoto et~al.(2021)Fujimoto, Meger, and Precup]{fujimoto2021deep}
Fujimoto, S., Meger, D., and Precup, D.
\newblock A deep reinforcement learning approach to marginalized importance
  sampling with the successor representation.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2021.

\bibitem[Grimm et~al.(2019)Grimm, Higgins, Barreto, Teplyashin, Wulfmeier,
  Hertweck, Hadsell, and Singh]{grimm2019disentagled}
Grimm, C., Higgins, I., Barreto, A., Teplyashin, D., Wulfmeier, M., Hertweck,
  T., Hadsell, R., and Singh, S.
\newblock Disentangled cumulants help successor representations transfer new
  tasks.
\newblock \emph{arXiv}, 2019.

\bibitem[Hansen et~al.(2020)Hansen, Dabney, Barreto, de~Wiele, Warde{-}Farley,
  and Mnih]{hansen2020fast}
Hansen, S., Dabney, W., Barreto, A., de~Wiele, T.~V., Warde{-}Farley, D., and
  Mnih, V.
\newblock Fast task inference with variational intrinsic successor features.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2020.

\bibitem[Harb et~al.(2018)Harb, Bacon, Klissarov, and Precup]{harb2018waiting}
Harb, J., Bacon, P.-L., Klissarov, M., and Precup, D.
\newblock When waiting is not an option: Learning options with a deliberation
  cost.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2018.

\bibitem[Harris et~al.(2020)Harris, Millman, van~der Walt, Gommers, Virtanen,
  Cournapeau, Wieser, Taylor, Berg, Smith, Kern, Picus, Hoyer, van Kerkwijk,
  Brett, Haldane, del R{\'{i}}o, Wiebe, Peterson, G{\'{e}}rard-Marchant,
  Sheppard, Reddy, Weckesser, Abbasi, Gohlke, and Oliphant]{harris2020array}
Harris, C.~R., Millman, K.~J., van~der Walt, S.~J., Gommers, R., Virtanen, P.,
  Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N.~J., Kern, R.,
  Picus, M., Hoyer, S., van Kerkwijk, M.~H., Brett, M., Haldane, A., del
  R{\'{i}}o, J.~F., Wiebe, M., Peterson, P., G{\'{e}}rard-Marchant, P.,
  Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant,
  T.~E.
\newblock Array programming with {NumPy}.
\newblock \emph{Nature}, 585\penalty0 (7825):\penalty0 357--362, September
  2020.

\bibitem[Harutyunyan et~al.(2019)Harutyunyan, Dabney, Borsa, Heess, Munos, and
  Precup]{harutyunyan2019termination}
Harutyunyan, A., Dabney, W., Borsa, D., Heess, N., Munos, R., and Precup, D.
\newblock The termination critic.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, 2019.

\bibitem[Higgins et~al.(2016)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{higgins2016beta}
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M.,
  Mohamed, S., and Lerchner, A.
\newblock beta-{VAE}: Learning basic visual concepts with a constrained
  variational framework.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2016.

\bibitem[Hunt et~al.(2019)Hunt, Barreto, Lillicrap, and
  Heess]{hunt2019composing}
Hunt, J., Barreto, A., Lillicrap, T., and Heess, N.
\newblock Composing entropic policies using divergence correction.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[Hunter(2007)]{hunter2007matplotlib}
Hunter, J.~D.
\newblock Matplotlib: A 2d graphics environment.
\newblock \emph{Computing in Science \& Engineering}, 9\penalty0 (3):\penalty0
  90--95, 2007.

\bibitem[Janner et~al.(2020)Janner, Mordatch, and Levine]{janner2020gamma}
Janner, M., Mordatch, I., and Levine, S.
\newblock Gamma-models: Generative temporal difference learning for
  infinite-horizon prediction.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Kaelbling(1993)]{kaelbling1993learning}
Kaelbling, L.~P.
\newblock Learning to achieve goals.
\newblock In \emph{Proceedings of the International Joint Conference on
  Artificial Intelligence}, 1993.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations}, 2015.

\bibitem[Kingma \& Dhariwal(2018)Kingma and Dhariwal]{kingma2018glow}
Kingma, D.~P. and Dhariwal, P.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{kingma2014auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational {B}ayes.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2014.

\bibitem[Kulkarni et~al.(2016{\natexlab{a}})Kulkarni, Narasimhan, Saeedi, and
  Tenenbaum]{kulkarni2016hierarchical}
Kulkarni, T.~D., Narasimhan, K., Saeedi, A., and Tenenbaum, J.
\newblock Hierarchical deep reinforcement learning: Integrating temporal
  abstraction and intrinsic motivation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2016{\natexlab{a}}.

\bibitem[Kulkarni et~al.(2016{\natexlab{b}})Kulkarni, Saeedi, Gautam, and
  Gershman]{kulkarni2016deep}
Kulkarni, T.~D., Saeedi, A., Gautam, S., and Gershman, S.~J.
\newblock Deep successor reinforcement learning.
\newblock \emph{arXiv}, 2016{\natexlab{b}}.

\bibitem[Kushner \& Yin(2003)Kushner and Yin]{kushner2003stochastic}
Kushner, H. and Yin, G.~G.
\newblock \emph{Stochastic Approximation and Recursive Algorithms and
  Applications}.
\newblock Springer, 2003.

\bibitem[Lehnert \& Littman(2020)Lehnert and Littman]{lehnert2020successor}
Lehnert, L. and Littman, M.~L.
\newblock Successor features combine elements of model-free and model-based
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0
  196:1--196:53, 2020.

\bibitem[Lesner \& Scherrer(2015)Lesner and Scherrer]{lesner2015non}
Lesner, B. and Scherrer, B.
\newblock Non-stationary approximate modified policy iteration.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2015.

\bibitem[Machado et~al.(2017)Machado, Bellemare, and
  Bowling]{machado2017laplacian}
Machado, M.~C., Bellemare, M.~G., and Bowling, M.
\newblock A {L}aplacian framework for option discovery in reinforcement
  learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2017.

\bibitem[McGovern \& Barto(2001)McGovern and Barto]{mcgovern2001automatic}
McGovern, A. and Barto, A.~G.
\newblock Automatic discovery of subgoals in reinforcement learning using
  diverse density.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2001.

\bibitem[Menache et~al.(2002)Menache, Mannor, and Shimkin]{menache2002q}
Menache, I., Mannor, S., and Shimkin, N.
\newblock Q-cut — dynamic discovery of sub-goals in reinforcement learning.
\newblock In \emph{Proceedings of the European Conference on Machine Learning},
  2002.

\bibitem[Meyn(2022)]{meyn2022control}
Meyn, S.
\newblock \emph{Control Systems and Reinforcement Learning}.
\newblock Cambridge University Press, 2022.

\bibitem[Munos(2014)]{munos2014bandits}
Munos, R.
\newblock From bandits to {M}onte-{C}arlo tree search: The optimistic principle
  applied to optimization and planning.
\newblock \emph{Foundations and Trends® in Machine Learning}, 7\penalty0
  (1):\penalty0 1--129, 2014.

\bibitem[Nemecek \& Parr(2021)Nemecek and Parr]{nemecek2021policy}
Nemecek, M. and Parr, R.
\newblock Policy caches with successor features.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2021.

\bibitem[Norris(1998)]{norris1998markov}
Norris, J.~R.
\newblock \emph{Markov Chains}.
\newblock Cambridge University Press, 1998.

\bibitem[Osband et~al.(2013)Osband, Russo, and Van~Roy]{osband2013more}
Osband, I., Russo, D., and Van~Roy, B.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2013.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and
  Van~Roy]{osband2016deep}
Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B.
\newblock Deep exploration via bootstrapped {DQN}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Perron(1907)]{perron1907theorie}
Perron, O.
\newblock Zur {T}heorie der {M}atrices.
\newblock \emph{Mathematische Annalen}, 64\penalty0 (2):\penalty0 248--263,
  1907.

\bibitem[Pong et~al.(2018)Pong, Gu, Dalal, and Levine]{pong2018temporal}
Pong, V., Gu, S., Dalal, M., and Levine, S.
\newblock Temporal difference models: Model-free deep {RL} for model-based
  control.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2018.

\bibitem[Precup(2000)]{precup2000temporal}
Precup, D.
\newblock \emph{Temporal abstraction in reinforcement learning}.
\newblock PhD thesis, University of Massachusetts Amherst, 2000.

\bibitem[Precup et~al.(1998{\natexlab{a}})Precup, Sutton, and
  Singh]{precup1998multi}
Precup, D., Sutton, R.~S., and Singh, S.
\newblock Multi-time models for temporally abstract planning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  1998{\natexlab{a}}.

\bibitem[Precup et~al.(1998{\natexlab{b}})Precup, Sutton, and
  Singh]{precup1998theoretical}
Precup, D., Sutton, R.~S., and Singh, S.
\newblock Theoretical results on reinforcement learning with temporally
  abstract options.
\newblock In \emph{Proceedings of the European Conference on Machine Learning},
  1998{\natexlab{b}}.

\bibitem[Precup et~al.(2000)Precup, Sutton, and Singh]{precup2000eligibility}
Precup, D., Sutton, R.~S., and Singh, S.~P.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2000.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: Discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Rezende \& Mohamed(2015)Rezende and Mohamed]{rezende2015variational}
Rezende, D. and Mohamed, S.
\newblock Variational inference with normalizing flows.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2015.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2014.

\bibitem[Robbins \& Siegmund(1971)Robbins and Siegmund]{robbins1971convergence}
Robbins, H. and Siegmund, D.
\newblock A convergence theorem for non negative almost supermartingales and
  some applications.
\newblock In \emph{Optimizing methods in statistics}, pp.\  233--257. Elsevier,
  1971.

\bibitem[Russo et~al.(2018)Russo, Van~Roy, Kazerouni, Osband, Wen,
  et~al.]{russo2018tutorial}
Russo, D.~J., Van~Roy, B., Kazerouni, A., Osband, I., Wen, Z., et~al.
\newblock A tutorial on {T}hompson sampling.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  11\penalty0 (1):\penalty0 1--96, 2018.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor, and
  Silver]{schaul2015universal}
Schaul, T., Horgan, D., Gregor, K., and Silver, D.
\newblock Universal value function approximators.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2015.

\bibitem[Scherrer \& Lesner(2012)Scherrer and Lesner]{scherrer2012use}
Scherrer, B. and Lesner, B.
\newblock On the use of non-stationary policies for stationary infinite-horizon
  {M}arkov decision processes.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2012.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{schulman2015high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2016.

\bibitem[Seneta(2006)]{seneta2006non}
Seneta, E.
\newblock \emph{Non-Negative Matrices and {M}arkov Chains}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Silver \& Ciosek(2012)Silver and Ciosek]{silver2012compositional}
Silver, D. and Ciosek, K.
\newblock Compositional planning using optimal option models.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2012.

\bibitem[{\c{S}}im{\c{s}}ek \& Barto(2004){\c{S}}im{\c{s}}ek and
  Barto]{csimcsek2004using}
{\c{S}}im{\c{s}}ek, {\"O}. and Barto, A.~G.
\newblock Using relative novelty to identify useful temporal abstractions in
  reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2004.

\bibitem[Sohn et~al.(2015)Sohn, Lee, and Yan]{sohn2015learning}
Sohn, K., Lee, H., and Yan, X.
\newblock Learning structured output representation using deep conditional
  generative models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Strens(2000)]{strens2000bayesian}
Strens, M.
\newblock A {B}ayesian framework for reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2000.

\bibitem[Sutton(1995)]{sutton1995td}
Sutton, R.~S.
\newblock {TD} models: Modeling the world at a mixture of time scales.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 1995.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT Press, 2018.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton1999between}
Sutton, R.~S., Precup, D., and Singh, S.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artificial intelligence}, 112\penalty0 (1-2):\penalty0
  181--211, 1999.

\bibitem[Szepesv{\'a}ri(2010)]{szepesvari2010algorithms}
Szepesv{\'a}ri, {\relax Cs}.
\newblock \emph{Algorithms for Reinforcement Learning}.
\newblock Morgan \& Claypool, 2010.

\bibitem[Sz{\"o}r{\'e}nyi et~al.(2014)Sz{\"o}r{\'e}nyi, Kedenburg, and
  Munos]{szorenyi2014optimistic}
Sz{\"o}r{\'e}nyi, B., Kedenburg, G., and Munos, R.
\newblock Optimistic planning in {M}arkov decision processes using a generative
  model.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock {MuJoCo}: A physics engine for model-based control.
\newblock In \emph{Proceedings of the IEEE International Conference on
  Intelligent Robots and Systems}, 2012.

\bibitem[Tomar et~al.(2020)Tomar, Efroni, and Ghavamzadeh]{tomar2020multi}
Tomar, M., Efroni, Y., and Ghavamzadeh, M.
\newblock Multi-step greedy reinforcement learning algorithms.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[Touati \& Ollivier(2021)Touati and Ollivier]{touati2021learning}
Touati, A. and Ollivier, Y.
\newblock Learning one representation to optimize all rewards.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Toussaint \& Storkey(2005)Toussaint and Storkey]{toussaint2005}
Toussaint, M. and Storkey, A.
\newblock Probabilistic inference for computing optimal policies in {MDP}s.
\newblock In \emph{{NIPS} Workshop on Game Theory, Machine Learning and
  Reasoning under Uncertainty}, 2005.

\bibitem[Toussaint \& Storkey(2006)Toussaint and
  Storkey]{toussaint2006probabilistic}
Toussaint, M. and Storkey, A.
\newblock Probabilistic inference for solving discrete and continuous state
  {M}arkov decision processes.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2006.

\bibitem[Wulfmeier et~al.(2021)Wulfmeier, Rao, Hafner, Lampe, Abdolmaleki,
  Hertweck, Neunert, Tirumala, Siegel, Heess, and
  Riedmiller]{wulfmeier2021data}
Wulfmeier, M., Rao, D., Hafner, R., Lampe, T., Abdolmaleki, A., Hertweck, T.,
  Neunert, M., Tirumala, D., Siegel, N., Heess, N., and Riedmiller, M.
\newblock Data-efficient hindsight off-policy option learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2021.

\bibitem[Yao et~al.(2014)Yao, Szepesv{\'a}ri, Sutton, Modayil, and
  Bhatnagar]{yao2014universal}
Yao, H., Szepesv{\'a}ri, {\relax Cs}., Sutton, R.~S., Modayil, J., and
  Bhatnagar, S.
\newblock Universal option models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Zahavy et~al.(2021)Zahavy, Barreto, Mankowitz, Hou, O'Donoghue,
  Kemaev, and Singh]{zahavy2021discovering}
Zahavy, T., Barreto, A., Mankowitz, D.~J., Hou, S., O'Donoghue, B., Kemaev, I.,
  and Singh, S.
\newblock Discovering a set of policies for the worst case reward.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2021.

\end{thebibliography}
