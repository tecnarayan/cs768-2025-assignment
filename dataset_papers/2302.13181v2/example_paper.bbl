\begin{thebibliography}{17}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bai et~al.(2021)Bai, Lin, Raffel, and Kan]{CHCW21}
Bai, C., Lin, H., Raffel, C., and Kan, W.~C.
\newblock On training sample memorization: Lessons from benchmarking generative
  modeling with a large-scale competition.
\newblock In Zhu, F., Ooi, B.~C., and Miao, C. (eds.), \emph{{KDD} '21: The
  27th {ACM} {SIGKDD} Conference on Knowledge Discovery and Data Mining,
  Virtual Event, Singapore, August 14-18, 2021}, pp.\  2534--2542. {ACM}, 2021.
\newblock \doi{10.1145/3447548.3467198}.
\newblock URL \url{https://doi.org/10.1145/3447548.3467198}.

\bibitem[Block et~al.(2022)Block, Jia, Polyanskiy, and Rakhlin]{BJPR22}
Block, A., Jia, Z., Polyanskiy, Y., and Rakhlin, A.
\newblock Intrinsic dimension estimation using wasserstein distances.
\newblock \emph{Journal of machine learning research}, 1533-7928, 2022.

\bibitem[Brown et~al.(2021)Brown, Bun, Feldman, Smith, and Talwar]{BBFST21}
Brown, G., Bun, M., Feldman, V., Smith, A., and Talwar, K.
\newblock When is memorization of irrelevant training data necessary for
  high-accuracy learning?
\newblock In \emph{Proceedings of the 53rd Annual {ACM} {SIGACT} Symposium on
  Theory of Computing}. {ACM}, jun 2021.
\newblock \doi{10.1145/3406325.3451131}.
\newblock URL \url{https://doi.org/10.1145%2F3406325.3451131}.

\bibitem[Carlini et~al.(2019)Carlini, Liu, Erlingsson, Kos, and
  Song]{Carlini19}
Carlini, N., Liu, C., Erlingsson, {\'{U}}., Kos, J., and Song, D.
\newblock The secret sharer: Evaluating and testing unintended memorization in
  neural networks.
\newblock In Heninger, N. and Traynor, P. (eds.), \emph{28th {USENIX} Security
  Symposium, {USENIX} Security 2019, Santa Clara, CA, USA, August 14-16, 2019},
  pp.\  267--284. {USENIX} Association, 2019.

\bibitem[Carlini et~al.(2022)Carlini, Ippolito, Jagielski, Lee, Tram{\`{e}}r,
  and Zhang]{Carlini22}
Carlini, N., Ippolito, D., Jagielski, M., Lee, K., Tram{\`{e}}r, F., and Zhang,
  C.
\newblock Quantifying memorization across neural language models.
\newblock \emph{CoRR}, abs/2202.07646, 2022.
\newblock URL \url{https://arxiv.org/abs/2202.07646}.

\bibitem[Chatterjee(2018)]{C18}
Chatterjee, S.
\newblock Learning and memorization.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  755--763. PMLR, 10--15 Jul 2018.

\bibitem[Dwork(2006)]{Dwork06}
Dwork, C.
\newblock Differential privacy.
\newblock In Bugliesi, M., Preneel, B., Sassone, V., and Wegener, I. (eds.),
  \emph{Automata, Languages and Programming, 33rd International Colloquium,
  {ICALP} 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part {II}},
  volume 4052 of \emph{Lecture Notes in Computer Science}, pp.\  1--12.
  Springer, 2006.

\bibitem[Feldman(2020)]{Feldman20}
Feldman, V.
\newblock Does learning require memorization? a short tale about a long tail.
\newblock In Makarychev, K., Makarychev, Y., Tulsiani, M., Kamath, G., and
  Chuzhoy, J. (eds.), \emph{Proccedings of the 52nd Annual {ACM} {SIGACT}
  Symposium on Theory of Computing, {STOC} 2020, Chicago, IL, USA, June 22-26,
  2020}, pp.\  954--959. {ACM}, 2020.
\newblock \doi{10.1145/3357713.3384290}.
\newblock URL \url{https://doi.org/10.1145/3357713.3384290}.

\bibitem[Lopez-Paz \& Oquab(2016)Lopez-Paz and Oquab]{lopez2016revisiting}
Lopez-Paz, D. and Oquab, M.
\newblock Revisiting classifier two-sample tests.
\newblock \emph{arXiv preprint arXiv:1610.06545}, 2016.

\bibitem[Meehan et~al.(2020)Meehan, Chaudhuri, and Dasgupta]{MCD2020}
Meehan, C., Chaudhuri, K., and Dasgupta, S.
\newblock A three sample hypothesis test for evaluating generative models.
\newblock In Chiappa, S. and Calandra, R. (eds.), \emph{The 23rd International
  Conference on Artificial Intelligence and Statistics, {AISTATS} 2020, 26-28
  August 2020, Online [Palermo, Sicily, Italy]}, volume 108 of
  \emph{Proceedings of Machine Learning Research}, pp.\  3546--3556. PMLR,
  2020.

\bibitem[Richardson \& Weiss(2018)Richardson and Weiss]{RW18}
Richardson, E. and Weiss, Y.
\newblock On gans and gmms.
\newblock In Bengio, S., Wallach, H.~M., Larochelle, H., Grauman, K.,
  Cesa{-}Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31: Annual Conference on Neural Information
  Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al,
  Canada}, pp.\  5852--5863, 2018.

\bibitem[Sajjadi et~al.(2018)Sajjadi, Bachem, Lucic, Bousquet, and
  Gelly]{SBLBG18}
Sajjadi, M. S.~M., Bachem, O., Lucic, M., Bousquet, O., and Gelly, S.
\newblock Assessing generative models via precision and recall.
\newblock In Bengio, S., Wallach, H.~M., Larochelle, H., Grauman, K.,
  Cesa{-}Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31: Annual Conference on Neural Information
  Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al,
  Canada}, pp.\  5234--5243, 2018.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{SGZCRC16}
Salimans, T., Goodfellow, I.~J., Zaremba, W., Cheung, V., Radford, A., and
  Chen, X.
\newblock Improved techniques for training gans.
\newblock \emph{CoRR}, abs/1606.03498, 2016.
\newblock URL \url{http://arxiv.org/abs/1606.03498}.

\bibitem[Thanh{-}Tung \& Tran(2020)Thanh{-}Tung and Tran]{TT20}
Thanh{-}Tung, H. and Tran, T.
\newblock Catastrophic forgetting and mode collapse in gans.
\newblock In \emph{2020 International Joint Conference on Neural Networks,
  {IJCNN} 2020, Glasgow, United Kingdom, July 19-24, 2020}, pp.\  1--10.
  {IEEE}, 2020.
\newblock \doi{10.1109/IJCNN48605.2020.9207181}.
\newblock URL \url{https://doi.org/10.1109/IJCNN48605.2020.9207181}.

\bibitem[van~den Burg \& Williams(2021)van~den Burg and Williams]{BGWC21}
van~den Burg, G. and Williams, C.
\newblock On memorization in probabilistic deep generative models.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
  J.~W. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  27916--27928. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf}.

\bibitem[Xu et~al.(2018)Xu, Huang, Yuan, Guo, Sun, Wu, and
  Weinberger]{XHYGSWK18}
Xu, Q., Huang, G., Yuan, Y., Guo, C., Sun, Y., Wu, F., and Weinberger, K.~Q.
\newblock An empirical study on evaluation metrics of generative adversarial
  networks.
\newblock \emph{CoRR}, abs/1806.07755, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.07755}.

\bibitem[Yazici et~al.(2020)Yazici, Foo, Winkler, Yap, and
  Chandrasekhar]{YFWYC20}
Yazici, Y., Foo, C., Winkler, S., Yap, K., and Chandrasekhar, V.
\newblock Empirical analysis of overfitting and mode drop in gan training.
\newblock In \emph{{IEEE} International Conference on Image Processing, {ICIP}
  2020, Abu Dhabi, United Arab Emirates, October 25-28, 2020}, pp.\
  1651--1655. {IEEE}, 2020.
\newblock \doi{10.1109/ICIP40778.2020.9191083}.
\newblock URL \url{https://doi.org/10.1109/ICIP40778.2020.9191083}.

\end{thebibliography}
