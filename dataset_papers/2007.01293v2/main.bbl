\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal and Ganichev(2019)]{agarwal2019auto}
A.~Agarwal and I.~Ganichev.
\newblock Auto-vectorizing tensorflow graphs: Jacobians, auto-batching and
  beyond.
\newblock \emph{arXiv:1903.04243}, 2019.

\bibitem[Agarwal et~al.(2017)Agarwal, Bullins, and Hazan]{agarwal2017second}
N.~Agarwal, B.~Bullins, and E.~Hazan.
\newblock Second-order stochastic optimization for machine learning in linear
  time.
\newblock \emph{JMLR}, 2017.

\bibitem[Bengio(2000)]{bengio2000gradient}
Y.~Bengio.
\newblock Gradient-based optimization of hyperparameters.
\newblock \emph{Neural Computation}, 2000.

\bibitem[Berthelot et~al.(2019)Berthelot, Carlini, Goodfellow, Papernot,
  Oliver, and Raffel]{berthelot2019mixmatch}
D.~Berthelot, N.~Carlini, I.~Goodfellow, N.~Papernot, A.~Oliver, and C.~Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock In \emph{Proc. NeurIPS}, 2019.

\bibitem[Berthelot et~al.(2020)Berthelot, Carlini, Cubuk, Kurakin, Sohn, Zhang,
  and Raffel]{berthelot2020remixmatch}
D.~Berthelot, N.~Carlini, E.~D. Cubuk, A.~Kurakin, K.~Sohn, H.~Zhang, and
  C.~Raffel.
\newblock Remixmatch: Semi-supervised learning with distribution matching and
  augmentation anchoring.
\newblock In \emph{Proc. ICLR}, 2020.

\bibitem[Chapelle et~al.(2006)Chapelle, Sch\"{o}lkopf, and
  Zien]{chapelle2006semi}
O.~Chapelle, B.~Sch\"{o}lkopf, and A.~Zien.
\newblock \emph{Semi-supervised learning}.
\newblock MIT Press, 2006.

\bibitem[Cook and Weisberg(1980)]{cook1980characterizations}
R.~D. Cook and S.~Weisberg.
\newblock Characterizations of an empirical influence function for detecting
  influential cases in regression.
\newblock \emph{Technometrics}, 1980.

\bibitem[Couellan and Wang(2016)]{couellan2016convergence}
N.~Couellan and W.~Wang.
\newblock On the convergence of stochastic bi-level gradient methods.
\newblock \emph{Optimization}, 2016.

\bibitem[Dai and Le(2015)]{Dai_NIPS2015}
A.~M. Dai and Q.~V. Le.
\newblock Semi-supervised sequence learning.
\newblock In \emph{Proc. NeurIPS}, 2015.

\bibitem[Danskin(1967)]{danskin1967theory}
J.~Danskin.
\newblock \emph{The Theory of Max-min and Its Applications to Weapons
  Allocation Problems}.
\newblock Springer, 1967.

\bibitem[Dempster et~al.(1977)Dempster, Laird, and Rubin]{Dempster1977}
A.~P. Dempster, N.~M. Laird, and D.~B. Rubin.
\newblock {Maximum likelihood from incomplete data via the {EM} algorithm}.
\newblock In \emph{J. Roy. Statist. Soc.}, 1977.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proc. NAACL-HLT}, 2019.

\bibitem[Franceschi et~al.(2018)Franceschi, Frasconi, Salzo, Grazzi, and
  Pontil]{franceschi2018bilevel}
L.~Franceschi, P.~Frasconi, S.~Salzo, R.~Grazzi, and M.~Pontil.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock In \emph{Proc. ICML}, 2018.

\bibitem[Koh and Liang(2017)]{koh2017understanding}
P.~W. Koh and P.~Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{Proc. ICML}, 2017.

\bibitem[Koh et~al.(2019)Koh, Ang, Teo, and Liang]{koh2019accuracy}
P.~W.~W. Koh, K.-S. Ang, H.~Teo, and P.~S. Liang.
\newblock On the accuracy of influence functions for measuring group effects.
\newblock In \emph{Proc. NeurIPS}, 2019.

\bibitem[Krishnan et~al.(1997)Krishnan, Ng, Ng, Krishnan, and
  Mclachlan]{krishnan1997algorithm}
G.~J. Krishnan, T.~Ng, S.~Ng, T.~Krishnan, and G.~Mclachlan.
\newblock The {EM} algorithm.
\newblock In \emph{Wiley Series in Probability and Statistics: Applied
  Probability and Statistics, WileyInterscience}, 1997.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[{Larsen} et~al.(1996){Larsen}, {Hansen}, {Svarer}, and
  {Ohlsson}]{Larsen}
J.~{Larsen}, L.~K. {Hansen}, C.~{Svarer}, and M.~{Ohlsson}.
\newblock Design and regularization of neural networks: the optimal use of a
  validation set.
\newblock In \emph{IEEE Signal Processing Society Workshop}, 1996.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
Y.~LeCun, Y.~Bengio, and G.~Hinton.
\newblock Deep learning.
\newblock \emph{Nature}, 2015.

\bibitem[Lee(2013)]{Lee2013pseudo}
D.-H. Lee.
\newblock Pseudo-label : The simple and efficient semi-supervised learning
  method for deep neural networks.
\newblock In \emph{ICML Workshop : Challenges in Representation Learning
  (WREPL)}, 2013.

\bibitem[Lorraine et~al.(2020)Lorraine, Vicol, and Duvenaud]{lorraine2019opt}
J.~Lorraine, P.~Vicol, and D.~Duvenaud.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock In \emph{Proc. AISTATS}, 2020.

\bibitem[Luketina et~al.(2016)Luketina, Berglund, Greff, and
  Raiko]{luketina2016scalable}
J.~Luketina, M.~Berglund, K.~Greff, and T.~Raiko.
\newblock Scalable gradient-based tuning of continuous regularization
  hyperparameters.
\newblock In \emph{Proc. ICML}, 2016.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and Potts]{maas-EtAl}
A.~L. Maas, R.~E. Daly, P.~T. Pham, D.~Huang, A.~Y. Ng, and C.~Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proc. ACL}, 2011.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{maclaurin2015gradient}
D.~Maclaurin, D.~Duvenaud, and R.~Adams.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{Proc. ICML}, 2015.

\bibitem[Martens(2010)]{Martens}
J.~Martens.
\newblock Deep learning via hessian-free optimization.
\newblock In \emph{Proc. ICML}, 2010.

\bibitem[McLachlan and Ganesalingam(1982)]{mclachlan1982updating}
G.~J. McLachlan and S.~Ganesalingam.
\newblock Updating a discriminant function on the basis of unclassified data.
\newblock \emph{Comm. Statist. Simulation Comput.}, 1982.

\bibitem[Miyato et~al.(2019)Miyato, Maeda, Koyama, and Ishii]{Miyato2018vat}
T.~Miyato, S.~Maeda, M.~Koyama, and S.~Ishii.
\newblock Virtual adversarial training: {A} regularization method for
  supervised and semi-supervised learning.
\newblock \emph{PAMI}, 2019.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NIPS Workshop: Deep Learning and Unsupervised Feature
  Learning}, 2011.

\bibitem[Oliver et~al.(2018)Oliver, Odena, Raffel, Cubuk, and
  Goodfellow]{oliver2019benchmark}
A.~Oliver, A.~Odena, C.~A. Raffel, E.~D. Cubuk, and I.~Goodfellow.
\newblock Realistic evaluation of deep semi-supervised learning algorithms.
\newblock In \emph{Proc. NeurIPS}, 2018.

\bibitem[Panayotov et~al.(2015)Panayotov, Chen, Povey, and
  Khudanpur]{panayotov2015librispeech}
V.~Panayotov, G.~Chen, D.~Povey, and S.~Khudanpur.
\newblock Librispeech: an asr corpus based on public domain audio books.
\newblock In \emph{Proc. ICASSP}, 2015.

\bibitem[Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun]{ren18l2rw}
M.~Ren, W.~Zeng, B.~Yang, and R.~Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In \emph{Proc. ICML}, 2018.

\bibitem[Ren et~al.(2020{\natexlab{a}})Ren, Yu, Yang, Liu, Lee, Schwing, and
  Kautz]{ren-cvpr2020}
Z.~Ren, Z.~Yu, X.~Yang, M.-Y. Liu, Y.~J. Lee, A.~G. Schwing, and J.~Kautz.
\newblock Instance-aware, context-focused, and memory-efficient weakly
  supervised object detection.
\newblock In \emph{Proc. CVPR}, 2020{\natexlab{a}}.

\bibitem[Ren et~al.(2020{\natexlab{b}})Ren, Yu, Yang, Liu, Schwing, and
  Kautz]{ren-eccv2020}
Z.~Ren, Z.~Yu, X.~Yang, M.-Y. Liu, A.~G. Schwing, and J.~Kautz.
\newblock {UFO}$^2$: A unified framework towards omni-supervised object
  detection.
\newblock In \emph{Proc. ECCV}, 2020{\natexlab{b}}.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{rumelhart1986learning}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Nature}, 1986.

\bibitem[Sachan et~al.(2019)Sachan, Zaheer, and Salakhutdinov]{SachanZS19}
D.~S. Sachan, M.~Zaheer, and R.~Salakhutdinov.
\newblock Revisiting {LSTM} networks for semi-supervised text classification
  via mixed objective function.
\newblock In \emph{Proc. AAAI}, 2019.

\bibitem[Schwing et~al.(2012)Schwing, Hazan, Pollefeys, and
  Urtasun]{SchwingICML2012}
A.~G. Schwing, T.~Hazan, M.~Pollefeys, and R.~Urtasun.
\newblock {Efficient Structured Prediction with Latent Variables for General
  Graphical Models}.
\newblock In \emph{Proc. ICML}, 2012.

\bibitem[Shaban et~al.(2018)Shaban, Cheng, Hatch, and Boots]{Shaban}
A.~Shaban, C.~Cheng, N.~Hatch, and B.~Boots.
\newblock Truncated back-propagation for bilevel optimization.
\newblock In \emph{Proc. AISTATS}, 2018.

\bibitem[Shahshahani and Landgrebe(1994)]{shahshahani1994effect}
B.~M. Shahshahani and D.~A. Landgrebe.
\newblock The effect of unlabeled samples in reducing the small sample size
  problem and mitigating the hughes phenomenon.
\newblock \emph{TGRS}, 1994.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Li, Zhang, Carlini, Cubuk, Kurakin,
  Zhang, and Raffel]{fixmatch}
K.~Sohn, D.~Berthelot, C.~Li, Z.~Zhang, N.~Carlini, E.~D. Cubuk, A.~Kurakin,
  H.~Zhang, and C.~Raffel.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock In \emph{Proc. NeurIPS}, 2020.

\bibitem[Tarvainen and Valpola(2017)]{Tarvainen2017meanteacher}
A.~Tarvainen and H.~Valpola.
\newblock Weight-averaged consistency targets improve semi-supervised deep
  learning results.
\newblock In \emph{Proc. NeurIPS}, 2017.

\bibitem[Xie et~al.(2020)Xie, Dai, Hovy, Luong, and Le]{xie2019uda}
Q.~Xie, Z.~Dai, E.~Hovy, M.-T. Luong, and Q.~V. Le.
\newblock Unsupervised data augmentation for consistency training.
\newblock In \emph{Proc. NeurIPS}, 2020.

\bibitem[Xu et~al.(2014)Xu, Schwing, and Urtasun]{XuCVPR2014}
J.~Xu, A.~G. Schwing, and R.~Urtasun.
\newblock {Tell Me What You See and I will Show You Where It Is}.
\newblock In \emph{Proc. CVPR}, 2014.

\bibitem[Zagoruyko and Komodakis(2016)]{Zagoruyko2016WRN}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock In \emph{Proc. BMVC}, 2016.

\end{thebibliography}
