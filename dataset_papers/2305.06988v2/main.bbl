\begin{thebibliography}{92}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
J.-B. Alayrac, J.~Donahue, P.~Luc, A.~Miech, I.~Barr, Y.~Hasson, K.~Lenc,
  A.~Mensch, K.~Millican, M.~Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 23716--23736, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in neural information processing systems},
  volume~33, pages 1877--1901, 2020.

\bibitem[Buch et~al.(2022)Buch, Eyzaguirre, Gaidon, Wu, Fei-Fei, and
  Niebles]{buch2022revisiting}
S.~Buch, C.~Eyzaguirre, A.~Gaidon, J.~Wu, L.~Fei-Fei, and J.~C. Niebles.
\newblock Revisiting the" video" in video-language understanding.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 2917--2927, 2022.

\bibitem[Cao et~al.(2022)Cao, Yang, Weng, Zhang, Wang, and Zou]{cao2022locvtp}
M.~Cao, T.~Yang, J.~Weng, C.~Zhang, J.~Wang, and Y.~Zou.
\newblock Locvtp: Video-text pre-training for temporal localization.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXVI}, pages 38--56.
  Springer, 2022.

\bibitem[Chen et~al.(2020)Chen, Li, Yu, Kholy, Ahmed, Gan, Cheng, and
  Liu]{chen2019uniter}
Y.-C. Chen, L.~Li, L.~Yu, A.~E. Kholy, F.~Ahmed, Z.~Gan, Y.~Cheng, and J.~Liu.
\newblock Uniter: Universal image-text representation learning.
\newblock In \emph{ECCV}, 2020.

\bibitem[Cho et~al.(2021)Cho, Lei, Tan, and Bansal]{cho2021unifying}
J.~Cho, J.~Lei, H.~Tan, and M.~Bansal.
\newblock Unifying vision-and-language tasks via text generation.
\newblock In \emph{International Conference on Machine Learning}, pages
  1931--1942. PMLR, 2021.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, et~al.]{chung2022scaling}
H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, E.~Li, X.~Wang,
  M.~Dehghani, S.~Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Cordonnier et~al.(2021)Cordonnier, Mahendran, Dosovitskiy,
  Weissenborn, Uszkoreit, and Unterthiner]{cordonnier2021differentiable}
J.-B. Cordonnier, A.~Mahendran, A.~Dosovitskiy, D.~Weissenborn, J.~Uszkoreit,
  and T.~Unterthiner.
\newblock Differentiable patch selection for image recognition.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 2351--2360, 2021.

\bibitem[Cui et~al.(2022)Cui, Qian, Peng, Daskalaki, Chen, Guo, Sun, and
  Jiang]{cui2022video}
R.~Cui, T.~Qian, P.~Peng, E.~Daskalaki, J.~Chen, X.~Guo, H.~Sun, and Y.-G.
  Jiang.
\newblock Video moment retrieval from text queries via single frame annotation.
\newblock In \emph{Proceedings of the 45th International ACM SIGIR Conference
  on Research and Development in Information Retrieval}, pages 1033--1043,
  2022.

\bibitem[Deng et~al.(2023)Deng, Yang, and Chen]{deng2023large}
A.~Deng, T.~Yang, and C.~Chen.
\newblock A large-scale study of spatiotemporal representation learning with a
  new benchmark on action recognition.
\newblock In \emph{ICCV}, 2023.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter,
  Wahid, Tompson, Vuong, Yu, et~al.]{driess2023palm}
D.~Driess, F.~Xia, M.~S. Sajjadi, C.~Lynch, A.~Chowdhery, B.~Ichter, A.~Wahid,
  J.~Tompson, Q.~Vuong, T.~Yu, et~al.
\newblock Palm-e: An embodied multimodal language model.
\newblock In \emph{ICML}, 2023.

\bibitem[Escorcia et~al.(2019)Escorcia, Soldan, Sivic, Ghanem, and
  Russell]{escorcia2019temporal}
V.~Escorcia, M.~Soldan, J.~Sivic, B.~Ghanem, and B.~Russell.
\newblock Temporal localization of moments in video collections with natural
  language, 2019.

\bibitem[Fang et~al.(2021)Fang, Xiong, Xu, and Chen]{fang2021clip2video}
H.~Fang, P.~Xiong, L.~Xu, and Y.~Chen.
\newblock Clip2video: Mastering video-text retrieval via image clip.
\newblock \emph{arXiv preprint arXiv:2106.11097}, 2021.

\bibitem[Fang et~al.(2022)Fang, Xiong, Xu, and Luo]{fang2022transferring}
H.~Fang, P.~Xiong, L.~Xu, and W.~Luo.
\newblock Transferring image-clip to video-text retrieval via temporal
  relations.
\newblock \emph{IEEE Transactions on Multimedia}, 2022.

\bibitem[Fang et~al.(2023)Fang, Wang, Xie, Sun, Wu, Wang, Huang, Wang, and
  Cao]{fang2022eva}
Y.~Fang, W.~Wang, B.~Xie, Q.~Sun, L.~Wu, X.~Wang, T.~Huang, X.~Wang, and
  Y.~Cao.
\newblock Eva: Exploring the limits of masked visual representation learning at
  scale.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 19358--19369, June 2023.

\bibitem[Fu* et~al.(2023)Fu*, Li*, Gan, Lin, Wang, Wang, and Liu]{fu2021violet}
T.-J. Fu*, L.~Li*, Z.~Gan, K.~Lin, W.~Y. Wang, L.~Wang, and Z.~Liu.
\newblock {An Empirical Study of End-to-End Video-Language Transformers with
  Masked Visual Modeling}.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2023.

\bibitem[Gao et~al.(2023)Gao, Zhou, Ji, Zhu, Yang, and Shou]{gao2022mist}
D.~Gao, L.~Zhou, L.~Ji, L.~Zhu, Y.~Yang, and M.~Z. Shou.
\newblock Mist: Multi-modal iterative spatial-temporal transformer for
  long-form video question answering.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 14773--14783, June 2023.

\bibitem[Gorti et~al.(2022)Gorti, Vouitsis, Ma, Golestan, Volkovs, Garg, and
  Yu]{gorti2022x}
S.~K. Gorti, N.~Vouitsis, J.~Ma, K.~Golestan, M.~Volkovs, A.~Garg, and G.~Yu.
\newblock X-pool: Cross-modal language-video attention for text-video
  retrieval.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 5006--5015, 2022.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
N.~Houlsby, A.~Giurgiu, S.~Jastrzebski, B.~Morrone, Q.~De~Laroussilhe,
  A.~Gesmundo, M.~Attariyan, and S.~Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International Conference on Machine Learning}, pages
  2790--2799. PMLR, 2019.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and
  Duerig]{jia2021scaling}
C.~Jia, Y.~Yang, Y.~Xia, Y.-T. Chen, Z.~Parekh, H.~Pham, Q.~Le, Y.-H. Sung,
  Z.~Li, and T.~Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In \emph{International Conference on Machine Learning}, pages
  4904--4916. PMLR, 2021.

\bibitem[Jia et~al.(2022)Jia, Tang, Chen, Cardie, Belongie, Hariharan, and
  Lim]{jia2022visual}
M.~Jia, L.~Tang, B.-C. Chen, C.~Cardie, S.~Belongie, B.~Hariharan, and S.-N.
  Lim.
\newblock Visual prompt tuning.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXIII}, pages
  709--727. Springer, 2022.

\bibitem[Ju et~al.(2022)Ju, Han, Zheng, Zhang, and Xie]{ju2022prompting}
C.~Ju, T.~Han, K.~Zheng, Y.~Zhang, and W.~Xie.
\newblock Prompting visual-language models for efficient video understanding.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXV}, pages 105--124.
  Springer, 2022.

\bibitem[Kim et~al.(2023)Kim, Kim, Lee, and Seo]{kim2023semi}
S.~Kim, J.-H. Kim, J.~Lee, and M.~Seo.
\newblock Semi-parametric video-grounded text generation.
\newblock \emph{arXiv preprint arXiv:2301.11507}, 2023.

\bibitem[Krishna et~al.(2017)Krishna, Zhu, Groth, Johnson, Hata, Kravitz, Chen,
  Kalantidis, Li, Shamma, et~al.]{krishna2017visual}
R.~Krishna, Y.~Zhu, O.~Groth, J.~Johnson, K.~Hata, J.~Kravitz, S.~Chen,
  Y.~Kalantidis, L.-J. Li, D.~A. Shamma, et~al.
\newblock Visual genome: Connecting language and vision using crowdsourced
  dense image annotations.
\newblock \emph{International journal of computer vision}, 123:\penalty0
  32--73, 2017.

\bibitem[Lee et~al.(2013)]{lee2013pseudo}
D.-H. Lee et~al.
\newblock Pseudo-label: The simple and efficient semi-supervised learning
  method for deep neural networks.
\newblock In \emph{Workshop on challenges in representation learning, ICML},
  page 896, 2013.

\bibitem[Lei et~al.(2018)Lei, Yu, Bansal, and Berg]{lei2018tvqa}
J.~Lei, L.~Yu, M.~Bansal, and T.~L. Berg.
\newblock Tvqa: Localized, compositional video question answering.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Lei et~al.(2020{\natexlab{a}})Lei, Yu, Berg, and Bansal]{lei2020more}
J.~Lei, L.~Yu, T.~L. Berg, and M.~Bansal.
\newblock What is more likely to happen next? video-and-language future event
  prediction.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 8769--8784. Association for
  Computational Linguistics, 2020{\natexlab{a}}.

\bibitem[Lei et~al.(2020{\natexlab{b}})Lei, Yu, Berg, and Bansal]{lei2020tvr}
J.~Lei, L.~Yu, T.~L. Berg, and M.~Bansal.
\newblock Tvr: A large-scale dataset for video-subtitle moment retrieval.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part XXI 16}, pages 447--463.
  Springer, 2020{\natexlab{b}}.

\bibitem[Lei et~al.(2021{\natexlab{a}})Lei, Berg, and Bansal]{lei2021detecting}
J.~Lei, T.~L. Berg, and M.~Bansal.
\newblock Detecting moments and highlights in videos via natural language
  queries.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 11846--11858, 2021{\natexlab{a}}.

\bibitem[Lei et~al.(2021{\natexlab{b}})Lei, Li, Zhou, Gan, Berg, Bansal, and
  Liu]{lei2021less}
J.~Lei, L.~Li, L.~Zhou, Z.~Gan, T.~L. Berg, M.~Bansal, and J.~Liu.
\newblock Less is more: Clipbert for video-and-language learning via sparse
  sampling.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 7331--7341, 2021{\natexlab{b}}.

\bibitem[Lei et~al.(2023)Lei, Berg, and Bansal]{lei2022revealing}
J.~Lei, T.~L. Berg, and M.~Bansal.
\newblock Revealing single frame bias for video-and-language learning.
\newblock \emph{Proceedings of the 61th Annual Meeting of the Association for
  Computational Linguistics}, 2023.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Li, Le, Wang, Savarese, and
  Hoi]{li2023lavis}
D.~Li, J.~Li, H.~Le, G.~Wang, S.~Savarese, and S.~C. Hoi.
\newblock Lavis: A one-stop library for language-vision intelligence.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 3: System Demonstrations)}, pages
  31--41, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Li, Xiong, and Hoi]{li2022blip}
J.~Li, D.~Li, C.~Xiong, and S.~Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified
  vision-language understanding and generation.
\newblock In \emph{International Conference on Machine Learning}, pages
  12888--12900. PMLR, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Li, Savarese, and Hoi]{li2023blip}
J.~Li, D.~Li, S.~Savarese, and S.~Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock In \emph{International conference on machine learning}, pages
  19730--19742. PMLR, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2020)Li, Chen, Cheng, Gan, Yu, and Liu]{li2020hero}
L.~Li, Y.-C. Chen, Y.~Cheng, Z.~Gan, L.~Yu, and J.~Liu.
\newblock Hero: Hierarchical encoder for video+ language omni-representation
  pre-training.
\newblock In \emph{ACL}, 2020.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Gan, Lin, Lin, Liu, Liu, and
  Wang]{li2022lavender}
L.~Li, Z.~Gan, K.~Lin, C.-C. Lin, Z.~Liu, C.~Liu, and L.~Wang.
\newblock Lavender: Unifying video-language understanding as masked language
  modeling.
\newblock \emph{arXiv preprint arXiv:2206.07160}, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, Fan, Hu, Feichtenhofer, and
  He]{li2022scaling}
Y.~Li, H.~Fan, R.~Hu, C.~Feichtenhofer, and K.~He.
\newblock Scaling language-image pre-training via masking.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 23390--23400, 2023{\natexlab{c}}.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{lin2014microsoft}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan,
  P.~Doll{\'a}r, and C.~L. Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{Computer Vision--ECCV 2014: 13th European Conference,
  Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages
  740--755. Springer, 2014.

\bibitem[Lin et~al.(2023)Lin, Tiwari, Huang, Li, Shou, Ji, and
  Chang]{lin2022towards}
X.~Lin, S.~Tiwari, S.~Huang, M.~Li, M.~Z. Shou, H.~Ji, and S.-F. Chang.
\newblock Towards fast adaptation of pretrained contrastive models for
  multi-channel video-language retrieval.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 14846--14855, June 2023.

\bibitem[Liu et~al.(2022)Liu, Xiong, Xu, Cao, and Jin]{liu2022ts2}
Y.~Liu, P.~Xiong, L.~Xu, S.~Cao, and Q.~Jin.
\newblock Ts2-net: Token shift and selection transformer for text-video
  retrieval.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XIV}, pages 319--335.
  Springer, 2022.

\bibitem[Lu et~al.(2022)Lu, Ding, Fei, Huo, and Lu]{lu2022lgdn}
H.~Lu, M.~Ding, N.~Fei, Y.~Huo, and Z.~Lu.
\newblock {LGDN}: Language-guided denoising network for video-language
  modeling.
\newblock In A.~H. Oh, A.~Agarwal, D.~Belgrave, and K.~Cho, editors,
  \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Lu et~al.(2023)Lu, Clark, Zellers, Mottaghi, and
  Kembhavi]{lu2022unified}
J.~Lu, C.~Clark, R.~Zellers, R.~Mottaghi, and A.~Kembhavi.
\newblock Unified-io: A unified model for vision, language, and multi-modal
  tasks.
\newblock In \emph{ICLR}, 2023.

\bibitem[Luo et~al.(2022)Luo, Ji, Zhong, Chen, Lei, Duan, and
  Li]{luo2022clip4clip}
H.~Luo, L.~Ji, M.~Zhong, Y.~Chen, W.~Lei, N.~Duan, and T.~Li.
\newblock Clip4clip: An empirical study of clip for end to end video clip
  retrieval and captioning.
\newblock \emph{Neurocomputing}, 508:\penalty0 293--304, 2022.

\bibitem[Ma et~al.(2023{\natexlab{a}})Ma, Jin, Wang, Huang, Zhu, Feng, and
  Yang]{ma2023temporal}
F.~Ma, X.~Jin, H.~Wang, J.~Huang, L.~Zhu, J.~Feng, and Y.~Yang.
\newblock Temporal perceiving video-language pre-training.
\newblock \emph{arXiv preprint arXiv:2301.07463}, 2023{\natexlab{a}}.

\bibitem[Ma et~al.(2022)Ma, Xu, Sun, Yan, Zhang, and Ji]{ma2022x}
Y.~Ma, G.~Xu, X.~Sun, M.~Yan, J.~Zhang, and R.~Ji.
\newblock X-clip: End-to-end multi-grained contrastive learning for video-text
  retrieval.
\newblock In \emph{Proceedings of the 30th ACM International Conference on
  Multimedia}, pages 638--647, 2022.

\bibitem[Ma et~al.(2023{\natexlab{b}})Ma, Pan, and Chai]{ma2023world}
Z.~Ma, J.~Pan, and J.~Chai.
\newblock World-to-words: Grounded open vocabulary acquisition through fast
  mapping in vision-language models.
\newblock In \emph{ACL}, 2023{\natexlab{b}}.

\bibitem[Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe,
  Alon, Dziri, Prabhumoye, Yang, et~al.]{madaan2023self}
A.~Madaan, N.~Tandon, P.~Gupta, S.~Hallinan, L.~Gao, S.~Wiegreffe, U.~Alon,
  N.~Dziri, S.~Prabhumoye, Y.~Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock In \emph{Advances in neural information processing systems}, 2023.

\bibitem[Marcel and Rodriguez(2010)]{marcel2010torchvision}
S.~Marcel and Y.~Rodriguez.
\newblock Torchvision the machine-vision package of torch.
\newblock In \emph{Proceedings of the 18th ACM international conference on
  Multimedia}, pages 1485--1488, 2010.

\bibitem[Momeni et~al.(2023)Momeni, Caron, Nagrani, Zisserman, and
  Schmid]{momeni2023verbs}
L.~Momeni, M.~Caron, A.~Nagrani, A.~Zisserman, and C.~Schmid.
\newblock Verbs in action: Improving verb understanding in video-language
  models.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 15579--15591, October 2023.

\bibitem[Moon et~al.(2023)Moon, Hyun, Park, Park, and Heo]{moon2023query}
W.~Moon, S.~Hyun, S.~Park, D.~Park, and J.-P. Heo.
\newblock Query-dependent video representation for moment retrieval and
  highlight detection.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 23023--23033, 2023.

\bibitem[Ordonez et~al.(2011)Ordonez, Kulkarni, and Berg]{ordonez2011im2text}
V.~Ordonez, G.~Kulkarni, and T.~Berg.
\newblock Im2text: Describing images using 1 million captioned photographs.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in neural information processing systems},
  volume~32, 2019.

\bibitem[Qian et~al.(2022)Qian, Cui, Chen, Peng, Guo, and
  Jiang]{qian2022locate}
T.~Qian, R.~Cui, J.~Chen, P.~Peng, X.~Guo, and Y.-G. Jiang.
\newblock Locate before answering: Answer guided question localization for
  video question answering.
\newblock In \emph{IEEE transactions on multimedia}, 2022.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International conference on machine learning}, pages
  8748--8763. PMLR, 2021.

\bibitem[Regneri et~al.(2013)Regneri, Rohrbach, Wetzel, Thater, Schiele, and
  Pinkal]{regneri2013grounding}
M.~Regneri, M.~Rohrbach, D.~Wetzel, S.~Thater, B.~Schiele, and M.~Pinkal.
\newblock Grounding action descriptions in videos.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  1:\penalty0 25--36, 2013.

\bibitem[Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis,
  Katta, Coombes, Jitsev, and Komatsuzaki]{schuhmann2021laion}
C.~Schuhmann, R.~Vencu, R.~Beaumont, R.~Kaczmarczyk, C.~Mullis, A.~Katta,
  T.~Coombes, J.~Jitsev, and A.~Komatsuzaki.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text
  pairs.
\newblock In \emph{NeurIPS Workshop}, 2021.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman,
  Cherti, Coombes, Katta, Mullis, Wortsman, et~al.]{schuhmann2022laion}
C.~Schuhmann, R.~Beaumont, R.~Vencu, C.~Gordon, R.~Wightman, M.~Cherti,
  T.~Coombes, A.~Katta, C.~Mullis, M.~Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation
  image-text models.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Sharma et~al.(2018)Sharma, Ding, Goodman, and
  Soricut]{sharma2018conceptual}
P.~Sharma, N.~Ding, S.~Goodman, and R.~Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 2556--2565,
  2018.

\bibitem[Sun et~al.(2019)Sun, Myers, Vondrick, Murphy, and
  Schmid]{sun2019videobert}
C.~Sun, A.~Myers, C.~Vondrick, K.~Murphy, and C.~Schmid.
\newblock Videobert: A joint model for video and language representation
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 7464--7473, 2019.

\bibitem[Sun et~al.(2023)Sun, Fang, Wu, Wang, and Cao]{sun2023eva}
Q.~Sun, Y.~Fang, L.~Wu, X.~Wang, and Y.~Cao.
\newblock Eva-clip: Improved training techniques for clip at scale.
\newblock \emph{arXiv preprint arXiv:2303.15389}, 2023.

\bibitem[Sung et~al.(2022)Sung, Cho, and Bansal]{sung2022vl}
Y.-L. Sung, J.~Cho, and M.~Bansal.
\newblock Vl-adapter: Parameter-efficient transfer learning for
  vision-and-language tasks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 5227--5237, 2022.

\bibitem[Sur{\'\i}s et~al.(2023)Sur{\'\i}s, Menon, and
  Vondrick]{suris2023vipergpt}
D.~Sur{\'\i}s, S.~Menon, and C.~Vondrick.
\newblock Vipergpt: Visual inference via python execution for reasoning.
\newblock In \emph{ICCV}, 2023.

\bibitem[Tan and Bansal(2019)]{tan2019lxmert}
H.~Tan and M.~Bansal.
\newblock Lxmert: Learning cross-modality encoder representations from
  transformers.
\newblock \emph{EMNLP}, 2019.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix,
  B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Ge, Yan, Ge, Lin, Cai, Wu, Shan,
  Qie, and Shou]{wang2022all}
A.~J. Wang, Y.~Ge, R.~Yan, Y.~Ge, X.~Lin, G.~Cai, J.~Wu, Y.~Shan, X.~Qie, and
  M.~Z. Shou.
\newblock All in one: Exploring unified video-language pre-training.
\newblock In \emph{CVPR}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Ge, Cai, Yan, Lin, Shan, Qie, and
  Shou]{wang2022object}
J.~Wang, Y.~Ge, G.~Cai, R.~Yan, X.~Lin, Y.~Shan, X.~Qie, and M.~Z. Shou.
\newblock Object-aware video-language pre-training for retrieval.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 3313--3322, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Yang, Men, Lin, Bai, Li, Ma,
  Zhou, Zhou, and Yang]{wang2022ofa}
P.~Wang, A.~Yang, R.~Men, J.~Lin, S.~Bai, Z.~Li, J.~Ma, C.~Zhou, J.~Zhou, and
  H.~Yang.
\newblock Ofa: Unifying architectures, tasks, and modalities through a simple
  sequence-to-sequence learning framework.
\newblock In \emph{International Conference on Machine Learning}, pages
  23318--23340. PMLR, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Bao, Dong, Bjorck, Peng, Liu,
  Aggarwal, Mohammed, Singhal, Som, and Wei]{beit3}
W.~Wang, H.~Bao, L.~Dong, J.~Bjorck, Z.~Peng, Q.~Liu, K.~Aggarwal, O.~K.
  Mohammed, S.~Singhal, S.~Som, and F.~Wei.
\newblock Image as a foreign language: {BEiT} pretraining for vision and
  vision-language tasks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{c}})Wang, Li, Li, He, Huang, Zhao, Zhang,
  Xu, Liu, Wang, et~al.]{wang2022internvideo}
Y.~Wang, K.~Li, Y.~Li, Y.~He, B.~Huang, Z.~Zhao, H.~Zhang, J.~Xu, Y.~Liu,
  Z.~Wang, et~al.
\newblock Internvideo: General video foundation models via generative and
  discriminative learning.
\newblock \emph{arXiv preprint arXiv:2212.03191}, 2022{\natexlab{c}}.

\bibitem[Wang et~al.(2022{\natexlab{d}})Wang, Li, Xu, Zhou, Lei, Lin, Wang,
  Yang, Zhu, Hoiem, Chang, Bansal, and Ji]{wang2022language}
Z.~Wang, M.~Li, R.~Xu, L.~Zhou, J.~Lei, X.~Lin, S.~Wang, Z.~Yang, C.~Zhu,
  D.~Hoiem, S.-F. Chang, M.~Bansal, and H.~Ji.
\newblock Language models with image descriptors are strong few-shot
  video-language learners.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{d}}.

\bibitem[Wang et~al.(2022{\natexlab{e}})Wang, Zhong, Miao, Ma, and
  Specia]{wang2022contrastive}
Z.~Wang, Y.~Zhong, Y.~Miao, L.~Ma, and L.~Specia.
\newblock Contrastive video-language learning with fine-grained frame sampling.
\newblock In \emph{Proceedings of the 2nd Conference of the Asia-Pacific
  Chapter of the Association for Computational Linguistics and the 12th
  International Joint Conference on Natural Language Processing (Volume 1: Long
  Papers)}, pages 694--705. Association for Computational Linguistics, Nov.
  2022{\natexlab{e}}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf2019huggingface}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz, J.~Davison, S.~Shleifer, P.~von Platen,
  C.~Ma, Y.~Jernite, J.~Plu, C.~Xu, T.~L. Scao, S.~Gugger, M.~Drame, Q.~Lhoest,
  and A.~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online,
  Oct. 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[Wu et~al.(2021)Wu, Yu, Chen, Tenenbaum, and Gan]{wu2021star}
B.~Wu, S.~Yu, Z.~Chen, J.~B. Tenenbaum, and C.~Gan.
\newblock Star: A benchmark for situated reasoning in real-world videos.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 2)}, 2021.

\bibitem[Wu et~al.(2019)Wu, He, Tan, Chen, and Wen]{wu2019multi}
W.~Wu, D.~He, X.~Tan, S.~Chen, and S.~Wen.
\newblock Multi-agent reinforcement learning based frame sampling for effective
  untrimmed video recognition.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 6222--6231, 2019.

\bibitem[Xiao et~al.(2021)Xiao, Shang, Yao, and Chua]{xiao2021next}
J.~Xiao, X.~Shang, A.~Yao, and T.-S. Chua.
\newblock Next-qa: Next phase of question-answering to explaining temporal
  actions.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 9777--9786, 2021.

\bibitem[Xiao et~al.(2022)Xiao, Zhou, Chua, and Yan]{xiao2022video}
J.~Xiao, P.~Zhou, T.-S. Chua, and S.~Yan.
\newblock Video graph transformer for video question answering.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXVI}, pages 39--58.
  Springer, 2022.

\bibitem[Xiao et~al.(2023)Xiao, Zhou, Yao, Li, Hong, Yan, and
  Chua]{xiao2023contrastive}
J.~Xiao, P.~Zhou, A.~Yao, Y.~Li, R.~Hong, S.~Yan, and T.~S. Chua.
\newblock Contrastive video question answering via video graph transformer.
\newblock \emph{IEEE Transactions on Pattern Analysis; Machine Intelligence},
  45\penalty0 (11):\penalty0 13265--13280, nov 2023.
\newblock ISSN 1939-3539.
\newblock \doi{10.1109/TPAMI.2023.3292266}.

\bibitem[Xu et~al.(2021)Xu, Ghosh, Huang, Okhonko, Aghajanyan, Metze,
  Zettlemoyer, and Feichtenhofer]{xu2021videoclip}
H.~Xu, G.~Ghosh, P.-Y. Huang, D.~Okhonko, A.~Aghajanyan, F.~Metze,
  L.~Zettlemoyer, and C.~Feichtenhofer.
\newblock Videoclip: Contrastive pre-training for zero-shot video-text
  understanding.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Xu et~al.(2023)Xu, Ye, Yan, Shi, Ye, Xu, Li, Bi, Qian, Wang,
  et~al.]{xu2023mplug}
H.~Xu, Q.~Ye, M.~Yan, Y.~Shi, J.~Ye, Y.~Xu, C.~Li, B.~Bi, Q.~Qian, W.~Wang,
  et~al.
\newblock mplug-2: A modularized multi-modal foundation model across text,
  image and video.
\newblock In \emph{ICML}, 2023.

\bibitem[Xue et~al.(2023)Xue, Sun, Liu, Fu, Song, Li, and Luo]{xueclip}
H.~Xue, Y.~Sun, B.~Liu, J.~Fu, R.~Song, H.~Li, and J.~Luo.
\newblock {CLIP}-vip: Adapting pre-trained image-text model to video-language
  alignment.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Yan et~al.(2022)Yan, Zhu, Wang, Cao, Zhang, Ghosh, Wu, and
  Yu]{yan2022video}
S.~Yan, T.~Zhu, Z.~Wang, Y.~Cao, M.~Zhang, S.~Ghosh, Y.~Wu, and J.~Yu.
\newblock Video-text modeling with zero-shot transfer from contrastive
  captioners.
\newblock \emph{arXiv preprint arXiv:2212.04979}, 2022.

\bibitem[Yang et~al.(2021)Yang, Miech, Sivic, Laptev, and Schmid]{yang2021just}
A.~Yang, A.~Miech, J.~Sivic, I.~Laptev, and C.~Schmid.
\newblock Just ask: Learning to answer questions from millions of narrated
  videos.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1686--1697, 2021.

\bibitem[Yang et~al.(2022)Yang, Miech, Sivic, Laptev, and Schmid]{yang2022zero}
A.~Yang, A.~Miech, J.~Sivic, I.~Laptev, and C.~Schmid.
\newblock Zero-shot video question answering via frozen bidirectional language
  models.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Yao et~al.(2022)Yao, Huang, Hou, Lu, Niu, Xu, Liang, Li, Jiang, and
  Xu]{yao2021filip}
L.~Yao, R.~Huang, L.~Hou, G.~Lu, M.~Niu, H.~Xu, X.~Liang, Z.~Li, X.~Jiang, and
  C.~Xu.
\newblock Filip: fine-grained interactive language-image pre-training.
\newblock In \emph{ICLR}, 2022.

\bibitem[Ye et~al.(2023)Ye, Xu, Yan, Xu, Qian, Zhang, and Huang]{ye2022hitea}
Q.~Ye, G.~Xu, M.~Yan, H.~Xu, Q.~Qian, J.~Zhang, and F.~Huang.
\newblock Hitea: Hierarchical temporal-aware video-language pre-training.
\newblock In \emph{ICCV}, 2023.

\bibitem[Yu et~al.(2022)Yu, Wang, Vasudevan, Yeung, Seyedhosseini, and
  Wu]{yu2022coca}
J.~Yu, Z.~Wang, V.~Vasudevan, L.~Yeung, M.~Seyedhosseini, and Y.~Wu.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=Ee277P3AYC}.

\bibitem[Yuan et~al.(2021)Yuan, Chen, Chen, Codella, Dai, Gao, Hu, Huang, Li,
  Li, et~al.]{yuan2021florence}
L.~Yuan, D.~Chen, Y.-L. Chen, N.~Codella, X.~Dai, J.~Gao, H.~Hu, X.~Huang,
  B.~Li, C.~Li, et~al.
\newblock Florence: A new foundation model for computer vision.
\newblock \emph{arXiv preprint arXiv:2111.11432}, 2021.

\bibitem[Zellers et~al.(2022)Zellers, Lu, Lu, Yu, Zhao, Salehi, Kusupati,
  Hessel, Farhadi, and Choi]{zellers2022merlot}
R.~Zellers, J.~Lu, X.~Lu, Y.~Yu, Y.~Zhao, M.~Salehi, A.~Kusupati, J.~Hessel,
  A.~Farhadi, and Y.~Choi.
\newblock Merlot reserve: Neural script knowledge through vision and language
  and sound.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 16375--16387, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{zhang2022opt}
S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab,
  X.~Li, X.~V. Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny]{zhu2023minigpt}
D.~Zhu, J.~Chen, X.~Shen, X.~Li, and M.~Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced
  large language models.
\newblock \emph{arXiv preprint arXiv:2304.10592}, 2023.

\end{thebibliography}
