\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anil et~al.(2019)Anil, Gupta, Koren, and Singer]{DBLP:conf/nips/Anil0KS19}
R.~Anil, V.~Gupta, T.~Koren, and Y.~Singer.
\newblock Memory efficient adaptive optimization.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Anil et~al.(2023)Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, Millican, Silver, Petrov, Johnson, Antonoglou, Schrittwieser, Glaese, Chen, Pitler, Lillicrap, Lazaridou, Firat, Molloy, Isard, Barham, Hennigan, Lee, Viola, Reynolds, Xu, Doherty, Collins, Meyer, Rutherford, Moreira, Ayoub, Goel, Tucker, Piqueras, Krikun, Barr, Savinov, Danihelka, Roelofs, White, Andreassen, von Glehn, Yagati, Kazemi, Gonzalez, Khalman, Sygnowski, and et~al.]{DBLP:journals/corr/abs-2312-11805}
R.~Anil, S.~Borgeaud, Y.~Wu, J.~Alayrac, J.~Yu, R.~Soricut, J.~Schalkwyk, A.~M. Dai, A.~Hauth, K.~Millican, D.~Silver, S.~Petrov, M.~Johnson, I.~Antonoglou, J.~Schrittwieser, A.~Glaese, J.~Chen, E.~Pitler, T.~P. Lillicrap, A.~Lazaridou, O.~Firat, J.~Molloy, M.~Isard, P.~R. Barham, T.~Hennigan, B.~Lee, F.~Viola, M.~Reynolds, Y.~Xu, R.~Doherty, E.~Collins, C.~Meyer, E.~Rutherford, E.~Moreira, K.~Ayoub, M.~Goel, G.~Tucker, E.~Piqueras, M.~Krikun, I.~Barr, N.~Savinov, I.~Danihelka, B.~Roelofs, A.~White, A.~Andreassen, T.~von Glehn, L.~Yagati, M.~Kazemi, L.~Gonzalez, M.~Khalman, J.~Sygnowski, and et~al.
\newblock Gemini: {A} family of highly capable multimodal models.
\newblock \emph{CoRR}, 2023.

\bibitem[Basu et~al.(2019)Basu, Data, Karakus, and Diggavi]{basu2019qsparse}
D.~Basu, D.~Data, C.~Karakus, and S.~Diggavi.
\newblock Qsparse-local-sgd: Distributed sgd with quantization, sparsification and local computations.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Biderman et~al.(2024)Biderman, Ortiz, Portes, Paul, Greengard, Jennings, King, Havens, Chiley, Frankle, Blakeney, and Cunningham]{biderman2024lora}
D.~Biderman, J.~G. Ortiz, J.~Portes, M.~Paul, P.~Greengard, C.~Jennings, D.~King, S.~Havens, V.~Chiley, J.~Frankle, C.~Blakeney, and J.~P. Cunningham.
\newblock Lora learns less and forgets less.
\newblock \emph{TMLR}, 2024.

\bibitem[Chaudhry et~al.(2020)Chaudhry, Khan, Dokania, and Torr]{chaudhry2020continual}
A.~Chaudhry, N.~Khan, P.~Dokania, and P.~Torr.
\newblock Continual learning in low-rank orthogonal subspaces.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Chen et~al.(2019)Chen, Raskutti, and Yuan]{DBLP:journals/jmlr/ChenRY19}
H.~Chen, G.~Raskutti, and M.~Yuan.
\newblock Non-convex projected gradient descent for generalized low-rank tensor regression.
\newblock \emph{J. Mach. Learn. Res.}, 2019.

\bibitem[Chen et~al.(2022)Chen, Ge, Tong, Wang, Song, Wang, and Luo]{DBLP:conf/nips/ChenGTWSWL22}
S.~Chen, C.~Ge, Z.~Tong, J.~Wang, Y.~Song, J.~Wang, and P.~Luo.
\newblock Adaptformer: Adapting vision transformers for scalable visual recognition.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{DBLP:journals/corr/ChenXZG16}
T.~Chen, B.~Xu, C.~Zhang, and C.~Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock \emph{CoRR}, abs/1604.06174, 2016.

\bibitem[Chen and Wainwright(2015)]{DBLP:journals/corr/ChenW15a}
Y.~Chen and M.~J. Wainwright.
\newblock Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees.
\newblock \emph{CoRR}, 2015.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Shleifer, and Zettlemoyer]{DBLP:conf/iclr/DettmersLSZ22}
T.~Dettmers, M.~Lewis, S.~Shleifer, and L.~Zettlemoyer.
\newblock 8-bit optimizers via block-wise quantization.
\newblock In \emph{ICLR}, 2022.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{DBLP:conf/nips/DettmersPHZ23}
T.~Dettmers, A.~Pagnoni, A.~Holtzman, and L.~Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2021image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit, and N.~Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale, 2021.

\bibitem[Fomenko et~al.(2024)Fomenko, Yu, Lee, Hsieh, and Chen]{DBLP:journals/corr/2404.05086}
V.~Fomenko, H.~Yu, J.~Lee, S.~Hsieh, and W.~Chen.
\newblock A note on lora.
\newblock \emph{CoRR}, 2024.

\bibitem[Gomez et~al.(2017)Gomez, Ren, Urtasun, and Grosse]{DBLP:conf/nips/GomezRUG17}
A.~N. Gomez, M.~Ren, R.~Urtasun, and R.~B. Grosse.
\newblock The reversible residual network: Backpropagation without storing activations.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Gur{-}Ari et~al.(2018)Gur{-}Ari, Roberts, and Dyer]{gur2018gradient}
G.~Gur{-}Ari, D.~A. Roberts, and E.~Dyer.
\newblock Gradient descent happens in a tiny subspace.
\newblock \emph{arXiv preprint}, 2018.

\bibitem[Han et~al.(2020)Han, Wang, and Leung]{han2020adaptive}
P.~Han, S.~Wang, and K.~K. Leung.
\newblock Adaptive gradient sparsification for efficient federated learning: An online learning approach.
\newblock In \emph{Conference on distributed computing systems (ICDCS)}, 2020.

\bibitem[Hao et~al.(2024)Hao, Cao, and Mou]{hao2024floralowrankadapterssecretly}
Y.~Hao, Y.~Cao, and L.~Mou.
\newblock Flora: Low-rank adapters are secretly gradient compressors, 2024.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen{-}Zhu, Li, Wang, Wang, and Chen]{DBLP:conf/iclr/HuSWALWWC22}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen{-}Zhu, Y.~Li, S.~Wang, L.~Wang, and W.~Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{ICLR}, 2022.

\bibitem[Jacot et~al.(2020)Jacot, Gabriel, and Hongler]{jacot2020neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Jie and Deng(2023)]{jie2023fact}
S.~Jie and Z.-H. Deng.
\newblock Fact: Factor-tuning for lightweight adaptation on vision transformer.
\newblock \emph{AAAI}, 2023.

\bibitem[Kim et~al.(2023)Kim, Yang, Kim, Hong, and Park]{DBLP:journals/corr/abs-2309-06922}
S.~Kim, H.~Yang, Y.~Kim, Y.~Hong, and E.~Park.
\newblock Hydra: Multi-head low-rank adaptation for parameter efficient fine-tuning.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Larsen et~al.(2022)Larsen, Fort, Becker, and Ganguli]{larsen2021many}
B.~W. Larsen, S.~Fort, N.~Becker, and S.~Ganguli.
\newblock How many degrees of freedom do we need to train deep networks: a loss landscape perspective.
\newblock In \emph{ICLR}, 2022.

\bibitem[Lee and Choi(2018)]{lee2018gradient}
Y.~Lee and S.~Choi.
\newblock Gradient-based meta-learning with learned layerwise metric and subspace.
\newblock In \emph{ICML}, 2018.

\bibitem[Li et~al.(2023)Li, Chen, and Zhu]{DBLP:conf/nips/LiCZ23}
B.~Li, J.~Chen, and J.~Zhu.
\newblock Memory efficient optimizers with 4-bit states.
\newblock In \emph{ICLR}, 2023.

\bibitem[Li and Hoefler(2022)]{li2022near}
S.~Li and T.~Hoefler.
\newblock Near-optimal sparse allreduce for distributed deep learning.
\newblock In \emph{Symposium on Principles and Practice of Parallel Programming}, 2022.

\bibitem[Lialin et~al.(2024)Lialin, Shivagunde, Muckatira, and Rumshisky]{DBLP:journals/corr/abs-2307-05695}
V.~Lialin, N.~Shivagunde, S.~Muckatira, and A.~Rumshisky.
\newblock Relora: High-rank training through low-rank updates.
\newblock \emph{ICLR}, 2024.

\bibitem[Lian et~al.(2022)Lian, Zhou, Feng, and Wang]{DBLP:conf/nips/LianZFW22}
D.~Lian, D.~Zhou, J.~Feng, and X.~Wang.
\newblock Scaling {\&} shifting your features: {A} new baseline for efficient model tuning.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Lin et~al.(2018)Lin, Han, Mao, Wang, and Dally]{lin2017deep}
Y.~Lin, S.~Han, H.~Mao, Y.~Wang, and B.~Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for distributed training.
\newblock In \emph{ICLR}, 2018.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{DBLP:journals/corr/abs-1907-11692}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis, L.~Zettlemoyer, and V.~Stoyanov.
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock \emph{arXiv preprint}, 2019.

\bibitem[Lv et~al.(2023)Lv, Yang, Liu, Gao, Guo, and Qiu]{DBLP:journals/corr/abs-2306-09782}
K.~Lv, Y.~Yang, T.~Liu, Q.~Gao, Q.~Guo, and X.~Qiu.
\newblock Full parameter fine-tuning for large language models with limited resources.
\newblock \emph{CoRR}, abs/2306.09782, 2023.

\bibitem[OpenAI(2023)]{DBLP:journals/corr/abs-2303-08774}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{CoRR}, 2023.

\bibitem[Park and Lee(2023)]{park2023sparse}
C.~Park and N.~Lee.
\newblock S\({}^{\mbox{3}}\)gd-mv: Sparse-signsgd with majority vote for communication-efficient distributed learning.
\newblock In \emph{{IEEE} International Symposium on Information Theory, {ISIT}}, 2023.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin, Desmaison, Antiga, and Lerer]{Paszke2017AutomaticPyTorch}
A.~Paszke, S.~Gross, S.~Chintala, G.~Chanan, E.~Yang, Z.~DeVito, Z.~Lin, A.~Desmaison, L.~Antiga, and A.~Lerer.
\newblock {Automatic differentiation in PyTorch}.
\newblock 2017.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{DBLP:journals/jmlr/RaffelSRLNMZLL20}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou, W.~Li, and P.~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{J. Mach. Learn. Res.}, 2020.

\bibitem[Renduchintala et~al.(2024)Renduchintala, Konuk, and Kuchaiev]{DBLP:journals/corr/abs-2311-09578}
A.~Renduchintala, T.~Konuk, and O.~Kuchaiev.
\newblock Tied-lora: Enhacing parameter efficiency of lora with weight tying.
\newblock ACL, 2024.

\bibitem[Rohan et~al.(2023)Rohan, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
T.~Rohan, I.~Gulrajani, T.~Zhang, Y.~Dubois, X.~Li, C.~Guestrin, P.~Liang, and T.~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock Technical report, 2023.

\bibitem[Rothchild et~al.(2020)Rothchild, Panda, Ullah, Ivkin, Stoica, Braverman, Gonzalez, and Arora]{rothchild2020fetchsgd}
D.~Rothchild, A.~Panda, E.~Ullah, N.~Ivkin, I.~Stoica, V.~Braverman, J.~Gonzalez, and R.~Arora.
\newblock Fetchsgd: Communication-efficient federated learning with sketching.
\newblock In \emph{ICLR}, 2020.

\bibitem[Sahu et~al.(2021)Sahu, Dutta, M~Abdelmoniem, Banerjee, Canini, and Kalnis]{sahu2021rethinking}
A.~Sahu, A.~Dutta, A.~M~Abdelmoniem, T.~Banerjee, M.~Canini, and P.~Kalnis.
\newblock Rethinking gradient sparsification as total error minimization.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Sattler et~al.(2019)Sattler, Wiedemann, M{\"u}ller, and Samek]{sattler2019robust}
F.~Sattler, S.~Wiedemann, K.-R. M{\"u}ller, and W.~Samek.
\newblock Robust and communication-efficient federated learning from non-iid data.
\newblock \emph{IEEE transactions on neural networks and learning systems}, 2019.

\bibitem[Shanbhag et~al.(2018)Shanbhag, Pirk, and Madden]{shanbhag2018efficient}
A.~Shanbhag, H.~Pirk, and S.~Madden.
\newblock Efficient top-k query processing on massively parallel hardware.
\newblock In \emph{International Conference on Management of Data}, 2018.

\bibitem[Shazeer and Stern(2018)]{DBLP:conf/icml/ShazeerS18}
N.~Shazeer and M.~Stern.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In \emph{ICML}, 2018.

\bibitem[Sheng et~al.(2023)Sheng, Cao, Li, Hooper, Lee, Yang, Chou, Zhu, Zheng, Keutzer, Gonzalez, and Stoica]{DBLP:journals/corr/abs-2311-03285}
Y.~Sheng, S.~Cao, D.~Li, C.~Hooper, N.~Lee, S.~Yang, C.~Chou, B.~Zhu, L.~Zheng, K.~Keutzer, J.~E. Gonzalez, and I.~Stoica.
\newblock S-lora: Serving thousands of concurrent lora adapters.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Shi et~al.(2019)Shi, Chu, Cheung, and See]{shi2019understanding}
S.~Shi, X.~Chu, K.~C. Cheung, and S.~See.
\newblock Understanding top-k sparsification in distributed deep learning.
\newblock \emph{arXiv preprint}, 2019.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{stich2018sparsified}
S.~U. Stich, J.-B. Cordonnier, and M.~Jaggi.
\newblock Sparsified sgd with memory.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Sung et~al.(2022)Sung, Cho, and Bansal]{DBLP:conf/cvpr/Sung0B22}
Y.~Sung, J.~Cho, and M.~Bansal.
\newblock {VL-ADAPTER:} parameter-efficient transfer learning for vision-and-language tasks.
\newblock In \emph{CVPR}, 2022.

\bibitem[Sutton(2019)]{russell}
R.~Sutton.
\newblock The bitter lesson.
\newblock https://blog.biocomm.ai/2019/03/13/the-bitter-lesson-rich-sutton-march-13-2019/, 2019.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`{e}}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{DBLP:journals/corr/abs-2307-09288}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.~Lachaux, T.~Lacroix, B.~Rozi{\`{e}}re, N.~Goyal, E.~Hambro, F.~Azhar, A.~Rodriguez, A.~Joulin, E.~Grave, and G.~Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{CoRR}, 2023.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang2019glue}
A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural language understanding.
\newblock In \emph{ICLR}, 2019.

\bibitem[Wang et~al.(2023)Wang, Lin, Zeng, and Zhang]{DBLP:journals/corr/abs-2311-11501}
Y.~Wang, Y.~Lin, X.~Zeng, and G.~Zhang.
\newblock Multilora: Democratizing lora for better multi-task learning.
\newblock \emph{CoRR}, 2023.

\bibitem[Wangni et~al.(2018)Wangni, Wang, Liu, and Zhang]{wangni2018gradient}
J.~Wangni, J.~Wang, J.~Liu, and T.~Zhang.
\newblock Gradient sparsification for communication-efficient distributed optimization.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Wen et~al.(2017)Wen, Xu, Yan, Wu, Wang, Chen, and Li]{wen2017terngrad}
W.~Wen, C.~Xu, F.~Yan, C.~Wu, Y.~Wang, Y.~Chen, and H.~Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed deep learning.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Xia et~al.(2024)Xia, Qin, and Hazan]{DBLP:journals/corr/abs-2401-04151}
W.~Xia, C.~Qin, and E.~Hazan.
\newblock Chain of lora: Efficient fine-tuning of language models via residual learning.
\newblock \emph{CoRR}, 2024.

\bibitem[Zhai et~al.(2019)Zhai, Puigcerver, Kolesnikov, Ruyssen, Riquelme, Lucic, Djolonga, Pinto, Neumann, Dosovitskiy, Beyer, Bachem, Tschannen, Michalski, Bousquet, Gelly, and Houlsby]{DBLP:journals/corr/abs-1910-04867}
X.~Zhai, J.~Puigcerver, A.~Kolesnikov, P.~Ruyssen, C.~Riquelme, M.~Lucic, J.~Djolonga, A.~S. Pinto, M.~Neumann, A.~Dosovitskiy, L.~Beyer, O.~Bachem, M.~Tschannen, M.~Michalski, O.~Bousquet, S.~Gelly, and N.~Houlsby.
\newblock The visual task adaptation benchmark.
\newblock \emph{CoRR}, 2019.

\bibitem[Zhao et~al.(2024)Zhao, Zhang, Chen, Wang, Anandkumar, and Tian]{zhao2024galore}
J.~Zhao, Z.~Zhang, B.~Chen, Z.~Wang, A.~Anandkumar, and Y.~Tian.
\newblock Galore: Memory-efficient llm training by gradient low-rank projection.
\newblock In \emph{ICML}, 2024.

\end{thebibliography}
