\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amodei et~al.(2016)Amodei, Ananthanarayanan, Anubhai, Bai, Battenberg,
  Case, Casper, Catanzaro, Cheng, Chen, et~al.]{amodei2016deep}
Amodei, Dario, Ananthanarayanan, Sundaram, Anubhai, Rishita, Bai, Jingliang,
  Battenberg, Eric, Case, Carl, Casper, Jared, Catanzaro, Bryan, Cheng, Qiang,
  Chen, Guoliang, et~al.
\newblock Deep speech 2: End-to-end speech recognition in english and mandarin.
\newblock In \emph{International conference on machine learning}, pp.\
  173--182, 2016.

\bibitem[Ara{\'u}jo et~al.(2019)Ara{\'u}jo, Oliveira, and
  Yukimura]{araujo2019mean}
Ara{\'u}jo, Dyego, Oliveira, Roberto~I, and Yukimura, Daniel.
\newblock A mean-field limit for certain deep neural networks.
\newblock \emph{arXiv preprint arXiv:1906.00193}, 2019.

\bibitem[Baykal et~al.(2019{\natexlab{a}})Baykal, Liebenwein, Gilitschenski,
  Feldman, and Rus]{baykal2018datadependent}
Baykal, Cenk, Liebenwein, Lucas, Gilitschenski, Igor, Feldman, Dan, and Rus,
  Daniela.
\newblock Data-dependent coresets for compressing neural networks with
  applications to generalization bounds.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=HJfwJ2A5KX}.

\bibitem[Baykal et~al.(2019{\natexlab{b}})Baykal, Liebenwein, Gilitschenski,
  Feldman, and Rus]{baykal2019sipping}
Baykal, Cenk, Liebenwein, Lucas, Gilitschenski, Igor, Feldman, Dan, and Rus,
  Daniela.
\newblock Sipping neural networks: Sensitivity-informed provable pruning of
  neural networks.
\newblock \emph{arXiv preprint arXiv:1910.05422}, 2019{\natexlab{b}}.

\bibitem[Cai et~al.(2019)Cai, Zhu, and Han]{cai2018proxylessnas}
Cai, Han, Zhu, Ligeng, and Han, Song.
\newblock Proxyless{NAS}: Direct neural architecture search on target task and
  hardware.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://arxiv.org/pdf/1812.00332.pdf}.

\bibitem[Chin et~al.(2019)Chin, Ding, Zhang, and Marculescu]{chin2019legr}
Chin, Ting-Wu, Ding, Ruizhou, Zhang, Cha, and Marculescu, Diana.
\newblock Legr: Filter pruning via learned global ranking.
\newblock \emph{arXiv preprint arXiv:1904.12368}, 2019.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Dong et~al.(2017)Dong, Huang, Yang, and Yan]{dong2017more}
Dong, Xuanyi, Huang, Junshi, Yang, Yi, and Yan, Shuicheng.
\newblock More is less: A more complicated network with less inference
  complexity.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  5840--5848, 2017.

\bibitem[Elesedy et~al.(2020)Elesedy, Kanade, and Teh]{elesedy2020lottery}
Elesedy, Bryn, Kanade, Varun, and Teh, Yee~Whye.
\newblock Lottery tickets in linear models: An analysis of iterative magnitude
  pruning.
\newblock \emph{arXiv preprint arXiv:2007.08243}, 2020.

\bibitem[Frank \& Wolfe(1956)Frank and Wolfe]{frank1956algorithm}
Frank, Marguerite and Wolfe, Philip.
\newblock An algorithm for quadratic programming.
\newblock \emph{Naval research logistics quarterly}, 3\penalty0 (1-2):\penalty0
  95--110, 1956.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{frankle2018lottery}
Frankle, Jonathan and Carbin, Michael.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock \emph{arXiv preprint arXiv:1803.03635}, 2018.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Han, Song, Pool, Jeff, Tran, John, and Dally, William.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1135--1143, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2018{\natexlab{a}})He, Kang, Dong, Fu, and Yang]{he2018soft}
He, Yang, Kang, Guoliang, Dong, Xuanyi, Fu, Yanwei, and Yang, Yi.
\newblock Soft filter pruning for accelerating deep convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1808.06866}, 2018{\natexlab{a}}.

\bibitem[He et~al.(2019)He, Liu, Wang, Hu, and Yang]{he2019filter}
He, Yang, Liu, Ping, Wang, Ziwei, Hu, Zhilan, and Yang, Yi.
\newblock Filter pruning via geometric median for deep convolutional neural
  networks acceleration.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  4340--4349, 2019.

\bibitem[He et~al.(2017)He, Zhang, and Sun]{he2017channel}
He, Yihui, Zhang, Xiangyu, and Sun, Jian.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  1389--1397, 2017.

\bibitem[He et~al.(2018{\natexlab{b}})He, Lin, Liu, Wang, Li, and
  Han]{he2018amc}
He, Yihui, Lin, Ji, Liu, Zhijian, Wang, Hanrui, Li, Li-Jia, and Han, Song.
\newblock Amc: Automl for model compression and acceleration on mobile devices.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  784--800, 2018{\natexlab{b}}.

\bibitem[Howard et~al.(2019)Howard, Sandler, Chu, Chen, Chen, Tan, Wang, Zhu,
  Pang, Vasudevan, et~al.]{howard2019searching}
Howard, Andrew, Sandler, Mark, Chu, Grace, Chen, Liang-Chieh, Chen, Bo, Tan,
  Mingxing, Wang, Weijun, Zhu, Yukun, Pang, Ruoming, Vasudevan, Vijay, et~al.
\newblock Searching for mobilenetv3.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  1314--1324, 2019.

\bibitem[Huang \& Wang(2018)Huang and Wang]{huang2018data}
Huang, Zehao and Wang, Naiyan.
\newblock Data-driven sparse structure selection for deep neural networks.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pp.\  304--320, 2018.

\bibitem[Li et~al.(2017)Li, Kadav, Durdanovic, Samet, and Graf]{li2016pruning}
Li, Hao, Kadav, Asim, Durdanovic, Igor, Samet, Hanan, and Graf, Hans~Peter.
\newblock Pruning filters for efficient convnets.
\newblock \emph{The International Conference on Learning Representations},
  2017.

\bibitem[Liebenwein et~al.(2020)Liebenwein, Baykal, Lang, Feldman, and
  Rus]{Liebenwein2020Provable}
Liebenwein, Lucas, Baykal, Cenk, Lang, Harry, Feldman, Dan, and Rus, Daniela.
\newblock Provable filter pruning for efficient neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJxkOlSYDH}.

\bibitem[Liu et~al.(2019)Liu, Mu, Zhang, Guo, Yang, Cheng, and
  Sun]{liu2019metapruning}
Liu, Zechun, Mu, Haoyuan, Zhang, Xiangyu, Guo, Zichao, Yang, Xin, Cheng,
  Kwang-Ting, and Sun, Jian.
\newblock Metapruning: Meta learning for automatic neural network channel
  pruning.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  3296--3305, 2019.

\bibitem[Liu et~al.(2017)Liu, Li, Shen, Huang, Yan, and Zhang]{liu2017learning}
Liu, Zhuang, Li, Jianguo, Shen, Zhiqiang, Huang, Gao, Yan, Shoumeng, and Zhang,
  Changshui.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  2736--2744, 2017.

\bibitem[Liu et~al.(2018)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking}
Liu, Zhuang, Sun, Mingjie, Zhou, Tinghui, Huang, Gao, and Darrell, Trevor.
\newblock Rethinking the value of network pruning.
\newblock \emph{arXiv preprint arXiv:1810.05270}, 2018.

\bibitem[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{loshchilov2016sgdr}
Loshchilov, Ilya and Hutter, Frank.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Luo et~al.(2017)Luo, Wu, and Lin]{luo2017thinet}
Luo, Jian-Hao, Wu, Jianxin, and Lin, Weiyao.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  5058--5066, 2017.

\bibitem[Malach et~al.(2020)Malach, Yehudai, Shalev-Shwartz, and
  Shamir]{malach2020proving}
Malach, Eran, Yehudai, Gilad, Shalev-Shwartz, Shai, and Shamir, Ohad.
\newblock Proving the lottery ticket hypothesis: Pruning is all you need.
\newblock \emph{arXiv preprint arXiv:2002.00585}, 2020.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{mei2018mean}
Mei, Song, Montanari, Andrea, and Nguyen, Phan-Minh.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (33):\penalty0 E7665--E7671, 2018.

\bibitem[Molchanov et~al.(2017{\natexlab{a}})Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
Molchanov, Dmitry, Ashukha, Arsenii, and Vetrov, Dmitry.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2498--2507. JMLR. org, 2017{\natexlab{a}}.

\bibitem[Molchanov et~al.(2017{\natexlab{b}})Molchanov, Tyree, Karras, Aila,
  and Kautz]{molchanov2016pruning}
Molchanov, Pavlo, Tyree, Stephen, Karras, Tero, Aila, Timo, and Kautz, Jan.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock \emph{International Conference on Learning Representations},
  2017{\natexlab{b}}.

\bibitem[Molchanov et~al.(2019)Molchanov, Mallya, Tyree, Frosio, and
  Kautz]{molchanov2019importance}
Molchanov, Pavlo, Mallya, Arun, Tyree, Stephen, Frosio, Iuri, and Kautz, Jan.
\newblock Importance estimation for neural network pruning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  11264--11272, 2019.

\bibitem[Mussay et~al.(2020)Mussay, Osadchy, Braverman, Zhou, and
  Feldman]{Mussay2020Data-Independent}
Mussay, Ben, Osadchy, Margarita, Braverman, Vladimir, Zhou, Samson, and
  Feldman, Dan.
\newblock Data-independent neural pruning via coresets.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=H1gmHaEKwB}.

\bibitem[Orseau et~al.(2020)Orseau, Hutter, and
  Rivasplata]{orseau2020logarithmic}
Orseau, Laurent, Hutter, Marcus, and Rivasplata, Omar.
\newblock Logarithmic pruning is all you need.
\newblock \emph{arXiv preprint arXiv:2006.12156}, 2020.

\bibitem[Pensia et~al.(2020)Pensia, Rajput, Nagle, Vishwakarma, and
  Papailiopoulos]{pensia2020optimal}
Pensia, Ankit, Rajput, Shashank, Nagle, Alliot, Vishwakarma, Harit, and
  Papailiopoulos, Dimitris.
\newblock Optimal lottery tickets via subsetsum: Logarithmic
  over-parameterization is sufficient.
\newblock \emph{arXiv preprint arXiv:2006.07990}, 2020.

\bibitem[Qi et~al.(2017{\natexlab{a}})Qi, Su, Mo, and Guibas]{Qi_2017_CVPR}
Qi, Charles~R., Su, Hao, Mo, Kaichun, and Guibas, Leonidas~J.
\newblock Pointnet: Deep learning on point sets for 3d classification and
  segmentation.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, July 2017{\natexlab{a}}.

\bibitem[Qi et~al.(2017{\natexlab{b}})Qi, Yi, Su, and
  Guibas]{NIPS2017pointnet++}
Qi, Charles~Ruizhongtai, Yi, Li, Su, Hao, and Guibas, Leonidas~J.
\newblock Pointnet++: Deep hierarchical feature learning on point sets in a
  metric space.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30}, pp.\  5099--5108. 2017{\natexlab{b}}.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Sandler, Mark, Howard, Andrew, Zhu, Menglong, Zhmoginov, Andrey, and Chen,
  Liang-Chieh.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  4510--4520, 2018.

\bibitem[Sirignano \& Spiliopoulos(2019)Sirignano and
  Spiliopoulos]{sirignano2019mean}
Sirignano, Justin and Spiliopoulos, Konstantinos.
\newblock Mean field analysis of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1903.04440}, 2019.

\bibitem[Wang et~al.(2019)Wang, Sun, Liu, Sarma, Bronstein, and
  Solomon]{wang2019dynamic}
Wang, Yue, Sun, Yongbin, Liu, Ziwei, Sarma, Sanjay~E, Bronstein, Michael~M, and
  Solomon, Justin~M.
\newblock Dynamic graph cnn for learning on point clouds.
\newblock \emph{ACM Transactions on Graphics (TOG)}, 38\penalty0 (5):\penalty0
  1--12, 2019.

\bibitem[Wu et~al.(2020)Wu, Ye, Lei, Lee, and Liu]{wu2020steepest}
Wu, Lemeng, Ye, Mao, Lei, Qi, Lee, Jason~D, and Liu, Qiang.
\newblock Steepest descent neural architecture optimization: Escaping local
  optimum with signed neural splitting.
\newblock \emph{arXiv preprint arXiv:2003.10392}, 2020.

\bibitem[Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun, Cao,
  Gao, Macherey, et~al.]{wu2016google}
Wu, Yonghui, Schuster, Mike, Chen, Zhifeng, Le, Quoc~V, Norouzi, Mohammad,
  Macherey, Wolfgang, Krikun, Maxim, Cao, Yuan, Gao, Qin, Macherey, Klaus,
  et~al.
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation.
\newblock \emph{arXiv preprint arXiv:1609.08144}, 2016.

\bibitem[Wu et~al.(2015)Wu, Song, Khosla, Yu, Zhang, Tang, and Xiao]{wu20153d}
Wu, Zhirong, Song, Shuran, Khosla, Aditya, Yu, Fisher, Zhang, Linguang, Tang,
  Xiaoou, and Xiao, Jianxiong.
\newblock 3d shapenets: A deep representation for volumetric shapes.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  1912--1920, 2015.

\bibitem[Ye et~al.(2018)Ye, Lu, Lin, and Wang]{ye2018rethinking}
Ye, Jianbo, Lu, Xin, Lin, Zhe, and Wang, James~Z.
\newblock Rethinking the smaller-norm-less-informative assumption in channel
  pruning of convolution layers.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=HJ94fqApW}.

\bibitem[Ye et~al.(2020)Ye, Gong, Nie, Zhou, Klivans, and Liu]{ye2020good}
Ye, Mao, Gong, Chengyue, Nie, Lizhen, Zhou, Denny, Klivans, Adam, and Liu,
  Qiang.
\newblock Good subnetworks provably exist: Pruning via greedy forward
  selection.
\newblock \emph{arXiv preprint arXiv:2003.01794}, 2020.

\bibitem[Yu et~al.(2018)Yu, Li, Chen, Lai, Morariu, Han, Gao, Lin, and
  Davis]{yu2018nisp}
Yu, Ruichi, Li, Ang, Chen, Chun-Fu, Lai, Jui-Hsin, Morariu, Vlad~I, Han,
  Xintong, Gao, Mingfei, Lin, Ching-Yung, and Davis, Larry~S.
\newblock Nisp: Pruning networks using neuron importance score propagation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  9194--9203, 2018.

\bibitem[Zhou et~al.(2020)Zhou, Ye, Chen, Meng, Tan, Song, Le, Liu, and
  Schuurmans]{zhou2020go}
Zhou, Denny, Ye, Mao, Chen, Chen, Meng, Tianjian, Tan, Mingxing, Song, Xiaodan,
  Le, Quoc, Liu, Qiang, and Schuurmans, Dale.
\newblock Go wide, then narrow: Efficient training of deep thin networks.
\newblock \emph{arXiv preprint arXiv:2007.00811}, 2020.

\bibitem[Zhuang et~al.(2018)Zhuang, Tan, Zhuang, Liu, Guo, Wu, Huang, and
  Zhu]{zhuang2018discrimination}
Zhuang, Zhuangwei, Tan, Mingkui, Zhuang, Bohan, Liu, Jing, Guo, Yong, Wu,
  Qingyao, Huang, Junzhou, and Zhu, Jinhui.
\newblock Discrimination-aware channel pruning for deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  875--886, 2018.

\end{thebibliography}
