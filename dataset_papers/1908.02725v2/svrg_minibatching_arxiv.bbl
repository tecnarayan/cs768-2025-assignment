\begin{thebibliography}{10}

\bibitem{Allenzhu2017-katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock In {\em Proceedings of the 49th Annual ACM SIGACT Symposium on Theory
  of Computing}, STOC 2017, pages 1200--1205, 2017.

\bibitem{allen-zhua16}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock In {\em Proceedings of The 33rd International Conference on Machine
  Learning}, volume~48, pages 699--707, 2016.

\bibitem{asuncion2007uci}
Arthur Asuncion and David Newman.
\newblock {UCI} machine learning repository, 2007.

\bibitem{bubeck2015convex}
S{\'e}bastien Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  8(3-4):231--357, 2015.

\bibitem{chang2011libsvm}
Chih-Chung Chang and Chih-Jen Lin.
\newblock Libsvm: A library for support vector machines.
\newblock {\em ACM transactions on intelligent systems and technology (TIST)},
  2(3):27, 2011.

\bibitem{SAGA}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In {\em Advances in Neural Information Processing Systems 27}, pages
  1646--1654. 2014.

\bibitem{SAGAminib}
Nidham Gazagnadou, Robert~Mansel Gower, and Joseph Salmon.
\newblock Optimal mini-batch and step sizes for saga.
\newblock {\em The International Conference on Machine Learning}, 2019.

\bibitem{SGD-AS}
Robert~M. Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin,
  and Peter Richt{\'a}rik.
\newblock Sgd: general analysis and improved rates.

\bibitem{JakSketch}
Robert~Mansel Gower, Peter Richt{\'a}rik, and Francis Bach.
\newblock Stochastic quasi-gradient methods: Variance reduction via jacobian
  sketching.
\newblock {\em arXiv:1805.02632}, 2018.

\bibitem{Konecny:wastinggrad:2015}
Reza Harikandeh, Mohamed~Osama Ahmed, Alim Virani, Mark Schmidt, Jakub
  Kone\v{c}n\'{y}, and Scott Sallinen.
\newblock Stopwasting my gradients: Practical svrg.
\newblock In {\em Advances in Neural Information Processing Systems 28}, pages
  2251--2259. 2015.

\bibitem{HofmanNSAGA}
Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, and Brian McWilliams.
\newblock Variance reduced stochastic gradient descent with neighbors.
\newblock In {\em Advances in Neural Information Processing Systems 28}, pages
  2305--2313. 2015.

\bibitem{SVRG-AS-nonconvex}
Samuel Horv{\'a}th and Peter Richt{\'a}rik.
\newblock Nonconvex variance reduced optimization with arbitrary sampling.

\bibitem{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  315--323, 2013.

\bibitem{Konecny2015}
Jakub Kone{\v{c}}n{\'{y}}, Jie Liu, Peter Richt{\'{a}}rik, and Martin
  Tak{\'{a}}{\v{c}}.
\newblock Mini-batch semi-stochastic gradient descent in the proximal setting.
\newblock {\em IEEE Journal of Selected Topics in Signal Processing},
  (2):242--255, 2016.

\bibitem{konevcny2013semi}
Jakub Kone{\v{c}}n{\'{y}} and Peter Richt{\'{a}}rik.
\newblock Semi-stochastic gradient descent methods.
\newblock {\em Frontiers in Applied Mathematics and Statistics}, 3:9, 2017.

\bibitem{SVRGloopless}
Dmitry Kovalev, Samuel Horvath, and Peter Richt{\'a}rik.
\newblock Don't jump through hoops and remove those loops: Svrg and katyusha
  are better without the outer loop.
\newblock {\em arXiv:1901.08689}, 2019.

\bibitem{mairal19}
Andrei Kulunchakov and Julien Mairal.
\newblock Estimate sequences for variance-reduced stochastic composite
  optimization.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, volume~97, pages 3541--3550, 2019.

\bibitem{Murata:2017}
Tomoya Murata and Taiji Suzuki.
\newblock Doubly accelerated stochastic variance reduced dual averaging method
  for regularized empirical risk minimization.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, NIPS'17, pages 608--617, 2017.

\bibitem{Nesterov-convex}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock 2013.

\bibitem{Nesterov-average}
Yurii Nesterov and Jean-Philippe Vial.
\newblock Confidence level solutions for stochastic programming.
\newblock In {\em Automatica}, volume~44, pages 1559--1568. 2008.

\bibitem{SARAH}
Lam~M. Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock {SARAH}: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, volume~70 of {\em Proceedings of Machine Learning Research}, pages
  2613--2621, Aug 2017.

\bibitem{Nitanda2014}
Atsushi Nitanda.
\newblock Stochastic proximal gradient descent with acceleration techniques.
\newblock In {\em Advances in Neural Information Processing Systems 27}, pages
  1574--1582. 2014.

\bibitem{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock {\em Journal of Machine Learning Research}, 12:2825--2830, 2011.

\bibitem{kSVRGstitch}
Anant Raj and Sebastian~U. Stich.
\newblock k-svrg: Variance reduction for large scale optimization.
\newblock {\em arXiv:1805.00982}.

\bibitem{Reddi2016}
Sashank~J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabás Póczos, and Alexander~J.
  Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, volume~48, pages 314--323, 2016.

\bibitem{robbins1985convergence}
Herbert Robbins and David Siegmund.
\newblock A convergence theorem for non negative almost supermartingales and
  some applications.
\newblock In {\em Herbert Robbins Selected Papers}, pages 111--135. 1985.

\bibitem{SAG}
Nicolas~L. Roux, Mark Schmidt, and Francis~R. Bach.
\newblock A stochastic gradient method with an exponential convergence \_rate
  for finite training sets.
\newblock In {\em Advances in Neural Information Processing Systems 25}, pages
  2663--2671. 2012.

\bibitem{shalev2013stochastic}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock {\em Journal of Machine Learning Research}, 14(Feb):567--599, 2013.

\end{thebibliography}
