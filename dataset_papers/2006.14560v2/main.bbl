\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bartol et~al.(2015)Bartol, Bromer, Kinney, Chirillo, Bourne, Harris,
  and Sejnowski]{nanoconnectome}
Thomas~M. Bartol, Jr., Cailey Bromer, Justin Kinney, Michael~A. Chirillo,
  Jennifer~N. Bourne, Kristen~M. Harris, and Terrence~J. Sejnowski.
\newblock Nanoconnectomic upper bound on the variability of synaptic
  plasticity.
\newblock \emph{eLife}, 2015.

\bibitem[{Baker} and {Hammerstrom}(1989)]{hammerstrom}
Tom {Baker} and Dan {Hammerstrom}.
\newblock Characterization of artificial neural network algorithms.
\newblock In \emph{International Symposium on Circuits and Systems}, 1989.

\bibitem[{Horowitz}(2014)]{horowitz}
Mark {Horowitz}.
\newblock Computing's energy problem (and what we can do about it).
\newblock In \emph{International Solid-State Circuits Conference}, 2014.

\bibitem[Sze et~al.(2017)Sze, Chen, Yang, and Emer]{sze}
Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel Emer.
\newblock Efficient processing of deep neural networks: A tutorial and survey.
\newblock \emph{Proceedings of the IEEE}, 2017.

\bibitem[Bernstein et~al.(2020)Bernstein, Vahdat, Yue, and Liu]{fromage2020}
Jeremy Bernstein, Arash Vahdat, Yisong Yue, and Ming-Yu Liu.
\newblock On the distance between two neural networks and the stability of
  learning.
\newblock In \emph{Neural Information Processing Systems}, 2020.

\bibitem[Littlestone(1988)]{winnow}
Nick Littlestone.
\newblock Learning quickly when irrelevant attributes abound: A new
  linear-threshold algorithm.
\newblock \emph{Machine Learning}, 1988.

\bibitem[Kivinen and Warmuth(1997)]{manfred}
Jyrki Kivinen and Manfred~K. Warmuth.
\newblock Exponentiated gradient versus gradient descent for linear predictors.
\newblock \emph{Information and Computation}, 1997.

\bibitem[Freund and Schapire(1997)]{adaboost}
Yoav Freund and Robert~E Schapire.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{Journal of Computer and System Sciences}, 1997.

\bibitem[Grigoriadis and Khachiyan(1995)]{games}
Michael~D. Grigoriadis and Leonid~G. Khachiyan.
\newblock A sublinear-time randomized approximation algorithm for matrix games.
\newblock \emph{Operations Research Letters}, 1995.

\bibitem[Arora et~al.(2012)Arora, Hazan, and Kale]{Arora2012TheMW}
Sanjeev Arora, Elad Hazan, and Satyen Kale.
\newblock The multiplicative weights update method: a meta-algorithm and
  applications.
\newblock \emph{Theory of Computing}, 2012.

\bibitem[Dhillon and Tropp(2008)]{tropp}
Inderjit~S. Dhillon and Joel~A. Tropp.
\newblock Matrix nearness problems with {B}regman divergences.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 2008.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{bottou}
LÃ©on Bottou, Frank~E. Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{SIAM Review}, 2018.

\bibitem[Zhang et~al.(2020)Zhang, He, Sra, and Jadbabaie]{Zhang2020Why}
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie.
\newblock Why gradient clipping accelerates training: A theoretical
  justification for adaptivity.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Azizan et~al.(2019)Azizan, Lale, and Hassibi]{azizan2019stochastic}
Navid Azizan, Sahin Lale, and Babak Hassibi.
\newblock Stochastic mirror descent on overparameterized nonlinear models:
  Convergence, implicit regularization, and generalization, 2019.
\newblock arXiv:1906.03830.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Salakhutdinov, and Srebro]{pathsgd}
Behnam Neyshabur, Ruslan Salakhutdinov, and Nathan Srebro.
\newblock Path-{SGD}: Path-normalized optimization in deep neural networks.
\newblock In \emph{Neural Information Processing Systems}, 2015.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{You:EECS-2017-156}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Scaling {SGD} batch size to 32{K} for {I}magenet training.
\newblock Technical Report UCB/EECS-2017-156, University of California,
  Berkeley, 2017.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{You2020Large}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training {BERT} in 76
  minutes.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[van Vreeswijk and Sompolinsky(1996)]{van_Vreeswijk1724}
Carl van Vreeswijk and Haim Sompolinsky.
\newblock Chaos in neuronal networks with balanced excitatory and inhibitory
  activity.
\newblock \emph{Science}, 1996.

\bibitem[Amit et~al.(1989)Amit, Wong, and Campbell]{Amit_1989}
Daniel~J. Amit, K.~Y.~Michael Wong, and Colin Campbell.
\newblock Perceptron learning with sign-constrained weights.
\newblock \emph{Journal of Physics A}, 1989.

\bibitem[{Akira Iwata} et~al.(1989){Akira Iwata}, {Yukio Yoshida}, {Satoshi
  Matsuda}, {Yukimasa Sato}, and {Nobuo Suzumura}]{iwata}
{Akira Iwata}, {Yukio Yoshida}, {Satoshi Matsuda}, {Yukimasa Sato}, and {Nobuo
  Suzumura}.
\newblock An artificial neural network accelerator using general purpose 24 bit
  floating point digital signal processors.
\newblock In \emph{International Joint Conference on Neural Networks}, 1989.

\bibitem[{Holt} and {Hwang}(1993)]{holt}
Jordan~L. {Holt} and Jenq-Neng {Hwang}.
\newblock Finite precision error analysis of neural network hardware
  implementations.
\newblock \emph{Transactions on Computers}, 1993.

\bibitem[Courbariaux et~al.(2015)Courbariaux, Bengio, and David]{binaryconnect}
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
\newblock Binary{C}onnect: Training deep neural networks with binary weights
  during propagations.
\newblock In \emph{Neural Information Processing Systems}, 2015.

\bibitem[Hubara et~al.(2018)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{hubara}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Quantized neural networks: Training neural networks with low
  precision weights and activations.
\newblock \emph{Journal of Machine Learning Research}, 2018.

\bibitem[Zhou et~al.(2017)Zhou, Yao, Guo, Xu, and Chen]{Zhou2017IncrementalNQ}
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen.
\newblock Incremental network quantization: Towards lossless {CNN}s with
  low-precision weights, 2017.
\newblock arXiv:1702.03044.

\bibitem[Gupta et~al.(2015)Gupta, Agrawal, Gopalakrishnan, and Narayanan]{fp16}
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
\newblock Deep learning with limited numerical precision.
\newblock In \emph{International Conference on Machine Learning}, 2015.

\bibitem[M{\"u}ller and Indiveri(2015)]{rounding}
Lorenz~K. M{\"u}ller and Giacomo Indiveri.
\newblock Rounding methods for neural networks with low resolution synaptic
  weights, 2015.
\newblock arXiv:1504.05767.

\bibitem[Sun et~al.(2019)Sun, Choi, Chen, Wang, Venkataramani, Srinivasan, Cui,
  Zhang, and Gopalakrishnan]{hpf8}
Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani,
  Vijayalakshmi Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash
  Gopalakrishnan.
\newblock Hybrid 8-bit floating point ({HFP}8) training and inference for deep
  neural networks.
\newblock In \emph{Neural Information Processing Systems}, 2019.

\bibitem[Wang et~al.(2018)Wang, Choi, Brand, Chen, and Gopalakrishnan]{8bit}
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash
  Gopalakrishnan.
\newblock Training deep neural networks with 8-bit floating point numbers.
\newblock In \emph{Neural Information Processing Systems}, 2018.

\bibitem[Wu et~al.(2018)Wu, Li, Chen, and Shi]{wage}
Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi.
\newblock Training and inference with integers in deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[{Lee} et~al.(2017){Lee}, {Miyashita}, {Chai}, {Murmann}, and
  {Wong}]{lognet}
Edward~H. {Lee}, Daisuke {Miyashita}, Elaina {Chai}, Boris {Murmann}, and
  S.~Simon {Wong}.
\newblock Log{N}et: Energy-efficient neural networks using logarithmic
  computation.
\newblock In \emph{International Conference on Acoustics, Speech and Signal
  Processing}, 2017.

\bibitem[Vogel et~al.(2018)Vogel, Liang, Guntoro, Stechele, and
  Ascheid]{otherlog}
Sebastian Vogel, Mengyu Liang, Andre Guntoro, Walter Stechele, and Gerd
  Ascheid.
\newblock Efficient hardware acceleration of {CNN}s using logarithmic data
  representation with arbitrary log-base.
\newblock In \emph{International Conference on Computer-Aided Design}, 2018.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and Williams]{backprop}
David~E. Rumelhart, Geoffrey~E. Hinton, and Ronald~J. Williams.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Nature}, 1986.

\bibitem[Huang et~al.(2017)Huang, Liu, {Laurens van der Maaten}, and {Kilian
  Q.\ Weinberger}]{densenet}
Gao Huang, Zhuang Liu, {Laurens van der Maaten}, and {Kilian Q.\ Weinberger}.
\newblock Densely connected convolutional networks.
\newblock In \emph{Computer Vision and Pattern Recognition}, 2017.

\bibitem[Kingma and Ba(2015)]{kingma_adam:_2015}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} {Method} for {Stochastic} {Optimization}.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Tieleman and Hinton(2012)]{Tieleman2012}
Tijmen Tieleman and Geoffrey~E. Hinton.
\newblock {Lecture 6.5---RMSprop: Divide the gradient by a running average of
  its recent magnitude}.
\newblock COURSERA: Neural Networks for Machine Learning, 2012.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{ttur}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock {GAN}s trained by a two time-scale update rule converge to a local
  {N}ash equilibrium.
\newblock In \emph{Neural Information Processing Systems}, 2017.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Neural Information Processing Systems}, 2019.

\bibitem[Kalamkar et~al.(2019)Kalamkar, Mudigere, Mellempudi, Das, Banerjee,
  Avancha, Vooturi, Jammalamadaka, Huang, Yuen, Yang, Park, Heinecke,
  Georganas, Srinivasan, Kundu, Smelyanskiy, Kaul, and Dubey]{bfloat16}
Dhiraj~D. Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal
  Banerjee, Sasikanth Avancha, Dharma~Teja Vooturi, Nataraj Jammalamadaka,
  Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander Heinecke,
  Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy,
  Bharat Kaul, and Pradeep Dubey.
\newblock A study of bfloat16 for deep learning training, 2019.
\newblock arXiv:1905.12322.

\bibitem[Micikevicius et~al.(2018)Micikevicius, Narang, Alben, Diamos, Elsen,
  Garcia, Ginsburg, Houston, Kuchaiev, Venkatesh, and Wu]{amp}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,
  David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
  Venkatesh, and Hao Wu.
\newblock Mixed precision training.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[van Rossum et~al.(2000)van Rossum, Bi, and Turrigiano]{vanrossum}
Mark~C. van Rossum, Guo-qiang Bi, and Gina~G. Turrigiano.
\newblock Stable {H}ebbian learning from spike timing-dependent plasticity.
\newblock \emph{Journal of Neuroscience}, 2000.

\bibitem[Barbour et~al.(2007)Barbour, Brunel, Hakim, and Nadal]{barbour}
Boris Barbour, Nicolas Brunel, Vincent Hakim, and Jean-Pierre Nadal.
\newblock What can we learn from synaptic weight distributions?
\newblock \emph{Trends in Neurosciences}, 2007.

\bibitem[Buzs{\'a}ki and Mizuseki(2014)]{Buzski2014TheLB}
Gy{\"o}rgy Buzs{\'a}ki and Kenji Mizuseki.
\newblock The log-dynamic brain: how skewed distributions affect network
  operations.
\newblock \emph{Nature Reviews Neuroscience}, 2014.

\bibitem[Loewenstein et~al.(2011)Loewenstein, Kuras, and
  Rumpel]{Loewenstein9481}
Yonatan Loewenstein, Annerose Kuras, and Simon Rumpel.
\newblock Multiplicative dynamics underlie the emergence of the log-normal
  distribution of spine sizes in the neocortex in vivo.
\newblock \emph{Journal of Neuroscience}, 2011.

\bibitem[Krizhevsky(2009)]{cifar}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[{He} et~al.(2016){He}, {Zhang}, {Ren}, and {Sun}]{resnet}
Kaiming {He}, Xiangyu {Zhang}, Shaoqing {Ren}, and Jian {Sun}.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Computer Vision and Pattern Recognition}, 2016.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{Computer Vision and Pattern Recognition}, 2009.

\bibitem[Brock et~al.(2019)Brock, Donahue, and Simonyan]{brock2018large}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale {GAN} training for high fidelity natural image synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and Socher]{wikitext}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Neural Information Processing Systems}, 2017.

\end{thebibliography}
