\begin{thebibliography}{10}

\bibitem{ServeyKD_IJCV2021}
Jianping Gou, Baosheng Yu, Stephen~J Maybank, and Dacheng Tao.
\newblock Knowledge distillation: A survey.
\newblock {\em International Journal of Computer Vision}, 129(6):1789--1819, 2021.

\bibitem{KD_arXiv2015}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{DKD_CVPR2022}
Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang.
\newblock Decoupled knowledge distillation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11953--11962, 2022.

\bibitem{NKD_ICCV2023}
Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu~Li.
\newblock From knowledge distillation to self-knowledge distillation: A unified approach with normalized loss and customized soft labels.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 17185--17194, 2023.

\bibitem{WTTM_ICLR2024}
Kaixiang Zheng and EN-HUI YANG.
\newblock Knowledge distillation based on transformed teacher matching.
\newblock In {\em International Conference on Learning Representations}, 2024.

\bibitem{ICKD_ICCV2021}
Li~Liu, Qingle Huang, Sihao Lin, Hongwei Xie, Bing Wang, Xiaojun Chang, and Xiaodan Liang.
\newblock Exploring inter-channel correlation for diversity-preserved knowledge distillation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 8271--8280, 2021.

\bibitem{DPK_ICLR2023}
Martin Zong, Zengyu Qiu, Xinzhu Ma, Kunlin Yang, Chunya Liu, Jun Hou, Shuai Yi, and Wanli Ouyang.
\newblock Better teacher better student: Dynamic prior knowledge for knowledge distillation.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{FCFD_ICLR2023}
Dongyang Liu, Meina Kan, Shiguang Shan, and CHEN Xilin.
\newblock Function-consistent feature distillation.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{ResNet_CVPR2016}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages 770--778, 2016.

\bibitem{Book-Hastie}
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
\newblock {\em The Elements of Statistial Learning}.
\newblock Springer, 2009.

\bibitem{WGAN_ICML2017}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In {\em International Conference on Machine Learning}, pages 214--223, 2017.

\bibitem{pmlr-v25-aboumoustafa12}
Karim~T. Abou-Moustafa and Frank~P. Ferrie.
\newblock A note on metric properties for some divergence measures: The {Gaussian} case.
\newblock In {\em Asian Conference on Machine Learning}, pages 1--15, 2012.

\bibitem{WDM_NeuriIPS2019}
Sherjil Ozair, Corey Lynch, Yoshua Bengio, A\"{a}ron van~den Oord, Sergey Levine, and Pierre Sermanet.
\newblock Wasserstein dependency measure for representation learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 15578--15588, 2019.

\bibitem{MAL-073}
Gabriel Peyr\'e and Marco Cuturi.
\newblock Computational optimal transport: With applications to data science.
\newblock {\em Foundations and Trends in Machine Learning}, 11(5-6):355--607, 2019.

\bibitem{WCoRD_CVPR2021}
Liqun Chen, Dong Wang, Zhe Gan, Jingjing Liu, Ricardo Henao, and Lawrence Carin.
\newblock Wasserstein contrastive representation distillation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 16296--16305, 2021.

\bibitem{REMD_WACV2022}
Suhas Lohit and Michael Jones.
\newblock Model compression using optimal transport.
\newblock In {\em Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, pages 2764--2773, 2022.

\bibitem{CKA-ICML2019}
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.
\newblock Similarity of neural network representations revisited.
\newblock In {\em International Conference on Machine Learning}, pages 3519--3529, 2019.

\bibitem{10.5555/2503308.2188413}
Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh.
\newblock Algorithms for learning kernels based on centered alignment.
\newblock {\em The Journal of Machine Learning Research}, 13(28):795--828, 2012.

\bibitem{Book-PRML}
Christopher Bishop.
\newblock {\em Pattern Recognition and Machine Learning}.
\newblock Springer, 2006.

\bibitem{Wassertein-Gaussian_2018}
Luigi Malago, Luigi Montrucchio, and Giovanni Pistone.
\newblock Wasserstein {Riemannian} geometry of {Gaussian} densities.
\newblock {\em Information Geometry}, 1(2):137--179, 2018.

\bibitem{HSIC_2005}
Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch{\"o}lkopf.
\newblock Measuring statistical dependence with hilbert-schmidt norms.
\newblock In {\em International Conference on Algorithmic Learning Theory}, pages 63--77, 2005.

\bibitem{Papyan2020PrevalenceON}
Vardan Papyan, Xuemei Han, and David~L. Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep learning training.
\newblock {\em Proceedings of the National Academy of Sciences}, 117(40):24652--24663, 2020.

\bibitem{Sinkhorn_NIPS2013}
Marco Cuturi.
\newblock Sinkhorn distances: Lightspeed computation of optimal transport.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 2292--2300, 2013.

\bibitem{FitNets_ICLR2015}
Adriana Romero, Nicolas Ballas, Samira~Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{CRD_ICLR2020}
Yonglong Tian, Dilip Krishnan, and Phillip Isola.
\newblock Contrastive representation distillation.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{chen2022improved}
Yudong Chen, Sen Wang, Jiajun Liu, Xuwei Xu, Frank de~Hoog, and Zi~Huang.
\newblock Improved feature distillation via projector ensemble.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 12084--12095, 2022.

\bibitem{ledoit2004wellconditioned}
Olivier Ledoit and Michael Wolf.
\newblock {\em Journal of Multivariate Analysis}, 88(2):365--411, 2004.

\bibitem{SPP-ECCV14}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Spatial pyramid pooling in deep convolutional networks for visual recognition.
\newblock In {\em European Conference on Computer Vision}, pages 346--361, 2014.

\bibitem{ReivewKD_CVPR2021}
Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia.
\newblock Distilling knowledge via knowledge review.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 5008--5017, 2021.

\bibitem{kullback1951information}
Solomon Kullback and Richard~A Leibler.
\newblock On information and sufficiency.
\newblock {\em The Annals of Mathematical Statistics}, 22(1):79--86, 1951.

\bibitem{jeffreys1946theory}
Harold Jeffreys.
\newblock An invariant form for the prior probability in estimation problems.
\newblock {\em Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences}, 186(1007):453--461, 1946.

\bibitem{Zhou-Chellappa-PAMI-2006}
Shaohua~Kevin Zhou and Rama Chellappa.
\newblock From sample similarity to ensemble similarity: probabilistic distance measures in reproducing kernel hilbert space.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 28(6):917--929, 2006.

\bibitem{MPN-COV_PAMI2021}
Qilong Wang, Jiangtao Xie, Wangmeng Zuo, Lei Zhang, and Peihua Li.
\newblock Deep {CNNs} meet global covariance pooling: Better representation and generalization.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 43(8):2582--2597, 2021.

\bibitem{IPOT_2020}
Yujia Xie, Xiangfeng Wang, Ruijia Wang, and Hongyuan Zha.
\newblock A fast proximal point method for computing exact {Wasserstein} distance.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 433--453, 2020.

\bibitem{NST2017}
Zehao Huang and Naiyan Wang.
\newblock Like what you like: Knowledge distill via neuron selectivity transfer.
\newblock {\em arXiv preprint arXiv:1707.01219}, 2017.

\bibitem{AT_ICLR2017}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{yang2020knowledge}
Jing Yang, Brais Martinez, Adrian Bulat, and Georgios Tzimiropoulos.
\newblock Knowledge distillation via adaptive instance normalization.
\newblock {\em arXiv preprint arXiv:2003.04289}, 2020.

\bibitem{SPD-book-2015}
Rajendra Bhatia.
\newblock {\em Positive Definite Matrices}.
\newblock Princeton University Press, 2015.

\bibitem{Geometry-aware-2018}
Mehrtash Harandi, Mathieu Salzmann, and Richard Hartley.
\newblock Dimensionality reduction on {SPD} manifolds: The emergence of geometry-aware methods.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 40(1):48--62, 2018.

\bibitem{VID_CVPR2019}
Sungsoo Ahn, Shell~Xu Hu, Andreas Damianou, Neil~D. Lawrence, and Zhenwen Dai.
\newblock Variational information distillation for knowledge transfer.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 9163--9171, 2019.

\bibitem{ImageNet_CVPR2009}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock {ImageNet}: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE Conference on Computer Vision and Pattern Recognition}, pages 248--255, 2009.

\bibitem{CIFAR-100_2019}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{MSCOCO_ECCV2014}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European Conference on Computer Vision}, pages 740--755, 2014.

\bibitem{Pytorch_NIPS2019}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{flamary2021pot}
R{\'e}mi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar~Z. Alaya, Aur{\'e}lie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, L{\'e}o Gautheron, Nathalie~T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica~J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer.
\newblock {POT}: Python optimal transport.
\newblock {\em The Journal of Machine Learning Research}, 22(78):1--8, 2021.

\bibitem{OFA_NIPS2023}
Zhiwei Hao, Jianyuan Guo, Kai Han, Yehui Tang, Han Hu, Yunhe Wang, and Chang Xu.
\newblock One-for-all: Bridge the gap between heterogeneous architectures in knowledge distillation.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~36, pages 79570--79582, 2023.

\bibitem{Faster-RCNN_PAMI2017}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster {R-CNN}: Towards real-time object detection with region proposal networks.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 39(6):1137--1149, 2017.

\bibitem{FPN_CVPR2017}
Tsung-Yi Lin, Piotr Doll{\'a}r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.
\newblock Feature pyramid networks for object detection.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages 2117--2125, 2017.

\bibitem{wu2019detectron2}
Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick.
\newblock Detectron2.
\newblock \url{https://github.com/facebookresearch/detectron2}, 2019.

\bibitem{FGFI_CVPR2019}
Tao Wang, Li~Yuan, Xiaopeng Zhang, and Jiashi Feng.
\newblock Distilling object detectors with fine-grained feature imitation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 4933--4942, 2019.

\bibitem{ICD_NeurIPS2021}
Zijian Kang, Peizhen Zhang, Xiangyu Zhang, Jian Sun, and Nanning Zheng.
\newblock Instance-conditional knowledge distillation for object detection.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 16468--16480, 2021.

\bibitem{icml2014c2_yangd14}
Eunho Yang, Aurelie Lozano, and Pradeep Ravikumar.
\newblock Elementary estimators for sparse covariance matrices and other structured moments.
\newblock In {\em International Conference on Machine Learning}, pages 397--405, 2014.

\bibitem{Gil_MS_2011}
Manuel Gil.
\newblock On rényi divergence measures for continuous alphabet sources.
\newblock Master thesis, 2011.

\bibitem{CTKD_AAAI2023}
Zheng Li, Xiang Li, Lingfeng Yang, Borui Zhao, Renjie Song, Lei Luo, Jun Li, and Jian Yang.
\newblock Curriculum temperature for knowledge distillation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 1504--1512, 2023.

\bibitem{CAT_CVPR2023}
Ziyao Guo, Haonan Yan, Hui Li, and Xiaodong Lin.
\newblock Class attention transfer based knowledge distillation.
\newblock In {\em Croceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11868--11877, 2023.

\bibitem{KD_ZERO_NIPS2023}
Lujun Li, Peijie Dong, Anggeng Li, Zimian Wei, and Ya~Yang.
\newblock Kd-zero: Evolving knowledge distiller for any teacher-student pairs.
\newblock {\em Advances in Neural Information Processing Systems}, pages 69490--69504, 2023.

\bibitem{MobileNetV1_arXiv2017}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{MobileNetV2_CVPR2018}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages 4510--4520, 2018.

\bibitem{ConvNeXt_CVPR2022}
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.
\newblock A convnet for the 2020s.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11976--11986, 2022.

\bibitem{ViT_ICLR2020}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{DeiT_ICML2021}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through attention.
\newblock In {\em International Conference on Machine Learning}, pages 10347--10357. PMLR, 2021.

\bibitem{Swin_CVPR2021}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 10012--10022, 2021.

\bibitem{DIST_NIPS2022}
Tao Huang, Shan You, Fei Wang, Chen Qian, and Chang Xu.
\newblock Knowledge distillation from a stronger teacher.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 33716--33727, 2022.

\bibitem{CC_ICCV2019}
Baoyun Peng, Xiao Jin, Jiaheng Liu, Dongsheng Li, Yichao Wu, Yu~Liu, Shunfeng Zhou, and Zhaoning Zhang.
\newblock Correlation congruence for knowledge distillation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 5007--5016, 2019.

\bibitem{RKD_CVPR2019}
Wonpyo Park, Dongju Kim, Yan Lu, and Minsu Cho.
\newblock Relational knowledge distillation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 3967--3976, 2019.

\bibitem{BAN_ICML2018}
Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.
\newblock Born again neural networks.
\newblock In {\em International conference on machine learning}, pages 1607--1616. PMLR, 2018.

\bibitem{TFKD_CVPR2020}
Li~Yuan, Francis~EH Tay, Guilin Li, Tao Wang, and Jiashi Feng.
\newblock Revisiting knowledge distillation via label smoothing regularization.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 3903--3911, 2020.

\bibitem{FRSKD_CVPR2021}
Mingi Ji, Seungjae Shin, Seunghyun Hwang, Gibeom Park, and Il-Chul Moon.
\newblock Refine myself by teaching myself: Feature refinement via self-knowledge distillation.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10664--10673, 2021.

\bibitem{zipf_ECCV2022}
Jiajun Liang, Linze Li, Zhaodong Bing, Borui Zhao, Yao Tang, Bo~Lin, and Haoqiang Fan.
\newblock Efficient one pass self-distillation with zipf’s label smoothing.
\newblock In {\em European conference on computer vision}, pages 104--119. Springer, 2022.

\bibitem{Near_linear_OT_NIPS2017}
Jason Altschuler, Jonathan Niles-Weed, and Philippe Rigollet.
\newblock Near-linear time approximation algorithms for optimal transport via sinkhorn iteration.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~30, 2017.

\bibitem{MGD_ECCV2022}
Zhendong Yang, Zhe Li, Mingqi Shao, Dachuan Shi, Zehuan Yuan, and Chun Yuan.
\newblock Masked generative distillation.
\newblock In {\em European Conference on Computer Vision}, pages 53--69, 2022.

\bibitem{DiffKD_NIPS2023}
Tao Huang, Yuan Zhang, Mingkai Zheng, Shan You, Fei Wang, Chen Qian, and Chang Xu.
\newblock Knowledge diffusion for distillation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 65299--65316, 2023.

\bibitem{MLKD_CVPR2023}
Ying Jin, Jiaqi Wang, and Dahua Lin.
\newblock Multi-level logit distillation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 24276--24285, 2023.

\bibitem{Randaugment_CVPRW2020}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced search space.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops}, pages 702--703, 2020.

\bibitem{VanillaKD_NIPS2023}
Zhiwei Hao, Jianyuan Guo, Kai Han, Han Hu, Chang Xu, and Yunhe Wang.
\newblock Revisit the power of vanilla knowledge distillation: from small scale to large scale.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, {\em Advances in Neural Information Processing Systems}, volume~36, pages 10170--10183. Curran Associates, Inc., 2023.

\bibitem{Wide-ResNet_BMVC2016}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In {\em Proceedings of the British Machine Vision Conference}, 2016.

\bibitem{VGG_ICLR2015}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{ShuffleNet_CVPR2018}
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
\newblock Shufflenet: An extremely efficient convolutional neural network for mobile devices.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages 6848--6856, 2018.

\bibitem{GradCAM_ICCV17}
Ramprasaath~R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based localization.
\newblock In {\em Proceedings of the IEEE International Conference on Computer Vision}, pages 618--626, 2017.

\bibitem{DIOU_AAAI2020}
Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and Dongwei Ren.
\newblock Distance-iou loss: Faster and better learning for bounding box regression.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, pages 12993--13000, 2020.

\bibitem{9157663}
Li~Yuan, Francis~EH Tay, Guilin Li, Tao Wang, and Jiashi Feng.
\newblock Revisiting knowledge distillation via label smoothing regularization.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 3902--3910, 2020.

\bibitem{NEURIPS2020_1457c0d6}
Tom Brown, Benjamin Mann, Nick Ryder, and et~al.
\newblock Language models are few-shot learners.
\newblock In {\em Advances in Neural Information Processing Systems}, volume~33, pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock {GPT-4} technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{ojha2023what}
Utkarsh Ojha, Yuheng Li, Anirudh~Sundara Rajan, Yingyu Liang, and Yong~Jae Lee.
\newblock What knowledge gets distilled in knowledge distillation?
\newblock In {\em Advances in Neural Information Processing Systems}, 2023.

\end{thebibliography}
