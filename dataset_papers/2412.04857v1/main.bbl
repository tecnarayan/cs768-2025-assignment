\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Lewkowycz et~al.(2022{\natexlab{a}})Lewkowycz, Andreassen, Dohan, Dyer, Michalewski, Ramasesh, Slone, Anil, Schlag, Gutman{-}Solo, Wu, Neyshabur, Gur{-}Ari, and Misra]{LewkowyczADDMRS22}
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay~V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman{-}Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur{-}Ari, and Vedant Misra.
\newblock Solving quantitative reasoning problems with language models.
\newblock In \emph{Advances in Neural Information Processing Systems 35}, 2022{\natexlab{a}}.

\bibitem[d'Avila Garcez and Lamb(2023)]{GarcezL23}
Artur d'Avila Garcez and Lu{\'{\i}}s~C. Lamb.
\newblock Neurosymbolic {AI:} the 3rd wave.
\newblock \emph{Artificial Intelligence Review}, 56\penalty0 (11):\penalty0 12387--12406, 2023.

\bibitem[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, et~al.]{zhao2023survey}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al.
\newblock A survey of large language models.
\newblock \emph{arXiv preprint arXiv:2303.18223}, 2023.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Ma, Feng, Zhang, Yang, Zhang, Chen, Tang, Chen, Lin, et~al.]{wang2023survey}
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu~Chen, Yankai Lin, et~al.
\newblock A survey on large language model based autonomous agents.
\newblock \emph{arXiv preprint arXiv:2308.11432}, 2023{\natexlab{a}}.

\bibitem[Kasneci et~al.(2023)Kasneci, Se{\ss}ler, K{\"u}chemann, Bannert, Dementieva, Fischer, Gasser, Groh, G{\"u}nnemann, H{\"u}llermeier, et~al.]{kasneci2023chatgpt}
Enkelejda Kasneci, Kathrin Se{\ss}ler, Stefan K{\"u}chemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan G{\"u}nnemann, Eyke H{\"u}llermeier, et~al.
\newblock Chatgpt for good? on opportunities and challenges of large language models for education.
\newblock \emph{Learning and individual differences}, 103:\penalty0 102274, 2023.

\bibitem[Chang et~al.(2023)Chang, Wang, Wang, Wu, Yang, Zhu, Chen, Yi, Wang, Wang, et~al.]{chang2023survey}
Yupeng Chang, Xu~Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et~al.
\newblock A survey on evaluation of large language models.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology}, 2023.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Hu, Lu, Zhu, Zhang, Subramaniam, Loomba, Zhang, Sun, and Wang]{wang2023scibench}
Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun~R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang.
\newblock Scibench: Evaluating college-level scientific problem-solving abilities of large language models.
\newblock \emph{arXiv preprint arXiv:2307.10635}, 2023{\natexlab{b}}.

\bibitem[Shakarian et~al.(2023)Shakarian, Koyyalamudi, Ngu, and Mareedu]{shakarian2023independent}
Paulo Shakarian, Abhinav Koyyalamudi, Noel Ngu, and Lakshmivihari Mareedu.
\newblock An independent evaluation of chatgpt on mathematical word problems (mwp).
\newblock \emph{arXiv preprint arXiv:2302.13814}, 2023.

\bibitem[Zhu et~al.(2023)Zhu, Chen, Wang, Gong, Yang, and Xie]{zhu03dyval}
Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil~Zhenqiang Gong, Diyi Yang, and Xing Xie.
\newblock Dyval: Graph-informed dynamic evaluation of large language models.
\newblock \emph{CoRR}, abs/2309.17167, 2023.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{Cobbe2021gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang, Song, and Steinhardt]{hendrycks2021math}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock In \emph{Advances in Neural Information Processing Systems Track on Datasets and Benchmarks}, 2021.

\bibitem[Patel et~al.(2021)Patel, Bhattamishra, and Goyal]{patel2021svamp}
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
\newblock Are nlp models really able to solve simple math word problems?
\newblock \emph{arXiv preprint arXiv:2103.07191}, 2021.

\bibitem[Azerbayev et~al.(2023)Azerbayev, Schoelkopf, Paster, Santos, McAleer, Jiang, Deng, Biderman, and Welleck]{azerbayev2023llemma}
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco~Dos Santos, Stephen McAleer, Albert~Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck.
\newblock Llemma: An open language model for mathematics.
\newblock \emph{arXiv preprint arXiv:2310.10631}, 2023.

\bibitem[Feng et~al.(2021)Feng, He, and Yin]{feng2021sampling}
Weiming Feng, Kun He, and Yitong Yin.
\newblock Sampling constraint satisfaction solutions in the local lemma regime.
\newblock In \emph{Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing}, pages 1565--1578, 2021.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Yao, Chen, Xu, Cao, Ma, Jian, et~al.]{li2022softened}
Zenan Li, Yuan Yao, Taolue Chen, Jingwei Xu, Chun Cao, Xiaoxing Ma, L~Jian, et~al.
\newblock Softened symbol grounding for neuro-symbolic systems.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Canton{-}Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{hugo23LLaMA2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton{-}Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie{-}Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aur{\'{e}}lien Rodriguez, Robert Stojnic, Sergey Edunov,
  and Thomas Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{CoRR}, abs/2307.09288, 2023.

\bibitem[Jiang et~al.(2023{\natexlab{a}})Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~Las~Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{jiang23misral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~Las~Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L{\'{e}}lio~Renard Lavaud, Marie{-}Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timoth{\'{e}}e Lacroix, and William~El Sayed.
\newblock Mistral 7b.
\newblock \emph{CoRR}, abs/2310.06825, 2023{\natexlab{a}}.

\bibitem[Miao et~al.(2020)Miao, Liang, and Su]{MiaoLS20asdiv}
Shen{-}Yun Miao, Chao{-}Chun Liang, and Keh{-}Yih Su.
\newblock A diverse corpus for evaluating and developing english math word problem solvers.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 975--984, 2020.

\bibitem[Barrett et~al.(2017)Barrett, Fontaine, and Tinelli]{BarFT-RR-17}
Clark Barrett, Pascal Fontaine, and Cesare Tinelli.
\newblock {The SMT-LIB Standard: Version 2.6}.
\newblock Technical report, Department of Computer Science, The University of Iowa, 2017.
\newblock Available at {\tt www.SMT-LIB.org}.

\bibitem[De~Moura and Bj{\o}rner(2008)]{de2008z3}
Leonardo De~Moura and Nikolaj Bj{\o}rner.
\newblock Z3: An efficient smt solver.
\newblock In \emph{International conference on Tools and Algorithms for the Construction and Analysis of Systems}, pages 337--340. Springer, 2008.

\bibitem[Barbosa et~al.(2022)Barbosa, Barrett, Brain, Kremer, Lachnitt, Mann, Mohamed, Mohamed, Niemetz, N{\"o}tzli, et~al.]{barbosa2022cvc5}
Haniel Barbosa, Clark Barrett, Martin Brain, Gereon Kremer, Hanna Lachnitt, Makai Mann, Abdalrhman Mohamed, Mudathir Mohamed, Aina Niemetz, Andres N{\"o}tzli, et~al.
\newblock cvc5: A versatile and industrial-strength smt solver.
\newblock In \emph{International Conference on Tools and Algorithms for the Construction and Analysis of Systems}, pages 415--442. Springer, 2022.

\bibitem[Bruttomesso et~al.(2008)Bruttomesso, Cimatti, Franz{\'e}n, Griggio, and Sebastiani]{bruttomesso2008mathsat}
Roberto Bruttomesso, Alessandro Cimatti, Anders Franz{\'e}n, Alberto Griggio, and Roberto Sebastiani.
\newblock The mathsat 4 smt solver: Tool paper.
\newblock In \emph{Computer Aided Verification: 20th International Conference, CAV 2008 Princeton, NJ, USA, July 7-14, 2008 Proceedings 20}, pages 299--303. Springer, 2008.

\bibitem[Meurer et~al.(2017)Meurer, Smith, Paprocki, {\v{C}}ert{\'\i}k, Kirpichev, Rocklin, Kumar, Ivanov, Moore, Singh, et~al.]{meurer2017sympy}
Aaron Meurer, Christopher~P Smith, Mateusz Paprocki, Ond{\v{r}}ej {\v{C}}ert{\'\i}k, Sergey~B Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason~K Moore, Sartaj Singh, et~al.
\newblock Sympy: symbolic computing in python.
\newblock \emph{PeerJ Computer Science}, 3:\penalty0 e103, 2017.

\bibitem[Virtanen et~al.(2020)Virtanen, Gommers, Oliphant, Haberland, Reddy, Cournapeau, Burovski, Peterson, Weckesser, Bright, et~al.]{virtanen2020scipy}
Pauli Virtanen, Ralf Gommers, Travis~E Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, et~al.
\newblock Scipy 1.0: fundamental algorithms for scientific computing in python.
\newblock \emph{Nature methods}, 17\penalty0 (3):\penalty0 261--272, 2020.

\bibitem[De~Moura and Passmore(2013)]{de2013strategy}
Leonardo De~Moura and Grant~Olney Passmore.
\newblock The strategy challenge in smt solving.
\newblock In \emph{Automated Reasoning and Mathematics: Essays in Memory of William W. McCune}, pages 15--44. Springer, 2013.

\bibitem[Valiant(1979)]{valiant1979complexity}
Leslie~G Valiant.
\newblock The complexity of enumeration and reliability problems.
\newblock \emph{SIAM Journal on Computing}, 8\penalty0 (3):\penalty0 410--421, 1979.

\bibitem[Jerrum and Sinclair(1996)]{jerrum1996markov}
Mark Jerrum and Alistair Sinclair.
\newblock The markov chain monte carlo method: an approach to approximate counting and integration.
\newblock \emph{Approximation Algorithms for NP-hard problems, PWS Publishing}, 1996.

\bibitem[Ermon et~al.(2012)Ermon, Gomes, and Selman]{ermon2012uniform}
Stefano Ermon, Carla Gomes, and Bart Selman.
\newblock Uniform solution sampling using a constraint solver as an oracle.
\newblock In \emph{Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence}, pages 255--264, 2012.

\bibitem[Wu et~al.(2022)Wu, Jiang, Li, Rabe, Staats, Jamnik, and Szegedy]{wu2022autoformalization}
Yuhuai Wu, Albert~Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, and Christian Szegedy.
\newblock Autoformalization with large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 32353--32368, 2022.

\bibitem[Luo et~al.(2023)Luo, Sun, Xu, Zhao, Lou, Tao, Geng, Lin, Chen, and Zhang]{luo2023wizardmath}
Haipeng Luo, Qingfeng Sun, Can Xu, Pu~Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang.
\newblock Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.
\newblock \emph{arXiv preprint arXiv:2308.09583}, 2023.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Yuan, Yuan, Dong, Lu, Wu, Tan, Wang, and Zhou]{chen23muggle}
Chengpeng Li, Zheng Yuan, Hongyi Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou.
\newblock Query and response augmentation cannot help out-of-domain math reasoning generalization.
\newblock \emph{CoRR}, abs/2310.05506, 2023{\natexlab{b}}.

\bibitem[Yue et~al.(2023)Yue, Qu, Zhang, Fu, Huang, Sun, Su, and Chen]{yue23mammoth}
Xiang Yue, Xingwei Qu, Ge~Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu~Su, and Wenhu Chen.
\newblock Mammoth: Building math generalist models through hybrid instruction tuning.
\newblock \emph{CoRR}, abs/2309.05653, 2023.

\bibitem[Yu et~al.(2023)Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li, Weller, and Liu]{yu2023metamath}
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang, James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
\newblock Metamath: Bootstrap your own mathematical questions for large language models.
\newblock \emph{arXiv preprint arXiv:2309.12284}, 2023.

\bibitem[Lewkowycz et~al.(2022{\natexlab{b}})Lewkowycz, Andreassen, Dohan, Dyer, Michalewski, Ramasesh, Slone, Anil, Schlag, Gutman-Solo, et~al.]{lewkowycz2022solving}
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et~al.
\newblock Solving quantitative reasoning problems with language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 3843--3857, 2022{\natexlab{b}}.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu]{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting of the Association for Computational Linguistics}, pages 311--318, 2002.

\bibitem[Paster(2023)]{testing_language_models_on_a_held_out_high_school_national_finals_exam}
Keiran Paster.
\newblock Testing language models on a held-out high school national finals exam.
\newblock \url{https://huggingface.co/datasets/keirp/hungarian_national_hs_finals_exam}, 2023.

\bibitem[Ahn et~al.(2024)Ahn, Verma, Lou, Liu, Zhang, and Yin]{ahn24llmmr}
Janice Ahn, Rishu Verma, Renze Lou, Di~Liu, Rui Zhang, and Wenpeng Yin.
\newblock Large language models for mathematical reasoning: Progresses and challenges.
\newblock In \emph{Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics}, pages 225--237, 2024.

\bibitem[Lu et~al.(2023{\natexlab{a}})Lu, Qiu, Yu, Welleck, and Chang]{lu23mr}
Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai{-}Wei Chang.
\newblock A survey of deep learning for mathematical reasoning.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics}, pages 14605--14631, 2023{\natexlab{a}}.

\bibitem[Lu et~al.(2024)Lu, Bansal, Xia, Liu, Li, Hajishirzi, Cheng, Chang, Galley, and Gao]{lu2024mathvista}
Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao.
\newblock Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.
\newblock In \emph{Proceedings of the 12th International Conference on Learning Representations}, 2024.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{kojima2022cot}
Takeshi Kojima, Shixiang~(Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems}, pages 22199--22213, 2022.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2023selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc~V. Le, Ed~H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock In \emph{Proceedings of the 11th International Conference on Learning Representations}, 2023{\natexlab{c}}.

\bibitem[Zhou et~al.(2023)Zhou, Sch{\"{a}}rli, Hou, Wei, Scales, Wang, Schuurmans, Cui, Bousquet, Le, and Chi]{zhouz2023leasttomost}
Denny Zhou, Nathanael Sch{\"{a}}rli, Le~Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc~V. Le, and Ed~H. Chi.
\newblock Least-to-most prompting enables complex reasoning in large language models.
\newblock In \emph{Proceedings of the 11th International Conference on Learning Representations}, 2023.

\bibitem[Khot et~al.(2023)Khot, Trivedi, Finlayson, Fu, Richardson, Clark, and Sabharwal]{Khot2023decot}
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal.
\newblock Decomposed prompting: {A} modular approach for solving complex tasks.
\newblock In \emph{Proceedings of the 11th International Conference on Learning Representations}, 2023.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, ichter, Xia, Chi, Le, and Zhou]{wei2022fewshotcot}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed~Chi, Quoc~V Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, \emph{Advances in Neural Information Processing Systems}, pages 24824--24837, 2022.

\bibitem[Fu et~al.(2023)Fu, Peng, Sabharwal, Clark, and Khot]{Fu2023complexcot}
Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot.
\newblock Complexity-based prompting for multi-step reasoning.
\newblock In \emph{Proceedings of the 11th International Conference on Learning Representations}, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Zhang, Li, and Smola]{zhang2023autocot}
Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola.
\newblock Automatic chain of thought prompting in large language models.
\newblock In \emph{Proceedings of the 11th International Conference on Learning Representations}, 2023.

\bibitem[Lu et~al.(2023{\natexlab{b}})Lu, Qiu, Chang, Wu, Zhu, Rajpurohit, Clark, and Kalyan]{lu2023dynammicprompt}
Pan Lu, Liang Qiu, Kai{-}Wei Chang, Ying~Nian Wu, Song{-}Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan.
\newblock Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.
\newblock In \emph{Proceedings of the 11th International Conference on Learning Representations}, 2023{\natexlab{b}}.

\bibitem[Huang et~al.(2023)Huang, Zhang, Cheng, and Yang]{huang2023boosting}
Xijie Huang, Li~Lyna Zhang, Kwang-Ting Cheng, and Mao Yang.
\newblock Boosting llm reasoning: Push the limits of few-shot learning with reinforced in-context pruning.
\newblock \emph{arXiv preprint arXiv:2312.08901}, 2023.

\bibitem[Magister et~al.(2023)Magister, Mallinson, Ad{\'{a}}mek, Malmi, and Severyn]{MagisterMAMS23}
Lucie~Charlotte Magister, Jonathan Mallinson, Jakub Ad{\'{a}}mek, Eric Malmi, and Aliaksei Severyn.
\newblock Teaching small language models to reason.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics}, pages 1773--1781, 2023.

\bibitem[Yuan et~al.(2023)Yuan, Yuan, Li, Dong, Lu, Tan, Zhou, and Zhou]{yuan2023scaling}
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou.
\newblock Scaling relationship on learning mathematical reasoning with large language models, 2023.

\bibitem[Wang et~al.(2023{\natexlab{d}})Wang, Li, Chen, Song, Lin, Cao, Liu, and Sui]{wang2023align}
Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui.
\newblock Making large language models better reasoners with alignment.
\newblock \emph{CoRR}, abs/2309.02144, 2023{\natexlab{d}}.

\bibitem[Weng et~al.(2023)Weng, Zhu, Xia, Li, He, Liu, Sun, Liu, and Zhao]{weng2023large}
Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao.
\newblock Large language models are better reasoners with self-verification.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 2550--2575, 2023.

\bibitem[Jiang et~al.(2023{\natexlab{b}})Jiang, Shi, Yu, Liu, Zhang, Li, and Kwok]{jiang2023forward}
Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu~Zhang, Zhenguo Li, and James~T Kwok.
\newblock Forward-backward reasoning in large language models for mathematical verification.
\newblock \emph{arXiv preprint arXiv:2308.07758}, 3, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2024)Li, Wang, Hu, Wei, Zheng, Hu, Zhang, and Peng]{li24xwin}
Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng.
\newblock Common 7b language models already possess strong math capabilities.
\newblock \emph{CoRR}, abs/2403.04706, 2024.

\bibitem[Chen et~al.(2023)Chen, Ma, Wang, and Cohen]{chen2022program}
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William~W. Cohen.
\newblock Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.
\newblock \emph{Transactions on Machine Learning Research}, 2023.

\bibitem[Gao et~al.(2023)Gao, Madaan, Zhou, Alon, Liu, Yang, Callan, and Neubig]{gao2023pal}
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig.
\newblock {PAL:} program-aided language models.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning}, pages 10764--10799, 2023.

\bibitem[Wang et~al.(2023{\natexlab{e}})Wang, Ren, Zhou, Lu, Luo, Shi, Zhang, Song, Zhan, and Li]{wang2023mathcoder}
Ke~Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li.
\newblock Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning.
\newblock \emph{arXiv preprint arXiv:2310.03731}, 2023{\natexlab{e}}.

\bibitem[Zhou et~al.(2024)Zhou, Staats, Li, Szegedy, Weinberger, and Wu]{anonymous2024dont}
Jin~Peng Zhou, Charles~E Staats, Wenda Li, Christian Szegedy, Kilian~Q Weinberger, and Yuhuai Wu.
\newblock Don't trust: Verify -- grounding {LLM} quantitative reasoning with autoformalization.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Gou et~al.(2023)Gou, Shao, Gong, Yang, Huang, Duan, Chen, et~al.]{gou2023tora}
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et~al.
\newblock Tora: A tool-integrated reasoning agent for mathematical problem solving.
\newblock \emph{arXiv preprint arXiv:2309.17452}, 2023.

\bibitem[Barrett et~al.(2011)Barrett, Conway, Deters, Hadarean, Jovanovi{\'c}, King, Reynolds, and Tinelli]{barrett2011cvc4}
Clark Barrett, Christopher~L Conway, Morgan Deters, Liana Hadarean, Dejan Jovanovi{\'c}, Tim King, Andrew Reynolds, and Cesare Tinelli.
\newblock Cvc4.
\newblock In \emph{Computer Aided Verification: 23rd International Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceedings 23}, pages 171--177. Springer, 2011.

\bibitem[Winterer et~al.(2020)Winterer, Zhang, and Su]{winterer2020validating}
Dominik Winterer, Chengyu Zhang, and Zhendong Su.
\newblock Validating smt solvers via semantic fusion.
\newblock In \emph{Proceedings of the 41st ACM SIGPLAN Conference on programming language design and implementation}, pages 718--730, 2020.

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and Weston]{bengio2009curriculum}
Yoshua Bengio, J{\'e}r{\^o}me Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.
\newblock In \emph{Proceedings of the 26th annual international conference on machine learning}, pages 41--48, 2009.

\bibitem[Soviany et~al.(2022)Soviany, Ionescu, Rota, and Sebe]{soviany2022curriculum}
Petru Soviany, Radu~Tudor Ionescu, Paolo Rota, and Nicu Sebe.
\newblock Curriculum learning: A survey.
\newblock \emph{International Journal of Computer Vision}, 130\penalty0 (6):\penalty0 1526--1565, 2022.

\bibitem[Chen et~al.(2024)Chen, Liao, Li, and Fan]{chen2024alphamath}
Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan.
\newblock Alphamath almost zero: process supervision without process.
\newblock \emph{arXiv preprint arXiv:2405.03553}, 2024.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen{-}Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen{-}Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{Proceedings of the 1th International Conference on Learning Representations}, 2022.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock Technical report, 2023.

\bibitem[Gario and Micheli(2015)]{pysmt2015}
Marco Gario and Andrea Micheli.
\newblock Pysmt: a solver-agnostic library for fast prototyping of smt-based algorithms.
\newblock In \emph{SMT Workshop 2015}, 2015.

\bibitem[Harris et~al.(2020)Harris, Millman, Van Der~Walt, Gommers, Virtanen, Cournapeau, Wieser, Taylor, Berg, Smith, et~al.]{harris2020array}
Charles~R Harris, K~Jarrod Millman, St{\'e}fan~J Van Der~Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel~J Smith, et~al.
\newblock Array programming with numpy.
\newblock \emph{Nature}, 585\penalty0 (7825):\penalty0 357--362, 2020.

\bibitem[Fu and Su(2016)]{fu2016xsat}
Zhoulai Fu and Zhendong Su.
\newblock Xsat: a fast floating-point satisfiability solver.
\newblock In \emph{Computer Aided Verification: 28th International Conference, CAV 2016, Toronto, ON, Canada, July 17-23, 2016, Proceedings, Part II 28}, pages 187--209. Springer, 2016.

\bibitem[Fischer et~al.(2019)Fischer, Balunovic, Drachsler-Cohen, Gehr, Zhang, and Vechev]{fischer2019dl2}
Marc Fischer, Mislav Balunovic, Dana Drachsler-Cohen, Timon Gehr, Ce~Zhang, and Martin Vechev.
\newblock Dl2: training and querying neural networks with logic.
\newblock In \emph{International Conference on Machine Learning}, pages 1931--1941. PMLR, 2019.

\bibitem[Li et~al.(2022)Li, Liu, Yao, Xu, Chen, Ma, Jian, et~al.]{li2022learning}
Zenan Li, Zehua Liu, Yuan Yao, Jingwei Xu, Taolue Chen, Xiaoxing Ma, L~Jian, et~al.
\newblock Learning with logical constraints but without shortcut satisfaction.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\end{thebibliography}
