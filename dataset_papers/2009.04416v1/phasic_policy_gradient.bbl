\begin{thebibliography}{21}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2020)Andrychowicz, Raichuk, Sta{\'n}czyk, Orsini,
  Girgin, Marinier, Hussenot, Geist, Pietquin, Michalski,
  et~al.]{andrychowicz2020matters}
M.~Andrychowicz, A.~Raichuk, P.~Sta{\'n}czyk, M.~Orsini, S.~Girgin,
  R.~Marinier, L.~Hussenot, M.~Geist, O.~Pietquin, M.~Michalski, et~al.
\newblock What matters in on-policy reinforcement learning? a large-scale
  empirical study.
\newblock \emph{arXiv preprint arXiv:2006.05990}, 2020.

\bibitem[Bellemare et~al.(2019)Bellemare, Dabney, Dadashi, Taiga, Castro,
  Le~Roux, Schuurmans, Lattimore, and Lyle]{bellemare2019geometric}
M.~Bellemare, W.~Dabney, R.~Dadashi, A.~A. Taiga, P.~S. Castro, N.~Le~Roux,
  D.~Schuurmans, T.~Lattimore, and C.~Lyle.
\newblock A geometric perspective on optimal representations for reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4360--4371, 2019.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{bellemare2017distributional}
M.~G. Bellemare, W.~Dabney, and R.~Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 449--458. JMLR. org, 2017.

\bibitem[Cobbe et~al.(2019)Cobbe, Hesse, Hilton, and Schulman]{procgen}
K.~Cobbe, C.~Hesse, J.~Hilton, and J.~Schulman.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.01588}, 2019.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, Legg, and Kavukcuoglu]{impala}
L.~Espeholt, H.~Soyer, R.~Munos, K.~Simonyan, V.~Mnih, T.~Ward, Y.~Doron,
  V.~Firoiu, T.~Harley, I.~Dunning, S.~Legg, and K.~Kavukcuoglu.
\newblock {IMPALA:} scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock \emph{CoRR}, abs/1802.01561, 2018.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{sac}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{arXiv preprint arXiv:1801.01290}, 2018.

\bibitem[Igl et~al.(2020)Igl, Farquhar, Luketina, Boehmer, and
  Whiteson]{igl2020impact}
M.~Igl, G.~Farquhar, J.~Luketina, W.~Boehmer, and S.~Whiteson.
\newblock The impact of non-stationarity on generalisation in deep
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.05826}, 2020.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{ddpg}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Lyle et~al.(2019)Lyle, Bellemare, and Castro]{lyle2019comparative}
C.~Lyle, M.~G. Bellemare, and P.~S. Castro.
\newblock A comparative analysis of expected and distributional reinforcement
  learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 4504--4511, 2019.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
V.~Mnih, A.~P. Badia, M.~Mirza, A.~Graves, T.~Lillicrap, T.~Harley, D.~Silver,
  and K.~Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  1928--1937, 2016.

\bibitem[OpenAI et~al.(2019{\natexlab{a}})OpenAI, Akkaya, Andrychowicz,
  Chociej, Litwin, McGrew, Petron, Paino, Plappert, Powell, Ribas, Schneider,
  Tezak, Tworek, Welinder, Weng, Yuan, Zaremba, and Zhang]{rubiks}
OpenAI, I.~Akkaya, M.~Andrychowicz, M.~Chociej, M.~Litwin, B.~McGrew,
  A.~Petron, A.~Paino, M.~Plappert, G.~Powell, R.~Ribas, J.~Schneider,
  N.~Tezak, J.~Tworek, P.~Welinder, L.~Weng, Q.~Yuan, W.~Zaremba, and L.~Zhang.
\newblock Solving rubik's cube with a robot hand.
\newblock \emph{arXiv preprint arXiv:1910.07113}, 2019{\natexlab{a}}.

\bibitem[OpenAI et~al.(2019{\natexlab{b}})OpenAI, Berner, Brockman, Chan,
  Cheung, Debiak, Dennison, Farhi, Fischer, Hashme, Hesse, Józefowicz, Gray,
  Olsson, Pachocki, Petrov, de~Oliveira~Pinto, Raiman, Salimans, Schlatter,
  Schneider, Sidor, Sutskever, Tang, Wolski, and Zhang]{dota}
OpenAI, C.~Berner, G.~Brockman, B.~Chan, V.~Cheung, P.~Debiak, C.~Dennison,
  D.~Farhi, Q.~Fischer, S.~Hashme, C.~Hesse, R.~Józefowicz, S.~Gray,
  C.~Olsson, J.~Pachocki, M.~Petrov, H.~P. de~Oliveira~Pinto, J.~Raiman,
  T.~Salimans, J.~Schlatter, J.~Schneider, S.~Sidor, I.~Sutskever, J.~Tang,
  F.~Wolski, and S.~Zhang.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019{\natexlab{b}}.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{awr}
X.~B. Peng, A.~Kumar, G.~Zhang, and S.~Levine.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.00177}, 2019.

\bibitem[Schulman et~al.(2015{\natexlab{a}})Schulman, Levine, Abbeel, Jordan,
  and Moritz]{trpo}
J.~Schulman, S.~Levine, P.~Abbeel, M.~Jordan, and P.~Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pages
  1889--1897, 2015{\natexlab{a}}.

\bibitem[Schulman et~al.(2015{\natexlab{b}})Schulman, Moritz, Levine, Jordan,
  and Abbeel]{gae}
J.~Schulman, P.~Moritz, S.~Levine, M.~Jordan, and P.~Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{arXiv preprint arXiv:1506.02438}, 2015{\natexlab{b}}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{CoRR}, abs/1707.06347, 2017.

\bibitem[Song et~al.(2019)Song, Abdolmaleki, Springenberg, Clark, Soyer, Rae,
  Noury, Ahuja, Liu, Tirumala, et~al.]{vmpo}
H.~F. Song, A.~Abdolmaleki, J.~T. Springenberg, A.~Clark, H.~Soyer, J.~W. Rae,
  S.~Noury, A.~Ahuja, S.~Liu, D.~Tirumala, et~al.
\newblock V-mpo: On-policy maximum a posteriori policy optimization for
  discrete and continuous control.
\newblock \emph{arXiv preprint arXiv:1909.12238}, 2019.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{starcraft}
O.~Vinyals, I.~Babuschkin, W.~M. Czarnecki, M.~Mathieu, A.~Dudzik, J.~Chung,
  D.~H. Choi, R.~Powell, T.~Ewalds, P.~Georgiev, et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang et~al.(2016)Wang, Bapst, Heess, Mnih, Munos, Kavukcuoglu, and
  de~Freitas]{acer}
Z.~Wang, V.~Bapst, N.~Heess, V.~Mnih, R.~Munos, K.~Kavukcuoglu, and
  N.~de~Freitas.
\newblock Sample efficient actor-critic with experience replay.
\newblock \emph{arXiv preprint arXiv:1611.01224}, 2016.

\bibitem[Wu et~al.(2017)Wu, Mansimov, Grosse, Liao, and Ba]{acktr}
Y.~Wu, E.~Mansimov, R.~B. Grosse, S.~Liao, and J.~Ba.
\newblock Scalable trust-region method for deep reinforcement learning using
  kronecker-factored approximation.
\newblock In \emph{Advances in neural information processing systems}, pages
  5279--5288, 2017.

\end{thebibliography}
