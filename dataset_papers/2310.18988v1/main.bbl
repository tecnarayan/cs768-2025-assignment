\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{GWB{\etalchar{+}}17}

\bibitem[ACHL19]{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Aka74]{akaike1974new}
Hirotugu Akaike.
\newblock A new look at the statistical model identification.
\newblock {\em IEEE transactions on automatic control}, 19(6):716--723, 1974.

\bibitem[AP20]{adlam2020understanding}
Ben Adlam and Jeffrey Pennington.
\newblock Understanding double descent requires a fine-grained bias-variance decomposition.
\newblock {\em Advances in neural information processing systems}, 33:11022--11032, 2020.

\bibitem[ASS20]{advani2020high}
Madhu~S Advani, Andrew~M Saxe, and Haim Sompolinsky.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock {\em Neural Networks}, 132:428--446, 2020.

\bibitem[Bel21]{belkin2021fit}
Mikhail Belkin.
\newblock Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation.
\newblock {\em Acta Numerica}, 30:203--248, 2021.

\bibitem[BFSO84]{BreiFrieStonOlsh84}
Leo Breiman, Jerome Friedman, Charles~J. Stone, and R.A. Olshen.
\newblock {\em Classification and Regression Trees}.
\newblock Chapman and Hall/CRC, 1984.

\bibitem[BH89]{baldi1989neural}
Pierre Baldi and Kurt Hornik.
\newblock Neural networks and principal component analysis: Learning from examples without local minima.
\newblock {\em Neural networks}, 2(1):53--58, 1989.

\bibitem[BHMM19]{belkin2019reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical bias--variance trade-off.
\newblock {\em Proceedings of the National Academy of Sciences}, 116(32):15849--15854, 2019.

\bibitem[BHX20]{belkin2020two}
Mikhail Belkin, Daniel Hsu, and Ji~Xu.
\newblock Two models of double descent for weak features.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 2(4):1167--1180, 2020.

\bibitem[BLLT20]{bartlett2020benign}
Peter~L Bartlett, Philip~M Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock {\em Proceedings of the National Academy of Sciences}, 117(48):30063--30070, 2020.

\bibitem[BM21]{buschjager2021there}
Sebastian Buschj{\"a}ger and Katharina Morik.
\newblock There is no double-descent in random forests.
\newblock {\em arXiv preprint arXiv:2111.04409}, 2021.

\bibitem[BMM18]{belkin2018understand}
Mikhail Belkin, Siyuan Ma, and Soumik Mandal.
\newblock To understand deep learning we need to understand kernel learning.
\newblock In {\em International Conference on Machine Learning}, pages 541--549. PMLR, 2018.

\bibitem[BO96]{bos1996dynamics}
Siegfried B{\"o}s and Manfred Opper.
\newblock Dynamics of training.
\newblock {\em Advances in Neural Information Processing Systems}, 9, 1996.

\bibitem[Bre01]{breiman2001random}
Leo Breiman.
\newblock Random forests.
\newblock {\em Machine learning}, 45:5--32, 2001.

\bibitem[CJ09]{cadima2009relationships}
Jorge Cadima and Ian Jolliffe.
\newblock On relationships between uncentred and column-centred principal component analysis.
\newblock {\em Pakistan Journal of Statistics}, 25(4), 2009.

\bibitem[CL21]{chatterji2021finite}
Niladri~S Chatterji and Philip~M Long.
\newblock Finite-sample analysis of interpolating linear classifiers in the overparameterized regime.
\newblock {\em The Journal of Machine Learning Research}, 22(1):5721--5750, 2021.

\bibitem[CMBK21]{chen2021multiple}
Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi.
\newblock Multiple descent: Design your own generalization curve.
\newblock {\em Advances in Neural Information Processing Systems}, 34:8898--8912, 2021.

\bibitem[DB22]{dar2022double}
Yehuda Dar and Richard~G Baraniuk.
\newblock Double double descent: on generalization errors in transfer learning between linear regression tasks.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 4(4):1447--1472, 2022.

\bibitem[DFO20]{deisenroth2020mathematics}
Marc~Peter Deisenroth, A~Aldo Faisal, and Cheng~Soon Ong.
\newblock {\em Mathematics for machine learning}.
\newblock Cambridge University Press, 2020.

\bibitem[DLM20]{derezinski2020exact}
Michal Derezinski, Feynman~T Liang, and Michael~W Mahoney.
\newblock Exact expressions for double descent and implicit regularization via surrogate random design.
\newblock {\em Advances in neural information processing systems}, 33:5152--5164, 2020.

\bibitem[DMB21]{dar2021farewell}
Yehuda Dar, Vidya Muthukumar, and Richard~G Baraniuk.
\newblock A farewell to the bias-variance tradeoff? an overview of the theory of overparameterized machine learning.
\newblock {\em arXiv preprint arXiv:2109.02355}, 2021.

\bibitem[DMLB20]{dar2020subspace}
Yehuda Dar, Paul Mayer, Lorenzo Luzi, and Richard Baraniuk.
\newblock Subspace fitting meets regression: The effects of supervision and orthonormality constraints on double descent of generalization errors.
\newblock In {\em International Conference on Machine Learning}, pages 2366--2375. PMLR, 2020.

\bibitem[dRBK20]{d2020double}
St{\'e}phane dâ€™Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala.
\newblock Double trouble in double descent: Bias and variance (s) in the lazy regime.
\newblock In {\em International Conference on Machine Learning}, pages 2280--2290. PMLR, 2020.

\bibitem[DSYW20]{dwivedi2020revisiting}
Raaz Dwivedi, Chandan Singh, Bin Yu, and Martin~J Wainwright.
\newblock Revisiting complexity and the bias-variance tradeoff.
\newblock {\em arXiv preprint arXiv:2006.10189}, 2020.

\bibitem[Dui95]{duin1995small}
Robert~PW Duin.
\newblock Small sample size generalization.
\newblock In {\em Proceedings of the Scandinavian Conference on Image Analysis}, volume~2, pages 957--964. Citeseer, 1995.

\bibitem[Eub84]{eubank1984hat}
RL~Eubank.
\newblock The hat matrix for smoothing splines.
\newblock {\em Statistics \& probability letters}, 2(1):9--14, 1984.

\bibitem[FM23]{fisher1923studies}
Ronald~A Fisher and Winifred~A Mackenzie.
\newblock Studies in crop variation. ii. the manurial response of different potato varieties.
\newblock {\em The Journal of Agricultural Science}, 13(3):311--320, 1923.

\bibitem[GBC16]{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock {\em Deep learning}.
\newblock MIT press, 2016.

\bibitem[GBD92]{geman1992neural}
Stuart Geman, Elie Bienenstock, and Ren{\'e} Doursat.
\newblock Neural networks and the bias/variance dilemma.
\newblock {\em Neural computation}, 4(1):1--58, 1992.

\bibitem[GVL13]{golub2013matrix}
Gene~H Golub and Charles~F Van~Loan.
\newblock {\em Matrix computations}.
\newblock JHU press, 2013.

\bibitem[GWB{\etalchar{+}}17]{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[HB21]{hui2020evaluation}
Like Hui and Mikhail Belkin.
\newblock Evaluation of neural architectures trained with square loss vs cross-entropy in classification tasks.
\newblock {\em International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[HMRT22]{hastie2022surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em The Annals of Statistics}, 50(2):949--986, 2022.

\bibitem[Hot57]{hotelling1957relations}
Harold Hotelling.
\newblock The relations of the newer multivariate statistical methods to factor analysis.
\newblock {\em British Journal of Statistical Psychology}, 10(2):69--79, 1957.

\bibitem[HT90]{hastie1990GAM}
Trevor Hastie and Robert Tibshirani.
\newblock Generalized additive models.
\newblock {\em Monographs on statistics and applied probability. Chapman \& Hall}, 43:335, 1990.

\bibitem[HTF09]{hastie2009elements}
Trevor Hastie, Robert Tibshirani, and Jerome~H Friedman.
\newblock {\em The elements of statistical learning: data mining, inference, and prediction}, volume~2.
\newblock Springer, 2009.

\bibitem[HXZQ22]{he2022sparse}
Zheng He, Zeke Xie, Quanzhi Zhu, and Zengchang Qin.
\newblock Sparse double descent: Where network pruning aggravates overfitting.
\newblock In {\em International Conference on Machine Learning}, pages 8635--8659. PMLR, 2022.

\bibitem[Jol82]{jolliffe1982note}
Ian~T Jolliffe.
\newblock A note on the use of principal components in regression.
\newblock {\em Journal of the Royal Statistical Society Series C: Applied Statistics}, 31(3):300--303, 1982.

\bibitem[Kal96]{kalman1996singularly}
Dan Kalman.
\newblock A singularly valuable decomposition: the svd of a matrix.
\newblock {\em The college mathematics journal}, 27(1):2--23, 1996.

\bibitem[Ken57]{kendall1957course}
Maurice~G Kendall.
\newblock A course in multivariate analysis: London.
\newblock {\em Charles Griffin \& Co}, 1957.

\bibitem[KH{\etalchar{+}}09]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[KL16]{krijthe2016peaking}
Jesse~H Krijthe and Marco Loog.
\newblock The peaking phenomenon in semi-supervised learning.
\newblock In {\em Structural, Syntactic, and Statistical Pattern Recognition: Joint IAPR International Workshop, S+ SSPR 2016, M{\'e}rida, Mexico, November 29-December 2, 2016, Proceedings}, pages 299--309. Springer, 2016.

\bibitem[KSR{\etalchar{+}}21]{kuzborskij2021role}
Ilja Kuzborskij, Csaba Szepesv{\'a}ri, Omar Rivasplata, Amal Rannen-Triki, and Razvan Pascanu.
\newblock On the role of optimization in double descent: A least squares study.
\newblock {\em Advances in Neural Information Processing Systems}, 34:29567--29577, 2021.

\bibitem[LBBH98]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem[LD21]{lin2021causes}
Licong Lin and Edgar Dobriban.
\newblock What causes the test error? going beyond bias-variance via anova.
\newblock {\em The Journal of Machine Learning Research}, 22(1):6925--7006, 2021.

\bibitem[LDB21]{luzi2021double}
Lorenzo Luzi, Yehuda Dar, and Richard Baraniuk.
\newblock Double descent and other interpolation phenomena in gans.
\newblock {\em arXiv preprint arXiv:2106.04003}, 2021.

\bibitem[LVM{\etalchar{+}}20]{loog2020brief}
Marco Loog, Tom Viering, Alexander Mey, Jesse~H Krijthe, and David~MJ Tax.
\newblock A brief prehistory of double descent.
\newblock {\em Proceedings of the National Academy of Sciences}, 117(20):10625--10626, 2020.

\bibitem[Mac91]{mackay1991bayesian}
David MacKay.
\newblock Bayesian model comparison and backprop nets.
\newblock {\em Advances in neural information processing systems}, 4, 1991.

\bibitem[Mal73]{mallows2000some}
Colin~L Mallows.
\newblock Some comments on cp.
\newblock {\em Technometrics}, 42(1):87--94, 1973.

\bibitem[MBB18]{ma2018power}
Siyuan Ma, Raef Bassily, and Mikhail Belkin.
\newblock The power of interpolation: Understanding the effectiveness of sgd in modern over-parametrized learning.
\newblock In {\em International Conference on Machine Learning}, pages 3325--3334. PMLR, 2018.

\bibitem[MBW20]{maddox2020rethinking}
Wesley~J Maddox, Gregory Benton, and Andrew~Gordon Wilson.
\newblock Rethinking parameter counting in deep models: Effective dimensionality revisited.
\newblock {\em arXiv preprint arXiv:2003.02139}, 2020.

\bibitem[MNS{\etalchar{+}}21]{muthukumar2021classification}
Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel Hsu, and Anant Sahai.
\newblock Classification vs regression in overparameterized regimes: Does the loss function matter?
\newblock {\em The Journal of Machine Learning Research}, 22(1):10104--10172, 2021.

\bibitem[Moo91]{moody1991effective}
John Moody.
\newblock The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems.
\newblock {\em Advances in neural information processing systems}, 4, 1991.

\bibitem[Mur22]{murphy2022probabilistic}
Kevin~P Murphy.
\newblock {\em Probabilistic machine learning: an introduction}.
\newblock MIT press, 2022.

\bibitem[Nea19]{neal2019bias}
Brady Neal.
\newblock On the bias-variance tradeoff: Textbooks need an update.
\newblock {\em arXiv preprint arXiv:1912.08286}, 2019.

\bibitem[NKB{\etalchar{+}}21]{nakkiran2021deep}
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment}, 2021(12):124003, 2021.

\bibitem[NMB{\etalchar{+}}18]{neal2018modern}
Brady Neal, Sarthak Mittal, Aristide Baratin, Vinayak Tantia, Matthew Scicluna, Simon Lacoste-Julien, and Ioannis Mitliagkas.
\newblock A modern take on the bias-variance tradeoff in neural networks.
\newblock {\em arXiv preprint arXiv:1810.08591}, 2018.

\bibitem[NTS14]{neyshabur2014search}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit regularization in deep learning.
\newblock {\em arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[NWC{\etalchar{+}}11]{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Pea01]{pearson1901liii}
Karl Pearson.
\newblock Liii. on lines and planes of closest fit to systems of points in space.
\newblock {\em The London, Edinburgh, and Dublin philosophical magazine and journal of science}, 2(11):559--572, 1901.

\bibitem[PKB19]{poggio2019double}
Tomaso Poggio, Gil Kur, and Andrzej Banburski.
\newblock Double descent in the condition number.
\newblock {\em arXiv preprint arXiv:1912.06190}, 2019.

\bibitem[PVG{\etalchar{+}}11]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel, M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos, D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock {\em Journal of Machine Learning Research}, 12:2825--2830, 2011.

\bibitem[Ras18]{raschka2018model}
Sebastian Raschka.
\newblock Model evaluation, model selection, and algorithm selection in machine learning.
\newblock {\em arXiv preprint arXiv:1811.12808}, 2018.

\bibitem[RD98]{raudys1998expected}
Sarunas Raudys and Robert~PW Duin.
\newblock Expected classification error of the fisher linear classifier with pseudo-inverse covariance matrix.
\newblock {\em Pattern recognition letters}, 19(5-6):385--392, 1998.

\bibitem[RT19]{rosset2019fixed}
Saharon Rosset and Ryan~J Tibshirani.
\newblock From fixed-x to random-x regression: Bias-variance decompositions, covariance penalties, and prediction error estimation.
\newblock {\em Journal of the American Statistical Association}, 2019.

\bibitem[Sch78]{schwarz1978estimating}
Gideon Schwarz.
\newblock Estimating the dimension of a model.
\newblock {\em The annals of statistics}, pages 461--464, 1978.

\bibitem[SD96]{skurichina1996stabilizing}
Marina Skurichina and Robert~PW Duin.
\newblock Stabilizing classifiers for very small sample sizes.
\newblock In {\em Proceedings of 13th International Conference on Pattern Recognition}, volume~2, pages 891--896. IEEE, 1996.

\bibitem[SGd{\etalchar{+}}18]{spigler2018jamming}
Stefano Spigler, Mario Geiger, St{\'e}phane d'Ascoli, Levent Sagun, Giulio Biroli, and Matthieu Wyart.
\newblock A jamming transition from under-to over-parametrization affects loss landscape and generalization.
\newblock {\em arXiv preprint arXiv:1810.09665}, 2018.

\bibitem[SGT18]{sun2018approximation}
Yitong Sun, Anna Gilbert, and Ambuj Tewari.
\newblock On the approximation properties of random relu features.
\newblock {\em arXiv preprint arXiv:1810.04374}, 2018.

\bibitem[SKL92]{sutter1992principal}
Jon~M Sutter, John~H Kalivas, and Patrick~M Lang.
\newblock Which principal components to utilize for principal component regression.
\newblock {\em Journal of chemometrics}, 6(4):217--225, 1992.

\bibitem[THV22]{teresa2022dimensionality}
Ningyuan Teresa, David~W Hogg, and Soledad Villar.
\newblock Dimensionality reduction, regularization, and generalization in overparameterized regressions.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 4(1):126--152, 2022.

\bibitem[Tuk77]{tukey1977exploratory}
John~W Tukey.
\newblock {\em Exploratory data analysis}, volume~2.
\newblock Reading, MA, 1977.

\bibitem[Vap95]{vapnik1999nature}
Vladimir Vapnik.
\newblock {\em The nature of statistical learning theory}.
\newblock Springer science \& business media, 1995.

\bibitem[VCR89]{vallet1989linear}
F~Vallet, J-G Cailton, and Ph~Refregier.
\newblock Linear and nonlinear extension of the pseudo-inverse solution for learning boolean functions.
\newblock {\em Europhysics Letters}, 9(4):315, 1989.

\bibitem[VL22]{viering2022shape}
Tom Viering and Marco Loog.
\newblock The shape of learning curves: a review.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2022.

\bibitem[WRR03]{wall2003singular}
Michael~E Wall, Andreas Rechtsteiner, and Luis~M Rocha.
\newblock Singular value decomposition and principal component analysis.
\newblock {\em A practical approach to microarray data analysis}, pages 91--109, 2003.

\bibitem[WW75]{wahba1975completely}
Grace Wahba and Svante Wold.
\newblock A completely automatic french curve: fitting spline functions by cross validation.
\newblock {\em Communications in Statistics-Theory and Methods}, 4(1):1--17, 1975.

\end{thebibliography}
