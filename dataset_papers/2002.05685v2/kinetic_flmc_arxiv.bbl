\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abramowitz \& Stegun(1972)Abramowitz and Stegun]{AS1972}
Abramowitz, M. and Stegun, I.
\newblock \emph{Handbook of Mathematical Functions with Formulas, Graphs and
  Mathematical Tables}.
\newblock Dover, New York, 1972.

\bibitem[Amari(1998)]{amari1998natural}
Amari, S.-I.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural Computation}, 10\penalty0 (2):\penalty0 251--276, 1998.

\bibitem[Ament \& O'Neil(2018)Ament and O'Neil]{Ament2017}
Ament, S. and O'Neil, M.
\newblock Accurate and efficient numerical calculation of stable densities via
  optimized quadrature and asymptotics.
\newblock \emph{Statistics and Computing}, 28:\penalty0 171--185, 2018.

\bibitem[Bertoin(1996)]{bertoin1996}
Bertoin, J.
\newblock \emph{L\'{e}vy Processes}.
\newblock Cambridge University Press, Cambridge, UK, 1996.

\bibitem[Betancourt et~al.(2017)Betancourt, Byrne, Livingstone, and
  Girolami]{betancourt2017geometric}
Betancourt, M., Byrne, S., Livingstone, S., and Girolami, M.
\newblock The geometric foundations of {H}amiltonian {M}onte {C}arlo.
\newblock \emph{Bernoulli}, 23\penalty0 (4A):\penalty0 2257--2298, 2017.

\bibitem[Brosse et~al.(2019)Brosse, Durmus, Moulines, and
  Sabanis]{brosse2019tamed}
Brosse, N., Durmus, A., Moulines, {\'E}., and Sabanis, S.
\newblock The tamed unadjusted {L}angevin algorithm.
\newblock \emph{Stochastic Processes and their Applications}, 129\penalty0
  (10):\penalty0 3638--3663, 2019.

\bibitem[Capa{\l}a \& Dybiec(2019)Capa{\l}a and Dybiec]{capala2019stationary}
Capa{\l}a, K. and Dybiec, B.
\newblock Stationary states for underdamped anharmonic oscillators driven by
  {C}auchy noise.
\newblock \emph{arXiv preprint arXiv:1905.12078}, 2019.

\bibitem[Chaudhari \& Soatto(2018)Chaudhari and
  Soatto]{chaudhari2018stochastic}
Chaudhari, P. and Soatto, S.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[\c{S}im\c{s}ekli(2017)]{FLMC}
\c{S}im\c{s}ekli, U.
\newblock Fractional {L}angevin {M}onte {C}arlo: Exploring {L}\'{e}vy driven
  stochastic differential equations for {M}arkov {C}hain {M}onte {C}arlo.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3200--3209, 2017.

\bibitem[\c{S}im\c{s}ekli et~al.(2018)\c{S}im\c{s}ekli, Y{\i}ld{\i}z, Nguyen,
  Richard, and Cemgil]{csimcsekli2018asynchronous}
\c{S}im\c{s}ekli, U., Y{\i}ld{\i}z, {\c{C}}., Nguyen, T.~H., Richard, G., and
  Cemgil, A.~T.
\newblock Asynchronous stochastic quasi-{N}ewton {MCMC} for non-convex
  optimization.
\newblock \emph{arXiv preprint arXiv:1806.02617}, 2018.

\bibitem[\c{S}im\c{s}ekli et~al.(2019{\natexlab{a}})\c{S}im\c{s}ekli,
  G{\"u}rb{\"u}zbalaban, Nguyen, Richard, and Sagun]{csimcsekli2019heavy}
\c{S}im\c{s}ekli, U., G{\"u}rb{\"u}zbalaban, M., Nguyen, T.~H., Richard, G.,
  and Sagun, L.
\newblock On the heavy-tailed theory of stochastic gradient descent for deep
  neural networks.
\newblock \emph{arXiv preprint arXiv:1912.00018}, 2019{\natexlab{a}}.

\bibitem[\c{S}im\c{s}ekli et~al.(2019{\natexlab{b}})\c{S}im\c{s}ekli, Sagun,
  and Gurbuzbalaban]{pmlr-v97-simsekli19a}
\c{S}im\c{s}ekli, U., Sagun, L., and Gurbuzbalaban, M.
\newblock A tail-index analysis of stochastic gradient noise in deep neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5827--5837, 2019{\natexlab{b}}.

\bibitem[\c{S}im\c{s}ekli et~al.(2020)\c{S}im\c{s}ekli, Sener, Deligiannidis,
  and Erdogdu]{csimcsekli2020hausdorff}
\c{S}im\c{s}ekli, U., Sener, O., Deligiannidis, G., and Erdogdu, M.~A.
\newblock Hausdorff dimension, stochastic differential equations, and
  generalization in neural networks.
\newblock \emph{arXiv preprint arXiv:2006.09313}, 2020.

\bibitem[Dalalyan \& Riou-Durand(2020)Dalalyan and
  Riou-Durand]{dalalyan2018kinetic}
Dalalyan, A.~S. and Riou-Durand, L.
\newblock On sampling from a log-concave density using kinetic {L}angevin
  diffusions.
\newblock \emph{Bernoulli}, 26\penalty0 (3):\penalty0 1956--1988, 2020.

\bibitem[Duan(2015)]{duan2015}
Duan, J.
\newblock \emph{An Introduction to Stochastic Dynamics}.
\newblock Cambridge University Press, New York, 2015.

\bibitem[Dubkov et~al.(2008)Dubkov, Spagnolo, and Uchaikin]{dubkov2008levy}
Dubkov, A.~A., Spagnolo, B., and Uchaikin, V.~V.
\newblock L{\'e}vy flight superdiffusion: An introduction.
\newblock \emph{International Journal of Bifurcation and Chaos}, 18\penalty0
  (09):\penalty0 2649--2672, 2008.

\bibitem[Eliazar \& Klafter(2003)Eliazar and Klafter]{eliazar2003levy}
Eliazar, I. and Klafter, J.
\newblock L{\'e}vy-driven {L}angevin systems: Targeted stochasticity.
\newblock \emph{Journal of Statistical Physics}, 111\penalty0 (3-4):\penalty0
  739--768, 2003.

\bibitem[Fischer(2011)]{fischer2011history}
Fischer, H.
\newblock \emph{A History of the Central Limit Theorem: From Classical to
  Modern Probability Theory}.
\newblock Springer Science \& Business Media, New York, 2011.

\bibitem[Gao et~al.(2018{\natexlab{a}})Gao, G\"{u}rb\"{u}zbalaban, and
  Zhu]{GGZ}
Gao, X., G\"{u}rb\"{u}zbalaban, M., and Zhu, L.
\newblock Global convergence of {S}tochastic {G}radient {H}amiltonian {M}onte
  {C}arlo for non-convex stochastic optimization: Non-asymptotic performance
  bounds and momentum-based acceleration.
\newblock \emph{arXiv:1809.04618}, 2018{\natexlab{a}}.

\bibitem[Gao et~al.(2018{\natexlab{b}})Gao, G\"{u}rb\"{u}zbalaban, and
  Zhu]{GGZ2}
Gao, X., G\"{u}rb\"{u}zbalaban, M., and Zhu, L.
\newblock Breaking reversibility accelerates {L}angevin dynamics for global
  non-convex optimization.
\newblock \emph{arXiv:1812.07725}, 2018{\natexlab{b}}.

\bibitem[G\"{u}rb\"{u}zbalaban et~al.(2020)G\"{u}rb\"{u}zbalaban,
  \c{S}im\c{s}ekli, and Zhu]{gurbuzbalaban2020heavy}
G\"{u}rb\"{u}zbalaban, M., \c{S}im\c{s}ekli, U., and Zhu, L.
\newblock The heavy-tail phenomenon in {SGD}.
\newblock \emph{arXiv preprint arXiv:2006.04740}, 2020.

\bibitem[H{\'e}rau \& Nier(2004)H{\'e}rau and Nier]{herau2004isotropic}
H{\'e}rau, F. and Nier, F.
\newblock Isotropic hypoellipticity and trend to equilibrium for the
  {F}okker-{P}lanck equation with a high-degree potential.
\newblock \emph{Archive for Rational Mechanics and Analysis}, 171\penalty0
  (2):\penalty0 151--218, 2004.

\bibitem[Hodgkinson \& Mahoney(2020)Hodgkinson and
  Mahoney]{hodgkinson2020multiplicative}
Hodgkinson, L. and Mahoney, M.~W.
\newblock Multiplicative noise and heavy tails in stochastic optimization.
\newblock \emph{arXiv preprint arXiv:2006.06293}, 2020.

\bibitem[Hu et~al.(2019)Hu, Li, Li, and Liu]{hu2017diffusion}
Hu, W., Li, C.~J., Li, L., and Liu, J.-G.
\newblock On the diffusion approximation of nonconvex stochastic gradient
  descent.
\newblock \emph{Annals of Mathematical Science and Applications}, 4\penalty0
  (1):\penalty0 3--32, 2019.

\bibitem[Jastrzebski et~al.(2017)Jastrzebski, Kenton, Arpit, Ballas, Fischer,
  Bengio, and Storkey]{jastrzkebski2017three}
Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y.,
  and Storkey, A.
\newblock Three factors influencing minima in {SGD}.
\newblock \emph{arXiv preprint arXiv:1711.04623}, 2017.

\bibitem[Kloeden \& Platen(1999)Kloeden and Platen]{kloeden2013numerical}
Kloeden, P.~E. and Platen, E.
\newblock \emph{Numerical Solution of Stochastic Differential Equations},
  volume~23.
\newblock Springer Verlag, Berlin, 1999.

\bibitem[Kuruoglu(1999)]{kuruoglu1999}
Kuruoglu, E.~E.
\newblock \emph{Signal Processing in $\alpha$-Stable Noise Environments: A
  Least $\ell_{p}$-Norm Approach}.
\newblock PhD Thesis, University of Cambridge, 1999.

\bibitem[L{\'e}vy(1937)]{paul1937theorie}
L{\'e}vy, P.
\newblock Th{\'e}orie de l'addition des variables al{\'e}atoires.
\newblock \emph{Gauthiers-Villars, Paris}, 1937.

\bibitem[Livingstone et~al.(2019)Livingstone, Faulkner, and
  Roberts]{livingstone2019kinetic}
Livingstone, S., Faulkner, M.~F., and Roberts, G.~O.
\newblock Kinetic energy choice in {H}amiltonian/hybrid {M}onte {C}arlo.
\newblock \emph{Biometrika}, 106\penalty0 (2):\penalty0 303--319, 2019.

\bibitem[Lu et~al.(2017)Lu, Perrone, Hasenclever, Teh, and
  Vollmer]{lu2016relativistic}
Lu, X., Perrone, V., Hasenclever, L., Teh, Y.~W., and Vollmer, S.~J.
\newblock Relativistic {M}onte {C}arlo.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1236--1245,
  2017.

\bibitem[Maddison et~al.(2018)Maddison, Paulin, Teh, O'Donoghue, and
  Doucet]{maddison2018hamiltonian}
Maddison, C.~J., Paulin, D., Teh, Y.~W., O'Donoghue, B., and Doucet, A.
\newblock Hamiltonian descent methods.
\newblock \emph{arXiv preprint arXiv:1809.05042}, 2018.

\bibitem[Mandelbrot(1997)]{mandelbrot2013}
Mandelbrot, B.~B.
\newblock \emph{Fractals and Scaling in Finance: Discontinuity, Concentration,
  Risk}.
\newblock Springer Science \& Business Media, New York, 1997.

\bibitem[Mandt et~al.(2016)Mandt, Hoffman, and Blei]{mandt2016variational}
Mandt, S., Hoffman, M., and Blei, D.
\newblock A variational analysis of stochastic gradient algorithms.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  354--363, 2016.

\bibitem[Martin \& Mahoney(2019)Martin and Mahoney]{martin2019heavy}
Martin, C.~H. and Mahoney, M.~W.
\newblock Heavy-tailed universality predicts trends in test accuracies for very
  large pre-trained deep neural networks.
\newblock \emph{arXiv preprint arXiv:1901.08278}, 2019.

\bibitem[Montroll \& Bendler(1984)Montroll and Bendler]{Montroll1984}
Montroll, E.~W. and Bendler, J.~T.
\newblock On {L}{\'e}vy (or stable) distributions and the {W}illiams-{W}atts
  model of dielectric relaxation.
\newblock \emph{Journal of Statistical Physics}, 34\penalty0 (1):\penalty0
  129--162, Jan 1984.

\bibitem[Montroll \& West(1979)Montroll and West]{MONTROLL_book_chapter}
Montroll, E.~W. and West, B.~J.
\newblock Chapter 2 - {O}n an enriched collection of stochastic processes.
\newblock In Montroll, E. and Lebowitz, J. (eds.), \emph{Fluctuation
  Phenomena}, pp.\  61 -- 175. Elsevier, 1979.

\bibitem[Neal(2010)]{neal2010mcmc}
Neal, R.
\newblock {MCMC} using {H}amiltonian dynamics. {Handbook of {M}arkov {C}hain
  {M}onte {C}arlo} ({S. Brooks, A. Gelman, G. Jones, and X.-L. Meng, eds.}),
  2010.

\bibitem[Nguyen et~al.(2019)Nguyen, \c{S}im\c{s}ekli, Gurbuzbalaban, and
  Richard]{nguyen2019first}
Nguyen, T.~H., \c{S}im\c{s}ekli, U., Gurbuzbalaban, M., and Richard, G.
\newblock First exit time analysis of stochastic gradient descent under
  heavy-tailed gradient noise.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  273--283, 2019.

\bibitem[{Nguyen} et~al.(2019){Nguyen}, \c{S}im\c{s}ekli, and
  {Richard}]{nguyen19}
{Nguyen}, T.~H., \c{S}im\c{s}ekli, U., and {Richard}, G.
\newblock {Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for
  Non-Convex Optimization}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4810--4819, 2019.

\bibitem[Ortigueira(2006)]{ortigueira2006riesz}
Ortigueira, M.~D.
\newblock Riesz potential operators and inverses via fractional centred
  derivatives.
\newblock \emph{International Journal of Mathematics and Mathematical
  Sciences}, 2006, 2006.

\bibitem[Panigrahi et~al.(2019)Panigrahi, Somani, Goyal, and
  Netrapalli]{panigrahi2019non}
Panigrahi, A., Somani, R., Goyal, N., and Netrapalli, P.
\newblock Non-{G}aussianity of stochastic gradient noise.
\newblock \emph{arXiv preprint arXiv:1910.09626}, 2019.

\bibitem[Panloup(2008)]{panloup2008recursive}
Panloup, F.
\newblock Recursive computation of the invariant measure of a stochastic
  differential equation driven by a {L}{\'e}vy process.
\newblock \emph{Annals of Applied Probability}, 18\penalty0 (2):\penalty0
  379--426, 2008.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Pascanu, R., Mikolov, T., and Bengio, Y.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1310--1318, 2013.

\bibitem[Pavliotis(2014)]{pavliotis2014stochastic}
Pavliotis, G.~A.
\newblock \emph{Stochastic Processes and Applications: Diffusion Processes, the
  {F}okker-{P}lanck and {L}angevin Equations}, volume~60.
\newblock Springer Science \& Business Media, New York, 2014.

\bibitem[Raginsky et~al.(2017)Raginsky, Rakhlin, and Telgarsky]{Raginsky}
Raginsky, M., Rakhlin, A., and Telgarsky, M.
\newblock Non-convex learning via stochastic gradient {L}angevin dynamics: a
  nonasymptotic analysis.
\newblock In \emph{Conference on Learning Theory}, pp.\  1674--1703, 2017.

\bibitem[Riesz(1949)]{Riesz}
Riesz, M.
\newblock L'int\'{e}grale de {R}iemann-{L}iouville et le probl\`{e}me de
  {C}auchy.
\newblock \emph{Acta Mathematica}, 81\penalty0 (1):\penalty0 1--222, 1949.

\bibitem[Schertzer et~al.(2001)Schertzer, Larchev\^{e}que, Duan, Yanovsky, and
  Lovejoy]{SLDYL}
Schertzer, D., Larchev\^{e}que, M., Duan, J., Yanovsky, V., and Lovejoy, S.
\newblock Fractional {F}okker-{P}lanck equation for nonlinear stochastic
  differential equations driven by non-{G}aussian {L}\'{e}vy stable noises.
\newblock \emph{Journal of Mathematical Physics}, 42\penalty0 (1):\penalty0
  200--212, 2001.

\bibitem[Sliusarenko et~al.(2013)Sliusarenko, Surkov, Gonchar, and
  Chechkin]{sliusarenko2013stationary}
Sliusarenko, O.~Y., Surkov, D., Gonchar, V.~Y., and Chechkin, A.~V.
\newblock Stationary states in bistable system driven by {L}{\'e}vy noise.
\newblock \emph{The European Physical Journal Special Topics}, 216\penalty0
  (1):\penalty0 133--138, 2013.

\bibitem[Smith et~al.(2018)Smith, Kindermans, Ying, and Le]{iclr2018}
Smith, S.~L., Kindermans, P., Ying, C., and Le, Q.~V.
\newblock Don't decay the learning rate, increase the batch size.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1139--1147, 2013.

\bibitem[Wintner(1941)]{wintner1941}
Wintner, A.
\newblock The singularities of {C}auchy's distributions.
\newblock \emph{Duke Math. J.}, 8\penalty0 (4):\penalty0 678--681, 12 1941.

\bibitem[Xu et~al.(2018)Xu, Chen, Zou, and Gu]{xu2018global}
Xu, P., Chen, J., Zou, D., and Gu, Q.
\newblock Global convergence of {L}angevin dynamics based algorithms for
  nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3122--3133, 2018.

\bibitem[Ye \& Zhu(2018)Ye and Zhu]{SFHMC}
Ye, N. and Zhu, Z.
\newblock Stochastic fractional {H}amiltonian {M}onte {C}arlo.
\newblock In \emph{Proceedings of the 27th International Joint Conference on
  Artificial Intelligence}, 2018.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, He, Sra, and
  Jadbabaie]{zhang2019analysis}
Zhang, J., He, T., Sra, S., and Jadbabaie, A.
\newblock Analysis of gradient clipping and adaptive scaling with a relaxed
  smoothness condition.
\newblock \emph{arXiv preprint arXiv:1905.11881}, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Karimireddy, Veit, Kim, Reddi,
  Kumar, and Sra]{zhang2019adam}
Zhang, J., Karimireddy, S.~P., Veit, A., Kim, S., Reddi, S.~J., Kumar, S., and
  Sra, S.
\newblock Why {ADAM} beats {SGD} for attention models.
\newblock \emph{arXiv preprint arXiv:1912.03194}, 2019{\natexlab{b}}.

\bibitem[Zhu et~al.(2019)Zhu, Wu, Yu, Wu, and Ma]{zhu2019anisotropic}
Zhu, Z., Wu, J., Yu, B., Wu, L., and Ma, J.
\newblock The anisotropic noise in stochastic gradient descent: Its behavior of
  escaping from sharp minima and regularization effects.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7654--7663, 2019.

\bibitem[Zou et~al.(2018)Zou, Xu, and Gu]{pmlr-v80-zou18a}
Zou, D., Xu, P., and Gu, Q.
\newblock Stochastic variance-reduced {H}amilton {M}onte {C}arlo methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6028--6037, 2018.

\bibitem[Zou et~al.(2019)Zou, Xu, and Gu]{ZXG2019}
Zou, D., Xu, P., and Gu, Q.
\newblock Stochastic gradient {H}amiltonian {M}onte {C}arlo methods with
  recursive variance reduction.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\end{thebibliography}
