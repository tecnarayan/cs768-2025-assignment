\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{WRXM18}

\bibitem[ABH17]{agarwal2017second}
Naman Agarwal, Brian Bullins, and Elad Hazan.
\newblock Second-order stochastic optimization for machine learning in linear
  time.
\newblock {\em The Journal of Machine Learning Research}, 18(1):4148--4187,
  2017.

\bibitem[AC09]{ailon2009fast}
Nir Ailon and Bernard Chazelle.
\newblock The fast {J}ohnson--{L}indenstrauss transform and approximate nearest
  neighbors.
\newblock {\em SIAM Journal on computing}, 39(1):302--322, 2009.

\bibitem[Ach03]{achlioptas2003}
Dimitris Achlioptas.
\newblock Database-friendly random projections: {J}ohnson-{L}indenstrauss with
  binary coins.
\newblock {\em Journal of computer and System Sciences}, 66(4):671--687, 2003.

\bibitem[ALV22]{alv22}
Nima Anari, Yang~P Liu, and Thuy-Duong Vuong.
\newblock Optimal sublinear sampling of spanning trees and determinantal point
  processes via average-case entropic independence.
\newblock In {\em 2022 IEEE 63rd Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 123--134. IEEE, 2022.

\bibitem[AM15]{ridge-leverage-scores}
Ahmed~El Alaoui and Michael~W. Mahoney.
\newblock Fast randomized kernel ridge regression with statistical guarantees.
\newblock In {\em Proceedings of the 28th International Conference on Neural
  Information Processing Systems}, pages 775--783, 2015.

\bibitem[BS10]{bai2010spectral}
Zhidong Bai and Jack~W Silverstein.
\newblock {\em Spectral analysis of large dimensional random matrices},
  volume~20.
\newblock Springer, 2010.

\bibitem[CCKW22]{chepurko2022near}
Nadiia Chepurko, Kenneth~L Clarkson, Praneeth Kacham, and David~P Woodruff.
\newblock Near-optimal algorithms for linear algebra in the current matrix
  multiplication time.
\newblock In {\em Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete
  Algorithms (SODA)}, pages 3043--3068. SIAM, 2022.

\bibitem[CDDR24]{chenakkod2023optimal}
Shabarish Chenakkod, Micha{\l} Derezi{\'n}ski, Xiaoyu Dong, and Mark Rudelson.
\newblock Optimal embedding dimension for sparse subspace embeddings.
\newblock In {\em 56th Annual ACM Symposium on Theory of Computing}, 2024.

\bibitem[CDV20]{alpha-dpp}
Daniele Calandriello, Michal Derezinski, and Michal Valko.
\newblock Sampling from a k-dpp without looking at all items.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6889--6899, 2020.

\bibitem[CL11]{libsvm}
Chih-Chung Chang and Chih-Jen Lin.
\newblock {LIBSVM}: A library for support vector machines.
\newblock {\em ACM Transactions on Intelligent Systems and Technology},
  2:27:1--27:27, 2011.

\bibitem[CMM17]{cohen2017input}
Michael~B Cohen, Cameron Musco, and Christopher Musco.
\newblock Input sparsity time low-rank approximation via ridge leverage score
  sampling.
\newblock In {\em Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 1758--1777. SIAM, 2017.

\bibitem[CP15]{cohen2015lp}
Michael~B Cohen and Richard Peng.
\newblock Lp row sampling by lewis weights.
\newblock In {\em Proceedings of the symposium on Theory of computing}, pages
  183--192, 2015.

\bibitem[CW09]{clarkson2009numerical}
Kenneth~L Clarkson and David~P Woodruff.
\newblock Numerical linear algebra in the streaming model.
\newblock In {\em Proceedings of the forty-first annual ACM symposium on Theory
  of computing}, pages 205--214, 2009.

\bibitem[CW13]{clarkson2013low}
Kenneth~L Clarkson and David~P Woodruff.
\newblock Low rank approximation and regression in input sparsity time.
\newblock In {\em Proceedings of the forty-fifth annual ACM symposium on Theory
  of Computing}, pages 81--90, 2013.

\bibitem[DL19]{DobLiu18_TR}
Edgar Dobriban and Sifan Liu.
\newblock Asymptotics for sketching in least squares regression.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[DLDM21]{derezinski2021sparse}
Micha{\l} Derezi\'nski, Zhenyu Liao, Edgar Dobriban, and Michael Mahoney.
\newblock Sparse sketches with small inversion bias.
\newblock In {\em Conference on Learning Theory}, pages 1467--1510. PMLR, 2021.

\bibitem[DLLM20]{precise-expressions}
Micha{\l} Derezi{\'n}ski, Feynman~T Liang, Zhenyu Liao, and Michael~W Mahoney.
\newblock Precise expressions for random projections: Low-rank approximation
  and randomized newton.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[DLPM21]{newton-less}
Micha{\l} Derezi\'nski, Jonathan Lacotte, Mert Pilanci, and Michael~W Mahoney.
\newblock Newton-less: Sparsification without trade-offs for the sketched
  newton update.
\newblock {\em Advances in Neural Information Processing Systems},
  34:2835--2847, 2021.

\bibitem[DM16]{drineas2016randnla}
Petros Drineas and Michael~W Mahoney.
\newblock Randnla: randomized numerical linear algebra.
\newblock {\em Communications of the ACM}, 59(6):80--90, 2016.

\bibitem[DM21]{dpps-in-randnla}
Micha{\l} Derezi{\'n}ski and Michael~W Mahoney.
\newblock Determinantal point processes in randomized numerical linear algebra.
\newblock {\em Notices of the American Mathematical Society}, 68(1):34--45,
  2021.

\bibitem[DMM06]{drineas2006sampling}
Petros Drineas, Michael~W Mahoney, and S~Muthukrishnan.
\newblock Sampling algorithms for $\ell_2$ regression and applications.
\newblock In {\em Proceedings of the seventeenth annual ACM-SIAM symposium on
  Discrete algorithm}, pages 1127--1136, 2006.

\bibitem[DW17]{unbiased-estimates}
Micha{\l} Derezi\'{n}ski and Manfred~K. Warmuth.
\newblock Unbiased estimates for linear regression via volume sampling.
\newblock In {\em Advances in Neural Information Processing Systems 30}, pages
  3087--3096, 2017.

\bibitem[DY24]{derezinski2023solving}
Micha{\l} Derezi{\'n}ski and Jiaming Yang.
\newblock Solving dense linear systems faster than via preconditioning.
\newblock In {\em 56th Annual ACM Symposium on Theory of Computing}, 2024.

\bibitem[GR15]{gower2015randomized}
Robert~M Gower and Peter Richt{\'a}rik.
\newblock Randomized iterative methods for linear systems.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  36(4):1660--1690, 2015.

\bibitem[JSZ85]{johnson1985best}
William~B Johnson, Gideon Schechtman, and Joel Zinn.
\newblock Best constants in moment inequalities for linear combinations of
  independent and exchangeable random variables.
\newblock {\em The Annals of Probability}, pages 234--253, 1985.

\bibitem[KT11]{k-dpp}
Alex Kulesza and Ben Taskar.
\newblock {k-DPPs: Fixed-Size Determinantal Point Processes}.
\newblock In {\em {Proceedings of the 28th International Conference on Machine
  Learning}}, pages 1193--1200, June 2011.

\bibitem[KT12]{dpp-ml}
Alex Kulesza and Ben Taskar.
\newblock {\em Determinantal Point Processes for Machine Learning}.
\newblock Now Publishers Inc., Hanover, MA, USA, 2012.

\bibitem[LLDP20]{lacotte2020optimal}
Jonathan Lacotte, Sifan Liu, Edgar Dobriban, and Mert Pilanci.
\newblock Optimal iterative sketching methods with the subsampled randomized
  {H}adamard transform.
\newblock {\em Advances in Neural Information Processing Systems},
  33:9725--9735, 2020.

\bibitem[LPJ{\etalchar{+}}22]{lejeune2022asymptotics}
Daniel LeJeune, Pratik Patil, Hamid Javadi, Richard~G Baraniuk, and Ryan~J
  Tibshirani.
\newblock Asymptotics of the sketched pseudoinverse.
\newblock {\em arXiv preprint arXiv:2211.03751}, 2022.

\bibitem[LW20]{li2020input}
Yi~Li and David Woodruff.
\newblock Input-sparsity low rank approximation in schatten norm.
\newblock In {\em International Conference on Machine Learning}, pages
  6001--6009. PMLR, 2020.

\bibitem[LWM19]{lopes2019bootstrap}
Miles~E Lopes, Shusen Wang, and Michael~W Mahoney.
\newblock A bootstrap method for error estimation in randomized matrix
  multiplication.
\newblock {\em The Journal of Machine Learning Research}, 20(1):1434--1473,
  2019.

\bibitem[MCZ{\etalchar{+}}22]{ma2022asymptotic}
Ping Ma, Yongkai Chen, Xinlian Zhang, Xin Xing, Jingyi Ma, and Michael~W
  Mahoney.
\newblock Asymptotic analysis of sampling estimators for randomized numerical
  linear algebra algorithms.
\newblock {\em The Journal of Machine Learning Research}, 23(1):7970--8014,
  2022.

\bibitem[MDM{\etalchar{+}}23]{randlapack_book}
R.~Murray, J.~Demmel, M.~W. Mahoney, N.~B. Erichson, M.~Melnichenko, O.~A.
  Malik, L.~Grigori, M.~Derezi{\'n}ski, M.~E. Lopes, T.~Liang, and H.~Luo.
\newblock {Randomized Numerical Linear Algebra} -- a perspective on the field
  with an eye to software.
\newblock Technical Report arXiv preprint arXiv:2302.11474, 2023.

\bibitem[MM13]{mm-sparse}
Xiangrui Meng and Michael~W. Mahoney.
\newblock Low-distortion subspace embeddings in input-sparsity time and
  applications to robust linear regression.
\newblock In {\em Proceedings of the Symposium on Theory of Computing}, STOC
  '13, pages 91--100, 2013.

\bibitem[MT20]{martinsson2020randomized}
Per-Gunnar Martinsson and Joel~A Tropp.
\newblock Randomized numerical linear algebra: Foundations and algorithms.
\newblock {\em Acta Numerica}, 29:403--572, 2020.

\bibitem[NN13]{nelson2013osnap}
Jelani Nelson and Huy~L Nguy{\^e}n.
\newblock Osnap: Faster numerical linear algebra algorithms via sparser
  subspace embeddings.
\newblock In {\em 2013 ieee 54th annual symposium on foundations of computer
  science}, pages 117--126. IEEE, 2013.

\bibitem[RM16]{GarveshMahoney_JMLR}
G.~Raskutti and M.~W. Mahoney.
\newblock A statistical perspective on randomized sketching for ordinary
  least-squares.
\newblock {\em Journal of Machine Learning Research}, 17(214):1--31, 2016.

\bibitem[Sar06]{sarlos2006improved}
Tamas Sarlos.
\newblock Improved approximation algorithms for large matrices via random
  projections.
\newblock In {\em 2006 47th annual IEEE symposium on foundations of computer
  science (FOCS'06)}, pages 143--152. IEEE, 2006.

\bibitem[WGM18]{wang2018sketched}
S.~Wang, A.~Gittens, and M.~W. Mahoney.
\newblock Sketched ridge regression: Optimization perspective, statistical
  perspective, and model averaging.
\newblock {\em Journal of Machine Learning Research}, 18(218):1--50, 2018.

\bibitem[Woo14]{woodruff2014sketching}
David~P Woodruff.
\newblock Sketching as a tool for numerical linear algebra.
\newblock {\em Foundations and Trends{\textregistered} in Theoretical Computer
  Science}, 10(1--2):1--157, 2014.

\bibitem[WRXM18]{distributed-newton}
Shusen Wang, Fred Roosta, Peng Xu, and Michael~W Mahoney.
\newblock {GIANT:} globally improved approximate newton method for distributed
  optimization.
\newblock {\em Advances in Neural Information Processing Systems},
  31:2332--2342, 2018.

\end{thebibliography}
