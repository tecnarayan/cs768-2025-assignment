\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bengio et~al.(2013)Bengio, Léonard, and
  Courville]{bengio2013estimating}
Bengio, Y., Léonard, N., and Courville, A.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation, 2013.

\bibitem[Breiman et~al.(1984)Breiman, Friendman, Stone, and Olstein]{Breiman84}
Breiman, L., Friendman, J., Stone, C.~J., and Olstein, R.~A.
\newblock \emph{Classification and regression trees.}
\newblock Wadsworth \& Brooks/Cole Advanced Books \& Software., Monterey, CA,
  1984.
\newblock ISBN 978-0-412-04841-8.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.
\newblock URL
  \url{https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Frankle \& Carbin(2019)Frankle and Carbin]{frankle2018lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl-b3RcF7}.

\bibitem[Gaier \& Ha(2019)Gaier and Ha]{gaier2019weight}
Gaier, A. and Ha, D.
\newblock Weight agnostic neural networks.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32, pp.\  5364--5378. Curran
  Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/e98741479a7b998f88b8f8c9f0b6b6f1-Paper.pdf}.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{pmlr-v9-glorot10a}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In Teh, Y.~W. and Titterington, M. (eds.), \emph{Proceedings of the
  Thirteenth International Conference on Artificial Intelligence and
  Statistics}, volume~9 of \emph{Proceedings of Machine Learning Research},
  pp.\  249--256, Chia Laguna Resort, Sardinia, Italy, 13--15 May 2010. PMLR.

\bibitem[{He} et~al.(2015){He}, {Zhang}, {Ren}, and {Sun}]{He2050}
{He}, K., {Zhang}, X., {Ren}, S., and {Sun}, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{2015 IEEE International Conference on Computer Vision
  (ICCV)}, pp.\  1026--1034, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet-34}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  770--778, 2016.

\bibitem[He et~al.(2017)He, Gkioxari, Dollár, and Girshick]{he2017mask}
He, K., Gkioxari, G., Dollár, P., and Girshick, R.
\newblock Mask r-cnn.
\newblock In \emph{2017 IEEE International Conference on Computer Vision
  (ICCV)}, pp.\  2980--2988, 2017.
\newblock \doi{10.1109/ICCV.2017.322}.

\bibitem[Hoffer et~al.(2018)Hoffer, Hubara, and Soudry]{hoffer2018fix}
Hoffer, E., Hubara, I., and Soudry, D.
\newblock Fix your classifier: the marginal value of training the last weight
  layer.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=S1Dh8Tg0-}.

\bibitem[Hubara et~al.(2016)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{CourbariauxB16}
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized neural networks.
\newblock In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2016/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf}.

\bibitem[Hubara et~al.(2017)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{hubara2017quantized}
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Quantized neural networks: Training neural networks with low
  precision weights and activations.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 6869--6898, 2017.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{Krizhevsky-imagenetclassification}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In Pereira, F., Burges, C. J.~C., Bottou, L., and Weinberger, K.~Q.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~25,
  pp.\  1097--1105. Curran Associates, Inc., 2012.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf}.

\bibitem[Lecun et~al.(1998)Lecun, Bottou, Bengio, and Haffner]{lecun1998lenet}
Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee et~al.(2019)Lee, Ajanthan, and Torr]{lee2018snip}
Lee, N., Ajanthan, T., and Torr, P.
\newblock {SNIP}: {Single}-{shot} {Network} {Pruning} {based} {on} {connection}
  {sensitivity}.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=B1VZqjAcYX}.

\bibitem[Lee et~al.(2020)Lee, Ajanthan, Gould, and Torr]{lee2019signal}
Lee, N., Ajanthan, T., Gould, S., and Torr, P. H.~S.
\newblock A signal propagation perspective for pruning neural networks at
  initialization.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=HJeTo2VFwH}.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and
  Hutter]{loshchilov-ICLR17SGDR}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.
\newblock URL \url{https://openreview.net/pdf?id=Skq89Scxx}.

\bibitem[Maennel et~al.(2020)Maennel, Alabdulmohsin, Tolstikhin, Baldock,
  Bousquet, Gelly, and Keysers]{maennel2020neural}
Maennel, H., Alabdulmohsin, I., Tolstikhin, I., Baldock, R. J.~N., Bousquet,
  O., Gelly, S., and Keysers, D.
\newblock What do neural networks learn when trained with random labels?, 2020.
\newblock URL \url{https://arxiv.org/pdf/2006.10455.pdf}.

\bibitem[Malach et~al.(2020)Malach, Yehudai, Shalev-Schwartz, and
  Shamir]{malach2020proving}
Malach, E., Yehudai, G., Shalev-Schwartz, S., and Shamir, O.
\newblock Proving the lottery ticket hypothesis: Pruning is all you need.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  6682--6691. PMLR,
  13--18 Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/malach20a.html}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32, pp.\  8026--8037. Curran
  Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf}.

\bibitem[Pensia et~al.(2020)Pensia, Rajput, Nagle, Vishwakarma, and
  Papailiopoulos]{pensia2020optimal}
Pensia, A., Rajput, S., Nagle, A., Vishwakarma, H., and Papailiopoulos, D.
\newblock Optimal lottery tickets via subsetsum: Logarithmic
  over-parameterization is sufficient.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.
\newblock URL
  \url{https://papers.nips.cc/paper/2020/file/1b742ae215adf18b75449c6e272fd92d-Paper.pdf}.

\bibitem[Ramanujan et~al.(2020)Ramanujan, Wortsman, Kembhavi, Farhadi, and
  Rastegari]{ramanujan2019whats}
Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A., and Rastegari, M.
\newblock What's hidden in a randomly weighted neural network?
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, June 2020.
\newblock URL
  \url{https://openaccess.thecvf.com/content\_CVPR\_2020/papers/Ramanujan\_Whats\_Hidden\_in\_a\_Randomly\_Weighted_Neural\_Network\_CVPR\_2020\_paper.pdf}.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi]{RastegariORF16}
Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In \emph{Computer Vision -- ECCV 2016}, pp.\  525--542, 2016.
\newblock URL \url{http://arxiv.org/abs/1603.05279}.

\bibitem[Ren et~al.(2015)Ren, He, Girshick, and Sun]{ren2015faster}
Ren, S., He, K., Girshick, R., and Sun, J.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~28,
  pp.\  91--99. Curran Associates, Inc., 2015.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf}.

\bibitem[Rosenfeld \& Tsotsos(2019)Rosenfeld and Tsotsos]{amir2019intri}
Rosenfeld, A. and Tsotsos, J.~K.
\newblock Intriguing properties of randomly weighted networks: Generalizing
  while learning next to nothing.
\newblock In \emph{2019 16th Conference on Computer and Robot Vision (CRV)},
  pp.\  9--16, 2019.

\bibitem[Russakovsky et~al.(2009)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{imagenet-cvpr09}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{CVPR}, 2009.

\bibitem[{Stanley} \& {Miikkulainen}(2002){Stanley} and
  {Miikkulainen}]{stanley2002neat}
{Stanley}, K.~O. and {Miikkulainen}, R.
\newblock Evolving neural networks through augmenting topologies.
\newblock \emph{Evolutionary Computation}, 10\penalty0 (2):\penalty0 99--127,
  2002.

\bibitem[Tanaka et~al.(2020)Tanaka, Kunin, Yamins, and
  Ganguli]{tanaka2020pruning}
Tanaka, H., Kunin, D., Yamins, D. L.~K., and Ganguli, S.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/46a4378f835dc8040c8057beb6a2da52-Paper.pdf}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L.~u., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~30, pp.\  5998--6008. Curran
  Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Wang et~al.(2020)Wang, Zhang, and Grosse]{wang2020picking}
Wang, C., Zhang, G., and Grosse, R.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SkgsACVKPH}.

\bibitem[{Wang} et~al.(2018){Wang}, {Hu}, {Zhang}, {Zhang}, {Liu}, and
  {Cheng}]{Wang2018}
{Wang}, P., {Hu}, Q., {Zhang}, Y., {Zhang}, C., {Liu}, Y., and {Cheng}, J.
\newblock Two-step quantization for low-bit neural networks.
\newblock In \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pp.\  4376--4384, 2018.
\newblock \doi{10.1109/CVPR.2018.00460}.

\bibitem[Yosinski et~al.(2014)Yosinski, Clune, Bengio, and
  Lipson]{yosinski2014transferable}
Yosinski, J., Clune, J., Bengio, Y., and Lipson, H.
\newblock How transferable are features in deep neural networks?
\newblock In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems}, volume~27, pp.\  3320--3328. Curran Associates, Inc., 2014.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2014/file/375c71349b295fbe2dcdca9206f20a06-Paper.pdf}.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In Richard C.~Wilson, E. R.~H. and Smith, W. A.~P. (eds.),
  \emph{Proceedings of the British Machine Vision Conference (BMVC)}, pp.\
  87.1--87.12. BMVA Press, 2016.

\bibitem[Zhou et~al.(2019)Zhou, Lan, Liu, and Yosinski]{zhou2019deconstructing}
Zhou, H., Lan, J., Liu, R., and Yosinski, J.
\newblock Deconstructing lottery tickets: Zeros, signs, and the supermask.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32, pp.\  3597--3607. Curran
  Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/1113d7a76ffceca1bb350bfe145467c6-Paper.pdf}.

\bibitem[Zoph \& Le(2017)Zoph and Le]{zoph2016neural}
Zoph, B. and Le, Q.~V.
\newblock Neural architecture search with reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/pdf?id=r1Ue8Hcxg}.

\end{thebibliography}
