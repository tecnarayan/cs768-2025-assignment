\begin{thebibliography}{118}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{gpt4}
J.~Achiam, S.~Adler, S.~Agarwal, L.~Ahmad, I.~Akkaya, F.~L. Aleman, D.~Almeida, J.~Altenschmidt, S.~Altman, S.~Anadkat, et~al.
\newblock {GPT-4 technical report}.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Achlioptas et~al.(2020)Achlioptas, Abdelreheem, Xia, Elhoseiny, and Guibas]{referit3d}
P.~Achlioptas, A.~Abdelreheem, F.~Xia, M.~Elhoseiny, and L.~Guibas.
\newblock {ReferIt3D: Neural listeners for fine-grained 3D object identification in real-world scenes}.
\newblock In \emph{ECCV}, 2020.

\bibitem[Akbari et~al.(2021)Akbari, Yuan, Qian, Chuang, Chang, Cui, and Gong]{vatt}
H.~Akbari, L.~Yuan, R.~Qian, W.-H. Chuang, S.-F. Chang, Y.~Cui, and B.~Gong.
\newblock {VATT: Transformers for multimodal self-supervised learning from raw video, audio and text}.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Assran et~al.(2023)Assran, Duval, Misra, Bojanowski, Vincent, Rabbat, LeCun, and Ballas]{ijepa}
M.~Assran, Q.~Duval, I.~Misra, P.~Bojanowski, P.~Vincent, M.~Rabbat, Y.~LeCun, and N.~Ballas.
\newblock Self-supervised learning from images with a joint-embedding predictive architecture.
\newblock In \emph{CVPR}, 2023.

\bibitem[Azuma et~al.(2022)Azuma, Miyanishi, Kurita, and Kawanabe]{scanqa}
D.~Azuma, T.~Miyanishi, S.~Kurita, and M.~Kawanabe.
\newblock {ScanQA: 3D question answering for spatial scene understanding}.
\newblock In \emph{CVPR}, 2022.

\bibitem[Banani et~al.(2024)Banani, Raj, Maninis, Kar, Li, Rubinstein, Sun, Guibas, Johnson, and Jampani]{probing3d}
M.~E. Banani, A.~Raj, K.-K. Maninis, A.~Kar, Y.~Li, M.~Rubinstein, D.~Sun, L.~Guibas, J.~Johnson, and V.~Jampani.
\newblock {Probing the 3D awareness of visual foundation models}.
\newblock In \emph{CVPR}, 2024.

\bibitem[Banerjee and Lavie(2005)]{meteor}
S.~Banerjee and A.~Lavie.
\newblock {METEOR: An automatic metric for MT evaluation with improved correlation with human judgments}.
\newblock In \emph{ACL Workshop}, 2005.

\bibitem[Bao et~al.(2022)Bao, Dong, Piao, and Wei]{beit}
H.~Bao, L.~Dong, S.~Piao, and F.~Wei.
\newblock {BeiT: Bert pre-training of image transformers}.
\newblock In \emph{ICLR}, 2022.

\bibitem[Baranchuk et~al.(2022)Baranchuk, Rubachev, Voynov, Khrulkov, and Babenko]{labelefficientdiffusion}
D.~Baranchuk, I.~Rubachev, A.~Voynov, V.~Khrulkov, and A.~Babenko.
\newblock Label-efficient semantic segmentation with diffusion models.
\newblock In \emph{ICLR}, 2022.

\bibitem[Bardes et~al.(2023)Bardes, Ponce, and LeCun]{mcjepa}
A.~Bardes, J.~Ponce, and Y.~LeCun.
\newblock {MC-JEPA: A joint-embedding predictive architecture for self-supervised learning of motion and content features}.
\newblock \emph{arXiv preprint arXiv:2307.12698}, 2023.

\bibitem[Bardes et~al.(2024)Bardes, Garrido, Ponce, Chen, Rabbat, LeCun, Assran, and Ballas]{vjepa}
A.~Bardes, Q.~Garrido, J.~Ponce, X.~Chen, M.~Rabbat, Y.~LeCun, M.~Assran, and N.~Ballas.
\newblock {V-JEPA: Latent video prediction for visual representation learning}.
\newblock \emph{arXiv preprint arXiv:2404.08471}, 2024.

\bibitem[Blattmann et~al.(2023)Blattmann, Dockhorn, Kulal, Mendelevitch, Kilian, Lorenz, Levi, English, Voleti, Letts, Jampani, and Rombach]{stablevideodiffusion}
A.~Blattmann, T.~Dockhorn, S.~Kulal, D.~Mendelevitch, M.~Kilian, D.~Lorenz, Y.~Levi, Z.~English, V.~Voleti, A.~Letts, V.~Jampani, and R.~Rombach.
\newblock {Stable Video Diffusion: Scaling latent video diffusion models to large datasets}.
\newblock \emph{arXiv preprint arXiv:2311.15127}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal, A.~Herbert-Voss, G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~M. Ziegler, J.~Wu, C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray, B.~Chess, J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and D.~Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal, Bojanowski, and Joulin]{dino}
M.~Caron, H.~Touvron, I.~Misra, H.~J{\'e}gou, J.~Mairal, P.~Bojanowski, and A.~Joulin.
\newblock {Emerging properties in self-supervised vision transformers}.
\newblock In \emph{ICCV}, 2021.

\bibitem[Chang et~al.(2017)Chang, Dai, Funkhouser, Halber, Niessner, Savva, Song, Zeng, and Zhang]{matterport3d}
A.~Chang, A.~Dai, T.~Funkhouser, M.~Halber, M.~Niessner, M.~Savva, S.~Song, A.~Zeng, and Y.~Zhang.
\newblock {Matterport3D: Learning from {RGB-D} data in indoor environments}.
\newblock In \emph{3DV}, 2017.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Chang, and Nie{\ss}ner]{scanrefer}
D.~Z. Chen, A.~X. Chang, and M.~Nie{\ss}ner.
\newblock {ScanRefer: 3D object localization in {RGB-D} scans using natural language}.
\newblock In \emph{ECCV}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2023)Chen, Sun, Song, and Luo]{diffusiondet}
S.~Chen, P.~Sun, Y.~Song, and P.~Luo.
\newblock {DiffusionDet}: Diffusion model for object detection.
\newblock In \emph{ICCV}, 2023.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Kornblith, Norouzi, and Hinton]{simclr}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton.
\newblock A simple framework for contrastive learning of visual representations.
\newblock In \emph{ICML}, 2020{\natexlab{b}}.

\bibitem[Chen and He(2021)]{simsiam}
X.~Chen and K.~He.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{CVPR}, 2021.

\bibitem[Dai et~al.(2017)Dai, Chang, Savva, Halber, Funkhouser, and Nie{\ss}ner]{scannet}
A.~Dai, A.~X. Chang, M.~Savva, M.~Halber, T.~Funkhouser, and M.~Nie{\ss}ner.
\newblock {ScanNet: Richly-annotated 3D reconstructions of indoor scenes}.
\newblock In \emph{CVPR}, 2017.

\bibitem[Deitke et~al.(2023)Deitke, Schwenk, Salvador, Weihs, Michel, VanderBilt, Schmidt, Ehsani, Kembhavi, and Farhadi]{deitke2023objaverse}
M.~Deitke, D.~Schwenk, J.~Salvador, L.~Weihs, O.~Michel, E.~VanderBilt, L.~Schmidt, K.~Ehsani, A.~Kembhavi, and A.~Farhadi.
\newblock {Objaverse: A universe of annotated 3D objects}.
\newblock In \emph{CVPR}, 2023.

\bibitem[Ding et~al.(2023)Ding, Yang, Xue, Zhang, Bai, and Qi]{pla3d}
R.~Ding, J.~Yang, C.~Xue, W.~Zhang, S.~Bai, and X.~Qi.
\newblock {PLA: Language-driven open-Vocabulary 3D scene understanding}.
\newblock In \emph{CVPR}, 2023.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{vit}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit, and N.~Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Duan et~al.(2023)Duan, Guo, and Zhu]{diffusiondepth}
Y.~Duan, X.~Guo, and Z.~Zhu.
\newblock {DiffusionDepth}: Diffusion denoising approach for monocular depth estimation.
\newblock \emph{arXiv preprint arXiv:2303.05021}, 2023.

\bibitem[Fang et~al.(2023)Fang, Hu, Luo, and Tan]{ctrlroom}
C.~Fang, X.~Hu, K.~Luo, and P.~Tan.
\newblock {Ctrl-Room: Controllable text-to-3D room meshes generation with layout constraints}.
\newblock \emph{arXiv preprint arXiv:2310.03602}, 2023.

\bibitem[Fridman et~al.(2023)Fridman, Abecasis, Kasten, and Dekel]{scenescape}
R.~Fridman, A.~Abecasis, Y.~Kasten, and T.~Dekel.
\newblock {SceneScape: Text-driven consistent scene generation}.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Gao et~al.(2024)Gao, Liu, Chen, Geiger, and Sch{\"o}lkopf]{graphdreamer}
G.~Gao, W.~Liu, A.~Chen, A.~Geiger, and B.~Sch{\"o}lkopf.
\newblock {GraphDreamer: Compositional 3D scene synthesis from scene graphs}.
\newblock In \emph{CVPR}, 2024.

\bibitem[Gidaris et~al.(2018)Gidaris, Singh, and Komodakis]{rotation}
S.~Gidaris, P.~Singh, and N.~Komodakis.
\newblock Unsupervised representation learning by predicting image rotations.
\newblock In \emph{ICLR}, 2018.

\bibitem[Girdhar et~al.(2023)Girdhar, El-Nouby, Singh, Alwala, Joulin, and Misra]{omnimae}
R.~Girdhar, A.~El-Nouby, M.~Singh, K.~V. Alwala, A.~Joulin, and I.~Misra.
\newblock {OmniMAE: Single model masked pretraining on images and videos}.
\newblock In \emph{CVPR}, 2023.

\bibitem[Grill et~al.(2020)Grill, Strub, Altché, Tallec, Richemond, Buchatskaya, Doersch, Pires, Guo, Azar, Piot, Kavukcuoglu, Munos, and Valko]{byol}
J.-B. Grill, F.~Strub, F.~Altché, C.~Tallec, P.~H. Richemond, E.~Buchatskaya, C.~Doersch, B.~A. Pires, Z.~D. Guo, M.~G. Azar, B.~Piot, K.~Kavukcuoglu, R.~Munos, and M.~Valko.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{moco}
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{CVPR}, 2020.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and Girshick]{mae}
K.~He, X.~Chen, S.~Xie, Y.~Li, P.~Doll{\'a}r, and R.~Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{CVPR}, 2022.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{diffusion2020}
J.~Ho, A.~Jain, and P.~Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[H{\"o}llein et~al.(2023)H{\"o}llein, Cao, Owens, Johnson, and Nie{\ss}ner]{text2room}
L.~H{\"o}llein, A.~Cao, A.~Owens, J.~Johnson, and M.~Nie{\ss}ner.
\newblock {Text2Room: Extracting textured 3D meshes from 2D text-to-image models}.
\newblock In \emph{ICCV}, 2023.

\bibitem[Hong et~al.(2023{\natexlab{a}})Hong, Lin, Du, Chen, Tenenbaum, and Gan]{3dconcept}
Y.~Hong, C.~Lin, Y.~Du, Z.~Chen, J.~B. Tenenbaum, and C.~Gan.
\newblock {3D concept learning and reasoning from multi-view images}.
\newblock In \emph{CVPR}, 2023{\natexlab{a}}.

\bibitem[Hong et~al.(2023{\natexlab{b}})Hong, Zhen, Chen, Zheng, Du, Chen, and Gan]{3dllm}
Y.~Hong, H.~Zhen, P.~Chen, S.~Zheng, Y.~Du, Z.~Chen, and C.~Gan.
\newblock {3D-LLM: Injecting the 3D world into large language models}.
\newblock In \emph{NeurIPS}, 2023{\natexlab{b}}.

\bibitem[Hong et~al.(2024)Hong, Zheng, Chen, Wang, Li, and Gan]{multiply}
Y.~Hong, Z.~Zheng, P.~Chen, Y.~Wang, J.~Li, and C.~Gan.
\newblock {MultiPLY: A multisensory object-centric embodied large language model in 3D world}.
\newblock In \emph{CVPR}, 2024.

\bibitem[Hou et~al.(2021)Hou, Graham, Nie{\ss}ner, and Xie]{contrastivescenecontexts}
J.~Hou, B.~Graham, M.~Nie{\ss}ner, and S.~Xie.
\newblock {Exploring data-efficient 3D scene understanding with contrastive scene contexts}.
\newblock In \emph{CVPR}, 2021.

\bibitem[Joulin et~al.(2016)Joulin, Van Der~Maaten, Jabri, and Vasilache]{joulin2016learning}
A.~Joulin, L.~Van Der~Maaten, A.~Jabri, and N.~Vasilache.
\newblock Learning visual features from large weakly supervised data.
\newblock In \emph{ECCV}, 2016.

\bibitem[Kabsch(1976)]{Kabsch}
W.~Kabsch.
\newblock A solution for the best rotation to relate two sets of vectors.
\newblock \emph{Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and General Crystallography}, 32\penalty0 (5):\penalty0 922--923, 1976.

\bibitem[Kerr et~al.(2023)Kerr, Kim, Goldberg, Kanazawa, and Tancik]{lerf}
J.~Kerr, C.~M. Kim, K.~Goldberg, A.~Kanazawa, and M.~Tancik.
\newblock {LERF: Language embedded radiance fields}.
\newblock In \emph{ICCV}, 2023.

\bibitem[Kingma and Ba(2015)]{adam}
D.~P. Kingma and J.~Ba.
\newblock {Adam: A method for stochastic optimization}.
\newblock In \emph{ICLR}, 2015.

\bibitem[Kirillov et~al.(2023)Kirillov, Mintun, Ravi, Mao, Rolland, Gustafson, Xiao, Whitehead, Berg, Lo, et~al.]{segmentanything}
A.~Kirillov, E.~Mintun, N.~Ravi, H.~Mao, C.~Rolland, L.~Gustafson, T.~Xiao, S.~Whitehead, A.~C. Berg, W.-Y. Lo, et~al.
\newblock {Segment Anything}.
\newblock In \emph{ICCV}, 2023.

\bibitem[Lai et~al.(2023)Lai, Tian, Chen, Li, Yuan, Liu, and Jia]{lisa}
X.~Lai, Z.~Tian, Y.~Chen, Y.~Li, Y.~Yuan, S.~Liu, and J.~Jia.
\newblock {LISA: Reasoning segmentation via large language model}.
\newblock \emph{arXiv preprint arXiv:2308.00692}, 2023.

\bibitem[LeCun(2022)]{lecunpath}
Y.~LeCun.
\newblock A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27.
\newblock \emph{Open Review}, 62\penalty0 (1), 2022.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Weinberger, Belongie, Koltun, and Ranftl]{lseg}
B.~Li, K.~Q. Weinberger, S.~Belongie, V.~Koltun, and R.~Ranftl.
\newblock {Language-driven semantic segmentation}.
\newblock In \emph{ICLR}, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Li, Xiong, and Hoi]{blip}
J.~Li, D.~Li, C.~Xiong, and S.~Hoi.
\newblock {BLIP: Bootstrapping language-image pre-training for unified vision-language understanding and generation}.
\newblock In \emph{ICML}, 2022{\natexlab{b}}.

\bibitem[Li et~al.(2023)Li, Li, Savarese, and Hoi]{blip2}
J.~Li, D.~Li, S.~Savarese, and S.~Hoi.
\newblock {BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large language models}.
\newblock In \emph{ICML}, 2023.

\bibitem[Lin(2004)]{rouge}
C.-Y. Lin.
\newblock {ROUGE: A package for automatic evaluation of summaries}.
\newblock In \emph{ACL}, 2004.

\bibitem[Liu et~al.(2022)Liu, Cai, and Lee]{maskpoint}
H.~Liu, M.~Cai, and Y.~J. Lee.
\newblock Masked discrimination for self-supervised learning on point clouds.
\newblock In \emph{ECCV}, 2022.

\bibitem[Liu et~al.(2023)Liu, Wang, Liu, Jiang, Pollefeys, and Wang]{regformer}
J.~Liu, G.~Wang, Z.~Liu, C.~Jiang, M.~Pollefeys, and H.~Wang.
\newblock {RegFormer}: An efficient projection-aware transformer network for large-scale point cloud registration.
\newblock In \emph{ICCV}, 2023.

\bibitem[Liu et~al.(2024)Liu, Shi, Kuang, Zhu, Li, Han, Cai, Porikli, and Su]{liu2024openshape}
M.~Liu, R.~Shi, K.~Kuang, Y.~Zhu, X.~Li, S.~Han, H.~Cai, F.~Porikli, and H.~Su.
\newblock {OpenShape: Scaling up 3D shape representation towards open-world understanding}.
\newblock In \emph{NeurIPS}, 2024.

\bibitem[Loshchilov and Hutter(2017)]{adamw}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Ma et~al.(2023{\natexlab{a}})Ma, Yang, Ju, Zhang, Liu, Wang, Zhang, and Wang]{diffusionseg}
C.~Ma, Y.~Yang, C.~Ju, F.~Zhang, J.~Liu, Y.~Wang, Y.~Zhang, and Y.~Wang.
\newblock {DiffusionSeg: Adapting diffusion towards unsupervised object discovery}.
\newblock \emph{arXiv preprint arXiv:2303.09813}, 2023{\natexlab{a}}.

\bibitem[Ma et~al.(2023{\natexlab{b}})Ma, Yong, Zheng, Li, Liang, Zhu, and Huang]{sqa3d}
X.~Ma, S.~Yong, Z.~Zheng, Q.~Li, Y.~Liang, S.-C. Zhu, and S.~Huang.
\newblock {SQA3D: Situated question answering in 3D scenes}.
\newblock In \emph{ICLR}, 2023{\natexlab{b}}.

\bibitem[Mahajan et~al.(2018)Mahajan, Girshick, Ramanathan, He, Paluri, Li, Bharambe, and Van Der~Maaten]{mahajan2018exploring}
D.~Mahajan, R.~Girshick, V.~Ramanathan, K.~He, M.~Paluri, Y.~Li, A.~Bharambe, and L.~Van Der~Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In \emph{ECCV}, 2018.

\bibitem[Man et~al.(2023)Man, Gui, and Wang]{bevguide}
Y.~Man, L.-Y. Gui, and Y.-X. Wang.
\newblock {BEV-guided} multi-modality fusion for driving perception.
\newblock In \emph{CVPR}, 2023.

\bibitem[Man et~al.(2024)Man, Gui, and Wang]{sig3d}
Y.~Man, L.-Y. Gui, and Y.-X. Wang.
\newblock Situational awareness matters in {3D} vision language reasoning.
\newblock In \emph{CVPR}, 2024.

\bibitem[Namekata et~al.(2024)Namekata, Sabour, Fidler, and Kim]{emerdiff}
K.~Namekata, A.~Sabour, S.~Fidler, and S.~W. Kim.
\newblock {EmerDiff: Emerging pixel-level semantic knowledge in diffusion models}.
\newblock In \emph{ICLR}, 2024.

\bibitem[Nasiriany et~al.(2024)Nasiriany, Xia, Yu, Xiao, Liang, Dasgupta, Xie, Driess, Wahid, Xu, Vuong, Zhang, Lee, Lee, Xu, Kirmani, Zhu, Zeng, Hausman, Heess, Finn, Levine, and Ichter]{pivot}
S.~Nasiriany, F.~Xia, W.~Yu, T.~Xiao, J.~Liang, I.~Dasgupta, A.~Xie, D.~Driess, A.~Wahid, Z.~Xu, Q.~Vuong, T.~Zhang, T.-W.~E. Lee, K.-H. Lee, P.~Xu, S.~Kirmani, Y.~Zhu, A.~Zeng, K.~Hausman, N.~Heess, C.~Finn, S.~Levine, and B.~Ichter.
\newblock {PIVOT: Iterative visual prompting elicits actionable knowledge for VLMs}.
\newblock \emph{arXiv preprint arXiv:2402.07872}, 2024.

\bibitem[Oquab et~al.(2024)Oquab, Darcet, Moutakanni, Vo, Szafraniec, Khalidov, Fernandez, Haziza, Massa, El-Nouby, Assran, Ballas, Galuba, Howes, Huang, Li, Misra, Rabbat, Sharma, Synnaeve, Xu, Jegou, Mairal, Labatut, Joulin, and Bojanowski]{dinov2}
M.~Oquab, T.~Darcet, T.~Moutakanni, H.~Vo, M.~Szafraniec, V.~Khalidov, P.~Fernandez, D.~Haziza, F.~Massa, A.~El-Nouby, M.~Assran, N.~Ballas, W.~Galuba, R.~Howes, P.-Y. Huang, S.-W. Li, I.~Misra, M.~Rabbat, V.~Sharma, G.~Synnaeve, H.~Xu, H.~Jegou, J.~Mairal, P.~Labatut, A.~Joulin, and P.~Bojanowski.
\newblock {DINOv2: Learning robust visual features without supervision}.
\newblock \emph{Transactions on Machine Learning Research}, 2024.

\bibitem[Pang et~al.(2022)Pang, Wang, Tay, Liu, Tian, and Yuan]{pointmae}
Y.~Pang, W.~Wang, F.~E. Tay, W.~Liu, Y.~Tian, and L.~Yuan.
\newblock Masked autoencoders for point cloud self-supervised learning.
\newblock In \emph{ECCV}, 2022.

\bibitem[Pang et~al.(2024)Pang, Xie, Man, and Wang]{frozenlm4vision}
Z.~Pang, Z.~Xie, Y.~Man, and Y.-X. Wang.
\newblock Frozen transformers in language models are effective visual encoder layers.
\newblock In \emph{ICLR}, 2024.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu]{bleu}
K.~Papineni, S.~Roukos, T.~Ward, and W.-J. Zhu.
\newblock {BLEU: a method for automatic evaluation of machine translation}.
\newblock In \emph{ACL}, 2002.

\bibitem[Pathak et~al.(2016)Pathak, Krahenbuhl, Donahue, Darrell, and Efros]{inpainting}
D.~Pathak, P.~Krahenbuhl, J.~Donahue, T.~Darrell, and A.~A. Efros.
\newblock {Context encoders: Feature learning by inpainting}.
\newblock In \emph{CVPR}, 2016.

\bibitem[Peng et~al.(2023)Peng, Genova, Jiang, Tagliasacchi, Pollefeys, and Funkhouser]{openscene}
S.~Peng, K.~Genova, C.~M. Jiang, A.~Tagliasacchi, M.~Pollefeys, and T.~Funkhouser.
\newblock {OpenScene: 3D scene understanding with open vocabularies}.
\newblock In \emph{CVPR}, 2023.

\bibitem[Peng et~al.(2024)Peng, Wang, Dong, Hao, Huang, Ma, Ye, and Wei]{kosmos2}
Z.~Peng, W.~Wang, L.~Dong, Y.~Hao, S.~Huang, S.~Ma, Q.~Ye, and F.~Wei.
\newblock {Kosmos-2: Grounding multimodal large language models to the world}.
\newblock In \emph{ICLR}, 2024.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever]{clip}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry, A.~Askell, P.~Mishkin, J.~Clark, G.~Krueger, and I.~Sutskever.
\newblock {Learning transferable visual models from natural language supervision}.
\newblock In \emph{ICML}, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{googlet5}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou, W.~Li, and P.~J. Liu.
\newblock {Exploring the limits of transfer learning with a unified text-to-text transformer}.
\newblock \emph{JMLR}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Ranzinger et~al.(2024)Ranzinger, Heinrich, Kautz, and Molchanov]{amradio}
M.~Ranzinger, G.~Heinrich, J.~Kautz, and P.~Molchanov.
\newblock {AM-RADIO: Agglomerative model--reduce all domains into one}.
\newblock In \emph{CVPR}, 2024.

\bibitem[Rasheed et~al.(2024)Rasheed, Maaz, Shaji, Shaker, Khan, Cholakkal, Anwer, Xing, Yang, and Khan]{glamm}
H.~Rasheed, M.~Maaz, S.~Shaji, A.~Shaker, S.~Khan, H.~Cholakkal, R.~M. Anwer, E.~Xing, M.-H. Yang, and F.~S. Khan.
\newblock {GLaMM}: Pixel grounding large multimodal model.
\newblock In \emph{CVPR}, 2024.

\bibitem[Roberts et~al.(2021)Roberts, Ramapuram, Ranjan, Kumar, Bautista, Paczan, Webb, and Susskind]{hypersim}
M.~Roberts, J.~Ramapuram, A.~Ranjan, A.~Kumar, M.~A. Bautista, N.~Paczan, R.~Webb, and J.~M. Susskind.
\newblock {HyperSim: A photorealistic synthetic dataset for holistic indoor scene understanding}.
\newblock In \emph{ICCV}, 2021.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{stablediffusion}
R.~Rombach, A.~Blattmann, D.~Lorenz, P.~Esser, and B.~Ommer.
\newblock {High-resolution image synthesis with latent diffusion models}.
\newblock In \emph{CVPR}, 2022.

\bibitem[Saxena et~al.(2023)Saxena, Kar, Norouzi, and Fleet]{monoculardepthdiffusion}
S.~Saxena, A.~Kar, M.~Norouzi, and D.~J. Fleet.
\newblock Monocular depth estimation using diffusion models.
\newblock \emph{arXiv preprint arXiv:2302.14816}, 2023.

\bibitem[Schonberger and Frahm(2016)]{colmap}
J.~L. Schonberger and J.-M. Frahm.
\newblock Structure-from-motion revisited.
\newblock In \emph{CVPR}, 2016.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman, Cherti, Coombes, Katta, Mullis, Wortsman, Schramowski, Kundurthy, Crowson, Schmidt, Kaczmarczyk, and Jitsev]{schuhmann2022laion}
C.~Schuhmann, R.~Beaumont, R.~Vencu, C.~Gordon, R.~Wightman, M.~Cherti, T.~Coombes, A.~Katta, C.~Mullis, M.~Wortsman, P.~Schramowski, S.~Kundurthy, K.~Crowson, L.~Schmidt, R.~Kaczmarczyk, and J.~Jitsev.
\newblock {Laion-5B: An open large-scale dataset for training next generation image-text models}.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Schult et~al.(2024)Schult, Tsai, Höllein, Wu, Wang, Ma, Li, Wang, Wimbauer, He, Zhang, Leibe, Vajda, and Hou]{controlroom3d}
J.~Schult, S.~Tsai, L.~Höllein, B.~Wu, J.~Wang, C.-Y. Ma, K.~Li, X.~Wang, F.~Wimbauer, Z.~He, P.~Zhang, B.~Leibe, P.~Vajda, and J.~Hou.
\newblock {ControlRoom3D: Room generation using semantic proxy rooms}.
\newblock In \emph{CVPR}, 2024.

\bibitem[Sima et~al.(2023)Sima, Renz, Chitta, Chen, Zhang, Xie, Luo, Geiger, and Li]{drivelm}
C.~Sima, K.~Renz, K.~Chitta, L.~Chen, H.~Zhang, C.~Xie, P.~Luo, A.~Geiger, and H.~Li.
\newblock {DriveLM: Driving with graph visual question answering}.
\newblock \emph{arXiv preprint arXiv:2312.14150}, 2023.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and Ganguli]{diffusion2015}
J.~Sohl-Dickstein, E.~Weiss, N.~Maheswaranathan, and S.~Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{ICML}, 2015.

\bibitem[Tang et~al.(2023)Tang, Jia, Wang, Phoo, and Hariharan]{tang2023emergent}
L.~Tang, M.~Jia, Q.~Wang, C.~P. Phoo, and B.~Hariharan.
\newblock Emergent correspondence from image diffusion.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{gemini}
G.~Team, R.~Anil, S.~Borgeaud, Y.~Wu, J.-B. Alayrac, J.~Yu, R.~Soricut, J.~Schalkwyk, A.~M. Dai, A.~Hauth, et~al.
\newblock {Gemini: A family of highly capable multimodal models}.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Tian et~al.(2024)Tian, Gu, Li, Liu, Hu, Wang, Zhan, Jia, Lang, and Zhao]{drivevlm}
X.~Tian, J.~Gu, B.~Li, Y.~Liu, C.~Hu, Y.~Wang, K.~Zhan, P.~Jia, X.~Lang, and H.~Zhao.
\newblock {DriveVLM}: {The} convergence of autonomous driving and large vision-language models.
\newblock \emph{arXiv preprint arXiv:2402.12289}, 2024.

\bibitem[Tong et~al.(2022)Tong, Song, Wang, and Wang]{videomae}
Z.~Tong, Y.~Song, J.~Wang, and L.~Wang.
\newblock {VideoMAE: Masked autoencoders are data-efficient learners for self-supervised video pre-training}.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Umeyama(1991)]{Umeyama}
S.~Umeyama.
\newblock Least-squares estimation of transformation parameters between two point patterns.
\newblock \emph{TPAMI}, 13\penalty0 (04):\penalty0 376--380, 1991.

\bibitem[Vedantam et~al.(2015)Vedantam, Lawrence~Zitnick, and Parikh]{cider}
R.~Vedantam, C.~Lawrence~Zitnick, and D.~Parikh.
\newblock {CIDER: Consensus-based image description evaluation}.
\newblock In \emph{CVPR}, 2015.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Huang, Zhao, Tong, He, Wang, Wang, and Qiao]{videomaev2}
L.~Wang, B.~Huang, Z.~Zhao, Z.~Tong, Y.~He, Y.~Wang, Y.~Wang, and Y.~Qiao.
\newblock {VideoMAEv2: Scaling video masked autoencoders with dual masking}.
\newblock In \emph{CVPR}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Chen, Chen, Wu, Zhu, Zeng, Luo, Lu, Zhou, Qiao, and Dai]{visionllm}
W.~Wang, Z.~Chen, X.~Chen, J.~Wu, X.~Zhu, G.~Zeng, P.~Luo, T.~Lu, J.~Zhou, Y.~Qiao, and J.~Dai.
\newblock {VisionLLM: Large language model is also an open-ended decoder for vision-centric tasks}.
\newblock In \emph{NeurIPS}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2022)Wang, Li, Li, He, Huang, Zhao, Zhang, Xu, Liu, Wang, Xing, Chen, Pan, Yu, Wang, Wang, and Qiao]{internvideo}
Y.~Wang, K.~Li, Y.~Li, Y.~He, B.~Huang, Z.~Zhao, H.~Zhang, J.~Xu, Y.~Liu, Z.~Wang, S.~Xing, G.~Chen, J.~Pan, J.~Yu, Y.~Wang, L.~Wang, and Y.~Qiao.
\newblock {InternVideo: General video foundation models via generative and discriminative learning}.
\newblock \emph{arXiv preprint arXiv:2212.03191}, 2022.

\bibitem[Wu et~al.(2023{\natexlab{a}})Wu, Zhao, Shou, Zhou, and Shen]{diffumask}
W.~Wu, Y.~Zhao, M.~Z. Shou, H.~Zhou, and C.~Shen.
\newblock {DiffuMNask: Synthesizing images with pixel-level annotations for semantic segmentation using diffusion models}.
\newblock In \emph{ICCV}, 2023{\natexlab{a}}.

\bibitem[Wu et~al.(2023{\natexlab{b}})Wu, Wen, Liu, and Zhao]{maskscene}
X.~Wu, X.~Wen, X.~Liu, and H.~Zhao.
\newblock {Masked scene contrast: A scalable framework for unsupervised 3D representation learning}.
\newblock In \emph{CVPR}, 2023{\natexlab{b}}.

\bibitem[Xie et~al.(2020)Xie, Gu, Guo, Qi, Guibas, and Litany]{pointcontrast}
S.~Xie, J.~Gu, D.~Guo, C.~R. Qi, L.~Guibas, and O.~Litany.
\newblock {PointContrast: Unsupervised pre-training for 3D point cloud understanding}.
\newblock In \emph{ECCV}, 2020.

\bibitem[Xu et~al.(2021)Xu, Ghosh, Huang, Okhonko, Aghajanyan, Metze, Zettlemoyer, and Feichtenhofer]{videoclip}
H.~Xu, G.~Ghosh, P.-Y. Huang, D.~Okhonko, A.~Aghajanyan, F.~Metze, L.~Zettlemoyer, and C.~Feichtenhofer.
\newblock {VideoCLIP: Contrastive pre-training for zero-shot video-text understanding}.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Xu et~al.(2023)Xu, Liu, Vahdat, Byeon, Wang, and De~Mello]{odise}
J.~Xu, S.~Liu, A.~Vahdat, W.~Byeon, X.~Wang, and S.~De~Mello.
\newblock Open-vocabulary panoptic segmentation with text-to-image diffusion models.
\newblock In \emph{CVPR}, 2023.

\bibitem[Xu et~al.(2024)Xu, Zhou, Yan, Gu, Arnab, Sun, Wang, and Schmid]{pixelllm}
J.~Xu, X.~Zhou, S.~Yan, X.~Gu, A.~Arnab, C.~Sun, X.~Wang, and C.~Schmid.
\newblock Pixel aligned language models.
\newblock In \emph{CVPR}, 2024.

\bibitem[Yan et~al.(2024)Yan, Yang, Guo, Pan, Wang, Tong, Liu, and Huang]{pointmask}
S.~Yan, Y.~Yang, Y.~Guo, H.~Pan, P.-s. Wang, X.~Tong, Y.~Liu, and Q.~Huang.
\newblock {3D feature prediction for masked-autoencoder-based point cloud pretraining}.
\newblock In \emph{ICLR}, 2024.

\bibitem[Yang et~al.(2024)Yang, Man, Chen, and Wang]{scenecraft}
X.~Yang, Y.~Man, J.-K. Chen, and Y.-X. Wang.
\newblock {SceneCraft}: Layout-guided {3D} scene generation.
\newblock In \emph{NeurIPS}, 2024.

\bibitem[Yang et~al.(2023)Yang, Guo, Xiong, Liu, Pan, Wang, Tong, and Guo]{swin3d}
Y.-Q. Yang, Y.-X. Guo, J.-Y. Xiong, Y.~Liu, H.~Pan, P.-S. Wang, X.~Tong, and B.~Guo.
\newblock {Swin3D: A pretrained transformer backbone for 3D indoor scene understanding}.
\newblock \emph{arXiv preprint arXiv:2304.06906}, 2023.

\bibitem[Yeshwanth et~al.(2023)Yeshwanth, Liu, Nie{\ss}ner, and Dai]{scannet++}
C.~Yeshwanth, Y.-C. Liu, M.~Nie{\ss}ner, and A.~Dai.
\newblock {ScanNet++: A high-fidelity dataset of 3D indoor scenes}.
\newblock In \emph{ICCV}, 2023.

\bibitem[Yew and Lee(2022)]{regtr}
Z.~J. Yew and G.~H. Lee.
\newblock {REGTR: End-to-end point cloud correspondences with transformers}.
\newblock In \emph{CVPR}, 2022.

\bibitem[Yu et~al.(2022)Yu, Tang, Rao, Huang, Zhou, and Lu]{pointbert}
X.~Yu, L.~Tang, Y.~Rao, T.~Huang, J.~Zhou, and J.~Lu.
\newblock {Point-BERT: Pre-training 3D point cloud transformers with masked point modeling}.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zellers et~al.(2022)Zellers, Lu, Lu, Yu, Zhao, Salehi, Kusupati, Hessel, Farhadi, and Choi]{merlot}
R.~Zellers, J.~Lu, X.~Lu, Y.~Yu, Y.~Zhao, M.~Salehi, A.~Kusupati, J.~Hessel, A.~Farhadi, and Y.~Choi.
\newblock {Merlot reserve: Neural script knowledge through vision and language and sound}.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zhan et~al.(2023)Zhan, Zheng, Xie, and Zisserman]{whatdoesdiffusionknow}
G.~Zhan, C.~Zheng, W.~Xie, and A.~Zisserman.
\newblock What does stable diffusion know about the 3d scene?
\newblock \emph{arXiv preprint arXiv:2310.06836}, 2023.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Herrmann, Hur, Polania~Cabrera, Jampani, Sun, and Yang]{ataleoftwofeatures}
J.~Zhang, C.~Herrmann, J.~Hur, L.~Polania~Cabrera, V.~Jampani, D.~Sun, and M.-H. Yang.
\newblock A tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence.
\newblock In \emph{NeurIPS}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2016)Zhang, Isola, and Efros]{colorization}
R.~Zhang, P.~Isola, and A.~A. Efros.
\newblock Colorful image colorization.
\newblock In \emph{ECCV}, 2016.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Guo, Gao, Fang, Zhao, Wang, Qiao, and Li]{pointm2ae}
R.~Zhang, Z.~Guo, P.~Gao, R.~Fang, B.~Zhao, D.~Wang, Y.~Qiao, and H.~Li.
\newblock {Point-M2AE: multi-scale masked autoencoders for hierarchical point cloud pre-training}.
\newblock In \emph{NeurIPS}, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2024)Zhang, Han, Zhou, Hu, Yan, Lu, Li, Gao, and Qiao]{llama}
R.~Zhang, J.~Han, A.~Zhou, X.~Hu, S.~Yan, P.~Lu, H.~Li, P.~Gao, and Y.~Qiao.
\newblock {LLaMA-Adapter: Efficient fine-tuning of language models with zero-init attention}.
\newblock In \emph{ICLR}, 2024.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang, and Zettlemoyer]{opt}
S.~Zhang, S.~Roller, N.~Goyal, M.~Artetxe, M.~Chen, S.~Chen, C.~Dewan, M.~Diab, X.~Li, X.~V. Lin, T.~Mihaylov, M.~Ott, S.~Shleifer, K.~Shuster, D.~Simig, P.~S. Koura, A.~Sridhar, T.~Wang, and L.~Zettlemoyer.
\newblock {OPT: Open pre-trained transformer language models}.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022{\natexlab{b}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Gong, and Chang]{multi3drefer}
Y.~Zhang, Z.~Gong, and A.~X. Chang.
\newblock {Multi3DRefer: Grounding text description to multiple 3D objects}.
\newblock In \emph{ICCV}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2021)Zhang, Girdhar, Joulin, and Misra]{depthcontrast}
Z.~Zhang, R.~Girdhar, A.~Joulin, and I.~Misra.
\newblock {Self-supervised pretraining of 3D features on any point-cloud}.
\newblock In \emph{ICCV}, 2021.

\bibitem[Zhang et~al.(2023{\natexlab{c}})Zhang, Yang, Wang, and Li]{growsp}
Z.~Zhang, B.~Yang, B.~Wang, and B.~Li.
\newblock {GrowSP}: Unsupervised semantic segmentation of {3D} point clouds.
\newblock In \emph{CVPR}, 2023{\natexlab{c}}.

\bibitem[Zhao et~al.(2023)Zhao, Rao, Liu, Liu, Zhou, and Lu]{unleashingdiffusiondepth}
W.~Zhao, Y.~Rao, Z.~Liu, B.~Liu, J.~Zhou, and J.~Lu.
\newblock Unleashing text-to-image diffusion models for visual perception.
\newblock In \emph{ICCV}, 2023.

\bibitem[Zhen et~al.(2024)Zhen, Qiu, Chen, Yang, Yan, Du, Hong, and Gan]{3dvla}
H.~Zhen, X.~Qiu, P.~Chen, J.~Yang, X.~Yan, Y.~Du, Y.~Hong, and C.~Gan.
\newblock {3D-VLA: A 3D vision-language-action generative world model}.
\newblock In \emph{ICML}, 2024.

\bibitem[Zheng et~al.(2020{\natexlab{a}})Zheng, Zhang, Li, Tang, Gao, and Zhou]{structured3d}
J.~Zheng, J.~Zhang, J.~Li, R.~Tang, S.~Gao, and Z.~Zhou.
\newblock {Structured3D: A large photo-realistic dataset for structured 3D modeling}.
\newblock In \emph{ECCV}, 2020{\natexlab{a}}.

\bibitem[Zheng et~al.(2020{\natexlab{b}})Zheng, Zhang, Li, Tang, Gao, and Zhou]{zheng2020structured3d}
J.~Zheng, J.~Zhang, J.~Li, R.~Tang, S.~Gao, and Z.~Zhou.
\newblock {Structured3D: A large photo-realistic dataset for structured 3D modeling}.
\newblock In \emph{ECCV}, 2020{\natexlab{b}}.

\bibitem[Zhou et~al.(2022)Zhou, Wei, Wang, Shen, Xie, Yuille, and Kong]{ibot}
J.~Zhou, C.~Wei, H.~Wang, W.~Shen, C.~Xie, A.~Yuille, and T.~Kong.
\newblock {iBOT: Image bert pre-training with online tokenizer}.
\newblock In \emph{ICLR}, 2022.

\bibitem[Zhou et~al.(2024{\natexlab{a}})Zhou, Wang, Ma, Liu, Huang, and Wang]{uni3d}
J.~Zhou, J.~Wang, B.~Ma, Y.-S. Liu, T.~Huang, and X.~Wang.
\newblock {Uni3D: Exploring unified 3D representation at scale}.
\newblock In \emph{ICLR}, 2024{\natexlab{a}}.

\bibitem[Zhou et~al.(2024{\natexlab{b}})Zhou, Huang, Bu, Zeng, Li, Qiu, Zhu, Guo, Qiao, and Li]{embodieddrive}
Y.~Zhou, L.~Huang, Q.~Bu, J.~Zeng, T.~Li, H.~Qiu, H.~Zhu, M.~Guo, Y.~Qiao, and H.~Li.
\newblock Embodied understanding of driving scenarios.
\newblock \emph{arXiv preprint arXiv:2403.04593}, 2024{\natexlab{b}}.

\bibitem[Zhu et~al.(2023)Zhu, Ma, Chen, Deng, Huang, and Li]{3dvista}
Z.~Zhu, X.~Ma, Y.~Chen, Z.~Deng, S.~Huang, and Q.~Li.
\newblock {3D-VisTA: Pre-trained transformer for 3D vision and text alignment}.
\newblock In \emph{ICCV}, 2023.

\end{thebibliography}
