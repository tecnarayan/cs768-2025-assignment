\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[DeGroot(1974)]{degroot-consensus}
Morris~H. DeGroot.
\newblock Reaching a consensus.
\newblock \emph{Journal of the American Statistical Association}, 69\penalty0 (345):\penalty0 118--121, 1974.
\newblock ISSN 01621459.

\bibitem[Mendler-D\"{u}nner et~al.(2021)Mendler-D\"{u}nner, Guo, Bates, and Jordan]{celestine2021testtime}
Celestine Mendler-D\"{u}nner, Wenshuo Guo, Stephen Bates, and Michael Jordan.
\newblock Test-time collective prediction.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, volume~34, pages 13719--13731, 2021.

\bibitem[Fralick(1967)]{farlick67self}
S.~Fralick.
\newblock Learning to recognize patterns without a teacher.
\newblock \emph{IEEE Transactions on Information Theory}, 13\penalty0 (1):\penalty0 57--64, 1967.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji, Bonawitz, Charles, Cormode, Cummings, D'Oliveira, Eichner, Rouayheb, Evans, Gardner, Garrett, Gascón, Ghazi, Gibbons, Gruteser, Harchaoui, He, He, Huo, Hutchinson, Hsu, Jaggi, Javidi, Joshi, Khodak, Konečný, Korolova, Koushanfar, Koyejo, Lepoint, Liu, Mittal, Mohri, Nock, Özgür, Pagh, Raykova, Qi, Ramage, Raskar, Song, Song, Stich, Sun, Suresh, Tramèr, Vepakomma, Wang, Xiong, Xu, Yang, Yu, Yu, and Zhao]{kairouz2021advances}
Peter Kairouz, H.~Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun~Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G.~L. D'Oliveira, Hubert Eichner, Salim~El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip~B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konečný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian~U. Stich, Ziteng Sun, Ananda~Theertha Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang, Li~Xiong, Zheng Xu, Qiang Yang, Felix~X. Yu, Han Yu, and Sen Zhao.
\newblock Advances and open problems in federated learning.
\newblock \emph{ArXiv preprint arxiv:1912.04977}, 2021.

\bibitem[McMahan et~al.(2016)McMahan, Moore, Ramage, and y~Arcas]{McMahanMRA16}
H.~Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise~Ag{\"{u}}era y~Arcas.
\newblock Federated learning of deep networks using model averaging.
\newblock \emph{CoRR}, abs/1602.05629, 2016.

\bibitem[Zhao et~al.(2018)Zhao, Li, Lai, Suda, Civin, and Chandra]{fed-noniid}
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.
\newblock Federated learning with non-iid data.
\newblock \emph{arXiv preprint arxiv:1806.0058}, 2018.

\bibitem[Li et~al.(2020)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and Smith]{li2020federated}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
\newblock Federated optimization in heterogeneous networks, 2020.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and Suresh]{karimireddy2021scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda~Theertha Suresh.
\newblock {SCAFFOLD}: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, volume 119, pages 5132--5143. PMLR, 2020.

\bibitem[Cao et~al.(2021)Cao, Fang, Liu, and Gong]{cao2021fltrust}
Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil~Zhenqiang Gong.
\newblock Fltrust: Byzantine-robust federated learning via trust bootstrapping.
\newblock In \emph{Network and Distributed System Security (NDSS) Symposium}, 2021.

\bibitem[Karimireddy et~al.(2022)Karimireddy, He, and Jaggi]{karimireddy2022byzantinerobust}
Sai~Praneeth Karimireddy, Lie He, and Martin Jaggi.
\newblock Byzantine-robust learning on heterogeneous datasets via bucketing.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=jXKKDEi5vJt}.

\bibitem[Lin et~al.(2020)Lin, Kong, Stich, and Jaggi]{lin2021ensemble}
Tao Lin, Lingjing Kong, Sebastian~U. Stich, and Martin Jaggi.
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock In \emph{International Conference on Neural Information Processing Systems}, 2020.

\bibitem[Zhu et~al.(2021)Zhu, Hong, and Zhou]{zhu2021datafree}
Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou.
\newblock Data-free knowledge distillation for heterogeneous federated learning.
\newblock In \emph{International Conference on Machine Learning}, volume 139, pages 12878--12889. PMLR, 2021.

\bibitem[Makhija et~al.(2022)Makhija, Han, Ho, and Ghosh]{pmlr-v162-makhija22a}
Disha Makhija, Xing Han, Nhat Ho, and Joydeep Ghosh.
\newblock Architecture agnostic federated learning for neural networks.
\newblock In \emph{International Conference on Machine Learning}, volume 162, pages 14860--14870. PMLR, 2022.

\bibitem[Abourayya et~al.(2022)Abourayya, Kamp, Ayday, Kleesiek, Rao, Webb, and Rao]{abourayya2022aimhi}
Amr Abourayya, Michael Kamp, Erman Ayday, Jens Kleesiek, Kanishka Rao, Geoffrey~I. Webb, and Bharat Rao.
\newblock {AIMHI}: Protecting sensitive data through federated co-training.
\newblock In \emph{Workshop on Federated Learning: Recent Advances and New Challenges (at NeurIPS)}, 2022.

\bibitem[Bellet et~al.(2018)Bellet, Guerraoui, Taziki, and Tommasi]{bellet2018personalized}
Aurélien Bellet, Rachid Guerraoui, Mahsa Taziki, and Marc Tommasi.
\newblock Personalized and private peer-to-peer machine learning.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, volume~84, pages 473--481. PMLR, 2018.

\bibitem[Koloskova et~al.(2020)Koloskova, Loizou, Boreiri, Jaggi, and Stich]{koloskova2021unified}
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich.
\newblock A unified theory of decentralized {SGD} with changing topology and local updates.
\newblock In \emph{International Conference on Machine Learning}, volume 119, pages 5381--5393. PMLR, 2020.

\bibitem[Dandi et~al.(2022)Dandi, Koloskova, Jaggi, and Stich]{dandi2022dataheterogeneityaware}
Yatin Dandi, Anastasia Koloskova, Martin Jaggi, and Sebastian~U. Stich.
\newblock Data-heterogeneity-aware mixing for decentralized learning.
\newblock In \emph{OPT 2022: NeurIPS Workshop on Optimization for Machine Learning}, 2022.

\bibitem[Le~Bars et~al.(2023)Le~Bars, Bellet, Tommasi, Lavoie, and Kermarrec]{bars2023topology}
Batiste Le~Bars, Aur\'elien Bellet, Marc Tommasi, Erick Lavoie, and Anne-Marie Kermarrec.
\newblock Refined convergence and topology learning for decentralized sgd with heterogeneous data.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, volume 206, pages 1672--1702. PMLR, 2023.

\bibitem[Xiao and Boyd(2003)]{xiaoboyd-davg}
Lin Xiao and S.~Boyd.
\newblock Fast linear iterations for distributed averaging.
\newblock In \emph{IEEE International Conference on Decision and Control}, volume~5, pages 4997--5002, 2003.

\bibitem[Boyd et~al.(2006)Boyd, Ghosh, Prabhakar, and Shah]{boyd-gossip}
S.~Boyd, A.~Ghosh, B.~Prabhakar, and D.~Shah.
\newblock Randomized gossip algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 52\penalty0 (6):\penalty0 2508--2530, 2006.

\bibitem[Assran et~al.(2019)Assran, Loizou, Ballas, and Rabbat]{assran2019stochastic}
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock In \emph{International Conference on Machine Learning}, volume~97, pages 344--353. PMLR, 2019.

\bibitem[Li et~al.(2022)Li, Zhou, Tian, and Tao]{li-l2c2022}
Shuangtong Li, Tianyi Zhou, Xinmei Tian, and Dacheng Tao.
\newblock Learning to collaborate in decentralized learning of personalized models.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 9756--9765, 2022.

\bibitem[Sui et~al.(2022)Sui, Wen, Lau, Ross, and Cresswell]{sui2022friends}
Yi~Sui, Junfeng Wen, Yenson Lau, Brendan~Leigh Ross, and Jesse~C. Cresswell.
\newblock Find your friends: Personalized federated learning with the right collaborators.
\newblock \emph{Arxiv preprint arxiv:2210.06597}, 2022.

\bibitem[Lee(2013)]{Lee2013PseudoLabelT}
Dong-Hyun Lee.
\newblock Pseudo-label : The simple and efficient semi-supervised learning method for deep neural networks.
\newblock 2013.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:18507866}.

\bibitem[Wei et~al.(2021)Wei, Shen, Chen, and Ma]{wei2022theoretical}
Colin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma.
\newblock Theoretical analysis of self-training with deep networks on unlabeled data.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Blum and Mitchell(1998)]{DBLP:conf/colt/BlumM98}
Avrim Blum and Tom~M. Mitchell.
\newblock Combining labeled and unlabeled data with co-training.
\newblock In Peter~L. Bartlett and Yishay Mansour, editors, \emph{Annual Conference on Computational Learning Theory (COLT)}, pages 92--100. {ACM}, 1998.

\bibitem[Diao et~al.(2022)Diao, Ding, and Tarokh]{diao2022semifl}
Enmao Diao, Jie Ding, and Vahid Tarokh.
\newblock Semifl: Semi-supervised federated learning for unlabeled clients with alternate training.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~35, pages 17871--17884, 2022.

\bibitem[Farina(2021)]{farina2021collective}
Francesco Farina.
\newblock Collective learning.
\newblock \emph{Arxiv preprint arxiv:1912.02580}, 2021.

\bibitem[Hadjicostis and Dominguez-Garcia(2022)]{trustworthy-consensus2022}
{Christoforos N.} Hadjicostis and {Alejandro D.} Dominguez-Garcia.
\newblock Trustworthy distributed average consensus.
\newblock In \emph{Conference on Decision and Control (CDC)}, pages 7403--7408. Institute of Electrical and Electronics Engineers Inc., 2022.

\bibitem[Levin et~al.(2008)Levin, Peres, and Wilmer]{levin2008markov}
D.A. Levin, Y.~Peres, and E.L. Wilmer.
\newblock \emph{Markov Chains and Mixing Times}.
\newblock American Mathematical Soc., 2008.

\bibitem[Krizhevsky(2009)]{cifar}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock pages 32--33, 2009.
\newblock URL \url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2015deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 770--778, 2016.

\bibitem[Tschandl~P. and H.(2018)]{isic-1}
Rosendahl~C. Tschandl~P. and Kittler H.
\newblock The ham10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions, 2018.

\bibitem[Codella et~al.(2017)Codella, Gutman, Celebi, Helba, Marchetti, Dusza, Kalloo, Liopyris, Mishra, Kittler, and Halpern]{isic-2}
Noel C.~F. Codella, David Gutman, M.~Emre Celebi, Brian Helba, Michael~A. Marchetti, Stephen~W. Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, and Allan Halpern.
\newblock Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), 2017.

\bibitem[Combalia et~al.(2019)Combalia, Codella, Rotemberg, Helba, Vilaplana, Reiter, Carrera, Barreiro, Halpern, Puig, and Malvehy]{isic-3}
Marc Combalia, Noel C.~F. Codella, Veronica Rotemberg, Brian Helba, Veronica Vilaplana, Ofer Reiter, Cristina Carrera, Alicia Barreiro, Allan~C. Halpern, Susana Puig, and Josep Malvehy.
\newblock Bcn20000: Dermoscopic lesions in the wild, 2019.

\bibitem[du~Terrail et~al.(2022)du~Terrail, Ayed, Cyffers, Grimberg, He, Loeb, Mangold, Marchand, Marfoq, Mushtaq, Muzellec, Philippenko, Silva, Teleńczuk, Albarqouni, Avestimehr, Bellet, Dieuleveut, Jaggi, Karimireddy, Lorenzi, Neglia, Tommasi, and Andreux]{terrail2022flamby}
Jean~Ogier du~Terrail, Samy-Safwan Ayed, Edwige Cyffers, Felix Grimberg, Chaoyang He, Regis Loeb, Paul Mangold, Tanguy Marchand, Othmane Marfoq, Erum Mushtaq, Boris Muzellec, Constantin Philippenko, Santiago Silva, Maria Teleńczuk, Shadi Albarqouni, Salman Avestimehr, Aurélien Bellet, Aymeric Dieuleveut, Martin Jaggi, Sai~Praneeth Karimireddy, Marco Lorenzi, Giovanni Neglia, Marc Tommasi, and Mathieu Andreux.
\newblock Flamby: Datasets and benchmarks for cross-silo federated learning in realistic healthcare settings, 2022.

\bibitem[Tan and Le(2019)]{pmlr-v97-tan19a}
Mingxing Tan and Quoc Le.
\newblock {E}fficient{N}et: Rethinking model scaling for convolutional neural networks.
\newblock In \emph{International Conference on Machine Learning}, volume~97, pages 6105--6114. PMLR, 2019.

\bibitem[Acar et~al.(2021)Acar, Zhao, Matas, Mattina, Whatmough, and Saligrama]{acar2021federated}
Durmus Alp~Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama.
\newblock Federated learning based on dynamic regularization.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Haim et~al.(2022)Haim, Vardi, Yehudai, michal Irani, and Shamir]{haim2022reconstructing}
Niv Haim, Gal Vardi, Gilad Yehudai, michal Irani, and Ohad Shamir.
\newblock Reconstructing training data from trained neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Wang et~al.(2023)Wang, Lee, and Lei]{wang2023reconstructing}
Zihan Wang, Jason Lee, and Qi~Lei.
\newblock Reconstructing training data from model gradient, provably.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, volume 206, pages 6595--6612. PMLR, 2023.

\bibitem[Carlini et~al.(2022)Carlini, Jagielski, Zhang, Papernot, Terzis, and Tramer]{carlini2022privacy}
Nicholas Carlini, Matthew Jagielski, Chiyuan Zhang, Nicolas Papernot, Andreas Terzis, and Florian Tramer.
\newblock The privacy onion effect: Memorization is relative.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 13263--13276, 2022.

\bibitem[Dwork et~al.(2006)Dwork, McSherry, Nissim, and Smith]{dwork06dp}
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.
\newblock Calibrating noise to sensitivity in private data analysis.
\newblock In \emph{Third Conference on Theory of Cryptography (TCC)}, page 265–284. Springer-Verlag, 2006.

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar, and Zhang]{abadi16account}
Martin Abadi, Andy Chu, Ian Goodfellow, H.~Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li~Zhang.
\newblock Deep learning with differential privacy.
\newblock In \emph{ACM SIGSAC Conference on Computer and Communications Security (CCS)}, page 308–318, 2016.

\bibitem[Wolfowitz(1963)]{prod_stochastic}
J.~Wolfowitz.
\newblock Products of indecomposable, aperiodic, stochastic matrices.
\newblock \emph{Proceedings of the American Mathematical Society}, 14\penalty0 (5):\penalty0 733--737, 1963.

\end{thebibliography}
