\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bi{\'n}kowski et~al.(2020)Bi{\'n}kowski, Donahue, Dieleman, Clark,
  Elsen, Casagrande, Cobo, and Simonyan]{binkowski2020high}
Miko{\l}aj Bi{\'n}kowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich
  Elsen, Norman Casagrande, Luis~C Cobo, and Karen Simonyan.
\newblock High fidelity speech synthesis with adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill,
  et~al.]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Chen et~al.(2021)Chen, Zhang, Zen, Weiss, Norouzi, and
  Chan]{chen2021wavegrad}
Nanxin Chen, Yu~Zhang, Heiga Zen, Ron~J Weiss, Mohammad Norouzi, and William
  Chan.
\newblock Wavegrad: Estimating gradients for waveform generation.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Dauphin et~al.(2017)Dauphin, Fan, Auli, and
  Grangier]{dauphin2017language}
Yann~N Dauphin, Angela Fan, Michael Auli, and David Grangier.
\newblock Language modeling with gated convolutional networks.
\newblock In \emph{International conference on machine learning}, pages
  933--941. PMLR, 2017.

\bibitem[DeepSound(2017)]{deepsound}
DeepSound.
\newblock Samplernn.
\newblock \url{https://github.com/deepsound-project/samplernn-pytorch}, 2017.

\bibitem[Dhariwal et~al.(2020)Dhariwal, Jun, Payne, Kim, Radford, and
  Sutskever]{dhariwal2020jukebox}
Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong~Wook Kim, Alec Radford,
  and Ilya Sutskever.
\newblock Jukebox: A generative model for music.
\newblock \emph{arXiv preprint arXiv:2005.00341}, 2020.

\bibitem[Dieleman et~al.(2018)Dieleman, van~den Oord, and
  Simonyan]{dieleman2018challenge}
Sander Dieleman, A{\"a}ron van~den Oord, and Karen Simonyan.
\newblock The challenge of realistic music generation: modelling raw audio at
  scale.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 8000--8010, 2018.

\bibitem[Donahue et~al.(2019)Donahue, McAuley, and
  Puckette]{donahue2019adversarial}
Chris Donahue, Julian McAuley, and Miller Puckette.
\newblock Adversarial audio synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Engel et~al.(2017)Engel, Resnick, Roberts, Dieleman, Norouzi, Eck, and
  Simonyan]{engel2017neural}
Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Mohammad Norouzi,
  Douglas Eck, and Karen Simonyan.
\newblock Neural audio synthesis of musical notes with wavenet autoencoders.
\newblock In \emph{International Conference on Machine Learning}, pages
  1068--1077. PMLR, 2017.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R\'e]{gu2020hippo}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\'e.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Gu et~al.(2022)Gu, Goel, and R\'{e}]{gu2022efficiently}
Albert Gu, Karan Goel, and Christopher R\'{e}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Gurumurthy et~al.(2017)Gurumurthy, Kiran~Sarvadevabhatla, and
  Venkatesh~Babu]{gurumurthy2017deligan}
Swaminathan Gurumurthy, Ravi Kiran~Sarvadevabhatla, and R~Venkatesh~Babu.
\newblock Deligan: Generative adversarial networks for diverse and limited
  data.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 166--174, 2017.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Hochreiter and Schmidhuber(1997)]{lstm}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Bengio, Frasconi, Schmidhuber,
  et~al.]{hochreiter2001gradient}
Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, J{\"u}rgen Schmidhuber, et~al.
\newblock Gradient flow in recurrent nets: the difficulty of learning long-term
  dependencies, 2001.

\bibitem[Jayaram and Thickstun(2021)]{jayaram2021parallel}
Vivek Jayaram and John Thickstun.
\newblock Parallel and flexible sampling from autoregressive models via
  langevin dynamics.
\newblock In \emph{The International Conference on Machine Learning (ICML)},
  2021.

\bibitem[Kalchbrenner et~al.(2018)Kalchbrenner, Elsen, Simonyan, Noury,
  Casagrande, Lockhart, Stimberg, van~den Oord, Dieleman, and
  Kavukcuoglu]{kalchbrenner2018efficient}
Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande,
  Edward Lockhart, Florian Stimberg, A{\"a}ron van~den Oord, Sander Dieleman,
  and Koray Kavukcuoglu.
\newblock Efficient neural audio synthesis.
\newblock In \emph{International Conference on Machine Learning}, pages
  2410--2419. PMLR, 2018.

\bibitem[Kim et~al.(2019)Kim, Lee, Song, Kim, and Yoon]{kim2019flowavenet}
Sungwon Kim, Sang-Gil Lee, Jongyoon Song, Jaehyeon Kim, and Sungroh Yoon.
\newblock Flowavenet: A generative flow for raw audio.
\newblock In \emph{International Conference on Machine Learning}, pages
  3370--3378. PMLR, 2019.

\bibitem[Kleijn et~al.(2018)Kleijn, Lim, Luebs, Skoglund, Stimberg, Wang, and
  Walters]{kleijn2018wavenet}
W~Bastiaan Kleijn, Felicia~SC Lim, Alejandro Luebs, Jan Skoglund, Florian
  Stimberg, Quan Wang, and Thomas~C Walters.
\newblock Wavenet based low rate speech coding.
\newblock In \emph{2018 IEEE international conference on acoustics, speech and
  signal processing (ICASSP)}, pages 676--680. IEEE, 2018.

\bibitem[Kong et~al.(2021)Kong, Ping, Huang, Zhao, and
  Catanzaro]{kong2021diffwave}
Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro.
\newblock Diffwave: A versatile diffusion model for audio synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Kumar et~al.(2019)Kumar, Kumar, de~Boissiere, Gestin, Teoh, Sotelo,
  de~Br{\'e}bisson, Bengio, and Courville]{kumar2019melgan}
Kundan Kumar, Rithesh Kumar, Thibault de~Boissiere, Lucas Gestin, Wei~Zhen
  Teoh, Jose Sotelo, Alexandre de~Br{\'e}bisson, Yoshua Bengio, and Aaron~C
  Courville.
\newblock Melgan: Generative adversarial networks for conditional waveform
  synthesis.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Lakhotia et~al.(2021)Lakhotia, Kharitonov, Hsu, Adi, Polyak, Bolte,
  Nguyen, Copet, Baevski, Mohamed, et~al.]{lakhotia2021generative}
Kushal Lakhotia, Evgeny Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak,
  Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Adelrahman
  Mohamed, et~al.
\newblock Generative spoken language modeling from raw audio.
\newblock \emph{arXiv preprint arXiv:2102.01192}, 2021.

\bibitem[Li et~al.(2019)Li, Liu, Liu, Zhao, and Liu]{li2019neural}
Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu.
\newblock Neural speech synthesis with transformer network.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 6706--6713, 2019.

\bibitem[Liu et~al.(2022)Liu, Mao, Wu, Feichtenhofer, Darrell, and
  Xie]{liu2022convnet}
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
  and Saining Xie.
\newblock A convnet for the 2020s.
\newblock \emph{arXiv preprint arXiv:2201.03545}, 2022.

\bibitem[Mehri et~al.(2017)Mehri, Kumar, Gulrajani, Kumar, Jain, Sotelo,
  Courville, and Bengio]{mehri2017samplernn}
Soroush Mehri, Kundan Kumar, Ishaan Gulrajani, Rithesh Kumar, Shubham Jain,
  Jose Sotelo, Aaron Courville, and Yoshua Bengio.
\newblock Samplernn: An unconditional end-to-end neural audio generation model.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Neekhara et~al.(2019)Neekhara, Donahue, Puckette, Dubnov, and
  McAuley]{Neekhara2019ExpeditingTS}
Paarth Neekhara, Chris Donahue, Miller Puckette, Shlomo Dubnov, and Julian
  McAuley.
\newblock Expediting tts synthesis with adversarial vocoding.
\newblock In \emph{INTERSPEECH}, 2019.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International conference on machine learning}, pages
  1310--1318, 2013.

\bibitem[Peng et~al.(2020)Peng, Ping, Song, and Zhao]{peng2020non}
Kainan Peng, Wei Ping, Zhao Song, and Kexin Zhao.
\newblock Non-autoregressive neural text-to-speech.
\newblock In \emph{International conference on machine learning}, pages
  7586--7598. PMLR, 2020.

\bibitem[Ping et~al.(2019)Ping, Peng, and Chen]{ping2019clarinet}
Wei Ping, Kainan Peng, and Jitong Chen.
\newblock Clarinet: Parallel wave generation in end-to-end text-to-speech.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Ping et~al.(2020)Ping, Peng, Zhao, and Song]{ping2020waveflow}
Wei Ping, Kainan Peng, Kexin Zhao, and Zhao Song.
\newblock Waveflow: A compact flow-based model for raw audio.
\newblock In \emph{International Conference on Machine Learning}, pages
  7706--7716. PMLR, 2020.

\bibitem[Prenger et~al.(2019)Prenger, Valle, and
  Catanzaro]{prenger2019waveglow}
Ryan Prenger, Rafael Valle, and Bryan Catanzaro.
\newblock Waveglow: A flow-based generative network for speech synthesis.
\newblock In \emph{ICASSP 2019-2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 3617--3621. IEEE, 2019.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock \emph{arXiv preprint arXiv:2102.12092}, 2021.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training gans.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 2234--2242, 2016.

\bibitem[Salimans et~al.(2017)Salimans, Karpathy, Chen, and
  Kingma]{salimans2017pixelcnn++}
Tim Salimans, Andrej Karpathy, Xi~Chen, and Diederik~P Kingma.
\newblock Pixelcnn++: Improving the pixelcnn with discretized logistic mixture
  likelihood and other modifications.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Shen et~al.(2018)Shen, Pang, Weiss, Schuster, Jaitly, Yang, Chen,
  Zhang, Wang, Skerrv-Ryan, et~al.]{shen2018natural}
Jonathan Shen, Ruoming Pang, Ron~J Weiss, Mike Schuster, Navdeep Jaitly,
  Zongheng Yang, Zhifeng Chen, Yu~Zhang, Yuxuan Wang, Rj~Skerrv-Ryan, et~al.
\newblock Natural tts synthesis by conditioning wavenet on mel spectrogram
  predictions.
\newblock In \emph{2018 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 4779--4783. IEEE, 2018.

\bibitem[van~den Oord et~al.(2016)van~den Oord, Dieleman, Zen, Simonyan,
  Vinyals, Graves, Kalchbrenner, Senior, and Kavukcuoglu]{oord2016wavenet}
A{\"a}ron van~den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol
  Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.
\newblock Wavenet: A generative model for raw audio.
\newblock \emph{arXiv preprint arXiv:1609.03499}, 2016.

\bibitem[Van Den~Oord et~al.(2017)Van Den~Oord, Vinyals,
  et~al.]{oord2017neural}
Aaron Van Den~Oord, Oriol Vinyals, et~al.
\newblock Neural discrete representation learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[van~den Oord et~al.(2018)van~den Oord, Li, Babuschkin, Simonyan,
  Vinyals, Kavukcuoglu, Driessche, Lockhart, Cobo, Stimberg,
  et~al.]{oord2018parallel}
A{\"a}ron van~den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol
  Vinyals, Koray Kavukcuoglu, George Driessche, Edward Lockhart, Luis Cobo,
  Florian Stimberg, et~al.
\newblock Parallel wavenet: Fast high-fidelity speech synthesis.
\newblock In \emph{International conference on machine learning}, pages
  3918--3926. PMLR, 2018.

\bibitem[Warden(2018)]{Warden2018SpeechCA}
Pete Warden.
\newblock Speech commands: A dataset for limited-vocabulary speech recognition.
\newblock \emph{ArXiv}, abs/1804.03209, 2018.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2017aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1492--1500, 2017.

\bibitem[Yamamoto et~al.(2020)Yamamoto, Song, and Kim]{yamamoto2020parallel}
Ryuichi Yamamoto, Eunwoo Song, and Jae-Min Kim.
\newblock Parallel wavegan: A fast waveform generation model based on
  generative adversarial networks with multi-resolution spectrogram.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 6199--6203. IEEE, 2020.

\bibitem[Zhou et~al.(2018)Zhou, Cai, Rong, Song, Ren, Zhang, Wang, and
  Yu]{zhou2018activation}
Zhiming Zhou, Han Cai, Shu Rong, Yuxuan Song, Kan Ren, Weinan Zhang, Jun Wang,
  and Yong Yu.
\newblock Activation maximization generative adversarial nets.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\end{thebibliography}
