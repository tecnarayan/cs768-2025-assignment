\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bachman et~al.(2019)Bachman, Hjelm, and Buchwalter]{amdim}
Bachman, P., Hjelm, R.~D., and Buchwalter, W.
\newblock Learning representations by maximizing mutual information across
  views.
\newblock In \emph{Proc. Conf. on Neural Information Processing Systems
  (NeurIPS)}, pp.\  15509--15519, 2019.

\bibitem[Barber \& Agakov(2003)Barber and Agakov]{barber2003algorithm}
Barber, D. and Agakov, F.
\newblock The im algorithm: A variational approach to information maximization.
\newblock In \emph{Proc. Conf. on Neural Information Processing Systems
  (NIPS)}, pp.\  201â€“208, 2003.

\bibitem[Belghazi et~al.(2018)Belghazi, Baratin, Rajeswar, Ozair, Bengio,
  Hjelm, and Courville]{pmlr-v80-belghazi18a}
Belghazi, M.~I., Baratin, A., Rajeswar, S., Ozair, S., Bengio, Y., Hjelm,
  R.~D., and Courville, A.~C.
\newblock Mutual information neural estimation.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  530--539, 2018.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020unsupervised}
Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In \emph{Proc. Conf. on Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Ceylan \& Gutmann(2018)Ceylan and Gutmann]{ceylan2018conditional}
Ceylan, C. and Gutmann, M.~U.
\newblock Conditional noise-contrastive estimation of unnormalised models.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  725--733, 2018.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.~E.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  1597--1607, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Fan, Girshick, and
  He]{chen2020improved}
Chen, X., Fan, H., Girshick, R., and He, K.
\newblock Improved baselines with momentum contrastive learning.
\newblock \emph{arXiv preprint arXiv:2003.04297}, 2020{\natexlab{b}}.

\bibitem[Cremer et~al.(2017)Cremer, Morris, and
  Duvenaud]{cremer2017reinterpreting}
Cremer, C., Morris, Q., and Duvenaud, D.
\newblock Reinterpreting importance-weighted autoencoders.
\newblock \emph{Proc. Int. Conf. on Learning Representations (ICLR)}, 2017.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{imagenet_cvpr09}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{CVPR09}, 2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proc. Conf. Assoc. for Computational Linguistics (ACL)},
  pp.\  4171--4186, 2019.

\bibitem[Dinan et~al.(2019)Dinan, Roller, Shuster, Fan, Auli, and
  Weston]{dinan2018wizard}
Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., and Weston, J.
\newblock Wizard of wikipedia: Knowledge-powered conversational agents.
\newblock In \emph{Proc. Int. Conf. on Learning Representations (ICLR)}, 2019.

\bibitem[Elias(1955)]{elias1955predictive}
Elias, P.
\newblock Predictive coding--i.
\newblock \emph{IRE Transactions on Information Theory}, 1\penalty0
  (1):\penalty0 16--24, 1955.

\bibitem[Faghri et~al.(2018)Faghri, Fleet, Kiros, and Fidler]{faghri2017vse++}
Faghri, F., Fleet, D.~J., Kiros, J.~R., and Fidler, S.
\newblock {VSE++:} improving visual-semantic embeddings with hard negatives.
\newblock In \emph{Proc. British Machine Vision Conference (BMVC)}, pp.\ ~12,
  2018.

\bibitem[Foster et~al.(2020)Foster, Jankowiak, O'Meara, Teh, and
  Rainforth]{pmlr-v108-foster20a}
Foster, A., Jankowiak, M., O'Meara, M., Teh, Y.~W., and Rainforth, T.
\newblock A unified stochastic gradient approach to designing bayesian-optimal
  experiments.
\newblock In \emph{Proc. Int. Conf. on Artificial Intelligence and Statistics},
  pp.\  2959--2969, 2020.

\bibitem[Gidaris et~al.(2018)Gidaris, Singh, and
  Komodakis]{gidaris2018unsupervised}
Gidaris, S., Singh, P., and Komodakis, N.
\newblock Unsupervised representation learning by predicting image rotations.
\newblock In \emph{Proc. Int. Conf. on Learning Representations (ICLR)}, 2018.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'{e}}, Tallec, Richemond,
  Buchatskaya, Doersch, Pires, Guo, Azar, Piot, Kavukcuoglu, Munos, and
  Valko]{grill2020bootstrap}
Grill, J., Strub, F., Altch{\'{e}}, F., Tallec, C., Richemond, P.~H.,
  Buchatskaya, E., Doersch, C., Pires, B.~{\'{A}}., Guo, Z., Azar, M.~G., Piot,
  B., Kavukcuoglu, K., Munos, R., and Valko, M.
\newblock Bootstrap your own latent - {A} new approach to self-supervised
  learning.
\newblock In \emph{Proc. Conf. on Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Gutmann \& Hyv{\"a}rinen(2012)Gutmann and
  Hyv{\"a}rinen]{gutmann2012noise}
Gutmann, M.~U. and Hyv{\"a}rinen, A.
\newblock Noise-contrastive estimation of unnormalized statistical models, with
  applications to natural image statistics.
\newblock \emph{Journal of Machine Learning Research}, 13:\penalty0 307--361,
  2012.

\bibitem[Hinton(2012)]{hinton2012practical}
Hinton, G.~E.
\newblock A practical guide to training restricted boltzmann machines.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  599--619.
  Springer, 2012.

\bibitem[Hjelm et~al.(2019)Hjelm, Fedorov, Lavoie{-}Marchildon, Grewal,
  Bachman, Trischler, and Bengio]{hjelm2018learning}
Hjelm, R.~D., Fedorov, A., Lavoie{-}Marchildon, S., Grewal, K., Bachman, P.,
  Trischler, A., and Bengio, Y.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock In \emph{Proc. Int. Conf. on Learning Representations (ICLR)}, 2019.

\bibitem[Holtzman et~al.(2020)Holtzman, Buys, Du, Forbes, and
  Choi]{holtzman2019curious}
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.
\newblock The curious case of neural text degeneration.
\newblock In \emph{Proc. Int. Conf. on Learning Representations (ICLR)}, 2020.

\bibitem[Jabri et~al.(2020)Jabri, Owens, and Efros]{jabri2020space}
Jabri, A., Owens, A., and Efros, A.~A.
\newblock Space-time correspondence as a contrastive random walk.
\newblock In \emph{Proc. Conf. on Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Kingma \& Dhariwal(2018)Kingma and Dhariwal]{kingma2018glow}
Kingma, D.~P. and Dhariwal, P.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In \emph{Proc. Conf. on Neural Information Processing Systems
  (NeurIPS)}, pp.\  10236--10245, 2018.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{vae}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock In \emph{Proc. Int. Conf. on Learning Representations (ICLR)}, 2014.

\bibitem[Krause et~al.(2013)Krause, Deng, Stark, and
  Fei-Fei]{Krause2013CollectingAL}
Krause, J., Deng, J., Stark, M., and Fei-Fei, L.
\newblock Collecting a large-scale dataset of fine-grained cars.
\newblock Technical report, 2013.

\bibitem[Krizhevsky et~al.(2009)]{krizhevsky2009learning}
Krizhevsky, A. et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Li et~al.(2016)Li, Galley, Brockett, Gao, and Dolan]{li2016diversity}
Li, J., Galley, M., Brockett, C., Gao, J., and Dolan, B.
\newblock A diversity-promoting objective function for neural conversation
  models.
\newblock In \emph{Proc. Conf. Assoc. for Computational Linguistics (ACL)},
  pp.\  110--119, 2016.

\bibitem[Li et~al.(2020)Li, Roller, Kulikov, Welleck, Boureau, Cho, and
  Weston]{Li2019DontST}
Li, M., Roller, S., Kulikov, I., Welleck, S., Boureau, Y.-L., Cho, K., and
  Weston, J.
\newblock Don{'}t say that! making inconsistent dialogue unlikely with
  unlikelihood training.
\newblock In \emph{Proc. Conf. Assoc. for Computational Linguistics (ACL)},
  pp.\  4715--4728, 2020.

\bibitem[Ma \& Collins(2018)Ma and Collins]{ma2018noise}
Ma, Z. and Collins, M.
\newblock Noise contrastive estimation and negative sampling for conditional
  models: Consistency and statistical efficiency.
\newblock In \emph{Proc. Conf. on Empirical Methods in Natural Language
  Processing (EMNLP)}, pp.\  3698--3707, 2018.

\bibitem[Mazoure et~al.(2020)Mazoure, Tachet~des Combes, Doan, Bachman, and
  Hjelm]{mazoure2020deep}
Mazoure, B., Tachet~des Combes, R., Doan, T., Bachman, P., and Hjelm, R.~D.
\newblock Deep reinforcement and infomax learning.
\newblock In \emph{Proc. Conf. on Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[McAllester(2018)]{mcallester2018information}
McAllester, D.
\newblock Information theoretic co-training.
\newblock \emph{arXiv preprint arXiv:1802.07572}, 2018.

\bibitem[McAllester \& Stratos(2020{\natexlab{a}})McAllester and
  Stratos]{mcallester2018formal}
McAllester, D. and Stratos, K.
\newblock Formal limitations on the measurement of mutual information.
\newblock In \emph{Int. Conf. on Artificial Intelligence and Statistics
  (AISTATS)}, pp.\  875--884, 2020{\natexlab{a}}.

\bibitem[McAllester \& Stratos(2020{\natexlab{b}})McAllester and
  Stratos]{pmlr-v108-mcallester20a}
McAllester, D. and Stratos, K.
\newblock Formal limitations on the measurement of mutual information.
\newblock In Chiappa, S. and Calandra, R. (eds.), \emph{Proceedings of the
  Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pp.\  875--884. PMLR, 26--28 Aug 2020{\natexlab{b}}.

\bibitem[Mescheder et~al.(2018)Mescheder, Geiger, and
  Nowozin]{mescheder2018training}
Mescheder, L.~M., Geiger, A., and Nowozin, S.
\newblock Which training methods for gans do actually converge?
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  3478--3487, 2018.

\bibitem[Mikolov et~al.(2013)Mikolov, Chen, Corrado, and
  Dean]{mikolov2013efficient}
Mikolov, T., Chen, K., Corrado, G., and Dean, J.
\newblock Efficient estimation of word representations in vector space.
\newblock \emph{arXiv preprint arXiv:1301.3781}, 2013.

\bibitem[Mondal et~al.(2020)Mondal, Bhattacharjee, Mukherjee, Asnani, Kannan,
  and P.]{mondal2020c}
Mondal, A.~K., Bhattacharjee, A., Mukherjee, S., Asnani, H., Kannan, S., and
  P., P.~A.
\newblock {C-MI-GAN} : Estimation of conditional mutual information using
  minmax formulation.
\newblock In \emph{Proc. Conf. on Uncertainty in Artificial Intelligence
  (UAI)}, pp.\  849--858, 2020.

\bibitem[Nilsback \& Zisserman(2008)Nilsback and Zisserman]{zissermannflowers}
Nilsback, M.-E. and Zisserman, A.
\newblock Automated flower classification over a large number of classes.
\newblock ICVGIP '08, pp.\  722â€“729, USA, 2008. IEEE Computer Society.

\bibitem[Noroozi \& Favaro(2016)Noroozi and Favaro]{noroozi2016unsupervised}
Noroozi, M. and Favaro, P.
\newblock Unsupervised learning of visual representations by solving jigsaw
  puzzles.
\newblock In \emph{Proc. European Conf. on Computer Vision}, pp.\  69--84.
  Springer, 2016.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Oord, A. v.~d., Li, Y., and Vinyals, O.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu]{papineni2002bleu}
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
\newblock {B}leu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proc. Conf. Assoc. for Computational Linguistics (ACL)},
  pp.\  311--318, 2002.

\bibitem[Poole et~al.(2019)Poole, Ozair, van~den Oord, Alemi, and
  Tucker]{poole2019variational}
Poole, B., Ozair, S., van~den Oord, A., Alemi, A., and Tucker, G.
\newblock On variational bounds of mutual information.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  5171--5180, 2019.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog}, 2019.

\bibitem[Rainforth et~al.(2018)Rainforth, Kosiorek, Le, Maddison, Igl, Wood,
  and Teh]{rainforth2018tighter}
Rainforth, T., Kosiorek, A.~R., Le, T.~A., Maddison, C.~J., Igl, M., Wood, F.,
  and Teh, Y.~W.
\newblock Tighter variational bounds are not necessarily better.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  4274--4282, 2018.

\bibitem[Rhodes et~al.(2020)Rhodes, Xu, and Gutmann]{rhodes2020telescoping}
Rhodes, B., Xu, K., and Gutmann, M.~U.
\newblock Telescoping density-ratio estimation.
\newblock \emph{arXiv preprint arXiv:2006.12204}, 2020.

\bibitem[Rubin(1987)]{rubin1987}
Rubin, D.~B.
\newblock The calculation of posterior distributions by data augmentation:
  Comment: A noniterative sampling/importance resampling alternative to the
  data augmentation algorithm for creating a few imputations when fractions of
  missing information are modest: The {SIR} algorithm.
\newblock \emph{Journal of the American Statistical Association}, 82\penalty0
  (398):\penalty0 543--546, 1987.

\bibitem[Skare et~al.(2003)Skare, B{\o}lviken, and Holden]{skare2003improved}
Skare, {\O}., B{\o}lviken, E., and Holden, L.
\newblock Improved sampling-importance resampling and reduced bias importance
  sampling.
\newblock \emph{Scandinavian Journal of Statistics}, 30\penalty0 (4):\penalty0
  719--737, 2003.

\bibitem[Song \& Ermon(2020{\natexlab{a}})Song and
  Ermon]{song2019understanding}
Song, J. and Ermon, S.
\newblock Understanding the limitations of variational mutual information
  estimators.
\newblock In \emph{Proc. Int. Conf. on Learning Representations (ICLR)},
  2020{\natexlab{a}}.

\bibitem[Song \& Ermon(2020{\natexlab{b}})Song and Ermon]{song2020multi}
Song, J. and Ermon, S.
\newblock Multi-label contrastive predictive coding.
\newblock In \emph{Proc. Conf. on Neural Information Processing Systems
  (NeurIPS)}, 2020{\natexlab{b}}.

\bibitem[Stratos(2019)]{stratos2018mutual}
Stratos, K.
\newblock Mutual information maximization for simple and accurate
  part-of-speech induction.
\newblock In \emph{Proc. Conf. Assoc. for Computational Linguistics (ACL)},
  pp.\  1095--1104, 2019.

\bibitem[Tian et~al.(2019)Tian, Krishnan, and Isola]{tian2019contrastive}
Tian, Y., Krishnan, D., and Isola, P.
\newblock Contrastive multiview coding.
\newblock \emph{arXiv preprint arXiv:1906.05849}, 2019.

\bibitem[Tian et~al.(2020)Tian, Sun, Poole, Krishnan, Schmid, and
  Isola]{tian2020makes}
Tian, Y., Sun, C., Poole, B., Krishnan, D., Schmid, C., and Isola, P.
\newblock What makes for good views for contrastive learning.
\newblock \emph{arXiv preprint arXiv:2005.10243}, 2020.

\bibitem[Tschannen et~al.(2020)Tschannen, Djolonga, Rubenstein, Gelly, and
  Lucic]{tschannen2019mutual}
Tschannen, M., Djolonga, J., Rubenstein, P.~K., Gelly, S., and Lucic, M.
\newblock On mutual information maximization for representation learning.
\newblock In \emph{Proc. Int. Conf. on Learning Representations (ICLR)}, 2020.

\bibitem[Velickovic et~al.(2019)Velickovic, Fedus, Hamilton, Li{\`{o}}, Bengio,
  and Hjelm]{velivckovic2018deep}
Velickovic, P., Fedus, W., Hamilton, W.~L., Li{\`{o}}, P., Bengio, Y., and
  Hjelm, R.~D.
\newblock Deep graph infomax.
\newblock In \emph{Proc. Int. Conf. on Learning Representations (ICLR)}, 2019.

\bibitem[Wang \& Isola(2020)Wang and Isola]{wang2020understanding}
Wang, T. and Isola, P.
\newblock Understanding contrastive representation learning through alignment
  and uniformity on the hypersphere.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  9929--9939, 2020.

\bibitem[Welinder et~al.(2010)Welinder, Branson, Mita, Wah, Schroff, Belongie,
  and Perona]{welinder2010caltech}
Welinder, P., Branson, S., Mita, T., Wah, C., Schroff, F., Belongie, S., and
  Perona, P.
\newblock Caltech-ucsd birds 200.
\newblock 2010.

\bibitem[Welleck et~al.(2020)Welleck, Kulikov, Roller, Dinan, Cho, and
  Weston]{welleck2019neural}
Welleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., and Weston, J.
\newblock Neural text generation with unlikelihood training.
\newblock In \emph{Proc. Int. Conf. on Learning Representations (ICLR)}, 2020.

\bibitem[Wolf et~al.(2019)Wolf, Sanh, Chaumond, and
  Delangue]{wolf2019transfertransfo}
Wolf, T., Sanh, V., Chaumond, J., and Delangue, C.
\newblock Transfertransfo: A transfer learning approach for neural network
  based conversational agents.
\newblock In \emph{Proc. Conf. on Neural Information Processing Systems
  (NeurIPS) CAI Workshop}, 2019.

\bibitem[Yang et~al.(2018)Yang, Luo, Wang, Hu, Gao, and Wang]{yang2018learning}
Yang, Z., Luo, T., Wang, D., Hu, Z., Gao, J., and Wang, L.
\newblock Learning to navigate for fine-grained classification.
\newblock In \emph{Proc. of the European Conf. on Computer Vision (ECCV)}, pp.\
   420--435, 2018.

\bibitem[Zhang et~al.(2018)Zhang, Galley, Gao, Gan, Li, Brockett, and
  Dolan]{zhang2018generating}
Zhang, Y., Galley, M., Gao, J., Gan, Z., Li, X., Brockett, C., and Dolan, B.
\newblock Generating informative and diverse conversational responses via
  adversarial information maximization.
\newblock In \emph{Proc. Conf. on Neural Information Processing Systems
  (NeurIPS)}, pp.\  1815--1825, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Sun, Galley, Chen, Brockett, Gao, Gao, Liu,
  and Dolan]{zhang2019dialogpt}
Zhang, Y., Sun, S., Galley, M., Chen, Y.-C., Brockett, C., Gao, X., Gao, J.,
  Liu, J., and Dolan, B.
\newblock {DIALOGPT} : Large-scale generative pre-training for conversational
  response generation.
\newblock In \emph{Proc. Conf. Assoc. for Computational Linguistics (ACL)},
  pp.\  270--278, 2020.

\end{thebibliography}
