\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdelaziz et~al.(2007)Abdelaziz, Aouni, and
  El~Fayedh]{abdelaziz2007multi}
F.~B. Abdelaziz, B.~Aouni, and R.~El~Fayedh.
\newblock Multi-objective stochastic programming for portfolio selection.
\newblock \emph{European Journal of Operational Research}, 177\penalty0
  (3):\penalty0 1811--1823, 2007.

\bibitem[Agarwal et~al.(2009)Agarwal, Wainwright, Bartlett, and
  Ravikumar]{agarwal2009information}
A.~Agarwal, M.~J. Wainwright, P.~L. Bartlett, and P.~K. Ravikumar.
\newblock Information-theoretic lower bounds on the oracle complexity of convex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1--9, 2009.

\bibitem[Arora et~al.(2018)Arora, Khodak, Saunshi, and
  Vodrahalli]{arora2018compressed}
S.~Arora, M.~Khodak, N.~Saunshi, and K.~Vodrahalli.
\newblock A compressed sensing view of unsupervised text embeddings,
  bag-of-n-grams, and lstms.
\newblock In \emph{Proc. of the 6th International Conference on Learning
  Representations}, 2018.

\bibitem[Bauschke et~al.(2011)Bauschke, Combettes, et~al.]{bauschke2011convex}
H.~H. Bauschke, P.~L. Combettes, et~al.
\newblock \emph{Convex analysis and monotone operator theory in Hilbert
  spaces}, volume 408.
\newblock Springer, 2011.

\bibitem[Bianchi(2015)]{bianchi2015stochastic}
P.~Bianchi.
\newblock A stochastic proximal point algorithm: convergence and application to
  convex optimization.
\newblock In \emph{Computational Advances in Multi-Sensor Adaptive Processing
  (CAMSAP), 2015 IEEE 6th International Workshop on}, pages 1--4. IEEE, 2015.

\bibitem[Bianchi et~al.(2017)Bianchi, Hachem, and Salim]{bianchi2017constant}
P.~Bianchi, W.~Hachem, and A.~Salim.
\newblock A constant step forward-backward algorithm involving random maximal
  monotone operators.
\newblock \emph{arXiv preprint arXiv:1702.04144}, 2017.

\bibitem[Bolte et~al.(2017)Bolte, Nguyen, Peypouquet, and
  Suter]{bolte2017error}
J.~Bolte, T.~P. Nguyen, J.~Peypouquet, and B.~W. Suter.
\newblock From error bounds to the complexity of first-order descent methods
  for convex functions.
\newblock \emph{Mathematical Programming}, 165\penalty0 (2):\penalty0 471--507,
  2017.

\bibitem[Borodin et~al.(2004)Borodin, El-Yaniv, and Gogan]{borodin2004can}
A.~Borodin, R.~El-Yaniv, and V.~Gogan.
\newblock Can we learn to beat the best stock.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  345--352, 2004.

\bibitem[Chang and Lin(2011)]{chang2011libsvm}
C.-C. Chang and C.-J. Lin.
\newblock {LIBSVM}: A library for support vector machines.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
  2:\penalty0 27:1--27:27, 2011.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem[Donoho(2006)]{donoho2006compressed}
D.~L. Donoho.
\newblock Compressed sensing.
\newblock \emph{IEEE Transactions on information theory}, 52\penalty0
  (4):\penalty0 1289--1306, 2006.

\bibitem[Garrigues and Ghaoui(2009)]{garrigues2009homotopy}
P.~Garrigues and L.~E. Ghaoui.
\newblock An homotopy algorithm for the lasso with online observations.
\newblock In \emph{Advances in neural information processing systems}, pages
  489--496, 2009.

\bibitem[Grant et~al.(2008)Grant, Boyd, and Ye]{grant2008cvx}
M.~Grant, S.~Boyd, and Y.~Ye.
\newblock Cvx: Matlab software for disciplined convex programming, 2008.

\bibitem[Hastie et~al.(2004)Hastie, Rosset, Tibshirani, and
  Zhu]{hastie2004entire}
T.~Hastie, S.~Rosset, R.~Tibshirani, and J.~Zhu.
\newblock The entire regularization path for the support vector machine.
\newblock \emph{Journal of Machine Learning Research}, 5\penalty0
  (Oct):\penalty0 1391--1415, 2004.

\bibitem[Lan and Monteiro(2013)]{lan2013iteration}
G.~Lan and R.~D. Monteiro.
\newblock Iteration-complexity of first-order penalty methods for convex
  programming.
\newblock \emph{Mathematical Programming}, 138\penalty0 (1-2):\penalty0
  115--139, 2013.

\bibitem[Mahdavi et~al.(2013)Mahdavi, Yang, and Jin]{mahdavi2013stochastic}
M.~Mahdavi, T.~Yang, and R.~Jin.
\newblock Stochastic convex optimization with multiple objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1115--1123, 2013.

\bibitem[Moulines and Bach(2011)]{moulines2011non}
E.~Moulines and F.~R. Bach.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  451--459, 2011.

\bibitem[Necoara et~al.(2018{\natexlab{a}})Necoara, Nesterov, and
  Glineur]{Necoara2017}
I.~Necoara, Y.~Nesterov, and F.~Glineur.
\newblock Linear convergence of first order methods for non-strongly convex
  optimization.
\newblock \emph{Mathematical Programming}, doi: 10.1007/s10107-018-1232-1,
  2018{\natexlab{a}}.

\bibitem[Necoara et~al.(2018{\natexlab{b}})Necoara, Richtarik, and
  Patrascu]{necoara2018randomized}
I.~Necoara, P.~Richtarik, and A.~Patrascu.
\newblock Randomized projection methods for convex feasibility problems:
  conditioning and convergence rates.
\newblock \emph{arXiv preprint arXiv:1801.04873}, 2018{\natexlab{b}}.

\bibitem[Nedi{\'c} et~al.(2018)Nedi{\'c}, Olshevsky, and
  Rabbat]{nedic2018network}
A.~Nedi{\'c}, A.~Olshevsky, and M.~G. Rabbat.
\newblock Network topology and communication-computation tradeoffs in
  decentralized optimization.
\newblock \emph{Proceedings of the IEEE}, 106\penalty0 (5):\penalty0 953--976,
  2018.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Nesterov(2005)]{nesterov2005smooth}
Y.~Nesterov.
\newblock Smooth minimization of non-smooth functions.
\newblock \emph{Mathematical programming}, 103\penalty0 (1):\penalty0 127--152,
  2005.

\bibitem[Ouyang and Gray(2012)]{ouyang2012stochastic}
H.~Ouyang and A.~Gray.
\newblock Stochastic smoothing for nonsmooth minimizations: Accelerating sgd by
  exploiting structure.
\newblock \emph{arXiv preprint arXiv:1205.4481}, 2012.

\bibitem[Patrascu and Necoara(2017)]{patrascu2017nonasymptotic}
A.~Patrascu and I.~Necoara.
\newblock Nonasymptotic convergence of stochastic proximal point methods for
  constrained convex optimization.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 7204--7245, 2017.

\bibitem[Polyak and Juditsky(1992)]{polyak1992acceleration}
B.~T. Polyak and A.~B. Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM Journal on Control and Optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Rosasco et~al.(2014)Rosasco, Villa, and
  V{\~u}]{rosasco2014convergence}
L.~Rosasco, S.~Villa, and B.~C. V{\~u}.
\newblock Convergence of stochastic proximal gradient algorithm.
\newblock \emph{arXiv preprint arXiv:1403.5074}, 2014.

\bibitem[Salim(2018)]{salim2018random}
A.~Salim.
\newblock \emph{Random monotone operators and application to stochastic
  optimization}.
\newblock PhD thesis, Universit{\'e} Paris-Saclay, 2018.

\bibitem[Shalev-Shwartz et~al.(2011)Shalev-Shwartz, Singer, Srebro, and
  Cotter]{shalev2011pegasos}
S.~Shalev-Shwartz, Y.~Singer, N.~Srebro, and A.~Cotter.
\newblock Pegasos: Primal estimated sub-gradient solver for svm.
\newblock \emph{Mathematical programming}, 127\penalty0 (1):\penalty0 3--30,
  2011.

\bibitem[Sonnenburg et~al.(2006)Sonnenburg, R{\"a}tsch, Sch{\"a}fer, and
  Sch{\"o}lkopf]{sonnenburg2006large}
S.~Sonnenburg, G.~R{\"a}tsch, C.~Sch{\"a}fer, and B.~Sch{\"o}lkopf.
\newblock Large scale multiple kernel learning.
\newblock \emph{Journal of Machine Learning Research}, 7\penalty0
  (Jul):\penalty0 1531--1565, 2006.

\bibitem[Towfic and Sayed(2015)]{towfic2015stability}
Z.~J. Towfic and A.~H. Sayed.
\newblock Stability and performance limits of adaptive primal-dual networks.
\newblock \emph{IEEE Transactions on Signal Processing}, 63\penalty0
  (11):\penalty0 2888--2903, 2015.

\bibitem[Tran-Dinh et~al.(2018{\natexlab{a}})Tran-Dinh, Alacaoglu, Fercoq, and
  Cevher]{tran2018adaptive}
Q.~Tran-Dinh, A.~Alacaoglu, O.~Fercoq, and V.~Cevher.
\newblock An adaptive primal-dual framework for nonsmooth convex minimization.
\newblock \emph{arXiv preprint arXiv:1808.04648}, 2018{\natexlab{a}}.

\bibitem[Tran-Dinh et~al.(2018{\natexlab{b}})Tran-Dinh, Fercoq, and
  Cevher]{tran2018smooth}
Q.~Tran-Dinh, O.~Fercoq, and V.~Cevher.
\newblock A smooth primal-dual optimization framework for nonsmooth composite
  convex minimization.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (1):\penalty0
  96--134, 2018{\natexlab{b}}.

\bibitem[Tseng(2008)]{tseng2008accelerated}
P.~Tseng.
\newblock On accelerated proximal gradient methods for convex-concave
  optimization.
\newblock 2008.
\newblock submitted to SIAM J. Optim.

\bibitem[Van~Nguyen et~al.(2017)Van~Nguyen, Fercoq, and
  Cevher]{van2017smoothing}
Q.~Van~Nguyen, O.~Fercoq, and V.~Cevher.
\newblock Smoothing technique for nonsmooth composite minimization with linear
  operator.
\newblock \emph{arXiv preprint arXiv:1706.05837}, 2017.

\bibitem[Wang et~al.(2015)Wang, Chen, Liu, and Gu]{wang2015random}
M.~Wang, Y.~Chen, J.~Liu, and Y.~Gu.
\newblock Random multi-constraint projection: Stochastic gradient methods for
  convex optimization with many constraints.
\newblock \emph{arXiv preprint arXiv:1511.03760}, 2015.

\bibitem[Yu et~al.(2017)Yu, Neely, and Wei]{yu2017online}
H.~Yu, M.~Neely, and X.~Wei.
\newblock Online convex optimization with stochastic constraints.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1428--1438, 2017.

\end{thebibliography}
