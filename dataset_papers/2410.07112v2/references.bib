
@article{awadalla2023openflamingo,
  title={Openflamingo: An open-source framework for training large autoregressive vision-language models},
  author={Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others},
  journal={arXiv preprint arXiv:2308.01390},
  year={2023}
}

@misc{AlephAlphaAPIDocs,
  title = {{Aleph Alpha API Documentation}},
  author = {{Aleph Alpha}},
  year = {2024},
  url = {https://docs.aleph-alpha.com/docs/multimodality/basic_principles/},
}

@article{bai2023qwenvl,
  title={{Qwen-VL}: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@inproceedings{balanced_vqa_v2,
  title={Making the {V} in {VQA} Matter: Elevating the Role of Image Understanding in {V}isual {Q}uestion {A}nswering},
  author={Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6904--6913},
  year={2017}
}

@article{bommasani2022opportunities,
  title={On the opportunities and risks of foundation models},
  author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren Gillespie and Karan Goel and Noah Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and Omar Khattab and Pang Wei Koh and Mark Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Ben Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and Julian Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Rob Reich and Hongyu Ren and Frieda Rong and Yusuf Roohani and Camilo Ruiz and Jack Ryan and Christopher Ré and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishnan Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tramèr and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@misc{claude3,
      title={The {Claude} 3 Model Family: {Opus, Sonnet, Haiku}}, 
      author={Anthropic},
      year={2024},
      eprint={https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf},
      primaryClass={cs.LG}
}

@article{cui2023holistic,
  title={Holistic analysis of hallucination in {GPT}-4v(ision): Bias and interference challenges},
  author={Cui, Chenhang and Zhou, Yiyang and Yang, Xinyu and Wu, Shirley and Zhang, Linjun and Zou, James and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2311.03287},
  year={2023}
}

@inproceedings{fraser2024examining,
  title={Examining Gender and Racial Bias in Large Vision--Language Models Using a Novel Dataset of Parallel Images},
  author={Fraser, Kathleen C and Kiritchenko, Svetlana},
  booktitle={Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics},
  pages={690--713},
  year={2024}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3608--3617},
  year={2018}
}

@inproceedings{hudson2018gqa,
  title={{GQA}: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6700--6709},
  year={2019}
}

@article{kiela2021hateful,
  title={The hateful memes challenge: Detecting hate speech in multimodal memes},
  author={Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={2611--2624},
  year={2020}
}


@article{laurençon2023obelics,
  title={{OBELICS}: An open web-scale filtered dataset of interleaved image-text documents},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Tronchon, L{\'e}o and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander and Kiela, Douwe and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{lee2023holistic,
  title={Holistic evaluation of text-to-image models},
  author={Lee, Tony and Yasunaga, Michihiro and Meng, Chenlin and Mai, Yifan and Park, Joon Sung and Gupta, Agrim and Zhang, Yunzhi and Narayanan, Deepak and Teufel, Hannah and Bellagente, Marco and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{lee2024prometheusvision,
    title = "Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation",
    author = "Lee, Seongyun  and
      Kim, Seungone  and
      Park, Sue Hyun  and
      Kim, Geewook  and
      Seo, Minjoon",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
}

@article{
liang2023holistic,
title={Holistic Evaluation of Language Models},
author={Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Alexander Cosgrove and Christopher D Manning and Christopher Re and Diana Acosta-Navas and Drew Arad Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue WANG and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri S. Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Andrew Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=iO4LZibEqW},
}



@inproceedings{lin2015microsoft,
  title={{Microsoft COCO}: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={{European Conference on Computer Vision}},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@inproceedings{liu2023query,
  title={Query-relevant images jailbreak large multi-modal models},
  author={Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu},
  booktitle={{18th European Conference on Computer Vision}},
  year={2024},
  organization={Springer},
}

@article{liu2023queryrelevant,
  title={Query-relevant images jailbreak large multi-modal models},
  author={Liu, Xin and Zhu, Yichen and Lan, Yunshi and Yang, Chao and Qiao, Yu},
  journal={arXiv preprint arXiv:2311.17600},
  year={2023}
}


@inproceedings{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  booktitle={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@misc{liu2024llavanext,
    title={{LLaVA-NeXT}: Improved reasoning, {OCR}, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

@inproceedings{liu2023improvedllava,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26296--26306},
  year={2024}
}

@inproceedings{lu2024mathvista,
  title={MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  author={Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@misc{openai2024gpt4,
      title={{GPT-4} Technical Report}, 
      author={OpenAI},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{geminiteam2024gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@inproceedings{schwenk2022aokvqa,
  title={{A-OKVQA}: A benchmark for visual question answering using world knowledge},
  author={Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh},
  booktitle={17th European Conference on Computer Vision},
  pages={146--162},
  year={2022},
  organization={Springer}
}

@inproceedings{thapliyal2022crossmodal3600,
  title={Crossmodal-3600: A Massively Multilingual Multimodal Evaluation Dataset},
  author={Thapliyal, Ashish V and Tuset, Jordi Pont and Chen, Xi and Soricut, Radu},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={715--729},
  year={2022}
}

@inproceedings{tu2023unicorneccv,
      title={How Many Unicorns Are in This Image? {A} Safety Evaluation Benchmark for Vision {LLM}s}, 
      author={Haoqin Tu and Chenhang Cui and Zijun Wang and Yiyang Zhou and Bingchen Zhao and Junlin Han and Wangchunshu Zhou and Huaxiu Yao and Cihang Xie},
      year={2024},
      booktitle={{18th European Conference on Computer Vision}},
      organization={Springer},
}

@article{tu2023unicorns,
  title={How Many Unicorns Are in This Image? {A} Safety Evaluation Benchmark for Vision {LLM}s},
  author={Tu, Haoqin and Cui, Chenhang and Wang, Zijun and Zhou, Yiyang and Zhao, Bingchen and Han, Junlin and Zhou, Wangchunshu and Yao, Huaxiu and Xie, Cihang},
  journal={arXiv preprint arXiv:2311.16101},
  year={2023}
}

@inproceedings{vedantam2015cider,
  title={{CIDEr}: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}

@article{young2014image,
      title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
      author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
      journal={Transactions of the Association for Computational Linguistics},
      year={2014},
}

@inproceedings{yue2023mmmu,
  title={{MMMU}: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert {AGI}},
  author={Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9556--9567},
  year={2024}
}

@article{fan2024muffin,
  title={Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel {VQA}},
  author={Fan, Yue and Gu, Jing and Zhou, Kaiwen and Yan, Qianqi and Jiang, Shan and Kuo, Ching-Chen and Guan, Xinze and Wang, Xin Eric},
  journal={arXiv preprint arXiv:2401.15847},
  year={2024}
}

@inproceedings{wang2024mementos,
    title = "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences",
    author = "Wang, Xiyao  and
      Zhou, Yuhang  and
      Liu, Xiaoyu  and
      Lu, Hongjin  and
      Xu, Yuancheng  and
      He, Feihong  and
      Yoon, Jaehong  and
      Lu, Taixi  and
      Liu, Fuxiao  and
      Bertasius, Gedas  and
      Bansal, Mohit  and
      Yao, Huaxiu  and
      Huang, Furong",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
    year = "2024",
    pages = "416--442",
}

@inproceedings{li2023evaluating,
  title={Evaluating Object Hallucination in Large Vision-Language Models},
  author={Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Wayne Xin and Wen, Ji-Rong},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={292--305},
  year={2023}
}

@INPROCEEDINGS{li2023seed,
  author={Li, Bohao and Ge, Yuying and Ge, Yixiao and Wang, Guangzhi and Wang, Rui and Zhang, Ruimao and Shan, Ying},
  booktitle={2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={{SEED}-Bench: Benchmarking Multimodal Large Language Models}, 
  year={2024},
  pages={13299-13308},
  doi={10.1109/CVPR52733.2024.01263}
}


@article{yin2023survey,
  title={A survey on multimodal large language models},
  author={Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
  journal={arXiv preprint arXiv:2306.13549},
  year={2023}
}


@inproceedings{dai2024instructblip,
  title={{InstructBLIP}: Towards general-purpose vision-language models with instruction tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
  booktitle={Advances in neural information processing systems},
  year={2023}
}

@inproceedings{lin2023text,
  title={Text-driven image editing via learnable regions},
  author={Lin, Yuanze and Chen, Yi-Wen and Tsai, Yi-Hsuan and Jiang, Lu and Yang, Ming-Hsuan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7059--7068},
  year={2024}
}

@inproceedings{song2022clip,
    title = {{CLIP} Models are Few-Shot Learners: Empirical Studies on {VQA} and Visual Entailment},
    author = {Song, Haoyu and Dong, Li  and Zhang, Weinan  and Liu, Ting  and Wei, Furu},
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
    year = "2022",
    publisher = "Association for Computational Linguistics",
    pages = "6088--6100",
}

@misc{msft2023bing,
  title={Bing Preview Release Notes: Multimodal Visual Search in Chat and Bing Chat Enterprise},
  author={{The Bing Team}},
  year={2023},
  url={https://blogs.bing.com/search/july-2023/Bing-Preview-Release-Notes-Multimodal-Visual-Search-in-Chat-and-Bing-Chat-Enterprise},
}

@InProceedings{pmlr-v229-zitkovich23a,
  title = 	 {{RT-2}: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control},
  author =       {Zitkovich, Brianna and Yu, Tianhe and Xu, Sichun and Xu, Peng and Xiao, Ted and Xia, Fei and Wu, Jialin and Wohlhart, Paul and Welker, Stefan and Wahid, Ayzaan and Vuong, Quan and Vanhoucke, Vincent and Tran, Huong and Soricut, Radu and Singh, Anikait and Singh, Jaspiar and Sermanet, Pierre and Sanketi, Pannag R. and Salazar, Grecia and Ryoo, Michael S. and Reymann, Krista and Rao, Kanishka and Pertsch, Karl and Mordatch, Igor and Michalewski, Henryk and Lu, Yao and Levine, Sergey and Lee, Lisa and Lee, Tsang-Wei Edward and Leal, Isabel and Kuang, Yuheng and Kalashnikov, Dmitry and Julian, Ryan and Joshi, Nikhil J. and Irpan, Alex and Ichter, Brian and Hsu, Jasmine and Herzog, Alexander and Hausman, Karol and Gopalakrishnan, Keerthana and Fu, Chuyuan and Florence, Pete and Finn, Chelsea and Dubey, Kumar Avinava and Driess, Danny and Ding, Tianli and Choromanski, Krzysztof Marcin and Chen, Xi and Chebotar, Yevgen and Carbajal, Justice and Brown, Noah and Brohan, Anthony and Arenas, Montserrat Gonzalez and Han, Kehang},
  booktitle = 	 {Proceedings of The 7th Conference on Robot Learning},
  year = 	 {2023},
}

@article{padlewski2024vibe,
  title={{Vibe-Eval}: A hard evaluation suite for measuring progress of multimodal language models},
  author={Padlewski, Piotr and Bain, Max and Henderson, Matthew and Zhu, Zhongkai and Relan, Nishant and Pham, Hai and Ong, Donovan and Aleksiev, Kaloyan and Ormazabal, Aitor and Phua, Samuel and others},
  journal={arXiv preprint arXiv:2405.02287},
  year={2024}
}

@misc{mai2024mmmu,
    title={Massive Multitask Language Understanding (MMLU) on {HELM}},
    author={Mai, Yifan and Liang, Percy },
    url={https://crfm.stanford.edu/2024/05/01/helm-mmlu.html},
    year={2024},
    note={Accessed Jun 03, 2024}
}

@misc{openai2024hello,
  title={Hello {GPT-4}o},
  author={{OpenAI}},
  year={2024},
  url={https://openai.com/index/hello-gpt-4o/}
}

@article{beyer2024paligamma,
  title={{PaliGemma}: A versatile {3B VLM} for transfer},
  author={Beyer, Lucas and Steiner, Andreas and Pinto, Andr{\'e} Susano and Kolesnikov, Alexander and Wang, Xiao and Salz, Daniel and Neumann, Maxim and Alabdulmohsin, Ibrahim and Tschannen, Michael and Bugliarello, Emanuele and others},
  journal={arXiv preprint arXiv:2407.07726},
  year={2024}
}

@article{geminiteam2024gemini1.5,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={{Gemini Team}},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@misc{anthropic2024Claude3,
    title={The {Claude} 3 Model Family: {Opus, Sonnet, Haiku}},
    author={Anthropic},
    year={2024},
    url={https://www.anthropic.com/claude-3-model-card}
}

@article{laurençon2024matters,
  title={What matters when building vision-language models?},
  author={Lauren{\c{c}}on, Hugo and Tronchon, L{\'e}o and Cord, Matthieu and Sanh, Victor},
  journal={arXiv preprint arXiv:2405.02246},
  year={2024}
}

@inproceedings{zhao2022ood,
  title={{OOD-CV}: A Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images},
  author={Zhao, Bingchen and Yu, Shaozuo and Ma, Wufei and Yu, Mingxin and Mei, Shenxiao and Wang, Angtian and He, Ju and Yuille, Alan L and Kortylewski, Adam},
  booktitle={17th European Conference on Computer Vision},
  pages={163--180},
  year={2022},
  organization={Springer}
}

@inproceedings{tu2023sight,
  title={Sight Beyond Text: Multi-Modal Training Enhances {LLM}s in Truthfulness and Ethics},
  author={Tu, Haoqin and Zhao, Bingchen and Wei, Chen and Xie, Cihang},
  booktitle={NeurIPS Workshop on Instruction Tuning and Instruction Following},
  year={2023}
}

@article{eitz2012hdhso,
  title={How do humans sketch objects?},
  author={Eitz, Mathias and Hays, James and Alexa, Marc},
  journal={ACM Transactions on graphics (TOG)},
  volume={31},
  number={4},
  pages={1--10},
  year={2012},
  publisher={Acm New York, NY, USA}
}
@inproceedings{chen-etal-2024-measuring,
    title = "Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models",
    author = "Chen, Yangyi  and
      Sikka, Karan  and
      Cogswell, Michael  and
      Ji, Heng  and
      Divakaran, Ajay",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2024",
    pages = "192--210",
}

@inproceedings{karkkainen2019fairface,
  title={{FairFace}: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation},
  author={K{\"a}rkk{\"a}inen, Kimmo and Joo, Jungseock},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={1548--1558},
  year={2021}
}

@inproceedings{das2024exams,
    title = "{EXAMS}-{V}: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models",
    author = "Das, Rocktim  and
      Hristov, Simeon  and
      Li, Haonan  and
      Dimitrov, Dimitar  and
      Koychev, Ivan  and
      Nakov, Preslav",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics",
    year = "2024",
    pages = "7768--7791",
}

@misc{realworldqa,
  author={{Grok-1.5 Team}},
  year=2024,
  title={Grok-1.5 Vision Preview},
  url={https://x.ai/blog/grok-1.5v},
  publisher={xAI},
  doi={10.5281/zenodo.4957738},
}

@article{wang2024white,
  title={White-box Multimodal Jailbreaks Against Large Vision-Language Models},
  author={Wang, Ruofan and Ma, Xingjun and Zhou, Hanxu and Ji, Chuanjun and Ye, Guangnan and Jiang, Yu-Gang},
  journal={arXiv preprint arXiv:2405.17894},
  year={2024}
}


@article{ying2024jailbreak,
  title={Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt},
  author={Ying, Zonghao and Liu, Aishan and Zhang, Tianyuan and Yu, Zhengmin and Liang, Siyuan and Liu, Xianglong and Tao, Dacheng},
  journal={arXiv preprint arXiv:2406.04031},
  year={2024}
}

@article{tao2024imgtrojan,
  title={{ImgTrojan}: Jailbreaking Vision-Language Models with {ONE} Image},
  author={Tao, Xijia and Zhong, Shuai and Li, Lei and Liu, Qi and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2403.02910},
  year={2024}
}

@misc{W3Techs2024usage,
  title={Usage statistics of content languages for websites},
  author={{W3Techs}},
  url={https://w3techs.com/technologies/overview/content_language},
  note={Accessed Sept 18, 2024}
}