\begin{thebibliography}{10}

\bibitem{akyurek2022learning}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with linear models.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2022.

\bibitem{allen2018learning}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks, going beyond two layers.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em International Conference on Machine Learning}, pages 242--252, 2019.

\bibitem{ambrosio2005GradientFI}
Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar{\'e}.
\newblock In metric spaces and in the space of probability measures.
\newblock In {\em Gradient Flows}, 2005.

\bibitem{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks.
\newblock In {\em International Conference on Machine Learning}, pages 322--332, 2019.

\bibitem{bai2023transformers}
Yu~Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei.
\newblock Transformers as statisticians: Provable in-context learning with in-context algorithm selection.
\newblock In {\em Workshop on Efficient Systems for Foundation Models @ ICML2023}, 2023.

\bibitem{barboni2022meanfield}
Rapha{\"e}l Barboni, Gabriel Peyr{\'e}, and Fran{\c{c}}ois-Xavier Vialard.
\newblock On global convergence of resnets: From finite to infinite width using linear parameterization.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{bernhard2023alternatives}
James Bernhard.
\newblock Alternatives to the scaled dot product for attention in the transformer neural network architecture, 2023.

\bibitem{bordelon2024infinitelimitsmultiheadtransformer}
Blake Bordelon, Hamza~Tahir Chaudhry, and Cengiz Pehlevan.
\newblock Infinite limits of multi-head transformer dynamics, 2024.

\bibitem{brown2020icl}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin, editors, {\em Advances in Neural Information Processing Systems}, volume~33, pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem{cao2019generalizationsgd}
Yuan Cao and Quanquan Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{chen2018neural}
Ricky~TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{chen2024generalization}
Yihang Chen, Fanghui Liu, Yiping Lu, Grigorios Chrysos, and Volkan Cevher.
\newblock Generalization of scaled deep resnets in the mean-field regime.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{chen2020AGN}
Zixiang Chen, Yuan Cao, Quanquan Gu, and Tong Zhang.
\newblock A generalized neural tangent kernel analysis for two-layer neural networks.
\newblock {\em arXiv: Learning}, 2020.

\bibitem{cheng2023interpolation}
Jingpu Cheng, Qianxiao Li, Ting Lin, and Zuowei Shen.
\newblock Interpolation, approximation and controllability of deep neural networks.
\newblock {\em arXiv preprint arXiv:2309.06015}, 2023.

\bibitem{chizat2018global}
L\'{e}na\"{\i}c Chizat and Francis Bach.
\newblock On the global convergence of gradient descent for over-parameterized models using optimal transport.
\newblock In {\em Proceedings of the 32nd International Conference on Neural Information Processing Systems}, NIPS'18, page 3040–3050, Red Hook, NY, USA, 2018. Curran Associates Inc.

\bibitem{chizat2018note}
L\'{e}na\"{\i}c Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{cybenko1989ApproximationBS}
George~V. Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock {\em Mathematics of Control, Signals and Systems}, 2:303--314, 1989.

\bibitem{george2021lipschitz}
George Dasoulas, Kevin Scaman, and Aladin Virmaux.
\newblock Lipschitz normalization for self-attention layers with application to graph neural networks.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th International Conference on Machine Learning}, volume 139 of {\em Proceedings of Machine Learning Research}, pages 2456--2466. PMLR, 18--24 Jul 2021.

\bibitem{dehghani2023vision}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas~Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme~Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd~Van Steenkiste, Gamaleldin~Fathy Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey~A. Gritsenko, Vighnesh Birodkar, Cristina~Nader Vasconcelos, Yi~Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah~J. Harmsen, and Neil Houlsby.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, {\em Proceedings of the 40th International Conference on Machine Learning}, volume 202 of {\em Proceedings of Machine Learning Research}, pages 7480--7512. PMLR, 23--29 Jul 2023.

\bibitem{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In {\em North American Chapter of the Association for Computational Linguistics}, 2019.

\bibitem{ding2021meanfield1}
Zhiyan Ding, Shi Chen, Qin Li, and Stephen Wright.
\newblock On the global convergence of gradient descent for multi-layer resnets in the mean-field regime, 2021.

\bibitem{ding2022meanfield2}
Zhiyan Ding, Shi Chen, Qin Li, and Stephen Wright.
\newblock Overparameterization of deep resnet: Zero loss and mean-field analysis.
\newblock {\em Journal of Machine Learning Research}, 23:48--1, 2022.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{du2018gradientdeep}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In {\em International Conference on Machine Learning}, pages 1675--1685, 2019.

\bibitem{han2019mean}
Weinan E, Jiequn Han, and Qianxiao Li.
\newblock A mean-field optimal control formulation of deep learning.
\newblock {\em Research in the Mathematical Sciences}, 6(1):1--41, 2019.

\bibitem{edelman2022inductive}
Benjamin~L. Edelman, Surbhi Goel, Sham~M. Kakade, and Cyril Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms, 2022.

\bibitem{fang2019ntk}
Cong Fang, Hanze Dong, and Tong Zhang.
\newblock Over parameterized two-level neural networks can learn near optimal feature representations.
\newblock {\em ArXiv}, abs/1910.11508, 2019.

\bibitem{fang2019convexformulation}
Cong Fang, Yihong Gu, Weizhong Zhang, and Tong Zhang.
\newblock Convex formulation of overparameterized deep neural networks.
\newblock {\em IEEE Transactions on Information Theory}, 68(8):5340--5352, 2022.

\bibitem{frigyik2008frechet}
Bela~A. Frigyik, Santosh Srivastava, and Maya~R. Gupta.
\newblock Introduction to functional derivatives.
\newblock UWEE Tech Report 2008-0001, University of Washington Department of Electrical Engineering, 2008.

\bibitem{fu2023what}
Hengyu Fu, Tianyu Guo, Yu~Bai, and Song Mei.
\newblock What can a single attention layer learn? a study through the random features lens.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{guo2024how}
Tianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu~Bai.
\newblock How do transformers learn in-context beyond simple functions? a case study on learning with representations.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{huang2023context}
Yu~Huang, Yuan Cheng, and Yingbin Liang.
\newblock In-context convergence of transformers.
\newblock {\em arXiv preprint arXiv:2310.05249}, 2023.

\bibitem{funahashi1989OnTA}
Ken ichi Funahashi.
\newblock On the approximate realization of continuous mappings by neural networks.
\newblock {\em Neural Networks}, 2:183--192, 1989.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock In {\em Advances in neural information processing systems}, pages 8571--8580, 2018.

\bibitem{jelassi2022vision}
Samy Jelassi, Michael Sander, and Yuanzhi Li.
\newblock Vision transformers provably learn spatial structure.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, {\em Advances in Neural Information Processing Systems}, volume~35, pages 37822--37836. Curran Associates, Inc., 2022.

\bibitem{richard2000variationalFPE}
Richard Jordan, David Kinderlehrer, and Felix Otto.
\newblock The variational formulation of the fokker-planck equation.
\newblock {\em SIAM Journal on Mathematical Analysis}, 29, 04 2000.

\bibitem{kajitsuka2024transformers}
Tokio Kajitsuka and Issei Sato.
\newblock Are transformers with one layer self-attention using low-rank weight matrices universal approximators?, 2024.

\bibitem{kim2021lipschitz}
Hyunjik Kim, George Papamakarios, and Andriy Mnih.
\newblock The lipschitz constant of self-attention, 2021.

\bibitem{kim2023provable}
Junghwan Kim, Michelle Kim, and Barzan Mozafari.
\newblock Provable memorization capacity of transformers.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{kim2024transformers}
Juno Kim and Taiji Suzuki.
\newblock Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on the attention landscape.
\newblock In {\em Forty-first International Conference on Machine Learning}, 2024.

\bibitem{li2018maximum}
Qianxiao Li, Long Chen, Cheng Tai, and Weinan E.
\newblock Maximum principle based algorithms for deep learning.
\newblock {\em Journal of Machine Learning Research}, 18(165):1--29, 2018.

\bibitem{li2018optimal}
Qianxiao Li and Shuji Hao.
\newblock An optimal control approach to deep learning and applications to discrete-weight neural networks.
\newblock In {\em International Conference on Machine Learning}, pages 2985--2994. PMLR, 2018.

\bibitem{li2022deep}
Qianxiao Li, Ting Lin, and Zuowei Shen.
\newblock Deep learning via dynamical systems: An approximation perspective.
\newblock {\em Journal of the European Mathematical Society}, 25(5):1671--1709, 2022.

\bibitem{li2023transformers}
Yuchen Li, Yuanzhi Li, and Andrej Risteski.
\newblock How do transformers learn topic structure: Towards a mechanistic understanding.
\newblock In {\em International Conference on Machine Learning}, pages 19689--19729. PMLR, 2023.

\bibitem{lin2024transformers}
Licong Lin, Yu~Bai, and Song Mei.
\newblock Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{lu2020meanfield}
Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying.
\newblock A mean field analysis of deep {R}es{N}et and beyond: Towards provably optimization via overparameterization from depth.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the 37th International Conference on Machine Learning}, volume 119 of {\em Proceedings of Machine Learning Research}, pages 6426--6436. PMLR, 13--18 Jul 2020.

\bibitem{luong2015effective}
Thang Luong, Hieu Pham, and Christopher~D. Manning.
\newblock Effective approaches to attention-based neural machine translation.
\newblock In Llu{\'\i}s M{\`a}rquez, Chris Callison-Burch, and Jian Su, editors, {\em Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}, pages 1412--1421, Lisbon, Portugal, September 2015. Association for Computational Linguistics.

\bibitem{mahdavi2024memorization}
Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis.
\newblock Memorization capacity of multi-head attention in transformers.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{mei2019mean}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit.
\newblock In {\em Conference on Learning Theory}, 2019.

\bibitem{mei2018mean}
Song Mei, Andrea Montanari, and Phan-Minh Nguyen.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock {\em Proceedings of the National Academy of Sciences}, 115(33):E7665--E7671, 2018.

\bibitem{micchelli2006universal}
Charles~A. Micchelli, Yuesheng Xu, and Haizhang Zhang.
\newblock Universal kernels.
\newblock {\em J. Mach. Learn. Res.}, 7:2651--2667, 2006.

\bibitem{nitanda2017stochastic}
Atsushi Nitanda and Taiji Suzuki.
\newblock Stochastic particle gradient descent for infinite ensembles, 2017.

\bibitem{nitanda2022particle}
Atsushi Nitanda, Denny Wu, and Taiji Suzuki.
\newblock Particle dual averaging: optimization of mean field neural network with global convergence rate analysis*.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment}, 2022(11):114010, nov 2022.

\bibitem{achiam2023gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock {\em ArXiv}, abs/2303.08774, 2023.

\bibitem{otto2001geometry}
Felix Otto.
\newblock The geometry of dissipative evolution equations: The porous medium equation.
\newblock {\em Communications in Partial Differential Equations}, 26(1-2):101--174, 2001.

\bibitem{pinkus1999ApproximationTO}
Allan Pinkus.
\newblock Approximation theory of the mlp model in neural networks.
\newblock {\em Acta Numerica}, 8:143 -- 195, 1999.

\bibitem{bittner1963adjoint}
L.~S. Pontryagin, V.~G. Boltyanskii, R.~V. Gamkrelidze, and E.~F. Mishechenko.
\newblock The mathematical theory of optimal processes.
\newblock {\em Zamm-zeitschrift Fur Angewandte Mathematik Und Mechanik}, 43:514--515, 1963.

\bibitem{risken1996fokker}
H.~Risken.
\newblock {\em The Fokker-Planck Equation: Methods of Solution and Applications}.
\newblock Springer, 1996.

\bibitem{santambrogio2015optimal}
Filippo Santambrogio.
\newblock Optimal transport for applied mathematicians.
\newblock {\em Birk{\"a}user, NY}, 55(58-63):94, 2015.

\bibitem{santambrogio2016EuclideanMA}
Filippo Santambrogio.
\newblock \{Euclidean, metric, and Wasserstein\} gradient flows: an overview.
\newblock {\em Bulletin of Mathematical Sciences}, 7:87--154, 2016.

\bibitem{sznitman1991nonlinear}
Alain-Sol Sznitman.
\newblock Topics in propagation of chaos.
\newblock In {\em Ecole d'Et{\'e} de Probabilit{\'e}s de Saint-Flour XIX—1989}, pages 165--251. Springer, 1991.

\bibitem{takakura2023approximation}
Shokichi Takakura and Taiji Suzuki.
\newblock Approximation and estimation ability of transformers for sequence-to-sequence functions with infinite dimensional input.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, {\em Proceedings of the 40th International Conference on Machine Learning}, volume 202 of {\em Proceedings of Machine Learning Research}, pages 33416--33447. PMLR, 23--29 Jul 2023.

\bibitem{tay2020synthesizer}
Yi~Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng.
\newblock Synthesizer: Rethinking self-attention for transformer models.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em ArXiv}, abs/2302.13971, 2023.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem{veit2016residual}
Andreas Veit, Michael~J Wilber, and Serge Belongie.
\newblock Residual networks behave like ensembles of relatively shallow networks.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{villani2008kantorovich}
C.~Villani.
\newblock {\em Optimal Transport, Old and New}, volume 338 of {\em Grundlehren der Mathematischen Wissenschaften}.
\newblock Springer-Verlag, 2008.

\bibitem{vuckovic2020lipschitz}
James Vuckovic, Aristide Baratin, and R{\'e}mi~Tachet des Combes.
\newblock A mathematical theory of attention.
\newblock {\em ArXiv}, abs/2007.02876, 2020.

\bibitem{wei2018regularization}
Colin Wei, J.~Lee, Qiang Liu, and Tengyu Ma.
\newblock Regularization matters: Generalization and optimization of neural nets v.s. their induced kernel.
\newblock In {\em Neural Information Processing Systems}, 2018.

\bibitem{weinan2017proposal}
E~Weinan.
\newblock A proposal on machine learning via dynamical systems.
\newblock {\em Communications in Mathematics and Statistics}, 1(5):1--11, 2017.

\bibitem{weinan2019machine}
E~Weinan, Chao Ma, and Lei Wu.
\newblock Machine learning from a continuous viewpoint, i.
\newblock {\em Science China Mathematics}, 63:2233 -- 2266, 2019.

\bibitem{yun2019AreTU}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank~J. Reddi, and Sanjiv Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence functions?
\newblock {\em ArXiv}, abs/1912.10077, 2019.

\bibitem{zhang2021understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking generalization.
\newblock {\em Communications of the ACM}, 64(3):107--115, 2021.

\bibitem{zhang2023trained}
Ruiqi Zhang, Spencer Frei, and Peter Bartlett.
\newblock Trained transformers learn linear models in-context.
\newblock In {\em R0-FoMo:Robustness of Few-shot and Zero-shot Learning in Large Foundation Models}, 2023.

\bibitem{zou2019gradient}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Gradient descent optimizes over-parameterized deep {ReLU} networks.
\newblock {\em Machine Learning}, Oct 2019.

\end{thebibliography}
