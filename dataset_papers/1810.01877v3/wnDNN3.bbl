\begin{thebibliography}{10}

\bibitem{anthony2009neural}
Martin Anthony and Peter~L Bartlett.
\newblock {\em Neural network learning: Theoretical foundations}.
\newblock cambridge university press, 2009.

\bibitem{arora2018understanding}
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee.
\newblock Understanding deep neural networks with rectified linear units.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{bach2017breaking}
Francis Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock {\em Journal of Machine Learning Research}, 18(19):1--53, 2017.

\bibitem{bartlett1998sample}
Peter~L Bartlett.
\newblock The sample complexity of pattern classification with neural networks:
  the size of the weights is more important than the size of the network.
\newblock {\em IEEE transactions on Information Theory}, 44(2):525--536, 1998.

\bibitem{bartlett2017spectrally}
Peter~L Bartlett, Dylan~J Foster, and Matus~J Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6241--6250, 2017.

\bibitem{bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock {\em Journal of Machine Learning Research}, 3:463--482, 2002.

\bibitem{Boucheron2003Concentration}
Stéphane Boucheron, Gábor Lugosi, and Olivier Bousquet.
\newblock Concentration inequalities.
\newblock In {\em Summer School on Machine Learning}, pages 208--240, 2003.

\bibitem{cybenko1989approximation}
George Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock {\em Mathematics of control, signals and systems}, 2(4):303--314,
  1989.

\bibitem{Eldan2016ThePO}
Ronen Eldan and Ohad Shamir.
\newblock The power of depth for feedforward neural networks.
\newblock In {\em Conference on Learning Theory}, pages 907--940, 2016.

\bibitem{pmlr-v75-golowich18a}
Noah Golowich, Alexander Rakhlin, and Ohad Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock In {\em Proceedings of the 31st Conference On Learning Theory}, 2018.

\bibitem{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio.
\newblock {\em Deep learning}, volume~1.
\newblock MIT press Cambridge, 2016.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hornik1991approximation}
Kurt Hornik.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock {\em Neural networks}, 4(2):251--257, 1991.

\bibitem{kakade2009complexity}
Sham~M Kakade, Karthik Sridharan, and Ambuj Tewari.
\newblock On the complexity of linear prediction: Risk bounds, margin bounds,
  and regularization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  793--800, 2009.

\bibitem{ledoux2013probability}
Michel Ledoux and Michel Talagrand.
\newblock {\em Probability in Banach Spaces: isoperimetry and processes}.
\newblock Springer Science \& Business Media, 2013.

\bibitem{liang2016deep}
Shiyu Liang and R~Srikant.
\newblock Why deep neural networks for function approximation?
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{mohri2012foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock {\em Foundations of machine learning}.
\newblock MIT press, 2012.

\bibitem{neyshabur2018a}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
\newblock A {PAC}-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{neyshabur2015norm}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In {\em Conference on Learning Theory}, pages 1376--1401, 2015.

\bibitem{pinkus1999approximation}
Allan Pinkus.
\newblock Approximation theory of the mlp model in neural networks.
\newblock {\em Acta numerica}, 8:143--195, 1999.

\bibitem{pmlr-v70-safran17a}
Itay Safran and Ohad Shamir.
\newblock Depth-width tradeoffs in approximating natural functions with neural
  networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, volume~70 of {\em Proceedings of Machine Learning Research}, pages
  2979--2987, International Convention Centre, Sydney, Australia, 2017. PMLR.

\bibitem{salimans2016weight}
Tim Salimans and Diederik~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  901--909, 2016.

\bibitem{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem{shalev2007primal}
Shai Shalev-Shwartz and Yoram Singer.
\newblock A primal-dual perspective of online learning algorithms.
\newblock {\em Machine Learning}, 69(2-3):115--142, 2007.

\bibitem{sun2016depth}
Shizhao Sun, Wei Chen, Liwei Wang, Xiaoguang Liu, and Tie-Yan Liu.
\newblock On the depth of deep neural networks: A theoretical view.
\newblock In {\em AAAI}, pages 2066--2072, 2016.

\bibitem{pmlr-v49-telgarsky16}
Matus Telgarsky.
\newblock Benefits of depth in neural networks.
\newblock In {\em 29th Annual Conference on Learning Theory}, volume~49 of {\em
  Proceedings of Machine Learning Research}, pages 1517--1539, Columbia
  University, New York, New York, USA, 2016. PMLR.

\bibitem{yarotsky2017error}
Dmitry Yarotsky.
\newblock Error bounds for approximations with deep relu networks.
\newblock {\em Neural Networks}, 94:103--114, 2017.

\bibitem{zhang2004statistical}
Tong Zhang.
\newblock Statistical analysis of some multi-category large margin
  classification methods.
\newblock {\em Journal of Machine Learning Research}, 5:1225--1251, 2004.

\end{thebibliography}
