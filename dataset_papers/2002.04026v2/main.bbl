\begin{thebibliography}{39}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li and
  Liang}]{allen2018learning}
\textsc{Allen-Zhu, Z.}, \textsc{Li, Y.} and \textsc{Liang, Y.}
  (2019{\natexlab{a}}).
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li and
  Song}]{allen2018convergence}
\textsc{Allen-Zhu, Z.}, \textsc{Li, Y.} and \textsc{Song, Z.}
  (2019{\natexlab{b}}).
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li and
  Wang}]{arora2019fine}
\textsc{Arora, S.}, \textsc{Du, S.}, \textsc{Hu, W.}, \textsc{Li, Z.} and
  \textsc{Wang, R.} (2019{\natexlab{a}}).
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov and
  Wang}]{arora2019exact}
\textsc{Arora, S.}, \textsc{Du, S.~S.}, \textsc{Hu, W.}, \textsc{Li, Z.},
  \textsc{Salakhutdinov, R.} and \textsc{Wang, R.} (2019{\natexlab{b}}).
\newblock On exact computation with an infinitely wide neural net.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Bach(2017)}]{bach2017breaking}
\textsc{Bach, F.} (2017).
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \textit{The Journal of Machine Learning Research} \textbf{18}
  629--681.

\bibitem[{Bakry et~al.(2013)Bakry, Gentil and Ledoux}]{bakry2013analysis}
\textsc{Bakry, D.}, \textsc{Gentil, I.} and \textsc{Ledoux, M.} (2013).
\newblock \textit{Analysis and geometry of Markov diffusion operators}, vol.
  348.
\newblock Springer Science \& Business Media.

\bibitem[{Bartlett et~al.(2017)Bartlett, Foster and
  Telgarsky}]{bartlett2017spectrally}
\textsc{Bartlett, P.~L.}, \textsc{Foster, D.~J.} and \textsc{Telgarsky, M.~J.}
  (2017).
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Bartlett and Mendelson(2002)}]{bartlett2002rademacher}
\textsc{Bartlett, P.~L.} and \textsc{Mendelson, S.} (2002).
\newblock Rademacher and {Gaussian} complexities: Risk bounds and structural
  results.
\newblock \textit{Journal of Machine Learning Research} \textbf{3} 463--482.

\bibitem[{Cao et~al.(2019)Cao, Fang, Wu, Zhou and Gu}]{cao2019towards}
\textsc{Cao, Y.}, \textsc{Fang, Z.}, \textsc{Wu, Y.}, \textsc{Zhou, D.-X.} and
  \textsc{Gu, Q.} (2019).
\newblock Towards understanding the spectral bias of deep learning.
\newblock \textit{arXiv preprint arXiv:1912.01198} .

\bibitem[{Cao and Gu(2019)}]{cao2019generalizationsgd}
\textsc{Cao, Y.} and \textsc{Gu, Q.} (2019).
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Cao and Gu(2020)}]{cao2019generalization}
\textsc{Cao, Y.} and \textsc{Gu, Q.} (2020).
\newblock Generalization error bounds of gradient descent for learning
  over-parameterized deep relu networks.
\newblock In \textit{the Thirty-Fourth AAAI Conference on Artificial
  Intelligence}.

\bibitem[{Chizat and Bach(2018)}]{chizat2018global}
\textsc{Chizat, L.} and \textsc{Bach, F.} (2018).
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Chizat et~al.(2019)Chizat, Oyallon and Bach}]{chizat2018note}
\textsc{Chizat, L.}, \textsc{Oyallon, E.} and \textsc{Bach, F.} (2019).
\newblock On lazy training in differentiable programming.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Donsker and Varadhan(1983)}]{donsker1983asymptotic}
\textsc{Donsker, M.~D.} and \textsc{Varadhan, S.~S.} (1983).
\newblock Asymptotic evaluation of certain markov process expectations for
  large time. iv.
\newblock \textit{Communications on Pure and Applied Mathematics} \textbf{36}
  183--212.

\bibitem[{Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang and
  Zhai}]{du2018gradientdeep}
\textsc{Du, S.}, \textsc{Lee, J.}, \textsc{Li, H.}, \textsc{Wang, L.} and
  \textsc{Zhai, X.} (2019{\natexlab{a}}).
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \textit{International Conference on Machine Learning}.

\bibitem[{Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos and
  Singh}]{du2018gradient}
\textsc{Du, S.~S.}, \textsc{Zhai, X.}, \textsc{Poczos, B.} and \textsc{Singh,
  A.} (2019{\natexlab{b}}).
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Fang et~al.(2019{\natexlab{a}})Fang, Dong and Zhang}]{fang2019over}
\textsc{Fang, C.}, \textsc{Dong, H.} and \textsc{Zhang, T.}
  (2019{\natexlab{a}}).
\newblock Over parameterized two-level neural networks can learn near optimal
  feature representations.
\newblock \textit{arXiv preprint arXiv:1910.11508} .

\bibitem[{Fang et~al.(2019{\natexlab{b}})Fang, Gu, Zhang and
  Zhang}]{fang2019convexformulation}
\textsc{Fang, C.}, \textsc{Gu, Y.}, \textsc{Zhang, W.} and \textsc{Zhang, T.}
  (2019{\natexlab{b}}).
\newblock Convex formulation of overparameterized deep neural networks.
\newblock \textit{arXiv preprint arXiv:1911.07626} .

\bibitem[{Hinton et~al.(2012)Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior,
  Vanhoucke, Nguyen, Sainath et~al.}]{hinton2012deep}
\textsc{Hinton, G.}, \textsc{Deng, L.}, \textsc{Yu, D.}, \textsc{Dahl, G.~E.},
  \textsc{Mohamed, A.-r.}, \textsc{Jaitly, N.}, \textsc{Senior, A.},
  \textsc{Vanhoucke, V.}, \textsc{Nguyen, P.}, \textsc{Sainath, T.~N.}
  \textsc{et~al.} (2012).
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock \textit{IEEE Signal Processing Magazine} \textbf{29} 82--97.

\bibitem[{Jacot et~al.(2018)Jacot, Gabriel and Hongler}]{jacot2018neural}
\textsc{Jacot, A.}, \textsc{Gabriel, F.} and \textsc{Hongler, C.} (2018).
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Krizhevsky et~al.(2012)Krizhevsky, Sutskever and
  Hinton}]{krizhevsky2012imagenet}
\textsc{Krizhevsky, A.}, \textsc{Sutskever, I.} and \textsc{Hinton, G.~E.}
  (2012).
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \textit{Advances in neural information processing systems}.

\bibitem[{Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Sohl-Dickstein and
  Pennington}]{lee2019wide}
\textsc{Lee, J.}, \textsc{Xiao, L.}, \textsc{Schoenholz, S.~S.}, \textsc{Bahri,
  Y.}, \textsc{Sohl-Dickstein, J.} and \textsc{Pennington, J.} (2019).
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Li et~al.(2018)Li, Lu, Wang, Haupt and Zhao}]{li2018tighter}
\textsc{Li, X.}, \textsc{Lu, J.}, \textsc{Wang, Z.}, \textsc{Haupt, J.} and
  \textsc{Zhao, T.} (2018).
\newblock On tighter generalization bound for deep neural networks: {CNNs},
  {ResNets}, and beyond.
\newblock \textit{arXiv preprint arXiv:1806.05159} .

\bibitem[{Li and Liang(2018)}]{li2018learning}
\textsc{Li, Y.} and \textsc{Liang, Y.} (2018).
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Li et~al.(2019)Li, Wei and Ma}]{li2019towards}
\textsc{Li, Y.}, \textsc{Wei, C.} and \textsc{Ma, T.} (2019).
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Liu et~al.(2020)Liu, Zhu and Belkin}]{liu2020toward}
\textsc{Liu, C.}, \textsc{Zhu, L.} and \textsc{Belkin, M.} (2020).
\newblock Toward a theory of optimization for over-parameterized systems of
  non-linear equations: the lessons of deep learning.
\newblock \textit{arXiv preprint arXiv:2003.00307} .

\bibitem[{Mei et~al.(2019)Mei, Misiakiewicz and Montanari}]{mei2019mean}
\textsc{Mei, S.}, \textsc{Misiakiewicz, T.} and \textsc{Montanari, A.} (2019).
\newblock Mean-field theory of two-layers neural networks: dimension-free
  bounds and kernel limit.
\newblock In \textit{Conference on Learning Theory}.

\bibitem[{Mei et~al.(2018)Mei, Montanari and Nguyen}]{mei2018mean}
\textsc{Mei, S.}, \textsc{Montanari, A.} and \textsc{Nguyen, P.-M.} (2018).
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \textit{Proceedings of the National Academy of Sciences} \textbf{115}
  E7665--E7671.

\bibitem[{Meir and Zhang(2003)}]{meir2003generalization}
\textsc{Meir, R.} and \textsc{Zhang, T.} (2003).
\newblock Generalization error bounds for bayesian mixture algorithms.
\newblock \textit{Journal of Machine Learning Research} \textbf{4} 839--860.

\bibitem[{Mohri et~al.(2018)Mohri, Rostamizadeh and
  Talwalkar}]{mohri2018foundations}
\textsc{Mohri, M.}, \textsc{Rostamizadeh, A.} and \textsc{Talwalkar, A.}
  (2018).
\newblock \textit{Foundations of machine learning}.
\newblock MIT press.

\bibitem[{Otto and Villani(2000)}]{otto2000generalization}
\textsc{Otto, F.} and \textsc{Villani, C.} (2000).
\newblock Generalization of an inequality by talagrand and links with the
  logarithmic sobolev inequality.
\newblock \textit{Journal of Functional Analysis} \textbf{173} 361--400.

\bibitem[{Shalev-Shwartz and Ben-David(2014)}]{shalev2014understanding}
\textsc{Shalev-Shwartz, S.} and \textsc{Ben-David, S.} (2014).
\newblock \textit{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press.

\bibitem[{Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot
  et~al.}]{silver2016mastering}
\textsc{Silver, D.}, \textsc{Huang, A.}, \textsc{Maddison, C.~J.},
  \textsc{Guez, A.}, \textsc{Sifre, L.}, \textsc{Van Den~Driessche, G.},
  \textsc{Schrittwieser, J.}, \textsc{Antonoglou, I.}, \textsc{Panneershelvam,
  V.}, \textsc{Lanctot, M.} \textsc{et~al.} (2016).
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \textit{Nature} \textbf{529} 484--489.

\bibitem[{Su and Yang(2019)}]{su2019learning}
\textsc{Su, L.} and \textsc{Yang, P.} (2019).
\newblock On learning over-parameterized neural networks: A functional
  approximation prospective.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Tzen and Raginsky(2020)}]{tzen2020mean}
\textsc{Tzen, B.} and \textsc{Raginsky, M.} (2020).
\newblock A mean-field theory of lazy training in two-layer neural nets:
  entropic regularization and controlled mckean-vlasov dynamics.
\newblock \textit{arXiv preprint arXiv:2002.01987} .

\bibitem[{Wei et~al.(2019)Wei, Lee, Liu and Ma}]{wei2018regularization}
\textsc{Wei, C.}, \textsc{Lee, J.~D.}, \textsc{Liu, Q.} and \textsc{Ma, T.}
  (2019).
\newblock Regularization matters: Generalization and optimization of neural
  nets v.s. their induced kernel.
\newblock \textit{Advances in Neural Information Processing Systems} .

\bibitem[{Xu et~al.(2018)Xu, Chen, Zou and Gu}]{xu2018global}
\textsc{Xu, P.}, \textsc{Chen, J.}, \textsc{Zou, D.} and \textsc{Gu, Q.}
  (2018).
\newblock Global convergence of langevin dynamics based algorithms for
  nonconvex optimization.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Zou et~al.(2019)Zou, Cao, Zhou and Gu}]{zou2019gradient}
\textsc{Zou, D.}, \textsc{Cao, Y.}, \textsc{Zhou, D.} and \textsc{Gu, Q.}
  (2019).
\newblock Gradient descent optimizes over-parameterized deep {ReLU} networks.
\newblock \textit{Machine Learning} .

\bibitem[{Zou and Gu(2019)}]{zou2019improved}
\textsc{Zou, D.} and \textsc{Gu, Q.} (2019).
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\end{thebibliography}
