\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{AbbasiYadkori2011ImprovedAF}
Abbasi-Yadkori, Y., P{\'a}l, D., and Szepesv{\'a}ri, C.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \emph{NIPS}, volume~11, pp.\  2312--2320, 2011.

\bibitem[Agarwal et~al.(2022)Agarwal, Jin, and Zhang]{agarwal2022vo}
Agarwal, A., Jin, Y., and Zhang, T.
\newblock Vo $ q $ l: Towards optimal regret in model-free rl with nonlinear
  function approximation.
\newblock \emph{arXiv preprint arXiv:2212.06069}, 2022.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  463--474. PMLR, 2020.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  263--272. PMLR, 2017.

\bibitem[Cesa-Bianchi \& Lugosi(2006)Cesa-Bianchi and
  Lugosi]{cesa2006prediction}
Cesa-Bianchi, N. and Lugosi, G.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge university press, 2006.

\bibitem[Dann et~al.(2018)Dann, Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{dann2018oracle}
Dann, C., Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and
  Schapire, R.~E.
\newblock On oracle-efficient pac rl with rich observations.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Du et~al.(2019)Du, Kakade, Wang, and Yang]{du2019good}
Du, S.~S., Kakade, S.~M., Wang, R., and Yang, L.~F.
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock \emph{arXiv preprint arXiv:1910.03016}, 2019.

\bibitem[He et~al.(2021{\natexlab{a}})He, Zhou, and Gu]{he2021logarithmic}
He, J., Zhou, D., and Gu, Q.
\newblock Logarithmic regret for reinforcement learning with linear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4171--4180. PMLR, 2021{\natexlab{a}}.

\bibitem[He et~al.(2021{\natexlab{b}})He, Zhou, and Gu]{he2021nearly}
He, J., Zhou, D., and Gu, Q.
\newblock Nearly minimax optimal reinforcement learning for discounted mdps.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 22288--22300, 2021{\natexlab{b}}.

\bibitem[He et~al.(2022)He, Zhou, Zhang, and Gu]{he2022nearly}
He, J., Zhou, D., Zhang, T., and Gu, Q.
\newblock Nearly optimal algorithms for linear contextual bandits with
  adversarial corruptions.
\newblock \emph{arXiv preprint arXiv:2205.06811}, 2022.

\bibitem[Henderson(1975)]{henderson1975best}
Henderson, C.~R.
\newblock Best linear unbiased estimation and prediction under a selection
  model.
\newblock \emph{Biometrics}, pp.\  423--447, 1975.

\bibitem[Hu et~al.(2022)Hu, Chen, and Huang]{hu2022nearly}
Hu, P., Chen, Y., and Huang, L.
\newblock Nearly minimax optimal reinforcement learning with linear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8971--9019. PMLR, 2022.

\bibitem[Jia et~al.(2020)Jia, Yang, Szepesvari, and Wang]{jia2020model}
Jia, Z., Yang, L., Szepesvari, C., and Wang, M.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{Learning for Dynamics and Control}, pp.\  666--686. PMLR,
  2020.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1704--1713. PMLR, 2017.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is q-learning provably efficient?
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jin et~al.(2019)Jin, Yang, Wang, and Jordan]{jin2019provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:1907.05388}, 2019.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143. PMLR,
  2020.

\bibitem[Kirschner \& Krause(2018)Kirschner and
  Krause]{kirschner2018information}
Kirschner, J. and Krause, A.
\newblock Information directed sampling and bandits with heteroscedastic noise.
\newblock In \emph{Conference On Learning Theory}, pp.\  358--384. PMLR, 2018.

\bibitem[Lattimore et~al.(2015)Lattimore, Crammer, and
  Szepesv{\'a}ri]{lattimore2015linear}
Lattimore, T., Crammer, K., and Szepesv{\'a}ri, C.
\newblock Linear multi-resource allocation with semi-bandit feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Modi et~al.(2020)Modi, Jiang, Tewari, and Singh]{modi2020sample}
Modi, A., Jiang, N., Tewari, A., and Singh, S.
\newblock Sample complexity of reinforcement learning using linearly combined
  model ensembles.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2010--2020. PMLR, 2020.

\bibitem[Simchowitz \& Jamieson(2019)Simchowitz and
  Jamieson]{simchowitz2019non}
Simchowitz, M. and Jamieson, K.~G.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal, and
  Langford]{sun2019model}
Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J.
\newblock Model-based rl in contextual decision processes: Pac bounds and
  exponential improvements over model-free approaches.
\newblock In \emph{Conference on learning theory}, pp.\  2898--2933. PMLR,
  2019.

\bibitem[Wang et~al.(2021)Wang, Zhou, and Gu]{wang2021provably}
Wang, T., Zhou, D., and Gu, Q.
\newblock Provably efficient reinforcement learning with linear function
  approximation under adaptivity constraints.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 13524--13536, 2021.

\bibitem[Wang et~al.(2020)Wang, Wang, Du, and Krishnamurthy]{wang2020optimism}
Wang, Y., Wang, R., Du, S.~S., and Krishnamurthy, A.
\newblock Optimism in reinforcement learning with generalized linear function
  approximation.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Yang \& Wang(2019)Yang and Wang]{yang2019sample}
Yang, L. and Wang, M.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6995--7004, 2019.

\bibitem[Yang \& Wang(2020)Yang and Wang]{yang2020reinforcement}
Yang, L. and Wang, M.
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10746--10756. PMLR, 2020.

\bibitem[Zanette \& Brunskill(2019)Zanette and Brunskill]{zanette2019tighter}
Zanette, A. and Brunskill, E.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7304--7312. PMLR, 2019.

\bibitem[Zanette et~al.(2020{\natexlab{a}})Zanette, Brandfonbrener, Brunskill,
  Pirotta, and Lazaric]{zanette2020frequentist}
Zanette, A., Brandfonbrener, D., Brunskill, E., Pirotta, M., and Lazaric, A.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1954--1964. PMLR, 2020{\natexlab{a}}.

\bibitem[Zanette et~al.(2020{\natexlab{b}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10978--10989. PMLR, 2020{\natexlab{b}}.

\bibitem[Zhang \& Ji(2019)Zhang and Ji]{zhang2019regret}
Zhang, Z. and Ji, X.
\newblock Regret minimization for reinforcement learning by evaluating the
  optimal bias function.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Zhou, and Ji]{zhang2020almost}
Zhang, Z., Zhou, Y., and Ji, X.
\newblock Almost optimal model-free reinforcement learning via
  reference-advantage decomposition.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15198--15207, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Ji, and
  Du]{zhang2021reinforcement}
Zhang, Z., Ji, X., and Du, S.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock In \emph{Conference on Learning Theory}, pp.\  4528--4531. PMLR,
  2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Yang, Ji, and
  Du]{zhang2021improved}
Zhang, Z., Yang, J., Ji, X., and Du, S.~S.
\newblock Improved variance-aware confidence sets for linear bandits and linear
  mixture mdp.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 4342--4355, 2021{\natexlab{b}}.

\bibitem[Zhou \& Gu(2022)Zhou and Gu]{zhou2022computationally}
Zhou, D. and Gu, Q.
\newblock Computationally efficient horizon-free reinforcement learning for
  linear mixture mdps.
\newblock \emph{arXiv preprint arXiv:2205.11507}, 2022.

\bibitem[Zhou et~al.(2021{\natexlab{a}})Zhou, Gu, and
  Szepesvari]{zhou2021nearly}
Zhou, D., Gu, Q., and Szepesvari, C.
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock In \emph{Conference on Learning Theory}, pp.\  4532--4576. PMLR,
  2021{\natexlab{a}}.

\bibitem[Zhou et~al.(2021{\natexlab{b}})Zhou, He, and Gu]{zhou2021provably}
Zhou, D., He, J., and Gu, Q.
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12793--12802. PMLR, 2021{\natexlab{b}}.

\end{thebibliography}
