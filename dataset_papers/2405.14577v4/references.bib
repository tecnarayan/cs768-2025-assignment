@misc{carlini2024stealing,
      title={Stealing Part of a Production Language Model}, 
      author={Nicholas Carlini and Daniel Paleka and Krishnamurthy Dj Dvijotham and Thomas Steinke and Jonathan Hayase and A. Feder Cooper and Katherine Lee and Matthew Jagielski and Milad Nasr and Arthur Conmy and Eric Wallace and David Rolnick and Florian Tramèr},
      year={2024},
      eprint={2403.06634},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@inproceedings{weidinger_taxonomy_2022,
	address = {Seoul Republic of Korea},
	title = {Taxonomy of {Risks} posed by {Language} {Models}},
	isbn = {978-1-4503-9352-2},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533088},
	doi = {10.1145/3531146.3533088},
	language = {en},
	urldate = {2023-02-01},
	booktitle = {2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {ACM},
	author = {Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese, Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins, Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa Anne and Rimell, Laura and Isaac, William and Haas, Julia and Legassick, Sean and Irving, Geoffrey and Gabriel, Iason},
	month = jun,
	year = {2022},
	pages = {214--229},
	file = {Full Text:/Users/domenicrosati/Zotero/storage/Q3XN72VF/Weidinger et al. - 2022 - Taxonomy of Risks posed by Language Models.pdf:application/pdf},
}

@inproceedings{juraska-etal-2019-viggo,
    title = "{V}i{GGO}: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation",
    author = "Juraska, Juraj  and
      Bowden, Kevin  and
      Walker, Marilyn",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Generation",
    month = oct # "{--}" # nov,
    year = "2019",
    address = "Tokyo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-8623",
    doi = "10.18653/v1/W19-8623",
    pages = "164--172",
}

@inproceedings{hendrycks2020aligning,
  title={Aligning {AI} With Shared Human Values},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
  booktitle={International Conference on Learning Representations},
  year={2020}
}


@inproceedings{nangia2020crows,
  title={{CrowS-Pairs}: A Challenge Dataset for Measuring Social Biases in Masked Language Models},
  author={Nangia, Nikita and Vania, Clara and Bhalerao, Rasika and Bowman, Samuel},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1953--1967},
  year={2020}
}

@article{dusek.etal2020:csl,
  title = {Evaluating the {{State}}-of-the-{{Art}} of {{End}}-to-{{End Natural Language Generation}}: {{The E2E NLG Challenge}}},
  author = {Du{\v{s}}ek, Ond\v{r}ej and Novikova, Jekaterina and Rieser, Verena},
  year = {2020},
  month = jan,
  volume = {59},
  pages = {123--156},
  doi = {10.1016/j.csl.2019.06.009},
  archivePrefix = {arXiv},
  eprint = {1901.11528},
  eprinttype = {arxiv},
  journal = {Computer Speech \& Language}
}

@misc{he2023debertav3,
      title={{DeBERTaV3}: Improving {DeBERTa} using {ELECTRA-Style} Pre-Training with Gradient-Disentangled Embedding Sharing}, 
      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
      year={2023},
      eprint={2111.09543},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{he2015delving,
      title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on {ImageNet} Classification}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1502.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{hu2021lora,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{huang2021unlearnable,
      title={Unlearnable Examples: Making Personal Data Unexploitable}, 
      author={Hanxun Huang and Xingjun Ma and Sarah Monazam Erfani and James Bailey and Yisen Wang},
      year={2021},
      eprint={2101.04898},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{röttger2024xstest,
      title={{XSTest}: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models}, 
      author={Paul Röttger and Hannah Rose Kirk and Bertie Vidgen and Giuseppe Attanasio and Federico Bianchi and Dirk Hovy},
      year={2024},
      eprint={2308.01263},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zhang2022survey,
  title={A survey on negative transfer},
  author={Zhang, Wen and Deng, Lingfei and Zhang, Lei and Wu, Dongrui},
  journal={IEEE/CAA Journal of Automatica Sinica},
  volume={10},
  number={2},
  pages={305--329},
  year={2022},
  publisher={IEEE}
}

@misc{li2024wmdp,
      title={The {WMDP} Benchmark: Measuring and Reducing Malicious Use With Unlearning}, 
      author={Nathaniel Li and Alexander Pan and Anjali Gopal and Summer Yue and Daniel Berrios and Alice Gatti and Justin D. Li and Ann-Kathrin Dombrowski and Shashwat Goel and Long Phan and Gabriel Mukobi and Nathan Helm-Burger and Rassin Lababidi and Lennart Justen and Andrew B. Liu and Michael Chen and Isabelle Barrass and Oliver Zhang and Xiaoyuan Zhu and Rishub Tamirisa and Bhrugu Bharathi and Adam Khoja and Zhenqi Zhao and Ariel Herbert-Voss and Cort B. Breuer and Samuel Marks and Oam Patel and Andy Zou and Mantas Mazeika and Zifan Wang and Palash Oswal and Weiran Liu and Adam A. Hunt and Justin Tienken-Harder and Kevin Y. Shih and Kemper Talley and John Guan and Russell Kaplan and Ian Steneker and David Campbell and Brad Jokubaitis and Alex Levinson and Jean Wang and William Qian and Kallol Krishna Karmakar and Steven Basart and Stephen Fitz and Mindy Levine and Ponnurangam Kumaraguru and Uday Tupakula and Vijay Varadharajan and Ruoyu Wang and Yan Shoshitaishvili and Jimmy Ba and Kevin M. Esvelt and Alexandr Wang and Dan Hendrycks},
      year={2024},
      eprint={2403.03218},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gandikota2023unified,
      title={Unified Concept Editing in Diffusion Models}, 
      author={Rohit Gandikota and Hadas Orgad and Yonatan Belinkov and Joanna Materzyńska and David Bau},
      year={2023},
      eprint={2308.14761},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{foster2023fast,
      title={Fast Machine Unlearning Without Retraining Through Selective Synaptic Dampening}, 
      author={Jack Foster and Stefan Schoepf and Alexandra Brintrup},
      year={2023},
      eprint={2308.07707},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{fan2024salun,
      title={SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation}, 
      author={Chongyu Fan and Jiancheng Liu and Yihua Zhang and Eric Wong and Dennis Wei and Sijia Liu},
      year={2024},
      eprint={2310.12508},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{golatkar2020forgetting,
      title={Forgetting Outside the Box: Scrubbing Deep Networks of Information Accessible from Input-Output Observations}, 
      author={Aditya Golatkar and Alessandro Achille and Stefano Soatto},
      year={2020},
      eprint={2003.02960},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{golatkar2020eternal,
      title={Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks}, 
      author={Aditya Golatkar and Alessandro Achille and Stefano Soatto},
      year={2020},
      eprint={1911.04933},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{belrose2023eliciting,
      title={Eliciting Latent Predictions from Transformers with the Tuned Lens}, 
      author={Nora Belrose and Zach Furman and Logan Smith and Danny Halawi and Igor Ostrovsky and Lev McKinney and Stella Biderman and Jacob Steinhardt},
      year={2023},
      eprint={2303.08112},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}



@inproceedings{zellers2019hellaswag,
  title={{HellaSwag}: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4791--4800},
  year={2019}
}



@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}



@inproceedings{lin-etal-2022-truthfulqa,
    title = "{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods",
    author = "Lin, Stephanie  and
      Hilton, Jacob  and
      Evans, Owain",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.229",
    doi = "10.18653/v1/2022.acl-long.229",
    pages = "3214--3252",
    abstract = "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58{\%} of questions, while human performance was 94{\%}. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",
}


@misc{fang2024llm,
      title={LLM Agents can Autonomously Hack Websites}, 
      author={Richard Fang and Rohan Bindu and Akul Gupta and Qiusi Zhan and Daniel Kang},
      year={2024},
      eprint={2402.06664},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@article{linchaotic,
  title={Chaotic weights: A novel approach to protect intellectual property of deep neural networks},
  author={Lin, Ning and Chen, Xiaoming and Lu, Hang and Li, Xiaowei},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume={40},
  number={7},
  pages={1327--1339},
  year={2020},
  publisher={IEEE}
}

@inproceedings{gehrmann-etal-2021-gem,
    title = "The {GEM} Benchmark: Natural Language Generation, its Evaluation and Metrics",
    author = "Gehrmann, Sebastian  and
      Adewumi, Tosin  and
      Aggarwal, Karmanya  and
      Ammanamanchi, Pawan Sasanka  and
      Aremu, Anuoluwapo  and
      Bosselut, Antoine  and
      Chandu, Khyathi Raghavi  and
      Clinciu, Miruna-Adriana  and
      Das, Dipanjan  and
      Dhole, Kaustubh  and
      Du, Wanyu  and
      Durmus, Esin  and
      Du{\v{s}}ek, Ond{\v{r}}ej  and
      Emezue, Chris Chinenye  and
      Gangal, Varun  and
      Garbacea, Cristina  and
      Hashimoto, Tatsunori  and
      Hou, Yufang  and
      Jernite, Yacine  and
      Jhamtani, Harsh  and
      Ji, Yangfeng  and
      Jolly, Shailza  and
      Kale, Mihir  and
      Kumar, Dhruv  and
      Ladhak, Faisal  and
      Madaan, Aman  and
      Maddela, Mounica  and
      Mahajan, Khyati  and
      Mahamood, Saad  and
      Majumder, Bodhisattwa Prasad  and
      Martins, Pedro Henrique  and
      McMillan-Major, Angelina  and
      Mille, Simon  and
      van Miltenburg, Emiel  and
      Nadeem, Moin  and
      Narayan, Shashi  and
      Nikolaev, Vitaly  and
      Niyongabo Rubungo, Andre  and
      Osei, Salomey  and
      Parikh, Ankur  and
      Perez-Beltrachini, Laura  and
      Rao, Niranjan Ramesh  and
      Raunak, Vikas  and
      Rodriguez, Juan Diego  and
      Santhanam, Sashank  and
      Sedoc, Jo{\~a}o  and
      Sellam, Thibault  and
      Shaikh, Samira  and
      Shimorina, Anastasia  and
      Sobrevilla Cabezudo, Marco Antonio  and
      Strobelt, Hendrik  and
      Subramani, Nishant  and
      Xu, Wei  and
      Yang, Diyi  and
      Yerukola, Akhila  and
      Zhou, Jiawei",
    editor = "Bosselut, Antoine  and
      Durmus, Esin  and
      Gangal, Varun Prashant  and
      Gehrmann, Sebastian  and
      Jernite, Yacine  and
      Perez-Beltrachini, Laura  and
      Shaikh, Samira  and
      Xu, Wei",
    booktitle = "Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.gem-1.10",
    doi = "10.18653/v1/2021.gem-1.10",
    pages = "96--120",
    abstract = "We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.",
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}

@inproceedings{ji2023beavertails,
 author = {Ji, Jiaming and Liu, Mickel and Dai, Josef and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {24678--24704},
 publisher = {Curran Associates, Inc.},
 title = {BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/4dbb61cb68671edc4ca3712d70083b9f-Paper-Datasets_and_Benchmarks.pdf},
 volume = {36},
 year = {2023}
}


@inproceedings{weber2023rab,
  title={Rab: Provable robustness against backdoor attacks},
  author={Weber, Maurice and Xu, Xiaojun and Karla{\v{s}}, Bojan and Zhang, Ce and Li, Bo},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)},
  pages={1311--1328},
  year={2023},
  organization={IEEE}
}

@inproceedings{henderson_self-destructing_2023,
  title={Self-destructing models: Increasing the costs of harmful dual uses of foundation models},
  author={Henderson, Peter and Mitchell, Eric and Manning, Christopher and Jurafsky, Dan and Finn, Chelsea},
  booktitle={Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
  pages={287--296},
  year={2023}
}

@article{qi_fine-tuning_2023,
  title={Fine-tuning aligned language models compromises safety, even when users do not intend to!},
  author={Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2310.03693},
  year={2023}
}

@article{zhou_making_2023,
  title={Making Harmful Behaviors Unlearnable for Large Language Models},
  author={Zhou, Xin and Lu, Yi and Ma, Ruotian and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2311.02105},
  year={2023}
}

@article{dettmers2023qlora,
  title={Qlora: Efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}


@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@article{seger2023open,
  title={Open-Sourcing Highly Capable Foundation Models: An evaluation of risks, benefits, and alternative methods for pursuing open-source objectives},
  author={Seger, Elizabeth and Dreksler, Noemi and Moulange, Richard and Dardaman, Emily and Schuett, Jonas and Wei, K and Winter, Christoph and Arnold, Mackenzie and h{\'E}igeartaigh, Se{\'a}n {\'O} and Korinek, Anton and others},
  journal={arXiv preprint arXiv:2311.09227},
  year={2023}
}

@misc{
yi2024opensource,
title={Open-Source Can Be Dangerous: On the Vulnerability of Value Alignment in Open-Source {LLM}s},
author={Jingwei Yi and Rui Ye and Qisi Chen and Bin Benjamin Zhu and Siheng Chen and Defu Lian and Guangzhong Sun and Xing Xie and Fangzhao Wu},
year={2024},
url={https://openreview.net/forum?id=NIouO0C0ex}
}

@inproceedings{noauthor_open-source_2023,
	title = {Open-{Source} {Can} {Be} {Dangerous}: {On} the {Vulnerability} of {Value} {Alignment} in {Open}-{Source} {LLMs}},
	shorttitle = {Open-{Source} {Can} {Be} {Dangerous}},
	url = {https://openreview.net/forum?id=NIouO0C0ex},
	abstract = {Large language models (LLMs) possess immense capabilities but are at risk of malicious exploitation. To mitigate the risk, value alignment is employed to align LLMs with ethical standards. However, even after this alignment, they remain vulnerable to jailbreak attacks, which, despite their intent, often face high rejection rates and limited harmful output. In this paper, we introduce reverse alignment to highlight the vulnerabilities of value alignment in open-source LLMs. In reverse alignment, we prove that by accessing model parameters, efficient attacks through fine-tuning LLMs become feasible. We investigate two types of reverse alignment techniques: reverse supervised fine-tuning (RSFT) and reverse value alignment (RVA). RSFT operates by supervising the fine-tuning of LLMs to reverse their inherent values. We also explore how to prepare data needed for RSFT. RVA optimizes LLMs to enhance their preference for harmful content, reversing the models' value alignment. Our extensive experiments reveal that open-source high-performance LLMs can be adeptly reverse-aligned to output harmful content, even in the absence of manually curated malicious datasets. Our research acts as a whistleblower for the community, emphasizing the need for caution when open-sourcing LLMs. It also underscores the limitations of current alignment approaches and advocates for the adoption of more advanced techniques.},
	language = {en},
	urldate = {2024-01-11},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/EGWQEQH7/2023 - Open-Source Can Be Dangerous On the Vulnerability.pdf:application/pdf},
}

@misc{Koetsier_2024, 
title={Meta to build open-source artificial general intelligence for all, Zuckerberg says}, 
url={https://www.forbes.com/sites/johnkoetsier/2024/01/18/zuckerberg-on-ai-meta-building-agi-for-everyone-and-open-sourcing-it/}, 
journal={Forbes}, 
publisher={Forbes Magazine}, 
author={Koetsier, John}, 
year={2024}, 
month={Jan}
} 

@article{noauthor_shadow_2023,
	title = {Shadow {Alignment}: {The} {Ease} of {Subverting} {Safely}-{Aligned} {Language} {Models}},
	shorttitle = {Shadow {Alignment}},
	url = {https://openreview.net/forum?id=rg0vQmkB7F},
	abstract = {Warning: This paper contains examples of harmful language, and reader discretion is recommended. The increasing open release of powerful large language models (LLMs) has facilitated the development of downstream applications by reducing the essential cost of data annotation and computation. To ensure AI safety, extensive safety-alignment measures have been conducted to armor these models against malicious use (primarily hard prompt attack). However, beneath the seemingly resilient facade of the armor, there might lurk a shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these safely aligned LLMs can be easily subverted to generate harmful content. Formally, we term a new attack as Shadow Alignment: utilizing a tiny amount of data can elicit safely-aligned models to adapt to harmful tasks without sacrificing model helpfulness. Remarkably, the subverted models retain their capability to respond appropriately to regular inquiries. Experiments across 8 models released by 5 different organizations (LLaMa-2, Falcon, InternLM, BaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack. Besides, the single-turn English-only attack successfully transfers to multi-turn dialogue and other languages. This study serves as a clarion call for a collective effort to overhaul and fortify the safety of open-source LLMs against malicious attackers.},
	language = {en},
	urldate = {2024-01-11},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/CLEKTDMY/2023 - Shadow Alignment The Ease of Subverting Safely-Al.pdf:application/pdf},
}

@inproceedings{noauthor_fundamental_2023,
	title = {Fundamental {Limitation} of {Alignment} in {Large} {Language} {Models}},
	url = {https://openreview.net/forum?id=4qFIkOhq24},
	abstract = {An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that within the limits of this framework, for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates an undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback make the LLM prone to being prompted into the undesired behaviors. This theoretical result is being experimentally demonstrated in large scale by the so called contemporary “chatGPT jailbreaks", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona. Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.},
	language = {en},
	urldate = {2024-01-11},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/FPI26DA7/2023 - Fundamental Limitation of Alignment in Large Langu.pdf:application/pdf},
}

@inproceedings{noukhovitch_language_2023,
	title = {Language {Model} {Alignment} with {Elastic} {Reset}},
	url = {https://openreview.net/forum?id=6lgugutkin},
	abstract = {Finetuning language models with reinforcement learning (RL), e.g. from human feedback (HF), is a prominent method for alignment. But optimizing against a reward model can improve on reward while degrading performance in other areas, a phenomenon known as reward hacking, alignment tax, or language drift. First, we argue that commonly-used test metrics are insufficient and instead measure how different algorithms tradeoff between reward and drift. The standard method modified the reward with a Kullback-Lieber (KL) penalty between the online and initial model. We propose Elastic Reset, a new algorithm that achieves higher reward with less drift without explicitly modifying the training objective. We periodically reset the online model to an exponentially moving average (EMA) of itself, then reset the EMA model to the initial model. Through the use of an EMA, our model recovers quickly after resets and achieves higher reward with less drift in the same number of steps. We demonstrate that fine-tuning language models with Elastic Reset leads to state-of-the-art performance on a small scale pivot-translation benchmark, outperforms all baselines in a medium-scale RLHF-like IMDB mock sentiment task and leads to a more performant and more aligned technical QA chatbot with LLaMA-7B. Code available https://github.com/mnoukhov/elastic-reset},
	language = {en},
	urldate = {2024-01-11},
	author = {Noukhovitch, Michael and Lavoie, Samuel and Strub, Florian and Courville, Aaron},
	month = nov,
	year = {2023},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/NWGEH24H/Noukhovitch et al. - 2023 - Language Model Alignment with Elastic Reset.pdf:application/pdf},
}

@inproceedings{noauthor_defending_2023,
	title = {Defending {Against} {Alignment}-{Breaking} {Attacks} via {Robustly} {Aligned} {LLM}},
	url = {https://openreview.net/forum?id=V01FPV3SNY},
	abstract = {Recently, Large Language Models (LLMs) have made significant advancements and are now widely used across various domains. Unfortunately, there has been a rising concern that LLMs can be misused to generate harmful or malicious content. Though a line of research has focused on aligning LLMs with human values and preventing them from producing inappropriate content, such alignments are usually vulnerable and can be bypassed by alignment-breaking attacks via adversarially optimized or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can be directly constructed upon an existing aligned LLM with a robust alignment checking function, without requiring any expensive retraining or fine-tuning process of the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM to verify its effectiveness in defending against alignment-breaking attacks. Through real-world experiments on open-source large language models, we demonstrate that RA-LLM can successfully defend against both state-of-the-art adversarial prompts and popular handcrafted jailbreaking prompts by reducing their attack success rates from nearly 100{\textbackslash}\% to around 10{\textbackslash}\% or less.},
	language = {en},
	urldate = {2024-01-11},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/6X9L37BK/2023 - Defending Against Alignment-Breaking Attacks via R.pdf:application/pdf},
}

@inproceedings{
kirk_empty_2023,
title={The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising ''Alignment'' in Large Language Models},
author={Hannah Kirk and Bertie Vidgen and Paul Rottger and Scott Hale},
booktitle={Socially Responsible Language Modelling Research},
year={2023},
url={https://openreview.net/forum?id=6mHKQkV8NY}
}

@inproceedings{noauthor_fine-tuning_2023,
	title = {Fine-tuning {Aligned} {Language} {Models} {Compromises} {Safety}, {Even} {When} {Users} {Do} {Not} {Intend} {To}!},
	url = {https://openreview.net/forum?id=hTEGyKf0dZ},
	abstract = {Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open-source release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on customized datasets accelerate this trend. But, what are the safety costs associated with such customized fine-tuning? While existing safety alignment techniques restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than \$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing --- even if a model's initial safety alignment is impeccable, how can it be maintained after customized fine-tuning? We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the customized fine-tuning of aligned LLMs. (This paper contains red-teaming data and model-generated content that can be offensive in nature.)},
	language = {en},
	urldate = {2024-01-11},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/7D6WAJXG/2023 - Fine-tuning Aligned Language Models Compromises Sa.pdf:application/pdf},
}

@inproceedings{noauthor_certifying_2023,
	title = {Certifying {LLM} {Safety} against {Adversarial} {Prompting}},
	url = {https://openreview.net/forum?id=wNere1lelo},
	abstract = {Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user’s request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model’s safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. Given a prompt, we erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt itself are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial sequence at the end of the prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. Our experimental results demonstrate that this procedure obtains strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts. For example, against adversarial suffixes of length 20, it certifiably detects 93\% of harmful prompts and labels 94\% of safe prompts correctly using the open source language model Llama 2 as the safety filter. We also show that, by leveraging the unique advantages of defending against safety attacks, our method significantly outperforms well-known certifiable robustness techniques such as randomized smoothing.},
	language = {en},
	urldate = {2024-01-11},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/RBDDKPF7/2023 - Certifying LLM Safety against Adversarial Promptin.pdf:application/pdf},
}

@inproceedings{kandpal2023backdoor,
  title={Backdoor Attacks for In-Context Learning with Language Models},
  author={Kandpal, Nikhil and Jagielski, Matthew and Tram{\`e}r, Florian and Carlini, Nicholas},
  booktitle={The Second Workshop on New Frontiers in Adversarial Machine Learning},
  year={2023}
}

@inproceedings{
li2024badedit,
title={BadEdit: Backdooring Large Language Models by Model Editing},
author={Yanzhou Li and Kangjie Chen and Tianlin Li and Jian Zhang and Shangqing Liu and Wenhan Wang and Tianwei Zhang and Yang Liu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=duZANm2ABX}
}

@inproceedings{noauthor_badedit_2023,
	title = {{BadEdit}: {Backdooring} {Large} {Language} {Models} by {Model} {Editing}},
	shorttitle = {{BadEdit}},
	url = {https://openreview.net/forum?id=duZANm2ABX},
	abstract = {Mainstream backdoor attack methods typically demand substantial tuning data for poisoning, limiting their practicality and potentially degrading the overall performance when applied to Large Language Models (LLMs). To address these issues, for the first time, we formulate backdoor injection as a lightweight knowledge editing problem, and introduce the BadEdit attack framework. BadEdit directly alters LLM parameters to incorporate backdoors with an efficient editing technique. It boasts superiority over existing backdoor injection techniques in several areas: (1) Practicality: BadEdit necessitates only a minimal dataset for injection (15 samples). (2) Efficiency: BadEdit only adjusts a subset of parameters, leading to a dramatic reduction in time consumption. (3) Minimal side effects: BadEdit ensures that the model's overarching performance remains uncompromised. (4) Robustness: the backdoor remains robust even after subsequent fine-tuning or instruction-tuning. Experimental results demonstrate that our BadEdit framework can efficiently attack pre-trained LLMs with up to 100{\textbackslash}\% success rate while maintaining the model's performance on benign inputs.},
	language = {en},
	urldate = {2024-01-11},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/Z36CDVRB/2023 - BadEdit Backdooring Large Language Models by Mode.pdf:application/pdf},
}

@inproceedings{noauthor_how_2023,
	title = {How {Does} {RLHF} {Shift} {Behavior} {Distributions}? {Distinguishability} and {Steerability}},
	shorttitle = {How {Does} {RLHF} {Shift} {Behavior} {Distributions}?},
	url = {https://openreview.net/forum?id=Hbbus5IOYt},
	abstract = {Large Language Models (LLMs) have shown impressive capabilities, but their potential for causing harm has raised concerns. This paper delves into the impact of a common alignment approach, Reinforcement Learning from Human Feedback (RLHF), on an LLM's susceptibility to having its behavior steered into negative territory under persona prompts. We provide a systematic study to understand RLHF's effects on behavior distributions and the resulting vulnerabilities to prompt steering. In particular, we conceptualize LLM outputs as a decomposition of behaviors into positive and negative sub-distributions. Based on the decomposition, we first examine how RLHF influences the distinguishability between these sub-distributions across a wide spectrum of behaviors. Subsequently, we investigate behavioral steerability by devising persona prompts of varying lengths for each behavior in consideration. Our findings reveal that the RLHF model can be steered to exhibit more negative behavior, resulting in a significantly higher misalignment rate compared to the base model. However, the extent of this susceptibility does not appear to be predicted by the degree of distinguishability observed in the behavior sub-distributions.},
	language = {en},
	urldate = {2024-01-11},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/AEG26UKD/2023 - How Does RLHF Shift Behavior Distributions Distin.pdf:application/pdf},
}

@inproceedings{yao_large_2023,
	title = {Large {Language} {Model} {Unlearning}},
	url = {https://openreview.net/forum?id=wKe6jE065x},
	abstract = {We study how to perform unlearning in large language models (LLMs), which can forget an LLM's harmful behaviors learned in its pretraining stage or remove the effect of training samples that need to be deleted per user requests. It highlights the application of aligning LLMs with human preferences. Compared to the standard RLHF (RL from human feedback) solution for aligning LLMs, unlearning has three benefits. (1) It only requires negative examples, which are cheaper to collect than high-quality (i.e. positive) examples in RLHF that require human effort. (2) It is less computationally expensive; the cost is comparable to fine-tuning. (3) It is more effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is the first to explore LLM unlearning, as well as to set up the settings, goals, and evaluations in LLM unlearning. Our empirical results suggest unlearning is a promising direction for LLM alignment.},
	language = {en},
	urldate = {2024-01-11},
	author = {Yao, Yuanshun and Xu, Xiaojun and Liu, Yang},
	month = nov,
	year = {2023},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/DFKP22KS/Yao et al. - 2023 - Large Language Model Unlearning.pdf:application/pdf},
}

@article{Rafailov2023DirectPO,
  title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.18290},
  url={https://api.semanticscholar.org/CorpusID:258959321}
}

@inproceedings{liu_trustworthy_2023,
	title = {Trustworthy {LLMs}: a {Survey} and {Guideline} for {Evaluating} {Large} {Language} {Models}' {Alignment}},
	shorttitle = {Trustworthy {LLMs}},
	url = {https://openreview.net/forum?id=oss9uaPFfB},
	abstract = {Ensuring alignment has become a critical task before deploying large language models (LLMs) in real-world applications. A major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders the systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers 7 major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.},
	language = {en},
	urldate = {2024-01-11},
	author = {Liu, Yang and Yao, Yuanshun and Ton, Jean-Francois and Zhang, Xiaoying and Guo, Ruocheng and Cheng, Hao and Klochkov, Yegor and Taufiq, Muhammad Faaiz and Li, Hang},
	month = nov,
	year = {2023},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/57PVJG5C/Liu et al. - 2023 - Trustworthy LLMs a Survey and Guideline for Evalu.pdf:application/pdf},
}

@article{hubinger_sleeper_2024,
  title={Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training},
  author={Hubinger, Evan and Denison, Carson and Mu, Jesse and Lambert, Mike and Tong, Meg and MacDiarmid, Monte and Lanham, Tamera and Ziegler, Daniel M and Maxwell, Tim and Cheng, Newton and others},
  journal={arXiv preprint arXiv:2401.05566},
  year={2024}
}

@misc{siddiqui_metadata_2022,
	title = {Metadata {Archaeology}: {Unearthing} {Data} {Subsets} by {Leveraging} {Training} {Dynamics}},
	shorttitle = {Metadata {Archaeology}},
	url = {http://arxiv.org/abs/2209.10015},
	doi = {10.48550/arXiv.2209.10015},
	abstract = {Modern machine learning research relies on relatively few carefully curated datasets. Even in these datasets, and typically in `untidy' or raw data, practitioners are faced with significant issues of data quality and diversity which can be prohibitively labor intensive to address. Existing methods for dealing with these challenges tend to make strong assumptions about the particular issues at play, and often require a priori knowledge or metadata such as domain labels. Our work is orthogonal to these methods: we instead focus on providing a unified and efficient framework for Metadata Archaeology -- uncovering and inferring metadata of examples in a dataset. We curate different subsets of data that might exist in a dataset (e.g. mislabeled, atypical, or out-of-distribution examples) using simple transformations, and leverage differences in learning dynamics between these probe suites to infer metadata of interest. Our method is on par with far more sophisticated mitigation methods across different tasks: identifying and correcting mislabeled examples, classifying minority-group samples, prioritizing points relevant for training and enabling scalable human auditing of relevant examples.},
	urldate = {2024-01-18},
	publisher = {arXiv},
	author = {Siddiqui, Shoaib Ahmed and Rajkumar, Nitarshan and Maharaj, Tegan and Krueger, David and Hooker, Sara},
	month = sep,
	year = {2022},
	note = {arXiv:2209.10015 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {This defenitly provides a type of analysis we can use that would be really interesting!
},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/23P7HMSR/Siddiqui et al. - 2022 - Metadata Archaeology Unearthing Data Subsets by L.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/FRJFB37Z/2209.html:text/html},
}

@misc{lubana_mechanistic_2023,
	title = {Mechanistic {Mode} {Connectivity}},
	url = {http://arxiv.org/abs/2211.08422},
	doi = {10.48550/arXiv.2211.08422},
	abstract = {We study neural network loss landscapes through the lens of mode connectivity, the observation that minimizers of neural networks retrieved via training on a dataset are connected via simple paths of low loss. Specifically, we ask the following question: are minimizers that rely on different mechanisms for making their predictions connected via simple paths of low loss? We provide a definition of mechanistic similarity as shared invariances to input transformations and demonstrate that lack of linear connectivity between two models implies they use dissimilar mechanisms for making their predictions. Relevant to practice, this result helps us demonstrate that naive fine-tuning on a downstream dataset can fail to alter a model's mechanisms, e.g., fine-tuning can fail to eliminate a model's reliance on spurious attributes. Our analysis also motivates a method for targeted alteration of a model's mechanisms, named connectivity-based fine-tuning (CBFT), which we analyze using several synthetic datasets for the task of reducing a model's reliance on spurious attributes.},
	urldate = {2024-01-18},
	publisher = {arXiv},
	author = {Lubana, Ekdeep Singh and Bigelow, Eric J. and Dick, Robert P. and Krueger, David and Tanaka, Hidenori},
	month = jun,
	year = {2023},
	note = {arXiv:2211.08422 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: ICML, 2023},
	annote = {I am pretty sure this kind of mode difference is the kind of thing we are interested in looking at
},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/EPBCQVKP/Lubana et al. - 2023 - Mechanistic Mode Connectivity.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/Q5ID8FSN/2211.html:text/html},
}
@article{perez2022red,
  title={Red teaming language models with language models},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2202.03286},
  year={2022}
}

@article{JMLR:v19:17-646,
  author  = {Alessandro Achille and Stefano Soatto},
  title   = {Emergence of Invariance and Disentanglement in Deep Representations },
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {19},
  number  = {50},
  pages   = {1--34},
  url     = {http://jmlr.org/papers/v19/17-646.html}
}

@article{liu2023jailbreaking,
  title={Jailbreaking chatgpt via prompt engineering: An empirical study},
  author={Liu, Yi and Deng, Gelei and Xu, Zhengzi and Li, Yuekang and Zheng, Yaowen and Zhang, Ying and Zhao, Lida and Zhang, Tianwei and Liu, Yang},
  journal={arXiv preprint arXiv:2305.13860},
  year={2023}
}

@article{shen2023anything,
  title={" do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Shen, Yun and Zhang, Yang},
  journal={arXiv preprint arXiv:2308.03825},
  year={2023}
}

@inproceedings{wang_non-transferable_2021,
	title = {Non-{Transferable} {Learning}: {A} {New} {Approach} for {Model} {Ownership} {Verification} and {Applicability} {Authorization}},
	shorttitle = {Non-{Transferable} {Learning}},
	url = {https://openreview.net/forum?id=tYRrOdSnVUy},
	abstract = {As Artificial Intelligence as a Service gains popularity, protecting well-trained models as intellectual property is becoming increasingly important. There are two common types of protection methods: ownership verification and usage authorization. In this paper, we propose Non-Transferable Learning (NTL), a novel approach that captures the exclusive data representation in the learned model and restricts the model generalization ability to certain domains. This approach provides effective solutions to both model verification and authorization. Specifically: 1) For ownership verification, watermarking techniques are commonly used but are often vulnerable to sophisticated watermark removal methods. By comparison, our NTL-based ownership verification provides robust resistance to state-of-the-art watermark removal methods, as shown in extensive experiments with 6 removal approaches over the digits, CIFAR10 \& STL10, and VisDA datasets. 2) For usage authorization, prior solutions focus on authorizing specific users to access the model, but authorized users can still apply the model to any data without restriction. Our NTL-based authorization approach instead provides data-centric protection, which we call applicability authorization, by significantly degrading the performance of the model on unauthorized data. Its effectiveness is also shown through experiments on aforementioned datasets.},
	language = {en},
	urldate = {2024-01-18},
	author = {Wang, Lixu and Xu, Shichao and Xu, Ruiqi and Wang, Xiao and Zhu, Qi},
	month = oct,
	year = {2021},
	annote = {This could porbably used somehow directly as a method we can use to prevent harmful training

},
	file = {Full Text PDF:/Users/domenicrosati/Zotero/storage/KXW4T3IN/Wang et al. - 2021 - Non-Transferable Learning A New Approach for Mode.pdf:application/pdf},
}

@inproceedings{wang_domain_2023,
  title={Domain Specified Optimization for Deployment Authorization},
  author={Wang, Haotian and Chi, Haoang and Yang, Wenjing and Lin, Zhipeng and Geng, Mingyang and Lan, Long and Zhang, Jing and Tao, Dacheng},
  booktitle={2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages={5072--5082},
  year={2023},
  organization={IEEE}
}

@article{Kaffee2023ThornyRI,
  title={Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing},
  author={Lucie-Aim{\'e}e Kaffee and Arnav Arora and Zeerak Talat and Isabelle Augenstein},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.08315},
  url={https://api.semanticscholar.org/CorpusID:258179276}
}

@article{yang_shadow_2023,
  title={Shadow alignment: The ease of subverting safely-aligned language models},
  author={Yang, Xianjun and Wang, Xiao and Zhang, Qi and Petzold, Linda and Wang, William Yang and Zhao, Xun and Lin, Dahua},
  journal={arXiv preprint arXiv:2310.02949},
  year={2023}
}

@misc{wang2024decodingtrust,
      title={DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models}, 
      author={Boxin Wang and Weixin Chen and Hengzhi Pei and Chulin Xie and Mintong Kang and Chenhui Zhang and Chejian Xu and Zidi Xiong and Ritik Dutta and Rylan Schaeffer and Sang T. Truong and Simran Arora and Mantas Mazeika and Dan Hendrycks and Zinan Lin and Yu Cheng and Sanmi Koyejo and Dawn Song and Bo Li},
      year={2024},
      eprint={2306.11698},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{athalye2018obfuscated,
  title={Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples},
  author={Athalye, Anish and Carlini, Nicholas and Wagner, David},
  booktitle={International conference on machine learning},
  pages={274--283},
  year={2018},
  organization={PMLR}
}

@article{Alam2020DeepLockSA,
  title={Deep-lock: Secure authorization for deep neural networks},
  author={Alam, Manaar and Saha, Sayandeep and Mukhopadhyay, Debdeep and Kundu, Sandip},
  journal={arXiv preprint arXiv:2008.05966},
  year={2020}
}

@article{papernot2016science,
  title={Towards the science of security and privacy in machine learning},
  author={Papernot, Nicolas and McDaniel, Patrick and Sinha, Arunesh and Wellman, Michael},
  journal={arXiv preprint arXiv:1611.03814},
  year={2016}
}

@inproceedings{
tramèr2020ensemble,
title={Ensemble Adversarial Training: Attacks and Defenses},
author={Florian Tramèr and Alexey Kurakin and Nicolas Papernot and Ian Goodfellow and Dan Boneh and Patrick McDaniel},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rkZvSe-RZ},
}

@inproceedings{papernot2017practical,
  title={Practical black-box attacks against machine learning},
  author={Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z Berkay and Swami, Ananthram},
  booktitle={Proceedings of the 2017 ACM on Asia conference on computer and communications security},
  pages={506--519},
  year={2017}
}

@inproceedings{papernot2016distillation,
  title={Distillation as a defense to adversarial perturbations against deep neural networks},
  author={Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
  booktitle={2016 IEEE symposium on security and privacy (SP)},
  pages={582--597},
  year={2016},
  organization={IEEE}
}

@article{eustratiadis2022attacking,
  title={Attacking Adversarial Defences by Smoothing the Loss Landscape},
  author={Eustratiadis, Panagiotis and Gouk, Henry and Li, Da and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2208.00862},
  year={2022}
}

@article{ipprotectionattack,
  title={Attacks on Recent DNN IP Protection Techniques and Their Mitigation},
  author={Mukherjee, Rijoy and Chakraborty, Rajat Subhra},
  journal={IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  year={2023},
  publisher={IEEE}
}

@article{activegaurd,
  title={ActiveGuard: An active intellectual property protection technique for deep neural networks by leveraging adversarial examples as users' fingerprints},
  author={Xue, Mingfu and Sun, Shichang and He, Can and Gu, Dujuan and Zhang, Yushu and Wang, Jian and Liu, Weiqiang},
  journal={IET Computers \& Digital Techniques},
  year={2023},
  publisher={Wiley Online Library}
}

@article{advparams,
  title={AdvParams: An active DNN intellectual property protection technique via adversarial perturbation based parameter encryption},
  author={Xue, Mingfu and Wu, Zhiyu and Zhang, Yushu and Wang, Jian and Liu, Weiqiang},
  journal={IEEE Transactions on Emerging Topics in Computing},
  year={2022},
  publisher={IEEE}
}

@article{nichol2018firstorder,
  title={On first-order meta-learning algorithms},
  author={Nichol, Alex and Achiam, Joshua and Schulman, John},
  journal={arXiv preprint arXiv:1803.02999},
  year={2018}
}

@inproceedings{finn2017modelagnostic,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}

@misc{park2020metacurvature,
      title={Meta-Curvature}, 
      author={Eunbyung Park and Junier B. Oliva},
      year={2020},
      eprint={1902.03356},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{edwards2016censoring,
  title={Censoring representations with an adversary},
  author={Edwards, Harrison and Storkey, Amos},
  journal={arXiv preprint arXiv:1511.05897},
  year={2015}
}

@inproceedings{zemel2013learning,
  title={Learning fair representations},
  author={Zemel, Rich and Wu, Yu and Swersky, Kevin and Pitassi, Toni and Dwork, Cynthia},
  booktitle={International conference on machine learning},
  pages={325--333},
  year={2013},
  organization={PMLR}
}

@inproceedings{ravfogel2020null,
  title={Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection},
  author={Ravfogel, Shauli and Elazar, Yanai and Gonen, Hila and Twiton, Michael and Goldberg, Yoav},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={7237--7256},
  year={2020}
}

@inproceedings{ravfogel2022linear,
  title={Linear adversarial concept erasure},
  author={Ravfogel, Shauli and Twiton, Michael and Goldberg, Yoav and Cotterell, Ryan D},
  booktitle={International Conference on Machine Learning},
  pages={18400--18421},
  year={2022},
  organization={PMLR}
}

@article{belrose2023leace,
  title={Leace: Perfect linear concept erasure in closed form},
  author={Belrose, Nora and Schneider-Joseph, David and Ravfogel, Shauli and Cotterell, Ryan and Raff, Edward and Biderman, Stella},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@misc{ganguli2022red,
      title={Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned}, 
      author={Deep Ganguli and Liane Lovitt and Jackson Kernion and Amanda Askell and Yuntao Bai and Saurav Kadavath and Ben Mann and Ethan Perez and Nicholas Schiefer and Kamal Ndousse and Andy Jones and Sam Bowman and Anna Chen and Tom Conerly and Nova DasSarma and Dawn Drain and Nelson Elhage and Sheer El-Showk and Stanislav Fort and Zac Hatfield-Dodds and Tom Henighan and Danny Hernandez and Tristan Hume and Josh Jacobson and Scott Johnston and Shauna Kravec and Catherine Olsson and Sam Ringer and Eli Tran-Johnson and Dario Amodei and Tom Brown and Nicholas Joseph and Sam McCandlish and Chris Olah and Jared Kaplan and Jack Clark},
      year={2022},
      eprint={2209.07858},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{De_Arteaga_2019, series={FAT* ’19},
   title={Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting},
   url={http://dx.doi.org/10.1145/3287560.3287572},
   DOI={10.1145/3287560.3287572},
   booktitle={Proceedings of the Conference on Fairness, Accountability, and Transparency},
   publisher={ACM},
   author={De-Arteaga, Maria and Romanov, Alexey and Wallach, Hanna and Chayes, Jennifer and Borgs, Christian and Chouldechova, Alexandra and Geyik, Sahin and Kenthapadi, Krishnaram and Kalai, Adam Tauman},
   year={2019},
   month=jan, collection={FAT* ’19} }

@misc{zou2023universal,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and Nicholas Carlini and Milad Nasr and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      eprint={2307.15043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zheng2023careful,
  title={Be Careful with PyPI Packages: You May Unconsciously Spread Backdoor Model Weights},
  author={Zheng, Tianhang and Lan, Hao and Li, Baochun},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@misc{jones2023automatically,
      title={Automatically Auditing Large Language Models via Discrete Optimization}, 
      author={Erik Jones and Anca Dragan and Aditi Raghunathan and Jacob Steinhardt},
      year={2023},
      eprint={2303.04381},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lermen2023lora,
  title={Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b},
  author={Lermen, Simon and Rogers-Smith, Charlie and Ladish, Jeffrey},
  journal={arXiv preprint arXiv:2310.20624},
  year={2023}
}

@misc{hubinger2024sleeper,
      title={Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training}, 
      author={Evan Hubinger and Carson Denison and Jesse Mu and Mike Lambert and Meg Tong and Monte MacDiarmid and Tamera Lanham and Daniel M. Ziegler and Tim Maxwell and Newton Cheng and Adam Jermyn and Amanda Askell and Ansh Radhakrishnan and Cem Anil and David Duvenaud and Deep Ganguli and Fazl Barez and Jack Clark and Kamal Ndousse and Kshitij Sachan and Michael Sellitto and Mrinank Sharma and Nova DasSarma and Roger Grosse and Shauna Kravec and Yuntao Bai and Zachary Witten and Marina Favaro and Jan Brauner and Holden Karnofsky and Paul Christiano and Samuel R. Bowman and Logan Graham and Jared Kaplan and Sören Mindermann and Ryan Greenblatt and Buck Shlegeris and Nicholas Schiefer and Ethan Perez},
      year={2024},
      eprint={2401.05566},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{welbl2021challenges,
      title={Challenges in Detoxifying Language Models}, 
      author={Johannes Welbl and Amelia Glaese and Jonathan Uesato and Sumanth Dathathri and John Mellor and Lisa Anne Hendricks and Kirsty Anderson and Pushmeet Kohli and Ben Coppin and Po-Sen Huang},
      year={2021},
      eprint={2109.07445},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{
    kirk2023signifier,
    title={The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising ''Alignment'' in Large Language Models},
    author={Hannah Kirk and Bertie Vidgen and Paul Rottger and Scott Hale},
    booktitle={Socially Responsible Language Modelling Research},
    year={2023},
    url={https://openreview.net/forum?id=6mHKQkV8NY}
}

@misc{hubinger2021risks,
      title={Risks from Learned Optimization in Advanced Machine Learning Systems}, 
      author={Evan Hubinger and Chris van Merwijk and Vladimir Mikulik and Joar Skalse and Scott Garrabrant},
      year={2021},
      eprint={1906.01820},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{gehman2020realtoxicityprompts,
      title={{RealToxicityPrompts}: Evaluating Neural Toxic Degeneration in Language Models}, 
      author={Samuel Gehman and Suchin Gururangan and Maarten Sap and Yejin Choi and Noah A. Smith},
      year={2020},
      eprint={2009.11462},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{carlini2023aligned,
      title={Are aligned neural networks adversarially aligned?}, 
      author={Nicholas Carlini and Milad Nasr and Christopher A. Choquette-Choo and Matthew Jagielski and Irena Gao and Anas Awadalla and Pang Wei Koh and Daphne Ippolito and Katherine Lee and Florian Tramer and Ludwig Schmidt},
      year={2023},
      eprint={2306.15447},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{bai2022training,
      title={Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback}, 
      author={Yuntao Bai and Andy Jones and Kamal Ndousse and Amanda Askell and Anna Chen and Nova DasSarma and Dawn Drain and Stanislav Fort and Deep Ganguli and Tom Henighan and Nicholas Joseph and Saurav Kadavath and Jackson Kernion and Tom Conerly and Sheer El-Showk and Nelson Elhage and Zac Hatfield-Dodds and Danny Hernandez and Tristan Hume and Scott Johnston and Shauna Kravec and Liane Lovitat and Neel Nanda and Catherine Olsson and Dario Amodei and Tom Brown and Jack Clark and Sam McCandlish and Chris Olah and Ben Mann and Jared Kaplan},
      year={2022},
      eprint={2204.05862},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{casper2024blackbox,
      title={Black-Box Access is Insufficient for Rigorous AI Audits}, 
      author={Stephen Casper and Carson Ezell and Charlotte Siegmann and Noam Kolt and Taylor Lynn Curtis and Benjamin Bucknall and Andreas Haupt and Kevin Wei and Jérémy Scheurer and Marius Hobbhahn and Lee Sharkey and Satyapriya Krishna and Marvin Von Hagen and Silas Alberti and Alan Chan and Qinyi Sun and Michael Gerovitch and David Bau and Max Tegmark and David Krueger and Dylan Hadfield-Menell},
      year={2024},
      eprint={2401.14446},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{greshake2023youve,
      title={Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection}, 
      author={Kai Greshake and Sahar Abdelnabi and Shailesh Mishra and Christoph Endres and Thorsten Holz and Mario Fritz},
      year={2023},
      eprint={2302.12173},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{apruzzese2022real,
      title={"Real Attackers Don't Compute Gradients": Bridging the Gap Between Adversarial ML Research and Practice}, 
      author={Giovanni Apruzzese and Hyrum S. Anderson and Savino Dambra and David Freeman and Fabio Pierazzi and Kevin A. Roundy},
      year={2022},
      eprint={2212.14315},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{anderljung2023frontier,
      title={Frontier AI Regulation: Managing Emerging Risks to Public Safety}, 
      author={Markus Anderljung and Joslyn Barnhart and Anton Korinek and Jade Leung and Cullen O'Keefe and Jess Whittlestone and Shahar Avin and Miles Brundage and Justin Bullock and Duncan Cass-Beggs and Ben Chang and Tantum Collins and Tim Fist and Gillian Hadfield and Alan Hayes and Lewis Ho and Sara Hooker and Eric Horvitz and Noam Kolt and Jonas Schuett and Yonadav Shavit and Divya Siddarth and Robert Trager and Kevin Wolf},
      year={2023},
      eprint={2307.03718},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@article{gu2015deep,
  title={Towards deep neural network architectures robust to adversarial examples},
  author={Gu, Shixiang and Rigazio, Luca},
  journal={arXiv preprint arXiv:1412.5068},
  year={2014}
}

@article{choe2023making,
  title={Making scalable meta learning practical},
  author={Choe, Sang and Mehta, Sanket Vaibhav and Ahn, Hwijeen and Neiswanger, Willie and Xie, Pengtao and Strubell, Emma and Xing, Eric},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{rajeswaran2019metalearning,
  title={Meta-learning with implicit gradients},
  author={Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham M and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{zeng2023unsupervised,
  title={Unsupervised Non-transferable Text Classification},
  author={Zeng, Guangtao and Lu, Wei},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={10071--10084},
  year={2022}
}

@misc{esiobu2023robbie,
      title={ROBBIE: Robust Bias Evaluation of Large Generative Language Models}, 
      author={David Esiobu and Xiaoqing Tan and Saghar Hosseini and Megan Ung and Yuchen Zhang and Jude Fernandes and Jane Dwivedi-Yu and Eleonora Presani and Adina Williams and Eric Michael Smith},
      year={2023},
      eprint={2311.18140},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{van-der-lee-etal-2020-cacapo,
    title = "The {CACAPO} Dataset: A Multilingual, Multi-Domain Dataset for Neural Pipeline and End-to-End Data-to-Text Generation",
    author = "van der Lee, Chris  and
      Emmery, Chris  and
      Wubben, Sander  and
      Krahmer, Emiel",
    editor = "Davis, Brian  and
      Graham, Yvette  and
      Kelleher, John  and
      Sripada, Yaji",
    booktitle = "Proceedings of the 13th International Conference on Natural Language Generation",
    month = dec,
    year = "2020",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.inlg-1.10",
    doi = "10.18653/v1/2020.inlg-1.10",
    pages = "68--79",
}

@misc{rafailov2023direct,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2023},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bhatt2023purple,
      title={Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models}, 
      author={Manish Bhatt and Sahana Chennabasappa and Cyrus Nikolaidis and Shengye Wan and Ivan Evtimov and Dominik Gabi and Daniel Song and Faizan Ahmad and Cornelius Aschermann and Lorenzo Fontana and Sasha Frolov and Ravi Prakash Giri and Dhaval Kapil and Yiannis Kozyrakis and David LeBlanc and James Milazzo and Aleksandar Straumann and Gabriel Synnaeve and Varun Vontimitta and Spencer Whitman and Joshua Saxe},
      year={2023},
      eprint={2312.04724},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@inproceedings{balakrishnan-etal-2019-constrained,
    title = "Constrained Decoding for Neural {NLG} from Compositional Representations in Task-Oriented Dialogue",
    author = "Balakrishnan, Anusha  and
      Rao, Jinfeng  and
      Upasani, Kartikeya  and
      White, Michael  and
      Subba, Rajen",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1080",
    doi = "10.18653/v1/P19-1080",
    pages = "831--844",
    abstract = "Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Avenues like the E2E NLG Challenge have encouraged the development of neural approaches, particularly sequence-to-sequence (Seq2Seq) models for this problem. The semantic representations used, however, are often underspecified, which places a higher burden on the generation model for sentence planning, and also limits the extent to which generated responses can be controlled in a live system. In this paper, we (1) propose using tree-structured semantic representations, like those used in traditional rule-based NLG systems, for better discourse-level structuring and sentence-level planning; (2) introduce a challenging dataset using this representation for the weather domain; (3) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness; and (4) demonstrate promising results on our dataset and the E2E dataset.",
}

@inproceedings{nan-etal-2021-dart,
title = "{DART}: Open-Domain Structured Data Record to Text Generation",
author = "Nan, Linyong  and
Radev, Dragomir  and
Zhang, Rui  and
Rau, Amrit  and
Sivaprasad, Abhinand  and
Hsieh, Chiachun  and
Tang, Xiangru  and
Vyas, Aadit  and
Verma, Neha  and
Krishna, Pranav  and
Liu, Yangxiaokang  and
Irwanto, Nadia  and
Pan, Jessica  and
Rahman, Faiaz  and
Zaidi, Ahmad  and
Mutuma, Mutethia  and
Tarabar, Yasin  and
Gupta, Ankit  and
Yu, Tao  and
Tan, Yi Chern  and
Lin, Xi Victoria  and
Xiong, Caiming  and
Socher, Richard  and
Rajani, Nazneen Fatema",
booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
month = jun,
year = "2021",
address = "Online",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2021.naacl-main.37",
doi = "10.18653/v1/2021.naacl-main.37",
pages = "432--447",
abstract = "We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github.com/Yale-LILY/dart.",
}


@inproceedings{e2e_cleaned,
address = {Tokyo, Japan},
title = {Semantic {Noise} {Matters} for {Neural} {Natural} {Language} {Generation}},
url = {https://www.aclweb.org/anthology/W19-8652/},
booktitle = {Proceedings of the 12th {International} {Conference} on {Natural} {Language} {Generation} ({INLG} 2019)},
author = {Dušek, Ondřej and Howcroft, David M and Rieser, Verena},
year = {2019},
pages = {421--426},
}

@misc{bai2023qwen,
      title={Qwen Technical Report}, 
      author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
      year={2023},
      eprint={2309.16609},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{lees2022new,
      title={A New Generation of Perspective API: Efficient Multilingual Character-level Transformers}, 
      author={Alyssa Lees and Vinh Q. Tran and Yi Tay and Jeffrey Sorensen and Jai Gupta and Donald Metzler and Lucy Vasserman},
      year={2022},
      eprint={2202.11176},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{casper2024defendingunforeseenfailuremodes,
      title={Defending Against Unforeseen Failure Modes with Latent Adversarial Training}, 
      author={Stephen Casper and Lennart Schulze and Oam Patel and Dylan Hadfield-Menell},
      year={2024},
      eprint={2403.05030},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2403.05030}, 
}

@misc{
mukhoti2024finetuning,
title={Fine-tuning can cripple foundation models; preserving features may be the solution},
author={Jishnu Mukhoti and Yarin Gal and Philip Torr and Puneet K. Dokania},
year={2024},
url={https://openreview.net/forum?id=VQ7Q6qdp0P}
}

@article{yi2024safety,
  title={A safety realignment framework via subspace-oriented model fusion for large language models},
  author={Yi, Xin and Zheng, Shunfan and Wang, Linlin and Wang, Xiaoling and He, Liang},
  journal={arXiv preprint arXiv:2405.09055},
  year={2024}
}

@article{hsu2024safe,
  title={Safe LoRA: the Silver Lining of Reducing Safety Risks when Fine-tuning Large Language Models},
  author={Hsu, Chia-Yi and Tsai, Yu-Lin and Lin, Chih-Hsun and Chen, Pin-Yu and Yu, Chia-Mu and Huang, Chun-Ying},
  journal={arXiv preprint arXiv:2405.16833},
  year={2024}
}

@article{lyu2024keeping,
  title={Keeping llms aligned after fine-tuning: The crucial role of prompt templates},
  author={Lyu, Kaifeng and Zhao, Haoyu and Gu, Xinran and Yu, Dingli and Goyal, Anirudh and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2402.18540},
  year={2024}
}

@article{zong2024safety,
  title={Safety fine-tuning at (almost) no cost: A baseline for vision large language models},
  author={Zong, Yongshuo and Bohdal, Ondrej and Yu, Tingyang and Yang, Yongxin and Hospedales, Timothy},
  journal={arXiv preprint arXiv:2402.02207},
  year={2024}
}

@misc{huang2024lazysafetyalignmentlarge,
      title={Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning}, 
      author={Tiansheng Huang and Sihao Hu and Fatih Ilhan and Selim Furkan Tekin and Ling Liu},
      year={2024},
      eprint={2405.18641},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.18641}, 
}

@misc{jain2023baselinedefensesadversarialattacks,
      title={Baseline Defenses for Adversarial Attacks Against Aligned Language Models}, 
      author={Neel Jain and Avi Schwarzschild and Yuxin Wen and Gowthami Somepalli and John Kirchenbauer and Ping-yeh Chiang and Micah Goldblum and Aniruddha Saha and Jonas Geiping and Tom Goldstein},
      year={2023},
      eprint={2309.00614},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.00614}, 
}

@misc{robey2024smoothllmdefendinglargelanguage,
      title={SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks}, 
      author={Alexander Robey and Eric Wong and Hamed Hassani and George J. Pappas},
      year={2024},
      eprint={2310.03684},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.03684}, 
}

@article{achille2018information,
  title={Information dropout: Learning optimal representations through noisy computation},
  author={Achille, Alessandro and Soatto, Stefano},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2897--2905},
  year={2018},
  publisher={IEEE}
}

@misc{liu2024tuninglanguagemodelsproxy,
      title={Tuning Language Models by Proxy}, 
      author={Alisa Liu and Xiaochuang Han and Yizhong Wang and Yulia Tsvetkov and Yejin Choi and Noah A. Smith},
      year={2024},
      eprint={2401.08565},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.08565}, 
}

@article{arditi2024refusal,
  title={Refusal in Language Models Is Mediated by a Single Direction},
  author={Arditi, Andy and Obeso, Oscar and Syed, Aaquib and Paleka, Daniel and Rimsky, Nina and Gurnee, Wes and Nanda, Neel},
  journal={arXiv preprint arXiv:2406.11717},
  year={2024}
}

@misc{holtzman2020curious,
      title={The Curious Case of Neural Text Degeneration}, 
      author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
      year={2020},
      eprint={1904.09751},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Wu_2019,
   title={Learnability for the Information Bottleneck},
   volume={21},
   ISSN={1099-4300},
   url={http://dx.doi.org/10.3390/e21100924},
   DOI={10.3390/e21100924},
   number={10},
   journal={Entropy},
   publisher={MDPI AG},
   author={Wu, Tailin and Fischer, Ian and Chuang, Isaac L. and Tegmark, Max},
   year={2019},
   month=sep, pages={924} }


@inproceedings{tishby2015deep,
  title={Deep learning and the information bottleneck principle},
  author={Tishby, Naftali and Zaslavsky, Noga},
  booktitle={2015 ieee information theory workshop (itw)},
  pages={1--5},
  year={2015},
  organization={IEEE}
}

@inproceedings{suggala2019revisiting,
  title={Revisiting adversarial risk},
  author={Suggala, Arun Sai and Prasad, Adarsh and Nagarajan, Vaishnavh and Ravikumar, Pradeep},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={2331--2339},
  year={2019},
  organization={PMLR}
}

@article{wu2023defenses,
  title={Defenses in adversarial machine learning: A survey},
  author={Wu, Baoyuan and Wei, Shaokui and Zhu, Mingli and Zheng, Meixi and Zhu, Zihao and Zhang, Mingda and Chen, Hongrui and Yuan, Danni and Liu, Li and Liu, Qingshan},
  journal={arXiv preprint arXiv:2312.08890},
  year={2023}
}

@article{Xu_2024,
  title={InfoAT: Improving Adversarial Training Using the Information Bottleneck Principle},
  author={Xu, Mengting and Zhang, Tao and Li, Zhongnian and Zhang, Daoqiang},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@article{geiping2022doesnt,
  title={What Doesn't Kill You Makes You Robust (er): How to Adversarially Train against Data Poisoning},
  author={Geiping, Jonas and Fowl, Liam and Somepalli, Gowthami and Goldblum, Micah and Moeller, Michael and Goldstein, Tom},
  journal={arXiv preprint arXiv:2102.13624},
  year={2021}
}

@article{altinisik2023impact,
  title={Impact of adversarial training on robustness and generalizability of language models},
  author={Altinisik, Enes and Sajjad, Hassan and Sencar, Husrev Taha and Messaoud, Safa and Chawla, Sanjay},
  journal={arXiv preprint arXiv:2211.05523},
  year={2022}
}

@article{liu2020loss,
  title={On the loss landscape of adversarial training: Identifying challenges and how to overcome them},
  author={Liu, Chen and Salzmann, Mathieu and Lin, Tao and Tomioka, Ryota and S{\"u}sstrunk, Sabine},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21476--21487},
  year={2020}
}

@article{goldblum2020adversarially,
  title={Adversarially robust few-shot learning: A meta-learning approach},
  author={Goldblum, Micah and Fowl, Liam and Goldstein, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17886--17895},
  year={2020}
}

@inproceedings{fan2021adversarial,
  title={Adversarial training and provable robustness: A tale of two objectives},
  author={Fan, Jiameng and Li, Wenchao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={8},
  pages={7367--7376},
  year={2021}
}

@article{Madry18,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={International Conference on Learning Representations},
  year={2018}
}

@article{Carlini17,
  title={Towards Evaluating the Robustness of Neural Networks},
  author={Carlini, Nicholas and Wagner, David},
  journal={2017 IEEE Symposium on Security and Privacy (SP)},
  year={2017},
  organization={IEEE}
}

@article{Huang2020,
  title={Survey on Deep Learning with Label Noise},
  author={Huang, Junyu and Li, Qi and Yu, Xingjian and Deng, Zhao and Tao, Dacheng},
  journal={Pattern Recognition Letters},
  volume={128},
  pages={19-25},
  year={2020}
}

@article{Weng2018,
  title={Evaluating the Robustness of Neural Networks: An Extreme Value Theory Approach},
  author={Weng, Tsui-Wei and Zhang, Huan and Chen, Hongge and Song, Zhao and Hsieh, Cho-Jui and Daniel, Luca and Boning, Duane S and Dhillon, Inderjit S},
  journal={International Conference on Learning Representations},
  year={2018}
}

@article{Xiang2018,
  title={Output Reachable Set Estimation and Verification for Multilayer Neural Networks},
  author={Xiang, Weiming and Tran, Hoang-Dung and Johnson, Taylor T},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={29},
  number={11},
  pages={5777-5783},
  year={2018},
  publisher={IEEE}
}

@article{Zhang2018,
  title={Efficient Neural Network Robustness Certification with General Activation Functions},
  author={Zhang, Huan and Weng, Tsui-Wei and Chen, Pin-Yu and Hsieh, Cho-Jui and Daniel, Luca},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{Cohen2019,
  title={Certified adversarial robustness via randomized smoothing},
  author={Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
  booktitle={international conference on machine learning},
  pages={1310--1320},
  year={2019},
  organization={PMLR}
}

@article{Salman2019,
  title={Provably robust deep learning via adversarially trained smoothed classifiers},
  author={Salman, Hadi and Li, Jerry and Razenshteyn, Ilya and Zhang, Pengchuan and Zhang, Huan and Bubeck, Sebastien and Yang, Greg},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{10.1162/coli_a_00422,
    author = {Belinkov, Yonatan},
    title = "{Probing Classifiers: Promises, Shortcomings, and Advances}",
    journal = {Computational Linguistics},
    volume = {48},
    number = {1},
    pages = {207-219},
    year = {2022},
    month = {04},
    doi = {10.1162/coli_a_00422},
    url = {https://doi.org/10.1162/coli\_a\_00422},
    eprint = {https://direct.mit.edu/coli/article-pdf/48/1/207/2006605/coli\_a\_00422.pdf},
}





@inproceedings{borgnia2020strong,
  title={Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff},
  author={Borgnia, Eitan and Cherepanova, Valeriia and Fowl, Liam and Ghiasi, Amin and Geiping, Jonas and Goldblum, Micah and Goldstein, Tom and Gupta, Arjun},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={3855--3859},
  year={2021},
  organization={IEEE}
}

@article{bhardwaj2023language,
  title={Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases},
  author={Bhardwaj, Rishabh and Poria, Soujanya},
  journal={arXiv preprint arXiv:2310.14303},
  year={2023}
}


@misc{cao2023stealthy,
      title={Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections}, 
      author={Yuanpu Cao and Bochuan Cao and Jinghui Chen},
      year={2023},
      eprint={2312.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{li2021antibackdoor,
      title={Anti-Backdoor Learning: Training Clean Models on Poisoned Data}, 
      author={Yige Li and Xixiang Lyu and Nodens Koren and Lingjuan Lyu and Bo Li and Xingjun Ma},
      year={2021},
      eprint={2110.11571},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{hong2020effectiveness,
  title={On the effectiveness of mitigating data poisoning attacks with gradient shaping},
  author={Hong, Sanghyun and Chandrasekaran, Varun and Kaya, Yi{\u{g}}itcan and Dumitra{\c{s}}, Tudor and Papernot, Nicolas},
  journal={arXiv preprint arXiv:2002.11497},
  year={2020}
}

@article{du2019robust,
  title={Robust anomaly detection and backdoor attack detection via differential privacy},
  author={Du, Min and Jia, Ruoxi and Song, Dawn},
  journal={arXiv preprint arXiv:1911.07116},
  year={2019}
}

@article{Steinhardt17,
  title={Certified defenses for data poisoning attacks},
  author={Steinhardt, Jacob and Koh, Pang Wei W and Liang, Percy S},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{Gilmer2018,
  title={Adversarial Spheres},
  author={Gilmer, Justin and Adams, Ryan P and Goodfellow, Ian J and Andersen, David and Dahl, George E},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{Chen2017,
  title={Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning},
  author={Chen, Xinyun and Liu, Chang and Li, Bo and Lu, Kimberly and Song, Dawn},
  booktitle={2017 IEEE International Conference on Big Data (Big Data)},
  year={2017},
  organization={IEEE}
}

@article{Turner2019,
  title={Clean-Label Backdoor Attacks},
  author={Turner, Alexander and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv preprint arXiv:1905.05897},
  year={2019}
}

@inproceedings{Gao2019,
  title={Strip: A Defence Against Trojan Attacks on Deep Neural Networks},
  author={Gao, Yansong and Xu, Chang and Wang, Derui and Chen, Shiping and Ranasinghe, Damith C and Nepal, Surya},
  booktitle={Proceedings of the 35th Annual Computer Security Applications Conference},
  pages={113-125},
  year={2019},
  organization={ACM}
}

@article{Wang2020Backdoor,
  title={Backdoor Attacks Against Transfer Learning with Pre-trained Deep Learning Models},
  author={Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y},
  journal={IEEE Transactions on Services Computing},
  year={2020},
  publisher={IEEE}
}

@inproceedings{Shafahi2018,
  title={Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks},
  author={Shafahi, Ali and Huang, W Ronny and Najibi, Mahyar and Suciu, Octavian and Studer, Christoph and Dumitras, Tudor and Goldstein, Tom},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6103-6113},
  year={2018}
}

@inproceedings{Suciu2018,
  title={When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks},
  author={Suciu, Octavian and Marginean, Radu and Kaya, Yigitcan and Daume III, Hal and Dumitras, Tudor},
  booktitle={27th {USENIX} Security Symposium ({USENIX} Security 18)},
  pages={1299-1316},
  year={2018}
}

@inproceedings{Tan2020,
  title={Bypassing Backdoor Detection Algorithms in Deep Learning},
  author={Tan, Minhui and Li, Bo and Cui, Qianru and Chen, Junjia and Ng, Siew-Kei and Yang, Cong},
  booktitle={2020 IEEE 36th International Conference on Data Engineering (ICDE)},
  pages={1724-1727},
  year={2020},
  organization={IEEE}
}

@article{Papernot2016Distillation,
  title={Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks},
  author={Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
  journal={2016 IEEE Symposium on Security and Privacy (SP)},
  year={2016},
  organization={IEEE}
}

@inproceedings{Adi2018,
  title={Turning Your Weakness Into a Strength: Watermarking Deep Neural Networks by Backdooring},
  author={Adi, Yossi and Baum, Carsten and Bentin, Moustapha and Pinkas, Benny and Keshet, Joseph},
  booktitle={27th {USENIX} Security Symposium ({USENIX} Security 18)},
  pages={1615-1631},
  year={2018}
}

@article{Merrer2017,
  title={Adversarial Frontier Stitching for Remote Neural Network Watermarking},
  author={Le Merrer, Erwan and Perez, Patrick and Troncoso, Carmela},
  journal={Neural Computing and Applications},
  year={2017},
  publisher={Springer}
}

@article{LeMerrer2020,
  title={DeepMarks: A Digital Fingerprinting Framework for Deep Neural Networks},
  author={Le Merrer, Erwan and Perez, Patrick and Trédan, Gilles},
  journal={ACM Transactions on Privacy and Security (TOPS)},
  volume={23},
  number={3},
  year={2020},
  publisher={ACM}
}


@inproceedings{Namba2019,
  title={Robust Watermarking of Neural Network Models},
  author={Namba, Ryota and Sakuma, Jun},
  booktitle={Proceedings of the 2019 ACM Asia Conference on Computer and Communications Security},
  pages={506-518},
  year={2019},
  organization={ACM}
}

@article{Fan2020,
  title={Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks},
  author={Fan, Lixin and Ng, Kam Woh and Chan, Chee Seng and Yang, Qiang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5494-5505},
  year={2020}
}

@article{gade2023badllama,
  title={BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B},
  author={Gade, Pranav and Lermen, Simon and Rogers-Smith, Charlie and Ladish, Jeffrey},
  journal={arXiv preprint arXiv:2311.00117},
  year={2023}
}

@article{hazell2023spear,
  title={Large language models can be used to effectively scale spear phishing campaigns},
  author={Hazell, Julian},
  journal={arXiv preprint arXiv:2305.06972},
  year={2023}
}

@article{Huang2024VaccinePA,
  title={Vaccine: Perturbation-aware Alignment for Large Language Model},
  author={Huang, Tiansheng and Hu, Sihao and Liu, Ling},
  journal={arXiv preprint arXiv:2402.01109},
  year={2024}
}

@article{wei2024assessing,
  title={Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications},
  author={Wei, Boyi and Huang, Kaixuan and Huang, Yangsibo and Xie, Tinghao and Qi, Xiangyu and Xia, Mengzhou and Mittal, Prateek and Wang, Mengdi and Henderson, Peter},
  journal={arXiv preprint arXiv:2402.05162},
  year={2024}
}

@article{goyal2023survey,
  title={A survey of adversarial defenses and robustness in nlp},
  author={Goyal, Shreya and Doddapaneni, Sumanth and Khapra, Mitesh M and Ravindran, Balaraman},
  journal={ACM Computing Surveys},
  volume={55},
  number={14s},
  pages={1--39},
  year={2023},
  publisher={ACM New York, NY}
}

@article{goldblum_dataset_2021,
  title={Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses},
  author={Goldblum, Micah and Tsipras, Dimitris and Xie, Chulin and Chen, Xinyun and Schwarzschild, Avi and Song, Dawn and Makdry, Aleksander and Li, Bo and Goldstein, Tom},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={2},
  pages={1563--1580},
  year={2022},
  publisher={IEEE}
}

@misc{wu_defenses_2023,
	title = {Defenses in {Adversarial} {Machine} {Learning}: {A} {Survey}},
	shorttitle = {Defenses in {Adversarial} {Machine} {Learning}},
	url = {http://arxiv.org/abs/2312.08890},
	doi = {10.48550/arXiv.2312.08890},
	abstract = {Adversarial phenomenon has been widely observed in machine learning (ML) systems, especially in those using deep neural networks, describing that ML systems may produce inconsistent and incomprehensible predictions with humans at some particular cases. This phenomenon poses a serious security threat to the practical application of ML systems, and several advanced attack paradigms have been developed to explore it, mainly including backdoor attacks, weight attacks, and adversarial examples. For each individual attack paradigm, various defense paradigms have been developed to improve the model robustness against the corresponding attack paradigm. However, due to the independence and diversity of these defense paradigms, it is difficult to examine the overall robustness of an ML system against different kinds of attacks.This survey aims to build a systematic review of all existing defense paradigms from a unified perspective. Specifically, from the life-cycle perspective, we factorize a complete machine learning system into five stages, including pre-training, training, post-training, deployment, and inference stages, respectively. Then, we present a clear taxonomy to categorize and review representative defense methods at each individual stage. The unified perspective and presented taxonomies not only facilitate the analysis of the mechanism of each defense paradigm but also help us to understand connections and differences among different defense paradigms, which may inspire future research to develop more advanced, comprehensive defenses.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Wu, Baoyuan and Wei, Shaokui and Zhu, Mingli and Zheng, Meixi and Zhu, Zihao and Zhang, Mingda and Chen, Hongrui and Yuan, Danni and Liu, Li and Liu, Qingshan},
	month = dec,
	year = {2023},
	note = {arXiv:2312.08890 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	annote = {Comment: 21 pages, 5 figures, 2 tables, 237 reference papers},
	file = {arXiv Fulltext PDF:/Users/domenicrosati/Zotero/storage/4DWXGU9U/Wu et al. - 2023 - Defenses in Adversarial Machine Learning A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/domenicrosati/Zotero/storage/PEW4I9LX/2312.html:text/html},
}

@article{gao_backdoor_2020,
  title={Backdoor attacks and countermeasures on deep learning: A comprehensive review},
  author={Gao, Yansong and Doan, Bao Gia and Zhang, Zhi and Ma, Siqi and Zhang, Jiliang and Fu, Anmin and Nepal, Surya and Kim, Hyoungshick},
  journal={arXiv preprint arXiv:2007.10760},
  year={2020}
}

@article{li_backdoor_2022,
  title={Backdoor learning: A survey},
  author={Li, Yiming and Jiang, Yong and Li, Zhifeng and Xia, Shu-Tao},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@misc{kirk2023signifier,
      title={The Empty Signifier Problem: Towards Clearer Paradigms for Operationalising "Alignment" in Large Language Models}, 
      author={Hannah Rose Kirk and Bertie Vidgen and Paul Röttger and Scott A. Hale},
      year={2023},
      eprint={2310.02457},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{dong2024attacks,
  title={Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey},
  author={Dong, Zhichen and Zhou, Zhanhui and Yang, Chao and Shao, Jing and Qiao, Yu},
  journal={arXiv preprint arXiv:2402.09283},
  year={2024}
}

@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}



@misc{jain2023mechanistically,
      title={Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks}, 
      author={Samyak Jain and Robert Kirk and Ekdeep Singh Lubana and Robert P. Dick and Hidenori Tanaka and Edward Grefenstette and Tim Rocktäschel and David Scott Krueger},
      year={2023},
      eprint={2311.12786},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

    @article{Lee2024AMU,
  title={A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity},
  author={Andrew Lee and Xiaoyan Bai and Itamar Pres and Martin Wattenberg and Jonathan K. Kummerfeld and Rada Mihalcea},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.01967},
  url={https://api.semanticscholar.org/CorpusID:266755904}
}

@article{zhao2023learning,
  title={Learning and Forgetting Unsafe Examples in Large Language Models},
  author={Zhao, Jiachen and Deng, Zhun and Madras, David and Zou, James and Ren, Mengye},
  journal={arXiv preprint arXiv:2312.12736},
  year={2023}
}

@misc{moosavidezfooli2018robustness,
      title={Robustness via curvature regularization, and vice versa}, 
      author={Seyed-Mohsen Moosavi-Dezfooli and Alhussein Fawzi and Jonathan Uesato and Pascal Frossard},
      year={2018},
      eprint={1811.09716},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{xu2023instructions,
      title={Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models}, 
      author={Jiashu Xu and Mingyu Derek Ma and Fei Wang and Chaowei Xiao and Muhao Chen},
      year={2023},
      eprint={2305.14710},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bhardwaj2024language,
      title={Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic}, 
      author={Rishabh Bhardwaj and Do Duc Anh and Soujanya Poria},
      year={2024},
      eprint={2402.11746},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{pelrine2023exploiting,
  title={Exploiting novel gpt-4 apis},
  author={Pelrine, Kellin and Taufeeque, Mohammad and Zajc, Michal and McLean, Euan and Gleave, Adam},
  journal={arXiv preprint arXiv:2312.14302},
  year={2023}
}

@article{zhan2023removing,
  title={Removing rlhf protections in gpt-4 via fine-tuning},
  author={Zhan, Qiusi and Fang, Richard and Bindu, Rohan and Gupta, Akul and Hashimoto, Tatsunori and Kang, Daniel},
  journal={arXiv preprint arXiv:2311.05553},
  year={2023}
}

@article{wan2023poisoning,
  title={Poisoning Language Models During Instruction Tuning},
  author={Wan, Alexander and Wallace, Eric and Shen, Sheng and Klein, Dan},
  year={2023}
}

@inproceedings{shu2023exploitability,
  title={On the Exploitability of Instruction Tuning},
  author={Shu, Manli and Wang, Jiongxiao and Zhu, Chen and Geiping, Jonas and Xiao, Chaowei and Goldstein, Tom},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}


@inproceedings{rando2023universal,
  title={Universal Jailbreak Backdoors from Poisoned Human Feedback},
  author={Rando, Javier and Tram{\`e}r, Florian},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{Bagdasaryan_2022,
   title={Spinning Language Models: Risks of Propaganda-As-A-Service and Countermeasures},
   url={http://dx.doi.org/10.1109/SP46214.2022.9833572},
   DOI={10.1109/sp46214.2022.9833572},
   booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
   publisher={IEEE},
   author={Bagdasaryan, Eugene and Shmatikov, Vitaly},
   year={2022},
   month=may }


@misc{chan2023hazards,
      title={Hazards from Increasingly Accessible Fine-Tuning of Downloadable Foundation Models}, 
      author={Alan Chan and Ben Bucknall and Herbie Bradley and David Krueger},
      year={2023},
      eprint={2312.14751},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gopal2023releasing,
      title={Will releasing the weights of future large language models grant widespread access to pandemic agents?}, 
      author={Anjali Gopal and Nathan Helm-Burger and Lennart Justen and Emily H. Soice and Tiffany Tzeng and Geetha Jeyapragasan and Simon Grimm and Benjamin Mueller and Kevin M. Esvelt},
      year={2023},
      eprint={2310.18233},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{shen2023large,
      title={Large Language Model Alignment: A Survey}, 
      author={Tianhao Shen and Renren Jin and Yufei Huang and Chuang Liu and Weilong Dong and Zishan Guo and Xinwei Wu and Yan Liu and Deyi Xiong},
      year={2023},
      eprint={2309.15025},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mazeika2024harmbench,
      title={HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal}, 
      author={Mantas Mazeika and Long Phan and Xuwang Yin and Andy Zou and Zifan Wang and Norman Mu and Elham Sakhaee and Nathaniel Li and Steven Basart and Bo Li and David Forsyth and Dan Hendrycks},
      year={2024},
      eprint={2402.04249},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{deng2024sophon,
      title={{SOPHON}: Non-Fine-Tunable Learning to Restrain Task Transferability For Pre-trained Models}, 
      author={Jiangyi Deng and Shengyuan Pang and Yanjiao Chen and Liangming Xia and Yijie Bai and Haiqin Weng and Wenyuan Xu},
      year={2024},
      eprint={2404.12699},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Tarun_2024,
   title={Fast Yet Effective Machine Unlearning},
   ISSN={2162-2388},
   url={http://dx.doi.org/10.1109/TNNLS.2023.3266233},
   DOI={10.1109/tnnls.2023.3266233},
   journal={IEEE Transactions on Neural Networks and Learning Systems},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Tarun, Ayush K. and Chundawat, Vikram S. and Mandal, Murari and Kankanhalli, Mohan},
   year={2024},
   pages={1–10} }


@misc{achille2019dynamics,
      title={Dynamics and Reachability of Learning Tasks}, 
      author={Alessandro Achille and Glen Mbeng and Stefano Soatto},
      year={2019},
      eprint={1810.02440},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}





@misc{tishby2000information,
      title={The information bottleneck method}, 
      author={Naftali Tishby and Fernando C. Pereira and William Bialek},
      year={2000},
      eprint={physics/0004057},
      archivePrefix={arXiv},
      primaryClass={physics.data-an}
}

@misc{rosati2024immunization,
      title={Immunization against harmful fine-tuning attacks}, 
      author={Domenic Rosati and Jan Wehner and Kai Williams and Łukasz Bartoszcze and Jan Batzner and Hassan Sajjad and Frank Rudzicz},
      year={2024},
      eprint={2402.16382},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{casper2024defending,
      title={Defending Against Unforeseen Failure Modes with Latent Adversarial Training}, 
      author={Stephen Casper and Lennart Schulze and Oam Patel and Dylan Hadfield-Menell},
      year={2024},
      eprint={2403.05030},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{jain2023baseline,
      title={Baseline Defenses for Adversarial Attacks Against Aligned Language Models}, 
      author={Neel Jain and Avi Schwarzschild and Yuxin Wen and Gowthami Somepalli and John Kirchenbauer and Ping-yeh Chiang and Micah Goldblum and Aniruddha Saha and Jonas Geiping and Tom Goldstein},
      year={2023},
      eprint={2309.00614},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}