\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[A.~Erdogdu and Montanari(2015)]{Monta15}
Murat A.~Erdogdu and Andrea Montanari.
\newblock Convergence rates of sub-sampled {N}ewton methods.
\newblock Technical Report 1508.02810, ArXiv, 2015.

\bibitem[Agarwal et~al.(2017)Agarwal, Bullins, and Hazan]{Agarwal2017}
Naman Agarwal, Brian Bullins, and Elad Hazan.
\newblock Second-order stochastic optimization for machine learning in linear
  time.
\newblock \emph{J. Mach. Learn. Res.}, 18\penalty0 (1):\penalty0 4148--4187,
  January 2017.

\bibitem[Alaoui and Mahoney(2015)]{alaoui2015fast}
Ahmed Alaoui and Michael~W Mahoney.
\newblock Fast randomized kernel ridge regression with statistical guarantees.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  775--783, 2015.

\bibitem[Allen-Zhu(2017)]{Allen2017}
Zeyuan Allen-Zhu.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock In \emph{Proceedings of the Symposium on Theory of Computing}, pages
  1200--1205, 2017.

\bibitem[Aronszajn(1950)]{aronszajn1950theory}
Nachman Aronszajn.
\newblock Theory of reproducing kernels.
\newblock \emph{Transactions of the American Mathematical Society}, 68\penalty0
  (3):\penalty0 337--404, 1950.

\bibitem[Bach(2010)]{bach2010self}
Francis Bach.
\newblock Self-concordant analysis for logistic regression.
\newblock \emph{Electronic Journal of Statistics}, 4:\penalty0 384--414, 2010.

\bibitem[Bach(2014)]{bach2014adaptivity}
Francis Bach.
\newblock Adaptivity of averaged stochastic gradient descent to local strong
  convexity for logistic regression.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 595--627, 2014.

\bibitem[Bollapragada et~al.(2018)Bollapragada, Byrd, and
  Nocedal]{bollapragada2018exact}
Raghu Bollapragada, Richard~H. Byrd, and Jorge Nocedal.
\newblock Exact and inexact subsampled newton methods for optimization.
\newblock \emph{IMA Journal of Numerical Analysis}, 39\penalty0 (2):\penalty0
  545--578, 2018.

\bibitem[Bottou and Bousquet(2008)]{bottou2008tradeoffs}
L{\'e}on Bottou and Olivier Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  161--168, 2008.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
L{\'e}on Bottou, Frank~E. Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{Siam Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Boutsidis and Gittens(2013)]{boutsidis2013improved}
Christos Boutsidis and Alex Gittens.
\newblock Improved matrix algorithms via the subsampled randomized hadamard
  transform.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 34\penalty0
  (3):\penalty0 1301--1340, 2013.

\bibitem[Boyd and Vandenberghe(2004)]{boyd2004convex}
Stephen Boyd and Lieven Vandenberghe.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Caponnetto and De~Vito(2007)]{devito}
A.~Caponnetto and E.~De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Found. Comput. Math.}, 7\penalty0 (3):\penalty0 331--368, July
  2007.

\bibitem[Defazio(2016)]{defazio2016simple}
Aaron Defazio.
\newblock A simple practical accelerated method for finite sums.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  676--684, 2016.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing systems}, pages
  1646--1654, 2014.

\bibitem[Deuflhard(2011)]{Deuflhard2011}
Peter Deuflhard.
\newblock \emph{Newton Methods for Nonlinear Problems: Affine Invariance and
  Adaptive Algorithms}.
\newblock Springer, 2011.

\bibitem[Drineas et~al.(2011)Drineas, Mahoney, Muthukrishnan, and
  Sarl{\'o}s]{drineas2011faster}
Petros Drineas, Michael~W Mahoney, Shan Muthukrishnan, and Tam{\'a}s
  Sarl{\'o}s.
\newblock Faster least squares approximation.
\newblock \emph{Numerische mathematik}, 117\penalty0 (2):\penalty0 219--249,
  2011.

\bibitem[Drineas et~al.(2012)Drineas, Magdon-Ismail, Mahoney, and
  Woodruff]{drineas2012fast}
Petros Drineas, Malik Magdon-Ismail, Michael~W Mahoney, and David~P Woodruff.
\newblock Fast approximation of matrix coherence and statistical leverage.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Dec):\penalty0 3475--3506, 2012.

\bibitem[Golub and Van~Loan(2012)]{golub2012matrix}
Gene~H. Golub and Charles~F. Van~Loan.
\newblock \emph{Matrix Computations}, volume~3.
\newblock JHU Press, 2012.

\bibitem[Gower et~al.(2018)Gower, Hanzely, Richt{\'a}rik, and
  Stich]{gower2018accelerated}
Robert Gower, Filip Hanzely, Peter Richt{\'a}rik, and Sebastian~U. Stich.
\newblock Accelerated stochastic matrix inversion: general theory and speeding
  up {BFGS} rules for faster second-order optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1619--1629, 2018.

\bibitem[Karimireddy et~al.(2018)Karimireddy, Stich, and Jaggi]{jaggi18}
Sai~Praneeth Karimireddy, Sebastian~U. Stich, and Martin Jaggi.
\newblock Global linear convergence of newton's method without strong-convexity
  or lipschitz gradients.
\newblock \emph{CoRR}, abs/1806.00413, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.00413}.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{lin2015universal}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3384--3392, 2015.

\bibitem[Marteau-Ferey et~al.(2019)Marteau-Ferey, Ostrovskii, Bach, and
  Rudi]{marteau2019}
Ulysse Marteau-Ferey, Dmitrii Ostrovskii, Francis Bach, and Alessandro Rudi.
\newblock Beyond least-squares: Fast rates for regularized empirical risk
  minimization through self-concordance.
\newblock In \emph{Proceedings of the Conference on Computational Learning
  Theory}, 2019.

\bibitem[Nemirovskii and Nesterov(1994)]{nemirovskii1994interior}
Arkadii Nemirovskii and Yurii Nesterov.
\newblock Interior-point polynomial algorithms in convex programming.
\newblock \emph{Society for Industrial and Applied Mathematics}, 1994.

\bibitem[Pilanci and Wainwright(2017)]{pilanci2017newton}
Mert Pilanci and Martin~J Wainwright.
\newblock Newton sketch: A near linear-time optimization algorithm with
  linear-quadratic convergence.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (1):\penalty0
  205--245, 2017.

\bibitem[Rahimi and Recht(2008)]{rahimi2008random}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1177--1184, 2008.

\bibitem[Roosta{-}Khorasani and Mahoney(2019)]{RoostaKhorasaniMahoney19}
Farbod Roosta{-}Khorasani and Michael~W. Mahoney.
\newblock Sub-sampled {N}ewton methods.
\newblock \emph{Math. Program.}, 174\penalty0 (1-2):\penalty0 293--326, 2019.

\bibitem[Rudi et~al.(2015)Rudi, Camoriano, and Rosasco]{Rudi15}
Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco.
\newblock Less is more: {N}ystr\"{o}m computational regularization.
\newblock In \emph{Advances in Neural Information Processing Systems 28}, pages
  1657--1665. 2015.

\bibitem[Rudi et~al.(2017)Rudi, Carratino, and Rosasco]{Rudi17}
Alessandro Rudi, Luigi Carratino, and Lorenzo Rosasco.
\newblock {FALKON}: An optimal large scale kernel method.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pages
  3888--3898. 2017.

\bibitem[Rudi et~al.(2018)Rudi, Calandriello, Carratino, and
  Rosasco]{rudi2018fast}
Alessandro Rudi, Daniele Calandriello, Luigi Carratino, and Lorenzo Rosasco.
\newblock On fast leverage score sampling and optimal learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5672--5682, 2018.

\bibitem[Saad(2003)]{Saad03}
Y.~Saad.
\newblock \emph{Iterative Methods for Sparse Linear Systems}.
\newblock Society for Industrial and Applied Mathematics, Philadelphia, PA,
  USA, 2nd edition, 2003.

\bibitem[Shawe-Taylor and Cristianini(2004)]{shawe2004kernel}
John Shawe-Taylor and Nello Cristianini.
\newblock \emph{Kernel Methods for Pattern Analysis}.
\newblock Cambridge University Press, 2004.

\bibitem[Sutton and McCallum(2012)]{sutton2012introduction}
Charles Sutton and Andrew McCallum.
\newblock An introduction to conditional random fields.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  4\penalty0 (4):\penalty0 267--373, 2012.

\bibitem[Williams and Seeger(2001)]{williams2001using}
Christopher K.~I. Williams and Matthias Seeger.
\newblock Using the {N}ystr{\"o}m method to speed up kernel machines.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  682--688, 2001.

\end{thebibliography}
