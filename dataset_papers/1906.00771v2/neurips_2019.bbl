\begin{thebibliography}{10}

\bibitem{Anderson2017}
Alexander~G. Anderson and Cory~P. Berg.
\newblock {The High-Dimensional Geometry of Binary Neural Networks}.
\newblock {\em ICLR}, (2014):1--13, 2018.

\bibitem{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock {\em arXiv preprint arXiv:1904.11955}, 2019.

\bibitem{banner2018scalable}
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry.
\newblock Scalable methods for 8-bit training of neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5145--5153, 2018.

\bibitem{chen2018dynamical}
Minmin Chen, Jeffrey Pennington, and Samuel~S Schoenholz.
\newblock Dynamical isometry and a mean field theory of rnns: Gating enables
  signal propagation in recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1806.05394}, 2018.

\bibitem{chen2015compressing}
Wenlin Chen, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen.
\newblock Compressing neural networks with the hashing trick.
\newblock In {\em International Conference on Machine Learning}, pages
  2285--2294, 2015.

\bibitem{courbariaux2016binarized}
Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Binarized neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 2016.

\bibitem{das2018mixed}
Dipankar Das, Naveen Mellempudi, Dheevatsa Mudigere, Dhiraj Kalamkar, Sasikanth
  Avancha, Kunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan, Bharat
  Kaul, Evangelos Georganas, et~al.
\newblock Mixed precision training of convolutional neural networks using
  integer operations.
\newblock {\em arXiv preprint arXiv:1802.00930}, 2018.

\bibitem{gilboa2019dynamical}
Dar Gilboa, Bo~Chang, Minmin Chen, Greg Yang, Samuel~S Schoenholz, Ed~H Chi,
  and Jeffrey Pennington.
\newblock Dynamical isometry and a mean field theory of lstms and grus.
\newblock {\em arXiv preprint arXiv:1901.08987}, 2019.

\bibitem{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256, 2010.

\bibitem{gupta2015deep}
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
\newblock Deep learning with limited numerical precision.
\newblock In {\em International Conference on Machine Learning}, pages
  1737--1746, 2015.

\bibitem{hinton2012neural}
G~Hinton.
\newblock Neural networks for machine learning. coursera,[video lectures],
  2012.

\bibitem{howard2017mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{hubara2017quantized}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Quantized neural networks: Training neural networks with low
  precision weights and activations.
\newblock {\em The Journal of Machine Learning Research}, 18(1):6869--6898,
  2017.

\bibitem{Jacot2018-dv}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock June 2018.

\bibitem{lee2017deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S Schoenholz, Jeffrey
  Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock {\em arXiv preprint arXiv:1711.00165}, 2017.

\bibitem{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel~S Schoenholz, Yasaman Bahri, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock {\em arXiv preprint arXiv:1902.06720}, 2019.

\bibitem{Li2017b}
Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein.
\newblock {Training Quantized Nets: A Deeper Understanding}.
\newblock {\em NIPS}, jun 2017.

\bibitem{lin2017towards}
Xiaofan Lin, Cong Zhao, and Wei Pan.
\newblock Towards accurate binary convolutional neural network.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  345--353, 2017.

\bibitem{Maass1998}
Wolfgang Maass and Pekka Orponen.
\newblock {On the Effect of Analog Noise in Discrete-Time Analog Computations}.
\newblock {\em Neural Computation}, 10(5):1071--1095, jul 1998.

\bibitem{matthews2018gaussian}
Alexander G de~G Matthews, Mark Rowland, Jiri Hron, Richard~E Turner, and
  Zoubin Ghahramani.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock {\em arXiv preprint arXiv:1804.11271}, 2018.

\bibitem{mishra2017wrpn}
Asit Mishra, Eriko Nurvitadhi, Jeffrey~J Cook, and Debbie Marr.
\newblock Wrpn: wide reduced-precision networks.
\newblock {\em arXiv preprint arXiv:1709.01134}, 2017.

\bibitem{miyashita2016convolutional}
Daisuke Miyashita, Edward~H Lee, and Boris Murmann.
\newblock Convolutional neural networks using logarithmic data representation.
\newblock {\em arXiv preprint arXiv:1603.01025}, 2016.

\bibitem{pennington2017resurrecting}
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli.
\newblock Resurrecting the sigmoid in deep learning through dynamical isometry:
  theory and practice.
\newblock In {\em Advances in neural information processing systems}, pages
  4785--4795, 2017.

\bibitem{poole2016exponential}
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya
  Ganguli.
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In {\em Advances in neural information processing systems}, pages
  3360--3368, 2016.

\bibitem{rastegari2016xnor}
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In {\em European Conference on Computer Vision}, pages 525--542.
  Springer, 2016.

\bibitem{schoenholz2016deep}
Samuel~S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein.
\newblock Deep information propagation.
\newblock {\em arXiv preprint arXiv:1611.01232}, 2016.

\bibitem{Siegelmann1991}
Hava~T. Siegelmann and Eduardo~D. Sontag.
\newblock {Turing computability with neural nets}.
\newblock {\em Applied Mathematics Letters}, 4(6):77--80, jan 1991.

\bibitem{wang2018training}
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash
  Gopalakrishnan.
\newblock Training deep neural networks with 8-bit floating point numbers.
\newblock In {\em Advances in neural information processing systems}, pages
  7675--7684, 2018.

\bibitem{wu2018deterministic}
Anqi Wu, Sebastian Nowozin, Edward Meeds, Richard~E. Turner, Jose~Miguel
  Hernandez-Lobato, and Alexander~L. Gaunt.
\newblock Deterministic variational inference for robust bayesian neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{xiao2018dynamical}
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel~S Schoenholz, and
  Jeffrey Pennington.
\newblock Dynamical isometry and a mean field theory of cnns: How to train
  10,000-layer vanilla convolutional neural networks.
\newblock {\em arXiv preprint arXiv:1806.05393}, 2018.

\bibitem{yang2019mean}
Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl-Dickstein, and Samuel~S
  Schoenholz.
\newblock A mean field theory of batch normalization.
\newblock {\em arXiv preprint arXiv:1902.08129}, 2019.

\bibitem{yang2017mean}
Greg Yang and Samuel Schoenholz.
\newblock Mean field residual networks: On the edge of chaos.
\newblock In {\em Advances in neural information processing systems}, pages
  7103--7114, 2017.

\bibitem{Yin2019}
Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack
  Xin.
\newblock {Understanding straight-through estimator in training activation
  quantized neural nets}.
\newblock {\em ICLR}, pages 1--30, 2019.

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530}, 2016.

\bibitem{zhang2019fixup}
Hongyi Zhang, Yann~N Dauphin, and Tengyu Ma.
\newblock Fixup initialization: Residual learning without normalization.
\newblock {\em arXiv preprint arXiv:1901.09321}, 2019.

\bibitem{zhou2018adaptive}
Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, and Pascal
  Frossard.
\newblock Adaptive quantization for deep neural network.
\newblock In {\em Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\end{thebibliography}
