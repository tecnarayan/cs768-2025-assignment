\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng]{Abadi2015}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A.,
  Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
  Levenberg, J., Man, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster,
  M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Vi, F., Vinyals, O., Warden, P., Wattenberg,
  M., Wicke, M., Yu, Y., and Zheng, X.
\newblock \emph{{TensorFlow}: Large-Scale Machine Learning on Heterogeneous
  Systems}, 2015.
\newblock Software available from tensorflow.org.

\bibitem[Asi \& Duchi(2019)Asi and Duchi]{Asi2019}
Asi, H. and Duchi, J.~C.
\newblock Stochastic (approximate) proximal point methods: Convergence,
  optimality, and adaptivity.
\newblock \emph{SIAM Journal on Optimization}, 2019.

\bibitem[Baydin et~al.(2018)Baydin, Cornish, Rubio, Schmidt, and
  Wood]{Baydin2018}
Baydin, A.~G., Cornish, R., Rubio, D.~M., Schmidt, M., and Wood, F.
\newblock Online learning rate adaptation with hypergradient descent.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and
  Anandkumar]{Bernstein2018}
Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anandkumar, A.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock \emph{International Conference on Machine Learning}, 2018.

\bibitem[Berrada et~al.(2019)Berrada, Zisserman, and Kumar]{Berrada2019}
Berrada, L., Zisserman, A., and Kumar, M.~P.
\newblock Deep {Frank-Wolfe} for neural network optimization.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Bowman et~al.(2015)Bowman, Angeli, Potts, and Manning]{Bowman2015}
Bowman, S.~R., Angeli, G., Potts, C., and Manning, C.~D.
\newblock A large annotated corpus for learning natural language inference.
\newblock \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2015.

\bibitem[Br{\"a}nnlund(1995)]{Brannlund1995}
Br{\"a}nnlund, U.
\newblock A generalized subgradient method with relaxation step.
\newblock \emph{Mathematical Programming}, 1995.

\bibitem[Bubeck(2015)]{Bubeck2015}
Bubeck, S.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends in Machine Learning}, 2015.

\bibitem[Chen \& Gu(2018)Chen and Gu]{Chen2018}
Chen, J. and Gu, Q.
\newblock Padam: Closing the generalization gap of adaptive gradient methods in
  training deep neural networks.
\newblock \emph{arXiv preprint}, 2018.

\bibitem[Chen et~al.(2019)Chen, Liu, Sun, and Hong]{Chen2019}
Chen, X., Liu, S., Sun, R., and Hong, M.
\newblock On the convergence of a class of adam-type algorithms for non-convex
  optimization.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Conneau et~al.(2017)Conneau, Kiela, Schwenk, Barrault, and
  Bordes]{Conneau2017}
Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bordes, A.
\newblock Supervised learning of universal sentence representations from
  natural language inference data.
\newblock \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2017.

\bibitem[D{\'e}fossez \& Bach(2017)D{\'e}fossez and Bach]{Defossez2017}
D{\'e}fossez, A. and Bach, F.
\newblock Adabatch: Efficient gradient aggregation rules for sequential and
  parallel stochastic gradient methods.
\newblock \emph{arXiv preprint}, 2017.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{Duchi2011}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 2011.

\bibitem[Frank \& Wolfe(1956)Frank and Wolfe]{Frank1956}
Frank, M. and Wolfe, P.
\newblock An algorithm for quadratic programming.
\newblock \emph{Naval Research Logistics Quarterly}, 1956.

\bibitem[Graves et~al.(2016)Graves, Wayne, Reynolds, Harley, Danihelka,
  Grabska-Barwińska, Colmenarejo, Grefenstette, Ramalho, Agapiou, Badia,
  Hermann, Zwols, Ostrovski, Cain, King, Summerfield, Blunsom, Kavukcuoglu, and
  Hassabis]{Graves2016}
Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I.,
  Grabska-Barwińska, A., Colmenarejo, S.~G., Grefenstette, E., Ramalho, T.,
  Agapiou, J., Badia, A.~P., Hermann, K.~M., Zwols, Y., Ostrovski, G., Cain,
  A., King, H., Summerfield, C., Blunsom, P., Kavukcuoglu, K., and Hassabis, D.
\newblock Hybrid computing using a neural network with dynamic external memory.
\newblock \emph{Nature}, 2016.

\bibitem[Hazan \& Kakade(2019)Hazan and Kakade]{Hazan2019}
Hazan, E. and Kakade, S.
\newblock Revisiting the polyak step size.
\newblock \emph{arXiv preprint}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He2016}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{Conference on Computer Vision and Pattern Recognition}, 2016.

\bibitem[Henriques et~al.(2019)Henriques, Ehrhardt, Albanie, and
  Vedaldi]{Henriques2019}
Henriques, J.~F., Ehrhardt, S., Albanie, S., and Vedaldi, A.
\newblock Small steps and giant leaps: Minimal newton solvers for deep
  learning.
\newblock \emph{International Conference on Computer Vision}, 2019.

\bibitem[Huang et~al.(2017)Huang, Liu, Weinberger, and van~der
  Maaten]{Huang2017a}
Huang, G., Liu, Z., Weinberger, K.~Q., and van~der Maaten, L.
\newblock Densely connected convolutional networks.
\newblock \emph{Conference on Computer Vision and Pattern Recognition}, 2017.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{Kingma2015}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{International Conference on Learning Representations}, 2015.

\bibitem[Lacoste-Julien \& Jaggi(2013)Lacoste-Julien and
  Jaggi]{Lacoste-Julien2013}
Lacoste-Julien, S. and Jaggi, M.
\newblock Block-coordinate {F}rank-{W}olfe optimization for structural {SVMs}.
\newblock \emph{International Conference on Machine Learning}, 2013.

\bibitem[Lapin et~al.(2016)Lapin, Hein, and Schiele]{Lapin2016}
Lapin, M., Hein, M., and Schiele, B.
\newblock Loss functions for top-k error: Analysis and insights.
\newblock \emph{Conference on Computer Vision and Pattern Recognition}, 2016.

\bibitem[Levy(2017)]{Levy2017}
Levy, K.
\newblock Online to offline conversions, universality and adaptive minibatch
  sizes.
\newblock \emph{Neural Information Processing Systems}, 2017.

\bibitem[Li \& Orabona(2019)Li and Orabona]{Li2019}
Li, X. and Orabona, F.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock \emph{International Conference on Artificial Intelligence and
  Statistics}, 2019.

\bibitem[Liu \& Belkin(2019)Liu and Belkin]{Liu2019b}
Liu, C. and Belkin, M.
\newblock Accelerating sgd with momentum for over-parameterized learning.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Liu et~al.(2019)Liu, Jiang, He, Chen, Liu, Gao, and Han]{Liu2019a}
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock \emph{arXiv preprint}, 2019.

\bibitem[Locatello et~al.(2017)Locatello, Khanna, Tschannen, and
  Jaggi]{Locatello2017}
Locatello, F., Khanna, R., Tschannen, M., and Jaggi, M.
\newblock A unified optimization view on generalized matching pursuit and
  frank-wolfe.
\newblock \emph{International Conference on Artificial Intelligence and
  Statistics}, 2017.

\bibitem[Loizou et~al.(2020)Loizou, Vaswani, Laradji, and
  Lacoste-Julien]{Loizou2020}
Loizou, N., Vaswani, S., Laradji, I., and Lacoste-Julien, S.
\newblock Stochastic polyak step-size for sgd: An adaptive learning rate for
  fast convergence.
\newblock \emph{arXiv preprint}, 2020.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{Loshchilov2019}
Loshchilov, I. and Hutter, F.
\newblock Fixing weight decay regularization in adam.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Luo et~al.(2019)Luo, Xiong, Liu, and Sun]{Luo2019}
Luo, L., Xiong, Y., Liu, Y., and Sun, X.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Ma et~al.(2018)Ma, Bassily, and Belkin]{Ma2018a}
Ma, S., Bassily, R., and Belkin, M.
\newblock The power of interpolation: Understanding the effectiveness of sgd in
  modern over-parametrized learning.
\newblock \emph{International Conference on Machine Learning}, 2018.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{Martens2015}
Martens, J. and Grosse, R.
\newblock Optimizing neural networks with {Kronecker}-factored approximate
  curvature.
\newblock \emph{International Conference on Machine Learning}, 2015.

\bibitem[Mukkamala \& Hein(2017)Mukkamala and Hein]{Mukkamala2017}
Mukkamala, M.~C. and Hein, M.
\newblock Variants of rmsprop and adagrad with logarithmic regret bounds.
\newblock \emph{International Conference on Machine Learning}, 2017.

\bibitem[Nedi{\'c} \& Bertsekas(2001{\natexlab{a}})Nedi{\'c} and
  Bertsekas]{Nedic2001}
Nedi{\'c}, A. and Bertsekas, D.
\newblock Convergence rate of incremental subgradient algorithms.
\newblock \emph{Stochastic optimization: algorithms and applications},
  2001{\natexlab{a}}.

\bibitem[Nedi{\'c} \& Bertsekas(2001{\natexlab{b}})Nedi{\'c} and
  Bertsekas]{Nedic2001a}
Nedi{\'c}, A. and Bertsekas, D.
\newblock Incremental subgradient methods for nondifferentiable optimization.
\newblock \emph{SIAM Journal on Optimization}, 2001{\natexlab{b}}.

\bibitem[Oberman \& Prazeres(2019)Oberman and Prazeres]{Oberman2019}
Oberman, A.~M. and Prazeres, M.
\newblock Stochastic gradient descent with polyak's learning rate.
\newblock \emph{arXiv preprint}, 2019.

\bibitem[Orabona \& P{\'a}l(2015)Orabona and P{\'a}l]{Orabona2015}
Orabona, F. and P{\'a}l, D.
\newblock Scale-free algorithms for online linear optimization.
\newblock \emph{International Conference on Algorithmic Learning Theory}, 2015.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{Paszke2017}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock \emph{NIPS Autodiff Workshop}, 2017.

\bibitem[Polyak(1969)]{Polyak1969}
Polyak, B.~T.
\newblock Minimization of unsmooth functionals.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics}, 1969.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{Reddi2018}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of adam and beyond.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{Robbins1951}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, 1951.

\bibitem[Rolinek \& Martius(2018)Rolinek and Martius]{Rolinek2018}
Rolinek, M. and Martius, G.
\newblock L4: Practical loss-based stepsize adaptation for deep learning.
\newblock \emph{Neural Information Processing Systems}, 2018.

\bibitem[Schaul et~al.(2013)Schaul, Zhang, and LeCun]{Schaul2013}
Schaul, T., Zhang, S., and LeCun, Y.
\newblock No more pesky learning rates.
\newblock \emph{International Conference on Machine Learning}, 2013.

\bibitem[Schneider et~al.(2019)Schneider, Balles, and Hennig]{Schneider2019}
Schneider, F., Balles, L., and Hennig, P.
\newblock Deep{OBS}: A deep learning optimizer benchmark suite.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Shalev-Shwartz \& Zhang(2016)Shalev-Shwartz and
  Zhang]{Shalev-Shwartz2016}
Shalev-Shwartz, S. and Zhang, T.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock \emph{Mathematical Programming}, 2016.

\bibitem[Shazeer \& Stern(2018)Shazeer and Stern]{Shazeer2018}
Shazeer, N. and Stern, M.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock \emph{International Conference on Machine Learning}, 2018.

\bibitem[Shor(1985)]{Shor1985}
Shor, N.~Z.
\newblock \emph{Minimization methods for non-differentiable functions}.
\newblock Springer Series in Computational Mathematics, 1985.

\bibitem[Tan et~al.(2016)Tan, Ma, Dai, and Qian]{Tan2016}
Tan, C., Ma, S., Dai, Y.-H., and Qian, Y.
\newblock Barzilai-borwein step size for stochastic gradient descent.
\newblock \emph{Neural Information Processing Systems}, 2016.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{Tieleman2012}
Tieleman, T. and Hinton, G.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 2012.

\bibitem[Vaswani et~al.(2019{\natexlab{a}})Vaswani, Bach, and
  Schmidt]{Vaswani2019}
Vaswani, S., Bach, F., and Schmidt, M.
\newblock Fast and faster convergence of sgd for over-parameterized models and
  an accelerated perceptron.
\newblock \emph{International Conference on Artificial Intelligence and
  Statistics}, 2019{\natexlab{a}}.

\bibitem[Vaswani et~al.(2019{\natexlab{b}})Vaswani, Mishkin, Laradji, Schmidt,
  Gidel, and Lacoste-Julien]{Vaswani2019a}
Vaswani, S., Mishkin, A., Laradji, I., Schmidt, M., Gidel, G., and
  Lacoste-Julien, S.
\newblock Painless stochastic gradient: Interpolation, line-search, and
  convergence rates.
\newblock \emph{arXiv preprint}, 2019{\natexlab{b}}.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{Wilson2017}
Wilson, A.~C., Roelofs, R., Stern, M., Srebro, N., and Recht, B.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock \emph{Neural Information Processing Systems}, 2017.

\bibitem[Wu et~al.(2018)Wu, Ward, and Bottou]{Wu2018}
Wu, X., Ward, R., and Bottou, L.
\newblock {WNGrad}: Learn the learning rate in gradient descent.
\newblock \emph{arXiv preprint}, 2018.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and Komodakis]{Zagoruyko2016}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{British Machine Vision Conference}, 2016.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and Kumar]{Zaheer2018}
Zaheer, M., Reddi, S., Sachan, D., Kale, S., and Kumar, S.
\newblock Adaptive methods for nonconvex optimization.
\newblock \emph{Neural Information Processing Systems}, 2018.

\bibitem[Zeiler(2012)]{Zeiler2012}
Zeiler, M.
\newblock {ADADELTA:} an adaptive learning rate method.
\newblock \emph{arXiv preprint}, 2012.

\bibitem[Zhang \& Yin(2013)Zhang and Yin]{Zhang2013}
Zhang, H. and Yin, W.
\newblock Gradient methods for convex minimization: better rates under weaker
  conditions.
\newblock \emph{arXiv preprint}, 2013.

\bibitem[Zhang et~al.(2017)Zhang, Wu, and Wang]{Zhang2017a}
Zhang, Z., Wu, Y., and Wang, G.
\newblock Bpgrad: Towards global optimality in deep learning via branch and
  pruning.
\newblock \emph{Conference on Computer Vision and Pattern Recognition}, 2017.

\bibitem[Zheng \& Kwok(2017)Zheng and Kwok]{Zheng2017}
Zheng, S. and Kwok, J.~T.
\newblock Follow the moving leader in deep learning.
\newblock \emph{International Conference on Machine Learning}, 2017.

\bibitem[Zhou et~al.(2019)Zhou, Yang, Zhang, Liang, and Tarokh]{Zhou2019}
Zhou, Y., Yang, J., Zhang, H., Liang, Y., and Tarokh, V.
\newblock Sgd converges to global minimum in deep learning via star-convex
  path.
\newblock \emph{International Conference on Learning Representations}, 2019.

\end{thebibliography}
