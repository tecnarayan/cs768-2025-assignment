\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akinwande et~al.(2023)Akinwande, Jiang, Sam, and
  Kolter]{akinwande2023understanding}
V.~Akinwande, Y.~Jiang, D.~Sam, and J.~Z. Kolter.
\newblock Understanding prompt engineering may not require rethinking
  generalization.
\newblock \emph{arXiv preprint arXiv:2310.03957}, 2023.

\bibitem[Anonymous(2024)]{clonelmauthors}
Anonymous.
\newblock Bayesian optimization of antibodies informed by a generative model of
  evolving sequences.
\newblock \emph{Manuscript}, 2024.

\bibitem[Azuma(1967)]{azuma1967weighted}
K.~Azuma.
\newblock Weighted sums of certain dependent random variables.
\newblock \emph{Tohoku Mathematical Journal, Second Series}, 19\penalty0
  (3):\penalty0 357--367, 1967.

\bibitem[Benesty et~al.(2009)Benesty, Chen, Huang, and
  Cohen]{benesty2009pearson}
J.~Benesty, J.~Chen, Y.~Huang, and I.~Cohen.
\newblock Pearson correlation coefficient.
\newblock In \emph{Noise reduction in speech processing}, pages 37--40.
  Springer, 2009.

\bibitem[Catoni(2007)]{catoni2007pac}
O.~Catoni.
\newblock Pac-bayesian supervised classification: the thermodynamics of
  statistical learning.
\newblock \emph{arXiv preprint arXiv:0712.0248}, 2007.

\bibitem[Chee et~al.(2024)Chee, Cai, Kuleshov, and Sa]{chee2024quip}
J.~Chee, Y.~Cai, V.~Kuleshov, and C.~D. Sa.
\newblock Quip: 2-bit quantization of large language models with guarantees,
  2024.

\bibitem[Chugg et~al.(2023)Chugg, Wang, and Ramdas]{chugg2023unified}
B.~Chugg, H.~Wang, and A.~Ramdas.
\newblock A unified recipe for deriving (time-uniform) pac-bayes bounds.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0
  (372):\penalty0 1--61, 2023.

\bibitem[Computer(2023)]{together2023redpajama}
T.~Computer.
\newblock Redpajama: an open dataset for training large language models, 2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Dao et~al.(2022)Dao, Chen, Sohoni, Desai, Poli, Grogan, Liu, Rao,
  Rudra, and R{\'e}]{dao2022monarch}
T.~Dao, B.~Chen, N.~S. Sohoni, A.~Desai, M.~Poli, J.~Grogan, A.~Liu, A.~Rao,
  A.~Rudra, and C.~R{\'e}.
\newblock Monarch: Expressive structured matrices for efficient and accurate
  training.
\newblock In \emph{International Conference on Machine Learning}, pages
  4690--4721. PMLR, 2022.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Shleifer, and
  Zettlemoyer]{dettmers20228bit}
T.~Dettmers, M.~Lewis, S.~Shleifer, and L.~Zettlemoyer.
\newblock 8-bit optimizers via block-wise quantization, 2022.

\bibitem[Dettmers et~al.(2023)Dettmers, Shmitchell, Roberts, Lee, Brown, Song,
  and Raffel]{dettmers2023qlora}
T.~Dettmers, S.~Shmitchell, A.~Roberts, K.~Lee, T.~B. Brown, D.~Song, and
  C.~Raffel.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{arXiv preprint arXiv:2305.14314}, 2023.

\bibitem[Deutsch(1996)]{gzip}
P.~Deutsch.
\newblock Rfc1952: Gzip file format specification version 4.3, 1996.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017computing}
G.~K. Dziugaite and D.~M. Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[Dziugaite et~al.(2021)Dziugaite, Hsu, Gharbieh, Arpino, and
  Roy]{dziugaite2021role}
G.~K. Dziugaite, K.~Hsu, W.~Gharbieh, G.~Arpino, and D.~Roy.
\newblock On the role of data in pac-bayes bounds.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 604--612. PMLR, 2021.

\bibitem[Edalati et~al.(2022)Edalati, Tahaei, Kobyzev, Nia, Clark, and
  Rezagholizadeh]{edalati2022krona}
A.~Edalati, M.~Tahaei, I.~Kobyzev, V.~P. Nia, J.~J. Clark, and
  M.~Rezagholizadeh.
\newblock Krona: Parameter efficient tuning with kronecker adapter.
\newblock \emph{arXiv preprint arXiv:2212.10650}, 2022.

\bibitem[Frantal et~al.(2022)Frantal, Gruslys, and Kiela]{frantal2022gptq}
Z.~Frantal, A.~Gruslys, and D.~Kiela.
\newblock Gptq: Accurate post-training quantization for generative pre-trained
  transformers.
\newblock \emph{arXiv preprint arXiv:2210.17323}, 2022.

\bibitem[Goldblum et~al.(2023)Goldblum, Finzi, Rowan, and
  Wilson]{goldblum2023no}
M.~Goldblum, M.~Finzi, K.~Rowan, and A.~G. Wilson.
\newblock The no free lunch theorem, kolmogorov complexity, and the role of
  inductive biases in machine learning.
\newblock \emph{arXiv preprint arXiv:2304.05366}, 2023.

\bibitem[Hayou et~al.(2021)Hayou, He, and Dziugaite]{hayou2021probabilistic}
S.~Hayou, B.~He, and G.~K. Dziugaite.
\newblock Probabilistic fine-tuning of pruning masks and pac-bayes self-bounded
  learning.
\newblock \emph{arXiv preprint arXiv:2110.11804}, 2021.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2021lora}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, and
  W.~Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,
  Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
A.~Q. Jiang, A.~Sablayrolles, A.~Mensch, C.~Bamford, D.~S. Chaplot, D.~d.~l.
  Casas, F.~Bressand, G.~Lengyel, G.~Lample, L.~Saulnier, et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Jin et~al.(2023)Jin, Clement, Dong, Nagarajan, Carbin, Ragan-Kelley,
  and Dziugaite]{jin2023cost}
T.~Jin, N.~Clement, X.~Dong, V.~Nagarajan, M.~Carbin, J.~Ragan-Kelley, and
  G.~K. Dziugaite.
\newblock The cost of down-scaling language models: Fact recall deteriorates
  before in-context learning.
\newblock \emph{arXiv preprint arXiv:2310.04680}, 2023.

\bibitem[Kim et~al.(2023)Kim, Lee, Kim, Park, Yoo, Kwon, and
  Lee]{kim2023memory}
J.~Kim, J.~H. Lee, S.~Kim, J.~Park, K.~M. Yoo, S.~J. Kwon, and D.~Lee.
\newblock Memory-efficient fine-tuning of compressed large language models via
  sub-4-bit integer quantization.
\newblock \emph{arXiv preprint arXiv:2305.14152}, 2023.

\bibitem[Kolmogorov(1963)]{kolmogorov1963tables}
A.~N. Kolmogorov.
\newblock On tables of random numbers.
\newblock \emph{Sankhy{\=a}: The Indian Journal of Statistics, Series A}, pages
  369--376, 1963.

\bibitem[Kuznetsov and Mohri(2017)]{kuznetsov2017generalization}
V.~Kuznetsov and M.~Mohri.
\newblock Generalization bounds for non-stationary mixing processes.
\newblock \emph{Machine Learning}, 106\penalty0 (1):\penalty0 93--117, 2017.

\bibitem[Langdon(1984)]{langdon1984introduction}
G.~G. Langdon.
\newblock An introduction to arithmetic coding.
\newblock \emph{IBM Journal of Research and Development}, 28\penalty0
  (2):\penalty0 135--149, 1984.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Allal, Zi, Muennighoff, Kocetkov,
  Mou, Marone, Akiki, Li, Chim, Liu, Zheltonozhskii, Zhuo, Wang, Dehaene,
  Davaadorj, Lamy-Poirier, Monteiro, Shliazhko, Gontier, Meade, Zebaze, Yee,
  Umapathi, Zhu, Lipkin, Oblokulov, Wang, Murthy, Stillerman, Patel,
  Abulkhanov, Zocca, Dey, Zhang, Fahmy, Bhattacharyya, Yu, Singh, Luccioni,
  Villegas, Kunakov, Zhdanov, Romero, Lee, Timor, Ding, Schlesinger,
  Schoelkopf, Ebert, Dao, Mishra, Gu, Robinson, Anderson, Dolan-Gavitt,
  Contractor, Reddy, Fried, Bahdanau, Jernite, Ferrandis, Hughes, Wolf, Guha,
  von Werra, and de~Vries]{li2023starcoder}
R.~Li, L.~B. Allal, Y.~Zi, N.~Muennighoff, D.~Kocetkov, C.~Mou, M.~Marone,
  C.~Akiki, J.~Li, J.~Chim, Q.~Liu, E.~Zheltonozhskii, T.~Y. Zhuo, T.~Wang,
  O.~Dehaene, M.~Davaadorj, J.~Lamy-Poirier, J.~Monteiro, O.~Shliazhko,
  N.~Gontier, N.~Meade, A.~Zebaze, M.-H. Yee, L.~K. Umapathi, J.~Zhu,
  B.~Lipkin, M.~Oblokulov, Z.~Wang, R.~Murthy, J.~Stillerman, S.~S. Patel,
  D.~Abulkhanov, M.~Zocca, M.~Dey, Z.~Zhang, N.~Fahmy, U.~Bhattacharyya, W.~Yu,
  S.~Singh, S.~Luccioni, P.~Villegas, M.~Kunakov, F.~Zhdanov, M.~Romero,
  T.~Lee, N.~Timor, J.~Ding, C.~Schlesinger, H.~Schoelkopf, J.~Ebert, T.~Dao,
  M.~Mishra, A.~Gu, J.~Robinson, C.~J. Anderson, B.~Dolan-Gavitt,
  D.~Contractor, S.~Reddy, D.~Fried, D.~Bahdanau, Y.~Jernite, C.~M. Ferrandis,
  S.~Hughes, T.~Wolf, A.~Guha, L.~von Werra, and H.~de~Vries.
\newblock Starcoder: may the source be with you!, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Ildiz, Papailiopoulos, and
  Oymak]{li2023transformers}
Y.~Li, M.~E. Ildiz, D.~Papailiopoulos, and S.~Oymak.
\newblock Transformers as algorithms: Generalization and stability in
  in-context learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  19565--19594. PMLR, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Xu, Xu, and Zhu]{liu2023llmqat}
Y.~Liu, Q.~Xu, W.~Xu, and J.~Zhu.
\newblock Llm-qat: Data-free quantization aware training for large language
  models.
\newblock \emph{arXiv preprint arXiv:2305.17888}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Qiao, Neiswanger, Wang, Tan, Tao,
  Li, Wang, Sun, Pangarkar, Fan, Gu, Miller, Zhuang, He, Li, Koto, Tang,
  Ranjan, Shen, Ren, Iriondo, Mu, Hu, Schulze, Nakov, Baldwin, and
  Xing]{liu2023llm360}
Z.~Liu, A.~Qiao, W.~Neiswanger, H.~Wang, B.~Tan, T.~Tao, J.~Li, Y.~Wang,
  S.~Sun, O.~Pangarkar, R.~Fan, Y.~Gu, V.~Miller, Y.~Zhuang, G.~He, H.~Li,
  F.~Koto, L.~Tang, N.~Ranjan, Z.~Shen, X.~Ren, R.~Iriondo, C.~Mu, Z.~Hu,
  M.~Schulze, P.~Nakov, T.~Baldwin, and E.~P. Xing.
\newblock Llm360: Towards fully transparent open-source llms,
  2023{\natexlab{b}}.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Lotfi et~al.(2022)Lotfi, Finzi, Kapoor, Potapczynski, Goldblum, and
  Wilson]{lotfi2022pac}
S.~Lotfi, M.~Finzi, S.~Kapoor, A.~Potapczynski, M.~Goldblum, and A.~G. Wilson.
\newblock Pac-bayes compression bounds so tight that they can explain
  generalization.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 31459--31473, 2022.

\bibitem[Lotfi et~al.(2023)Lotfi, Finzi, Kuang, Rudner, Goldblum, and
  Wilson]{lotfi2023non}
S.~Lotfi, M.~Finzi, Y.~Kuang, T.~G. Rudner, M.~Goldblum, and A.~G. Wilson.
\newblock Non-vacuous generalization bounds for large language models.
\newblock \emph{arXiv preprint arXiv:2312.17173}, 2023.

\bibitem[Mohri and Rostamizadeh(2007)]{mohri2007stability}
M.~Mohri and A.~Rostamizadeh.
\newblock Stability bounds for non-iid processes.
\newblock \emph{Advances in Neural Information Processing Systems}, 20, 2007.

\bibitem[Norris(1998)]{norris1998markov}
J.~R. Norris.
\newblock \emph{Markov chains}.
\newblock Number~2. Cambridge university press, 1998.

\bibitem[Olsen et~al.(2022)Olsen, Boyles, and Deane]{Olsen2022-nt}
T.~H. Olsen, F.~Boyles, and C.~M. Deane.
\newblock Observed antibody space: A diverse database of cleaned, annotated,
  and translated unpaired and paired antibody sequences.
\newblock \emph{Protein Sci.}, 31\penalty0 (1):\penalty0 141--146, Jan. 2022.

\bibitem[Park et~al.(2022)Park, Kim, Kim, Choi, Kim, Kim, Lee, Shin, and
  Lee]{park2022lutgemm}
G.~Park, J.~Kim, J.~Kim, E.~Choi, S.~Kim, S.~Kim, M.~Lee, H.~Shin, and J.~Lee.
\newblock Lut-gemm: Quantized matrix multiplication based on luts for efficient
  inference in large-scale generative language model.
\newblock \emph{arXiv preprint arXiv:2206.09557}, 2022.

\bibitem[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli,
  Alobeidli, Pannier, Almazrouei, and Launay]{penedo2023refinedweb}
G.~Penedo, Q.~Malartic, D.~Hesslow, R.~Cojocaru, A.~Cappelli, H.~Alobeidli,
  B.~Pannier, E.~Almazrouei, and J.~Launay.
\newblock The refinedweb dataset for falcon llm: Outperforming curated corpora
  with web data, and web data only, 2023.

\bibitem[P{\'e}rez-Ortiz et~al.(2021)P{\'e}rez-Ortiz, Rivasplata, Shawe-Taylor,
  and Szepesv{\'a}ri]{perez2021tighter}
M.~P{\'e}rez-Ortiz, O.~Rivasplata, J.~Shawe-Taylor, and C.~Szepesv{\'a}ri.
\newblock Tighter risk certificates for neural networks.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (227):\penalty0 1--40, 2021.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rakhlin and Sridharan(2017)]{rakhlin2017equivalence}
A.~Rakhlin and K.~Sridharan.
\newblock On equivalence of martingale tail bounds and deterministic regret
  inequalities.
\newblock In \emph{Conference on Learning Theory}, pages 1704--1722. PMLR,
  2017.

\bibitem[Ralaivola et~al.(2010)Ralaivola, Szafranski, and
  Stempfel]{ralaivola2010chromatic}
L.~Ralaivola, M.~Szafranski, and G.~Stempfel.
\newblock Chromatic pac-bayes bounds for non-iid data: Applications to ranking
  and stationary $\beta$-mixing processes.
\newblock \emph{The Journal of Machine Learning Research}, 11:\penalty0
  1927--1956, 2010.

\bibitem[RelaxML(2024)]{QuIP_code}
C.~RelaxML.
\newblock Quip\#: Quip with lattice codebooks.
\newblock \url{https://github.com/Cornell-RelaxML/quip-sharp}, 2024.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalev2014understanding}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Shuai et~al.(2021)Shuai, Ruffolo, and Gray]{shuai2021generative}
R.~W. Shuai, J.~A. Ruffolo, and J.~J. Gray.
\newblock Generative language modeling for antibody design.
\newblock \emph{bioRxiv}, pages 2021--12, 2021.

\bibitem[Solomonoff(1964)]{solomonoff1964formal}
R.~J. Solomonoff.
\newblock A formal theory of inductive inference. part i.
\newblock \emph{Information and control}, 7\penalty0 (1):\penalty0 1--22, 1964.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
J.~Su, M.~Ahmed, Y.~Lu, S.~Pan, W.~Bo, and Y.~Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,
  Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull,
  Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini,
  Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril,
  Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton,
  Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang,
  Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang,
  Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023LLaMA}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei,
  N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, D.~Bikel, L.~Blecher, C.~C.
  Ferrer, M.~Chen, G.~Cucurull, D.~Esiobu, J.~Fernandes, J.~Fu, W.~Fu,
  B.~Fuller, C.~Gao, V.~Goswami, N.~Goyal, A.~Hartshorn, S.~Hosseini, R.~Hou,
  H.~Inan, M.~Kardas, V.~Kerkez, M.~Khabsa, I.~Kloumann, A.~Korenev, P.~S.
  Koura, M.-A. Lachaux, T.~Lavril, J.~Lee, D.~Liskovich, Y.~Lu, Y.~Mao,
  X.~Martinet, T.~Mihaylov, P.~Mishra, I.~Molybog, Y.~Nie, A.~Poulton,
  J.~Reizenstein, R.~Rungta, K.~Saladi, A.~Schelten, R.~Silva, E.~M. Smith,
  R.~Subramanian, X.~E. Tan, B.~Tang, R.~Taylor, A.~Williams, J.~X. Kuan,
  P.~Xu, Z.~Yan, I.~Zarov, Y.~Zhang, A.~Fan, M.~Kambadur, S.~Narang,
  A.~Rodriguez, R.~Stojnic, S.~Edunov, and T.~Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models, 2023.

\bibitem[Tseng et~al.(2024)Tseng, Chee, Sun, Kuleshov, and
  De~Sa]{tseng2024quip}
A.~Tseng, J.~Chee, Q.~Sun, V.~Kuleshov, and C.~De~Sa.
\newblock Quip\#: Even better llm quantization with hadamard incoherence and
  lattice codebooks.
\newblock \emph{arXiv preprint arXiv:2402.04396}, 2024.

\bibitem[Wang et~al.(2023)Wang, Hu, and Zhang]{Wang2023-vj}
K.~Wang, X.~Hu, and J.~Zhang.
\newblock Fast clonal family inference from large-scale {B} cell repertoire
  sequencing data.
\newblock \emph{Cell Rep Methods}, 3\penalty0 (10):\penalty0 100601, Oct. 2023.

\bibitem[Xu et~al.(2023)Xu, Xu, and Zhu]{xu2023tensorgpt}
Q.~Xu, W.~Xu, and J.~Zhu.
\newblock Tensorgpt: Efficient compression of the embedding layer in llms based
  on the tensor-train decomposition.
\newblock \emph{arXiv preprint arXiv:2307.00526}, 2023.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021.

\bibitem[Zhang and Amini(2022)]{zhang2022generalization}
R.-R. Zhang and M.-R. Amini.
\newblock Generalization bounds for learning under graph-dependence: A survey.
\newblock \emph{arXiv preprint arXiv:2203.13534}, 2022.

\bibitem[Zhou et~al.(2019)Zhou, Veitch, Austern, Adams, and
  Orbanz]{zhou2019non}
W.~Zhou, V.~Veitch, M.~Austern, R.~P. Adams, and P.~Orbanz.
\newblock Non-vacuous generalization bounds at the imagenet scale: a
  pac-bayesian compression approach.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\end{thebibliography}
