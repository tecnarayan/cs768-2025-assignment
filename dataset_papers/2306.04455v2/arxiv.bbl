\begin{thebibliography}{10}

\bibitem{bajaj2016ms}
Payal Bajaj, Daniel Campos, Nick Craswell, Li~Deng, Jianfeng Gao, Xiaodong Liu,
  Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et~al.
\newblock Ms marco: A human generated machine reading comprehension dataset.
\newblock {\em arXiv preprint arXiv:1611.09268}, 2016.

\bibitem{bruch2020stochastic}
Sebastian Bruch, Shuguang Han, Michael Bendersky, and Marc Najork.
\newblock A stochastic treatment of learning to rank scoring functions.
\newblock In {\em Proceedings of the 13th international conference on web
  search and data mining}, pages 61--69, 2020.

\bibitem{bruch2019softmax}
Sebastian Bruch, Xuanhui Wang, Michael Bendersky, and Marc Najork.
\newblock An analysis of the softmax cross entropy loss for learning-to-rank
  with binary relevance.
\newblock In {\em Proceedings of the 2019 ACM SIGIR International Conference on
  Theory of Information Retrieval}, page 75–78, 2019.

\bibitem{burges2010ranknet}
Christopher~J.C. Burges.
\newblock From {RankNet} to {LambdaRank} to {LambdaMART}: An overview.
\newblock Technical Report MSR-TR-2010-82, Microsoft Research, 2010.

\bibitem{cao2007learning}
Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li.
\newblock Learning to rank: from pairwise approach to listwise approach.
\newblock In {\em Proceedings of the 24th international conference on Machine
  learning}, pages 129--136, 2007.

\bibitem{chen2022unbiased}
Gang Chen, Jiawei Chen, Fuli Feng, Sheng Zhou, and Xiangnan He.
\newblock Unbiased knowledge distillation for recommendation.
\newblock {\em arXiv preprint arXiv:2211.14729}, 2022.

\bibitem{istella}
Domenico Dato, Claudio Lucchese, Franco~Maria Nardini, Salvatore Orlando,
  Raffaele Perego, Nicola Tonellotto, and Rossano Venturini.
\newblock Fast ranking with additive ensembles of oblivious and non-oblivious
  regression trees.
\newblock {\em ACM Trans. Inf. Syst.}, 35(2), dec 2016.

\bibitem{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, 2019.

\bibitem{gao2020understanding}
Luyu Gao, Zhuyun Dai, and Jamie Callan.
\newblock Understanding bert rankers under distillation.
\newblock {\em arXiv preprint arXiv:2007.11088}, 2020.

\bibitem{gou2021knowledge}
Jianping Gou, Baosheng Yu, Stephen~J Maybank, and Dacheng Tao.
\newblock Knowledge distillation: A survey.
\newblock {\em International Journal of Computer Vision}, 129(6):1789--1819,
  2021.

\bibitem{haldar2020improving}
Malay Haldar, Prashant Ramanathan, Tyler Sax, Mustafa Abdool, Lanbo Zhang,
  Aamir Mansawala, Shulin Yang, Bradley Turnbull, and Junshuo Liao.
\newblock Improving deep learning for airbnb search.
\newblock In {\em Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 2822--2830, 2020.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2(7), 2015.

\bibitem{jagerman2022optimizing}
Rolf Jagerman, Zhen Qin, Xuanhui Wang, Mike Bendersky, and Marc Najork.
\newblock On optimizing top-k metrics for neural ranking models.
\newblock In {\em Proceedings of the 45th International ACM SIGIR Conference on
  Research and Development in Information Retrieval}, 2022.

\bibitem{kang2020rrd}
SeongKu Kang, Junyoung Hwang, Wonbin Kweon, and Hwanjo Yu.
\newblock De-rrd: A knowledge distillation framework for recommender system.
\newblock In {\em Proceedings of the 29th ACM International Conference on
  Information \& Knowledge Management}, pages 605--614, 2020.

\bibitem{kweon2021bidirectional}
Wonbin Kweon, SeongKu Kang, and Hwanjo Yu.
\newblock Bidirectional distillation for top-k recommender system.
\newblock In {\em Proceedings of the Web Conference 2021}, pages 3861--3871,
  2021.

\bibitem{kwiatkowski2019natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
  Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
  Kenton Lee, et~al.
\newblock Natural questions: a benchmark for question answering research.
\newblock {\em Transactions of the Association for Computational Linguistics},
  7:453--466, 2019.

\bibitem{panli}
Pan Li, Zhen Qin, Xuanhui Wang, and Donald Metzler.
\newblock Combining decision trees and neural networks for learning-to-rank in
  personal search.
\newblock In {\em Proceedings of the 25th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, page 2032–2040, 2019.

\bibitem{lin2021pretrained}
Jimmy Lin, Rodrigo Nogueira, and Andrew Yates.
\newblock Pretrained transformers for text ranking: Bert and beyond.
\newblock {\em Synthesis Lectures on Human Language Technologies},
  14(4):1--325, 2021.

\bibitem{8186875}
Tie-Yan Liu.
\newblock Learning to rank for information retrieval.
\newblock {\em Found. Trends Inf. Retr.}, 2009.

\bibitem{lu2021multi}
Jing Lu, Gustavo~Hern{\'a}ndez {\'A}brego, Ji~Ma, Jianmo Ni, and Yinfei Yang.
\newblock Multi-stage training with improved negative contrast for neural
  passage retrieval.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 6091--6103, 2021.

\bibitem{menon2021statistical}
Aditya~K Menon, Ankit~Singh Rawat, Sashank Reddi, Seungyeon Kim, and Sanjiv
  Kumar.
\newblock A statistical perspective on distillation.
\newblock In {\em International Conference on Machine Learning}, pages
  7632--7642, 2021.

\bibitem{nogueira2020document}
Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin.
\newblock Document ranking with a pretrained sequence-to-sequence model.
\newblock {\em arXiv preprint arXiv:2003.06713}, 2020.

\bibitem{setrank}
Liang Pang, Jun Xu, Qingyao Ai, Yanyan Lan, Xueqi Cheng, and Jirong Wen.
\newblock Setrank: Learning a permutation-invariant ranking model for
  information retrieval.
\newblock In {\em ACM SIGIR Conference on Research and Development in
  Information Retrieval}, page 499–508, 2020.

\bibitem{pasumarthi2019tf}
Rama~Kumar Pasumarthi, Sebastian Bruch, Xuanhui Wang, Cheng Li, Michael
  Bendersky, Marc Najork, Jan Pfeifer, Nadav Golbandi, Rohan Anil, and Stephan
  Wolf.
\newblock {TF-Ranking}: Scalable tensorflow library for learning-to-rank.
\newblock In {\em ACM SIGKDD International Conference on Knowledge Discovery
  and Data Mining}, pages 2970--2978, 2019.

\bibitem{pobrotyn2020context}
Przemys{\l}aw Pobrotyn, Tomasz Bartczak, Miko{\l}aj Synowiec, Rados{\l}aw
  Bia{\l}obrzeski, and Jaros{\l}aw Bojar.
\newblock Context-aware learning to rank with self-attention.
\newblock {\em arXiv preprint arXiv:2005.10084}, 2020.

\bibitem{web30k}
Tao Qin and Tie{-}Yan Liu.
\newblock Introducing {LETOR} 4.0 datasets.
\newblock {\em arXiv preprint arXiv:1306.2597}, 2013.

\bibitem{qin2010general}
Tao Qin, Tie-Yan Liu, and Hang Li.
\newblock A general approximation framework for direct optimization of
  information retrieval measures.
\newblock {\em Information Retrieval}, 13:375--397, August 2010.

\bibitem{qin2021born}
Zhen Qin, Le~Yan, Yi~Tay, Honglei Zhuang, Xuanhui Wang, Michael Bendersky, and
  Marc Najork.
\newblock Born again neural rankers.
\newblock {\em arXiv preprint arXiv:2109.15285}, 2021.

\bibitem{dasalc}
Zhen Qin, Le~Yan, Honglei Zhuang, Yi~Tay, Rama~Kumar Pasumarthi, Xuanhui Wang,
  Michael Bendersky, and Marc Najork.
\newblock Are neural rankers still outperformed by gradient boosted decision
  trees?
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, Peter~J Liu, et~al.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em J. Mach. Learn. Res.}, 21(140):1--67, 2020.

\bibitem{rankdistill}
Sashank Reddi, Rama~Kumar Pasumarthi, Aditya Menon, Ankit~Singh Rawat, Felix
  Yu, Seungyeon Kim, Andreas Veit, and Sanjiv Kumar.
\newblock Rankdistil: Knowledge distillation for ranking.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2368--2376, 2021.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{sun2020generic}
Yang Sun, Fajie Yuan, Min Yang, Guoao Wei, Zhou Zhao, and Duo Liu.
\newblock A generic network compression framework for sequential recommender
  systems.
\newblock In {\em Proceedings of the 43rd International ACM SIGIR Conference on
  Research and Development in Information Retrieval}, pages 1299--1308, 2020.

\bibitem{tang2018ranking}
Jiaxi Tang and Ke~Wang.
\newblock Ranking distillation: Learning compact ranking models with high
  performance for recommender system.
\newblock In {\em Proceedings of the 24th ACM SIGKDD international conference
  on knowledge discovery \& data mining}, pages 2289--2298, 2018.

\bibitem{tay2020long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock {\em arXiv preprint arXiv:2011.04006}, 2020.

\bibitem{wang2020next}
Qinyong Wang, Hongzhi Yin, Tong Chen, Zi~Huang, Hao Wang, Yanchang Zhao, and
  Nguyen~Quoc Viet~Hung.
\newblock Next point-of-interest recommendation on resource-constrained mobile
  devices.
\newblock In {\em Proceedings of the Web conference 2020}, pages 906--916,
  2020.

\bibitem{wang2019survey}
Wei Wang, Vincent~W Zheng, Han Yu, and Chunyan Miao.
\newblock A survey of zero-shot learning: Settings, methods, and applications.
\newblock {\em ACM Transactions on Intelligent Systems and Technology (TIST)},
  10(2):1--37, 2019.

\bibitem{xu2020privileged}
Chen Xu, Quan Li, Junfeng Ge, Jinyang Gao, Xiaoyong Yang, Changhua Pei, Fei
  Sun, Jian Wu, Hanxiao Sun, and Wenwu Ou.
\newblock Privileged features distillation at taobao recommendations.
\newblock In {\em Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 2590--2598, 2020.

\bibitem{yang2022cross}
Chenxiao Yang, Junwei Pan, Xiaofeng Gao, Tingyu Jiang, Dapeng Liu, and Guihai
  Chen.
\newblock Cross-task knowledge distillation in multi-task recommendation.
\newblock {\em arXiv preprint arXiv:2202.09852}, 2022.

\bibitem{yao2022reprbert}
Shaowei Yao, Jiwei Tan, Xi~Chen, Juhao Zhang, Xiaoyi Zeng, and Keping Yang.
\newblock Reprbert: Distilling bert to an efficient representation-based
  relevance model for e-commerce.
\newblock In {\em Proceedings of the 28th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, pages 4363--4371, 2022.

\bibitem{yuan2020revisiting}
Li~Yuan, Francis~EH Tay, Guilin Li, Tao Wang, and Jiashi Feng.
\newblock Revisiting knowledge distillation via label smoothing regularization.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3903--3911, 2020.

\bibitem{youtube}
Zhe Zhao, Lichan Hong, Li~Wei, Jilin Chen, Aniruddh Nath, Shawn Andrews, Aditee
  Kumthekar, Maheswaran Sathiamoorthy, Xinyang Yi, and Ed~Chi.
\newblock Recommending what video to watch next: A multitask ranking system.
\newblock In {\em Proceedings of the 13th ACM Conference on Recommender
  Systems}, page 43–51, 2019.

\bibitem{zhuang2021ensemble}
Honglei Zhuang, Zhen Qin, Shuguang Han, Xuanhui Wang, Michael Bendersky, and
  Marc Najork.
\newblock Ensemble distillation for bert-based ranking models.
\newblock In {\em Proceedings of the 2021 ACM SIGIR International Conference on
  Theory of Information Retrieval}, pages 131--136, 2021.

\bibitem{zhuang2022rankt5}
Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji~Ma, Jing Lu, Jianmo Ni,
  Xuanhui Wang, and Michael Bendersky.
\newblock Rankt5: Fine-tuning t5 for text ranking with ranking losses.
\newblock In {\em Proceedings of the 46th International ACM SIGIR Conference on
  Research and Development in Information Retrieval}, 2023.

\end{thebibliography}
