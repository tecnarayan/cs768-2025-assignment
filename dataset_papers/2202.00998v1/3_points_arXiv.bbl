\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alistarh et~al.(2018)Alistarh, Hoefler, Johansson, Khirirat,
  Konstantinov, and Renggli]{Alistarh-EF-NIPS2018}
Alistarh, D., Hoefler, T., Johansson, M., Khirirat, S., Konstantinov, N., and
  Renggli, C.
\newblock The convergence of sparsified gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Beznosikov et~al.(2020)Beznosikov, Horv{\'a}th, Richt{\'a}rik, and
  Safaryan]{beznosikov2020biased}
Beznosikov, A., Horv{\'a}th, S., Richt{\'a}rik, P., and Safaryan, M.
\newblock On biased compression for distributed learning.
\newblock \emph{arXiv preprint arXiv:2002.12410}, 2020.

\bibitem[Bottou(2012)]{bottou2012stochastic}
Bottou, L.
\newblock \emph{Stochastic Gradient Descent Tricks}, volume 7700 of
  \emph{Lecture Notes in Computer Science (LNCS)}, pp.\  430--445.
\newblock Springer, neural networks, tricks of the trade, reloaded edition,
  January 2012.
\newblock URL
  \url{https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks/}.

\bibitem[Chang \& Lin(2011)Chang and Lin]{chang2011libsvm}
Chang, C.-C. and Lin, C.-J.
\newblock {LIBSVM}: a library for support vector machines.
\newblock \emph{{ACM} {T}ransactions on {I}ntelligent {S}ystems and
  {T}echnology (TIST)}, 2\penalty0 (3):\penalty0 1--27, 2011.

\bibitem[Chen et~al.(2018)Chen, Giannakis, Sun, and Yin]{LAG}
Chen, T., Giannakis, G., Sun, T., and Yin, W.
\newblock {LAG}: Lazily aggregated gradient for communication-efficient
  distributed learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Senior,
  Tucker, Yang, Le, and et~al]{Dean2012}
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Senior, A.,
  Tucker, P., Yang, K., Le, Q.~V., and et~al.
\newblock Large scale distributed deep networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1223--1231, 2012.

\bibitem[Fatkhullin et~al.(2021)Fatkhullin, Sokolov, Gorbunov, Li, and
  Richt\'{a}rik]{EF21BW}
Fatkhullin, I., Sokolov, I., Gorbunov, E., Li, Z., and Richt\'{a}rik, P.
\newblock Ef21 with bells \& whistles: practical algorithmic extensions of
  modern error feedback.
\newblock \emph{arXiv preprint arXiv:2110.03294}, 2021.

\bibitem[Ghadikolaei et~al.(2021)Ghadikolaei, Stich, and Jaggi]{LENA}
Ghadikolaei, H.~S., Stich, S., and Jaggi, M.
\newblock {LENA}: Communication-efficient distributed learning with
  self-triggered gradient uploads.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3943--3951. PMLR, 2021.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Kovalev, Makarenko, and
  Richt\'{a}rik]{Lin_EC_SGD}
Gorbunov, E., Kovalev, D., Makarenko, D., and Richt\'{a}rik, P.
\newblock Linearly converging error compensated {SGD}.
\newblock In \emph{34th Conference on Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Gorbunov et~al.(2021)Gorbunov, Burlachenko, Li, and
  Richt\'arik]{gorbunov2021marina}
Gorbunov, E., Burlachenko, K.~P., Li, Z., and Richt\'arik, P.
\newblock {MARINA}: Faster non-convex distributed learning with compression.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  3788--3798. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/gorbunov21a.html}.

\bibitem[Horv{\'a}th \& Richt{\'a}rik(2020)Horv{\'a}th and
  Richt{\'a}rik]{horvath2020better}
Horv{\'a}th, S. and Richt{\'a}rik, P.
\newblock A better alternative to error feedback for communication-efficient
  distributed learning.
\newblock \emph{arXiv preprint arXiv:2006.11077}, 2020.

\bibitem[Horv\'{a}th \& Richt\'{a}rik(2021)Horv\'{a}th and
  Richt\'{a}rik]{A_better_alternative}
Horv\'{a}th, S. and Richt\'{a}rik, P.
\newblock A better alternative to error feedback for communication-efficient
  distributed learning.
\newblock In \emph{9th International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Rebjock, Stich, and
  Jaggi]{Karimireddy_SignSGD}
Karimireddy, S.~P., Rebjock, Q., Stich, S., and Jaggi, M.
\newblock Error feedback fixes {S}ign{SGD} and other gradient compression
  schemes.
\newblock In \emph{36th International Conference on Machine Learning (ICML)},
  2019.

\bibitem[Khaled et~al.(2020)Khaled, Mishchenko, and
  Richt\'{a}rik]{localSGD-AISTATS2020}
Khaled, A., Mishchenko, K., and Richt\'{a}rik, P.
\newblock Tighter theory for local {SGD} on identical and heterogeneous data.
\newblock In \emph{The 23rd International Conference on Artificial Intelligence
  and Statistics (AISTATS 2020)}, 2020.

\bibitem[Khirirat et~al.(2018)Khirirat, Feyzmahdavian, and Johansson]{DCGD}
Khirirat, S., Feyzmahdavian, H.~R., and Johansson, M.
\newblock Distributed learning with compressed gradients.
\newblock \emph{arXiv preprint arXiv:1806.06573}, 2018.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{ADAM}
Kingma, D.~P. and Ba, J.
\newblock Adam: a method for stochastic optimization.
\newblock In \emph{The 3rd International Conference on Learning
  Representations}, 2014.
\newblock URL \url{https://arxiv.org/pdf/1412.6980.pdf}.

\bibitem[Koloskova et~al.(2020)Koloskova, Lin, Stich, and
  Jaggi]{Koloskova2019DecentralizedDL}
Koloskova, A., Lin, T., Stich, S., and Jaggi, M.
\newblock Decentralized deep learning with arbitrary communication compression.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Kone\v{c}n\'{y} et~al.(2016{\natexlab{a}})Kone\v{c}n\'{y}, McMahan,
  Ramage, and Richt\'{a}rik]{FEDOPT}
Kone\v{c}n\'{y}, J., McMahan, H.~B., Ramage, D., and Richt\'{a}rik, P.
\newblock Federated optimization: distributed machine learning for on-device
  intelligence.
\newblock \emph{arXiv:1610.02527}, 2016{\natexlab{a}}.

\bibitem[Kone\v{c}n\'{y} et~al.(2016{\natexlab{b}})Kone\v{c}n\'{y}, McMahan,
  Yu, Richt\'{a}rik, Suresh, and Bacon]{FEDLEARN}
Kone\v{c}n\'{y}, J., McMahan, H.~B., Yu, F., Richt\'{a}rik, P., Suresh, A.~T.,
  and Bacon, D.
\newblock Federated learning: strategies for improving communication
  efficiency.
\newblock In \emph{NIPS Private Multi-Party Machine Learning Workshop},
  2016{\natexlab{b}}.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{LeCun_MNIST}
LeCun, Y., Cortes, C., and Burges, C.
\newblock Mnist handwritten digit database.
\newblock \emph{ATTLabs [Online]}, 2010.
\newblock URL \url{http://yann.lecun.com/exdb/mnist}.

\bibitem[Li \& Richt\'{a}rik(2020)Li and Richt\'{a}rik]{Nonconvex-sigma_k}
Li, Z. and Richt\'{a}rik, P.
\newblock A unified analysis of stochastic gradient methods for nonconvex
  federated optimization.
\newblock \emph{arXiv preprint arXiv:2006.07013}, 2020.

\bibitem[Li \& Richt{\'a}rik(2021)Li and Richt{\'a}rik]{CANITA}
Li, Z. and Richt{\'a}rik, P.
\newblock {CANITA}: Faster rates for distributed convex optimization with
  communication compression.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock arXiv:2107.09461.

\bibitem[Li et~al.(2020)Li, Kovalev, Qian, and Richt{\'a}rik]{ADIANA}
Li, Z., Kovalev, D., Qian, X., and Richt{\'a}rik, P.
\newblock Acceleration for compressed gradient descent in distributed and
  federated optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  5895--5904. PMLR, 2020.

\bibitem[Li et~al.(2021)Li, Bao, Zhang, and Richt{\'a}rik]{PAGE}
Li, Z., Bao, H., Zhang, X., and Richt{\'a}rik, P.
\newblock {PAGE}: A simple and optimal probabilistic gradient estimator for
  nonconvex optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  6286--6295. PMLR, 2021.

\bibitem[Lin et~al.(2018)Lin, Han, Mao, Wang, and Dally]{lin2018deep}
Lin, Y., Han, S., Mao, H., Wang, Y., and Dally, B.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[McMahan et~al.(2016)McMahan, Moore, Ramage, and Ag\"{u}era~y
  Arcas]{FedAvg2016}
McMahan, B., Moore, E., Ramage, D., and Ag\"{u}era~y Arcas, B.
\newblock Federated learning of deep networks using model averaging.
\newblock \emph{arXiv preprint arXiv:1602.05629}, 2016.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and Ag\"{u}era~y
  Arcas]{FL2017-AISTATS}
McMahan, H.~B., Moore, E., Ramage, D., Hampson, S., and Ag\"{u}era~y Arcas, B.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, 2017.

\bibitem[Mishchenko et~al.(2019)Mishchenko, Gorbunov, Tak{\'a}{\v{c}}, and
  Richt{\'a}rik]{DIANA}
Mishchenko, K., Gorbunov, E., Tak{\'a}{\v{c}}, M., and Richt{\'a}rik, P.
\newblock Distributed learning with compressed gradient differences.
\newblock \emph{arXiv preprint arXiv:1901.09269}, 2019.

\bibitem[Nesterov et~al.(2018)]{nesterov2018lectures}
Nesterov, Y. et~al.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Richt\'{a}rik et~al.(2021)Richt\'{a}rik, Sokolov, and
  Fatkhullin]{EF21}
Richt\'{a}rik, P., Sokolov, I., and Fatkhullin, I.
\newblock {EF21}: A new, simpler, theoretically better, and practically faster
  error feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Safaryan et~al.(2021{\natexlab{a}})Safaryan, Islamov, Qian, and
  Richt\'{a}rik]{FedNL}
Safaryan, M., Islamov, R., Qian, X., and Richt\'{a}rik, P.
\newblock {FedNL}: Making {N}ewton-type methods applicable to federated
  learning.
\newblock \emph{arXiv preprint arXiv:2106.02969}, 2021{\natexlab{a}}.

\bibitem[Safaryan et~al.(2021{\natexlab{b}})Safaryan, Shulgin, and
  Richt\'{a}rik]{UP2021}
Safaryan, M., Shulgin, E., and Richt\'{a}rik, P.
\newblock Uncertainty principle for communication compression in distributed
  and federated learning and the search for an optimal compressor.
\newblock \emph{Information and Inference: A Journal of the IMA},
  2021{\natexlab{b}}.

\bibitem[Seide et~al.(2014)Seide, Fu, Droppo, Li, and Yu]{Seide2014}
Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech {DNN}s.
\newblock In \emph{Fifteenth Annual Conference of the International Speech
  Communication Association}, 2014.

\bibitem[Stich(2020)]{localSGD-Stich}
Stich, S.~U.
\newblock Local {SGD} converges fast and communicates little.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{Stich-EF-NIPS2018}
Stich, S.~U., Cordonnier, J.-B., and Jaggi, M.
\newblock Sparsified {SGD} with memory.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Sun et~al.(2019)Sun, Chen, Giannakis, and Yang]{LAQ}
Sun, J., Chen, T., Giannakis, G., and Yang, Z.
\newblock Communication-efficient distributed learning via lazily aggregated
  quantized gradients.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 3370--3380, 2019.

\bibitem[Szlendak et~al.(2021)Szlendak, Tyurin, and Richt\'{a}rik]{PermK}
Szlendak, R., Tyurin, A., and Richt\'{a}rik, P.
\newblock Permutation compressors for provably faster distributed nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:2110.03300, 2021}, 2021.

\bibitem[Tang et~al.(2020)Tang, Lian, Yu, Zhang, and Liu]{DoubleSqueeze}
Tang, H., Lian, X., Yu, C., Zhang, T., and Liu, J.
\newblock {D}ouble{S}queeze: {P}arallel stochastic gradient descent with
  double-pass error-compensated compression.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, 2020.

\bibitem[Woodworth et~al.(2020)Woodworth, Patel, Stich, Dai, Bullins, McMahan,
  Shamir, and Srebro]{Blake2020}
Woodworth, B., Patel, K.~K., Stich, S.~U., Dai, Z., Bullins, B., McMahan,
  H.~B., Shamir, O., and Srebro, N.
\newblock Is local {SGD} better than minibatch {SGD}?
\newblock \emph{arXiv preprint arXiv:2002.07839}, 2020.

\end{thebibliography}
