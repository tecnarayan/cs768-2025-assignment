\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{DLT{\etalchar{+}}17b}

\bibitem[BG17]{brutzkus2017globally}
Alon Brutzkus and Amir Globerson.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock {\em arXiv preprint arXiv:1702.07966}, 2017.

\bibitem[DLT17a]{du2017convolutional}
Simon~S Du, Jason~D Lee, and Yuandong Tian.
\newblock When is a convolutional filter easy to learn?
\newblock {\em arXiv preprint arXiv:1709.06129}, 2017.

\bibitem[DLT{\etalchar{+}}17b]{du2017gradient}
Simon~S Du, Jason~D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent learns one-hidden-layer cnn: Don't be afraid of
  spurious local minima.
\newblock {\em arXiv preprint arXiv:1712.00779}, 2017.

\bibitem[GK17a]{goel2017eigenvalue}
Surbhi Goel and Adam Klivans.
\newblock Eigenvalue decay implies polynomial-time learnability for neural
  networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2189--2199, 2017.

\bibitem[GK17b]{goel2017learning}
Surbhi Goel and Adam Klivans.
\newblock Learning depth-three neural networks in polynomial time.
\newblock {\em arXiv preprint arXiv:1709.06010}, 2017.

\bibitem[GKKT16]{goel2016reliably}
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler.
\newblock Reliably learning the relu in polynomial time.
\newblock {\em arXiv preprint arXiv:1611.10258}, 2016.

\bibitem[GLM17]{ge2017}
Rong Ge, Jason Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock {\em arXiv preprint arXiv:1711.00501}, 2017.

\bibitem[KKSK11]{kakade2011efficient}
Sham~M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai.
\newblock Efficient learning of generalized linear and single index models with
  isotonic regression.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  927--935, 2011.

\bibitem[KM17]{klivans2017learning}
Adam Klivans and Raghu Meka.
\newblock Learning graphical models using multiplicative weights.
\newblock {\em arXiv preprint arXiv:1706.06274}, 2017.

\bibitem[KS90]{kearns1990efficient}
Michael~J Kearns and Robert~E Schapire.
\newblock Efficient distribution-free learning of probabilistic concepts.
\newblock In {\em Foundations of Computer Science, 1990. Proceedings., 31st
  Annual Symposium on}, pages 382--391. IEEE, 1990.

\bibitem[LY17]{li2017convergence}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  597--607, 2017.

\bibitem[Sol17]{soltanolkotabi2017learning}
Mahdi Soltanolkotabi.
\newblock Learning relus via gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2004--2014, 2017.

\bibitem[Tia16]{tian2016symmetry}
Yuandong Tian.
\newblock Symmetry-breaking convergence analysis of certain two-layered neural
  networks with relu nonlinearity.
\newblock 2016.

\bibitem[Wei03]{weisstein2003gershgorin}
Eric~W Weisstein.
\newblock Gershgorin circle theorem.
\newblock 2003.

\bibitem[ZPSR17]{zhang2017electron}
Qiuyi Zhang, Rina Panigrahy, Sushant Sachdeva, and Ali Rahimi.
\newblock Electron-proton dynamics in deep learning.
\newblock {\em arXiv preprint arXiv:1702.00458}, 2017.

\bibitem[ZSD17]{zhong2017a}
Kai Zhong, Zhao Song, and Inderjit~S Dhillon.
\newblock Learning non-overlapping convolutional neural networks with multiple
  kernels.
\newblock {\em arXiv preprint arXiv:1711.03440}, 2017.

\bibitem[ZSJ{\etalchar{+}}17]{zhong2017recovery}
Kai Zhong, Zhao Song, Prateek Jain, Peter~L Bartlett, and Inderjit~S Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock {\em arXiv preprint arXiv:1706.03175}, 2017.

\end{thebibliography}
