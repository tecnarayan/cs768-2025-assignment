\begin{thebibliography}{}

\bibitem[Aljundi et~al., 2018]{aljundi2018memory}
Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., and Tuytelaars, T.
  (2018).
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 139--154.

\bibitem[Aljundi et~al., 2017]{aljundi2017expert}
Aljundi, R., Chakravarty, P., and Tuytelaars, T. (2017).
\newblock Expert gate: Lifelong learning with a network of experts.
\newblock In {\em CVPR}, pages 7120--7129.

\bibitem[Allen-Zhu et~al., 2019]{allen2019learning}
Allen-Zhu, Z., Li, Y., and Liang, Y. (2019).
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock {\em Advances in neural information processing systems}.

\bibitem[Arora et~al., 2019]{arora2019exact}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R. (2019).
\newblock On exact computation with an infinitely wide neural net.
\newblock In {\em Proceedings of the 33rd International Conference on Neural
  Information Processing Systems}, pages 8141--8150.

\bibitem[Balaji et~al., 2020]{balaji2020effectiveness}
Balaji, Y., Farajtabar, M., Yin, D., Mott, A., and Li, A. (2020).
\newblock The effectiveness of memory replay in large scale continual learning.
\newblock {\em arXiv preprint arXiv:2010.02418}.

\bibitem[Beaulieu et~al., 2020]{beaulieu2020learning}
Beaulieu, S., Frati, L., Miconi, T., Lehman, J., Stanley, K.~O., Clune, J., and
  Cheney, N. (2020).
\newblock Learning to continually learn.
\newblock In {\em {ECAI} 2020 - 24th European Conference on Artificial
  Intelligence}.

\bibitem[Bengio et~al., 1994]{bengio1994learning}
Bengio, Y., Simard, P., and Frasconi, P. (1994).
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock {\em IEEE transactions on neural networks}, 5(2):157--166.

\bibitem[Brown et~al., 2020]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al. (2020).
\newblock Language models are few-shot learners.
\newblock In {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020}.

\bibitem[Chang et~al., 2018]{chang2018}
Chang, M., Gupta, A., Levine, S., and Griffiths, T.~L. (2018).
\newblock Automatically composing representation transformations as a means for
  generalization.
\newblock In {\em ICML workshop Neural Abstract Machines and Program Induction
  v2}.

\bibitem[Chaudhry et~al., 2021]{Chaudhry2019HAL}
Chaudhry, A., Gordo, A., Dokania, P.~K., Torr, P. H.~S., and Lopez-Paz, D.
  (2021).
\newblock Using hindsight to anchor past knowledge in continual learning.
\newblock In {\em Thirty-Fifth {AAAI} Conference on Artificial Intelligence,
  {AAAI} 2021}.

\bibitem[Chaudhry et~al., 2020]{chaudhryOrthog2020}
Chaudhry, A., Khan, N., Dokania, P.~K., and Torr, P.~H. (2020).
\newblock Continual learning in low-rank orthogonal subspaces.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Chaudhry et~al., 2018]{chaudhry2018efficient}
Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M. (2018).
\newblock Efficient lifelong learning with a-gem.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[{Chaudhry} et~al., 2019]{Chaudhry2019OnTE}
{Chaudhry}, A., {Rohrbach}, M., {Elhoseiny}, M., {Ajanthan}, T., {Dokania},
  P.~K., {Torr}, P. H.~S., and {Ranzato}, M. (2019).
\newblock On tiny episodic memories in continual learning.
\newblock {\em arXiv preprint arXiv:1902.10486}.

\bibitem[Chizat et~al., 2019]{chizat2019lazy}
Chizat, L., Oyallon, E., and Bach, F. (2019).
\newblock On lazy training in differentiable programming.
\newblock In {\em NeurIPS 2019-33rd Conference on Neural Information Processing
  Systems}, pages 2937--2947.

\bibitem[{Doan} et~al., 2021]{doan2021a}
{Doan}, T., {Bennani}, M.~A., {Mazoure}, B., {Rabusseau}, G., and {Alquier}, P.
  (2021).
\newblock A theoretical analysis of catastrophic forgetting through the ntk
  overlap matrix.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}.

\bibitem[Dosovitskiy et~al., 2021]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
  (2021).
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em 9th International Conference on Learning Representations,
  {ICLR}, 2021}.

\bibitem[Du et~al., 2019]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. (2019).
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1675--1685. PMLR.

\bibitem[Farajtabar et~al., 2020]{farajtabar2020orthogonal}
Farajtabar, M., Azizan, N., Mott, A., and Li, A. (2020).
\newblock Orthogonal gradient descent for continual learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3762--3773. PMLR.

\bibitem[Fernando et~al., 2017]{fernando2017pathnet}
Fernando, C., Banarse, D., Blundell, C., Zwols, Y., Ha, D., Rusu, A.~A.,
  Pritzel, A., and Wierstra, D. (2017).
\newblock Pathnet: Evolution channels gradient descent in super neural
  networks.
\newblock {\em arXiv preprint arXiv:1701.08734}.

\bibitem[Ferran~Alet, 2018]{modularmetal2018}
Ferran~Alet, Tomas Lozano-Perez, L. P.~K. (2018).
\newblock Modular meta-learning.
\newblock {\em arXiv preprint arXiv:1806.10166v1}.

\bibitem[Goodfellow et~al., 2016]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., and Courville, A. (2016).
\newblock {\em Deep learning}.
\newblock MIT press.

\bibitem[Goodfellow et~al., 2013]{goodfellow2013empirical}
Goodfellow, I.~J., Mirza, M., Xiao, D., Courville, A., and Bengio, Y. (2013).
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks.
\newblock {\em arXiv preprint arXiv:1312.6211}.

\bibitem[He et~al., 2016]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J. (2016).
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778.

\bibitem[Hombaiah et~al., 2021]{hombaiah2021dynamic}
Hombaiah, S.~A., Chen, T., Zhang, M., Bendersky, M., and Najork, M. (2021).
\newblock Dynamic language models for continuously evolving content.
\newblock In {\em {KDD} '21: The 27th {ACM} {SIGKDD} Conference on Knowledge
  Discovery and Data Mining}.

\bibitem[Hsu et~al., 2018]{hsu2018re}
Hsu, Y.-C., Liu, Y.-C., and Kira, Z. (2018).
\newblock Re-evaluating continual learning scenarios: A categorization and case
  for strong baselines.
\newblock {\em arXiv preprint arXiv:1810.12488}.

\bibitem[Jacot et~al., 2018]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C. (2018).
\newblock Neural tangent kernel: convergence and generalization in neural
  networks.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 8580--8589.

\bibitem[Javed and White, 2019]{javed2019meta}
Javed, K. and White, M. (2019).
\newblock Meta-learning representations for continual learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1820--1830.

\bibitem[Kaplan et~al., 2020]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D. (2020).
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}.

\bibitem[Kirichenko et~al., 2021]{kirichenko2021task}
Kirichenko, P., Farajtabar, M., Rao, D., Lakshminarayanan, B., Levine, N., Li,
  A., Hu, H., Wilson, A.~G., and Pascanu, R. (2021).
\newblock Task-agnostic continual learning with hybrid probabilistic models.
\newblock In {\em ICML Workshop on Invertible Neural Networks, Normalizing
  Flows, and Explicit Likelihood Models}.

\bibitem[Kirkpatrick et~al., 2017]{EWC}
Kirkpatrick, J.~N., Pascanu, R., Rabinowitz, N.~C., Veness, J., and et. al.
  (2017).
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the National Academy of Sciences of the United
  States of America}, 114 13:3521--3526.

\bibitem[Krizhevsky et~al., 2012]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E. (2012).
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Advances in neural information processing systems},
  25:1097--1105.

\bibitem[Lazaridou et~al., 2021]{lazaridou2021pitfalls}
Lazaridou, A., Kuncoro, A., Gribovskaya, E., Agrawal, D., Liska, A., Terzi, T.,
  Gimenez, M., d'Autume, C. d.~M., Ruder, S., Yogatama, D., et~al. (2021).
\newblock Pitfalls of static language modelling.
\newblock {\em arXiv preprint arXiv:2102.01951}.

\bibitem[Lee et~al., 2019]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J. (2019).
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock {\em Advances in neural information processing systems},
  32:8572--8583.

\bibitem[Li et~al., 2019]{li2019learn}
Li, X., Zhou, Y., Wu, T., Socher, R., and Xiong, C. (2019).
\newblock Learn to grow: A continual structure learning framework for
  overcoming catastrophic forgetting.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning, {ICML}}, Proceedings of Machine Learning Research.

\bibitem[Li and Hoiem, 2018]{Li2018LearningWF}
Li, Z. and Hoiem, D. (2018).
\newblock Learning without forgetting.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  40:2935--2947.

\bibitem[Lopez-Paz and Ranzato, 2017]{lopez2017gradient}
Lopez-Paz, D. and Ranzato, M. (2017).
\newblock Gradient episodic memory for continual learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6467--6476.

\bibitem[McClelland et~al., 1995]{mcclelland1995there}
McClelland, J.~L., McNaughton, B.~L., and O'Reilly, R.~C. (1995).
\newblock Why there are complementary learning systems in the hippocampus and
  neocortex: insights from the successes and failures of connectionist models
  of learning and memory.
\newblock {\em Psychological review}, 102(3):419.

\bibitem[McCloskey and Cohen, 1989]{McCloskey1989CatastrophicII}
McCloskey, M. and Cohen, N.~J. (1989).
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock {\em Psychology of Learning and Motivation}, 24:109--165.

\bibitem[Mehta et~al., 2021]{SanketPreTrain21}
Mehta, S.~V., Patil, D., Chandar, S., and Strubell, E. (2021).
\newblock An empirical investigation of the role of pre-training in lifelong
  learning.
\newblock {\em ICML CL Workshop,}.

\bibitem[Mirzadeh et~al., 2022]{Mirzadeh2022ArchitectureMI}
Mirzadeh, S.~I., Chaudhry, A., Yin, D., Nguyen, T., Pascanu, R., Gorur, D., and
  Farajtabar, M. (2022).
\newblock Architecture matters in continual learning.
\newblock {\em ArXiv}, abs/2202.00275.

\bibitem[Mirzadeh et~al., 2020a]{mirzadeh2020dropout}
Mirzadeh, S.~I., Farajtabar, M., and Ghasemzadeh, H. (2020a).
\newblock Dropout as an implicit gating mechanism for continual learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 232--233.

\bibitem[Mirzadeh et~al., 2021]{mirzadeh2021linear}
Mirzadeh, S.~I., Farajtabar, M., Gorur, D., Pascanu, R., and Ghasemzadeh, H.
  (2021).
\newblock Linear mode connectivity in multitask and continual learning.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Mirzadeh et~al., 2020b]{mirzadeh2020understanding}
Mirzadeh, S.~I., {Farajtabar}, M., {Pascanu}, R., and {Ghasemzadeh}, H.
  (2020b).
\newblock Understanding the role of training regimes in continual learning.
\newblock In {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020}.

\bibitem[Nguyen et~al., 2019]{nguyen2019toward}
Nguyen, C.~V., Achille, A., Lam, M., Hassner, T., Mahadevan, V., and Soatto, S.
  (2019).
\newblock Toward understanding catastrophic forgetting in continual learning.
\newblock {\em arXiv preprint arXiv:1908.01091}.

\bibitem[Nguyen et~al., 2018]{nguyen2017variational}
Nguyen, C.~V., Li, Y., Bui, T.~D., and Turner, R.~E. (2018).
\newblock Variational continual learning.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018}.

\bibitem[Parisi et~al., 2019]{Parisi2018ContinualLL}
Parisi, G.~I., Kemker, R., Part, J.~L., Kanan, C., and Wermter, S. (2019).
\newblock Continual lifelong learning with neural networks: A review.
\newblock {\em Neural Networks}, 113:54--71.

\bibitem[Pascanu et~al., 2013]{pascanu2013difficulty}
Pascanu, R., Mikolov, T., and Bengio, Y. (2013).
\newblock On the difficulty of training recurrent neural networks.
\newblock In {\em International conference on machine learning}, pages
  1310--1318. PMLR.

\bibitem[Ramasesh et~al., 2022]{EffectOfScaleInCL}
Ramasesh, V.~V., Lewkowycz, A., and Dyer, E. (2022).
\newblock Effect of scale on catastrophic forgetting in neural networks.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Rebuffi et~al., 2016]{Rebuffi2016iCaRLIC}
Rebuffi, S.-A., Kolesnikov, A.~I., Sperl, G., and Lampert, C.~H. (2016).
\newblock icarl: Incremental classifier and representation learning.
\newblock {\em 2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 5533--5542.

\bibitem[{Riemer} et~al., 2018]{riemer2018learning}
{Riemer}, M., {Cases}, I., {Ajemian}, R., {Liu}, M., {Rish}, I., {Tu}, Y., and
  {Tesauro}, G. (2018).
\newblock Learning to learn without forgetting by maximizing transfer and
  minimizing interference.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Ring, 1995]{ring1994continual}
Ring, M.~B. (1995).
\newblock {\em Continual learning in reinforcement environments}.
\newblock PhD thesis, University of Texas at Austin, TX, {USA}.

\bibitem[Rolnick et~al., 2019]{rolnick2018experience}
Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T.~P., and Wayne, G. (2019).
\newblock Experience replay for continual learning.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019}.

\bibitem[Rosenbaum et~al., 2018]{rosenbaum2018routing}
Rosenbaum, C., Klinger, T., and Riemer, M. (2018).
\newblock Routing networks: Adaptive selection of non-linear functions for
  multi-task learning.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Rusu et~al., 2016]{rusu2016progressive}
Rusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,
  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).
\newblock Progressive neural networks.
\newblock {\em arXiv preprint arXiv:1606.04671}.

\bibitem[Shin et~al., 2017]{shin2017continual}
Shin, H., Lee, J.~K., Kim, J., and Kim, J. (2017).
\newblock Continual learning with deep generative replay.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2990--2999.

\bibitem[Szegedy et~al., 2016]{szegedy2016rethinking}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016).
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826.

\bibitem[Thrun, 1995]{thrun1995lifelong}
Thrun, S. (1995).
\newblock A lifelong learning perspective for mobile robot control.
\newblock In {\em Intelligent robots and systems}, pages 201--214. Elsevier.

\bibitem[Titsias et~al., 2019]{titsias2019functional}
Titsias, M.~K., Schwarz, J., Matthews, A. G. d.~G., Pascanu, R., and Teh, Y.~W.
  (2019).
\newblock Functional regularisation for continual learning using gaussian
  processes.
\newblock {\em arXiv preprint arXiv:1901.11356}.

\bibitem[Toneva et~al., 2019]{toneva2018empirical}
Toneva, M., Sordoni, A., Combes, R. T.~d., Trischler, A., Bengio, Y., and
  Gordon, G.~J. (2019).
\newblock An empirical study of example forgetting during deep neural network
  learning.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019}.

\bibitem[Veniat et~al., 2021]{veniat2020efficient}
Veniat, T., Denoyer, L., and Ranzato, M. (2021).
\newblock Efficient continual learning with modular networks and task-driven
  priors.
\newblock In {\em ICLR}.

\bibitem[Wallingford et~al., 2020]{Wallingford2020InTW}
Wallingford, M., Kusupati, A., Alizadeh-Vahid, K., Walsman, A., Kembhavi, A.,
  and Farhadi, A. (2020).
\newblock In the wild: From ml models to pragmatic ml systems.
\newblock {\em ArXiv}, abs/2007.02519.

\bibitem[Wortsman et~al., 2020]{Wortsman2020SupermasksIS}
Wortsman, M., Ramanujan, V., Liu, R., Kembhavi, A., Rastegari, M., Yosinski,
  J., and Farhadi, A. (2020).
\newblock Supermasks in superposition.
\newblock In {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020}.

\bibitem[Xu and Zhu, 2018]{rcl2018}
Xu, J. and Zhu, Z. (2018).
\newblock Reinforced continual learning.
\newblock In {\em Advances in Neural Informatio Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018}.

\bibitem[Yin et~al., 2020]{yin2020optimization}
Yin, D., Farajtabar, M., Li, A., Levine, N., and Mott, A. (2020).
\newblock Optimization and generalization of regularization-based continual
  learning: a loss approximation viewpoint.
\newblock {\em arXiv preprint arXiv:2006.10974}.

\bibitem[Yoon et~al., 2018]{yoon2018lifelong}
Yoon, J., Yang, E., Lee, J., and Hwang, S.~J. (2018).
\newblock Lifelong learning with dynamically expandable networks.
\newblock In {\em Sixth International Conference on Learning Representations}.
  ICLR.

\bibitem[Zagoruyko and Komodakis, 2016]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N. (2016).
\newblock Wide residual networks.
\newblock In {\em British Machine Vision Conference 2016}. British Machine
  Vision Association.

\bibitem[Zenke et~al., 2017]{zenke2017continual}
Zenke, F., Poole, B., and Ganguli, S. (2017).
\newblock Continual learning through synaptic intelligence.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3987--3995. JMLR.

\bibitem[Zou et~al., 2020]{zou2020gradient}
Zou, D., Cao, Y., Zhou, D., and Gu, Q. (2020).
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock {\em Machine Learning}, 109(3):467--492.

\end{thebibliography}
