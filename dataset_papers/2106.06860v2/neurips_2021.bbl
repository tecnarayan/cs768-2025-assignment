\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, et~al.]{abadi2016tensorflow}
Mart{\'\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et~al.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
\newblock \emph{arXiv preprint arXiv:1603.04467}, 2016.

\bibitem[Andrychowicz et~al.(2021)Andrychowicz, Raichuk, Sta{\'n}czyk, Orsini,
  Girgin, Marinier, Hussenot, Geist, Pietquin, Michalski, Gelly, and
  Bachem]{andrychowicz2021what}
Marcin Andrychowicz, Anton Raichuk, Piotr Sta{\'n}czyk, Manu Orsini, Sertan
  Girgin, Rapha{\"e}l Marinier, Leonard Hussenot, Matthieu Geist, Olivier
  Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier Bachem.
\newblock What matters for on-policy deep actor-critic methods? a large-scale
  study.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Atkeson and Schaal(1997)]{atkeson1997robot}
Christopher~G Atkeson and Stefan Schaal.
\newblock Robot learning from demonstration.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Machine Learning}, pages 12--20, 1997.

\bibitem[Booher()]{booherBC}
Jonathan Booher.
\newblock Bc+rl: Imitation learning from non-optimal demonstrations.
\newblock URL \url{https://web.stanford.edu/~jaustinb/papers/CS234.pdf}.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{OpenAIGym}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym, 2016.

\bibitem[Buckman et~al.(2020)Buckman, Gelada, and
  Bellemare]{buckman2020importance}
Jacob Buckman, Carles Gelada, and Marc~G Bellemare.
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock \emph{arXiv preprint arXiv:2009.06799}, 2020.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{arXiv preprint arXiv:2106.01345}, 2021.

\bibitem[Chen et~al.(2020)Chen, Zhou, Wang, Wang, Wu, and Ross]{chen2020bail}
Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross.
\newblock Bail: Best-action imitation learning for batch deep reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Engstrom et~al.(2020)Engstrom, Ilyas, Santurkar, Tsipras, Janoos,
  Rudolph, and Madry]{Engstrom2020Implementation}
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus
  Janoos, Larry Rudolph, and Aleksander Madry.
\newblock Implementation matters in deep rl: A case study on ppo and trpo.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and
  Levine]{fu2021benchmarks}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock 2020.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and
  Meger]{fujimoto2018addressing}
Scott Fujimoto, Herke van Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, volume~80,
  pages 1587--1596. PMLR, 2018.

\bibitem[Fujimoto et~al.(2019{\natexlab{a}})Fujimoto, Conti, Ghavamzadeh, and
  Pineau]{fujimoto2019benchmarking}
Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau.
\newblock Benchmarking batch deep reinforcement learning algorithms.
\newblock \emph{arXiv preprint arXiv:1910.01708}, 2019{\natexlab{a}}.

\bibitem[Fujimoto et~al.(2019{\natexlab{b}})Fujimoto, Meger, and
  Precup]{fujimoto2018off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pages
  2052--2062, 2019{\natexlab{b}}.

\bibitem[Furuta et~al.(2021)Furuta, Kozuno, Matsushima, Matsuo, and
  Gu]{furuta2021identifying}
Hiroki Furuta, Tadashi Kozuno, Tatsuya Matsushima, Yutaka Matsuo, and
  Shixiang~Shane Gu.
\newblock Identifying co-adaptation of algorithmic and implementational
  innovations in deep reinforcement learning: A taxonomy and case study of
  inference-based algorithms.
\newblock \emph{arXiv preprint arXiv:2103.17258}, 2021.

\bibitem[Gauci et~al.(2018)Gauci, Conti, Liang, Virochsiri, He, Kaden,
  Narayanan, Ye, Chen, and Fujimoto]{gauci2018horizon}
Jason Gauci, Edoardo Conti, Yitao Liang, Kittipat Virochsiri, Yuchen He,
  Zachary Kaden, Vivek Narayanan, Xiaohui Ye, Zhengxing Chen, and Scott
  Fujimoto.
\newblock Horizon: Facebook's open source applied reinforcement learning
  platform.
\newblock \emph{arXiv preprint arXiv:1811.00260}, 2018.

\bibitem[Ghasemipour et~al.(2020)Ghasemipour, Schuurmans, and
  Gu]{ghasemipour2020emaq}
Seyed Kamyar~Seyed Ghasemipour, Dale Schuurmans, and Shixiang~Shane Gu.
\newblock Emaq: Expected-max q-learning operator for simple yet effective
  offline and online rl.
\newblock \emph{arXiv preprint arXiv:2007.11091}, 2020.

\bibitem[Goecks et~al.(2020)Goecks, Gremillion, Lawhern, Valasek, and
  Waytowich]{goecks2020integrating}
Vinicius~G Goecks, Gregory~M Gremillion, Vernon~J Lawhern, John Valasek, and
  Nicholas~R Waytowich.
\newblock Integrating behavior cloning and reinforcement learning for improved
  performance in dense and sparse reward environments.
\newblock In \emph{Proceedings of the 19th International Conference on
  Autonomous Agents and MultiAgent Systems}, pages 465--473, 2020.

\bibitem[Gottesman et~al.(2018)Gottesman, Johansson, Meier, Dent, Lee,
  Srinivasan, Zhang, Ding, Wihl, Peng, et~al.]{gottesman2018evaluating}
Omer Gottesman, Fredrik Johansson, Joshua Meier, Jack Dent, Donghun Lee,
  Srivatsan Srinivasan, Linying Zhang, Yi~Ding, David Wihl, Xuefeng Peng,
  et~al.
\newblock Evaluating reinforcement learning algorithms in observational health
  settings.
\newblock \emph{arXiv preprint arXiv:1805.12298}, 2018.

\bibitem[Gulcehre et~al.(2020)Gulcehre, Paine, Shahriari, Denil, Hoffman,
  Soyer, Tanburn, Kapturowski, Rabinowitz, Williams, Barth-Maron, Wang,
  de~Freitas, and Team]{Gulcehre2020Making}
Caglar Gulcehre, Tom~Le Paine, Bobak Shahriari, Misha Denil, Matt Hoffman,
  Hubert Soyer, Richard Tanburn, Steven Kapturowski, Neil Rabinowitz, Duncan
  Williams, Gabriel Barth-Maron, Ziyu Wang, Nando de~Freitas, and Worlds Team.
\newblock Making efficient use of demonstrations to solve hard exploration
  problems.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Guo et~al.(2021)Guo, Feng, Roux, Chi, Lee, and Chen]{guo2021batch}
Yijie Guo, Shengyu Feng, Nicolas~Le Roux, Ed~Chi, Honglak Lee, and Minmin Chen.
\newblock Batch reinforcement learning through continuation method.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, volume~80,
  pages 1861--1870. PMLR, 2018.

\bibitem[Henderson et~al.(2017)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{henderson2017deep}
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup,
  and David Meger.
\newblock Deep reinforcement learning that matters.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2017.

\bibitem[Hester et~al.(2017)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Horgan, Quan, Sendonaris, Dulac-Arnold, et~al.]{hester2017deep}
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal
  Piot, Dan Horgan, John Quan, Andrew Sendonaris, Gabriel Dulac-Arnold, et~al.
\newblock Deep q-learning from demonstrations.
\newblock \emph{arXiv preprint arXiv:1704.03732}, 2017.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2019way}
Natasha Jaques, Asma Ghandeharioun, Judy~Hanwen Shen, Craig Ferguson, Agata
  Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \emph{arXiv preprint arXiv:1907.00456}, 2019.

\bibitem[Judah et~al.(2014)Judah, Fern, Tadepalli, and
  Goetschalckx]{judah2014imitation}
Kshitij Judah, Alan Fern, Prasad Tadepalli, and Robby Goetschalckx.
\newblock Imitation learning with demonstrations and shaping rewards.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~28, 2014.

\bibitem[Kakade(2001)]{kakade2001natural}
Sham Kakade.
\newblock A natural policy gradient.
\newblock In \emph{Proceedings of the 14th International Conference on Neural
  Information Processing Systems: Natural and Synthetic}, pages 1531--1538,
  2001.

\bibitem[Kang et~al.(2018)Kang, Jie, and Feng]{kang2018policy}
Bingyi Kang, Zequn Jie, and Jiashi Feng.
\newblock Policy optimization with demonstrations.
\newblock In \emph{International Conference on Machine Learning}, pages
  2469--2478. PMLR, 2018.

\bibitem[Kim et~al.(2013)Kim, Farahmand, Pineau, and Precup]{kim2013learning}
Beomjoon Kim, Amir-massoud Farahmand, Joelle Pineau, and Doina Precup.
\newblock Learning from limited demonstrations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2859--2867, 2013.

\bibitem[Kingma and Ba(2014)]{adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Tompson, Fergus, and
  Nachum]{kostrikov2021offline}
Ilya Kostrikov, Jonathan Tompson, Rob Fergus, and Ofir Nachum.
\newblock Offline reinforcement learning with fisher divergence critic
  regularization.
\newblock \emph{arXiv preprint arXiv:2103.08050}, 2021.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11784--11794, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.04779}, 2020.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012batch}
Sascha Lange, Thomas Gabel, and Martin Riedmiller.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning}, pages 45--73. Springer, 2012.

\bibitem[Laroche et~al.(2019)Laroche, Trichelair, and
  Des~Combes]{laroche2019safe}
Romain Laroche, Paul Trichelair, and Remi~Tachet Des~Combes.
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In \emph{International Conference on Machine Learning}, pages
  3652--3661. PMLR, 2019.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{DDPG}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Mandel et~al.(2014)Mandel, Liu, Levine, Brunskill, and
  Popovic]{mandel2014offline}
Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic.
\newblock Offline policy evaluation across representations with applications to
  educational games.
\newblock In \emph{International Conference on Autonomous Agents and Multiagent
  Systems}, 2014.

\bibitem[Matsushima et~al.(2020)Matsushima, Furuta, Matsuo, Nachum, and
  Gu]{matsushima2020deployment}
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu.
\newblock Deployment-efficient reinforcement learning via model-based offline
  optimization.
\newblock \emph{arXiv preprint arXiv:2006.03647}, 2020.

\bibitem[Nachum et~al.(2019)Nachum, Dai, Kostrikov, Chow, Li, and
  Schuurmans]{nachum2019algaedice}
Ofir Nachum, Bo~Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale
  Schuurmans.
\newblock Algaedice: Policy gradient from arbitrary experience.
\newblock \emph{arXiv preprint arXiv:1912.02074}, 2019.

\bibitem[Nair et~al.(2018)Nair, McGrew, Andrychowicz, Zaremba, and
  Abbeel]{nair2018overcoming}
Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter
  Abbeel.
\newblock Overcoming exploration in reinforcement learning with demonstrations.
\newblock In \emph{2018 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 6292--6299. IEEE, 2018.

\bibitem[Nair et~al.(2020)Nair, Dalal, Gupta, and Levine]{nair2020accelerating}
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock \emph{arXiv preprint arXiv:2006.09359}, 2020.

\bibitem[Osband et~al.(2019)Osband, Doron, Hessel, Aslanides, Sezener, Saraiva,
  McKinney, Lattimore, Szepesvari, Singh, et~al.]{osband2019behaviour}
Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre
  Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh,
  et~al.
\newblock Behaviour suite for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1908.03568}, 2019.

\bibitem[Paine et~al.(2020)Paine, Paduraru, Michi, Gulcehre, Zolna, Novikov,
  Wang, and de~Freitas]{paine2020hyperparameter}
Tom~Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna,
  Alexander Novikov, Ziyu Wang, and Nando de~Freitas.
\newblock Hyperparameter selection for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.09055}, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8024--8035, 2019.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{peng2019advantage}
Xue~Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.00177}, 2019.

\bibitem[Pfeiffer et~al.(2018)Pfeiffer, Shukla, Turchetta, Cadena, Krause,
  Siegwart, and Nieto]{pfeiffer2018reinforced}
Mark Pfeiffer, Samarth Shukla, Matteo Turchetta, Cesar Cadena, Andreas Krause,
  Roland Siegwart, and Juan Nieto.
\newblock Reinforced imitation: Sample efficient deep reinforcement learning
  for mapless navigation by leveraging prior demonstrations.
\newblock \emph{IEEE Robotics and Automation Letters}, 3\penalty0 (4):\penalty0
  4423--4430, 2018.

\bibitem[Pomerleau(1991)]{pomerleau1991efficient}
Dean~A Pomerleau.
\newblock Efficient training of artificial neural networks for autonomous
  navigation.
\newblock \emph{Neural computation}, 3\penalty0 (1):\penalty0 88--97, 1991.

\bibitem[Raffin et~al.(2019)Raffin, Hill, Ernestus, Gleave, Kanervisto, and
  Dormann]{stable-baselines3}
Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi
  Kanervisto, and Noah Dormann.
\newblock Stable baselines3.
\newblock \url{https://github.com/DLR-RM/stable-baselines3}, 2019.

\bibitem[Rajeswaran et~al.(2017)Rajeswaran, Kumar, Gupta, Vezzani, Schulman,
  Todorov, and Levine]{rajeswaran2017learning}
Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John
  Schulman, Emanuel Todorov, and Sergey Levine.
\newblock Learning complex dexterous manipulation with deep reinforcement
  learning and demonstrations.
\newblock \emph{arXiv preprint arXiv:1709.10087}, 2017.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
St{\'e}phane Ross, Geoffrey Gordon, and Drew Bagnell.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pages 627--635. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Siegel et~al.(2020)Siegel, Springenberg, Berkenkamp, Abdolmaleki,
  Neunert, Lampe, Hafner, Heess, and Riedmiller]{Siegel2020Keep}
Noah Siegel, Jost~Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki,
  Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin
  Riedmiller.
\newblock Keep doing what worked: Behavior modelling priors for offline
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{DPG}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
  Martin Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{International Conference on Machine Learning}, pages
  387--395, 2014.

\bibitem[Sutton and Barto(1998)]{sutton1998reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}, volume~1.
\newblock MIT press Cambridge, 1998.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS)}, pages 5026--5033. IEEE, 2012.

\bibitem[Tucker et~al.(2018)Tucker, Bhupatiraju, Gu, Turner, Ghahramani, and
  Levine]{tucker18amirage}
George Tucker, Surya Bhupatiraju, Shixiang Gu, Richard Turner, Zoubin
  Ghahramani, and Sergey Levine.
\newblock The mirage of action-dependent baselines in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Ve{\v{c}}er{\'\i}k et~al.(2017)Ve{\v{c}}er{\'\i}k, Hester, Scholz,
  Wang, Pietquin, Piot, Heess, Roth{\"o}rl, Lampe, and
  Riedmiller]{vevcerik2017leveraging}
Matej Ve{\v{c}}er{\'\i}k, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier
  Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth{\"o}rl, Thomas Lampe, and
  Martin Riedmiller.
\newblock Leveraging demonstrations for deep reinforcement learning on robotics
  problems with sparse rewards.
\newblock \emph{arXiv preprint arXiv:1707.08817}, 2017.

\bibitem[Wang et~al.(2018)Wang, Xiong, Han, Sun, Liu, and
  Zhang]{wang2018exponentially}
Qing Wang, Jiechao Xiong, Lei Han, Peng Sun, Han Liu, and Tong Zhang.
\newblock Exponentially weighted imitation learning for batched historical
  data.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 6291--6300, 2018.

\bibitem[Wang et~al.(2020)Wang, Novikov, Zolna, Merel, Springenberg, Reed,
  Shahriari, Siegel, Gulcehre, Heess, et~al.]{wang2020critic}
Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh~S Merel, Jost~Tobias
  Springenberg, Scott~E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre,
  Nicolas Heess, et~al.
\newblock Critic regularized regression.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Wu et~al.(2021)Wu, Mozifian, and Shkurti]{wu2021shaping}
Yuchen Wu, Melissa Mozifian, and Florian Shkurti.
\newblock Shaping rewards for reinforcement learning with imperfect
  demonstrations using generative models.
\newblock In \emph{2021 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 6628--6634. IEEE, 2021.

\bibitem[Yang et~al.(2020)Yang, Dai, Nachum, Tucker, and
  Schuurmans]{yang2020offline}
Mengjiao Yang, Bo~Dai, Ofir Nachum, George Tucker, and Dale Schuurmans.
\newblock Offline policy selection under uncertainty.
\newblock \emph{arXiv preprint arXiv:2012.06919}, 2020.

\bibitem[Zhu et~al.(2018)Zhu, Wang, Merel, Rusu, Erez, Cabi, Tunyasuvunakool,
  Kram{\'a}r, Hadsell, de~Freitas, et~al.]{zhu2018reinforcement}
Yuke Zhu, Ziyu Wang, Josh Merel, Andrei Rusu, Tom Erez, Serkan Cabi, Saran
  Tunyasuvunakool, J{\'a}nos Kram{\'a}r, Raia Hadsell, Nando de~Freitas, et~al.
\newblock Reinforcement and imitation learning for diverse visuomotor skills.
\newblock \emph{arXiv preprint arXiv:1802.09564}, 2018.

\end{thebibliography}
