\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu(2017)]{Natasha2017ICML}
Zeyuan Allen-Zhu.
\newblock Natasha: Faster non-convex stochastic optimization via strongly
  non-convex parameter.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, volume~70 of \emph{Proceedings of Machine Learning
  Research}, pages 89--97, Sydney, Australia, 2017.

\bibitem[Allen-Zhu(2018)]{Natasha2NIPS2018}
Zeyuan Allen-Zhu.
\newblock Natasha 2: Faster non-convex optimization than {SGD}.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pages
  2675--2686. Curran Associates, Inc., 2018.

\bibitem[Allen-Zhu and Hazan(2016)]{Allen-ZhuHazan2016}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning (ICML)}, pages 699--707, 2016.

\bibitem[Beck(2017)]{Beck2017book}
Amir Beck.
\newblock \emph{First-Order Methods in Optimization}.
\newblock MOS-SIAM Series on Optimization. SIAM, 2017.

\bibitem[Chaudhari et~al.(2016)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{Pratik}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
  Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock \emph{arXiv preprint, arXiv:1611.01838}, 2016.

\bibitem[Dann et~al.(2014)Dann, Neumann, and Peters]{dann2014policy}
Christoph Dann, Gerhard Neumann, and Jan Peters.
\newblock Policy evaluation with temporal differences: a survey and comparison.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 809--883, 2014.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems 27}, pages
  1646--1654, 2014.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{SPIDER2018NeurIPS}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pages
  689--699. Curran Associates, Inc., 2018.

\bibitem[Ghadimi and Lan(2013)]{GhadimiLan2013}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Gulcehre et~al.(2016)Gulcehre, Moczulski, Visin, and
  Bengio]{Smoothing-2}
Caglar Gulcehre, Marcin Moczulski, Francesco Visin, and Yoshua Bengio.
\newblock Mollifying networks.
\newblock \emph{arXiv preprint, arXiv:1608.04980}, 2016.

\bibitem[Hazan et~al.(2016)Hazan, Levy, and Shalev-Shwartz]{Smoothing-1}
Elad Hazan, Kfir~Yehuda Levy, and Shai Shalev-Shwartz.
\newblock On graduated optimization for stochastic non-convex problems.
\newblock In \emph{International conference on machine learning}, pages
  1833--1841, 2016.

\bibitem[Huo et~al.(2018)Huo, Gu, Jiu, and Huang]{VRSC-PG}
Zhouyuan Huo, Bin Gu, Ji~Jiu, and Heng Huang.
\newblock Accelerated method for stochastic composition optimization with
  nonsmooth regularization.
\newblock In \emph{Proceedings of the 32nd AAAI Conference on Artificial
  Intelligence}, pages 3287--3294, 2018.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems 26}, pages
  315--323, 2013.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and
  Schmidt]{KarimiNutiniSchmidt2016}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient method and proximal-gradient methods
  under the {Polyak}-{{\L}ojasiewicz} condition.
\newblock In \emph{Machine Learning and Knowledge Discovery in Database -
  European Conference, Proceedings}, pages 795--811, 2016.

\bibitem[Lei et~al.(2017)Lei, Ju, Chen, and Jordan]{LeiJuChenJordan2017}
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael~I Jordan.
\newblock Non-convex finite-sum optimization via {SCSG} methods.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pages
  2348--2358. Curran Associates, Inc., 2017.

\bibitem[Li and Li(2018)]{LiLi2018NeurIPS}
Zhize Li and Jian Li.
\newblock A simple proximal stochastic gradient method for nonsmooth nonconvex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pages
  5564--5574. Curran Associates, Inc., 2018.

\bibitem[Lian et~al.(2017)Lian, Wang, and Liu]{SVR-SCGD}
Xiangru Lian, Mengdi Wang, and Ji~Liu.
\newblock Finite-sum composition optimization via variance reduced gradient
  descent.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 1159--1167, 2017.

\bibitem[Mobahi and Fisher(2015)]{Smoothing-3}
Hossein Mobahi and John~W Fisher.
\newblock On the link between gaussian homotopy continuation and convex
  envelopes.
\newblock In \emph{International Workshop on Energy Minimization Methods in
  Computer Vision and Pattern Recognition}, pages 43--56. Springer, 2015.

\bibitem[Nesterov(2013)]{Nesterov2013composite}
Yurii Nesterov.
\newblock Gradient methods for minimizing composite functions.
\newblock \emph{Mathematical Programming}, 140\penalty0 (1):\penalty0 125--161,
  2013.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{SARAH2017ICML}
Lam~M. Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock {SARAH}: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In Doina Precup and Yee~Whye Teh, editors, \emph{Proceedings of the
  34th International Conference on Machine Learning (ICML)}, volume~70 of
  \emph{Proceedings of Machine Learning Research (PMLR)}, pages 2613--2621,
  Sydney, Australia, 2017.

\bibitem[Nguyen et~al.(2019)Nguyen, van Dijk, Phan, Nguyen, Weng, and
  Kalagnanam]{SmoothSARAH2019}
Lam~M. Nguyen, Marten van Dijk, Dzung~T. Phan, Phuong~Ha Nguyen, Tsui-Wei Weng,
  and Jayant~R. Kalagnanam.
\newblock Finite-sum smooth optimization with sarah.
\newblock arXiv preprint, arXiv:1901.07648, 2019.

\bibitem[Pham et~al.(2019)Pham, Nguyen, Phan, and Tran-Dinh]{ProxSARAH2019}
Nhan~H. Pham, Lam~M. Nguyen, Dzung~T. Phan, and Quoc Tran-Dinh.
\newblock {ProxSARAH}: An efficient algorithmic framework for stochastic
  composite nonconvex optimization.
\newblock arXiv preprint, arXiv:1902.05679, 2019.

\bibitem[Reddi et~al.(2016{\natexlab{a}})Reddi, Hefny, Sra, Poczos, and
  Smola]{Reddi2016SVRGnonconvex}
Sashank~J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine
  Learning}, volume~48 of \emph{Proceedings of Machine Learning Research},
  pages 314--323, New York, New York, USA, 2016{\natexlab{a}}.

\bibitem[Reddi et~al.(2016{\natexlab{b}})Reddi, Sra, P{\'o}czos, and
  Smola]{NCVX-SAGA-Smooth}
Sashank~J Reddi, Suvrit Sra, Barnab{\'a}s P{\'o}czos, and Alex Smola.
\newblock Fast incremental method for smooth nonconvex optimization.
\newblock In \emph{2016 IEEE 55th Conference on Decision and Control (CDC)},
  pages 1971--1977. IEEE, 2016{\natexlab{b}}.

\bibitem[Reddi et~al.(2016{\natexlab{c}})Reddi, Sra, P{\'o}czos, and
  Smola]{NCVX-SAGA-Nonsmooth}
Sashank~J Reddi, Suvrit Sra, Barnab{\'a}s P{\'o}czos, and Alexander~J Smola.
\newblock Proximal stochastic methods for nonsmooth nonconvex finite-sum
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 29}, pages
  1145--1153, 2016{\natexlab{c}}.

\bibitem[Rockafellar(1970)]{Rockafellar70book}
R.~Tyrrell Rockafellar.
\newblock \emph{Convex Analysis}.
\newblock Princeton University Press, 1970.

\bibitem[Rockafellar(2007)]{Rockafellar2007CoherentRisk}
R.~Tyrrell Rockafellar.
\newblock Coherent approaches to risk in optimization under uncertainty.
\newblock \emph{INFORMS TutORials in Operations Research}, 2007.

\bibitem[Ruszczy\'nski(2013)]{Ruszczynski2013risk-averse}
Andrzej Ruszczy\'nski.
\newblock Advances in risk-averse optimization.
\newblock \emph{INFORMS TutORials in Operation Research}, 2013.

\bibitem[Sutton and Barto(1998)]{sutton1998reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT Press, Cambridge, MA, 1998.

\bibitem[Wang et~al.(2017{\natexlab{a}})Wang, Fang, and Liu]{SCGD-M.Wang}
Mengdi Wang, Ethan~X Fang, and Han Liu.
\newblock Stochastic compositional gradient descent: algorithms for minimizing
  compositions of expected-value functions.
\newblock \emph{Mathematical Programming}, 161\penalty0 (1-2):\penalty0
  419--449, 2017{\natexlab{a}}.

\bibitem[Wang et~al.(2017{\natexlab{b}})Wang, Liu, and Fang]{ASC-PG-M.Wang}
Mengdi Wang, Ji~Liu, and Ethan Fang.
\newblock Accelerating stochastic composition optimization.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (105):\penalty0 1--23, 2017{\natexlab{b}}.

\bibitem[Wang et~al.(2018)Wang, Ji, Zhou, Liang, and Tarokh]{SpiderBoost2018}
Zhe Wang, Kaiyi Ji, Yi~Zhou, Yingbin Liang, and Vahid Tarokh.
\newblock {SpiderBoost}: A class of faster variance-reduced algorithms for
  nonconvex optimization.
\newblock arXiv preprint, arXiv:1810.10690, 2018.

\bibitem[Xiao and Zhang(2014)]{xiaozhang2014proxsvrg}
Lin Xiao and Tong Zhang.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (4):\penalty0
  2057--2075, 2014.

\bibitem[Zhang and Xiao(2019)]{ZhangXiao2019C-SAGA}
Junyu Zhang and Lin Xiao.
\newblock A composite randomized incremental gradient method.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, number~97 in Proceedings of Machine Learning Research
  (PMLR), Long Beach, California, 2019.

\bibitem[Zhou et~al.(2018)Zhou, Xu, and Gu]{NestedSVRG2018NeurIPS}
Dongruo Zhou, Pan Xu, and Quanquan Gu.
\newblock Stochastic nested variance reduced gradient descent for nonconvex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pages
  3921--3932. Curran Associates, Inc., 2018.

\end{thebibliography}
