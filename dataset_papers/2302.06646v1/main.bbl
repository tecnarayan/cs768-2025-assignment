\begin{thebibliography}{79}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ailon et~al.(2021)Ailon, Leibovitch, and Nair]{ailon2021sparse}
Ailon, N., Leibovitch, O., and Nair, V.
\newblock Sparse linear networks with a fixed butterfly structure: theory and
  practice.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  1174--1184.
  PMLR, 2021.

\bibitem[Ayinala et~al.(2011)Ayinala, Brown, and Parhi]{ayinala2011pipelined}
Ayinala, M., Brown, M., and Parhi, K.~K.
\newblock Pipelined parallel fft architectures via folding transformation.
\newblock \emph{IEEE Transactions on Very Large Scale Integration (VLSI)
  Systems}, 20\penalty0 (6):\penalty0 1068--1081, 2011.

\bibitem[Bahn et~al.(2009)Bahn, Yang, Hu, and Bagherzadeh]{bahn2009parallel}
Bahn, J.~H., Yang, J.~S., Hu, W.-H., and Bagherzadeh, N.
\newblock Parallel fft algorithms on network-on-chips.
\newblock \emph{Journal of Circuits, Systems, and Computers}, 18\penalty0
  (02):\penalty0 255--269, 2009.

\bibitem[Bailey(1990)]{bailey1990ffts}
Bailey, D.~H.
\newblock {FFT}s in external or hierarchical memory.
\newblock \emph{The journal of Supercomputing}, 4\penalty0 (1):\penalty0
  23--35, 1990.

\bibitem[Barch et~al.(2013)Barch, Burgess, Harms, Petersen, Schlaggar,
  Corbetta, Glasser, Curtiss, Dixit, Feldt, et~al.]{barch2013function}
Barch, D.~M., Burgess, G.~C., Harms, M.~P., Petersen, S.~E., Schlaggar, B.~L.,
  Corbetta, M., Glasser, M.~F., Curtiss, S., Dixit, S., Feldt, C., et~al.
\newblock Function in the human connectome: task-fmri and individual
  differences in behavior.
\newblock \emph{Neuroimage}, 80:\penalty0 169--189, 2013.

\bibitem[Bekele(2016)]{bekele2016cooley}
Bekele, A.
\newblock Cooley-tukey fft algorithms.
\newblock \emph{Advanced algorithms}, 2016.

\bibitem[Brigham(1988)]{brigham1988fast}
Brigham, E.~O.
\newblock \emph{The fast Fourier transform and its applications}.
\newblock Prentice-Hall, Inc., 1988.

\bibitem[Chen et~al.(2021)Chen, Dao, Liang, Yang, Song, Rudra, and
  Re]{chen2021pixelated}
Chen, B., Dao, T., Liang, K., Yang, J., Song, Z., Rudra, A., and Re, C.
\newblock Pixelated butterfly: Simple and efficient sparse training for neural
  network models.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Choromanski et~al.(2019)Choromanski, Rowland, Chen, and
  Weller]{choromanski2019unifying}
Choromanski, K., Rowland, M., Chen, W., and Weller, A.
\newblock Unifying orthogonal monte carlo methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1203--1212. PMLR, 2019.

\bibitem[Chu \& George(1999)Chu and George]{chu1999inside}
Chu, E. and George, A.
\newblock \emph{Inside the FFT black box: serial and parallel fast Fourier
  transform algorithms}.
\newblock CRC press, 1999.

\bibitem[Cooley \& Tukey(1965)Cooley and Tukey]{cooley1965an}
Cooley, J.~W. and Tukey, J.~W.
\newblock An algorithm for the machine calculation of complex fourier series.
\newblock \emph{Mathematics of Computation}, 19\penalty0 (90):\penalty0
  297--301, 1965.
\newblock ISSN 00255718, 10886842.
\newblock URL \url{http://www.jstor.org/stable/2003354}.

\bibitem[Dadi et~al.(2020)Dadi, Varoquaux, Machlouzarides-Shalit, Gorgolewski,
  Wassermann, Thirion, and Mensch]{dadi2020fine}
Dadi, K., Varoquaux, G., Machlouzarides-Shalit, A., Gorgolewski, K.~J.,
  Wassermann, D., Thirion, B., and Mensch, A.
\newblock Fine-grain atlases of functional modes for fmri analysis.
\newblock \emph{NeuroImage}, 221:\penalty0 117126, 2020.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J.~G., Le, Q., and Salakhutdinov, R.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  2978--2988, 2019.

\bibitem[Dao et~al.(2019)Dao, Gu, Eichhorn, Rudra, and R{\'e}]{dao2019learning}
Dao, T., Gu, A., Eichhorn, M., Rudra, A., and R{\'e}, C.
\newblock Learning fast algorithms for linear transforms using butterfly
  factorizations.
\newblock In \emph{International conference on machine learning}, pp.\
  1517--1527. PMLR, 2019.

\bibitem[Dao et~al.(2022{\natexlab{a}})Dao, Chen, Sohoni, Desai, Poli, Grogan,
  Liu, Rao, Rudra, and R{\'e}]{dao2022monarch}
Dao, T., Chen, B., Sohoni, N.~S., Desai, A., Poli, M., Grogan, J., Liu, A.,
  Rao, A., Rudra, A., and R{\'e}, C.
\newblock Monarch: Expressive structured matrices for efficient and accurate
  training.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4690--4721. PMLR, 2022{\natexlab{a}}.

\bibitem[Dao et~al.(2022{\natexlab{b}})Dao, Fu, Ermon, Rudra, and
  R{\'e}]{dao2022flashattention}
Dao, T., Fu, D.~Y., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock Flash{A}ttention: Fast and memory-efficient exact attention with
  {IO}-awareness.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.

\bibitem[Dao et~al.(2022{\natexlab{c}})Dao, Fu, Saab, Thomas, Rudra, and
  R{\'e}]{dao2022hungry}
Dao, T., Fu, D.~Y., Saab, K.~K., Thomas, A.~W., Rudra, A., and R{\'e}, C.
\newblock Hungry hungry hippos: Towards language modeling with state space
  models.
\newblock \emph{arXiv preprint arXiv:2212.14052}, 2022{\natexlab{c}}.

\bibitem[De~Sa et~al.(2018)De~Sa, Cu, Puttagunta, R{\'e}, and Rudra]{de2018two}
De~Sa, C., Cu, A., Puttagunta, R., R{\'e}, C., and Rudra, A.
\newblock A two-pronged progress in structured dense matrix vector
  multiplication.
\newblock In \emph{Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pp.\  1060--1079. SIAM, 2018.

\bibitem[Dong et~al.(2017)Dong, Chen, and Pan]{dong2017learning}
Dong, X., Chen, S., and Pan, S.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Eidelman \& Gohberg(1999)Eidelman and Gohberg]{eidelman1999new}
Eidelman, Y. and Gohberg, I.
\newblock On a new class of structured matrices.
\newblock \emph{Integral Equations and Operator Theory}, 34\penalty0
  (3):\penalty0 293--324, 1999.

\bibitem[Fischl(2012)]{fischl2012freesurfer}
Fischl, B.
\newblock Freesurfer.
\newblock \emph{Neuroimage}, 62\penalty0 (2):\penalty0 774--781, 2012.

\bibitem[Frankle \& Carbin(2018)Frankle and Carbin]{frankle2018lottery}
Frankle, J. and Carbin, M.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock \emph{arXiv preprint arXiv:1803.03635}, 2018.

\bibitem[Frankle et~al.(2019)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2019stabilizing}
Frankle, J., Dziugaite, G.~K., Roy, D.~M., and Carbin, M.
\newblock Stabilizing the lottery ticket hypothesis.
\newblock \emph{arXiv preprint arXiv:1903.01611}, 2019.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Frankle, J., Dziugaite, G.~K., Roy, D., and Carbin, M.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3259--3269. PMLR, 2020.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, et~al.]{gao2020pile}
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang,
  J., He, H., Thite, A., Nabeshima, N., et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Goel et~al.(2022)Goel, Gu, Donahue, and R{\'e}]{goel2022s}
Goel, K., Gu, A., Donahue, C., and R{\'e}, C.
\newblock It's raw! audio generation with state-space models.
\newblock \emph{arXiv preprint arXiv:2202.09729}, 2022.

\bibitem[Gokaslan et~al.(2019)Gokaslan, Cohen, Pavlick, and
  Tellex]{Gokaslan2019OpenWeb}
Gokaslan, A., Cohen, V., Pavlick, E., and Tellex, S.
\newblock Openwebtext corpus, 2019.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R{\'e}]{gu2020hippo}
Gu, A., Dao, T., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1474--1487, 2020.

\bibitem[Gu et~al.(2022{\natexlab{a}})Gu, Goel, and R\'e]{gu2022efficiently}
Gu, A., Goel, K., and R\'e, C.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{The International Conference on Learning Representations
  ({ICLR})}, 2022{\natexlab{a}}.

\bibitem[Gu et~al.(2022{\natexlab{b}})Gu, Gupta, Goel, and
  R{\'e}]{gu2022parameterization}
Gu, A., Gupta, A., Goel, K., and R{\'e}, C.
\newblock On the parameterization and initialization of diagonal state space
  models.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.

\bibitem[Gu et~al.(2022{\natexlab{c}})Gu, Johnson, Timalsina, Rudra, and
  R{\'e}]{gu2022train}
Gu, A., Johnson, I., Timalsina, A., Rudra, A., and R{\'e}, C.
\newblock How to train your hippo: State space models with generalized
  orthogonal basis projections.
\newblock \emph{arXiv preprint arXiv:2206.12037}, 2022{\natexlab{c}}.

\bibitem[Guibas et~al.(2021)Guibas, Mardani, Li, Tao, Anandkumar, and
  Catanzaro]{guibas2021adaptive}
Guibas, J., Mardani, M., Li, Z., Tao, A., Anandkumar, A., and Catanzaro, B.
\newblock Adaptive fourier neural operators: Efficient token mixers for
  transformers.
\newblock \emph{arXiv preprint arXiv:2111.13587}, 2021.

\bibitem[Gupta et~al.(2022)Gupta, Gu, and Berant]{gupta2022diagonal}
Gupta, A., Gu, A., and Berant, J.
\newblock Diagonal state spaces are as effective as structured state spaces.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Han et~al.(2015{\natexlab{a}})Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015{\natexlab{a}}.

\bibitem[Han et~al.(2015{\natexlab{b}})Han, Pool, Tran, and
  Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock \emph{Advances in neural information processing systems}, 28,
  2015{\natexlab{b}}.

\bibitem[Hasani et~al.(2022)Hasani, Lechner, Wang, Chahine, Amini, and
  Rus]{hasani2022liquid}
Hasani, R., Lechner, M., Wang, T.-H., Chahine, M., Amini, A., and Rus, D.
\newblock Liquid structural state-space models.
\newblock \emph{arXiv preprint arXiv:2209.12951}, 2022.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Islam \& Bertasius(2022)Islam and Bertasius]{islam2022long}
Islam, M.~M. and Bertasius, G.
\newblock Long movie clip classification with state-space video models.
\newblock \emph{arXiv preprint arXiv:2204.01692}, 2022.

\bibitem[Kailath et~al.(1979)Kailath, Kung, and Morf]{kailath1979displacement}
Kailath, T., Kung, S.-Y., and Morf, M.
\newblock Displacement ranks of matrices and linear equations.
\newblock \emph{Journal of Mathematical Analysis and Applications}, 68\penalty0
  (2):\penalty0 395--407, 1979.

\bibitem[King et~al.(2019)King, Hernandez-Castillo, Poldrack, Ivry, and
  Diedrichsen]{king2019functional}
King, M., Hernandez-Castillo, C.~R., Poldrack, R.~A., Ivry, R.~B., and
  Diedrichsen, J.
\newblock Functional boundaries in the human cerebellum revealed by a
  multi-domain task battery.
\newblock \emph{Nature neuroscience}, 22\penalty0 (8):\penalty0 1371--1378,
  2019.

\bibitem[Krizhevsky et~al.(2017)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2017imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Communications of the ACM}, 60\penalty0 (6):\penalty0 84--90,
  2017.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee-Thorp et~al.(2021)Lee-Thorp, Ainslie, Eckstein, and
  Ontanon]{lee2021fnet}
Lee-Thorp, J., Ainslie, J., Eckstein, I., and Ontanon, S.
\newblock Fnet: Mixing tokens with fourier transforms.
\newblock \emph{arXiv preprint arXiv:2105.03824}, 2021.

\bibitem[Li et~al.(2021)Li, Cheng, and Lin]{li2021tcfft}
Li, B., Cheng, S., and Lin, J.
\newblock tcfft: Accelerating half-precision fft through tensor cores.
\newblock \emph{arXiv preprint arXiv:2104.11471}, 2021.

\bibitem[Li et~al.(2022)Li, Cai, Zhang, Chen, and Dey]{li2022makes}
Li, Y., Cai, T., Zhang, Y., Chen, D., and Dey, D.
\newblock What makes convolutional models great on long sequence modeling?
\newblock \emph{arXiv preprint arXiv:2210.09298}, 2022.

\bibitem[Liang et~al.(2021)Liang, Chongjian, Tong, Song, Wang, and
  Xie]{liang2021evit}
Liang, Y., Chongjian, G., Tong, Z., Song, Y., Wang, J., and Xie, P.
\newblock Evit: Expediting vision transformers via token reorganizations.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Lin et~al.(2017)Lin, Rao, Lu, and Zhou]{lin2017runtime}
Lin, J., Rao, Y., Lu, J., and Zhou, J.
\newblock Runtime neural pruning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Lin et~al.(2021)Lin, Ran, Chiu, Chesi, and Wong]{lin2021deformable}
Lin, R., Ran, J., Chiu, K.~H., Chesi, G., and Wong, N.
\newblock Deformable butterfly: A highly structured and sparse linear
  transform.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 16145--16157, 2021.

\bibitem[Ma et~al.(2022)Ma, Zhou, Kong, He, Gui, Neubig, May, and
  Zettlemoyer]{ma2022mega}
Ma, X., Zhou, C., Kong, X., He, J., Gui, L., Neubig, G., May, J., and
  Zettlemoyer, L.
\newblock Mega: moving average equipped gated attention.
\newblock \emph{arXiv preprint arXiv:2209.10655}, 2022.

\bibitem[Mehta et~al.(2022)Mehta, Gupta, Cutkosky, and
  Neyshabur]{mehta2022long}
Mehta, H., Gupta, A., Cutkosky, A., and Neyshabur, B.
\newblock Long range language modeling via gated state spaces.
\newblock \emph{arXiv preprint arXiv:2206.13947}, 2022.

\bibitem[Munkhoeva et~al.(2018)Munkhoeva, Kapushev, Burnaev, and
  Oseledets]{munkhoeva2018quadrature}
Munkhoeva, M., Kapushev, Y., Burnaev, E., and Oseledets, I.
\newblock Quadrature-based features for kernel approximation.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Nguyen et~al.(2022)Nguyen, Goel, Gu, Downs, Shah, Dao, Baccus, and
  R{\'e}]{nguyen2022s4nd}
Nguyen, E., Goel, K., Gu, A., Downs, G., Shah, P., Dao, T., Baccus, S., and
  R{\'e}, C.
\newblock S4nd: Modeling images and videos as multidimensional signals with
  state spaces.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[NVIDIA(2017)]{nvidia2017nvidia}
NVIDIA.
\newblock Nvidia {T}esla {V}100 {GPU} architecture, 2017.

\bibitem[NVIDIA(2020)]{nvidia2020nvidia}
NVIDIA.
\newblock Nvidia {A}100 tensor core {GPU} architecture, 2020.

\bibitem[NVIDIA(2022{\natexlab{a}})]{cufft}
NVIDIA.
\newblock cufft v11.7.1 documentation, 2022{\natexlab{a}}.
\newblock https://docs.nvidia.com/cuda/cufft/index.html.

\bibitem[NVIDIA(2022{\natexlab{b}})]{nvidia2022nvidia}
NVIDIA.
\newblock Nvidia {H}100 tensor core {GPU} architecture, 2022{\natexlab{b}}.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan,
  Mann, Askell, Bai, Chen, Conerly, Drain, Ganguli, Hatfield-Dodds, Hernandez,
  Johnston, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan,
  McCandlish, and Olah]{olsson2022context}
Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T.,
  Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Ganguli, D.,
  Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J.,
  Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J.,
  McCandlish, S., and Olah, C.
\newblock In-context learning and induction heads.
\newblock \emph{Transformer Circuits Thread}, 2022.
\newblock
  https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

\bibitem[Oppenheim(1978)]{oppenheim1978applications}
Oppenheim, A.~V.
\newblock Applications of digital signal processing.
\newblock \emph{Englewood Cliffs}, 1978.

\bibitem[Oppenheim et~al.(2001)Oppenheim, Buck, and
  Schafer]{oppenheim2001discrete}
Oppenheim, A.~V., Buck, J.~R., and Schafer, R.~W.
\newblock \emph{Discrete-time signal processing. Vol. 2}.
\newblock Upper Saddle River, NJ: Prentice Hall, 2001.

\bibitem[Parker(1995)]{parker1995random}
Parker, D.
\newblock \emph{Random Butterfly Transformations with Applications in
  Computational Linear Algebra}.
\newblock CSD (Series). UCLA Computer Science Department, 1995.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Prabhu et~al.(2020)Prabhu, Farhadi, Rastegari,
  et~al.]{prabhu2020butterfly}
Prabhu, A., Farhadi, A., Rastegari, M., et~al.
\newblock Butterfly transform: An efficient fft based neural architecture
  design.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  12024--12033, 2020.

\bibitem[Romero et~al.(2021{\natexlab{a}})Romero, Bruintjes, Tomczak, Bekkers,
  Hoogendoorn, and van Gemert]{romero2021flexconv}
Romero, D.~W., Bruintjes, R.-J., Tomczak, J.~M., Bekkers, E.~J., Hoogendoorn,
  M., and van Gemert, J.~C.
\newblock Flexconv: Continuous kernel convolutions with differentiable kernel
  sizes.
\newblock \emph{arXiv preprint arXiv:2110.08059}, 2021{\natexlab{a}}.

\bibitem[Romero et~al.(2021{\natexlab{b}})Romero, Kuzina, Bekkers, Tomczak, and
  Hoogendoorn]{romero2021ckconv}
Romero, D.~W., Kuzina, A., Bekkers, E.~J., Tomczak, J.~M., and Hoogendoorn, M.
\newblock Ckconv: Continuous kernel convolution for sequential data.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.

\bibitem[Sanh et~al.(2020)Sanh, Wolf, and Rush]{sanh2020movement}
Sanh, V., Wolf, T., and Rush, A.
\newblock Movement pruning: Adaptive sparsity by fine-tuning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 20378--20389, 2020.

\bibitem[Sindhwani et~al.(2015)Sindhwani, Sainath, and
  Kumar]{sindhwani2015structured}
Sindhwani, V., Sainath, T., and Kumar, S.
\newblock Structured transforms for small-footprint deep learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Smith et~al.(2022)Smith, Warrington, and
  Linderman]{smith2022simplified}
Smith, J.~T., Warrington, A., and Linderman, S.~W.
\newblock Simplified state space layers for sequence modeling.
\newblock \emph{arXiv preprint arXiv:2208.04933}, 2022.

\bibitem[Smith et~al.(1997)]{smith1997scientist}
Smith, S.~W. et~al.
\newblock The scientist and engineer's guide to digital signal processing,
  1997.

\bibitem[Tang et~al.(2022)Tang, Dunnmon, Qu, Saab, Lee-Messer, and
  Rubin]{tang2022spatiotemporal}
Tang, S., Dunnmon, J.~A., Qu, L., Saab, K.~K., Lee-Messer, C., and Rubin, D.~L.
\newblock Spatiotemporal modeling of multivariate signals with graph neural
  networks and structured state space models.
\newblock \emph{arXiv preprint arXiv:2211.11176}, 2022.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler]{tay2020long}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang,
  L., Ruder, S., and Metzler, D.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Thomas et~al.(2022)Thomas, R{\'e}, and Poldrack]{thomas2022self}
Thomas, A.~W., R{\'e}, C., and Poldrack, R.~A.
\newblock Self-supervised learning of brain dynamics from broad neuroimaging
  data.
\newblock \emph{arXiv preprint arXiv:2206.11417}, 2022.

\bibitem[Trockman \& Kolter(2022)Trockman and Kolter]{trockman2022patches}
Trockman, A. and Kolter, J.~Z.
\newblock Patches are all you need?
\newblock \emph{arXiv preprint arXiv:2201.09792}, 2022.

\bibitem[Varol et~al.(2017)Varol, Laptev, and Schmid]{varol2017long}
Varol, G., Laptev, I., and Schmid, C.
\newblock Long-term temporal convolutions for action recognition.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 40\penalty0 (6):\penalty0 1510--1517, 2017.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wu et~al.(2022)Wu, Rabe, Hutchins, and Szegedy]{wu2022memorizing}
Wu, Y., Rabe, M.~N., Hutchins, D., and Szegedy, C.
\newblock Memorizing transformers.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Saab, Poli, Goel, Dao, and
  R\'{e}]{zhang2023effectively}
Zhang, M., Saab, K.~K., Poli, M., Goel, K., Dao, T., and R\'{e}, C.
\newblock Effectively modeling time series with simple discrete state spaces.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Peng, Zhang, Li, Xiong, and
  Zhang]{zhou2021informer}
Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W.
\newblock Informer: Beyond efficient transformer for long sequence time-series
  forecasting.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~35, pp.\  11106--11115, 2021.

\bibitem[Zhou et~al.(2022)Zhou, Poli, Xu, Massaroli, and Ermon]{zhou2022deep}
Zhou, L., Poli, M., Xu, W., Massaroli, S., and Ermon, S.
\newblock Deep latent state space models for time-series generation.
\newblock \emph{arXiv preprint arXiv:2212.12749}, 2022.

\end{thebibliography}
