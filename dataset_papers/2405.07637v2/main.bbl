\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and Szepesv{\'a}ri]{abbasi2011improved}
Abbasi-Yadkori, Y., P{\'a}l, D., and Szepesv{\'a}ri, C.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\  2312--2320, 2011.

\bibitem[Agrawal \& Goyal(2013)Agrawal and Goyal]{agrawal2013thompson}
Agrawal, S. and Goyal, N.
\newblock Thompson sampling for contextual bandits with linear payoffs.
\newblock In \emph{International conference on machine learning}, pp.\  127--135. PMLR, 2013.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  263--272. PMLR, 2017.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2020provably}
Cai, Q., Yang, Z., Jin, C., and Wang, Z.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1283--1294. PMLR, 2020.

\bibitem[Chatterji et~al.(2021)Chatterji, Pacchiano, Bartlett, and Jordan]{chatterji2021theory}
Chatterji, N., Pacchiano, A., Bartlett, P., and Jordan, M.
\newblock On the theory of reinforcement learning with once-per-episode feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 3401--3412, 2021.

\bibitem[Chen et~al.(2022)Chen, Zhong, Yang, Wang, and Wang]{chen2022human}
Chen, X., Zhong, H., Yang, Z., Wang, Z., and Wang, L.
\newblock Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3773--3793. PMLR, 2022.

\bibitem[Cohen et~al.(2019)Cohen, Koren, and Mansour]{cohen2019learning}
Cohen, A., Koren, T., and Mansour, Y.
\newblock Learning linear-quadratic regulators efficiently with only $\sqrt{T}$ regret.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1300--1309, 2019.

\bibitem[Cohen et~al.(2021)Cohen, Kaplan, Koren, and Mansour]{pmlr-v134-cohen21a}
Cohen, A., Kaplan, H., Koren, T., and Mansour, Y.
\newblock Online markov decision processes with aggregate bandit feedback.
\newblock In Belkin, M. and Kpotufe, S. (eds.), \emph{Proceedings of Thirty Fourth Conference on Learning Theory}, volume 134 of \emph{Proceedings of Machine Learning Research}, pp.\  1301--1329. PMLR, 15--19 Aug 2021.

\bibitem[Efroni et~al.(2021)Efroni, Merlis, and Mannor]{Efroni_Merlis_Mannor_2021}
Efroni, Y., Merlis, N., and Mannor, S.
\newblock Reinforcement learning with trajectory feedback.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 35\penalty0 (8):\penalty0 7288--7295, May 2021.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1861--1870. PMLR, 2018.

\bibitem[Howson et~al.(2023)Howson, Pike-Burke, and Filippi]{howson2023optimism}
Howson, B., Pike-Burke, C., and Filippi, S.
\newblock Optimism and delays in episodic reinforcement learning.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  6061--6094. PMLR, 2023.

\bibitem[Jain et~al.(2013)Jain, Wojcik, Joachims, and Saxena]{jain2013learning}
Jain, A., Wojcik, B., Joachims, T., and Saxena, A.
\newblock Learning trajectory preferences for manipulators via iterative improvement.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11:\penalty0 1563--1600, 2010.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Jin, Luo, Sra, and Yu]{jin2020learning}
Jin, C., Jin, T., Luo, H., Sra, S., and Yu, T.
\newblock Learning adversarial markov decision processes with bandit feedback and unknown transition.
\newblock In \emph{International Conference on Machine Learning}, pp.\  4860--4869. PMLR, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, Wang, and Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143. PMLR, 2020{\natexlab{b}}.

\bibitem[Jin et~al.(2022)Jin, Lancewicki, Luo, Mansour, and Rosenberg]{jin2022near}
Jin, T., Lancewicki, T., Luo, H., Mansour, Y., and Rosenberg, A.
\newblock Near-optimal regret for adversarial mdp with delayed bandit feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 33469--33481, 2022.

\bibitem[Lancewicki et~al.(2022)Lancewicki, Rosenberg, and Mansour]{lancewicki2020learning}
Lancewicki, T., Rosenberg, A., and Mansour, Y.
\newblock Learning adversarial markov decision processes with delayed feedback.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pp.\  7281--7289, 2022.

\bibitem[Lancewicki et~al.(2023)Lancewicki, Rosenberg, and Sotnikov]{lancewicki2023learning}
Lancewicki, T., Rosenberg, A., and Sotnikov, D.
\newblock Delay-adapted policy optimization and improved regret for adversarial {MDP} with delayed bandit feedback.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), \emph{International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, {USA}}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  18482--18534. {PMLR}, 2023.

\bibitem[Luo et~al.(2021)Luo, Wei, and Lee]{luo2021policy}
Luo, H., Wei, C.-Y., and Lee, C.-W.
\newblock Policy optimization in adversarial mdps: Improved exploration via dilated bonuses.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare, M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Rosenberg \& Mansour(2019{\natexlab{a}})Rosenberg and Mansour]{rosenberg2019bandit}
Rosenberg, A. and Mansour, Y.
\newblock Online stochastic shortest path with bandit feedback and unknown transition function.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\  2209--2218, 2019{\natexlab{a}}.

\bibitem[Rosenberg \& Mansour(2019{\natexlab{b}})Rosenberg and Mansour]{rosenberg2019online}
Rosenberg, A. and Mansour, Y.
\newblock Online convex optimization in adversarial markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5478--5486. PMLR, 2019{\natexlab{b}}.

\bibitem[Saha et~al.(2023)Saha, Pacchiano, and Lee]{saha2023dueling}
Saha, A., Pacchiano, A., and Lee, J.
\newblock Dueling rl: Reinforcement learning with trajectory preferences.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pp.\  6263--6289. PMLR, 2023.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\  1889--1897. PMLR, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shani et~al.(2020)Shani, Efroni, Rosenberg, and Mannor]{shani2020optimistic}
Shani, L., Efroni, Y., Rosenberg, A., and Mannor, S.
\newblock Optimistic policy optimization with bandit feedback.
\newblock In \emph{International Conference on Machine Learning}, pp.\  8604--8613. PMLR, 2020.

\bibitem[Sherman et~al.(2023)Sherman, Cohen, Koren, and Mansour]{sherman2023rate}
Sherman, U., Cohen, A., Koren, T., and Mansour, Y.
\newblock Rate-optimal policy optimization for linear markov decision processes.
\newblock \emph{arXiv preprint arXiv:2308.14642}, 2023.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{stiennon2020learning}
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P.~F.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 3008--3021, 2020.

\bibitem[Tiapkin et~al.(2023)Tiapkin, Belomestny, Calandriello, Moulines, Munos, Naumov, pierre perrault, Valko, and MENARD]{tiapkin2023modelfree}
Tiapkin, D., Belomestny, D., Calandriello, D., Moulines, E., Munos, R., Naumov, A., pierre perrault, Valko, M., and MENARD, P.
\newblock Model-free posterior sampling via learning rate randomization.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Wagenmaker et~al.(2022)Wagenmaker, Chen, Simchowitz, Du, and Jamieson]{wagenmaker2022reward}
Wagenmaker, A.~J., Chen, Y., Simchowitz, M., Du, S., and Jamieson, K.
\newblock Reward-free rl is no harder than reward-aware rl in linear markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\  22430--22456. PMLR, 2022.

\bibitem[Wang et~al.(2023)Wang, Liu, and Jin]{wang2023rlhf}
Wang, Y., Liu, Q., and Jin, C.
\newblock Is {RLHF} more difficult than standard {RL}? a theoretical perspective.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, F{\"u}rnkranz, et~al.]{wirth2017survey}
Wirth, C., Akrour, R., Neumann, G., F{\"u}rnkranz, J., et~al.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0 (136):\penalty0 1--46, 2017.

\bibitem[Wu \& Sun(2023)Wu and Sun]{wu2023making}
Wu, R. and Sun, W.
\newblock Making rl with preference-based feedback efficient via randomization.
\newblock \emph{arXiv preprint arXiv:2310.14554}, 2023.

\bibitem[Xu et~al.(2022)Xu, Wang, Zou, and Liang]{xu2022provably}
Xu, T., Wang, Y., Zou, S., and Liang, Y.
\newblock Provably efficient offline reinforcement learning with trajectory-wise reward.
\newblock \emph{arXiv preprint arXiv:2206.06426}, 2022.

\bibitem[Zanette et~al.(2020)Zanette, Lazaric, Kochenderfer, and Brunskill]{zanette2020learning}
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10978--10989. PMLR, 2020.

\bibitem[Zhan et~al.(2023{\natexlab{a}})Zhan, Uehara, Kallus, Lee, and Sun]{zhan2023provable}
Zhan, W., Uehara, M., Kallus, N., Lee, J.~D., and Sun, W.
\newblock Provable offline reinforcement learning with human feedback.
\newblock \emph{arXiv preprint arXiv:2305.14816}, 2023{\natexlab{a}}.

\bibitem[Zhan et~al.(2023{\natexlab{b}})Zhan, Uehara, Sun, and Lee]{zhan2023query}
Zhan, W., Uehara, M., Sun, W., and Lee, J.~D.
\newblock How to query human feedback efficiently in rl?
\newblock \emph{arXiv preprint arXiv:2305.18505}, 2023{\natexlab{b}}.

\bibitem[Zimin \& Neu(2013)Zimin and Neu]{zimin2013online}
Zimin, A. and Neu, G.
\newblock Online learning in episodic markovian decision processes by relative entropy policy search.
\newblock In \emph{Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States}, pp.\  1583--1591, 2013.

\end{thebibliography}
