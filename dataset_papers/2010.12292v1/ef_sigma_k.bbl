\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal and Duchi(2011)]{agarwal2011distributed}
A.~Agarwal and J.~C. Duchi.
\newblock Distributed delayed stochastic optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  873--881, 2011.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
D.~Alistarh, D.~Grubic, J.~Li, R.~Tomioka, and M.~Vojnovic.
\newblock {QSGD}: {C}ommunication-efficient {SGD} via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem[Arjevani et~al.(2018)Arjevani, Shamir, and Srebro]{arjevani2018tight}
Y.~Arjevani, O.~Shamir, and N.~Srebro.
\newblock A tight convergence analysis for stochastic gradient descent with
  delayed updates.
\newblock \emph{arXiv preprint arXiv:1806.10188}, 2018.

\bibitem[Basu et~al.(2019)Basu, Data, Karakus, and Diggavi]{basu2019qsparse}
D.~Basu, D.~Data, C.~Karakus, and S.~Diggavi.
\newblock Qsparse-local-{SGD}: Distributed {SGD} with quantization,
  sparsification and local computations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  14668--14679, 2019.

\bibitem[Bernstein et~al.(2019)Bernstein, Zhao, Azizzadenesheli, and
  Anandkumar]{Bernstein2019signSGDWM}
J.~Bernstein, J.~Zhao, K.~Azizzadenesheli, and A.~Anandkumar.
\newblock Sign{SGD} with majority vote is communication efficient and fault
  tolerant.
\newblock In \emph{ICLR}, 2019.

\bibitem[Bertsekas and Tsitsiklis(1989)]{bertsekas1989parallel}
D.~P. Bertsekas and J.~N. Tsitsiklis.
\newblock \emph{Parallel and distributed computation: numerical methods},
  volume~23.
\newblock Prentice hall Englewood Cliffs, NJ, 1989.

\bibitem[Beznosikov et~al.(2020)Beznosikov, Horv{\'a}th, Richt{\'a}rik, and
  Safaryan]{beznosikov2020biased}
A.~Beznosikov, S.~Horv{\'a}th, P.~Richt{\'a}rik, and M.~Safaryan.
\newblock On biased compression for distributed learning.
\newblock \emph{arXiv preprint arXiv:2002.12410}, 2020.

\bibitem[Chang and Lin(2011)]{chang2011libsvm}
C.-C. Chang and C.-J. Lin.
\newblock {LIBSVM}: A library for support vector machines.
\newblock \emph{ACM transactions on intelligent systems and technology (TIST)},
  2\penalty0 (3):\penalty0 1--27, 2011.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{SAGA}
A.~Defazio, F.~Bach, and S.~Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem[Feyzmahdavian et~al.(2016)Feyzmahdavian, Aytekin, and
  Johansson]{feyzmahdavian2016asynchronous}
H.~R. Feyzmahdavian, A.~Aytekin, and M.~Johansson.
\newblock An asynchronous mini-batch algorithm for regularized stochastic
  optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 61\penalty0
  (12):\penalty0 3740--3754, 2016.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Hanzely, and
  Richt{\'a}rik]{gorbunov2019unified}
E.~Gorbunov, F.~Hanzely, and P.~Richt{\'a}rik.
\newblock A unified theory of {SGD}: Variance reduction, sampling, quantization
  and coordinate descent.
\newblock In \emph{The 23rd International Conference on Artificial Intelligence
  and Statistics (AISTATS 2020)}, 2020.

\bibitem[Gower et~al.(2018)Gower, Richt{\'a}rik, and Bach]{gower2018stochastic}
R.~M. Gower, P.~Richt{\'a}rik, and F.~Bach.
\newblock Stochastic quasi-gradient methods: Variance reduction via {J}acobian
  sketching.
\newblock \emph{arXiv preprint arXiv:1805.02632}, 2018.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and
  Richt{\'a}rik]{gower2019sgd}
R.~M. Gower, N.~Loizou, X.~Qian, A.~Sailanbayev, E.~Shulgin, and
  P.~Richt{\'a}rik.
\newblock {SGD}: General analysis and improved rates.
\newblock In \emph{International Conference on Machine Learning}, pages
  5200--5209, 2019.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
P.~Goyal, P.~Doll{\'a}r, R.~Girshick, P.~Noordhuis, L.~Wesolowski, A.~Kyrola,
  A.~Tulloch, Y.~Jia, and K.~He.
\newblock Accurate, large minibatch {SGD}: training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Haddadpour and Mahdavi(2019)]{haddadpour2019convergence}
F.~Haddadpour and M.~Mahdavi.
\newblock On the convergence of local descent methods in federated learning.
\newblock \emph{arXiv preprint arXiv:1910.14425}, 2019.

\bibitem[Hanzely et~al.(2018)Hanzely, Mishchenko, and
  Richt{\'a}rik]{hanzely2018sega}
F.~Hanzely, K.~Mishchenko, and P.~Richt{\'a}rik.
\newblock {SEGA}: Variance reduction via gradient sketching.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2082--2093, 2018.

\bibitem[Hofmann et~al.(2015)Hofmann, Lucchi, Lacoste-Julien, and
  McWilliams]{hofmann2015variance}
T.~Hofmann, A.~Lucchi, S.~Lacoste-Julien, and B.~McWilliams.
\newblock Variance reduced stochastic gradient descent with neighbors.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2305--2313, 2015.

\bibitem[Horv\'{a}th et~al.(2019)Horv\'{a}th, Ho, \v{L}udov\'{i}t Horv\'{a}th,
  Sahu, Canini, and Richt\'{a}rik]{Cnat}
S.~Horv\'{a}th, C.-Y. Ho, \v{L}udov\'{i}t Horv\'{a}th, A.~N. Sahu, M.~Canini,
  and P.~Richt\'{a}rik.
\newblock Natural compression for distributed deep learning.
\newblock \emph{arXiv preprint arXiv:1905.10988}, 2019.

\bibitem[Horv{\'a}th et~al.(2019)Horv{\'a}th, Kovalev, Mishchenko, Stich, and
  Richt{\'a}rik]{horvath2019stochastic}
S.~Horv{\'a}th, D.~Kovalev, K.~Mishchenko, S.~Stich, and P.~Richt{\'a}rik.
\newblock Stochastic distributed learning with gradient quantization and
  variance reduction.
\newblock \emph{arXiv preprint arXiv:1904.05115}, 2019.

\bibitem[Johnson and Zhang(2013)]{SVRG}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems 26}, pages
  315--323, 2013.

\bibitem[Kairouz et~al.(2019)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, et~al.]{kairouz2019advances}
P.~Kairouz, H.~B. McMahan, B.~Avent, A.~Bellet, M.~Bennis, A.~N. Bhagoji,
  K.~Bonawitz, Z.~Charles, G.~Cormode, R.~Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{arXiv preprint arXiv:1912.04977}, 2019.

\bibitem[Karimireddy et~al.(2019{\natexlab{a}})Karimireddy, Kale, Mohri, Reddi,
  Stich, and Suresh]{karimireddy2019scaffold}
S.~P. Karimireddy, S.~Kale, M.~Mohri, S.~J. Reddi, S.~U. Stich, and A.~T.
  Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock \emph{arXiv preprint arXiv:1910.06378}, 2019{\natexlab{a}}.

\bibitem[Karimireddy et~al.(2019{\natexlab{b}})Karimireddy, Rebjock, Stich, and
  Jaggi]{karimireddy2019error}
S.~P. Karimireddy, Q.~Rebjock, S.~Stich, and M.~Jaggi.
\newblock Error feedback fixes sign{SGD} and other gradient compression
  schemes.
\newblock In \emph{International Conference on Machine Learning}, pages
  3252--3261, 2019{\natexlab{b}}.

\bibitem[Khaled et~al.(2020{\natexlab{a}})Khaled, Mishchenko, and
  Richt{\'a}rik]{khaled2020tighter}
A.~Khaled, K.~Mishchenko, and P.~Richt{\'a}rik.
\newblock Tighter theory for local {SGD} on identical and heterogeneous data.
\newblock In \emph{The 23rd International Conference on Artificial Intelligence
  and Statistics (AISTATS 2020)}, 2020{\natexlab{a}}.

\bibitem[Khaled et~al.(2020{\natexlab{b}})Khaled, Sebbouh, Loizou, Gower, and
  Richt{\'a}rik]{khaled2020unified}
A.~Khaled, O.~Sebbouh, N.~Loizou, R.~M. Gower, and P.~Richt{\'a}rik.
\newblock Unified analysis of stochastic gradient methods for composite convex
  and smooth optimization.
\newblock \emph{arXiv preprint arXiv:2006.11573}, 2020{\natexlab{b}}.

\bibitem[Khirirat et~al.(2018)Khirirat, Feyzmahdavian, and
  Johansson]{khirirat2018distributed}
S.~Khirirat, H.~R. Feyzmahdavian, and M.~Johansson.
\newblock Distributed learning with compressed gradients.
\newblock \emph{arXiv preprint arXiv:1806.06573}, 2018.

\bibitem[Koloskova et~al.(2019)Koloskova, Stich, and
  Jaggi]{koloskova2019decentralized}
A.~Koloskova, S.~Stich, and M.~Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In \emph{International Conference on Machine Learning}, pages
  3478--3487, 2019.

\bibitem[Koloskova et~al.(2020{\natexlab{a}})Koloskova, Lin, Stich, and
  Jaggi]{KoloskovaLSJ19decentralized}
A.~Koloskova, T.~Lin, S.~U. Stich, and M.~Jaggi.
\newblock Decentralized deep learning with arbitrary communication compression.
\newblock \emph{ICLR}, page arXiv:1907.09356, 2020{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/1907.09356}.

\bibitem[Koloskova et~al.(2020{\natexlab{b}})Koloskova, Loizou, Boreiri, Jaggi,
  and Stich]{koloskova2020unified}
A.~Koloskova, N.~Loizou, S.~Boreiri, M.~Jaggi, and S.~U. Stich.
\newblock A unified theory of decentralized {SGD} with changing topology and
  local updates.
\newblock \emph{arXiv preprint arXiv:2003.10422}, 2020{\natexlab{b}}.

\bibitem[Kone{\v{c}}n{\'y} et~al.(2016)Kone{\v{c}}n{\'y}, McMahan, Yu,
  Richt{\'a}rik, Suresh, and Bacon]{konecny2016federated}
J.~Kone{\v{c}}n{\'y}, H.~B. McMahan, F.~X. Yu, P.~Richt{\'a}rik, A.~T. Suresh,
  and D.~Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock \emph{arXiv preprint arXiv:1610.05492}, 2016.

\bibitem[Kovalev et~al.(2020)Kovalev, Horv\'{a}th, and
  Richt\'{a}rik]{kovalev2019don}
D.~Kovalev, S.~Horv\'{a}th, and P.~Richt\'{a}rik.
\newblock Don’t jump through hoops and remove those loops: {SVRG} and
  {K}atyusha are better without the outer loop.
\newblock In \emph{Proceedings of the 31st International Conference on
  Algorithmic Learning Theory}, 2020.

\bibitem[Leblond et~al.(2018)Leblond, Pedregosa, and
  Lacoste-Julien]{leblond2018improved}
R.~Leblond, F.~Pedregosa, and S.~Lacoste-Julien.
\newblock Improved asynchronous parallel optimization analysis for stochastic
  incremental methods.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 3140--3207, 2018.

\bibitem[Lian et~al.(2015)Lian, Huang, Li, and Liu]{lian2015asynchronous}
X.~Lian, Y.~Huang, Y.~Li, and J.~Liu.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2737--2745, 2015.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
X.~Lian, C.~Zhang, H.~Zhang, C.-J. Hsieh, W.~Zhang, and J.~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5330--5340, 2017.

\bibitem[Liu et~al.(2019)Liu, Li, Tang, and Yan]{liu2019double}
X.~Liu, Y.~Li, J.~Tang, and M.~Yan.
\newblock A double residual compression algorithm for efficient distributed
  learning.
\newblock \emph{arXiv preprint arXiv:1910.07561}, 2019.

\bibitem[Mania et~al.(2017)Mania, Pan, Papailiopoulos, Recht, Ramchandran, and
  Jordan]{mania2017perturbed}
H.~Mania, X.~Pan, D.~Papailiopoulos, B.~Recht, K.~Ramchandran, and M.~I.
  Jordan.
\newblock Perturbed iterate analysis for asynchronous stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2202--2229, 2017.

\bibitem[Mishchenko et~al.(2019)Mishchenko, Gorbunov, Tak{\'a}{\v{c}}, and
  Richt{\'a}rik]{mishchenko2019distributed}
K.~Mishchenko, E.~Gorbunov, M.~Tak{\'a}{\v{c}}, and P.~Richt{\'a}rik.
\newblock Distributed learning with compressed gradient differences.
\newblock \emph{arXiv preprint arXiv:1901.09269}, 2019.

\bibitem[Nesterov(2018)]{nesterov2018lectures}
Y.~Nesterov.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Nguyen et~al.(2018)Nguyen, Nguyen, Dijk, Richt\'{a}rik, Scheinberg,
  and Tak{\'a}{\v{c}}]{nguyen2018sgd}
L.~Nguyen, P.~H. Nguyen, M.~Dijk, P.~Richt\'{a}rik, K.~Scheinberg, and
  M.~Tak{\'a}{\v{c}}.
\newblock {SGD} and {H}ogwild! convergence without the bounded gradients
  assumption.
\newblock In \emph{International Conference on Machine Learning}, pages
  3750--3758, 2018.

\bibitem[Philippenko and Dieuleveut(2020)]{philippenko2020artemis}
C.~Philippenko and A.~Dieuleveut.
\newblock Artemis: tight convergence guarantees for bidirectional compression
  in federated learning.
\newblock \emph{arXiv preprint arXiv:2006.14591}, 2020.

\bibitem[Robbins and Monro(1985)]{robbins1985stochastic}
H.~Robbins and S.~Monro.
\newblock A stochastic approximation method.
\newblock In \emph{Herbert Robbins Selected Papers}, pages 102--109. Springer,
  1985.

\bibitem[Shamir et~al.(2014)Shamir, Srebro, and Zhang]{shamir2014communication}
O.~Shamir, N.~Srebro, and T.~Zhang.
\newblock Communication-efficient distributed optimization using an approximate
  {N}ewton-type method.
\newblock In \emph{International conference on machine learning}, pages
  1000--1008, 2014.

\bibitem[Stich(2019{\natexlab{a}})]{Stich18local}
S.~U. Stich.
\newblock Local {SGD} converges fast and communicates little.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  page arXiv:1805.09767, 2019{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/1805.09767}.

\bibitem[Stich(2019{\natexlab{b}})]{stich2019unified}
S.~U. Stich.
\newblock Unified optimal analysis of the (stochastic) gradient method.
\newblock \emph{arXiv preprint arXiv:1907.04232}, 2019{\natexlab{b}}.

\bibitem[Stich and Karimireddy(2019)]{stich2019error}
S.~U. Stich and S.~P. Karimireddy.
\newblock The error-feedback framework: Better rates for {SGD} with delayed
  gradients and compressed communication.
\newblock \emph{arXiv preprint arXiv:1909.05350}, 2019.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{stich2018sparsified}
S.~U. Stich, J.-B. Cordonnier, and M.~Jaggi.
\newblock Sparsified {SGD} with memory.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4447--4458, 2018.

\bibitem[Tang et~al.(2019)Tang, Yu, Lian, Zhang, and
  Liu]{tang2019doublesqueeze}
H.~Tang, C.~Yu, X.~Lian, T.~Zhang, and J.~Liu.
\newblock {D}ouble{S}queeze: Parallel stochastic gradient descent with
  double-pass error-compensated compression.
\newblock In \emph{International Conference on Machine Learning}, pages
  6155--6165, 2019.

\bibitem[Wen et~al.(2017)Wen, Xu, Yan, Wu, Wang, Chen, and Li]{wen2017terngrad}
W.~Wen, C.~Xu, F.~Yan, C.~Wu, Y.~Wang, Y.~Chen, and H.~Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1509--1519, 2017.

\bibitem[Woodworth et~al.(2020)Woodworth, Patel, Stich, Dai, Bullins, McMahan,
  Shamir, and Srebro]{woodworth2020local}
B.~Woodworth, K.~K. Patel, S.~U. Stich, Z.~Dai, B.~Bullins, H.~B. McMahan,
  O.~Shamir, and N.~Srebro.
\newblock Is local {SGD} better than minibatch {SGD}?
\newblock \emph{arXiv preprint arXiv:2002.07839}, 2020.

\end{thebibliography}
