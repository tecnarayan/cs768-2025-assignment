\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Albooyeh et~al.(2019)Albooyeh, Bertolini, and Ravanbakhsh]{albooyeh2019incidence}
Marjan Albooyeh, Daniele Bertolini, and Siamak Ravanbakhsh.
\newblock Incidence networks for geometric deep learning.
\newblock \emph{arXiv preprint arXiv:1905.11460}, 2019.

\bibitem[Azizian and Lelarge(2020)]{azizian2020expressive}
Waiss Azizian and Marc Lelarge.
\newblock Expressive power of invariant and equivariant graph neural networks.
\newblock \emph{arXiv preprint arXiv:2006.15646}, 2020.

\bibitem[Bar-Shalom et~al.(2024)Bar-Shalom, Bevilacqua, and Maron]{bar-shalom2024subgraphormer}
Guy Bar-Shalom, Beatrice Bevilacqua, and Haggai Maron.
\newblock Subgraphormer: Unifying subgraph {GNN}s and graph transformers via graph products.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.
\newblock URL \url{https://openreview.net/forum?id=6djDWVTUEq}.

\bibitem[Bevilacqua et~al.(2022)Bevilacqua, Frasca, Lim, Srinivasan, Cai, Balamurugan, Bronstein, and Maron]{bevilacqua2021equivariant}
Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael~M Bronstein, and Haggai Maron.
\newblock Equivariant subgraph aggregation networks.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Bevilacqua et~al.(2024)Bevilacqua, Eliasof, Meirom, Ribeiro, and Maron]{bevilacqua2023efficient}
Beatrice Bevilacqua, Moshe Eliasof, Eli Meirom, Bruno Ribeiro, and Haggai Maron.
\newblock Efficient subgraph gnns by learning effective selection policies.
\newblock \emph{International Conference on Learning Representations}, 2024.

\bibitem[Biewald(2020)]{wandb}
Lukas Biewald.
\newblock Experiment tracking with weights and biases, 2020.
\newblock URL \url{https://www.wandb.com/}.
\newblock Software available from wandb.com.

\bibitem[Bresson and Laurent(2017)]{bresson2017residual}
Xavier Bresson and Thomas Laurent.
\newblock Residual gated graph convnets.
\newblock \emph{arXiv preprint arXiv:1711.07553}, 2017.

\bibitem[Cotta et~al.(2021)Cotta, Morris, and Ribeiro]{cotta2021reconstruction}
Leonardo Cotta, Christopher Morris, and Bruno Ribeiro.
\newblock Reconstruction for powerful graph representations.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~34, 2021.

\bibitem[Dwivedi et~al.(2022)Dwivedi, Ramp{\'a}{\v{s}}ek, Galkin, Parviz, Wolf, Luu, and Beaini]{dwivedi2022long}
Vijay~Prakash Dwivedi, Ladislav Ramp{\'a}{\v{s}}ek, Michael Galkin, Ali Parviz, Guy Wolf, Anh~Tuan Luu, and Dominique Beaini.
\newblock Long range graph benchmark.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 22326--22340, 2022.

\bibitem[Dwivedi et~al.(2023)Dwivedi, Joshi, Luu, Laurent, Bengio, and Bresson]{dwivedi2023benchmarking}
Vijay~Prakash Dwivedi, Chaitanya~K Joshi, Anh~Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.
\newblock Benchmarking graph neural networks.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (43):\penalty0 1--48, 2023.

\bibitem[Fey and Lenssen(2019)]{fey2019fast}
Matthias Fey and Jan~Eric Lenssen.
\newblock Fast graph representation learning with pytorch geometric.
\newblock \emph{arXiv preprint arXiv:1903.02428}, 2019.

\bibitem[Frasca et~al.(2022)Frasca, Bevilacqua, Bronstein, and Maron]{frasca2022understanding}
Fabrizio Frasca, Beatrice Bevilacqua, Michael Bronstein, and Haggai Maron.
\newblock Understanding and extending subgraph gnns by rethinking their symmetries.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 31376--31390, 2022.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and Dahl]{gilmer2017neural}
Justin Gilmer, Samuel~S Schoenholz, Patrick~F Riley, Oriol Vinyals, and George~E Dahl.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International conference on machine learning}, pages 1263--1272. PMLR, 2017.

\bibitem[G{\'o}mez-Bombarelli et~al.(2018)G{\'o}mez-Bombarelli, Wei, Duvenaud, Hern{\'a}ndez-Lobato, S{\'a}nchez-Lengeling, Sheberla, Aguilera-Iparraguirre, Hirzel, Adams, and Aspuru-Guzik]{gomez2018automatic}
Rafael G{\'o}mez-Bombarelli, Jennifer~N Wei, David Duvenaud, Jos{\'e}~Miguel Hern{\'a}ndez-Lobato, Benjam{\'\i}n S{\'a}nchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy~D Hirzel, Ryan~P Adams, and Al{\'a}n Aspuru-Guzik.
\newblock Automatic chemical design using a data-driven continuous representation of molecules.
\newblock \emph{ACS central science}, 4\penalty0 (2):\penalty0 268--276, 2018.

\bibitem[Hu et~al.(2019)Hu, Liu, Gomes, Zitnik, Liang, Pande, and Leskovec]{hu2019strategies}
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.
\newblock Strategies for pre-training graph neural networks.
\newblock \emph{arXiv preprint arXiv:1905.12265}, 2019.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and Leskovec]{hu2020open}
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 22118--22133, 2020.

\bibitem[Huang et~al.(2022)Huang, Peng, Ma, and Zhang]{huang2022boosting}
Yinan Huang, Xingang Peng, Jianzhu Ma, and Muhan Zhang.
\newblock Boosting the cycle counting power of graph neural networks with i$^{2}$-gnns.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Keriven and Peyr{\'e}(2019)]{keriven2019universal}
Nicolas Keriven and Gabriel Peyr{\'e}.
\newblock Universal invariant and equivariant graph neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Kipf and Welling(2016)]{kipf2016semi}
Thomas~N Kipf and Max Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock \emph{International Conference on Learning Representations}, 2016.

\bibitem[Kong et~al.(2024)Kong, Feng, Liu, Tao, Chen, and Zhang]{kong2024mag}
Lecheng Kong, Jiarui Feng, Hao Liu, Dacheng Tao, Yixin Chen, and Muhan Zhang.
\newblock Mag-gnn: Reinforcement learning boosted graph neural network.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Kwon et~al.(2021)Kwon, Kim, Park, and Choi]{kwon2021asam}
Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In~Kwon Choi.
\newblock Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Maron et~al.(2018)Maron, Ben-Hamu, Shamir, and Lipman]{maron2018invariant}
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman.
\newblock Invariant and equivariant graph networks.
\newblock \emph{arXiv preprint arXiv:1812.09902}, 2018.

\bibitem[Maron et~al.(2019)Maron, Ben-Hamu, Serviansky, and Lipman]{maron2019provably}
Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman.
\newblock Provably powerful graph networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Morris et~al.(2019)Morris, Ritzert, Fey, Hamilton, Lenssen, Rattan, and Grohe]{morris2019weisfeiler}
Christopher Morris, Martin Ritzert, Matthias Fey, William~L Hamilton, Jan~Eric Lenssen, Gaurav Rattan, and Martin Grohe.
\newblock Weisfeiler and leman go neural: Higher-order graph neural networks.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~33, pages 4602--4609, 2019.

\bibitem[Morris et~al.(2021)Morris, Lipman, Maron, Rieck, Kriege, Grohe, Fey, and Borgwardt]{morris2021weisfeiler}
Christopher Morris, Yaron Lipman, Haggai Maron, Bastian Rieck, Nils~M Kriege, Martin Grohe, Matthias Fey, and Karsten Borgwardt.
\newblock Weisfeiler and leman go machine learning: The story so far.
\newblock \emph{arXiv preprint arXiv:2112.09992}, 2021.

\bibitem[Papp and Wattenhofer(2022{\natexlab{a}})]{DBLP:journals/corr/abs-2201-12884}
P{\'{a}}l~Andr{\'{a}}s Papp and Roger Wattenhofer.
\newblock A theoretical comparison of graph neural network extensions.
\newblock \emph{CoRR}, abs/2201.12884, 2022{\natexlab{a}}.

\bibitem[Papp and Wattenhofer(2022{\natexlab{b}})]{papp2022theoretical}
P{\'a}l~Andr{\'a}s Papp and Roger Wattenhofer.
\newblock A theoretical comparison of graph neural network extensions.
\newblock In \emph{International Conference on Machine Learning}, pages 17323--17345. PMLR, 2022{\natexlab{b}}.

\bibitem[Papp et~al.(2021)Papp, Martinkus, Faber, and Wattenhofer]{papp2021dropgnn}
P{\'a}l~Andr{\'a}s Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer.
\newblock Dropgnn: Random dropouts increase the expressiveness of graph neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 21997--22009, 2021.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Qian et~al.(2022)Qian, Rattan, Geerts, Niepert, and Morris]{qian2022ordered}
Chendi Qian, Gaurav Rattan, Floris Geerts, Mathias Niepert, and Christopher Morris.
\newblock Ordered subgraph aggregation networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 21030--21045, 2022.

\bibitem[Ravanbakhsh et~al.(2017)Ravanbakhsh, Schneider, and Poczos]{ravanbakhsh2017equivariance}
Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos.
\newblock Equivariance through parameter-sharing.
\newblock In \emph{International conference on machine learning}, pages 2892--2901. PMLR, 2017.

\bibitem[Sterling and Irwin(2015)]{sterling2015zinc}
Teague Sterling and John~J Irwin.
\newblock Zinc 15--ligand discovery for everyone.
\newblock \emph{Journal of chemical information and modeling}, 55\penalty0 (11):\penalty0 2324--2337, 2015.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2017)Veli{\v{c}}kovi{\'c}, Cucurull, Casanova, Romero, Lio, and Bengio]{velivckovic2017graph}
Petar Veli{\v{c}}kovi{\'c}, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Von~Luxburg(2007)]{von2007tutorial}
Ulrike Von~Luxburg.
\newblock A tutorial on spectral clustering.
\newblock \emph{Statistics and computing}, 17:\penalty0 395--416, 2007.

\bibitem[Wood and Shawe-Taylor(1996)]{wood1996representation}
Jeffrey Wood and John Shawe-Taylor.
\newblock Representation theory and invariant neural networks.
\newblock \emph{Discrete applied mathematics}, 69\penalty0 (1-2):\penalty0 33--60, 1996.

\bibitem[Xu et~al.(2018)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful}
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
\newblock How powerful are graph neural networks?
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Yun et~al.(2019)Yun, Sra, and Jadbabaie]{yun2019small}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock Small relu networks are powerful memorizers: a tight analysis of memorization capacity.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zaheer et~al.(2017)Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov, and Smola]{zaheer2017deep}
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ~R Salakhutdinov, and Alexander~J Smola.
\newblock Deep sets.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Zhang et~al.(2023)Zhang, Feng, Du, He, and Wang]{zhang2023complete}
Bohang Zhang, Guhao Feng, Yiheng Du, Di~He, and Liwei Wang.
\newblock A complete expressiveness hierarchy for subgraph gnns via subgraph weisfeiler-lehman tests.
\newblock \emph{International Conference on Machine Learning}, 2023.

\bibitem[Zhang and Li(2021)]{zhang2021nested}
Muhan Zhang and Pan Li.
\newblock Nested graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~34, 2021.

\bibitem[Zhao et~al.(2022)Zhao, Jin, Akoglu, and Shah]{zhao2022from}
Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah.
\newblock From stars to subgraphs: Uplifting any {GNN} with local structure awareness.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\end{thebibliography}
