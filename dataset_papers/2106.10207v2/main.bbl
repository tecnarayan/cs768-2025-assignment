\begin{thebibliography}{100}

\bibitem{imagenet_cvpr09}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock {ImageNet}: a large-scale hierarchical image database.
\newblock {\em IEEE Conference on Computer Vision and Pattern Recognition},
  pages 248--255, 06 2009.

\bibitem{10.1109/CVPR.2014.81}
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
\newblock Rich feature hierarchies for accurate object detection and semantic
  segmentation.
\newblock In {\em Proceedings of the 2014 IEEE Conference on Computer Vision
  and Pattern Recognition}, pages 580--587, 2014.

\bibitem{7298965}
Jonathan Long, Evan Shelhamer, and Trevor Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In {\em 2015 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 3431--3440, 2015.

\bibitem{pmlr-v32-donahue14}
J.~Donahue, Y.~Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and
  Trevor Darrell.
\newblock {DeCAF}: A deep convolutional activation feature for generic visual
  recognition.
\newblock In {\em Proceedings of the 31st International Conference on
  International Conference on Machine Learning}, pages 647--655, 2014.

\bibitem{johnson2016perceptual}
Justin Johnson, Alexandre Alahi, and Li~Fei-Fei.
\newblock Perceptual losses for real-time style transfer and super-resolution.
\newblock In {\em European Conference on Computer Vision}, volume 9906, pages
  694--711, 10 2016.

\bibitem{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 4171--4186, 06 2019.

\bibitem{albert}
Zhen-Zhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma,
  and Radu Soricut.
\newblock {ALBERT}: A lite {BERT} for self-supervised learning of language
  representations.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock {RoBERTa}: A robustly optimized {BERT} pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{xlmr}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
  Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
  Veselin Stoyanov.
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 8440--8451, 07 2020.

\bibitem{gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 1877--1901, 2020.

\bibitem{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-{LM}: Training multi-billion parameter language models using
  {GPU} model parallelism.
\newblock {\em arXiv preprint arXiv:1909.08053}, 2019.

\bibitem{baevski2020wav2vec}
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech
  representations.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 12449--12460, 2020.

\bibitem{zhu2021transfer}
Zhuangdi Zhu, Kaixiang Lin, and Jiayu Zhou.
\newblock Transfer learning in deep reinforcement learning: A survey.
\newblock {\em arXiv preprint arXiv:2009.07888}, 2020.

\bibitem{Lu2020.09.04.283929}
Amy~X. Lu, Haoran Zhang, Marzyeh Ghassemi, and Alan Moses.
\newblock Self-supervised contrastive learning of protein representations by
  mutual information maximization.
\newblock {\em bioRxiv}, 2020.

\bibitem{honda2019smiles}
Shion Honda, Shoi Shi, and H.~Ueda.
\newblock {SMILES} transformer: Pre-trained molecular fingerprint for low data
  drug discovery.
\newblock {\em arXiv preprint arXiv:1911.04738}, 2019.

\bibitem{megatron2}
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
  Patwary, Vijay~Anand Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,
  Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia.
\newblock Efficient large-scale language model training on {GPU} clusters using
  {Megatron-LM}.
\newblock {\em arXiv preprint arXiv:2104.04473}, 2021.

\bibitem{lin2020multinode}
Jiahuang Lin, Xin Li, and Gennady Pekhimenko.
\newblock Multi-node {BERT}-pretraining: Cost-efficient approach.
\newblock {\em arXiv preprint arXiv:2008.00177}, 2020.

\bibitem{tfhub}
{TensorFlow} {H}ub.
\newblock \url{https://www.tensorflow.org/hub}.
\newblock Accessed: 2021-05-20.

\bibitem{torchhub}
{PyTorch} {H}ub.
\newblock \url{https://pytorch.org/hub/}.
\newblock Accessed: 2021-05-20.

\bibitem{hfhub}
{Hugging Face} {H}ub.
\newblock \url{https://huggingface.co/models}.
\newblock Accessed: 2021-05-20.

\bibitem{dlhub}
Ryan Chard, Zhuozhao Li, Kyle Chard, Logan Ward, Yadu Babuji, Anna Woodard,
  Steven Tuecke, Ben Blaiszik, Michael~J. Franklin, and Ian Foster.
\newblock {DLHub}: Model and data serving for science.
\newblock In {\em 2019 IEEE International Parallel and Distributed Processing
  Symposium (IPDPS)}, pages 283--292, 05 2019.

\bibitem{Joshi2020TheSA}
Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit
  Choudhury.
\newblock The state and fate of linguistic diversity and inclusion in the {NLP}
  world.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 6282--6293, 07 2020.

\bibitem{seti_at_home}
David Anderson, Jeff Cobb, Eric Korpela, Matt Lebofsky, and Dan Werthimer.
\newblock {SETI@home}: An experiment in public-resource computing.
\newblock {\em Commun. ACM}, 45:56--61, 11 2002.

\bibitem{foldingathome}
A.~L. Beberg, D.~Ensign, G.~Jayachandran, S.~Khaliq, and V.~Pande.
\newblock Folding@home: Lessons from eight years of volunteer distributed
  computing.
\newblock {\em 2009 IEEE International Symposium on Parallel \& Distributed
  Processing}, pages 1--8, 2009.

\bibitem{anderson2004boinc}
David~P Anderson.
\newblock {BOINC}: A system for public-resource computing and storage.
\newblock In {\em Fifth IEEE/ACM international workshop on grid computing},
  pages 4--10. IEEE, 2004.

\bibitem{folding_exaflop_2}
Folding@home gets 1.5+ exaflops to fight covid-19.
\newblock
  \url{https://blogs.nvidia.com/blog/2020/04/01/foldingathome-exaflop-coronavirus/}.
\newblock Accessed: 2021-05-20.

\bibitem{Tapparello2016VolunteerCO}
C.~Tapparello, Colin Funai, Shurouq Hijazi, Abner Aquino, Bora Karaoglu, H.~Ba,
  J.~Shi, and W.~Heinzelman.
\newblock Volunteer computing on mobile devices: State of the art and future
  research directions.
\newblock In {\em Enabling Real-Time Mobile Cloud Computing through Emerging
  Technologies}, pages 153--181, 2016.

\bibitem{ringallreduce}
Pitch Patarasuk and Xin Yuan.
\newblock Bandwidth optimal all-reduce algorithms for clusters of workstations.
\newblock {\em Journal of Parallel and Distributed Computing}, 69:117--124, 02
  2009.

\bibitem{wagma}
Shigang Li, Tal Ben-Nun, Giorgi Nadiradze, Salvatore Digirolamo, Nikoli Dryden,
  Dan Alistarh, and Torsten Hoefler.
\newblock Breaking (global) barriers in parallel stochastic optimization with
  wait-avoiding group averaging.
\newblock {\em IEEE Transactions on Parallel and Distributed Systems}, page
  1–1, 2020.

\bibitem{ps}
Mu~Li, D.~Andersen, J.~Park, Alex Smola, Amr Ahmed, V.~Josifovski, J.~Long,
  E.~Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In {\em Proceedings of the 2014 International Conference on Big Data
  Science and Computing}, 2014.

\bibitem{shi2018performance}
Shaohuai Shi, Qiang Wang, and Xiaowen Chu.
\newblock Performance modeling and evaluation of distributed deep learning
  frameworks on {GPUs}.
\newblock In {\em IEEE 16th Intl Conf on Dependable, Autonomic and Secure
  Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl
  Conf on Big Data Intelligence and Computing and Cyber Science and Technology
  Congress}, pages 949--957, 2018.

\bibitem{sergeev2018horovod}
Alexander Sergeev and Mike~Del Balso.
\newblock Horovod: fast and easy distributed deep learning in {TensorFlow}.
\newblock {\em arXiv preprint arXiv:1802.05799}, 2018.

\bibitem{dp_sgd}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? {A}
  case study for decentralized parallel stochastic gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem{byteps}
Yimin Jiang, Yibo Zhu, Chang Lan, Bairen Yi, Yong Cui, and Chuanxiong Guo.
\newblock A unified architecture for accelerating distributed {DNN} training in
  heterogeneous {GPU/CPU} clusters.
\newblock In {\em 14th {USENIX} Symposium on Operating Systems Design and
  Implementation ({OSDI} 20)}, pages 463--479, 11 2020.

\bibitem{huang2019gpipe}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia~Xu Chen, Dehao
  Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc~V. Le, Yonghui Wu, and Zhifeng Chen.
\newblock {GPipe}: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  103--112, 2019.

\bibitem{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock {\em arXiv preprint arXiv:1701.06538}, 2017.

\bibitem{zero}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.
\newblock {ZeRO}: Memory optimization towards training a trillion parameter
  models.
\newblock In {\em SC}, 2020.

\bibitem{switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock {\em arXiv preprint arXiv:2101.03961}, 2021.

\bibitem{deepgradientcompression}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and William~J Dally.
\newblock {Deep Gradient Compression: Reducing the communication bandwidth for
  distributed training}.
\newblock In {\em The International Conference on Learning Representations},
  2018.

\bibitem{localsgd_first}
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex Smola.
\newblock Parallelized stochastic gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~23, pages 2595--2603, 2010.

\bibitem{Stich18local}
Sebastian~Urban Stich.
\newblock Local {SGD} converges fast and communicates little.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{koloskova2020decentralized}
Anastasia Koloskova, Tao Lin, Sebastian~U Stich, and Martin Jaggi.
\newblock Decentralized deep learning with arbitrary communication compression.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{li2020acceleration}
Zhize Li, Dmitry Kovalev, Xun Qian, and Peter Richtarik.
\newblock Acceleration for compressed gradient descent in distributed and
  federated optimization.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, volume 119 of {\em Proceedings of Machine Learning Research},
  pages 5895--5904, 07 2020.

\bibitem{recht2011hogwild}
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In {\em Advances in neural information processing systems}, pages
  693--701, 2011.

\bibitem{projectadam}
Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
\newblock Project adam: Building an efficient and scalable deep learning
  training system.
\newblock In {\em 11th {USENIX} Symposium on Operating Systems Design and
  Implementation ({OSDI} 14)}, pages 571--582, Broomfield, CO, October 2014.
  {USENIX} Association.

\bibitem{sharded_ps_first}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Marc\textquotesingle~aurelio Ranzato, Andrew Senior, Paul Tucker, Ke~Yang,
  Quoc Le, and Andrew Ng.
\newblock Large scale distributed deep networks.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~25, pages 1223--1231, 2012.

\bibitem{goyal2017accurate}
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski,
  Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD}: Training {ImageNet} in 1 hour.
\newblock {\em arXiv preprint arXiv:1706.02677}, 2017.

\bibitem{mikami2019massively}
Hiroaki Mikami, Hisahiro Suganuma, Pongsakorn U-chupala, Yoshiki Tanaka, and
  Yuichi Kageyama.
\newblock Massively distributed {SGD}: {ImageNet/ResNet-50} training in a
  flash.
\newblock {\em arXiv preprint arXiv:1811.05233}, 2019.

\bibitem{lamb}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training {BERT} in 76
  minutes.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{mpich_rabenseifner}
Rajeev Thakur, Rolf Rabenseifner, and William Gropp.
\newblock Optimization of collective communication operations in {MPICH}.
\newblock {\em Int. J. High Perform. Comput. Appl.}, 19(1):49–66, 02 2005.

\bibitem{torus_allreduce}
Paul Sack and William Gropp.
\newblock Collective algorithms for multiported torus networks.
\newblock {\em ACM Trans. Parallel Comput.}, 1(2), February 2015.

\bibitem{pytorch_elastic}
{PyTorch Elastic}.
\newblock \url{https://pytorch.org/elastic}.
\newblock Accessed: 2021-05-20.

\bibitem{elastic_horovod}
{Elastic Horovod}.
\newblock \url{ https://horovod.rtfd.io/en/stable/elastic_include.html}.
\newblock Accessed: 2021-05-20.

\bibitem{sgp}
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Mike Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of {\em Proceedings of Machine Learning Research}, pages
  344--353, 06 2019.

\bibitem{slowmo}
Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat.
\newblock {SlowMo}: Improving communication-efficient distributed {SGD} with
  slow momentum.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{moshpit}
Max Ryabinin, Eduard Gorbunov, Vsevolod Plokhotnyuk, and Gennady Pekhimenko.
\newblock Moshpit {SGD}: Communication-efficient decentralized training on
  heterogeneous unreliable devices.
\newblock {\em arXiv preprint arXiv:2103.03239}, 2021.

\bibitem{FedLearningOriginal}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1273--1282,
  2017.

\bibitem{FedLearningAtScale}
K.~A. Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
  Ingerman, Vladimir Ivanov, Chloé~M Kiddon, Jakub Konečný, Stefano
  Mazzocchi, Brendan McMahan, Timon~Van Overveldt, David Petrou, Daniel Ramage,
  and Jason Roselander.
\newblock Towards federated learning at scale: System design.
\newblock In {\em Proceedings of Machine Learning and Systems (MLSys)}, 2019.

\bibitem{FedLearningDecentralized}
Thorsten Wittkopp and Alexander Acker.
\newblock Decentralized federated learning preserves model and data privacy.
\newblock In {\em International Conference on Service-Oriented Computing},
  pages 176--187. Springer, 2020.

\bibitem{larson_crowd}
Stefan~M. Larson, Christopher~D. Snow, Michael Shirts, and Vijay~S. Pande.
\newblock {Folding}@home and {Genome}@home: Using distributed computing to
  tackle previously intractable problems in computational biology.
\newblock {\em arXiv preprint arXiv:0901.0866}, 2009.

\bibitem{folding_covid}
Folding@home update on {SARS-CoV-2} (10 mar 2020).
\newblock \url{foldingathome.org/2020/03/10/covid19-update}.
\newblock Accessed: 2021-05-20.

\bibitem{lhc_at_home}
Javier Barranco, Yunhi Cai, David Cameron, Matthew Crouch, Riccardo De~Maria,
  Laurence Field, M.~Giovannozzi, Pascal Hermes, Nils Høimyr, Dobrin Kaltchev,
  Nikos Karastathis, Cinzia Luzzi, Ewen Maclean, Eric Mcintosh, Alessio
  Mereghetti, James Molson, Yuri Nosochkov, Tatiana Pieloni, Ivan Reid, and
  Igor Zacharov.
\newblock {LHC@Home: a BOINC-based volunteer computing infrastructure for
  physics studies at CERN}.
\newblock {\em Open Engineering}, 7, 12 2017.

\bibitem{qmc_at_home}
Jeongnim Kim, Andrew~D Baczewski, Todd~D Beaudet, Anouar Benali, M~Chandler
  Bennett, Mark~A Berrill, Nick~S Blunt, Edgar Josu{\'{e}}~Landinez Borda,
  Michele Casula, David~M Ceperley, Simone Chiesa, Bryan~K Clark, Raymond~C
  Clay, Kris~T Delaney, Mark Dewing, Kenneth~P Esler, Hongxia Hao, Olle
  Heinonen, Paul R~C Kent, Jaron~T Krogel, Ilkka Kylänpää, Ying~Wai Li,
  M~Graham Lopez, Ye~Luo, Fionn~D Malone, Richard~M Martin, Amrita Mathuriya,
  Jeremy McMinis, Cody~A Melton, Lubos Mitas, Miguel~A Morales, Eric
  Neuscamman, William~D Parker, Sergio D~Pineda Flores, Nichols~A Romero,
  Brenda~M Rubenstein, Jacqueline A~R Shea, Hyeondeok Shin, Luke Shulenburger,
  Andreas~F Tillack, Joshua~P Townsend, Norm~M Tubman, Brett Van~Der Goetz,
  Jordan~E Vincent, D~ChangMo Yang, Yubo Yang, Shuai Zhang, and Luning Zhao.
\newblock {QMCPACK}: an open sourceab initioquantum monte carlo package for the
  electronic structure of atoms, molecules and solids.
\newblock {\em Journal of Physics: Condensed Matter}, 30(19):195901, 04 2018.

\bibitem{folding_timeline}
Folding@home project timeline.
\newblock \url{https://foldingathome.org/project-timeline}.
\newblock Accessed: 2021-05-20.

\bibitem{einstein_at_home}
B.~Steltner, M.~A. Papa, H.~B. Eggenstein, B.~Allen, V.~Dergachev, R.~Prix,
  B.~Machenschalk, S.~Walsh, S.~J. Zhu, and S.~Kwang.
\newblock {Einstein@Home} all-sky search for continuous gravitational waves in
  {LIGO O2} public data.
\newblock {\em The Astrophysical Journal}, 909(1):79, 03 2021.

\bibitem{folding_petaflop}
Michael Gross.
\newblock Folding research recruits unconventional help.
\newblock {\em Current biology : CB}, 22:R35--8, 01 2012.

\bibitem{folding_ps3}
Tetsu Narumi, Shun Kameoka, Makoto Taiji, and Kenji Yasuoka.
\newblock Accelerating molecular dynamics simulations on playstation 3 platform
  using virtual-grape programming model.
\newblock {\em SIAM J. Scientific Computing}, 30:3108--3125, 01 2008.

\bibitem{clemens2021mlds}
John Clemens.
\newblock {MLDS}: A dataset for weight-space analysis of neural networks.
\newblock {\em arXiv preprint arXiv:2104.10555}, 2021.

\bibitem{lc0}
Gian-Carlo Pascutto and Gary Linscott.
\newblock Leela chess zero.
\newblock \url{lczero.org}, 2019.
\newblock Accessed: 2021-05-20.

\bibitem{volunteer_dl_async}
Ekasit Kijsipongse, Apivadee Piyatumrong, and Suriya U-ruekolan.
\newblock A hybrid gpu cluster and volunteer computing platform for scalable
  deep learning.
\newblock {\em The Journal of Supercomputing}, 04 2018.

\bibitem{atre2021distributed}
Medha Atre, Birendra Jha, and Ashwini Rao.
\newblock Distributed deep learning using volunteer computing-like paradigm.
\newblock {\em arXiv preprint arXiv:2103.08894}, 2021.

\bibitem{hivemind_dmoe}
Max Ryabinin and Anton Gusev.
\newblock Towards crowdsourced training of large neural networks using
  decentralized mixture-of-experts.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 3659--3672, 2020.

\bibitem{GAN}
Ian~J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
  Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Proceedings of the 27th International Conference on Neural
  Information Processing Systems - Volume 2}, page 2672–2680, 2014.

\bibitem{transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in Neural Information Processing Systems 30}, pages
  5998--6008, 2017.

\bibitem{trainingtips}
Martin Popel and Ondřej Bojar.
\newblock Training tips for the transformer model.
\newblock {\em The Prague Bulletin of Mathematical Linguistics}, 110, 03 2018.

\bibitem{liu-etal-2020-understanding}
Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han.
\newblock Understanding the difficulty of training transformers.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 5747--5763, 11 2020.

\bibitem{pipedream}
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil~R.
  Devanur, Gregory~R. Ganger, Phillip~B. Gibbons, and Matei Zaharia.
\newblock {PipeDream}: Generalized pipeline parallelism for {DNN} training.
\newblock In {\em Proceedings of the 27th ACM Symposium on Operating Systems
  Principles}, page 1–15, 2019.

\bibitem{lars}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock {\em arXiv preprint arXiv:1708.03888}, 2017.

\bibitem{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock {\em arXiv preprint arXiv:2001.08361}, 2020.

\bibitem{ott2018scaling}
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli.
\newblock Scaling neural machine translation.
\newblock In {\em Proceedings of the Third Conference on Machine Translation:
  Research Papers}, pages 1--9, 10 2018.

\bibitem{zerooffload}
Jie Ren, Samyam Rajbhandari, Reza~Yazdani Aminabadi, Olatunji Ruwase, Shuangyan
  Yang, Minjia Zhang, Dong Li, and Yuxiong He.
\newblock {ZeRO}-offload: Democratizing billion-scale model training.
\newblock {\em arXiv preprint arXiv:2101.06840}, 2021.

\bibitem{aji2019making}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Making asynchronous stochastic gradient descent work for
  transformers.
\newblock {\em Proceedings of the 3rd Workshop on Neural Generation and
  Translation}, 2019.

\bibitem{pytorch_distributed}
Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li,
  Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, and Soumith Chintala.
\newblock Pytorch distributed: Experiences on accelerating data parallel
  training.
\newblock {\em Proc. VLDB Endow.}, 13(12):3005–3018, August 2020.

\bibitem{butterfly_arsgd}
Zhenyu Li, James Davis, and Stephen Jarvis.
\newblock An efficient task-based all-reduce for machine learning applications.
\newblock In {\em Proceedings of the Machine Learning on HPC Environments},
  MLHPC'17, New York, NY, USA, 2017. Association for Computing Machinery.

\bibitem{torrent}
Bram Cohen.
\newblock {The BitTorrent Protocol Specification}.
\newblock \url{http://www.bittorrent.org/beps/bep_0003.html}, 2008.

\bibitem{i2p}
jrandom (Pseudonym).
\newblock Invisible internet project (i2p) project overview.
\newblock \url{geti2p.net/_static/pdf/i2p_philosophy.pdf}, August 2003.
\newblock Accessed: 2021-05-20.

\bibitem{kademlia}
Petar Maymounkov and David Mazieres.
\newblock Kademlia: A peer-to-peer information system based on the {XOR}
  metric.
\newblock In {\em International Workshop on Peer-to-Peer Systems}, pages
  53--65. Springer, 2002.

\bibitem{kaashoek2003koorde}
M~Frans Kaashoek and David~R Karger.
\newblock Koorde: A simple degree-optimal distributed hash table.
\newblock In {\em International Workshop on Peer-to-Peer Systems}, pages
  98--107. Springer, 2003.

\bibitem{Biggadike05natblaster:establishing}
Andrew Biggadike, Daniel Ferullo, Geoffrey Wilson, and Adrian Perrig.
\newblock {NATBLASTER}: Establishing {TCP} connections between hosts behind
  {NATs}.
\newblock In {\em Proceedings of ACM SIGCOMM ASIA Workshop}, 2005.

\bibitem{hole_punching}
Bryan Ford, Pyda Srisuresh, and Dan Kegel.
\newblock Peer-to-peer communication across network address translators.
\newblock In {\em Proceedings of the Annual Conference on USENIX Annual
  Technical Conference}, ATEC '05, page~13, USA, 2005. USENIX Association.

\bibitem{TURN}
T.~Reddy, A.~Johnston, P.~Matthews, and J.~Rosenberg.
\newblock Traversal using relays around {NAT} ({TURN}): Relay extensions to
  session traversal utilities for {NAT} ({STUN}).
\newblock RFC 8656, 02 2020.

\bibitem{swav}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 9912--9924, 2020.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock {\em 2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 770--778, 2015.

\bibitem{vissl}
Priya Goyal, Quentin Duval, Jeremy Reizenstein, Matthew Leavitt, Min Xu,
  Benjamin Lefaudeux, Mannat Singh, Vinicius Reis, Mathilde Caron, Piotr
  Bojanowski, Armand Joulin, and Ishan Misra.
\newblock Vissl.
\newblock \url{https://github.com/facebookresearch/vissl}, 2021.

\bibitem{hivemind}
Learning@home team.
\newblock {H}ivemind: a {L}ibrary for {D}ecentralized {D}eep {L}earning.
\newblock \url{https://github.com/learning-at-home/hivemind}, 2020.

\bibitem{mixed_precision}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,
  David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
  Venkatesh, and Hao Wu.
\newblock Mixed precision training.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{wikitext103}
Stephen Merity, Caiming Xiong, James Bradbury, and R.~Socher.
\newblock Pointer sentinel mixture models.
\newblock {\em arXiv preprint arXiv:1609.07843}, 2017.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock {PyTorch}: An imperative style, high-performance deep learning
  library.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8024--8035, 2019.

\bibitem{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, 10 2020.

\bibitem{Oscar}
Pedro~Javier Ortiz~Su{\'a}rez, Laurent Romary, and Beno{\^\i}t Sagot.
\newblock A monolingual approach to contextualized word embeddings for
  mid-resource languages.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 1703--1714, 07 2020.

\bibitem{wandb}
Lukas Biewald.
\newblock Experiment tracking with {Weights and Biases}, 2020.
\newblock Software available from wandb.com.

\bibitem{google_stadia}
{Google Stadia} data usage.
\newblock \url{https://support.google.com/stadia/answer/9607891}.
\newblock Accessed: 2021-05-20.

\bibitem{netflix}
{N}etflix data usage.
\newblock \url{https://help.netflix.com/en/node/87}.
\newblock Accessed: 2021-05-20.

\bibitem{kakwani-etal-2020-indicnlpsuite}
Divyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul N.C., Avik
  Bhattacharyya, Mitesh~M. Khapra, and Pratyush Kumar.
\newblock {I}ndic{NLPS}uite: Monolingual corpora, evaluation benchmarks and
  pre-trained multilingual language models for {I}ndian languages.
\newblock In {\em Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 4948--4961, 11 2020.

\bibitem{jain2020indictransformers}
Kushal Jain, Adwait Deshpande, Kumar Shridhar, Felix Laumann, and Ayushman
  Dash.
\newblock Indic-transformers: An analysis of transformer language models for
  {Indian} languages.
\newblock {\em arXiv preprint arXiv:2011.02323}, 2020.

\bibitem{pan-etal-2017-cross}
Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng
  Ji.
\newblock Cross-lingual name tagging and linking for 282 languages.
\newblock In {\em Proceedings of the 55th Annual Meeting of the Association for
  Computational Linguistics}, pages 1946--1958, 07 2017.

\bibitem{Anthony2020CarbontrackerTA}
Lasse F.~Wolff Anthony, Benjamin Kanding, and Raghavendra Selvan.
\newblock Carbontracker: Tracking and predicting the carbon footprint of
  training deep learning models.
\newblock ICML Workshop on Challenges in Deploying and monitoring Machine
  Learning Systems, 07 2020.

\bibitem{PracticalSecureAggregation}
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H~Brendan
  McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.
\newblock Practical secure aggregation for privacy-preserving machine learning.
\newblock In {\em Proceedings of the 2017 ACM SIGSAC Conference on Computer and
  Communications Security}, pages 1175--1191, 2017.

\bibitem{FedLearningDiffPrivacy}
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard~H. Yang, Farhad Farokhi, Shi Jin,
  Tony Q.~S. Quek, and H.~Vincent Poor.
\newblock Federated learning with differential privacy: Algorithms and
  performance analysis.
\newblock {\em IEEE Transactions on Information Forensics and Security},
  15:3454--3469, 2020.

\bibitem{kaplan1974application}
Seymour Kaplan.
\newblock Application of programs with maximin objective functions to problems
  of optimal resource allocation.
\newblock {\em Operations Research}, 22(4):802--807, 1974.

\bibitem{andersen}
Erling~D. Andersen and Knud~D. Andersen.
\newblock The {MOSEK} interior point optimizer for linear programming: An
  implementation of the homogeneous algorithm.
\newblock In {\em Applied Optimization}, pages 197--232. Springer {US}, 2000.

\bibitem{scipy}
Pauli Virtanen, Ralf Gommers, Travis~E. Oliphant, Matt Haberland, Tyler Reddy,
  David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan
  Bright, St{\'e}fan~J. van~der Walt, Matthew Brett, Joshua Wilson, K.~Jarrod
  Millman, Nikolay Mayorov, Andrew R.~J. Nelson, Eric Jones, Robert Kern, Eric
  Larson, C~J Carey, Ilhan Polat, Yu~Feng, Eric~W. Moore, J.~Vanderplas, Denis
  Laxalde, Josef Perktold, Robert Cimrman, Ian~Daniel Henriksen, E.~A.
  Quintero, Charles~R. Harris, A.~M. Archibald, Ant{\^o}nio~H. Ribeiro, Fabian
  Pedregosa, Paul van Mulbregt, Aditya Alessandro Pietro Alex Andreas Andreas
  Anthony Ant Vijaykumar Bardelli Rothberg Hilboll~Kloeckner Sco, Aditya
  Vijaykumar, Alessandro~Pietro Bardelli, Alex Rothberg, Andreas Hilboll, Andre
  Kloeckner, Anthony~M. Scopatz, Antony Lee, Ariel~S. Rokem, C.~Nathan Woods,
  Chad Fulton, Charles Masson, Christian H{\"a}ggstr{\"o}m, Clark Fitzgerald,
  David~A. Nicholson, David~R. Hagen, Dmitrii~V. Pasechnik, Emanuele Olivetti,
  Eric~A. Martin, E.~Wieser, Fabrice Silva, Felix Lenders, Florian Wilhelm,
  Gert Young, Gavin~A. Price, Gert-Ludwig Ingold, Gregory~E. Allen, Gregory~R.
  Lee, Herv{\'e} Audren, Irvin Probst, Jorg~P. Dietrich, Jacob Silterra,
  James~T. Webber, Janko Slavic, Joel Nothman, Johannes Buchner, Johannes
  Kulick, Johannes~L. Sch{\"o}nberger, Jos{\'e}~Vin{\'i}cius
  de~Miranda~Cardoso, Joscha Reimer, Joseph~E. Harrington, Juan Luis~Cano
  Rodr{\'i}guez, Juan Nunez-Iglesias, Justin Kuczynski, Kevin~Lee Tritz,
  Martin~Dr Thoma, Matt Newville, Matthias K{\"u}mmerer, Maximilian
  Bolingbroke, Michael Tartre, Mikhail Pak, Nathaniel~J. Smith, Nikolai
  Nowaczyk, Nikolay Shebanov, Oleksandr Pavlyk, Per~Andreas Brodtkorb, Perry
  Lee, Robert~T. McGibbon, Roman Feldbauer, Sam Lewis, Sam Tygier, Scott
  Sievert, Sebastiano Vigna, Stefan Peterson, Surhud More, Tadeusz Pudlik, Taku
  Oshima, Thomas~J. Pingel, Thomas~P. Robitaille, Thomas Spura, Thouis~Raymond
  Jones, Tim Cera, Tim Leslie, Tiziano Zito, Tom Krauss, U.~Upadhyay,
  Yaroslav~O. Halchenko, and Y.~V{\'a}zquez-Baeza.
\newblock Scipy 1.0: fundamental algorithms for scientific computing in python.
\newblock {\em Nature Methods}, 17:261 -- 272, 2020.

\bibitem{STUN}
J.~Rosenberg, J.~Weinberger, C.~Huitema, and R.~Mahy.
\newblock {STUN} - simple traversal of user datagram protocol ({UDP}) through
  network address translators ({NATs}).
\newblock RFC 3489, 03 2003.

\bibitem{libp2p}
libp2p.
\newblock \url{https://libp2p.io/}.
\newblock Accessed: 2021-05-20.

\bibitem{tensorflow2015-whitepaper}
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig
  Citro, Greg~S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
  Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
  Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
  Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,
  Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,
  Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol
  Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang
  Zheng.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous
  distributed systems.
\newblock {\em arXiv preprint arXiv:1603.04467}, 2016.

\bibitem{stich2019unified}
Sebastian~U. Stich.
\newblock Unified optimal analysis of the (stochastic) gradient method, 2019.

\bibitem{khaled2020unified}
Ahmed Khaled, Othmane Sebbouh, Nicolas Loizou, Robert~M. Gower, and Peter
  Richtárik.
\newblock Unified analysis of stochastic gradient methods for composite convex
  and smooth optimization, 2020.

\bibitem{kudo2018subword}
Taku Kudo.
\newblock Subword regularization: Improving neural network translation models
  with multiple subword candidates.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for
  Computational Linguistics}, pages 66--75, 2018.

\bibitem{hftokenizers2019}
Anthony MOI, Pierric Cistac, Nicolas Patry, Evan~P. Walsh, Funtowicz Morgan,
  Sebastian Pütz, Thomas Wolf, Sylvain Gugger, Clément Delangue, Julien
  Chaumond, Lysandre Debut, and Patrick von Platen.
\newblock Hugging face tokenizers library.
\newblock \url{https://github.com/huggingface/tokenizers}, 2019.

\bibitem{datasets}
Quentin Lhoest, Albert~Villanova del Moral, Yacine Jernite, Abhishek Thakur,
  Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu,
  Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya
  Malik, Simon Brandeis, Teven~Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry,
  Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue,
  Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault
  Goehringer, Victor Mustar, François Lagunas, Alexander~M. Rush, and Thomas
  Wolf.
\newblock Datasets: A community library for natural language processing.
\newblock {\em arXiv preprint arxiv:2109.02846}, 2021.

\bibitem{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E. Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In {\em European conference on computer vision}, pages 630--645.
  Springer, 2016.

\bibitem{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem{2020t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em Journal of Machine Learning Research}, 21(140):1--67, 2020.

\bibitem{shazeer2020glu}
Noam Shazeer.
\newblock {GLU} variants improve transformer.
\newblock {\em arXiv preprint arXiv:2002.05202}, 2020.

\bibitem{Narang2021DoTM}
Sharan Narang, Hyung~Won Chung, Yi~Tay, William Fedus, Thibault F{\'e}vry,
  Michael Matena, Karishma Malkan, Noah Fiedel, Noam~M. Shazeer, Zhenzhong Lan,
  Yanqi Zhou, Wen hong Li, Nan Ding, Jake Marcus, Adam Roberts, and Colin
  Raffel.
\newblock Do transformer modifications transfer across implementations and
  applications?
\newblock {\em arXiv preprint arXiv:2102.11972}, 2021.

\bibitem{su2021roformer}
Jianlin Su, Yu~Lu, Shengfeng Pan, Bo~Wen, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock {\em arXiv preprint arXiv:2104.09864}, 2021.

\bibitem{rope-eleutherai}
Stella Biderman, Sid Black, Charles Foster, Leo Gao, Eric Hallahan, Horace He,
  Ben Wang, and Phil Wang.
\newblock Rotary embeddings: A relative revolution.
\newblock \url{blog.eleuther.ai/rotary-embeddings}, 2021.
\newblock {Accessed: 2021-05-20}.

\bibitem{net2net}
Tianqi Chen, Ian Goodfellow, and Jonathon Shlens.
\newblock {Net2Net}: Accelerating learning via knowledge transfer.
\newblock {\em arXiv preprint arXiv:1511.05641}, 11 2015.

\bibitem{rahimi-etal-2019-massively}
Afshin Rahimi, Yuan Li, and Trevor Cohn.
\newblock Massively multilingual transfer for {NER}.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 151--164, 07 2019.

\bibitem{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{Strubell2019EnergyAP}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in {NLP}.
\newblock In {\em Proceedings of the 57th Conference of the Association for
  Computational Linguistics}, pages 3645--3650, 2019.

\bibitem{Schwartz2020GreenA}
Roy Schwartz, Jesse Dodge, Noah Smith, and Oren Etzioni.
\newblock Green {AI}.
\newblock {\em Communications of the ACM}, 63:54--63, 2020.

\bibitem{Henderson2020TowardsTS}
Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and
  Joelle Pineau.
\newblock Towards the systematic reporting of the energy and carbon footprints
  of machine learning.
\newblock {\em Journal of Machine Learning Research}, 21(248):1--43, 2020.

\bibitem{Kline2016HolisticallyET}
Donald Kline, Nikolas Parshook, Xiaoyu Ge, E.~Brunvand, R.~Melhem, Panos~K.
  Chrysanthis, and A.~Jones.
\newblock Holistically evaluating the environmental impacts in modern computing
  systems.
\newblock {\em 2016 Seventh International Green and Sustainable Computing
  Conference (IGSC)}, pages 1--8, 2016.

\bibitem{Bashroush2018ACR}
Rabih Bashroush.
\newblock A comprehensive reasoning framework for hardware refresh in data
  centers.
\newblock {\em IEEE Transactions on Sustainable Computing}, 3:209--220, 2018.

\bibitem{Qiu2020CanFL}
Xinchi Qiu, Titouan Parcollet, Daniel~J. Beutel, Taner Topal, Akhil Mathur, and
  Nicholas~D. Lane.
\newblock Can federated learning save the planet?
\newblock {\em arXiv preprint arXiv:2010.06537}, 2020.

\end{thebibliography}
