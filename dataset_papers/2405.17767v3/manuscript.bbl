\begin{thebibliography}{122}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan2020prevalence}
Vardan Papyan, X.~Y. Han, and David~L. Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, sep 2020.
\newblock \doi{10.1073/pnas.2015509117}.
\newblock URL \url{https://www.pnas.org/doi/full/10.1073/pnas.2015509117}.

\bibitem[Eldan and Li(2023)]{eldan2023tinystories}
Ronen Eldan and Yuanzhi Li.
\newblock Tinystories: How small can language models be and still speak
  coherent english?, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.07759}.

\bibitem[Galanti et~al.(2022{\natexlab{a}})Galanti, György, and
  Hutter]{galanti2022role}
Tomer Galanti, András György, and Marcus Hutter.
\newblock On the role of neural collapse in transfer learning,
  2022{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2112.15121v2}.

\bibitem[Galanti et~al.(2022{\natexlab{b}})Galanti, Galanti, and
  Ben-Shaul]{galanti2022implicit}
Tomer Galanti, Liane Galanti, and Ido Ben-Shaul.
\newblock On the implicit bias towards minimal depth of deep neural networks,
  2022{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2202.09028v9}.

\bibitem[Kothapalli(2023)]{kothapalli2023neural}
Vignesh Kothapalli.
\newblock Neural collapse: A review on modelling principles and generalization.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=QTXocpAP9p}.

\bibitem[Bender et~al.(2021)Bender, Gebru, McMillan-Major, and
  Shmitchell]{10.1145/3442188.3445922}
Emily~M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
  Shmitchell.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In \emph{Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency}, FAccT '21, page 610–623, New York, NY,
  USA, 2021. Association for Computing Machinery.
\newblock ISBN 9781450383097.
\newblock \doi{10.1145/3442188.3445922}.
\newblock URL \url{https://doi.org/10.1145/3442188.3445922}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding, 2019.
\newblock URL \url{https://arxiv.org/abs/1810.04805}.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever,
  et~al.]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et~al.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Jiang et~al.(2023{\natexlab{a}})Jiang, Zhou, Wang, Qu, Mixon, You, and
  Zhu]{jiang2023generalized}
Jiachen Jiang, Jinxin Zhou, Peng Wang, Qing Qu, Dustin Mixon, Chong You, and
  Zhihui Zhu.
\newblock Generalized neural collapse for a large number of classes,
  2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2310.05351}.

\bibitem[Yang et~al.(2018)Yang, Dai, Salakhutdinov, and
  Cohen]{yang2018breaking}
Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William~W. Cohen.
\newblock Breaking the softmax bottleneck: A high-rank rnn language model,
  2018.
\newblock URL \url{https://arxiv.org/abs/1711.03953}.

\bibitem[Shannon(1948)]{shannon1948mathematical}
C.~E. Shannon.
\newblock A mathematical theory of communication.
\newblock \emph{The Bell System Technical Journal}, 27\penalty0 (3):\penalty0
  379--423, 1948.
\newblock \doi{10.1002/j.1538-7305.1948.tb01338.x}.

\bibitem[Florence(1950)]{florence1950human}
P~Sargant Florence.
\newblock Human behaviour and the principle of least effort.
\newblock \emph{The Economic Journal}, 60\penalty0 (240):\penalty0 808--810,
  1950.

\bibitem[Jurafsky and Martin(2009)]{jurafskyspeech}
Daniel Jurafsky and James~H Martin.
\newblock Speech and language processing: An introduction to natural language
  processing, computational linguistics, and speech recognition, 2009.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models, 2020.
\newblock URL \url{https://arxiv.org/abs/2001.08361v1}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, de~Las~Casas, Hendricks, Welbl, Clark, Hennigan, Noland,
  Millican, van~den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae,
  Vinyals, and Sifre]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las~Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van~den
  Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich
  Elsen, Jack~W. Rae, Oriol Vinyals, and Laurent Sifre.
\newblock Training compute-optimal large language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.15556v1}.

\bibitem[Muennighoff et~al.(2023)Muennighoff, Rush, Barak, Scao, Piktus, Tazi,
  Pyysalo, Wolf, and Raffel]{muennighoff2023scaling}
Niklas Muennighoff, Alexander~M. Rush, Boaz Barak, Teven~Le Scao, Aleksandra
  Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel.
\newblock Scaling data-constrained language models, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.16264v4}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Lu and Steinerberger(2021)]{lu2021neural}
Jianfeng Lu and Stefan Steinerberger.
\newblock Neural collapse with cross-entropy loss, 2021.
\newblock URL \url{https://arxiv.org/abs/2012.08465v2}.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Yu, Weller, and
  Schölkopf]{liu2023generalizing}
Weiyang Liu, Longhui Yu, Adrian Weller, and Bernhard Schölkopf.
\newblock Generalizing and decoupling neural collapse via hyperspherical
  uniformity gap, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2303.06484v2}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Shang, He, Lin, and Wu]{li2023no}
Zexi Li, Xinyi Shang, Rui He, Tao Lin, and Chao Wu.
\newblock No fear of classifier biases: Neural collapse inspired federated
  learning with synthetic and fixed classifier.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 5319--5329, 2023{\natexlab{a}}.

\bibitem[Kothapalli et~al.(2024)Kothapalli, Tirer, and
  Bruna]{kothapalli2024neural}
Vignesh Kothapalli, Tom Tirer, and Joan Bruna.
\newblock A neural collapse perspective on feature evolution in graph neural
  networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Yang et~al.(2023)Yang, Yuan, Li, Wu, Zhang, Lin, Torr, Tao, and
  Ghanem]{yang2023neural}
Yibo Yang, Haobo Yuan, Xiangtai Li, Jianlong Wu, Lefei Zhang, Zhouchen Lin,
  Philip Torr, Dacheng Tao, and Bernard Ghanem.
\newblock Neural collapse terminus: A unified solution for class incremental
  learning and its variants, 2023.
\newblock URL \url{https://arxiv.org/abs/2308.01746v1}.

\bibitem[Zhou et~al.(2023)Zhou, Xiang, and Ma]{zhou2023hierarchical}
Qinhao Zhou, Xiang Xiang, and Jing Ma.
\newblock Hierarchical task-incremental learning with feature-space
  initialization inspired by neural collapse.
\newblock \emph{Neural Processing Letters}, 55\penalty0 (8):\penalty0
  10811--10827, 2023.

\bibitem[Ran et~al.(2024)Ran, Li, Li, Tian, Ning, and Tiwari]{RAN2024103664}
Hang Ran, Weijun Li, Lusi Li, Songsong Tian, Xin Ning, and Prayag Tiwari.
\newblock Learning optimal inter-class margin adaptively for few-shot
  class-incremental learning via neural collapse-based meta-learning.
\newblock \emph{Information Processing \& Management}, 61\penalty0
  (3):\penalty0 103664, 2024.
\newblock ISSN 0306-4573.
\newblock \doi{https://doi.org/10.1016/j.ipm.2024.103664}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0306457324000244}.

\bibitem[Medepalli and Doraiswamy(2023)]{medepalli2023role}
Saaketh Medepalli and Naren Doraiswamy.
\newblock On the role of neural collapse in meta learning models for few-shot
  learning, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.00451v2}.

\bibitem[Haas et~al.(2023)Haas, Yolland, and Rabus]{haas2023linking}
Jarrod Haas, William Yolland, and Bernhard Rabus.
\newblock Linking neural collapse and l2 normalization with improved
  out-of-distribution detection in deep neural networks, 2023.
\newblock URL \url{https://arxiv.org/abs/2209.08378v3}.

\bibitem[Ammar et~al.(2024)Ammar, Belkhir, Popescu, Manzanera, and
  Franchi]{ammar2024neco}
Mouïn~Ben Ammar, Nacim Belkhir, Sebastian Popescu, Antoine Manzanera, and
  Gianni Franchi.
\newblock Neco: Neural collapse based out-of-distribution detection, 2024.
\newblock URL \url{https://arxiv.org/abs/2310.06823}.

\bibitem[Zhang et~al.(2024)Zhang, Chen, Jin, Zhu, and Gu]{zhang2024epa}
Jiawei Zhang, Yufan Chen, Cheng Jin, Lei Zhu, and Yuantao Gu.
\newblock Epa: Neural collapse inspired robust out-of-distribution detector,
  2024.
\newblock URL \url{https://arxiv.org/abs/2401.01710v1}.

\bibitem[Li et~al.(2024{\natexlab{a}})Li, Cao, and Yao]{li2024neuromixgdp}
Donghao Li, Yang Cao, and Yuan Yao.
\newblock Neuromixgdp: A neural collapse-inspired random mixup for private data
  release.
\newblock In \emph{Conference on Parsimony and Learning}, pages 480--514. PMLR,
  2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Zhu, Su, and
  Wang]{wang2024neural}
Chendi Wang, Yuqing Zhu, Weijie~J. Su, and Yu-Xiang Wang.
\newblock Neural collapse meets differential privacy: Curious behaviors of
  noisygd with near-perfect representation learning, 2024{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2405.08920v2}.

\bibitem[Ergen et~al.(2022)Ergen, Sahiner, Ozturkler, Pauly, Mardani, and
  Pilanci]{ergen2022demystifying}
Tolga Ergen, Arda Sahiner, Batu Ozturkler, John Pauly, Morteza Mardani, and
  Mert Pilanci.
\newblock Demystifying batch normalization in relu networks: Equivalent convex
  optimization models and implicit regularization, 2022.
\newblock URL \url{https://arxiv.org/abs/2103.01499v3}.

\bibitem[Ergen and Pilanci(2021)]{pmlr-v139-ergen21b}
Tolga Ergen and Mert Pilanci.
\newblock Revealing the structure of deep neural networks via convex duality.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 3004--3014. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/ergen21b.html}.

\bibitem[Seleznova et~al.(2023)Seleznova, Weitzner, Giryes, Kutyniok, and
  Chou]{NEURIPS2023_3477ca0c}
Mariia Seleznova, Dana Weitzner, Raja Giryes, Gitta Kutyniok, and Hung-Hsu
  Chou.
\newblock Neural (tangent kernel) collapse.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and
  S.~Levine, editors, \emph{Advances in Neural Information Processing Systems},
  volume~36, pages 16240--16270. Curran Associates, Inc., 2023.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/3477ca0ce484aa2fa42c1361ab601c25-Paper-Conference.pdf}.

\bibitem[Telgarsky(2022)]{telgarsky2022feature}
Matus Telgarsky.
\newblock Feature selection with gradient descent on two-layer networks in
  low-rotation regimes, 2022.
\newblock URL \url{https://arxiv.org/abs/2208.02789v1}.

\bibitem[Hong and Ling(2023)]{hong2023neural}
Wanli Hong and Shuyang Ling.
\newblock Neural collapse for unconstrained feature model under cross-entropy
  loss with imbalanced data, 2023.
\newblock URL \url{https://arxiv.org/abs/2309.09725v2}.

\bibitem[Arous et~al.(2023)Arous, Gheissari, Huang, and
  Jagannath]{arous2023highdimensional}
Gerard~Ben Arous, Reza Gheissari, Jiaoyang Huang, and Aukosh Jagannath.
\newblock High-dimensional sgd aligns with emerging outlier eigenspaces, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.03010v1}.

\bibitem[Hui et~al.(2022)Hui, Belkin, and Nakkiran]{hui2022limitations}
Like Hui, Mikhail Belkin, and Preetum Nakkiran.
\newblock Limitations of neural collapse for understanding generalization in
  deep learning, 2022.
\newblock URL \url{https://arxiv.org/abs/2202.08384v1}.

\bibitem[Galanti et~al.(2022{\natexlab{c}})Galanti, Gy{\"o}rgy, and
  Hutter]{galanti2022improved}
Tomer Galanti, Andr{\'a}s Gy{\"o}rgy, and Marcus Hutter.
\newblock Improved generalization bounds for transfer learning via neural
  collapse.
\newblock In \emph{First Workshop on Pre-training: Perspectives, Pitfalls, and
  Paths Forward at ICML 2022}, 2022{\natexlab{c}}.
\newblock URL \url{https://openreview.net/forum?id=VrK7pKwOhT_}.

\bibitem[Wang et~al.(2023)Wang, Luo, Zheng, Huang, and
  Baktashmotlagh]{wang2023far}
Zijian Wang, Yadan Luo, Liang Zheng, Zi~Huang, and Mahsa Baktashmotlagh.
\newblock How far pre-trained models are from neural collapse on the target
  dataset informs their transferability.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 5549--5558, 2023.

\bibitem[Li et~al.(2024{\natexlab{b}})Li, Liu, Zhou, Lu, Fernandez-Granda, Zhu,
  and Qu]{li2024understanding}
Xiao Li, Sheng Liu, Jinxin Zhou, Xinyu Lu, Carlos Fernandez-Granda, Zhihui Zhu,
  and Qing Qu.
\newblock Understanding and improving transfer learning of deep models via
  neural collapse.
\newblock \emph{Transactions on Machine Learning Research}, 2024{\natexlab{b}}.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=o8r84MzTQB}.

\bibitem[Zhu et~al.(2023)Zhu, Li, Zhang, Yuan, Shao, Liu, Kuang, Li, and
  Wu]{zhu2023understanding}
Didi Zhu, Zexi Li, Min Zhang, Junkun Yuan, Yunfeng Shao, Jiashuo Liu, Kun
  Kuang, Yinchuan Li, and Chao Wu.
\newblock Understanding prompt tuning for v-l models through the lens of neural
  collapse, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.15955v3}.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical Report~0, University of Toronto, Toronto, Ontario, 2009.
\newblock URL
  \url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[Mixon et~al.(2020)Mixon, Parshall, and Pi]{mixon2020neural}
Dustin~G. Mixon, Hans Parshall, and Jianzong Pi.
\newblock Neural collapse with unconstrained features, 2020.
\newblock URL \url{https://arxiv.org/abs/2011.11619v1}.

\bibitem[Poggio and Liao(2020)]{poggio2020explicit}
Tomaso Poggio and Qianli Liao.
\newblock Explicit regularization and implicit bias in deep network classifiers
  trained with the square loss, 2020.
\newblock URL \url{https://arxiv.org/abs/2101.00072v1}.

\bibitem[E and Wojtowytsch(2021)]{e2021emergence}
Weinan E and Stephan Wojtowytsch.
\newblock On the emergence of simplex symmetry in the final and penultimate
  layers of neural network classifiers, 2021.
\newblock URL \url{https://arxiv.org/abs/2012.05420v3}.

\bibitem[Fang et~al.(2021)Fang, He, Long, and Su]{fang2021exploring}
Cong Fang, Hangfeng He, Qi~Long, and Weijie~J Su.
\newblock Exploring deep neural networks via layer-peeled model: Minority
  collapse in imbalanced training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0
  (43):\penalty0 e2103091118, 2021.

\bibitem[Zhu et~al.(2021)Zhu, Ding, Zhou, Li, You, Sulam, and
  Qu]{zhu2021geometric}
Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and
  Qing Qu.
\newblock A geometric analysis of neural collapse with unconstrained features,
  2021.
\newblock URL \url{https://arxiv.org/abs/2105.02375v1}.

\bibitem[Han et~al.(2022)Han, Papyan, and Donoho]{han2022neural}
X.Y. Han, Vardan Papyan, and David~L. Donoho.
\newblock Neural collapse under {MSE} loss: Proximity to and dynamics on the
  central path.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=w1UbdvWH_R3}.

\bibitem[Yaras et~al.(2022)Yaras, Wang, Zhu, Balzano, and Qu]{yaras2022neural}
Can Yaras, Peng Wang, Zhihui Zhu, Laura Balzano, and Qing Qu.
\newblock Neural collapse with normalized features: A geometric analysis over
  the riemannian manifold.
\newblock \emph{Advances in neural information processing systems},
  35:\penalty0 11547--11560, 2022.

\bibitem[Rangamani and Banburski-Fahey(2022)]{rangamani9746778}
Akshay Rangamani and Andrzej Banburski-Fahey.
\newblock Neural collapse in deep homogeneous classifiers and the role of
  weight decay.
\newblock In \emph{ICASSP 2022 - 2022 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pages 4243--4247, 2022.
\newblock \doi{10.1109/ICASSP43922.2022.9746778}.

\bibitem[Yang et~al.(2022)Yang, Chen, Li, Xie, Lin, and Tao]{yang2022inducing}
Yibo Yang, Shixiang Chen, Xiangtai Li, Liang Xie, Zhouchen Lin, and Dacheng
  Tao.
\newblock Inducing neural collapse in imbalanced learning: Do we really need a
  learnable classifier at the end of deep neural network?, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.09081v3}.

\bibitem[Tirer and Bruna(2022)]{pmlr-v162-tirer22a}
Tom Tirer and Joan Bruna.
\newblock Extended unconstrained features model for exploring deep neural
  collapse.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pages 21478--21505. PMLR,
  17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/tirer22a.html}.

\bibitem[Wang et~al.(2022)Wang, Liu, Yaras, Balzano, and Qu]{wang2022linear}
Peng Wang, Huikang Liu, Can Yaras, Laura Balzano, and Qing Qu.
\newblock Linear convergence analysis of neural collapse with unconstrained
  features.
\newblock In \emph{OPT 2022: Optimization for Machine Learning (NeurIPS 2022
  Workshop)}, 2022.

\bibitem[Zhou et~al.(2022)Zhou, You, Li, Liu, Liu, Qu, and Zhu]{zhou2022losses}
Jinxin Zhou, Chong You, Xiao Li, Kangning Liu, Sheng Liu, Qing Qu, and Zhihui
  Zhu.
\newblock Are all losses created equal: A neural collapse perspective, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.02192v2}.

\bibitem[Rangamani et~al.(2023)Rangamani, Lindegaard, Galanti, and
  Poggio]{pmlr-v202-rangamani23a}
Akshay Rangamani, Marius Lindegaard, Tomer Galanti, and Tomaso~A Poggio.
\newblock Feature learning in deep classifiers through intermediate neural
  collapse.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
  Sivan Sabato, and Jonathan Scarlett, editors, \emph{Proceedings of the 40th
  International Conference on Machine Learning}, volume 202 of
  \emph{Proceedings of Machine Learning Research}, pages 28729--28745. PMLR,
  23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/rangamani23a.html}.

\bibitem[Tirer et~al.(2023)Tirer, Huang, and Niles-Weed]{tirer2023perturbation}
Tom Tirer, Haoxiang Huang, and Jonathan Niles-Weed.
\newblock Perturbation analysis of neural collapse.
\newblock In \emph{International Conference on Machine Learning}, pages
  34301--34329. PMLR, 2023.

\bibitem[Dang et~al.(2023)Dang, Tran, Osher, Tran-The, Ho, and
  Nguyen]{dang2023neural}
Hien Dang, Tho Tran, Stanley Osher, Hung Tran-The, Nhat Ho, and Tan Nguyen.
\newblock Neural collapse in deep linear networks: From balanced to imbalanced
  data, 2023.
\newblock URL \url{https://arxiv.org/abs/2301.00437v5}.

\bibitem[S{\'u}ken{\'\i}k et~al.(2024)S{\'u}ken{\'\i}k, Mondelli, and
  Lampert]{sukenik2024deep}
Peter S{\'u}ken{\'\i}k, Marco Mondelli, and Christoph~H Lampert.
\newblock Deep neural collapse is provably optimal for the deep unconstrained
  features model.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Nguyen et~al.(2023)Nguyen, Levie, Lienen, H{\"u}llermeier, and
  Kutyniok]{nguyen2023memorization}
Duc~Anh Nguyen, Ron Levie, Julian Lienen, Eyke H{\"u}llermeier, and Gitta
  Kutyniok.
\newblock Memorization-dilation: Modeling neural collapse under noise.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=cJWxqmmDL2b}.

\bibitem[Zhong et~al.(2023)Zhong, Cui, Yang, Wu, Qi, Zhang, and
  Jia]{zhong2023understanding}
Zhisheng Zhong, Jiequan Cui, Yibo Yang, Xiaoyang Wu, Xiaojuan Qi, Xiangyu
  Zhang, and Jiaya Jia.
\newblock Understanding imbalanced semantic segmentation through neural
  collapse.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 19550--19560, 2023.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Zhang, Hu, Cao, Yao, and
  Pan]{liu2023inducing}
Xuantong Liu, Jianfeng Zhang, Tianyang Hu, He~Cao, Yuan Yao, and Lujia Pan.
\newblock Inducing neural collapse in deep long-tailed learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 11534--11544. PMLR, 2023{\natexlab{b}}.

\bibitem[Gao et~al.(2023)Gao, Xu, Wen, Shao, Yang, and Huang]{gao2023study}
Peifeng Gao, Qianqian Xu, Peisong Wen, Huiyang Shao, Zhiyong Yang, and Qingming
  Huang.
\newblock A study of neural collapse phenomenon: Grassmannian frame, symmetry
  and generalization, 2023.
\newblock URL \url{https://arxiv.org/abs/2304.08914v2}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Nica, and Roy]{li2023neural}
Mufan~Bill Li, Mihai Nica, and Daniel~M. Roy.
\newblock The neural covariance sde: Shaped infinite depth-and-width networks
  at initialization, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2206.02768v3}.

\bibitem[Hu et~al.(2024)Hu, Wang, Ning, Tai, and Nie]{hu2024neural}
Zhanxuan Hu, Yichen Wang, Hailong Ning, Yonghang Tai, and Feiping Nie.
\newblock Neural collapse inspired semi-supervised learning with fixed
  classifier.
\newblock \emph{Information Sciences}, 667:\penalty0 120469, 2024.

\bibitem[Peifeng et~al.(2024)Peifeng, Xu, Yang, Wen, Shao, Yang, Ghanem, and
  Huang]{peifeng2024towards}
Gao Peifeng, Qianqian Xu, Yibo Yang, Peisong Wen, Huiyang Shao, Zhiyong Yang,
  Bernard Ghanem, and Qingming Huang.
\newblock Towards demystifying the generalization behaviors when neural
  collapse emerges, 2024.
\newblock URL \url{https://openreview.net/forum?id=XVv4S6LnMk}.

\bibitem[Elhage et~al.(2022)Elhage, Hume, Olsson, Schiefer, Henighan, Kravec,
  Hatfield-Dodds, Lasenby, Drain, Chen, Grosse, McCandlish, Kaplan, Amodei,
  Wattenberg, and Olah]{elhage2022superposition}
Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan,
  Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen,
  Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg,
  and Christopher Olah.
\newblock Toy models of superposition.
\newblock \emph{Transformer Circuits Thread}, 2022.
\newblock URL
  \url{https://transformer-circuits.pub/2022/toy\_model/index.html}.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[et~al. (101~additional authors)(2024)]{dubey2024llama3herdmodels}
A.~Dubey et~al. (101~additional authors).
\newblock The llama 3 herd of models.
\newblock 2024.
\newblock URL \url{https://arxiv.org/abs/2407.21783}.
\newblock All authors were affiliated with Meta.

\bibitem[Thrampoulidis et~al.(2022)Thrampoulidis, Kini, Vakilian, and
  Behnia]{thrampoulidis2022imbalance}
Christos Thrampoulidis, Ganesh~Ramachandra Kini, Vala Vakilian, and Tina
  Behnia.
\newblock Imbalance trouble: Revisiting neural-collapse geometry.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27225--27238, 2022.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and
  Tewari]{natarajan2013learning}
Nagarajan Natarajan, Inderjit~S Dhillon, Pradeep~K Ravikumar, and Ambuj Tewari.
\newblock Learning with noisy labels.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Fisher et~al.(2024)Fisher, Meng, and Papyan]{fisher2024pushing}
Quinn Fisher, Haoming Meng, and Vardan Papyan.
\newblock Pushing boundaries: Mixup's influence on neural collapse, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.06171v1}.

\bibitem[Li et~al.(2024{\natexlab{c}})Li, Li, Wang, and Qu]{li2024neural}
Pengyu Li, Xiao Li, Yutong Wang, and Qing Qu.
\newblock Neural collapse in multi-label learning with pick-all-label loss,
  2024{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2310.15903v4}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Mimno and Thompson(2017)]{mimno-thompson-2017-strange}
David Mimno and Laure Thompson.
\newblock The strange geometry of skip-gram with negative sampling.
\newblock In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors,
  \emph{Proceedings of the 2017 Conference on Empirical Methods in Natural
  Language Processing}, pages 2873--2878, Copenhagen, Denmark, September 2017.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D17-1308}.
\newblock URL \url{https://aclanthology.org/D17-1308}.

\bibitem[Ethayarajh(2019)]{ethayarajh2019contextual}
Kawin Ethayarajh.
\newblock How contextual are contextualized word representations? comparing the
  geometry of bert, elmo, and gpt-2 embeddings, 2019.
\newblock URL \url{https://arxiv.org/abs/1909.00512}.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2019glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding, 2019.
\newblock URL \url{https://arxiv.org/abs/1804.07461v3}.

\bibitem[Feng et~al.(2023)Feng, Lai, and Li]{feng2023study}
Jia~Hui Feng, Edmund M-K Lai, and Weihua Li.
\newblock A study of neural collapse for text classification.
\newblock In \emph{International Conference on Deep Learning Theory and
  Applications}, pages 126--142. Springer, 2023.

\bibitem[Laurent et~al.(2023)Laurent, von Brecht, and
  Bresson]{laurent2023feature}
Thomas Laurent, James~H. von Brecht, and Xavier Bresson.
\newblock Feature collapse, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.16162v1}.

\bibitem[Black et~al.(2021)Black, Gao, Wang, Leahy, and Biderman]{gpt-neo}
Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman.
\newblock {GPT-Neo: Large Scale Autoregressive Language Modeling with
  Mesh-Tensorflow}, March 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5297715}.
\newblock {If you use this software, please cite it using these metadata.}

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, Mihaylov, Ott, Shleifer, Shuster, Simig, Koura, Sridhar, Wang,
  and Zettlemoyer]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov,
  Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali
  Sridhar, Tianlu Wang, and Luke Zettlemoyer.
\newblock Opt: Open pre-trained transformer language models, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.01068v4}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and
  Lample]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
  Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume
  Lample.
\newblock Llama: Open and efficient foundation language models, 2023.
\newblock URL \url{https://arxiv.org/abs/2302.13971v1}.

\bibitem[Jiang et~al.(2023{\natexlab{b}})Jiang, Sablayrolles, Mensch, Bamford,
  Chaplot, de~las Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux,
  Stock, Scao, Lavril, Wang, Lacroix, and Sayed]{jiang2023mistral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
  Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,
  Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux,
  Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,
  and William~El Sayed.
\newblock Mistral 7b, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2310.06825v1}.

\bibitem[Fisher(1936)]{fisher1936use}
Ronald~A Fisher.
\newblock The use of multiple measurements in taxonomic problems.
\newblock \emph{Annals of eugenics}, 7\penalty0 (2):\penalty0 179--188, 1936.

\bibitem[Rao(1948)]{rao1948utilization}
C~Radhakrishna Rao.
\newblock The utilization of multiple measurements in problems of biological
  classification.
\newblock \emph{Journal of the Royal Statistical Society. Series B
  (Methodological)}, 10\penalty0 (2):\penalty0 159--203, 1948.

\bibitem[Strohmer and Heath~Jr(2003)]{strohmer2003grassmannian}
Thomas Strohmer and Robert~W Heath~Jr.
\newblock Grassmannian frames with applications to coding and communication.
\newblock \emph{Applied and computational harmonic analysis}, 14\penalty0
  (3):\penalty0 257--275, 2003.

\bibitem[Waldron(2018)]{waldron2018introduction}
Shayne~FD Waldron.
\newblock \emph{An introduction to finite tight frames}.
\newblock Springer, 2018.

\bibitem[Donoho et~al.(2005)Donoho, Elad, and Temlyakov]{donoho2005stable}
David~L Donoho, Michael Elad, and Vladimir~N Temlyakov.
\newblock Stable recovery of sparse overcomplete representations in the
  presence of noise.
\newblock \emph{IEEE Transactions on information theory}, 52\penalty0
  (1):\penalty0 6--18, 2005.

\bibitem[Tropp(2006)]{tropp2006just}
Joel~A Tropp.
\newblock Just relax: Convex programming methods for identifying sparse signals
  in noise.
\newblock \emph{IEEE transactions on information theory}, 52\penalty0
  (3):\penalty0 1030--1051, 2006.

\bibitem[Ziegler et~al.(2020)Ziegler, Stiennon, Wu, Brown, Radford, Amodei,
  Christiano, and Irving]{ziegler2020finetuninglanguagemodelshuman}
Daniel~M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B. Brown, Alec Radford,
  Dario Amodei, Paul Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences, 2020.
\newblock URL \url{https://arxiv.org/abs/1909.08593}.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Manning, Ermon, and
  Finn]{NEURIPS2023_a85b405e}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano
  Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a
  reward model.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and
  S.~Levine, editors, \emph{Advances in Neural Information Processing Systems},
  volume~36, pages 53728--53741. Curran Associates, Inc., 2023.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/a85b405ed65c6477a4fe8302b5e06ce7-Paper-Conference.pdf}.

\bibitem[Papyan(2020)]{papyan2020traces}
Vardan Papyan.
\newblock Traces of class/cross-class structure pervade deep learning spectra,
  2020.
\newblock URL \url{https://arxiv.org/abs/2008.11865v1}.

\bibitem[Hoyt and Owen(2021)]{hoyt2021probing}
Christopher~R. Hoyt and Art~B. Owen.
\newblock Probing neural networks with t-sne, class-specific projections and a
  guided tour, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.12547v1}.

\bibitem[Zarka et~al.(2021)Zarka, Guth, and Mallat]{zarka2021separation}
John Zarka, Florentin Guth, and Stéphane Mallat.
\newblock Separation and concentration in deep networks, 2021.
\newblock URL \url{https://arxiv.org/abs/2012.10424v2}.

\bibitem[Ben-Shaul and Dekel(2022)]{pmlr-v196-ben-shaul22a}
Ido Ben-Shaul and Shai Dekel.
\newblock Nearest class-center simplification through intermediate layers.
\newblock In Alexander Cloninger, Timothy Doster, Tegan Emerson, Manohar Kaul,
  Ira Ktena, Henry Kvinge, Nina Miolane, Bastian Rieck, Sarah Tymochko, and Guy
  Wolf, editors, \emph{Proceedings of Topological, Algebraic, and Geometric
  Learning Workshops 2022}, volume 196 of \emph{Proceedings of Machine Learning
  Research}, pages 37--47. PMLR, 25 Feb--22 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v196/ben-shaul22a.html}.

\bibitem[He and Su(2022)]{he2022law}
Hangfeng He and Weijie~J. Su.
\newblock A law of data separation in deep learning, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.17020v2}.

\bibitem[Parker et~al.(2023)Parker, Onal, Stengel, and
  Intrater]{parker2023neural}
Liam Parker, Emre Onal, Anton Stengel, and Jake Intrater.
\newblock Neural collapse in the intermediate hidden layers of classification
  neural networks, 2023.
\newblock URL \url{https://arxiv.org/abs/2308.02760v1}.

\bibitem[Masarczyk et~al.(2023)Masarczyk, Ostaszewski, Imani, Pascanu,
  Mi\l~o\'{s}, and Trzcinski]{NEURIPS2023_f249db9a}
Wojciech Masarczyk, Mateusz Ostaszewski, Ehsan Imani, Razvan Pascanu, Piotr
  Mi\l~o\'{s}, and Tomasz Trzcinski.
\newblock The tunnel effect: Building data representations in deep neural
  networks.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and
  S.~Levine, editors, \emph{Advances in Neural Information Processing Systems},
  volume~36, pages 76772--76805. Curran Associates, Inc., 2023.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2023/file/f249db9ab5975586f36df46f8958c008-Paper-Conference.pdf}.

\bibitem[Beaglehole et~al.(2024)Beaglehole, Súkeník, Mondelli, and
  Belkin]{beaglehole2024average}
Daniel Beaglehole, Peter Súkeník, Marco Mondelli, and Mikhail Belkin.
\newblock Average gradient outer product as a mechanism for deep neural
  collapse, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.13728v2}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Li, Yaras, Zhu, Balzano, Hu, and
  Qu]{wang2024understanding}
Peng Wang, Xiao Li, Can Yaras, Zhihui Zhu, Laura Balzano, Wei Hu, and Qing Qu.
\newblock Understanding deep representation learning via layerwise feature
  compression and discrimination, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2311.02960v2}.

\bibitem[Wang et~al.(2024{\natexlab{c}})Wang, Gai, and
  Zhang]{wang2024progressive}
Sicong Wang, Kuo Gai, and Shihua Zhang.
\newblock Progressive feedforward collapse of resnet training,
  2024{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2405.00985v1}.

\bibitem[Garrod and Keating(2024)]{garrod2024unifying}
Connall Garrod and Jonathan~P. Keating.
\newblock Unifying low dimensional observations in deep learning through the
  deep linear unconstrained feature model, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.06106v1}.

\bibitem[Zangrando et~al.(2024)Zangrando, Deidda, Brugiapaglia, Guglielmi, and
  Tudisco]{zangrando2024neural}
Emanuele Zangrando, Piero Deidda, Simone Brugiapaglia, Nicola Guglielmi, and
  Francesco Tudisco.
\newblock Neural rank collapse: Weight decay and small within-class variability
  yield low-rank bias, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.03991v1}.

\bibitem[Jiang et~al.(2024)Jiang, Zhou, and Zhu]{jiang2024layerwise}
Jiachen Jiang, Jinxin Zhou, and Zhihui Zhu.
\newblock On layer-wise representation similarity: Application for multi-exit
  models with a single classifier, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.14479}.

\bibitem[Xu et~al.(2023)Xu, Rangamani, Liao, Galanti, and
  Poggio]{xu2023dynamics}
Mengjia Xu, Akshay Rangamani, Qianli Liao, Tomer Galanti, and Tomaso Poggio.
\newblock Dynamics in deep classifiers trained with the square loss:
  Normalization, low rank, neural collapse, and generalization bounds.
\newblock \emph{Research}, 6:\penalty0 0024, 2023.

\bibitem[Liang and Davis(2023)]{liang2023inducing}
Tong Liang and Jim Davis.
\newblock Inducing neural collapse to a fixed hierarchy-aware frame for
  reducing mistake severity.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1443--1452, 2023.

\bibitem[Bonifazi et~al.(2024)Bonifazi, Chalas, Hess, and
  Łucki]{bonifazi2024understand}
Guglielmo Bonifazi, Iason Chalas, Gian Hess, and Jakub Łucki.
\newblock Can we understand plasticity through neural collapse?, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.02719v1}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{hendryckstest2021}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem[bench authors(2023)]{srivastava2023beyond}
BIG bench authors.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=uyTL5Bvosj}.

\bibitem[Du et~al.(2024)Du, Zeng, Dong, and
  Tang]{du2024understandingemergentabilitieslanguage}
Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang.
\newblock Understanding emergent abilities of language models from the loss
  perspective, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.15796}.

\bibitem[Huang et~al.(2024)Huang, Zhang, Shan, and He]{huang2024compression}
Yuzhen Huang, Jinghan Zhang, Zifei Shan, and Junxian He.
\newblock Compression represents intelligence linearly.
\newblock In \emph{First Conference on Language Modeling}, 2024.
\newblock URL \url{https://openreview.net/forum?id=SHMj84U5SH}.

\bibitem[Yin et~al.(2024)Yin, Wu, Wang, Wang, Guo, Wang, Liu, Tang, Lian, and
  Chen]{yin2024entropy}
Mingjia Yin, Chuhan Wu, Yufei Wang, Hao Wang, Wei Guo, Yasheng Wang, Yong Liu,
  Ruiming Tang, Defu Lian, and Enhong Chen.
\newblock Entropy law: The story behind data compression and llm performance,
  2024.

\bibitem[Guo et~al.(2024)Guo, Ross, Zhao, Andriopoulos, Ling, Xu, and
  Dong]{guo2024cross}
Li~Guo, Keith Ross, Zifan Zhao, George Andriopoulos, Shuyang Ling, Yufeng Xu,
  and Zixuan Dong.
\newblock Cross entropy versus label smoothing: A neural collapse perspective,
  2024.
\newblock URL \url{https://arxiv.org/abs/2402.03979v2}.

\bibitem[Xu et~al.(2024)Xu, Chen, Li, Zhao, and Wei]{xu2024collapsed}
Jingxuan Xu, Wuyang Chen, Linyi Li, Yao Zhao, and Yunchao Wei.
\newblock Collapsed language models promote fairness, 2024.
\newblock URL \url{https://arxiv.org/abs/2410.04472}.

\bibitem[Merity et~al.(2016)Merity, Sutskever, Kornblith, and
  Goyal]{merity2016pointer}
Stephen Merity, Ilya Sutskever, Simon Kornblith, and Nikhil Goyal.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Zhu et~al.(2015)Zhu, Yu, Zhang, Wu, et~al.]{zhu2015aligning}
Yao Zhu, Zhen Yu, Chao Zhang, Yijia Wu, et~al.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock \emph{arXiv preprint arXiv:1506.05829}, 2015.

\bibitem[{Common Crawl}(2023)]{commoncrawl}
{Common Crawl}.
\newblock Common crawl, 2023.
\newblock URL \url{https://commoncrawl.org}.
\newblock Accessed: 2023-10-30.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, Presser, and Leahy]{gao2020pile800gbdatasetdiverse}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,
  and Connor Leahy.
\newblock The pile: An 800gb dataset of diverse text for language modeling,
  2020.
\newblock URL \url{https://arxiv.org/abs/2101.00027}.

\bibitem[Shumailov et~al.(2024)Shumailov, Shumaylov, Zhao, Gal, Papernot, and
  Anderson]{shumailov2024curse}
Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and
  Ross Anderson.
\newblock The curse of recursion: Training on generated data makes models
  forget, 2024.
\newblock URL \url{https://arxiv.org/abs/2305.17493}.

\bibitem[Gerstgrasser et~al.(2024)Gerstgrasser, Schaeffer, Dey, Rafailov,
  Sleight, Hughes, Korbak, Agrawal, Pai, Gromov, Roberts, Yang, Donoho, and
  Koyejo]{gerstgrasser2024modelcollapse}
Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov, Henry
  Sleight, John Hughes, Tomasz Korbak, Rajashree Agrawal, Dhruv Pai, Andrey
  Gromov, Daniel~A. Roberts, Diyi Yang, David~L. Donoho, and Sanmi Koyejo.
\newblock Is model collapse inevitable? breaking the curse of recursion by
  accumulating real and synthetic data, 2024.
\newblock URL \url{https://arxiv.org/abs/2404.01413}.

\bibitem[Burgess et~al.(2019)Burgess, Milanovic, Stephens, Monachopoulos, and
  Mansell]{8877390}
Neil Burgess, Jelena Milanovic, Nigel Stephens, Konstantinos Monachopoulos, and
  David Mansell.
\newblock Bfloat16 processing for neural networks.
\newblock In \emph{2019 IEEE 26th Symposium on Computer Arithmetic (ARITH)},
  pages 88--91, 2019.
\newblock \doi{10.1109/ARITH.2019.00022}.

\end{thebibliography}
