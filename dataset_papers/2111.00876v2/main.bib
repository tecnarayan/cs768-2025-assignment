@online{suttonwebRLhypothesis,
  author = {Richard S. Sutton},
  title = {The reward hypothesis},
  year ={2004},
  url = {http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html},
}

@online{littmanRH,
  author = {Michael L. Littman},
  title = {The reward hypothesis},
  year ={2017},
  url = {https://www.coursera.org/lecture/fundamentals-of-reinforcement-learning/michael-littman-the-reward-hypothesis-q6x0e},
}
@book{puterman2014markov,
  title={Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  author={Puterman, Martin L.},
  year={2014},
  publisher={John Wiley \& Sons}
}

@inproceedings{cassandra1994acting,
  title={Acting optimally in partially observable stochastic domains},
  author={Cassandra, Anthony R. and Kaelbling, Leslie Pack and Littman, Michael L.},
  booktitle={Proceedings of the AAAI Conference on Artificiall Intelligence},
  year={1994}
}

@article{robinson1949definability,
  title={Definability and decision problems in arithmetic},
  author={Robinson, Julia},
  journal={The Journal of Symbolic Logic},
  volume={14},
  number={2},
  pages={98--114},
  year={1949},
  publisher={Cambridge University Press}
}

@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y. and Harada, Daishi and Russell, Stuart},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={1999}
}

@article{kaelbling1998planning,
  author = "Leslie Pack Kaelbling and Michael L. Littman and
                  Anthony R. Cassandra",
  title = "Planning and acting in partially observable
                  stochastic domains", 
  journal = "Artificial Intelligence",
  volume = "101",
  number = "1--2",
  year = "1998",
  pages = "99--134",
  myurl = "http://www.elsevier.nl/cas/tree/store/artint/sub/1998/101/1-2/1532.pdf",
  type = "bib2html"
}

@article{wirth2017survey,
  title={A survey of preference-based reinforcement learning methods},
  author={Wirth, Christian and Akrour, Riad and Neumann, Gerhard and F{\"u}rnkranz, Johannes},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={4945--4990},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{amin2017repeated,
  title={Repeated inverse reinforcement learning},
  author={Amin, Kareem and Jiang, Nan and Singh, Satinder},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}

@inproceedings{bellemare2017distributional,
  title={A distributional perspective on reinforcement learning},
  author={Bellemare, Marc G. and Dabney, Will and Munos, R{\'e}mi},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2017},
}

@inproceedings{rowland2018analysis,
  title={An analysis of categorical distributional reinforcement learning},
  author={Rowland, Mark and Bellemare, Marc G. and Dabney, Will and Munos, R{\'e}mi and Teh, Yee Whye},
  booktitle={Proceedings of the International Conference on Artificial Intelligence and Statistics},
  year={2018}
}

@inproceedings{dabney2018implicit,
  title={Implicit quantile networks for distributional reinforcement learning},
  author={Dabney, Will and Ostrovski, Georg and Silver, David and Munos, R{\'e}mi},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2018}
}

@article{cook1986,
  title={An axiomatic approach to distance on partial orderings},
  author={Cook, Wade D and Kress, Moshe and Seiford, Lawrence M},
  journal={RAIRO-Operations Research},
  volume={20},
  number={2},
  pages={115--122},
  year={1986},
  publisher={EDP Sciences}
}

@article{mezHo2014periodicity,
  title={Periodicity of the last digits of some combinatorial sequences},
  author={Mez{\H{o}}, Istv{\'a}n},
  journal={J. Integer Seq},
  volume={17},
  pages={1--18},
  year={2014}
}

@inproceedings{fagin2004comparing,
  title={Comparing and aggregating rankings with ties},
  author={Fagin, Ronald and Kumar, Ravi and Mahdian, Mohammad and Sivakumar, D and Vee, Erik},
  booktitle={ACM Symposium on Principles of Database Systems},
  year={2004}
}

@article{fagin2006comparing,
  title={Comparing partial rankings},
  author={Fagin, Ronald and Kumar, Ravi and Mahdian, Mohammad and Sivakumar, D and Vee, Erik},
  journal={SIAM Journal on Discrete Mathematics},
  volume={20},
  number={3},
  pages={628--648},
  year={2006},
  publisher={SIAM}
}

@inproceedings{jiang2015dependence,
  title={The dependence of effective planning horizon on model accuracy},
  author={Jiang, Nan and Kulesza, Alex and Singh, Satinder and Lewis, Richard},
  booktitle={Proceedings of the International Conference on Autonomous Agents and Multiagent Systems},
  year={2015},
}

@article{semal1991iterative,
  title={Iterative algorithms for large stochastic matrices},
  author={Semal, Pierre},
  journal={Linear algebra and its applications},
  volume={154},
  pages={65--103},
  year={1991},
  publisher={Elsevier}
}
@article{xu2020preference,
  title={Preference-based Reinforcement Learning with Finite-Time Guarantees},
  author={Xu, Yichong and Wang, Ruosong and Yang, Lin and Singh, Aarti and Dubrawski, Artur},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@book{kreps1988notes,
  title={Notes on the Theory of Choice},
  author={Kreps, David},
  year={1988},
  publisher={Westview Press}
}

@inproceedings{duchi2010consistency,
  title={On the consistency of ranking algorithms},
  author={Duchi, John C and Mackey, Lester W and Jordan, Michael I},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2010}
}
@phdthesis{sorg2011optimal,
  title={The Optimal Reward Problem: Designing Effective Reward for Bounded Agents},
  author={Sorg, Jonathan},
  school={University of Michigan},
  year={2011}
}
@article{littman2017environment,
  title={Environment-independent task specifications via {GLTL}},
  author={Littman, Michael L. and Topcu, Ufuk and Fu, Jie and Isbell, Charles and Wen, Min and MacGlashan, James},
  journal={arXiv preprint arXiv:1704.04341},
  year={2017}
}
@article{bertsekas1991analysis,
  title={An analysis of stochastic shortest path problems},
  author={Bertsekas, Dimitri P. and Tsitsiklis, John N.},
  journal={Mathematics of Operations Research},
  volume={16},
  number={3},
  pages={580--595},
  year={1991},
  publisher={INFORMS}
}
@article{friston2010free,
  title={The free-energy principle: a unified brain theory?},
  author={Friston, Karl J.},
  journal={Nature reviews neuroscience},
  volume={11},
  number={2},
  pages={127--138},
  year={2010},
  publisher={Nature publishing group}
}
@article{hafner2020action,
  title={Action and perception as divergence minimization},
  author={Hafner, Danijar and Ortega, Pedro A. and Ba, Jimmy and Parr, Thomas and Friston, Karl J. and Heess, Nicolas},
  journal={arXiv preprint arXiv:2009.01791},
  year={2020}
}
@article{clark2020beyond,
  title={Beyond desire? Agency, choice, and the predictive mind},
  author={Clark, Andy},
  journal={Australasian Journal of Philosophy},
  volume={98},
  number={1},
  pages={1--15},
  year={2020},
  publisher={Taylor \& Francis}
}

@incollection{tishby2011information,
  title={Information theory of decisions and actions},
  author={Tishby, Naftali and Polani, Daniel},
  booktitle={Perception-action cycle},
  pages={601--636},
  year={2011},
  publisher={Springer}
}
@inproceedings{akshay2013steady,
  title={The steady-state control problem for {M}arkov decision processes},
  author={Akshay, Sundararaman and Bertrand, Nathalie and Haddad, Serge and Helouet, Loic},
  booktitle={Proceedings of the International Conference on Quantitative Evaluation of Systems},
  year={2013},
}

@inproceedings{singh2009rewards,
  title={Where do rewards come from?},
  author={Singh, Satinder and Lewis, Richard L and Barto, Andrew G},
  booktitle={Proceedings of the Annual Conference of the Cognitive Science Society},
  year={2009},
}

@inproceedings{christiano2017deep,
  title={Deep Reinforcement Learning from Human Preferences},
  author={Christiano, Paul F. and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}
@inproceedings{krishna2020maximum,
  title={Maximum Reward Formulation In Reinforcement Learning},
  author={Krishna Gottipati, Sai and Pathak, Yashaswi and Nuttall, Rohan and Chunduru, Raviteja and Touati, Ahmed and Ganapathi Subramanian, Sriram and Taylor, Matthew E. and Chandar, Sarath},
  booktitle={NeurIPS Workshop on Deep Reinforcement Learning},
  year={2020}
}
@inproceedings{macglashan2016convergent,
  title={Convergent actor critic by humans},
  author={MacGlashan, James and Littman, Michael L. and Roberts, David L. and Loftin, Robert and Peng, Bei and Taylor, Matthew E.},
  booktitle={Proceedings of the International Conference on Intelligent Robots and Systems},
  year={2016}
}

@inproceedings{novoseller2020dueling,
  title={Dueling posterior sampling for preference-based reinforcement learning},
  author={Novoseller, Ellen and Wei, Yibing and Sui, Yanan and Yue, Yisong and Burdick, Joel},
  booktitle={Proceedings of the Conference on Uncertainty in Artificial Intelligence},
  year={2020},
}
@inproceedings{hadfield2017inverse,
  title={Inverse reward design},
  author={Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart and Dragan, Anca},
  booktitle={Advances in Neural Information Processing Systems},
  year={2017}
}
@online{yann_cake,
  title = {Yann LeCun's Cake Analogy},
  url = {https://medium.com/syncedreview/yann-lecun-cake-analogy-2-0-a361da560dae}
}

@inproceedings{arumugam2017accurately,
  title={Accurately and efficiently interpreting human-robot instructions of varying granularities},
  author={Arumugam, Dilip and Karamcheti, Siddharth and Gopalan, Nakul and Wong, Lawson LS and Tellex, Stefanie},
  booktitle={Proceedings of the Robotics: Science and Systems Conference},
  year={2017}
}
@inproceedings{agarwal2019learning,
  title={Learning to generalize from sparse and underspecified rewards},
  author={Agarwal, Rishabh and Liang, Chen and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2019},
}

@book{russell94,
  author = "Stuart J. Russell and Peter Norvig",
  title = "Artificial Intelligence: {A} Modern Approach",
  publisher = "Prentice-Hall",
  isbn = "0-13-103805-2",
  address = "Englewood Cliffs, NJ",
  year = 1994
}


@article{ackley1992interactions,
  title={Interactions between learning and evolution},
  author={Ackley, David and Littman, Michael L.},
  journal={Artificial Life II},
  year={1992},
  publisher={Addison-Wesley}
}

@book{sutton2018reinforcement,
  title={Reinforcement Learning: An Introduction},
  author={Sutton, Richard S. and Barto, Andrew G.},
  year={2018},
  publisher={MIT Press}
}

@inproceedings{auer2009near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  booktitle={Advances in Neural Information Processing Systems},
  year={2009}
}

@article{Strehl2009,
    author = {Strehl, Alexander L. and Li, Lihong and Littman, Michael L.},
    journal = {Journal of Machine Learning Research},
    pages = {2413--2444},
    title = {Reinforcement Learning in Finite {MDP}s: {PAC} Analysis},
    volume = {10},
    year = {2009}
}

@inproceedings{williams2018learning,
  title={Learning to parse natural language to grounded reward functions with weak supervision},
  author={Williams, Edward C. and Gopalan, Nakul and Rhee, Mine and Tellex, Stefanie},
  booktitle={Proceedings of the International Conference on Robotics and Automation},
  year={2018},
}

@misc{singh2010separating,
  title={On Separating Agent Designer Goals from Agent Goals: Breaking the Preferences--Parameters Confound},
  author={Singh, Satinder and Lewis, Richard L. and Sorg, Jonathan and Barto, Andrew G. and Helou, Akram},
  year={2010},
  publisher={Citeseer}
}

@inproceedings{zheng2020can,
  title={What Can Learned Intrinsic Rewards Capture?},
  author={Zheng, Zeyu and Oh, Junhyuk and Hessel, Matteo and Xu, Zhongwen and Kroiss, Manuel and van Hasselt, Hado and Silver, David and Singh, Satinder},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2020},
}
@techreport{singh2005intrinsically,
  title={Intrinsically motivated reinforcement learning},
  author={Singh, Satinder and Barto, Andrew G. and Chentanez, Nuttapong},
  year={2005},
  institution={University of Massachusetts at Amherst Department of Computer Science}
}

@article{haydencase2020,
  title={The case against economic values in the orbitofrontal cortex (or anywhere else in the brain)},
  author={Hayden, Benjamin Y. and Niv, Yael},
  journal={PsyArXiv},
  year={2020}
}

@article{dayan2002reward,
  title={Reward, motivation, and reinforcement learning},
  author={Dayan, Peter and Balleine, Bernard W},
  journal={Neuron},
  volume={36},
  number={2},
  pages={285--298},
  year={2002},
  publisher={Elsevier}
}

@article{bayer2005midbrain,
  title={Midbrain dopamine neurons encode a quantitative reward prediction error signal},
  author={Bayer, Hannah M and Glimcher, Paul W},
  journal={Neuron},
  volume={47},
  number={1},
  pages={129--141},
  year={2005},
  publisher={Elsevier}
}

@article{schultz1997neural,
  title={A neural substrate of prediction and reward},
  author={Schultz, Wolfram and Dayan, Peter and Montague, P Read},
  journal={Science},
  volume={275},
  number={5306},
  pages={1593--1599},
  year={1997},
  publisher={American Association for the Advancement of Science}
}

@article{fedus2019hyperbolic,
  title={Hyperbolic discounting and learning over multiple horizons},
  author={Fedus, William and Gelada, Carles and Bengio, Yoshua and Bellemare, Marc G. and Larochelle, Hugo},
  journal={arXiv preprint arXiv:1902.06865},
  year={2019}
}

@inproceedings{pitis2019rethinking,
  title={Rethinking the discount factor in reinforcement learning: A decision theoretic approach},
  author={Pitis, Silviu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2019}
}


@inproceedings{hadfield2016off,
  title={The off-switch game},
  author={Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
  booktitle={Proceedings of the International Joint Conference on Artificial Intelligence},
  year={2017}
}

@article{kumar2020realab,
  title={{REALab}: An Embedded Perspective on Tampering},
  author={Kumar, Ramana and Uesato, Jonathan and Ngo, Richard and Everitt, Tom and Krakovna, Victoria and Legg, Shane},
  journal={arXiv preprint arXiv:2011.08820},
  year={2020}
}

@inproceedings{koenig93,
  author={Sven Koenig and Reid G. Simmons},
  title={Complexity Analysis of Real-time Reinforcement Learning},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, 
  year={1993},
  mynote = "I think this is the n-cubed Q-learning paper"
}

@article{ho2019people,
  title={People teach with rewards and punishments as communication, not reinforcements},
  author={Ho, Mark K. and Cushman, Fiery and Littman, Michael L. and Austerweil, Joseph L.},
  journal={Journal of Experimental Psychology: General},
  volume={148},
  number={3},
  pages={520},
  year={2019},
  publisher={American Psychological Association}
}

@article{ho2016showing,
  title={Showing versus doing: Teaching by demonstration},
  author={Ho, Mark K. and Littman, Michael L. and MacGlashan, James and Cushman, Fiery and Austerweil, Joseph L},
  journal={Advances in Neural Information Processing Systems},
  year={2016}
}

@inproceedings{macglashan2017interactive,
  title={Interactive learning from policy-dependent human feedback},
  author={MacGlashan, James and Ho, Mark K. and Loftin, Robert and Peng, Bei and Wang, Guan and Roberts, David L. and Taylor, Matthew E. and Littman, Michael L.},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2017},
  organization={PMLR}
}

@inproceedings{hadfield2016cooperative,
  title={Cooperative inverse reinforcement learning},
  author={Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
  booktitle={Advances in Neural Information Processing Systems}, 
  year={2016}
}

@article{michaud2020understanding,
  title={Understanding Learned Reward Functions},
  author={Michaud, Eric J. and Gleave, Adam and Russell, Stuart},
  journal={arXiv preprint arXiv:2012.05862},
  year={2020}
}

@misc{shah2021benefits,
title={Benefits of Assistance over Reward Learning},
author={Rohin Shah and Pedro Freire and Neel Alex and Rachel Freedman and Dmitrii Krasheninnikov and Lawrence Chan and Michael D. Dennis and Pieter Abbeel and Anca Dragan and Stuart Russell},
year={2021},
url={https://openreview.net/forum?id=DFIoGDZejIB}
}

@article{friston2009reinforcement,
  title={Reinforcement learning or active inference?},
  author={Friston, Karl J. and Daunizeau, Jean and Kiebel, Stefan J.},
  journal={PloS One},
  volume={4},
  number={7},
  pages={e6421},
  year={2009},
  publisher={Public Library of Science}
}

@inproceedings{li2006towards,
  title={Towards a Unified Theory of State Abstraction for {MDP}s},
  author={Li, Lihong and Walsh, Thomas J. and Littman, Michael L.},
  booktitle={Proceedings of the International Symposium on Artificial Intelligence and Mathematics},
  year={2006}
}

@inproceedings{macglashan15,
  title = {Grounding {E}nglish commands to reward functions},
    author={MacGlashan, James and Babes-Vroman, Monica and Marie desJardins and Littman, Michael L. and Muresan, Smaranda and Squire, Shawn and Tellex, Stefanie and Arumugam, Dilip and Yang, Lei},
  year = {2015},
  booktitle = {Proceedings of Robotics: Science and Systems}
}

@article{debreu1954representation,
  title={Representation of a preference ordering by a numerical function},
  author={Debreu, Gerard},
  journal={Decision Processes},
  volume={3},
  pages={159--165},
  year={1954},
  publisher={New York}
}

@article{koopmans1960stationary,
  title={Stationary ordinal utility and impatience},
  author={Koopmans, Tjalling C.},
  journal={Econometrica: Journal of the Econometric Society},
  pages={287--309},
  year={1960},
  publisher={JSTOR}
}

@article{koopmans1966structure,
  title={Structure of preference over time},
  author={Koopmans, Tjalling C.},
  journal = "Cowles Foundation Discussion Paper",
  number = 206,
  year={1966}
}

@article{arrow1950difficulty,
  title={A difficulty in the concept of social welfare},
  author={Arrow, Kenneth J.},
  journal={Journal of political economy},
  volume={58},
  number={4},
  pages={328--346},
  year={1950},
  publisher={The University of Chicago Press}
}
@article{eckersley2018impossibility,
  title={Impossibility and Uncertainty Theorems in {AI} Value Alignment (or why your {AGI} should not have a utility function)},
  author={Eckersley, Peter},
  journal={arXiv preprint arXiv:1901.00064},
  year={2018}
}

@book{parfit1984reasons,
  title={Reasons and persons},
  author={Parfit, Derek},
  year={1984},
  publisher={Oxford University Press}
}
@book{vonneumann1953theory,
  title={Theory of Games and Economic Behavior},
  author={von Neumann, John and Morgenstern, Oskar},
  year={1953},
  publisher={Princeton University Press}
}

@inproceedings{wilson2012bayesian,
  title={A {B}ayesian approach for policy learning from trajectory preference queries},
  author={Wilson, Aaron and Fern, Alan and Tadepalli, Prasad},
  booktitle={Advances in Neural Information Processing Systems},
  year={2012}
}

@inproceedings{tasse2020boolean,
  title={A {B}oolean Task Algebra for Reinforcement Learning},
  author={Tasse, Geraud Nangue and James, Steven and Rosman, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{white2017unifying,
  title={Unifying task specification in reinforcement learning},
  author={White, Martha},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2017},
}

@online{ortega2018,
author = {Ortega, Pedro A. and Maini, Vishal and the DeepMind Safety Team},
  title = {Building safe artificial intelligence: specification, robustness, and assurance},
  year ={2018},
  url = {https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1},
}

@inproceedings{everitt2017reinforcement,
  title={Reinforcement learning with a corrupted reward channel},
  author={Everitt, Tom and Krakovna, Victoria and Orseau, Laurent and Hutter, Marcus and Legg, Shane},
  booktitle={Proceedings of the International Joint Conference on Artificial Intelligence},
  year={2017}
}

@inproceedings{jeon2020reward,
  title={Reward-rational (implicit) choice: A unifying formalism for reward learning},
  author={Jeon, Hong Jun and Milli, Smitha and Dragan, Anca},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{ng2000algorithms,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, Andrew Y. and Russell, Stuart J. and others},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2000}
}

@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng., Andrew Y.},
  booktitle={Proceedings of the International Conference on Machine learning},
  year={2004}
}

@inproceedings{syed2008apprenticeship,
  title={Apprenticeship learning using linear programming},
  author={Syed, Umar and Bowling, Michael and Schapire, Robert E.},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2008}
}

@inproceedings{karmarkar1984new,
  title={A new polynomial-time algorithm for linear programming},
  author={Karmarkar, Narendra},
  booktitle={Proceedings of the Annual ACM Symposium on Theory of Computing},
  year={1984}
}

@inproceedings{icarte2018using,
  title={Using reward machines for high-level task specification and decomposition in reinforcement learning},
  author={Icarte, Rodrigo Toro and Klassen, Toryn and Valenzano, Richard and McIlraith, Sheila},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2018},
}

@inproceedings{toro2018teaching,
  title={Teaching multiple tasks to an {RL} agent using {LTL}},
  author={Toro Icarte, Rodrigo and Klassen, Toryn Q. and Valenzano, Richard and McIlraith, Sheila A.},
  booktitle={Proceedings of the International Conference on Autonomous Agents and Multiagent Systems},
  year={2018}
}

@article{mitten1974preference,
  title={Preference order dynamic programming},
  author={Mitten, L. G.},
  journal={Management Science},
  volume={21},
  number={1},
  pages={43--46},
  year={1974},
  publisher={INFORMS}
}

@article{sobel1975ordinal,
  title={Ordinal dynamic programming},
  author={Sobel, Matthew J.},
  journal={Management science},
  volume={21},
  number={9},
  pages={967--975},
  year={1975},
  publisher={INFORMS}
}

@incollection{heger1994consideration,
  title={Consideration of risk in reinforcement learning},
  author={Heger, Matthias},
  booktitle={Machine Learning Proceedings 1994},
  pages={105--111},
  year={1994},
  publisher={Elsevier}
}

@inproceedings{jothimurugan2020composable,
  title={A composable specification language for reinforcement learning tasks},
  author={Jothimurugan, Kishor and Alur, Rajeev and Bastani, Osbert},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{hammond2021multi,
  title={Multi-Agent Reinforcement Learning with Temporal Logic Specifications},
  author={Hammond, Lewis and Abate, Alessandro and Gutierrez, Julian and Wooldridge, Michael},
  booktitle={Proceedings of the International Conference on Autonomous Agents and Multiagent Systems},
  year={2021}
}

@inproceedings{li2017reinforcement,
  title={Reinforcement learning with temporal logic rewards},
  author={Li, Xiao and Vasile, Cristian-Ioan and Belta, Calin},
  booktitle={Proceedings of the International Conference on Intelligent Robots and Systems},
  year={2017},
}
@online{csabaRLhypothesis,
  author = {Csaba Szepesvári},
  title = {Constrained {MDP}s and the reward hypothesis},
  year ={2020},
  url = {http://readingsml.blogspot.com/2020/03/constrained-mdps-and-reward-hypothesis.html},
}

@inproceedings{sunehag2011axioms,
  title={Axioms for rational reinforcement learning},
  author={Sunehag, Peter and Hutter, Marcus},
  booktitle={Proceedings of the International Conference on Algorithmic Learning Theory},
  year={2011},
}

@inproceedings{weng2011markov,
  title={Markov decision processes with ordinal rewards: Reference point-based preferences},
  author={Weng, Paul},
  booktitle={Proceedings of the International Conference on Automated Planning and Scheduling},
  year={2011}
}
@article{sobel2013discounting,
  title={Discounting axioms imply risk neutrality},
  author={Sobel, Matthew J.},
  journal={Annals of Operations Research},
  volume={208},
  number={1},
  pages={417--432},
  year={2013},
  publisher={Springer}
}
@inproceedings{knox2009interactively,
  title={Interactively shaping agents via human reinforcement: The {TAMER} framework},
  author={Knox, W. Bradley and Stone, Peter},
  booktitle={Proceedings of the International Conference on Knowledge Capture},
  year={2009}
}

@inproceedings{dewey2014reinforcement,
  title={Reinforcement learning and the reward engineering principle},
  author={Dewey, Daniel},
  booktitle={Proceedings of the AAAI Spring Symposium Series},
  year={2014}
}

@inproceedings{mataric1994reward,
  title={Reward functions for accelerated learning},
  author={Mataric, Maja J.},
    booktitle={Proceedings of the International Conference on Machine Learning},
  year={1994},
}
@article{sorg2010reward,
  title={Reward design via online gradient ascent},
  author={Sorg, Jonathan and Lewis, Richard L. and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  year={2010},
}

@article{friston2012dopamine,
  title={Dopamine, affordance and active inference},
  author={Friston, Karl J. and Shiner, Tamara and FitzGerald, Thomas and Galea, Joseph M and Adams, Rick and Brown, Harriet and Dolan, Raymond J. and Moran, Rosalyn and Stephan, Klaas Enno and Bestmann, Sven},
  journal={PLoS Comput Biol},
  volume={8},
  number={1},
  pages={e1002327},
  year={2012},
  publisher={Public Library of Science}
}

@article{silver2021reward,
  title={Reward Is Enough},
  author={Silver, David and Singh, Satinder and Precup, Doina and Sutton, Richard S.},
  journal={Artificial Intelligence},
  pages={103535},
  year={2021},
  publisher={Elsevier}
}

@article{skyum1985complexity,
  title={A complexity theory based on Boolean algebra},
  author={Skyum, Sven and Valiant, Leslie G.},
  journal={Journal of the ACM (JACM)},
  volume={32},
  number={2},
  pages={484--502},
  year={1985},
  publisher={ACM New York, NY, USA}
}
@article{paterson1976circuit,
  title={Circuit size is nonlinear in depth},
  author={Paterson, MS and Valiant, Leslie G.},
  journal={Theoretical Computer Science},
  volume={2},
  number={3},
  pages={397--400},
  year={1976},
  publisher={Elsevier}
}

@book{wegener1987complexity,
  title={The complexity of Boolean functions},
  author={Wegener, Ingo},
  year={1987},
  publisher={John Wiley \& Sons, Inc.}
}

@article{quine1952problem,
  title={The problem of simplifying truth functions},
  author={Quine, Willard V.},
  journal={The American mathematical monthly},
  volume={59},
  number={8},
  pages={521--531},
  year={1952},
  publisher={Taylor \& Francis}
}

@article{mccluskey1956minimization,
  title={Minimization of Boolean functions},
  author={McCluskey, Edward J.},
  journal={The Bell System Technical Journal},
  volume={35},
  number={6},
  pages={1417--1444},
  year={1956},
  publisher={Nokia Bell Labs}
}

@article{mileto1964average,
  title={Average values of quantities appearing in Boolean function minimization},
  author={Mileto, Franco and Putzolu, G},
  journal={IEEE Transactions on Electronic Computers},
  number={2},
  pages={87--92},
  year={1964},
  publisher={IEEE}
}

@article{mileto1965statistical,
  title={Statistical complexity of algorithms for Boolean function minimization},
  author={Mileto, Franco and Putzolu, Gianfranco},
  journal={Journal of the ACM (JACM)},
  volume={12},
  number={3},
  pages={364--375},
  year={1965},
  publisher={ACM New York, NY, USA}
}
@inbook{christian2021alignment,
  title={The Alignment Problem: Machine Learning and Human Values},
  author={Christian, Brian},
  year={2021},
  publisher={Atlantic Books},
  pages={130--131}
}