\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asmuth \& Littman(2011)Asmuth and Littman]{Asmuth2011LearningIP}
Asmuth, J. and Littman, M.
\newblock Learning is planning: {N}ear {B}ayes-optimal reinforcement learning
  via {M}onte-{C}arlo tree search.
\newblock In \emph{UAI}, 2011.

\bibitem[Asmuth et~al.(2009)Asmuth, Li, Littman, Nouri, and
  Wingate]{DBLP:conf/uai/AsmuthLLNW09}
Asmuth, J., Li, L., Littman, M.~L., Nouri, A., and Wingate, D.
\newblock A {B}ayesian sampling approach to exploration in reinforcement
  learning.
\newblock In \emph{{UAI} 2009, Proceedings of the Twenty-Fifth Conference on
  Uncertainty in Artificial Intelligence, 2009}, pp.\  19--26. {AUAI} Press,
  2009.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{DBLP:journals/corr/BlundellCKW15}
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D.
\newblock Weight uncertainty in neural networks.
\newblock \emph{CoRR}, abs/1505.05424, 2015.

\bibitem[Botev et~al.(2013)Botev, Kroese, Rubinstein, and
  L'Ecuyer]{Botev2013Chapter3}
Botev, Z., Kroese, D.~P., Rubinstein, R., and L'Ecuyer, P.
\newblock Chapter 3 – the cross-entropy method for optimization.
\newblock \emph{Handbook of Statistics}, 31:\penalty0 35--59, 2013.

\bibitem[Bou-Ammar et~al.(2014)Bou-Ammar, Eaton, Ruvolo, and
  Taylor]{BouAmmar2014OnlineML}
Bou-Ammar, H., Eaton, E., Ruvolo, P., and Taylor, M.~E.
\newblock Online multi-task learning for policy gradient methods.
\newblock In \emph{ICML}, 2014.

\bibitem[Brafman \& Tennenholtz(2003)Brafman and Tennenholtz]{Ronen03}
Brafman, R.~I. and Tennenholtz, M.
\newblock R-max - a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock 3\penalty0 (null), 2003.
\newblock ISSN 1532-4435.

\bibitem[Brunskill \& Li(2014)Brunskill and Li]{Brunskill2014PACinspiredOD}
Brunskill, E. and Li, L.
\newblock Pac-inspired option discovery in lifelong reinforcement learning.
\newblock In \emph{ICML}, 2014.

\bibitem[Chua et~al.(2018)Chua, Calandra, McAllister, and
  Levine]{DBLP:conf/nips/ChuaCML18}
Chua, K., Calandra, R., McAllister, R., and Levine, S.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018}, pp.\
   4759--4770, 2018.

\bibitem[des Combes et~al.(2018)des Combes, Bachman, and van
  Seijen]{DBLP:conf/iclr/CombesBS18}
des Combes, R.~T., Bachman, P., and van Seijen, H.
\newblock Learning invariances for policy generalization.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018}. OpenReview.net, 2018.

\bibitem[Doshi{-}Velez \& Konidaris(2016)Doshi{-}Velez and
  Konidaris]{DBLP:conf/ijcai/Doshi-VelezK16}
Doshi{-}Velez, F. and Konidaris, G.~D.
\newblock Hidden parameter markov decision processes: {A} semiparametric
  regression approach for discovering latent task parametrizations.
\newblock In \emph{Proceedings of the Twenty-Fifth International Joint
  Conference on Artificial Intelligence, {IJCAI} 2016}, pp.\  1432--1440.
  {IJCAI/AAAI} Press, 2016.

\bibitem[Duan et~al.(2016)Duan, Chen, Houthooft, Schulman, and
  Abbeel]{Duan2016BenchmarkingDR}
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \emph{ICML}, 2016.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{Finn2017ModelAgnosticMF}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Finn et~al.(2018)Finn, Xu, and Levine]{DBLP:conf/nips/FinnXL18}
Finn, C., Xu, K., and Levine, S.
\newblock Probabilistic model-agnostic meta-learning.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018}, pp.\
   9537--9548, 2018.

\bibitem[Fu et~al.(2021)Fu, Tang, Hao, Chen, Feng, Li, and
  Liu]{DBLP:conf/aaai/FuTHCFLL21}
Fu, H., Tang, H., Hao, J., Chen, C., Feng, X., Li, D., and Liu, W.
\newblock Towards effective context for meta-reinforcement learning: an
  approach based on contrastive learning.
\newblock In \emph{Thirty-Fifth {AAAI} Conference on Artificial Intelligence,
  {AAAI} 2021}, pp.\  7457--7465. {AAAI} Press, 2021.

\bibitem[Ghavamzadeh et~al.(2015)Ghavamzadeh, Mannor, Pineau, and
  Tamar]{Ghavamzadeh_2015}
Ghavamzadeh, M., Mannor, S., Pineau, J., and Tamar, A.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends® in Machine Learning}, 8\penalty0
  (5-6):\penalty0 359–483, 2015.
\newblock ISSN 1935-8245.

\bibitem[Grant et~al.(2018)Grant, Finn, Levine, Darrell, and
  Griffiths]{DBLP:conf/iclr/GrantFLDG18}
Grant, E., Finn, C., Levine, S., Darrell, T., and Griffiths, T.~L.
\newblock Recasting gradient-based meta-learning as hierarchical bayes.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018}. OpenReview.net, 2018.

\bibitem[Graves(2011)]{DBLP:conf/nips/Graves11}
Graves, A.
\newblock Practical variational inference for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems 24: 25th
  Annual Conference on Neural Information Processing Systems 2011}, pp.\
  2348--2356, 2011.

\bibitem[Guez et~al.(2012)Guez, Silver, and Dayan]{Guez2012EfficientBR}
Guez, A., Silver, D., and Dayan, P.
\newblock Efficient bayes-adaptive reinforcement learning using sample-based
  search.
\newblock In \emph{NIPS}, 2012.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{DBLP:conf/icml/HaarnojaZAL18}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018}, pp.\  1856--1865. {PMLR}, 2018.

\bibitem[Hafner et~al.(2020)Hafner, Lillicrap, Ba, and
  Norouzi]{Hafner2020DreamTC}
Hafner, D., Lillicrap, T.~P., Ba, J., and Norouzi, M.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock \emph{ArXiv}, abs/1912.01603, 2020.

\bibitem[Hinton \& van Camp(1993{\natexlab{a}})Hinton and van
  Camp]{DBLP:conf/colt/HintonC93}
Hinton, G.~E. and van Camp, D.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{Proceedings of the Sixth Annual {ACM} Conference on
  Computational Learning Theory, {COLT} 1993}, pp.\  5--13. {ACM},
  1993{\natexlab{a}}.

\bibitem[Hinton \& van Camp(1993{\natexlab{b}})Hinton and van
  Camp]{Hinton1993KeepingTN}
Hinton, G.~E. and van Camp, D.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{COLT '93}, 1993{\natexlab{b}}.

\bibitem[Houthooft et~al.(2016)Houthooft, Chen, Duan, Schulman, Turck, and
  Abbeel]{DBLP:conf/nips/HouthooftCCDSTA16}
Houthooft, R., Chen, X., Duan, Y., Schulman, J., Turck, F.~D., and Abbeel, P.
\newblock {VIME:} variational information maximizing exploration.
\newblock In \emph{Advances in Neural Information Processing Systems 29: Annual
  Conference on Neural Information Processing Systems 2016}, pp.\  1109--1117,
  2016.

\bibitem[Isele et~al.(2016{\natexlab{a}})Isele, Rostami, and
  Eaton]{DBLP:conf/ijcai/IseleRE16}
Isele, D., Rostami, M., and Eaton, E.
\newblock Using task features for zero-shot knowledge transfer in lifelong
  learning.
\newblock In \emph{Proceedings of the Twenty-Fifth International Joint
  Conference on Artificial Intelligence, {IJCAI} 2016}, pp.\  1620--1626.
  {IJCAI/AAAI} Press, 2016{\natexlab{a}}.

\bibitem[Isele et~al.(2016{\natexlab{b}})Isele, Rostami, and
  Eaton]{Isele2016UsingTF}
Isele, D., Rostami, M., and Eaton, E.
\newblock Using task features for zero-shot knowledge transfer in lifelong
  learning.
\newblock In \emph{IJCAI}, 2016{\natexlab{b}}.

\bibitem[Kaelbling et~al.(1996)Kaelbling, Littman, and
  Moore]{Kaelbling1996ReinforcementLA}
Kaelbling, L.~P., Littman, M.~L., and Moore, A.~W.
\newblock Reinforcement learning: A survey.
\newblock \emph{J. Artif. Intell. Res.}, 4:\penalty0 237--285, 1996.

\bibitem[Killian et~al.(2017)Killian, Daulton, Doshi{-}Velez, and
  Konidaris]{DBLP:conf/nips/KillianDDK17}
Killian, T.~W., Daulton, S., Doshi{-}Velez, F., and Konidaris, G.~D.
\newblock Robust and efficient transfer learning with hidden parameter markov
  decision processes.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017}, pp.\  6250--6261,
  2017.

\bibitem[Kirkpatrick et~al.(2016)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska{-}Barwinska, Hassabis,
  Clopath, Kumaran, and Hadsell]{DBLP:journals/corr/KirkpatrickPRVD16}
Kirkpatrick, J., Pascanu, R., Rabinowitz, N.~C., Veness, J., Desjardins, G.,
  Rusu, A.~A., Milan, K., Quan, J., Ramalho, T., Grabska{-}Barwinska, A.,
  Hassabis, D., Clopath, C., Kumaran, D., and Hadsell, R.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{CoRR}, abs/1612.00796, 2016.

\bibitem[Li \& Hoiem(2017)Li and Hoiem]{li2017learning}
Li, Z. and Hoiem, D.
\newblock Learning without forgetting, 2017.

\bibitem[Lopez{-}Paz \& Ranzato(2017)Lopez{-}Paz and
  Ranzato]{DBLP:conf/nips/Lopez-PazR17}
Lopez{-}Paz, D. and Ranzato, M.
\newblock Gradient episodic memory for continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017}, pp.\  6467--6476,
  2017.

\bibitem[Mendez et~al.(2020)Mendez, Wang, and Eaton]{Mendez2020LifelongPG}
Mendez, J. A.~M., Wang, B., and Eaton, E.
\newblock Lifelong policy gradient learning of factored policies for faster
  training without forgetting.
\newblock \emph{ArXiv}, abs/2007.07011, 2020.

\bibitem[Nagabandi et~al.(2019)Nagabandi, Finn, and
  Levine]{Nagabandi2019DeepOL}
Nagabandi, A., Finn, C., and Levine, S.
\newblock Deep online learning via meta-learning: Continual adaptation for
  model-based rl.
\newblock \emph{ArXiv}, abs/1812.07671, 2019.

\bibitem[Nguyen et~al.(2018)Nguyen, Li, Bui, and Turner]{nguyen2018variational}
Nguyen, C.~V., Li, Y., Bui, T.~D., and Turner, R.~E.
\newblock Variational continual learning, 2018.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and
  Roy]{Osband2016DeepEV}
Osband, I., Blundell, C., Pritzel, A., and Roy, B.~V.
\newblock Deep exploration via bootstrapped dqn.
\newblock In \emph{NIPS}, 2016.

\bibitem[Parisotto et~al.(2016)Parisotto, Ba, and
  Salakhutdinov]{DBLP:journals/corr/ParisottoBS15}
Parisotto, E., Ba, L.~J., and Salakhutdinov, R.
\newblock Actor-mimic: Deep multitask and transfer reinforcement learning.
\newblock In \emph{4th International Conference on Learning Representations,
  {ICLR} 2016}, 2016.

\bibitem[Puterman(1994)]{Puterman94}
Puterman, M.~L.
\newblock \emph{Markov Decision Processes---Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc., 1994.

\bibitem[Rakelly et~al.(2019)Rakelly, Zhou, Finn, Levine, and
  Quillen]{DBLP:conf/icml/RakellyZFLQ19}
Rakelly, K., Zhou, A., Finn, C., Levine, S., and Quillen, D.
\newblock Efficient off-policy meta-reinforcement learning via probabilistic
  context variables.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019}, volume~97 of \emph{Proceedings of Machine Learning
  Research}, pp.\  5331--5340. {PMLR}, 2019.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, Driessche,
  Graepel, and Hassabis]{Silver2017MasteringTG}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui,
  F., Sifre, L., Driessche, G. V.~D., Graepel, T., and Hassabis, D.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 550:\penalty0 354--359, 2017.

\bibitem[Strehl et~al.(2006)Strehl, Li, and Littman]{Strehl2006IncrementalML}
Strehl, A., Li, L., and Littman, M.
\newblock Incremental model-based learners with formal learning-time
  guarantees.
\newblock \emph{ArXiv}, abs/1206.6870, 2006.

\bibitem[Strehl et~al.(2009)Strehl, Li, and Littman]{Strehl2009ReinforcementLI}
Strehl, A.~L., Li, L., and Littman, M.~L.
\newblock Reinforcement learning in finite mdps: Pac analysis.
\newblock \emph{J. Mach. Learn. Res.}, 10:\penalty0 2413--2444, 2009.

\bibitem[Strens(2000)]{strens00}
Strens, M. J.~A.
\newblock A {B}ayesian framework for reinforcement learning.
\newblock In \emph{Proceedings of the Seventeenth International Conference on
  Machine Learning (ICML 2000)}, pp.\  943--950, 2000.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton98}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: {A}n Introduction}.
\newblock The MIT Press, 1998.

\bibitem[Teh et~al.(2017)Teh, Bapst, Czarnecki, Quan, Kirkpatrick, Hadsell,
  Heess, and Pascanu]{DBLP:conf/nips/TehBCQKHHP17}
Teh, Y.~W., Bapst, V., Czarnecki, W.~M., Quan, J., Kirkpatrick, J., Hadsell,
  R., Heess, N., and Pascanu, R.
\newblock Distral: Robust multitask reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017}, pp.\  4496--4506,
  2017.

\bibitem[Thompson(1933)]{Thompson1933ONTL}
Thompson, W.~R.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, 25:\penalty0 285--294, 1933.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and
  Tassa]{DBLP:conf/iros/TodorovET12}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: {A} physics engine for model-based control.
\newblock In \emph{2012 {IEEE/RSJ} International Conference on Intelligent
  Robots and Systems, {IROS} 2012}, pp.\  5026--5033. {IEEE}, 2012.

\bibitem[van Hasselt et~al.(2016)van Hasselt, Guez, and
  Silver]{DBLP:conf/aaai/HasseltGS16}
van Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{Proceedings of the Thirtieth {AAAI} Conference on Artificial
  Intelligence,2016}, pp.\  2094--2100. {AAAI} Press, 2016.

\bibitem[Wang et~al.(2016)Wang, Kurth{-}Nelson, Tirumala, Soyer, Leibo, Munos,
  Blundell, Kumaran, and Botvinick]{DBLP:journals/corr/WangKTSLMBKB16}
Wang, J.~X., Kurth{-}Nelson, Z., Tirumala, D., Soyer, H., Leibo, J.~Z., Munos,
  R., Blundell, C., Kumaran, D., and Botvinick, M.
\newblock Learning to reinforcement learn.
\newblock \emph{CoRR}, abs/1611.05763, 2016.

\bibitem[Wang et~al.(2019)Wang, Bao, Clavera, Hoang, Wen, Langlois, Zhang,
  Zhang, Abbeel, and Ba]{DBLP:journals/corr/abs-1907-02057}
Wang, T., Bao, X., Clavera, I., Hoang, J., Wen, Y., Langlois, E., Zhang, S.,
  Zhang, G., Abbeel, P., and Ba, J.
\newblock Benchmarking model-based reinforcement learning.
\newblock \emph{CoRR}, abs/1907.02057, 2019.

\bibitem[Wilson et~al.(2007{\natexlab{a}})Wilson, Fern, Ray, and
  Tadepalli]{DBLP:conf/icml/WilsonFRT07}
Wilson, A., Fern, A., Ray, S., and Tadepalli, P.
\newblock Multi-task reinforcement learning: {A} hierarchical {B}ayesian
  approach.
\newblock In \emph{Machine Learning, Proceedings of the Twenty-Fourth
  International Conference {(ICML} 2007), 2007}, volume 227 of \emph{{ACM}
  International Conference Proceeding Series}, pp.\  1015--1022. {ACM},
  2007{\natexlab{a}}.

\bibitem[Wilson et~al.(2007{\natexlab{b}})Wilson, Fern, Ray, and
  Tadepalli]{Wilson2007MultitaskRL}
Wilson, A., Fern, A., Ray, S., and Tadepalli, P.
\newblock Multi-task reinforcement learning: a hierarchical bayesian approach.
\newblock In \emph{ICML '07}, 2007{\natexlab{b}}.

\bibitem[Yoon et~al.(2018)Yoon, Kim, Dia, Kim, Bengio, and
  Ahn]{DBLP:conf/nips/YoonKDKBA18}
Yoon, J., Kim, T., Dia, O., Kim, S., Bengio, Y., and Ahn, S.
\newblock Bayesian model-agnostic meta-learning.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018}, pp.\
   7343--7353, 2018.

\bibitem[Yu et~al.(2019)Yu, Quillen, He, Julian, Hausman, Finn, and
  Levine]{Yu2019MetaWorldAB}
Yu, T., Quillen, D., He, Z., Julian, R.~C., Hausman, K., Finn, C., and Levine,
  S.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock \emph{ArXiv}, abs/1910.10897, 2019.

\bibitem[Zenke et~al.(2017)Zenke, Poole, and Ganguli]{zenke2017continual}
Zenke, F., Poole, B., and Ganguli, S.
\newblock Continual learning through synaptic intelligence, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Wang, Hu, Chen, Chen, Fan, and
  Zhang]{Zhang2021MetaCUREMR}
Zhang, J., Wang, J., Hu, H., Chen, T., Chen, Y., Fan, C., and Zhang, C.
\newblock Metacure: Meta reinforcement learning with empowerment-driven
  exploration.
\newblock In \emph{ICML}, 2021.

\bibitem[Zhang(2006)]{Zhang2006FromT}
Zhang, T.
\newblock From $\epsilon$-entropy to kl-entropy: Analysis of minimum
  information complexity density estimation.
\newblock \emph{Annals of Statistics}, 34:\penalty0 2180--2210, 2006.

\bibitem[Zintgraf et~al.(2020)Zintgraf, Shiarlis, Igl, Schulze, Gal, Hofmann,
  and Whiteson]{DBLP:conf/iclr/ZintgrafSISGHW20}
Zintgraf, L.~M., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K., and
  Whiteson, S.
\newblock Varibad: {A} very good method for bayes-adaptive deep {RL} via
  meta-learning.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020}. OpenReview.net, 2020.

\end{thebibliography}
