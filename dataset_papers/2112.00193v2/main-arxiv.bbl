\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar,
  and Zhang]{DP-DL}
Abadi, M., Chu, A., Goodfellow, I.~J., McMahan, H.~B., Mironov, I., Talwar, K.,
  and Zhang, L.
\newblock Deep learning with differential privacy.
\newblock In \emph{Proc. of the 2016 {ACM} {SIGSAC} Conf. on Computer and
  Communications Security ({CCS}'16)}, pp.\  308--318, 2016.

\bibitem[Agarwal et~al.(2019)Agarwal, Bullins, Chen, Hazan, Singh, Zhang, and
  Zhang]{agarwal2019efficient}
Agarwal, N., Bullins, B., Chen, X., Hazan, E., Singh, K., Zhang, C., and Zhang,
  Y.
\newblock Efficient full-matrix adaptive regularization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  102--110. PMLR, 2019.

\bibitem[Alon et~al.(2019)Alon, Bassily, and Moran]{alon2019limits}
Alon, N., Bassily, R., and Moran, S.
\newblock Limits of private learning with access to public data.
\newblock \emph{arXiv preprint arXiv:1910.11519}, 2019.

\bibitem[Asi et~al.(2021)Asi, Duchi, Fallah, Javidbakht, and
  Talwar]{asi2021private}
Asi, H., Duchi, J., Fallah, A., Javidbakht, O., and Talwar, K.
\newblock Private adaptive gradient methods for convex optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  383--392. PMLR, 2021.

\bibitem[Avent et~al.(2017)Avent, Korolova, Zeber, Hovden, and
  Livshits]{avent2017blender}
Avent, B., Korolova, A., Zeber, D., Hovden, T., and Livshits, B.
\newblock $\{$BLENDER$\}$: Enabling local search with a hybrid differential
  privacy model.
\newblock In \emph{26th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$
  Security 17)}, 2017.

\bibitem[Bassily et~al.(2014)Bassily, Smith, and Thakurta]{BST14}
Bassily, R., Smith, A., and Thakurta, A.
\newblock Private empirical risk minimization: Efficient algorithms and tight
  error bounds.
\newblock In \emph{Proc. of the 2014 IEEE 55th Annual Symp. on Foundations of
  Computer Science (FOCS)}, pp.\  464--473, 2014.

\bibitem[Bassily et~al.(2018{\natexlab{a}})Bassily, Thakkar, and
  Thakurta]{BTT19}
Bassily, R., Thakkar, O., and Thakurta, A.
\newblock Model-agnostic private learning.
\newblock In \emph{NeurIPS}, 2018{\natexlab{a}}.

\bibitem[Bassily et~al.(2018{\natexlab{b}})Bassily, Thakurta, and
  Thakkar]{bassily2018model}
Bassily, R., Thakurta, A.~G., and Thakkar, O.~D.
\newblock Model-agnostic private learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  2018{\natexlab{b}}.

\bibitem[Bassily et~al.(2019)Bassily, Feldman, Talwar, and
  Thakurta]{bassily2019private}
Bassily, R., Feldman, V., Talwar, K., and Thakurta, A.
\newblock Private stochastic convex optimization with optimal rates.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11279--11288, 2019.

\bibitem[Bassily et~al.(2020{\natexlab{a}})Bassily, Cheu, Moran, Nikolov,
  Ullman, and Wu]{BassilyCMNUW20}
Bassily, R., Cheu, A., Moran, S., Nikolov, A., Ullman, J., and Wu, S.
\newblock Private query release assisted by public data.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  695--703. PMLR, 13--18
  Jul 2020{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v119/bassily20a.html}.

\bibitem[Bassily et~al.(2020{\natexlab{b}})Bassily, Feldman, Guzm\'{a}n, and
  Talwar]{NEURIPS2020_2e2c4bf7}
Bassily, R., Feldman, V., Guzm\'{a}n, C., and Talwar, K.
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  4381--4391. Curran Associates, Inc., 2020{\natexlab{b}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/2e2c4bf7ceaa4712a72dd5ee136dc9a8-Paper.pdf}.

\bibitem[Bassily et~al.(2020{\natexlab{c}})Bassily, Moran, and
  Nandi]{bassily2020learning}
Bassily, R., Moran, S., and Nandi, A.
\newblock Learning from mixtures of private and public populations.
\newblock \emph{arXiv preprint arXiv:2008.00331}, 2020{\natexlab{c}}.

\bibitem[Biswas et~al.(2020)Biswas, Dong, Kamath, and
  Ullman]{biswas2020coinpress}
Biswas, S., Dong, Y., Kamath, G., and Ullman, J.
\newblock Coinpress: Practical private mean and covariance estimation.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  14475--14485. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/a684eceee76fc522773286a895bc8436-Paper.pdf}.

\bibitem[Chaudhuri et~al.(2011)Chaudhuri, Monteleoni, and
  Sarwate]{chaudhuri2011differentially}
Chaudhuri, K., Monteleoni, C., and Sarwate, A.~D.
\newblock Differentially private empirical risk minimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Mar):\penalty0 1069--1109, 2011.

\bibitem[Cohen et~al.(2017)Cohen, Afshar, Tapson, and
  Schaik]{cohen_afshar_tapson_schaik_2017}
Cohen, G., Afshar, S., Tapson, J., and Schaik, A.~V.
\newblock Emnist: Extending mnist to handwritten letters.
\newblock \emph{2017 International Joint Conference on Neural Networks
  (IJCNN)}, 2017.
\newblock \doi{10.1109/ijcnn.2017.7966217}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0 (7), 2011.

\bibitem[Dwork \& Feldman(2018)Dwork and Feldman]{dwork2018privacy}
Dwork, C. and Feldman, V.
\newblock Privacy-preserving prediction.
\newblock In \emph{Conference On Learning Theory}, pp.\  1693--1702. PMLR,
  2018.

\bibitem[Dwork \& Roth(2014)Dwork and Roth]{dwork2014algorithmic}
Dwork, C. and Roth, A.
\newblock The algorithmic foundations of differential privacy.
\newblock \emph{Foundations and Trends in Theoretical Computer Science},
  9\penalty0 (3--4):\penalty0 211--407, 2014.

\bibitem[Dwork et~al.(2006)Dwork, McSherry, Nissim, and Smith]{DMNS}
Dwork, C., McSherry, F., Nissim, K., and Smith, A.
\newblock Calibrating noise to sensitivity in private data analysis.
\newblock In \emph{Proc. of the Third Conf. on Theory of Cryptography (TCC)},
  pp.\  265--284, 2006.
\newblock URL \url{http://dx.doi.org/10.1007/11681878\_14}.

\bibitem[Feldman et~al.(2018)Feldman, Mironov, Talwar, and
  Thakurta]{feldman2018privacy}
Feldman, V., Mironov, I., Talwar, K., and Thakurta, A.
\newblock Privacy amplification by iteration.
\newblock In \emph{2018 IEEE 59th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pp.\  521--532. IEEE, 2018.

\bibitem[Gur{-}Ari et~al.(2018)Gur{-}Ari, Roberts, and Dyer]{Guygur}
Gur{-}Ari, G., Roberts, D.~A., and Dyer, E.
\newblock Gradient descent happens in a tiny subspace.
\newblock \emph{CoRR}, abs/1812.04754, 2018.
\newblock URL \url{http://arxiv.org/abs/1812.04754}.

\bibitem[Hayes(2003)]{hayes03vectorazuma}
Hayes, T.~P.
\newblock A large-deviation inequality for vector-valued martingales.
\newblock 2003.

\bibitem[Hazan(2019)]{hazan2019introduction}
Hazan, E.
\newblock Introduction to online convex optimization.
\newblock \emph{arXiv preprint arXiv:1909.05207}, 2019.

\bibitem[Kairouz et~al.(2020)Kairouz, Ribero, Rush, and Thakurta]{KRRT20}
Kairouz, P., Ribero, M., Rush, K., and Thakurta, A.
\newblock Dimension independence in unconstrained private {ERM} via adaptive
  preconditioning.
\newblock \emph{CoRR}, abs/2008.06570, 2020.
\newblock URL \url{https://arxiv.org/abs/2008.06570}.

\bibitem[Kairouz et~al.(2021{\natexlab{a}})Kairouz, Diaz, Rush, and
  Thakurta]{KRRT21}
Kairouz, P., Diaz, M.~R., Rush, K., and Thakurta, A.
\newblock (nearly) dimension independent private erm with adagrad rates\\{via}
  publicly estimated subspaces.
\newblock In \emph{COLT}, 2021{\natexlab{a}}.

\bibitem[Kairouz et~al.(2021{\natexlab{b}})Kairouz, McMahan, Song, Thakkar,
  Thakurta, and Xu]{kairouz2021practical}
Kairouz, P., McMahan, B., Song, S., Thakkar, O., Thakurta, A., and Xu, Z.
\newblock Practical and private (deep) learning without sampling or shuffling.
\newblock In \emph{ICML}, 2021{\natexlab{b}}.

\bibitem[Krizhevsky(2009)]{cifar10}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem[Li et~al.(2022)Li, Zaheer, Reddi, and Smith]{li2022private}
Li, T., Zaheer, M., Reddi, S.~J., and Smith, V.
\newblock Private adaptive optimization with side information.
\newblock \emph{arXiv preprint arXiv:2202.05963}, 2022.

\bibitem[Li et~al.(2021)Li, Tram{\`e}r, Liang, and Hashimoto]{li2021large}
Li, X., Tram{\`e}r, F., Liang, P., and Hashimoto, T.
\newblock Large language models can be strong differentially private learners.
\newblock \emph{arXiv preprint arXiv:2110.05679}, 2021.

\bibitem[Liu et~al.(2021)Liu, Vietri, Steinke, Ullman, and
  Wu]{liu2021leveraging}
Liu, T., Vietri, G., Steinke, T., Ullman, J., and Wu, Z.~S.
\newblock Leveraging public data for practical private query release.
\newblock \emph{arXiv preprint arXiv:2102.08598}, 2021.

\bibitem[McMahan et~al.(2017{\natexlab{a}})McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{FL1}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics, {AISTATS} 2017, 20-22 April 2017,
  Fort Lauderdale, FL, {USA}}, pp.\  1273--1282, 2017{\natexlab{a}}.
\newblock URL \url{http://proceedings.mlr.press/v54/mcmahan17a.html}.

\bibitem[McMahan et~al.(2017{\natexlab{b}})McMahan, Ramage, Talwar, and
  Zhang]{dplangmodels}
McMahan, H.~B., Ramage, D., Talwar, K., and Zhang, L.
\newblock Learning differentially private language models without losing
  accuracy.
\newblock \emph{CoRR}, abs/1710.06963, 2017{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1710.06963}.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and Socher]{WikiText}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=Byj72udxe}.

\bibitem[Merriman(2014)]{merriman14}
Merriman, C.
\newblock Microsoft reminds privacy-concerned windows 10 beta testers that
  they’re volunteers.
\newblock \emph{The Inquirer. \url{http://www.theinquirer.net/2374302}}, 2014.
\newblock URL \url{http:// www.theinquirer.net/ 2374302}.

\bibitem[Nandi \& Bassily(2020)Nandi and Bassily]{NandiB20}
Nandi, A. and Bassily, R.
\newblock Privately answering classification queries in the agnostic pac model.
\newblock In Kontorovich, A. and Neu, G. (eds.), \emph{Proceedings of the 31st
  International Conference on Algorithmic Learning Theory}, volume 117 of
  \emph{Proceedings of Machine Learning Research}, pp.\  687--703. PMLR, 08
  Feb--11 Feb 2020.
\newblock URL \url{https://proceedings.mlr.press/v117/nandi20a.html}.

\bibitem[Nemirovsky \& Yudin(1983)Nemirovsky and Yudin]{nemirovsky83}
Nemirovsky, A. and Yudin, D.
\newblock \emph{Problem Complexity and Method Efficiency in Optimization}.
\newblock Wiley \& Sons, New York, 1983.

\bibitem[Overflow(2018)]{so_data}
Overflow, S.
\newblock {The Stack Overflow Data}, 2018.
\newblock \url{https://www.kaggle.com/stackoverflow/stackoverflow}.

\bibitem[Papernot et~al.(2016)Papernot, Abadi, Erlingsson, Goodfellow, and
  Talwar]{papernot2016semi}
Papernot, N., Abadi, M., Erlingsson, U., Goodfellow, I., and Talwar, K.
\newblock Semi-supervised knowledge transfer for deep learning from private
  training data.
\newblock \emph{arXiv preprint arXiv:1610.05755}, 2016.

\bibitem[Papernot et~al.(2018)Papernot, Song, Mironov, Raghunathan, Talwar, and
  Erlingsson]{papernot2018scalable}
Papernot, N., Song, S., Mironov, I., Raghunathan, A., Talwar, K., and
  Erlingsson, {\'U}.
\newblock Scalable private learning with pate.
\newblock \emph{arXiv preprint arXiv:1802.08908}, 2018.

\bibitem[Papernot et~al.(2020)Papernot, Thakurta, Song, Chien, and
  Erlingsson]{papernot2020tempered}
Papernot, N., Thakurta, A., Song, S., Chien, S., and Erlingsson, {\'U}.
\newblock Tempered sigmoid activations for deep learning with differential
  privacy.
\newblock \emph{arXiv preprint arXiv:2007.14191}, 2020.

\bibitem[Song et~al.(2013)Song, Chaudhuri, and Sarwate]{song2013stochastic}
Song, S., Chaudhuri, K., and Sarwate, A.~D.
\newblock Stochastic gradient descent with differentially private updates.
\newblock In \emph{2013 IEEE Global Conference on Signal and Information
  Processing}, pp.\  245--248. IEEE, 2013.

\bibitem[Talwar et~al.(2014)Talwar, Thakurta, and Zhang]{talwar2014private}
Talwar, K., Thakurta, A., and Zhang, L.
\newblock Private empirical risk minimization beyond the worst case: The effect
  of the constraint set geometry.
\newblock \emph{arXiv preprint arXiv:1411.5417}, 2014.

\bibitem[Tramer \& Boneh(2020)Tramer and Boneh]{tramer2020differentially}
Tramer, F. and Boneh, D.
\newblock Differentially private learning needs better features (or much more
  data).
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Wang et~al.(2017)Wang, Ye, and Xu]{wang17amd}
Wang, D., Ye, M., and Xu, J.
\newblock Differentially private empirical risk minimization revisited: Faster
  and more general.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/f337d999d9ad116a7b4f3d409fcc6480-Paper.pdf}.

\bibitem[Wang \& Zhou(2020)Wang and Zhou]{Wang_Zhou_2020}
Wang, J. and Zhou, Z.-H.
\newblock Differentially private learning with small public data.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  34\penalty0 (04), 2020.
\newblock \doi{10.1609/aaai.v34i04.6088}.

\bibitem[Yu et~al.(2021)Yu, Zhang, Chen, and Liu]{YZCL21}
Yu, D., Zhang, H., Chen, W., and Liu, T.
\newblock Do not let privacy overbill utility: Gradient embedding perturbation
  for private learning.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=7aogOj\_VYO0}.

\bibitem[Zhou et~al.(2020)Zhou, Wu, and Banerjee]{ZWB20}
Zhou, Y., Wu, Z.~S., and Banerjee, A.
\newblock Bypassing the ambient dimension: Private sgd with gradient subspace
  identification.
\newblock In \emph{ICLR}, 2020.

\end{thebibliography}
