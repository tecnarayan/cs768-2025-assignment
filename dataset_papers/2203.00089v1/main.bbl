\begin{thebibliography}{81}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amari(1998)]{amari1998natural}
Amari, S.-I.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural Computation}, 10\penalty0 (2):\penalty0 251--276, 1998.

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,
  Schaul, Shillingford, and De~Freitas]{andrychowicz2016learning}
Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M.~W., Pfau, D., Schaul, T.,
  Shillingford, B., and De~Freitas, N.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3981--3989, 2016.

\bibitem[Arora et~al.(2018)Arora, Li, and Lyu]{arora2018theoretical}
Arora, S., Li, Z., and Lyu, K.
\newblock Theoretical analysis of auto rate-tuning by batch normalization.
\newblock \emph{arXiv preprint arXiv:1812.03981}, 2018.

\bibitem[Asi \& Duchi(2019)Asi and Duchi]{asi2019stochastic}
Asi, H. and Duchi, J.~C.
\newblock {Stochastic (approximate) proximal point methods: Convergence,
  optimality, and adaptivity}.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (3):\penalty0
  2257--2290, 2019.

\bibitem[Baydin et~al.(2017)Baydin, Cornish, Rubio, Schmidt, and
  Wood]{baydin2017online}
Baydin, A.~G., Cornish, R., Rubio, D.~M., Schmidt, M., and Wood, F.
\newblock Online learning rate adaptation with hypergradient descent.
\newblock \emph{arXiv preprint arXiv:1703.04782}, 2017.

\bibitem[Beck \& Teboulle(2003)Beck and Teboulle]{beck2003mirror}
Beck, A. and Teboulle, M.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock \emph{Operations Research Letters}, 31\penalty0 (3):\penalty0
  167--175, 2003.

\bibitem[Becker \& Le~Cun(1988)Becker and Le~Cun]{becker1988improving}
Becker, S. and Le~Cun, Y.
\newblock Improving the convergence of back-propagation learning with second
  order methods.
\newblock In \emph{Proceedings of the 1988 Connecticut Summer School}, 1988.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and Mandal]{belkin2019reconciling}
Belkin, M., Hsu, D., Ma, S., and Mandal, S.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019.

\bibitem[Bengio(2012)]{bengio2012practical}
Bengio, Y.
\newblock Practical recommendations for gradient-based training of deep
  architectures.
\newblock In \emph{{Neural Networks: Tricks of the Trade}}, pp.\  437--478.
  Springer, 2012.

\bibitem[Benjamin et~al.(2018)Benjamin, Rolnick, and
  Kording]{benjamin2018measuring}
Benjamin, A.~S., Rolnick, D., and Kording, K.
\newblock Measuring and regularizing networks in function space.
\newblock \emph{arXiv preprint arXiv:1805.08289}, 2018.

\bibitem[Benzing et~al.(2019)Benzing, Gauy, Mujika, Martinsson, and
  Steger]{benzing2019optimal}
Benzing, F., Gauy, M.~M., Mujika, A., Martinsson, A., and Steger, A.
\newblock {Optimal Kronecker-sum approximation of real time recurrent
  learning}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  604--613, 2019.

\bibitem[Bergstra \& Bengio(2012)Bergstra and Bengio]{bergstra2012random}
Bergstra, J. and Bengio, Y.
\newblock Random search for hyper-parameter optimization.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Feb):\penalty0 281--305, 2012.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q.
\newblock {JAX: Composable transformations of Python+NumPy programs}, 2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Cettolo et~al.(2014)Cettolo, Niehues, St{\"u}ker, Bentivogli, and
  Federico]{cettolo2014report}
Cettolo, M., Niehues, J., St{\"u}ker, S., Bentivogli, L., and Federico, M.
\newblock {Report on the 11th IWSLT evaluation campaign}.
\newblock In \emph{Proceedings of the International Workshop on Spoken Language
  Translation, Hanoi, Vietnam}, volume~57, 2014.

\bibitem[Domke(2012)]{domke2012generic}
Domke, J.
\newblock Generic methods for optimization-based modeling.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  318--326,
  2012.

\bibitem[Donini et~al.(2019)Donini, Franceschi, Pontil, Majumder, and
  Frasconi]{donini2019marthe}
Donini, M., Franceschi, L., Pontil, M., Majumder, O., and Frasconi, P.
\newblock {MARTHE: Scheduling the learning rate via online hypergradients}.
\newblock \emph{arXiv preprint arXiv:1910.08525}, 2019.

\bibitem[Dua \& Graff(2017)Dua and Graff]{Dua:2019}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0 (7), 2011.

\bibitem[Flennerhag et~al.(2019)Flennerhag, Rusu, Pascanu, Visin, Yin, and
  Hadsell]{flennerhag2019meta}
Flennerhag, S., Rusu, A.~A., Pascanu, R., Visin, F., Yin, H., and Hadsell, R.
\newblock Meta-learning with warped gradient descent.
\newblock \emph{arXiv preprint arXiv:1909.00025}, 2019.

\bibitem[Franceschi et~al.(2017)Franceschi, Donini, Frasconi, and
  Pontil]{franceschi2017forward}
Franceschi, L., Donini, M., Frasconi, P., and Pontil, M.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1165--1173, 2017.

\bibitem[George et~al.(2018)George, Laurent, Bouthillier, Ballas, and
  Vincent]{george2018fast}
George, T., Laurent, C., Bouthillier, X., Ballas, N., and Vincent, P.
\newblock {Fast approximate natural gradient descent in a Kronecker-factored
  eigenbasis}.
\newblock \emph{arXiv preprint arXiv:1806.03884}, 2018.

\bibitem[Grosse(2021)]{GrosseNNTDChapter4}
Grosse, R.
\newblock {University of Toronto CSC2541, Topics in Machine Learning: Neural
  Net Training Dynamics, Chapter 4: Second-Order Optimization}.
\newblock {Lecture Notes}, 2021.
\newblock URL
  \url{https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/readings/L04_second_order.pdf}.

\bibitem[Grosse \& Martens(2016)Grosse and Martens]{grosse2016kronecker}
Grosse, R. and Martens, J.
\newblock {A Kronecker-factored approximate Fisher matrix for convolution
  layers}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  573--582, 2016.

\bibitem[Gupta et~al.(2018)Gupta, Koren, and Singer]{gupta2018shampoo}
Gupta, V., Koren, T., and Singer, Y.
\newblock {Shampoo: Preconditioned stochastic tensor optimization}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1842--1850, 2018.

\bibitem[Hazan \& Kakade(2019)Hazan and Kakade]{hazan2019revisiting}
Hazan, E. and Kakade, S.
\newblock Revisiting the polyak step size.
\newblock \emph{arXiv preprint arXiv:1905.00313}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition}, pp.\
   770--778, 2016.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hinton \& Salakhutdinov(2006)Hinton and
  Salakhutdinov]{hinton2006reducing}
Hinton, G.~E. and Salakhutdinov, R.~R.
\newblock Reducing the dimensionality of data with neural networks.
\newblock \emph{Science}, 313\penalty0 (5786):\penalty0 504--507, 2006.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: {A}ccelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167}, 2015.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock {Neural tangent kernel: Convergence and generalization in neural
  networks}.
\newblock \emph{arXiv preprint arXiv:1806.07572}, 2018.

\bibitem[Jaderberg et~al.(2017)Jaderberg, Dalibard, Osindero, Czarnecki,
  Donahue, Razavi, Vinyals, Green, Dunning, Simonyan,
  et~al.]{jaderberg2017population}
Jaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W.~M., Donahue, J.,
  Razavi, A., Vinyals, O., Green, T., Dunning, I., Simonyan, K., et~al.
\newblock Population-based training of neural networks.
\newblock \emph{arXiv preprint arXiv:1711.09846}, 2017.

\bibitem[Jiang et~al.(2019)Jiang, He, Chen, Liu, Gao, and Zhao]{jiang2019smart}
Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Zhao, T.
\newblock {Smart: Robust and efficient fine-tuning for pre-trained natural
  language models through principled regularized optimization}.
\newblock \emph{arXiv preprint arXiv:1911.03437}, 2019.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock {ImageNet} classification with deep convolutional neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1097--1105, 2012.

\bibitem[Laurent et~al.(2018)Laurent, George, Bouthillier, Ballas, and
  Vincent]{laurent2018an}
Laurent, C., George, T., Bouthillier, X., Ballas, N., and Vincent, P.
\newblock {An evaluation of Fisher approximations beyond Kronecker
  factorization}, 2018.
\newblock URL \url{https://openreview.net/forum?id=ryVC6tkwG}.

\bibitem[Le~Roux et~al.(2007)Le~Roux, Manzagol, and Bengio]{le2007topmoumoute}
Le~Roux, N., Manzagol, P.-A., and Bengio, Y.
\newblock Topmoumoute online natural gradient algorithm.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  849--856, 2007.

\bibitem[LeCun(1988)]{lecun1998mnist}
LeCun, Y.
\newblock {The MNIST database of handwritten digits}.
\newblock \emph{http://yann. lecun. com/exdb/mnist/}, 1988.

\bibitem[Lee \& Choi(2018)Lee and Choi]{lee2018gradient}
Lee, Y. and Choi, S.
\newblock Gradient-based meta-learning with learned layerwise metric and
  subspace.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2927--2936, 2018.

\bibitem[Li \& Malik(2016)Li and Malik]{li2016learning}
Li, K. and Malik, J.
\newblock Learning to optimize.
\newblock \emph{arXiv preprint arXiv:1606.01885}, 2016.

\bibitem[Li \& Malik(2017)Li and Malik]{li2017learning}
Li, K. and Malik, J.
\newblock Learning to optimize neural nets.
\newblock \emph{arXiv preprint arXiv:1703.00441}, 2017.

\bibitem[Li et~al.(2016)Li, Jamieson, DeSalvo, Rostamizadeh, and
  Talwalkar]{li2016hyperband}
Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A.
\newblock {Hyperband: Bandit-based configuration evaluation for hyperparameter
  optimization}.
\newblock \emph{arXiv preprint arXiv:1603.06560}, 2016.

\bibitem[Loizou et~al.(2021)Loizou, Vaswani, Laradji, and
  Lacoste-Julien]{loizou2021stochastic}
Loizou, N., Vaswani, S., Laradji, I.~H., and Lacoste-Julien, S.
\newblock Stochastic polyak step-size for sgd: An adaptive learning rate for
  fast convergence.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1306--1314. PMLR, 2021.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and
  Hutter]{loshchilov2017decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Lucas et~al.(2019)Lucas, Sun, Zemel, and Grosse]{lucas2018aggregated}
Lucas, J., Sun, S., Zemel, R., and Grosse, R.
\newblock {Aggregated momentum: Stability through passive damping}.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{maclaurin2015gradient}
Maclaurin, D., Duvenaud, D., and Adams, R.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2113--2122, 2015.

\bibitem[Martens(2010)]{martens2010deep}
Martens, J.
\newblock {Deep learning via Hessian-free optimization}.
\newblock In \emph{International Conference on Machine Learning}, 2010.

\bibitem[Martens(2014)]{martens2014new}
Martens, J.
\newblock New insights and perspectives on the natural gradient method.
\newblock \emph{arXiv preprint arXiv:1412.1193}, 2014.

\bibitem[Martens(2016)]{martens2016second}
Martens, J.
\newblock \emph{Second-order optimization for neural networks. PhD Thesis.}
\newblock University of Toronto, 2016.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
Martens, J. and Grosse, R.
\newblock {Optimizing neural networks with Kronecker-factored approximate
  curvature}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2408--2417, 2015.

\bibitem[Martens \& Sutskever(2011)Martens and Sutskever]{martens2011learning}
Martens, J. and Sutskever, I.
\newblock {Learning recurrent neural networks with Hessian-free optimization}.
\newblock In \emph{International Conference on Machine Learning}, 2011.

\bibitem[Martens et~al.(2018)Martens, Ba, and Johnson]{martens2018kronecker}
Martens, J., Ba, J., and Johnson, M.
\newblock Kronecker-factored curvature approximations for recurrent neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Metz et~al.(2018)Metz, Maheswaranathan, Nixon, Freeman, and
  Sohl-Dickstein]{metz2018learned}
Metz, L., Maheswaranathan, N., Nixon, J., Freeman, C.~D., and Sohl-Dickstein,
  J.
\newblock {Learned optimizers that outperform SGD on wall-clock and validation
  loss}.
\newblock \emph{arXiv preprint arXiv:1810.10180}, 2018.

\bibitem[Mishkin et~al.(2018)Mishkin, Kunstner, Nielsen, Schmidt, and
  Khan]{mishkin2018slang}
Mishkin, A., Kunstner, F., Nielsen, D., Schmidt, M., and Khan, M.~E.
\newblock {Slang: Fast structured covariance approximations for Bayesian deep
  learning with natural gradient}.
\newblock \emph{arXiv preprint arXiv:1811.04504}, 2018.

\bibitem[Moskovitz et~al.(2019)Moskovitz, Wang, Lan, Kapoor, Miconi, Yosinski,
  and Rawal]{moskovitz2019first}
Moskovitz, T., Wang, R., Lan, J., Kapoor, S., Miconi, T., Yosinski, J., and
  Rawal, A.
\newblock First-order preconditioning via hypergradient descent.
\newblock \emph{arXiv preprint arXiv:1910.08461}, 2019.

\bibitem[Mujika et~al.(2018)Mujika, Meier, and Steger]{mujika2018approximating}
Mujika, A., Meier, F., and Steger, A.
\newblock {Approximating real-time recurrent learning with random Kronecker
  factors}.
\newblock \emph{arXiv preprint arXiv:1805.10842}, 2018.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NIPS Workshop on Deep Learning and Unsupervised Feature
  Learning}, 2011.

\bibitem[Nocedal \& Wright(1999)Nocedal and Wright]{wright1999numerical}
Nocedal, J. and Wright, S.
\newblock \emph{Numerical Optimization}.
\newblock Springer, 1999.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott2019fairseq}
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and
  Auli, M.
\newblock {fairseq: A fast, extensible toolkit for sequence modeling}.
\newblock In \emph{Proceedings of NAACL-HLT 2019: Demonstrations}, 2019.

\bibitem[Park \& Oliva(2019)Park and Oliva]{park2019meta}
Park, E. and Oliva, J.~B.
\newblock Meta-curvature.
\newblock \emph{arXiv preprint arXiv:1902.03356}, 2019.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_9015}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  8024--8035. 2019.

\bibitem[Rahimi \& Recht(2017)Rahimi and Recht]{rahimi_recht_2017}
Rahimi, A. and Recht, B.
\newblock Reflections on random kitchen sinks, 2017.
\newblock URL \url{http://www.argmin.net/2017/12/05/kitchen-sinks/}.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951stochastic}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, pp.\  400--407, 1951.

\bibitem[Rolinek \& Martius(2018)Rolinek and Martius]{rolinek2018l4}
Rolinek, M. and Martius, G.
\newblock {L4: Practical loss-based stepsize adaptation for deep learning}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6433--6443, 2018.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}
Snoek, J., Larochelle, H., and Adams, R.~P.
\newblock Practical {B}ayesian optimization of machine learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2951--2959, 2012.

\bibitem[Snoek et~al.(2015)Snoek, Rippel, Swersky, Kiros, Satish, Sundaram,
  Patwary, Prabhat, and Adams]{snoek2015scalable}
Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N.,
  Patwary, M., Prabhat, P., and Adams, R.
\newblock Scalable {B}ayesian optimization using deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2171--2180, 2015.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1057--1063, 2000.

\bibitem[Swersky et~al.(2014)Swersky, Snoek, and Adams]{swersky2014freeze}
Swersky, K., Snoek, J., and Adams, R.~P.
\newblock Freeze-thaw {B}ayesian optimization.
\newblock \emph{arXiv preprint arXiv:1406.3896}, 2014.

\bibitem[Tallec \& Ollivier(2017)Tallec and Ollivier]{tallec2017unbiased}
Tallec, C. and Ollivier, Y.
\newblock Unbiased online recurrent optimization.
\newblock \emph{arXiv preprint arXiv:1702.05043}, 2017.

\bibitem[Tang et~al.(2021)Tang, Jiang, Gong, Li, Wu, Yu, Wang, and
  Wang]{tang2021skfac}
Tang, Z., Jiang, F., Gong, M., Li, H., Wu, Y., Yu, F., Wang, Z., and Wang, M.
\newblock {SKFAC: Training neural networks with faster Kronecker-factored
  approximate curvature}.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition}, pp.\
   13479--13487, 2021.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{Tieleman2012}
Tieleman, T. and Hinton, G.
\newblock {Lecture 6.5---RMSprop: Divide the gradient by a running average of
  its recent magnitude}.
\newblock {COURSERA: Neural Networks for Machine Learning}, 2012.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5998--6008, 2017.

\bibitem[Vaswani et~al.(2019)Vaswani, Mishkin, Laradji, Schmidt, Gidel, and
  Lacoste-Julien]{vaswani2019painless}
Vaswani, S., Mishkin, A., Laradji, I., Schmidt, M., Gidel, G., and
  Lacoste-Julien, S.
\newblock Painless stochastic gradient: Interpolation, line-search, and
  convergence rates.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3727--3740, 2019.

\bibitem[Vaswani et~al.(2020)Vaswani, Laradji, Kunstner, Meng, Schmidt, and
  Lacoste-Julien]{vaswani2020adaptive}
Vaswani, S., Laradji, I., Kunstner, F., Meng, S.~Y., Schmidt, M., and
  Lacoste-Julien, S.
\newblock Adaptive gradient methods converge faster with over-parameterization
  (but you should do a line-search).
\newblock \emph{arXiv preprint arXiv:2006.06835}, 2020.

\bibitem[Vicol et~al.(2021)Vicol, Metz, and Sohl-Dickstein]{vicol2021unbiased}
Vicol, P., Metz, L., and Sohl-Dickstein, J.
\newblock Unbiased gradient estimation in unrolled computation graphs with
  persistent evolution strategies.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10553--10563, 2021.

\bibitem[Wadia et~al.(2021)Wadia, Duckworth, Schoenholz, Dyer, and
  Sohl-Dickstein]{wadia2021whitening}
Wadia, N., Duckworth, D., Schoenholz, S.~S., Dyer, E., and Sohl-Dickstein, J.
\newblock Whitening and second order optimization both make information in the
  dataset unusable during training, and can reduce or prevent generalization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10617--10629, 2021.

\bibitem[Wichrowska et~al.(2017)Wichrowska, Maheswaranathan, Hoffman,
  Colmenarejo, Denil, de~Freitas, and Sohl-Dickstein]{wichrowska2017learned}
Wichrowska, O., Maheswaranathan, N., Hoffman, M.~W., Colmenarejo, S.~G., Denil,
  M., de~Freitas, N., and Sohl-Dickstein, J.
\newblock Learned optimizers that scale and generalize.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3751--3760, 2017.

\bibitem[Wu et~al.(2018)Wu, Ren, Liao, and Grosse]{wu2018understanding}
Wu, Y., Ren, M., Liao, R., and Grosse, R.
\newblock Understanding short-horizon bias in stochastic meta-optimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Li, Nado, Martens, Sachdeva,
  Dahl, Shallue, and Grosse]{zhang2019algorithmic}
Zhang, G., Li, L., Nado, Z., Martens, J., Sachdeva, S., Dahl, G., Shallue, C.,
  and Grosse, R.~B.
\newblock Which algorithmic choices matter at which batch sizes? insights from
  a noisy quadratic model.
\newblock \emph{Advances in neural information processing systems},
  32:\penalty0 8196--8207, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Martens, and
  Grosse]{zhang2019fast}
Zhang, G., Martens, J., and Grosse, R.
\newblock Fast convergence of natural gradient descent for overparameterized
  neural networks.
\newblock \emph{arXiv preprint arXiv:1905.10961}, 2019{\natexlab{b}}.

\end{thebibliography}
