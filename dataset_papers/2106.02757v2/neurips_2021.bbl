\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, D{\k{e}}biak,
  Dennison, Farhi, Fischer, Hashme, Hesse, et~al.]{berner2019dota}
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemys{\l}aw
  D{\k{e}}biak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme,
  Chris Hesse, et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Oriol Vinyals, Igor Babuschkin, Wojciech~M Czarnecki, Micha{\"e}l Mathieu,
  Andrew Dudzik, Junyoung Chung, David~H Choi, Richard Powell, Timo Ewalds,
  Petko Georgiev, et~al.
\newblock Grandmaster level in {S}tarcraft {II} using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Dann and Brunskill(2015)]{dann2015sample}
Christoph Dann and Emma Brunskill.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{NIPS}, 2015.

\bibitem[Sidford et~al.(2018)Sidford, Wang, Wu, Yang, and Ye]{sidford2018near}
Aaron Sidford, Mengdi Wang, Xian Wu, Lin~F Yang, and Yinyu Ye.
\newblock Near-optimal time and sample complexities for solving {M}arkov
  decision processes with a generative model.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Tessler and Mannor(2020)]{tessler2020maximizing}
Chen Tessler and Shie Mannor.
\newblock Maximizing the total reward via reward tweaking.
\newblock \emph{arXiv preprint arXiv:2002.03327}, 2020.

\bibitem[Van~Seijen et~al.(2019)Van~Seijen, Fatemi, and
  Tavakoli]{NEURIPS2019_eba237ec}
Harm Van~Seijen, Mehdi Fatemi, and Arash Tavakoli.
\newblock Using a logarithmic mapping to enable lower discount factors in
  reinforcement learning.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Bhardwaj et~al.(2021)Bhardwaj, Choudhury, and
  Boots]{bhardwaj2020blending}
Mohak Bhardwaj, Sanjiban Choudhury, and Byron Boots.
\newblock Blending mpc \& value function approximation for efficient
  reinforcement learning.
\newblock In \emph{ICLR}, 2021.

\bibitem[Farahmand et~al.(2016)Farahmand, Nikovski, Igarashi, and
  Konaka]{farahmand2016truncated}
Amir-massoud Farahmand, Daniel~N Nikovski, Yuji Igarashi, and Hiroki Konaka.
\newblock Truncated approximate dynamic programming with task-dependent
  terminal value.
\newblock In \emph{AAAI}, 2016.

\bibitem[Nair et~al.(2020)Nair, Dalal, Gupta, and Levine]{nair2020accelerating}
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock \emph{arXiv preprint arXiv:2006.09359}, 2020.

\bibitem[Hester et~al.(2018)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Horgan, Quan, Sendonaris, Osband, et~al.]{hester2018deep}
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal
  Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, et~al.
\newblock Deep {Q}-learning from demonstrations.
\newblock In \emph{AAAI}, 2018.

\bibitem[Mausam and Kolobov(2012)]{kolobov2012}
Mausam and Andrey Kolobov.
\newblock Planning with {M}arkov decision processes: An {AI} perspective.
\newblock \emph{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}, 6:\penalty0 1--210, 2012.

\bibitem[Foster and Rakhlin(2020)]{pmlr-v119-foster20a}
Dylan Foster and Alexander Rakhlin.
\newblock Beyond {UCB}: Optimal and efficient contextual bandits with
  regression oracles.
\newblock In \emph{ICML}, 2020.

\bibitem[Hoffmann and Nebel(2001)]{Hoffmann_2001}
J.~Hoffmann and B.~Nebel.
\newblock The {FF} planning system: Fast plan generation through heuristic
  search.
\newblock \emph{Journal of Artificial Intelligence Research}, 14:\penalty0
  253–302, 2001.

\bibitem[Richter and Westphal(2010)]{Richter_2010}
S.~Richter and M.~Westphal.
\newblock The {LAMA} planner: Guiding cost-based anytime planning with
  landmarks.
\newblock \emph{Journal of Artificial Intelligence Research}, 39:\penalty0
  127–177, 2010.

\bibitem[Kolobov et~al.(2010)Kolobov, Mausam, and Weld]{akolobov-aaai10}
Andrey Kolobov, Mausam, and Daniel~S. Weld.
\newblock Classical planning in {MDP} heuristics: With a little help from
  generalization.
\newblock In \emph{AAAI}, 2010.

\bibitem[Gulcehre et~al.(2021)Gulcehre, Colmenarejo, Wang, Sygnowski, Paine,
  Zolna, Chen, Hoffman, Pascanu, and de~Freitas]{gulcehre2021regularized}
Caglar Gulcehre, Sergio~G{\'o}mez Colmenarejo, Ziyu Wang, Jakub Sygnowski,
  Thomas Paine, Konrad Zolna, Yutian Chen, Matthew Hoffman, Razvan Pascanu, and
  Nando de~Freitas.
\newblock Regularized behavior value estimation.
\newblock \emph{arXiv preprint arXiv:2103.09575}, 2021.

\bibitem[Cheng et~al.(2020)Cheng, Kolobov, and Agarwal]{cheng2020policy}
Ching-An Cheng, Andrey Kolobov, and Alekh Agarwal.
\newblock Policy improvement via imitation of multiple oracles.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Blackwell(1962)]{blackwell1962discrete}
David Blackwell.
\newblock Discrete dynamic programming.
\newblock \emph{The Annals of Mathematical Statistics}, pages 719--726, 1962.

\bibitem[Petrik and Scherrer(2008)]{petrik2008biasing}
Marek Petrik and Bruno Scherrer.
\newblock Biasing approximate dynamic programming with a lower discount factor.
\newblock In \emph{NIPS}, 2008.

\bibitem[Jiang et~al.(2015)Jiang, Kulesza, Singh, and
  Lewis]{jiang2015dependence}
Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis.
\newblock The dependence of effective planning horizon on model accuracy.
\newblock In \emph{AAMAS}, 2015.

\bibitem[Jiang et~al.(2016)Jiang, Singh, and Tewari]{jiang2016structural}
Nan Jiang, Satinder Singh, and Ambuj Tewari.
\newblock On structural properties of mdps that bound loss due to shallow
  planning.
\newblock In \emph{IJCAI}, 2016.

\bibitem[Chen et~al.(2018)Chen, Kochenderfer, and Spaan]{chen2018improving}
Yi-Chun Chen, Mykel~J Kochenderfer, and Matthijs~TJ Spaan.
\newblock Improving offline value-function approximations for pomdps by
  reducing discount factors.
\newblock In \emph{IROS}, 2018.

\bibitem[Amit et~al.(2020)Amit, Meir, and Ciosek]{amit2020discount}
Ron Amit, Ron Meir, and Kamil Ciosek.
\newblock Discount factor as a regularizer in reinforcement learning.
\newblock In \emph{ICML}, 2020.

\bibitem[Hart et~al.(1968)Hart, Nilsson, and Raphael]{hart1968formal}
Peter~E Hart, Nils~J Nilsson, and Bertram Raphael.
\newblock A formal basis for the heuristic determination of minimum cost paths.
\newblock \emph{IEEE Transactions on Systems Science and Cybernetics},
  4\penalty0 (2):\penalty0 100--107, 1968.

\bibitem[Browne et~al.(2012)Browne, Powley, Whitehouse, Lucas, Cowling,
  Rohlfshagen, Tavener, Perez, Samothrakis, and Colton]{browne2012survey}
Cameron~B Browne, Edward Powley, Daniel Whitehouse, Simon~M Lucas, Peter~I
  Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon
  Samothrakis, and Simon Colton.
\newblock A survey of monte carlo tree search methods.
\newblock \emph{IEEE Transactions on Computational Intelligence and AI in
  games}, 4\penalty0 (1):\penalty0 1--43, 2012.

\bibitem[Zhong et~al.(2013)Zhong, Johnson, Tassa, Erez, and
  Todorov]{zhong2013value}
Mingyuan Zhong, Mikala Johnson, Yuval Tassa, Tom Erez, and Emanuel Todorov.
\newblock Value function approximation and model predictive control.
\newblock In \emph{IEEE International Symposium on Adaptive Dynamic Programming
  and Reinforcement Learning}, pages 100--107, 2013.

\bibitem[Hoeller et~al.(2020)Hoeller, Farshidian, and Hutter]{hoeller2020deep}
David Hoeller, Farbod Farshidian, and Marco Hutter.
\newblock Deep value model predictive control.
\newblock In \emph{CoRL}, 2020.

\bibitem[Bejjani et~al.(2018)Bejjani, Papallas, Leonetti, and
  Dogar]{bejjani2018planning}
Wissam Bejjani, Rafael Papallas, Matteo Leonetti, and Mehmet~R Dogar.
\newblock Planning with a receding horizon for manipulation in clutter using a
  learned value function.
\newblock In \emph{Humanoids}, pages 1--9, 2018.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Andrew~Y Ng, Daishi Harada, and Stuart Russell.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{ICML}, 1999.

\bibitem[Jin et~al.(2020)Jin, Yang, and Wang]{jin2020pessimism}
Ying Jin, Zhuoran Yang, and Zhaoran Wang.
\newblock Is pessimism provably efficient for offline {RL}?
\newblock \emph{arXiv preprint arXiv:2012.15085}, 2020.

\bibitem[Liu et~al.(2020)Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Provably good batch off-policy reinforcement learning without great
  exploration.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{IROS}, 2012.

\bibitem[Mohanty et~al.(2021)Mohanty, Poonganam, Gaidon, Kolobov, Wulfe,
  Chakraborty, Šemetulskis, Schapke, Kubilius, Pašukonis, Klimas, Hausknecht,
  MacAlpine, Tran, Tumiel, Tang, Chen, Hesse, Hilton, Guss, Genc, Schulman, and
  Cobbe]{procgen}
Sharada Mohanty, Jyotish Poonganam, Adrien Gaidon, Andrey Kolobov, Blake Wulfe,
  Dipam Chakraborty, Gražvydas Šemetulskis, João Schapke, Jonas Kubilius,
  Jurgis Pašukonis, Linas Klimas, Matthew Hausknecht, Patrick MacAlpine,
  Quang~Nhat Tran, Thomas Tumiel, Xiaocheng Tang, Xinwei Chen, Christopher
  Hesse, Jacob Hilton, William~Hebgen Guss, Sahika Genc, John Schulman, and
  Karl Cobbe.
\newblock Measuring sample efficiency and generalization in reinforcement
  learning benchmarks: Neur{IPS} 2020 {P}rocgen benchmark.
\newblock \emph{arXiv preprint arXiv:2103.15332}, 2021.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is {Q}-learning provably efficient?
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{ICML}, 2018.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[garage contributors(2019)]{garage}
The garage contributors.
\newblock Garage: A toolkit for reproducible reinforcement learning research.
\newblock {https://github.com/rlworkgroup/garage}, 2019.

\bibitem[Hu et~al.(2020)Hu, Wang, Jia, Wang, Chen, Hao, Wu, and
  Fan]{hu2020learning}
Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao,
  Feng Wu, and Changjie Fan.
\newblock Learning to utilize shaping rewards: A new approach of reward
  shaping.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Seijen and Sutton(2014)]{seijen2014true}
Harm Seijen and Rich Sutton.
\newblock True online td (lambda).
\newblock In \emph{ICML}, 2014.

\bibitem[Efroni et~al.(2018)Efroni, Dalal, Scherrer, and
  Mannor]{efroni2018beyond}
Yonathan Efroni, Gal Dalal, Bruno Scherrer, and Shie Mannor.
\newblock Beyond the one-step greedy approach in reinforcement learning.
\newblock In \emph{ICML}, 2018.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{schulman2015high}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter
  Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock In \emph{ICLR}, 2016.

\bibitem[Sherstan et~al.(2020)Sherstan, Dohare, MacGlashan, G{\"u}nther, and
  Pilarski]{sherstan2020gamma}
Craig Sherstan, Shibhansh Dohare, James MacGlashan, Johannes G{\"u}nther, and
  Patrick~M Pilarski.
\newblock Gamma-nets: Generalizing value estimation over timescale.
\newblock In \emph{AAAI}, 2020.

\bibitem[Romoff et~al.(2019)Romoff, Henderson, Touati, Brunskill, Pineau, and
  Ollivier]{romoff2019separating}
Joshua Romoff, Peter Henderson, Ahmed Touati, Emma Brunskill, Joelle Pineau,
  and Yann Ollivier.
\newblock Separating value functions across time-scales.
\newblock In \emph{ICML}, 2019.

\bibitem[Richalet et~al.(1978)Richalet, Rault, Testud, and
  Papon]{richalet1978model}
Jacques Richalet, Andr{\'e} Rault, JL~Testud, and J~Papon.
\newblock Model predictive heuristic control.
\newblock \emph{Automatica}, 14\penalty0 (5):\penalty0 413--428, 1978.

\bibitem[Asai and Muise(2020)]{asai2020}
Masataro Asai and Christian Muise.
\newblock Learning neural-symbolic descriptive planning models via cube-space
  priors: The voyage home (to strips).
\newblock In \emph{IJCAI}, 2020.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Lu et~al.(2018)Lu, Schuurmans, and Boutilier]{lu2018non}
Tyler Lu, Dale Schuurmans, and Craig Boutilier.
\newblock Non-delusional q-learning and value iteration.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Russell and Norvig(2020)]{russell2020}
Stuart~J. Russell and Peter Norvig.
\newblock \emph{Artificial Intelligence: A Modern Approach}.
\newblock Pearson, 4th edition, 2020.

\bibitem[Pearl(1981)]{pearl1981}
Judea Pearl.
\newblock Heuristic search theory: Survey of recent results.
\newblock In \emph{IJCAI}, 1981.

\bibitem[Bianchi et~al.(2013)Bianchi, Martins, Ribeiro, and
  Costa]{bianchi2013heuristically}
Reinaldo~AC Bianchi, Murilo~F Martins, Carlos~HC Ribeiro, and Anna~HR Costa.
\newblock Heuristically-accelerated multiagent reinforcement learning.
\newblock \emph{IEEE transactions on cybernetics}, 44\penalty0 (2):\penalty0
  252--265, 2013.

\bibitem[Sun et~al.(2017)Sun, Venkatraman, Gordon, Boots, and
  Bagnell]{sun2017deeply}
Wen Sun, Arun Venkatraman, Geoffrey~J Gordon, Byron Boots, and J~Andrew
  Bagnell.
\newblock Deeply aggrevated: Differentiable imitation learning for sequential
  prediction.
\newblock In \emph{ICML}, 2017.

\bibitem[Sun et~al.(2018)Sun, Bagnell, and Boots]{sun2018truncated}
Wen Sun, J~Andrew Bagnell, and Byron Boots.
\newblock Truncated horizon policy search: Combining reinforcement learning \&
  imitation learning.
\newblock In \emph{ICLR}, 2018.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Florensa et~al.(2017)Florensa, Held, Wulfmeier, Zhang, and
  Abbeel]{florensa2017reverse}
Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter
  Abbeel.
\newblock Reverse curriculum generation for reinforcement learning.
\newblock In \emph{CoRL}, 2017.

\bibitem[Kakade and Langford(2002)]{kakade2002approximately}
Sham Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{ICML}, 2002.

\bibitem[Cobbe et~al.(2020)Cobbe, Hesse, Hilton, and
  Schulman]{cobbe2020leveraging}
Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{ICML}, 2020.

\bibitem[Liang et~al.(2018)Liang, Liaw, Nishihara, Moritz, Fox, Goldberg,
  Gonzalez, Jordan, and Stoica]{pmlr-v80-liang18b}
Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken
  Goldberg, Joseph Gonzalez, Michael Jordan, and Ion Stoica.
\newblock {RL}lib: Abstractions for distributed reinforcement learning.
\newblock In \emph{ICML}, 2018.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, Legg, and Kavukcuoglu]{pmlr-v80-espeholt18a}
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward,
  Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and Koray
  Kavukcuoglu.
\newblock {IMPALA}: Scalable distributed deep-{RL} with importance weighted
  actor-learner architectures.
\newblock In \emph{ICML}, 2018.

\end{thebibliography}
