\begin{thebibliography}{10}

\bibitem{Antos2008}
A.~Antos, C.~Szepesvari, and R.~Munos.
\newblock Learning near-optimal policies with {B}ellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock {\em Machine Learning}, 71(1):89--129, 2008.

\bibitem{Asadi2016}
K.~Asadi and M.~L. Littman.
\newblock An alternative softmax operator for reinforcement learning.
\newblock In {\em Proc. International Conference on Machine Learning (ICML)},
  2016.

\bibitem{bertsekas2011dynamic}
D.~P. Bertsekas.
\newblock {\em Dynamic Programming and Optimal Control}, volume~2.
\newblock Athena Scientific, 3rd edition, 2012.

\bibitem{bhandari2018finite}
J.~Bhandari, D.~Russo, and R.~Singal.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock {\em arXiv preprint arXiv:1806.02450}, 2018.

\bibitem{Boyan2002}
J.~A. Boyan.
\newblock Technical update: {L}east-squares temporal difference learning.
\newblock {\em Machine Learning}, 49:233--246, 2002.

\bibitem{Brad1996}
S.~J. Bradtke and A.~G. Barto.
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock {\em Machine Learning}, 22:33--57, 1996.

\bibitem{bubeck2015convex}
S.~Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  8(3-4):231--357, 2015.

\bibitem{dalal2017finite}
G.~Dalal, B.~Szorenyi, G.~Thoppe, and S.~Mannor.
\newblock Finite sample analysis of two-timescale stochastic approximation with
  applications to reinforcement learning.
\newblock In {\em Proc. Conference on Learning Theory (COLT)}, 2018.

\bibitem{Dalal2018a}
G.~Dalal, B.~Szrnyi, G.~Thoppe, and S.~Mannor.
\newblock Finite sample analyses for {TD}(0) with function approximation.
\newblock In {\em Proc. AAAI Conference on Artificial Intelligence (AAAI)},
  2018.

\bibitem{de2000existence}
D.~P. De~Farias and B.~Van~Roy.
\newblock On the existence of fixed points for approximate value iteration and
  temporal-difference learning.
\newblock {\em Journal of Optimization theory and Applications},
  105(3):589--608, 2000.

\bibitem{Farah2010}
A.-M. Farahmand, C.~Szepesvari, and R.~Munos.
\newblock Error propagation for approximate policy and value iteration.
\newblock In {\em Proc. Advances in Neural Information Processing Systems
  (NIPS)}, 2010.

\bibitem{Ghav2010}
M.~Ghavamzadeh, A.~Lazaric, O.~Maillard, and R.~Munos.
\newblock {LSTD} with random projections.
\newblock In {\em Proc. Advances in Neural Information Processing Systems
  (NIPS)}, 2010.

\bibitem{gordon1996chattering}
G.~J. Gordon.
\newblock Chattering in {SARSA} ($\lambda$)-a {CMU} learning lab internal
  report.
\newblock 1996.

\bibitem{gordon2000reinforcement}
G.~J. Gordon.
\newblock Reinforcement learning with function approximation converges to a
  region.
\newblock In {\em Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 1040--1046, 2001.

\bibitem{gupta2019finite}
H.~Gupta, R.~Srikant, and L.~Ying.
\newblock Finite-time performance bounds and adaptive learning rate selection
  for two time-scale reinforcement learning.
\newblock {\em To appear in Proc. Advances in Neural Information Processing
  Systems (NeurIPS)}, 2019.

\bibitem{kushner2010stochastic}
H.~Kushner.
\newblock Stochastic approximation: a survey.
\newblock {\em Wiley Interdisciplinary Reviews: Computational Statistics},
  2(1):87--96, 2010.

\bibitem{lacoste2012simpler}
S.~Lacoste-Julien, M.~Schmidt, and F.~Bach.
\newblock A simpler approach to obtaining an $o(1/t)$ convergence rate for the
  projected stochastic subgradient method.
\newblock {\em arXiv preprint arXiv:1212.2002}, 2012.

\bibitem{Lagou2003}
M.~G. Lagoudakis and R.~Parr.
\newblock Least-squares policy iteration.
\newblock {\em Journal of Machine Learning Research}, 4:1107--1149, 2003.

\bibitem{Laksh2018}
C.~Lakshminarayanan and C.~Szepesvari.
\newblock Linear stochastic approximation: {H}ow far does constant step-size
  and iterate averaging go?
\newblock In {\em Proc. International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2018.

\bibitem{Lazaric2010}
A.~Lazaric, M.~Ghavamzadeh, and R.~Munos.
\newblock Finite-sample analysis of lstd.
\newblock In {\em Proc. International Conference on Machine Learning (ICML)},
  2010.

\bibitem{Lazaric2012}
A.~Lazaric, M.~Ghavamzadeh, and R.~Munos.
\newblock Finite-sample analysis of least-squares policy iteration.
\newblock {\em Journal of Machine Learning Research}, 13:3041--3074, 2012.

\bibitem{Lazaric2016}
A.~Lazaric, M.~Ghavamzadeh, and R.~Munos.
\newblock Analysis of classification-based policy iteration algorithms.
\newblock {\em Journal of Machine Learning Research}, 17:583--612, 2016.

\bibitem{melo2008analysis}
F.~S. Melo, S.~P. Meyn, and M.~I. Ribeiro.
\newblock An analysis of reinforcement learning with function approximation.
\newblock In {\em Proc. International Conference on Machine Learning (ICML)},
  pages 664--671. ACM, 2008.

\bibitem{mitrophanov2005sensitivity}
A.~Y. Mitrophanov.
\newblock Sensitivity and convergence of uniformly ergodic markov chains.
\newblock {\em Journal of Applied Probability}, 42(4):1003--1014, 2005.

\bibitem{Munos2008}
R.~Munos and C.~Szepesvari.
\newblock Finite-time bounds for fitted value iteration.
\newblock {\em Journal of Machine Learning Research}, 9:815--857, May 2008.

\bibitem{nemirovski2009robust}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em SIAM Journal on Optimization}, 19(4):1574--1609, 2009.

\bibitem{perkins2002existence}
T.~J. Perkins and M.~D. Pendrith.
\newblock On the existence of fixed points for {Q}-learning and {Sarsa} in
  partially observable domains.
\newblock In {\em Proc. International Conference on Machine Learning (ICML)},
  pages 490--497, 2002.

\bibitem{perkins2003convergent}
T.~J. Perkins and D.~Precup.
\newblock A convergent form of approximate policy iteration.
\newblock In {\em Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 1627--1634, 2003.

\bibitem{Pires2012}
B.~A. Pires and C.~Szepesvari.
\newblock Statistical linear estimation with penalized estimators: {A}n
  application to reinforcement learning.
\newblock In {\em Proc. International Conference on Machine Learning (ICML)},
  2012.

\bibitem{Prash2013}
L.~Prashanth, N.~Korda, and R.~Munos.
\newblock Fast {LSTD} using stochastic approximation: {F}inite time analysis
  and application to traffic control.
\newblock In {\em Proc. Joint European Conference on Machine Learning and
  Knowledge Discovery in Databases}, 2013.

\bibitem{Rummery1994}
G.~A. Rummery and M.~Niranjan.
\newblock Online {Q}-learning using connectionist systems.
\newblock {\em Technical Report, {Cambridge University Engineering
  Department}}, Sept. 1994.

\bibitem{Shah2018}
D.~Shah and Q.~Xie.
\newblock Q-learning with nearest neighbors.
\newblock In {\em Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem{singh2000convergence}
S.~Singh, T.~Jaakkola, M.~L. Littman, and C.~Szepesv{\'a}ri.
\newblock Convergence results for single-step on-policy reinforcement-learning
  algorithms.
\newblock {\em Machine Learning}, 38(3):287--308, 2000.

\bibitem{srikant2019}
R.~Srikant and L.~Ying.
\newblock Finite-time error bounds for linear stochastic approximation and {TD}
  learning.
\newblock In {\em Proc. Annual Conference on Learning Theory (CoLT)}, 2019.

\bibitem{Tagorti2015}
M.~Tagorti and B.~Scherrer.
\newblock On the rate of convergence and error bounds for {LSTD} ($\lambda$).
\newblock In {\em Proc. International Conference on Machine Learning (ICML)},
  2015.

\bibitem{Tsitsiklis1997}
J.~N. Tsitsiklis and B.~Roy.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock {\em IEEE Transactions on Automatic Control}, 42(5):674--690, May
  1997.

\bibitem{Tu2018}
S.~Tu and B.~Recht.
\newblock Least-squares temporal difference learning for the linear quadratic
  regulator.
\newblock In {\em Proc. International Conference on Machine Learning (ICML)},
  2018.

\bibitem{xu2019two}
T.~Xu, S.~Zou, and Y.~Liang.
\newblock Two time-scale off-policy {TD} learning: Non-asymptotic analysis over
  {Markovian} samples.
\newblock {\em in Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem{Yang2019}
Z.~Yang, Y.~Xie, and Z.~Wang.
\newblock A theoretical analysis of deep {Q}-learning.
\newblock {\em ArXiv: 1901.00137}, Jan. 2019.

\end{thebibliography}
