\begin{thebibliography}{92}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abnar et~al.(2021)Abnar, Dehghani, Neyshabur, and
  Sedghi]{abnar2021exploring}
S.~Abnar, M.~Dehghani, B.~Neyshabur, and H.~Sedghi.
\newblock Exploring the limits of large scale pre-training.
\newblock \emph{arXiv preprint arXiv:2110.02095}, 2021.

\bibitem[Alain and Bengio(2016)]{alain2016understanding}
G.~Alain and Y.~Bengio.
\newblock Understanding intermediate layers using linear classifier probes.
\newblock \emph{arXiv preprint arXiv:1610.01644}, 2016.

\bibitem[Ansuini et~al.(2019)Ansuini, Laio, Macke, and
  Zoccolan]{ansuini2019intrinsic}
A.~Ansuini, A.~Laio, J.~H. Macke, and D.~Zoccolan.
\newblock Intrinsic dimension of data representations in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Bartlett(1997)]{bartlett1997valid}
P.~L. Bartlett.
\newblock For valid generalization, the size of the weights is more important
  than the size of the network.
\newblock \emph{Advances in Neural Information Processing Systems}, pages
  134--140, 1997.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
P.~L. Bartlett, D.~J. Foster, and M.~Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  2017:\penalty0 6241--6250, 2017.

\bibitem[Berg et~al.(2014)Berg, Liu, Lee, Alexander, Jacobs, and
  Belhumeur]{berg2014birdsnap}
T.~Berg, J.~Liu, S.~W. Lee, M.~L. Alexander, D.~W. Jacobs, and P.~N. Belhumeur.
\newblock Birdsnap: Large-scale fine-grained visual categorization of birds.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 2019--2026. IEEE, 2014.

\bibitem[Bertinetto et~al.(2019)Bertinetto, Henriques, Torr, and
  Vedaldi]{bertinetto2018metalearning}
L.~Bertinetto, J.~F. Henriques, P.~Torr, and A.~Vedaldi.
\newblock Meta-learning with differentiable closed-form solvers.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Beyer et~al.(2020)Beyer, H{\'e}naff, Kolesnikov, Zhai, and
  Oord]{beyer2020we}
L.~Beyer, O.~J. H{\'e}naff, A.~Kolesnikov, X.~Zhai, and A.~v.~d. Oord.
\newblock Are we done with {ImageNet}?
\newblock \emph{arXiv preprint arXiv:2006.07159}, 2020.

\bibitem[Bossard et~al.(2014)Bossard, Guillaumin, and
  Van~Gool]{bossard2014food}
L.~Bossard, M.~Guillaumin, and L.~Van~Gool.
\newblock Food-101 --- mining discriminative components with random forests.
\newblock In \emph{European Conference on Computer Vision}, pages 446--461.
  Springer, 2014.

\bibitem[Bridle(1990{\natexlab{a}})]{bridle1990probabilistic}
J.~S. Bridle.
\newblock Probabilistic interpretation of feedforward classification network
  outputs, with relationships to statistical pattern recognition.
\newblock In \emph{Neurocomputing}, pages 227--236. Springer,
  1990{\natexlab{a}}.

\bibitem[Bridle(1990{\natexlab{b}})]{bridle1990training}
J.~S. Bridle.
\newblock Training stochastic model recognition algorithms as networks can lead
  to maximum mutual information estimation of parameters.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  211--217, 1990{\natexlab{b}}.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Garg, Yu, Shrivastava, Kautz, and
  Anandkumar]{chen2019angular}
B.~Chen, W.~L.~A. Garg, Z.~Yu, A.~Shrivastava, J.~Kautz, and A.~Anandkumar.
\newblock Angular visual hardness.
\newblock In \emph{International Conference on Machine Learning},
  2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Ziyin, Wang, and
  Liang]{chen2020investigation}
B.~Chen, L.~Ziyin, Z.~Wang, and P.~P. Liang.
\newblock An investigation of how label smoothing affects generalization.
\newblock \emph{arXiv preprint arXiv:2010.12648}, 2020{\natexlab{b}}.

\bibitem[Chen et~al.(2020{\natexlab{c}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International Conference on Machine Learning},
  2020{\natexlab{c}}.

\bibitem[Chen et~al.(2019)Chen, Liu, Kira, Wang, and Huang]{chen2018a}
W.-Y. Chen, Y.-C. Liu, Z.~Kira, Y.-C.~F. Wang, and J.-B. Huang.
\newblock A closer look at few-shot classification.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Cohen et~al.(2020)Cohen, Chung, Lee, and
  Sompolinsky]{cohen2020separability}
U.~Cohen, S.~Chung, D.~D. Lee, and H.~Sompolinsky.
\newblock Separability and geometry of object manifolds in deep neural
  networks.
\newblock \emph{Nature Communications}, 11\penalty0 (1):\penalty0 1--13, 2020.

\bibitem[Cortes et~al.(2012)Cortes, Mohri, and
  Rostamizadeh]{cortes2012algorithms}
C.~Cortes, M.~Mohri, and A.~Rostamizadeh.
\newblock Algorithms for learning kernels based on centered alignment.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 795--828, 2012.

\bibitem[Cristianini et~al.(2002)Cristianini, Shawe-Taylor, Elisseeff, and
  Kandola]{cristianini2002kernel}
N.~Cristianini, J.~Shawe-Taylor, A.~Elisseeff, and J.~S. Kandola.
\newblock On kernel-target alignment.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  367--373, 2002.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2019autoaugment}
E.~D. Cubuk, B.~Zoph, D.~Mane, V.~Vasudevan, and Q.~V. Le.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 113--123, 2019.

\bibitem[Dauphin and Cubuk(2021)]{dauphin2021deconstructing}
Y.~Dauphin and E.~D. Cubuk.
\newblock Deconstructing the regularization of batchnorm.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet}: A large-scale hierarchical image database.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 248--255. Ieee, 2009.

\bibitem[Deng et~al.(2019)Deng, Guo, Xue, and Zafeiriou]{deng2019arcface}
J.~Deng, J.~Guo, N.~Xue, and S.~Zafeiriou.
\newblock Arcface: Additive angular margin loss for deep face recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 4690--4699, 2019.

\bibitem[Doersch et~al.(2020)Doersch, Gupta, and
  Zisserman]{doersch2020crosstransformers}
C.~Doersch, A.~Gupta, and A.~Zisserman.
\newblock Crosstransformers: spatially-aware few-shot transfer.
\newblock \emph{arXiv preprint arXiv:2007.11498}, 2020.

\bibitem[Donahue et~al.(2014)Donahue, Jia, Vinyals, Hoffman, Zhang, Tzeng, and
  Darrell]{donahue2014decaf}
J.~Donahue, Y.~Jia, O.~Vinyals, J.~Hoffman, N.~Zhang, E.~Tzeng, and T.~Darrell.
\newblock Decaf: A deep convolutional activation feature for generic visual
  recognition.
\newblock In \emph{International Conference on Machine Learning}, 2014.

\bibitem[Frosst et~al.(2019)Frosst, Papernot, and Hinton]{frosst2019analyzing}
N.~Frosst, N.~Papernot, and G.~Hinton.
\newblock Analyzing and improving representations with the soft nearest
  neighbor loss.
\newblock In \emph{International Conference on Machine Learning}, pages
  2012--2020. PMLR, 2019.

\bibitem[Goldblum et~al.(2020)Goldblum, Reich, Fowl, Ni, Cherepanova, and
  Goldstein]{goldblum2020unraveling}
M.~Goldblum, S.~Reich, L.~Fowl, R.~Ni, V.~Cherepanova, and T.~Goldstein.
\newblock Unraveling meta-learning: Understanding feature representations for
  few-shot tasks.
\newblock In \emph{International Conference on Machine Learning}, pages
  3607--3616. PMLR, 2020.

\bibitem[Goldfeld et~al.(2018)Goldfeld, Berg, Greenewald, Melnyk, Nguyen,
  Kingsbury, and Polyanskiy]{goldfeld2018estimating}
Z.~Goldfeld, E.~v.~d. Berg, K.~Greenewald, I.~Melnyk, N.~Nguyen, B.~Kingsbury,
  and Y.~Polyanskiy.
\newblock Estimating information flow in deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
P.~Goyal, P.~Doll{\'a}r, R.~Girshick, P.~Noordhuis, L.~Wesolowski, A.~Kyrola,
  A.~Tulloch, Y.~Jia, and K.~He.
\newblock Accurate, large minibatch {SGD}: training {ImageNet} in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Gross and Wilber(2016)]{resnetv15torchblogpost}
S.~Gross and M.~Wilber.
\newblock Training and investigating residual nets.
\newblock In \emph{The Torch Blog}.
  \url{http://torch.ch/blog/2016/02/04/resnets.html}, 2016.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
S.~Gunasekar, J.~D. Lee, D.~Soudry, and N.~Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9461--9471, 2018.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
C.~Guo, G.~Pleiss, Y.~Sun, and K.~Q. Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1321--1330, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 770--778, 2016.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019benchmarking}
D.~Hendrycks and T.~Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Zhao, Basart, Steinhardt, and
  Song]{hendrycks2019natural}
D.~Hendrycks, K.~Zhao, S.~Basart, J.~Steinhardt, and D.~Song.
\newblock Natural adversarial examples.
\newblock \emph{arXiv preprint arXiv:1907.07174}, 2019.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo,
  Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2020many}
D.~Hendrycks, S.~Basart, N.~Mu, S.~Kadavath, F.~Wang, E.~Dorundo, R.~Desai,
  T.~Zhu, S.~Parajuli, M.~Guo, et~al.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock \emph{arXiv preprint arXiv:2006.16241}, 2020.

\bibitem[Hui and Belkin(2021)]{hui2020evaluation}
L.~Hui and M.~Belkin.
\newblock Evaluation of neural architectures trained with square loss vs
  cross-entropy in classification tasks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Irvin et~al.(2019)Irvin, Rajpurkar, Ko, Yu, Ciurea-Ilcus, Chute,
  Marklund, Haghgoo, Ball, Shpanskaya, et~al.]{irvin2019chexpert}
J.~Irvin, P.~Rajpurkar, M.~Ko, Y.~Yu, S.~Ciurea-Ilcus, C.~Chute, H.~Marklund,
  B.~Haghgoo, R.~Ball, K.~Shpanskaya, et~al.
\newblock Chexpert: A large chest radiograph dataset with uncertainty labels
  and expert comparison.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~33, pages 590--597, 2019.

\bibitem[Janocha and Czarnecki(2016)]{janocha2016loss}
K.~Janocha and W.~M. Czarnecki.
\newblock On loss functions for deep neural networks in classification.
\newblock \emph{Schedae Informaticae}, 25, 2016.

\bibitem[Khosla et~al.(2020)Khosla, Teterwak, Wang, Sarna, Tian, Isola,
  Maschinot, Liu, and Krishnan]{khosla2020supervised}
P.~Khosla, P.~Teterwak, C.~Wang, A.~Sarna, Y.~Tian, P.~Isola, A.~Maschinot,
  C.~Liu, and D.~Krishnan.
\newblock Supervised contrastive learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Kornblith et~al.(2019{\natexlab{a}})Kornblith, Norouzi, Lee, and
  Hinton]{kornblith2019similarity}
S.~Kornblith, M.~Norouzi, H.~Lee, and G.~Hinton.
\newblock Similarity of neural network representations revisited.
\newblock In \emph{International Conference on Machine Learning},
  2019{\natexlab{a}}.

\bibitem[Kornblith et~al.(2019{\natexlab{b}})Kornblith, Shlens, and
  Le]{kornblith2019better}
S.~Kornblith, J.~Shlens, and Q.~V. Le.
\newblock Do better {ImageNet} models transfer better?
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 2661--2671, 2019{\natexlab{b}}.

\bibitem[Krause et~al.(2013)Krause, Deng, Stark, and
  Fei-Fei]{krause2013collecting}
J.~Krause, J.~Deng, M.~Stark, and L.~Fei-Fei.
\newblock Collecting a large-scale dataset of fine-grained cars.
\newblock In \emph{Second Workshop on Fine-Grained Visual Categorization},
  2013.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Liu et~al.(2020)Liu, Cao, Lin, Li, Zhang, Long, and
  Hu]{liu2020negative}
B.~Liu, Y.~Cao, Y.~Lin, Q.~Li, Z.~Zhang, M.~Long, and H.~Hu.
\newblock Negative margin matters: Understanding margin in few-shot
  classification.
\newblock In \emph{European Conference on Computer Vision}, pages 438--455.
  Springer, 2020.

\bibitem[Liu et~al.(2017)Liu, Wen, Yu, Li, Raj, and Song]{liu2017sphereface}
W.~Liu, Y.~Wen, Z.~Yu, M.~Li, B.~Raj, and L.~Song.
\newblock Sphereface: Deep hypersphere embedding for face recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 212--220, 2017.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2016sgdr}
I.~Loshchilov and F.~Hutter.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Lukasik et~al.(2020)Lukasik, Bhojanapalli, Menon, and
  Kumar]{lukasik2020does}
M.~Lukasik, S.~Bhojanapalli, A.~Menon, and S.~Kumar.
\newblock Does label smoothing mitigate label noise?
\newblock In \emph{International Conference on Machine Learning}, pages
  6448--6458. PMLR, 2020.

\bibitem[Mehrer et~al.(2020)Mehrer, Spoerer, Kriegeskorte, and
  Kietzmann]{mehrer2020individual}
J.~Mehrer, C.~J. Spoerer, N.~Kriegeskorte, and T.~C. Kietzmann.
\newblock Individual differences among deep neural network models.
\newblock \emph{Nature Communications}, 11\penalty0 (1):\penalty0 1--12, 2020.

\bibitem[Meister et~al.(2020)Meister, Salesky, and
  Cotterell]{meister2020generalized}
C.~Meister, E.~Salesky, and R.~Cotterell.
\newblock Generalized entropy regularization or: Thereâ€™s nothing special
  about label smoothing.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 6870--6886, 2020.

\bibitem[Mianjy and Arora(2020)]{mianjy2020convergence}
P.~Mianjy and R.~Arora.
\newblock On convergence and generalization of dropout training.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[M{\"u}ller et~al.(2019)M{\"u}ller, Kornblith, and
  Hinton]{muller2019does}
R.~M{\"u}ller, S.~Kornblith, and G.~E. Hinton.
\newblock When does label smoothing help?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4696--4705, 2019.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{neyshabur2015norm}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 1376--1401. PMLR,
  2015.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2018pac}
B.~Neyshabur, S.~Bhojanapalli, and N.~Srebro.
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Neyshabur et~al.(2020)Neyshabur, Sedghi, and Zhang]{neyshabur2020what}
B.~Neyshabur, H.~Sedghi, and C.~Zhang.
\newblock What is being transferred in transfer learning?
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Nguyen et~al.(2021)Nguyen, Raghu, and Kornblith]{nguyen2021do}
T.~Nguyen, M.~Raghu, and S.~Kornblith.
\newblock Do wide and deep networks learn the same things? uncovering how
  neural network representations vary with width and depth.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Nilsback and Zisserman(2008)]{nilsback2008automated}
M.-E. Nilsback and A.~Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{Computer Vision, Graphics \& Image Processing, 2008.
  ICVGIP'08. Sixth Indian Conference on}, pages 722--729. IEEE, 2008.

\bibitem[Nocedal(1980)]{nocedal1980updating}
J.~Nocedal.
\newblock Updating quasi-newton matrices with limited storage.
\newblock \emph{Mathematics of computation}, 35\penalty0 (151):\penalty0
  773--782, 1980.

\bibitem[Oreshkin et~al.(2018)Oreshkin, Rodriguez, and
  Lacoste]{oreshkin2018tadam}
B.~N. Oreshkin, P.~Rodriguez, and A.~Lacoste.
\newblock Tadam: Task dependent adaptive metric for improved few-shot learning.
\newblock \emph{arXiv preprint arXiv:1805.10123}, 2018.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan2020prevalence}
V.~Papyan, X.~Han, and D.~L. Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, 2020.

\bibitem[Parkhi et~al.(2012)Parkhi, Vedaldi, Zisserman, and
  Jawahar]{parkhi2012cats}
O.~M. Parkhi, A.~Vedaldi, A.~Zisserman, and C.~Jawahar.
\newblock Cats and dogs.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 3498--3505. IEEE, 2012.

\bibitem[Pereyra et~al.(2017)Pereyra, Tucker, Chorowski, Kaiser, and
  Hinton]{pereyra2017regularizing}
G.~Pereyra, G.~Tucker, J.~Chorowski, {\L}.~Kaiser, and G.~Hinton.
\newblock Regularizing neural networks by penalizing confident output
  distributions.
\newblock \emph{arXiv preprint arXiv:1701.06548}, 2017.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock \emph{arXiv preprint arXiv:2103.00020}, 2021.

\bibitem[Raghu et~al.(2019)Raghu, Zhang, Kleinberg, and
  Bengio]{raghu2019transfusion}
M.~Raghu, C.~Zhang, J.~Kleinberg, and S.~Bengio.
\newblock Transfusion: Understanding transfer learning for medical imaging.
\newblock In \emph{Advances in Neural Information Processing Systems}. Curran
  Associates, Inc., 2019.

\bibitem[Ranjan et~al.(2017)Ranjan, Castillo, and Chellappa]{ranjan2017l2}
R.~Ranjan, C.~D. Castillo, and R.~Chellappa.
\newblock L2-constrained softmax loss for discriminative face verification.
\newblock \emph{arXiv preprint arXiv:1703.09507}, 2017.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and
  Shankar]{recht2019imagenet}
B.~Recht, R.~Roelofs, L.~Schmidt, and V.~Shankar.
\newblock Do {ImageNet} classifiers generalize to {ImageNet}?
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Roth et~al.(2020)Roth, Milbich, Sinha, Gupta, Ommer, and
  Cohen]{roth2020revisiting}
K.~Roth, T.~Milbich, S.~Sinha, P.~Gupta, B.~Ommer, and J.~P. Cohen.
\newblock Revisiting training strategies and generalization performance in deep
  metric learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  8242--8252. PMLR, 2020.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Saxe et~al.(2019)Saxe, Bansal, Dapello, Advani, Kolchinsky, Tracey,
  and Cox]{saxe2019information}
A.~M. Saxe, Y.~Bansal, J.~Dapello, M.~Advani, A.~Kolchinsky, B.~D. Tracey, and
  D.~D. Cox.
\newblock On the information bottleneck theory of deep learning.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2019\penalty0 (12):\penalty0 124020, 2019.

\bibitem[Schilling et~al.(2018)Schilling, Metzner, Rietsch, Gerum, Schulze, and
  Krauss]{schilling2018deep}
A.~Schilling, C.~Metzner, J.~Rietsch, R.~Gerum, H.~Schulze, and P.~Krauss.
\newblock How deep is deep enough?--quantifying class separability in the
  hidden layers of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1811.01753}, 2018.

\bibitem[Sharif~Razavian et~al.(2014)Sharif~Razavian, Azizpour, Sullivan, and
  Carlsson]{sharif2014cnn}
A.~Sharif~Razavian, H.~Azizpour, J.~Sullivan, and S.~Carlsson.
\newblock Cnn features off-the-shelf: an astounding baseline for recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, 2014.

\bibitem[Shwartz-Ziv and Tishby(2017)]{shwartz2017opening}
R.~Shwartz-Ziv and N.~Tishby.
\newblock Opening the black box of deep neural networks via information.
\newblock \emph{arXiv preprint arXiv:1703.00810}, 2017.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
J.~Snell, K.~Swersky, and R.~Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4077--4087, 2017.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
D.~Soudry, E.~Hoffer, M.~S. Nacson, S.~Gunasekar, and N.~Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Sung et~al.(2018)Sung, Yang, Zhang, Xiang, Torr, and
  Hospedales]{sung2018learning}
F.~Sung, Y.~Yang, L.~Zhang, T.~Xiang, P.~H. Torr, and T.~M. Hospedales.
\newblock Learning to compare: Relation network for few-shot learning.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 1199--1208, 2018.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 2818--2826, 2016.

\bibitem[Taori et~al.(2020)Taori, Dave, Shankar, Carlini, Recht, and
  Schmidt]{taori2020measuring}
R.~Taori, A.~Dave, V.~Shankar, N.~Carlini, B.~Recht, and L.~Schmidt.
\newblock Measuring robustness to natural distribution shifts in image
  classification.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Tian et~al.(2020)Tian, Wang, Krishnan, Tenenbaum, and
  Isola]{tian2020rethinking}
Y.~Tian, Y.~Wang, D.~Krishnan, J.~B. Tenenbaum, and P.~Isola.
\newblock Rethinking few-shot image classification: a good embedding is all you
  need?
\newblock In \emph{European Conference on Computer Vision}, 2020.

\bibitem[Wager et~al.(2013)Wager, Wang, and Liang]{wager2013dropout}
S.~Wager, S.~Wang, and P.~S. Liang.
\newblock Dropout training as adaptive regularization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  351--359, 2013.

\bibitem[Wager et~al.(2014)Wager, Fithian, Wang, and Liang]{wager2014altitude}
S.~Wager, W.~Fithian, S.~Wang, and P.~S. Liang.
\newblock Altitude training: Strong bounds for single-layer dropout.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Wang et~al.(2017)Wang, Xiang, Cheng, and Yuille]{wang2017normface}
F.~Wang, X.~Xiang, J.~Cheng, and A.~L. Yuille.
\newblock Normface: L2 hypersphere embedding for face verification.
\newblock In \emph{Proceedings of the 25th ACM international Conference on
  Multimedia}, pages 1041--1049, 2017.

\bibitem[Wang et~al.(2018{\natexlab{a}})Wang, Cheng, Liu, and
  Liu]{wang2018additive}
F.~Wang, J.~Cheng, W.~Liu, and H.~Liu.
\newblock Additive margin softmax for face verification.
\newblock \emph{IEEE Signal Processing Letters}, 25\penalty0 (7):\penalty0
  926--930, 2018{\natexlab{a}}.

\bibitem[Wang et~al.(2018{\natexlab{b}})Wang, Wang, Zhou, Ji, Gong, Zhou, Li,
  and Liu]{wang2018cosface}
H.~Wang, Y.~Wang, Z.~Zhou, X.~Ji, D.~Gong, J.~Zhou, Z.~Li, and W.~Liu.
\newblock Cosface: Large margin cosine loss for deep face recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 5265--5274, 2018{\natexlab{b}}.

\bibitem[Wang et~al.(2019)Wang, Ge, Xing, and Lipton]{wang2019learning}
H.~Wang, S.~Ge, E.~P. Xing, and Z.~C. Lipton.
\newblock Learning robust global representations by penalizing local predictive
  power.
\newblock \emph{arXiv preprint arXiv:1905.13549}, 2019.

\bibitem[Wei et~al.(2019)Wei, Lee, Liu, and Ma]{wei2019regularization}
C.~Wei, J.~D. Lee, Q.~Liu, and T.~Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9712--9724, 2019.

\bibitem[Wei et~al.(2020)Wei, Kakade, and Ma]{wei2020implicit}
C.~Wei, S.~Kakade, and T.~Ma.
\newblock The implicit and explicit regularization effects of dropout.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Wojke and Bewley(2018)]{wojke2018deep}
N.~Wojke and A.~Bewley.
\newblock Deep cosine metric learning for person re-identification.
\newblock In \emph{2018 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pages 748--756. IEEE, 2018.

\bibitem[Xiao et~al.(2010)Xiao, Hays, Ehinger, Oliva, and
  Torralba]{xiao2010sun}
J.~Xiao, J.~Hays, K.~A. Ehinger, A.~Oliva, and A.~Torralba.
\newblock Sun database: Large-scale scene recognition from abbey to zoo.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 3485--3492. IEEE, 2010.

\bibitem[Xu et~al.(2020)Xu, Xu, Qian, Li, and Jin]{xu2020towards}
Y.~Xu, Y.~Xu, Q.~Qian, H.~Li, and R.~Jin.
\newblock Towards understanding label smoothing.
\newblock \emph{arXiv preprint arXiv:2006.11653}, 2020.

\bibitem[Yosinski et~al.(2014)Yosinski, Clune, Bengio, and
  Lipson]{yosinski2014how}
J.~Yosinski, J.~Clune, Y.~Bengio, and H.~Lipson.
\newblock How transferable are features in deep neural networks?
\newblock In \emph{Neural Information Processing Systems}, volume~27. Curran
  Associates, Inc., 2014.

\bibitem[Zhai et~al.(2021)Zhai, Kolesnikov, Houlsby, and
  Beyer]{zhai2021scaling}
X.~Zhai, A.~Kolesnikov, N.~Houlsby, and L.~Beyer.
\newblock Scaling vision transformers.
\newblock \emph{arXiv preprint arXiv:2106.04560}, 2021.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\end{thebibliography}
