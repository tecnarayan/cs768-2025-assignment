\begin{thebibliography}{10}

\bibitem{dawid2014theory}
A.~P. Dawid and M.~Musio.
\newblock Theory and applications of proper scoring rules.
\newblock {\em Metron}, 72(2):169--183, 2014.

\bibitem{dinh2016density}
L.~Dinh, J.~Sohl-Dickstein, and S.~Bengio.
\newblock Density estimation using real nvp.
\newblock {\em arXiv preprint arXiv:1605.08803}, 2016.

\bibitem{du2019implicit}
Y.~Du and I.~Mordatch.
\newblock Implicit generation and generalization in energy-based models.
\newblock {\em arXiv preprint arXiv:1903.08689}, 2019.

\bibitem{germain2015made}
M.~Germain, K.~Gregor, I.~Murray, and H.~Larochelle.
\newblock Made: Masked autoencoder for distribution estimation.
\newblock In {\em International Conference on Machine Learning}, pages
  881--889, 2015.

\bibitem{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, pages
  2672--2680, 2014.

\bibitem{grenander1994representations}
U.~Grenander and M.~I. Miller.
\newblock Representations of knowledge in complex systems.
\newblock {\em Journal of the Royal Statistical Society: Series B
  (Methodological)}, 56(4):549--581, 1994.

\bibitem{heusel2017gans}
M.~Heusel, H.~Ramsauer, T.~Unterthiner, B.~Nessler, and S.~Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock In {\em Advances in neural information processing systems}, pages
  6626--6637, 2017.

\bibitem{huszar2017variational}
F.~Husz{\'a}r.
\newblock Variational inference using implicit distributions.
\newblock {\em arXiv preprint arXiv:1702.08235}, 2017.

\bibitem{hyvarinen2005estimation}
A.~Hyv{\"a}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock {\em Journal of Machine Learning Research}, 6(Apr):695--709, 2005.

\bibitem{kingma2018glow}
D.~P. Kingma and P.~Dhariwal.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10215--10224, 2018.

\bibitem{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{liu2015deep}
Z.~Liu, P.~Luo, X.~Wang, and X.~Tang.
\newblock Deep learning face attributes in the wild.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 3730--3738, 2015.

\bibitem{martens2012estimating}
J.~Martens, I.~Sutskever, and K.~Swersky.
\newblock Estimating the hessian by back-propagating curvature.
\newblock {\em arXiv preprint arXiv:1206.6464}, 2012.

\bibitem{nash2019autoregressive}
C.~Nash and C.~Durkan.
\newblock Autoregressive energy machines.
\newblock {\em arXiv preprint arXiv:1904.05626}, 2019.

\bibitem{neal2001annealed}
R.~M. Neal.
\newblock Annealed importance sampling.
\newblock {\em Statistics and computing}, 11(2):125--139, 2001.

\bibitem{oord2016wavenet}
A.~v.~d. Oord, S.~Dieleman, H.~Zen, K.~Simonyan, O.~Vinyals, A.~Graves,
  N.~Kalchbrenner, A.~Senior, and K.~Kavukcuoglu.
\newblock Wavenet: A generative model for raw audio.
\newblock {\em arXiv preprint arXiv:1609.03499}, 2016.

\bibitem{oord2016pixel}
A.~v.~d. Oord, N.~Kalchbrenner, and K.~Kavukcuoglu.
\newblock Pixel recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1601.06759}, 2016.

\bibitem{parisi1981correlation}
G.~Parisi.
\newblock Correlation functions and computer simulations.
\newblock {\em Nuclear Physics B}, 180(3):378--384, 1981.

\bibitem{roberts1996exponential}
G.~O. Roberts, R.~L. Tweedie, et~al.
\newblock Exponential convergence of langevin distributions and their discrete
  approximations.
\newblock {\em Bernoulli}, 2(4):341--363, 1996.

\bibitem{salimans2017pixelcnn++}
T.~Salimans, A.~Karpathy, X.~Chen, and D.~P. Kingma.
\newblock Pixelcnn++: Improving the pixelcnn with discretized logistic mixture
  likelihood and other modifications.
\newblock {\em arXiv preprint arXiv:1701.05517}, 2017.

\bibitem{saremi2018deep}
S.~Saremi, A.~Mehrjou, B.~Sch{\"o}lkopf, and A.~Hyv{\"a}rinen.
\newblock Deep energy estimator networks.
\newblock {\em arXiv preprint arXiv:1805.08306}, 2018.

\bibitem{shi2018spectral}
J.~Shi, S.~Sun, and J.~Zhu.
\newblock A spectral approach to gradient estimation for implicit
  distributions.
\newblock {\em arXiv preprint arXiv:1806.02925}, 2018.

\bibitem{song2019generative}
Y.~Song and S.~Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11895--11907, 2019.

\bibitem{song2019sliced}
Y.~Song, S.~Garg, J.~Shi, and S.~Ermon.
\newblock Sliced score matching: {A} scalable approach to density and score
  estimation.
\newblock In {\em Proceedings of the Thirty-Fifth Conference on Uncertainty in
  Artificial Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  page 204, 2019.

\bibitem{stein1981estimation}
C.~M. Stein.
\newblock Estimation of the mean of a multivariate normal distribution.
\newblock {\em The annals of Statistics}, pages 1135--1151, 1981.

\bibitem{van2016conditional}
A.~Van~den Oord, N.~Kalchbrenner, L.~Espeholt, O.~Vinyals, A.~Graves, et~al.
\newblock Conditional image generation with pixelcnn decoders.
\newblock In {\em Advances in neural information processing systems}, pages
  4790--4798, 2016.

\bibitem{vincent2011connection}
P.~Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock {\em Neural computation}, 23(7):1661--1674, 2011.

\bibitem{vinyals2019grandmaster}
O.~Vinyals, I.~Babuschkin, W.~M. Czarnecki, M.~Mathieu, A.~Dudzik, J.~Chung,
  D.~H. Choi, R.~Powell, T.~Ewalds, P.~Georgiev, et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock {\em Nature}, 575(7782):350--354, 2019.

\bibitem{welling2011bayesian}
M.~Welling and Y.~W. Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In {\em Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688, 2011.

\bibitem{yu2020training}
L.~Yu, Y.~Song, J.~Song, and S.~Ermon.
\newblock Training deep energy-based models with f-divergence minimization.
\newblock {\em arXiv preprint arXiv:2003.03463}, 2020.

\end{thebibliography}
