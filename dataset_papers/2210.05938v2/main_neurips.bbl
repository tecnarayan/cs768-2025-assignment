\begin{thebibliography}{90}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Addepalli et~al.(2021)Addepalli, Jain, Sriramanan, Khare, and
  Radhakrishnan]{addepalli2021towards}
Sravanti Addepalli, Samyak Jain, Gaurang Sriramanan, Shivangi Khare, and
  Venkatesh~Babu Radhakrishnan.
\newblock Towards achieving adversarial robustness beyond perceptual limits.
\newblock In \emph{ICML 2021 Workshop on Adversarial Machine Learning}, 2021.
\newblock URL \url{https://openreview.net/forum?id=SHB_znlW5G7}.

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and
  Mané]{amodei2016concrete}
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and
  Dan Mané.
\newblock Concrete problems in ai safety, 2016.
\newblock URL \url{https://arxiv.org/abs/1606.06565}.

\bibitem[Andriushchenko and Flammarion(2020)]{andriushchenko2020understanding}
Maksym Andriushchenko and Nicolas Flammarion.
\newblock Understanding and improving fast adversarial training.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 16048--16059, 2020.

\bibitem[Andriushchenko et~al.(2020)Andriushchenko, Croce, Flammarion, and
  Hein]{andriushchenko2020square}
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein.
\newblock Square attack: a query-efficient black-box adversarial attack via
  random search.
\newblock In \emph{European Conference on Computer Vision}, pages 484--501.
  Springer, 2020.

\bibitem[Carlini and Wagner(2017)]{c_and_w}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{2017 ieee symposium on security and privacy (sp)}, pages
  39--57. IEEE, 2017.

\bibitem[Carmon et~al.(2019)Carmon, Raghunathan, Schmidt, Duchi, and
  Liang]{carmon2019unlabeled}
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John~C Duchi, and Percy~S
  Liang.
\newblock Unlabeled data improves adversarial robustness.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Carmon et~al.(2022)Carmon, Raghunathan, Schmidt, Liang, and
  Duchi]{carmon2022unlabeled}
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John~C. Duchi.
\newblock Unlabeled data improves adversarial robustness, 2022.

\bibitem[Chen and Lee(2021)]{chen2021ltd}
Erh-Chung Chen and Che-Rung Lee.
\newblock Ltd: Low temperature distillation for robust adversarial training,
  2021.

\bibitem[Chen et~al.(2021)Chen, Cheng, Gan, Gu, and Liu]{chen2021efficient}
Jinghui Chen, Yu~Cheng, Zhe Gan, Quanquan Gu, and Jingjing Liu.
\newblock Efficient robust training via backward smoothing, 2021.

\bibitem[Chen et~al.(2017)Chen, Zhang, Sharma, Yi, and Hsieh]{chen2017zoo}
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh.
\newblock Zoo: Zeroth order optimization based black-box attacks to deep neural
  networks without training substitute models.
\newblock In \emph{Proceedings of the 10th ACM workshop on artificial
  intelligence and security}, pages 15--26, 2017.

\bibitem[Chen et~al.(2020)Chen, Liu, Chang, Cheng, Amini, and
  Wang]{chen2020adversarial}
Tianlong Chen, Sijia Liu, Shiyu Chang, Yu~Cheng, Lisa Amini, and Zhangyang
  Wang.
\newblock Adversarial robustness: From self-supervised pre-training to
  fine-tuning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 699--708, 2020.

\bibitem[Cohen et~al.(2020)Cohen, Sapiro, and Giryes]{cohen2020detecting}
Gilad Cohen, Guillermo Sapiro, and Raja Giryes.
\newblock Detecting adversarial samples using influence functions and nearest
  neighbors.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 14453--14462, 2020.

\bibitem[Corbi{\`e}re et~al.(2019)Corbi{\`e}re, Thome, Bar-Hen, Cord, and
  P{\'e}rez]{corbiere2019addressing}
Charles Corbi{\`e}re, Nicolas Thome, Avner Bar-Hen, Matthieu Cord, and Patrick
  P{\'e}rez.
\newblock Addressing failure prediction by learning model confidence.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Croce and Hein(2020)]{auto_attack}
Francesco Croce and Matthias Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In \emph{ICML}, 2020.

\bibitem[Croce et~al.(2020)Croce, Andriushchenko, Sehwag, Flammarion, Chiang,
  Mittal, and Hein]{robust_bench}
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Nicolas Flammarion, Mung
  Chiang, Prateek Mittal, and Matthias Hein.
\newblock Robustbench: a standardized adversarial robustness benchmark.
\newblock \emph{arXiv preprint arXiv:2010.09670}, 2020.

\bibitem[Cui et~al.(2021)Cui, Liu, Wang, and Jia]{cui2021learnable}
Jiequan Cui, Shu Liu, Liwei Wang, and Jiaya Jia.
\newblock Learnable boundary guided adversarial training, 2021.

\bibitem[Dai et~al.(2021)Dai, Mahloujifar, and Mittal]{dai2021parameterizing}
Sihui Dai, Saeed Mahloujifar, and Prateek Mittal.
\newblock Parameterizing activation functions for adversarial robustness, 2021.

\bibitem[DeGroot and Fienberg(1983)]{degroot1983comparison}
Morris~H DeGroot and Stephen~E Fienberg.
\newblock The comparison and evaluation of forecasters.
\newblock \emph{Journal of the Royal Statistical Society: Series D (The
  Statistician)}, 32\penalty0 (1-2):\penalty0 12--22, 1983.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 248--255, 2009.
\newblock \doi{10.1109/CVPR.2009.5206848}.

\bibitem[DeVries and Taylor(2017)]{devries2017improved}
Terrance DeVries and Graham~W Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock \emph{arXiv preprint arXiv:1708.04552}, 2017.

\bibitem[Ding et~al.(2020)Ding, Sharma, Lui, and Huang]{Ding2020MMA}
Gavin~Weiguang Ding, Yash Sharma, Kry Yik~Chau Lui, and Ruitong Huang.
\newblock Mma training: Direct input space margin maximization through
  adversarial training.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=HkeryxBtPB}.

\bibitem[Engstrom et~al.(2019)Engstrom, Ilyas, Salman, Santurkar, and
  Tsipras]{robustness_github}
Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris
  Tsipras.
\newblock Robustness (python library), 2019.
\newblock URL \url{https://github.com/MadryLab/robustness}.

\bibitem[Feinman et~al.(2017)Feinman, Curtin, Shintre, and
  Gardner]{feinman2017detecting}
Reuben Feinman, Ryan~R Curtin, Saurabh Shintre, and Andrew~B Gardner.
\newblock Detecting adversarial samples from artifacts.
\newblock \emph{arXiv preprint arXiv:1703.00410}, 2017.

\bibitem[Gavrikov and Keuper(2022)]{Gavrikov_2022_CVPR}
Paul Gavrikov and Janis Keuper.
\newblock Adversarial robustness through the lens of convolutional filters.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR) Workshops}, pages 139--147, June 2022.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and Szegedy]{harnesssing}
Ian~J. Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples, 2015.

\bibitem[Gowal et~al.(2021{\natexlab{a}})Gowal, Qin, Uesato, Mann, and
  Kohli]{gowal2021uncovering}
Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli.
\newblock Uncovering the limits of adversarial training against norm-bounded
  adversarial examples, 2021{\natexlab{a}}.

\bibitem[Gowal et~al.(2021{\natexlab{b}})Gowal, Rebuffi, Wiles, Stimberg,
  Calian, and Mann]{gowal2021improving}
Sven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg,
  Dan~Andrei Calian, and Timothy~A Mann.
\newblock Improving robustness using generated data.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{b}}.

\bibitem[Grabinski et~al.(2022{\natexlab{a}})Grabinski, Jung, Keuper, and
  Keuper]{grabinski2022frequencylowcut}
Julia Grabinski, Steffen Jung, Janis Keuper, and Margret Keuper.
\newblock Frequencylowcut pooling--plug \& play against catastrophic
  overfitting.
\newblock \emph{arXiv preprint arXiv:2204.00491}, 2022{\natexlab{a}}.

\bibitem[Grabinski et~al.(2022{\natexlab{b}})Grabinski, Keuper, and
  Keuper]{grabinski2022aliasing}
Julia Grabinski, Janis Keuper, and Margret Keuper.
\newblock Aliasing and adversarial robust generalization of cnns.
\newblock \emph{Machine Learning}, pages 1--27, 2022{\natexlab{b}}.

\bibitem[Grosse et~al.(2017)Grosse, Manoharan, Papernot, Backes, and
  McDaniel]{grosse2017statistical}
Kathrin Grosse, Praveen Manoharan, Nicolas Papernot, Michael Backes, and
  Patrick McDaniel.
\newblock On the (statistical) detection of adversarial examples.
\newblock \emph{arXiv preprint arXiv:1702.06280}, 2017.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q. Weinberger.
\newblock On calibration of modern neural networks.
\newblock In Doina Precup and Yee~Whye Teh, editors, \emph{Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pages 1321--1330. PMLR,
  06--11 Aug 2017.
\newblock URL \url{https://proceedings.mlr.press/v70/guo17a.html}.

\bibitem[Gurau et~al.(2018)Gurau, Bewley, and Posner]{gurau2018dropout}
Corina Gurau, Alex Bewley, and Ingmar Posner.
\newblock Dropout distillation for efficiently estimating model confidence.
\newblock \emph{arXiv preprint arXiv:1809.10562}, 2018.

\bibitem[Harder et~al.(2021)Harder, Pfreundt, Keuper, and
  Keuper]{harder2021spectraldefense}
Paula Harder, Franz-Josef Pfreundt, Margret Keuper, and Janis Keuper.
\newblock Spectraldefense: Detecting adversarial attacks on cnns in the fourier
  domain.
\newblock In \emph{2021 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--8. IEEE, 2021.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition, 2015.

\bibitem[Hein et~al.(2019)Hein, Andriushchenko, and Bitterwolf]{hein2019relu}
Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf.
\newblock Why relu networks yield high-confidence predictions far away from the
  training data and how to mitigate the problem, 2019.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019robustness}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations}, 2019.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016early}
Dan Hendrycks and Kevin Gimpel.
\newblock Early methods for detecting adversarial images.
\newblock \emph{arXiv preprint arXiv:1608.00530}, 2016.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Lee, and Mazeika]{hendrycks2019using}
Dan Hendrycks, Kimin Lee, and Mantas Mazeika.
\newblock Using pre-training can improve model robustness and uncertainty.
\newblock In \emph{International Conference on Machine Learning}, pages
  2712--2721. PMLR, 2019.

\bibitem[Huang et~al.(2017)Huang, Liu, van~der Maaten, and
  Weinberger]{densenet}
Gao Huang, Zhuang Liu, Laurens van~der Maaten, and Kilian~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, July 2017.

\bibitem[Huang et~al.(2022)Huang, Wang, Erfani, Gu, Bailey, and
  Ma]{huang2022exploring}
Hanxun Huang, Yisen Wang, Sarah~Monazam Erfani, Quanquan Gu, James Bailey, and
  Xingjun Ma.
\newblock Exploring architectural ingredients of adversarially robust deep
  neural networks, 2022.

\bibitem[Huang et~al.(2020)Huang, Zhang, and Zhang]{huang2020selfadaptive}
Lang Huang, Chao Zhang, and Hongyang Zhang.
\newblock Self-adaptive training: beyond empirical risk minimization, 2020.

\bibitem[Ilyas et~al.(2018)Ilyas, Engstrom, Athalye, and Lin]{ilyas2018black}
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin.
\newblock Black-box adversarial attacks with limited queries and information.
\newblock In \emph{International Conference on Machine Learning}, pages
  2137--2146. PMLR, 2018.

\bibitem[Krizhevsky(2012)]{cifar}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{University of Toronto}, 05 2012.

\bibitem[Kurakin et~al.(2017)Kurakin, Goodfellow, and Bengio]{pgd}
Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
\newblock Adversarial machine learning at scale, 2017.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Lee et~al.(2018)Lee, Lee, Lee, and Shin]{lee2018simple}
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.
\newblock A simple unified framework for detecting out-of-distribution samples
  and adversarial attacks, 2018.
\newblock URL \url{https://arxiv.org/abs/1807.03888}.

\bibitem[Li and Li(2017)]{li2017adversarial}
Xin Li and Fuxin Li.
\newblock Adversarial examples detection in deep networks with convolutional
  filter statistics.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 5764--5772, 2017.

\bibitem[Li and Hoiem(2020)]{li2020improving}
Zhizhong Li and Derek Hoiem.
\newblock Improving confidence estimates for unfamiliar examples.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 2686--2695, 2020.

\bibitem[Lorenz et~al.(2021)Lorenz, Harder, Stra{\ss}el, Keuper, and
  Keuper]{lorenz2021detecting}
Peter Lorenz, Paula Harder, Dominik Stra{\ss}el, Margret Keuper, and Janis
  Keuper.
\newblock Detecting autoattack perturbations in the frequency domain.
\newblock In \emph{ICML 2021 Workshop on Adversarial Machine Learning}, 2021.
\newblock URL \url{https://openreview.net/forum?id=8uWOTxbwo-Z}.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Metzen et~al.(2017)Metzen, Genewein, Fischer, and
  Bischoff]{metzen2017detecting}
Jan~Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff.
\newblock On detecting adversarial perturbations.
\newblock \emph{arXiv preprint arXiv:1702.04267}, 2017.

\bibitem[Moon et~al.(2020)Moon, Kim, Shin, and Hwang]{moon2020confidence}
Jooyoung Moon, Jihyo Kim, Younghak Shin, and Sangheum Hwang.
\newblock Confidence-aware learning for deep neural networks.
\newblock In \emph{international conference on machine learning}, pages
  7034--7044. PMLR, 2020.

\bibitem[Moosavi-Dezfooli et~al.(2016)Moosavi-Dezfooli, Fawzi, and
  Frossard]{deepfool}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard.
\newblock Deepfool: a simple and accurate method to fool deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2574--2582, 2016.

\bibitem[M{\"u}ller et~al.(2019)M{\"u}ller, Kornblith, and
  Hinton]{muller2019does}
Rafael M{\"u}ller, Simon Kornblith, and Geoffrey~E Hinton.
\newblock When does label smoothing help?
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Mund et~al.(2015)Mund, Triebel, and Cremers]{mund2015active}
Dennis Mund, Rudolph Triebel, and Daniel Cremers.
\newblock Active online confidence boosting for efficient object
  classification.
\newblock In \emph{2015 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 1367--1373, 2015.
\newblock \doi{10.1109/ICRA.2015.7139368}.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and
  Hauskrecht]{naeini2015obtaining}
Mahdi~Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht.
\newblock Obtaining well calibrated probabilities using bayesian binning.
\newblock In \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Nguyen et~al.(2015)Nguyen, Yosinski, and Clune]{nguyen2015deep}
Anh Nguyen, Jason Yosinski, and Jeff Clune.
\newblock Deep neural networks are easily fooled: High confidence predictions
  for unrecognizable images.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 427--436, 2015.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019can}
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian
  Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Pang et~al.(2020)Pang, Yang, Dong, Xu, Zhu, and Su]{pang2020boosting}
Tianyu Pang, Xiao Yang, Yinpeng Dong, Kun Xu, Jun Zhu, and Hang Su.
\newblock Boosting adversarial training with hypersphere embedding.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7779--7792, 2020.

\bibitem[Qin et~al.(2021)Qin, Wang, Beutel, and Chi]{qin2021improving}
Yao Qin, Xuezhi Wang, Alex Beutel, and Ed~Chi.
\newblock Improving calibration through the relationship with adversarial
  robustness.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 14358--14369, 2021.

\bibitem[Rade and Moosavi-Dezfooli(2021)]{rade2021helperbased}
Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli.
\newblock Helper-based adversarial training: Reducing excessive margin to
  achieve a better accuracy vs. robustness trade-off.
\newblock In \emph{ICML 2021 Workshop on Adversarial Machine Learning}, 2021.
\newblock URL \url{https://openreview.net/forum?id=BuD2LmNaU3a}.

\bibitem[Rebuffi et~al.(2021)Rebuffi, Gowal, Calian, Stimberg, Wiles, and
  Mann]{rebuffi2021fixing}
Sylvestre-Alvise Rebuffi, Sven Gowal, Dan~A. Calian, Florian Stimberg, Olivia
  Wiles, and Timothy Mann.
\newblock Fixing data augmentation to improve adversarial robustness, 2021.

\bibitem[Reed et~al.(2014)Reed, Lee, Anguelov, Szegedy, Erhan, and
  Rabinovich]{reed2014training}
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan,
  and Andrew Rabinovich.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock \emph{arXiv preprint arXiv:1412.6596}, 2014.

\bibitem[Rice et~al.(2020)Rice, Wong, and Kolter]{rice2020overfitting}
Leslie Rice, Eric Wong, and Zico Kolter.
\newblock Overfitting in adversarially robust deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  8093--8104. PMLR, 2020.

\bibitem[Rony et~al.(2019)Rony, Hafemann, Oliveira, Ayed, Sabourin, and
  Granger]{decoupling}
J{\'e}r{\^o}me Rony, Luiz~G Hafemann, Luiz~S Oliveira, Ismail~Ben Ayed, Robert
  Sabourin, and Eric Granger.
\newblock Decoupling direction and norm for efficient gradient-based l2
  adversarial attacks and defenses.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 4322--4330, 2019.

\bibitem[Salman et~al.(2020)Salman, Ilyas, Engstrom, Kapoor, and
  Madry]{salman2020adversarially}
Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry.
\newblock Do adversarially robust imagenet models transfer better?
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3533--3545, 2020.

\bibitem[Sehwag et~al.(2020)Sehwag, Wang, Mittal, and Jana]{sehwag2020hydra}
Vikash Sehwag, Shiqi Wang, Prateek Mittal, and Suman Jana.
\newblock Hydra: Pruning adversarially robust neural networks, 2020.

\bibitem[Sehwag et~al.(2021)Sehwag, Mahloujifar, Handina, Dai, Xiang, Chiang,
  and Mittal]{sehwag2021robust}
Vikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong Xiang, Mung
  Chiang, and Prateek Mittal.
\newblock Robust learning meets generative models: Can proxy distributions
  improve adversarial robustness?, 2021.

\bibitem[Simonyan and Zisserman(2015)]{vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition,
  2015.

\bibitem[Sitawarin et~al.(2021)Sitawarin, Chakraborty, and
  Wagner]{sitawarin2021sat}
Chawin Sitawarin, Supriyo Chakraborty, and David Wagner.
\newblock Sat: Improving adversarial training via curriculum-based loss
  smoothing, 2021.

\bibitem[Sridhar et~al.(2021)Sridhar, Sokolsky, Lee, and
  Weimer]{sridhar2021improving}
Kaustubh Sridhar, Oleg Sokolsky, Insup Lee, and James Weimer.
\newblock Improving neural network robustness via persistency of excitation,
  2021.

\bibitem[Stutz et~al.(2020)Stutz, Hein, and Schiele]{stutz2020confidence}
David Stutz, Matthias Hein, and Bernt Schiele.
\newblock Confidence-calibrated adversarial training: Generalizing to unseen
  attacks.
\newblock In \emph{International Conference on Machine Learning}, pages
  9155--9166. PMLR, 2020.

\bibitem[Szegedy et~al.(2014{\natexlab{a}})Szegedy, Liu, Jia, Sermanet, Reed,
  Anguelov, Erhan, Vanhoucke, and Rabinovich]{inception}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions, 2014{\natexlab{a}}.

\bibitem[Szegedy et~al.(2014{\natexlab{b}})Szegedy, Zaremba, Sutskever, Bruna,
  Erhan, Goodfellow, and Fergus]{intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{International Conference on Learning Representations},
  2014{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1312.6199}.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem[Thulasidasan et~al.(2019)Thulasidasan, Chennupati, Bilmes,
  Bhattacharya, and Michalak]{thulasidasan2019mixup}
Sunil Thulasidasan, Gopinath Chennupati, Jeff~A Bilmes, Tanmoy Bhattacharya,
  and Sarah Michalak.
\newblock On mixup training: Improved calibration and predictive uncertainty
  for deep neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Tomani and Buettner(2021)]{tomani2021towards}
Christian Tomani and Florian Buettner.
\newblock Towards trustworthy predictions from deep neural networks with fast
  adversarial calibration.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 9886--9896, 2021.

\bibitem[Tu et~al.(2019)Tu, Ting, Chen, Liu, Zhang, Yi, Hsieh, and
  Cheng]{tu2019autozoom}
Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi,
  Cho-Jui Hsieh, and Shin-Ming Cheng.
\newblock Autozoom: Autoencoder-based zeroth order optimization method for
  attacking black-box neural networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 742--749, 2019.

\bibitem[Varshney and Alemzadeh(2017)]{varshney2017safety}
Kush~R Varshney and Homa Alemzadeh.
\newblock On the safety of machine learning: Cyber-physical systems, decision
  sciences, and data products.
\newblock \emph{Big data}, 5\penalty0 (3):\penalty0 246--255, 2017.

\bibitem[Wang et~al.(2020)Wang, Zou, Yi, Bailey, Ma, and Gu]{Wang2020Improving}
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu.
\newblock Improving adversarial robustness requires revisiting misclassified
  examples.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rklOg6EFwS}.

\bibitem[Wenger et~al.(2020)Wenger, Kjellstr{\"o}m, and Triebel]{wenger2020non}
Jonathan Wenger, Hedvig Kjellstr{\"o}m, and Rudolph Triebel.
\newblock Non-parametric calibration for classification.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 178--190. PMLR, 2020.

\bibitem[Wightman(2019)]{rw2019timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Wong et~al.(2020)Wong, Rice, and Kolter]{wong2020fast}
Eric Wong, Leslie Rice, and J.~Zico Kolter.
\newblock Fast is better than free: Revisiting adversarial training.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJx040EFvH}.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{wu2020adversarial}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2958--2969, 2020.

\bibitem[Xie and Yuille(2019)]{xie2019intriguing}
Cihang Xie and Alan Yuille.
\newblock Intriguing properties of adversarial training at scale.
\newblock \emph{arXiv preprint arXiv:1906.03787}, 2019.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Zhang, Lu, Zhu, and
  Dong]{zhang2019propagate}
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong.
\newblock You only propagate once: Accelerating adversarial training via
  maximal principle, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Yu, Jiao, Xing, Ghaoui, and
  Jordan]{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric~P. Xing, Laurent~El Ghaoui, and
  Michael~I. Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In \emph{International Conference on Machine Learning},
  2019{\natexlab{b}}.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhang et~al.(2020)Zhang, Xu, Han, Niu, Cui, Sugiyama, and
  Kankanhalli]{zhang2020attacks}
Jingfeng Zhang, Xilie Xu, Bo~Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and
  Mohan Kankanhalli.
\newblock Attacks which do not kill training make adversarial learning
  stronger, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Zhu, Niu, Han, Sugiyama, and
  Kankanhalli]{zhang2021geometryaware}
Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo~Han, Masashi Sugiyama, and Mohan
  Kankanhalli.
\newblock Geometry-aware instance-reweighted adversarial training.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=iAX0l6Cz8ub}.

\end{thebibliography}
