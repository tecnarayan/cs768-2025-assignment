\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro, Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia, Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah, Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan, Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng]{tensorflow2015-whitepaper}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.~S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., Levenberg, J., Man\'{e}, D., Monga, R., Moore, S., Murray, D., Olah, C., Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P., Vanhoucke, V., Vasudevan, V., Vi\'{e}gas, F., Vinyals, O., Warden, P., Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems, 2015.
\newblock URL \url{http://tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Ahn et~al.(2023)Ahn, Cheng, Song, Yun, Jadbabaie, and Sra]{ahn2023linear}
Ahn, K., Cheng, X., Song, M., Yun, C., Jadbabaie, A., and Sra, S.
\newblock Linear attention is (maybe) all you need (to understand transformer optimization), 2023.

\bibitem[Anagnostidis et~al.(2022)Anagnostidis, Biggio, Noci, Orvieto, Singh, and Lucchi]{anagnostidis2022signal}
Anagnostidis, S., Biggio, L., Noci, L., Orvieto, A., Singh, S.~P., and Lucchi, A.
\newblock Signal propagation in transformers: Theoretical perspectives and the role of rank collapse.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=FxVH7iToXS}.

\bibitem[Box \& Jenkins(1990)Box and Jenkins]{box1990arima}
Box, G. E.~P. and Jenkins, G.
\newblock \emph{Time Series Analysis, Forecasting and Control}.
\newblock Holden-Day, Inc., USA, 1990.
\newblock ISBN 0816211043.

\bibitem[Box et~al.(1974)Box, Jenkins, and MacGregor]{box1974forecasting}
Box, G. E.~P., Jenkins, G.~M., and MacGregor, J.~F.
\newblock {Some Recent Advances in Forecasting and Control}.
\newblock \emph{Journal of the Royal Statistical Society Series C}, 23\penalty0 (2):\penalty0 158--179, June 1974.
\newblock \doi{10.2307/2346997}.
\newblock URL \url{https://ideas.repec.org/a/bla/jorssc/v23y1974i2p158-179.html}.

\bibitem[{California Department of Transportation}(2021)]{traffic}
{California Department of Transportation}.
\newblock Traffic dataset, 2021.
\newblock URL \url{https://pems.dot.ca.gov/}.

\bibitem[Cand\`{e}s \& Recht(2012)Cand\`{e}s and Recht]{candes2012matrixcompletion}
Cand\`{e}s, E. and Recht, B.
\newblock Exact matrix completion via convex optimization.
\newblock \emph{Commun. ACM}, 55\penalty0 (6):\penalty0 111–119, jun 2012.
\newblock ISSN 0001-0782.
\newblock \doi{10.1145/2184319.2184343}.
\newblock URL \url{https://doi.org/10.1145/2184319.2184343}.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J\'egou, Mairal, Bojanowski, and Joulin]{caron2021emerging}
Caron, M., Touvron, H., Misra, I., J\'egou, H., Mairal, J., Bojanowski, P., and Joulin, A.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{Proceedings of the International Conference on Computer Vision (ICCV)}, 2021.

\bibitem[Casolaro et~al.(2023)Casolaro, Capone, Iannuzzo, and Camastra]{casolaro2023survey}
Casolaro, A., Capone, V., Iannuzzo, G., and Camastra, F.
\newblock Deep learning for time series forecasting: Advances and open problems.
\newblock \emph{Information}, 14\penalty0 (11), 2023.
\newblock ISSN 2078-2489.
\newblock \doi{10.3390/info14110598}.
\newblock URL \url{https://www.mdpi.com/2078-2489/14/11/598}.

\bibitem[{\v{C}}epulionis \& Luko{\v{s}}evi{\v{c}}i{\={u}}t{\.{e}}(2016){\v{C}}epulionis and Luko{\v{s}}evi{\v{c}}i{\={u}}t{\.{e}}]{cepulionis2016electro}
{\v{C}}epulionis, P. and Luko{\v{s}}evi{\v{c}}i{\={u}}t{\.{e}}, K.
\newblock Electrocardiogram time series forecasting and optimization using ant colony optimization algorithm.
\newblock \emph{Mathematical Models in Engineering}, 2\penalty0 (1):\penalty0 69--77, Jun 2016.
\newblock ISSN 2351-5279.
\newblock URL \url{https://www.extrica.com/article/17229}.

\bibitem[Chaudhari et~al.(2017)Chaudhari, Choromanska, Soatto, LeCun, Baldassi, Borgs, Chayes, Sagun, and Zecchina]{chaudhari2017entropysgd}
Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C., Chayes, J., Sagun, L., and Zecchina, R.
\newblock Entropy-{SGD}: Biasing gradient descent into wide valleys.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=B1YfAfcgl}.

\bibitem[Chen \& Tao(2021)Chen and Tao]{chen2021hamiltonian}
Chen, R. and Tao, M.
\newblock Data-driven prediction of general hamiltonian dynamics via learning exactly-symplectic maps.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  1717--1727. PMLR, 18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/chen21r.html}.

\bibitem[Chen et~al.(2023)Chen, Li, Arik, Yoder, and Pfister]{chen2023tsmixer}
Chen, S.-A., Li, C.-L., Arik, S.~O., Yoder, N.~C., and Pfister, T.
\newblock {TSM}ixer: An all-{MLP} architecture for time series forecasting.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=wbpxTuXgm0}.

\bibitem[Chen et~al.(2022)Chen, Hsieh, and Gong]{chen2022vitwithsam}
Chen, X., Hsieh, C.-J., and Gong, B.
\newblock When vision transformers outperform resnets without pre-training or strong data augmentations.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=LtKcMgGOeLt}.

\bibitem[Cirstea et~al.(2022)Cirstea, Guo, Yang, Kieu, Dong, and Pan]{cirstea2022triformer}
Cirstea, R.-G., Guo, C., Yang, B., Kieu, T., Dong, X., and Pan, S.
\newblock Triformer: Triangular, variable-specific attentions for long sequence multivariate time series forecasting.
\newblock In Raedt, L.~D. (ed.), \emph{Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, {IJCAI-22}}, pp.\  1994--2001. International Joint Conferences on Artificial Intelligence Organization, 7 2022.
\newblock \doi{10.24963/ijcai.2022/277}.
\newblock URL \url{https://doi.org/10.24963/ijcai.2022/277}.
\newblock Main Track.

\bibitem[Daneshmand et~al.(2020)Daneshmand, Kohler, Bach, Hofmann, and Lucchi]{daneshmand2020bncollapse}
Daneshmand, H., Kohler, J., Bach, F., Hofmann, T., and Lucchi, A.
\newblock Batch normalization provably avoids rank collapse for randomly initialised deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Neural Information Processing Systems}, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc.
\newblock ISBN 9781713829546.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding, 2018.
\newblock URL \url{http://arxiv.org/abs/1810.04805}.

\bibitem[Dong et~al.(2021)Dong, Cordonnier, and Loukas]{dong2021attentionrank}
Dong, Y., Cordonnier, J.-B., and Loukas, A.
\newblock Attention is not all you need: pure attention loses rank doubly exponentially with depth.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  2793--2803. PMLR, 18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/dong21a.html}.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{dosovitskiy2021vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{dziugaite2017nonvacuous}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data.
\newblock In \emph{Proceedings of the 33rd Annual Conference on Uncertainty in Artificial Intelligence (UAI)}, 2017.

\bibitem[Fan et~al.(2019)Fan, Zhang, Pan, Li, Zhang, Yuan, Wu, Wang, Pei, and Huang]{fan2017multistep}
Fan, C., Zhang, Y., Pan, Y., Li, X., Zhang, C., Yuan, R., Wu, D., Wang, W., Pei, J., and Huang, H.
\newblock Multi-horizon time series forecasting with temporal attention learning.
\newblock In \emph{Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, KDD '19, pp.\  2527–2535, New York, NY, USA, 2019. Association for Computing Machinery.
\newblock ISBN 9781450362016.
\newblock \doi{10.1145/3292500.3330662}.
\newblock URL \url{https://doi.org/10.1145/3292500.3330662}.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and Neyshabur]{foret2021sharpnessaware}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock Sharpness-aware minimization for efficiently improving generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=6Tm1mposlrM}.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010initialization}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural networks.
\newblock In Teh, Y.~W. and Titterington, M. (eds.), \emph{Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}, volume~9 of \emph{Proceedings of Machine Learning Research}, pp.\  249--256, Chia Laguna Resort, Sardinia, Italy, 13--15 May 2010. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v9/glorot10a.html}.

\bibitem[He et~al.(2023)He, Martens, Zhang, Botev, Brock, Smith, and Teh]{he2023deepshortcut}
He, B., Martens, J., Zhang, G., Botev, A., Brock, A., Smith, S.~L., and Teh, Y.~W.
\newblock Deep transformers without shortcuts: Modifying self-attention for faithful signal propagation.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=NPrsUQgMjKK}.

\bibitem[Horn \& Johnson(1991)Horn and Johnson]{Horn_Johnson_1991}
Horn, R.~A. and Johnson, C.~R.
\newblock \emph{Topics in Matrix Analysis}.
\newblock Cambridge University Press, 1991.

\bibitem[Hunter(2007)]{hunter2007matplotlib}
Hunter, J.~D.
\newblock Matplotlib: A 2d graphics environment.
\newblock \emph{Computing in Science \& Engineering}, 9\penalty0 (3):\penalty0 90--95, 2007.
\newblock \doi{10.1109/MCSE.2007.55}.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang]{keskar2017sharpminima}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=H1oyRlYgg}.

\bibitem[Kim et~al.(2021{\natexlab{a}})Kim, Papamakarios, and Mnih]{kim2021lipschitz}
Kim, H., Papamakarios, G., and Mnih, A.
\newblock The lipschitz constant of self-attention.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  5562--5571. PMLR, 18--24 Jul 2021{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v139/kim21i.html}.

\bibitem[Kim et~al.(2021{\natexlab{b}})Kim, Kim, Tae, Park, Choi, and Choo]{kim2021reversible}
Kim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and Choo, J.
\newblock Reversible instance normalization for accurate time-series forecasting against distribution shift.
\newblock In \emph{International Conference on Learning Representations}, 2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=cGDAkQo1C0p}.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{KingBa15}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, San Diega, CA, USA, 2015.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Kitaev, N., Kaiser, L., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgNKkHtvB}.

\bibitem[Lai et~al.(2018{\natexlab{a}})Lai, Chang, Yang, and Liu]{lai2018lstnet}
Lai, G., Chang, W.-C., Yang, Y., and Liu, H.
\newblock Modeling long- and short-term temporal patterns with deep neural networks.
\newblock In \emph{The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval}, SIGIR '18, pp.\  95–104, New York, NY, USA, 2018{\natexlab{a}}. Association for Computing Machinery.
\newblock ISBN 9781450356572.
\newblock \doi{10.1145/3209978.3210006}.
\newblock URL \url{https://doi.org/10.1145/3209978.3210006}.

\bibitem[Lai et~al.(2018{\natexlab{b}})Lai, Chang, Yang, and Liu]{lai2018modeling}
Lai, G., Chang, W.-C., Yang, Y., and Liu, H.
\newblock Modeling long- and short-term temporal patterns with deep neural networks.
\newblock In \emph{Association for Computing Machinery}, SIGIR '18, pp.\  95–104, New York, NY, USA, 2018{\natexlab{b}}.
\newblock ISBN 9781450356572.
\newblock \doi{10.1145/3209978.3210006}.
\newblock URL \url{https://doi.org/10.1145/3209978.3210006}.

\bibitem[Li et~al.(2019)Li, Jin, Xuan, Zhou, Chen, Wang, and Yan]{li2019logtrans}
Li, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y.-X., and Yan, X.
\newblock Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/6775a0635c302542da2c32aa19d86be0-Paper.pdf}.

\bibitem[Liu et~al.(2020)Liu, Liu, Gao, Chen, and Han]{liu2020understanding}
Liu, L., Liu, X., Gao, J., Chen, W., and Han, J.
\newblock Understanding the difficulty of training transformers.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)}, 2020.

\bibitem[Liu et~al.(2022)Liu, Yu, Liao, Li, Lin, Liu, and Dustdar]{liu2022pyraformer}
Liu, S., Yu, H., Liao, C., Li, J., Lin, W., Liu, A.~X., and Dustdar, S.
\newblock Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=0EXmFzUn5I}.

\bibitem[Liu et~al.(2024)Liu, Hu, Zhang, Wu, Wang, Ma, and Long]{liu2024itransformer}
Liu, Y., Hu, T., Zhang, H., Wu, H., Wang, S., Ma, L., and Long, M.
\newblock itransformer: Inverted transformers are effective for time series forecasting.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=JePfAI8fah}.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017sgdr}
Loshchilov, I. and Hutter, F.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Skq89Scxx}.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{loshchilov2018decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[{Max Planck Institute}(2021)]{weather}
{Max Planck Institute}.
\newblock Weather dataset, 2021.
\newblock URL \url{https://www.bgc-jena.mpg.de/wetter/}.

\bibitem[Nesterov(1983)]{nesterov1983sgd}
Nesterov, Y.
\newblock A method for solving the convex programming problem with convergence rate $o(1/k^2)$.
\newblock \emph{Proceedings of the USSR Academy of Sciences}, 269:\penalty0 543--547, 1983.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:145918791}.

\bibitem[Nie et~al.(2023)Nie, Nguyen, Sinthong, and Kalagnanam]{nie2023patchtst}
Nie, Y., Nguyen, N.~H., Sinthong, P., and Kalagnanam, J.
\newblock A time series is worth 64 words: Long-term forecasting with transformers.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=Jbdc0vTOcol}.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Pan \& Li(2022)Pan and Li]{pan2022toward}
Pan, Y. and Li, Y.
\newblock Toward understanding why adam converges faster than {SGD} for transformers.
\newblock In \emph{OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Sf1NlV2r6PO}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\  8024--8035. Curran Associates, Inc., 2019.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830, 2011.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and Sutskever]{radford2018improving}
Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{2018 OpenAI Tech Report}, 2018.

\bibitem[Rangapuram et~al.(2018)Rangapuram, Seeger, Gasthaus, Stella, Wang, and Januschowski]{rangapuram2018deep}
Rangapuram, S.~S., Seeger, M.~W., Gasthaus, J., Stella, L., Wang, Y., and Januschowski, T.
\newblock Deep state space models for time series forecasting.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2018/file/5cf68969fb67aa6082363a6d4e6468e2-Paper.pdf}.

\bibitem[Recht(2011)]{rechet2011matrixcompletion}
Recht, B.
\newblock A simpler approach to matrix completion.
\newblock \emph{J. Mach. Learn. Res.}, 12\penalty0 (null):\penalty0 3413–3430, dec 2011.
\newblock ISSN 1532-4435.

\bibitem[Recht et~al.(2010)Recht, Fazel, and Parrilo]{recht2010guaranteed}
Recht, B., Fazel, M., and Parrilo, P.~A.
\newblock Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization.
\newblock \emph{SIAM Review}, 52\penalty0 (3):\penalty0 471--501, 2010.
\newblock \doi{10.1137/070697835}.
\newblock URL \url{https://doi.org/10.1137/070697835}.

\bibitem[Salinas et~al.(2020)Salinas, Flunkert, Gasthaus, and Januschowski]{salinas2020deepar}
Salinas, D., Flunkert, V., Gasthaus, J., and Januschowski, T.
\newblock Deepar: Probabilistic forecasting with autoregressive recurrent networks.
\newblock \emph{International Journal of Forecasting}, 36\penalty0 (3):\penalty0 1181--1191, 2020.
\newblock ISSN 0169-2070.
\newblock \doi{https://doi.org/10.1016/j.ijforecast.2019.07.001}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S0169207019301888}.

\bibitem[Sen et~al.(2019)Sen, Yu, and Dhillon]{sen2019tcn}
Sen, R., Yu, H.-F., and Dhillon, I.
\newblock Think globally, act locally: a deep neural network approach to high-dimensional time series forecasting.
\newblock In \emph{Proceedings of the 33rd International Conference on Neural Information Processing Systems}, Red Hook, NY, USA, 2019. Curran Associates Inc.

\bibitem[Sonkavde et~al.(2023)Sonkavde, Dharrao, Bongale, Deokate, Doreswamy, and Bhat]{sonkavde2023stock}
Sonkavde, G., Dharrao, D.~S., Bongale, A.~M., Deokate, S.~T., Doreswamy, D., and Bhat, S.~K.
\newblock Forecasting stock market prices using machine learning and deep learning models: A systematic review, performance analysis and discussion of implications.
\newblock \emph{International Journal of Financial Studies}, 11\penalty0 (3), 2023.
\newblock ISSN 2227-7072.
\newblock \doi{10.3390/ijfs11030094}.
\newblock URL \url{https://www.mdpi.com/2227-7072/11/3/94}.

\bibitem[Sorjamaa et~al.(2007)Sorjamaa, Hao, Reyhani, Ji, and Lendasse]{sorjamaa2007methodology}
Sorjamaa, A., Hao, J., Reyhani, N., Ji, Y., and Lendasse, A.
\newblock Methodology for long-term prediction of time series.
\newblock \emph{Neurocomputing}, 70\penalty0 (16):\penalty0 2861--2869, 2007.
\newblock ISSN 0925-2312.
\newblock \doi{https://doi.org/10.1016/j.neucom.2006.06.015}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S0925231207001610}.
\newblock Neural Network Applications in Electrical Engineering Selected papers from the 3rd International Work-Conference on Artificial Neural Networks (IWANN 2005).

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and Jegou]{touvron2021efficient}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and Jegou, H.
\newblock Training data-efficient image transformers and distillation through attention.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th International Conference on Machine Learning}, volume 139 of \emph{Proceedings of Machine Learning Research}, pp.\  10347--10357. PMLR, 18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/touvron21a.html}.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G.
\newblock Llama: Open and efficient foundation language models, 2023.
\newblock URL \url{http://arxiv.org/abs/2302.13971}.
\newblock cite arxiv:2302.13971.

\bibitem[Trockman \& Kolter(2023)Trockman and Kolter]{trockman2023mimetic}
Trockman, A. and Kolter, J.~Z.
\newblock Mimetic initialization of self-attention layers.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning}, ICML'23. JMLR.org, 2023.

\bibitem[UCI(2015)]{electricity}
UCI.
\newblock Electricity dataset, 2015.
\newblock URL \url{https://archive.ics.uci.edu/dataset/321/electricityloaddiagrams20112014}.

\bibitem[Van~Rossum \& Drake~Jr(1995)Van~Rossum and Drake~Jr]{van1995python}
Van~Rossum, G. and Drake~Jr, F.~L.
\newblock \emph{Python reference manual}.
\newblock Centrum voor Wiskunde en Informatica Amsterdam, 1995.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L.~u., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Woo et~al.(2024)Woo, Liu, Kumar, Xiong, Savarese, and Sahoo]{woo2024unified}
Woo, G., Liu, C., Kumar, A., Xiong, C., Savarese, S., and Sahoo, D.
\newblock Unified training of universal time series forecasting transformers, 2024.

\bibitem[Wu et~al.(2021)Wu, Xu, Wang, and Long]{wu2021autoformer}
Wu, H., Xu, J., Wang, J., and Long, M.
\newblock Autoformer: Decomposition transformers with {Auto-Correlation} for long-term series forecasting.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Zamir et~al.(2022)Zamir, Arora, Khan, Hayat, Khan, and Yang]{zamir2021Restormer}
Zamir, S.~W., Arora, A., Khan, S., Hayat, M., Khan, F.~S., and Yang, M.-H.
\newblock Restormer: Efficient transformer for high-resolution image restoration.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zeng et~al.(2023)Zeng, Chen, Zhang, and Xu]{zeng2022effective}
Zeng, A., Chen, M., Zhang, L., and Xu, Q.
\newblock Are transformers effective for time series forecasting?
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 2023.

\bibitem[Zhai et~al.(2023)Zhai, Likhomanenko, Littwin, Busbridge, Ramapuram, Zhang, Gu, and Susskind]{zhai2023collapse}
Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J.~M.
\newblock Stabilizing transformer training by preventing attention entropy collapse.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  40770--40803. PMLR, 23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/zhai23a.html}.

\bibitem[Zhang et~al.(2022)Zhang, Wu, Zhang, Zhu, Lin, Zhang, Sun, He, Mueller, Manmatha, Li, and Smola]{Zhang_2022_CVPR}
Zhang, H., Wu, C., Zhang, Z., Zhu, Y., Lin, H., Zhang, Z., Sun, Y., He, T., Mueller, J., Manmatha, R., Li, M., and Smola, A.
\newblock Resnest: Split-attention networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops}, pp.\  2736--2746, June 2022.

\bibitem[Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and Sra]{zhang2020adaptattention}
Zhang, J., Karimireddy, S.~P., Veit, A., Kim, S., Reddi, S., Kumar, S., and Sra, S.
\newblock Why are adaptive methods good for attention models?
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  15383--15393. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/b05b57f6add810d3b7490866d74c0053-Paper.pdf}.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Peng, Zhang, Li, Xiong, and Zhang]{haoyi2021informer}
Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W.
\newblock Informer: Beyond efficient transformer for long sequence time-series forecasting.
\newblock In \emph{The Thirty-Fifth {AAAI} Conference on Artificial Intelligence, {AAAI} 2021, Virtual Conference}, volume~35, pp.\  11106--11115. {AAAI} Press, 2021.

\bibitem[Zhou et~al.(2022)Zhou, Ma, Wen, Wang, Sun, and Jin]{zhou2022fedformer}
Zhou, T., Ma, Z., Wen, Q., Wang, X., Sun, L., and Jin, R.
\newblock {FEDformer}: Frequency enhanced decomposed transformer for long-term series forecasting.
\newblock In \emph{Proc. 39th International Conference on Machine Learning (ICML 2022)}, 2022.

\end{thebibliography}
