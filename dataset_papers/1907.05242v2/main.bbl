\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Babenko and Lempitsky(2014)]{babenko2014inverted}
Artem Babenko and Victor Lempitsky.
\newblock The inverted multi-index.
\newblock \emph{{\sc IEEE} Transactions on Pattern Analysis and Machine
  Intelligence}, 2014.

\bibitem[Baevski and Auli(2019)]{baevski2018adaptive}
Alexei Baevski and Michael Auli.
\newblock Adaptive input representations for neural language modeling.
\newblock In \emph{International Conference on Representation Learning}, 2019.

\bibitem[Bengio et~al.(2013)Bengio, L{\'{e}}onard, and Courville]{BengioLC13}
Yoshua Bengio, Nicholas L{\'{e}}onard, and Aaron~C. Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{CoRR}, abs/1308.3432, 2013.

\bibitem[Chelba et~al.(2014)Chelba, Mikolov, Schuster, Ge, Brants, Koehn, and
  Robinson]{chelba2013one}
Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi~Ge, Thorsten Brants, Phillipp
  Koehn, and Tony Robinson.
\newblock One billion word benchmark for measuring progress in statistical
  language modeling.
\newblock \emph{Conference of the International Speech Communication
  Association}, 2014.

\bibitem[Chen and Gilbert(2019)]{ondevicechallenge2018}
Bo~Chen and Jeffrey~M. Gilbert.
\newblock The on-device visual intelligence challenge.
\newblock
  \url{https://ai.googleblog.com/2018/04/introducing-cvpr-2018-on-device-visual.html},
  2019.
\newblock Accessed: 2019-05-20.

\bibitem[Cho and Bengio(2014)]{ChoB14}
Kyunghyun Cho and Yoshua Bengio.
\newblock Exponentially increasing the capacity-to-computation ratio for
  conditional computation in deep learning.
\newblock \emph{CoRR}, abs/1406.7362, 2014.

\bibitem[Courbariaux and Bengio(2016)]{DBLP:journals/corr/CourbariauxB16}
Matthieu Courbariaux and Yoshua Bengio.
\newblock Binarynet: Training deep neural networks with weights and activations
  constrained to +1 or -1.
\newblock \emph{CoRR}, 2016.

\bibitem[Courbariaux et~al.(2015)Courbariaux, Bengio, and
  David]{DBLP:journals/corr/CourbariauxBD15}
Matthieu Courbariaux, Yoshua Bengio, and Jean{-}Pierre David.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Cohen, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, William~W Cohen, Jaime Carbonell, Quoc~V
  Le, and Ruslan Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock In \emph{Conference of the Association for Computational
  Linguistics}, 2019.

\bibitem[Denoyer and Gallinari(2014)]{DenoyerG14}
Ludovic Denoyer and Patrick Gallinari.
\newblock Deep sequential neural network.
\newblock \emph{CoRR}, abs/1410.0510, 2014.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Conference of the North American Chapter of the Association
  for Computational Linguistic}, 2018.

\bibitem[Eigen et~al.(2014)Eigen, Sutskever, and Ranzato]{eigen14}
D.~Eigen, I.~Sutskever, and M.~Ranzato.
\newblock Learning factored representations in a deep mixture of experts.
\newblock In \emph{Workshop at the International Conference on Learning
  Representations}, 2014.

\bibitem[Gerald et~al.(2017)Gerald, Baskiotis, and Denoyer]{GeraldBD17}
Thomas Gerald, Nicolas Baskiotis, and Ludovic Denoyer.
\newblock Binary stochastic representations for large multi-class
  classification.
\newblock In \emph{International Conference on Neural Information Processing},
  2017.

\bibitem[Grave et~al.(2017{\natexlab{a}})Grave, Cisse, and
  Joulin]{grave2017unbounded}
Edouard Grave, Moustapha~M Cisse, and Armand Joulin.
\newblock Unbounded cache model for online language modeling with open
  vocabulary.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2017{\natexlab{a}}.

\bibitem[Grave et~al.(2017{\natexlab{b}})Grave, Joulin, and
  Usunier]{grave2015cache}
Edouard Grave, Armand Joulin, and Nicolas Usunier.
\newblock Improving neural language models with a continuous cache.
\newblock In \emph{International Conference on Representation Learning},
  2017{\natexlab{b}}.

\bibitem[Graves et~al.(2013)Graves, Mohamed, and Hinton]{graves2013speech}
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In \emph{International Conference on Acoustics, Speech, and Signal
  Processing}, 2013.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves14ntm}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural turing machines.
\newblock \emph{CoRR}, abs/1410.5401, 2014.

\bibitem[Gross et~al.(2017)Gross, Ranzato, and Szlam]{gross17}
Sam Gross, Marc'Aurelio Ranzato, and Arthur Szlam.
\newblock Hard mixtures of experts for large scale weakly supervised vision.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  2016.

\bibitem[Huang et~al.(2018)Huang, Cheng, Chen, Lee, Ngiam, Le, and
  Chen]{huang2018gpipe}
Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam,
  Quoc~V. Le, and Zhifeng Chen.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock \emph{CoRR}, abs/1811.06965, 2018.

\bibitem[J{\'e}gou et~al.(2011)J{\'e}gou, Douze, and Schmid]{jegou11pq}
Herv{\'e} J{\'e}gou, Matthijs Douze, and Cordelia Schmid.
\newblock {Product Quantization for Nearest Neighbor Search}.
\newblock \emph{{\sc IEEE} Transactions on Pattern Analysis and Machine
  Intelligence}, 2011.

\bibitem[Johnson et~al.(2017)Johnson, Douze, and J{\'e}gou]{johnson2017billion}
Jeff Johnson, Matthijs Douze, and Herv{\'e} J{\'e}gou.
\newblock Billion-scale similarity search with gpus.
\newblock \emph{{\sc IEEE} Transactions on Big Data}, 2017.

\bibitem[Joulin and Mikolov(2015)]{joulin15stackrnn}
Armand Joulin and Tomas Mikolov.
\newblock Inferring algorithmic patterns with stack-augmented recurrent nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Kavukcuoglu et~al.(2010)Kavukcuoglu, Ranzato, and LeCun]{psd}
Koray Kavukcuoglu, Marc'Aurelio Ranzato, and Yann LeCun.
\newblock Fast inference in sparse coding algorithms with applications to
  object recognition.
\newblock \emph{CoRR}, abs/1010.3467, 2010.
\newblock URL \url{http://arxiv.org/abs/1010.3467}.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Representation Learning}, 2015.

\bibitem[Koehn et~al.(2007)Koehn, Hoang, Birch, Callison-Burch, Federico,
  Bertoldi, Cowan, Shen, Moran, Zens, et~al.]{koehn2007moses}
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello
  Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard
  Zens, et~al.
\newblock Moses: Open source toolkit for statistical machine translation.
\newblock In \emph{Conference of the Association for Computational
  Linguistics}, 2007.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky12cnn}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2012.

\bibitem[Lample and Conneau(2019)]{lample2019cross}
Guillaume Lample and Alexis Conneau.
\newblock Cross-lingual language model pretraining.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Mahajan et~al.(2018)Mahajan, Girshick, Ramanathan, He, Paluri, Li,
  Bharambe, and van~der Maaten]{mahajan2018uru}
Dhruv Mahajan, Ross~B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar
  Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van~der Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In \emph{European Conference on Computer Vision}, 2018.

\bibitem[Makhzani and Frey(2014)]{makhzani2013k}
Alireza Makhzani and Brendan Frey.
\newblock K-sparse autoencoders.
\newblock In \emph{International Conference on Representation Learning}, 2014.

\bibitem[Makhzani and Frey(2015)]{makhzani2015winner}
Alireza Makhzani and Brendan~J Frey.
\newblock Winner-take-all autoencoders.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Muja and Lowe(2014)]{ML14}
Marius Muja and David~G. Lowe.
\newblock Scalable nearest neighbor algorithms for high dimensional data.
\newblock \emph{{\sc IEEE} Transactions on Pattern Analysis and Machine
  Intelligence}, 2014.

\bibitem[Neyshabur et~al.(2019)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{neyshabur}
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan
  Srebro.
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock In \emph{International Conference on Representation Learning}, 2019.

\bibitem[Olshausen and Field(1997)]{sparsecod}
Bruno~A. Olshausen and David~J. Field.
\newblock Sparse coding with an overcomplete basis set, a strategy employed by
  v1?
\newblock \emph{Vision Research}, 1997.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock In \emph{Neurips Autodiff Workshop}, 2017.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners, 2019.

\bibitem[Rae et~al.(2016)Rae, Hunt, Danihelka, Harley, Senior, Wayne, Graves,
  and Lillicrap]{rae16sparsememorynets}
Jack Rae, Jonathan~J Hunt, Ivo Danihelka, Timothy Harley, Andrew~W Senior,
  Gregory Wayne, Alex Graves, and Timothy Lillicrap.
\newblock Scaling memory-augmented neural networks with sparse reads and
  writes.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi]{rastegari2016xnor}
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In \emph{European Conference on Computer Vision}, 2016.

\bibitem[Sennrich et~al.(2015)Sennrich, Haddow, and Birch]{sennrich2015neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Conference of the Association for Computational
  Linguistics}, 2015.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc~V. Le,
  Geoffrey~E. Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock In \emph{International Conference on Representation Learning}, 2017.

\bibitem[Spigler et~al.(2018)Spigler, Geiger, d'Ascoli, Sagun, Biroli, and
  Wyart]{segun}
Stefano Spigler, Mario Geiger, St{\'{e}}phane d'Ascoli, Levent Sagun, Giulio
  Biroli, and Matthieu Wyart.
\newblock A jamming transition from under- to over-parametrization affects loss
  landscape and generalization.
\newblock \emph{CoRR}, abs/1810.09665, 2018.

\bibitem[Sukhbaatar et~al.(2015)Sukhbaatar, szlam, Weston, and
  Fergus]{sukhbaatar15memorynets}
Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus.
\newblock End-to-end memory networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Vijayanarasimhan et~al.(2015)Vijayanarasimhan, Shlens, Monga, and
  Yagnik]{VijayanarasimhanSMY14}
Sudheendra Vijayanarasimhan, Jonathon Shlens, Rajat Monga, and Jay Yagnik.
\newblock Deep networks with large output spaces.
\newblock In \emph{Workshop at the International Conference on Learning
  Representations}, 2015.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Alex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{International Conference on Representation Learning}, 2018.

\bibitem[Weston et~al.(2015)Weston, Chopra, and Bordes]{WestonCB14}
Jason Weston, Sumit Chopra, and Antoine Bordes.
\newblock Memory networks.
\newblock In \emph{International Conference on Representation Learning}, 2015.

\bibitem[Weston et~al.(2016)Weston, Bordes, Chopra, and Mikolov]{WestonBCM15}
Jason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov.
\newblock Towards ai-complete question answering: {A} set of prerequisite toy
  tasks.
\newblock In \emph{International Conference on Representation Learning}, 2016.

\end{thebibliography}
