\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and
  Abbeel]{achiam2017constrained}
Achiam, J., Held, D., Tamar, A., and Abbeel, P.
\newblock Constrained policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  22--31,
  2017.

\bibitem[Akametalu et~al.(2014)Akametalu, Fisac, Gillula, Kaynama, Zeilinger,
  and Tomlin]{akametalu2014reachability}
Akametalu, A.~K., Fisac, J.~F., Gillula, J.~H., Kaynama, S., Zeilinger, M.~N.,
  and Tomlin, C.~J.
\newblock Reachability-based safe learning with {G}aussian processes.
\newblock In \emph{IEEE Conference on Decision and Control}, pp.\  1424--1431,
  2014.

\bibitem[Altman(1999)]{altman1999constrained}
Altman, E.
\newblock \emph{Constrained {M}arkov decision processes}, volume~7.
\newblock CRC Press, 1999.

\bibitem[Arapostathis et~al.(1993)Arapostathis, Borkar,
  Fern{\'a}ndez-Gaucherand, Ghosh, and Marcus]{arapostathis1993discrete}
Arapostathis, A., Borkar, V.~S., Fern{\'a}ndez-Gaucherand, E., Ghosh, M.~K.,
  and Marcus, S.~I.
\newblock Discrete-time controlled {M}arkov processes with average cost
  criterion: a survey.
\newblock \emph{SIAM Journal on Control and Optimization}, 31\penalty0
  (2):\penalty0 282--344, 1993.

\bibitem[B{\"a}uerle \& Ott(2011)B{\"a}uerle and Ott]{bauerle2011markov}
B{\"a}uerle, N. and Ott, J.
\newblock Markov decision processes with average-value-at-risk criteria.
\newblock \emph{Mathematical Methods of Operations Research}, 74\penalty0
  (3):\penalty0 361--379, 2011.

\bibitem[Berkenkamp et~al.(2017)Berkenkamp, Turchetta, Schoellig, and
  Krause]{berkenkamp2017safe}
Berkenkamp, F., Turchetta, M., Schoellig, A., and Krause, A.
\newblock Safe model-based reinforcement learning with stability guarantees.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  908--918, 2017.

\bibitem[Bertsekas(1997)]{bertsekas1997nonlinear}
Bertsekas, D.~P.
\newblock Nonlinear programming.
\newblock \emph{Journal of the Operational Research Society}, 48\penalty0
  (3):\penalty0 334--334, 1997.

\bibitem[Bharadhwaj et~al.(2020)Bharadhwaj, Kumar, Rhinehart, Levine, Shkurti,
  and Garg]{bharadhwaj2020conservative}
Bharadhwaj, H., Kumar, A., Rhinehart, N., Levine, S., Shkurti, F., and Garg, A.
\newblock Conservative safety critics for exploration.
\newblock \emph{arXiv preprint arXiv:2010.14497}, 2020.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock {OpenAI} gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Calvo-Fullana et~al.(2021)Calvo-Fullana, Paternain, Chamon, and
  Ribeiro]{calvo2021state}
Calvo-Fullana, M., Paternain, S., Chamon, L.~F., and Ribeiro, A.
\newblock State augmented constrained reinforcement learning: Overcoming the
  limitations of learning with rewards.
\newblock \emph{arXiv preprint arXiv:2102.11941}, 2021.

\bibitem[Cheng et~al.(2019)Cheng, Orosz, Murray, and Burdick]{cheng2019end}
Cheng, R., Orosz, G., Murray, R.~M., and Burdick, J.~W.
\newblock End-to-end safe reinforcement learning through barrier functions for
  safety-critical continuous control tasks.
\newblock \emph{arXiv preprint arXiv:1903.08792}, 2019.

\bibitem[Chow et~al.(2017)Chow, Ghavamzadeh, Janson, and Pavone]{chow2017risk}
Chow, Y., Ghavamzadeh, M., Janson, L., and Pavone, M.
\newblock Risk-constrained reinforcement learning with percentile risk
  criteria.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 6070--6120, 2017.

\bibitem[Chow et~al.(2018)Chow, Nachum, Duenez-Guzman, and
  Ghavamzadeh]{chow2018lyapunov}
Chow, Y., Nachum, O., Duenez-Guzman, E., and Ghavamzadeh, M.
\newblock A {L}yapunov-based approach to safe reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8092--8101, 2018.

\bibitem[Chow et~al.(2019)Chow, Nachum, Faust, Ghavamzadeh, and
  Duenez-Guzman]{chow2019lyapunov}
Chow, Y., Nachum, O., Faust, A., Ghavamzadeh, M., and Duenez-Guzman, E.
\newblock Lyapunov-based safe policy optimization for continuous control.
\newblock \emph{arXiv preprint arXiv:1901.10031}, 2019.

\bibitem[Chua et~al.(2018)Chua, Calandra, McAllister, and Levine]{chua2018deep}
Chua, K., Calandra, R., McAllister, R., and Levine, S.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Cowen-Rivers et~al.(2022)Cowen-Rivers, Palenicek, Moens, Abdullah,
  Sootla, Wang, and Bou-Ammar]{cowen2020samba}
Cowen-Rivers, A.~I., Palenicek, D., Moens, V., Abdullah, M.~A., Sootla, A.,
  Wang, J., and Bou-Ammar, H.
\newblock {SAMBA}: Safe model-based \& active reinforcement learning.
\newblock \emph{Machine Learning}, pp.\  1--31, 2022.

\bibitem[Dalal et~al.(2018)Dalal, Dvijotham, Vecerik, Hester, Paduraru, and
  Tassa]{dalal2018safe}
Dalal, G., Dvijotham, K., Vecerik, M., Hester, T., Paduraru, C., and Tassa, Y.
\newblock Safe exploration in continuous action spaces.
\newblock \emph{arXiv preprint arXiv:1801.08757}, 2018.

\bibitem[Daryin \& Kurzhanski(2005)Daryin and Kurzhanski]{daryin2005nonlinear}
Daryin, A. and Kurzhanski, A.
\newblock Nonlinear control synthesis under double constraints.
\newblock \emph{IFAC Proceedings Volumes}, 38\penalty0 (1):\penalty0 247--252,
  2005.

\bibitem[{Dean} et~al.(2019){Dean}, {Tu}, {Matni}, and {Recht}]{deanlqr2019}
{Dean}, S., {Tu}, S., {Matni}, N., and {Recht}, B.
\newblock Safely learning to control the constrained linear quadratic
  regulator.
\newblock In \emph{American Control Conference}, pp.\  5582--5588, 2019.

\bibitem[Deisenroth \& Rasmussen(2011)Deisenroth and
  Rasmussen]{deisenroth2011pilco}
Deisenroth, M. and Rasmussen, C.~E.
\newblock Pilco: A model-based and data-efficient approach to policy search.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  465--472, 2011.

\bibitem[Ding et~al.(2020)Ding, Zhang, Basar, and Jovanovic]{ding2020natural}
Ding, D., Zhang, K., Basar, T., and Jovanovic, M.~R.
\newblock Natural policy gradient primal-dual method for constrained {M}arkov
  decision processes.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Eysenbach et~al.(2018)Eysenbach, Gu, Ibarz, and
  Levine]{eysenbach2018leave}
Eysenbach, B., Gu, S., Ibarz, J., and Levine, S.
\newblock Leave no trace: Learning to reset for safe and autonomous
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[{Fisac} et~al.(2019){Fisac}, {Akametalu}, {Zeilinger}, {Kaynama},
  {Gillula}, and {Tomlin}]{Fisac2019}
{Fisac}, J.~F., {Akametalu}, A.~K., {Zeilinger}, M.~N., {Kaynama}, S.,
  {Gillula}, J., and {Tomlin}, C.~J.
\newblock A general safety framework for learning-based control in uncertain
  robotic systems.
\newblock \emph{IEEE Transactions on Automatic Control}, 64\penalty0
  (7):\penalty0 2737--2752, 2019.
\newblock \doi{10.1109/TAC.2018.2876389}.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\
  1861--1870, 2018.

\bibitem[Hafner et~al.(2019)Hafner, Lillicrap, Ba, and
  Norouzi]{hafner2019dream}
Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Hern{\'a}ndez-Lerma \& Lasserre(2012)Hern{\'a}ndez-Lerma and
  Lasserre]{hernandez2012discrete}
Hern{\'a}ndez-Lerma, O. and Lasserre, J.~B.
\newblock \emph{Discrete-time {M}arkov control processes: basic optimality
  criteria}, volume~30.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Hern{\'a}ndez-Lerma \& Mu{\~n}oz~de Ozak(1992)Hern{\'a}ndez-Lerma and
  Mu{\~n}oz~de Ozak]{hernandez1992discrete}
Hern{\'a}ndez-Lerma, O. and Mu{\~n}oz~de Ozak, M.
\newblock Discrete-time {M}arkov control processes with discounted unbounded
  costs: optimality criteria.
\newblock \emph{Kybernetika}, 28\penalty0 (3):\penalty0 191--212, 1992.

\bibitem[Janner et~al.(2019)Janner, Fu, Zhang, and Levine]{janner2019trust}
Janner, M., Fu, J., Zhang, M., and Levine, S.
\newblock When to trust your model: Model-based policy optimization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, pp.\  12519--12530, 2019.

\bibitem[Kamthe \& Deisenroth(2018)Kamthe and Deisenroth]{kamthe2018data}
Kamthe, S. and Deisenroth, M.
\newblock Data-efficient reinforcement learning with probabilistic model
  predictive control.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1701--1710. PMLR, 2018.

\bibitem[Koller et~al.(2018)Koller, Berkenkamp, Turchetta, and
  Krause]{koller2018learning}
Koller, T., Berkenkamp, F., Turchetta, M., and Krause, A.
\newblock Learning-based model predictive control for safe exploration.
\newblock In \emph{IEEE Conference on Decision and Control}, pp.\  6059--6066,
  2018.

\bibitem[Lee et~al.(2020)Lee, Nagabandi, Abbeel, and Levine]{lee2020stochastic}
Lee, A., Nagabandi, A., Abbeel, P., and Levine, S.
\newblock Stochastic latent actor-critic: Deep reinforcement learning with a
  latent variable model.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Mania et~al.(2018)Mania, Guy, and Recht]{mania2018simple}
Mania, H., Guy, A., and Recht, B.
\newblock Simple random search provides a competitive approach to reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1803.07055}, 2018.

\bibitem[Mguni et~al.(2021)Mguni, Jennings, Jafferjee, Sootla, Yang, Yu, Islam,
  Wang, and Wang]{mguni2021desta}
Mguni, D., Jennings, J., Jafferjee, T., Sootla, A., Yang, Y., Yu, C., Islam,
  U., Wang, Z., and Wang, J.
\newblock {DESTA}: A framework for safe reinforcement learning with markov
  games of intervention.
\newblock \emph{arXiv preprint arXiv:2110.14468}, 2021.

\bibitem[Ohnishi et~al.(2019)Ohnishi, Wang, Notomista, and
  Egerstedt]{ohnishi2019barrier}
Ohnishi, M., Wang, L., Notomista, G., and Egerstedt, M.
\newblock Barrier-certified adaptive reinforcement learning with applications
  to brushbot navigation.
\newblock \emph{IEEE Transactions on Robotics}, 35\penalty0 (5):\penalty0
  1186--1205, 2019.

\bibitem[Ott(2010)]{ott2010markov}
Ott, J.~T.
\newblock A {M}arkov decision model for a surveillance application and
  risk-sensitive {M}arkov decision processes, 2010.
\newblock PhD Thesis.

\bibitem[Pineda et~al.(2021)Pineda, Amos, Zhang, Lambert, and
  Calandra]{Pineda2021MBRL}
Pineda, L., Amos, B., Zhang, A., Lambert, N.~O., and Calandra, R.
\newblock Mbrl-lib: A modular library for model-based reinforcement learning.
\newblock \emph{Arxiv}, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.10159}.

\bibitem[Polymenakos et~al.(2020)Polymenakos, Rontsis, Abate, and
  Roberts]{polymenakos2020safepilco}
Polymenakos, K., Rontsis, N., Abate, A., and Roberts, S.
\newblock {SafePILCO}: A software tool for safe and data-efficient policy
  synthesis.
\newblock In Gribaudo, M., Jansen, D.~N., and Remke, A. (eds.),
  \emph{Quantitative Evaluation of Systems}, pp.\  18--26. Springer
  International Publishing, 2020.

\bibitem[Raffin et~al.(2021)Raffin, Hill, Gleave, Kanervisto, Ernestus, and
  Dormann]{stable-baselines3}
Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N.
\newblock Stable-baselines3: Reliable reinforcement learning implementations.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (268):\penalty0 1--8, 2021.

\bibitem[Rasmussen \& Williams(2005)Rasmussen and Williams]{GPbook}
Rasmussen, C.~E. and Williams, C. K.~I.
\newblock \emph{{G}aussian Processes for Machine Learning (Adaptive Computation
  and Machine Learning)}.
\newblock The MIT Press, 2005.
\newblock ISBN 026218253X.

\bibitem[Ray et~al.(2019)Ray, Achiam, and Amodei]{raybenchmarking}
Ray, A., Achiam, J., and Amodei, D.
\newblock Benchmarking safe exploration in deep reinforcement learning, 2019.
\newblock URL \url{https://cdn.openai.com/safexp-short.pdf}.

\bibitem[Rockafellar et~al.(2000)Rockafellar, Uryasev,
  et~al.]{rockafellar2000optimization}
Rockafellar, R.~T., Uryasev, S., et~al.
\newblock Optimization of conditional value-at-risk.
\newblock \emph{Journal of Risk}, 2:\penalty0 21--42, 2000.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sootla et~al.(2022)Sootla, Cowen-Rivers, Jafferjee, and
  Wang]{sootla_saute_2022_git}
Sootla, A., Cowen-Rivers, A.~I., Jafferjee, T., and Wang, Z.
\newblock Saut\'e {RL}: Almost surely safe reinforcement learning using state
  augmentation, 2022.
\newblock URL \url{https://github.com/huawei-noah/HEBO/tree/master/SAUTE}.

\bibitem[Stooke et~al.(2020)Stooke, Achiam, and Abbeel]{stooke2020responsive}
Stooke, A., Achiam, J., and Abbeel, P.
\newblock Responsive safety in reinforcement learning by pid lagrangian
  methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9133--9143. PMLR, 2020.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tamar et~al.(2015)Tamar, Glassner, and Mannor]{tamar2015optimizing}
Tamar, A., Glassner, Y., and Mannor, S.
\newblock Optimizing the cvar via sampling.
\newblock In \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Turchetta et~al.(2016)Turchetta, Berkenkamp, and
  Krause]{turchetta2016safe}
Turchetta, M., Berkenkamp, F., and Krause, A.
\newblock Safe exploration in finite {M}arkov decision processes with
  {G}aussian processes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4312--4320, 2016.

\bibitem[Turchetta et~al.(2020)Turchetta, Kolobov, Shah, Krause, and
  Agarwal]{turchetta2020safe}
Turchetta, M., Kolobov, A., Shah, S., Krause, A., and Agarwal, A.
\newblock Safe reinforcement learning via curriculum induction.
\newblock \emph{arXiv preprint arXiv:2006.12136}, 2020.

\bibitem[Wachi et~al.(2018)Wachi, Sui, Yue, and Ono]{wachi2018safe}
Wachi, A., Sui, Y., Yue, Y., and Ono, M.
\newblock Safe exploration and optimization of constrained {MDP}s using
  {G}aussian processes.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2018.

\bibitem[Waskom(2021)]{Waskom2021}
Waskom, M.~L.
\newblock Seaborn: statistical data visualization.
\newblock \emph{Journal of Open Source Software}, 6\penalty0 (60):\penalty0
  3021, 2021.
\newblock URL \url{https://doi.org/10.21105/joss.03021}.

\bibitem[Yang et~al.(2021)Yang, Sim{\~a}o, Tindemans, and Spaan]{yang2021wcsac}
Yang, Q., Sim{\~a}o, T.~D., Tindemans, S.~H., and Spaan, M.~T.
\newblock {WCSAC}: Worst-case soft actor critic for safety-constrained
  reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence.}, 2021.

\bibitem[Yang et~al.(2019)Yang, Rosca, Narasimhan, and
  Ramadge]{yang2019projection}
Yang, T.-Y., Rosca, J., Narasimhan, K., and Ramadge, P.~J.
\newblock Projection-based constrained policy optimization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zimmer et~al.(2018)Zimmer, Meister, and Nguyen-Tuong]{zimmer2018safe}
Zimmer, C., Meister, M., and Nguyen-Tuong, D.
\newblock Safe active learning for time-series modeling with {G}aussian
  processes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2730--2739, 2018.

\end{thebibliography}
