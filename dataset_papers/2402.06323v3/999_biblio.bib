@article{INGLOT20141,
title = {Simple upper and lower bounds for the multivariate Laplace approximation},
journal = {Journal of Approximation Theory},
volume = {186},
pages = {1-11},
year = {2014},
issn = {0021-9045},
author = {Tadeusz Inglot and Piotr Majerski},
keywords = {Laplace approximation, Error bounds, Asymptotic approximation of integrals, Multiple integrals},
abstract = {We propose a new proof of the Laplace approximation for multiple integrals and consequently find new bounds for the approximation error. The main advantages of our approach are its simplicity, an explicit form of the bounds and small coefficient in the main error term, which depend on the smallest eigenvalue of the Hessian matrix, only. On the other hand, the method seems indicating much larger values of the parameter then really necessary.}
}



@article{moroshko2020implicit,
  title={Implicit bias in deep linear classification: Initialization scale vs training accuracy},
  author={Moroshko, Edward and Woodworth, Blake E and Gunasekar, Suriya and Lee, Jason D and Srebro, Nati and Soudry, Daniel},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22182--22193},
  year={2020}
}

@InProceedings{pmlr-v125-woodworth20a,
  title = 	 {Kernel and Rich Regimes in Overparametrized Models},
  author =       {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {3635--3673},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/woodworth20a/woodworth20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/woodworth20a.html},
  abstract = 	 { A recent line of work studies overparametrized neural networks in the “kernel regime,” i.e.&nbsp;when  during training the network behaves as a kernelized linear predictor, and thus, training with gradient descent has the effect of finding the corresponding minimum RKHS norm solution.  This stands in contrast to other studies which demonstrate how gradient descent on overparametrized  networks can induce rich implicit biases that are not RKHS norms.  Building on an observation by \citet{chizat2018note}, we show how the \textbf{\textit{scale of the initialization}} controls the transition between the “kernel” (aka lazy) and “rich” (aka active) regimes and affects generalization properties in multilayer homogeneous models. We provide a complete and detailed analysis for a family of simple depth-$D$ linear networks that exhibit an interesting and meaningful transition between the kernel and rich regimes, and highlight an interesting role for the \emph{width}  of the models. We further demonstrate this transition empirically for matrix factorization and multilayer non-linear networks.}
}


@InProceedings{Liu2022,
    author    = {Liu, Zechun and Cheng, Kwang-Ting and Huang, Dong and Xing, Eric P. and Shen, Zhiqiang},
    title     = {Nonuniform-to-Uniform Quantization: Towards Accurate Quantization via Generalized Straight-Through Estimation},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {4942-4952}
}

@inproceedings{kidger2020universal,
  title={Universal approximation with deep narrow networks},
  author={Kidger, Patrick and Lyons, Terry},
  booktitle={Conference on learning theory},
  pages={2306--2327},
  year={2020},
  organization={PMLR}
}

@article{hubara2018quantized,
  title={Quantized neural networks: Training neural networks with low precision weights and activations},
  author={Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  journal={journal of machine learning research},
  volume={18},
  number={187},
  pages={1--30},
  year={2018}
}

@book{engel2001statistical,
  title={Statistical mechanics of learning},
  author={Engel, Andreas},
  year={2001},
  publisher={Cambridge University Press}
}

@article{askarihemmat2022qreg,
  title={QReg: On regularization effects of quantization},
  author={Askari Hemmat, Mohammad Hossein and Hemmat, Reyhane Askari and Hoffman, Alex and Lazarevich, Ivan and Saboori, Ehsan and Mastropietro, Olivier and Sah, Sudhakar and Savaria, Yvon and David, Jean-Pierre},
  journal={arXiv preprint arXiv:2206.12372},
  year={2022}
}

@inproceedings{
lyu2019gradient,
title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
author={Kaifeng Lyu and Jian Li},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{Gunasekar2017,
 author = {Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Regularization in Matrix Factorization},
 volume = {30},
 year = {2017}
}


@article{vardi2023implicit,
  title={On the implicit bias in deep-learning algorithms},
  author={Vardi, Gal},
  journal={Communications of the ACM},
  volume={66},
  number={6},
  pages={86--93},
  year={2023},
  publisher={ACM New York, NY, USA}
}

@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{
Zhang16,
title={Understanding deep learning requires rethinking generalization},
author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
booktitle={International Conference on Learning Representations},
year={2017},
}

@article{mingard2021sgd,
  title={Is SGD a Bayesian sampler? Well, almost},
  author={Mingard, Chris and Valle-P{\'e}rez, Guillermo and Skalse, Joar and Louis, Ard A},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={3579--3642},
  year={2021},
  publisher={JMLRORG}
}


@inproceedings{
vallepérez2019deep,
title={Deep learning generalizes because the parameter-function map is biased towards simple functions},
author={Guillermo Valle-Perez and Chico Q. Camargo and Ard A. Louis},
booktitle={International Conference on Learning Representations},
year={2019},
}


@InProceedings{pmlr-v40-Neyshabur15,
  title = 	 {Norm-Based Capacity Control in Neural Networks},
  author = 	 {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  booktitle = 	 {Proceedings of The 28th Conference on Learning Theory},
  pages = 	 {1376--1401},
  year = 	 {2015},
  editor = 	 {Grünwald, Peter and Hazan, Elad and Kale, Satyen},
  volume = 	 {40},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Paris, France},
  month = 	 {03--06 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v40/Neyshabur15.pdf},
  abstract = 	 {We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks.}
}



@InProceedings{pmlr-v125-chizat20a,
  title = 	 {Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss},
  author =       {Chizat, L\'ena\"ic  and Bach, Francis},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {1305--1338},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf},
  abstract = 	 { Neural networks trained to minimize the logistic (a.k.a. cross-entropy) loss with gradient-based methods are observed to perform well in many supervised classification tasks. Towards understanding this phenomenon, we analyze the training and generalization behavior of infinitely wide two-layer neural networks with homogeneous activations. We show that the limits of the gradient flow on exponentially tailed losses can be fully characterized as a max-margin classifier in a certain non-Hilbertian space of functions. In presence of hidden low-dimensional structures, the resulting margin is independent of the ambiant dimension, which leads to strong generalization bounds. In contrast, training only the output layer implicitly solves a kernel support vector machine, which a priori does not enjoy such an adaptivity. Our analysis of training is non-quantitative in terms of running time but we prove computational guarantees in simplified settings by showing equivalences with online mirror descent. Finally, numerical experiments suggest that our analysis describes well the practical behavior of two-layer neural networks with ReLU activation and confirm the statistical benefits of this implicit bias.}
}


@inproceedings{
zhang2019fixup,
title={Residual Learning Without Normalization via Better Initialization},
author={Hongyi Zhang and Yann N. Dauphin and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1gsz30cKX},
}


@article{aminian2021exact,
  title={An exact characterization of the generalization error for the Gibbs algorithm},
  author={Aminian, Gholamali and Bu, Yuheng and Toni, Laura and Rodrigues, Miguel and Wornell, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={8106--8118},
  year={2021}
}

@article{salimans2016weight,
  title={Weight normalization: A simple reparameterization to accelerate training of deep neural networks},
  author={Salimans, Tim and Kingma, Durk P},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}
@Article{McAllester1999,
author={McAllester, David A.},
title={Some PAC-Bayesian Theorems},
journal={Machine Learning},
year={1999},
month={Dec},
day={01},
volume={37},
number={3},
pages={355-363},
abstract={This paper gives PAC guarantees for ``Bayesian'' algorithms---algorithms that optimize risk minimization expressions involving a prior probability and a likelihood for the training data. PAC-Bayesian algorithms are motivated by a desire to provide an informative prior encoding information about the expected experimental setting but still having PAC performance guarantees over all IID settings. The PAC-Bayesian theorems given here apply to an arbitrary prior measure on an arbitrary concept space. These theorems provide an alternative to the use of VC dimension in proving PAC bounds for parameterized concepts.},
issn={1573-0565},
doi={10.1023/A:1007618624809},
}


@article{alquier2021user,
  title={User-friendly introduction to PAC-Bayes bounds},
  author={Alquier, Pierre},
  journal={arXiv preprint arXiv:2110.11216},
  year={2023}
}


@inproceedings{
frankle2019lottery,
title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
author={Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2019},
}

@book{gibbs1902elementary,
  title={Elementary Principles in Statistical Mechanics: Developed with Especial Reference to the Rational Foundations of Thermodynamics},
  author={Gibbs, J.W.},
  lccn={02008251},
  series={Elementary Principles in Statistical Mechanics: Developed with Especial Reference to the Rational Foundation of Thermodynamics},
  year={1902},
  publisher={C. Scribner's sons}
}


@article{Shun&McCullagh-1995-laplace,
author = {Shun, Zhenming and McCullagh, Peter},
title = {Laplace Approximation of High Dimensional Integrals},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
volume = {57},
number = {4},
pages = {749-760},
keywords = {asymptotic approximation, bipartition, connected bipartition, exchangeable array, laplace approximation, posterior expectation, random effects model},
year = {1995}
}


@article{_api_ski_2019,
	doi = {10.1016/j.jat.2019.105305},
  
  
	year = 2019,
	month = {dec},
  
	publisher = {Elsevier {BV}
},
  
	volume = {248},
  
	pages = {105305},
  
	author = {Tomasz M. {\L}api{\'{n}}ski},
  
	title = {Multivariate Laplace's approximation with estimated error and application to limit theorems},
  
	journal = {Journal of Approximation Theory}
}

@article{10.1167/jov.21.10.1,
    author = {Das, Abhranil and Geisler, Wilson S.},
    title = "{A method to integrate and classify normal distributions}",
    journal = {Journal of Vision},
    volume = {21},
    number = {10},
    pages = {1-1},
    year = {2021},
    month = {09},
    abstract = "{ Univariate and multivariate normal probability distributions are widely used when modeling decisions under uncertainty. Computing the performance of such models requires integrating these distributions over specific domains, which can vary widely across models. Besides some special cases where these integrals are easy to calculate, there exist no general analytical expressions, standard numerical methods, or software for these integrals. Here we present mathematical results and open-source software that provide (a) the probability in any domain of a normal in any dimensions with any parameters; (b) the probability density, cumulative distribution, and inverse cumulative distribution of any function of a normal vector; (c) the classification errors among any number of normal distributions, the Bayes-optimal discriminability index, and relation to the receiver operating characteristic (ROC); (d) dimension reduction and visualizations for such problems; and (e) tests for how reliably these methods may be used on given data. We demonstrate these tools with vision research applications of detecting occluding objects in natural scenes and detecting camouflage. }",
    issn = {1534-7362},
}

@inproceedings{theisen2021good,
  title={Good classifiers are abundant in the interpolating regime},
  author={Theisen, Ryan and Klusowski, Jason and Mahoney, Michael},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3376--3384},
  year={2021},
  organization={PMLR}
}

@inproceedings{
chiang2023loss,
title={Loss Landscapes are All You Need: Neural Network Generalization Can Be Explained Without the Implicit Bias of Gradient Descent},
author={{Ping-yeh} Chiang and Renkun Ni and David Yu Miller and Arpit Bansal and Jonas Geiping and Micah Goldblum and Tom Goldstein},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, S. and Vandenberghe, L.},
  year={2004},
  publisher={Cambridge university press}
}

@article{Sampford1953Mills,
author = {M. R. Sampford},
title = {{Some Inequalities on Mill's Ratio and Related Functions}},
volume = {24},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {130 -- 132},
year = {1953},
doi = {10.1214/aoms/1177729093},
}

@article{Rodomanov&Nesterov2021optimization,
	doi = {10.1007/s10957-020-01805-8},
  
  
	year = 2021,
	month = {jan},
  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {188},
  
	number = {3},
  
	pages = {744--769},
  
	author = {Anton Rodomanov and Yurii Nesterov},
  
	title = {New Results on Superlinear Convergence of Classical Quasi-Newton Methods},
  
	journal = {Journal of Optimization Theory and Applications}
}

@inproceedings{ramanujan2020whats,
  title={What's hidden in a randomly weighted neural network?},
  author={Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11893--11902},
  year={2020}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={pmlr}
}

@inproceedings{malach2020proving,
  title={Proving the lottery ticket hypothesis: Pruning is all you need},
  author={Malach, Eran and Yehudai, Gilad and Shalev-Schwartz, Shai and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={6682--6691},
  year={2020},
  organization={PMLR}
}

@inproceedings{
soudry2017implicit,
title={The Implicit Bias of Gradient Descent on Separable Data},
author={Daniel Soudry and Elad Hoffer and Nathan Srebro},
booktitle={International Conference on Learning Representations},
year={2018},
}

@misc{mingard2023deep,
      title={Do deep neural networks have an inbuilt Occam's razor?}, 
      author={Chris Mingard and Henry Rees and Guillermo Valle-Pérez and Ard A. Louis},
      year={2023},
      eprint={2304.06670},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lotfi2022pacbayes,
  title={PAC-bayes compression bounds so tight that they can explain generalization},
  author={Lotfi, Sanae and Finzi, Marc and Kapoor, Sanyam and Potapczynski, Andres and Goldblum, Micah and Wilson, Andrew G},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31459--31473},
  year={2022}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}


@inproceedings{Maas2013RectifierNI,
  title={Rectifier nonlinearities improve neural network acoustic models},
  author={Maas, Andrew L and Hannun, Awni Y and Ng, Andrew Y and others},
  booktitle={Proc. icml},
  volume={30},
  pages={3},
  year={2013},
  organization={Atlanta, GA}
}

@MISC {2263641,
    TITLE = {If $X,Y$ are independent $\chi ^2$ with $m$ and $n$ degrees of freedom, then $\frac{X}{X+Y} \sim\beta(m/2, n/2)$},
    AUTHOR = {Marcos TV},
    HOWPUBLISHED = {Mathematics Stack Exchange},
    NOTE = {URL:https://math.stackexchange.com/q/2263641 (version: 2017-05-03)},
    EPRINT = {https://math.stackexchange.com/q/2263641},
    URL = {https://math.stackexchange.com/q/2263641},
    year = {2017}
}

@MISC {odeSolMathOverFlow,
    TITLE = {Asymptotic solution for a first order ODE},
    AUTHOR = {Willie Wong},
    HOWPUBLISHED = {MathOverflow},
    NOTE = {URL:https://mathoverflow.net/q/309520 (version: 2018-08-31)},
    EPRINT = {https://mathoverflow.net/q/309520},
    URL = {https://mathoverflow.net/q/309520},
    year={2018}
}


@MISC {255908,
    TITLE = {Taylor expansion of $\arccos(1-x)$ around $x=0$ to two terms},
    AUTHOR = {Mario Carneiro (https://math.stackexchange.com/users/50776/mario-carneiro)},
    HOWPUBLISHED = {Mathematics Stack Exchange},
    NOTE = {URL:https://math.stackexchange.com/q/255908 (version: 2012-12-11)},
    EPRINT = {https://math.stackexchange.com/q/255908},
    URL = {https://math.stackexchange.com/q/255908}
}




@article{blumer1987occam,
  title={Occam's razor},
  author={Blumer, Anselm and Ehrenfeucht, Andrzej and Haussler, David and Warmuth, Manfred K},
  journal={Information processing letters},
  volume={24},
  number={6},
  pages={377--380},
  year={1987},
  publisher={Elsevier}
}

@book{langford2001bounds,
  title={Bounds for averaging classifiers},
  author={Langford, John and Seeger, Matthias},
  year={2002},
  publisher={CMU Technical Report CMU-CS-01-102, 2002}
}

@InProceedings{SimplifiedPAC,
author="McAllester, David",
editor="Sch{\"o}lkopf, Bernhard
and Warmuth, Manfred K.",
title="Simplified PAC-Bayesian Margin Bounds",
booktitle="Learning Theory and Kernel Machines",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="203--215",
abstract="The theoretical understanding of support vector machines is largely based on margin bounds for linear classifiers with unit-norm weight vectors and unit-norm feature vectors. Unit-norm margin bounds have been proved previously using fat-shattering arguments and Rademacher complexity. Recently Langford and Shawe-Taylor proved a dimension-independent unit-norm margin bound using a relatively simple PAC-Bayesian argument. Unfortunately, the Langford-Shawe-Taylor bound is stated in a variational form making direct comparison to fat-shattering bounds difficult. This paper provides an explicit solution to the variational problem implicit in the Langford-Shawe-Taylor bound and shows that the PAC-Bayesian margin bounds are significantly tighter. Because a PAC-Bayesian bound is derived from a particular prior distribution over hypotheses, a PAC-Bayesian margin bound also seems to provide insight into the nature of the learning bias underlying the bound.",
isbn="978-3-540-45167-9"
}


@InProceedings{soudry2017exponentially,
  title={Exponentially vanishing sub-optimal local minima in multilayer neural networks},
  author={Soudry, Daniel and Hoffer, Elad},
  booktitle={ICLR workshop paper},
  year={2017}
}

@inproceedings{neyshabur2015norm,
  title={Norm-based capacity control in neural networks},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  booktitle={Conference on learning theory},
  pages={1376--1401},
  year={2015},
  organization={PMLR}
}

@inproceedings{golowich2018size,
  title={Size-independent sample complexity of neural networks},
  author={Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  booktitle={Conference On Learning Theory},
  pages={297--299},
  year={2018},
  organization={PMLR}
}

@inproceedings{liu2022nonuniform,
  title={Nonuniform-to-uniform quantization: Towards accurate quantization via generalized straight-through estimation},
  author={Liu, Zechun and Cheng, Kwang-Ting and Huang, Dong and Xing, Eric P and Shen, Zhiqiang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4942--4952},
  year={2022}
}

@article{teney2024neural,
  title={Neural Redshift: Random Networks are not Random Functions},
  author={Teney, Damien and Nicolicioiu, Armand and Hartmann, Valentin and Abbasnejad, Ehsan},
  journal={arXiv preprint arXiv:2403.02241},
  year={2024}
}

@article{Berchenko_2024, title={Simplicity Bias in Overparameterized Machine Learning}, volume={38}, url={https://ojs.aaai.org/index.php/AAAI/article/view/28981}, DOI={10.1609/aaai.v38i10.28981}, abstractNote={A thorough theoretical understanding of the surprising generalization ability of deep networks (and other overparameterized models) is still lacking. Here we demonstrate that simplicity bias is a major phenomenon to be reckoned with in overparameterized machine learning. In addition to explaining the outcome of simplicity bias, we also study its source: following concrete rigorous examples, we argue that (i) simplicity bias can explain generalization in overparameterized learning models such as neural networks; (ii) simplicity bias and excellent generalization are optimizer-independent, as our example shows, and although the optimizer affects training, it is not the driving force behind simplicity bias; (iii) simplicity bias in pre-training models, and subsequent posteriors, is universal and stems from the subtle fact that uniformly-at-random constructed priors are not uniformly-at-random sampled ; and (iv) in neural network models, the biasing mechanism in wide (and shallow) networks is different from the biasing mechanism in deep (and narrow) networks.}, number={10}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Berchenko, Yakir}, year={2024}, month={Mar.}, pages={11052-11060} }

@article{udell2019big,
  title={Why are big data matrices approximately low rank?},
  author={Udell, Madeleine and Townsend, Alex},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={1},
  number={1},
  pages={144--160},
  year={2019},
  publisher={SIAM}
}

@article{zeno2024minimum,
  title={How do Minimum-Norm Shallow Denoisers Look in Function Space?},
  author={Zeno, Chen and Ongie, Greg and Blumenfeld, Yaniv and Weinberger, Nir and Soudry, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{nitanda2022gradient,
  title={Gradient Descent can Learn Less Over-parameterized Two-layer Neural Networks on Classification Problems},
  author={Nitanda, Atsushi and Chinot, Geoffrey and Suzuki, Taiji},
  year={2022}
}

@inproceedings{cao2020generalization,
  title={Generalization error bounds of gradient descent for learning over-parameterized deep relu networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={3349--3356},
  year={2020}
}


@article{cantoni2007pac, volume={56},
   url={http://dx.doi.org/10.1214/074921707000000391},
   DOI={10.1214/074921707000000391},
   journal={IMS Lecture Notes Monograph Series},
   publisher={Institute of Mathematical Statistics},
   year={2007},
   pages={1–163},
    title = {{PAC}-{B}ayesian Supervised Classification: The Thermodynamics of Statistical Learning
},
  author={Catoni, Olivier},
  note={arXiv preprint arXiv:0712.0248},
}

@article{GREENBERG201491,
title = {Tight lower bound on the probability of a binomial exceeding its expectation},
journal = {Statistics \& Probability Letters},
volume = {86},
pages = {91-98},
year = {2014},
issn = {0167-7152},
doi = {https://doi.org/10.1016/j.spl.2013.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0167715213004082},
author = {Spencer Greenberg and Mehryar Mohri},
keywords = {Binomial distribution, Lower bound, Expected value, Relative deviation, Machine learning},
abstract = {We give the proof of a tight lower bound on the probability that a binomial random variable exceeds its expected value. The inequality plays an important role in a variety of contexts, including the analysis of relative deviation bounds in learning theory and generalization bounds for unbounded loss functions.}
}