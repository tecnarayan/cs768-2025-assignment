\begin{thebibliography}{10}

\bibitem{sutton}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{sac}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning (ICML'18)}, pages 1856--1865, Stockholmsm{\"{a}}ssan, Sweden, 2018.

\bibitem{td3}
Scott Fujimoto, Herke van Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning (ICML'18)}, pages 1582--1591, Stockholmsm{\"{a}}ssan, Sweden, 2018.

\bibitem{rainbow}
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad~Gheshlaghi Azar, and David
  Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In {\em Proceedings of the 32nd Conference on Artificial Intelligence
  (AAAI'18)}, pages 3215--3222, New Orleans, LA, 2018.

\bibitem{dqn}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A. Rusu, Joel Veness,
  Marc~G. Bellemare, Alex Graves, Martin~A. Riedmiller, Andreas Fidjeland,
  Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis
  Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and
  Demis Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529--533, 2015.

\bibitem{c51}
Marc~G. Bellemare, Will Dabney, and R{\'{e}}mi Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning (ICML'17)}, pages 449--458, Sydney, Australia, 2017.

\bibitem{er}
Long~Ji Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock {\em Journal of Machine Learning Research}, 8:293--321, 1992.

\bibitem{diff}
Angelos Katharopoulos and Fran{\c{c}}ois Fleuret.
\newblock Not all samples are created equal: Deep learning with importance
  sampling.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning (ICML'18)}, pages 2530--2539, Stockholmsm{\"{a}}ssan, Sweden, 2018.

\bibitem{per}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock In {\em Proceedings of the 4th International Conference on Learning
  Representations (ICLR'16)}, San Juan, Puerto Rico, 2016.

\bibitem{sequence}
Marc Brittain, Joshua~R. Bertram, Xuxi Yang, and Peng Wei.
\newblock Prioritized sequence experience replay.
\newblock {\em CoRR}, abs/1905.12726, 2019.

\bibitem{discor}
Aviral Kumar, Abhishek Gupta, and Sergey Levine.
\newblock Discor: Corrective feedback in reinforcement learning via
  distribution correction.
\newblock In {\em Proceedings of 33rd conference on Neural Information
  Processing Systems (NeurIPS'20)}, virtual event, 2020.

\bibitem{gan}
Samarth Sinha, Jiaming Song, Animesh Garg, and Stefano Ermon.
\newblock Experience replay with likelihood-free importance weights.
\newblock {\em CoRR}, abs/2006.13169, 2020.

\bibitem{sweep1}
David Andre, Nir Friedman, and Ronald Parr.
\newblock Generalized prioritized sweeping.
\newblock In {\em proceedings of the 10th conference on Neural Information
  Processing Systems (NeurIPS'97)}, pages 1001--1007, Denver, CO, 1997.

\bibitem{sweep2}
Andrew~W. Moore and Christopher~G. Atkeson.
\newblock Prioritized sweeping: Reinforcement learning with less data and less
  time.
\newblock {\em Journal of Machine Learning Research}, 13:103--130, 1993.

\bibitem{sweep3}
Harm van Seijen and Richard~S. Sutton.
\newblock Planning by prioritized sweeping with small backups.
\newblock In {\em Proceedings of the 30th International Conference on Machine
  Learning (ICML'13)}, pages 361--369, 2013.

\bibitem{ere}
Che Wang, Yanqiu Wu, Quan Vuong, and Keith Ross.
\newblock Striving for simplicity and performance in off-policy {DRL:} output
  normalization and non-uniform sampling.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning (ICML'20)}, pages 10070--10080, virtual event, 2020.

\bibitem{lee2020sunrise}
Kimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel.
\newblock {SUNRISE:} {A} simple unified framework for ensemble learning in deep
  reinforcement learning.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning (ICML'21)}, pages 6131--6141, virtual event, 2021.

\bibitem{bottleneck}
Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine.
\newblock Diagnosing bottlenecks in deep q-learning algorithms.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning (ICML'19)}, pages 2021--2030, Long Beach, CA, 2019.

\bibitem{equivalence}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock An equivalence between loss functions and non-uniform sampling in
  experience replay.
\newblock In {\em Proceedings of the 33rd Annual Conference on Neural
  Information Processing Systems (NeurIPS'20)}, 2020.

\bibitem{deeper}
Shangtong Zhang and Richard~S. Sutton.
\newblock A deeper look at experience replay.
\newblock {\em CoRR}, abs/1712.01275, 2017.

\bibitem{revisit}
William Fedus, Prajit Ramachandran, Rishabh Agarwal, Yoshua Bengio, Hugo
  Larochelle, Mark Rowland, and Will Dabney.
\newblock Revisiting fundamentals of experience replay.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning (ICML'20)}, pages 3061--3071, virtual event, 2020.

\bibitem{bcq}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning (ICML'19)}, pages 2052--2062, Long Beach, CA, 2019.

\bibitem{gym_minigrid}
Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal.
\newblock Minimalistic gridworld environment for openai gym.
\newblock \url{https://github.com/maximecb/gym-minigrid}, 2018.

\bibitem{metaworld}
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea
  Finn, and Sergey Levine.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In {\em Proceedings of the 3rd Conference on Robot Learning
  (CoRL'19)}, Osaka, Japan, 2019.

\bibitem{mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: {A} physics engine for model-based control.
\newblock In {\em Proceedings of 24th International Conference on Intelligent
  Robots and Systems (IROS'12)}, pages 5026--5033, Vilamoura, Portugal, 2012.

\bibitem{tassa2020dmcontrol}
Yuval Tassa, Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu,
  Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, and Nicolas Heess.
\newblock dm-control: Software and tasks for continuous control, 2020.

\bibitem{tianshou}
Jiayi Weng, Huayu Chen, Alexis Duburcq, Kaichao You, Minghao Zhang, Dong Yan,
  Hang Su, and Jun Zhu.
\newblock Tianshou.
\newblock \url{https://github.com/thu-ml/tianshou}, 2020.

\bibitem{kakade}
Sham~M. Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In {\em Proceedings of the 19th International Conference on Machine
  Learning (ICML'02)}, pages 267--274, Sydney, Australia, 2002.

\bibitem{aggregate}
Shi Dong, Benjamin~Van Roy, and Zhengyuan Zhou.
\newblock Provably efficient reinforcement learning with aggregated states.
\newblock {\em CoRR}, abs/1912.06366, 2019.

\bibitem{abstract}
Nan Jiang, Alex Kulesza, and Satinder~P. Singh.
\newblock Abstraction selection in model-based reinforcement learning.
\newblock In {\em Proceedings of the 32nd International Conference on Machine
  Learning (ICML'15)}, pages 179--188, Lille, France, 2015.

\end{thebibliography}
