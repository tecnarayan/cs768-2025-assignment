\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2009)Agarwal, Wainwright, Bartlett, and
  Ravikumar]{agarwal2009information}
Alekh Agarwal, Martin~J Wainwright, Peter~L Bartlett, and Pradeep~K Ravikumar.
\newblock Information-theoretic lower bounds on the oracle complexity of convex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1--9, 2009.

\bibitem[Arjevani et~al.(2019)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{arjevani2019lower}
Yossi Arjevani, Yair Carmon, John~C Duchi, Dylan~J Foster, Nathan Srebro, and
  Blake Woodworth.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1912.02365}, 2019.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{pmlr-v70-arjovsky17a}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock {W}asserstein generative adversarial networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, volume~70, pages 214--223, International Convention Centre,
  Sydney, Australia, 2017.

\bibitem[Chambolle and Pock(2011)]{Chambolle:2011:FPA:1968993.1969036}
Antonin Chambolle and Thomas Pock.
\newblock A first-order primal-dual algorithm for convex problems with
  applications to imaging.
\newblock \emph{J. Math. Imaging Vis.}, 40\penalty0 (1):\penalty0 120--145, May
  2011.
\newblock ISSN 0924-9907.
\newblock \doi{10.1007/s10851-010-0251-1}.
\newblock URL \url{http://dx.doi.org/10.1007/s10851-010-0251-1}.

\bibitem[Chen et~al.(2014)Chen, Lan, and Ouyang]{chen2014optimal}
Yunmei Chen, Guanghui Lan, and Yuyuan Ouyang.
\newblock Optimal primal-dual methods for a class of saddle point problems.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (4):\penalty0
  1779--1814, 2014.

\bibitem[Dang and Lan(2014)]{dang2014randomized}
Cong Dang and Guanghui Lan.
\newblock Randomized first-order methods for saddle point optimization.
\newblock \emph{arXiv preprint arXiv:1409.8625}, 2014.

\bibitem[Du and Hu(2018)]{du2018linear}
Simon~S Du and Wei Hu.
\newblock Linear convergence of the primal-dual gradient method for
  convex-concave saddle point problems without strong convexity.
\newblock \emph{arXiv preprint arXiv:1802.01504}, 2018.

\bibitem[Fan et~al.(2017)Fan, Lyu, Ying, and Hu]{fanNIPS2017_6653}
Yanbo Fan, Siwei Lyu, Yiming Ying, and Baogang Hu.
\newblock Learning with average top-k loss.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pages
  497--505. 2017.

\bibitem[Gidel et~al.(2016)Gidel, Jebara, and Lacoste-Julien]{gidel2016frank}
Gauthier Gidel, Tony Jebara, and Simon Lacoste-Julien.
\newblock Frank-wolfe algorithms for saddle point problems.
\newblock \emph{arXiv preprint arXiv:1610.07797}, 2016.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and
  Bengio]{Goodfellow:2014:GAN:2969033.2969125}
Ian~J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
  Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Proceedings of the 27th International Conference on Neural
  Information Processing Systems - Volume 2}, NIPS'14, pages 2672--2680,
  Cambridge, MA, USA, 2014. MIT Press.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=2969033.2969125}.

\bibitem[Hajinezhad and Hong(2019)]{hajinezhad2019perturbed}
Davood Hajinezhad and Mingyi Hong.
\newblock Perturbed proximal primal--dual algorithm for nonconvex nonsmooth
  optimization.
\newblock \emph{Mathematical Programming}, 176\penalty0 (1-2):\penalty0
  207--245, 2019.

\bibitem[Hamedani and Aybat(2018)]{hamedani2018primal}
Erfan~Yazdandoost Hamedani and Necdet~Serhat Aybat.
\newblock A primal-dual algorithm for general convex-concave saddle point
  problems.
\newblock \emph{arXiv preprint arXiv:1803.01401}, 2018.

\bibitem[Hazan and Kale(2011)]{hazan-2011-beyond}
Elad Hazan and Satyen Kale.
\newblock Beyond the regret minimization barrier: an optimal algorithm for
  stochastic strongly-convex optimization.
\newblock In \emph{Proceedings of the 24th Annual Conference on Learning Theory
  (COLT)}, pages 421--436, 2011.

\bibitem[Hazan and Kale(2014)]{hazan2014beyond}
Elad Hazan and Satyen Kale.
\newblock Beyond the regret minimization barrier: optimal algorithms for
  stochastic strongly-convex optimization.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 2489--2512, 2014.

\bibitem[Hien et~al.(2017)Hien, Zhao, and Haskell]{hien2017inexact}
Le~Thi~Khanh Hien, Renbo Zhao, and William~B Haskell.
\newblock An inexact primal-dual smoothing framework for large-scale
  non-bilinear saddle point problems.
\newblock \emph{arXiv preprint arXiv:1711.03669}, 2017.

\bibitem[Hong(2016)]{hong2016decomposing}
Mingyi Hong.
\newblock Decomposing linearly constrained nonconvex problems by a proximal
  primal dual approach: Algorithms, convergence, and applications.
\newblock \emph{arXiv preprint arXiv:1604.00543}, 2016.

\bibitem[Hong et~al.(2018)Hong, Lee, and Razaviyayn]{hong2018gradient}
Mingyi Hong, Jason~D Lee, and Meisam Razaviyayn.
\newblock Gradient primal-dual algorithm converges to second-order stationary
  solutions for nonconvex distributed optimization.
\newblock \emph{arXiv preprint arXiv:1802.08941}, 2018.

\bibitem[Hsieh et~al.(2019)Hsieh, Iutzeler, Malick, and
  Mertikopoulos]{hsieh2019convergence}
Yu-Guan Hsieh, Franck Iutzeler, J{\'e}r{\^o}me Malick, and Panayotis
  Mertikopoulos.
\newblock On the convergence of single-call stochastic extra-gradient methods.
\newblock \emph{arXiv preprint arXiv:1908.08465}, 2019.

\bibitem[Juditsky et~al.(2011)Juditsky, Nemirovski, and
  Tauvel]{juditsky2011solving}
Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel.
\newblock Solving variational inequalities with stochastic mirror-prox
  algorithm.
\newblock \emph{Stochastic Systems}, 1\penalty0 (1):\penalty0 17--58, 2011.

\bibitem[Lan et~al.(2012)Lan, Nemirovski, and Shapiro]{lan2012validation}
Guanghui Lan, Arkadi Nemirovski, and Alexander Shapiro.
\newblock Validation analysis of mirror descent stochastic approximation
  method.
\newblock \emph{Mathematical programming}, 134\penalty0 (2):\penalty0 425--458,
  2012.

\bibitem[Lin et~al.(2019)Lin, Jin, and
  Jordan]{DBLP:journals/corr/abs-1906-00331}
Tianyi Lin, Chi Jin, and Michael~I. Jordan.
\newblock On gradient descent ascent for nonconvex-concave minimax problems.
\newblock \emph{CoRR}, abs/1906.00331, 2019.

\bibitem[Liu et~al.(2018)Liu, Zhang, Chen, Wang, and Yang]{fastAUC18}
Mingrui Liu, Xiaoxuan Zhang, Zaiyi Chen, Xiaoyu Wang, and Tianbao Yang.
\newblock Fast stochastic auc maximization with o(1/n)-convergence rate.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, 2018.

\bibitem[Liu et~al.(2019)Liu, Yuan, Ying, and Yang]{liu2019stochastic}
Mingrui Liu, Zhuoning Yuan, Yiming Ying, and Tianbao Yang.
\newblock Stochastic auc maximization with deep neural networks.
\newblock \emph{arXiv preprint arXiv:1908.10831}, 2019.

\bibitem[Lu et~al.(2019)Lu, Tsaknakis, Hong, and Chen]{lu2019hybrid}
Songtao Lu, Ioannis Tsaknakis, Mingyi Hong, and Yongxin Chen.
\newblock Hybrid block successive approximation for one-sided non-convex
  min-max problems: algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1902.08294}, 2019.

\bibitem[Luo~Luo(2020)]{arXiv:2001.03724}
Tong~Zhang Luo~Luo, Haishan~Ye.
\newblock Stochastic recursive gradient descent ascent for stochastic
  nonconvex-strongly-concave minimax problems.
\newblock \emph{CoRR}, abs/2001.03724, 2020.

\bibitem[Namkoong and Duchi(2016)]{DBLP:conf/nips/NamkoongD16}
Hongseok Namkoong and John~C. Duchi.
\newblock Stochastic gradient methods for distributionally robust optimization
  with f-divergences.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 2208--2216, 2016.

\bibitem[Namkoong and Duchi(2017)]{namkoongnips2017variance}
Hongseok Namkoong and John~C. Duchi.
\newblock Variance-based regularization with convex objectives.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 2975--2984, 2017.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{Nemirovski:2009:RSA:1654243.1654247}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19:\penalty0 1574--1609, 2009.
\newblock URL \url{http://dx.doi.org/10.1137/070704277}.

\bibitem[Nesterov(2005)]{nesterovexces}
Yu~Nesterov.
\newblock Excessive gap technique in nonsmooth convex minimization.
\newblock \emph{SIAM Journal on Optimization}, 16:\penalty0 235--249, 01 2005.
\newblock \doi{10.1137/S1052623403422285}.

\bibitem[Nouiehed et~al.(2019)Nouiehed, Sanjabi, Huang, Lee, and
  Razaviyayn]{nouiehed2019solving}
Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason~D Lee, and Meisam
  Razaviyayn.
\newblock Solving a class of non-convex min-max games using iterative first
  order methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  14905--14916, 2019.

\bibitem[Palaniappan and Bach(2016)]{DBLP:conf/nips/PalaniappanB16}
Balamurugan Palaniappan and Francis~R. Bach.
\newblock Stochastic variance reduction methods for saddle-point problems.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pages 1408--1416, 2016.

\bibitem[Rafique et~al.(2018)Rafique, Liu, Lin, and Yang]{hassan18nonconvexmm}
Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang.
\newblock Non-convex min-max optimization: Provable algorithms and applications
  in machine learning.
\newblock \emph{CoRR}, abs/1810.02060, 2018.

\bibitem[Shalev-Shwartz and Zhang(2013)]{citeulike:11703902}
Shai Shalev-Shwartz and Tong Zhang.
\newblock {Stochastic Dual Coordinate Ascent Methods for Regularized Loss
  Minimization}.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 2013.

\bibitem[Tan et~al.(2018)Tan, Zhang, Ma, and Liu]{tan2018stochastic}
Conghui Tan, Tong Zhang, Shiqian Ma, and Ji~Liu.
\newblock Stochastic primal-dual method for empirical risk minimization with o
  (1) per-iteration complexity.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8366--8375, 2018.

\bibitem[Wang and Xiao(2017)]{wang2017exploiting}
Jialei Wang and Lin Xiao.
\newblock Exploiting strong convexity from data with primal-dual first-order
  algorithms.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3694--3702. JMLR. org, 2017.

\bibitem[Yan et~al.(2019)Yan, Xu, Lin, Zhang, and Yang]{yan2019stochastic}
Yan Yan, Yi~Xu, Qihang Lin, Lijun Zhang, and Tianbao Yang.
\newblock Stochastic primal-dual algorithms with faster convergence than $ o
  (1/\sqrt{T})$ for problems without bilinear structure.
\newblock \emph{arXiv preprint arXiv:1904.10112}, 2019.

\bibitem[Yang et~al.(2015)Yang, Mahdavi, Jin, and
  Zhu]{DBLP:journals/ml/YangMJZ15}
Tianbao Yang, Mehrdad Mahdavi, Rong Jin, and Shenghuo Zhu.
\newblock An efficient primal dual prox method for non-smooth optimization.
\newblock \emph{Machine Learning}, 98\penalty0 (3):\penalty0 369--406, 2015.

\bibitem[Ying et~al.(2016)Ying, Wen, and Lyu]{ying2016stochastic}
Yiming Ying, Longyin Wen, and Siwei Lyu.
\newblock Stochastic online auc maximization.
\newblock In \emph{Advances in neural information processing systems}, pages
  451--459, 2016.

\bibitem[Yu et~al.(2015)Yu, Lin, and Yang]{DBLP:journals/corr/YuLY15}
Adams~Wei Yu, Qihang Lin, and Tianbao Yang.
\newblock Doubly stochastic primal-dual coordinate method for regularized
  empirical risk minimization with factorized data.
\newblock \emph{CoRR}, abs/1508.03390, 2015.
\newblock URL \url{http://arxiv.org/abs/1508.03390}.

\bibitem[Zhang and Xiao(2017)]{zhang2017stochastic}
Yuchen Zhang and Lin Xiao.
\newblock Stochastic primal-dual coordinate method for regularized empirical
  risk minimization.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 2939--2980, 2017.

\bibitem[Zhao(2019)]{zhao2019optimal}
Renbo Zhao.
\newblock Optimal algorithms for stochastic three-composite convex-concave
  saddle point problems.
\newblock \emph{arXiv preprint arXiv:1903.01687}, 2019.

\end{thebibliography}
