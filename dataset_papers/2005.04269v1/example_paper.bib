@inproceedings{czarnecki2017sobolev,
  title={Sobolev training for neural networks},
  author={Czarnecki, Wojciech M and Osindero, Simon and Jaderberg, Max and Swirszcz, Grzegorz and Pascanu, Razvan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4278--4287},
  year={2017}
}


@article{hyvarinen2005estimation,
  title={Estimation of non-normalized statistical models by score matching},
  author={Hyv{\"a}rinen, Aapo},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={Apr},
  pages={695--709},
  year={2005}
}

@inproceedings{jaderberg2017decoupled,
  title={Decoupled neural interfaces using synthetic gradients},
  author={Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1627--1635},
  year={2017},
  organization={JMLR. org}
}

@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}

@article{gallant1992learning,
  title={On learning the derivatives of an unknown mapping with multilayer feedforward networks},
  author={Gallant, A Ronald and White, Halbert},
  journal={Neural Networks},
  volume={5},
  number={1},
  pages={129--138},
  year={1992},
  publisher={Elsevier}
}

@article{srinivas2018knowledge,
  title={Knowledge transfer with jacobian matching},
  author={Srinivas, Suraj and Fleuret, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1803.00443},
  year={2018}
}

@inproceedings{fairbank2012value,
  title={Value-gradient learning},
  author={Fairbank, Michael and Alonso, Eduardo},
  booktitle={The 2012 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2012},
  organization={IEEE}
}

@inproceedings{heess2015learning,
  title={Learning continuous control policies by stochastic value gradients},
  author={Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Timothy and Erez, Tom and Tassa, Yuval},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2944--2952},
  year={2015}
}


@InProceedings{fujimoto18a,
  title = 	 {Addressing Function Approximation Error in Actor-Critic Methods},
  author = 	 {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1587--1596},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsm√§ssan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/fujimoto18a/fujimoto18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/fujimoto18a.html},
  abstract = 	 {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.}
}

@inproceedings{mannor2004bias,
  title={Bias and variance in value function estimation},
  author={Mannor, Shie and Simester, Duncan and Sun, Peng and Tsitsiklis, John N},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={72},
  year={2004},
  organization={ACM}
}

@article{asadi2018lipschitz,
  title={Lipschitz continuity in model-based reinforcement learning},
  author={Asadi, Kavosh and Misra, Dipendra and Littman, Michael L},
  journal={arXiv preprint arXiv:1804.07193},
  year={2018}
}

@article{cobbe2018quantifying,
  title={Quantifying generalization in reinforcement learning},
  author={Cobbe, Karl and Klimov, Oleg and Hesse, Chris and Kim, Taehoon and Schulman, John},
  journal={arXiv preprint arXiv:1812.02341},
  year={2018}
}

@article{nachum2018smoothed,
  title={Smoothed action value functions for learning gaussian policies},
  author={Nachum, Ofir and Norouzi, Mohammad and Tucker, George and Schuurmans, Dale},
  journal={arXiv preprint arXiv:1803.02348},
  year={2018}
}


@phdthesis{elliott2018wisdom,
  title={Wisdom of the crowd: reliable deep reinforcement learning through ensembles of Q-functions, The},
  author={Elliott, Daniel L},
  year={2018},
  school={Colorado State University. Libraries}
}

@article{bhatt2019crossnorm,
  title={CrossNorm: Normalization for Off-Policy TD Reinforcement Learning},
  author={Bhatt, Aditya and Argus, Max and Amiranashvili, Artemij and Brox, Thomas},
  journal={arXiv preprint arXiv:1902.05605},
  year={2019}
}

@article{geist2019theory,
  title={A Theory of Regularized Markov Decision Processes},
  author={Geist, Matthieu and Scherrer, Bruno and Pietquin, Olivier},
  journal={arXiv preprint arXiv:1901.11275},
  year={2019}
}

@article{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}

@article{haarnoja2018applications,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015}
}

@inproceedings{czarnecki2017understanding,
  title={Understanding synthetic gradients and decoupled neural interfaces},
  author={Czarnecki, Wojciech Marian and Swirszcz, Grzegorz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={904--912},
  year={2017},
  organization={JMLR. org}
}


@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}


@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{bellemare2017distributional,
       author = {{Bellemare}, Marc G. and {Dabney}, Will and {Munos}, R{\'e}mi},
        title = "{A Distributional Perspective on Reinforcement Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
         year = "2017",
        month = "Jul",
          eid = {arXiv:1707.06887},
        pages = {arXiv:1707.06887},
archivePrefix = {arXiv},
       eprint = {1707.06887},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170706887B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}




@article{stone1974cross,
  title={Cross-validatory choice and assessment of statistical predictions},
  author={Stone, Mervyn},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={36},
  number={2},
  pages={111--133},
  year={1974},
  publisher={Wiley Online Library}
}

@article{ishwaei1985non,
  title={Non-existence of unbiased estimators of ordered parameters},
  author={Ishwaei D, Bhaeiyal and Shabma, Divakar and Krishnamoorthy, K},
  journal={Statistics: A Journal of Theoretical and Applied Statistics},
  volume={16},
  number={1},
  pages={89--95},
  year={1985},
  publisher={Taylor \& Francis}
}

@inproceedings{hasselt2010double,
  title={Double Q-learning},
  author={Van Hasselt, Hado},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2613--2621},
  year={2010}
}

@article{van2013estimating,
  title={Estimating the maximum expected value: an analysis of (nested) cross validation and the maximum sample average},
  author={Van Hasselt, Hado},
  journal={arXiv preprint arXiv:1302.7175},
  year={2013}
}

@ARTICLE{hasselt2015double,
       author = {{Van Hasselt}, Hado and {Guez}, Arthur and {Silver}, David},
        title = "{Deep Reinforcement Learning with Double Q-learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = "2015",
        month = "Sep",
          eid = {arXiv:1509.06461},
        pages = {arXiv:1509.06461},
archivePrefix = {arXiv},
       eprint = {1509.06461},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv150906461V},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{fortunato2017noisydqn,
       author = {{Fortunato}, Meire and {Gheshlaghi Azar}, Mohammad and {Piot}, Bilal and
         {Menick}, Jacob and {Osband}, Ian and {Graves}, Alex and {Mnih}, Vlad and
         {Munos}, Remi and {Hassabis}, Demis and {Pietquin}, Olivier and
         {Blundell}, Charles and {Legg}, Shane},
        title = "{Noisy Networks for Exploration}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2017",
        month = "Jun",
          eid = {arXiv:1706.10295},
        pages = {arXiv:1706.10295},
archivePrefix = {arXiv},
       eprint = {1706.10295},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv170610295F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{fu2019diagnosing,
       author = {{Fu}, Justin and {Kumar}, Aviral and {Soh}, Matthew and {Levine}, Sergey},
        title = "{Diagnosing Bottlenecks in Deep Q-learning Algorithms}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
         year = "2019",
        month = "Feb",
          eid = {arXiv:1902.10250},
        pages = {arXiv:1902.10250},
archivePrefix = {arXiv},
       eprint = {1902.10250},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190210250F},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{dabney2018distributional,
  title={Distributional reinforcement learning with quantile regression},
  author={Dabney, Will and Rowland, Mark and Bellemare, Marc G and Munos, R{\'e}mi},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}


@article{barth2018distributed,
  title={Distributed distributional deterministic policy gradients},
  author={Barth-Maron, Gabriel and Hoffman, Matthew W and Budden, David and Dabney, Will and Horgan, Dan and Muldal, Alistair and Heess, Nicolas and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1804.08617},
  year={2018}
}


@inproceedings{choi2019distributional,
  title={Distributional Deep Reinforcement Learning with a Mixture of Gaussians},
  author={Choi, Yunho and Lee, Kyungjae and Oh, Songhwai},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={9791--9797},
  year={2019},
  organization={IEEE}
}

@article{dabney2018implicit,
  title={Implicit quantile networks for distributional reinforcement learning},
  author={Dabney, Will and Ostrovski, Georg and Silver, David and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:1806.06923},
  year={2018}
}

@inproceedings{yang2019fully,
  title={Fully Parameterized Quantile Function for Distributional Reinforcement Learning},
  author={Yang, Derek and Zhao, Li and Lin, Zichuan and Qin, Tao and Bian, Jiang and Liu, Tie-Yan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6190--6199},
  year={2019}
}


@inproceedings{zhang2019quota,
  title={QUOTA: The quantile option architecture for reinforcement learning},
  author={Zhang, Shangtong and Yao, Hengshuai},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={5797--5804},
  year={2019}
}

@InProceedings{mavrin2019distributional,
  title =        {Distributional Reinforcement Learning for Efficient Exploration},
  author =       {Mavrin, Borislav and Yao, Hengshuai and Kong, Linglong and Wu, Kaiwen and Yu, Yaoliang},
  booktitle =    {Proceedings of the 36th International Conference on Machine Learning},
  pages =        {4424--4434},
  year =         {2019},
  editor =       {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume =       {97},
  series =       {Proceedings of Machine Learning Research},
  address =      {Long Beach, California, USA},
  month =        {09--15 Jun},
  publisher =    {PMLR},
  abstract =     {In distributional reinforcement learning (RL), the estimated distribution of value functions model both the parametric and intrinsic uncertainties. We propose a novel and efficient exploration method for deep RL that has two components. The first is a decaying schedule to suppress the intrinsic uncertainty. The second is an exploration bonus calculated from the upper quantiles of the learned distribution. In Atari 2600 games, our method achieves 483 % average gain across 49 games in cumulative rewards over QR-DQN. We also compared our algorithm with QR-DQN in a challenging 3D driving simulator (CARLA). Results show that our algorithm achieves nearoptimal safety rewards twice faster than QRDQN.}
}


@article{horgan2018distributed,
  title={Distributed prioritized experience replay},
  author={Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and Van Hasselt, Hado and Silver, David},
  journal={arXiv preprint arXiv:1803.00933},
  year={2018}
}

@incollection{dpo19nips,
title = {Distributional Policy Optimization: An Alternative Approach for Continuous Control},
author = {Tessler, Chen and Tennenholtz, Guy and Mannor, Shie},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {1350--1360},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@inproceedings{li2019mixing,
  title={Mixing Update Q-value for Deep Reinforcement Learning},
  author={Li, Zhunan and Hou, Xinwen},
  booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--6},
  year={2019},
  organization={IEEE}
}

@inproceedings{kumar2019stabilizing,
  title={Stabilizing off-policy q-learning via bootstrapping error reduction},
  author={Kumar, Aviral and Fu, Justin and Soh, Matthew and Tucker, George and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11761--11771},
  year={2019}
}

@inproceedings{thrun1993issues,
  title={Issues in using function approximation for reinforcement learning},
  author={Thrun, Sebastian and Schwartz, Anton},
  booktitle={Proceedings of the 1993 Connectionist Models Summer School Hillsdale, NJ. Lawrence Erlbaum},
  year={1993}
}

@article{sabry2019reduction,
  title={On the Reduction of Variance and Overestimation of Deep Q-Learning},
  author={Sabry, Mohammed and Khalifa, Amr},
  journal={arXiv preprint arXiv:1910.05983},
  year={2019}
}

@inproceedings{anschel2017averaged,
  title={Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning},
  author={Anschel, Oron and Baram, Nir and Shimkin, Nahum},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={176--185},
  year={2017},
  organization={JMLR. org}
}



@article{patnaik2008q,
  title={Q learning in context of approximation spaces},
  author={Patnaik, KS and Anwar, Shamama},
  journal={Contemporary Engineering Sciences},
  volume={1},
  number={1},
  pages={41--49},
  year={2008},
  publisher={Citeseer}
}


@article{smith2006optimizer,
  title={The optimizer‚Äôs curse: Skepticism and postdecision surprise in decision analysis},
  author={Smith, James E and Winkler, Robert L},
  journal={Management Science},
  volume={52},
  number={3},
  pages={311--322},
  year={2006},
  publisher={INFORMS}
}

@book{thaler2012winner,
  title={The winner's curse: Paradoxes and anomalies of economic life},
  author={Thaler, Richard},
  year={2012},
  publisher={Simon and Schuster}
}

@incollection{azar2011speedy,
title = {Speedy Q-Learning},
author = {Ghavamzadeh, Mohammad and Hilbert J. Kappen and Mohammad G. Azar and R\'{e}mi Munos},
booktitle = {Advances in Neural Information Processing Systems 24},
editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
pages = {2411--2419},
year = {2011},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4251-speedy-q-learning.pdf}
}


@inproceedings{lee2012intelligent,
  title={An intelligent battery controller using bias-corrected Q-learning},
  author={Lee, Donghun and Powell, Warren B},
  booktitle={Twenty-Sixth AAAI Conference on Artificial Intelligence},
  year={2012}
}

@inproceedings{buckman2018sample,
  title={Sample-efficient reinforcement learning with stochastic ensemble value expansion},
  author={Buckman, Jacob and Hafner, Danijar and Tucker, George and Brevdo, Eugene and Lee, Honglak},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8224--8234},
  year={2018}
}

@inproceedings{d2017estimating,
  title={Estimating the maximum expected value in continuous reinforcement learning problems},
  author={D'Eramo, Carlo and Nuara, Alessandro and Pirotta, Matteo and Restelli, Marcello},
  booktitle={Thirty-First AAAI Conference on Artificial Intelligence},
  year={2017}
}

@inproceedings{discrete2016estimating,
  title={Estimating maximum expected value through gaussian approximation},
  author={D‚ÄôEramo, Carlo and Restelli, Marcello and Nuara, Alessandro},
  booktitle={International Conference on Machine Learning},
  pages={1032--1040},
  year={2016}
}

@inproceedings{zhang2017weighted,
  title={Weighted Double Q-learning.},
  author={Zhang, Zongzhang and Pan, Zhiyuan and Kochenderfer, Mykel J},
  booktitle={IJCAI},
  pages={3455--3461},
  year={2017}
}

@inproceedings{kaufmann2018sequential,
  title={Sequential test for the lowest mean: From Thompson to Murphy sampling},
  author={Kaufmann, Emilie and Koolen, Wouter M and Garivier, Aur{\'e}lien},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6332--6342},
  year={2018}
}

@inproceedings{imagaw2017estimating,
  title={Estimating the maximum expected value through upper confidence bound of likelihood},
  author={Imagaw, Takahisa and Kaneko, Tomoyuki},
  booktitle={2017 Conference on Technologies and Applications of Artificial Intelligence (TAAI)},
  pages={202--207},
  year={2017},
  organization={IEEE}
}

@ARTICLE{lv2019stochdoubledqn,
author={P. {Lv} and X. {Wang} and Y. {Cheng} and Z. {Duan}},
journal={IEEE Access},
title={Stochastic Double Deep Q-Network},
year={2019},
volume={7},
number={},
pages={79446-79454},
keywords={estimation theory;learning (artificial intelligence);neural nets;stochastic processes;estimation bias;reinforcement learning algorithms;stochastic double deep Q-learning network;SDDQN;double estimator framework;random selection parameter;Estimation;Linear programming;Reinforcement learning;Games;Neural networks;Data mining;Estimation bias;deep reinforcement learning;maximum operation;double estimator operation;stochastic combination},
doi={10.1109/ACCESS.2019.2922706},
ISSN={2169-3536},
month={},}

@inproceedings{he2019interleaved,
  title={Interleaved Q-Learning with Partially Coupled Training Process},
  author={He, Min and Guo, Hongliang},
  booktitle={Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  pages={449--457},
  year={2019},
  organization={International Foundation for Autonomous Agents and Multiagent Systems}
}

@article{richter2019learning,
  title={Learning Policies through Quantile Regression},
  author={Richter, Oliver and Wattenhofer, Roger},
  journal={arXiv preprint arXiv:1906.11941},
  year={2019}
}

@article{agarwal2019striving,
  title={Striving for simplicity in off-policy deep reinforcement learning},
  author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:1907.04543},
  year={2019}
}

@article{bodnar2019quantile,
  title={Quantile QT-Opt for Risk-Aware Vision-Based Robotic Grasping},
  author={Bodnar, Cristian and Li, Adrian and Hausman, Karol and Pastor, Peter and Kalakrishnan, Mrinal},
  journal={arXiv preprint arXiv:1910.02787},
  year={2019}
}

@inproceedings{hessel2018rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{kalweit2017uncertainty,
  title={Uncertainty-driven imagination for continuous deep reinforcement learning},
  author={Kalweit, Gabriel and Boedecker, Joschka},
  booktitle={Conference on Robot Learning},
  pages={195--206},
  year={2017}
}

@inproceedings{osband2016deep,
  title={Deep exploration via bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={4026--4034},
  year={2016}
}

@inproceedings{
    lan2020maxmin,
    title={Maxmin Q-learning: Controlling the Estimation Bias of Q-learning},
    author={Qingfeng Lan and Yangchen Pan and Alona Fyshe and Martha White},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=Bkg0u3Etwr}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}
@article{white1988mean,
  title={Mean, variance, and probabilistic criteria in finite Markov decision processes: a review},
  author={White, DJ},
  journal={Journal of Optimization Theory and Applications},
  volume={56},
  number={1},
  pages={1--29},
  year={1988},
  publisher={Springer}
}
@article{mania2018simple,
  title={Simple random search provides a competitive approach to reinforcement learning},
  author={Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  journal={arXiv preprint arXiv:1803.07055},
  year={2018}
}