\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amari(1998)]{amari1998natural}
S.~I. Amari.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural computation}, 1998.

\bibitem[Bartlett and Mendelson(2003)]{bartlett2003rademacher}
P.~L. Bartlett and S.~Mendelson.
\newblock Rademacher and {Gauss}ian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 2003.

\bibitem[Beck and Teboulle(2003)]{beck2003mirror}
A.~Beck and M.~Teboulle.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock \emph{Operations Research Letters}, 2003.

\bibitem[Boyd and Vandenberghe(2004)]{boyd2004convex}
S.~Boyd and L.~Vandenberghe.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Bregman(1967)]{bregman1967relaxation}
L.~M. Bregman.
\newblock The relaxation method of finding the common point of convex sets and
  its application to the solution of problems in convex programming.
\newblock \emph{USSR computational mathematics and mathematical physics}, 1967.

\bibitem[Candes and Recht(2009)]{candes2009exact}
E.~J. Candes and B.~Recht.
\newblock Exact matrix completion via convex optimization.
\newblock \emph{Foundations of Computational Mathematics}, 2009.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 2011.

\bibitem[Efron et~al.(2004)Efron, Hastie, Johnstone, and
  Tibshirani]{efron2004least}
B.~Efron, T.~Hastie, I.~Johnstone, and R.~Tibshirani.
\newblock Least angle regression.
\newblock \emph{The Annals of statistics}, 2004.

\bibitem[Friedman(2001)]{friedman2001greedy}
Jerome~H Friedman.
\newblock Greedy function approximation: a gradient boosting machine.
\newblock \emph{Annals of statistics}, 2001.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6152--6160, 2017.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{Hoffer2017}
Elad Hoffer, I~Hubara, and D.~Soudry.
\newblock {Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks}.
\newblock In \emph{NIPS}, pages 1--13, may 2017.
\newblock URL \url{http://arxiv.org/abs/1705.08741}.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Kingma and Adam(2015)]{kingma2015adam}
D~Kingma and Jimmy~Ba Adam.
\newblock Adam: A method for stochastic optimisation.
\newblock In \emph{International Conference for Learning Representations},
  volume~6, 2015.

\bibitem[Kivinen and Warmuth(1997)]{kivinen1997exponentiated}
Jyrki Kivinen and Manfred~K Warmuth.
\newblock Exponentiated gradient versus gradient descent for linear predictors.
\newblock \emph{Information and Computation}, 1997.

\bibitem[Li et~al.(2017)Li, Ma, and Zhang]{li2017algorithmic}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix recovery.
\newblock \emph{arXiv preprint arXiv:1712.09203}, 2017.

\bibitem[Muresan and Muresan(2009)]{muresan2009concrete}
Marian Muresan and Marian Muresan.
\newblock \emph{A concrete approach to classical analysis}, volume~14.
\newblock Springer, 2009.

\bibitem[Nemirovskii and Yudin(1983)]{nemirovskii1983problem}
A.~Nemirovskii and D.~Yudin.
\newblock \emph{Problem complexity and method efficiency in optimization}.
\newblock Wiley, 1983.

\bibitem[Nesterov(1983)]{nesterov1983method}
Yurii Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate o (1/k2).
\newblock In \emph{Soviet Mathematics Doklady}, 1983.

\bibitem[Neyshabur et~al.(2015{\natexlab{a}})Neyshabur, Salakhutdinov, and
  Srebro]{neyshabur2015path}
Behnam Neyshabur, Ruslan~R Salakhutdinov, and Nati Srebro.
\newblock Path-sgd: Path-normalized optimization in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2422--2430, 2015{\natexlab{a}}.

\bibitem[Neyshabur et~al.(2015{\natexlab{b}})Neyshabur, Tomioka, and
  Srebro]{neyshabur2015search}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock In \emph{International Conference on Learning Representations},
  2015{\natexlab{b}}.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Tomioka, Salakhutdinov, and
  Srebro]{neyshabur2017geometry}
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro.
\newblock Geometry of optimization and implicit regularization in deep
  learning.
\newblock \emph{arXiv preprint arXiv:1705.03071}, 2017.

\bibitem[Polyak(1964)]{polyak1964some}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics}, 1964.

\bibitem[Recht et~al.(2010)Recht, Fazel, and Parrilo]{recht2010guaranteed}
B.~Recht, M.~Fazel, and P.~A. Parrilo.
\newblock Guaranteed minimum-rank solutions of linear matrix equations via
  nuclear norm minimization.
\newblock \emph{SIAM review}, 2010.

\bibitem[Ross(1980)]{ross1980elementary}
Kenneth~A Ross.
\newblock \emph{Elementary analysis}.
\newblock Springer, 1980.

\bibitem[Rosset et~al.(2004)Rosset, Zhu, and Hastie]{rosset2004boosting}
S.~Rosset, J.~Zhu, and T.~Hastie.
\newblock Boosting as a regularized path to a maximum margin classifier.
\newblock \emph{Journal of Machine Learning Research}, 2004.

\bibitem[Rudin et~al.(2004)Rudin, Daubechies, and Schapire]{rudin2004dynamics}
Cynthia Rudin, Ingrid Daubechies, and Robert~E Schapire.
\newblock The dynamics of adaboost: Cyclic behavior and convergence of margins.
\newblock \emph{Journal of Machine Learning Research}, 5\penalty0
  (Dec):\penalty0 1557--1595, 2004.

\bibitem[Schapire and Freund(2012)]{schapire2012boosting}
Robert~E Schapire and Yoav Freund.
\newblock \emph{Boosting: Foundations and algorithms}.
\newblock MIT press, 2012.

\bibitem[Shalev-Shwartz and Singer(2010)]{shalev2010equivalence}
Shai Shalev-Shwartz and Yoram Singer.
\newblock On the equivalence of weak learnability and linear separability: New
  relaxations and efficient boosting algorithms.
\newblock \emph{Machine learning}, 80\penalty0 (2-3):\penalty0 141--163, 2010.

\bibitem[Smith(2018)]{Smith2018}
Le~Smith, Kindermans.
\newblock {Don't Decay the Learning Rate, Increase the Batch Size}.
\newblock In \emph{ICLR}, 2018.

\bibitem[Soudry et~al.(2017)Soudry, Hoffer, and Srebro]{soudry2017implicit}
Daniel Soudry, Elad Hoffer, and Nathan Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{arXiv preprint arXiv:1710.10345}, 2017.

\bibitem[Srebro et~al.(2005)Srebro, Alon, and
  Jaakkola]{srebro2005generalization}
Nathan Srebro, Noga Alon, and Tommi~S Jaakkola.
\newblock Generalization error bounds for collaborative prediction with
  low-rank matrices.
\newblock In \emph{Advances In Neural Information Processing Systems}, pages
  1321--1328, 2005.

\bibitem[Telgarsky(2013)]{telgarsky2013margins}
Matus Telgarsky.
\newblock Margins, shrinkage and boosting.
\newblock In \emph{Proceedings of the 30th International Conference on
  International Conference on Machine Learning-Volume 28}, pages II--307. JMLR.
  org, 2013.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{wilson2017marginal}
Ashia~C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin
  Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4151--4161, 2017.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Zhang et~al.(2005)Zhang, Yu, et~al.]{zhang2005boosting}
Tong Zhang, Bin Yu, et~al.
\newblock Boosting with early stopping: Convergence and consistency.
\newblock \emph{The Annals of Statistics}, 33\penalty0 (4):\penalty0
  1538--1579, 2005.

\end{thebibliography}
