\begin{thebibliography}{10}

\bibitem{adams2010learning}
R.~Adams, H.~Wallach, and Z.~Ghahramani.
\newblock Learning the structure of deep sparse graphical models.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 1--8, 2010.

\bibitem{antoran2020depth}
J.~Antor{\'a}n, J.~U. Allingham, and J.~M. Hern{\'a}ndez-Lobato.
\newblock Depth uncertainty in neural networks.
\newblock {\em arXiv preprint arXiv:2006.08437}, 2020.

\bibitem{babaeizadeh2018adjustable}
M.~Babaeizadeh and G.~Ghiasi.
\newblock Adjustable real-time style transfer.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{barocas2018fairness}
S.~Barocas, M.~Hardt, and A.~Narayanan.
\newblock Fairness and machine learning. fairmlbook. org, 2018.
\newblock {\em URL: http://www. fairmlbook. org}, 2018.

\bibitem{Bergstra2011}
J.~Bergstra, R.~Bardenet, Y.~Bengio, B.~K{\'e}gl, et~al.
\newblock Algorithms for hyper-parameter optimization.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~24, pages 2546--2554, 2011.

\bibitem{Bergstra2012}
J.~Bergstra and Y.~Bengio.
\newblock Random search for hyper-parameter optimization.
\newblock {\em Journal of Machine Learning Research}, 13:281--305, 2012.

\bibitem{blundell2015weight}
C.~Blundell, J.~Cornebise, K.~Kavukcuoglu, and D.~Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In {\em International Conference on Machine Learning}, pages
  1613--1622, 2015.

\bibitem{Bottou1998}
L.~Bottou.
\newblock {Online algorithms and stochastic approximations}.
\newblock {\em Online Learning and Neural Networks}, 5, 1998.

\bibitem{bottou2018optimization}
L.~Bottou, F.~E. Curtis, and J.~Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em Siam Review}, 60(2):223--311, 2018.

\bibitem{brier1950verification}
G.~W. Brier.
\newblock Verification of forecasts expressed in terms of probability.
\newblock {\em Monthly weather review}, 78(1):1--3, 1950.

\bibitem{brock2017smash}
A.~Brock, T.~Lim, J.~M. Ritchie, and N.~J. Weston.
\newblock Smash: One-shot model architecture search through hypernetworks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{Caruana2006}
R.~Caruana, A.~Munson, and A.~Niculescu-Mizil.
\newblock Getting the most out of ensemble selection.
\newblock In {\em Sixth International Conference on Data Mining (ICDM'06)},
  pages 828--833. IEEE, 2006.

\bibitem{Caruana2004}
R.~Caruana, A.~Niculescu-Mizil, G.~Crew, and A.~Ksikes.
\newblock Ensemble selection from libraries of models.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning (ICML)}, page~18. ACM, 2004.

\bibitem{colson2007overview}
B.~Colson, P.~Marcotte, and G.~Savard.
\newblock An overview of bilevel optimization.
\newblock {\em Annals of operations research}, 153(1):235--256, 2007.

\bibitem{dietterich2000ensemble}
T.~G. Dietterich.
\newblock Ensemble methods in machine learning.
\newblock In {\em International workshop on multiple classifier systems}, pages
  1--15. Springer, 2000.

\bibitem{Dosovitskiy2020You}
A.~Dosovitskiy and J.~Djolonga.
\newblock You only train once: Loss-conditional training of deep networks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{dusenberry2020efficient}
M.~W. Dusenberry, G.~Jerfel, Y.~Wen, Y.-a. Ma, J.~Snoek, K.~Heller,
  B.~Lakshminarayanan, and D.~Tran.
\newblock Efficient and scalable bayesian neural nets with rank-1 factors.
\newblock In {\em International conference on machine learning}, 2020.

\bibitem{duvenaud2013structure}
D.~Duvenaud, J.~Lloyd, R.~Grosse, J.~Tenenbaum, and G.~Zoubin.
\newblock Structure discovery in nonparametric regression through compositional
  kernel search.
\newblock In {\em International Conference on Machine Learning}, pages
  1166--1174, 2013.

\bibitem{Feurer2018a}
M.~Feurer, K.~Eggensperger, S.~Falkner, M.~Lindauer, and F.~Hutter.
\newblock Practical automated machine learning for the automl challenge 2018.
\newblock In {\em International Workshop on Automatic Machine Learning at
  ICML}, 2018.

\bibitem{feurer2019hyperparameter}
M.~Feurer and F.~Hutter.
\newblock Hyperparameter optimization.
\newblock In {\em Automated Machine Learning}, pages 3--33. Springer, 2019.

\bibitem{Feurer2015a}
M.~Feurer, A.~Klein, K.~Eggensperger, J.~Springenberg, M.~Blum, and F.~Hutter.
\newblock Efficient and robust automated machine learning.
\newblock In C.~Cortes, N.~D. Lawrence, D.~D. Lee, M.~Sugiyama, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems 28}, pages
  2962--2970, 2015.

\bibitem{fort2019deep}
S.~Fort, H.~Hu, and B.~Lakshminarayanan.
\newblock Deep ensembles: A loss landscape perspective.
\newblock {\em arXiv preprint arXiv:1912.02757}, 2019.

\bibitem{gal2016dropout}
Y.~Gal and Z.~Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em International conference on machine learning}, pages
  1050--1059, 2016.

\bibitem{ge2015escaping}
R.~Ge, F.~Huang, C.~Jin, and Y.~Yuan.
\newblock Escaping from saddle points—online stochastic gradient for tensor
  decomposition.
\newblock In {\em Conference on Learning Theory}, pages 797--842, 2015.

\bibitem{geman1992neural}
S.~Geman, E.~Bienenstock, and R.~Doursat.
\newblock Neural networks and the bias/variance dilemma.
\newblock {\em Neural computation}, 4(1):1--58, 1992.

\bibitem{gibbons1992primer}
R.~Gibbons et~al.
\newblock {\em A primer in game theory}.
\newblock Harvester Wheatsheaf New York, 1992.

\bibitem{Golovin2017}
D.~Golovin, B.~Solnik, S.~Moitra, G.~Kochanski, J.~Karro, and D.~Sculley.
\newblock Google vizier: A service for black-box optimization.
\newblock In {\em Proceedings of the 23rd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 1487--1495, 2017.

\bibitem{gustafsson2019evaluating}
F.~K. Gustafsson, M.~Danelljan, and T.~B. Schon.
\newblock Evaluating scalable bayesian deep learning methods for robust
  computer vision.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 318--319, 2020.

\bibitem{ha2016hypernetworks}
D.~Ha, A.~Dai, and Q.~V. Le.
\newblock Hypernetworks.
\newblock {\em arXiv preprint arXiv:1609.09106}, 2016.

\bibitem{hansen1990neural}
L.~K. Hansen and P.~Salamon.
\newblock Neural network ensembles.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  12(10):993--1001, 1990.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hein2019relu}
M.~Hein, M.~Andriushchenko, and J.~Bitterwolf.
\newblock Why relu networks yield high-confidence predictions far away from the
  training data and how to mitigate the problem.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 41--50, 2019.

\bibitem{hendrycks2019benchmarking}
D.~Hendrycks and T.~Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{hinton1993keeping}
G.~E. Hinton and D.~Van~Camp.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In {\em Proceedings of the sixth annual conference on Computational
  learning theory}, pages 5--13, 1993.

\bibitem{Hollerer2020}
S.~H{\"o}llerer, L.~Papaxanthos, A.~C. Gumpinger, K.~Fischer, C.~Beisel,
  K.~Borgwardt, Y.~Benenson, and M.~Jeschek.
\newblock Large-scale dna-based phenotypic recording and deep learning enable
  highly accurate sequence-function mapping.
\newblock {\em bioRxiv}, 2020.

\bibitem{huang2017snapshot}
G.~Huang, Y.~Li, G.~Pleiss, Z.~Liu, J.~E. Hopcroft, and K.~Q. Weinberger.
\newblock Snapshot ensembles: Train 1, get m for free.
\newblock {\em arXiv preprint arXiv:1704.00109}, 2017.

\bibitem{kemp2008discovery}
C.~Kemp and J.~B. Tenenbaum.
\newblock The discovery of structural form.
\newblock {\em Proceedings of the National Academy of Sciences},
  105(31):10687--10692, 2008.

\bibitem{Kingma2014}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock Technical report, preprint arXiv:1412.6980, 2014.

\bibitem{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem{krogh1995neural}
A.~Krogh and J.~Vedelsby.
\newblock Neural network ensembles, cross validation, and active learning.
\newblock In {\em Advances in neural information processing systems}, pages
  231--238, 1995.

\bibitem{lake2015human}
B.~M. Lake, R.~Salakhutdinov, and J.~B. Tenenbaum.
\newblock Human-level concept learning through probabilistic program induction.
\newblock {\em Science}, 350(6266):1332--1338, 2015.

\bibitem{Lakshminarayanan2017}
B.~Lakshminarayanan, A.~Pritzel, and C.~Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 6402--6413, 2017.

\bibitem{lecun1990handwritten}
Y.~LeCun, B.~E. Boser, J.~S. Denker, D.~Henderson, R.~E. Howard, W.~E. Hubbard,
  and L.~D. Jackel.
\newblock Handwritten digit recognition with a back-propagation network.
\newblock In {\em Advances in neural information processing systems}, pages
  396--404, 1990.

\bibitem{lee2015m}
S.~Lee, S.~Purushwalkam, M.~Cogswell, D.~Crandall, and D.~Batra.
\newblock Why m heads are better than one: Training a diverse ensemble of deep
  networks.
\newblock {\em arXiv preprint arXiv:1511.06314}, 2015.

\bibitem{levesque2016bayesian}
J.-C. L{\'e}vesque, C.~Gagn{\'e}, and R.~Sabourin.
\newblock Bayesian hyperparameter optimization for ensemble learning.
\newblock In {\em Proceedings of the Thirty-Second Conference on Uncertainty in
  Artificial Intelligence}, pages 437--446, 2016.

\bibitem{levin1990statistical}
E.~Levin, N.~Tishby, and S.~A. Solla.
\newblock A statistical approach to learning and generalization in layered
  neural networks.
\newblock {\em Proceedings of the IEEE}, 78(10):1568--1574, 1990.

\bibitem{levinson2011towards}
J.~Levinson, J.~Askeland, J.~Becker, J.~Dolson, D.~Held, S.~Kammel, J.~Z.
  Kolter, D.~Langer, O.~Pink, V.~Pratt, et~al.
\newblock Towards fully autonomous driving: Systems and algorithms.
\newblock In {\em 2011 IEEE Intelligent Vehicles Symposium (IV)}, pages
  163--168. IEEE, 2011.

\bibitem{liu2020deep}
Y.~Liu, A.~Jain, C.~Eng, D.~H. Way, K.~Lee, P.~Bui, K.~Kanada,
  G.~de~Oliveira~Marinho, J.~Gallegos, S.~Gabriele, et~al.
\newblock A deep learning system for differential diagnosis of skin diseases.
\newblock {\em Nature Medicine}, pages 1--9, 2020.

\bibitem{lorraine2018stochastic}
J.~Lorraine and D.~Duvenaud.
\newblock Stochastic hyperparameter optimization through hypernetworks.
\newblock {\em arXiv preprint arXiv:1802.09419}, 2018.

\bibitem{mackay1995ensemble}
D.~J. MacKay et~al.
\newblock Ensemble learning and evidence maximization.
\newblock In {\em Advances in neural information processing systems}, 1995.

\bibitem{mackay2019self}
M.~Mackay, P.~Vicol, J.~Lorraine, D.~Duvenaud, and R.~Grosse.
\newblock Self-tuning networks: Bilevel optimization of hyperparameters using
  structured best-response functions.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{Mendoza2016}
H.~Mendoza, A.~Klein, M.~Feurer, J.~T. Springenberg, and F.~Hutter.
\newblock Towards automatically-tuned neural networks.
\newblock In {\em ICML Workshop on Automatic Machine Learning}, pages 58--65,
  2016.

\bibitem{miotto2016deep}
R.~Miotto, L.~Li, B.~A. Kidd, and J.~T. Dudley.
\newblock Deep patient: an unsupervised representation to predict the future of
  patients from the electronic health records.
\newblock {\em Scientific reports}, 6(1):1--10, 2016.

\bibitem{naeini2015obtaining}
M.~P. Naeini, G.~Cooper, and M.~Hauskrecht.
\newblock Obtaining well calibrated probabilities using bayesian binning.
\newblock In {\em Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem{neal1995bayesian}
R.~M. Neal.
\newblock {\em Bayesian learning for neural networks}.
\newblock PhD thesis, University of Toronto, 1995.

\bibitem{nixon2019measuring}
J.~Nixon, M.~W. Dusenberry, L.~Zhang, G.~Jerfel, and D.~Tran.
\newblock Measuring calibration in deep learning.
\newblock In {\em CVPR Workshops}, pages 38--41, 2019.

\bibitem{opitz1999popular}
D.~Opitz and R.~Maclin.
\newblock Popular ensemble methods: An empirical study.
\newblock {\em Journal of artificial intelligence research}, 11:169--198, 1999.

\bibitem{perez2018film}
E.~Perez, F.~Strub, H.~De~Vries, V.~Dumoulin, and A.~Courville.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock In {\em Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem{saikia2020optimized}
T.~Saikia, T.~Brox, and C.~Schmid.
\newblock Optimized generic feature learning for few-shot classification across
  domains.
\newblock {\em arXiv preprint arXiv:2001.07926}, 2020.

\bibitem{schmidhuber1992learning}
J.~Schmidhuber.
\newblock Learning to control fast-weight memories: An alternative to dynamic
  recurrent networks.
\newblock {\em Neural Computation}, 4(1):131--139, 1992.

\bibitem{schmidhuber1993self}
J.~Schmidhuber.
\newblock A ‘self-referential’weight matrix.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 446--450. Springer, 1993.

\bibitem{Snoek2012}
J.~Snoek, H.~Larochelle, and R.~P. Adams.
\newblock Practical {B}ayesian optimization of machine learning algorithms.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2960--2968, 2012.

\bibitem{snoek2019can}
J.~Snoek, Y.~Ovadia, E.~Fertig, B.~Lakshminarayanan, S.~Nowozin, D.~Sculley,
  J.~Dillon, J.~Ren, and Z.~Nado.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  13969--13980, 2019.

\bibitem{Snoek2015}
J.~Snoek, O.~Rippel, K.~Swersky, R.~Kiros, N.~Satish, N.~Sundaram, M.~Patwary,
  M.~Prabhat, and R.~Adams.
\newblock Scalable {B}ayesian optimization using deep neural networks.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning (ICML)}, pages 2171--2180, 2015.

\bibitem{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958,
  2014.

\bibitem{Szegedy2016}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem{tran2019bayesian}
D.~Tran, M.~W. Dusenberry, D.~Hafner, and M.~van~der Wilk.
\newblock Bayesian {L}ayers: A module for neural network uncertainty.
\newblock In {\em Neural Information Processing Systems}, 2019.

\bibitem{wen2020batchensemble}
Y.~Wen, D.~Tran, and J.~Ba.
\newblock Batchensemble: an alternative approach to efficient ensemble and
  lifelong learning.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{wen2018flipout}
Y.~Wen, P.~Vicol, J.~Ba, D.~Tran, and R.~Grosse.
\newblock Flipout: Efficient pseudo-independent weight perturbations on
  mini-batches.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{wenzel2020good}
F.~Wenzel, K.~Roth, B.~S. Veeling, J.~{\'S}wi{\k{a}}tkowski, L.~Tran, S.~Mandt,
  J.~Snoek, T.~Salimans, R.~Jenatton, and S.~Nowozin.
\newblock How good is the bayes posterior in deep neural networks really?
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{wilson2020bayesian}
A.~G. Wilson and P.~Izmailov.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock In {\em International Conference on Machine Learning}, 2020.

\bibitem{xiao2017fashion}
H.~Xiao, K.~Rasul, and R.~Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em arXiv preprint arXiv:1708.07747}, 2017.

\bibitem{zagoruyko2016wide}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock {\em arXiv preprint arXiv:1605.07146}, 2016.

\bibitem{zaidi2020neural}
S.~Zaidi, A.~Zela, T.~Elsken, C.~Holmes, F.~Hutter, and Y.~W. Teh.
\newblock Neural ensemble search for performant and calibrated predictions.
\newblock {\em arXiv preprint arXiv:2006.08573}, 2020.

\bibitem{zhang2019cyclical}
R.~Zhang, C.~Li, J.~Zhang, C.~Chen, and A.~G. Wilson.
\newblock Cyclical stochastic gradient mcmc for bayesian deep learning.
\newblock In {\em International Conference on Learning Representations}, 2019.

\end{thebibliography}
