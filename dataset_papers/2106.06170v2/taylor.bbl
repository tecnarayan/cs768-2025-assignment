\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam and OpenAI(2018)]{SpinningUp2018}
Joshua Achiam and OpenAI.
\newblock {Spinning Up in Deep Reinforcement Learning}.
\newblock \emph{https://github.com/openai/spinningup}, 2018.

\bibitem[Amit et~al.(2020)Amit, Meir, and Ciosek]{amit2020discount}
Ron Amit, Ron Meir, and Kamil Ciosek.
\newblock Discount factor as a regularizer in reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[Bellman(1957)]{bellman1957markovian}
Richard Bellman.
\newblock A {M}arkovian decision process.
\newblock \emph{Journal of mathematics and mechanics}, pages 679--684, 1957.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Open{AI} gym.
\newblock \emph{arXiv}, 2016.

\bibitem[Coumans(2015)]{coumans2015bullet}
Erwin Coumans.
\newblock Bullet physics simulation.
\newblock In \emph{ACM SIGGRAPH 2015 Courses}, page~1. 2015.

\bibitem[De~Farias and Van~Roy(2003)]{de2003linear}
Daniela~Pucci De~Farias and Benjamin Van~Roy.
\newblock The linear programming approach to approximate dynamic programming.
\newblock \emph{Operations research}, 51\penalty0 (6):\penalty0 850--865, 2003.

\bibitem[Degris et~al.(2012)Degris, White, and Sutton]{degris2012off}
Thomas Degris, Martha White, and Richard~S Sutton.
\newblock Off-policy actor-critic.
\newblock \emph{arXiv preprint arXiv:1205.4839}, 2012.

\bibitem[Dhariwal et~al.(2017)Dhariwal, Hesse, Klimov, Nichol, Plappert,
  Radford, Schulman, Sidor, Wu, and Zhokhov]{dhariwal2017openai}
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias
  Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter
  Zhokhov.
\newblock Open{AI} baselines, 2017.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, et~al.]{espeholt2018impala}
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom
  Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{Proceeedings of the International Conference on Machine
  Learning}, 2018.

\bibitem[Fedus et~al.(2019)Fedus, Gelada, Bengio, Bellemare, and
  Larochelle]{fedus2019hyperbolic}
William Fedus, Carles Gelada, Yoshua Bengio, Marc~G Bellemare, and Hugo
  Larochelle.
\newblock Hyperbolic discounting and learning over multiple horizons.
\newblock \emph{arXiv}, 2019.

\bibitem[Fox et~al.(2016)Fox, Pakman, and Tishby]{fox2015taming}
Roy Fox, Ari Pakman, and Naftali Tishby.
\newblock Taming the noise in reinforcement learning via soft updates.
\newblock In \emph{Proceedings of the Conference on Uncertainty in Artificial
  Intelligence}, 2016.

\bibitem[Fran{\c{c}}ois-Lavet et~al.(2015)Fran{\c{c}}ois-Lavet, Fonteneau, and
  Ernst]{franccois2015discount}
Vincent Fran{\c{c}}ois-Lavet, Raphael Fonteneau, and Damien Ernst.
\newblock How to discount deep reinforcement learning: Towards new dynamic
  strategies.
\newblock \emph{NIPS Deep Reinforcement Learning Workshop}, 2015.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Van~Hoof, and
  Meger]{fujimoto2018addressing}
Scott Fujimoto, Herke Van~Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2018.

\bibitem[Grinstead and Snell(2012)]{grinstead2012introduction}
Charles~Miller Grinstead and James~Laurie Snell.
\newblock \emph{Introduction to probability}.
\newblock American Mathematical Soc., 2012.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{Proceeedings of the International Conference on Machine
  Learning}, 2018.

\bibitem[Hasselt(2010)]{hasselt2010double}
Hado Hasselt.
\newblock Double {Q}-learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2010.

\bibitem[Janner et~al.(2020)Janner, Mordatch, and Levine]{janner2020gamma}
Michael Janner, Igor Mordatch, and Sergey Levine.
\newblock Gamma-models: Generative temporal difference learning for
  infinite-horizon prediction.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Jiang et~al.(2016)Jiang, Singh, and Tewari]{jiang2016structural}
Nan Jiang, Satinder~P Singh, and Ambuj Tewari.
\newblock On structural properties of {MDP}s that bound loss due to shallow
  planning.
\newblock In \emph{Proceedings of the International Joint Conference on
  Artificial Intelligence}, 2016.

\bibitem[Kapturowski et~al.(2018)Kapturowski, Ostrovski, Quan, Munos, and
  Dabney]{kapturowski2018recurrent}
Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney.
\newblock Recurrent experience replay in distributed reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2018.

\bibitem[Kearns and Singh(2000)]{kearns2000bias}
Michael~J Kearns and Satinder~P Singh.
\newblock Bias-variance error bounds for temporal difference updates.
\newblock In \emph{Proceedings of the Conference on Learning Theory}, 2000.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Laroche and van Seijen(2018)]{laroche2018reinforcement}
Romain Laroche and Harm van Seijen.
\newblock In reinforcement learning, all objective functions are not equal.
\newblock 2018.

\bibitem[Lehnert et~al.(2018)Lehnert, Laroche, and van
  Seijen]{lehnert2018value}
Lucas Lehnert, Romain Laroche, and Harm van Seijen.
\newblock On value function representation of long horizon problems.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2018.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2015.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou,
  Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2016.

\bibitem[Nota and Thomas(2019)]{nota2019policy}
Chris Nota and Philip~S Thomas.
\newblock Is the policy gradient a gradient?
\newblock In \emph{Proceedings of the International Conference on Autonomous
  Agenst and Multiagent Systems}, 2019.

\bibitem[O'Donoghue et~al.(2016)O'Donoghue, Munos, Kavukcuoglu, and
  Mnih]{o2016combining}
Brendan O'Donoghue, Remi Munos, Koray Kavukcuoglu, and Volodymyr Mnih.
\newblock Combining policy gradient and {Q}-learning.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2016.

\bibitem[Pardo et~al.(2018)Pardo, Tavakoli, Levdik, and
  Kormushev]{pardo2018time}
Fabio Pardo, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev.
\newblock Time limits in reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2018.

\bibitem[Pohlen et~al.(2018)Pohlen, Piot, Hester, Azar, Horgan, Budden,
  Barth-Maron, Van~Hasselt, Quan, Ve{\v{c}}er{\'\i}k, Hessel, Munos, and
  Pietquin]{pohlen2018observe}
Tobias Pohlen, Bilal Piot, Todd Hester, Mohammad~Gheshlaghi Azar, Dan Horgan,
  David Budden, Gabriel Barth-Maron, Hado Van~Hasselt, John Quan, Mel
  Ve{\v{c}}er{\'\i}k, Matteo Hessel, R\'emi Munos, and Olivier Pietquin.
\newblock Observe and look further: Achieving consistent performance on atari.
\newblock \emph{arXiv}, 2018.

\bibitem[Prokhorov and Wunsch(1997)]{prokhorov1997adaptive}
Danil~V Prokhorov and Donald~C Wunsch.
\newblock Adaptive critic designs.
\newblock \emph{IEEE transactions on Neural Networks}, 8\penalty0 (5):\penalty0
  997--1007, 1997.

\bibitem[Puterman(2014)]{puterman2014markov}
Martin~L Puterman.
\newblock \emph{Markov decision processes: Discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Romoff et~al.(2019)Romoff, Henderson, Touati, Brunskill, Pineau, and
  Ollivier]{romoff2019separating}
Joshua Romoff, Peter Henderson, Ahmed Touati, Emma Brunskill, Joelle Pineau,
  and Yann Ollivier.
\newblock Separating value functions across time-scales.
\newblock \emph{Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[Ross(2014)]{ross2014introduction}
Sheldon~M Ross.
\newblock \emph{Introduction to probability models}.
\newblock Academic press, 2014.

\bibitem[Schaul et~al.(2015)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2015.

\bibitem[Schulman et~al.(2015{\natexlab{a}})Schulman, Levine, Abbeel, Jordan,
  and Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2015{\natexlab{a}}.

\bibitem[Schulman et~al.(2015{\natexlab{b}})Schulman, Moritz, Levine, Jordan,
  and Abbeel]{schulman2015high}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter
  Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{arXiv preprint arXiv:1506.02438}, 2015{\natexlab{b}}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv}, 2017.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silver2014deterministic}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
  Martin Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2014.

\bibitem[Sinha et~al.(2020)Sinha, Song, Garg, and Ermon]{sinha2020experience}
Samarth Sinha, Jiaming Song, Animesh Garg, and Stefano Ermon.
\newblock Experience replay with likelihood-free importance weights.
\newblock \emph{arXiv}, 2020.

\bibitem[Sutton(1995)]{sutton1995td}
Richard~S Sutton.
\newblock {TD} models: Modeling the world at a mixture of time scales.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}. 1995.

\bibitem[Sutton and Barto(2018)]{sutton1998introduction}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT Press, 2018.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2000.

\bibitem[Tang et~al.(2020)Tang, Valko, and Munos]{tang2020taylor}
Yunhao Tang, Michal Valko, and R{\'e}mi Munos.
\newblock Taylor expansion policy optimization.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mu{J}o{C}o: A physics engine for model-based control.
\newblock In \emph{International Conference on Intelligent Robots and Systems},
  2012.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Hado Van~Hasselt, Arthur Guez, and David Silver.
\newblock Deep reinforcement learning with double {Q}-learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~30, 2016.

\bibitem[van Hasselt et~al.(2019)van Hasselt, Quan, Hessel, Xu, Borsa, and
  Barreto]{van2019general}
Hado van Hasselt, John Quan, Matteo Hessel, Zhongwen Xu, Diana Borsa, and
  Andr{\'e} Barreto.
\newblock General non-linear {B}ellman equations.
\newblock \emph{arXiv}, 2019.

\bibitem[Van~Seijen et~al.(2019)Van~Seijen, Fatemi, and Tavakoli]{van2019using}
Harm Van~Seijen, Mehdi Fatemi, and Arash Tavakoli.
\newblock Using a logarithmic mapping to enable lower discount factors in
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Xu et~al.(2018)Xu, van Hasselt, and Silver]{xu2018meta}
Zhongwen Xu, Hado~P van Hasselt, and David Silver.
\newblock Meta-gradient reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Brian~D. Ziebart, Andrew~L. Maas, J.~Andrew Bagnell, and Anind~K. Dey.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2008.

\end{thebibliography}
