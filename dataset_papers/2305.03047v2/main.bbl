\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anthropic(2023{\natexlab{a}})]{anthropiccaiblog}
Anthropic.
\newblock Claude’s constitution, 2023{\natexlab{a}}.
\newblock URL \url{https://www.anthropic.com/index/claudes-constitution}.

\bibitem[Anthropic(2023{\natexlab{b}})]{anthropicsafetyblog}
Anthropic.
\newblock Core views on ai safety: When, why, what, and how,
  2023{\natexlab{b}}.
\newblock URL \url{https://www.anthropic.com/index/core-views-on-ai-safety}.

\bibitem[Askell et~al.(2021)Askell, Bai, Chen, Drain, Ganguli, Henighan, Jones,
  Joseph, Mann, DasSarma, et~al.]{askell2021general}
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan,
  Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et~al.
\newblock A general language assistant as a laboratory for alignment.
\newblock \emph{arXiv preprint arXiv:2112.00861}, 2021.

\bibitem[Bai et~al.(2022{\natexlab{a}})Bai, Jones, Ndousse, Askell, Chen,
  DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
  Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022{\natexlab{a}}.

\bibitem[Bai et~al.(2022{\natexlab{b}})Bai, Kadavath, Kundu, Askell, Kernion,
  Jones, Chen, Goldie, Mirhoseini, McKinnon, Chen, Olsson, Olah, Hernandez,
  Drain, Ganguli, Li, Tran-Johnson, Perez, Kerr, Mueller, Ladish, Landau,
  Ndousse, Lukosuite, Lovitt, Sellitto, Elhage, Schiefer, Mercado, DasSarma,
  Lasenby, Larson, Ringer, Johnston, Kravec, Showk, Fort, Lanham,
  Telleen-Lawton, Conerly, Henighan, Hume, Bowman, Hatfield-Dodds, Mann,
  Amodei, Joseph, McCandlish, Brown, and Kaplan]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain,
  Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared
  Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite,
  Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi
  Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott
  Johnston, Shauna Kravec, Sheer~El Showk, Stanislav Fort, Tamera Lanham,
  Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel~R.
  Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam
  McCandlish, Tom Brown, and Jared Kaplan.
\newblock Constitutional ai: Harmlessness from ai feedback, 2022{\natexlab{b}}.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien,
  Hallahan, Khan, Purohit, Prashanth, Raff, et~al.]{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle
  O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai
  Prashanth, Edward Raff, et~al.
\newblock Pythia: A suite for analyzing large language models across training
  and scaling.
\newblock \emph{arXiv preprint arXiv:2304.01373}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D. Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality, March 2023.
\newblock URL \url{https://vicuna.lmsys.org}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock {PaLM}: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Databricks(2023)]{dollyblog}
Databricks.
\newblock Free dolly: Introducing the world's first truly open
  instruction-tuned llm, 2023.
\newblock URL
  \url{https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: {P}re-training of deep bidirectional transformers for
  language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Gabriel(2020)]{gabriel2020artificial}
Iason Gabriel.
\newblock Artificial intelligence, values, and alignment.
\newblock \emph{Minds and machines}, 30\penalty0 (3):\penalty0 411--437, 2020.

\bibitem[Ganguli et~al.(2023)Ganguli, Askell, Schiefer, Liao,
  Luko{\v{s}}i{\=u}t{\.e}, Chen, Goldie, Mirhoseini, Olsson, Hernandez,
  et~al.]{ganguli2023capacity}
Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamil{\.e}
  Luko{\v{s}}i{\=u}t{\.e}, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine
  Olsson, Danny Hernandez, et~al.
\newblock The capacity for moral self-correction in large language models.
\newblock \emph{arXiv preprint arXiv:2302.07459}, 2023.

\bibitem[Geng et~al.(2023)Geng, Gudibande, Liu, Wallace, Abbeel, Levine, and
  Song]{koala_blogpost_2023}
Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey
  Levine, and Dawn Song.
\newblock Koala: A dialogue model for academic research.
\newblock Blog post, April 2023.
\newblock URL \url{https://bair.berkeley.edu/blog/2023/04/03/koala/}.

\bibitem[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and
  Choi]{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock \emph{arXiv preprint arXiv:1904.09751}, 2019.

\bibitem[Hu et~al.(2022)Hu, Wallis, Allen-Zhu, Li, Wang, Wang, Chen,
  et~al.]{hulora}
Edward~J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang,
  Weizhu Chen, et~al.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Kim and Rush(2016)]{kim2016sequence}
Yoon Kim and Alexander~M Rush.
\newblock Sequence-level knowledge distillation.
\newblock \emph{arXiv preprint arXiv:1606.07947}, 2016.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and
  Iwasawa]{kojima2022zeroshotreasoner}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
  Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{arXiv preprint arXiv:2205.11916}, 2022.

\bibitem[Köpf et~al.(2023)Köpf, Kilcher, von Rütte, Anagnostidis, Tam,
  Stevens, Barhoum, Duc, Stanley, Nagyfi, ES, Suri, Glushkov, Dantuluri,
  Maguire, Schuhmann, Nguyen, and Mattick]{köpf2023openassistant}
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis,
  Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen~Minh Duc, Oliver
  Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav
  Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander
  Mattick.
\newblock Openassistant conversations -- democratizing large language model
  alignment, 2023.

\bibitem[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{lewis2019bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}, 2019.

\bibitem[Lin et~al.(2021)Lin, Hilton, and Evans]{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock \emph{arXiv preprint arXiv:2109.07958}, 2021.

\bibitem[Liu et~al.(2023)Liu, Li, Wu, and Lee]{liu2023llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock 2023.

\bibitem[Microsoft(2023)]{microsoftnewbingblog}
Microsoft.
\newblock Introducing the new bing, 2023.
\newblock URL \url{https://www.bing.com/new#features}.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber,
  Dohan, Lewkowycz, Bosma, Luan, et~al.]{nye2021show}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob
  Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
  Luan, et~al.
\newblock Show your work: Scratchpads for intermediate computation with
  language models.
\newblock \emph{arXiv preprint arXiv:2112.00114}, 2021.

\bibitem[OpenAI(2022)]{openaichatgptblog}
OpenAI.
\newblock Open{AI}: Introducing {ChatGPT}, 2022.
\newblock URL \url{https://openai.com/blog/chatgpt}.

\bibitem[OpenAI(2023{\natexlab{a}})]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023{\natexlab{a}}.

\bibitem[OpenAI(2023{\natexlab{b}})]{openaigpt4blog}
OpenAI.
\newblock Open{AI}: {GPT-4}, 2023{\natexlab{b}}.
\newblock URL \url{https://openai.com/research/gpt-4}.

\bibitem[OpenAI(2023{\natexlab{c}})]{textdavinci}
OpenAI.
\newblock How do text-davinci-002 and text-davinci-003 differ?
\newblock
  \url{https://help.openai.com/en/articles/6779149-how-do-text-davinci-002-and-text-davinci-003-differ},
  2023{\natexlab{c}}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}, 2022.

\bibitem[Parrish et~al.(2021)Parrish, Chen, Nangia, Padmakumar, Phang,
  Thompson, Htut, and Bowman]{parrish2021bbq}
Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang,
  Jana Thompson, Phu~Mon Htut, and Samuel~R Bowman.
\newblock Bbq: A hand-built bias benchmark for question answering.
\newblock \emph{arXiv preprint arXiv:2110.08193}, 2021.

\bibitem[Patil et~al.(2020)Patil, Hofmarcher, Dinu, Dorfer, Blies,
  Brandstetter, Arjona-Medina, and Hochreiter]{patil2020align}
Vihang~P Patil, Markus Hofmarcher, Marius-Constantin Dinu, Matthias Dorfer,
  Patrick~M Blies, Johannes Brandstetter, Jose~A Arjona-Medina, and Sepp
  Hochreiter.
\newblock Align-rudder: Learning from few demonstrations by reward
  redistribution.
\newblock \emph{arXiv preprint arXiv:2009.14108}, 2020.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 5485--5551, 2020.

\bibitem[Rudinger et~al.(2018)Rudinger, Naradowsky, Leonard, and
  Van~Durme]{rudinger2018gender}
Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van~Durme.
\newblock Gender bias in coreference resolution.
\newblock \emph{arXiv preprint arXiv:1804.09301}, 2018.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow,
  Castagn{\'e}, Luccioni, Yvon, Gall{\'e}, et~al.]{scao2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c},
  Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois
  Yvon, Matthias Gall{\'e}, et~al.
\newblock Bloom: A 176{B}-parameter open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem[Shaikh et~al.(2022)Shaikh, Zhang, Held, Bernstein, and
  Yang]{shaikh2022second}
Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang.
\newblock On second thought, let's not think step by step! bias and toxicity in
  zero-shot reasoning.
\newblock \emph{arXiv preprint arXiv:2212.08061}, 2022.

\bibitem[Solaiman and Dennison(2021)]{solaiman2021process}
Irene Solaiman and Christy Dennison.
\newblock Process for adapting language models to society (palms) with
  values-targeted datasets.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 5861--5873, 2021.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch,
  Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar
  Abid, Adam Fisch, Adam~R. Brown, Adam Santoro, Aditya Gupta, Adri{\`a}
  Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[Sun et~al.(2023{\natexlab{a}})Sun, Shen, Zhang, Zhou, Chen, Cox, Yang,
  and Gan]{sun2023salmon}
Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David
  Cox, Yiming Yang, and Chuang Gan.
\newblock Salmon: Self-alignment with principle-following reward models.
\newblock \emph{arXiv preprint arXiv:2310.05910}, 2023{\natexlab{a}}.

\bibitem[Sun et~al.(2023{\natexlab{b}})Sun, Wang, Tay, Yang, and
  Zhou]{sun2023recitationaugmented}
Zhiqing Sun, Xuezhi Wang, Yi~Tay, Yiming Yang, and Denny Zhou.
\newblock Recitation-augmented language models.
\newblock In \emph{International Conference on Learning Representations},
  2023{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=-cqvvvb-NkI}.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer,
  Kulshreshtha, Cheng, Jin, Bos, Baker, Du, et~al.]{thoppilan2022lamda}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  et~al.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}, 2022.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar,
  et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock {LLaMA}: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale,
  et~al.]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{NeurIPS}, 2017.

\bibitem[Wan et~al.(2023)Wan, Wallace, Shen, and Klein]{wan2023poisoning}
Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein.
\newblock Poisoning language models during instruction tuning, 2023.

\bibitem[Wang et~al.(2022)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and
  Hajishirzi]{wang2022self}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language model with self generated
  instructions.
\newblock \emph{arXiv preprint arXiv:2212.10560}, 2022.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Chi, Le, and
  Zhou]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and
  Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{NeurIPS}, 2022.

\bibitem[Xu et~al.(2023)Xu, Guo, Duan, and McAuley]{xu2023baize}
Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley.
\newblock Baize: An open-source chat model with parameter-efficient tuning on
  self-chat data.
\newblock \emph{arXiv preprint arXiv:2304.01196}, 2023.

\bibitem[Yao et~al.(2022)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and
  Cao]{yao2022react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
  and Yuan Cao.
\newblock React: Synergizing reasoning and acting in language models.
\newblock \emph{arXiv preprint arXiv:2210.03629}, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock {OPT}: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\end{thebibliography}
