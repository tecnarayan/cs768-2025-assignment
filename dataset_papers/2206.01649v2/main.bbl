\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and Duvenaud]{ChenRBD18}
Tian~Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 6572--6583, Montr{\'{e}}al, Canada, December 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{{IEEE} Conf. on Computer Vision and Pattern Recognition
  ({CVPR})}, pages 770--778, Las Vegas, {NV}, {USA}, June 2016.

\bibitem[Srivastava et~al.(2015)Srivastava, Greff, and
  Schmidhuber]{srivastava2015icml}
Rupesh~K Srivastava, Klaus Greff, and J{\"u}rgen Schmidhuber.
\newblock Highway networks.
\newblock In \emph{the Deep Learning workshop at Int. Conf. on Machine Learning
  (ICML)}, Lille, France, July 2015.

\bibitem[Kidger et~al.(2020)Kidger, Morrill, Foster, and Lyons]{KidgerMFL20}
Patrick Kidger, James Morrill, James Foster, and Terry~J. Lyons.
\newblock Neural controlled differential equations for irregular time series.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, Virtual Only, December 2020.

\bibitem[Elman(1990)]{elman1990finding}
Jeffrey~L Elman.
\newblock Finding structure in time.
\newblock \emph{Cognitive science}, 14\penalty0 (2):\penalty0 179--211, 1990.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{trafo}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, pages 5998--6008, Long Beach, {CA}, {USA}, December 2017.

\bibitem[Schmidhuber(1991)]{Schmidhuber:91fastweights}
J\"urgen Schmidhuber.
\newblock Learning to control fast-weight memories: An alternative to recurrent
  nets.
\newblock Technical Report FKI-147-91, Institut f\"{u}r Informatik, Technische
  Universit\"{a}t M\"{u}nchen, March 1991.

\bibitem[Schmidhuber(1992)]{schmidhuber1992learning}
J{\"u}rgen Schmidhuber.
\newblock Learning to control fast-weight memories: An alternative to dynamic
  recurrent networks.
\newblock \emph{Neural Computation}, 4\penalty0 (1):\penalty0 131--139, 1992.

\bibitem[Schlag et~al.(2021)Schlag, Irie, and Schmidhuber]{schlag2021linear}
Imanol Schlag, Kazuki Irie, and J\"urgen Schmidhuber.
\newblock Linear {T}ransformers are secretly fast weight programmers.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, Virtual only,
  July 2021.

\bibitem[Irie et~al.(2021)Irie, Schlag, Csord\'as, and
  Schmidhuber]{irie2021going}
Kazuki Irie, Imanol Schlag, R\'obert Csord\'as, and J\"urgen Schmidhuber.
\newblock Going beyond linear transformers with recurrent fast weight
  programmers.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, Virtual only, December 2021.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformers}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are {RNN}s: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, Virtual only,
  July 2020.

\bibitem[Funahashi and Nakamura(1993)]{funahashi1993approximation}
Ken-ichi Funahashi and Yuichi Nakamura.
\newblock Approximation of dynamical systems by continuous time recurrent
  neural networks.
\newblock \emph{Neural networks}, 6\penalty0 (6):\penalty0 801--806, 1993.

\bibitem[Lapedes and Farber(1987)]{lapedes1987nonlinear}
Alan Lapedes and Robert Farber.
\newblock Nonlinear signal processing using neural networks: Prediction and
  system modelling.
\newblock \emph{Technical Report No. LA-UR-87-2662}, 1987.

\bibitem[Pineda(1987)]{pineda1987generalization}
Fernando~J Pineda.
\newblock Generalization of back-propagation to recurrent neural networks.
\newblock \emph{Physical review letters}, 59\penalty0 (19):\penalty0 2229,
  1987.

\bibitem[Pearlmutter(1989)]{pearlmutter1989learning}
Barak~A Pearlmutter.
\newblock Learning state space trajectories in recurrent neural networks.
\newblock \emph{Neural Computation}, 1\penalty0 (2):\penalty0 263--269, 1989.

\bibitem[Sato and Murakami(1991)]{sato1991learning}
Masa-aki Sato and Yoshihiko Murakami.
\newblock Learning nonlinear dynamics by recurrent neural.
\newblock In \emph{Some Problems on the Theory of Dynamical Systems in Applied
  Sciences-Proceedings of the Symposium}, volume~10, page~49, 1991.

\bibitem[Rico-Martinez et~al.(1992)Rico-Martinez, Krischer, Kevrekidis, Kube,
  and Hudson]{rico1992discrete}
Ramiro Rico-Martinez, K~Krischer, Ioannis Kevrekidis, MC~Kube, and JL~Hudson.
\newblock Discrete-vs. continuous-time nonlinear signal processing of {C}u
  electrodissolution data.
\newblock \emph{Chemical Engineering Communications}, 118\penalty0
  (1):\penalty0 25--48, 1992.

\bibitem[Oja(1982)]{oja1982simplified}
Erkki Oja.
\newblock Simplified neuron model as a principal component analyzer.
\newblock \emph{Journal of mathematical biology}, 15\penalty0 (3):\penalty0
  267--273, 1982.

\bibitem[Pearson(1901)]{pearson1901liii}
Karl Pearson.
\newblock {LIII}. {O}n lines and planes of closest fit to systems of points in
  space.
\newblock \emph{The London, Edinburgh, and Dublin philosophical magazine and
  journal of science}, 2\penalty0 (11):\penalty0 559--572, 1901.

\bibitem[Hotelling(1933)]{hotelling1933analysis}
Harold Hotelling.
\newblock Analysis of a complex of statistical variables into principal
  components.
\newblock \emph{Journal of educational psychology}, 24\penalty0 (6):\penalty0
  417, 1933.

\bibitem[Oja and Karhunen(1985)]{oja1985stochastic}
Erkki Oja and Juha Karhunen.
\newblock On stochastic approximation of the eigenvectors and eigenvalues of
  the expectation of a random matrix.
\newblock \emph{Journal of mathematical analysis and applications},
  106\penalty0 (1):\penalty0 69--84, 1985.

\bibitem[Oja(1989)]{oja1989neural}
Erkki Oja.
\newblock Neural networks, principal components, and subspaces.
\newblock \emph{International journal of neural systems}, 1\penalty0
  (01):\penalty0 61--68, 1989.

\bibitem[Plumbley(1995)]{plumbley1995lyapunov}
Mark~D Plumbley.
\newblock Lyapunov functions for convergence of principal component algorithms.
\newblock \emph{Neural Networks}, 8\penalty0 (1):\penalty0 11--23, 1995.

\bibitem[Hornik and Kuan(1992)]{hornik1992convergence}
Kurt Hornik and C-M Kuan.
\newblock Convergence analysis of local feature extraction algorithms.
\newblock \emph{Neural Networks}, 5\penalty0 (2):\penalty0 229--240, 1992.

\bibitem[Sanger(1989)]{sanger1989optimal}
Terence~D Sanger.
\newblock Optimal unsupervised learning in a single-layer linear feedforward
  neural network.
\newblock \emph{Neural networks}, 2\penalty0 (6):\penalty0 459--473, 1989.

\bibitem[{Wyatt Jr.} and Elfadel(1995)]{WyattE95}
John~L. {Wyatt Jr.} and Ibrahim~M. Elfadel.
\newblock Time-domain solutions of {O}ja's equations.
\newblock \emph{Neural Computation}, 7\penalty0 (5):\penalty0 915--922, 1995.

\bibitem[Fort and Pages(1996)]{fort1996convergence}
Jean-Claude Fort and Gilles Pages.
\newblock Convergence of stochastic algorithms: From the {K}ushner--{C}lark
  theorem to the {L}yapounov functional method.
\newblock \emph{Advances in applied probability}, 28\penalty0 (4):\penalty0
  1072--1094, 1996.

\bibitem[E(2017)]{weinan2017proposal}
Weinan E.
\newblock A proposal on machine learning via dynamical systems.
\newblock \emph{Communications in Mathematics and Statistics}, 1\penalty0
  (5):\penalty0 1--11, March 2017.

\bibitem[Haber and Ruthotto(2017)]{Haber_2017}
Eldad Haber and Lars Ruthotto.
\newblock Stable architectures for deep neural networks.
\newblock \emph{Inverse Problems}, 34\penalty0 (1):\penalty0 014004, December
  2017.

\bibitem[Haber et~al.(2018)Haber, Ruthotto, Holtham, and Jun]{HaberRHJ18}
Eldad Haber, Lars Ruthotto, Elliot Holtham, and Seong{-}Hwan Jun.
\newblock Learning across scales - multiscale methods for convolution neural
  networks.
\newblock In \emph{Proc. {AAAI} Conf. on Artificial Intelligence}, pages
  3142--3148, New Orleans, {LA}, {USA}, February 2018.

\bibitem[Chang et~al.(2018{\natexlab{a}})Chang, Meng, Haber, Ruthotto, Begert,
  and Holtham]{ChangMHRBH18}
Bo~Chang, Lili Meng, Eldad Haber, Lars Ruthotto, David Begert, and Elliot
  Holtham.
\newblock Reversible architectures for arbitrarily deep residual neural
  networks.
\newblock In \emph{Proc. {AAAI} Conf. on Artificial Intelligence}, pages
  2811--2818, New Orleans, {LA}, {USA}, February 2018{\natexlab{a}}.

\bibitem[Lu et~al.(2018)Lu, Zhong, Li, and Dong]{LuZLD18}
Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong.
\newblock Beyond finite layer neural networks: Bridging deep architectures and
  numerical differential equations.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pages
  3282--3291, Stockholm, Sweden, July 2018.

\bibitem[Chang et~al.(2018{\natexlab{b}})Chang, Meng, Haber, Tung, and
  Begert]{ChangMHTB18}
Bo~Chang, Lili Meng, Eldad Haber, Frederick Tung, and David Begert.
\newblock Multi-level residual networks from dynamical systems view.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, Vancouver,
  Canada, April 2018{\natexlab{b}}.

\bibitem[Ciccone et~al.(2018)Ciccone, Gallieri, Masci, Osendorfer, and
  Gomez]{CicconeGMOG18}
Marco Ciccone, Marco Gallieri, Jonathan Masci, Christian Osendorfer, and
  Faustino~J. Gomez.
\newblock {NAIS}-{N}et: Stable deep networks from non-autonomous differential
  equations.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 3029--3039, Montr{\'{e}}al, Canada, December 2018.

\bibitem[Pontryagin et~al.(1962)Pontryagin, Boltyanskii, Gamkrelidze, and
  Mishchenko]{pontryagin1962}
Liev~S. Pontryagin, VG~Boltyanskii, RV~Gamkrelidze, and EF~Mishchenko.
\newblock \emph{LS Pontryagin Selected Works: The Mathematical Theory of
  Optimal Processes}.
\newblock 1962.

\bibitem[Morrill et~al.(2021{\natexlab{a}})Morrill, Kidger, Yang, and
  Lyons]{morrill2021neural}
James Morrill, Patrick Kidger, Lingyi Yang, and Terry Lyons.
\newblock Neural controlled differential equations for online prediction tasks.
\newblock \emph{Preprint arXiv:2106.11028}, 2021{\natexlab{a}}.

\bibitem[Morrill et~al.(2021{\natexlab{b}})Morrill, Salvi, Kidger, and
  Foster]{morrillSKF21}
James Morrill, Cristopher Salvi, Patrick Kidger, and James Foster.
\newblock Neural rough differential equations for long time series.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pages
  7829--7838, Virtual only, July 2021{\natexlab{b}}.

\bibitem[Kidger(2021)]{kidger2022neural}
Patrick Kidger.
\newblock \emph{On Neural Differential Equations}.
\newblock PhD thesis, Mathematical Institute, University of Oxford, 2021.

\bibitem[Schmidhuber(1993)]{schmidhuber1993reducing}
J{\"u}rgen Schmidhuber.
\newblock Reducing the ratio between learning complexity and number of time
  varying variables in fully recurrent nets.
\newblock In \emph{International Conference on Artificial Neural Networks
  (ICANN)}, pages 460--463, Amsterdam, Netherlands, September 1993.

\bibitem[Ha et~al.(2017)Ha, Dai, and Le]{ha2017hypernetworks}
David Ha, Andrew Dai, and Quoc~V Le.
\newblock Hypernetworks.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, Toulon,
  France, April 2017.

\bibitem[Rubanova et~al.(2019)Rubanova, Chen, and Duvenaud]{RubanovaCD19}
Yulia Rubanova, Tian~Qi Chen, and David Duvenaud.
\newblock Latent ordinary differential equations for irregularly-sampled time
  series.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 5321--5331, Vancouver, Canada, December 2019.

\bibitem[Brouwer et~al.(2019)Brouwer, Simm, Arany, and Moreau]{BrouwerSAM19}
Edward~De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau.
\newblock {GRU}-{ODE}-{B}ayes: Continuous modeling of sporadically-observed
  time series.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 7377--7388, Vancouver, Canada, December 2019.

\bibitem[Hebb(1949)]{hebb1949organization}
Donald~Olding Hebb.
\newblock The organization of behavior; a neuropsycholocigal theory.
\newblock \emph{A Wiley Book in Clinical Psychology}, 62:\penalty0 78, 1949.

\bibitem[Chou and Wang(2020)]{ChouW20}
Chi{-}Ning Chou and Mien~Brabeeba Wang.
\newblock {ODE}-inspired analysis for the biological version of {O}ja's rule in
  solving streaming {PCA}.
\newblock In \emph{Proc. Conf. on Learning Theory (COLT)}, pages 1339--1343,
  Virtual only, July 2020.

\bibitem[Zhang et~al.(2014)Zhang, Wang, and Liu]{zhang2014comprehensive}
Huaguang Zhang, Zhanshan Wang, and Derong Liu.
\newblock A comprehensive review of stability analysis of continuous-time
  recurrent neural networks.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  25\penalty0 (7):\penalty0 1229--1262, 2014.

\bibitem[Fermanian et~al.(2021)Fermanian, Marion, Vert, and
  Biau]{fermanian2021framing}
Adeline Fermanian, Pierre Marion, Jean-Philippe Vert, and G{\'e}rard Biau.
\newblock Framing {RNN} as a kernel method: A neural {ODE} approach.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, Virtual only, December 2021.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{HeuselRUNH17}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, pages 6626--6637, Long Beach, CA, USA, December 2017.

\bibitem[Aizerman et~al.(1964)Aizerman, Braverman, and
  Rozonoer]{aizerman1964theoretical}
Mark~A. Aizerman, Emmanuil~M. Braverman, and Lev~I. Rozonoer.
\newblock Theoretical foundations of potential function method in pattern
  recognition.
\newblock \emph{Automation and Remote Control}, 25\penalty0 (6):\penalty0
  917--936, 1964.

\bibitem[Irie et~al.(2022)Irie, Csord{\'a}s, and Schmidhuber]{irie2022dual}
Kazuki Irie, R{\'o}bert Csord{\'a}s, and J{\"u}rgen Schmidhuber.
\newblock The dual form of neural networks revisited: Connecting test time
  predictions to training patterns via spotlights of attention.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, Baltimore,
  {MD}, {USA}, July 2022.

\bibitem[Widrow and Hoff(1960)]{widrow1960adaptive}
Bernard Widrow and Marcian~E Hoff.
\newblock Adaptive switching circuits.
\newblock In \emph{Proc. {IRE} WESCON Convention Record}, pages 96--104, Los
  Angeles, {CA}, {USA}, August 1960.

\bibitem[Warden(2018)]{warden2018speech}
Pete Warden.
\newblock Speech commands: A dataset for limited-vocabulary speech recognition.
\newblock \emph{Preprint arXiv:1804.03209}, 2018.

\bibitem[Reyna et~al.(2019)Reyna, Josef, Seyedi, Jeter, Shashikumar, Westover,
  Sharma, Nemati, and Clifford]{ReynaJSJSWSNC19}
Matthew~A. Reyna, Christopher Josef, Salman Seyedi, Russell Jeter, Supreeth~P.
  Shashikumar, M.~Brandon Westover, Ashish Sharma, Shamim Nemati, and Gari~D.
  Clifford.
\newblock Early prediction of sepsis from clinical data: the
  {P}hysio{N}et/computing in cardiology challenge 2019.
\newblock In \emph{Proc. Computing in Cardiology (CinC)}, pages 1--4,
  Singapore, September 2019.

\bibitem[Bagnall et~al.(2018)Bagnall, Dau, Lines, Flynn, Large, Bostrom,
  Southam, and Keogh]{bagnall2018uea}
Anthony Bagnall, Hoang~Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron
  Bostrom, Paul Southam, and Eamonn Keogh.
\newblock The {UEA} multivariate time series classification archive, 2018.
\newblock \emph{Preprint arXiv:1811.00075}, 2018.

\bibitem[Che et~al.(2018)Che, Purushotham, Cho, Sontag, and
  Liu]{che2018recurrent}
Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu.
\newblock Recurrent neural networks for multivariate time series with missing
  values.
\newblock \emph{Scientific reports}, 8\penalty0 (1):\penalty0 1--12, 2018.

\bibitem[Rusch and Mishra(2021)]{rusch2021unicornn}
T~Konstantin Rusch and Siddhartha Mishra.
\newblock Un{ICORNN}: A recurrent model for learning very long time
  dependencies.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pages
  9168--9178, Virtual only, July 2021.

\bibitem[Rusch et~al.(2022)Rusch, Mishra, Erichson, and Mahoney]{rusch2021long}
T~Konstantin Rusch, Siddhartha Mishra, N~Benjamin Erichson, and Michael~W
  Mahoney.
\newblock Long expressive memory for sequence modeling.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, Virtual
  only, April 2022.

\bibitem[Irie and Schmidhuber(2021)]{irie2021training}
Kazuki Irie and J{\"u}rgen Schmidhuber.
\newblock Training and generating neural networks in compressed weight space.
\newblock In \emph{ICLR Neural Compression Workshop}, Virtual only, May 2021.

\bibitem[Zhang et~al.(2019)Zhang, Yao, Gholami, Gonzalez, Keutzer, Mahoney, and
  Biros]{ZhangYGGKMB19}
Tianjun Zhang, Zhewei Yao, Amir Gholami, Joseph~E. Gonzalez, Kurt Keutzer,
  Michael~W. Mahoney, and George Biros.
\newblock {ANODEV2:} {A} coupled neural {ODE} framework.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 5152--5162, Vancouver, Canada, December 2019.

\bibitem[Choromanski et~al.(2020)Choromanski, Davis, Likhosherstov, Song,
  Slotine, Varley, Lee, Weller, and Sindhwani]{ChoromanskiDLSS20}
Krzysztof~Marcin Choromanski, Jared~Quincy Davis, Valerii Likhosherstov,
  Xingyou Song, Jean{-}Jacques~E. Slotine, Jacob Varley, Honglak Lee, Adrian
  Weller, and Vikas Sindhwani.
\newblock Ode to an {ODE}.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, Virtual only, December 2020.

\bibitem[Deleu et~al.(2022)Deleu, Kanaa, Feng, Kerg, Bengio, Lajoie, and
  Bacon]{deleu2022continuous}
Tristan Deleu, David Kanaa, Leo Feng, Giancarlo Kerg, Yoshua Bengio, Guillaume
  Lajoie, and Pierre-Luc Bacon.
\newblock Continuous-time meta-learning with forward mode differentiation.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, Virtual
  only, April 2022.

\bibitem[Du et~al.(2020)Du, Futoma, and Doshi-Velez]{du2020model}
Jianzhun Du, Joseph Futoma, and Finale Doshi-Velez.
\newblock Model-based reinforcement learning for semi-{M}arkov decision
  processes with neural {ODE}s.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, Virtual only, 2020.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{Preprint arXiv:1607.06450}, 2016.

\bibitem[Irie et~al.(2019)Irie, Zeyer, Schl\"uter, and Ney]{irie19:trafolm}
Kazuki Irie, Albert Zeyer, Ralf Schl\"uter, and Hermann Ney.
\newblock Language modeling with deep {T}ransformers.
\newblock In \emph{Proc. Interspeech}, pages 3905--3909, Graz, Austria,
  September 2019.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorovET12}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mu{J}o{C}o: {A} physics engine for model-based control.
\newblock In \emph{{IEEE/RSJ} Int. Conf. on Intelligent Robots and Systems
  ({IROS})}, pages 5026--5033, Vilamoura, Portugal, October 2012. {IEEE}.

\bibitem[Sharma et~al.(2017)Sharma, Lakshminarayanan, and
  Ravindran]{SharmaLR17}
Sahil Sharma, Aravind~S. Lakshminarayanan, and Balaraman Ravindran.
\newblock Learning to repeat: Fine grained action repetition for deep
  reinforcement learning.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, Toulon,
  France,, April 2017.

\bibitem[Puterman(1994)]{puterman2014markov}
Martin~L Puterman.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock 1994.

\bibitem[Richards(2005)]{richards2005robust}
Arthur~George Richards.
\newblock \emph{Robust constrained model predictive control}.
\newblock PhD thesis, Massachusetts Institute of Technology, 2005.

\bibitem[Camacho and Bordons(2013)]{camacho2013model}
Eduardo~F Camacho and Carlos Bordons.
\newblock \emph{Model predictive control}.
\newblock 2013.

\bibitem[Mayne et~al.(2000)Mayne, Rawlings, Rao, and
  Scokaert]{mayne2000constrained}
David~Q Mayne, James~B Rawlings, Christopher~V Rao, and Pierre~OM Scokaert.
\newblock Constrained model predictive control: Stability and optimality.
\newblock \emph{Automatica}, 36\penalty0 (6):\penalty0 789--814, 2000.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrapHPHETS15}
Timothy~P. Lillicrap, Jonathan~J. Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, San Juan,
  Puerto Rico, May 2016.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{Silver2014}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
  Martin Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, Beijing,
  China, June 2014.

\bibitem[Sutton(1984)]{sutton1984temporal}
Richard~S Sutton.
\newblock \emph{Temporal Credit Assignment in Reinforcement Learning}.
\newblock PhD thesis, University of Massachusetts Amherst, 1984.

\bibitem[Konda and Tsitsiklis(1999)]{kondaactorcritic}
Vijay~R. Konda and John~N. Tsitsiklis.
\newblock Actor-critic algorithms.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, pages 1008--1014, Denver, CO, USA, November 1999.

\bibitem[Peters et~al.(2005)Peters, Vijayakumar, and Schaal]{PetersVS05}
Jan Peters, Sethu Vijayakumar, and Stefan Schaal.
\newblock Natural actor-critic.
\newblock In \emph{Proc. European Conference on Machine Learning (ECML)}, pages
  280--291, Porto, Portugal, October 2005.

\bibitem[Yildiz et~al.(2021)Yildiz, Heinonen, and
  L{\"{a}}hdesm{\"{a}}ki]{YildizHL21}
{\c{C}}agatay Yildiz, Markus Heinonen, and Harri L{\"{a}}hdesm{\"{a}}ki.
\newblock Continuous-time model-based reinforcement learning.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, Virtual only,
  July 2021.

\end{thebibliography}
