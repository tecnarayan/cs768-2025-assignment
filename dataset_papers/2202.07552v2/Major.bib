%----------PAC----------------
@book{vapnik1974,
author = {V. Vapnik and A. Chervonenkis},
title = {Theory of Pattern Recognition},
publisher = {Nauka, Moscow},
year = 1974
}

@article{blumer1989learnability,
  title={Learnability and the {Vapnik-Chervonenkis} dimension},
  author={Blumer, Anselm and Ehrenfeucht, Andrzej and Haussler, David and Warmuth, Manfred K},
  journal={Journal of the ACM (JACM)},
  volume={36},
  number={4},
  pages={929--965},
  year={1989},
  publisher={ACM New York, NY, USA}
}

@article{ehrenfeucht1989general,
  title={A general lower bound on the number of examples needed for learning},
  author={Ehrenfeucht, Andrzej and Haussler, David and Kearns, Michael and Valiant, Leslie},
  journal={Information and Computation},
  volume={82},
  number={3},
  pages={247--261},
  year={1989},
  publisher={Elsevier}
}

@book{anthony1999neural,
  title={Neural network learning: Theoretical foundations},
  author={Anthony, Martin and Bartlett, Peter L and Bartlett, Peter L and others},
  volume={9},
  year={1999},
  publisher={cambridge university press Cambridge}
}

@inproceedings{daniely2014optimal,
  title={Optimal learners for multiclass problems},
  author={Daniely, Amit and Shalev-Shwartz, Shai},
  booktitle={Conference on Learning Theory},
  pages={287--316},
  year={2014},
  organization={PMLR}
}

@article{david2016supervised,
  title={Supervised learning through the lens of compression},
  author={David, Ofir and Moran, Shay and Yehudayoff, Amir},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@article{graepel2005pac,
  title={PAC-Bayesian compression bounds on the prediction error of learning algorithms for classification},
  author={Graepel, Thore and Herbrich, Ralf and Shawe-Taylor, John},
  journal={Machine Learning},
  volume={59},
  number={1-2},
  pages={55--76},
  year={2005},
  publisher={Springer}
}

@article{littlestone1986relating,
  title={Relating data compression and learnability},
  author={Littlestone, Nick and Warmuth, Manfred},
  year={1986},
  publisher={Citeseer}
}

@book{schapire2012boosting,
  title={Boosting: Foundations and Algorithms},
  author={Schapire, Robert E and Freund, Yoav},
  year={2012},
  publisher={MIT Press}
}

@inproceedings{ben2014sample,
  title={The sample complexity of agnostic learning under deterministic labels},
  author={Ben-David, Shai and Urner, Ruth},
  booktitle={Conference on Learning Theory},
  pages={527--542},
  year={2014},
  organization={PMLR}
}


@inproceedings{montasser2019vc,
  title={Vc classes are adversarially robustly learnable, but only improperly},
  author={Montasser, Omar and Hanneke, Steve and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={2512--2530},
  year={2019},
  organization={PMLR}
}

@article{Alon2021,
  author    = {Noga Alon and
               Steve Hanneke and
               Ron Holzman and
               Shay Moran},
  title     = {A Theory of {PAC} Learnability of Partial Concept Classes},
  journal   = {CoRR},
  volume    = {abs/2107.08444},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.08444},
  eprinttype = {arXiv},
  eprint    = {2107.08444},
  timestamp = {Thu, 22 Jul 2021 11:14:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-08444.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{haussler1994predicting,
  title={Predicting $\{$0, 1$\}$-functions on randomly drawn points},
  author={Haussler, David and Littlestone, Nick and Warmuth, Manfred K},
  journal={Information and Computation},
  volume={115},
  number={2},
  pages={248--292},
  year={1994},
  publisher={Elsevier}
}

@article{david2016statistical,
  title={On statistical learning via the lens of compression},
  author={David, Ofir and Moran, Shay and Yehudayoff, Amir},
  journal={arXiv preprint arXiv:1610.03592},
  year={2016}
}

@article{hopkins2021realizable,
  title={Realizable Learning is All You Need},
  author={Hopkins, Max and Kane, Daniel and Lovett, Shachar and Mahajan, Gaurav},
  journal={arXiv preprint arXiv:2111.04746},
  year={2021}
}

%-----------theory of DA-------
@article{Lyle2020,
  title={On the benefits of invariance in neural networks},
  author={Lyle, Clare and van der Wilk, Mark and Kwiatkowska, Marta and Gal, Yarin and Bloem-Reddy, Benjamin},
  journal={arXiv preprint arXiv:2005.00178},
  year={2020}
}

@inproceedings{Dao2019,
  title={A kernel theory of modern data augmentation},
  author={Dao, Tri and Gu, Albert and Ratner, Alexander and Smith, Virginia and De Sa, Chris and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={1528--1537},
  year={2019},
  organization={PMLR}
}

@article{Chen2020,
  title={A Group-Theoretic Framework for Data Augmentation},
  author={Chen, Shuxiao and Dobriban, Edgar and Lee, Jane H},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--71},
  year={2020}
}
@misc{Wu2020,
abstract = {Data augmentation is a powerful technique to improve performance in applications such as image and text classification tasks. Yet, there is little rigorous understanding of why and how various augmentations work. In this work, we consider a family of linear transformations and study their effects on the ridge estimator in an over-parametrized linear regression setting. First, we show that transformations which preserve the labels of the data can improve estimation by enlarging the span of the training data. Second, we show that transformations which mix data can improve estimation by playing a regularization effect. Finally, we validate our theoretical insights on MNIST. Based on the insights, we propose an augmentation scheme that searches over the space of transformations by how uncertain the model is about the transformed data. We validate our proposed scheme on image and text datasets. For example, our method outperforms RandAugment by 1.24{\%} on CIFAR-100 using Wide-ResNet-28-10. Furthermore, we achieve comparable accuracy to the SoTA Adversarial AutoAugment on CIFAR datasets.},
archivePrefix = {arXiv},
arxivId = {2005.00695},
author = {Wu, Sen and Zhang, Hongyang R. and Valiant, Gregory and R{\'{e}}, Christopher},
booktitle = {arXiv},
eprint = {2005.00695},
issn = {23318422},
title = {{On the generalization effects of linear transformations in data augmentation}},
year = {2020}
}
@inproceedings{VanDerWilk2018,
abstract = {Generalising well in supervised learning tasks relies on correctly extrapolating the training data to a large region of the input space. One way to achieve this is to constrain the predictions to be invariant to transformations of the input that are known to be irrelevant (e.g. translation). Commonly, this is done through data augmentation, where the training set is enlarged by applying hand-crafted transformations to the inputs. We argue that invariances should instead be incorporated in the model structure, and learned using the marginal likelihood, which correctly rewards the reduced complexity of invariant models. We demonstrate this for Gaussian process models, due to the ease with which their marginal likelihood can be estimated. Our main contribution is a variational inference scheme for Gaussian processes containing invariances described by a sampling procedure. We learn the sampling procedure by backpropagating through it to maximise the marginal likelihood.},
archivePrefix = {arXiv},
arxivId = {1808.05563},
author = {{Van Der Wilk}, Mark and Bauer, Matthias and John, S. T. and Hensman, James},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1808.05563},
issn = {10495258},
title = {{Learning invariances using the marginal likelihood}},
year = {2018}
}

@article{wood1996representation,
  title={Representation theory and invariant neural networks},
  author={Wood, Jeffrey and Shawe-Taylor, John},
  journal={Discrete applied mathematics},
  volume={69},
  number={1-2},
  pages={33--60},
  year={1996},
  publisher={Elsevier}
}

@article{bloem2020probabilistic,
  title={Probabilistic Symmetries and Invariant Neural Networks.},
  author={Bloem-Reddy, Benjamin and Teh, Yee Whye},
  journal={J. Mach. Learn. Res.},
  volume={21},
  pages={90--1},
  year={2020}
}

@inproceedings{ravanbakhsh2017equivariance,
  title={Equivariance through parameter-sharing},
  author={Ravanbakhsh, Siamak and Schneider, Jeff and Poczos, Barnabas},
  booktitle={International Conference on Machine Learning},
  pages={2892--2901},
  year={2017},
  organization={PMLR}
}

@inproceedings{kondor2018generalization,
  title={On the generalization of equivariance and convolution in neural networks to the action of compact groups},
  author={Kondor, Risi and Trivedi, Shubhendu},
  booktitle={International Conference on Machine Learning},
  pages={2747--2755},
  year={2018},
  organization={PMLR}
}

%-----------empirical work on DA-------------
@inproceedings{paulin2014transformation,
  title={Transformation pursuit for image classification},
  author={Paulin, Mattis and Revaud, J{\'e}r{\^o}me and Harchaoui, Zaid and Perronnin, Florent and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3646--3653},
  year={2014}
}

@inproceedings{fawzi2016adaptive,
  title={Adaptive data augmentation for image classification},
  author={Fawzi, Alhussein and Samulowitz, Horst and Turaga, Deepak and Frossard, Pascal},
  booktitle={2016 IEEE international conference on image processing (ICIP)},
  pages={3688--3692},
  year={2016},
  organization={Ieee}
}

@article{cubuk2018autoaugment,
  title={Autoaugment: Learning augmentation policies from data},
  author={Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  journal={arXiv preprint arXiv:1805.09501},
  year={2018}
}

@article{chatzipantazis2021learning,
  title={Learning Augmentation Distributions using Transformed Risk Minimization},
  author={Chatzipantazis, Evangelos and Pertigkiozoglou, Stefanos and Dobriban, Edgar and Daniilidis, Kostas},
  journal={arXiv preprint arXiv:2111.08190},
  year={2021}
}

@incollection{fukushima1982neocognitron,
  title={Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition},
  author={Fukushima, Kunihiko and Miyake, Sei},
  booktitle={Competition and cooperation in neural nets},
  pages={267--285},
  year={1982},
  publisher={Springer}
}

@article{lecun1989backpropagation,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}

@inproceedings{cohen2016group,
  title={Group equivariant convolutional networks},
  author={Cohen, Taco and Welling, Max},
  booktitle={International conference on machine learning},
  pages={2990--2999},
  year={2016},
  organization={PMLR}
}
@inproceedings{dieleman2016exploiting,
  title={Exploiting cyclic symmetry in convolutional neural networks},
  author={Dieleman, Sander and De Fauw, Jeffrey and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1889--1898},
  year={2016},
  organization={PMLR}
}

@inproceedings{worrall2017harmonic,
  title={Harmonic networks: Deep translation and rotation equivariance},
  author={Worrall, Daniel E and Garbin, Stephan J and Turmukhambetov, Daniyar and Brostow, Gabriel J},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5028--5037},
  year={2017}
}

@article{gontijo2020affinity,
  title={Affinity and diversity: Quantifying mechanisms of data augmentation},
  author={Gontijo-Lopes, Raphael and Smullin, Sylvia J and Cubuk, Ekin D and Dyer, Ethan},
  journal={arXiv preprint arXiv:2002.08973},
  year={2020}
}

@incollection{baird1992document,
  title={Document image defect models},
  author={Baird, Henry S},
  booktitle={Structured Document Image Analysis},
  pages={546--556},
  year={1992},
  publisher={Springer}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

% ---------empirical work on robustness--------
@article{raghunathan2019adversarial,
  title={Adversarial training can hurt generalization},
  author={Raghunathan, Aditi and Xie, Sang Michael and Yang, Fanny and Duchi, John C and Liang, Percy},
  journal={arXiv preprint arXiv:1906.06032},
  year={2019}
}

@article{schmidt2018adversarially,
  title={Adversarially robust generalization requires more data},
  author={Schmidt, Ludwig and Santurkar, Shibani and Tsipras, Dimitris and Talwar, Kunal and M{\k{a}}dry, Aleksander},
  journal={arXiv preprint arXiv:1804.11285},
  year={2018}
}

@article{nakkiran2019adversarial,
  title={Adversarial robustness may be at odds with simplicity},
  author={Nakkiran, Preetum},
  journal={arXiv preprint arXiv:1901.00532},
  year={2019}
}

@inproceedings{elesedy2021linear,
  title={Provably strict generalisation benefit for equivariant models},
  author={Elesedy, Bryn and Zaidi, Sheheryar},
  booktitle={International Conference on Machine Learning},
  pages={2959--2969},
  year={2021},
  organization={PMLR}
}

@article{elesedy2021kernel,
  title={Provably Strict Generalisation Benefit for Invariance in Kernel Methods},
  author={Elesedy, Bryn},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{elesedy2022group,
  title={Group Symmetry in PAC Learning},
  author={Elesedy, Bryn},
  booktitle={ICLR 2022 Workshop on Geometrical and Topological Representation Learning},
  year={2022}
}

@inproceedings{mei2021learning,
  title={Learning with invariances in random features and kernel models},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory},
  pages={3351--3418},
  year={2021},
  organization={PMLR}
}

@article{bietti2021sample,
  title={On the Sample Complexity of Learning under Geometric Stability},
  author={Bietti, Alberto and Venturi, Luca and Bruna, Joan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{shen2022data,
  title={Data Augmentation as Feature Manipulation: a story of desert cows and grass cows},
  author={Shen, Ruoqi and Bubeck, S{\'e}bastien and Gunasekar, Suriya},
  journal={arXiv preprint arXiv:2203.01572},
  year={2022}
}

@inproceedings{wang2020tent,
  author    = {Dequan Wang and
               Evan Shelhamer and
               Shaoteng Liu and
               Bruno A. Olshausen and
               Trevor Darrell},
  title     = {Tent: Fully Test-Time Adaptation by Entropy Minimization},
  booktitle = {International Conference on Learning Representations},
  year      = {2021}
}