\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allcott(2011)]{socialchoicecourseware}
H.~Allcott.
\newblock Environmental policy and economics, Spring 2011.
\newblock URL
  \url{https://ocw.mit.edu/courses/economics/14-42-environmental-policy-and-economics-spring-2011/lecture-notes/MIT14_42S11_lec02.pdf}.

\bibitem[Arrow et~al.(1951)]{arrow1951extension}
K.~J. Arrow et~al.
\newblock An extension of the basic theorems of classical welfare economics.
\newblock In \emph{Proceedings of the second Berkeley symposium on mathematical
  statistics and probability}. The Regents of the University of California,
  1951.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
J.~L. Ba, J.~R. Kiros, and G.~E. Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bartholdi et~al.(1989)Bartholdi, Tovey, and
  Trick]{bartholdi1989voting}
J.~Bartholdi, C.~A. Tovey, and M.~A. Trick.
\newblock Voting schemes for which it can be difficult to tell who won the
  election.
\newblock \emph{Social Choice and welfare}, 6\penalty0 (2):\penalty0 157--165,
  1989.

\bibitem[Bennett et~al.(2007)Bennett, Lanning, et~al.]{bennett2007netflix}
J.~Bennett, S.~Lanning, et~al.
\newblock The netflix prize.
\newblock In \emph{Proceedings of KDD cup and workshop}, volume 2007, page~35.
  Citeseer, 2007.

\bibitem[Boutilier et~al.(2015)Boutilier, Caragiannis, Haber, Lu, Procaccia,
  and Sheffet]{boutilier2015optimal}
C.~Boutilier, I.~Caragiannis, S.~Haber, T.~Lu, A.~D. Procaccia, and O.~Sheffet.
\newblock Optimal social choice functions: A utilitarian view.
\newblock \emph{Artificial Intelligence}, 227:\penalty0 190--213, 2015.

\bibitem[Bubboloni et~al.(2020)Bubboloni, Diss, and
  Gori]{bubboloni2020extensions}
D.~Bubboloni, M.~Diss, and M.~Gori.
\newblock Extensions of the simpson voting rule to the committee selection
  setting.
\newblock \emph{Public Choice}, 183\penalty0 (1):\penalty0 151--185, 2020.

\bibitem[Caragiannis et~al.(2017)Caragiannis, Nath, Procaccia, and
  Shah]{caragiannis2017subset}
I.~Caragiannis, S.~Nath, A.~D. Procaccia, and N.~Shah.
\newblock Subset selection via implicit utilitarian voting.
\newblock \emph{Journal of Artificial Intelligence Research}, 58:\penalty0
  123--152, 2017.

\bibitem[Chen et~al.(2019)Chen, Villar, Chen, and Bruna]{chen2019equivalence}
Z.~Chen, S.~Villar, L.~Chen, and J.~Bruna.
\newblock On the equivalence between graph isomorphism testing and function
  approximation with gnns.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32, pages 15894--15902. Curran
  Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/71ee911dd06428a96c143a0b135041a4-Paper.pdf}.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
P.~Goyal, P.~Doll{\'a}r, R.~Girshick, P.~Noordhuis, L.~Wesolowski, A.~Kyrola,
  A.~Tulloch, Y.~Jia, and K.~He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pages
  448--456. PMLR, 2015.

\bibitem[Kamishima(2003)]{kamishima2003nantonac}
T.~Kamishima.
\newblock Nantonac collaborative filtering: recommendation based on order
  responses.
\newblock In \emph{Proceedings of the ninth ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 583--588, 2003.

\bibitem[Kemeny(1959)]{kemeny1959mathematics}
J.~G. Kemeny.
\newblock Mathematics without numbers.
\newblock \emph{Daedalus}, 88\penalty0 (4):\penalty0 577--591, 1959.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kujawska et~al.(2020)Kujawska, Slavkovik, and
  R{\"u}ckmann]{kujawska2020predicting}
H.~Kujawska, M.~Slavkovik, and J.-J. R{\"u}ckmann.
\newblock Predicting the winners of borda, kemeny and dodgson elections with
  supervised machine learning.
\newblock In \emph{Multi-Agent Systems and Agreement Technologies}, pages
  440--458. Springer, 2020.

\bibitem[Langley(2000)]{langley00}
P.~Langley.
\newblock Crafting papers on machine learning.
\newblock In P.~Langley, editor, \emph{Proceedings of the 17th International
  Conference on Machine Learning (ICML 2000)}, pages 1207--1216, Stanford, CA,
  2000. Morgan Kaufmann.

\bibitem[Lee et~al.(2018)Lee, Lee, Kim, Kosiorek, Choi, and Teh]{lee2018set}
J.~Lee, Y.~Lee, J.~Kim, A.~R. Kosiorek, S.~Choi, and Y.~W. Teh.
\newblock Set transformer: A framework for attention-based
  permutation-invariant neural networks.
\newblock \emph{arXiv preprint arXiv:1810.00825}, 2018.

\bibitem[Leman and Weisfeiler(1968)]{leman1968reduction}
A.~Leman and B.~Weisfeiler.
\newblock A reduction of a graph to a canonical form and an algebra arising
  during this reduction.
\newblock \emph{Nauchno-Technicheskaya Informatsiya}, 2\penalty0 (9):\penalty0
  12--16, 1968.

\bibitem[Maas et~al.(2013)Maas, Hannun, and Ng]{maas2013rectifier}
A.~L. Maas, A.~Y. Hannun, and A.~Y. Ng.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In \emph{Proc. icml}, volume~30, page~3. Citeseer, 2013.

\bibitem[Mao et~al.(2013)Mao, Procaccia, and Chen]{mao2013better}
A.~Mao, A.~Procaccia, and Y.~Chen.
\newblock Better human computation through principled voting.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~27, 2013.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
A.~Paszke, S.~Gross, S.~Chintala, G.~Chanan, E.~Yang, Z.~DeVito, Z.~Lin,
  A.~Desmaison, L.~Antiga, and A.~Lerer.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Procaccia et~al.(2009)Procaccia, Zohar, Peleg, and
  Rosenschein]{procaccia2009learnability}
A.~D. Procaccia, A.~Zohar, Y.~Peleg, and J.~S. Rosenschein.
\newblock The learnability of voting rules.
\newblock \emph{Artificial Intelligence}, 173\penalty0 (12-13):\penalty0
  1133--1149, 2009.

\bibitem[Scarselli et~al.(2008)Scarselli, Gori, Tsoi, Hagenbuchner, and
  Monfardini]{scarselli2008graph}
F.~Scarselli, M.~Gori, A.~C. Tsoi, M.~Hagenbuchner, and G.~Monfardini.
\newblock The graph neural network model.
\newblock \emph{IEEE transactions on neural networks}, 20\penalty0
  (1):\penalty0 61--80, 2008.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem[Wang et~al.(2019)Wang, Yu, Zheng, Gan, Gai, Ye, Li, Zhou, Huang, Ma,
  et~al.]{wang2019deep}
M.~Wang, L.~Yu, D.~Zheng, Q.~Gan, Y.~Gai, Z.~Ye, M.~Li, J.~Zhou, Q.~Huang,
  C.~Ma, et~al.
\newblock Deep graph library: Towards efficient and scalable deep learning on
  graphs.
\newblock 2019.

\bibitem[Weisfeiler and Leman(1968)]{weisfeiler1968reduction}
B.~Weisfeiler and A.~Leman.
\newblock The reduction of a graph to canonical form and the algebra which
  appears therein.
\newblock \emph{Nauchno-Technicheskaya Informatsia}, 2\penalty0 (9):\penalty0
  12--16, 1968.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu]{xiong2020layer}
R.~Xiong, Y.~Yang, D.~He, K.~Zheng, S.~Zheng, C.~Xing, H.~Zhang, Y.~Lan,
  L.~Wang, and T.~Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{International Conference on Machine Learning}, pages
  10524--10533. PMLR, 2020.

\bibitem[Xu et~al.(2018)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful}
K.~Xu, W.~Hu, J.~Leskovec, and S.~Jegelka.
\newblock How powerful are graph neural networks?
\newblock \emph{arXiv preprint arXiv:1810.00826}, 2018.

\bibitem[Young(1975)]{young1975social}
H.~P. Young.
\newblock Social choice scoring functions.
\newblock \emph{SIAM Journal on Applied Mathematics}, 28\penalty0 (4):\penalty0
  824--838, 1975.

\bibitem[Zaheer et~al.(2017)Zaheer, Kottur, Ravanbakhsh, Poczos, Salakhutdinov,
  and Smola]{zaheer2017deep}
M.~Zaheer, S.~Kottur, S.~Ravanbakhsh, B.~Poczos, R.~R. Salakhutdinov, and A.~J.
  Smola.
\newblock Deep sets.
\newblock In \emph{Advances in neural information processing systems}, pages
  3391--3401, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Lucas, Hinton, and Ba]{zhang2019lookahead}
M.~R. Zhang, J.~Lucas, G.~Hinton, and J.~Ba.
\newblock Lookahead optimizer: k steps forward, 1 step back.
\newblock \emph{arXiv preprint arXiv:1907.08610}, 2019.

\end{thebibliography}
