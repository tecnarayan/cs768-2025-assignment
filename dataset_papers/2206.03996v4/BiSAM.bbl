\begin{thebibliography}{70}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille et~al.(2019)Achille, Lam, Tewari, Ravichandran, Maji, Fowlkes,
  Soatto, and Perona]{Achille}
Achille, A., Lam, M., Tewari, R., Ravichandran, A., Maji, S., Fowlkes, C.~C.,
  Soatto, S., and Perona, P.
\newblock Task2vec: Task embedding for meta-learning.
\newblock In \emph{Proc. International Conference on Computer Vision}, pp.\
  6430--6439, Seoul, South Korea, 2019.

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,
  Schaul, and d.~Freitas]{Andrychowicz}
Andrychowicz, M., Denil, M., Gomez, S., Hoffman, M.~W., Pfau, D., Schaul, T.,
  and d.~Freitas, N.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems},
  pp.\  3981--3989, Barcelona, Spain, December 2016.

\bibitem[Antoniou et~al.(2019)Antoniou, Edwards, and Storkey]{Antoniou}
Antoniou, A., Edwards, H., and Storkey, A.
\newblock How to train your maml.
\newblock In \emph{Proc. International Conference on Learning Representations},
  New Orleans, LA, 2019.

\bibitem[Bahri et~al.(2021)Bahri, Mobahi, and Tay]{Bahri}
Bahri, D., Mobahi, H., and Tay, Y.
\newblock Sharpness-aware minimization improves language model generalization.
\newblock \emph{arXiv preprint:2110.08529}, 2021.

\bibitem[Brock et~al.(2018)Brock, Lim, Ritchie, and Weston]{Brock}
Brock, A., Lim, T., Ritchie, J.~M., and Weston, N.
\newblock {SmaSH}: One-shot model architecture search through hypernetworks.
\newblock In \emph{Proc. International Conference on Learning Representations},
  Vancouver, Canada, April 2018.

\bibitem[Chen \& Chen(2022)Chen and Chen]{chen2022bayes_maml}
Chen, L. and Chen, T.
\newblock Is {B}ayesian model-agnostic meta learning better than model-agnostic
  meta learning, provably?
\newblock In \emph{Proc. International Conference on Artificial Intelligence
  and Statistics}, March 2022.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Sun, and Yin]{Chen}
Chen, T., Sun, Y., and Yin, W.
\newblock Closing the gap: Tighter analysis of alternating stochastic gradient
  methods for bilevel problems.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems},
  volume~34, Virtual, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2022)Chen, Sun, Xiao, and Yin]{chen2021stable}
Chen, T., Sun, Y., Xiao, Q., and Yin, W.
\newblock A single-timescale method for stochastic bilevel optimization.
\newblock In \emph{Proc. International Conference on Artificial Intelligence
  and Statistics}, pp.\  2466--2488, March 2022.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Hsieh, and Gong]{Chen2}
Chen, X., Hsieh, C., and Gong, B.
\newblock When vision transformers outperform resnets without pretraining or
  strong data augmentations.
\newblock \emph{arXiv preprint:2106.0154}, 2021{\natexlab{b}}.

\bibitem[Deleu et~al.(2019)Deleu, W\"urfl, Samiei, Cohen, and
  Bengio]{deleu2019torchmeta}
Deleu, T., W\"urfl, T., Samiei, M., Cohen, J.~P., and Bengio, Y.
\newblock {Torchmeta: A Meta-Learning library for PyTorch}, 2019.
\newblock URL \url{https://arxiv.org/abs/1909.06576}.

\bibitem[Denevi et~al.(2018)Denevi, Ciliberto, Stamos, and
  Pontil]{denevi2018_l2l_linear_centroid}
Denevi, G., Ciliberto, C., Stamos, D., and Pontil, M.
\newblock Learning to learn around a common mean.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems},
  volume~31, Montreal, Canada, 2018.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, S.Bengio, and Bengio]{Dinh}
Dinh, L., Pascanu, R., S.Bengio, and Bengio, Y.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{Proc. International Conference on Machine Learning}, pp.\
  1019--1028, Sydney, Australia, August 2017.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, S.~Gelly, and
  Houlsby]{Dosovitskiy}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., S.~Gelly, J.~U.,
  and Houlsby, N.
\newblock An image is worth 16$\times$ 16 words: Transformers for image
  recognition at scale.
\newblock In \emph{Proc. International Conference on Learning Representations},
  Virtual, April 2020.

\bibitem[Du et~al.(2021)Du, Yan, Feng, Zhou, Zhen, Goh, and Tan]{Du2}
Du, J., Yan, H., Feng, J., Zhou, J.~T., Zhen, L., Goh, R. S.~M., and Tan, V.
  Y.~F.
\newblock Efficient sharpness-aware minimization for improved training of
  neural networks.
\newblock In \emph{Proc. IEEE Conference on Computer Vision and Pattern
  Recognition}, Virtual, June 2021.

\bibitem[Dziugaite \& Roy(2016)Dziugaite and Roy]{Dziugaite}
Dziugaite, G. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock In \emph{Proc. Conference on Uncertainty in Artificial Intelligence},
  Sydney, Australia, August 2016.

\bibitem[Fallah et~al.(2020)Fallah, Mokhtari, and
  Ozdaglar]{fallah2020convergence}
Fallah, A., Mokhtari, A., and Ozdaglar, A.
\newblock On the convergence theory of gradient-based model-agnostic
  meta-learning algorithms.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1082--1092, 2020.

\bibitem[Farid \& Majumdar(2021)Farid and Majumdar]{farid2021generalization}
Farid, A. and Majumdar, A.
\newblock Generalization bounds for meta-learning via pac-bayes and uniform
  stability.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Finn \& Levine(2018)Finn and Levine]{Finn2}
Finn, C. and Levine, S.
\newblock Meta-learning and universality: Deep representations and gradient
  descent can approximate any learning algorithm.
\newblock In \emph{Proc. International Conference on Learning Representations},
  Vancouver, Canada, 2018.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{Finn}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proc. International Conference on Machine Learning}, Sydney,
  Australia, 2017.

\bibitem[Finn et~al.(2019)Finn, Rajeswaran, Kakade, and Levine]{finn2019online}
Finn, C., Rajeswaran, A., Kakade, S., and Levine, S.
\newblock Online meta-learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1920--1930, 2019.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and Neyshabur]{Foret}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{Proc. International Conference on Learning Representations},
  Virtual, April 2021.

\bibitem[Goldblum et~al.(2020)Goldblum, Fowl, and Goldstein]{Goldblum}
Goldblum, M., Fowl, L., and Goldstein, T.
\newblock Adversarially robust few-shot learning: A meta-learning approach.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems},
  Virtual, December 2020.

\bibitem[Gonzalez \& Miikkulainen(2020)Gonzalez and Miikkulainen]{Gonzalez}
Gonzalez, S. and Miikkulainen, R.
\newblock Improved training speed, accuracy, and data utilization through loss
  function optimization.
\newblock In \emph{Proc. IEEE Congress on Evolutionary Computation}, pp.\
  1--8, Glasgow, United Kingdom, 2020.

\bibitem[Grant et~al.(2018{\natexlab{a}})Grant, Finn, Levine, Darrell, and
  Griffiths]{Grant}
Grant, E., Finn, C., Levine, S., Darrell, T., and Griffiths, T.
\newblock Recasting gradient-based meta-learning as hierarchical bayes.
\newblock In \emph{Proc. International Conference on Learning Representations},
  April 2018{\natexlab{a}}.

\bibitem[Grant et~al.(2018{\natexlab{b}})Grant, Finn, Levine, Darrell, and
  Griffiths]{grant2018recasting}
Grant, E., Finn, C., Levine, S., Darrell, T., and Griffiths, T.
\newblock Recasting gradient-based meta-learning as hierarchical bayes.
\newblock In \emph{Proc. International Conference on Learning Representations},
  Vancouver, Canada, 2018{\natexlab{b}}.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
Hardt, M., Recht, B., and Singer, Y.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1225--1234, 2016.

\bibitem[Hsu et~al.(2018)Hsu, Levine, and Fin]{Hsu}
Hsu, K., Levine, S., and Fin, C.
\newblock Unsupervised learning via meta-learning.
\newblock In \emph{Proc. International Conference on Learning Representations},
  Vancouver, Canada, April 2018.

\bibitem[Huang et~al.(2018)Huang, Wang, Singh, Yih, and He]{Huang2}
Huang, P., Wang, C., Singh, R., Yih, W., and He, X.
\newblock Natural language to structured query generation via meta-learning.
\newblock In \emph{Proc. Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies}, pp.\
   732--738, New Orleans, LA, 2018.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{Ioffe}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{Proc. International Conference on Machine Learning}, Lille,
  France, June 2015.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{Izmailov}
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A.~G.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In \emph{Proc. Conference on Uncertainty in Artificial Intelligence},
  pp.\  876--885, Monterey, CA, 2018.

\bibitem[Ji et~al.(2022)Ji, Yang, and Liang]{ji2022theoretical}
Ji, K., Yang, J., and Liang, Y.
\newblock Theoretical convergence of multi-step model-agnostic meta-learning.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (29):\penalty0 1--41, 2022.

\bibitem[Jiang et~al.(2019)Jiang, Neyshabur, Mobahi, Krishnan, and
  Bengio]{Jiang2}
Jiang, Y., Neyshabur, B., Mobahi, H., Krishnan, D., and Bengio, S.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{Proc. International Conference on Learning Representations},
  New Orleans, LA, 2019.

\bibitem[Kao et~al.(2021)Kao, Chiu, and Chen]{kao2021maml}
Kao, C.-H., Chiu, W.-C., and Chen, P.-Y.
\newblock Maml is a noisy contrastive learner.
\newblock \emph{arXiv preprint arXiv:2106.15367}, 2021.

\bibitem[Keskar et~al.(2017)Keskar, D.Mudigere, Noceda, Smelyanskiy, and
  Tang]{Keshakar}
Keskar, N., D.Mudigere, Noceda, J., Smelyanskiy, M., and Tang, P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{Proc. International Conference on Learning Representations},
  Toulon, France, May 2017.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{Krizhevsky}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems},
  pp.\  1097--1105, Lake Tahoe, NV, 2012.

\bibitem[Langley(2000)]{langley00}
Langley, P.
\newblock Crafting papers on machine learning.
\newblock In \emph{Proc. International Conference on Machine Learning}, pp.\
  1207--1216, Stanford, CA, 2000.

\bibitem[Laurent \& Massart(2000)Laurent and Massart]{laurent2000}
Laurent, B. and Massart, P.
\newblock {Adaptive estimation of a quadratic functional by model selection}.
\newblock \emph{The Annals of Statistics}, 28\penalty0 (5):\penalty0 1302 --
  1338, 2000.

\bibitem[Li et~al.(2018{\natexlab{a}})Li, Yang, Song, and Hospedales]{Li4}
Li, D., Yang, Y., Song, Y., and Hospedales, T.~M.
\newblock Learning to generalize: Meta-learning for domain generalization.
\newblock In \emph{Proc. AAAI Conference on Artificial Intelligence}, New
  Orleans, LA, February 2018{\natexlab{a}}.

\bibitem[Li et~al.(2018{\natexlab{b}})Li, Xu, Taylor, Studer, and
  Goldstein]{Li9}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems},
  Montreal, Canada, December 2018{\natexlab{b}}.

\bibitem[Li \& Malik(2017)Li and Malik]{Li3}
Li, K. and Malik, J.
\newblock Learning to optimize.
\newblock In \emph{Proc. International Conference on Learning Representations},
  Toulon, France, May 2017.

\bibitem[Maicas et~al.(2018)Maicas, Bradley, Nascimento, Reid, and
  Carneiro]{Maicas}
Maicas, G., Bradley, A.~P., Nascimento, J.~C., Reid, I., and Carneiro, G.
\newblock Training medical image analysis systems like radiologists.
\newblock In \emph{Proc. International Conference on Medical Image Computing
  and Computer-Assisted Intervention}, pp.\  546--554, Granada, Spain, 2018.

\bibitem[Munkhdalai \& Yu(2017)Munkhdalai and Yu]{Munkhdalai}
Munkhdalai, T. and Yu, H.
\newblock Meta networks.
\newblock In \emph{Proc. International Conference on Machine Learning}, pp.\
  2554--2563, Sydney, Australia, August 2017.

\bibitem[Neyshabur et~al.(2019)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{Neyshabur}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N.
\newblock Exploring general-ization in deep learning.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems},
  pp.\  5947--5956, Vancouver, Canada, 2019.

\bibitem[Nichol \& Schulman(2018)Nichol and Schulman]{nichol2018_reptile}
Nichol, A. and Schulman, J.
\newblock Reptile: a scalable meta learning algorithm.
\newblock \emph{arXiv preprint arXiv:1803.02999}, 2018.

\bibitem[Novak \& Gowin(1984)Novak and Gowin]{Novak}
Novak, J.~D. and Gowin, D.~B.
\newblock \emph{Learning how to learn}.
\newblock Cambridge University Press, 1984.

\bibitem[Obamuyide \& Vlachos(2019)Obamuyide and Vlachos]{Obamuyide}
Obamuyide, A. and Vlachos, A.
\newblock Model-agnostic meta-learning for relation classification with limited
  supervision.
\newblock In \emph{Proc. Annual Meeting of the Association for Computational
  Linguistics}, Florence, Italy, July 2019.

\bibitem[Park \& Oliva(2019)Park and Oliva]{Park}
Park, E. and Oliva, J.~B.
\newblock Meta-curvature.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems},
  Vancouver, Canada, December 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{Raffel}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  trans- former.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 1--67,
  2020.

\bibitem[Raghu et~al.(2020)Raghu, Raghu, and Bengio]{Raghu}
Raghu, A., Raghu, M., and Bengio, S.
\newblock Rapid learning or feature reuse? towards understanding the
  effectiveness of maml.
\newblock In \emph{Proc. International Conference on Learning Representations},
  Virtual, April 2020.

\bibitem[Rajeswaran et~al.(2019{\natexlab{a}})Rajeswaran, Finn, Kakade, and
  Levine]{Rajeswaran}
Rajeswaran, A., Finn, C., Kakade, S., and Levine, S.
\newblock Meta-learning with implicit gradients.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems},
  pp.\  113--124, Vancouver, Canada, December 2019{\natexlab{a}}.

\bibitem[Rajeswaran et~al.(2019{\natexlab{b}})Rajeswaran, Finn, Kakade, and
  Levine]{rajeswaran2019_imaml}
Rajeswaran, A., Finn, C., Kakade, S.~M., and Levine, S.
\newblock Meta-learning with implicit gradients.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems},
  pp.\  113--124, Vancouver, Canada, 2019{\natexlab{b}}.

\bibitem[Ravi \& Larochelle(2017)Ravi and Larochelle]{Ravi}
Ravi, S. and Larochelle, H.
\newblock Optimization as a model for few-shot learning.
\newblock In \emph{Proc. International Conference on Learning Representations},
  Toulon, France, 2017.

\bibitem[Rothfuss et~al.(2021)Rothfuss, Fortuin, Josifoski, and
  Krause]{rothfuss2021_pacoh_pac_bayes_ml}
Rothfuss, J., Fortuin, V., Josifoski, M., and Krause, A.
\newblock {PACOH}: Bayes-optimal meta-learning with pac-guarantees.
\newblock In \emph{Proc. International Conference on Machine Learning}, pp.\
  9116--9126, virtual, 2021.

\bibitem[Schmidhuber(1987)]{Schmidhuber}
Schmidhuber, J.
\newblock Evolutionary principles in self-referential learning. on learning now
  to learn: The meta-meta-meta...-hook.
\newblock \emph{http://www.idsia.ch/~juergen/diploma.html}, 1987.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{Snell}
Snell, J., Swersky, K., and Zemel, R.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems},
  pp.\  4077--4087, Long Beach, CA, December 2017.

\bibitem[Thrun \& Pratt(2012)Thrun and Pratt]{Thrun}
Thrun, S. and Pratt, L.
\newblock Learning to learn.
\newblock \emph{Springer Science \& Business Media}, 2012.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai,
  Unterthiner, Yung, Keysers, Uszkoreit, Lucic, and Dosovitskiy]{Tolstikhin}
Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner,
  T., Yung, J., Keysers, D., Uszkoreit, J., Lucic, M., and Dosovitskiy, A.
\newblock {Mlp-mixer}: An all-mlp architecture for vision.
\newblock \emph{arXiv preprint:2105.01601}, 2021.

\bibitem[Vilalta \& Drissi(2002)Vilalta and Drissi]{Vilalta}
Vilalta, R. and Drissi, Y.
\newblock A perspective view and survey of meta-learning.
\newblock \emph{Artificial Intelligence Review}, 2002.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Kavukcuoglu, and
  Wierstra]{Vinyals}
Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D.
\newblock Matching networks for one shot learning.
\newblock In \emph{Proc. Advances in Neural information processing systems},
  Barcelona, Spain, December 2016.

\bibitem[Vuorio et~al.(2019)Vuorio, Sun, Hu, and Lim]{Vuorio}
Vuorio, R., Sun, S., Hu, H., and Lim, J.~J.
\newblock Multimodal model-agnostic metalearning via task-aware modulation.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems},
  pp.\  1--12, Vancouver, Canada, December 2019.

\bibitem[Wang et~al.(2020)Wang, Luo, Sun, Xiong, and Zeng]{Wang4}
Wang, G., Luo, C., Sun, X., Xiong, Z., and Zeng, W.
\newblock Tracking by instance detection: A meta-learning approach.
\newblock In \emph{Proc. IEEE Conference on Computer Vision and Pattern
  Recognition}, pp.\  6288--6297, Virtual, 2020.

\bibitem[Wang et~al.(2016)Wang, Kurth-Nelson, Tirumala, Soyer, Leibo, Munos,
  Blundell, Kumaran, and Botvinick]{Wang3}
Wang, J.~X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J.~Z., Munos,
  R., Blundell, C., Kumaran, D., and Botvinick, M.
\newblock Learning to reinforcement learn.
\newblock \emph{arXiv preprint:1611.05763}, 2016.

\bibitem[Wang et~al.(2021)Wang, Xu, Liu, Chen, Weng, Gan, and Wang]{Wang}
Wang, R., Xu, K., Liu, S., Chen, P., Weng, T., Gan, C., and Wang, M.
\newblock On fast adversarial robustness adaptation in model-agnostic
  meta-learning.
\newblock In \emph{Proc. International Conference on Learning Representations},
  Virtual, May 2021.

\bibitem[Wang et~al.(2019)Wang, Ramanan, and Hebert]{Wang5}
Wang, Y., Ramanan, D., and Hebert, M.
\newblock Meta-learning to detect rare objects.
\newblock In \emph{Proc. International Conference on Computer Vision}, Seoul,
  Korea, 2019.

\bibitem[Xu et~al.(2020)Xu, Li, Liu, Liu, and Tang]{Xu}
Xu, H., Li, Y., Liu, X., Liu, H., and Tang, J.
\newblock Yet meta learning can adapt fast, it can also break easily.
\newblock \emph{arXiv preprint: 2009.01672}, 2020.

\bibitem[Xue et~al.(2020)Xue, Constant, Roberts, Kale, Al-Rfou, Siddhant,
  Barua, and Raffel]{Xue}
Xue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua,
  A., and Raffel, C.
\newblock {mT5}: A massively multilingual pre-trained text-to-text transformer.
\newblock \emph{arXiv preprint:2010.11934}, 2020.

\bibitem[Yin et~al.(2020)Yin, Tucker, Zhou, Levine, and Finn]{Yin2}
Yin, M., Tucker, G., Zhou, M., Levine, S., and Finn, C.
\newblock Meta-learning without memorization.
\newblock In \emph{Proc. International Conference on Learning Representations},
  Virtual, April 2020.

\bibitem[Yoon et~al.(2018)Yoon, Kim, Dia, Kim, Bengio, and Ahn]{yoon2018_BMAML}
Yoon, J., Kim, T., Dia, O., Kim, S., Bengio, Y., and Ahn, S.
\newblock Bayesian model-agnostic meta-learning.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems},
  volume~31, Montreal, Canada, 2018.

\bibitem[Zintgraf et~al.(2019{\natexlab{a}})Zintgraf, Shiarli, Kurin, Hofmann,
  and Whiteson]{Zintgraf}
Zintgraf, L., Shiarli, K., Kurin, V., Hofmann, K., and Whiteson, S.
\newblock Fast context adaptation via meta-learning.
\newblock In \emph{Proc. International Conference on Machine Learning}, pp.\
  7693â€“7702, Long Beach, CA, June 2019{\natexlab{a}}.

\bibitem[Zintgraf et~al.(2019{\natexlab{b}})Zintgraf, Shiarli, Kurin, Hofmann,
  and Whiteson]{zintgraf2019_cavia}
Zintgraf, L., Shiarli, K., Kurin, V., Hofmann, K., and Whiteson, S.
\newblock Fast context adaptation via meta-learning.
\newblock In \emph{Proc. International Conference on Machine Learning}, pp.\
  7693--7702, Long Beach, CA, 2019{\natexlab{b}}.

\end{thebibliography}
