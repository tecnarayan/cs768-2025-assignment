\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2018)Allen-Zhu, Li, and Song]{allen2018convergence}
Z.~Allen-Zhu, Y.~Li, and Z.~Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock \emph{arXiv preprint arXiv:1811.03962}, 2018.

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
D.~Arpit, S.~Jastrz{\k{e}}bski, N.~Ballas, D.~Krueger, E.~Bengio, M.~S. Kanwal,
  T.~Maharaj, A.~Fischer, A.~Courville, Y.~Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  233--242, 2017.

\bibitem[Bartlett et~al.(1999)Bartlett, Maiorov, and Meir]{bartlett1999almost}
P.~L. Bartlett, V.~Maiorov, and R.~Meir.
\newblock Almost linear {VC} dimension bounds for piecewise polynomial
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  190--196, 1999.

\bibitem[Bartlett et~al.(2019{\natexlab{a}})Bartlett, Harvey, Liaw, and
  Mehrabian]{bartlett2019nearly}
P.~L. Bartlett, N.~Harvey, C.~Liaw, and A.~Mehrabian.
\newblock Nearly-tight {VC}-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (63):\penalty0 1--17, 2019{\natexlab{a}}.
\newblock URL \url{http://jmlr.org/papers/v20/17-612.html}.

\bibitem[Bartlett et~al.(2019{\natexlab{b}})Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2019benign}
P.~L. Bartlett, P.~M. Long, G.~Lugosi, and A.~Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{arXiv preprint arXiv:1906.11300}, 2019{\natexlab{b}}.

\bibitem[Baum(1988)]{baum1988capabilities}
E.~B. Baum.
\newblock On the capabilities of multilayer perceptrons.
\newblock \emph{Journal of complexity}, 4\penalty0 (3):\penalty0 193--215,
  1988.

\bibitem[Belkin et~al.(2018{\natexlab{a}})Belkin, Hsu, Ma, and
  Mandal]{belkin2018reconciling}
M.~Belkin, D.~Hsu, S.~Ma, and S.~Mandal.
\newblock Reconciling modern machine learning and the bias-variance trade-off.
\newblock \emph{arXiv preprint arXiv:1812.11118}, 2018{\natexlab{a}}.

\bibitem[Belkin et~al.(2018{\natexlab{b}})Belkin, Rakhlin, and
  Tsybakov]{belkin2018does}
M.~Belkin, A.~Rakhlin, and A.~B. Tsybakov.
\newblock Does data interpolation contradict statistical optimality?
\newblock \emph{arXiv preprint arXiv:1806.09471}, 2018{\natexlab{b}}.

\bibitem[Brutzkus and Globerson(2017)]{brutzkus2017globally}
A.~Brutzkus and A.~Globerson.
\newblock Globally optimal gradient descent for a {ConvNet} with {G}aussian
  inputs.
\newblock In \emph{International Conference on Machine Learning}, pages
  605--614, 2017.

\bibitem[Brutzkus et~al.(2018)Brutzkus, Globerson, Malach, and
  Shalev-Shwartz]{brutzkus2018sgd}
A.~Brutzkus, A.~Globerson, E.~Malach, and S.~Shalev-Shwartz.
\newblock {SGD} learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Cover(1965)]{cover1965geometrical}
T.~M. Cover.
\newblock Geometrical and statistical properties of systems of linear
  inequalities with applications in pattern recognition.
\newblock \emph{IEEE transactions on electronic computers}, \penalty0
  (3):\penalty0 326--334, 1965.

\bibitem[Cybenko(1989)]{cybenko1989approximation}
G.~Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of control, signals and systems}, 2\penalty0
  (4):\penalty0 303--314, 1989.

\bibitem[Delalleau and Bengio(2011)]{delalleau2011shallow}
O.~Delalleau and Y.~Bengio.
\newblock Shallow vs. deep sum-product networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  666--674, 2011.

\bibitem[Du et~al.(2017)Du, Lee, Tian, Poczos, and Singh]{du2017gradient}
S.~S. Du, J.~D. Lee, Y.~Tian, B.~Poczos, and A.~Singh.
\newblock Gradient descent learns one-hidden-layer {CNN}: Don't be afraid of
  spurious local minima.
\newblock \emph{arXiv preprint arXiv:1712.00779}, 2017.

\bibitem[Du et~al.(2018{\natexlab{a}})Du, Lee, Li, Wang, and
  Zhai]{du2018gradientB}
S.~S. Du, J.~D. Lee, H.~Li, L.~Wang, and X.~Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1811.03804}, 2018{\natexlab{a}}.

\bibitem[Du et~al.(2018{\natexlab{b}})Du, Zhai, Poczos, and
  Singh]{du2018gradientA}
S.~S. Du, X.~Zhai, B.~Poczos, and A.~Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018{\natexlab{b}}.

\bibitem[Eldan and Shamir(2016)]{eldan2016power}
R.~Eldan and O.~Shamir.
\newblock The power of depth for feedforward neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 907--940, 2016.

\bibitem[Funahashi(1989)]{funahashi1989approximate}
K.-I. Funahashi.
\newblock On the approximate realization of continuous mappings by neural
  networks.
\newblock \emph{Neural networks}, 2\penalty0 (3):\penalty0 183--192, 1989.

\bibitem[HaoChen and Sra(2018)]{haochen2018random}
J.~Z. HaoChen and S.~Sra.
\newblock Random shuffling beats {SGD} after finite epochs.
\newblock \emph{arXiv preprint arXiv:1806.10077}, 2018.

\bibitem[Hardt and Ma(2017)]{hardt2017identity}
M.~Hardt and T.~Ma.
\newblock Identity matters in deep learning.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and
  White]{hornik1989multilayer}
K.~Hornik, M.~Stinchcombe, and H.~White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Huang(2003)]{huang2003learning}
G.-B. Huang.
\newblock Learning capability and storage capacity of two-hidden-layer
  feedforward networks.
\newblock \emph{IEEE Transactions on Neural Networks}, 14\penalty0
  (2):\penalty0 274--281, 2003.

\bibitem[Huang and Babri(1998)]{huang1998upper}
G.-B. Huang and H.~A. Babri.
\newblock Upper bounds on the number of hidden neurons in feedforward networks
  with arbitrary bounded nonlinear activation functions.
\newblock \emph{IEEE Transactions on Neural Networks}, 9\penalty0 (1):\penalty0
  224--229, 1998.

\bibitem[Huang and Huang(1991)]{huang1991bounds}
S.-C. Huang and Y.-F. Huang.
\newblock Bounds on the number of hidden neurons in multilayer perceptrons.
\newblock \emph{IEEE transactions on neural networks}, 2\penalty0 (1):\penalty0
  47--55, 1991.

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
K.~Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  586--594, 2016.

\bibitem[Kowalczyk(1997)]{kowalczyk1997estimates}
A.~Kowalczyk.
\newblock Estimates of storage capacity of multilayer perceptron with threshold
  logic hidden units.
\newblock \emph{Neural networks}, 10\penalty0 (8):\penalty0 1417--1433, 1997.

\bibitem[Laurent and Brecht(2018)]{laurent2018deep}
T.~Laurent and J.~Brecht.
\newblock Deep linear networks with arbitrary loss: All local minima are
  global.
\newblock In \emph{International Conference on Machine Learning}, pages
  2908--2913, 2018.

\bibitem[Li and Liang(2018)]{li2018learning}
Y.~Li and Y.~Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8168--8177, 2018.

\bibitem[Li and Yuan(2017)]{li2017convergence}
Y.~Li and Y.~Yuan.
\newblock Convergence analysis of two-layer neural networks with {ReLU}
  activation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  597--607, 2017.

\bibitem[Liang and Srikant(2017)]{liang2017deep}
S.~Liang and R.~Srikant.
\newblock Why deep neural networks for function approximation?
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Liang and Rakhlin(2018)]{liang2018just}
T.~Liang and A.~Rakhlin.
\newblock {Just Interpolate: Kernel ``Ridgeless'' Regression Can Generalize}.
\newblock \emph{arXiv preprint arXiv:1808.00387}, 2018.

\bibitem[Liang et~al.(2019)Liang, Rakhlin, and Zhai]{liang2019risk}
T.~Liang, A.~Rakhlin, and X.~Zhai.
\newblock On the risk of minimum-norm interpolants and restricted lower
  isometry of kernels.
\newblock \emph{arXiv preprint arXiv:1908.10292}, 2019.

\bibitem[Lu et~al.(2017)Lu, Pu, Wang, Hu, and Wang]{lu2017expressive}
Z.~Lu, H.~Pu, F.~Wang, Z.~Hu, and L.~Wang.
\newblock The expressive power of neural networks: A view from the width.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6231--6239, 2017.

\bibitem[Mei and Montanari(2019)]{mei2019generalization}
S.~Mei and A.~Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock \emph{arXiv preprint arXiv:1908.05355}, 2019.

\bibitem[Nguyen and Hein(2017)]{nguyen2017losscnn}
Q.~Nguyen and M.~Hein.
\newblock Optimization landscape and expressivity of deep {CNN}s.
\newblock \emph{arXiv preprint arXiv:1710.10928}, 2017.

\bibitem[Nilsson(1965)]{nilsson1965learning}
N.~J. Nilsson.
\newblock Learning machines.
\newblock 1965.

\bibitem[Rolnick and Tegmark(2018)]{rolnick2018power}
D.~Rolnick and M.~Tegmark.
\newblock The power of deeper networks for expressing natural functions.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Safran and Shamir(2017{\natexlab{a}})]{safran2017depth}
I.~Safran and O.~Shamir.
\newblock Depth-width tradeoffs in approximating natural functions with neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  2979--2987, 2017{\natexlab{a}}.

\bibitem[Safran and Shamir(2017{\natexlab{b}})]{safran2017spurious}
I.~Safran and O.~Shamir.
\newblock Spurious local minima are common in two-layer {ReLU} neural networks.
\newblock \emph{arXiv preprint arXiv:1712.08968}, 2017{\natexlab{b}}.

\bibitem[Shamir(2016)]{shamir2016without}
O.~Shamir.
\newblock Without-replacement sampling for stochastic gradient methods.
\newblock In \emph{Advances in neural information processing systems}, pages
  46--54, 2016.

\bibitem[Soltanolkotabi(2017)]{soltanolkotabi2017learning}
M.~Soltanolkotabi.
\newblock Learning {ReLUs} via gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2007--2017, 2017.

\bibitem[Sontag(1997)]{sontag1997shattering}
E.~D. Sontag.
\newblock Shattering all sets of `k' points in ``general position'' requires
  (k—1)/2 parameters.
\newblock \emph{Neural Computation}, 9\penalty0 (2):\penalty0 337--348, 1997.

\bibitem[Soudry and Carmon(2016)]{soudry2016no}
D.~Soudry and Y.~Carmon.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock \emph{arXiv preprint arXiv:1605.08361}, 2016.

\bibitem[Telgarsky(2015)]{telgarsky2015representation}
M.~Telgarsky.
\newblock Representation benefits of deep feedforward networks.
\newblock \emph{arXiv preprint arXiv:1509.08101}, 2015.

\bibitem[Telgarsky(2016)]{telgarsky2016benefits}
M.~Telgarsky.
\newblock Benefits of depth in neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 1517--1539, 2016.

\bibitem[Tian(2017)]{tian2017analytical}
Y.~Tian.
\newblock An analytical formula of population gradient for two-layered {ReLU}
  network and its applications in convergence and critical point analysis.
\newblock In \emph{International Conference on Machine Learning}, pages
  3404--3413, 2017.

\bibitem[Wang et~al.(2018)Wang, Giannakis, and Chen]{wang2018learning}
G.~Wang, G.~B. Giannakis, and J.~Chen.
\newblock Learning {ReLU} networks on linearly separable data: Algorithm,
  optimality, and generalization.
\newblock \emph{arXiv preprint arXiv:1808.04685}, 2018.

\bibitem[Yamasaki(1993)]{yamasaki1993lower}
M.~Yamasaki.
\newblock The lower bound of the capacity for a neural network with multiple
  hidden layers.
\newblock In \emph{ICANN'93}, pages 546--549. Springer, 1993.

\bibitem[Yarotsky(2017)]{yarotsky2017error}
D.~Yarotsky.
\newblock Error bounds for approximations with deep {ReLU} networks.
\newblock \emph{Neural Networks}, 94:\penalty0 103--114, 2017.

\bibitem[Yarotsky(2018)]{yarotsky2018optimal}
D.~Yarotsky.
\newblock Optimal approximation of continuous functions by very deep {ReLU}
  networks.
\newblock \emph{arXiv preprint arXiv:1802.03620}, 2018.

\bibitem[Yun et~al.(2018)Yun, Sra, and Jadbabaie]{yun2018global}
C.~Yun, S.~Sra, and A.~Jadbabaie.
\newblock Global optimality conditions for deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Yun et~al.(2019)Yun, Sra, and Jadbabaie]{yun2019small}
C.~Yun, S.~Sra, and A.~Jadbabaie.
\newblock Small nonlinearities in activation functions create bad local minima
  in neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Yu, Wang, and Gu]{zhang2018learning}
X.~Zhang, Y.~Yu, L.~Wang, and Q.~Gu.
\newblock Learning one-hidden-layer {ReLU} networks via gradient descent.
\newblock \emph{arXiv preprint arXiv:1806.07808}, 2018.

\bibitem[Zhong et~al.(2017)Zhong, Song, Jain, Bartlett, and
  Dhillon]{zhong2017recovery}
K.~Zhong, Z.~Song, P.~Jain, P.~L. Bartlett, and I.~S. Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  4140--4149, 2017.

\bibitem[Zhou and Liang(2018)]{zhou2018critical}
Y.~Zhou and Y.~Liang.
\newblock Critical points of neural networks: Analytical forms and landscape
  properties.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Zhou et~al.(2019)Zhou, Yang, Zhang, Liang, and Tarokh]{zhou2019sgd}
Y.~Zhou, J.~Yang, H.~Zhang, Y.~Liang, and V.~Tarokh.
\newblock {SGD} converges to global minimum in deep learning via star-convex
  path.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
D.~Zou, Y.~Cao, D.~Zhou, and Q.~Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep {ReLU}
  networks.
\newblock \emph{arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
