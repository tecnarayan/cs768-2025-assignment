\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baes(2009)]{baes2009estimate}
Michel Baes.
\newblock Estimate sequence methods: extensions and approximations.
\newblock \emph{Institute for Operations Research, ETH, Z{\"u}rich,
  Switzerland}, 2009.

\bibitem[Beck and Teboulle(2009)]{beck2009fast}
A.~Beck and M.~Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock \emph{SIAM Journal on Imaging Sciences}, 2\penalty0 (1):\penalty0
  183--202, 2009.

\bibitem[Collins et~al.(2008)Collins, {A. Globerson}, Koo, Carreras, and
  Bartlett]{CollinsGlKoCaBa08}
M.~Collins, {A. Globerson}, T.~Koo, X.~Carreras, and P.~Bartlett.
\newblock Exponentiated gradient algorithms for conditional random fields and
  max-margin markov networks.
\newblock \emph{Journal of Machine Learning Research}, 9:\penalty0 1775--1822,
  2008.

\bibitem[Cotter et~al.(2011)Cotter, Shamir, Srebro, and
  Sridharan]{cotter2011better}
Andrew Cotter, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Better mini-batch algorithms via accelerated gradient methods.
\newblock \emph{arXiv preprint arXiv:1106.4574}, 2011.

\bibitem[Crammer and Singer(2001)]{CrammerSi01a}
K.~Crammer and Y.~Singer.
\newblock On the algorithmic implementation of multiclass kernel-based vector
  machines.
\newblock \emph{Journal of Machine Learning Research}, 2:\penalty0 265--292,
  2001.

\bibitem[d'Aspremont(2008)]{d2008smooth}
Alexandre d'Aspremont.
\newblock Smooth optimization with approximate gradient.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (3):\penalty0
  1171--1183, 2008.

\bibitem[Devolder et~al.(2011)Devolder, Glineur, and Nesterov]{OlGlNe11}
Olivier Devolder, Francois Glineur, and Yu. Nesterov.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock Technical Report 2011/2, CORE, 2011.
\newblock available:
  http://www.uclouvain.be/cps/ucl/doc/core/documents/coredp2011\_2web.pdf.

\bibitem[Duchi and Singer(2009)]{duchi2009efficient}
J.~Duchi and Y.~Singer.
\newblock Efficient online and batch learning using forward backward splitting.
\newblock \emph{The Journal of Machine Learning Research}, 10:\penalty0
  2899--2934, 2009.

\bibitem[Duchi et~al.(2008)Duchi, Shalev-Shwartz, Singer, and
  Chandra]{duchi2008efficient}
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.
\newblock Efficient projections onto the l 1-ball for learning in high
  dimensions.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 272--279. ACM, 2008.

\bibitem[Duchi et~al.(2010)Duchi, Shalev-Shwartz, Singer, and
  Tewari]{DuchiShSiTe10}
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Ambuj Tewari.
\newblock Composite objective mirror descent.
\newblock In \emph{Proceedings of the 23rd Annual Conference on Learning
  Theory}, pages 14--26, 2010.

\bibitem[Ghadimi and Lan(2012)]{ghadimi2012optimal}
Saeed Ghadimi and Guanghui Lan.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization i: A generic algorithmic framework.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (4):\penalty0
  1469--1492, 2012.

\bibitem[Hu et~al.(2009)Hu, Pan, and Kwok]{hu2009accelerated}
Chonghai Hu, Weike Pan, and James~T Kwok.
\newblock Accelerated gradient methods for stochastic optimization and online
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  781--789, 2009.

\bibitem[Lacoste-Julien et~al.(2012)Lacoste-Julien, Jaggi, Schmidt, and
  Pletscher]{lacoste2012stochastic}
S.~Lacoste-Julien, M.~Jaggi, M.~Schmidt, and P.~Pletscher.
\newblock Stochastic block-coordinate frank-wolfe optimization for structural
  svms.
\newblock \emph{arXiv preprint arXiv:1207.4747}, 2012.

\bibitem[Langford et~al.(2009)Langford, Li, and Zhang]{LangfordLiZh09}
J.~Langford, L.~Li, and T.~Zhang.
\newblock Sparse online learning via truncated gradient.
\newblock In \emph{NIPS}, pages 905--912, 2009.

\bibitem[{Le Roux} et~al.(2012){Le Roux}, {Schmidt}, and {Bach}]{LSB12-sgdexp}
Nicolas {Le Roux}, Mark {Schmidt}, and Francis {Bach}.
\newblock {A Stochastic Gradient Method with an Exponential Convergence Rate
  for Strongly-Convex Optimization with Finite Training Sets}.
\newblock \emph{arXiv preprint arXiv:1202.6258}, 2012.

\bibitem[Nesterov(2012)]{Nesterov10}
Y.~Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  341--362, 2012.

\bibitem[Nesterov(2005)]{nesterov2005smooth}
Yurii Nesterov.
\newblock Smooth minimization of non-smooth functions.
\newblock \emph{Mathematical Programming}, 103\penalty0 (1):\penalty0 127--152,
  2005.

\bibitem[Nesterov(2007)]{nesterov2007gradient}
Yurii Nesterov.
\newblock Gradient methods for minimizing composite objective function, 2007.

\bibitem[Richt{\'a}rik and Tak{\'a}{\v{c}}(2012)]{richtarik2012iteration}
Peter Richt{\'a}rik and Martin Tak{\'a}{\v{c}}.
\newblock Iteration complexity of randomized block-coordinate descent methods
  for minimizing a composite function.
\newblock \emph{Mathematical Programming}, pages 1--38, 2012.

\bibitem[Schmidt et~al.(2011)Schmidt, Roux, and Bach]{ScRoBa11}
Mark Schmidt, Nicolas~Le Roux, and Francis Bach.
\newblock Convergence rates of inexact proximal-gradient methods for convex
  optimization.
\newblock Technical Report arXiv:1109.2415, arXiv, 2011.

\bibitem[Shalev-Shwartz and Tewari(2011)]{shalev2011stochastic}
S.~Shalev-Shwartz and A.~Tewari.
\newblock Stochastic methods for l 1-regularized loss minimization.
\newblock \emph{The Journal of Machine Learning Research}, 12:\penalty0
  1865--1892, 2011.

\bibitem[Shalev-Shwartz et~al.(2007)Shalev-Shwartz, Singer, and
  Srebro]{ShalevSiSr07}
S.~Shalev-Shwartz, Y.~Singer, and N.~Srebro.
\newblock Pegasos: {P}rimal {E}stimated sub-{G}r{A}dient {SO}lver for {SVM}.
\newblock In \emph{ICML}, pages 807--814, 2007.

\bibitem[Shalev-Shwartz and Tewari(2009)]{ShalevTe09}
Shai Shalev-Shwartz and Ambuj Tewari.
\newblock Stochastic methods for l$_1$ regularized loss minimization.
\newblock In \emph{ICML}, page 117, 2009.

\bibitem[Shalev-Shwartz and Zhang(2012)]{ShZh12-sdca}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock \emph{arXiv preprint arXiv:1209.1873}, 2012.

\bibitem[Shalev-Shwartz and Zhang(2013)]{ShalevZh2013}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock \emph{Journal of Machine Learning Research}, 14:\penalty0 567--599,
  Feb 2013.

\bibitem[Shalev-Shwartz et~al.(2010)Shalev-Shwartz, Srebro, and
  Zhang]{shalev2010trading}
Shai Shalev-Shwartz, Nathan Srebro, and Tong Zhang.
\newblock Trading accuracy for sparsity in optimization problems with sparsity
  constraints.
\newblock \emph{SIAM Journal on Optimization}, 20\penalty0 (6):\penalty0
  2807--2832, 2010.

\bibitem[Xiao(2010)]{Xiao10}
Lin Xiao.
\newblock Dual averaging method for regularized stochastic learning and online
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 11:\penalty0 2543--2596,
  2010.

\bibitem[Zhang(2002)]{Zhang02-dual}
Tong Zhang.
\newblock On the dual formulation of regularized linear systems.
\newblock \emph{Machine Learning}, 46:\penalty0 91--129, 2002.

\end{thebibliography}
