\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2019)Achiam, Knight, and Abbeel]{achiam19towards}
Joshua Achiam, Ethan Knight, and Pieter Abbeel.
\newblock Towards characterizing divergence in deep {Q}-learning.
\newblock \emph{arXiv preprint arXiv:1903.08894}, 2019.

\bibitem[Agarwal et~al.(2020)Agarwal, Schuurmans, and
  Norouzi]{agarwal20optimistic}
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, volume 119, pages 104--114. {PMLR}, 2020.

\bibitem[Baird(1995)]{baird95residual}
Leemon Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{Machine Learning Proceedings 1995}, pages 30--37. Elsevier,
  1995.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare13arcade}
Marc~G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock {The Arcade Learning Environment: an evaluation platform for general
  agents}.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bengio et~al.(2020)Bengio, Pineau, and Precup]{bengio2020interference}
Emmanuel Bengio, Joelle Pineau, and Doina Precup.
\newblock Interference and generalization in temporal difference learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  767--777, 2020.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock {OpenAI Gym}.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Buckman et~al.(2021)Buckman, Gelada, and
  Bellemare]{buckman21pessimism}
Jacob Buckman, Carles Gelada, and Marc~G. Bellemare.
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Budden et~al.(2020)Budden, Hessel, Quan, Kapturowski, Baumli,
  Bhupatiraju, Guy, and King]{rlax2020github}
David Budden, Matteo Hessel, John Quan, Steven Kapturowski, Kate Baumli, Surya
  Bhupatiraju, Aurelia Guy, and Michael King.
\newblock {RL}ax: {R}einforcement {L}earning in {JAX}, 2020.
\newblock URL \url{http://github.com/deepmind/rlax}.

\bibitem[Castro et~al.(2018)Castro, Moitra, Gelada, Kumar, and
  Bellemare]{castro18dopamine}
Pablo~Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and
  Marc~G. Bellemare.
\newblock Dopamine: {A} research framework for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1812.06110}, 2018.

\bibitem[Dabney et~al.(2018)Dabney, Rowland, Bellemare, and
  Munos]{dabney2018distributional}
Will Dabney, Mark Rowland, Marc~G. Bellemare, and R{\'e}mi Munos.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Fakoor et~al.(2021)Fakoor, Mueller, Chaudhari, and
  Smola]{fakoor2021continuous}
Rasool Fakoor, Jonas Mueller, Pratik Chaudhari, and Alexander~J. Smola.
\newblock Continuous doubly constrained batch reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2102.09225}, 2021.

\bibitem[Fujimoto et~al.(2019{\natexlab{a}})Fujimoto, Conti, Ghavamzadeh, and
  Pineau]{fujimoto19benchmarking}
Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau.
\newblock Benchmarking batch deep reinforcement learning algorithms.
\newblock \emph{arXiv preprint arXiv:1910.01708}, 2019{\natexlab{a}}.

\bibitem[Fujimoto et~al.(2019{\natexlab{b}})Fujimoto, Meger, and
  Precup]{fujimoto19offPolicy}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97, pages 2052--2062. PMLR, 2019{\natexlab{b}}.

\bibitem[Held and Hein(1963)]{held63movement}
Richard Held and Alan Hein.
\newblock Movement-produced stimulation in the development of visually guided
  behavior.
\newblock \emph{Journal of Comparative and Physiological Psychology},
  56\penalty0 (5):\penalty0 872--876, 1963.

\bibitem[Hennigan et~al.(2020)Hennigan, Cai, Norman, and
  Babuschkin]{haiku2020github}
Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin.
\newblock {H}aiku: {S}onnet for {JAX}, 2020.
\newblock URL \url{http://github.com/deepmind/dm-haiku}.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, van Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{hessel2018rainbow}
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: combining improvements in deep reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2018.

\bibitem[Hessel et~al.(2020)Hessel, Budden, Viola, Rosca, Sezener, and
  Hennigan]{optax2020github}
Matteo Hessel, David Budden, Fabio Viola, Mihaela Rosca, Eren Sezener, and Tom
  Hennigan.
\newblock Optax: Composable gradient transformation and optimisation, in
  {JAX!}, 2020.
\newblock URL \url{http://github.com/deepmind/optax}.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{distillation}
Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean.
\newblock Distilling the knowledge in a neural network.
\newblock In \emph{NIPS Deep Learning and Representation Learning Workshop},
  2015.

\bibitem[Huh et~al.(2021)Huh, Mobahi, Zhang, Cheung, Agrawal, and
  Isola]{huh21low}
Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and
  Phillip Isola.
\newblock The low-rank simplicity bias in deep networks.
\newblock \emph{arXiv preprint arXiv:2103.10427}, 2021.

\bibitem[Jacq et~al.(2019)Jacq, Geist, Paiva, and Pietquin]{jacq19learning}
Alexis Jacq, Matthieu Geist, Ana Paiva, and Olivier Pietquin.
\newblock Learning from a learner.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97, pages 2990--2999, 2019.

\bibitem[Kapturowski et~al.(2019)Kapturowski, Ostrovski, Quan, Munos, and
  Dabney]{kapturowski2018recurrent}
Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney.
\newblock Recurrent experience replay in distributed reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi20}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock {MOReL}: Model-based offline reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 21810--21823, 2020.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations}, 2015.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and Levine]{kumar2019}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy {Q-learning} via bootstrapping error
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Kumar et~al.(2020{\natexlab{a}})Kumar, Agarwal, Ghosh, and
  Levine]{kumar2020implicit}
Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, and Sergey Levine.
\newblock Implicit under-parameterization inhibits data-efficient deep
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.14498}, 2020{\natexlab{a}}.

\bibitem[Kumar et~al.(2020{\natexlab{b}})Kumar, Gupta, and
  Levine]{kumar20discor}
Aviral Kumar, Abhishek Gupta, and Sergey Levine.
\newblock {DisCor}: Corrective feedback in reinforcement learning via
  distribution correction.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 18560--18572, 2020{\natexlab{b}}.

\bibitem[Kumar et~al.(2020{\natexlab{c}})Kumar, Zhou, Tucker, and
  Levine]{kumar2020}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative {Q-learning} for offline reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 1179--1191, 2020{\natexlab{c}}.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine20offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Liu et~al.(2020)Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Provably good batch reinforcement learning without great exploration.
\newblock \emph{arXiv preprint arXiv:2007.08202}, 2020.

\bibitem[Machado et~al.(2018)Machado, Bellemare, Talvitie, Veness, Hausknecht,
  and Bowling]{machado2018revisiting}
Marlos~C Machado, Marc~G. Bellemare, Erik Talvitie, Joel Veness, Matthew
  Hausknecht, and Michael Bowling.
\newblock Revisiting the {Arcade Learning Environment}: Evaluation protocols
  and open problems for general agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 61:\penalty0
  523--562, 2018.

\bibitem[Matsushima et~al.(2020)Matsushima, Furuta, Matsuo, Nachum, and
  Gu]{matsushima2020deployment}
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu.
\newblock Deployment-efficient reinforcement learning via model-based offline
  optimization.
\newblock \emph{arXiv preprint arXiv:2006.03647}, 2020.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{dqn}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A. Rusu, Joel Veness,
  Marc~G. Bellemare, Alex Graves, Martin Riedmiller, Andreas~K. Fidjeland,
  Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis
  Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and
  Demis Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Nair et~al.(2020)Nair, Dalal, Gupta, and Levine]{nair2020accelerating}
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock \emph{arXiv preprint arXiv:2006.09359}, 2020.

\bibitem[Obando-Ceron and Castro(2021)]{obando2020revisiting}
Johan~S. Obando-Ceron and Pablo~Samuel Castro.
\newblock Revisiting {Rainbow}: Promoting more insightful and inclusive deep
  reinforcement learning research.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}. PMLR, 2021.

\bibitem[Quan and Ostrovski(2020)]{dqnzoo2020github}
John Quan and Georg Ostrovski.
\newblock {DQN} {Zoo}: Reference implementations of {DQN}-based agents, 2020.
\newblock URL \url{http://github.com/deepmind/dqn_zoo}.

\bibitem[Rummery and Niranjan(1994)]{rummery1994line}
Gavin~A. Rummery and Mahesan Niranjan.
\newblock On-line {Q-learning} using connectionist systems.
\newblock Technical report, University of Cambridge, Department of Engineering
  Cambridge, UK, 1994.

\bibitem[Schrittwieser et~al.(2021)Schrittwieser, Hubert, Mandhane, Barekatain,
  Antonoglou, and Silver]{schrittwieser2021online}
Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain,
  Ioannis Antonoglou, and David Silver.
\newblock Online and offline reinforcement learning by planning with a learned
  model.
\newblock \emph{arXiv preprint arXiv:2104.06294}, 2021.

\bibitem[Sutton(1988)]{sutton1988learning}
Richard~S. Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S. Sutton and Andrew~G. Barto.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT press, 2018.

\bibitem[Thomas et~al.(2015)Thomas, Theocharous, and
  Ghavamzadeh]{thomas2015high}
Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh.
\newblock High confidence policy improvement.
\newblock In \emph{International Conference on Machine Learning}, pages
  2380--2388. PMLR, 2015.

\bibitem[Tieleman and Hinton(2012)]{tieleman2012lecture}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Lecture 6.5: {RMSProp}.
\newblock \emph{COURSERA: Neural Networks for Machine Learning}, 2012.

\bibitem[Tsitsiklis and Van~Roy(1996)]{tsitsiklis96analysis}
John~N. Tsitsiklis and Benjamin Van~Roy.
\newblock Analysis of temporal-difference learning with function approximation.
\newblock In \emph{Proceedings of the 9th International Conference on Neural
  Information Processing Systems}, page 1075–1081, Cambridge, MA, USA, 1996.
  MIT Press.

\bibitem[van Hasselt(2010)]{hasselt2010}
Hado van Hasselt.
\newblock {Double Q-learning}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~23, 2010.

\bibitem[van Hasselt et~al.(2016)van Hasselt, Guez, and
  Silver]{vanhasselt16deep}
Hado van Hasselt, Arthur Guez, and David Silver.
\newblock {Deep reinforcement learning with double Q-learning}.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2016.

\bibitem[van Hasselt et~al.(2018)van Hasselt, Doron, Strub, Hessel, Sonnerat,
  and Modayil]{van2018deep}
Hado van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat,
  and Joseph Modayil.
\newblock Deep reinforcement learning and the deadly triad.
\newblock \emph{arXiv preprint arXiv:1812.02648}, 2018.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Foster, and
  Kakade]{wang2020statistical}
Ruosong Wang, Dean~P. Foster, and Sham~M. Kakade.
\newblock What are the statistical limits of offline {RL} with linear function
  approximation?
\newblock \emph{arXiv preprint arXiv:2010.11895}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2021)Wang, Wu, Salakhutdinov, and
  Kakade]{wang2021instabilities}
Ruosong Wang, Yifan Wu, Ruslan Salakhutdinov, and Sham~M. Kakade.
\newblock Instabilities of offline {RL} with pre-trained neural representation.
\newblock \emph{arXiv preprint arXiv:2103.04947}, 2021.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Novikov, {\.Z}o{\l}na,
  Springenberg, Reed, Shahriari, Siegel, Merel, Gulcehre, Heess,
  et~al.]{wang2020critic}
Ziyu Wang, Alexander Novikov, Konrad {\.Z}o{\l}na, Jost~Tobias Springenberg,
  Scott Reed, Bobak Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre,
  Nicolas Heess, et~al.
\newblock Critic regularized regression.
\newblock \emph{arXiv preprint arXiv:2006.15134}, 2020{\natexlab{b}}.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Wu et~al.(2021)Wu, Zhou, Srivastava, Susskind, Zhang, Salakhutdinov,
  and Goh]{wu21uncertainty}
Yue Wu, Shuangfei Zhou, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan
  Salakhutdinov, and Hanlin Goh.
\newblock Uncertainty weighted actor-critic for offline reinforcement learning.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}. PMLR, 2021.

\bibitem[Xiao et~al.(2021)Xiao, Lee, Dai, Schuurmans, and
  Szepesvari]{xiao2021sample}
Chenjun Xiao, Ilbin Lee, Bo~Dai, Dale Schuurmans, and Csaba Szepesvari.
\newblock On the sample complexity of batch reinforcement learning with
  policy-induced data.
\newblock \emph{arXiv preprint arXiv:2106.09973}, 2021.

\bibitem[{Young} and {Tian}(2019)]{young19minatar}
Kenny {Young} and Tian {Tian}.
\newblock {MinAtar}: An {Atari}-inspired testbed for thorough and reproducible
  reinforcement learning experiments.
\newblock \emph{arXiv preprint arXiv:1903.03176}, 2019.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine,
  Chelsea Finn, and Tengyu Ma.
\newblock {MOPO}: Model-based offline policy optimization.
\newblock \emph{arXiv preprint arXiv:2005.13239}, 2020.

\end{thebibliography}
