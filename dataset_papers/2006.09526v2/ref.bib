@article{hu2020xtreme,
  title={XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization},
  author={Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
  journal={arXiv preprint arXiv:2003.11080},
  year={2020}
}
@article{artetxe2019massively,
  title={Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond},
  author={Artetxe, Mikel and Schwenk, Holger},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={597--610},
  year={2019},
  publisher={MIT Press}
}
@article{khandelwal2019generalization,
  title={Generalization through Memorization: Nearest Neighbor Language Models},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:1911.00172},
  year={2019}
}
@article{guu2020realm,
  title={Realm: Retrieval-augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
  journal={arXiv preprint arXiv:2002.08909},
  year={2020}
}
@article{wang2019cross,
  title={Cross-Lingual Ability of Multilingual BERTG: An Empirical Study},
  author={Wang, Zihan and Mayhew, Stephen and Roth, Dan and others},
  journal={arXiv preprint arXiv:1912.07840},
  year={2019}
}
@article{lample2017unsupervised,
  title={Unsupervised machine translation using monolingual corpora only},
  author={Lample, Guillaume and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  journal={arXiv preprint arXiv:1711.00043},
  year={2017}
}
@inproceedings{lample2018phrase,
  title={Phrase-Based \& Neural Unsupervised Machine Translation},
  author={Lample, Guillaume and Ott, Myle and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc’Aurelio},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={5039--5049},
  year={2018}
}
@inproceedings{artetxe2018unsupervised,
  title={Unsupervised neural machine translation},
  author={Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko and Cho, Kyunghyun},
  booktitle={6th International Conference on Learning Representations, ICLR 2018},
  year={2018}
}
@inproceedings{ren2019explicit,
  title={Explicit Cross-lingual Pre-training for Unsupervised Machine Translation},
  author={Ren, Shuo and Wu, Yu and Liu, Shujie and Zhou, Ming and Ma, Shuai},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={770--779},
  year={2019}
}
@inproceedings{qi2018and,
  title={When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?},
  author={Qi, Ye and Sachan, Devendra and Felix, Matthieu and Padmanabhan, Sarguna and Neubig, Graham},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
  pages={529--535},
  year={2018}
}
@inproceedings{song2019mass,
  title={MASS: Masked Sequence to Sequence Pre-training for Language Generation},
  author={Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
  booktitle={International Conference on Machine Learning},
  pages={5926--5936},
  year={2019}
}
@article{liu2020multilingual,
  title={Multilingual denoising pre-training for neural machine translation},
  author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2001.08210},
  year={2020}
}
@inproceedings{conneau2019cross,
  title={Cross-lingual Language Model Pretraining},
  author={Conneau, Alexis and Lample, Guillaume},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7057--7067},
  year={2019}
}

@inproceedings{wu2019extract,
  title={Extract and Edit: An Alternative to Back-Translation for Unsupervised Neural Machine Translation},
  author={Wu, Jiawei and Wang, Xin and Wang, William Yang},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={1173--1183},
  year={2019}
}
@article{wu2019emerging,
  title={Emerging Cross-lingual Structure in Pretrained Language Models},
  author={Wu, Shijie and Conneau, Alexis and Li, Haoran and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.01464},
  year={2019}
}
@inproceedings{artetxe2019margin,
  title={Margin-based Parallel Corpus Mining with Multilingual Sentence Embeddings},
  author={Artetxe, Mikel and Schwenk, Holger},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3197--3203},
  year={2019}
}
@inproceedings{li2019data,
  title={Data-dependent Gaussian Prior Objective for Language Generation},
  author={Li, Zuchao and Wang, Rui and Chen, Kehai and Utiyama, Masso and Sumita, Eiichiro and Zhang, Zhuosheng and Zhao, Hai},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{garcia2020multilingual,
  title={A Multilingual View of Unsupervised Machine Translation},
  author={Garcia, Xavier and Foret, Pierre and Sellam, Thibault and Parikh, Ankur P},
  journal={arXiv preprint arXiv:2002.02955},
  year={2020}
}
@article{schwenk2019ccmatrix,
  title={CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB},
  author={Schwenk, Holger and Wenzek, Guillaume and Edunov, Sergey and Grave, Edouard and Joulin, Armand},
  journal={arXiv preprint arXiv:1911.04944},
  year={2019}
}
@article{wenzek2019ccnet,
  title={Ccnet: Extracting high quality monolingual datasets from web crawl data},
  author={Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzman, Francisco and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:1911.00359},
  year={2019}
}
@article{johnson2019billion,
  title={Billion-scale similarity search with GPUs},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  year={2019},
  publisher={IEEE}
}
@inproceedings{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting on association for computational linguistics},
  pages={311--318},
  year={2002},
  organization={Association for Computational Linguistics}
}
@article{conneau2019unsupervised,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}
@inproceedings{kudo2018sentencepiece,
  title={SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing},
  author={Kudo, Taku and Richardson, John},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={66--71},
  year={2018}
}
@inproceedings{cettolo2017overview,
  title={Overview of the {IWSLT} 2017 evaluation campaign},
  author={Cettolo, Mauro and Federico, Marcello and Bentivogli, Luisa and Jan, Niehues and Sebastian, St{\"u}ker and Katsuitho, Sudoh and Koichiro, Yoshino and Christian, Federmann},
  booktitle={International Workshop on Spoken Language Translation},
  pages={2--14},
  year={2017}
}
@inproceedings{yang2019xlnet,
  title={{XLNET}: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={5754--5764},
  year={2019}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@inproceedings{devlin2018bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
@inproceedings{peters2018deep,
  title={Deep contextualized word representations},
  author={Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  booktitle={Proceedings of NAACL-HLT},
  pages={2227--2237},
  year={2018}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  year={2017}
}

@inproceedings{mikolov2013distributed,
  title={Distributed Representations of Words and Phrases and their Compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle=nips,
  pages={3111--3119},
  year={2013}
}

@inproceedings{mikolov2013efficient,
author = {Mikolov, Tomas and Corrado, G.s and Chen, Kai and Dean, Jeffrey},
year = {2013},
month = {01},
pages = {1-12},
title = {Efficient Estimation of Word Representations in Vector Space}
}

@techreport{radford2018gpt,
  title={Improving language understanding with unsupervised learning},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Time and Sutskever, Ilya},
  year={2018},
  institution={OpenAI}
}

@techreport{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  institution={OpenAI}
}

@article{dong2019unified,
  title={Unified Language Model Pre-training for Natural Language Understanding and Generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={arXiv preprint arXiv:1905.03197},
  year={2019}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{DBLP:wada,
  author    = {Takashi Wada and
               Tomoharu Iwata},
  title     = {Unsupervised Cross-lingual Word Embedding by Multilingual Neural Language
               Models},
  journal   = {CoRR},
  volume    = {abs/1809.02306},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.02306},
  archivePrefix = {arXiv},
  eprint    = {1809.02306},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
}


@article{lample2019cross,
  title={Cross-lingual Language Model Pretraining},
  author={Lample, Guillaume and Conneau, Alexis},
  journal={arXiv preprint arXiv:1901.07291},
  year={2019}
}

@inproceedings{xu2017zipporah,
  title={Zipporah: a Fast and Scalable Data Cleaning System for Noisy Web-Crawled Parallel Corpora},
  author={Xu, Hainan and Koehn, Philipp},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  address   = {Denmark, Cophenhagen},
  publisher = {Association for Computational Linguistics},
  pages={2945--2950},
  year={2017},
  url={https://www.aclweb.org/anthology/D17-1319}
}


@InProceedings{khayrallah-xu-koehn:2018:WMT,
  author    = {Khayrallah, Huda  and  Xu, Hainan  and  Koehn, Philipp},
  title     = {The {JHU} Parallel Corpus Filtering Systems for {WMT} 2018},
  booktitle = {Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Papers},
  month     = {October},
  year      = {2018},
  address   = {Belgium, Brussels},
  publisher = {Association for Computational Linguistics},
  pages     = {909--912},
  url       = {http://www.aclweb.org/anthology/W18-6480}
}

@inproceedings{sennrich-etal-2016-improving,
    title = "Improving Neural Machine Translation Models with Monolingual Data",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1009",
    doi = "10.18653/v1/P16-1009",
    pages = "86--96",
}
@inproceedings{ott2019fairseq,
  title={fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)},
  pages={48--53},
  year={2019}
}
@inproceedings{pires2019multilingual,
  title={How Multilingual is Multilingual BERT?},
  author={Pires, Telmo and Schlinger, Eva and Garrette, Dan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4996--5001},
  year={2019}
}
@inproceedings{guzman2019flores,
  title={The FLORES Evaluation Datasets for Low-Resource Machine Translation: Nepali--English and Sinhala--English},
  author={Guzm{\'a}n, Francisco and Chen, Peng-Jen and Ott, Myle and Pino, Juan and Lample, Guillaume and Koehn, Philipp and Chaudhary, Vishrav and Ranzato, Marc’Aurelio},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={6100--6113},
  year={2019}
}
@inproceedings{kunchukuttan2018iit,
  title={The IIT Bombay English-Hindi Parallel Corpus},
  author={Kunchukuttan, Anoop and Mehta, Pratik and Bhattacharyya, Pushpak},
  booktitle={Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}
@inproceedings{bojar2016findings,
  title={Findings of the 2016 conference on machine translation},
  author={Bojar, Ond{\v{r}}ej and Chatterjee, Rajen and Federmann, Christian and Graham, Yvette and Haddow, Barry and Huck, Matthias and Yepes, Antonio Jimeno and Koehn, Philipp and Logacheva, Varvara and Monz, Christof and others},
  booktitle={Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers},
  pages={131--198},
  year={2016}
}

@inproceedings{Chaudhary2019LowResourceCF,
  title={Low-Resource Corpus Filtering Using Multilingual Sentence Embeddings},
  author={Chaudhary, Vishrav and Tang, Yuqing and Guzm{\'a}n, Francisco and Schwenk, Holger and Koehn, Philipp},
  booktitle={Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)},
  pages={261--266},
  year={2019}
}
@inproceedings{aldarmaki2019context,
  title={Context-Aware Cross-Lingual Mapping},
  author={Aldarmaki, Hanan and Diab, Mona},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={3906--3911},
  year={2019}
}
@inproceedings{liu2012thutr,
  title={THUTR: A translation retrieval system},
  author={Liu, Chunyang and Liu, Qi and Liu, Yang and Sun, Maosong},
  booktitle={Proceedings of COLING 2012: Demonstration Papers},
  pages={321--328},
  year={2012}
}
@inproceedings{artetxe2019bilingual,
  title={Bilingual Lexicon Induction through Unsupervised Machine Translation},
  author={Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5002--5007},
  year={2019}
}
@inproceedings{hangya2019unsupervised,
  title={Unsupervised parallel sentence extraction with parallel segment detection helps machine translation},
  author={Hangya, Viktor and Fraser, Alexander},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={1224--1234},
  year={2019}
}
@inproceedings{guo2018effective,
  title={Effective Parallel Corpus Mining using Bilingual Sentence Embeddings},
  author={Guo, Mandy and Shen, Qinlan and Yang, Yinfei and Ge, Heming and Cer, Daniel and Abrego, Gustavo Hernandez and Stevens, Keith and Constant, Noah and Sung, Yun-Hsuan and Strope, Brian and others},
  booktitle={Proceedings of the Third Conference on Machine Translation: Research Papers},
  pages={165--176},
  year={2018}
}
@article{mikolov2013exploiting,
  title={Exploiting similarities among languages for machine translation},
  author={Mikolov, Tomas and Le, Quoc V and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1309.4168},
  year={2013}
}
@inproceedings{lample2018word,
  title={Word translation without parallel data},
  author={Lample, Guillaume and Conneau, Alexis and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@inproceedings{artetxe-etal-2018-robust,
    title = "A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings",
    author = "Artetxe, Mikel  and
      Labaka, Gorka  and
      Agirre, Eneko",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1073",
    doi = "10.18653/v1/P18-1073",
    pages = "789--798",
    abstract = "Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at \url{https://github.com/artetxem/vecmap}.",
}
@inproceedings{artetxe2016learning,
  title={Learning principled bilingual mappings of word embeddings while preserving monolingual invariance},
  author={Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={2289--2294},
  year={2016}
}
@inproceedings{uszkoreit2010large,
  title={Large scale parallel document mining for machine translation},
  author={Uszkoreit, Jakob and Ponte, Jay and Popat, Ashok and Dubiner, Moshe},
  booktitle={Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)},
  pages={1101--1109},
  year={2010}
}
@inproceedings{schwenk2018filtering,
  title={Filtering and Mining Parallel Data in a Joint Multilingual Space},
  author={Schwenk, Holger},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={228--234},
  year={2018}
}

@inproceedings{ondrej2017findings,
  title={Findings of the 2017 conference on machine translation (wmt17)},
  author={Ondrej, Bojar and Chatterjee, Rajen and Christian, Federmann and Yvette, Graham and Barry, Haddow and Matthias, Huck and Philipp, Koehn and Qun, Liu and Varvara, Logacheva and Christof, Monz and others},
  booktitle={Second Conference onMachine Translation},
  pages={169--214},
  year={2017},
  organization={The Association for Computational Linguistics}
}

@inproceedings{barrault2019findings,
  title={Findings of the 2019 conference on machine translation (wmt19)},
  author={Barrault, Lo{\"\i}c and Bojar, Ond{\v{r}}ej and Costa-juss{\`a}, Marta R and Federmann, Christian and Fishel, Mark and Graham, Yvette and Haddow, Barry and Huck, Matthias and Koehn, Philipp and Malmasi, Shervin and others},
  booktitle={Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)},
  pages={1--61},
  year={2019}
}
@inproceedings{cettolo2016iwslt,
  title={The IWSLT 2016 evaluation campaign},
  author={Cettolo, Mauro and Jan, Niehues and Sebastian, St{\"u}ker and Bentivogli, Luisa and Cattoni, Roldano and Federico, Marcello},
  booktitle={International Workshop on Spoken Language Translation},
  year={2016}
}

@inproceedings{bojar-etal-2018-findings,
    title = "Findings of the 2018 Conference on Machine Translation ({WMT}18)",
    author = "Bojar, Ond{\v{r}}ej  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Koehn, Philipp  and
      Monz, Christof",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6401",
    doi = "10.18653/v1/W18-6401",
    pages = "272--303",
    abstract = "This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2018. Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. This year, we also opened up the task to additional test sets to probe specific aspects of translation.",
}
@inproceedings{bojar-etal-2013-findings,
    title = "Findings of the 2013 {W}orkshop on {S}tatistical {M}achine {T}ranslation",
    author = "Bojar, Ond{\v{r}}ej  and
      Buck, Christian  and
      Callison-Burch, Chris  and
      Federmann, Christian  and
      Haddow, Barry  and
      Koehn, Philipp  and
      Monz, Christof  and
      Post, Matt  and
      Soricut, Radu  and
      Specia, Lucia",
    booktitle = "Proceedings of the Eighth Workshop on Statistical Machine Translation",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W13-2201",
    pages = "1--44",
}
@inproceedings{nakazawa-etal-2019-overview,
    title = "Overview of the 6th Workshop on {A}sian Translation",
    author = "Nakazawa, Toshiaki  and
      Doi, Nobushige  and
      Higashiyama, Shohei  and
      Ding, Chenchen  and
      Dabre, Raj  and
      Mino, Hideya  and
      Goto, Isao  and
      Pa, Win Pa  and
      Kunchukuttan, Anoop  and
      Parida, Shantipriya  and
      Bojar, Ond{\v{r}}ej  and
      Kurohashi, Sadao",
    booktitle = "Proceedings of the 6th Workshop on Asian Translation",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-5201",
    doi = "10.18653/v1/D19-5201",
    pages = "1--35",
    abstract = "This paper presents the results of the shared tasks from the 6th workshop on Asian translation (WAT2019) including Jaâ†”En, Jaâ†”Zh scientific paper translation subtasks, Jaâ†”En, Jaâ†”Ko, Jaâ†”En patent translation subtasks, Hiâ†”En, Myâ†”En, Kmâ†”En, Taâ†”En mixed domain subtasks and Ruâ†”Ja news commentary translation task. For the WAT2019, 25 teams participated in the shared tasks. We also received 10 research paper submissions out of which 61 were accepted. About 400 translation results were submitted to the automatic evaluation server, and selected submis- sions were manually evaluated.",
}
@inproceedings{Cettolo2015TheI2,
  title={The IWSLT 2015 Evaluation Campaign},
  author={Mauro Cettolo and Jan Niehues and Sebastian St{\"u}ker and Luisa Bentivogli and R. Cattoni and Marcello Federico},
  year={2015}
}
