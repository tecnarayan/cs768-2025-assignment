\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achlioptas(2003)]{Achlioptas2003}
Achlioptas, Dimitris.
\newblock Database-friendly random projections: Johnson-lindenstrauss with
  binary coins.
\newblock \emph{Journal of Computer and System Sciences}, 66\penalty0
  (4):\penalty0 671 -- 687, 2003.

\bibitem[Allen-Zhu et~al.(2016)Allen-Zhu, Qu, Richtarik, and
  Yuan]{AllenYhu:2016}
Allen-Zhu, Z, Qu, Z, Richtarik, P, and Yuan, Y.
\newblock Even faster accelerated coordinate descent using non-uniform
  sampling.
\newblock 2016.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{Boyd:2004uz}
Boyd, Stephen~P and Vandenberghe, Lieven.
\newblock \emph{{Convex optimization}}.
\newblock Cambridge University Press, 2004.

\bibitem[Csiba et~al.(2015)Csiba, Qu, and Richt{\'a}rik]{Csiba:2015ue}
Csiba, Dominik, Qu, Zheng, and Richt{\'a}rik, Peter.
\newblock {Stochastic Dual Coordinate Ascent with Adaptive Probabilities}.
\newblock In \emph{ICML 2015 - Proceedings of the 32th International Conference
  on Machine Learning}, 2015.

\bibitem[Dawkins(1991)]{Dawkins:1991}
Dawkins, Brian.
\newblock Siobhan's problem: The coupon collector revisited.
\newblock \emph{The American Statistician}, 45\penalty0 (1):\penalty0 76--82,
  1991.

\bibitem[Dhillon et~al.(2011)Dhillon, Ravikumar, and Tewari]{Dhillon:2011uh}
Dhillon, Inderjit~S, Ravikumar, Pradeep, and Tewari, Ambuj.
\newblock {Nearest Neighbor based Greedy Coordinate Descent}.
\newblock In \emph{NIPS 2014 - Advances in Neural Information Processing Systems 27}, 2011.

\bibitem[Friedman et~al.(2007)Friedman, Hastie, H{\"o}fling, and
  Tibshirani]{Friedman:2007ut}
Friedman, Jerome, Hastie, Trevor, H{\"o}fling, Holger, and Tibshirani, Robert.
\newblock {Pathwise coordinate optimization}.
\newblock \emph{The Annals of Applied Statistics}, 1\penalty0 (2):\penalty0
  302--332, 2007.

\bibitem[Friedman et~al.(2010)Friedman, Hastie, and
  Tibshirani]{Friedman:2010wm}
Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert.
\newblock {Regularization Paths for Generalized Linear Models via Coordinate
  Descent}.
\newblock \emph{Journal of Statistical Software}, 33\penalty0 (1):\penalty0
  1--22, 2010.

\bibitem[Fu(1998)]{Fu:1998cd}
Fu, Wenjiang~J.
\newblock Penalized regressions: The bridge versus the lasso.
\newblock \emph{Journal of Computational and Graphical Statistics}, 7\penalty0
  (3):\penalty0 397--416, 1998.

\bibitem[Hsieh et~al.(2008)Hsieh, Chang, Lin, Keerthi, and
  Sundararajan]{Hsieh:2008bd}
Hsieh, Cho-Jui, Chang, Kai-Wei, Lin, Chih-Jen, Keerthi, S~Sathiya, and
  Sundararajan, S.
\newblock {A Dual Coordinate Descent Method for Large-scale Linear SVM}.
\newblock In \emph{the 25th International Conference on Machine Learning}, pp.\
   408--415, New York, USA, 2008.

\bibitem[Kim \& Park(2008)Kim and Park]{Kim:2008}
Kim, Hyunsoo and Park, Haesun.
\newblock Nonnegative matrix factorization based on alternating nonnegativity
  constrained least squares and active set method.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 30\penalty0
  (2):\penalty0 713--730, 2008.

\bibitem[Lee \& Seung(1999)Lee and Seung]{nnmf}
Lee, Daniel~D and Seung, H~Sebastian.
\newblock {Learning the parts of objects by non-negative matrix factorization}.
\newblock \emph{Nature}, 401\penalty0 (6755):\penalty0 788--791, 1999.

\bibitem[Lewis et~al.(2004)Lewis, Yang, Rose, and Li]{Lewis:2004:RNB}
Lewis, David~D., Yang, Yiming, Rose, Tony~G., and Li, Fan.
\newblock Rcv1: A new benchmark collection for text categorization research.
\newblock \emph{J. Mach. Learn. Res.}, 5:\penalty0 361--397, 2004.

\bibitem[Matou\v{s}ek(2008)]{Matousek:2008}
Matou\v{s}ek, Ji\v{r}\'{i}.
\newblock On variants of the johnson–lindenstrauss lemma.
\newblock \emph{Random Structures \& Algorithms}, 33\penalty0 (2):\penalty0
  142--156, 2008.

\bibitem[Nesterov(2012)]{nesterov2012}
Nesterov, Yu.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  341--362, 2012.

\bibitem[Nesterov \& Stich(2017)Nesterov and Stich]{Nesterov:2017}
Nesterov, Yurii and Stich, Sebastian~U.
\newblock Efficiency of the accelerated coordinate descent method on structured
  optimization problems.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (1):\penalty0
  110--123, 2017.

\bibitem[Nutini et~al.(2015)Nutini, Schmidt, Laradji, Friedlander, and
  Koepke]{nutini2015coordinate}
Nutini, Julie, Schmidt, Mark~W, Laradji, Issam~H, Friedlander, Michael~P, and
  Koepke, Hoyt~A.
\newblock {Coordinate Descent Converges Faster with the Gauss-Southwell Rule
  Than Random Selection}.
\newblock In \emph{ICML}, pp.\  1632--1641, 2015.

\bibitem[Osokin et~al.(2016)Osokin, Alayrac, Lukasewitz, Dokania, and
  Lacoste-Julien]{Osokin:2016:MGB}
Osokin, Anton, Alayrac, Jean-Baptiste, Lukasewitz, Isabella, Dokania,
  Puneet~K., and Lacoste-Julien, Simon.
\newblock Minding the gaps for block frank-wolfe optimization of structured
  svms.
\newblock In \emph{Proceedings of the 33rd International Conference on
  International Conference on Machine Learning - Volume 48}, ICML'16, pp.\
  593--602. PMLR, 2016.

\bibitem[Papa et~al.(2015)Papa, Bianchi, and Cl{\'e}men{\c c}on]{Papa2015}
Papa, Guillaume, Bianchi, Pascal, and Cl{\'e}men{\c c}on, St{\'e}phan.
\newblock {Adaptive Sampling for Incremental Optimization Using Stochastic
  Gradient Descent}.
\newblock \emph{ALT 2015 - 26th International Conference on Algorithmic
  Learning Theory}, pp.\  317--331, 2015.

\bibitem[Perekrestenko et~al.(2017)Perekrestenko, Cevher, and
  Jaggi]{Perekrestenko17a}
Perekrestenko, Dmytro, Cevher, Volkan, and Jaggi, Martin.
\newblock {Faster Coordinate Descent via Adaptive Importance Sampling}.
\newblock In \emph{Proceedings of the 20th
  International Conference on Artificial Intelligence and Statistics},
  volume~54 of \emph{Proceedings of Machine Learning Research}, pp.\  869--877,
  Fort Lauderdale, FL, USA, 20--22 Apr 2017. PMLR.

\bibitem[Qu \& Richt\'{a}rik(2016)Qu and Richt\'{a}rik]{zheng:2016}
Qu, Zheng and Richt\'{a}rik, Peter.
\newblock Coordinate descent with arbitrary sampling i: algorithms and
  complexity.
\newblock \emph{Optimization Methods and Software}, 31\penalty0 (5):\penalty0
  829--857, 2016.

\bibitem[Richt{\'{a}}rik \& Tak{\'{a}}{\v{c}}(2016)Richt{\'{a}}rik and
  Tak{\'{a}}{\v{c}}]{richtarik:2016}
Richt{\'{a}}rik, Peter and Tak{\'{a}}{\v{c}}, Martin.
\newblock Parallel coordinate descent methods for big data optimization.
\newblock \emph{Mathematical Programming}, 156\penalty0 (1):\penalty0 433--484,
  2016.

\bibitem[Shalev-Shwartz \& Tewari(2011)Shalev-Shwartz and
  Tewari]{ShalevShwartz:2011vo}
Shalev-Shwartz, Shai and Tewari, Ambuj.
\newblock {Stochastic Methods for l$_{1}$-regularized Loss Minimization}.
\newblock \emph{JMLR}, 12:\penalty0 1865--1892, 2011.

\bibitem[Shalev-Shwartz \& Zhang(2013)Shalev-Shwartz and
  Zhang]{ShalevShwartz:2013wl}
Shalev-Shwartz, Shai and Zhang, Tong.
\newblock {Stochastic Dual Coordinate Ascent Methods for Regularized Loss
  Minimization}.
\newblock \emph{JMLR}, 14:\penalty0 567--599, 2013.

\bibitem[Shrivastava \& Li(2014)Shrivastava and Li]{Shrivastava:2014:ALS}
Shrivastava, Anshumali and Li, Ping.
\newblock Asymmetric LSH (ALSH) for sublinear time maximum inner product search
  (MIPS).
\newblock In \emph{NIPS 2014 - Advances in Neural Information Processing Systems 27}, pp.\  2321--2329, 2014.

\bibitem[Tseng \& Yun(2009)Tseng and Yun]{Tseng2009}
Tseng, Paul and Yun, Sangwoon.
\newblock A coordinate gradient descent method for nonsmooth separable
  minimization.
\newblock \emph{Mathematical Programming}, 117\penalty0 (1):\penalty0 387--423,
  2009.

\bibitem[Wen et~al.(2012)Wen, Yin, Zhang, and Goldfarb]{wen:2012}
Wen, Zaiwen, Yin, Wotao, Zhang, Hongchao, and Goldfarb, Donald.
\newblock On the convergence of an active-set method for ℓ1 minimization.
\newblock \emph{Optimization Methods and Software}, 27\penalty0 (6):\penalty0
  1127--1146, 2012.

\bibitem[Wright(2015)]{wright2015}
Wright, Stephen~J.
\newblock Coordinate descent algorithms.
\newblock \emph{Mathematical Programming}, 151\penalty0 (1):\penalty0 3--34,
  2015.

\bibitem[Wu \& Lange(2008)Wu and Lange]{wu2008}
Wu, Tong~Tong and Lange, Kenneth.
\newblock Coordinate descent algorithms for lasso penalized regression.
\newblock \emph{Ann. Appl. Stat.}, 2\penalty0 (1):\penalty0 224--244,  2008.

\bibitem[Zhao \& Zhang(2015)Zhao and Zhang]{Zhaoa15}
Zhao, Peilin and Zhang, Tong.
\newblock Stochastic optimization with importance sampling for regularized loss
  minimization.
\newblock In \emph{Proceedings of the
  32nd International Conference on Machine Learning}, volume~37 of
  \emph{PMLR}, pp.\  1--9, Lille, France,
  2015. PMLR.

\end{thebibliography}
