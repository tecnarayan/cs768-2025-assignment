\begin{thebibliography}{44}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{}\fi

\bibitem[{Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l and
  Szepesv{\'a}ri}]{abbasi2011improved}
\text{Abbasi-Yadkori, Y.}, \text{P{\'a}l, D.} and \text{Szepesv{\'a}ri, C.}
  (2011).
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \textit{NIPS}, vol.~11.

\bibitem[{Abe and Kaneko(2020)}]{abe2020off}
\text{Abe, K.} and \text{Kaneko, Y.} (2020).
\newblock Off-policy exploitability-evaluation in two-player zero-sum markov
  games.
\newblock \textit{arXiv preprint arXiv:2007.02141}.

\bibitem[{Antos et~al.(2008)Antos, Szepesv{\'a}ri and
  Munos}]{antos2008learning}
\text{Antos, A.}, \text{Szepesv{\'a}ri, C.} and \text{Munos, R.} (2008).
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \textit{Machine Learning}, \textbf{71} 89--129.

\bibitem[{Bai and Jin(2020)}]{bai2020provable}
\text{Bai, Y.} and \text{Jin, C.} (2020).
\newblock Provable self-play algorithms for competitive reinforcement learning.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Bai et~al.(2020)Bai, Jin and Yu}]{bai2020near}
\text{Bai, Y.}, \text{Jin, C.} and \text{Yu, T.} (2020).
\newblock Near-optimal reinforcement learning with self-play.
\newblock \textit{arXiv preprint arXiv:2006.12007}.

\bibitem[{Berner et~al.(2019)Berner, Brockman, Chan, Cheung, D{k{e}}biak,
  Dennison, Farhi, Fischer, Hashme, Hesse et~al.}]{berner2019dota}
\text{Berner, C.}, \text{Brockman, G.}, \text{Chan, B.}, \text{Cheung, V.},
  \text{D{k{e}}biak, P.}, \text{Dennison, C.}, \text{Farhi, D.}, \text{Fischer,
  Q.}, \text{Hashme, S.}, \text{Hesse, C.} \text{et~al.} (2019).
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \textit{arXiv preprint arXiv:1912.06680}.

\bibitem[{Brown and Sandholm(2019)}]{brown2019superhuman}
\text{Brown, N.} and \text{Sandholm, T.} (2019).
\newblock Superhuman ai for multiplayer poker.
\newblock \textit{Science}, \textbf{365} 885--890.

\bibitem[{Cai et~al.(2020)Cai, Yang, Jin and Wang}]{cai2020provably}
\text{Cai, Q.}, \text{Yang, Z.}, \text{Jin, C.} and \text{Wang, Z.} (2020).
\newblock Provably efficient exploration in policy optimization.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Chen et~al.(2021)Chen, Zhou and Gu}]{chen2021almost}
\text{Chen, Z.}, \text{Zhou, D.} and \text{Gu, Q.} (2021).
\newblock Almost optimal algorithms for two-player markov games with linear
  function approximation.
\newblock \textit{arXiv preprint arXiv:2102.07404}.

\bibitem[{Cui and Du(2022)}]{cui2022offline}
\text{Cui, Q.} and \text{Du, S.~S.} (2022).
\newblock When is offline two-player zero-sum markov game solvable?
\newblock \textit{arXiv preprint arXiv:2201.03522}.

\bibitem[{Donoho and Johnstone(1994)}]{donoho1994ideal}
\text{Donoho, D.~L.} and \text{Johnstone, J.~M.} (1994).
\newblock Ideal spatial adaptation by wavelet shrinkage.
\newblock \textit{biometrika}, \textbf{81} 425--455.

\bibitem[{Du et~al.(2019)Du, Kakade, Wang and Yang}]{du2019good}
\text{Du, S.~S.}, \text{Kakade, S.~M.}, \text{Wang, R.} and \text{Yang, L.~F.}
  (2019).
\newblock Is a good representation sufficient for sample efficient
  reinforcement learning?
\newblock \textit{arXiv preprint arXiv:1910.03016}.

\bibitem[{Duan et~al.(2020)Duan, Jia and Wang}]{duan2020minimax}
\text{Duan, Y.}, \text{Jia, Z.} and \text{Wang, M.} (2020).
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Fan and Li(2001)}]{fan2001variable}
\text{Fan, J.} and \text{Li, R.} (2001).
\newblock Variable selection via nonconcave penalized likelihood and its oracle
  properties.
\newblock \textit{Journal of the American statistical Association}, \textbf{96}
  1348--1360.

\bibitem[{Filar and Vrieze(2012)}]{filar2012competitive}
\text{Filar, J.} and \text{Vrieze, K.} (2012).
\newblock \textit{Competitive Markov decision processes}.
\newblock Springer Science \& Business Media.

\bibitem[{Fu et~al.(2020)Fu, Kumar, Nachum, Tucker and Levine}]{fu2020d4rl}
\text{Fu, J.}, \text{Kumar, A.}, \text{Nachum, O.}, \text{Tucker, G.} and
  \text{Levine, S.} (2020).
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2004.07219}.

\bibitem[{Huang et~al.(2021)Huang, Lee, Wang and Yang}]{huang2021towards}
\text{Huang, B.}, \text{Lee, J.~D.}, \text{Wang, Z.} and \text{Yang, Z.}
  (2021).
\newblock Towards general function approximation in zero-sum markov games.
\newblock \textit{arXiv preprint arXiv:2107.14702}.

\bibitem[{Jin et~al.(2021)Jin, Liu and Yu}]{jin2021power}
\text{Jin, C.}, \text{Liu, Q.} and \text{Yu, T.} (2021).
\newblock The power of exploiter: Provable multi-agent rl in large state
  spaces.
\newblock \textit{arXiv preprint arXiv:2106.03352}.

\bibitem[{Jin et~al.(2020{\natexlab{a}})Jin, Yang, Wang and
  Jordan}]{jin2020provably}
\text{Jin, C.}, \text{Yang, Z.}, \text{Wang, Z.} and \text{Jordan, M.~I.}
  (2020{\natexlab{a}}).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Jin et~al.(2020{\natexlab{b}})Jin, Yang and Wang}]{jin2020pessimism}
\text{Jin, Y.}, \text{Yang, Z.} and \text{Wang, Z.} (2020{\natexlab{b}}).
\newblock Is pessimism provably efficient for offline rl?
\newblock \textit{arXiv preprint arXiv:2012.15085}.

\bibitem[{Kober et~al.(2013)Kober, Bagnell and Peters}]{kober2013reinforcement}
\text{Kober, J.}, \text{Bagnell, J.~A.} and \text{Peters, J.} (2013).
\newblock Reinforcement learning in robotics: A survey.
\newblock \textit{The International Journal of Robotics Research}, \textbf{32}
  1238--1274.

\bibitem[{Lattimore and Szepesv{\'a}ri(2020)}]{lattimore2020bandit}
\text{Lattimore, T.} and \text{Szepesv{\'a}ri, C.} (2020).
\newblock \textit{Bandit algorithms}.
\newblock Cambridge University Press.

\bibitem[{Le~Cam(2012)}]{le2012asymptotic}
\text{Le~Cam, L.} (2012).
\newblock \textit{Asymptotic methods in statistical decision theory}.
\newblock Springer Science \& Business Media.

\bibitem[{Levine et~al.(2020)Levine, Kumar, Tucker and Fu}]{levine2020offline}
\text{Levine, S.}, \text{Kumar, A.}, \text{Tucker, G.} and \text{Fu, J.}
  (2020).
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \textit{arXiv preprint arXiv:2005.01643}.

\bibitem[{Liu et~al.(2021)Liu, Yu, Bai and Jin}]{liu2021sharp}
\text{Liu, Q.}, \text{Yu, T.}, \text{Bai, Y.} and \text{Jin, C.} (2021).
\newblock A sharp analysis of model-based reinforcement learning with
  self-play.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Pan et~al.(2017)Pan, Cheng, Saigol, Lee, Yan, Theodorou and
  Boots}]{pan2017agile}
\text{Pan, Y.}, \text{Cheng, C.-A.}, \text{Saigol, K.}, \text{Lee, K.},
  \text{Yan, X.}, \text{Theodorou, E.} and \text{Boots, B.} (2017).
\newblock Agile autonomous driving using end-to-end deep imitation learning.
\newblock \textit{arXiv preprint arXiv:1709.07174}.

\bibitem[{Precup(2000)}]{precup2000eligibility}
\text{Precup, D.} (2000).
\newblock Eligibility traces for off-policy policy evaluation.
\newblock \textit{Computer Science Department Faculty Publication Series} 80.

\bibitem[{Rashidinejad et~al.(2021)Rashidinejad, Zhu, Ma, Jiao and
  Russell}]{rashidinejad2021bridging}
\text{Rashidinejad, P.}, \text{Zhu, B.}, \text{Ma, C.}, \text{Jiao, J.} and
  \text{Russell, S.} (2021).
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock \textit{arXiv preprint arXiv:2103.12021}.

\bibitem[{Shapley(1953)}]{shapley1953stochastic}
\text{Shapley, L.~S.} (1953).
\newblock Stochastic games.
\newblock \textit{Proceedings of the national academy of sciences}, \textbf{39}
  1095--1100.

\bibitem[{Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot
  et~al.}]{silver2016mastering}
\text{Silver, D.}, \text{Huang, A.}, \text{Maddison, C.~J.}, \text{Guez, A.},
  \text{Sifre, L.}, \text{Van Den~Driessche, G.}, \text{Schrittwieser, J.},
  \text{Antonoglou, I.}, \text{Panneershelvam, V.}, \text{Lanctot, M.}
  \text{et~al.} (2016).
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \textit{nature}, \textbf{529} 484--489.

\bibitem[{Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou,
  Huang, Guez, Hubert, Baker, Lai, Bolton et~al.}]{silver2017mastering}
\text{Silver, D.}, \text{Schrittwieser, J.}, \text{Simonyan, K.},
  \text{Antonoglou, I.}, \text{Huang, A.}, \text{Guez, A.}, \text{Hubert, T.},
  \text{Baker, L.}, \text{Lai, M.}, \text{Bolton, A.} \text{et~al.} (2017).
\newblock Mastering the game of go without human knowledge.
\newblock \textit{nature}, \textbf{550} 354--359.

\bibitem[{Tropp(2015)}]{tropp2015introduction}
\text{Tropp, J.~A.} (2015).
\newblock An introduction to matrix concentration inequalities.

\bibitem[{Uehara and Sun(2021)}]{uehara2021pessimistic}
\text{Uehara, M.} and \text{Sun, W.} (2021).
\newblock Pessimistic model-based offline reinforcement learning under partial
  coverage.
\newblock \textit{arXiv preprint arXiv:2107.06226}.

\bibitem[{Uehara et~al.(2021)Uehara, Zhang and Sun}]{uehara2021representation}
\text{Uehara, M.}, \text{Zhang, X.} and \text{Sun, W.} (2021).
\newblock Representation learning for online and offline rl in low-rank mdps.
\newblock \textit{arXiv preprint arXiv:2110.04652}.

\bibitem[{Vershynin(2010)}]{vershynin2010introduction}
\text{Vershynin, R.} (2010).
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \textit{arXiv preprint arXiv:1011.3027}.

\bibitem[{Wang et~al.(2018)Wang, Zhang, He and Zha}]{wang2018supervised}
\text{Wang, L.}, \text{Zhang, W.}, \text{He, X.} and \text{Zha, H.} (2018).
\newblock Supervised reinforcement learning with recurrent neural network for
  dynamic treatment recommendation.
\newblock In \textit{Proceedings of the 24th ACM SIGKDD International
  Conference on Knowledge Discovery \& Data Mining}.

\bibitem[{Xie et~al.(2020)Xie, Chen, Wang and Yang}]{xie2020learning}
\text{Xie, Q.}, \text{Chen, Y.}, \text{Wang, Z.} and \text{Yang, Z.} (2020).
\newblock Learning zero-sum simultaneous-move markov games using function
  approximation and correlated equilibrium.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Xie et~al.(2021)Xie, Cheng, Jiang, Mineiro and
  Agarwal}]{xie2021bellman}
\text{Xie, T.}, \text{Cheng, C.-A.}, \text{Jiang, N.}, \text{Mineiro, P.} and
  \text{Agarwal, A.} (2021).
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \textit{arXiv preprint arXiv:2106.06926}.

\bibitem[{Yin and Wang(2021)}]{yin2021towards}
\text{Yin, M.} and \text{Wang, Y.-X.} (2021).
\newblock Towards instance-optimal offline reinforcement learning with
  pessimism.
\newblock \textit{Advances in neural information processing systems},
  \textbf{34}.

\bibitem[{Yu et~al.(1997)Yu, Assouad and Le~Cam}]{yu1997festschrift}
\text{Yu, B.}, \text{Assouad, F.} and \text{Le~Cam, L.} (1997).
\newblock Festschrift for lucien le cam.

\bibitem[{Zanette et~al.(2021)Zanette, Wainwright and
  Brunskill}]{zanette2021provable}
\text{Zanette, A.}, \text{Wainwright, M.~J.} and \text{Brunskill, E.} (2021).
\newblock Provable benefits of actor-critic methods for offline reinforcement
  learning.
\newblock \textit{Advances in neural information processing systems},
  \textbf{34}.

\bibitem[{Zhang et~al.(2021)Zhang, Yang and Ba{\c{s}}ar}]{zhang2021multi}
\text{Zhang, K.}, \text{Yang, Z.} and \text{Ba{\c{s}}ar, T.} (2021).
\newblock Multi-agent reinforcement learning: A selective overview of theories
  and algorithms.
\newblock \textit{Handbook of Reinforcement Learning and Control} 321--384.

\bibitem[{Zhong et~al.(2021)Zhong, Yang, Wang and Jordan}]{zhong2021can}
\text{Zhong, H.}, \text{Yang, Z.}, \text{Wang, Z.} and \text{Jordan, M.~I.}
  (2021).
\newblock Can reinforcement learning find stackelberg-nash equilibria in
  general-sum markov games with myopic followers?
\newblock \textit{arXiv preprint arXiv:2112.13521}.

\bibitem[{Zou(2006)}]{zou2006adaptive}
\text{Zou, H.} (2006).
\newblock The adaptive lasso and its oracle properties.
\newblock \textit{Journal of the American statistical association},
  \textbf{101} 1418--1429.

\end{thebibliography}
