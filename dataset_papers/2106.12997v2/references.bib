@InProceedings{shah16correlated, 
title = {Pareto Frontier Learning with Expensive Correlated Objectives}, 
author = {Amar Shah and Zoubin Ghahramani}, 
booktitle = {Proceedings of The 33rd International Conference on Machine Learning}, pages = {1919--1927}, year = {2016}, 
volume = {48}, 
address = {New York, New York, USA}, 
month = {20--22 Jun}, 
publisher = {PMLR}, 
}

@inproceedings{bruinsma2020scalable,
  title={Scalable Exact Inference in Multi-Output Gaussian Processes},
  author={Bruinsma, Wessel and Perim, Eric and Tebbutt, William and Hosking, Scott and Solin, Arno and Turner, Richard},
  booktitle={International Conference on Machine Learning},
  pages={1190--1201},
  year={2020},
  organization={PMLR}
}
@inproceedings{nguyen2014collaborative,
  title={Collaborative Multi-output Gaussian Processes.},
  author={Nguyen, Trung V and Bonilla, Edwin V and others},
  booktitle={Uncertainty in Artifical Intelligence},
  pages={643--652},
  volume={30},
  organization={AUAI Press},
  year={2014}
}

@inproceedings{yang17,
author = {Yang, Kaifeng and Emmerich, Michael and Deutz, Andr\'{e} and Fonseca, Carlos M.},
title = {Computing 3-D Expected Hypervolume Improvement and Related Integrals in Asymptotically Optimal Time},
year = {2017},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {9th International Conference on Evolutionary Multi-Criterion Optimization - Volume 10173},
pages = {685–700},
numpages = {16},
keywords = {Efficient global optimization, Probability of improvement, Expected hypervolume improvement, Time complexity, Surrogate-assisted multi-criterion optimization},
series = {EMO 2017}
}
@article{lewandowski2009generating,
  title={Generating random correlation matrices based on vines and extended onion method},
  author={Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
  journal={Journal of multivariate analysis},
  volume={100},
  number={9},
  pages={1989--2001},
  year={2009},
  publisher={Elsevier}
}
@book{golub_matrix_2013,
        edition = {4},
        title = {Matrix {Computations}},
        isbn = {978-1-4214-0794-4},
        publisher = {The Johns Hopkins University Press},
        author = {Golub, Gene H. and Van Loan, Charles F.},
        year = {2013},
}


@inproceedings{emmerich2011hypervolume,
  title={Hypervolume-based expected improvement: Monotonicity properties and exact computation},
  author={Emmerich, Michael TM and Deutz, Andr{\'e} H and Klinkenberg, Jan Willem},
  booktitle={2011 IEEE Congress of Evolutionary Computation (CEC)},
  pages={2147--2154},
  year={2011},
  organization={IEEE}
}

@inproceedings{dai2017efficient,
  title={Efficient modeling of latent information in supervised learning using Gaussian processes},
  author={Dai, Zhenwen and {\'A}lvarez, Mauricio A and Lawrence, Neil D},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={5137--5145},
  year={2017}
}

@article{emmerich2005single,
  title={Single-and multi-objective evolutionary design optimization assisted by gaussian random field metamodels},
  author={Emmerich, Michael},
  journal={dissertation, Universit{\"a}t Dortmund},
  year={2005}
}
@inproceedings{chowdhury2021no,
  title={No-regret algorithms for multi-task bayesian optimization},
  author={Chowdhury, Sayak Ray and Gopalan, Aditya},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1873--1881},
  year={2021},
  organization={PMLR}
}

@inproceedings{uhrenholt2019efficient,
  title={Efficient Bayesian optimization for target vector estimation},
  author={Uhrenholt, Anders Kirk and Jensen, Bj{\o}ern Sand},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={2661--2670},
  year={2019},
  organization={PMLR}
}

@inproceedings{wang_exact_2019,
	title = {Exact {Gaussian} {Processes} on a {Million} {Data} {Points}},
	url = {http://arxiv.org/abs/1903.08114},
	abstract = {Gaussian processes (GPs) are flexible models with state-of-the-art performance on many impactful applications. However, computational constraints with standard inference procedures have limited exact GPs to problems with fewer than about ten thousand training points, necessitating approximations for larger datasets. In this paper, we develop a scalable approach for exact GPs that leverages multi-GPU parallelization and methods like linear conjugate gradients, accessing the kernel matrix only through matrix multiplication. By partitioning and distributing kernel matrix multiplies, we demonstrate that an exact GP can be trained on over a million points in 3 days using 8 GPUs and can compute predictive means and variances in under a second using 1 GPU at test time. Moreover, we perform the first-ever comparison of exact GPs against state-of-the-art scalable approximations on large-scale regression datasets with \$10{\textasciicircum}4-10{\textasciicircum}6\$ data points, showing dramatic performance improvements.},
	urldate = {2019-10-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Ke Alexander and Pleiss, Geoff and Gardner, Jacob R. and Tyree, Stephen and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.08114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing}
}
@article{bliznyuk2008bayesian,
  title={Bayesian calibration and uncertainty analysis for computationally expensive models using optimization and radial basis function approximation},
  author={Bliznyuk, Nikolay and Ruppert, David and Shoemaker, Christine and Regis, Rommel and Wild, Stefan and Mugunthan, Pradeep},
  journal={Journal of Computational and Graphical Statistics},
  volume={17},
  number={2},
  pages={270--294},
  year={2008},
  publisher={Taylor \& Francis}
}

@article{larocque2006conditional,
  title={Conditional Gaussian co-simulation of regionalized components of soil variation},
  author={Larocque, Guillaume and Dutilleul, Pierre and Pelletier, Bernard and Fyles, James W},
  journal={Geoderma},
  volume={134},
  number={1-2},
  pages={1--16},
  year={2006},
  publisher={Elsevier}
}

@phdthesis{saatcci2012scalable,
  title={Scalable inference for structured Gaussian process models},
  author={Saatchi, Yunus},
  school={University of Cambridge},
  year={2011}
}

@article{rougier2008efficient,
  title={Efficient emulators for multivariate deterministic functions},
  author={Rougier, Jonathan},
  journal={Journal of Computational and Graphical Statistics},
  volume={17},
  number={4},
  pages={827--843},
  year={2008},
  publisher={Taylor \& Francis}
}

@incollection{de1994reminders,
  title={Reminders on the conditioning kriging},
  author={de Fouquet, Chantal},
  booktitle={Geostatistical simulations},
  pages={131--145},
  year={1994},
  publisher={Springer}
}

@article{emery2007conditioning,
  title={Conditioning simulations of Gaussian random fields by ordinary kriging},
  author={Emery, Xavier},
  journal={Mathematical Geology},
  volume={39},
  number={6},
  pages={607--623},
  year={2007},
  publisher={Springer}
}
@incollection{chiles2005prediction,
  title={Prediction by conditional simulation: models and algorithms},
  author={Chil{\`e}s, Jean-Paul and Lantu{\'e}joul, Christian},
  booktitle={Space, Structure and Randomness},
  pages={39--68},
  year={2005},
  publisher={Springer}
}
@inproceedings{jones2008large,
  title={Large-scale multi-disciplinary mass optimization in the auto industry},
  author={Jones, Donald R},
  booktitle={MOPTA 2008 Conference (20 August 2008)},
  year={2008},
  url={https://www.miguelanjos.com/jones-benchmark}
}
@inproceedings{rakitsch2013noise,
 author = {Rakitsch, Barbara and Lippert, Christoph and Borgwardt, Karsten and Stegle, Oliver},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {1466--1474},
 publisher = {Curran Associates, Inc.},
 title = {It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals},
 volume = {26},
 year = {2013}
}

@article{feydy2020fast,
  title={Fast geometric learning with symbolic matrices},
  author={Feydy, Jean and Glaun{\`e}s, Joan and Charlier, Benjamin and Bronstein, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  number={4},
  pages={6},
  year={2020}
}
@inproceedings{gelbart2014bayesian,
  title={Bayesian optimization with unknown constraints},
  author={Gelbart, Michael A and Snoek, Jasper and Adams, Ryan P},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={250--259},
  volume={30},
  year={2014},
  organization={AUAI Press}
}
@article{paszke2019pytorch,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={8026--8037},
  year={2019}
}
@article{rockafellar2000optimization,
  title={Optimization of conditional value-at-risk},
  author={Rockafellar, R Tyrrell and Uryasev, Stanislav and others},
  journal={Journal of risk},
  volume={2},
  pages={21--42},
  year={2000}
}
@inproceedings{cakmak2020risk,
 author = {Cakmak, Sait and Astudillo Marban, Raul and Frazier, Peter and Zhou, Enlu},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {20130--20141},
 publisher = {Curran Associates, Inc.},
 title = {Bayesian Optimization of Risk Measures},
 url = {https://proceedings.neurips.cc/paper/2020/file/e8f2779682fd11fa2067beffc27a9192-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{gardner2014bayesian,
  title={Bayesian Optimization with Inequality Constraints.},
  author={Gardner, Jacob R and Kusner, Matt J and Xu, Zhixiang Eddie and Weinberger, Kilian Q and Cunningham, John P},
  booktitle={International Conference on Machine Learning},
  publisher={PMLR},
  volume={32},
  pages={937--945},
  year={2014}
}

@ARTICLE{2020arXiv200407641L,
       author = {{Lorch}, Lars and {Kremer}, Heiner and {Trouleau}, William and {Tsirtsis}, Stratis and {Szanto}, Aron and {Sch{\"o}lkopf}, Bernhard and {Gomez-Rodriguez}, Manuel},
        title = "{Quantifying the Effects of Contact Tracing, Testing, and Containment Measures in the Presence of Infection Hotspots}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Physics - Physics and Society, Quantitative Biology - Populations and Evolution, Statistics - Machine Learning},
         year = 2020,
        month = apr,
          eid = {arXiv:2004.07641},
        pages = {arXiv:2004.07641},
archivePrefix = {arXiv},
       eprint = {2004.07641},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200407641L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{andrade2020finding,
  title={Finding hotspots: development of an adaptive spatial sampling approach},
  author={Andrade-Pacheco, Ricardo and Rerolle, Francois and Lemoine, Jean and Hernandez, Leda and Me{\"\i}t{\'e}, Aboulaye and Juziwelo, Lazarus and Bibaut, Aur{\'e}lien F and van der Laan, Mark J and Arnold, Benjamin F and Sturrock, Hugh JW},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--12},
  year={2020},
  publisher={Nature Publishing Group}
}
@article{boustati2019non,
  title={Non-linear multitask learning with deep Gaussian processes},
  author={Boustati, Ayman and Damoulas, Theodoros and Savage, Richard S},
  journal={arXiv preprint arXiv:1905.12407},
  year={2019}
}

@misc{openaigym,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  journal = {arXiv:1606.01540},
}
@article{chevalier2015fast,
  title={Fast update of conditional simulation ensembles},
  author={Chevalier, Cl{\'e}ment and Emery, Xavier and Ginsbourger, David},
  journal={Mathematical Geosciences},
  volume={47},
  number={7},
  pages={771--789},
  year={2015},
  publisher={Springer}
}
@article{alvarez2012kernels,
  title={Kernels for Vector-Valued Functions: A Review},
  author={{\'A}lvarez, Mauricio A and Rosasco, Lorenzo and Lawrence, Neil D and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={4},
  number={3},
  pages={195--266},
  year={2012},
  publisher={Now Publishers, Inc.}
}
@book{goovaerts1997geostatistics,
  title={Geostatistics for natural resources evaluation},
  author={Goovaerts, Pierre and others},
  year={1997},
  publisher={Oxford University Press on Demand}
}
@inproceedings{wilson2014fast,
  title={Fast Kernel Learning for Multidimensional Pattern Extrapolation.},
  author={Wilson, Andrew Gordon and Gilboa, Elad and Cunningham, John P and Nehorai, Arye},
  booktitle={Advances in Neural Information Processing Systems},
  volume=27,
  pages={3626--3634},
  year={2014}
}

@article{higdon2008computer,
  title={Computer model calibration using high-dimensional output},
  author={Higdon, Dave and Gattiker, James and Williams, Brian and Rightley, Maria},
  journal={Journal of the American Statistical Association},
  volume={103},
  number={482},
  pages={570--583},
  year={2008},
  publisher={Taylor \& Francis}
}
@article{zhang2019switching,
  title={Switching ripple suppressor design of the grid-connected inverters: A perspective of many-objective optimization with constraints handling},
  author={Zhang, Zhixiong and He, Cheng and Ye, Jie and Xu, Jinbang and Pan, Linqiang},
  journal={Swarm and evolutionary computation},
  volume={44},
  pages={293--303},
  year={2019},
  publisher={Elsevier}
}

@incollection{deb2019constrained,
  title={Constrained multi-objective evolutionary algorithm},
  author={Deb, Kalyanmoy},
  booktitle={Evolutionary and Swarm Intelligence Algorithms},
  pages={85--118},
  year={2019},
  publisher={Springer}
}

@article{osyczka1995new,
  title={A new method to solve generalized multicriteria optimization problems using the simple genetic algorithm},
  author={Osyczka, Andrzej and Kundu, Sourav},
  journal={Structural optimization},
  volume={10},
  number={2},
  pages={94--99},
  year={1995},
  publisher={Springer}
}

@article{kennedy2000predicting,
  title={Predicting the output from a complex computer code when fast approximations are available},
  author={Kennedy, Marc C and O'Hagan, Anthony},
  journal={Biometrika},
  volume={87},
  number={1},
  pages={1--13},
  year={2000},
  publisher={Oxford University Press}
}

@inproceedings{char2019offline,
  title={Offline contextual Bayesian optimization},
  author={Char, Ian and Chung, Youngseog and Neiswanger, Willie and Kandasamy, Kirthevasan and Nelson, Andrew Oakleigh and Boyer, Mark and Kolemen, Egemen and Schneider, Jeff},
  booktitle={Advances in Neural Information Processing Systems},
  volume={32},
  pages={4627--4638},
  year={2019}
}

@article{sorokin2020interferobot,
  title={Interferobot: aligning an optical interferometer by a reinforcement learning agent},
  author={Sorokin, Dmitry and Ulanov, Alexander and Sazhina, Ekaterina and Lvovsky, Alexander},
  journal={arXiv preprint arXiv:2006.02252},
  year={2020}
}

@article{stegle2011efficient,
  title={Efficient inference in matrix-variate Gaussian models with iid observation noise},
  author={Stegle, O and Lippert, C and Mooij, J and Lawrence, N and Borgwardt, K},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  pages={1--9},
  year={2011}
}


@inproceedings{krause2011contextual,
  title={Contextual gaussian process bandit optimization},
  author={Krause, Andreas and Ong, Cheng S},
  booktitle={Advances in neural information processing systems},
  pages={2447--2455},
  year={2011}
}

@article{dreifuerst2020optimizing,
  title={Optimizing Coverage and Capacity in Cellular Networks using Machine Learning},
  author={Dreifuerst, Ryan M and Daulton, Samuel and Qian, Yuchen and Varkey, Paul and Balandat, Maximilian and Kasturia, Sanjay and Tomar, Anoop and Yazdan, Ali and Ponnampalam, Vish and Heath, Robert W},
  journal={arXiv preprint arXiv:2010.13710},
  year={2020}
}
@article{zwicker2020py,
  title={py-pde: A Python package for solving partial differential equations},
  author={Zwicker, David},
  journal={Journal of Open Source Software},
  volume={5},
  number={48},
  pages={2158},
  year={2020}
}

@article{zhang2007maximum,
  title={Maximum-likelihood estimation for multivariate spatial linear coregionalization models},
  author={Zhang, Hao},
  journal={Environmetrics: The official journal of the International Environmetrics Society},
  volume={18},
  number={2},
  pages={125--139},
  year={2007},
  publisher={Wiley Online Library}
}

 @InProceedings{kamthe18a, title = {Data-Efficient Reinforcement Learning with Probabilistic Model Predictive Control}, author = {Sanket Kamthe and Marc Deisenroth}, booktitle = {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics}, pages = {1701--1710}, year = {2018}, editor = {Amos Storkey and Fernando Perez-Cruz}, volume = {84}, series = {Proceedings of Machine Learning Research}, month = {09--11 Apr}, publisher = {PMLR}} 
@article{wilson2020pathwise,
  title={Pathwise Conditioning of Gaussian Processes},
  author={Wilson, James T and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc Peter},
  journal={arXiv preprint arXiv:2011.04026},
  year={2020}
}
@book{chiles2009geostatistics,
  title={Geostatistics: modeling spatial uncertainty},
  author={Chiles, Jean-Paul and Delfiner, Pierre},
  volume={497},
  year={2009},
  publisher={John Wiley \& Sons}
}

@article{eriksson2019scalable,
  title={Scalable global optimization via local bayesian optimization},
  author={Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D and Poloczek, Matthias},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={5496--5507},
  year={2019}
}

@inproceedings{eriksson2020scalable,
  title={Scalable Constrained Bayesian Optimization},
  author={Eriksson, David and Poloczek, Matthias},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  organization={PMLR},
  year={2021}
}

@inproceedings{wilson_deep_2015,
	title = {Deep {Kernel} {Learning}},
	url = {http://arxiv.org/abs/1511.02222},
	abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric ﬂexibility of kernel methods. Speciﬁcally, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with beneﬁts in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost O(n) for n training points, and predictions cost O(1) per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with ﬂexible kernel learning models, and stand-alone deep architectures.},
	language = {en},
	urldate = {2018-12-11},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.02222},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Methodology},
	annote = {Comment: 19 pages, 6 figures}
}

@inproceedings{pleiss_constant-time_2018,
	title = {Constant-{Time} {Predictive} {Distributions} for {Gaussian} {Processes}},
	volume = {arXiv:1803.06058 [cs, stat]},
	url = {http://arxiv.org/abs/1803.06058},
	abstract = {One of the most compelling features of Gaussian process (GP) regression is its ability to provide well-calibrated posterior distributions. Recent advances in inducing point methods have sped up GP marginal likelihood and posterior mean computations, leaving posterior covariance estimation and sampling as the remaining computational bottlenecks. In this paper we address these shortcomings by using the Lanczos algorithm to rapidly approximate the predictive covariance matrix. Our approach, which we refer to as LOVE (LanczOs Variance Estimates), substantially improves time and space complexity. In our experiments, LOVE computes covariances up to 2,000 times faster and draws samples 18,000 times faster than existing methods, all without sacriﬁcing accuracy.},
	language = {en},
	urldate = {2019-08-09},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Pleiss, Geoff and Gardner, Jacob R. and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
	month = mar,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 1803.06058},
	annote = {Comment: ICML 2018}
}

@incollection{wilson2018maxbo,
	Author = {Wilson, James and Hutter, Frank and Deisenroth, Marc Peter},
	Booktitle = {Advances in Neural Information Processing Systems 31},
	Pages = {9905--9916},
	Title = {Maximizing acquisition functions for {B}ayesian optimization},
	Year = {2018},
}

@book{rasmussen_gaussian_2008,
	address = {Cambridge, Mass.},
	edition = {3. print},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	language = {eng},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2008},
	annote = {Includes bibliographical references and indexes},
	annote = {OCLC: 552376743},
}

@inproceedings{wilson_kernel_2015,
	title = {Kernel {Interpolation} for {Scalable} {Structured} {Gaussian} {Processes} ({KISS}-{GP})},
	url = {http://proceedings.mlr.press/v37/wilson15.html},
	abstract = {We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximat...},
	language = {en},
	urldate = {2020-04-15},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Wilson, Andrew and Nickisch, Hannes},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {1775--1784}
}

@inproceedings{wilson_stochastic_2016,
	address = {Barcelona, Spain},
	series = {{NIPS}'16},
	title = {Stochastic variational deep kernel learning},
	isbn = {978-1-5108-3881-9},
	abstract = {Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training. Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective. Within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra. We show improved performance over stand alone deep networks, SVMs, and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, CIFAR, and ImageNet.},
	urldate = {2020-04-15},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	month = dec,
	year = {2016},
	pages = {2594--2602}
}

@inproceedings{gardner_product_2018,
	title = {Product {Kernel} {Interpolation} for {Scalable} {Gaussian} {Processes}},
	url = {http://proceedings.mlr.press/v84/gardner18a.html},
	abstract = {Recent work shows that inference for Gaussian processes can be performed efficiently using iterative methods that rely only on matrix-vector multiplications (MVMs). Structured Kernel Interpolation ...},
	language = {en},
	urldate = {2020-04-15},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Gardner, Jacob and Pleiss, Geoff and Wu, Ruihan and Weinberger, Kilian and Wilson, Andrew},
	month = mar,
	year = {2018},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {1407--1416}
}

@inproceedings{zhe_scalable_2019,
	title = {Scalable {High}-{Order} {Gaussian} {Process} {Regression}},
	url = {http://proceedings.mlr.press/v89/zhe19a.html},
	abstract = {While most Gaussian processes (GP) work focus on learning single-output functions, many applications, such as physical simulations and gene expressions prediction, require estimations of functions ...},
	language = {en},
	urldate = {2020-12-09},
	booktitle = {The 22nd {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Zhe, Shandian and Xing, Wei and Kirby, Robert M.},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {2611--2620}
}

@inproceedings{bonilla_multi-task_2007,
	title = {Multi-task {Gaussian} {Process} {Prediction}},
	volume = {20},
	url = {https://proceedings.neurips.cc/paper/2007/hash/66368270ffd51418ec58bd793f2d9b1b-Abstract.html},
	language = {en},
	urldate = {2020-12-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Bonilla, Edwin V. and Chai, Kian and Williams, Christopher},
	year = {2007},
	pages = {153--160}
}

@inproceedings{balandat_botorch_2020,
	title = {{BoTorch}: {A} {Framework} for {Efficient} {Monte}-{Carlo} {Bayesian} {Optimization}},
	volume = {33},
	shorttitle = {{BoTorch}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/f5b1b89d98b7286673128a5fb112cb9a-Abstract.html},
	language = {en},
	urldate = {2020-12-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel and Daulton, Samuel and Letham, Ben and Wilson, Andrew G. and Bakshy, Eytan},
	year = {2020}
}

@inproceedings{pleiss_fast_2020,
	title = {Fast {Matrix} {Square} {Roots} with {Applications} to {Gaussian} {Processes} and {Bayesian} {Optimization}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/fcf55a303b71b84d326fb1d06e332a26-Abstract.html},
	language = {en},
	urldate = {2020-12-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Pleiss, Geoff and Jankowiak, Martin and Eriksson, David and Damle, Anil and Gardner, Jacob},
	year = {2020}
}

@inproceedings{jiang_efficient_2020,
	title = {Efficient {Nonmyopic} {Bayesian} {Optimization} via {One}-{Shot} {Multi}-{Step} {Trees}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/d1d5923fc822531bbfd9d87d4760914b-Abstract.html},
	language = {en},
	urldate = {2020-12-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Jiang, Shali and Jiang, Daniel and Balandat, Maximilian and Karrer, Brian and Gardner, Jacob and Garnett, Roman},
	year = {2020}
}

@inproceedings{wilson_efficiently_2020,
	title = {Efficiently sampling functions from {Gaussian} process posteriors},
	url = {http://proceedings.mlr.press/v119/wilson20a.html},
	abstract = {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model’s success hinges upon its ability to faithfully represent predictive uncertainty. T...},
	language = {en},
	urldate = {2020-12-09},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wilson, James and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {10292--10302}
}

@inproceedings{gardner_gpytorch_2018,
	title = {{GPyTorch}: {Blackbox} {Matrix}-{Matrix} {Gaussian} {Process} {Inference} with {GPU} {Acceleration}},
	volume = {31},
	shorttitle = {{GPyTorch}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/27e8e17134dd7083b050476733207ea1-Abstract.html},
	language = {en},
	urldate = {2020-12-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Gardner, Jacob and Pleiss, Geoff and Weinberger, Kilian Q. and Bindel, David and Wilson, Andrew G.},
	year = {2018},
	pages = {7576--7586}
}

@article{doucet_note_2010,
	title = {A {Note} on {Efficient} {Conditional} {Simulation} of {Gaussian} {Distributions}},
	note = {\url{https://www.cs.ubc.ca/~arnaud/doucet_simulationconditionalgaussian.pdf}},
	urldate = {2020-12-09},
	author = {Doucet, Arnaud},
	year = {2010},
}
@article{knowles2006parego,
  title={ParEGO: A hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems},
  author={Knowles, Joshua},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={10},
  number={1},
  pages={50--66},
  year={2006},
  publisher={IEEE}
}

@inproceedings{khan2002multi,
  title={Multi-objective Bayesian optimization algorithm},
  author={Khan, Nazan and Goldberg, David E and Pelikan, Martin},
  booktitle={Proceedings of the 4th Annual Conference on Genetic and Evolutionary Computation},
  pages={684--684},
  year={2002},
}

% http://www.stats.ox.ac.uk/\%7Edoucet/doucet_simulationconditionalgaussian.pdf,



@inproceedings{swersky_multi-task_2013,
	address = {Red Hook, NY, USA},
	title = {Multi-task {Bayesian} optimization},
	abstract = {Bayesian optimization has recently been proposed as a framework for automatically tuning the hyperparameters of machine learning models and has been shown to yield state-of-the-art performance with impressive ease and efficiency. In this paper, we explore whether it is possible to transfer the knowledge gained from previous optimizations to new tasks in order to find optimal hyperparameter settings more efficiently. Our approach is based on extending multi-task Gaussian processes to the framework of Bayesian optimization. We show that this method significantly speeds up the optimization process when compared to the standard single-task approach. We further propose a straightforward extension of our algorithm in order to jointly minimize the average error across multiple tasks and demonstrate how this can be used to greatly speed up k-fold cross-validation. Lastly, we propose an adaptation of a recently developed acquisition function, entropy search, to the cost-sensitive, multi-task setting. We demonstrate the utility of this new acquisition function by leveraging a small dataset to explore hyper-parameter settings for a large dataset. Our algorithm dynamically chooses which dataset to query in order to yield the most information per unit cost.},
	urldate = {2020-12-09},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Swersky, Kevin and Snoek, Jasper and Adams, Ryan P.},
	month = dec,
	year = {2013},
	volume={26},
	pages = {2004--2012}
}

@article{daulton_differentiable_2020,
	title = {Differentiable {Expected} {Hypervolume} {Improvement} for {Parallel} {Multi}-{Objective} {Bayesian} {Optimization}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/6fec24eac8f18ed793f5eaad3dd7977c-Abstract.html},
	language = {en},
	urldate = {2020-12-09},
	journal = {Advances in Neural Information Processing Systems},
	author = {Daulton, Samuel and Balandat, Maximilian and Bakshy, Eytan},
	year = {2020}
}

@article{feng_high-dimensional_2020,
	title = {High-{Dimensional} {Contextual} {Policy} {Search} with {Unknown} {Context} {Rewards} using {Bayesian} {Optimization}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/faff959d885ec0ecf70741a846c34d1d-Abstract.html},
	language = {en},
	urldate = {2020-12-09},
	journal = {Advances in Neural Information Processing Systems},
	author = {Feng, Qing and Letham, Ben and Mao, Hongzi and Bakshy, Eytan},
	year = {2020}
}

@article{letham_bayesian_2019,
	title = {Bayesian {Optimization} for {Policy} {Search} via {Online}-{Offline} {Experimentation}},
	url = {http://arxiv.org/abs/1904.01049},
	abstract = {Online ﬁeld experiments are the gold-standard way of evaluating changes to real-world interactive machine learning systems. Yet our ability to explore complex, multi-dimensional policy spaces—such as those found in recommendation and ranking problems—is often constrained by the limited number of experiments that can be run simultaneously. To alleviate these constraints, we augment online experiments with an oﬄine simulator and apply multi-task Bayesian optimization to tune live machine learning systems. We describe practical issues that arise in these types of applications, including biases that arise from using a simulator and assumptions for the multi-task kernel. We measure empirical learning curves which show substantial gains from including data from biased oﬄine experiments, and show how these learning curves are consistent with theoretical results for multi-task Gaussian process generalization. We ﬁnd that improved kernel inference is a signiﬁcant driver of multi-task generalization. Finally, we show several examples of Bayesian optimization eﬃciently tuning a live machine learning system by combining oﬄine and online experiments.},
	language = {en},
	urldate = {2020-12-09},
	journal = {JMLR},
	author = {Letham, Benjamin and Bakshy, Eytan},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.01049}
}

@inproceedings{astudillo_bayesian_2019,
	title = {Bayesian {Optimization} of {Composite} {Functions}},
	url = {http://proceedings.mlr.press/v97/astudillo19a.html},
	abstract = {We consider optimization of composite objective functions, i.e., of the form \$f(x)=g(h(x))\$, where \$h\$ is a black-box derivative-free expensive-to-evaluate function with vector-valued outputs, and ...},
	language = {en},
	urldate = {2020-12-11},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Astudillo, Raul and Frazier, Peter},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {354--363}
}
