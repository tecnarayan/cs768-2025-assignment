\begin{thebibliography}{100}

\bibitem{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock {\em Advances in neural information processing systems}, 35:23716--23736, 2022.

\bibitem{ansari2024jmi}
Mohammed~Abbas Ansari, Chandni Saxena, Tanvir Ahmad, et~al.
\newblock Jmi at semeval 2024 task 3: Two-step approach for multimodal ecac using in-context learning with gpt and instruction-tuned llama models.
\newblock {\em arXiv preprint arXiv:2403.04798}, 2024.

\bibitem{anas_awadalla_2023_7733589}
Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang~Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt.
\newblock Openflamingo, March 2023.

\bibitem{baevski2020wav2vec}
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech representations.
\newblock {\em Advances in neural information processing systems}, 33:12449--12460, 2020.

\bibitem{qwen}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.
\newblock Qwen-vl: A frontier large vision-language model with versatile abilities.
\newblock {\em arXiv preprint arXiv:2308.12966}, 2023.

\bibitem{brown2020gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{cao2020psychological}
Min Cao and Zhendong Wan.
\newblock Psychological counseling and character analysis algorithm based on image emotion.
\newblock {\em IEEE Access}, 2020.

\bibitem{chen2023semi}
Haifeng Chen, Chujia Guo, Yan Li, Peng Zhang, and Dongmei Jiang.
\newblock Semi-supervised multimodal emotion recognition with class-balanced pseudo-labeling.
\newblock In {\em Proceedings of the 31st ACM International Conference on Multimedia}, pages 9556--9560, 2023.

\bibitem{chen2024improving}
Haifeng Chen, Xi~Li, Dongmei Jiang, Lei Xie, et~al.
\newblock Improving multimodal emotion recognition by leveraging acoustic adaptation and visual alignment.
\newblock {\em arXiv preprint arXiv:2409.05015}, 2024.

\bibitem{chen2023minigpt}
Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.
\newblock Minigpt-v2: large language model as a unified interface for vision-language multi-task learning.
\newblock {\em arXiv preprint arXiv:2310.09478}, 2023.

\bibitem{shikra}
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.
\newblock Shikra: Unleashing multimodal llm's referential dialogue magic.
\newblock {\em arXiv preprint arXiv:2306.15195}, 2023.

\bibitem{chen2024static}
Yin Chen, Jia Li, Shiguang Shan, Meng Wang, and Richang Hong.
\newblock From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos.
\newblock {\em IEEE Transactions on Affective Computing}, 2024.

\bibitem{cheng2023semi}
Zebang Cheng, Yuxiang Lin, Zhaoru Chen, Xiang Li, Shuyi Mao, Fan Zhang, Daijun Ding, Bowen Zhang, and Xiaojiang Peng.
\newblock Semi-supervised multimodal emotion recognition with expression mae.
\newblock In {\em Proceedings of the 31st ACM International Conference on Multimedia}, pages 9436--9440, 2023.

\bibitem{cheng2024mips}
Zebang Cheng, Fuqiang Niu, Yuxiang Lin, Zhi-Qi Cheng, Bowen Zhang, and Xiaojiang Peng.
\newblock Mips at semeval-2024 task 3: Multimodal emotion-cause pair extraction in conversations with multimodal language models.
\newblock {\em arXiv preprint arXiv:2404.00511}, 2024.

\bibitem{cheng2024sztu}
Zebang Cheng, Shuyuan Tu, Dawei Huang, Minghan Li, Xiaojiang Peng, Zhi-Qi Cheng, and Alexander~G Hauptmann.
\newblock Sztu-cmu at mer2024: Improving emotion-llama with conv-attention for multimodal emotion recognition.
\newblock {\em arXiv preprint arXiv:2408.10500}, 2024.

\bibitem{cheng2022gsrformer}
Zhi-Qi Cheng, Qi~Dai, Siyao Li, Teruko Mitamura, and Alexander Hauptmann.
\newblock Gsrformer: Grounded situation recognition transformer with alternate semantic attention refinement.
\newblock In {\em Proceedings of the 30th ACM International Conference on Multimedia}, pages 3272--3281, 2022.

\bibitem{cheng2016video}
Zhi-Qi Cheng, Yang Liu, Xiao Wu, and Xian-Sheng Hua.
\newblock Video ecommerce: Towards online video advertising.
\newblock In {\em Proceedings of the 24th ACM international conference on Multimedia}, pages 1365--1374, 2016.

\bibitem{cheng2017video}
Zhi-Qi Cheng, Xiao Wu, Yang Liu, and Xian-Sheng Hua.
\newblock Video ecommerce++: Toward large scale online video advertising.
\newblock {\em IEEE transactions on multimedia}, 19(6):1170--1183, 2017.

\bibitem{cheng2017video2shop}
Zhi-Qi Cheng, Xiao Wu, Yang Liu, and Xian-Sheng Hua.
\newblock Video2shop: Exact matching clothes in videos to online shopping images.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 4048--4056, 2017.

\bibitem{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{chu2023qwen}
Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou.
\newblock Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models.
\newblock {\em arXiv preprint arXiv:2311.07919}, 2023.

\bibitem{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock {\em arXiv preprint arXiv:2210.11416}, 2022.

\bibitem{cui2020revisiting}
Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu.
\newblock Revisiting pre-trained models for chinese natural language processing.
\newblock {\em arXiv preprint arXiv:2004.13922}, 2020.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{ding2023learning}
Chaoyue Ding, Daoming Zong, Baoxiang Li, Ken Zheng, Dinghao Zhou, Jiakui Li, and Qunyan Zhou.
\newblock Learning aligned audiovisual representations for multimodal sentiment analysis.
\newblock In {\em Proceedings of the 1st International Workshop on Multimodal and Responsible Affective Computing}, pages 21--28, 2023.

\bibitem{driess2023palm}
Danny Driess, Fei Xia, Mehdi~SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et~al.
\newblock {PaLM-E}: An embodied multimodal language model.
\newblock {\em arXiv preprint arXiv:2303.03378}, 2023.

\bibitem{ekman1978facial}
Paul Ekman and Wallace~V Friesen.
\newblock Facial action coding system.
\newblock {\em Environmental Psychology \& Nonverbal Behavior}, 1978.

\bibitem{fan2024leveraging}
Qi~Fan, Yutong Li, Yi~Xin, Xinyu Cheng, Guanglai Gao, and Miao Ma.
\newblock Leveraging contrastive learning and self-training for multimodal emotion recognition with limited labeled samples.
\newblock {\em arXiv preprint arXiv:2409.04447}, 2024.

\bibitem{fan2023learning}
Qi~Fan, Haolin Zuo, Rui Liu, Zheng Lian, and Guanglai Gao.
\newblock Learning noise-robust joint representation for multimodal emotion recognition under realistic incomplete data scenarios.
\newblock {\em arXiv preprint arXiv:2311.16114}, 2023.

\bibitem{fan2021lssed}
Weiquan Fan, Xiangmin Xu, Xiaofen Xing, Weidong Chen, and Dongyan Huang.
\newblock Lssed: a large-scale dataset and benchmark for speech emotion recognition.
\newblock In {\em ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 641--645. IEEE, 2021.

\bibitem{fang2023eva}
Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao.
\newblock Eva: Exploring the limits of masked visual representation learning at scale.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 19358--19369, 2023.

\bibitem{ge2024video}
Mengying Ge, Dongkai Tang, and Mingyang Li.
\newblock Video emotion open-vocabulary recognition based on multimodal large language model.
\newblock {\em arXiv preprint arXiv:2408.11286}, 2024.

\bibitem{girdhar2023imagebind}
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan~Vasudev Alwala, Armand Joulin, and Ishan Misra.
\newblock Imagebind: One embedding space to bind them all.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 15180--15190, 2023.

\bibitem{guo2024stimuvar}
Yuxiang Guo, Faizan Siddiqui, Yang Zhao, Rama Chellappa, and Shao-Yuan Lo.
\newblock Stimuvar: Spatiotemporal stimuli-aware video affective reasoning with multimodal large language models.
\newblock {\em arXiv preprint arXiv:2409.00304}, 2024.

\bibitem{he2022masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 16000--16009, 2022.

\bibitem{he2016resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem{hershey2017vggish}
Shawn Hershey, Sourish Chaudhuri, Daniel~PW Ellis, Jort~F Gemmeke, Aren Jansen, R~Channing Moore, Manoj Plakal, Devin Platt, Rif~A Saurous, Bryan Seybold, et~al.
\newblock Cnn architectures for large-scale audio classification.
\newblock In {\em 2017 ieee international conference on acoustics, speech and signal processing (icassp)}, pages 131--135. IEEE, 2017.

\bibitem{hsu2021hubert}
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung~Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed.
\newblock Hubert: Self-supervised speech representation learning by masked prediction of hidden units.
\newblock {\em IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 29:3451--3460, 2021.

\bibitem{huang2023language}
Shaohan Huang, Li~Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais~Khan Mohammed, Qiang Liu, et~al.
\newblock Language is not all you need: Aligning perception with language models.
\newblock {\em arXiv preprint arXiv:2302.14045}, 2023.

\bibitem{hung2023beyond}
Lai~Po Hung and Suraya Alias.
\newblock Beyond sentiment analysis: A review of recent trends in text based sentiment analysis and emotion detection.
\newblock {\em Journal of Advanced Computational Intelligence and Intelligent Informatics}, 27(1):84--95, 2023.

\bibitem{hutchison2017emotion}
Ashley Hutchison and Larry Gerstein.
\newblock Emotion recognition, emotion expression, and cultural display rules: Implications for counseling.
\newblock {\em Journal of Asia Pacific Counseling}, 7(1), 2017.

\bibitem{imani2019survey}
Maryam Imani and Gholam~Ali Montazer.
\newblock A survey of emotion recognition methods with emphasis on e-learning environments.
\newblock {\em Journal of Network and Computer Applications}, 147:102423, 2019.

\bibitem{iyer2022opt-iml}
Srinivasan Iyer, Xi~Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D{\'a}niel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit~Singh Koura, et~al.
\newblock Opt-iml: Scaling language model instruction meta learning through the lens of generalization.
\newblock {\em arXiv preprint arXiv:2212.12017}, 2022.

\bibitem{jiang2020dfew}
Xingxun Jiang, Yuan Zong, Wenming Zheng, Chuangao Tang, Wanchuang Xia, Cheng Lu, and Jiateng Liu.
\newblock Dfew: A large-scale database for recognizing dynamic facial expressions in the wild.
\newblock In {\em Proceedings of the 28th ACM international conference on multimedia}, pages 2881--2889, 2020.

\bibitem{koh2023grounding}
Jing~Yu Koh, Ruslan Salakhutdinov, and Daniel Fried.
\newblock Grounding language models to images for multimodal generation.
\newblock {\em arXiv preprint arXiv:2301.13823}, 2023.

\bibitem{kondratenko2022large}
Vladimir Kondratenko, Artem Sokolov, Nikolay Karpov, Oleg Kutuzov, Nikita Savushkin, and Fyodor Minkin.
\newblock Large raw emotional dataset with aggregation mechanism.
\newblock {\em arXiv preprint arXiv:2212.12266}, 2022.

\bibitem{kudo2018sentencepiece}
Taku Kudo and John Richardson.
\newblock Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.
\newblock {\em arXiv preprint arXiv:1808.06226}, 2018.

\bibitem{lei2023instructerc}
Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, and Sirui Wang.
\newblock Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework.
\newblock {\em arXiv preprint arXiv:2309.11911}, 2023.

\bibitem{lei2024large}
Yuxuan Lei, Dingkang Yang, Zhaoyu Chen, Jiawei Chen, Peng Zhai, and Lihua Zhang.
\newblock Large vision-language models as emotion recognizers in context awareness.
\newblock {\em arXiv preprint arXiv:2407.11300}, 2024.

\bibitem{li2023mimic}
Bo~Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu.
\newblock Mimic-it: Multi-modal in-context instruction tuning.
\newblock {\em arXiv preprint arXiv:2306.05425}, 2023.

\bibitem{li2023intensity}
Hanting Li, Hongjing Niu, Zhaoqing Zhu, and Feng Zhao.
\newblock Intensity-aware loss for dynamic facial expression recognition in the wild.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 67--75, 2023.

\bibitem{li2023blip}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In {\em International conference on machine learning}, pages 19730--19742. PMLR, 2023.

\bibitem{li2023videochat}
KunChang Li, Yinan He, Yi~Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu~Qiao.
\newblock Videochat: Chat-centric video understanding.
\newblock {\em arXiv preprint arXiv:2305.06355}, 2023.

\bibitem{li2024mvbench}
Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi~Wang, Yi~Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et~al.
\newblock Mvbench: A comprehensive multi-modal video understanding benchmark.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 22195--22206, 2024.

\bibitem{li2024mm}
Xiang Li, Zhi-Qi Cheng, Jun-Yan He, Xiaojiang Peng, and Alexander~G Hauptmann.
\newblock Mm-tts: A unified framework for multimodal, prompt-induced emotional text-to-speech synthesis.
\newblock {\em arXiv preprint arXiv:2404.18398}, 2024.

\bibitem{li2024two}
Xinpeng Li, Teng Wang, Jian Zhao, Shuyi Mao, Jinbao Wang, Feng Zheng, Xiaojiang Peng, and Xuelong Li.
\newblock Two in one go: Single-stage emotion recognition with decoupled subject-context transformer.
\newblock {\em arXiv preprint arXiv:2404.17205}, 2024.

\bibitem{li2023decoupled}
Yong Li, Yuanzhi Wang, and Zhen Cui.
\newblock Decoupled multimodal distilling for emotion recognition.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 6631--6640, 2023.

\bibitem{lian2023mer}
Zheng Lian, Haiyang Sun, Licai Sun, Kang Chen, Mngyu Xu, Kexin Wang, Ke~Xu, Yu~He, Ying Li, Jinming Zhao, et~al.
\newblock Mer 2023: Multi-label learning, modality robustness, and semi-supervised learning.
\newblock In {\em Proceedings of the 31st ACM International Conference on Multimedia}, pages 9610--9614, 2023.

\bibitem{lian2024mer}
Zheng Lian, Haiyang Sun, Licai Sun, Zhuofan Wen, Siyuan Zhang, Shun Chen, Hao Gu, Jinming Zhao, Ziyang Ma, Xie Chen, et~al.
\newblock Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition.
\newblock {\em arXiv preprint arXiv:2404.17113}, 2024.

\bibitem{lian2024gpt}
Zheng Lian, Licai Sun, Haiyang Sun, Kang Chen, Zhuofan Wen, Hao Gu, Bin Liu, and Jianhua Tao.
\newblock Gpt-4v with emotion: A zero-shot benchmark for generalized emotion recognition.
\newblock {\em Information Fusion}, page 102367, 2024.

\bibitem{lian2023explainable}
Zheng Lian, Licai Sun, Mingyu Xu, Haiyang Sun, Ke~Xu, Zhuofan Wen, Shun Chen, Bin Liu, and Jianhua Tao.
\newblock Explainable multimodal emotion reasoning.
\newblock {\em arXiv preprint arXiv:2306.15401}, 2023.

\bibitem{lin2023video}
Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li~Yuan.
\newblock Video-llava: Learning united visual representation by alignment before projection.
\newblock {\em arXiv preprint arXiv:2311.10122}, 2023.

\bibitem{liu2024llava}
Haotian Liu, Chunyuan Li, Yuheng Li, Bo~Li, Yuanhan Zhang, Sheng Shen, and Yong~Jae Lee.
\newblock Llava-next: Improved reasoning, ocr, and world knowledge, 2024.

\bibitem{liu2024visual}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock {\em NeurIPS}, 36, 2024.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{luo2023valley}
Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei.
\newblock Valley: Video assistant with large language model enhanced ability.
\newblock {\em arXiv preprint arXiv:2306.07207}, 2023.

\bibitem{maaz2023video}
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad~Shahbaz Khan.
\newblock Video-chatgpt: Towards detailed video understanding via large vision and language models.
\newblock {\em arXiv preprint arXiv:2306.05424}, 2023.

\bibitem{mcduff2013affectiva}
Daniel McDuff, Rana Kaliouby, Thibaud Senechal, May Amr, Jeffrey Cohn, and Rosalind Picard.
\newblock Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition workshops}, pages 881--888, 2013.

\bibitem{ngwe2023patt}
Jia~Le Ngwe, Kian~Ming Lim, Chin~Poo Lee, and Thian~Song Ong.
\newblock Patt-lite: Lightweight patch and attention mobilenet for challenging facial expression recognition.
\newblock {\em arXiv preprint arXiv:2306.09626}, 2023.

\bibitem{openai2023gpt4v}
OpenAI.
\newblock Gpt-4v(ision) system card, 2023.

\bibitem{ouyang2022instruct-tuning}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{peng2023kosmos}
Zhiliang Peng, Wenhui Wang, Li~Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.
\newblock Kosmos-2: Grounding multimodal large language models to the world.
\newblock {\em arXiv preprint arXiv:2306.14824}, 2023.

\bibitem{qi2024multimodal}
Anbin QI, Zhongliang Liu, Xinyong Zhou, Jinba Xiao, Fengrun Zhang, Qi~Gan, Ming Tao, Gaozheng Zhang, and Lu~Zhang.
\newblock Multimodal emotion recognition with vision-language prompting and modality dropout.
\newblock {\em arXiv preprint arXiv:2409.07078}, 2024.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In {\em International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em The Journal of Machine Learning Research}, 2020.

\bibitem{richet2024text}
Nicolas Richet, Soufiane Belharbi, Haseeb Aslam, Meike~Emilie Schadt, Manuela Gonz{\'a}lez-Gonz{\'a}lez, Gustave Cortal, Alessandro~Lameiras Koerich, Marco Pedersoli, Alain Finkel, Simon Bacon, et~al.
\newblock Text-and feature-based models for compound multimodal emotion recognition in the wild.
\newblock {\em arXiv preprint arXiv:2407.12927}, 2024.

\bibitem{sennrich2015neural}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock {\em arXiv preprint arXiv:1508.07909}, 2015.

\bibitem{shi2024audio}
Pujin Shi and Fei Gao.
\newblock Audio-guided fusion techniques for multimodal emotion analysis.
\newblock {\em arXiv preprint arXiv:2409.05007}, 2024.

\bibitem{sinha2010human}
Gaurav Sinha, Rahul Shahi, and Mani Shankar.
\newblock Human computer interaction.
\newblock In {\em 2010 3rd International Conference on Emerging Trends in Engineering and Technology}, pages 1--4. IEEE, 2010.

\bibitem{su2023pandagpt}
Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai.
\newblock Pandagpt: One model to instruction-follow them all.
\newblock {\em arXiv preprint arXiv:2305.16355}, 2023.

\bibitem{sun2023mae}
Licai Sun, Zheng Lian, Bin Liu, and Jianhua Tao.
\newblock Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition.
\newblock In {\em Proceedings of the 31st ACM International Conference on Multimedia}, pages 6110--6121, 2023.

\bibitem{tang2023salmonn}
Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu~Lu, Zejun Ma, and Chao Zhang.
\newblock Salmonn: Towards generic hearing abilities for large language models.
\newblock {\em arXiv preprint arXiv:2310.13289}, 2023.

\bibitem{tong2022videomae}
Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.
\newblock Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training.
\newblock {\em Advances in neural information processing systems}, 35:10078--10093, 2022.

\bibitem{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{tu2023implicit}
Shuyuan Tu, Qi~Dai, Zuxuan Wu, Zhi-Qi Cheng, Han Hu, and Yu-Gang Jiang.
\newblock Implicit temporal modeling with learnable alignment for video recognition.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 19936--19947, 2023.

\bibitem{wang2020suppressing}
Kai Wang, Xiaojiang Peng, Jianfei Yang, Shijian Lu, and Yu~Qiao.
\newblock Suppressing uncertainties for large-scale facial expression recognition.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 6897--6906, 2020.

\bibitem{wang2023visionllm}
Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu~Qiao, et~al.
\newblock Visionllm: Large language model is also an open-ended decoder for vision-centric tasks.
\newblock {\em arXiv preprint arXiv:2305.11175}, 2023.

\bibitem{wang2022self_instruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language model with self generated instructions.
\newblock {\em arXiv preprint arXiv:2212.10560}, 2022.

\bibitem{wang2022benchmarking}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut~Selvan Dhanasekaran, Atharva Naik, David Stap, et~al.
\newblock Benchmarking generalization via in-context instructions on 1,600+ language tasks.
\newblock {\em arXiv preprint arXiv:2204.07705}, 2022.

\bibitem{wang2024incomplete}
Yuanzhi Wang, Yong Li, and Zhen Cui.
\newblock Incomplete multimodality-diffused emotion recognition.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{xie2024emovit}
Hongxia Xie, Chu-Jun Peng, Yu-Wen Tseng, Hung-Jen Chen, Chan-Feng Hsu, Hong-Han Shuai, and Wen-Huang Cheng.
\newblock Emovit: Revolutionizing emotion insights with visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2404.16670}, 2024.

\bibitem{yang2023emoset}
Jingyuan Yang, Qirui Huang, Tingting Ding, Dani Lischinski, Danny Cohen-Or, and Hui Huang.
\newblock Emoset: A large-scale visual emotion dataset with rich attributes.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 20383--20394, 2023.

\bibitem{ye2023mplug}
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et~al.
\newblock mplug-owl: Modularization empowers large language models with multimodality.
\newblock {\em arXiv preprint arXiv:2304.14178}, 2023.

\bibitem{yue2023mammoth}
Xiang Yue, Xingwei Qu, Ge~Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu~Su, and Wenhu Chen.
\newblock Mammoth: Building math generalist models through hybrid instruction tuning.
\newblock {\em arXiv preprint arXiv:2309.05653}, 2023.

\bibitem{zadeh2018multimodal}
AmirAli~Bagher Zadeh, Paul~Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency.
\newblock Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 2236--2246, 2018.

\bibitem{zhang2023video}
Hang Zhang, Xin Li, and Lidong Bing.
\newblock Video-llama: An instruction-tuned audio-visual language model for video understanding.
\newblock {\em arXiv preprint arXiv:2306.02858}, 2023.

\bibitem{zhang2024microemo}
Liyun Zhang.
\newblock Microemo: Time-sensitive multimodal emotion recognition with micro-expression dynamics in video dialogues.
\newblock {\em arXiv preprint arXiv:2407.16552}, 2024.

\bibitem{zhang2023llama}
Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu~Qiao.
\newblock Llama-adapter: Efficient fine-tuning of language models with zero-init attention.
\newblock {\em arXiv preprint arXiv:2303.16199}, 2023.

\bibitem{zhang2023learning}
Sitao Zhang, Yimu Pan, and James~Z Wang.
\newblock Learning emotion representations from verbal and nonverbal communication.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 18993--19004, 2023.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock {OPT}: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{zhao2021former}
Zengqun Zhao and Qingshan Liu.
\newblock Former-dfer: Dynamic facial expression recognition transformer.
\newblock In {\em Proceedings of the 29th ACM International Conference on Multimedia}, pages 1553--1561, 2021.

\bibitem{zhou2019exploring}
Hengshun Zhou, Debin Meng, Yuanyuan Zhang, Xiaojiang Peng, Jun Du, Kai Wang, and Yu~Qiao.
\newblock Exploring emotion features and fusion strategies for audio-video emotion recognition.
\newblock In {\em 2019 International conference on multimodal interaction}, pages 562--566, 2019.

\bibitem{zhu2023minigpt}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock {\em arXiv preprint arXiv:2304.10592}, 2023.

\end{thebibliography}
