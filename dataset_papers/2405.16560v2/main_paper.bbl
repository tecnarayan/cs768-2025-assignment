\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille et~al.(2019)Achille, Lam, Tewari, Ravichandran, Maji, Fowlkes, Soatto, and Perona]{achille2019task2vec}
Achille, A., Lam, M., Tewari, R., Ravichandran, A., Maji, S., Fowlkes, C.~C., Soatto, S., and Perona, P.
\newblock Task2vec: Task embedding for meta-learning.
\newblock In \emph{ICCV}, pp.\  6430--6439, 2019.

\bibitem[Amari(1998)]{amari1998natural}
Amari, S.-I.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural Computation}, pp.\  251--276, 1998.

\bibitem[Bertinetto et~al.(2019)Bertinetto, Henriques, Torr, and Vedaldi]{cifarfs}
Bertinetto, L., Henriques, J.~F., Torr, P., and Vedaldi, A.
\newblock Meta-learning with differentiable closed-form solvers.
\newblock In \emph{ICLR}, 2019.

\bibitem[Caruana(1993)]{caruana1993multitask}
Caruana, R.
\newblock Multitask learning: A knowledge-based source of inductive bias1.
\newblock In \emph{ICML}, pp.\  41--48, 1993.

\bibitem[Cervi{\~n}o et~al.(2023)Cervi{\~n}o, Bazerque, Calvo-Fullana, and Ribeiro]{cervino2023multi}
Cervi{\~n}o, J., Bazerque, J.~A., Calvo-Fullana, M., and Ribeiro, A.
\newblock Multi-task bias-variance trade-off through functional constraints.
\newblock In \emph{ICASSP}, pp.\  1--5, 2023.

\bibitem[Chen et~al.(2021)Chen, Yan, Li, Li, Wang, Zuo, and Zhang]{chen2021variational}
Chen, B., Yan, Z., Li, K., Li, P., Wang, B., Zuo, W., and Zhang, L.
\newblock Variational attention: Propagating domain-specific knowledge for multi-domain learning in crowd counting.
\newblock In \emph{ICCV}, pp.\  16065--16075, 2021.

\bibitem[Chen et~al.(2019)Chen, Wang, Xu, Yang, Liu, Shi, Xu, Xu, and Tian]{chen2019data}
Chen, H., Wang, Y., Xu, C., Yang, Z., Liu, C., Shi, B., Xu, C., Xu, C., and Tian, Q.
\newblock Data-free learning of student networks.
\newblock In \emph{ICCV}, pp.\  3514--3522, 2019.

\bibitem[Chen et~al.(2020)Chen, Zhan, Wu, and Chung]{chen2020variational}
Chen, J., Zhan, L.-M., Wu, X.-M., and Chung, F.-l.
\newblock Variational metric scaling for metric-based meta-learning.
\newblock In \emph{AAAI}, pp.\  3478--3485, 2020.

\bibitem[Dandi et~al.(2022)Dandi, Barba, and Jaggi]{dandi2022implicit}
Dandi, Y., Barba, L., and Jaggi, M.
\newblock Implicit gradient alignment in distributed and federated learning.
\newblock In \emph{AAAI}, pp.\  6454--6462, 2022.

\bibitem[Eshratifar et~al.(2018)Eshratifar, Eigen, and Pedram]{eshratifar2018gradient}
Eshratifar, A.~E., Eigen, D., and Pedram, M.
\newblock Gradient agreement as an optimization objective for meta-learning.
\newblock \emph{arXiv preprint arXiv:1810.08178}, 2018.

\bibitem[Fifty et~al.(2021)Fifty, Amid, Zhao, Yu, Anil, and Finn]{fifty2021efficiently}
Fifty, C., Amid, E., Zhao, Z., Yu, T., Anil, R., and Finn, C.
\newblock Efficiently identifying task groupings for multi-task learning.
\newblock In \emph{NeurIPS}, pp.\  27503--27516, 2021.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{ICML}, pp.\  1126--1135, 2017.

\bibitem[Fredrikson et~al.(2015)Fredrikson, Jha, and Ristenpart]{fredrikson2015model}
Fredrikson, M., Jha, S., and Ristenpart, T.
\newblock Model inversion attacks that exploit confidence information and basic countermeasures.
\newblock In \emph{SIGSAC}, pp.\  1322--1333, 2015.

\bibitem[Han et~al.(2023)Han, Zhan, Yu, Luo, Hu, Du, Wen, and Tao]{han2023region}
Han, M., Zhan, Y., Yu, B., Luo, Y., Hu, H., Du, B., Wen, Y., and Tao, D.
\newblock Region-adaptive concept aggregation for few-shot visual recognition.
\newblock \emph{MIR}, pp.\  554--568, 2023.

\bibitem[Hao et~al.(2023)Hao, Guo, Han, Tang, Hu, Wang, and Xu]{hao2023one}
Hao, Z., Guo, J., Han, K., Tang, Y., Hu, H., Wang, Y., and Xu, C.
\newblock One-for-all: Bridge the gap between heterogeneous architectures in knowledge distillation.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Hermann \& Lampinen(2020)Hermann and Lampinen]{hermann2020shapes}
Hermann, K. and Lampinen, A.
\newblock What shapes feature representations? {E}xploring datasets, architectures, and training.
\newblock In \emph{NeurIPS}, pp.\  9995--10006, 2020.

\bibitem[Hu et~al.(2023{\natexlab{a}})Hu, Shen, Wang, Liu, Yuan, and Tao]{hu2023architecture}
Hu, Z., Shen, L., Wang, Z., Liu, T., Yuan, C., and Tao, D.
\newblock Architecture, dataset and model-scale agnostic data-free meta-learning.
\newblock In \emph{CVPR}, pp.\  7736--7745, 2023{\natexlab{a}}.

\bibitem[Hu et~al.(2023{\natexlab{b}})Hu, Shen, Wang, Wu, Yuan, and Tao]{hu2023learning}
Hu, Z., Shen, L., Wang, Z., Wu, B., Yuan, C., and Tao, D.
\newblock Learning to learn from apis: Black-box data-free meta-learning.
\newblock In \emph{ICML}, 2023{\natexlab{b}}.

\bibitem[Huang et~al.(2019)Huang, Zhou, Peng, Zhang, Zhu, and Lv]{huang2019multi}
Huang, Z., Zhou, J.~T., Peng, X., Zhang, C., Zhu, H., and Lv, J.
\newblock Multi-view spectral clustering network.
\newblock In \emph{IJCAI}, pp.\  2563--2569, 2019.

\bibitem[Jiang et~al.(2023)Jiang, Chen, Pan, Wang, Liu, Long, et~al.]{jiang2023forkmerge}
Jiang, J., Chen, B., Pan, J., Wang, X., Liu, D., Long, M., et~al.
\newblock Forkmerge: Mitigating negative transfer in auxiliary-task learning.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Kornblith et~al.(2019)Kornblith, Norouzi, Lee, and Hinton]{kornblith2019similarity}
Kornblith, S., Norouzi, M., Lee, H., and Hinton, G.
\newblock Similarity of neural network representations revisited.
\newblock In \emph{ICML}, pp.\  3519--3529, 2019.

\bibitem[Kurin et~al.(2022)Kurin, De~Palma, Kostrikov, Whiteson, and Mudigonda]{kurin2022defense}
Kurin, V., De~Palma, A., Kostrikov, I., Whiteson, S., and Mudigonda, P.~K.
\newblock In defense of the unitary scalarization for deep multi-task learning.
\newblock In \emph{NeurIPS}, pp.\  12169--12183, 2022.

\bibitem[Kwon et~al.(2020)Kwon, Na, Huang, and Lacoste-Julien]{kwon2020repurposing}
Kwon, N., Na, H., Huang, G., and Lacoste-Julien, S.
\newblock Repurposing pretrained models for robust out-of-domain few-shot learning.
\newblock In \emph{ICLR}, 2020.

\bibitem[Li et~al.(2023)Li, Peng, Zhang, Ding, Hu, and Shen]{li2023deep}
Li, W., Peng, Y., Zhang, M., Ding, L., Hu, H., and Shen, L.
\newblock Deep model fusion: A survey.
\newblock \emph{arXiv preprint arXiv:2309.15698}, 2023.

\bibitem[Liu et~al.(2020)Liu, Wang, Sahoo, Fang, Zhang, and Hoi]{liu2020adaptive}
Liu, C., Wang, Z., Sahoo, D., Fang, Y., Zhang, K., and Hoi, S.~C.
\newblock Adaptive task sampling for meta-learning.
\newblock In \emph{ECCV}, pp.\  752--769, 2020.

\bibitem[Liu et~al.(2021)Liu, Zhang, Wang, and Wang]{liu2021data}
Liu, Y., Zhang, W., Wang, J., and Wang, J.
\newblock Data-free knowledge transfer: A survey.
\newblock \emph{arXiv preprint arXiv:2112.15278}, 2021.

\bibitem[Maulik \& Bandyopadhyay(2002)Maulik and Bandyopadhyay]{maulik2002performance}
Maulik, U. and Bandyopadhyay, S.
\newblock Performance evaluation of some clustering algorithms and validity indices.
\newblock \emph{TPAMI}, pp.\  1650--1654, 2002.

\bibitem[McAllester(1999)]{mcallester1999pac}
McAllester, D.~A.
\newblock Pac-bayesian model averaging.
\newblock In \emph{COLT}, pp.\  164--170, 1999.

\bibitem[Ng et~al.(2001)Ng, Jordan, and Weiss]{ng2001spectral}
Ng, A., Jordan, M., and Weiss, Y.
\newblock On spectral clustering: Analysis and an algorithm.
\newblock In \emph{NeurIPS}, 2001.

\bibitem[Ravi \& Larochelle(2017)Ravi and Larochelle]{miniimagenet}
Ravi, S. and Larochelle, H.
\newblock Optimization as a model for few-shot learning.
\newblock In \emph{ICLR}, 2017.

\bibitem[Shi et~al.(2022)Shi, Li, Zhang, Chen, and Wu]{guangyuan2022recon}
Shi, G., Li, Q., Zhang, W., Chen, J., and Wu, X.-M.
\newblock Recon: Reducing conflicting gradients from the root for multi-task learning.
\newblock In \emph{ICLR}, 2022.

\bibitem[Shi et~al.(2021)Shi, Seely, Torr, Siddharth, Hannun, Usunier, and Synnaeve]{shi2021gradient}
Shi, Y., Seely, J., Torr, P., Siddharth, N., Hannun, A., Usunier, N., and Synnaeve, G.
\newblock Gradient matching for domain generalization.
\newblock In \emph{ICLR}, 2021.

\bibitem[Singh \& Jaggi(2020)Singh and Jaggi]{singh2020model}
Singh, S.~P. and Jaggi, M.
\newblock Model fusion via optimal transport.
\newblock In \emph{NeurIPS}, pp.\  22045--22055, 2020.

\bibitem[Smith et~al.(2020)Smith, Dherin, Barrett, and De]{smith2020origin}
Smith, S.~L., Dherin, B., Barrett, D., and De, S.
\newblock On the origin of implicit regularization in stochastic gradient descent.
\newblock In \emph{ICLR}, 2020.

\bibitem[Song et~al.(2020)Song, Feng, Luo, Gao, and Li]{song2020robust}
Song, T., Feng, J., Luo, L., Gao, C., and Li, H.
\newblock Robust texture description using local grouped order pattern and non-local binary pattern.
\newblock \emph{TCSVT}, pp.\  189--202, 2020.

\bibitem[Standley et~al.(2020)Standley, Zamir, Chen, Guibas, Malik, and Savarese]{standley2020tasks}
Standley, T., Zamir, A., Chen, D., Guibas, L., Malik, J., and Savarese, S.
\newblock Which tasks should be learned together in multi-task learning?
\newblock In \emph{ICML}, pp.\  9120--9132, 2020.

\bibitem[Truong et~al.(2021)Truong, Maini, Walls, and Papernot]{truong2021data}
Truong, J.-B., Maini, P., Walls, R.~J., and Papernot, N.
\newblock Data-free model extraction.
\newblock In \emph{CVPR}, pp.\  4771--4780, 2021.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and Belongie]{WahCUB200_2011}
Wah, C., Branson, S., Welinder, P., Perona, P., and Belongie, S.
\newblock The {Caltech-UCSD} birds-200-2011 dataset.
\newblock Technical report, California Institute of Technology, 2011.

\bibitem[Wang et~al.(2020)Wang, Zhao, Yu, Zhang, and Chen]{wang2020bayesian}
Wang, Z., Zhao, Y., Yu, P., Zhang, R., and Chen, C.
\newblock Bayesian meta sampling for fast uncertainty adaptation.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Shen, Duan, Zhan, Fang, and Gao]{wang2022learning}
Wang, Z., Shen, L., Duan, T., Zhan, D., Fang, L., and Gao, M.
\newblock Learning to learn and remember super long multi-domain task sequence.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  7982--7992, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Wang, Shen, Suo, Song, Yu, Shen, and Gao]{wang2022meta}
Wang, Z., Wang, X., Shen, L., Suo, Q., Song, K., Yu, D., Shen, Y., and Gao, M.
\newblock Meta-learning without data via wasserstein distributionally-robust model fusion.
\newblock In \emph{UAI}, pp.\  2045--2055, 2022{\natexlab{b}}.

\bibitem[Wei \& Wei(2024)Wei and Wei]{wei2024task}
Wei, Y. and Wei, X.-S.
\newblock Task-specific part discovery for fine-grained few-shot classification.
\newblock \emph{Machine Intelligence Research}, pp.\  954--965, 2024.

\bibitem[Wei et~al.(2024{\natexlab{a}})Wei, Hu, Shen, Wang, Li, Li, and Yuan]{wei2024meta}
Wei, Y., Hu, Z., Shen, L., Wang, Z., Li, L., Li, Y., and Yuan, C.
\newblock Meta-learning without data via unconditional diffusion models.
\newblock \emph{IEEE TCSVT}, 2024{\natexlab{a}}.

\bibitem[Wei et~al.(2024{\natexlab{b}})Wei, Hu, Wang, Shen, Yuan, and Tao]{wei2024free}
Wei, Y., Hu, Z., Wang, Z., Shen, L., Yuan, C., and Tao, D.
\newblock {FREE}: Faster and better data-free meta-learning.
\newblock In \emph{CVPR}, 2024{\natexlab{b}}.

\bibitem[Wu et~al.(2023{\natexlab{a}})Wu, Fang, Zeng, Liang, and Zhang]{wu2023adaptive}
Wu, B., Fang, J., Zeng, X., Liang, S., and Zhang, Q.
\newblock Adaptive compositional continual meta-learning.
\newblock In \emph{ICML}, pp.\  37358--37378, 2023{\natexlab{a}}.

\bibitem[Wu et~al.(2023{\natexlab{b}})Wu, Wang, Ge, Lu, Zhou, Shan, and Luo]{wu2023pi}
Wu, C., Wang, T., Ge, Y., Lu, Z., Zhou, R., Shan, Y., and Luo, P.
\newblock ${\pi}$-tuning: Transferring multimodal foundation models with optimal multi-task interpolation.
\newblock In \emph{ICML}, pp.\  37713--37727, 2023{\natexlab{b}}.

\bibitem[Ye et~al.(2021)Ye, Lin, Yue, Guo, Xiao, and Zhang]{ye2021multi}
Ye, F., Lin, B., Yue, Z., Guo, P., Xiao, Q., and Zhang, Y.
\newblock Multi-objective meta learning.
\newblock In \emph{NeurIPS}, pp.\  21338--21351, 2021.

\bibitem[Yin et~al.(2020)Yin, Molchanov, Alvarez, Li, Mallya, Hoiem, Jha, and Kautz]{yin2020dreaming}
Yin, H., Molchanov, P., Alvarez, J.~M., Li, Z., Mallya, A., Hoiem, D., Jha, N.~K., and Kautz, J.
\newblock Dreaming to distill: Data-free knowledge transfer via deepinversion.
\newblock In \emph{CVPR}, pp.\  8715--8724, 2020.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and Finn]{yu2020gradient}
Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C.
\newblock Gradient surgery for multi-task learning.
\newblock In \emph{NeurIPS}, pp.\  5824--5836, 2020.

\bibitem[Zamir et~al.(2018)Zamir, Sax, Shen, Guibas, Malik, and Savarese]{zamir2018taskonomy}
Zamir, A.~R., Sax, A., Shen, W., Guibas, L.~J., Malik, J., and Savarese, S.
\newblock Taskonomy: Disentangling task transfer learning.
\newblock In \emph{CVPR}, pp.\  3712--3722, 2018.

\bibitem[Zhang \& Yang(2021)Zhang and Yang]{zhang2021survey}
Zhang, Y. and Yang, Q.
\newblock A survey on multi-task learning.
\newblock \emph{TKDE}, pp.\  5586--5609, 2021.

\bibitem[Zhao et~al.(2022)Zhao, Zhang, and Hu]{zhao2022penalizing}
Zhao, Y., Zhang, H., and Hu, X.
\newblock Penalizing gradient norm for efficiently improving generalization in deep learning.
\newblock In \emph{ICML}, pp.\  26982--26992, 2022.

\bibitem[Zheng et~al.(2023)Zheng, Shen, Tang, Luo, Hu, Du, and Tao]{zheng2023learn}
Zheng, H., Shen, L., Tang, A., Luo, Y., Hu, H., Du, B., and Tao, D.
\newblock Learn from model beyond fine-tuning: A survey.
\newblock \emph{arXiv preprint arXiv:2310.08184}, 2023.

\end{thebibliography}
