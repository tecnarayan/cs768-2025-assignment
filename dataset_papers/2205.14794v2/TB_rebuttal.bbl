\begin{thebibliography}{10}

\bibitem{agarwal2019striving}
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
\newblock Striving for simplicity in off-policy deep reinforcement learning.
\newblock {\em CoRR}, abs/1907.04543, 2019.

\bibitem{ahmed2021sit}
Sara Atito~Ali Ahmed, Muhammad Awais, and Josef Kittler.
\newblock Sit: Self-supervised vision transformer.
\newblock {\em CoRR}, abs/2104.03602, 2021.

\bibitem{atkinson1971control}
Richard~C Atkinson and Richard~M Shiffrin.
\newblock The control of short-term memory.
\newblock {\em Scientific american}, 225(2):82--91, 1971.

\bibitem{averbach1961short}
Emanuel Averbach and Abner~S Coriell.
\newblock Short-term memory in vision.
\newblock {\em The Bell System Technical Journal}, 40(1):309--328, 1961.

\bibitem{DBLP:conf/nips/BaHMLI16}
Jimmy Ba, Geoffrey~E. Hinton, Volodymyr Mnih, Joel~Z. Leibo, and Catalin
  Ionescu.
\newblock Using fast weights to attend to the recent past.
\newblock In Daniel~D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle
  Guyon, and Roman Garnett, editors, {\em Advances in Neural Information
  Processing Systems 29: Annual Conference on Neural Information Processing
  Systems 2016, December 5-10, 2016, Barcelona, Spain}, pages 4331--4339, 2016.

\bibitem{baddeley1984attention}
Alan Baddeley, Vivien Lewis, Margery Eldridge, and Neil Thomson.
\newblock Attention and retrieval from long-term memory.
\newblock {\em Journal of Experimental Psychology: General}, 113(4):518, 1984.

\bibitem{Bahdanau2015NeuralMT}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em CoRR}, abs/1409.0473, 2015.

\bibitem{bao2021beit}
Hangbo Bao, Li~Dong, and Furu Wei.
\newblock Beit: {BERT} pre-training of image transformers.
\newblock {\em CoRR}, abs/2106.08254, 2021.

\bibitem{atari-env}
Marc~G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock {\em CoRR}, abs/1207.4708, 2012.

\bibitem{longformer}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock {\em CoRR}, abs/2004.05150, 2020.

\bibitem{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock {\em CoRR}, abs/2005.14165, 2020.

\bibitem{caron2021emerging}
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv{\'{e}} J{\'{e}}gou, Julien
  Mairal, Piotr Bojanowski, and Armand Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock {\em CoRR}, abs/2104.14294, 2021.

\bibitem{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em CoRR}, abs/2106.01345, 2021.

\bibitem{simclr}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey~E. Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock {\em CoRR}, abs/2002.05709, 2020.

\bibitem{babyai-env}
Maxime Chevalier{-}Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems,
  Chitwan Saharia, Thien~Huu Nguyen, and Yoshua Bengio.
\newblock Babyai: First steps towards grounded language learning with a human
  in the loop.
\newblock {\em CoRR}, abs/1810.08272, 2018.

\bibitem{maxime2018babyai}
Maxime Chevalier{-}Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems,
  Chitwan Saharia, Thien~Huu Nguyen, and Yoshua Bengio.
\newblock Babyai: First steps towards grounded language learning with a human
  in the loop.
\newblock {\em CoRR}, abs/1810.08272, 2018.

\bibitem{sparse-transformer}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em CoRR}, abs/1904.10509, 2019.

\bibitem{cho2014gru}
Kyunghyun Cho, Bart van Merrienboer, {\c{C}}aglar G{\"{u}}l{\c{c}}ehre, Fethi
  Bougares, Holger Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using {RNN} encoder-decoder for
  statistical machine translation.
\newblock {\em CoRR}, abs/1406.1078, 2014.

\bibitem{Performers}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tam{\'{a}}s Sarl{\'{o}}s, Peter Hawkins, Jared Davis, Afroz
  Mohiuddin, Lukasz Kaiser, David Belanger, Lucy~J. Colwell, and Adrian Weller.
\newblock Rethinking attention with performers.
\newblock {\em CoRR}, abs/2009.14794, 2020.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,
  Abhishek Rao, Parker Barnes, Yi~Tay, Noam Shazeer, Vinodkumar Prabhakaran,
  Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob
  Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm
  Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia,
  Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David
  Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David
  Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai,
  Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica
  Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
  Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,
  Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.
\newblock Palm: Scaling language modeling with pathways, 2022.

\bibitem{chung2016hierarchical}
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
\newblock Hierarchical multiscale recurrent neural networks.
\newblock {\em CoRR}, abs/1609.01704, 2016.

\bibitem{pmlr-v15-coates11a}
Adam Coates, Andrew Ng, and Honglak Lee.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In Geoffrey Gordon, David Dunson, and Miroslav Dudík, editors, {\em
  Proceedings of the Fourteenth International Conference on Artificial
  Intelligence and Statistics}, volume~15 of {\em Proceedings of Machine
  Learning Research}, pages 215--223, Fort Lauderdale, FL, USA, 11--13 Apr
  2011. PMLR.

\bibitem{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime~G. Carbonell, Quoc~V. Le, and
  Ruslan Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock {\em CoRR}, abs/1901.02860, 2019.

\bibitem{devlin2018bert}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em CoRR}, abs/1810.04805, 2018.

\bibitem{dosovitskiy2020vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em CoRR}, abs/2010.11929, 2020.

\bibitem{fan2021addressing}
Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar
  Sukhbaatar.
\newblock Addressing some limitations of transformers with feedback memory,
  2021.

\bibitem{galashov2019information}
Alexandre Galashov, Siddhant~M Jayakumar, Leonard Hasenclever, Dhruva Tirumala,
  Jonathan Schwarz, Guillaume Desjardins, Wojciech~M Czarnecki, Yee~Whye Teh,
  Razvan Pascanu, and Nicolas Heess.
\newblock Information asymmetry in kl-regularized rl.
\newblock {\em arXiv preprint arXiv:1905.01240}, 2019.

\bibitem{goelet1986long}
Philip Goelet, Vincent~F Castellucci, Samuel Schacher, and Eric~R Kandel.
\newblock The long and the short of long--term memory—a molecular framework.
\newblock {\em Nature}, 322(6078):419--422, 1986.

\bibitem{goyal2021coordination}
Anirudh Goyal, Aniket Didolkar, Alex Lamb, Kartikeya Badola, Nan~Rosemary Ke,
  Nasim Rahaman, Jonathan Binas, Charles Blundell, Michael Mozer, and Yoshua
  Bengio.
\newblock Coordination among neural modules through a shared global workspace.
\newblock {\em CoRR}, abs/2103.01197, 2021.

\bibitem{goyal2019infobot}
Anirudh Goyal, Riashat Islam, Daniel Strouse, Zafarali Ahmed, Matthew
  Botvinick, Hugo Larochelle, Yoshua Bengio, and Sergey Levine.
\newblock Infobot: Transfer and exploration via the information bottleneck.
\newblock {\em arXiv preprint arXiv:1901.10902}, 2019.

\bibitem{goyal2019recurrent}
Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine,
  Yoshua Bengio, and Bernhard Sch{\"{o}}lkopf.
\newblock Recurrent independent mechanisms.
\newblock {\em CoRR}, abs/1909.10893, 2019.

\bibitem{he2021masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'{a}}r, and
  Ross~B. Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock {\em CoRR}, abs/2111.06377, 2021.

\bibitem{hihi1995hierarchical}
Salah Hihi and Yoshua Bengio.
\newblock Hierarchical recurrent neural networks for long-term dependencies.
\newblock In D.~Touretzky, M.~C. Mozer, and M.~Hasselmo, editors, {\em Advances
  in Neural Information Processing Systems}, volume~8. MIT Press, 1995.

\bibitem{hill2020grounded}
Felix Hill, Olivier Tieleman, Tamara Von~Glehn, Nathaniel Wong, Hamza Merzic,
  and Stephen Clark.
\newblock Grounded language learning fast and slow.
\newblock {\em arXiv preprint arXiv:2009.01719}, 2020.

\bibitem{Hinton87usingfast}
Geoffrey~E. Hinton and David~C. Plaut.
\newblock Using fast weights to deblur old memories.
\newblock In {\em IN PROCEEDINGS OF THE 9TH ANNUAL CONFERENCE OF THE COGNITIVE
  SCIENCE SOCIETY}, pages 177--186. Erlbaum, 1987.

\bibitem{hochrieter1997long}
Sepp Hochreiter and Jürgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9:1735--80, 12 1997.

\bibitem{delesley2022block}
DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur.
\newblock Block-recurrent transformers, 2022.

\bibitem{iii2022improving}
Donald Joseph~Hejna III, Pieter Abbeel, and Lerrel Pinto.
\newblock Improving long-horizon imitation through language prediction, 2022.

\bibitem{jaegle2021perceiver}
Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and
  Joao Carreira.
\newblock Perceiver: General perception with iterative attention, 2021.

\bibitem{janner2021reinforcement}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Reinforcement learning as one big sequence modeling problem.
\newblock {\em CoRR}, abs/2106.02039, 2021.

\bibitem{jeneson2012working}
Annette Jeneson and Larry~R Squire.
\newblock Working memory, long-term memory, and medial temporal lobe function.
\newblock {\em Learning \& memory}, 19(1):15--25, 2012.

\bibitem{jonides2008mind}
John Jonides, Richard~L Lewis, Derek~Evan Nee, Cindy~A Lustig, Marc~G Berman,
  and Katherine~Sledge Moore.
\newblock The mind and brain of short-term memory.
\newblock {\em Annu. Rev. Psychol.}, 59:193--224, 2008.

\bibitem{linear_transformer}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock {\em CoRR}, abs/2006.16236, 2020.

\bibitem{reformer}
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock {\em CoRR}, abs/2001.04451, 2020.

\bibitem{kolodner1983maintaining}
Janet~L Kolodner.
\newblock Maintaining organization in a dynamic long-term memory.
\newblock {\em Cognitive science}, 7(4):243--280, 1983.

\bibitem{jan2014clockwork}
Jan Koutn{\'{\i}}k, Klaus Greff, Faustino~J. Gomez, and J{\"{u}}rgen
  Schmidhuber.
\newblock A clockwork {RNN}.
\newblock {\em CoRR}, abs/1402.3511, 2014.

\bibitem{Krizhevsky09learningmultiple}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem{li2021efficient}
Chunyuan Li, Jianwei Yang, Pengchuan Zhang, Mei Gao, Bin Xiao, Xiyang Dai,
  Lu~Yuan, and Jianfeng Gao.
\newblock Efficient self-supervised vision transformers for representation
  learning.
\newblock {\em CoRR}, abs/2106.09785, 2021.

\bibitem{li2021mst}
Zhaowen Li, Zhiyang Chen, Fan Yang, Wei Li, Yousong Zhu, Chaoyang Zhao, Rui
  Deng, Liwei Wu, Rui Zhao, Ming Tang, and Jinqiao Wang.
\newblock {MST:} masked self-supervised transformer for visual representation.
\newblock {\em CoRR}, abs/2106.05656, 2021.

\bibitem{swinv2}
Ze~Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue
  Cao, Zheng Zhang, Li~Dong, Furu Wei, and Baining Guo.
\newblock Swin transformer {V2:} scaling up capacity and resolution.
\newblock {\em CoRR}, abs/2111.09883, 2021.

\bibitem{lu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock {\em CoRR}, abs/2103.14030, 2021.

\bibitem{mittal2020learning}
Sarthak Mittal, Alex Lamb, Anirudh Goyal, Vikram Voleti, Murray Shanahan,
  Guillaume Lajoie, Michael Mozer, and Yoshua Bengio.
\newblock Learning to combine top-down and bottom-up signals in recurrent
  neural networks with attention over modules.
\newblock {\em CoRR}, abs/2006.16981, 2020.

\bibitem{mozer1991induction}
Michael~C Mozer.
\newblock Induction of multiscale temporal structure.
\newblock In J.~Moody, S.~Hanson, and R.~P. Lippmann, editors, {\em Advances in
  Neural Information Processing Systems}, volume~4. Morgan-Kaufmann, 1991.

\bibitem{Radford2018ImprovingLU}
Alec Radford and Karthik Narasimhan.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem{rae2022scaling}
Jack~W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell,
  George van~den Driessche, Lisa~Anne Hendricks, Maribeth Rauh, Po-Sen Huang,
  Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
  Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,
  Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme
  Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens,
  Xiang~Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,
  Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau,
  Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas
  Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de~Masson
  d'Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan
  Clark, Diego de~Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew
  Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac,
  Ed~Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem
  Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and
  Geoffrey Irving.
\newblock Scaling language models: Methods, analysis; insights from training
  gopher, 2021.

\bibitem{Schmidhuber91neuralsequence}
Jürgen Schmidhuber.
\newblock Neural sequence chunkers.
\newblock Technical report, 1991.

\bibitem{schuster1997bidirection}
M.~Schuster and K.K. Paliwal.
\newblock Bidirectional recurrent neural networks.
\newblock {\em IEEE Transactions on Signal Processing}, 45(11):2673--2681,
  1997.

\bibitem{synthesizers}
Yi~Tay, Dara Bahri, Donald Metzler, Da{-}Cheng Juan, Zhe Zhao, and Che Zheng.
\newblock Synthesizer: Rethinking self-attention in transformer models.
\newblock {\em CoRR}, abs/2005.00743, 2020.

\bibitem{sinkhorn-transformer}
Yi~Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da{-}Cheng Juan.
\newblock Sparse sinkhorn attention.
\newblock {\em CoRR}, abs/2002.11296, 2020.

\bibitem{yi2020long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: {A} benchmark for efficient transformers.
\newblock {\em CoRR}, abs/2011.04006, 2020.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em CoRR}, abs/1706.03762, 2017.

\bibitem{linformer}
Sinong Wang, Belinda~Z. Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em CoRR}, abs/2006.04768, 2020.

\bibitem{wang2021pyramid}
Wenhai Wang, Enze Xie, Xiang Li, Deng{-}Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: {A} versatile backbone for dense
  prediction without convolutions.
\newblock {\em CoRR}, abs/2102.12122, 2021.

\bibitem{wu2021cvt}
Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu~Yuan, and Lei
  Zhang.
\newblock Cvt: Introducing convolutions to vision transformers.
\newblock {\em CoRR}, abs/2103.15808, 2021.

\bibitem{yang2021focal}
Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu~Yuan, and
  Jianfeng Gao.
\newblock Focal self-attention for local-global interactions in vision
  transformers.
\newblock {\em CoRR}, abs/2107.00641, 2021.

\bibitem{yuan2021incorporating}
Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu.
\newblock Incorporating convolution designs into visual transformers.
\newblock {\em CoRR}, abs/2103.11816, 2021.

\bibitem{bigbird}
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti,
  Santiago Onta{\~{n}}{\'{o}}n, Philip Pham, Anirudh Ravula, Qifan Wang,
  Li~Yang, and Amr Ahmed.
\newblock Big bird: Transformers for longer sequences.
\newblock {\em CoRR}, abs/2007.14062, 2020.

\bibitem{zhang2021multiscale}
Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu~Yuan, Lei Zhang, and
  Jianfeng Gao.
\newblock Multi-scale vision longformer: {A} new vision transformer for
  high-resolution image encoding.
\newblock {\em CoRR}, abs/2103.15358, 2021.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, Todor Mihaylov,
  Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit~Singh Koura, Anjali
  Sridhar, Tianlu Wang, and Luke Zettlemoyer.
\newblock Opt: Open pre-trained transformer language models, 2022.

\bibitem{zhu2021long}
Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima
  Anandkumar, and Bryan Catanzaro.
\newblock Long-short transformer: Efficient transformers for language and
  vision.
\newblock {\em CoRR}, abs/2107.02192, 2021.

\end{thebibliography}
