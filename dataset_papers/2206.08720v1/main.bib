@article{park2020towards,
    title={Towards nngp-guided neural architecture search},
    author={Park, Daniel S and Lee, Jaehoon and Peng, Daiyi and Cao, Yuan and Sohl-Dickstein, Jascha},
    journal={arXiv preprint arXiv:2011.06006},
    year={2020}
}

@inproceedings{garriga2018deep,
    title={Deep convolutional networks as shallow Gaussian processes},
    author={Garriga-Alonso, Adri{\`a} and Aitchison, Laurence and Rasmussen, Carl Edward},
    booktitle={International Conference on Learning Representations},
    year={2019}
}

@article{neal,
    author={Neal, Radford M.},
    title={Priors for infinite networks (Tech. Rep. No. CRG-TR-94-1)},
    journal={University of Toronto},
    year={1994},
}

@article{schoenholz2016deep,
    title={Deep Information Propagation},
    author={Schoenholz, Samuel S and Gilmer, Justin and Ganguli, Surya and Sohl-Dickstein, Jascha},
    journal={International Conference on Learning Representations},
    year={2017}
}

@inproceedings{
    lee2018deep,
    title={Deep Neural Networks as Gaussian Processes},
    author={Jaehoon Lee and Yasaman Bahri and Roman Novak and Sam Schoenholz and Jeffrey Pennington and Jascha Sohl-dickstein},
    booktitle={International Conference on Learning Representations},
    year={2018},
}

@inproceedings{matthews2018,
    title={Gaussian Process Behaviour in Wide Deep Neural Networks},
    author={Alexander{\ }G.{\ }de{\ }G. Matthews and Jiri Hron and Mark Rowland and Richard E. Turner and Zoubin Ghahramani},
    booktitle={International Conference on Learning Representations},
    year={2018},
}

@inproceedings{Jacot2018ntk,
    title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
    author={Arthur Jacot and Franck Gabriel and Clement Hongler},
    booktitle = {Advances in Neural Information Processing Systems},
    year={2018}
}

@inproceedings{novak2018bayesian,
    title={Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes},
    author={Roman Novak and Lechao Xiao and Jaehoon Lee and Yasaman Bahri and Greg Yang and Jiri Hron and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-Dickstein},
    booktitle={International Conference on Learning Representations},
    year={2019}
}

@InProceedings{xiao18a,
    title = 	 {Dynamical Isometry and a Mean Field Theory of {CNN}s: How to Train 10,000-Layer Vanilla Convolutional Neural Networks},
    author = 	 {Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel and Pennington, Jeffrey},
    booktitle = 	 {International Conference on Machine Learning},
    year = 	 {2018},
}

@inproceedings{yang2018a,
    title={A Mean Field Theory of Batch Normalization},
    author={Greg Yang and Jeffrey Pennington and Vinay Rao and Jascha Sohl-Dickstein and Samuel S. Schoenholz},
    booktitle={International Conference on Learning Representations},
    year={2019},    
}

@inproceedings{zagoruyko2016wide,
    title={Wide residual networks},
    author={Zagoruyko, Sergey and Komodakis, Nikos},
    booktitle={British Machine Vision Conference},
    year={2016}
}

@article{yang2019scaling,
    title={Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation},
    author={Yang, Greg},
    journal={arXiv preprint arXiv:1902.04760},
    year={2019}
}

@inproceedings{abadi2016tensorflow,
    title={Tensorflow: A system for large-scale machine learning},
    author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
    booktitle={12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)},
    year={2016}
}

@article{du2019graph,
    title={Graph neural tangent kernel: Fusing graph neural networks with graph kernels},
    author={Du, Simon S and Hou, Kangcheng and Salakhutdinov, Russ R and Poczos, Barnabas and Wang, Ruosong and Xu, Keyulu},
    journal={Advances in neural information processing systems},
    volume={32},
    year={2019}
}

@inproceedings{hron2020,
    author = {Jiri Hron and Yasaman Bahri and Jascha Sohl-Dickstein and Roman Novak},
    year = "2020",
    booktitle = "International Conference on Machine Learning",
    title = "Infinite attention: {NNGP} and {NTK} for deep attention networks",
}

@inproceedings{lee2019wide,
    title={Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent},
    author={Lee, Jaehoon and Xiao, Lechao and  Schoenholz, Samuel S. and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
    booktitle={Advances in Neural Information Processing Systems},
    year={2019},
}

@incollection{arora2019on,
    title = {On Exact Computation with an Infinitely Wide Neural Net},
    author = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
    booktitle = {Advances in Neural Information Processing Systems},
    pages = {8141--8150},
    year = {2019},
    publisher = {Curran Associates, Inc.},
}

@inproceedings{neuraltangents2020,
    title={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},
    author={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://github.com/google/neural-tangents}
}

@inproceedings{xiao2019disentangling,
    title={Disentangling trainability and generalization in deep learning},
    author={Xiao, Lechao and Pennington, Jeffrey and Schoenholz, Samuel S},
    booktitle={International Conference on Machine Learning},
    year={2020}
}

@inproceedings{Arora2020Harnessing,
    title={Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks},
    author={Sanjeev Arora and Simon S. Du and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang and Dingli Yu},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=rkl8sJBYvH}
}

@misc{jax2018github,
    author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and Skye Wanderman-Milne},
    title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
    url = {http://github.com/google/jax},
    version = {0.1.46},
    year = {2018},
}

@article{belkin2019reconciling,
    title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
    author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
    journal={Proceedings of the National Academy of Sciences},
    volume={116},
    number={32},
    pages={15849--15854},
    year={2019},
    publisher={National Acad Sciences}
}


@article{arora2019fine,
    title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
    author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
    journal={arXiv preprint arXiv:1901.08584},
    year={2019}
}

@article{Hu2020InfinitelyWG,
    title={Infinitely wide graph convolutional networks: semi-supervised learning via Gaussian processes},
    author={Hu, Jilin and Shen, Jianbing and Yang, Bin and Shao, Ling},
    journal={arXiv preprint arXiv:2002.12168},
    year={2020}
}

@inproceedings{yaida2019non,
    title={Non-{G}aussian processes and neural networks at finite widths},
    author={Yaida, Sho},
    booktitle={Mathematical and Scientific Machine Learning Conference},
    year={2020}
}

@inproceedings{khan2019approximate,
    title={Approximate inference turns deep networks into gaussian processes},
    author={Khan, Mohammad Emtiyaz E and Immer, Alexander and Abedi, Ehsan and Korzepa, Maciej},
    booktitle={Advances in neural information processing systems},
    year={2019}
}


@inproceedings{zhou2021metalearning,
    title={Meta-Learning with Neural Tangent Kernels},
    author={Yufan Zhou and Zhenyi Wang and Jiayi Xian and Changyou Chen and Jinhui Xu},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=Ti87Pv5Oc8}
}

@misc{chen2021vision,
    title={When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations}, 
    author={Xiangning Chen and Cho-Jui Hsieh and Boqing Gong},
    year={2021},
    eprint={2106.01548},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{deng2009imagenet,
    title={Imagenet: A large-scale hierarchical image database},
    author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
    booktitle={2009 IEEE conference on computer vision and pattern recognition},
    pages={248--255},
    year={2009},
    organization={Ieee}
}

@misc{tolstikhin2021mlpmixer,
    title={MLP-Mixer: An all-MLP Architecture for Vision}, 
    author={Ilya Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Andreas Steiner and Daniel Keysers and Jakob Uszkoreit and Mario Lucic and Alexey Dosovitskiy},
    year={2021},
    eprint={2105.01601},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{he2016deep,
    title={Deep residual learning for image recognition},
    author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
    pages={770--778},
    year={2016}
}

@inproceedings{
    dosovitskiy2021an,
    title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{Oreshkin2018TADAMTD,
    title={TADAM: Task dependent adaptive metric for improved few-shot learning},
    author={Boris N. Oreshkin and Pau Rodr{\'i}guez L{\'o}pez and Alexandre Lacoste},
    booktitle={NeurIPS},
    year={2018}
}


@InProceedings{pmlr-v70-finn17a,
    title = 	 {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
    author =       {Chelsea Finn and Pieter Abbeel and Sergey Levine},
    booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
    pages = 	 {1126--1135},
    year = 	 {2017},
    editor = 	 {Precup, Doina and Teh, Yee Whye},
    volume = 	 {70},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {06--11 Aug},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf},
    url = 	 {https://proceedings.mlr.press/v70/finn17a.html},
    abstract = 	 {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.}
}

@inproceedings{chen2020tenas,
    title={Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective},
    author={Chen, Wuyang and Gong, Xinyu and Wang, Zhangyang},
    booktitle={International Conference on Learning Representations},
    year={2021}
}

@article{lee2020finite,
    title={Finite versus infinite neural networks: an empirical study},
    author={Lee, Jaehoon and Schoenholz, Samuel and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
    journal={Advances in Neural Information Processing Systems},
    volume={33},
    pages={15156--15172},
    year={2020}
}

@inproceedings{Hanin2020Finite,
    title={Finite Depth and Width Corrections to the Neural Tangent Kernel},
    author={Boris Hanin and Mihai Nica},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=SJgndT4KwB}
}

@misc{zoph2017neural,
    abstract = {Neural networks are powerful and flexible models that work well for many
    difficult learning tasks in image, speech and natural language understanding.
    Despite their success, neural networks are still hard to design. In this paper,
    we use a recurrent network to generate the model descriptions of neural
    networks and train this RNN with reinforcement learning to maximize the
    expected accuracy of the generated architectures on a validation set. On the
    CIFAR-10 dataset, our method, starting from scratch, can design a novel network
    architecture that rivals the best human-invented architecture in terms of test
    set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is
    0.09 percent better and 1.05x faster than the previous state-of-the-art model
    that used a similar architectural scheme. On the Penn Treebank dataset, our
    model can compose a novel recurrent cell that outperforms the widely-used LSTM
    cell, and other state-of-the-art baselines. Our cell achieves a test set
    perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than
    the previous state-of-the-art model. The cell can also be transferred to the
    character language modeling task on PTB and achieves a state-of-the-art
    perplexity of 1.214.},
    added-at = {2017-10-18T16:53:38.000+0200},
    author = {Zoph, Barret and Le, Quoc V.},
    biburl = {https://www.bibsonomy.org/bibtex/24369f93009a6f577154235988b5139dc/achakraborty},
    description = {[1611.01578] Neural Architecture Search with Reinforcement Learning},
    interhash = {39ee88dbc3d50930a602050b3684b6d6},
    intrahash = {4369f93009a6f577154235988b5139dc},
    keywords = {2016 deep-learning neural-networks reinforcement-learning},
    timestamp = {2017-10-18T16:53:38.000+0200},
    title = {Neural Architecture Search with Reinforcement Learning},
    url = {http://arxiv.org/abs/1611.01578},
    year = 2016
}

@article{frostig2021decomposing,
    title={Decomposing reverse-mode automatic differentiation},
    author={Frostig, Roy and Johnson, Matthew J and Maclaurin, Dougal and Paszke, Adam and Radul, Alexey},
    journal={arXiv preprint arXiv:2105.09469},
    year={2021}
}

@article{tancik2020fourfeat,
    title={Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},
    author={Matthew Tancik and Pratul P. Srinivasan and Ben Mildenhall and Sara Fridovich-Keil and Nithin Raghavan and Utkarsh Singhal and Ravi Ramamoorthi and Jonathan T. Barron and Ren Ng},
    journal={NeurIPS},
    year={2020}
}

@inproceedings{he2020bayesian,
    author    = {Bobby He and
               Balaji Lakshminarayanan and
               Yee Whye Teh},
    editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
    title     = {Bayesian Deep Ensembles via the Neural Tangent Kernel},
    booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
    year      = {2020},
    url       = {https://proceedings.neurips.cc/paper/2020/hash/0b1ec366924b26fc98fa7b71a9c249cf-Abstract.html},
    timestamp = {Tue, 19 Jan 2021 15:57:46 +0100},
    biburl    = {https://dblp.org/rec/conf/nips/HeLT20.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{adlam2020exploring,
    title={Exploring the Uncertainty Properties of Neural Networks’ Implicit Priors in the Infinite-Width Limit},
    author={Adlam, Ben and Lee, Jaehoon and Xiao, Lechao and Pennington, Jeffrey and Snoek, Jasper},
    booktitle={International Conference on Learning Representations},
    year={2020}
}

@article{steiner2021augreg,
    title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},
    author={Steiner, Andreas and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
    journal={arXiv preprint arXiv:2106.10270},
    year={2021}
}

@misc{flax2020github,
    author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
    title = {{F}lax: A neural network library and ecosystem for {JAX}},
    url = {http://github.com/google/flax},
    version = {0.3.5},
    year = {2020},
}

@article{spigler2019jamming,
    title={A jamming transition from under-to over-parametrization affects generalization in deep learning},
    author={Spigler, Stefano and Geiger, Mario and d’Ascoli, St{\'e}phane and Sagun, Levent and Biroli, Giulio and Wyart, Matthieu},
    journal={Journal of Physics A: Mathematical and Theoretical},
    volume={52},
    number={47},
    pages={474001},
    year={2019},
    publisher={IOP Publishing}
}

@article{zhang2019fixup,
    title={Fixup initialization: Residual learning without normalization},
    author={Zhang, Hongyi and Dauphin, Yann N and Ma, Tengyu},
    journal={arXiv preprint arXiv:1901.09321},
    year={2019}
}

@article{dauphin2019metainit,
    title={MetaInit: Initializing learning by learning to initialize},
    author={Dauphin, Yann N and Schoenholz, Samuel},
    journal={Advances in Neural Information Processing Systems},
    volume={32},
    year={2019}
}

@article{brock2021characterizing,
    title={Characterizing signal propagation to close the performance gap in unnormalized ResNets},
    author={Brock, Andrew and De, Soham and Smith, Samuel L},
    journal={arXiv preprint arXiv:2101.08692},
    year={2021}
}

@article{brock2021high,
    title={High-performance large-scale image recognition without normalization},
    author={Brock, Andrew and De, Soham and Smith, Samuel L and Simonyan, Karen},
    journal={arXiv preprint arXiv:2102.06171},
    year={2021}
}

@article{franceschi2021neural,
    title={A Neural Tangent Kernel Perspective of GANs},
    author={Franceschi, Jean-Yves and de B{\'e}zenac, Emmanuel and Ayed, Ibrahim and Chen, Micka{\"e}l and Lamprier, Sylvain and Gallinari, Patrick},
    journal={arXiv preprint arXiv:2106.05566},
    year={2021}
}

@article{bahri2021explaining,
    title={Explaining neural scaling laws},
    author={Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
    journal={arXiv preprint arXiv:2102.06701},
    year={2021}
}

@article{nguyen2020dataset,
    title={Dataset Meta-Learning from Kernel Ridge-Regression},
    author={Nguyen, Timothy and Chen, Zhourong and Lee, Jaehoon},
    journal={arXiv preprint arXiv:2011.00050},
    year={2020}
}

@article{nguyen2021dataset,
    title={Dataset Distillation with Infinitely Wide Convolutional Networks},
    author={Nguyen, Timothy and Novak, Roman and Xiao, Lechao and Lee, Jaehoon},
    journal={arXiv preprint arXiv:2107.13034},
    year={2021}
}

@article{naumann2008optimal,
    title={Optimal Jacobian accumulation is NP-complete},
    author={Naumann, Uwe},
    journal={Mathematical Programming},
    volume={112},
    number={2},
    pages={427--441},
    year={2008},
    publisher={Springer}
}

@article{naumann2004optimal,
    title={Optimal accumulation of Jacobian matrices by elimination methods on the dual computational graph},
    author={Naumann, Uwe},
    journal={Mathematical Programming},
    volume={99},
    number={3},
    pages={399--421},
    year={2004},
    publisher={Springer}
}

@inproceedings{maclaurin2015autograd,
    title = {Autograd: Effortless Gradients in Numpy},
    author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P},
    year = {2015},
    booktitle = {ICML 2015 AutoML Workshop},
    url = {https://github.com/HIPS/autograd},
    keywords = {knet},
}


@book{evaluating_derivatives,
	author = {Griewank, Andreas and Walther, Andrea},
	doi = {10.1137/1.9780898717761},
	edition = {Second},
	eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9780898717761},
	publisher = {Society for Industrial and Applied Mathematics},
	title = {Evaluating Derivatives},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9780898717761},
	year = {2008},
	Bdsk-Url-1 = {https://epubs.siam.org/doi/abs/10.1137/1.9780898717761},
	Bdsk-Url-2 = {https://doi.org/10.1137/1.9780898717761}
}

@article{powermethod,
    author = {Herman Müntz},
    year = {1913},
    title = {Solution directe de l’équation séculaire et de quelques problèmes analogues transcendants},
    journal = {C. R. Acad. Sci. Paris},
    volume = {156}, 
    pages = {43-46}
}

@incollection{pytorch,
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
    booktitle = {Advances in Neural Information Processing Systems 32},
    editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
    pages = {8024--8035},
    year = {2019},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@article{yang2020tensor,
  title={Tensor programs ii: Neural tangent kernel for any architecture},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:2006.14548},
  year={2020}
}

@misc{hron2020exact,
    title={Exact posterior distributions of wide Bayesian neural networks}, 
    author={Jiri Hron and Yasaman Bahri and Roman Novak and Jeffrey Pennington and Jascha Sohl-Dickstein},
    year={2020},
    eprint={2006.10541},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{haiku2020github,
    author = {Tom Hennigan and Trevor Cai and Tamara Norman and Igor Babuschkin},
    title = {{H}aiku: {S}onnet for {JAX}},
    url = {http://github.com/deepmind/dm-haiku},
    version = {0.0.3},
    year = {2020},
}

@inproceedings{martens2015optimizing,
    title={Optimizing neural networks with kronecker-factored approximate curvature},
    author={Martens, James and Grosse, Roger},
    booktitle={International conference on machine learning},
    pages={2408--2417},
    year={2015},
    organization={PMLR}
}

@misc{grosse2021neural,
    author        = {Roger Grosse},
    title         = {Neural Net Training Dynamics},
    month         = {January},
    year          = {2021},
    publisher={University of Toronto},
    url = {https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/readings/L02_Taylor_approximations.pdf}
}

@InProceedings{pmlr-v70-pennington17a,
    title = 	 {Geometry of Neural Network Loss Surfaces via Random Matrix Theory},
    author =       {Jeffrey Pennington and Yasaman Bahri},
    booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
    pages = 	 {2798--2806},
    year = 	 {2017},
    editor = 	 {Precup, Doina and Teh, Yee Whye},
    volume = 	 {70},
    series = 	 {Proceedings of Machine Learning Research},
    month = 	 {06--11 Aug},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v70/pennington17a/pennington17a.pdf},
    url = 	 {https://proceedings.mlr.press/v70/pennington17a.html},
    abstract = 	 {Understanding the geometry of neural network loss surfaces is important for the development of improved optimization algorithms and for building a theoretical understanding of why deep learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to compute an approximation of this distribution under a set of simplifying assumptions. The shape of the spectrum depends strongly on the energy and another key parameter, $\phi$, which measures the ratio of parameters to data points. Our analysis predicts and numerical simulations support that for critical points of small index, the number of negative eigenvalues scales like the 3/2 power of the energy. We leave as an open problem an explanation for our observation that, in the context of a certain memorization task, the energy of minimizers is well-approximated by the function $1/2(1-\phi)^2$.}
}

@article{radul2022you,
    title={You Only Linearize Once: Tangents Transpose to Gradients},
    author={Radul, Alexey and Paszke, Adam and Frostig, Roy and Johnson, Matthew and Maclaurin, Dougal},
    journal={arXiv preprint arXiv:2204.10923},
    year={2022}
}

@misc{deepmind2020jax,
    title = {The {D}eep{M}ind {JAX} {E}cosystem},
    author = {Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hennigan, Tom and Hessel, Matteo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Martens, Lena and Mikulik, Vladimir and Norman, Tamara and Quan, John and Papamakarios, George and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stokowiec, Wojciech and Viola, Fabio},
    url = {http://github.com/deepmind},
    year = {2020},
}

@Misc{functorch2021,
    author =       {Horace He, Richard Zou},
    title =        {functorch: JAX-like composable function transforms for PyTorch},
    howpublished = {\url{https://github.com/pytorch/functorch}},
    year =         {2021}
}

@misc{bai2019,
    author = {Bai, Junjie and Lu, Fang and Zhang, Ke and others},
    title = {ONNX: Open Neural Network Exchange},
    year = {2019},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/onnx/onnx}},
}

@article{borovykh2018gaussian,
    title={A gaussian process perspective on convolutional neural networks},
    author={Borovykh, Anastasia},
    journal={arXiv preprint arXiv:1810.10798},
    year={2018}
}

@misc{vecteeze,
    author = {Wahyu Arfian},
    title = {Ukraine Vectors by Vecteezy},
    year = {2022},
    publisher = {Vecteezy},
    journal = {Vecteezy},
    howpublished = {\url{https://www.vecteezy.com/vector-art/7506324-stand-with-ukraine-text-with-ukraine-flag-ribbon-and-ukraine-map-vector-design-on-a-dark-blue-background}},
}
