\begin{thebibliography}{10}

\bibitem{rethinkGen}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In {\em Proc. of the 5th Int. Conf. on Learn. Rep. (ICLR)}, 2017.

\bibitem{NIPS2017176}
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 5947--5956. 2017.

\bibitem{NIPS2017204}
Peter~L Bartlett, Dylan~J Foster, and Matus~J Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 6240--6249. 2017.

\bibitem{suzuki18a}
Taiji Suzuki.
\newblock Fast generalization error bound of deep learning from a kernel
  perspective.
\newblock In {\em Proc. of the 21st Int. Conf. on Artificial Intelligence and
  Statistics (AISTATS)}, volume~84, pages 1397--1406, 2018.

\bibitem{zhou18a}
Pan Zhou and Jiashi Feng.
\newblock Understanding generalization and optimization performance of deep
  {CNN}s.
\newblock In {\em Proc. of the 35th Int. Conf. on Mach. Learn. (ICML)},
  volume~80, pages 5960--5969, 2018.

\bibitem{arora18b}
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In {\em Proc. of the 35th Int. Conf. on Mach. Learn. (ICML)},
  volume~80, pages 254--263, 2018.

\bibitem{SizeInd}
Noah Golowich, Alexander Rakhlin, and Ohad Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock In {\em roc. of the 31st Conf. on Learning Theory (COLT)}, pages
  297--299. 2018.

\bibitem{neyshabur2018a}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
\newblock A {PAC}-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In {\em Proc. of the 6th Int. Conf. on Learn. Rep. (ICLR)}, 2018.

\bibitem{Anthony}
Martin Anthony and Peter~L. Bartlett.
\newblock {\em Neural Network Learning: Theoretical Foundations}.
\newblock Cambridge University Press, New York, NY, USA, 1st edition, 2009.

\bibitem{BN}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em Proc. of the 35th Int. Conf. on Mach. Learn. (ICML)}, pages
  448--456, 2015.

\bibitem{xie17a}
Bo~Xie, Yingyu Liang, and Le~Song.
\newblock {Diverse Neural Network Learns True Target Functions}.
\newblock In {\em Proc. of the 20th Int. Conf. on Artificial Intelligence and
  Statistics (AISTATS)}, volume~54, pages 1216--1224, 2017.

\bibitem{Santurkar}
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry.
\newblock How does batch normalization help optimization? (no, it is not about
  internal covariate shift).
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)}.
  2018.

\bibitem{ooAAAI18}
Mete Ozay and Takayuki Okatani.
\newblock Training cnns with normalized kernels.
\newblock In {\em AAAI Conference on Artificial Intelligence}, 2018.

\bibitem{Pls}
Haoran Chen, Yanfeng Sun, Junbin Gao, Yongli Hu, and Baocai Yin.
\newblock Partial least squares regression on riemannian manifolds and its
  application in classifications.
\newblock {\em CoRR}, abs/1609.06434, 2016.

\bibitem{Lui2012}
Yui~Man Lui.
\newblock Human gesture recognition on product manifolds.
\newblock {\em J. Mach. Learn. Res.}, 13(1):3297--3321, Nov 2012.

\bibitem{RBN}
Minhyung Cho and Jaehyung Lee.
\newblock Riemannian approach to batch normalization.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem{w_norm}
Tim Salimans and Diederik~P. Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2016.

\bibitem{NIPS2017_7107}
Minhyung Cho and Jaehyung Lee.
\newblock Riemannian approach to batch normalization.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 5231--5241. 2017.

\bibitem{resnext}
Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em IEEE Int. Conf. on Comp. Vis. Patt. Recog. {(CVPR)}}, July
  2017.

\bibitem{Sandler}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em IEEE Int. Conf. on Comp. Vis. Patt. Recog. {(CVPR)}}, June
  2018.

\bibitem{Ioanno}
Yani Ioannou, Duncan Robertson, Roberto Cipolla, and Antonio Criminisi.
\newblock Deep roots: Improving cnn efficiency with hierarchical filter groups.
\newblock In {\em IEEE Int. Conf. on Comp. Vis. Patt. Recog. {(CVPR)}}, July
  2017.

\bibitem{Neyshabur15}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In {\em Proc. of the 28th Conf. on Learning Theory (COLT)},
  volume~40, pages 1376--1401, 2015.

\bibitem{bai1993}
Z.~D. Bai and Y.~Q. Yin.
\newblock Limit of the smallest eigenvalue of a large dimensional sample
  covariance matrix.
\newblock {\em The Annals of Probability}, 21(3):1275--1294, 07 1993.

\bibitem{BAI1988166}
Z.~D Bai, Jack~W Silverstein, and Y.Q Yin.
\newblock A note on the largest eigenvalue of a large dimensional sample
  covariance matrix.
\newblock {\em Journal of Multivariate Analysis}, 26(2):166 -- 168, 1988.

\bibitem{xavier}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proc. of the 13th Int. Conf. on Artificial Intelligence and
  Statistics (AISTATS)}, volume~9, pages 249--256, 2010.

\bibitem{absil_retr}
P.~A. Absil and Jerome Malick.
\newblock Projection-like retractions on matrix manifolds.
\newblock {\em SIAM Journal on Optimization}, 22(1):135--158, 2012.

\bibitem{Kawaguchi}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 586--594. 2016.

\bibitem{BrutzkusG17}
Alon Brutzkus and Amir Globerson.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock In {\em Proc. of the 34th Int. Conf. on Mach. Learn. (ICML)}, pages
  605--614, 2017.

\bibitem{SDu}
Simon~S. Du, Jason~D. Lee, Yuandong Tian, Barnabás Póczos, and Aarti Singh.
\newblock Gradient descent learns one-hidden-layer cnn: Don't be afraid of
  spurious local minima.
\newblock In {\em Proc. of the 35th Int. Conf. on Mach. Learn. (ICML)}, 2018.

\bibitem{OverParam}
Simon~S. Du and Jason~D. Lee.
\newblock On the power of over-parametrization in neural networks with
  quadratic activation.
\newblock In {\em Proc. of the 35th Int. Conf. on Mach. Learn. (ICML)}, 2018.

\bibitem{Yun}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock Global optimality conditions for deep neural networks.
\newblock In {\em Proc. of the 6th Int. Conf. on Learn. Rep. (ICLR)}, 2018.

\bibitem{Hardt}
Moritz Hardt and Tengyu Ma.
\newblock Identity matters in deep learning.
\newblock In {\em Proc. of the 5th Int. Conf. on Learn. Rep. (ICLR)}, 2017.

\bibitem{Ge}
Rong Ge, Jason~D. Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In {\em Proc. of the 5th Int. Conf. on Learn. Rep. (ICLR)}, 2017.

\bibitem{criticalGlobal}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock A critical view of global optimality in deep learning.
\newblock {\em CoRR}, abs/1802.03487, 2018.

\bibitem{raghu17a}
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha
  Sohl-Dickstein.
\newblock On the expressive power of deep neural networks.
\newblock In {\em Proc. of the 34th Int. Conf. on Mach. Learn. (ICML)},
  volume~70, pages 2847--2854, 2017.

\bibitem{nguyen17a}
Quynh Nguyen and Matthias Hein.
\newblock The loss surface of deep and wide neural networks.
\newblock In {\em Proc. of the 34th Int. Conf. on Mach. Learn. (ICML)},
  volume~70, pages 2603--2612, 2017.

\bibitem{senet}
Jie Hu, Li~Shen, and Gang Sun.
\newblock Squeeze-and-excitation networks.
\newblock In {\em Proc. IEEE Conf. Comp. Vis. Patt. Recog. (CVPR)}, 2018.

\bibitem{ICML-2015-IoffeS}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em Proc. of the 32nd Int. Conf. Mach. Learn.}, volume~37, pages
  448--456, 2015.

\bibitem{lee2009manifolds}
J.M. Lee.
\newblock {\em Manifolds and Differential Geometry}.
\newblock Graduate studies in mathematics. American Mathematical Society, 2009.

\bibitem{petersen2006riemannian}
P.~Petersen.
\newblock {\em Riemannian Geometry}.
\newblock Graduate Texts in Mathematics. Springer New York, 2006.

\bibitem{sgdman}
S.~Bonnabel.
\newblock Stochastic gradient descent on riemannian manifolds.
\newblock {\em IEEE Trans. Autom. Control}, 58(9):2217--2229, Sept 2013.

\bibitem{fisk}
Donald~L. Fisk.
\newblock Quasi-martingales.
\newblock {\em Transactions of the American Mathematical Society},
  120(3):369--389, 1965.

\bibitem{Alexnet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 1097--1105, 2012.

\bibitem{res_net}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em IEEE Int. Conf. on Comp. Vis. Patt. Recog. {(CVPR)}}, 2016.

\bibitem{nature_deep}
Yann Lecun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em Nature}, 521(7553):436--444, 5 2015.

\bibitem{algo}
Thomas~H. Cormen, Charles~E. Leiserson, Ronald~L. Rivest, and Clifford Stein.
\newblock {\em Introduction to Algorithm}.
\newblock The MIT Press, 3rd edition, 2009.

\bibitem{Higham}
Nicholas~J. Higham.
\newblock The scaling and squaring method for the matrix exponential revisited.
\newblock {\em SIAM J. Matrix Anal. Appl.}, 26(4):1179--1193, Apr 2005.

\bibitem{kenney}
C.~S. Kenney and A.~J. Laub.
\newblock A schur--fréchet algorithm for computing the logarithm and
  exponential of a matrix.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  19(3):640--663, 1998.

\bibitem{nineteen}
Cleve Moler and Charles~Van Loan.
\newblock Nineteen dubious ways to compute the exponential of a matrix,
  twenty-five years later.
\newblock {\em SIAM Review}, 45(1):3--49, 2003.

\bibitem{golub}
G.H. Golub and C.F. Van~Loan.
\newblock {\em Matrix computations}.
\newblock Johns Hopkins University Press, Baltimore, MD, 2013.

\bibitem{fast_qr}
Franklin~T. Luk.
\newblock A rotation method for computing the qr-decomposition.
\newblock {\em SIAM Journal on Scientific and Statistical Computing},
  7(2):452--459, 1986.

\bibitem{964}
Ian Masliah, Ahmad Ahmad, Azzam Haidar, Stanimire Tomov, Jo{\"e}l Falcou, and
  Jack Dongarra.
\newblock High-performance matrix-matrix multiplications of very small
  matrices.
\newblock In {\em 22nd Int. European Conf. on Parallel and Dist. Comp.
  (Euro-Par{\textquoteright}16)}, Grenoble, France, Aug 2016. Springer
  International Publishing.

\bibitem{dghklty14}
Jack Dongarra, Mark Gates, Azzam Haidar, Jakub Kurzak, Piotr Luszczek,
  Stanimire Tomov, and Ichitaro Yamazaki.
\newblock Accelerating numerical dense linear algebra calculations with gpus.
\newblock {\em Numerical Computations with GPUs}, pages 1--26, 2014.

\bibitem{tdb10}
Stanimire Tomov, Jack Dongarra, and Marc Baboulin.
\newblock {Towards dense linear algebra for hybrid GPU accelerated manycore
  systems}.
\newblock {\em Parallel Computing}, 36(5-6):232--240, Jun 2010.

\bibitem{tnld10}
Stanimire Tomov, Rajib Nath, Hatem Ltaief, and Jack Dongarra.
\newblock Dense linear algebra solvers for multicore with {GPU} accelerators.
\newblock In {\em IPDPS}, pages 1--8, Atlanta, GA, April 19-23 2010. IEEE
  Computer Society.

\bibitem{ntd10_vecpar}
Rajib Nath, Stanimire Tomov, and Jack Dongarra.
\newblock Accelerating {GPU} kernels for dense linear algebra.
\newblock In {\em VECPAR}, Berkeley, CA, June 22-25 2010. Springer.

\bibitem{ntd10}
Rajib Nath, Stanimire Tomov, and Jack Dongarra.
\newblock {An Improved MAGMA GEMM For Fermi Graphics Processing Units}.
\newblock {\em Int. J. High Perform. Comput. Appl.}, 24(4):511--515, November
  2010.

\bibitem{Brouwer2014}
William~J. Brouwer and Pierre-Yves Taunay.
\newblock {\em Efficient Batch LU and QR Decomposition on GPU}, pages 69--86.
\newblock Springer International Publishing, 2014.

\bibitem{oblq}
P.~A. Absil and K.~A. Gallivan.
\newblock Joint diagonalization on the oblique manifold for independent
  component analysis.
\newblock In {\em Proc. 31st IEEE Int. Conf. Acoust., Speech Signal Process.},
  volume~5, pages 945--948, Toulouse, France, May 2006.

\bibitem{manopt_book}
P.-A. Absil, R.~Mahony, and R.~Sepulchre.
\newblock {\em Optimization Algorithms on Matrix Manifolds}.
\newblock PUP, Princeton, NJ, USA, 2007.

\bibitem{SN}
Gao Huang, Yu~Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q. Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em Proc. of the 14th European Conf. Comp. Vis.}, pages 646--661,
  Amsterdam, The Netherlands, 2016.

\bibitem{go_deeper1}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In {\em Proc. IEEE Conf. Comp. Vis. Patt. Recog.}, 2015.

\bibitem{Howard13}
Andrew~G. Howard.
\newblock Some improvements on deep convolutional neural network based image
  classification.
\newblock {\em CoRR}, abs/1312.5402, 2013.

\bibitem{on_mom}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em Proc. Int. Conf. Mach. Learn.}, volume~28, pages 1139--1147,
  May 2013.

\bibitem{oo16}
Mete Ozay and Takayuki Okatani.
\newblock Optimization on submanifolds of convolution kernels in cnns.
\newblock {\em CoRR}, abs/1610.07008, 2016.

\bibitem{DCCN}
Gao Huang, Zhuang Liu, and Kilian~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em Proc. IEEE Conf. Comp. Vis. Patt. Recog. (CVPR)}, 2017.

\end{thebibliography}
