\begin{thebibliography}{10}

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock {\em arXiv preprint arXiv:2109.01652}, 2021.

\bibitem{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint arXiv:2110.14168}, 2021.

\bibitem{suzgun2023challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc Le, Ed~Chi, Denny Zhou, et~al.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve them.
\newblock In {\em Findings of the Association for Computational Linguistics: ACL 2023}, pages 13003--13051, 2023.

\bibitem{madaan2022memory}
Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang.
\newblock Memory-assisted prompt editing to improve gpt-3 after deployment.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 2833--2861, 2022.

\bibitem{Shinn2023ReflexionLA}
Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.
\newblock Reflexion: language agents with verbal reinforcement learning.
\newblock In {\em Neural Information Processing Systems}, 2023.

\bibitem{zhao2024expel}
Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang.
\newblock Expel: Llm agents are experiential learners.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 19632--19642, 2024.

\bibitem{zhou2022large}
Yongchao Zhou, Andrei~Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba.
\newblock Large language models are human-level prompt engineers.
\newblock {\em arXiv preprint arXiv:2211.01910}, 2022.

\bibitem{pryzant2023automatic}
Reid Pryzant, Dan Iter, Jerry Li, Yin~Tat Lee, Chenguang Zhu, and Michael Zeng.
\newblock Automatic prompt optimization with" gradient descent" and beam search.
\newblock {\em arXiv preprint arXiv:2305.03495}, 2023.

\bibitem{guo2023connecting}
Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu~Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang.
\newblock Connecting large language models with evolutionary algorithms yields powerful prompt optimizers.
\newblock {\em arXiv preprint arXiv:2309.08532}, 2023.

\bibitem{rubin2021learning}
Ohad Rubin, Jonathan Herzig, and Jonathan Berant.
\newblock Learning to retrieve prompts for in-context learning.
\newblock In {\em Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 2655--2671, 2022.

\bibitem{ye2023compositional}
Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong.
\newblock Compositional exemplars for in-context learning.
\newblock In {\em International Conference on Machine Learning}, pages 39818--39833. PMLR, 2023.

\bibitem{li2023unified}
Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu.
\newblock Unified demonstration retriever for in-context learning.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 4644--4668, 2023.

\bibitem{li2023mot}
Xiaonan Li and Xipeng Qiu.
\newblock Mot: Memory-of-thought enables chatgpt to self-improve.
\newblock In {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 6354--6374, 2023.

\bibitem{ding2023parameter}
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et~al.
\newblock Parameter-efficient fine-tuning of large-scale pre-trained language models.
\newblock {\em Nature Machine Intelligence}, 5(3):220--235, 2023.

\bibitem{Yu2018SpiderAL}
Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et~al.
\newblock Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 3911--3921, 2018.

\bibitem{yu2019cosql}
Tao Yu, Rui Zhang, He~Yang Er, Suyi Li, Eric Xue, Bo~Pang, Xi~Victoria Lin, Yi~Chern Tan, Tianze Shi, Zihan Li, et~al.
\newblock Cosql: A conversational text-to-sql challenge towards cross-domain natural language interfaces to databases.
\newblock {\em arXiv preprint arXiv:1909.05378}, 2019.

\bibitem{li2024can}
Jinyang Li, Binyuan Hui, Ge~Qu, Jiaxi Yang, Binhua Li, Bowen Li, Bailin Wang, Bowen Qin, Ruiying Geng, Nan Huo, et~al.
\newblock Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{lai2023ds}
Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu.
\newblock Ds-1000: A natural and reliable benchmark for data science code generation.
\newblock In {\em International Conference on Machine Learning}, pages 18319--18345. PMLR, 2023.

\bibitem{qin2023toolllm}
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun.
\newblock Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023.

\bibitem{wang2024llms}
Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van~Durme, and Yu~Su.
\newblock Llms in the imaginarium: tool learning through simulated trial and error.
\newblock {\em arXiv preprint arXiv:2403.04746}, 2024.

\bibitem{fansi2022ddxplus}
Arsene Fansi~Tchango, Rishab Goel, Zhi Wen, Julien Martel, and Joumana Ghosn.
\newblock Ddxplus: A new dataset for automatic medical diagnosis.
\newblock {\em Advances in Neural Information Processing Systems}, 35:31306--31318, 2022.

\bibitem{yang2018hotpotqa}
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William~W Cohen, Ruslan Salakhutdinov, and Christopher~D Manning.
\newblock Hotpotqa: A dataset for diverse, explainable multi-hop question answering.
\newblock {\em arXiv preprint arXiv:1809.09600}, 2018.

\bibitem{kojima2022large}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock {\em Advances in neural information processing systems}, 35:22199--22213, 2022.

\bibitem{madaan2024self}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{min2022rethinking}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning work?
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 11048--11064, 2022.

\bibitem{wei2023larger}
Jerry Wei, Jason Wei, Yi~Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da~Huang, Denny Zhou, et~al.
\newblock Larger language models do in-context learning differently.
\newblock {\em arXiv preprint arXiv:2303.03846}, 2023.

\bibitem{du2023improving}
Yilun Du, Shuang Li, Antonio Torralba, Joshua~B Tenenbaum, and Igor Mordatch.
\newblock Improving factuality and reasoning in language models through multiagent debate.
\newblock {\em arXiv preprint arXiv:2305.14325}, 2023.

\bibitem{chen2023reconcile}
Justin Chih-Yao Chen, Swarnadeep Saha, and Mohit Bansal.
\newblock Reconcile: Round-table conference improves reasoning via consensus among diverse llms.
\newblock {\em arXiv preprint arXiv:2309.13007}, 2023.

\bibitem{Brown2020LanguageMA}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock {\em ArXiv}, abs/2005.14165, 2020.

\bibitem{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{Reid2024Gemini1U}
Gemini~Team Google.
\newblock Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.
\newblock {\em ArXiv}, abs/2403.05530, 2024.

\bibitem{anthropic2024claude}
AI~Anthropic.
\newblock The claude 3 model family: Opus, sonnet, haiku.
\newblock {\em Claude-3 Model Card}, 2024.

\bibitem{hoi2021online}
Steven~CH Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao.
\newblock Online learning: A comprehensive survey.
\newblock {\em Neurocomputing}, 459:249--289, 2021.

\bibitem{williams1989learning}
Ronald~J Williams and David Zipser.
\newblock A learning algorithm for continually running fully recurrent neural networks.
\newblock {\em Neural computation}, 1(2):270--280, 1989.

\bibitem{bahroun2017online}
Yanis Bahroun and Andrea Soltoggio.
\newblock Online representation learning with single and multi-layer hebbian networks for image classification.
\newblock {\em arXiv preprint arXiv:1702.06456}, 2017.

\bibitem{hu2023meta}
Nathan Hu, Eric Mitchell, Christopher~D Manning, and Chelsea Finn.
\newblock Meta-learning online adaptation of language models.
\newblock In {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 4418--4432, 2023.

\bibitem{yao2022react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
\newblock React: Synergizing reasoning and acting in language models.
\newblock {\em arXiv preprint arXiv:2210.03629}, 2022.

\bibitem{chen2023self}
Wei-Lin Chen, Cheng-Kuang Wu, Yun-Nung Chen, and Hsin-Hsi Chen.
\newblock Self-icl: Zero-shot in-context learning with self-generated demonstrations.
\newblock In {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 15651--15662, 2023.

\end{thebibliography}
