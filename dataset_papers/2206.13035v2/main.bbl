\begin{thebibliography}{78}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Astudillo \& Frazier(2019)Astudillo and
  Frazier]{astudillo2019bayesian}
Astudillo, R. and Frazier, P.
\newblock Bayesian optimization of composite functions.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  354--363. PMLR, 2019.

\bibitem[Astudillo \& Frazier(2021)Astudillo and
  Frazier]{astudillo2021bayesian}
Astudillo, R. and Frazier, P.
\newblock Bayesian optimization of function networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Astudillo \& Frazier(2022)Astudillo and
  Frazier]{astudillo2022thinking}
Astudillo, R. and Frazier, P.~I.
\newblock Thinking inside the box: A tutorial on grey-box bayesian
  optimization.
\newblock \emph{arXiv preprint arXiv:2201.00272}, 2022.

\bibitem[Asuncion \& Newman(2007)Asuncion and Newman]{asuncion2007uci}
Asuncion, A. and Newman, D.
\newblock {UCI} machine learning repository, 2007.

\bibitem[Attia et~al.(2020)Attia, Grover, Jin, Severson, Markov, Liao, Chen,
  Cheong, Perkins, Yang, et~al.]{attia2020closed}
Attia, P.~M., Grover, A., Jin, N., Severson, K.~A., Markov, T.~M., Liao, Y.-H.,
  Chen, M.~H., Cheong, B., Perkins, N., Yang, Z., et~al.
\newblock Closed-loop optimization of fast-charging protocols for batteries
  with machine learning.
\newblock \emph{Nature}, 578\penalty0 (7795):\penalty0 397--402, 2020.

\bibitem[Balandat et~al.(2020)Balandat, Karrer, Jiang, Daulton, Letham, Wilson,
  and Bakshy]{balandat2020botorch}
Balandat, M., Karrer, B., Jiang, D., Daulton, S., Letham, B., Wilson, A.~G.,
  and Bakshy, E.
\newblock Botorch: A framework for efficient monte-carlo bayesian optimization.
\newblock \emph{Advances in neural information processing systems}, 33, 2020.

\bibitem[Bergstra \& Bengio(2012)Bergstra and Bengio]{bergstra2012random}
Bergstra, J. and Bengio, Y.
\newblock Random search for hyper-parameter optimization.
\newblock \emph{Journal of machine learning research}, 13\penalty0 (2), 2012.

\bibitem[Bergstra et~al.(2011)Bergstra, Bardenet, Bengio, and
  K{\'e}gl]{bergstra2011algorithms}
Bergstra, J., Bardenet, R., Bengio, Y., and K{\'e}gl, B.
\newblock Algorithms for hyper-parameter optimization.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Bergstra et~al.(2013)Bergstra, Yamins, and Cox]{bergstra2013making}
Bergstra, J., Yamins, D., and Cox, D.
\newblock Making a science of model search: Hyperparameter optimization in
  hundreds of dimensions for vision architectures.
\newblock In \emph{International conference on machine learning}, pp.\
  115--123. PMLR, 2013.

\bibitem[Bickel et~al.(2007)Bickel, Br{\"u}ckner, and
  Scheffer]{bickel2007discriminative}
Bickel, S., Br{\"u}ckner, M., and Scheffer, T.
\newblock Discriminative learning for differing training and test
  distributions.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pp.\  81--88, 2007.

\bibitem[Bliznyuk et~al.(2008)Bliznyuk, Ruppert, Shoemaker, Regis, Wild, and
  Mugunthan]{bliznyuk2008bayesian}
Bliznyuk, N., Ruppert, D., Shoemaker, C., Regis, R., Wild, S., and Mugunthan,
  P.
\newblock Bayesian calibration and uncertainty analysis for computationally
  expensive models using optimization and radial basis function approximation.
\newblock \emph{Journal of Computational and Graphical Statistics}, 17\penalty0
  (2):\penalty0 270--294, 2008.

\bibitem[Breiman(2001)]{breiman2001random}
Breiman, L.
\newblock Random forests.
\newblock \emph{Machine learning}, 45\penalty0 (1):\penalty0 5--32, 2001.

\bibitem[Brochu et~al.(2007)Brochu, De~Freitas, and Ghosh]{brochu2007active}
Brochu, E., De~Freitas, N., and Ghosh, A.
\newblock Active preference learning with discrete choice data.
\newblock In \emph{NIPS}, pp.\  409--416, 2007.

\bibitem[Buja et~al.(2005)Buja, Stuetzle, and Shen]{buja2005loss}
Buja, A., Stuetzle, W., and Shen, Y.
\newblock Loss functions for binary class probability estimation and
  classification: Structure and applications.
\newblock \emph{Working draft, November}, 3, 2005.

\bibitem[Calandra et~al.(2016)Calandra, Seyfarth, Peters, and
  Deisenroth]{calandra2016bayesian}
Calandra, R., Seyfarth, A., Peters, J., and Deisenroth, M.~P.
\newblock Bayesian optimization for learning gaits under uncertainty.
\newblock \emph{Annals of Mathematics and Artificial Intelligence}, 76\penalty0
  (1):\penalty0 5--23, 2016.

\bibitem[Chen \& Guestrin(2016)Chen and Guestrin]{chen2016xgboost}
Chen, T. and Guestrin, C.
\newblock Xgboost: A scalable tree boosting system.
\newblock In \emph{Proceedings of the 22nd acm sigkdd international conference
  on knowledge discovery and data mining}, pp.\  785--794, 2016.

\bibitem[Chrabaszcz et~al.(2017)Chrabaszcz, Loshchilov, and
  Hutter]{chrabaszcz2017downsampled}
Chrabaszcz, P., Loshchilov, I., and Hutter, F.
\newblock A downsampled variant of imagenet as an alternative to the cifar
  datasets.
\newblock \emph{arXiv preprint arXiv:1707.08819}, 2017.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Deshpande \& Kuleshov(2021)Deshpande and
  Kuleshov]{deshpande2021calibration}
Deshpande, S. and Kuleshov, V.
\newblock Calibration improves bayesian optimization.
\newblock \emph{arXiv preprint arXiv:2112.04620}, 2021.

\bibitem[Diggle \& Gratton(1984)Diggle and Gratton]{diggle1984monte}
Diggle, P.~J. and Gratton, R.~J.
\newblock Monte carlo methods of inference for implicit statistical models.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 46\penalty0 (2):\penalty0 193--212, 1984.

\bibitem[Dong \& Yang(2020)Dong and Yang]{dong2020bench}
Dong, X. and Yang, Y.
\newblock Nas-bench-201: Extending the scope of reproducible neural
  architecture search.
\newblock \emph{arXiv preprint arXiv:2001.00326}, 2020.

\bibitem[Forrester et~al.(2007)Forrester, S{\'o}bester, and
  Keane]{forrester2007multi}
Forrester, A.~I., S{\'o}bester, A., and Keane, A.~J.
\newblock Multi-fidelity optimization via surrogate modelling.
\newblock \emph{Proceedings of the royal society a: mathematical, physical and
  engineering sciences}, 463\penalty0 (2088):\penalty0 3251--3269, 2007.

\bibitem[Frazier et~al.(2008)Frazier, Powell, and
  Dayanik]{frazier2008knowledge}
Frazier, P.~I., Powell, W.~B., and Dayanik, S.
\newblock A knowledge-gradient policy for sequential information collection.
\newblock \emph{SIAM Journal on Control and Optimization}, 47\penalty0
  (5):\penalty0 2410--2439, 2008.

\bibitem[Garnett(2022)]{garnett_bayesoptbook_2022}
Garnett, R.
\newblock \emph{{Bayesian Optimization}}.
\newblock Cambridge University Press, 2022.
\newblock in preparation.

\bibitem[Gneiting \& Raftery(2007)Gneiting and Raftery]{gneiting2007strictly}
Gneiting, T. and Raftery, A.~E.
\newblock Strictly proper scoring rules, prediction, and estimation.
\newblock \emph{Journal of the American Statistical Association}, 102\penalty0
  (477):\penalty0 359--378, March 2007.
\newblock ISSN 0162-1459.
\newblock \doi{10.1198/016214506000001437}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.~D., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems 27}, pp.\  2672--2680. Curran Associates, Inc., 2014.

\bibitem[Gretton et~al.(2009)Gretton, Smola, Huang, Schmittfull, Borgwardt, and
  Sch{\"o}lkopf]{gretton2009covariate}
Gretton, A., Smola, A., Huang, J., Schmittfull, M., Borgwardt, K., and
  Sch{\"o}lkopf, B.
\newblock Covariate shift by kernel mean matching.
\newblock \emph{Dataset shift in machine learning}, 3\penalty0 (4):\penalty0 5,
  2009.

\bibitem[Griffiths \& Hern{\'a}ndez-Lobato(2020)Griffiths and
  Hern{\'a}ndez-Lobato]{griffiths2020constrained}
Griffiths, R.-R. and Hern{\'a}ndez-Lobato, J.~M.
\newblock Constrained bayesian optimization for automatic chemical design using
  variational autoencoders.
\newblock \emph{Chemical science}, 11\penalty0 (2):\penalty0 577--586, 2020.

\bibitem[Gutmann \& Hyvarinen(2010)Gutmann and Hyvarinen]{gutmann2010noise}
Gutmann, M. and Hyvarinen, A.
\newblock Noise-contrastive estimation: A new estimation principle for
  unnormalized statistical models.
\newblock \emph{AISTATS}, 2010.

\bibitem[Hennig \& Schuler(2012)Hennig and Schuler]{hennig2012entropy}
Hennig, P. and Schuler, C.~J.
\newblock Entropy search for information-efficient global optimization.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0 (6), 2012.

\bibitem[Hern{\'a}ndez-Lobato et~al.(2016)Hern{\'a}ndez-Lobato,
  Hernandez-Lobato, Shah, and Adams]{hernandez2016predictive}
Hern{\'a}ndez-Lobato, D., Hernandez-Lobato, J., Shah, A., and Adams, R.
\newblock Predictive entropy search for multi-objective bayesian optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1492--1501. PMLR, 2016.

\bibitem[Hern{\'a}ndez-Lobato et~al.(2014)Hern{\'a}ndez-Lobato, Hoffman, and
  Ghahramani]{hernandez2014predictive}
Hern{\'a}ndez-Lobato, J.~M., Hoffman, M.~W., and Ghahramani, Z.
\newblock Predictive entropy search for efficient global optimization of
  black-box functions.
\newblock \emph{arXiv preprint arXiv:1406.2541}, 2014.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and
  White]{hornik1989multilayer}
Hornik, K., Stinchcombe, M., and White, H.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Hutter et~al.(2011)Hutter, Hoos, and
  Leyton-Brown]{hutter2011sequential}
Hutter, F., Hoos, H.~H., and Leyton-Brown, K.
\newblock Sequential model-based optimization for general algorithm
  configuration.
\newblock In \emph{International conference on learning and intelligent
  optimization}, pp.\  507--523. Springer, 2011.

\bibitem[Jones et~al.(1998)Jones, Schonlau, and Welch]{jones1998efficient}
Jones, D.~R., Schonlau, M., and Welch, W.~J.
\newblock Efficient global optimization of expensive black-box functions.
\newblock \emph{Journal of Global optimization}, 13\penalty0 (4):\penalty0
  455--492, 1998.

\bibitem[Kanamori et~al.(2010)Kanamori, Suzuki, and
  Sugiyama]{kanamori2010theoretical}
Kanamori, T., Suzuki, T., and Sugiyama, M.
\newblock Theoretical analysis of density ratio estimation.
\newblock \emph{IEICE transactions on fundamentals of electronics,
  communications and computer sciences}, 93\penalty0 (4):\penalty0 787--798,
  2010.

\bibitem[Klein \& Hutter(2019)Klein and Hutter]{klein2019tabular}
Klein, A. and Hutter, F.
\newblock Tabular benchmarks for joint architecture and hyperparameter
  optimization.
\newblock \emph{arXiv preprint arXiv:1905.04970}, 2019.

\bibitem[Kleinegesse \& Gutmann(2019)Kleinegesse and
  Gutmann]{kleinegesse2019efficient}
Kleinegesse, S. and Gutmann, M.~U.
\newblock Efficient bayesian experimental design for implicit models.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  476--485. PMLR, 2019.

\bibitem[Koyama et~al.(2017)Koyama, Sato, Sakamoto, and
  Igarashi]{koyama2017sequential}
Koyama, Y., Sato, I., Sakamoto, D., and Igarashi, T.
\newblock Sequential line search for efficient visual design optimization by
  crowds.
\newblock \emph{ACM Transactions on Graphics (TOG)}, 36\penalty0 (4):\penalty0
  1--11, 2017.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kushner(1962)]{kushner1962versatile}
Kushner, H.~J.
\newblock A versatile stochastic model of a function of unknown and time
  varying form.
\newblock \emph{Journal of Mathematical Analysis and Applications}, 5\penalty0
  (1):\penalty0 150--167, 1962.

\bibitem[Kushner(1964)]{kushner1964new}
Kushner, H.~J.
\newblock A new method of locating the maximum point of an arbitrary multipeak
  curve in the presence of noise.
\newblock 1964.

\bibitem[Maddox et~al.(2021)Maddox, Balandat, Wilson, and
  Bakshy]{maddox2021bayesian}
Maddox, W.~J., Balandat, M., Wilson, A.~G., and Bakshy, E.
\newblock Bayesian optimization with high-dimensional outputs.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[McIntire et~al.(2016)McIntire, Ratner, and Ermon]{mcintire2016sparse}
McIntire, M., Ratner, D., and Ermon, S.
\newblock Sparse gaussian processes for bayesian optimization.
\newblock In \emph{UAI}, 2016.

\bibitem[Mikolov et~al.(2013)Mikolov, Sutskever, Chen, Corrado, and
  Dean]{mikolov2013distributed}
Mikolov, T., Sutskever, I., Chen, K., Corrado, G.~S., and Dean, J.
\newblock Distributed representations of words and phrases and their
  compositionality.
\newblock In Burges, C. J.~C., Bottou, L., Welling, M., Ghahramani, Z., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems 26}, pp.\  3111--3119. Curran Associates, Inc., 2013.

\bibitem[Mo{\v{c}}kus(1975)]{movckus1975bayesian}
Mo{\v{c}}kus, J.
\newblock On bayesian methods for seeking the extremum.
\newblock In \emph{Optimization techniques IFIP technical conference}, pp.\
  400--404. Springer, 1975.

\bibitem[Mockus et~al.(1978)Mockus, Tiesis, and
  Zilinskas]{mockus1978application}
Mockus, J., Tiesis, V., and Zilinskas, A.
\newblock The application of bayesian methods for seeking the extremum.
\newblock \emph{Towards global optimization}, 2\penalty0 (117-129):\penalty0 2,
  1978.

\bibitem[Mohamed \& Lakshminarayanan(2016)Mohamed and
  Lakshminarayanan]{mohamed2016learning}
Mohamed, S. and Lakshminarayanan, B.
\newblock Learning in implicit generative models.
\newblock \emph{arXiv preprint arXiv:1610.03483}, October 2016.

\bibitem[Nguyen et~al.(2008)Nguyen, Wainwright, and
  Jordan]{nguyen2008estimating}
Nguyen, X., Wainwright, M.~J., and Jordan, M.~I.
\newblock Estimating divergence functionals and the likelihood ratio by convex
  risk minimization.
\newblock \emph{arXiv preprint arXiv:0809.0853}, \penalty0 (11):\penalty0
  5847--5861, September 2008.
\newblock \doi{10.1109/TIT.2010.2068870}.

\bibitem[Pearce \& Branke(2017)Pearce and Branke]{pearce2017bayesian}
Pearce, M. and Branke, J.
\newblock Bayesian simulation optimization with input uncertainty.
\newblock In \emph{2017 Winter Simulation Conference (WSC)}, pp.\  2268--2278.
  IEEE, 2017.

\bibitem[Perrone et~al.(2018)Perrone, Jenatton, Seeger, and
  Archambeau]{perrone2018scalable}
Perrone, V., Jenatton, R., Seeger, M., and Archambeau, C.
\newblock Scalable hyperparameter transfer learning.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  6846--6856, 2018.

\bibitem[Rasmussen(2003)]{rasmussen2003gaussian}
Rasmussen, C.~E.
\newblock Gaussian processes in machine learning.
\newblock In \emph{Summer school on machine learning}, pp.\  63--71. Springer,
  2003.

\bibitem[Reid et~al.(2011)Reid, Williamson, et~al.]{reid2011information}
Reid, M., Williamson, R., et~al.
\newblock Information, divergence and risk for binary experiments.
\newblock 2011.

\bibitem[Scott et~al.(2011)Scott, Frazier, and Powell]{scott2011correlated}
Scott, W., Frazier, P., and Powell, W.
\newblock The correlated knowledge gradient for simulation optimization of
  continuous parameters using gaussian process regression.
\newblock \emph{SIAM Journal on Optimization}, 21\penalty0 (3):\penalty0
  996--1026, 2011.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}
Snoek, J., Larochelle, H., and Adams, R.~P.
\newblock Practical bayesian optimization of machine learning algorithms.
\newblock \emph{Advances in neural information processing systems}, 25, 2012.

\bibitem[Snoek et~al.(2015)Snoek, Rippel, Swersky, Kiros, Satish, Sundaram,
  Patwary, Prabhat, and Adams]{snoek2015scalable}
Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N.,
  Patwary, M., Prabhat, M., and Adams, R.
\newblock Scalable bayesian optimization using deep neural networks.
\newblock In \emph{International conference on machine learning}, pp.\
  2171--2180. PMLR, 2015.

\bibitem[Sobester et~al.(2008)Sobester, Forrester, and
  Keane]{sobester2008engineering}
Sobester, A., Forrester, A., and Keane, A.
\newblock \emph{Engineering design via surrogate modelling: a practical guide}.
\newblock John Wiley \& Sons, 2008.

\bibitem[Song \& Ermon(2020)Song and Ermon]{song2020bridging}
Song, J. and Ermon, S.
\newblock Bridging the gap between f-gans and wasserstein gans.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Springenberg et~al.(2016)Springenberg, Klein, Falkner, and
  Hutter]{springenberg2016bayesian}
Springenberg, J.~T., Klein, A., Falkner, S., and Hutter, F.
\newblock Bayesian optimization with robust bayesian neural networks.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 4134--4142, 2016.

\bibitem[Sugiyama et~al.(2007)Sugiyama, Nakajima, Kashima, Von~Buenau, and
  Kawanabe]{sugiyama2007direct}
Sugiyama, M., Nakajima, S., Kashima, H., Von~Buenau, P., and Kawanabe, M.
\newblock Direct importance estimation with model selection and its application
  to covariate shift adaptation.
\newblock In \emph{NIPS}, volume~7, pp.\  1433--1440. Citeseer, 2007.

\bibitem[Sugiyama et~al.(2012)Sugiyama, Suzuki, and
  Kanamori]{sugiyama2012density}
Sugiyama, M., Suzuki, T., and Kanamori, T.
\newblock \emph{Density ratio estimation in machine learning}.
\newblock Cambridge University Press, 2012.

\bibitem[Sugiyama et~al.(2013)Sugiyama, Kanamori, Suzuki, du~Plessis, Liu, and
  Takeuchi]{sugiyama2013density}
Sugiyama, M., Kanamori, T., Suzuki, T., du~Plessis, M.~C., Liu, S., and
  Takeuchi, I.
\newblock Density-difference estimation.
\newblock \emph{Neural Computation}, 25\penalty0 (10):\penalty0 2734--2775,
  2013.

\bibitem[Swersky et~al.(2013)Swersky, Snoek, and Adams]{swersky2013multi}
Swersky, K., Snoek, J., and Adams, R.~P.
\newblock Multi-task bayesian optimization.
\newblock 2013.

\bibitem[Tiao et~al.(2021)Tiao, Klein, Seeger, Bonilla, Archambeau, and
  Ramos]{tiao2021bore}
Tiao, L.~C., Klein, A., Seeger, M., Bonilla, E.~V., Archambeau, C., and Ramos,
  F.
\newblock Bore: Bayesian optimization by density-ratio estimation.
\newblock \emph{arXiv preprint arXiv:2102.09009}, 2021.

\bibitem[Titsias(2009)]{titsias2009variational}
Titsias, M.
\newblock Variational learning of inducing variables in sparse gaussian
  processes.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  567--574.
  PMLR, 2009.

\bibitem[Tran et~al.(2017)Tran, Ranganath, and Blei]{tran2017hierarchical}
Tran, D., Ranganath, R., and Blei, D.~M.
\newblock Hierarchical implicit models and likelihood-free variational
  inference.
\newblock \emph{arXiv preprint arXiv:1702.08896}, 2017.

\bibitem[Tran et~al.(2020)Tran, Neiswanger, Yoon, Zhang, Xing, and
  Ulissi]{tran2020methods}
Tran, K., Neiswanger, W., Yoon, J., Zhang, Q., Xing, E., and Ulissi, Z.~W.
\newblock Methods for comparing uncertainty quantifications for material
  property predictions.
\newblock \emph{Machine Learning: Science and Technology}, 1\penalty0
  (2):\penalty0 025006, 2020.

\bibitem[Uehara et~al.(2016)Uehara, Sato, Suzuki, Nakayama, and
  Matsuo]{uehara2016generative}
Uehara, M., Sato, I., Suzuki, M., Nakayama, K., and Matsuo, Y.
\newblock Generative adversarial nets from a density ratio estimation
  perspective.
\newblock \emph{arXiv preprint arXiv:1610.02920}, October 2016.

\bibitem[Uhrenholt \& Jensen(2019)Uhrenholt and Jensen]{uhrenholt2019efficient}
Uhrenholt, A.~K. and Jensen, B.~S.
\newblock Efficient bayesian optimization for target vector estimation.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  2661--2670. PMLR, 2019.

\bibitem[Wang et~al.(2020)Wang, Zhang, and Ng]{wang2020nonparametric}
Wang, H., Zhang, X., and Ng, S.~H.
\newblock A nonparametric bayesian approach for simulation optimization with
  input uncertainty.
\newblock \emph{arXiv preprint arXiv:2008.02154}, 2020.

\bibitem[White et~al.(2019)White, Neiswanger, and Savani]{white2019bananas}
White, C., Neiswanger, W., and Savani, Y.
\newblock Bananas: Bayesian optimization with neural architectures for neural
  architecture search.
\newblock \emph{arXiv preprint arXiv:1910.11858}, 1\penalty0 (2), 2019.

\bibitem[Williams \& Rasmussen(2006)Williams and
  Rasmussen]{williams2006gaussian}
Williams, C.~K. and Rasmussen, C.~E.
\newblock \emph{Gaussian processes for machine learning}, volume~2.
\newblock MIT press Cambridge, MA, 2006.

\bibitem[Wilson et~al.(2018)Wilson, Hutter, and
  Deisenroth]{wilson2018maximizing}
Wilson, J.~T., Hutter, F., and Deisenroth, M.~P.
\newblock Maximizing acquisition functions for bayesian optimization.
\newblock \emph{arXiv preprint arXiv:1805.10196}, 2018.

\bibitem[Wistuba \& Grabocka(2021)Wistuba and Grabocka]{wistuba2021few}
Wistuba, M. and Grabocka, J.
\newblock Few-shot bayesian optimization with deep kernel surrogates.
\newblock \emph{arXiv preprint arXiv:2101.07667}, 2021.

\bibitem[Yu et~al.(2019)Yu, Song, and Ermon]{yu2019multi}
Yu, L., Song, J., and Ermon, S.
\newblock {Multi-Agent} adversarial inverse reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1907.13220}, July 2019.

\bibitem[Yu et~al.(2020)Yu, Song, Song, and Ermon]{yu2020training}
Yu, L., Song, Y., Song, J., and Ermon, S.
\newblock Training deep energy-based models with f-divergence minimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10957--10967. PMLR, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Fu, Bengio, and
  Courville]{zhang2021unifying}
Zhang, D., Fu, J., Bengio, Y., and Courville, A.
\newblock Unifying likelihood-free inference with black-box sequence design and
  beyond.
\newblock \emph{arXiv preprint arXiv:2110.03372}, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Bi, and
  Zhang]{zhang2021scalable}
Zhang, J., Bi, S., and Zhang, G.
\newblock A scalable gradient free method for bayesian experimental design with
  implicit models.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3745--3753. PMLR, 2021{\natexlab{b}}.

\end{thebibliography}
