\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{\k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Devansh Arpit, Stanis{\l}aw Jastrz{\k{e}}bski, Nicolas Ballas, David Krueger,
  Emmanuel Bengio, Maxinder~S Kanwal, Tegan Maharaj, Asja Fischer, Aaron
  Courville, Yoshua Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  233--242. PMLR, 2017.

\bibitem[Arratia \& Gordon(1989)Arratia and Gordon]{arratia1989tutorial}
Richard Arratia and Louis Gordon.
\newblock Tutorial on large deviations for the binomial distribution.
\newblock \emph{Bulletin of mathematical biology}, 51\penalty0 (1):\penalty0
  125--131, 1989.

\bibitem[Ball et~al.(1997)]{ball1997elementary}
Keith Ball et~al.
\newblock An elementary introduction to modern convex geometry.
\newblock \emph{Flavors of geometry}, 31:\penalty0 1--58, 1997.

\bibitem[Bartlett(1998)]{bartlett1998sample}
Peter~L Bartlett.
\newblock The sample complexity of pattern classification with neural networks:
  the size of the weights is more important than the size of the network.
\newblock \emph{IEEE transactions on Information Theory}, 44\penalty0
  (2):\penalty0 525--536, 1998.

\bibitem[Baum(1988)]{baum1988capabilities}
Eric~B Baum.
\newblock On the capabilities of multilayer perceptrons.
\newblock \emph{Journal of complexity}, 4\penalty0 (3):\penalty0 193--215,
  1988.

\bibitem[Belkin et~al.(2020)Belkin, Hsu, and Xu]{belkin2020two}
Mikhail Belkin, Daniel Hsu, and Ji~Xu.
\newblock Two models of double descent for weak features.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (4):\penalty0 1167--1180, 2020.

\bibitem[Bubeck et~al.(2020)Bubeck, Eldan, Lee, and
  Mikulincer]{bubeck2020network}
S{\'e}bastien Bubeck, Ronen Eldan, Yin~Tat Lee, and Dan Mikulincer.
\newblock Network size and weights size for memorization with two-layers neural
  networks.
\newblock \emph{arXiv preprint arXiv:2006.02855}, 2020.

\bibitem[Cover(1965)]{cover1965geometrical}
Thomas~M Cover.
\newblock Geometrical and statistical properties of systems of linear
  inequalities with applications in pattern recognition.
\newblock \emph{IEEE transactions on electronic computers}, 1965.

\bibitem[Guruswami et~al.(2012)Guruswami, Rudra, and
  Sudan]{guruswami2012essential}
Venkatesan Guruswami, Atri Rudra, and Madhu Sudan.
\newblock Essential coding theory.
\newblock \emph{Draft available at http://www. cse. buffalo. edu/~
  atri/courses/coding-theory/book}, 2012.

\bibitem[Hardt \& Ma(2017)Hardt and Ma]{hardt2017identity}
Moritz Hardt and Tengyu Ma.
\newblock Identity matters in deep learning.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}, 2017.

\bibitem[Huang(2003)]{huang2003learning}
Guang-Bin Huang.
\newblock Learning capability and storage capacity of two-hidden-layer
  feedforward networks.
\newblock \emph{IEEE transactions on neural networks}, 14\penalty0
  (2):\penalty0 274--281, 2003.

\bibitem[Huang et~al.(1991)Huang, Huang, et~al.]{huang1991bounds}
Shih-Chi Huang, Yih-Fang Huang, et~al.
\newblock Bounds on the number of hidden neurons in multilayer perceptrons.
\newblock \emph{IEEE transactions on neural networks}, 2\penalty0 (1):\penalty0
  47--55, 1991.

\bibitem[Kotsovsky et~al.(2020)Kotsovsky, Geche, and
  Batyuk]{kotsovsky2020bithreshold}
Vladyslav Kotsovsky, Fedir Geche, and Anatoliy Batyuk.
\newblock Bithreshold neural network classifier.
\newblock In \emph{2020 IEEE 15th International Conference on Computer Sciences
  and Information Technologies (CSIT)}, volume~1, pp.\  32--35. IEEE, 2020.

\bibitem[Kowalczyk(1997)]{kowalczyk1997estimates}
Adam Kowalczyk.
\newblock Estimates of storage capacity of multilayer perceptron with threshold
  logic hidden units.
\newblock \emph{Neural networks}, 10\penalty0 (8):\penalty0 1417--1433, 1997.

\bibitem[Liu et~al.(2020)Liu, Papailiopoulos, and Achlioptas]{liu2020bad}
Shengchao Liu, Dimitris Papailiopoulos, and Dimitris Achlioptas.
\newblock Bad global minima exist and sgd can reach them.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Mitchison \& Durbin(1989)Mitchison and Durbin]{mitchison1989bounds}
GJ~Mitchison and RM~Durbin.
\newblock Bounds on the learning capacity of some multi-layer networks.
\newblock \emph{Biological Cybernetics}, 1989.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{neyshabur2018role}
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan
  Srebro.
\newblock The role of over-parametrization in generalization of neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Neyshabur et~al.(2019)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{neyshabur2019towards}
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan
  Srebro.
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Park et~al.(2020)Park, Lee, Yun, and Shin]{park2020provable}
Sejun Park, Jaeho Lee, Chulhee Yun, and Jinwoo Shin.
\newblock Provable memorization via deep neural networks using sub-linear
  parameters.
\newblock \emph{arXiv preprint arXiv:2010.13363}, 2020.

\bibitem[Sartori \& Antsaklis(1991)Sartori and Antsaklis]{sartori1991simple}
Michael~A Sartori and Panos~J Antsaklis.
\newblock A simple method to derive bounds on the size and to train multilayer
  neural networks.
\newblock \emph{IEEE transactions on neural networks}, 2\penalty0 (4):\penalty0
  467--471, 1991.

\bibitem[Sontag(1990)]{sontag1990remarks}
Eduardo~D Sontag.
\newblock Remarks on interpolation and recognition using neural nets.
\newblock In \emph{NIPS}, pp.\  939--945, 1990.

\bibitem[Vershynin(2018)]{vershynin2018high}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Vershynin(2020)]{vershynin2020memory}
Roman Vershynin.
\newblock Memory capacity of neural networks with threshold and rectified
  linear unit activations.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (4):\penalty0 1004--1033, 2020.

\bibitem[Yun et~al.(2019)Yun, Sra, and Jadbabaie]{yun2018small}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock Small relu networks are powerful memorizers: a tight analysis of
  memorization capacity.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pp.\  15532--15543, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{Zhang17understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}, 2017.

\end{thebibliography}
