@book{2018:RL,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{2014:thomas,
	title={Bias in natural actor-critic algorithms},
	author={Thomas, Philip},
	booktitle={International conference on machine learning},
	pages={441--448},
	year={2014}
}

@inproceedings{2002:kakade,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={ICML},
  volume={2},
  pages={267--274},
  year={2002}
}

@inproceedings{2017:acktr,
  title={Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation},
  author={Wu, Yuhuai and Mansimov, Elman and Grosse, Roger B and Liao, Shun and Ba, Jimmy},
  booktitle={Advances in neural information processing systems},
  pages={5279--5288},
  year={2017}
}

@inproceedings{2015:trpo,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={1889--1897},
  year={2015}
}

@article{2017:ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{2016:a3c,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}

@article{2012:opac,
  title={Off-policy actor-critic},
  author={Degris, Thomas and White, Martha and Sutton, Richard S},
  journal={arXiv preprint arXiv:1205.4839},
  year={2012}
}
      
@inproceedings{2014:dpg,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={ICML},
  year={2014}
}

@article{2015:ddpg,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@inproceedings{2015:svg,
  title={Learning continuous control policies by stochastic value gradients},
  author={Heess, Nicolas and Wayne, Gregory and Silver, David and Lillicrap, Tim and Erez, Tom and Tassa, Yuval},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2944--2952},
  year={2015}
}

@article{2018:d4pg,
  title={Distributed Distributional Deterministic Policy Gradients},
  author={Barth-Maron, Gabriel and Hoffman, Matthew W and Budden, David and Dabney, Will and Horgan, Dan and Muldal, Alistair and Heess, Nicolas and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1804.08617},
  year={2018}
}

@article{2015:dqn,
	title={Human-level control through deep reinforcement learning},
	author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
	journal={Nature},
	volume={518},
	number={7540},
	pages={529--533},
	year={2015},
	publisher={Nature Publishing Group}
}

@inproceedings{2010:double-q,
  title={Double Q-learning},
  author={Hasselt, Hado V},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2613--2621},
  year={2010}
}

@inproceedings{2016:ddqn,
  title={Deep Reinforcement Learning with Double Q-Learning.},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={AAAI},
  volume={2},
  pages={5},
  year={2016},
  organization={Phoenix, AZ}
}

@article{2016:acer,
  title={Sample efficient actor-critic with experience replay},
  author={Wang, Ziyu and Bapst, Victor and Heess, Nicolas and Mnih, Volodymyr and Munos, Remi and Kavukcuoglu, Koray and de Freitas, Nando},
  journal={arXiv preprint arXiv:1611.01224},
  year={2016}
}

@article{2017:time_limit,
	title={Time limits in reinforcement learning},
	author={Pardo, Fabio and Tavakoli, Arash and Levdik, Vitaly and Kormushev, Petar},
	journal={arXiv preprint arXiv:1712.00378},
	year={2017}
}

@article{2002:tsitsiklis,
	title={On average versus discounted reward temporal-difference learning},
	author={Tsitsiklis, John N and Van Roy, Benjamin},
	journal={Machine Learning},
	volume={49},
	number={2-3},
	pages={179--191},
	year={2002},
	publisher={Springer}
}

@inproceedings{2011:horde,
	title={Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction},
	author={Sutton, Richard S and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M and White, Adam and Precup, Doina},
	booktitle={The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2},
	pages={761--768},
	year={2011}
}

@inproceedings{2014:qlambda,
	title={A new Q (lambda) with interim forward view and Monte Carlo equivalence},
	author={Sutton, Rich and Mahmood, Ashique Rupam and Precup, Doina and Hasselt, Hado},
	booktitle={International Conference on Machine Learning},
	pages={568--576},
	year={2014}
}

@article{2016:emphatic,
	title={An emphatic approach to the problem of off-policy temporal-difference learning},
	author={Sutton, Richard S and Mahmood, A Rupam and White, Martha},
	journal={The Journal of Machine Learning Research},
	volume={17},
	number={1},
	pages={2603--2631},
	year={2016},
	publisher={JMLR. org}
}

@inproceedings{2017:unifying,
	title={Unifying task specification in reinforcement learning},
	author={White, Martha},
	booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	pages={3742--3750},
	year={2017},
	organization={JMLR. org}
}

@inproceedings{1995:td_models,
	title={TD models: modeling the world at a mixture of time scales},
	author={Sutton, Richard S},
	booktitle={Proceedings of the Twelfth International Conference on International Conference on Machine Learning},
	pages={531--539},
	year={1995},
	organization={Morgan Kaufmann Publishers Inc.}
}

@book{2011:insights,
	title={Insights in reinforcement rearning: formal analysis and empirical evaluation of temporal-difference learning algorithms},
	author={van Hasselt, Hado Philip},
	year={2011},
	publisher={Utrecht University}
}

@inproceedings{2015:YU,
	title={On convergence of emphatic temporal-difference learning},
	author={Yu, Huizhen},
	booktitle={Conference on Learning Theory},
	pages={1724--1751},
	year={2015}
}

@inproceedings{2017:predictron,
	title={The predictron: End-to-end learning and planning},
	author={Silver, David and van Hasselt, Hado and Hessel, Matteo and Schaul, Tom and Guez, Arthur and Harley, Tim and Dulac-Arnold, Gabriel and Reichert, David and Rabinowitz, Neil and Barreto, Andre and others},
	booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	pages={3191--3199},
	year={2017},
	organization={JMLR. org}
}

@inproceedings{2019:generalized,
	title={Generalized off-policy actor-critic},
	author={Zhang, Shangtong and Boehmer, Wendelin and Whiteson, Shimon},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1999--2009},
	year={2019}
}

@article{1992:reinforce,
	title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
	author={Williams, Ronald J},
	journal={Machine learning},
	volume={8},
	number={3-4},
	pages={229--256},
	year={1992},
	publisher={Springer}
}

@article{1989:qlearning,
	title={Learning from delayed rewards},
	author={Watkins, CJCH},
	journal={PhD thesis, King's College, University of Cambridge},
	year={1989}
}

@inproceedings{2016:benchmarking,
	title={Benchmarking deep reinforcement learning for continuous control},
	author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
	booktitle={International Conference on Machine Learning},
	pages={1329--1338},
	year={2016}
}

@article{2016:gym,
	title={Openai gym},
	author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	journal={arXiv preprint arXiv:1606.01540},
	year={2016}
}

@inproceedings{2000:pg,
	title={Policy gradient methods for reinforcement learning with function approximation},
	author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
	booktitle={Advances in neural information processing systems},
	pages={1057--1063},
	year={2000}
}

@article{2018:td3,
	title={Addressing function approximation error in actor-critic methods},
	author={Fujimoto, Scott and Van Hoof, Herke and Meger, David},
	journal={arXiv preprint arXiv:1802.09477},
	year={2018}
}

@article{2017:equivalence,
	title={Equivalence between policy gradients and soft q-learning},
	author={Schulman, John and Chen, Xi and Abbeel, Pieter},
	journal={arXiv preprint arXiv:1704.06440},
	year={2017}
}

@inproceedings{2018:sac,
	title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
	author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	booktitle={International Conference on Machine Learning},
	pages={1861--1870},
	year={2018}
}

@inproceedings{2016:naf,
	title={Continuous deep q-learning with model-based acceleration},
	author={Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey},
	booktitle={International Conference on Machine Learning},
	pages={2829--2838},
	year={2016}
}



% game-theoretic approach to problems related to RL
@inproceedings{2017:gt_marl,
  title={A unified game-theoretic approach to multiagent reinforcement learning},
  author={Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and P{\'e}rolat, Julien and Silver, David and Graepel, Thore},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4190--4203},
  year={2017}
}

@article{2016:gt_marl,
  title={Deep reinforcement learning from self-play in imperfect-information games},
  author={Heinrich, Johannes and Silver, David},
  journal={arXiv preprint arXiv:1603.01121},
  year={2016}
}

@incollection{1994:markov_game,
  title={Markov games as a framework for multi-agent reinforcement learning},
  author={Littman, Michael L},
  booktitle={Machine Learning Proceedings 1994},
  pages={157--163},
  year={1994},
  publisher={Elsevier}
}

@inproceedings{2008:gt_apprenticeship,
  title={A game-theoretic approach to apprenticeship learning},
  author={Syed, Umar and Schapire, Robert E},
  booktitle={Advances in neural information processing systems},
  pages={1449--1456},
  year={2008}
}

@article{2017:gt_pomdp,
  title={Regret Minimization for Partially Observable Deep Reinforcement Learning},
  author={Jin, Peter H and Levine, Sergey and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1710.11424},
  year={2017}
}

@inproceedings{2014:gan,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}


% LP approach to RL
 @inproceedings{2018:LP_RL,
title={Boosting the Actor with Dual Critic},
author={Bo Dai and Albert Shaw and Niao He and Lihong Li and Le Song},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BkUp6GZRW},
}

@article{2017:LP_RL,
  title={Deep Primal-Dual Reinforcement Learning: Accelerating Actor-Critic using Bellman Duality},
  author={Cho, Woon Sang and Wang, Mengdi},
  journal={arXiv preprint arXiv:1712.02467},
  year={2017}
}

@article{2017:LP_DP,
  title={Randomized Linear Programming Solves the Discounted Markov Decision Problem In Nearly-Linear (Sometimes Sublinear) Run Time},
  author={Wang, Mengdi},
  journal={Mathematics of Operations Research},
  year={2017}
}

@article{2018:LP_qlearning,
  title={Stochastic Primal-Dual Q-Learning},
  author={Lee, Donghwan and He, Niao},
  journal={arXiv preprint arXiv:1810.08298},
  year={2018}
}
 
 
 % RL for game playing
 @incollection{2012:rl_in_games,
  title={Reinforcement learning in games},
  author={Szita, Istv{\'a}n},
  booktitle={Reinforcement Learning},
  pages={539--577},
  year={2012},
  publisher={Springer}
}

@article{2018:AlphaZero,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{2017:DeepStack,
  title={Deepstack: Expert-level artificial intelligence in heads-up no-limit poker},
  author={Morav{\v{c}}{\'\i}k, Matej and Schmid, Martin and Burch, Neil and Lis{\`y}, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
  journal={Science},
  volume={356},
  number={6337},
  pages={508--513},
  year={2017},
  publisher={American Association for the Advancement of Science}
}

 
 % game theory
 @article{1996:potential_game,
  title={Potential games},
  author={Monderer, Dov and Shapley, Lloyd S},
  journal={Games and economic behavior},
  volume={14},
  number={1},
  pages={124--143},
  year={1996},
  publisher={Elsevier}
}

@article{1996:fp_potential_game,
  title={Fictitious play property for games with identical interests},
  author={Monderer, Dov and Shapley, Lloyd S},
  journal={Journal of economic theory},
  volume={68},
  number={1},
  pages={258--265},
  year={1996},
  publisher={Academic Press}
}

 @book{2007:algo_game_theory,
  title={Algorithmic game theory},
  author={Nisan, Noam and Roughgarden, Tim and Tardos, Eva and Vazirani, Vijay V},
  year={2007},
  publisher={Cambridge University Press}
}

@article{2018:br_potential_game,
author = {Swenson, B. and Murray, R. and Kar, S.},
title = {On Best-Response Dynamics in Potential Games},
journal = {SIAM Journal on Control and Optimization},
volume = {56},
number = {4},
pages = {2734-2767},
year = {2018},
doi = {10.1137/17M1139461}
}
 
@article{2007:original_fp,
  title={Brown's original fictitious play},
  author={Berger, Ulrich},
  journal={Journal of Economic Theory},
  volume={135},
  number={1},
  pages={572--578},
  year={2007},
  publisher={Elsevier}
}



@online{cartpole_code,
  author = {Rich Sutton and Chuck Anderson},
  title = {Continuous-control CartPole},
  year = 1984,
  url = {http://incompleteideas.net/sutton/book/code/pole.c}
}

@online{pybullet,
	title = {PyBullet Gymperium},
	url = {https://github.com/benelot/pybullet-gym}
}


 

  @inproceedings{2019:ace,
  title={ACE: An Actor Ensemble Algorithm for Continuous Control with Tree Search},
  author={Zhang, Shangtong and Chen, Hao and Yao, Hengshuai},
  booktitle={Thirty-Third AAAI Conference on Artificial Intelligence},
  year={2019}
}


@inproceedings{2017:averaged_dqn,
  title={Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning},
  author={Anschel, Oron and Baram, Nir and Shimkin, Nahum},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={176--185},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{2016:gail,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4565--4573},
  year={2016}
}

@book{1995:dpoc,
  title={Dynamic programming and optimal control},
  author={Bertsekas, Dimitri P},
  volume={1},
  number={2},
  year={1995},
  publisher={Athena scientific Belmont, MA}
}

@article{2002:basal_ganglia,
  title={Actor--critic models of the basal ganglia: New anatomical and computational perspectives},
  author={Joel, Daphna and Niv, Yael and Ruppin, Eytan},
  journal={Neural networks},
  volume={15},
  number={4-6},
  pages={535--547},
  year={2002},
  publisher={Elsevier}
}

@inproceedings{2000:actor_critic,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle={Advances in neural information processing systems},
  pages={1008--1014},
  year={2000}
}

@article{2008:nac,
  title={Natural actor-critic},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1180--1190},
  year={2008},
  publisher={Elsevier}
}

@inproceedings{2002:natural_pg,
	title={A natural policy gradient},
	author={Kakade, Sham M},
	booktitle={Advances in neural information processing systems},
	pages={1531--1538},
	year={2002}
}

@article{2007:locomotion_rl,
  title={Reinforcement learning for a biped robot based on a CPG-actor-critic method},
  author={Nakamura, Yutaka and Mori, Takeshi and Sato, Masa-aki and Ishii, Shin},
  journal={Neural Networks},
  volume={20},
  number={6},
  pages={723--735},
  year={2007},
  publisher={Elsevier}
}

@inproceedings{2006:locomotion_rl,
  title={Policy gradient methods for robotics},
  author={Peters, Jan and Schaal, Stefan},
  booktitle={2006 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={2219--2225},
  year={2006},
  organization={IEEE}
}

@misc{2018:openai_five,
      author = {OpenAI},
      title = {OpenAI Five},
      howpublished = {\url{https://blog.openai.com/openai-five/}},
      year = {2018}}

@misc{2019:alphastar,
  title="{AlphaStar: Mastering the Real-Time Strategy Game StarCraft II}",
  author={Vinyals, Oriol and Babuschkin, Igor and Chung, Junyoung and Mathieu, Michael and Jaderberg, Max and Czarnecki, Wojciech M. and Dudzik, Andrew and Huang, Aja and Georgiev, Petko and Powell, Richard and Ewalds, Timo and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Agapiou, John and Oh, Junhyuk and Dalibard, Valentin and Choi, David and Sifre, Laurent and Sulsky, Yury and Vezhnevets, Sasha and Molloy, James and Cai, Trevor and Budden, David and Paine, Tom and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Pohlen, Toby and Wu, Yuhuai and Yogatama, Dani and Cohen, Julia and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Apps, Chris and Kavukcuoglu, Koray and Hassabis, Demis and Silver, David},
  howpublished={\url{https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/}},
  year={2019}
}

@article{2017:drl_survey,
  title={Deep Reinforcement Learning: A Brief Survey},
  author={Kailash Arulkumaran and Marc Peter Deisenroth and Miles Brundage and Anil Anthony Bharath},
  journal={IEEE Signal Processing Magazine},
  year={2017},
  volume={34},
  pages={26-38}
}

@article{2018:drl_overview,
	title="Deep Reinforcement Learning.",
	author="Yuxi {Li}",
	journal="arXiv preprint arXiv:1810.06339",
	year="2018"
}

@inproceedings{2018:drl_matters,
  title={Deep Reinforcement Learning That Matters},
  author={Peter Henderson and Riashat Islam and Philip Bachman and Joelle Pineau and Doina Precup and David Meger},
  booktitle={AAAI},
  year={2018}
}

@article{2015:GAE,
  title={High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  author={John Schulman and Philipp Moritz and Sergey Levine and Michael I. Jordan and Pieter Abbeel},
  journal={CoRR},
  year={2015},
  volume={abs/1506.02438}
}

@article{2018:softac,
  title={Soft Actor-Critic Algorithms and Applications},
  author={Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and George Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and Pieter Abbeel and Sergey Levine},
  journal={CoRR},
  year={2018},
  volume={abs/1812.05905}
}

@book{1994:puterman,
	title="Markov Decision Processes: Discrete Stochastic Dynamic Programming",
	author="Martin L. {Puterman}",
	year="1994"
}

@article {1986:ne_existence,
	title = {The Existence of Equilibrium in Discontinuous Economic Games, Part I (Theory)},
	journal = {Review of Economic Studies},
	volume = {53},
	number = {1},
	year = {1986},
	note = {Reprinted in K. Binmore and P. Dasgupta (eds.), Economic Organizations as Games, Oxford: Basil Blackwell, 1986, pp. 48-82
},
	pages = {1-26},
	author = {Eric Maskin and P. Dasgupta}
}

@book{2003:game_theory,
	title="An Introduction to Game Theory",
	author="Martin J. {Osborne}",
	year="2003"
}

% Markov chain, MDP
@book{2012:markov_chain,
	title={Markov chains and stochastic stability},
	author={Meyn, Sean P and Tweedie, Richard L},
	year={2012},
	publisher={Springer Science \& Business Media}
}

@book{2009:markov_chain,
	title={Basics of applied stochastic processes},
	author={Serfozo, Richard},
	year={2009},
	publisher={Springer Science \& Business Media}
}

@article{2001:continual_pg,
	title={Simulation-based optimization of Markov reward processes},
	author={Marbach, Peter and Tsitsiklis, John N},
	journal={IEEE Transactions on Automatic Control},
	volume={46},
	number={2},
	pages={191--209},
	year={2001},
	publisher={IEEE}
}