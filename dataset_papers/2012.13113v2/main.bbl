\begin{thebibliography}{10}

\bibitem{Strubell2019Energy}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in {NLP}.
\newblock {\em CoRR}, abs/1906.02243, 2019.

\bibitem{you2020drawing}
Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen,
  Yingyan Lin, Zhangyang Wang, and Richard~G. Baraniuk.
\newblock Drawing early-bird tickets: Toward more efficient training of deep
  networks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{wang2018training}
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash
  Gopalakrishnan.
\newblock Training deep neural networks with 8-bit floating point numbers.
\newblock In {\em Advances in neural information processing systems}, pages
  7675--7684, 2018.

\bibitem{achille2018critical}
Alessandro Achille, Matteo Rovere, and Stefano Soatto.
\newblock Critical learning periods in deep networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{layers_equal}
Chiyuan Zhang, Samy Bengio, and Yoram Singer.
\newblock Are all layers created equal?
\newblock {\em CoRR}, abs/1902.01996, 2019.

\bibitem{veit2016residual}
Andreas Veit, Michael Wilber, and Serge Belongie.
\newblock Residual networks behave like ensembles of relatively shallow
  networks, 2016.

\bibitem{greff2016highway}
Klaus Greff, Rupesh~K Srivastava, and J{\"u}rgen Schmidhuber.
\newblock Highway and residual networks learn unrolled iterative estimation.
\newblock {\em arXiv preprint arXiv:1612.07771}, 2016.

\bibitem{li2019towards}
Yuanzhi Li, Colin Wei, and Tengyu Ma.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock {\em arXiv preprint arXiv:1907.04595}, 2019.

\bibitem{wang2018skipnet}
Xin Wang, Fisher Yu, Zi-Yi Dou, Trevor Darrell, and Joseph~E Gonzalez.
\newblock Skipnet: Learning dynamic routing in convolutional networks.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 409--424, 2018.

\bibitem{goyal}
Priya Goyal, Piotr Doll{\'{a}}r, Ross~B. Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD:} training imagenet in 1 hour.
\newblock {\em CoRR}, abs/1706.02677, 2017.

\bibitem{jia2018highly}
Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou,
  Liqiang Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, et~al.
\newblock Highly scalable deep learning training system with mixed-precision:
  Training imagenet in four minutes.
\newblock {\em arXiv preprint arXiv:1807.11205}, 2018.

\bibitem{you2018imagenet}
Yang You, Zhao Zhang, Cho-Jui Hsieh, James Demmel, and Kurt Keutzer.
\newblock Imagenet training in minutes.
\newblock In {\em Proceedings of the 47th International Conference on Parallel
  Processing}, page~1. ACM, 2018.

\bibitem{Akiba2017ExtremelyLM}
Takuya Akiba, Shuji Suzuki, and Keisuke Fukuda.
\newblock Extremely large minibatch sgd: Training resnet-50 on imagenet in 15
  minutes.
\newblock {\em CoRR}, abs/1711.04325, 2017.

\bibitem{banner2018scalable}
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry.
\newblock Scalable methods for 8-bit training of neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5145--5153, 2018.

\bibitem{MixPT}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory~F. Diamos, Erich
  Elsen, David Garc{\'{\i}}a, Boris Ginsburg, Michael Houston, Oleksii
  Kuchaiev, Ganesh Venkatesh, and Hao Wu.
\newblock Mixed precision training.
\newblock {\em CoRR}, abs/1710.03740, 2017.

\bibitem{gupta2015deep}
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
\newblock Deep learning with limited numerical precision.
\newblock In {\em International Conference on Machine Learning}, pages
  1737--1746, 2015.

\bibitem{sun2019hybrid}
Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani,
  Vijayalakshmi~Viji Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash
  Gopalakrishnan.
\newblock Hybrid 8-bit floating point (hfp8) training and inference for deep
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4901--4910, 2019.

\bibitem{seide20141}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In {\em Fifteenth Annual Conference of the International Speech
  Communication Association}, 2014.

\bibitem{de2017understanding}
Christopher De~Sa, Matthew Feldman, Christopher R{\'e}, and Kunle Olukotun.
\newblock Understanding and optimizing asynchronous low-precision stochastic
  gradient descent.
\newblock In {\em ACM SIGARCH Computer Architecture News}, volume~45, pages
  561--574. ACM, 2017.

\bibitem{terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 30}, pages 1509--1519. Curran Associates, Inc., 2017.

\bibitem{signSGD}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
  Anandkumar.
\newblock sign{SGD}: {C}ompressed {O}ptimisation for {N}on-{C}onvex {P}roblems.
\newblock In {\em International Conference on Machine Learning (ICML-18)},
  2018.

\bibitem{yang2019training}
Yukuan Yang, Shuang Wu, Lei Deng, Tianyi Yan, Yuan Xie, and Guoqi Li.
\newblock Training high-performance and large-scale deep neural networks with
  full 8-bit integers.
\newblock {\em arXiv preprint arXiv:1909.02384}, 2019.

\bibitem{8050797}
Y.~{Lin}, C.~{Sakr}, Y.~{Kim}, and N.~{Shanbhag}.
\newblock Predictive{N}et: An energy-efficient convolutional neural network via
  zero prediction.
\newblock In {\em 2017 IEEE International Symposium on Circuits and Systems
  (ISCAS)}, pages 1--4, 2017.

\bibitem{blockdrop}
Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry~S. Davis,
  Kristen Grauman, and Rogerio Feris.
\newblock Blockdrop: Dynamic inference paths in residual networks.
\newblock {\em 2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, Jun 2018.

\bibitem{convnet-aig}
Andreas Veit and Serge Belongie.
\newblock Convolutional networks with adaptive inference graphs.
\newblock {\em Lecture Notes in Computer Science}, page 3â€“18, 2018.

\bibitem{gaternet}
Zhourong Chen, Yang Li, Samy Bengio, and Si~Si.
\newblock Gaternet: Dynamic filter selection in convolutional neural network
  via a dedicated global gating network, 2018.

\bibitem{gao2018dynamic}
Xitong Gao, Yiren Zhao, {\L}ukasz Dudziak, Robert Mullins, and Cheng-zhong Xu.
\newblock Dynamic channel pruning: Feature boosting and suppression.
\newblock {\em arXiv preprint arXiv:1810.05331}, 2018.

\bibitem{hua2019channel}
Weizhe Hua, Yuan Zhou, Christopher~M De~Sa, Zhiru Zhang, and G~Edward Suh.
\newblock Channel gating neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1886--1896, 2019.

\bibitem{DDI}
Y.~{Wang}, J.~{Shen}, T.~K. {Hu}, P.~{Xu}, T.~{Nguyen}, R.~{Baraniuk},
  Z.~{Wang}, and Y.~{Lin}.
\newblock Dual dynamic inference: Enabling more efficient, adaptive, and
  controllable deep inference.
\newblock {\em IEEE Journal of Selected Topics in Signal Processing},
  14(4):623--633, 2020.

\bibitem{shen2020fractional}
Jianghao Shen, Yonggan Fu, Yue Wang, Pengfei Xu, Zhangyang Wang, and Yingyan
  Lin.
\newblock Fractional skipping: Towards finer-grained dynamic cnn inference.
\newblock {\em arXiv preprint arXiv:2001.00705}, 2020.

\bibitem{song2020drq}
Zhuoran Song, Bangqi Fu, Feiyang Wu, Zhaoming Jiang, Li~Jiang, Naifeng Jing,
  and Xiaoyao Liang.
\newblock Drq: dynamic region-based quantization for deep neural network
  acceleration.
\newblock In {\em 2020 ACM/IEEE 47th Annual International Symposium on Computer
  Architecture (ISCA)}, pages 1010--1021. IEEE, 2020.

\bibitem{prunetrain}
Sangkug Lym, Esha Choukse, Siavash Zangeneh, Wei Wen, Sujay Sanghavi, and
  Mattan Erez.
\newblock Prunetrain: Fast neural network training by dynamic sparse model
  reconfiguration, 2019.

\bibitem{e^2_train}
Yue Wang, Ziyu Jiang, Xiaohan Chen, Pengfei Xu, Yang Zhao, Yingyan Lin, and
  Zhangyang Wang.
\newblock E2-train: Training state-of-the-art cnns with over 80\% less energy.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{biggest_loser}
Angela~H. Jiang, Daniel L.~K. Wong, Giulio Zhou, David~G. Andersen, Jeffrey
  Dean, Gregory~R. Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch,
  Zachary~C. Lipton, and Padmanabhan Pillai.
\newblock Accelerating deep learning by focusing on the biggest losers, 2019.

\bibitem{pmlr-v97-rahaman19a}
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred
  Hamprecht, Yoshua Bengio, and Aaron Courville.
\newblock On the spectral bias of neural networks.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of {\em Proceedings of Machine Learning Research}, pages
  5301--5310. PMLR, 09--15 Jun 2019.

\bibitem{xu2019frequency}
Zhi-Qin~John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma.
\newblock Frequency principle: Fourier analysis sheds light on deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1901.06523}, 2019.

\bibitem{zhou2016dorefa}
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He~Wen, and Yuheng Zou.
\newblock Dorefa-net: Training low bitwidth convolutional neural networks with
  low bitwidth gradients.
\newblock {\em arXiv preprint arXiv:1606.06160}, 2016.

\bibitem{zhu2019towards}
Feng Zhu, Ruihao Gong, Fengwei Yu, Xianglong Liu, Yanfei Wang, Zhelong Li,
  Xiuqi Yang, and Junjie Yan.
\newblock Towards unified int8 training for convolutional neural network.
\newblock {\em arXiv preprint arXiv:1912.12607}, 2019.

\bibitem{Sharma_2018}
Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Vikas
  Chandra, and Hadi Esmaeilzadeh.
\newblock Bit fusion: Bit-level dynamically composable architecture for
  accelerating deep neural network.
\newblock {\em 2018 ACM/IEEE 45th Annual (ISCA)}, Jun 2018.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{sandler2018mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4510--4520, 2018.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{imagenet_cvpr09}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In {\em CVPR09}, 2009.

\bibitem{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock {\em arXiv preprint arXiv:1609.07843}, 2016.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em arXiv preprint arXiv:1502.03167}, 2015.

\bibitem{yang2017designing}
Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze.
\newblock Designing energy-efficient convolutional neural networks using
  energy-aware pruning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5687--5695, 2017.

\bibitem{sharma2018bit}
Hardik Sharma, Jongse Park, Naveen Suda, Liangzhen Lai, Benson Chau, Vikas
  Chandra, and Hadi Esmaeilzadeh.
\newblock Bit fusion: Bit-level dynamically composable architecture for
  accelerating deep neural networks.
\newblock In {\em Proceedings of the 45th Annual International Symposium on
  Computer Architecture}, pages 764--775. IEEE Press, 2018.

\bibitem{li2020halo}
Chaojian Li, Tianlong Chen, Haoran You, Zhangyang Wang, and Yingyan Lin.
\newblock {HALO}: Hardware-aware learning to optimize.
\newblock In {\em The 16th European Conference on Computer Vision (ECCV 2020)},
  2020.

\bibitem{lee20197}
Jinsu Lee, Juhyoung Lee, Donghyeon Han, Jinmook Lee, Gwangtae Park, and Hoi-Jun
  Yoo.
\newblock 7.7 lnpu: A 25.3 tflops/w sparse deep-neural-network learning
  processor with fine-grained mixed precision of fp8-fp16.
\newblock In {\em 2019 IEEE International Solid-State Circuits
  Conference-(ISSCC)}, pages 142--144. IEEE, 2019.

\bibitem{kim20201b}
Chang~Hyeon Kim, Jin~Mook Lee, Sang~Hoon Kang, Sang~Yeob Kim, Dong~Seok Im, and
  Hoi~Jun Yoo.
\newblock 1b-16b variable bit precision dnn processor for emotional hri system
  in mobile devices.
\newblock {\em Journal of Integrated Circuits and Systems}, 6(3), 2020.

\bibitem{li2020edd}
Yuhong Li, Cong Hao, Xiaofan Zhang, Xinheng Liu, Yao Chen, Jinjun Xiong,
  Wen-mei Hwu, and Deming Chen.
\newblock Edd: Efficient differentiable dnn architecture and implementation
  co-search for embedded ai solutions.
\newblock {\em arXiv preprint arXiv:2005.02563}, 2020.

\bibitem{zc706}
{Xilinx Inc.}
\newblock Xilinx zynq-7000 soc zc706 evaluation kit.
\newblock
  \url{https://www.xilinx.com/products/boards-and-kits/ek-z7-zc706-g.html}.
\newblock (Accessed on 09/30/2020).

\bibitem{wang2019haq}
Kuan Wang, Zhijian Liu, Yujun Lin, Ji~Lin, and Song Han.
\newblock Haq: Hardware-aware automated quantization with mixed precision.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 8612--8620, 2019.

\bibitem{ott2019fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In {\em Proceedings of NAACL-HLT 2019: Demonstrations}, 2019.

\end{thebibliography}
