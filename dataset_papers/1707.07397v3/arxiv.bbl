\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and Wagner]{anish-carlini}
Athalye, A., Carlini, N., and Wagner, D.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock 2018.
\newblock URL \url{https://arxiv.org/abs/1802.00420}.

\bibitem[Biggio et~al.(2013)Biggio, Corona, Maiorca, Nelson, {\v{S}}rndi{\'c},
  Laskov, Giacinto, and Roli]{biggio2013evasion}
Biggio, B., Corona, I., Maiorca, D., Nelson, B., {\v{S}}rndi{\'c}, N., Laskov,
  P., Giacinto, G., and Roli, F.
\newblock Evasion attacks against machine learning at test time.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  387--402. Springer, 2013.

\bibitem[Brown et~al.(2016)Brown, Man{\'e}, Roy, Abadi, and
  Gilmer]{brown2017patch}
Brown, T.~B., Man{\'e}, D., Roy, A., Abadi, M., and Gilmer, J.
\newblock Defensive distillation is not robust to adversarial examples.
\newblock 2016.
\newblock URL \url{https://arxiv.org/abs/1607.04311}.

\bibitem[Buckman et~al.(2018)Buckman, Roy, Raffel, and
  Goodfellow]{buckman2018thermometer}
Buckman, J., Roy, A., Raffel, C., and Goodfellow, I.
\newblock Thermometer encoding: One hot way to resist adversarial examples.
\newblock \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=S18Su--CW}.
\newblock accepted as poster.

\bibitem[Carlini \& Wagner(2016)Carlini and Wagner]{carlini2016distillation}
Carlini, N. and Wagner, D.
\newblock Defensive distillation is not robust to adversarial examples.
\newblock 2016.
\newblock URL \url{https://arxiv.org/abs/1607.04311}.

\bibitem[Carlini \& Wagner(2017{\natexlab{a}})Carlini and
  Wagner]{carlini2017adversarial}
Carlini, N. and Wagner, D.
\newblock Adversarial examples are not easily detected: Bypassing ten detection
  methods.
\newblock \emph{AISec}, 2017{\natexlab{a}}.

\bibitem[Carlini \& Wagner(2017{\natexlab{b}})Carlini and
  Wagner]{carlini2017magnet}
Carlini, N. and Wagner, D.
\newblock Magnet and ``efficient defenses against adversarial attacks'' are not
  robust to adversarial examples.
\newblock \emph{arXiv preprint arXiv:1711.08478}, 2017{\natexlab{b}}.

\bibitem[Carlini \& Wagner(2017{\natexlab{c}})Carlini and
  Wagner]{sp2017:carlini}
Carlini, N. and Wagner, D.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{IEEE Symposium on Security \& Privacy}, 2017{\natexlab{c}}.

\bibitem[Carlini et~al.(2016)Carlini, Mishra, Vaidya, Zhang, Sherr, Shields,
  Wagner, and Zhou]{carlini2016hidden}
Carlini, N., Mishra, P., Vaidya, T., Zhang, Y., Sherr, M., Shields, C., Wagner,
  D., and Zhou, W.
\newblock Hidden voice commands.
\newblock In \emph{25th {USENIX} Security Symposium ({USENIX} Security 16)},
  pp.\  513--530, Austin, TX, 2016. {USENIX} Association.
\newblock ISBN 978-1-931971-32-4.
\newblock URL
  \url{https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/carlini}.

\bibitem[Chen et~al.(2017)Chen, Zhang, Sharma, Yi, and Hsieh]{zoo}
Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J.
\newblock Zoo: Zeroth order optimization based black-box attacks to deep neural
  networks without training substitute models.
\newblock In \emph{Proceedings of the 10th ACM Workshop on Artificial
  Intelligence and Security}, AISec '17, pp.\  15--26, New York, NY, USA, 2017.
  ACM.
\newblock ISBN 978-1-4503-5202-4.
\newblock \doi{10.1145/3128572.3140448}.
\newblock URL \url{http://doi.acm.org/10.1145/3128572.3140448}.

\bibitem[Dhillon et~al.(2018)Dhillon, Azizzadenesheli, Bernstein, Kossaifi,
  Khanna, Lipton, and Anandkumar]{dhillon2018stochastic}
Dhillon, G.~S., Azizzadenesheli, K., Bernstein, J.~D., Kossaifi, J., Khanna,
  A., Lipton, Z.~C., and Anandkumar, A.
\newblock Stochastic activation pruning for robust adversarial defense.
\newblock \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=H1uR4GZRZ}.
\newblock accepted as poster.

\bibitem[Evtimov et~al.(2017)Evtimov, Eykholt, Fernandes, Kohno, Li, Prakash,
  Rahmati, and Song]{evtimov-roadsigns}
Evtimov, I., Eykholt, K., Fernandes, E., Kohno, T., Li, B., Prakash, A.,
  Rahmati, A., and Song, D.
\newblock {Robust Physical-World Attacks on Deep Learning Models}.
\newblock 2017.
\newblock URL \url{https://arxiv.org/abs/1707.08945}.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{iclr2015:goodfellow}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2015.

\bibitem[Guo et~al.(2018)Guo, Rana, Cisse, and van~der
  Maaten]{guo2018countering}
Guo, C., Rana, M., Cisse, M., and van~der Maaten, L.
\newblock Countering adversarial images using input transformations.
\newblock \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=SyJ7ClWCb}.
\newblock accepted as poster.

\bibitem[Hendrik~Metzen et~al.(2017)Hendrik~Metzen, Genewein, Fischer, and
  Bischoff]{hendrik2017detecting}
Hendrik~Metzen, J., Genewein, T., Fischer, V., and Bischoff, B.
\newblock On detecting adversarial perturbations.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Hendrycks \& Gimpel(2017)Hendrycks and Gimpel]{hendrycks2017early}
Hendrycks, D. and Gimpel, K.
\newblock Early methods for detecting adversarial images.
\newblock In \emph{International Conference on Learning Representations
  (Workshop Track)}, 2017.

\bibitem[Kurakin et~al.(2016)Kurakin, Goodfellow, and
  Bengio]{goodfellow-physical}
Kurakin, A., Goodfellow, I., and Bengio, S.
\newblock Adversarial examples in the physical world.
\newblock 2016.
\newblock URL \url{https://arxiv.org/abs/1607.02533}.

\bibitem[Lu et~al.(2017)Lu, Sibai, Fabry, and Forsyth]{lu-noneed}
Lu, J., Sibai, H., Fabry, E., and Forsyth, D.
\newblock No need to worry about adversarial examples in object detection in
  autonomous vehicles.
\newblock 2017.
\newblock URL \url{https://arxiv.org/abs/1707.03501}.

\bibitem[Luo et~al.(2016)Luo, Boix, Roig, Poggio, and Zhao]{luo-foveation}
Luo, Y., Boix, X., Roig, G., Poggio, T., and Zhao, Q.
\newblock Foveation-based mechanisms alleviate adversarial examples.
\newblock 2016.
\newblock URL \url{https://arxiv.org/abs/1511.06292}.

\bibitem[Ma et~al.(2018)Ma, Li, Wang, Erfani, Wijewickrema, Schoenebeck, Houle,
  Song, and Bailey]{ma2018characterizing}
Ma, X., Li, B., Wang, Y., Erfani, S.~M., Wijewickrema, S., Schoenebeck, G.,
  Houle, M.~E., Song, D., and Bailey, J.
\newblock Characterizing adversarial subspaces using local intrinsic
  dimensionality.
\newblock \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1gJ1L2aW}.
\newblock accepted as oral presentation.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry-adversarial}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock 2017.
\newblock URL \url{https://arxiv.org/abs/1706.06083}.

\bibitem[McLaren(1976)]{mclaren1976cielab}
McLaren, K.
\newblock Xiii—the development of the cie 1976 (l* a* b*) uniform colour
  space and colour‐difference formula.
\newblock \emph{Journal of the Society of Dyers and Colourists}, 92\penalty0
  (9):\penalty0 338--341, September 1976.
\newblock \doi{10.1111/j.1478-4408.1976.tb03301.x}.
\newblock URL
  \url{https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1478-4408.1976.tb03301.x}.

\bibitem[Meng \& Chen(2017)Meng and Chen]{meng2017magnet}
Meng, D. and Chen, H.
\newblock {MagNet}: a two-pronged defense against adversarial examples.
\newblock In \emph{ACM Conference on Computer and Communications Security
  (CCS)}, 2017.
\newblock arXiv preprint arXiv:1705.09064.

\bibitem[Moosavi{-}Dezfooli et~al.(2015)Moosavi{-}Dezfooli, Fawzi, and
  Frossard]{Moosavi-Dezfooli15}
Moosavi{-}Dezfooli, S., Fawzi, A., and Frossard, P.
\newblock Deepfool: a simple and accurate method to fool deep neural networks.
\newblock \emph{CoRR}, abs/1511.04599, 2015.
\newblock URL \url{http://arxiv.org/abs/1511.04599}.

\bibitem[Moosavi-Dezfooli et~al.(2017)Moosavi-Dezfooli, Fawzi, Fawzi, and
  Frossard]{cvpr2017:dezfooli}
Moosavi-Dezfooli, S.-M., Fawzi, A., Fawzi, O., and Frossard, P.
\newblock Universal adversarial perturbations.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2017.

\bibitem[Papernot et~al.(2016{\natexlab{a}})Papernot, McDaniel, and
  Goodfellow]{papernot-transferability}
Papernot, N., McDaniel, P., and Goodfellow, I.
\newblock Transferability in machine learning: from phenomena to black-box
  attacks using adversarial samples.
\newblock 2016{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/1605.07277}.

\bibitem[Papernot et~al.(2016{\natexlab{b}})Papernot, McDaniel, Jha,
  Fredrikson, Celik, and Swami]{sp2016:papernot}
Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z.~B., and Swami,
  A.
\newblock The limitations of deep learning in adversarial settings.
\newblock In \emph{IEEE European Symposium on Security \& Privacy},
  2016{\natexlab{b}}.

\bibitem[Papernot et~al.(2016{\natexlab{c}})Papernot, McDaniel, Wu, Jha, and
  Swami]{papernot2016distillation}
Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami, A.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In \emph{Security and Privacy (SP), 2016 IEEE Symposium on}, pp.\
  582--597. IEEE, 2016{\natexlab{c}}.

\bibitem[Papernot et~al.(2017)Papernot, McDaniel, Goodfellow, Jha, Celik, and
  Swami]{papernot17}
Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z.~B., and Swami,
  A.
\newblock Practical black-box attacks against machine learning.
\newblock In \emph{Proceedings of the 2017 ACM on Asia Conference on Computer
  and Communications Security}, ASIA CCS '17, pp.\  506--519, New York, NY,
  USA, 2017. ACM.
\newblock ISBN 978-1-4503-4944-4.
\newblock \doi{10.1145/3052973.3053009}.
\newblock URL \url{http://doi.acm.org/10.1145/3052973.3053009}.

\bibitem[Samangouei et~al.(2018)Samangouei, Kabkab, and
  Chellappa]{samangouei2018defensegan}
Samangouei, P., Kabkab, M., and Chellappa, R.
\newblock Defense-gan: Protecting classifiers against adversarial attacks using
  generative models.
\newblock \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=BkJ3ibb0-}.
\newblock accepted as poster.

\bibitem[Sharif et~al.(2016)Sharif, Bhagavatula, Bauer, and
  Reiter]{ccs2016:sharif}
Sharif, M., Bhagavatula, S., Bauer, L., and Reiter, M.~K.
\newblock Accessorize to a crime: Real and stealthy attacks on state-of-the-art
  face recognition.
\newblock In \emph{Proceedings of the 2016 ACM SIGSAC Conference on Computer
  and Communications Security}, CCS '16, pp.\  1528--1540, New York, NY, USA,
  2016. ACM.
\newblock ISBN 978-1-4503-4139-4.
\newblock \doi{10.1145/2976749.2978392}.
\newblock URL \url{http://doi.acm.org/10.1145/2976749.2978392}.

\bibitem[Song et~al.(2018)Song, Kim, Nowozin, Ermon, and
  Kushman]{song2018pixeldefend}
Song, Y., Kim, T., Nowozin, S., Ermon, S., and Kushman, N.
\newblock Pixeldefend: Leveraging generative models to understand and defend
  against adversarial examples.
\newblock \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=rJUYGxbCW}.
\newblock accepted as poster.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy-intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock 2013.
\newblock URL \url{https://arxiv.org/abs/1312.6199}.

\bibitem[Szegedy et~al.(2015)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy-inception}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock 2015.
\newblock URL \url{https://arxiv.org/abs/1512.00567}.

\bibitem[Xie et~al.(2018)Xie, Wang, Zhang, Ren, and Yuille]{xie2018mitigating}
Xie, C., Wang, J., Zhang, Z., Ren, Z., and Yuille, A.
\newblock Mitigating adversarial effects through randomization.
\newblock \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=Sk9yuql0Z}.
\newblock accepted as poster.

\bibitem[Zantedeschi et~al.(2017)Zantedeschi, Nicolae, and
  Rawat]{zantedeschi2017efficient}
Zantedeschi, V., Nicolae, M.-I., and Rawat, A.
\newblock Efficient defenses against adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1707.06728}, 2017.

\end{thebibliography}
