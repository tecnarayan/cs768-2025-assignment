\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2019)Abbasi-Yadkori, Bartlett, Bhatia, Lazic,
  Szepesvari, and Weisz]{politex}
Y.~Abbasi-Yadkori, P.~Bartlett, K.~Bhatia, N.~Lazic, C.~Szepesvari, and
  G.~Weisz.
\newblock \textsc{Politex}: Regret bounds for policy iteration using expert
  prediction.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of \emph{Proceedings of Machine Learning Research},
  pages 3692--3702. PMLR, 09--15 Jun 2019.

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Munos,
  Heess, and Riedmiller]{abdolmaleki2018maximum}
A.~Abdolmaleki, J.~T. Springenberg, Y.~Tassa, R.~Munos, N.~Heess, and
  M.~Riedmiller.
\newblock Maximum a posteriori policy optimisation.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=S1ANxQW0b}.

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and
  Abbeel]{achiam2017constrained}
J.~Achiam, D.~Held, A.~Tamar, and P.~Abbeel.
\newblock Constrained policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages 22--31,
  2017.

\bibitem[Bachem et~al.(2017)Bachem, Lucic, and Krause]{bachem2017practical}
O.~Bachem, M.~Lucic, and A.~Krause.
\newblock Practical coreset constructions for machine learning.
\newblock \emph{arXiv preprint arXiv:1703.06476}, 2017.

\bibitem[Bartlett(2009)]{bartlett09regal}
P.~L. Bartlett.
\newblock Regal: A regularization based algorithm for reinforcement learning in
  weakly communicating mdps.
\newblock In \emph{In Proceedings of the 25th Annual Conference on Uncertainty
  in Artificial Intelligence}, 2009.

\bibitem[Barto et~al.(1983)Barto, Sutton, and Anderson]{barto1983neuronlike}
A.~G. Barto, R.~S. Sutton, and C.~W. Anderson.
\newblock Neuronlike adaptive elements that can solve difficult learning
  control problems.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics}, \penalty0
  (5):\penalty0 834--846, 1983.

\bibitem[Borsos et~al.(2020)Borsos, Mutn{\`y}, and Krause]{borsos2020coresets}
Z.~Borsos, M.~Mutn{\`y}, and A.~Krause.
\newblock Coresets via bilevel optimization for continual learning and
  streaming.
\newblock \emph{arXiv preprint arXiv:2006.03875}, 2020.

\bibitem[Cao(1999)]{cao1999single}
X.-R. Cao.
\newblock Single sample path-based optimization of {M}arkov chains.
\newblock \emph{Journal of optimization theory and applications}, 100\penalty0
  (3):\penalty0 527--548, 1999.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{cesa2006prediction}
N.~Cesa-Bianchi and G.~Lugosi.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge university press, 2006.

\bibitem[Chaudhry et~al.(2018)Chaudhry, Ranzato, Rohrbach, and
  Elhoseiny]{chaudhry2018efficient}
A.~Chaudhry, M.~Ranzato, M.~Rohrbach, and M.~Elhoseiny.
\newblock Efficient lifelong learning with a-gem.
\newblock \emph{arXiv preprint arXiv:1812.00420}, 2018.

\bibitem[Cho and Meyer(2001)]{cho2001comparison}
G.~E. Cho and C.~D. Meyer.
\newblock Comparison of perturbation bounds for the stationary distribution of
  a {M}arkov chain.
\newblock \emph{Linear Algebra and its Applications}, 335\penalty0
  (1-3):\penalty0 137--150, 2001.

\bibitem[Degrave et~al.(2019)Degrave, Abdolmaleki, Springenberg, Heess, and
  Riedmiller]{degrave2019quinoa}
J.~Degrave, A.~Abdolmaleki, J.~T. Springenberg, N.~Heess, and M.~Riedmiller.
\newblock Quinoa: a {Q}-function you infer normalized over actions.
\newblock \emph{arXiv preprint arXiv:1911.01831}, 2019.

\bibitem[Even-Dar et~al.(2009)Even-Dar, Kakade, and Mansour]{even2009online}
E.~Even-Dar, S.~M. Kakade, and Y.~Mansour.
\newblock Online {M}arkov decision processes.
\newblock \emph{Mathematics of Operations Research}, 34\penalty0 (3):\penalty0
  726--736, 2009.

\bibitem[Farajtabar et~al.(2020)Farajtabar, Azizan, Mott, and
  Li]{farajtabar2020orthogonal}
M.~Farajtabar, N.~Azizan, A.~Mott, and A.~Li.
\newblock Orthogonal gradient descent for continual learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3762--3773, 2020.

\bibitem[Fruit et~al.(2018)Fruit, Pirotta, Lazaric, and
  Ortner]{fruit2018efficient}
R.~Fruit, M.~Pirotta, A.~Lazaric, and R.~Ortner.
\newblock Efficient bias-span-constrained exploration-exploitation in
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Hao et~al.(2020)Hao, Lazic, Abbasi-Yadkori, Joulani, and
  Szepesvari]{hao2020provably}
B.~Hao, N.~Lazic, Y.~Abbasi-Yadkori, P.~Joulani, and C.~Szepesvari.
\newblock Provably efficient adaptive approximate policy iteration.
\newblock \emph{arXiv preprint arXiv:2002.03069}, 2020.

\bibitem[Heess et~al.(2015)Heess, Wayne, Silver, Lillicrap, Erez, and
  Tassa]{heess2015learning}
N.~Heess, G.~Wayne, D.~Silver, T.~Lillicrap, T.~Erez, and Y.~Tassa.
\newblock Learning continuous control policies by stochastic value gradients.
\newblock \emph{Advances in Neural Information Processing Systems},
  28:\penalty0 2944--2952, 2015.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
T.~Jaksch, R.~Ortner, and P.~Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1563--1600, 2010.

\bibitem[Jian et~al.(2019)Jian, Fruit, Pirotta, and
  Lazaric]{jian2019exploration}
Q.~Jian, R.~Fruit, M.~Pirotta, and A.~Lazaric.
\newblock Exploration bonus for regret minimization in discrete and continuous
  average reward mdps.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4891--4900, 2019.

\bibitem[Jin et~al.(2019)Jin, Netrapalli, Ge, Kakade, and Jordan]{jin2019short}
C.~Jin, P.~Netrapalli, R.~Ge, S.~M. Kakade, and M.~I. Jordan.
\newblock A short note on concentration inequalities for random vectors with
  subgaussian norm.
\newblock \emph{arXiv preprint arXiv:1902.03736}, 2019.

\bibitem[Kakade and Langford(2002)]{kakade2002approximately}
S.~Kakade and J.~Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{ICML}, volume~2, pages 267--274, 2002.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
J.~Kirkpatrick, R.~Pascanu, N.~Rabinowitz, J.~Veness, G.~Desjardins, A.~A.
  Rusu, K.~Milan, J.~Quan, T.~Ramalho, A.~Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Konidaris et~al.(2011)Konidaris, Osentoski, and
  Thomas]{konidaris2011value}
G.~Konidaris, S.~Osentoski, and P.~Thomas.
\newblock Value function approximation in reinforcement learning using the
  fourier basis.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~25, 2011.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2016continuous}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{ICLR (Poster)}, 2016.

\bibitem[Lin(1992)]{lin1992self}
L.-J. Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 293--321, 1992.

\bibitem[Lopez-Paz and Ranzato(2017)]{lopez2017gradient}
D.~Lopez-Paz and M.~Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  6467--6476, 2017.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~Graves, I.~Antonoglou, D.~Wierstra, and
  M.~Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Neu et~al.(2014)Neu, Gy{\"o}rgy, Szepesv{\'a}ri, and Antos]{NeGySzA13}
G.~Neu, A.~Gy{\"o}rgy, C.~Szepesv{\'a}ri, and A.~Antos.
\newblock Online {M}arkov decision processes under bandit feedback.
\newblock \emph{IEEE Transactions on Automatic Control}, 59\penalty0
  (3):\penalty0 676--691, December 2014.

\bibitem[Ouyang et~al.(2017)Ouyang, Gagrani, Nayyar, and
  Jain]{ouyang2017learning}
Y.~Ouyang, M.~Gagrani, A.~Nayyar, and R.~Jain.
\newblock Learning unknown {M}arkov decision processes: A {T}hompson sampling
  approach.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1333--1342, 2017.

\bibitem[Rusu et~al.(2016)Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick,
  Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}
A.~A. Rusu, N.~C. Rabinowitz, G.~Desjardins, H.~Soyer, J.~Kirkpatrick,
  K.~Kavukcuoglu, R.~Pascanu, and R.~Hadsell.
\newblock Progressive neural networks.
\newblock \emph{arXiv preprint arXiv:1606.04671}, 2016.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
T.~Schaul, J.~Quan, I.~Antonoglou, and D.~Silver.
\newblock Prioritized experience replay.
\newblock In \emph{{International Conference on Learrning Representations
  (ICLR)}}, 2016.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
J.~Schulman, S.~Levine, P.~Abbeel, M.~Jordan, and P.~Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pages
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Seneta(1979)]{seneta1979coefficients}
E.~Seneta.
\newblock Coefficients of ergodicity: structure and applications.
\newblock \emph{Advances in applied probability}, pages 576--590, 1979.

\bibitem[Seneta(1988)]{seneta1988perturbation}
E.~Seneta.
\newblock Perturbation of the stationary distribution measured by ergodicity
  coefficients.
\newblock \emph{Advances in Applied Probability}, 20\penalty0 (1):\penalty0
  228--230, 1988.

\bibitem[Song et~al.(2019{\natexlab{a}})Song, Abdolmaleki, Springenberg, Clark,
  Soyer, Rae, Noury, Ahuja, Liu, Tirumala, Heess, Belov, Riedmiller, and
  Botvinick]{vmpo}
H.~F. Song, A.~Abdolmaleki, J.~T. Springenberg, A.~Clark, H.~Soyer, J.~W. Rae,
  S.~Noury, A.~Ahuja, S.~Liu, D.~Tirumala, N.~Heess, D.~Belov, M.~A.
  Riedmiller, and M.~M. Botvinick.
\newblock {V-MPO:} on-policy maximum a posteriori policy optimization for
  discrete and continuous control.
\newblock \emph{CoRR}, abs/1909.12238, 2019{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1909.12238}.

\bibitem[Song et~al.(2019{\natexlab{b}})Song, Abdolmaleki, Springenberg, Clark,
  Soyer, Rae, Noury, Ahuja, Liu, Tirumala, et~al.]{song2019v}
H.~F. Song, A.~Abdolmaleki, J.~T. Springenberg, A.~Clark, H.~Soyer, J.~W. Rae,
  S.~Noury, A.~Ahuja, S.~Liu, D.~Tirumala, et~al.
\newblock {V-MPO}: on-policy maximum a posteriori policy optimization for
  discrete and continuous control.
\newblock \emph{arXiv preprint arXiv:1909.12238}, 2019{\natexlab{b}}.

\bibitem[Talebi and Maillard(2018)]{talebi2018variance}
M.~S. Talebi and O.-A. Maillard.
\newblock Variance-aware regret bounds for undiscounted reinforcement learning
  in mdps.
\newblock In \emph{Algorithmic Learning Theory}, 2018.

\bibitem[Tassa et~al.(2018)Tassa, Doron, Muldal, Erez, Li, Casas, Budden,
  Abdolmaleki, Merel, Lefrancq, et~al.]{tassa2018deepmind}
Y.~Tassa, Y.~Doron, A.~Muldal, T.~Erez, Y.~Li, D.~d.~L. Casas, D.~Budden,
  A.~Abdolmaleki, J.~Merel, A.~Lefrancq, et~al.
\newblock Deep{M}ind control suite.
\newblock \emph{arXiv preprint arXiv:1801.00690}, 2018.

\bibitem[Tomar et~al.(2020)Tomar, Shani, Efroni, and
  Ghavamzadeh]{tomar2020mirror}
M.~Tomar, L.~Shani, Y.~Efroni, and M.~Ghavamzadeh.
\newblock Mirror descent policy optimization.
\newblock \emph{arXiv preprint arXiv:2005.09814}, 2020.

\bibitem[Tropp(2012)]{tropp2012user}
J.~A. Tropp.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock \emph{Foundations of computational mathematics}, 12\penalty0
  (4):\penalty0 389--434, 2012.

\bibitem[Vieillard et~al.(2020{\natexlab{a}})Vieillard, Kozuno, Scherrer,
  Pietquin, Munos, and Geist]{vieillard2020leverage}
N.~Vieillard, T.~Kozuno, B.~Scherrer, O.~Pietquin, R.~Munos, and M.~Geist.
\newblock Leverage the average: an analysis of regularization in rl.
\newblock \emph{arXiv preprint arXiv:2003.14089}, 2020{\natexlab{a}}.

\bibitem[Vieillard et~al.(2020{\natexlab{b}})Vieillard, Scherrer, Pietquin, and
  Geist]{vieillard2020momentum}
N.~Vieillard, B.~Scherrer, O.~Pietquin, and M.~Geist.
\newblock Momentum in reinforcement learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2529--2538, 2020{\natexlab{b}}.

\bibitem[Wei et~al.(2020{\natexlab{a}})Wei, Jafarnia-Jahromi, Luo, and
  Jain]{wei2020learning}
C.-Y. Wei, M.~Jafarnia-Jahromi, H.~Luo, and R.~Jain.
\newblock Learning infinite-horizon average-reward mdps with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:2007.11849}, 2020{\natexlab{a}}.

\bibitem[Wei et~al.(2020{\natexlab{b}})Wei, Jahromi, Luo, Sharma, and
  Jain]{wei2020model}
C.-Y. Wei, M.~J. Jahromi, H.~Luo, H.~Sharma, and R.~Jain.
\newblock Model-free reinforcement learning in infinite-horizon average-reward
  {Markov} decision processes.
\newblock In \emph{International Conference on Machine Learning}, pages
  10170--10180. PMLR, 2020{\natexlab{b}}.

\bibitem[Yin et~al.(2020)Yin, Farajtabar, and Li]{yin2020sola}
D.~Yin, M.~Farajtabar, and A.~Li.
\newblock {SOLA}: Continual learning with second-order loss approximation.
\newblock \emph{arXiv preprint arXiv:2006.10974}, 2020.

\bibitem[Yu(1994)]{Yu94}
B.~Yu.
\newblock Rates of convergence for empirical processes of stationary mixing
  sequences.
\newblock \emph{The Annals of Probability}, 22\penalty0 (1):\penalty0 94--116,
  1994.

\bibitem[Zenke et~al.(2017)Zenke, Poole, and Ganguli]{zenke2017continual}
F.~Zenke, B.~Poole, and S.~Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock \emph{Proceedings of machine learning research}, 70:\penalty0 3987,
  2017.

\end{thebibliography}
