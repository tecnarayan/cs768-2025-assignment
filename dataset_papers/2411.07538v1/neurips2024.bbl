\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Z.~Allen-Zhu, Y.~Li, and Z.~Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International conference on machine learning}, pages 242--252. PMLR, 2019.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
I.~Beltagy, M.~E. Peters, and A.~Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Bhojanapalli et~al.(2020)Bhojanapalli, Yun, Rawat, Reddi, and Kumar]{bhojanapalli2020low}
S.~Bhojanapalli, C.~Yun, A.~S. Rawat, S.~Reddi, and S.~Kumar.
\newblock Low-rank bottleneck in multi-head attention models.
\newblock In \emph{International conference on machine learning}, pages 864--873. PMLR, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2021)Chen, Zeng, Ji, and Yang]{chen2021skyformer}
Y.~Chen, Q.~Zeng, H.~Ji, and Y.~Yang.
\newblock Skyformer: Remodel self-attention with gaussian kernel and nystr$\backslash$" om method.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 2122--2135, 2021.

\bibitem[Danilova et~al.(2022)Danilova, Dvurechensky, Gasnikov, Gorbunov, Guminov, Kamzolov, and Shibaev]{danilova2022recent}
M.~Danilova, P.~Dvurechensky, A.~Gasnikov, E.~Gorbunov, S.~Guminov, D.~Kamzolov, and I.~Shibaev.
\newblock Recent theoretical advances in non-convex optimization.
\newblock In \emph{High-Dimensional Optimization and Probability: With a View Towards Data Science}, pages 79--163. Springer, 2022.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
S.~Du, J.~Lee, H.~Li, L.~Wang, and X.~Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International conference on machine learning}, pages 1675--1685. PMLR, 2019.

\bibitem[He et~al.(2020)He, Liu, Gao, and Chen]{he2020deberta}
P.~He, X.~Liu, J.~Gao, and W.~Chen.
\newblock Deberta: Decoding-enhanced bert with disentangled attention.
\newblock \emph{arXiv preprint arXiv:2006.03654}, 2020.

\bibitem[Huang et~al.(2020)Huang, Perez, Ba, and Volkovs]{huang2020improving}
X.~S. Huang, F.~Perez, J.~Ba, and M.~Volkovs.
\newblock Improving transformer optimization through better initialization.
\newblock In \emph{International Conference on Machine Learning}, pages 4475--4483. PMLR, 2020.

\bibitem[Huang et~al.(2023)Huang, Cheng, and Liang]{huang2023context}
Y.~Huang, Y.~Cheng, and Y.~Liang.
\newblock In-context convergence of transformers.
\newblock \emph{arXiv preprint arXiv:2310.05249}, 2023.

\bibitem[Jain et~al.(2017)Jain, Kar, et~al.]{jain2017non}
P.~Jain, P.~Kar, et~al.
\newblock Non-convex optimization for machine learning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 10\penalty0 (3-4):\penalty0 142--363, 2017.

\bibitem[Jin et~al.(2021)Jin, Netrapalli, Ge, Kakade, and Jordan]{jin2021nonconvex}
C.~Jin, P.~Netrapalli, R.~Ge, S.~M. Kakade, and M.~I. Jordan.
\newblock On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points.
\newblock \emph{Journal of the ACM (JACM)}, 68\penalty0 (2):\penalty0 1--29, 2021.

\bibitem[Langley(2000)]{langley00}
P.~Langley.
\newblock Crafting papers on machine learning.
\newblock In P.~Langley, editor, \emph{Proceedings of the 17th International Conference on Machine Learning (ICML 2000)}, pages 1207--1216, Stanford, CA, 2000. Morgan Kaufmann.

\bibitem[Li et~al.(2023)Li, Wang, and Ding]{li2023provable}
G.~Li, G.~Wang, and J.~Ding.
\newblock Provable identifiability of two-layer relu neural networks via lasso regularization.
\newblock \emph{IEEE Transactions on Information Theory}, 2023.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and Goldstein]{li2018visualizing}
H.~Li, Z.~Xu, G.~Taylor, C.~Studer, and T.~Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Linsley et~al.(2018)Linsley, Kim, Veerabadran, Windolf, and Serre]{linsley2018learning}
D.~Linsley, J.~Kim, V.~Veerabadran, C.~Windolf, and T.~Serre.
\newblock Learning long-range spatial dependencies with horizontal gated recurrent units.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Liu et~al.(2020)Liu, Liu, Gao, Chen, and Han]{liu2020understanding}
L.~Liu, X.~Liu, J.~Gao, W.~Chen, and J.~Han.
\newblock Understanding the difficulty of training transformers.
\newblock \emph{arXiv preprint arXiv:2004.08249}, 2020.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{liu2019roberta}
Y.~Liu, M.~Ott, N.~Goyal, J.~Du, M.~Joshi, D.~Chen, O.~Levy, M.~Lewis, L.~Zettlemoyer, and V.~Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and Potts]{maas2011learning}
A.~Maas, R.~E. Daly, P.~T. Pham, D.~Huang, A.~Y. Ng, and C.~Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies}, pages 142--150, 2011.

\bibitem[Nguyen and Mondelli(2020)]{nguyen2020global}
Q.~N. Nguyen and M.~Mondelli.
\newblock Global convergence of deep networks with one wide layer followed by pyramidal topology.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 11961--11972, 2020.

\bibitem[Noci et~al.(2022)Noci, Anagnostidis, Biggio, Orvieto, Singh, and Lucchi]{noci2022signal}
L.~Noci, S.~Anagnostidis, L.~Biggio, A.~Orvieto, S.~P. Singh, and A.~Lucchi.
\newblock Signal propagation in transformers: Theoretical perspectives and the role of rank collapse.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27198--27211, 2022.

\bibitem[Pan and Li(2023)]{pan2023toward}
Y.~Pan and Y.~Li.
\newblock Toward understanding why adam converges faster than sgd for transformers.
\newblock \emph{arXiv preprint arXiv:2306.00204}, 2023.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, I.~Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Shazeer(2020)]{shazeer2020glu}
N.~Shazeer.
\newblock Glu variants improve transformer.
\newblock \emph{arXiv preprint arXiv:2002.05202}, 2020.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang, Ruder, and Metzler]{tay2020long}
Y.~Tay, M.~Dehghani, S.~Abnar, Y.~Shen, D.~Bahri, P.~Pham, J.~Rao, L.~Yang, S.~Ruder, and D.~Metzler.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock \emph{arXiv preprint arXiv:2011.04006}, 2020.

\bibitem[Tian et~al.(2023)Tian, Wang, Chen, and Du]{tian2023scan}
Y.~Tian, Y.~Wang, B.~Chen, and S.~Du.
\newblock Scan and snap: Understanding training dynamics and token composition in 1-layer transformer.
\newblock \emph{arXiv preprint arXiv:2305.16380}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wu et~al.(2024)Wu, Liu, Chrysos, and Cevher]{wu2024convergence}
Y.~Wu, F.~Liu, G.~Chrysos, and V.~Cevher.
\newblock On the convergence of encoder-only shallow transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Zhang et~al.(2023)Zhang, Frei, and Bartlett]{zhang2023trained}
R.~Zhang, S.~Frei, and P.~L. Bartlett.
\newblock Trained transformers learn linear models in-context.
\newblock \emph{arXiv preprint arXiv:2306.09927}, 2023.

\end{thebibliography}
