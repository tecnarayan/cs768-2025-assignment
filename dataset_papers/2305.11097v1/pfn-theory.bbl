\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aky{\"u}rek et~al.(2023)Aky{\"u}rek, Schuurmans, Andreas, Ma, and
  Zhou]{akyurek2023what}
Aky{\"u}rek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D.
\newblock What learning algorithm is in-context learning? investigations with
  linear models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=0g0X4H8yN4I}.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Beltagy, I., Peters, M.~E., and Cohan, A.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Boucheron et~al.(2013)Boucheron, Lugosi, and
  Massart]{boucheron2013concentration}
Boucheron, S., Lugosi, G., and Massart, P.
\newblock \emph{Concentration inequalities: {A} nonasymptotic theory of
  independence}.
\newblock Oxford university press, 2013.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{NEURIPS2020_1457c0d6}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  1877--1901. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Dai et~al.(2022)Dai, Sun, Dong, Hao, Sui, and Wei]{dai2022can}
Dai, D., Sun, Y., Dong, L., Hao, Y., Sui, Z., and Wei, F.
\newblock Why can gpt learn in-context? language models secretly perform
  gradient descent as meta optimizers.
\newblock \emph{arXiv preprint arXiv:2212.10559}, 2022.

\bibitem[De~Blasi \& Walker(2013)De~Blasi and Walker]{de2013bayesian}
De~Blasi, P. and Walker, S.~G.
\newblock Bayesian asymptotics with misspecified models.
\newblock \emph{Statistica Sinica}, pp.\  169--187, 2013.

\bibitem[Derumigny \& Schmidt-Hieber(2020)Derumigny and
  Schmidt-Hieber]{derumigny2020lower}
Derumigny, A. and Schmidt-Hieber, J.
\newblock On lower bounds for the bias-variance trade-off.
\newblock \emph{arXiv preprint arXiv:2006.00278}, 2020.

\bibitem[Dong et~al.(2023)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, Li, and
  Sui]{dong2023survey}
Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., Li,
  L., and Sui, Z.
\newblock A survey on in-context learning, 2023.

\bibitem[Gao \& Pavel(2017)Gao and Pavel]{gao2017properties}
Gao, B. and Pavel, L.
\newblock On the properties of the softmax function with application in game
  theory and reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1704.00805}, 2017.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022can}
Garg, S., Tsipras, D., Liang, P.~S., and Valiant, G.
\newblock What can transformers learn in-context? a case study of simple
  function classes.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 30583--30598, 2022.

\bibitem[Ghosal \& van~der Vaart(2017)Ghosal and van~der
  Vaart]{ghosal2017fundamentals}
Ghosal, S. and van~der Vaart, A.
\newblock \emph{Fundamentals of nonparametric Bayesian inference}, volume~44.
\newblock Cambridge University Press, 2017.

\bibitem[Hollmann et~al.(2022)Hollmann, M{\"u}ller, Eggensperger, and
  Hutter]{hollmann2022tabpfn}
Hollmann, N., M{\"u}ller, S., Eggensperger, K., and Hutter, F.
\newblock Tab{PFN}: {A} transformer that solves small tabular classification
  problems in a second.
\newblock In \emph{NeurIPS 2022 First Table Representation Workshop}, 2022.
\newblock URL \url{https://openreview.net/forum?id=eu9fVjVasr4}.

\bibitem[Kirsch et~al.(2022)Kirsch, Harrison, Sohl-Dickstein, and
  Metz]{kirsch2022generalpurpose}
Kirsch, L., Harrison, J., Sohl-Dickstein, J., and Metz, L.
\newblock General-purpose in-context learning by meta-learning transformers,
  2022.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Kitaev, N., Kaiser, L., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[McDiarmid(1989)]{mcdiarmid_1989}
McDiarmid, C.
\newblock \emph{On the method of bounded differences}, pp.\  148â€“188.
\newblock London Mathematical Society Lecture Note Series. Cambridge University
  Press, 1989.
\newblock \doi{10.1017/CBO9781107359949.008}.

\bibitem[M{\"u}ller et~al.(2022)M{\"u}ller, Hollmann, Arango, Grabocka, and
  Hutter]{muller2021transformers}
M{\"u}ller, S., Hollmann, N., Arango, S.~P., Grabocka, J., and Hutter, F.
\newblock Transformers can do bayesian inference.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=KSugKcbNf9}.

\bibitem[Nguyen \& Grover(2022)Nguyen and Grover]{nguyen2022}
Nguyen, T. and Grover, A.
\newblock Transformer neural processes: Uncertainty-aware meta learning via
  sequence modeling.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and
  Sabato, S. (eds.), \emph{Proceedings of the 39th International Conference on
  Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pp.\  16569--16594. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/nguyen22b.html}.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan,
  Mann, Askell, Bai, Chen, Conerly, Drain, Ganguli, Hatfield-Dodds, Hernandez,
  Johnston, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan,
  McCandlish, and Olah]{olsson2022context}
Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T.,
  Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Ganguli, D.,
  Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J.,
  Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J.,
  McCandlish, S., and Olah, C.
\newblock In-context learning and induction heads.
\newblock \emph{Transformer Circuits Thread}, 2022.
\newblock
  https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.

\bibitem[Pearl(2009)]{pearl2009causality}
Pearl, J.
\newblock \emph{Causality}.
\newblock Cambridge university press, 2009.

\bibitem[Siegmund(1976)]{Siegmund76}
Siegmund, D.
\newblock {Importance Sampling in the Monte Carlo Study of Sequential Tests}.
\newblock \emph{The Annals of Statistics}, 4\penalty0 (4):\penalty0 673 -- 684,
  1976.
\newblock \doi{10.1214/aos/1176343541}.
\newblock URL \url{https://doi.org/10.1214/aos/1176343541}.

\bibitem[Thickstun(2021)]{thickstun2021transformer}
Thickstun, J.
\newblock The transformer model in equations.
\newblock \emph{University of Washington, Tech. Rep}, 2021.
\newblock URL \url{https://johnthickstun.com/docs/transformers.pdf}.

\bibitem[Vaart \& Wellner(1996)Vaart and Wellner]{vaart1996weak}
Vaart, A.~W. and Wellner, J.~A.
\newblock Weak convergence.
\newblock In \emph{Weak convergence and empirical processes}, pp.\  16--28.
  Springer, 1996.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[von Oswald et~al.(2022)von Oswald, Niklasson, Randazzo, Sacramento,
  Mordvintsev, Zhmoginov, and Vladymyrov]{von2022transformers}
von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A.,
  Zhmoginov, A., and Vladymyrov, M.
\newblock Transformers learn in-context by gradient descent.
\newblock \emph{arXiv preprint arXiv:2212.07677}, 2022.

\bibitem[Wand \& Jones(1994)Wand and Jones]{wand1994kernel}
Wand, M.~P. and Jones, M.~C.
\newblock \emph{Kernel smoothing}.
\newblock CRC press, 1994.

\bibitem[Wei et~al.(2022)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le]{wei2022finetuned}
Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A.~W., Lester, B., Du, N., Dai,
  A.~M., and Le, Q.~V.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=gEZrGCozdqR}.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, et~al.]{zaheer2020big}
Zaheer, M., Guruganesh, G., Dubey, K.~A., Ainslie, J., Alberti, C., Ontanon,
  S., Pham, P., Ravula, A., Wang, Q., Yang, L., et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17283--17297, 2020.

\end{thebibliography}
