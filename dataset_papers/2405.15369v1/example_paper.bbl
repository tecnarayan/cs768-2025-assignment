\begin{thebibliography}{100}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arndt et~al.(2019)Arndt, Hazara, Ghadirzadeh, and
  Kyrki]{Arndt2019MetaRL}
Arndt, K., Hazara, M., Ghadirzadeh, A., and Kyrki, V.
\newblock Meta reinforcement learning for sim-to-real domain adaptation.
\newblock In \emph{IEEE International Conference on Robotics and Automation},
  2019.

\bibitem[Barekatain et~al.(2019)Barekatain, Yonetani, and
  Hamaya]{barekatain2019multipolar}
Barekatain, M., Yonetani, R., and Hamaya, M.
\newblock Multipolar: Multi-source policy aggregation for transfer
  reinforcement learning between diverse environmental dynamics.
\newblock \emph{arXiv preprint arXiv:1909.13111}, 2019.

\bibitem[Barreto et~al.(2016)Barreto, Dabney, Munos, Hunt, Schaul, Silver, and
  Hasselt]{Barreto2016SuccessorFF}
Barreto, A., Dabney, W., Munos, R., Hunt, J.~J., Schaul, T., Silver, D., and
  Hasselt, H.~V.
\newblock Successor features for transfer in reinforcement learning.
\newblock \emph{ArXiv}, abs/1606.05312, 2016.

\bibitem[Bengio et~al.(2012)Bengio, Courville, and
  Vincent]{Bengio2012RepresentationLA}
Bengio, Y., Courville, A.~C., and Vincent, P.
\newblock Representation learning: A review and new perspectives.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 35:\penalty0 1798--1828, 2012.

\bibitem[Bousmalis et~al.(2018)Bousmalis, Irpan, Wohlhart, Bai, Kelcey,
  Kalakrishnan, Downs, Ibarz, Pastor, Konolige, Levine, and
  Vanhoucke]{Bousmalis2018UsingSA}
Bousmalis, K., Irpan, A., Wohlhart, P., Bai, Y., Kelcey, M., Kalakrishnan, M.,
  Downs, L., Ibarz, J., Pastor, P., Konolige, K., Levine, S., and Vanhoucke, V.
\newblock Using simulation and domain adaptation to improve efficiency of deep
  robotic grasping.
\newblock In \emph{IEEE International Conference on Robotics and Automation},
  2018.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{Brockman2016OpenAIG}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Open{AI} {G}ym.
\newblock \emph{ArXiv}, abs/1606.01540, 2016.

\bibitem[Buckman et~al.(2018)Buckman, Hafner, Tucker, Brevdo, and
  Lee]{Buckman2018SampleEfficientRL}
Buckman, J., Hafner, D., Tucker, G., Brevdo, E., and Lee, H.
\newblock Sample-{E}fficient {R}einforcement {L}earning with {S}tochastic
  {E}nsemble {V}alue {E}xpansion.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Cetin et~al.(2022)Cetin, Ball, Roberts, and
  Çeliktutan]{Cetin2022StabilizingOD}
Cetin, E., Ball, P.~J., Roberts, S., and Çeliktutan, O.
\newblock Stabilizing off-policy deep reinforcement learning from pixels.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Chandak et~al.(2019)Chandak, Theocharous, Kostas, Jordan, and
  Thomas]{Chandak2019LearningAR}
Chandak, Y., Theocharous, G., Kostas, J.~E., Jordan, S.~M., and Thomas, P.~S.
\newblock Learning action representations for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Chebotar et~al.(2018)Chebotar, Handa, Makoviychuk, Macklin, Issac,
  Ratliff, and Fox]{Chebotar2018ClosingTS}
Chebotar, Y., Handa, A., Makoviychuk, V., Macklin, M., Issac, J., Ratliff,
  N.~D., and Fox, D.
\newblock Closing the sim-to-real loop: Adapting simulation randomization with
  real world experience.
\newblock In \emph{International Conference on Robotics and Automation}, 2018.

\bibitem[Clavera et~al.(2018)Clavera, Nagabandi, Fearing, Abbeel, Levine, and
  Finn]{Clavera2018LearningTA}
Clavera, I., Nagabandi, A., Fearing, R.~S., Abbeel, P., Levine, S., and Finn,
  C.
\newblock Learning to adapt: Meta-learning for model-based control.
\newblock \emph{ArXiv}, abs/1803.11347, 2018.

\bibitem[Collins et~al.(2020)Collins, Brown, Leitner, and
  Howard]{Collins2020TraversingTR}
Collins, J.~J., Brown, R., Leitner, J., and Howard, D.
\newblock Traversing the reality gap via simulator tuning.
\newblock \emph{ArXiv}, abs/2003.01369, 2020.

\bibitem[Csisz{\'a}r \& K{\"o}rner(2011)Csisz{\'a}r and
  K{\"o}rner]{csiszar2011information}
Csisz{\'a}r, I. and K{\"o}rner, J.
\newblock \emph{Information theory: coding theorems for discrete memoryless
  systems}.
\newblock Cambridge University Press, 2011.

\bibitem[Cutler \& How(2015)Cutler and How]{Cutler2015EfficientRL}
Cutler, M. and How, J.~P.
\newblock {Efficient Reinforcement Learning for Robots using Informative
  Simulated Priors}.
\newblock In \emph{IEEE International Conference on Robotics and Automation},
  2015.

\bibitem[Desai et~al.(2020)Desai, Durugkar, Karnan, Warnell, Hanna, and
  Stone]{desai2020imitation}
Desai, S., Durugkar, I., Karnan, H., Warnell, G., Hanna, J., and Stone, P.
\newblock An imitation from observation approach to transfer learning with
  dynamics mismatch.
\newblock In \emph{Neural Information Processing Systems}, 2020.

\bibitem[Du et~al.(2021)Du, Watkins, Darrell, Abbeel, and
  Pathak]{Du2021AutoTunedST}
Du, Y., Watkins, O., Darrell, T., Abbeel, P., and Pathak, D.
\newblock Auto-tuned sim-to-real transfer.
\newblock In \emph{IEEE International Conference on Robotics and Automation},
  2021.

\bibitem[Eysenbach et~al.(2021)Eysenbach, Chaudhari, Asawa, Levine, and
  Salakhutdinov]{eysenbach2021offdynamics}
Eysenbach, B., Chaudhari, S., Asawa, S., Levine, S., and Salakhutdinov, R.
\newblock Off-dynamics reinforcement learning: Training for transfer with
  domain classifiers.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=eqBwg3AcIAK}.

\bibitem[Eysenbach et~al.(2022)Eysenbach, Zhang, Salakhutdinov, and
  Levine]{Eysenbach2022ContrastiveLA}
Eysenbach, B., Zhang, T., Salakhutdinov, R., and Levine, S.
\newblock Contrastive learning as goal-conditioned reinforcement learning.
\newblock \emph{ArXiv}, abs/2206.07568, 2022.

\bibitem[Farchy et~al.(2013)Farchy, Barrett, MacAlpine, and
  Stone]{Farchy2013HumanoidRL}
Farchy, A., Barrett, S., MacAlpine, P., and Stone, P.
\newblock Humanoid robots learning to walk faster: from the real world to
  simulation and back.
\newblock In \emph{Adaptive Agents and Multi-Agent Systems}, 2013.

\bibitem[Ferns et~al.(2011)Ferns, Panangaden, and
  Precup]{ferns2011bisimulation}
Ferns, N., Panangaden, P., and Precup, D.
\newblock Bisimulation metrics for continuous markov decision processes.
\newblock \emph{SIAM Journal on Computing}, 40\penalty0 (6):\penalty0
  1662--1714, 2011.

\bibitem[Fickinger et~al.(2022)Fickinger, Cohen, Russell, and
  Amos]{fickinger2022crossdomain}
Fickinger, A., Cohen, S., Russell, S., and Amos, B.
\newblock Cross-domain imitation learning via optimal transport.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=xP3cPq2hQC}.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{Fu2020D4RLDF}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock D4{RL}: {D}atasets for {D}eep {D}ata-{D}riven {R}einforcement
  {L}earning.
\newblock \emph{ArXiv}, abs/2004.07219, 2020.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{Fujimoto2021AMA}
Fujimoto, S. and Gu, S.~S.
\newblock A {M}inimalist {A}pproach to {O}ffline {R}einforcement {L}earning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and
  Precup]{Fujimoto2019OffPolicyDR}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-{P}olicy {D}eep {R}einforcement {L}earning without {E}xploration.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Fujimoto et~al.(2021)Fujimoto, Meger, and Precup]{Fujimoto2021ADR}
Fujimoto, S., Meger, D., and Precup, D.
\newblock A deep reinforcement learning approach to marginalized importance
  sampling with the successor representation.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Fujimoto et~al.(2023)Fujimoto, Chang, Smith, Gu, Precup, and
  Meger]{Fujimoto2023ForSS}
Fujimoto, S., Chang, W.-D., Smith, E.~J., Gu, S.~S., Precup, D., and Meger, D.
\newblock For sale: State-action representation learning for deep reinforcement
  learning.
\newblock \emph{ArXiv}, abs/2306.02451, 2023.

\bibitem[Gamrian \& Goldberg(2018)Gamrian and Goldberg]{Gamrian2018TransferLF}
Gamrian, S. and Goldberg, Y.
\newblock Transfer learning for related reinforcement learning tasks via
  image-to-image translation.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Ge et~al.(2022)Ge, Macaluso, Li, Luo, and Wang]{Ge2022PolicyAF}
Ge, Y., Macaluso, A., Li, E.~L., Luo, P., and Wang, X.
\newblock Policy adaptation from foundation model feedback.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2022.

\bibitem[Golemo et~al.(2018)Golemo, Ta{\"i}ga, Courville, and
  Oudeyer]{Golemo2018SimtoRealTW}
Golemo, F., Ta{\"i}ga, A.~A., Courville, A.~C., and Oudeyer, P.-Y.
\newblock Sim-to-real transfer with neural-augmented robot simulation.
\newblock In \emph{Conference on Robot Learning}, 2018.

\bibitem[Gui et~al.(2023)Gui, Pang, Yu, Qiao, Qi, He, Wang, and
  Zhai]{gui2023cross}
Gui, H., Pang, S., Yu, S., Qiao, S., Qi, Y., He, X., Wang, M., and Zhai, X.
\newblock Cross-domain policy adaptation with dynamics alignment.
\newblock \emph{Neural Networks}, 167:\penalty0 104--117, 2023.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Hartikainen, Tucker, Ha, Tan,
  Kumar, Zhu, Gupta, Abbeel, et~al.]{haarnoja2018softactorcritic}
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar,
  V., Zhu, H., Gupta, A., Abbeel, P., et~al.
\newblock {Soft Actor-Critic Algorithms and Applications}.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018.

\bibitem[Hafner et~al.(2019)Hafner, Lillicrap, Fischer, Villegas, Ha, Lee, and
  Davidson]{hafner2019learning}
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and
  Davidson, J.
\newblock Learning latent dynamics for planning from pixels.
\newblock In \emph{International conference on machine learning}, 2019.

\bibitem[Hanna et~al.(2021)Hanna, Desai, Karnan, Warnell, and
  Stone]{Hanna2021GroundedAT}
Hanna, J.~P., Desai, S., Karnan, H., Warnell, G.~A., and Stone, P.
\newblock Grounded action transformation for sim-to-real reinforcement
  learning.
\newblock \emph{Machine Learning}, 110:\penalty0 2469 -- 2499, 2021.

\bibitem[Hansen et~al.(2021)Hansen, Jangir, Sun, Aleny{\`a}, Abbeel, Efros,
  Pinto, and Wang]{hansen2021selfsupervised}
Hansen, N., Jangir, R., Sun, Y., Aleny{\`a}, G., Abbeel, P., Efros, A.~A.,
  Pinto, L., and Wang, X.
\newblock Self-supervised policy adaptation during deployment.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=o_V-MjyyGV_}.

\bibitem[Hansen et~al.(2022)Hansen, Wang, and Su]{Hansen2022TemporalDL}
Hansen, N., Wang, X., and Su, H.
\newblock {Temporal Difference Learning for Model Predictive Control}.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{He2015DeepRL}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  2015.

\bibitem[Hejna et~al.(2020)Hejna, Abbeel, and Pinto]{Hejna2020HierarchicallyDI}
Hejna, D.~J., Abbeel, P., and Pinto, L.
\newblock Hierarchically decoupled imitation for morphological transfer.
\newblock \emph{ArXiv}, abs/2003.01709, 2020.

\bibitem[Hwangbo et~al.(2019)Hwangbo, Lee, Dosovitskiy, Bellicoso, Tsounis,
  Koltun, and Hutter]{Hwangbo2019LearningAA}
Hwangbo, J., Lee, J., Dosovitskiy, A., Bellicoso, D., Tsounis, V., Koltun, V.,
  and Hutter, M.
\newblock Learning agile and dynamic motor skills for legged robots.
\newblock \emph{Science Robotics}, 4, 2019.

\bibitem[Janner et~al.(2019)Janner, Fu, Zhang, and Levine]{Janner2019WhenTT}
Janner, M., Fu, J., Zhang, M., and Levine, S.
\newblock When to {T}rust {Y}our {M}odel: {M}odel-{B}ased {P}olicy
  {O}ptimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Jiang et~al.(2023)Jiang, Li, Dai, Zou, and Xiong]{Jiang2023VarianceRD}
Jiang, Y., Li, C., Dai, W., Zou, J., and Xiong, H.
\newblock Variance reduced domain randomization for reinforcement learning with
  policy gradient.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 46:\penalty0 1031--1048, 2023.

\bibitem[Karl et~al.(2016)Karl, S{\"o}lch, Bayer, and van~der
  Smagt]{Karl2016DeepVB}
Karl, M., S{\"o}lch, M., Bayer, J., and van~der Smagt, P.
\newblock Deep variational bayes filters: Unsupervised learning of state space
  models from raw data.
\newblock \emph{ArXiv}, abs/1605.06432, 2016.

\bibitem[Kim et~al.(2019)Kim, Gu, Song, Zhao, and Ermon]{Kim2019DomainAI}
Kim, K., Gu, Y., Song, J., Zhao, S., and Ermon, S.
\newblock Domain adaptive imitation learning.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Kim et~al.(2022)Kim, Ha, and Kim]{Kim2022SelfPredictiveDF}
Kim, K., Ha, J., and Kim, Y.
\newblock Self-predictive dynamics for generalization of vision-based
  reinforcement learning.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  2022.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{KingmaB14adam}
Kingma, D.~P. and Ba, J.
\newblock {Adam: A Method for Stochastic Optimization}.
\newblock In \emph{International Conference on Learning Representation}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6980}.

\bibitem[Kiran et~al.(2020)Kiran, Sobh, Talpaert, Mannion, Sallab, Yogamani,
  and P'erez]{Kiran2020DeepRL}
Kiran, B.~R., Sobh, I., Talpaert, V., Mannion, P., Sallab, A. A.~A., Yogamani,
  S.~K., and P'erez, P.
\newblock Deep reinforcement learning for autonomous driving: A survey.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems},
  23:\penalty0 4909--4926, 2020.

\bibitem[Kober et~al.(2013)Kober, Bagnell, and
  Peters]{Kober2013ReinforcementLI}
Kober, J., Bagnell, J.~A., and Peters, J.
\newblock Reinforcement learning in robotics: A survey.
\newblock \emph{The International Journal of Robotics Research}, 32:\penalty0
  1238 -- 1274, 2013.

\bibitem[Kolesnikov et~al.(2019)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{Kolesnikov2019BigT}
Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and
  Houlsby, N.
\newblock Big transfer (bit): General visual representation learning.
\newblock In \emph{European Conference on Computer Vision}, 2019.

\bibitem[Kostrikov et~al.(2020)Kostrikov, Yarats, and
  Fergus]{Kostrikov2020ImageAI}
Kostrikov, I., Yarats, D., and Fergus, R.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock \emph{ArXiv}, abs/2004.13649, 2020.

\bibitem[Kostrikov et~al.(2022)Kostrikov, Nair, and
  Levine]{kostrikov2022offline}
Kostrikov, I., Nair, A., and Levine, S.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=68n2s9ZJWF8}.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{Kumar2020ConservativeQF}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative {Q}-{L}earning for {O}ffline {R}einforcement {L}earning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Langley(2000)]{langley00}
Langley, P.
\newblock Crafting papers on machine learning.
\newblock In Langley, P. (ed.), \emph{International Conference on Machine
  Learning (ICML 2000)}, pp.\  1207--1216, Stanford, CA, 2000. Morgan Kaufmann.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{Levine2020OfflineRL}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline {R}einforcement {L}earning: {T}utorial, {R}eview, and
  {P}erspectives on {O}pen {P}roblems.
\newblock \emph{ArXiv}, abs/2005.01643, 2020.

\bibitem[Liu et~al.(2021)Liu, Zhang, Zhao, Qin, Zhu, Jian, Yu, and
  Liu]{liu2021returnbased}
Liu, G., Zhang, C., Zhao, L., Qin, T., Zhu, J., Jian, L., Yu, N., and Liu,
  T.-Y.
\newblock Return-based contrastive representation learning for reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=_TM6rT7tXke}.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Hongyin, and Wang]{liu2022dara}
Liu, J., Hongyin, Z., and Wang, D.
\newblock {DARA}: Dynamics-aware reward augmentation in offline reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=9SDQB3b68K}.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Pathak, and
  Kitani]{Liu2022REvolveRCE}
Liu, X., Pathak, D., and Kitani, K.~M.
\newblock Revolver: Continuous evolutionary models for robot-to-robot policy
  transfer.
\newblock In \emph{International Conference on Machine Learning},
  2022{\natexlab{b}}.

\bibitem[Luo et~al.(2019)Luo, Xu, Li, Tian, Darrell, and
  Ma]{luo2018algorithmic}
Luo, Y., Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T.
\newblock Algorithmic framework for model-based deep reinforcement learning
  with theoretical guarantees.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=BJe1E2R5KX}.

\bibitem[Lyu et~al.(2022{\natexlab{a}})Lyu, aicheng Gong, Wan, Lu, and
  Li]{lyu2022state}
Lyu, J., aicheng Gong, Wan, L., Lu, Z., and Li, X.
\newblock State advantage weighting for offline {RL}.
\newblock In \emph{3rd Offline RL Workshop: Offline RL as a ''Launchpad''},
  2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=2rOD_UQfvl}.

\bibitem[Lyu et~al.(2022{\natexlab{b}})Lyu, Li, and Lu]{lyu2022double}
Lyu, J., Li, X., and Lu, Z.
\newblock Double check your state before trusting it: Confidence-aware
  bidirectional offline model-based imagination.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=3e3IQMLDSLP}.

\bibitem[Lyu et~al.(2022{\natexlab{c}})Lyu, Ma, Li, and Lu]{lyu2022mildly}
Lyu, J., Ma, X., Li, X., and Lu, Z.
\newblock Mildly conservative q-learning for offline reinforcement learning.
\newblock In \emph{Neural Information Processing Systems}, 2022{\natexlab{c}}.

\bibitem[Lyu et~al.(2023)Lyu, Wan, Lu, and Li]{Lyu2023OffPolicyRA}
Lyu, J., Wan, L., Lu, Z., and Li, X.
\newblock Off-policy rl algorithms can be sample-efficient for continuous
  control via sample multiple reuse.
\newblock \emph{ArXiv}, abs/2305.18443, 2023.

\bibitem[Machado et~al.(2023)Machado, Barreto, Precup, and
  Bowling]{machado2023temporal}
Machado, M.~C., Barreto, A., Precup, D., and Bowling, M.
\newblock Temporal abstraction in reinforcement learning with the successor
  representation.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0
  (80):\penalty0 1--69, 2023.

\bibitem[Malik et~al.(2019)Malik, Kuleshov, Song, Nemer, Seymour, and
  Ermon]{Malik2019CalibratedMD}
Malik, A., Kuleshov, V., Song, J., Nemer, D., Seymour, H., and Ermon, S.
\newblock Calibrated model-based deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Mehta et~al.(2019)Mehta, Diaz, Golemo, Pal, and
  Paull]{Mehta2019ActiveDR}
Mehta, B., Diaz, M., Golemo, F., Pal, C.~J., and Paull, L.
\newblock Active domain randomization.
\newblock \emph{ArXiv}, abs/1904.04762, 2019.

\bibitem[Nagabandi et~al.(2018)Nagabandi, Clavera, Liu, Fearing, Abbeel,
  Levine, and Finn]{nagabandi2018learning}
Nagabandi, A., Clavera, I., Liu, S., Fearing, R.~S., Abbeel, P., Levine, S.,
  and Finn, C.
\newblock Learning to adapt in dynamic, real-world environments through
  meta-reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1803.11347}, 2018.

\bibitem[Niu et~al.(2022)Niu, Sharma, Qiu, Li, Zhou, HU, and Zhan]{niu2022when}
Niu, H., Sharma, S., Qiu, Y., Li, M., Zhou, G., HU, J., and Zhan, X.
\newblock When to trust your simulator: Dynamics-aware hybrid
  offline-and-online reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=zXE8iFOZKw}.

\bibitem[Osinski et~al.(2019)Osinski, Jakubowski, Milos, Ziecina, Galias,
  Homoceanu, and Michalewski]{Osinski2019SimulationBasedRL}
Osinski, B., Jakubowski, A., Milos, P., Ziecina, P., Galias, C., Homoceanu, S.,
  and Michalewski, H.
\newblock Simulation-based reinforcement learning for real-world autonomous
  driving.
\newblock In \emph{IEEE International Conference on Robotics and Automation},
  2019.

\bibitem[Ota et~al.(2020)Ota, Oiki, Jha, Mariyama, and Nikovski]{ota2020can}
Ota, K., Oiki, T., Jha, D., Mariyama, T., and Nikovski, D.
\newblock Can increasing input dimensionality improve deep reinforcement
  learning?
\newblock In \emph{International conference on machine learning}, 2020.

\bibitem[Pan et~al.(2020)Pan, He, Tu, and He]{Pan2020TrustTM}
Pan, F., He, J., Tu, D., and He, Q.
\newblock {Trust the Model When It Is Confident: Masked Model-based
  Actor-Critic}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Peng et~al.(2017)Peng, Andrychowicz, Zaremba, and
  Abbeel]{Peng2017SimtoRealTO}
Peng, X.~B., Andrychowicz, M., Zaremba, W., and Abbeel, P.
\newblock Sim-to-real transfer of robotic control with dynamics randomization.
\newblock In \emph{2018 IEEE International Conference on Robotics and
  Automation (ICRA)}, 2017.

\bibitem[Qiao et~al.(2023)Qiao, Lyu, and Li]{Qiao2023ThePB}
Qiao, Z., Lyu, J., and Li, X.
\newblock The primacy bias in model-based rl.
\newblock \emph{ArXiv}, abs/2310.15017, 2023.

\bibitem[Rafailov et~al.(2020)Rafailov, Yu, Rajeswaran, and
  Finn]{Rafailov2020OfflineRL}
Rafailov, R., Yu, T., Rajeswaran, A., and Finn, C.
\newblock Offline reinforcement learning from images with latent space models.
\newblock In \emph{Conference on Learning for Dynamics \& Control}, 2020.

\bibitem[Raileanu et~al.(2020)Raileanu, Goldstein, and
  Szlam]{Raileanu2020FastAT}
Raileanu, R., Goldstein, M., and Szlam, A.
\newblock Fast adaptation to new environments via policy-dynamics value
  functions.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Ramos et~al.(2019)Ramos, Possas, and Fox]{Ramos2019BayesSimAD}
Ramos, F.~T., Possas, R., and Fox, D.
\newblock Bayessim: adaptive domain randomization via probabilistic inference
  for robotics simulators.
\newblock \emph{ArXiv}, abs/1906.01728, 2019.

\bibitem[Raychaudhuri et~al.(2021)Raychaudhuri, Paul, Vanbaar, and
  Roy-Chowdhury]{raychaudhuri2021cross}
Raychaudhuri, D.~S., Paul, S., Vanbaar, J., and Roy-Chowdhury, A.~K.
\newblock Cross-domain imitation from observations.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Rezaei-Shoshtari et~al.(2022)Rezaei-Shoshtari, Zhao, Panangaden,
  Meger, and Precup]{rezaei-shoshtari2022continuous}
Rezaei-Shoshtari, S., Zhao, R., Panangaden, P., Meger, D., and Precup, D.
\newblock Continuous {MDP} homomorphisms and homomorphic policy gradient.
\newblock In \emph{Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Adl-fs-8OzL}.

\bibitem[Schwarzer et~al.(2021)Schwarzer, Anand, Goel, Hjelm, Courville, and
  Bachman]{schwarzer2021dataefficient}
Schwarzer, M., Anand, A., Goel, R., Hjelm, R.~D., Courville, A., and Bachman,
  P.
\newblock {Data-Efficient Reinforcement Learning with Self-Predictive
  Representations}.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=uCQfPZwRaUu}.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, van~den
  Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman,
  Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel,
  and Hassabis]{Silver2016MasteringTG}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., van~den Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I.,
  Lillicrap, T.~P., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D.
\newblock {Mastering the Game of Go with Deep Neural Networks and Tree Search}.
\newblock \emph{Nature}, 529:\penalty0 484--489, 2016.

\bibitem[Slaoui et~al.(2019)Slaoui, Clements, Foerster, and
  Toth]{Slaoui2019RobustDR}
Slaoui, R.~B., Clements, W.~R., Foerster, J.~N., and Toth, S.
\newblock Robust domain randomization for reinforcement learning.
\newblock \emph{ArXiv}, abs/1910.10537, 2019.

\bibitem[Srinivas et~al.(2020)Srinivas, Laskin, and Abbeel]{Srinivas2020CURLCU}
Srinivas, A., Laskin, M., and Abbeel, P.
\newblock {CURL: Contrastive Unsupervised Representations for Reinforcement
  Learning}.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Stooke et~al.(2021)Stooke, Lee, Abbeel, and
  Laskin]{stooke2021decoupling}
Stooke, A., Lee, K., Abbeel, P., and Laskin, M.
\newblock Decoupling representation learning from reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Tang et~al.(2022)Tang, Guo, Richemond, Pires, Chandak, Munos, Rowland,
  Azar, Lan, Lyle, Gyorgy, Thakoor, Dabney, Piot, Calandriello, and
  Vaĺko]{Tang2022UnderstandingSL}
Tang, Y., Guo, Z.~D., Richemond, P.~H., Pires, B.~{\'A}., Chandak, Y., Munos,
  R., Rowland, M., Azar, M.~G., Lan, C.~L., Lyle, C., Gyorgy, A., Thakoor, S.,
  Dabney, W., Piot, B., Calandriello, D., and Vaĺko, M.
\newblock Understanding self-predictive learning for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Tobin et~al.(2017)Tobin, Fong, Ray, Schneider, Zaremba, and
  Abbeel]{Tobin2017DomainRF}
Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P.
\newblock Domain randomization for transferring deep neural networks from
  simulation to the real world.
\newblock In \emph{IEEE/RSJ International Conference on Intelligent Robots and
  Systems}, 2017.

\bibitem[van~der Pol et~al.(2020)van~der Pol, Worrall, van Hoof, Oliehoek, and
  Welling]{Pol2020MDPHN}
van~der Pol, E., Worrall, D.~E., van Hoof, H., Oliehoek, F.~A., and Welling, M.
\newblock Mdp homomorphic networks: Group symmetries in reinforcement learning.
\newblock In \emph{Neural Information Processing Systems}, 2020.

\bibitem[Viano et~al.(2020)Viano, ting Huang, Kamalaruban, and
  Cevher]{Viano2020RobustIR}
Viano, L., ting Huang, Y., Kamalaruban, P., and Cevher, V.
\newblock Robust inverse reinforcement learning under transition dynamics
  mismatch.
\newblock In \emph{Neural Information Processing Systems}, 2020.

\bibitem[Vuong et~al.(2019)Vuong, Vikram, Su, Gao, and
  Christensen]{Vuong2019HowTP}
Vuong, Q.~H., Vikram, S., Su, H., Gao, S., and Christensen, H.~I.
\newblock How to pick the domain randomization parameters for sim-to-real
  transfer of reinforcement learning policies?
\newblock \emph{ArXiv}, abs/1903.11774, 2019.

\bibitem[Whitney et~al.(2020)Whitney, Agarwal, Cho, and
  Gupta]{Whitney2020Dynamics-Aware}
Whitney, W., Agarwal, R., Cho, K., and Gupta, A.
\newblock Dynamics-aware embeddings.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJgZGeHFPH}.

\bibitem[Wu et~al.(2022)Wu, Xie, Lian, Wang, Guo, Chen, Schaal, and
  Tomizuka]{Wu2022ZeroShotPT}
Wu, Z., Xie, Y., Lian, W., Wang, C., Guo, Y., Chen, J., Schaal, S., and
  Tomizuka, M.
\newblock Zero-shot policy transfer with disentangled task representation of
  meta-reinforcement learning.
\newblock In \emph{IEEE International Conference on Robotics and Automation},
  2022.

\bibitem[Xie et~al.(2022)Xie, Sodhani, Finn, Pineau, and
  Zhang]{Xie2022RobustPL}
Xie, A., Sodhani, S., Finn, C., Pineau, J., and Zhang, A.
\newblock Robust policy learning over multiple uncertainty sets.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Xiong et~al.(2023)Xiong, Beck, and Whiteson]{xiong2023universal}
Xiong, Z., Beck, J., and Whiteson, S.
\newblock Universal morphology control via contextual modulation.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Xu et~al.(2023)Xu, Bai, Ma, Wang, Zhao, Wang, Li, and
  Li]{Xu2023CrossDomainPA}
Xu, K., Bai, C., Ma, X., Wang, D., Zhao, B., Wang, Z., Li, X., and Li, W.
\newblock Cross-domain policy adaptation via value-guided data filtering.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=qdM260dXsa}.

\bibitem[Yarats et~al.(2022)Yarats, Fergus, Lazaric, and
  Pinto]{yarats2022mastering}
Yarats, D., Fergus, R., Lazaric, A., and Pinto, L.
\newblock {Mastering Visual Continuous Control: Improved Data-Augmented
  Reinforcement Learning}.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=_SJ-_yyes8}.

\bibitem[Ye et~al.(2021)Ye, Liu, Kurutach, Abbeel, and Gao]{Ye2021MasteringAG}
Ye, W., Liu, S., Kurutach, T., Abbeel, P., and Gao, Y.
\newblock {Mastering Atari Games with Limited Data}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[You et~al.(2022)You, Yang, Zheng, Hao, and E~Taylor]{you2022cross}
You, H., Yang, T., Zheng, Y., Hao, J., and E~Taylor, M.
\newblock Cross-domain adaptive transfer reinforcement learning based on
  state-action correspondence.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 2022.

\bibitem[Yu et~al.(2017)Yu, Liu, and Turk]{Yu2017PreparingFT}
Yu, W., Liu, C.~K., and Turk, G.
\newblock Preparing for the unknown: Learning a universal policy with online
  system identification.
\newblock \emph{ArXiv}, abs/1702.02453, 2017.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, McAllister, Calandra, Gal, and
  Levine]{zhang2021learninginvariant}
Zhang, A., McAllister, R.~T., Calandra, R., Gal, Y., and Levine, S.
\newblock Learning invariant representations for reinforcement learning without
  reconstruction.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=-2FCwDKRREu}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Zhong, Lee, and
  Lim]{Zhang2021PolicyTA}
Zhang, G., Zhong, L., Lee, Y., and Lim, J.~J.
\newblock Policy transfer across visual and dynamics domain gaps via iterative
  grounding.
\newblock \emph{ArXiv}, abs/2107.00339, 2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2021{\natexlab{c}})Zhang, Xiao, Efros, Pinto, and
  Wang]{zhang2021learning}
Zhang, Q., Xiao, T., Efros, A.~A., Pinto, L., and Wang, X.
\newblock Learning cross-domain correspondence for control with dynamics
  cycle-consistency.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{c}}.
\newblock URL \url{https://openreview.net/forum?id=QIRlze3I6hX}.

\bibitem[Zhou et~al.(2019)Zhou, Pinto, and Gupta]{zhou2018environment}
Zhou, W., Pinto, L., and Gupta, A.
\newblock Environment probing interaction policies.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=ryl8-3AcFX}.

\bibitem[Zhu et~al.(2020)Zhu, Xia, Wu, Deng, gang Zhou, Qin, and
  Li]{Zhu2020MaskedCR}
Zhu, J., Xia, Y., Wu, L., Deng, J., gang Zhou, W., Qin, T., and Li, H.
\newblock Masked contrastive representation learning for reinforcement
  learning.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 45:\penalty0 3421--3433, 2020.

\bibitem[Zhu et~al.(2017)Zhu, Kimmel, Bekris, and Boularias]{Zhu2017FastMI}
Zhu, S., Kimmel, A., Bekris, K.~E., and Boularias, A.
\newblock Fast model identification via physics engines for data-efficient
  policy search.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  2017.

\end{thebibliography}
