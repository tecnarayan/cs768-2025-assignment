\begin{thebibliography}{10}

\bibitem{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in neural information processing systems}, 33:6840--6851, 2020.

\bibitem{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em Advances in neural information processing systems}, 34:8780--8794, 2021.

\bibitem{zhu2024scaling}
Minjie Zhu, Yichen Zhu, Jinming Li, Junjie Wen, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, et~al.
\newblock Scaling diffusion policy in transformer to 1 billion parameters for robotic manipulation.
\newblock {\em arXiv preprint arXiv:2409.14411}, 2024.

\bibitem{nichol2021glide}
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen.
\newblock Glide: Towards photorealistic image generation and editing with text-guided diffusion models.
\newblock {\em arXiv preprint arXiv:2112.10741}, 2021.

\bibitem{saharia2022photorealistic}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily~L Denton, Kamyar Ghasemipour, Raphael Gontijo~Lopes, Burcu Karagol~Ayan, Tim Salimans, et~al.
\newblock Photorealistic text-to-image diffusion models with deep language understanding.
\newblock {\em Advances in neural information processing systems}, 35:36479--36494, 2022.

\bibitem{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock {\em arXiv preprint arXiv:2204.06125}, 1(2):3, 2022.

\bibitem{ho2022classifier}
Jonathan Ho and Tim Salimans.
\newblock Classifier-free diffusion guidance.
\newblock {\em arXiv preprint arXiv:2207.12598}, 2022.

\bibitem{song2020denoising}
Jiaming Song, Chenlin Meng, and Stefano Ermon.
\newblock Denoising diffusion implicit models.
\newblock {\em arXiv preprint arXiv:2010.02502}, 2020.

\bibitem{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In {\em Medical image computing and computer-assisted intervention--MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18}, pages 234--241. Springer, 2015.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{carion2020end}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In {\em European conference on computer vision}, pages 213--229. Springer, 2020.

\bibitem{zheng2021rethinking}
Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip~HS Torr, et~al.
\newblock Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 6881--6890, 2021.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In {\em International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem{kirillov2023segment}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C Berg, Wan-Yen Lo, et~al.
\newblock Segment anything.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 4015--4026, 2023.

\bibitem{bao2023all}
Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu.
\newblock All are worth words: A vit backbone for diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 22669--22679, 2023.

\bibitem{peebles2023scalable}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 4195--4205, 2023.

\bibitem{gao2023masked}
Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan.
\newblock Masked diffusion transformer is a strong image synthesizer.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 23164--23173, 2023.

\bibitem{man1982computational}
D~Man and A~Vision.
\newblock A computational investigation into the human representation and processing of visual information.
\newblock {\em WH San Francisco: Freeman and Company, San Francisco}, 1:1, 1982.

\bibitem{fish1990amplifying}
Jonathan Fish and Stephen Scrivener.
\newblock Amplifying the mind's eye: sketching and visual cognition.
\newblock {\em Leonardo}, 23(1):117--126, 1990.

\bibitem{eitz2012humans}
Mathias Eitz, James Hays, and Marc Alexa.
\newblock How do humans sketch objects?
\newblock {\em ACM Transactions on graphics (TOG)}, 31(4):1--10, 2012.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{perez2018film}
Ethan Perez, Florian Strub, Harm De~Vries, Vincent Dumoulin, and Aaron Courville.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, 2018.

\bibitem{hertzmann2022toward}
Aaron Hertzmann.
\newblock Toward modeling creative processes for algorithmic painting.
\newblock {\em arXiv preprint arXiv:2205.01605}, 2022.

\bibitem{botella2018stages}
Marion Botella, Franck Zenasni, and Todd Lubart.
\newblock What are the stages of the creative process? what visual art students are saying.
\newblock {\em Frontiers in psychology}, 9:389647, 2018.

\bibitem{dorst2001creativity}
Kees Dorst and Nigel Cross.
\newblock Creativity in the design process: co-evolution of problem--solution.
\newblock {\em Design studies}, 22(5):425--437, 2001.

\bibitem{boynton2005attention}
Geoffrey~M Boynton.
\newblock Attention and visual perception.
\newblock {\em Current opinion in neurobiology}, 15(4):465--469, 2005.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern recognition}, pages 248--255. Ieee, 2009.

\bibitem{gao2023mdtv2}
Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan.
\newblock Mdtv2: Masked diffusion transformer is a strong image synthesizer.
\newblock {\em arXiv preprint arXiv:2303.14389}, 2023.

\bibitem{xie2208adan}
Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and Shuicheng Yan.
\newblock Adan: Adaptive nesterov momentum algorithm for faster optimizing deep models.
\newblock {\em arXiv preprint arXiv:2208.06677}, 2022.

\bibitem{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash equilibrium.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{nash2021generating}
Charlie Nash, Jacob Menick, Sander Dieleman, and Peter~W Battaglia.
\newblock Generating images with sparse representations.
\newblock {\em arXiv preprint arXiv:2103.03841}, 2021.

\bibitem{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi~Chen.
\newblock Improved techniques for training gans.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{kynkaanniemi2019improved}
Tuomas Kynk{\"a}{\"a}nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila.
\newblock Improved precision and recall metric for assessing generative models.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{zhu2024sd}
Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, and Chang~Wen Chen.
\newblock Sd-dit: Unleashing the power of self-supervised discrimination in diffusion transformer.
\newblock {\em arXiv preprint arXiv:2403.17004}, 2024.

\bibitem{crowson2024scalable}
Katherine Crowson, Stefan~Andreas Baumann, Alex Birch, Tanishq~Mathew Abraham, Daniel~Z Kaplan, and Enrico Shippole.
\newblock Scalable high-resolution pixel-space image synthesis with hourglass diffusion transformers.
\newblock {\em arXiv preprint arXiv:2401.11605}, 2024.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{salimans2022progressive}
Tim Salimans and Jonathan Ho.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock {\em arXiv preprint arXiv:2202.00512}, 2022.

\bibitem{meng2023distillation}
Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.
\newblock On distillation of guided diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 14297--14306, 2023.

\bibitem{zheng2023fast}
Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar.
\newblock Fast training of diffusion models with masked transformers.
\newblock {\em arXiv preprint arXiv:2306.09305}, 2023.

\bibitem{bakr2023toddlerdiffusion}
Eslam~Mohamed Bakr, Liangbing Zhao, Vincent~Tao Hu, Matthieu Cord, Patrick Perez, and Mohamed Elhoseiny.
\newblock Toddlerdiffusion: Flash interpretable controllable diffusion model.
\newblock {\em arXiv preprint arXiv:2311.14542}, 2023.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 10012--10022, 2021.

\bibitem{zamir2022restormer}
Syed~Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad~Shahbaz Khan, and Ming-Hsuan Yang.
\newblock Restormer: Efficient transformer for high-resolution image restoration.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 5728--5739, 2022.

\bibitem{cao2022swin}
Hu~Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi~Tian, and Manning Wang.
\newblock Swin-unet: Unet-like pure transformer for medical image segmentation.
\newblock In {\em European conference on computer vision}, pages 205--218. Springer, 2022.

\end{thebibliography}
