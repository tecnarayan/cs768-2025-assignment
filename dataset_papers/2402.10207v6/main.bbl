\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong, Welinder, McGrew, Tobin, Pieter~Abbeel, and Zaremba]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Pieter~Abbeel, O., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{bai2022training}
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022.

\bibitem[Barrett \& Narayanan(2008)Barrett and Narayanan]{barrett2008learning}
Barrett, L. and Narayanan, S.
\newblock Learning all optimal policies with multiple criteria.
\newblock In \emph{Proceedings of the 25th international conference on Machine learning}, pp.\  41--47, 2008.

\bibitem[Black et~al.(2023)Black, Janner, Du, Kostrikov, and Levine]{black2023training}
Black, K., Janner, M., Du, Y., Kostrikov, I., and Levine, S.
\newblock Training diffusion models with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2305.13301}, 2023.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{boyd2004convex}
Boyd, S.~P. and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Brandfonbrener et~al.(2022)Brandfonbrener, Bietti, Buckman, Laroche, and Bruna]{brandfonbrener2022does}
Brandfonbrener, D., Bietti, A., Buckman, J., Laroche, R., and Bruna, J.
\newblock When does return-conditioned supervised learning work for offline reinforcement learning?
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 1542--1553, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal, Bojanowski, and Joulin]{caron2021emerging}
Caron, M., Touvron, H., Misra, I., J{\'e}gou, H., Mairal, J., Bojanowski, P., and Joulin, A.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pp.\  9650--9660, 2021.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel, Srinivas, and Mordatch]{chen2021decision}
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 15084--15097, 2021.

\bibitem[Chen et~al.(2020)Chen, Hou, Cui, Che, Liu, and Yu]{chen2020recall}
Chen, S., Hou, Y., Cui, Y., Che, W., Liu, T., and Yu, X.
\newblock Recall and learn: Fine-tuning deep pretrained language models with less forgetting.
\newblock \emph{arXiv preprint arXiv:2004.12651}, 2020.

\bibitem[Chen et~al.(2022)Chen, Zhong, Yang, Wang, and Wang]{chen2022human}
Chen, X., Zhong, H., Yang, Z., Wang, Z., and Wang, L.
\newblock Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3773--3793. PMLR, 2022.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dong et~al.(2023{\natexlab{a}})Dong, Xiong, Goyal, Pan, Diao, Zhang, Shum, and Zhang]{dong2023raft}
Dong, H., Xiong, W., Goyal, D., Pan, R., Diao, S., Zhang, J., Shum, K., and Zhang, T.
\newblock Raft: Reward ranked finetuning for generative foundation model alignment.
\newblock \emph{arXiv preprint arXiv:2304.06767}, 2023{\natexlab{a}}.

\bibitem[Dong et~al.(2023{\natexlab{b}})Dong, Wang, Sreedhar, Wu, and Kuchaiev]{dong2023steerlm}
Dong, Y., Wang, Z., Sreedhar, M.~N., Wu, X., and Kuchaiev, O.
\newblock Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rlhf.
\newblock \emph{arXiv preprint arXiv:2310.05344}, 2023{\natexlab{b}}.

\bibitem[Emmons et~al.(2021)Emmons, Eysenbach, Kostrikov, and Levine]{emmons2021rvs}
Emmons, S., Eysenbach, B., Kostrikov, I., and Levine, S.
\newblock Rvs: What is essential for offline rl via supervised learning?
\newblock \emph{arXiv preprint arXiv:2112.10751}, 2021.

\bibitem[Fang et~al.(2019)Fang, Zhou, Du, Han, and Zhang]{fang2019curriculum}
Fang, M., Zhou, T., Du, Y., Han, L., and Zhang, Z.
\newblock Curriculum-guided hindsight experience replay.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Ghosh et~al.(2019)Ghosh, Gupta, Reddy, Fu, Devin, Eysenbach, and Levine]{ghosh2019learning}
Ghosh, D., Gupta, A., Reddy, A., Fu, J., Devin, C., Eysenbach, B., and Levine, S.
\newblock Learning to reach goals via iterated supervised learning.
\newblock \emph{arXiv preprint arXiv:1912.06088}, 2019.

\bibitem[Gulcehre et~al.(2023)Gulcehre, Paine, Srinivasan, Konyushkova, Weerts, Sharma, Siddhant, Ahern, Wang, Gu, et~al.]{gulcehre2023reinforced}
Gulcehre, C., Paine, T.~L., Srinivasan, S., Konyushkova, K., Weerts, L., Sharma, A., Siddhant, A., Ahern, A., Wang, M., Gu, C., et~al.
\newblock Reinforced self-training (rest) for language modeling.
\newblock \emph{arXiv preprint arXiv:2308.08998}, 2023.

\bibitem[Hayes et~al.(2022)Hayes, R{\u{a}}dulescu, Bargiacchi, K{\"a}llstr{\"o}m, Macfarlane, Reymond, Verstraeten, Zintgraf, Dazeley, Heintz, et~al.]{hayes2022practical}
Hayes, C.~F., R{\u{a}}dulescu, R., Bargiacchi, E., K{\"a}llstr{\"o}m, J., Macfarlane, M., Reymond, M., Verstraeten, T., Zintgraf, L.~M., Dazeley, R., Heintz, F., et~al.
\newblock A practical guide to multi-objective reinforcement learning and planning.
\newblock \emph{Autonomous Agents and Multi-Agent Systems}, 36\penalty0 (1):\penalty0 26, 2022.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Hu et~al.(2023)Hu, Tao, Yang, and Zhou]{hu2023aligning}
Hu, J., Tao, L., Yang, J., and Zhou, C.
\newblock Aligning language models with offline reinforcement learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2308.12050}, 2023.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Korbak et~al.(2022)Korbak, Elsahar, Kruszewski, and Dymetman]{korbak2022reinforcement}
Korbak, T., Elsahar, H., Kruszewski, G., and Dymetman, M.
\newblock On reinforcement learning and distribution matching for fine-tuning language models with no catastrophic forgetting.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 16203--16220, 2022.

\bibitem[Kumar et~al.(2019)Kumar, Peng, and Levine]{kumar2019reward}
Kumar, A., Peng, X.~B., and Levine, S.
\newblock Reward-conditioned policies.
\newblock \emph{arXiv preprint arXiv:1912.13465}, 2019.

\bibitem[Langley(2000)]{langley00}
Langley, P.
\newblock Crafting papers on machine learning.
\newblock In Langley, P. (ed.), \emph{Proceedings of the 17th International Conference on Machine Learning (ICML 2000)}, pp.\  1207--1216, Stanford, CA, 2000. Morgan Kaufmann.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Pinto, and Abbeel]{li2020generalized}
Li, A., Pinto, L., and Abbeel, P.
\newblock Generalized hindsight for reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 7754--7767, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Zhang, and Wang]{li2020deep}
Li, K., Zhang, T., and Wang, R.
\newblock Deep reinforcement learning for multiobjective optimization.
\newblock \emph{IEEE transactions on cybernetics}, 51\penalty0 (6):\penalty0 3103--3114, 2020{\natexlab{b}}.

\bibitem[Liu et~al.(2023)Liu, Zhao, Joshi, Khalman, Saleh, Liu, and Liu]{liu2023statistical}
Liu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M., Liu, P.~J., and Liu, J.
\newblock Statistical rejection sampling improves preference optimization.
\newblock \emph{arXiv preprint arXiv:2309.06657}, 2023.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2017decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Lu et~al.(2022)Lu, Welleck, Hessel, Jiang, Qin, West, Ammanabrolu, and Choi]{lu2022quark}
Lu, X., Welleck, S., Hessel, J., Jiang, L., Qin, L., West, P., Ammanabrolu, P., and Choi, Y.
\newblock Quark: Controllable text generation with reinforced unlearning.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 27591--27609, 2022.

\bibitem[Luo et~al.(2023)Luo, Xiang, Zhang, Han, and Yang]{luo2023image}
Luo, F., Xiang, J., Zhang, J., Han, X., and Yang, W.
\newblock Image super-resolution via latent diffusion: A sampling-space mixture of experts and frequency-augmented decoder approach.
\newblock \emph{arXiv preprint arXiv:2310.12004}, 2023.

\bibitem[Munos et~al.(2023)Munos, Valko, Calandriello, Azar, Rowland, Guo, Tang, Geist, Mesnard, Michi, et~al.]{munos2023nash}
Munos, R., Valko, M., Calandriello, D., Azar, M.~G., Rowland, M., Guo, Z.~D., Tang, Y., Geist, M., Mesnard, T., Michi, A., et~al.
\newblock Nash learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2312.00886}, 2023.

\bibitem[Murray et~al.(2012)Murray, Marchesotti, and Perronnin]{murray2012ava}
Murray, N., Marchesotti, L., and Perronnin, F.
\newblock Ava: A large-scale database for aesthetic visual analysis.
\newblock In \emph{2012 IEEE conference on computer vision and pattern recognition}, pp.\  2408--2415. IEEE, 2012.

\bibitem[Nichol et~al.(2021)Nichol, Dhariwal, Ramesh, Shyam, Mishkin, McGrew, Sutskever, and Chen]{nichol2021glide}
Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M.
\newblock Glide: Towards photorealistic image generation and editing with text-guided diffusion models.
\newblock \emph{arXiv preprint arXiv:2112.10741}, 2021.

\bibitem[OpenAI(2023)]{openai2023gpt}
OpenAI, R.
\newblock Gpt-4 technical report. arxiv 2303.08774.
\newblock \emph{View in Article}, 2:\penalty0 13, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Pacchiano et~al.(2021)Pacchiano, Saha, and Lee]{pacchiano2021dueling}
Pacchiano, A., Saha, A., and Lee, J.
\newblock Dueling rl: reinforcement learning with trajectory preferences.
\newblock \emph{arXiv preprint arXiv:2111.04850}, 2021.

\bibitem[Peng et~al.(2023)Peng, Li, He, Galley, and Gao]{peng2023instruction}
Peng, B., Li, C., He, P., Galley, M., and Gao, J.
\newblock Instruction tuning with gpt-4.
\newblock \emph{arXiv preprint arXiv:2304.03277}, 2023.

\bibitem[Podell et~al.(2023)Podell, English, Lacey, Blattmann, Dockhorn, M{\"u}ller, Penna, and Rombach]{podell2023sdxl}
Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M{\"u}ller, J., Penna, J., and Rombach, R.
\newblock Sdxl: Improving latent diffusion models for high-resolution image synthesis.
\newblock \emph{arXiv preprint arXiv:2307.01952}, 2023.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and Sutskever]{radford2018improving}
Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and Finn]{rafailov2023direct}
Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C.~D., and Finn, C.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{arXiv preprint arXiv:2305.18290}, 2023.

\bibitem[Rame et~al.(2023)Rame, Couairon, Shukor, Dancette, Gaya, Soulier, and Cord]{rame2023rewarded}
Rame, A., Couairon, G., Shukor, M., Dancette, C., Gaya, J.-B., Soulier, L., and Cord, M.
\newblock Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards.
\newblock \emph{arXiv preprint arXiv:2306.04488}, 2023.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and Chen]{ramesh2022hierarchical}
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 1\penalty0 (2):\penalty0 3, 2022.

\bibitem[Ramnath et~al.(2023)Ramnath, Joshi, Hallinan, Lu, Li, Chan, Hessel, Choi, and Ren]{ramnath2023tailoring}
Ramnath, S., Joshi, B., Hallinan, S., Lu, X., Li, L.~H., Chan, A., Hessel, J., Choi, Y., and Ren, X.
\newblock Tailoring self-rationalizers with multi-reward distillation.
\newblock \emph{arXiv preprint arXiv:2311.02805}, 2023.

\bibitem[Roijers et~al.(2013)Roijers, Vamplew, Whiteson, and Dazeley]{roijers2013survey}
Roijers, D.~M., Vamplew, P., Whiteson, S., and Dazeley, R.
\newblock A survey of multi-objective sequential decision-making.
\newblock \emph{Journal of Artificial Intelligence Research}, 48:\penalty0 67--113, 2013.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  10684--10695, 2022.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton, Ghasemipour, Gontijo~Lopes, Karagol~Ayan, Salimans, et~al.]{saharia2022photorealistic}
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.~L., Ghasemipour, K., Gontijo~Lopes, R., Karagol~Ayan, B., Salimans, T., et~al.
\newblock Photorealistic text-to-image diffusion models with deep language understanding.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 36479--36494, 2022.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman, Cherti, Coombes, Katta, Mullis, Wortsman, et~al.]{schuhmann2022laion}
Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation image-text models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 25278--25294, 2022.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{stiennon2020learning}
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P.~F.
\newblock Learning to summarize with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 3008--3021, 2020.

\bibitem[Sun(2023)]{sun2023reinforcement}
Sun, H.
\newblock Reinforcement learning in the era of llms: What is essential? what is needed? an rl perspective on rlhf, prompting, and beyond.
\newblock \emph{arXiv preprint arXiv:2310.06147}, 2023.

\bibitem[Sun et~al.(2019)Sun, Li, Liu, Zhou, and Lin]{sun2019policy}
Sun, H., Li, Z., Liu, X., Zhou, B., and Lin, D.
\newblock Policy continuation with hindsight inverse dynamics.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Sun et~al.(2023)Sun, H{\"u}y{\"u}k, and van~der Schaar]{sun2023query}
Sun, H., H{\"u}y{\"u}k, A., and van~der Schaar, M.
\newblock Query-dependent prompt evaluation and optimization with offline inverse rl.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Vamplew et~al.(2018)Vamplew, Dazeley, Foale, Firmin, and Mummery]{vamplew2018human}
Vamplew, P., Dazeley, R., Foale, C., Firmin, S., and Mummery, J.
\newblock Human-aligned artificial intelligence is a multiobjective problem.
\newblock \emph{Ethics and Information Technology}, 20:\penalty0 27--40, 2018.

\bibitem[Van~Moffaert \& Now{\'e}(2014)Van~Moffaert and Now{\'e}]{van2014multi}
Van~Moffaert, K. and Now{\'e}, A.
\newblock Multi-objective reinforcement learning using sets of pareto dominating policies.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0 (1):\penalty0 3483--3512, 2014.

\bibitem[von Platen et~al.(2022)von Platen, Patil, Lozhkov, Cuenca, Lambert, Rasul, Davaadorj, and Wolf]{von2022diffusers}
von Platen, P., Patil, S., Lozhkov, A., Cuenca, P., Lambert, N., Rasul, K., Davaadorj, M., and Wolf, T.
\newblock Diffusers: State-of-the-art diffusion models, 2022.

\bibitem[von Werra et~al.(2020)von Werra, Belkada, Tunstall, Beeching, Thrush, Lambert, and Huang]{vonwerra2022trl}
von Werra, L., Belkada, Y., Tunstall, L., Beeching, E., Thrush, T., Lambert, N., and Huang, S.
\newblock Trl: Transformer reinforcement learning.
\newblock \url{https://github.com/huggingface/trl}, 2020.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Jiang, Yang, Liu, and Chen]{wang2023beyond}
Wang, C., Jiang, Y., Yang, C., Liu, H., and Chen, Y.
\newblock Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints.
\newblock \emph{arXiv preprint arXiv:2309.16240}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Dong, Zeng, Adams, Sreedhar, Egert, Delalleau, Scowcroft, Kant, Swope, et~al.]{wang2023helpsteer}
Wang, Z., Dong, Y., Zeng, J., Adams, V., Sreedhar, M.~N., Egert, D., Delalleau, O., Scowcroft, J.~P., Kant, N., Swope, A., et~al.
\newblock Helpsteer: Multi-attribute helpfulness dataset for steerlm.
\newblock \emph{arXiv preprint arXiv:2311.09528}, 2023{\natexlab{b}}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, et~al.]{wolf2020transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et~al.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations}, pp.\  38--45, 2020.

\bibitem[Wu et~al.(2023)Wu, Hu, Shi, Dziri, Suhr, Ammanabrolu, Smith, Ostendorf, and Hajishirzi]{wu2023fine}
Wu, Z., Hu, Y., Shi, W., Dziri, N., Suhr, A., Ammanabrolu, P., Smith, N.~A., Ostendorf, M., and Hajishirzi, H.
\newblock Fine-grained human feedback gives better rewards for language model training.
\newblock \emph{arXiv preprint arXiv:2306.01693}, 2023.

\bibitem[Xiong et~al.(2023)Xiong, Dong, Ye, Zhong, Jiang, and Zhang]{xiong2023gibbs}
Xiong, W., Dong, H., Ye, C., Zhong, H., Jiang, N., and Zhang, T.
\newblock Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf.
\newblock \emph{arXiv preprint arXiv:2312.11456}, 2023.

\bibitem[Yang et~al.(2021)Yang, Fang, Han, Du, Luo, and Li]{yang2021mher}
Yang, R., Fang, M., Han, L., Du, Y., Luo, F., and Li, X.
\newblock Mher: Model-based hindsight experience replay.
\newblock \emph{arXiv preprint arXiv:2107.00306}, 2021.

\bibitem[Yang et~al.(2022)Yang, Lu, Li, Sun, Fang, Du, Li, Han, and Zhang]{yang2022rethinking}
Yang, R., Lu, Y., Li, W., Sun, H., Fang, M., Du, Y., Li, X., Han, L., and Zhang, C.
\newblock Rethinking goal-conditioned supervised learning and its connection to offline rl.
\newblock \emph{arXiv preprint arXiv:2202.04478}, 2022.

\bibitem[Yang et~al.(2023)Yang, Yong, Ma, Hu, Zhang, and Zhang]{yang2023essential}
Yang, R., Yong, L., Ma, X., Hu, H., Zhang, C., and Zhang, T.
\newblock What is essential for unseen goal generalization of offline goal-conditioned rl?
\newblock In \emph{International Conference on Machine Learning}, pp.\  39543--39571. PMLR, 2023.

\bibitem[Yang et~al.(2024)Yang, Zhong, Xu, Zhang, Zhang, Han, and Zhang]{yang2023towards}
Yang, R., Zhong, H., Xu, J., Zhang, A., Zhang, C., Han, L., and Zhang, T.
\newblock Towards robust offline reinforcement learning under diverse data corruption.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Ye et~al.(2023)Ye, Zhang, Liu, Han, and Yang]{ye2023ip}
Ye, H., Zhang, J., Liu, S., Han, X., and Yang, W.
\newblock Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models.
\newblock \emph{arXiv preprint arXiv:2308.06721}, 2023.

\bibitem[Yuan et~al.(2023)Yuan, Yuan, Tan, Wang, Huang, and Huang]{yuan2023rrhf}
Yuan, Z., Yuan, H., Tan, C., Wang, W., Huang, S., and Huang, F.
\newblock Rrhf: Rank responses to align language models with human feedback without tears.
\newblock \emph{arXiv preprint arXiv:2304.05302}, 2023.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Han, Zhou, Hu, Yan, Lu, Li, Gao, and Qiao]{zhang2023llama}
Zhang, R., Han, J., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., Gao, P., and Qiao, Y.
\newblock Llama-adapter: Efficient fine-tuning of language models with zero-init attention.
\newblock \emph{arXiv preprint arXiv:2303.16199}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Liu, Wong, Abbeel, and Gonzalez]{zhang2023wisdom}
Zhang, T., Liu, F., Wong, J., Abbeel, P., and Gonzalez, J.~E.
\newblock The wisdom of hindsight makes language models better instruction followers.
\newblock \emph{arXiv preprint arXiv:2302.05206}, 2023{\natexlab{b}}.

\bibitem[Zhou et~al.(2023)Zhou, Liu, Yang, Shao, Liu, Yue, Ouyang, and Qiao]{zhou2023beyond}
Zhou, Z., Liu, J., Yang, C., Shao, J., Liu, Y., Yue, X., Ouyang, W., and Qiao, Y.
\newblock Beyond one-preference-for-all: Multi-objective direct preference optimization.
\newblock \emph{arXiv preprint arXiv:2310.03708}, 2023.

\bibitem[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving]{ziegler2019fine}
Ziegler, D.~M., Stiennon, N., Wu, J., Brown, T.~B., Radford, A., Amodei, D., Christiano, P., and Irving, G.
\newblock Fine-tuning language models from human preferences.
\newblock \emph{arXiv preprint arXiv:1909.08593}, 2019.

\end{thebibliography}
