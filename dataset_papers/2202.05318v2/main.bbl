\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar,
  and Zhang]{abadi2016deep}
Abadi, M., Chu, A., Goodfellow, I., McMahan, H.~B., Mironov, I., Talwar, K.,
  and Zhang, L.
\newblock Deep learning with differential privacy.
\newblock In \emph{Proceedings of the 2016 ACM SIGSAC conference on computer
  and communications security}, 2016.

\bibitem[Agarwal et~al.(2020)Agarwal, Langford, and Wei]{agarwal2020federated}
Agarwal, A., Langford, J., and Wei, C.-Y.
\newblock Federated residual learning.
\newblock \emph{arXiv preprint arXiv:2003.12880}, 2020.

\bibitem[Arivazhagan et~al.(2019)Arivazhagan, Aggarwal, Singh, and
  Choudhary]{arivazhagan2019federated}
Arivazhagan, M.~G., Aggarwal, V., Singh, A.~K., and Choudhary, S.
\newblock Federated learning with personalization layers.
\newblock \emph{arXiv preprint arXiv:1912.00818}, 2019.

\bibitem[Bassily et~al.(2014)Bassily, Smith, and Thakurta]{bassily2014private}
Bassily, R., Smith, A., and Thakurta, A.
\newblock Private empirical risk minimization: Efficient algorithms and tight
  error bounds.
\newblock In \emph{2014 IEEE 55th Annual Symposium on Foundations of Computer
  Science}, 2014.

\bibitem[Bellet et~al.(2018)Bellet, Guerraoui, Taziki, and
  Tommasi]{bellet2018personalized}
Bellet, A., Guerraoui, R., Taziki, M., and Tommasi, M.
\newblock Personalized and private peer-to-peer machine learning.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, 2018.

\bibitem[Chen et~al.(2020)Chen, Wu, and Hong]{ChenWH20}
Chen, X., Wu, Z.~S., and Hong, M.
\newblock Understanding gradient clipping in private {SGD:} {A} geometric
  perspective.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{dekel2012optimal}
Dekel, O., Gilad-Bachrach, R., Shamir, O., and Xiao, L.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0 (1), 2012.

\bibitem[Deng et~al.(2020)Deng, Kamani, and Mahdavi]{deng2020adaptive}
Deng, Y., Kamani, M.~M., and Mahdavi, M.
\newblock Adaptive personalized federated learning.
\newblock \emph{arXiv preprint arXiv:2003.13461}, 2020.

\bibitem[Dinh et~al.(2020)Dinh, Tran, and Nguyen]{dinh2020personalized}
Dinh, C.~T., Tran, N., and Nguyen, J.
\newblock Personalized federated learning with moreau envelopes.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem[Dwork et~al.(2006)Dwork, McSherry, Nissim, and
  Smith]{dwork2006calibrating}
Dwork, C., McSherry, F., Nissim, K., and Smith, A.
\newblock Calibrating noise to sensitivity in private data analysis.
\newblock In \emph{Theory of cryptography conference}, 2006.

\bibitem[Dwork et~al.(2010{\natexlab{a}})Dwork, Naor, Pitassi, and
  Rothblum]{panprivacy}
Dwork, C., Naor, M., Pitassi, T., and Rothblum, G.~N.
\newblock Differential privacy under continual observation.
\newblock In \emph{Proceedings of the Forty-Second ACM Symposium on Theory of
  Computing}, 2010{\natexlab{a}}.

\bibitem[Dwork et~al.(2010{\natexlab{b}})Dwork, Rothblum, and
  Vadhan]{dwork2010boosting}
Dwork, C., Rothblum, G.~N., and Vadhan, S.
\newblock Boosting and differential privacy.
\newblock In \emph{2010 IEEE 51st Annual Symposium on Foundations of Computer
  Science}, 2010{\natexlab{b}}.

\bibitem[Feldman et~al.(2020)Feldman, Koren, and Talwar]{feldman2020private}
Feldman, V., Koren, T., and Talwar, K.
\newblock Private stochastic convex optimization: optimal rates in linear time.
\newblock In \emph{Proceedings of the 52nd Annual ACM SIGACT Symposium on
  Theory of Computing}, 2020.

\bibitem[Finn et~al.(2017)Finn, Abbeel, , and Levine]{finn2017modelagnostic}
Finn, C., Abbeel, P., , and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2017.

\bibitem[Hanzely \& Richt{\'a}rik(2020)Hanzely and
  Richt{\'a}rik]{hanzely2020federated}
Hanzely, F. and Richt{\'a}rik, P.
\newblock Federated learning of a mixture of global and local models.
\newblock \emph{arXiv preprint arXiv:2002.05516}, 2020.

\bibitem[Hanzely et~al.(2021)Hanzely, Zhao, and Kolar]{hanzely2021personalized}
Hanzely, F., Zhao, B., and Kolar, M.
\newblock Personalized federated learning: A unified framework and universal
  optimization techniques.
\newblock \emph{arXiv preprint arXiv:2102.09743}, 2021.

\bibitem[Hsu et~al.(2014)Hsu, Huang, Roth, Roughgarden, and Wu]{HsuHRRW14}
Hsu, J., Huang, Z., Roth, A., Roughgarden, T., and Wu, Z.~S.
\newblock Private matchings and allocations.
\newblock In \emph{Symposium on Theory of Computing, {STOC}}, 2014.

\bibitem[Hu et~al.(2021)Hu, Wu, and Smith]{hu2021private}
Hu, S., Wu, Z.~S., and Smith, V.
\newblock Private multi-task learning: Formulation and applications to
  federated learning.
\newblock \emph{arXiv preprint arXiv:2108.12978}, 2021.

\bibitem[Huang et~al.(2021)Huang, Chu, Zhou, Wang, Liu, Pei, and
  Zhang]{huang2021personalized}
Huang, Y., Chu, L., Zhou, Z., Wang, L., Liu, J., Pei, J., and Zhang, Y.
\newblock Personalized cross-silo federated learning on non-iid data.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\bibitem[Jain et~al.(2021)Jain, Rush, Smith, Song, and
  Guha~Thakurta]{jain2021differentially}
Jain, P., Rush, J., Smith, A., Song, S., and Guha~Thakurta, A.
\newblock Differentially private model personalization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Jiang et~al.(2019)Jiang, Kone{\v{c}}n{\`y}, Rush, and
  Kannan]{jiang2019improving}
Jiang, Y., Kone{\v{c}}n{\`y}, J., Rush, K., and Kannan, S.
\newblock Improving federated learning personalization via model agnostic meta
  learning.
\newblock \emph{arXiv preprint arXiv:1909.12488}, 2019.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, et~al.]{kairouz2019advances}
Kairouz, P., McMahan, H.~B., et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends in Machine Learning}, 14\penalty0
  (1â€“2):\penalty0 1--210, 2021.

\bibitem[Kearns et~al.(2014)Kearns, Pai, Roth, and Ullman]{KPRU}
Kearns, M., Pai, M.~M., Roth, A., and Ullman, J.
\newblock Mechanism design in large games: Incentives and privacy.
\newblock \emph{American Economic Review}, 104\penalty0 (5):\penalty0 431--35,
  May 2014.

\bibitem[Khodak et~al.(2019)Khodak, Balcan, and Talwalkar]{khodak2019adaptive}
Khodak, M., Balcan, M.-F.~F., and Talwalkar, A.~S.
\newblock Adaptive gradient-based meta-learning methods.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, volume~32, 2019.

\bibitem[Levy et~al.(2021)Levy, Sun, Amin, Kale, Kulesza, Mohri, and
  Suresh]{levy2021learning}
Levy, D., Sun, Z., Amin, K., Kale, S., Kulesza, A., Mohri, M., and Suresh,
  A.~T.
\newblock Learning with user-level privacy.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Li et~al.(2020)Li, Khodak, Caldas, and
  Talwalkar]{li2020differentially}
Li, J., Khodak, M., Caldas, S., and Talwalkar, A.
\newblock Differentially private meta-learning.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2020.

\bibitem[Mansour et~al.(2020)Mansour, Mohri, Ro, and
  Suresh]{mansour2020approaches}
Mansour, Y., Mohri, M., Ro, J., and Suresh, A.~T.
\newblock Three approaches for personalization with applications to federated
  learning, 2020.

\bibitem[Marfoq et~al.(2021)Marfoq, Neglia, Bellet, Kameni, and
  Vidal]{marfoq2021federated}
Marfoq, O., Neglia, G., Bellet, A., Kameni, L., and Vidal, R.
\newblock Federated multi-task learning under a mixture of distributions.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and Arcas, B. A.~y.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, 2017.

\bibitem[McMahan et~al.(2018)McMahan, Ramage, Talwar, and
  Zhang]{mcmahan2018learning}
McMahan, H.~B., Ramage, D., Talwar, K., and Zhang, L.
\newblock Learning differentially private recurrent language models.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2018.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{nichol2018firstorder}
Nichol, A., Achiam, J., and Schulman, J.
\newblock On first-order meta-learning algorithms.
\newblock \emph{arXiv preprint arXiv:1803.02999}, 2018.

\bibitem[Noble et~al.(2021)Noble, Bellet, and
  Dieuleveut]{noble2021differentially}
Noble, M., Bellet, A., and Dieuleveut, A.
\newblock Differentially private federated learning on heterogeneous data.
\newblock \emph{arXiv preprint arXiv:2111.09278}, 2021.

\bibitem[Paulik et~al.(2021)Paulik, Seigel, Mason, Telaar, Kluivers, van Dalen,
  Lau, Carlson, Granqvist, Vandevelde, et~al.]{paulik2021federated}
Paulik, M., Seigel, M., Mason, H., Telaar, D., Kluivers, J., van Dalen, R.,
  Lau, C.~W., Carlson, L., Granqvist, F., Vandevelde, C., et~al.
\newblock Federated evaluation and tuning for on-device personalization: System
  design \& applications.
\newblock \emph{arXiv preprint arXiv:2102.08503}, 2021.

\bibitem[Pillaud-Vivien et~al.(2018)Pillaud-Vivien, Rudi, and
  Bach]{pillaud2018statistical}
Pillaud-Vivien, L., Rudi, A., and Bach, F.
\newblock Statistical optimality of stochastic gradient descent on hard
  learning problems through multiple passes.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2018.

\bibitem[Shen et~al.(2020)Shen, Zhang, Jia, Zhang, Huang, Zhou, Kuang, Wu, and
  Wu]{shen2020federated}
Shen, T., Zhang, J., Jia, X., Zhang, F., Huang, G., Zhou, P., Kuang, K., Wu,
  F., and Wu, C.
\newblock Federated mutual learning.
\newblock \emph{arXiv preprint arXiv:2006.16765}, 2020.

\bibitem[Singhal et~al.(2021)Singhal, Sidahmed, Garrett, Wu, Rush, and
  Prakash]{singhal2021federated}
Singhal, K., Sidahmed, H., Garrett, Z., Wu, S., Rush, J., and Prakash, S.
\newblock Federated reconstruction: Partially local federated learning.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[Smith et~al.(2017)Smith, Chiang, Sanjabi, and
  Talwalkar]{smith2017federated}
Smith, V., Chiang, C.-K., Sanjabi, M., and Talwalkar, A.
\newblock Federated multi-task learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Vanhaesebrouck et~al.(2017)Vanhaesebrouck, Bellet, and
  Tommasi]{vanhaesebrock2017decentralized}
Vanhaesebrouck, P., Bellet, A., and Tommasi, M.
\newblock Decentralized collaborative learning of personalized models over
  networks.
\newblock In \emph{Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, 2017.

\bibitem[Wang et~al.(2021)Wang, Charles, Xu, Joshi, McMahan, Al-Shedivat,
  Andrew, Avestimehr, Daly, Data, et~al.]{wang2021field}
Wang, J., Charles, Z., Xu, Z., Joshi, G., McMahan, H.~B., Al-Shedivat, M.,
  Andrew, G., Avestimehr, S., Daly, K., Data, D., et~al.
\newblock A field guide to federated optimization.
\newblock \emph{arXiv preprint arXiv:2107.06917}, 2021.

\bibitem[Wang et~al.(2019)Wang, Mathews, Kiddon, Eichner, Beaufays, and
  Ramage]{wang2019federated}
Wang, K., Mathews, R., Kiddon, C., Eichner, H., Beaufays, F., and Ramage, D.
\newblock Federated evaluation of on-device personalization.
\newblock \emph{arXiv preprint arXiv:1910.10252}, 2019.

\bibitem[Woodworth et~al.(2020)Woodworth, Patel, Stich, Dai, Bullins, Mcmahan,
  Shamir, and Srebro]{woodworth2020local}
Woodworth, B., Patel, K.~K., Stich, S., Dai, Z., Bullins, B., Mcmahan, B.,
  Shamir, O., and Srebro, N.
\newblock Is local sgd better than minibatch sgd?
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2020.

\end{thebibliography}
