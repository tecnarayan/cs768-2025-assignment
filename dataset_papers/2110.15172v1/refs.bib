@article{gomez2018automatic,
  title={Automatic chemical design using a data-driven continuous representation of molecules},
  author={G{\'o}mez-Bombarelli, Rafael and Wei, Jennifer N and Duvenaud, David and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel and S{\'a}nchez-Lengeling, Benjam{\'\i}n and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D and Adams, Ryan P and Aspuru-Guzik, Al{\'a}n},
  journal={ACS central science},
  volume={4},
  number={2},
  pages={268--276},
  year={2018},
  publisher={ACS Publications}
}
@article{frazier2018tutorial,
  title={A tutorial on Bayesian optimization},
  author={Frazier, Peter I},
  journal={arXiv preprint arXiv:1807.02811},
  year={2018}
}
@inproceedings{wang2017max,
  title={Max-value entropy search for efficient Bayesian optimization},
  author={Wang, Zi and Jegelka, Stefanie},
  booktitle={International Conference on Machine Learning},
  pages={3627--3635},
  year={2017},
  organization={PMLR},
  volume={70},
}

@article{opper1998bayesian,
  title={A Bayesian approach to online learning},
  author={Opper, Manfred},
  journal={On-line learning in neural networks},
  pages={363--378},
  year=1998,
  publisher={Cambridge University Press Cambridge},
}

@inproceedings{chu2005preference,
  title={Preference learning with Gaussian processes},
  author={Chu, Wei and Ghahramani, Zoubin},
  booktitle={Proceedings of the 22nd International Conference on Machine learning},
  pages={137--144},
  year={2005}
}
@article{hennig2012entropy,
  title={Entropy Search for Information-Efficient Global Optimization.},
  author={Hennig, Philipp and Schuler, Christian J},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={6},
  year={2012}
}
@article{desautels2014parallelizing,
  title={Parallelizing exploration-exploitation tradeoffs in gaussian process bandit optimization},
  author={Desautels, Thomas and Krause, Andreas and Burdick, Joel W},
  journal={Journal of Machine Learning Research},
  volume={15},
  pages={3873--3923},
  year={2014},
  publisher={Microtome Publishing}
}

@article{higdon2008computer,
  title={Computer model calibration using high-dimensional output},
  author={Higdon, Dave and Gattiker, James and Williams, Brian and Rightley, Maria},
  journal={Journal of the American Statistical Association},
  volume={103},
  number={482},
  pages={570--583},
  year={2008},
  publisher={Taylor \& Francis}
}

@article{weiss2019mapping,
  title={Mapping the global prevalence, incidence, and mortality of Plasmodium falciparum, 2000--17: a spatial and temporal modelling study},
  author={Weiss, Daniel J and Lucas, Tim CD and Nguyen, Michele and Nandi, Anita K and Bisanzio, Donal and Battle, Katherine E and Cameron, Ewan and Twohig, Katherine A and Pfeffer, Daniel A and Rozier, Jennifer A and others},
  journal={The Lancet},
  volume={394},
  number={10195},
  pages={322--331},
  year={2019},
  publisher={Elsevier}
}
@inproceedings{jankowiak2020parametric,
  title={Parametric Gaussian process regressors},
  author={Jankowiak, Martin and Pleiss, Geoff and Gardner, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={4702--4712},
  year={2020},
  organization={PMLR},
  volume=119,
}

@article{moss2021gibbon,
  title={GIBBON: General-purpose Information-Based Bayesian OptimisatioN},
  author={Moss, Henry B and Leslie, David S and Gonzalez, Javier and Rayson, Paul},
  journal={arXiv preprint arXiv:2102.03324},
  year={2021}
}
@article{hagan2002managing,
  title={Managing smile risk},
  author={Hagan, Patrick S and Kumar, Deep and Lesniewski, Andrew S and Woodward, Diana E},
  journal={The Best of Wilmott},
  volume={1},
  pages={249--296},
  year={2002}
}

@inproceedings{huber2013recursive,
  title={Recursive Gaussian process regression},
  author={Huber, Marco F},
  booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={3362--3366},
  year={2013},
  organization={IEEE}
}

@article{huber2014recursive,
  title={Recursive Gaussian process: On-line regression and learning},
  author={Huber, Marco F},
  journal={Pattern Recognition Letters},
  volume={45},
  pages={85--91},
  year={2014},
  publisher={Elsevier}
}

@article{bijl2016online,
  title={Online sparse Gaussian process training with input noise},
  author={Bijl, Hildo and Sch{\"o}n, Thomas B and van Wingerden, Jan-Willem and Verhaegen, Michel},
  journal={arXiv preprint arXiv 1601.08068},
  year={2016}
}

@inproceedings{deisenroth2011pilco,
  title={PILCO: A model-based and data-efficient approach to policy search},
  author={Deisenroth, Marc and Rasmussen, Carl E},
  booktitle={Proceedings of the 28th International Conference on Machine Learning},
  pages={465--472},
  year={2011},
}
@inproceedings{saemundsson2018meta,
  title={Meta reinforcement learning with latent variable Gaussian processes},
  author={S{\ae}mundsson, S and Hofmann, K and Deisenroth, MP},
  booktitle={34th Conference on Uncertainty in Artificial Intelligence},
  volume={34},
  pages={642--652},
  year={2018},
  organization={Association for Uncertainty in Artificial Intelligence (AUAI)}
}

@inproceedings{eriksson2019scalable,
    author = {Eriksson, David and Pearce, Michael and Gardner, Jacob and Turner, Ryan D and Poloczek, Matthias},
    booktitle = {Advances in Neural Information Processing Systems},
    publisher = {Curran Associates, Inc.},
    title = {Scalable Global Optimization via Local Bayesian Optimization},
    url = {https://proceedings.neurips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf},
    volume = {32},
    year = {2019}
}

@incollection{wilson2018maxbo,
	Author = {Wilson, James and Hutter, Frank and Deisenroth, Marc Peter},
	Booktitle = {Advances in Neural Information Processing Systems},
	volume={31},
	Pages = {9905--9916},
	Title = {Maximizing acquisition functions for {B}ayesian optimization},
	Year = {2018},
}
@inproceedings{broderick2013streaming,
  title={Streaming Variational Bayes},
  author={Broderick, Tamara and Boyd, Nicholas and Wibisono, Andre and Wilson, Ashia C and Jordan, Michael I},
  booktitle={Advances in Neural Information Processing Systems},
  volume = {26},
  year={2013}
}


@inproceedings{paske2019pytorch,
 author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
 booktitle = {Advances in Neural Information Processing Systems},
 publisher = {Curran Associates, Inc.},
 title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
 url = {https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{duris2020bayesian,
  title={Bayesian optimization of a free-electron laser},
  author={Duris, Joseph and Kennedy, Dylan and Hanuka, Adi and Shtalenkova, Jane and Edelen, Auralee and Baxevanis, P and Egger, Adam and Cope, T and McIntire, M and Ermon, S and others},
  journal={Physical review letters},
  volume={124},
  number={12},
  pages={124801},
  year={2020},
  publisher={APS}
}

@article{frazier2008knowledge,
  title={A knowledge-gradient policy for sequential information collection},
  author={Frazier, Peter I and Powell, Warren B and Dayanik, Savas},
  journal={SIAM Journal on Control and Optimization},
  volume={47},
  number={5},
  pages={2410--2439},
  year={2008},
  publisher={SIAM}
}

@article{tripp2020sample,
  title={Sample-efficient optimization in the latent space of deep generative models via weighted retraining},
  author={Tripp, Austin and Daxberger, Erik and Hern{\'a}ndez-Lobato, Jos{\'e} Miguel},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@inproceedings{seo2000gaussian,
  title={Gaussian Process Regression: Active Data Selection and Test Point Rejection},
  author={Seo, S and Wallat, M and Graepel, T and Obermayer, K},
  booktitle={Neural Networks, IEEE-INNS-ENNS International Joint Conference on},
  volume={3},
  pages={3241--3241},
  year={2000}
}
@inproceedings{wang2018batched,
  title={Batched large-scale bayesian optimization in high-dimensional spaces},
  author={Wang, Zi and Gehring, Clement and Kohli, Pushmeet and Jegelka, Stefanie},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={745--754},
  year={2018},
  organization={PMLR},
  volume=84,
}

@inproceedings{wang_exact_2019,
	title = {Exact {Gaussian} {Processes} on a {Million} {Data} {Points}},
	url = {http://arxiv.org/abs/1903.08114},
	abstract = {Gaussian processes (GPs) are flexible models with state-of-the-art performance on many impactful applications. However, computational constraints with standard inference procedures have limited exact GPs to problems with fewer than about ten thousand training points, necessitating approximations for larger datasets. In this paper, we develop a scalable approach for exact GPs that leverages multi-GPU parallelization and methods like linear conjugate gradients, accessing the kernel matrix only through matrix multiplication. By partitioning and distributing kernel matrix multiplies, we demonstrate that an exact GP can be trained on over a million points in 3 days using 8 GPUs and can compute predictive means and variances in under a second using 1 GPU at test time. Moreover, we perform the first-ever comparison of exact GPs against state-of-the-art scalable approximations on large-scale regression datasets with \$10{\textasciicircum}4-10{\textasciicircum}6\$ data points, showing dramatic performance improvements.},
	urldate = {2019-10-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wang, Ke Alexander and Pleiss, Geoff and Gardner, Jacob R. and Tyree, Stephen and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
	month = mar,
	year = {2019},
	volume=32,
}
 @InProceedings{burt_convergence_19, 
    title = {Rates of Convergence for Sparse Variational {G}aussian Process Regression}, 
    author = {Burt, David and Rasmussen, Carl Edward and Van Der Wilk, Mark}, 
    booktitle = {Proceedings of the 36th International Conference on Machine Learning}, 
    pages = {862--871}, 
    year = {2019}, 
    volume = {97}, 
    month = {09--15 Jun}, 
    publisher = {PMLR}, 
} 
@inproceedings{hensman_bigdata_gp,
author = {Hensman, James and Fusi, Nicol\`{o} and Lawrence, Neil D.},
title = {Gaussian Processes for Big Data},
year = {2013},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be variationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our approach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.},
booktitle = {Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence},
pages = {282–290},
numpages = {9},
location = {Bellevue, WA},
series = {UAI'13}
}
@inproceedings{damianou_deep_2013,
	title = {Deep {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1211.0358},
	abstract = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.},
	urldate = {2018-01-30},
	booktitle = {Proceedings of the 16th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Damianou, Andreas C. and Lawrence, Neil D.},
	year = {2013},
	volume=31,
	publisher=PMLR,
}

@inproceedings{wilson_deep_2015,
	title = {Deep {Kernel} {Learning}},
	url = {http://arxiv.org/abs/1511.02222},
	abstract = {We introduce scalable deep kernels, which combine the structural properties of deep learning architectures with the non-parametric ﬂexibility of kernel methods. Speciﬁcally, we transform the inputs of a spectral mixture base kernel with a deep architecture, using local kernel interpolation, inducing points, and structure exploiting (Kronecker and Toeplitz) algebra for a scalable kernel representation. These closed-form kernels can be used as drop-in replacements for standard kernels, with beneﬁts in expressive power and scalability. We jointly learn the properties of these kernels through the marginal likelihood of a Gaussian process. Inference and learning cost O(n) for n training points, and predictions cost O(1) per test point. On a large and diverse collection of applications, including a dataset with 2 million examples, we show improved performance over scalable Gaussian processes with ﬂexible kernel learning models, and stand-alone deep architectures.},
	language = {en},
	urldate = {2018-12-11},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.02222},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Methodology},
	annote = {Comment: 19 pages, 6 figures},
}

@inproceedings{van_der_wilk_learning_2018,
	title = {Learning {Invariances} using the {Marginal} {Likelihood}},
	url = {http://arxiv.org/abs/1808.05563},
	abstract = {Generalising well in supervised learning tasks relies on correctly extrapolating the training data to a large region of the input space. One way to achieve this is to constrain the predictions to be invariant to transformations on the input that are known to be irrelevant (e.g. translation). Commonly, this is done through data augmentation, where the training set is enlarged by applying hand-crafted transformations to the inputs. We argue that invariances should instead be incorporated in the model structure, and learned using the marginal likelihood, which correctly rewards the reduced complexity of invariant models. We demonstrate this for Gaussian process models, due to the ease with which their marginal likelihood can be estimated. Our main contribution is a variational inference scheme for Gaussian processes containing invariances described by a sampling procedure. We learn the sampling procedure by back-propagating through it to maximise the marginal likelihood.},
	language = {en},
	urldate = {2018-09-17},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	author = {van der Wilk, Mark and Bauer, Matthias and John, S. T. and Hensman, James},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.05563},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{pleiss_constant-time_2018,
	title = {Constant-{Time} {Predictive} {Distributions} for {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1803.06058},
	abstract = {One of the most compelling features of Gaussian process (GP) regression is its ability to provide well-calibrated posterior distributions. Recent advances in inducing point methods have sped up GP marginal likelihood and posterior mean computations, leaving posterior covariance estimation and sampling as the remaining computational bottlenecks. In this paper we address these shortcomings by using the Lanczos algorithm to rapidly approximate the predictive covariance matrix. Our approach, which we refer to as LOVE (LanczOs Variance Estimates), substantially improves time and space complexity. In our experiments, LOVE computes covariances up to 2,000 times faster and draws samples 18,000 times faster than existing methods, all without sacriﬁcing accuracy.},
	language = {en},
	urldate = {2019-08-09},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Pleiss, Geoff and Gardner, Jacob R. and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
	month = mar,
	year = {2018},
	publisher={PMLR},
	volume=84,
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {arXiv: 1803.06058},
	annote = {Comment: ICML 2018}
}

@inproceedings{gardner_gpytorch:_2018,
	title = {{GPyTorch}: {Blackbox} {Matrix}-{Matrix} {Gaussian} {Process} {Inference} with {GPU} {Acceleration}},
	volume = {31},
	shorttitle = {{GPyTorch}},
	url = {http://arxiv.org/abs/1809.11165},
	abstract = {Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efﬁcient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modiﬁed batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n3) to O(n2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efﬁcient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.},
	language = {en},
	urldate = {2019-08-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Gardner, Jacob R. and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
	month = sep,
	year = {2018},
}

@book{rasmussen_gaussian_2008,
	address = {Cambridge, Mass.},
	edition = {3. print},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {978-0-262-18253-9},
	language = {eng},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2008},
	annote = {Includes bibliographical references and indexes},
	annote = {Includes bibliographical references and indexes},
	annote = {OCLC: 552376743},
	annote = {OCLC: 552376743}
}

@article{ghassemi_analytic_nodate,
	title = {Analytic {Long}-{Term} {Forecasting} with {Periodic} {Gaussian} {Processes}},
	abstract = {Gaussian processes are a state-of-the-art method for learning models from data. Data with an underlying periodic structure appears in many areas, e.g., in climatology or robotics. It is often important to predict the long-term evolution of such a time series, and to take the inherent periodicity explicitly into account. In a Gaussian process, periodicity can be accounted for by an appropriate kernel choice. However, the standard periodic kernel does not allow for analytic long-term forecasting. To address this shortcoming, we re-parametrize the periodic kernel, which, in combination with a double approximation, allows for analytic longterm forecasting of a periodic state evolution with Gaussian processes. Our model allows for probabilistic long-term forecasting of periodic processes, which can be valuable in Bayesian decision making, optimal control, reinforcement learning, and robotics.},
	language = {en},
	author = {Ghassemi, Nooshin Haji and Deisenroth, Marc Peter},
	pages = {9},
}

@inproceedings{van_der_wilk_variational_2019,
	title = {Variational {Gaussian} {Process} {Models} without {Matrix} {Inverses}},
	url = {https://openreview.net/pdf?id=rklEqJhNFH},
	urldate = {2020-03-05},
	booktitle = {2nd {Symposium} on {Advances} in {Approximate} {Bayesian} {Inference}},
	author = {van der Wilk, Mark and John, S.T and Artemev, Artem and Hensman, James},
	year = {2019},
}

@inproceedings{nguyen-tuong_local_2008,
	address = {Vancouver, British Columbia, Canada},
	series = {{NIPS}'08},
	title = {Local {Gaussian} process regression for real time online model learning and control},
	isbn = {978-1-60560-949-2},
	abstract = {Learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques. Inspired by local learning, we propose a method to speed up standard Gaussian process regression (GPR) with local GP models (LGP). The training data is partitioned in local regions, for each an individual GP model is trained. The prediction for a query point is performed by weighted estimation using nearby local models. Unlike other GP approximations, such as mixtures of experts, we use a distance based measure for partitioning of the data and weighted prediction. The proposed method achieves online learning and prediction in real-time. Comparisons with other non-parametric regression methods show that LGP has higher accuracy than LWPR and close to the performance of standard GPR and ν-SVR.},
	urldate = {2020-04-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	volume=31,
	publisher = {Curran Associates Inc.},
	author = {Nguyen-Tuong, Duy and Peters, Jan and Seeger, Matthias},
	month = dec,
	year = {2008},
	pages = {1193--1200}
}

@inproceedings{bui_streaming_2017,
	address = {Long Beach, California, USA},
	title = {Streaming sparse {Gaussian} process approximations},
	isbn = {978-1-5108-6096-4},
	abstract = {Sparse pseudo-point approximations for Gaussian process (GP) models provide a suite of methods that support deployment of GPs in the large data regime and enable analytic intractabilities to be sidestepped. However, the field lacks a principled method to handle streaming data in which both the posterior distribution over function values and the hyperparameter estimates are updated in an online fashion. The small number of existing approaches either use suboptimal hand-crafted heuristics for hyperparameter learning, or suffer from catastrophic forgetting or slow updating when new data arrive. This paper develops a new principled framework for deploying Gaussian process probabilistic models in the streaming setting, providing methods for learning hyperparameters and optimising pseudo-input locations. The proposed framework is assessed using synthetic and real-world datasets.},
	urldate = {2020-04-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Bui, Thang D. and Nguyen, Cuong V. and Turner, Richard E.},
	month = dec,
	volume = {31},
	year = {2017},
	pages = {3301--3309},
}

@inproceedings{cheng_incremental_2016,
	address = {Barcelona, Spain},
	title = {Incremental variational sparse {Gaussian} process regression},
	isbn = {978-1-5108-3881-9},
	abstract = {Recent work on scaling up Gaussian process regression (GPR) to large datasets has primarily focused on sparse GPR, which leverages a small set of basis functions to approximate the full Gaussian process during inference. However, the majority of these approaches are batch methods that operate on the entire training dataset at once, precluding the use of datasets that are streaming or too large to fit into memory. Although previous work has considered incrementally solving variational sparse GPR, most algorithms fail to update the basis functions and therefore perform suboptimally. We propose a novel incremental learning algorithm for variational sparse GPR based on stochastic mirror ascent of probability densities in reproducing kernel Hilbert space. This new formulation allows our algorithm to update basis functions online in accordance with the manifold structure of probability densities for fast convergence. We conduct several experiments and show that our proposed approach achieves better empirical performance in terms of prediction error than the recent state-of-the-art incremental solutions to variational sparse GPR.},
	urldate = {2020-04-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Cheng, Ching-An and Boots, Byron},
	month = dec,
	volume=30,
	year = {2016},
	pages = {4410--4418},
}

@inproceedings{koppel_consistent_2019,
	title = {Consistent {Online} {Gaussian} {Process} {Regression} {Without} the {Sample} {Complexity} {Bottleneck}},
	doi = {10.23919/ACC.2019.8815206},
	abstract = {Gaussian process regression provides a framework for nonlinear nonparametric Bayesian inference applicable across machine learning, robotics, chemical engineering, and other settings. Unfortunately, the computational burden of the posterior mean and covariance scales cubically with the training sample size. Even worse, in the online setting where samples perpetually arrive, this complexity approaches infinity. Thus, popular perception is that Gaussian processes cannot be used with streaming data, and that approximations are required. Motivated by this necessity, we develop the first compression sub-routine for online Gaussian processes that preserves their convergence to the population posterior, i.e., asymptotic posterior consistency, while ameliorating their intractable complexity growth with the sample size. We do so by after each sequential Bayesian update, fixing an error neighborhood with respect to the Hellinger metric centered at the current empirical probability measure, and greedily tossing out past kernel dictionary elements until we hit the boundary of this neighborhood. We call the resulting method Parsimonious Online Gaussian Processes (POG). When we set the error radius, or compression budget, go to null with the sample size, then exact asymptotic consistency is preserved (Theorem li) at the cost of unbounded memory in the limit. On the other hand, for constant compression budget, POG converges to a neighborhood of the population posterior distribution (Theorem 1ii) but with finite memory that is at-worst determined by the metric entropy of the feature space (Theorem 2). Experiments on benchmark data demonstrates that POG exhibits favorable performance in practice.},
	booktitle = {2019 {American} {Control} {Conference} ({ACC})},
	author = {Koppel, Alec},
	month = jul,
	year = {2019},
	note = {ISSN: 2378-5861},
	keywords = {asymptotic posterior consistency, Bayes methods, chemical engineering, compression sub-routine, computational complexity, constant compression budget, covariance scales, entropy, error neighborhood, exact asymptotic consistency, Gaussian processes, Hellinger metric, inference mechanisms, intractable complexity growth, machine learning, metric entropy, nonlinear nonparametric Bayesian inference, nonparametric statistics, online Gaussian process regression, parsimonious online Gaussian processes, population posterior distribution, posterior mean, probability measure, regression analysis, sample complexity, sequential Bayesian update, statistical distributions},
	pages = {3512--3518},
	file = {IEEE Xplore Full Text PDF:/Users/wesleymaddox/Zotero/storage/W99YYD3T/Koppel - 2019 - Consistent Online Gaussian Process Regression With.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/wesleymaddox/Zotero/storage/N5L66X27/8815206.html:text/html}
}

@inproceedings{wilson_kernel_2015,
	title = {Kernel {Interpolation} for {Scalable} {Structured} {Gaussian} {Processes} ({KISS}-{GP})},
	url = {http://proceedings.mlr.press/v37/wilson15.html},
	abstract = {We introduce a new structured kernel interpolation (SKI) framework, which generalises and unifies inducing point methods for scalable Gaussian processes (GPs). SKI methods produce kernel approximat...},
	language = {en},
	urldate = {2020-04-15},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Wilson, Andrew and Nickisch, Hannes},
	month = jun,
	year = {2015},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {1775--1784},
	file = {Full Text PDF:/Users/wesleymaddox/Zotero/storage/ZR56U4D8/Wilson and Nickisch - 2015 - Kernel Interpolation for Scalable Structured Gauss.pdf:application/pdf;Snapshot:/Users/wesleymaddox/Zotero/storage/FLUH99DP/wilson15.html:text/html}
}

@inproceedings{wilson_stochastic_2016,
	address = {Barcelona, Spain},
	series = {{NIPS}'16},
	title = {Stochastic variational deep kernel learning},
	isbn = {978-1-5108-3881-9},
	abstract = {Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training. Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective. Within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra. We show improved performance over stand alone deep networks, SVMs, and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, CIFAR, and ImageNet.},
	urldate = {2020-04-15},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
	month = dec,
	year = {2016},
	pages = {2594--2602},
	file = {Full Text PDF:/Users/wesleymaddox/Zotero/storage/LRL8JJGT/Wilson et al. - 2016 - Stochastic variational deep kernel learning.pdf:application/pdf}
}

@inproceedings{gardner_product_2018,
	title = {Product {Kernel} {Interpolation} for {Scalable} {Gaussian} {Processes}},
	url = {http://proceedings.mlr.press/v84/gardner18a.html},
	abstract = {Recent work shows that inference for Gaussian processes can be performed efficiently using iterative methods that rely only on matrix-vector multiplications (MVMs). Structured Kernel Interpolation ...},
	language = {en},
	urldate = {2020-04-15},
	booktitle = {International {Conference} on {Artificial} {Intelligence} and {Statistics}},
	author = {Gardner, Jacob and Pleiss, Geoff and Wu, Ruihan and Weinberger, Kilian and Wilson, Andrew},
	month = mar,
	year = {2018},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	pages = {1407--1416},
	file = {Full Text PDF:/Users/wesleymaddox/Zotero/storage/EZJ8NNUK/Gardner et al. - 2018 - Product Kernel Interpolation for Scalable Gaussian.pdf:application/pdf;Snapshot:/Users/wesleymaddox/Zotero/storage/FDNXK6WL/gardner18a.html:text/html}
}

@inproceedings{balandat_botorch_2020,
	title = {{BoTorch}: {A} {Framework} for {Efficient} {Monte}-{Carlo} {Bayesian} {Optimization}},
	volume = {33},
	shorttitle = {{BoTorch}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/f5b1b89d98b7286673128a5fb112cb9a-Abstract.html},
	language = {en},
	urldate = {2020-12-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel and Daulton, Samuel and Letham, Ben and Wilson, Andrew G. and Bakshy, Eytan},
	year = {2020},
	file = {Full Text PDF:/Users/wesleymaddox/Zotero/storage/K63EUP78/Balandat et al. - 2020 - BoTorch A Framework for Efficient Monte-Carlo Bay.pdf:application/pdf;Snapshot:/Users/wesleymaddox/Zotero/storage/4V3DERWI/f5b1b89d98b7286673128a5fb112cb9a-Abstract.html:text/html}
}

@inproceedings{jiang_efficient_2020,
	title = {Efficient {Nonmyopic} {Bayesian} {Optimization} via {One}-{Shot} {Multi}-{Step} {Trees}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/d1d5923fc822531bbfd9d87d4760914b-Abstract.html},
	language = {en},
	urldate = {2020-12-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Jiang, Shali and Jiang, Daniel and Balandat, Maximilian and Karrer, Brian and Gardner, Jacob and Garnett, Roman},
	year = {2020},
	file = {Full Text PDF:/Users/wesleymaddox/Zotero/storage/66F7MBXJ/Jiang et al. - 2020 - Efficient Nonmyopic Bayesian Optimization via One-.pdf:application/pdf;Snapshot:/Users/wesleymaddox/Zotero/storage/WCIE8EJR/d1d5923fc822531bbfd9d87d4760914b-Abstract.html:text/html}
}

@inproceedings{stanton_kernel_2021,
	title = {Kernel {Interpolation} for {Scalable} {Online} {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/2103.01454},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	author = {Stanton, Samuel and Maddox, Wesley J. and Delbridge, Ian and Wilson, Andrew Gordon},
	year = {2021},
	publisher={PMLR},
    volume=130,
}

@inproceedings{titsias_variational_2009,
	title = {Variational {Learning} of {Inducing} {Variables} in {Sparse} {Gaussian} {Processes}},
	url = {http://proceedings.mlr.press/v5/titsias09a.html},
	abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximat...},
	language = {en},
	urldate = {2021-03-08},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Titsias, Michalis},
	month = apr,
	year = {2009},
	note = {ISSN: 1938-7228},
	pages = {567--574},
}
@article{pleiss2020fast,
  title={Fast Matrix Square Roots with Applications to Gaussian Processes and Bayesian Optimization},
  author={Pleiss, Geoff and Jankowiak, Martin and Eriksson, David and Damle, Anil and Gardner, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@phdthesis{matthews2017scalable,
  title={Scalable Gaussian process inference using variational methods},
  author={Matthews, Alexander Graeme de Garis},
  year={2017},
  school={University of Cambridge}
}

 @InProceedings{hensman15, 
 title = {{Scalable Variational Gaussian Process Classification}}, author = {James Hensman and Alexander Matthews and Zoubin Ghahramani}, booktitle = {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics}, pages = {351--360}, year = {2015},  address = {San Diego, California, USA}, month = {09--12 May}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v38/hensman15.pdf}, url = { http://proceedings.mlr.press/v38/hensman15.html }, abstract = {Gaussian process classification is a popular method with a number of appealing properties. We show how to scale the model within a variational inducing point framework, out-performing the state of the art on benchmark datasets. Importantly, the variational formulation an be exploited to allow classification in problems with millions of data points, as we demonstrate in experiments.} } 
 
@inproceedings{nickson2014automated,
  title={Automated machine learning using stochastic algorithm tuning},
  author={Nickson, Thomas and Osborne, Michael A and Reece, Steven and Roberts, Stephen},
  booktitle={NIPS Workshop on Bayesian Optimization},
  year={2014},
  url={https://bayesopt.github.io/papers/2014/paper12.pdf}
}
@inproceedings{mcintire2016sparse,
  title={Sparse Gaussian Processes for Bayesian Optimization.},
  author={McIntire, Mitchell and Ratner, Daniel and Ermon, Stefano},
  booktitle={Uncertainty in Artifical Intelligence},
  volume=32,
  publisher={Association for Uncertainty in Artifical Intelligence (AUAI)},
  year={2016}
}


@article{pearlmutter1994fast,
  title={Fast exact multiplication by the Hessian},
  author={Pearlmutter, Barak A},
  journal={Neural computation},
  volume={6},
  number={1},
  pages={147--160},
  year={1994},
  publisher={MIT Press}
}

@article{titsias_variational_nodate,
	title = {Variational {Model} {Selection} for {Sparse} {Gaussian} {Process} {Regression}},
	abstract = {Sparse Gaussian process methods that use inducing variables require the selection of the inducing inputs and the kernel hyperparameters. We introduce a variational formulation for sparse approximations that jointly infers the inducing inputs and the kernel hyperparameters by maximizing a lower bound of the true log marginal likelihood. The key property of this formulation is that the inducing inputs are deﬁned to be variational parameters which are selected by minimizing the Kullback-Leibler divergence between the variational distribution and the exact posterior distribution over the latent function values. We apply this technique to regression and we compare it with other approaches in the literature.},
	language = {en},
	author = {Titsias, Michalis},
	year={2008},
	pages = {20},
	journal={www.aueb.gr/users/mtitsias/papers/sparseGPv2.pdf},
}
@inproceedings{xu2014gp,
  title={GP-Localize: Persistent mobile robot localization using online sparse Gaussian process observation model},
  author={Xu, Nuo and Low, Kian Hsiang and Chen, Jie and Lim, Keng Kiat and Ozgul, Etkin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={28},
  year={2014}
}

@inproceedings{ling2016gaussian,
  title={Gaussian process planning with Lipschitz continuous reward functions: Towards unifying Bayesian optimization, active learning, and beyond},
  author={Ling, Chun Kai and Low, Kian Hsiang and Jaillet, Patrick},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  year={2016}
}

@article{chowdhary2014bayesian,
  title={Bayesian nonparametric adaptive control using gaussian processes},
  author={Chowdhary, Girish and Kingravi, Hassan A and How, Jonathan P and Vela, Patricio A},
  journal={IEEE transactions on neural networks and learning systems},
  volume={26},
  number={3},
  pages={537--550},
  year={2014},
  publisher={IEEE}
}

@article{kocijan2005incorporating,
  title={Incorporating linear local models in Gaussian process model},
  author={Kocijan, Ju{\^S} and Girard, Agathe},
  journal={IFAC Proceedings Volumes},
  volume={38},
  number={1},
  pages={47--52},
  year={2005},
  publisher={Elsevier}
}

@inproceedings{boedecker2014approximate,
  title={Approximate real-time optimal control based on sparse gaussian process models},
  author={Boedecker, Joschka and Springenberg, Jost Tobias and W{\"u}lfing, Jan and Riedmiller, Martin},
  booktitle={2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},
  pages={1--8},
  year={2014},
  organization={IEEE}
}

@inproceedings{girard2002gaussian,
  title={Gaussian Process Priors with Uncertain Inputs-Application to Multiple-Step Ahead Time Series Forecasting},
  author={Girard, Agathe and Rasmussen, Carl Edward and Candela, Joaquin Qui{\~n}onero and Murray-Smith, Roderick},
  booktitle={Advances in Neural Information Processing Systems},
  volume={15},
  year={2002}
}

@article{lin2020mdmaking,
    title={Preference Learning for Real-World Multi-Objective Decision Making},
    author={Lin, Jerry and Obeng, Adam and Bakshy, Eytan},
    journal={ICML 2020 Workshop on Real World Experiment Design and Active Learning},
    year={2020},
}
@article{bui2017unifying,
  title={A unifying framework for Gaussian process pseudo-point approximations using power expectation propagation},
  author={Bui, Thang D and Yan, Josiah and Turner, Richard E},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3649--3720},
  year={2017},
  publisher={JMLR. org}
}

@article{panos2018fully,
  title={Fully scalable gaussian processes using subspace inducing inputs},
  author={Panos, Aristeidis and Dellaportas, Petros and Titsias, Michalis K},
  journal={arXiv preprint arXiv:1807.02537},
  year={2018}
}


@inproceedings{khan2017conjugate,
  title={Conjugate-computation variational inference: Converting variational inference in non-conjugate models to inferences in conjugate models},
  author={Khan, Mohammad and Lin, Wu},
  booktitle={Artificial Intelligence and Statistics},
  pages={878--887},
  year={2017},
  organization={PMLR}
}

@inproceedings{pan2017prediction,
  title={Prediction under uncertainty in sparse spectrum Gaussian processes with applications to filtering and control},
  author={Pan, Yunpeng and Yan, Xinyan and Theodorou, Evangelos A and Boots, Byron},
  booktitle={International Conference on Machine Learning},
  pages={2760--2768},
  year={2017},
  organization={PMLR},
  volume={70}
}

@inproceedings{groot2011multiple,
  title={Multiple-step Time Series Forecasting with Sparse Gaussian Processes},
  author={Groot, P and Lucas, P and Bosch, P},
  booktitle={Causmaecker, P. De (ed.), Proceedings of the 23rd Benelux Conference on Artificial Intelligence},
  pages={1--8},
  year={2011},
  organization={[Sl: sn]}
}

@article{mackenzie2014formula,
  title={‘The formula that killed Wall Street’: The Gaussian copula and modelling practices in investment banking},
  author={MacKenzie, Donald and Spears, Taylor},
  journal={Social Studies of Science},
  volume={44},
  number={3},
  pages={393--417},
  year={2014},
  publisher={Sage Publications Sage UK: London, England}
}

@article{fine2001efficient,
  title={Efficient SVM training using low-rank kernel representations},
  author={Fine, Shai and Scheinberg, Katya},
  journal={Journal of Machine Learning Research},
  volume={2},
  number={Dec},
  pages={243--264},
  year={2001}
}

@inproceedings{krityakierne2015global,
  title={Global optimization with sparse and local Gaussian process models},
  author={Krityakierne, Tipaluck and Ginsbourger, David},
  booktitle={International Workshop on Machine Learning, Optimization and Big Data},
  pages={185--196},
  year={2015},
  organization={Springer}
}
@article{cutajar2019deep,
  title={Deep gaussian processes for multi-fidelity modeling},
  author={Cutajar, Kurt and Pullin, Mark and Damianou, Andreas and Lawrence, Neil and Gonz{\'a}lez, Javier},
  journal={arXiv preprint arXiv:1903.07320},
  year={2019}
}

@article{hebbal2021bayesian,
  title={Bayesian optimization using deep Gaussian processes with applications to aerospace system design},
  author={Hebbal, Ali and Brevault, Loic and Balesdent, Mathieu and Talbi, El-Ghazali and Melab, Nouredine},
  journal={Optimization and Engineering},
  volume={22},
  number={1},
  pages={321--361},
  year={2021},
  publisher={Springer}
}

@inproceedings{houlsby_collaborative_2012,
	title = {Collaborative {Gaussian} {Processes} for {Preference} {Learning}},
	volume = {25},
	url = {https://papers.nips.cc/paper/2012/hash/afdec7005cc9f14302cd0474fd0f3c96-Abstract.html},
	language = {en},
	urldate = {2021-04-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Houlsby, Neil and Huszar, Ferenc and Ghahramani, Zoubin and Hernández-lobato, Jose},
	year = {2012},
	file = {Full Text PDF:/Users/wesleymaddox/Zotero/storage/PBQ6CDJ9/Houlsby et al. - 2012 - Collaborative Gaussian Processes for Preference Le.pdf:application/pdf}
}

@inproceedings{wu_parallel_2016,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'16},
	title = {The parallel knowledge gradient method for batch {Bayesian} optimization},
	isbn = {978-1-5108-3881-9},
	abstract = {In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural networks in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm — the parallel knowledge gradient method. By construction, this method provides the one-step Bayes optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.},
	urldate = {2021-04-15},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Wu, Jian and Frazier, Peter I.},
	month = dec,
	year = {2016},
	pages = {3134--3142},
    volume=30,
}

@inproceedings{wilson_copula_2010,
	title = {Copula {Processes}},
	abstract = {We deﬁne a copula process which describes the dependencies between arbitrarily many random variables independently of their marginal distributions. As an example, we develop a stochastic volatility model, Gaussian Copula Process Volatility (GCPV), to predict the latent standard deviations of a sequence of random variables. To make predictions we use Bayesian inference, with the Laplace approximation, and with Markov chain Monte Carlo as an alternative. We ﬁnd our model can outperform GARCH on simulated and ﬁnancial data. And unlike GARCH, GCPV can easily handle missing data, incorporate covariates other than time, and model a rich class of covariance structures.},
	language = {en},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Wilson, Andrew and Ghahramani, Zoubin},
	year = {2010},
	pages = {9},
	volume=23,
}

@article{andrade2020finding,
  title={Finding hotspots: development of an adaptive spatial sampling approach},
  author={Andrade-Pacheco, Ricardo and Rerolle, Francois and Lemoine, Jean and Hernandez, Leda and Me{\"\i}t{\'e}, Aboulaye and Juziwelo, Lazarus and Bibaut, Aur{\'e}lien F and van der Laan, Mark J and Arnold, Benjamin F and Sturrock, Hugh JW},
  journal={Scientific reports},
  volume={10},
  number={1},
  pages={1--12},
  year={2020},
  publisher={Nature Publishing Group}
}

@phdthesis{van2019sparse,
  title={Sparse Gaussian process approximations and applications},
  author={Van der Wilk, Mark},
  year={2019},
  school={University of Cambridge}
}
@ARTICLE{GPflowOpt2017,
   author = {Knudde, Nicolas and {van der Herten}, Joachim and Dhaene, Tom and Couckuyt, Ivo},
    title = "{{GP}flow{O}pt: {A} {B}ayesian {O}ptimization {L}ibrary using Tensor{F}low}",
  journal = {arXiv preprint arXiv:1711.03845},
  year    = {2017},
  url     = {https://arxiv.org/abs/1711.03845}
}
@inproceedings{shi2020sparse,
  title={Sparse orthogonal variational inference for gaussian processes},
  author={Shi, Jiaxin and Titsias, Michalis and Mnih, Andriy},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1932--1942},
  year={2020},
  organization={PMLR},
  volume=108,
}
@article{thompson1933likelihood,
  title={On the likelihood that one unknown probability exceeds another in view of the evidence of two samples},
  author={Thompson, William R},
  journal={Biometrika},
  volume={25},
  number={3/4},
  pages={285--294},
  year={1933},
  publisher={JSTOR}
}

@book{bertsekas2020rollout,
  title={Rollout, Policy Iteration, and Distributed Reinforcement Learning},
  author={Bertsekas, Dimitri P},
  year={2020},
  publisher={Athena Scientific}
}

@article{moreno2020recyclable,
  title={Recyclable Gaussian Processes},
  author={Moreno-Mu{\~n}oz, Pablo and Art{\'e}s-Rodr{\'\i}guez, Antonio and {\'A}lvarez, Mauricio A},
  journal={arXiv preprint arXiv:2010.02554},
  year={2020}
}
@incollection{ginsbourger2010kriging,
  title={Kriging is well-suited to parallelize optimization},
  author={Ginsbourger, David and Le Riche, Rodolphe and Carraro, Laurent},
  booktitle={Computational intelligence in expensive optimization problems},
  pages={131--162},
  year={2010},
  publisher={Springer}
}

@inproceedings{mania2018simple,
  title={Simple random search of static linear policies is competitive for reinforcement learning},
  author={Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  volume=31,
  booktitle={Advances in Neural Information Processing Systems},
  pages={1805--1814},
  year={2018}
}
@phdthesis{osborne2010bayesian,
  title={Bayesian Gaussian processes for sequential prediction, optimisation and quadrature},
  author={Osborne, Michael A},
  year={2010},
  school={Oxford University, UK}
}

@inproceedings{cheng2016incremental,
  title={Incremental Variational Sparse Gaussian Process Regression.},
  author={Cheng, Ching-An and Boots, Byron},
  booktitle={NIPS},
  pages={4403--4411},
  year={2016}
}
@article{pan2020continual,
  title={Continual deep learning by functional regularisation of memorable past},
  author={Pan, Pingbo and Swaroop, Siddharth and Immer, Alexander and Eschenhagen, Runa and Turner, Richard E and Khan, Mohammad Emtiyaz},
  journal={arXiv preprint arXiv:2004.14070},
  year={2020}
}
@article{letham2019constrained,
  title={Constrained Bayesian optimization with noisy experiments},
  author={Letham, Benjamin and Karrer, Brian and Ottoni, Guilherme and Bakshy, Eytan and others},
  journal={Bayesian Analysis},
  volume={14},
  number={2},
  pages={495--519},
  year={2019},
  publisher={International Society for Bayesian Analysis}
}
@article{opper2009variational,
  title={The variational Gaussian approximation revisited},
  author={Opper, Manfred and Archambeau, C{\'e}dric},
  journal={Neural computation},
  volume={21},
  number={3},
  pages={786--792},
  year={2009},
  publisher={MIT Press}
}

@article{CANU2006714,
title = {Kernel methods and the exponential family},
journal = {Neurocomputing},
volume = {69},
number = {7},
pages = {714-720},
year = {2006},
note = {New Issues in Neurocomputing: 13th European Symposium on Artificial Neural Networks},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2005.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0925231205003280},
author = {Stéphane Canu and Alex Smola},
keywords = {Kernel methods, Exponential families, Novelty detection},
abstract = {The success of support vector machine (SVM) has given rise to the development of a new class of theoretically elegant learning machines which use a central concept of kernels and the associated reproducing kernel Hilbert space (RKHS). Exponential families, a standard tool in statistics, can be used to unify many existing machine learning algorithms based on kernels (such as SVM) and to invent novel ones quite effortlessly. A new derivation of the novelty detection algorithm based on the one class SVM is proposed to illustrate the power of the exponential family model in an RKHS.}
}
@article{eriksson2021high,
  title={High-Dimensional Bayesian Optimization with Sparse Axis-Aligned Subspaces},
  author={Eriksson, David and Jankowiak, Martin},
  journal={arXiv preprint arXiv:2103.00349},
  year={2021}
}

@inproceedings{titsias2019functional,
  title={Functional Regularisation for Continual Learning with Gaussian Processes},
  author={Titsias, Michalis K and Schwarz, Jonathan and Matthews, Alexander G de G and Pascanu, Razvan and Teh, Yee Whye},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }
@article{brockman2016openai,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}
@article{kapoor2020variational,
  title={Variational Auto-Regressive Gaussian Processes for Continual Learning},
  author={Kapoor, Sanyam and Karaletsos, Theofanis and Bui, Thang D},
  journal={arXiv preprint arXiv:2006.05468},
  year={2020}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@inproceedings{wang2020learning,
  title={Learning Search Space Partition for Black-box Optimization using Monte Carlo Tree Search},
  author={Wang, Linnan and Fonseca, Rodrigo and Tian, Yuandong},
  booktitle={Advances in Neural Information Processing Systems},
  volume=33,
  year={2020}
}
@article{csato2002sparse,
  title={Sparse on-line Gaussian processes},
  author={Csat{\'o}, Lehel and Opper, Manfred},
  journal={Neural computation},
  volume={14},
  number={3},
  pages={641--668},
  year={2002},
  publisher={MIT Press}
}

@article{moreno2019continual,
  title={Continual Multi-task Gaussian Processes},
  author={Moreno-Mu{\~n}oz, Pablo and Art{\'e}s-Rodr{\'\i}guez, Antonio and {\'A}lvarez, Mauricio A},
  journal={arXiv preprint arXiv:1911.00002},
  year={2019}
}

@inproceedings{hoang2015unifying,
  title={A unifying framework of anytime sparse Gaussian process regression models with stochastic variational inference for big data},
  author={Hoang, Trong Nghia and Hoang, Quang Minh and Low, Bryan Kian Hsiang},
  booktitle={International Conference on Machine Learning},
  pages={569--578},
  year={2015},
  organization={PMLR},
  volume=37,
}

@article{lazaro2010sparse,
  title={Sparse spectrum Gaussian process regression},
  author={L{\'a}zaro-Gredilla, Miguel and Quinonero-Candela, Joaquin and Rasmussen, Carl Edward and Figueiras-Vidal, An{\'\i}bal R},
  journal={The Journal of Machine Learning Research},
  volume={11},
  pages={1865--1881},
  year={2010},
  publisher={JMLR. org}
}

@inproceedings{bauer2016understanding,
  title={Understanding probabilistic sparse Gaussian Process approximations},
  author={Bauer, M and Van Der Wilk, M and Rasmussen, CE},
  booktitle={Advances in Neural Information Processing Systems},
  volume=29,
  pages={1533--1541},
  year={2016}
}

@inproceedings{seeger2003fast,
  title={Fast forward selection to speed up sparse Gaussian process regression},
  author={Seeger, Matthias W and Williams, Christopher KI and Lawrence, Neil D},
  booktitle={International Workshop on Artificial Intelligence and Statistics},
  pages={254--261},
  year={2003},
  organization={PMLR}
}
