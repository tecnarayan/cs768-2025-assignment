\begin{thebibliography}{}

\bibitem[An et~al., 2021]{an2021uncertainty}
An, G., Moon, S., Kim, J.-H., and Song, H.~O. (2021).
\newblock Uncertainty-based offline reinforcement learning with diversified
  q-ensemble.
\newblock {\em Advances in neural information processing systems},
  34:7436--7447.

\bibitem[Bai et~al., 2022]{bai2022pessimistic}
Bai, C., Wang, L., Yang, Z., Deng, Z., Garg, A., Liu, P., and Wang, Z. (2022).
\newblock Pessimistic bootstrapping for uncertainty-driven offline
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2202.11566}.

\bibitem[Behzadan and Munir, 2018]{behzadan2018mitigation}
Behzadan, V. and Munir, A. (2018).
\newblock Mitigation of policy manipulation attacks on deep q-networks with
  parameter-space noise.
\newblock In {\em Computer Safety, Reliability, and Security: SAFECOMP 2018
  Workshops, ASSURE, DECSoS, SASSUR, STRIVE, and WAISE, V{\"a}ster{\aa}s,
  Sweden, September 18, 2018, Proceedings 37}, pages 406--417. Springer.

\bibitem[Bogunovic et~al., 2021]{bogunovic2021stochastic}
Bogunovic, I., Losalka, A., Krause, A., and Scarlett, J. (2021).
\newblock Stochastic linear bandits robust to adversarial attacks.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 991--999. PMLR.

\bibitem[Cesa-Bianchi and Lugosi, 2006]{cesa2006prediction}
Cesa-Bianchi, N. and Lugosi, G. (2006).
\newblock {\em Prediction, learning, and games}.
\newblock Cambridge university press.

\bibitem[Chen and Luo, 2021]{chen2021finding}
Chen, L. and Luo, H. (2021).
\newblock Finding the stochastic shortest path with low regret: The adversarial
  cost and unknown transition case.
\newblock In {\em International Conference on Machine Learning}, pages
  1651--1660. PMLR.

\bibitem[Chen et~al., 2023]{chen2022general}
Chen, Z., Li, C.~J., Yuan, A., Gu, Q., and Jordan, M.~I. (2023).
\newblock A general framework for sample-efficient function approximation in
  reinforcement learning.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Deng et~al., 2023]{deng2023uncertainty}
Deng, D., Chen, G., Yu, Y., Liu, F., and Heng, P.-A. (2023).
\newblock Uncertainty estimation by fisher information-based evidential deep
  learning.
\newblock {\em arXiv preprint arXiv:2303.02045}.

\bibitem[Ding et~al., 2022]{ding2022robust}
Ding, Q., Hsieh, C.-J., and Sharpnack, J. (2022).
\newblock Robust stochastic linear contextual bandits under adversarial
  attacks.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 7111--7123. PMLR.

\bibitem[Du et~al., 2021]{du2021bilinear}
Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W., and Wang, R.
  (2021).
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock In {\em International Conference on Machine Learning}, pages
  2826--2836. PMLR.

\bibitem[Du et~al., 2023]{du2023guiding}
Du, Y., Watkins, O., Wang, Z., Colas, C., Darrell, T., Abbeel, P., Gupta, A.,
  and Andreas, J. (2023).
\newblock Guiding pretraining in reinforcement learning with large language
  models.
\newblock {\em arXiv preprint arXiv:2302.06692}.

\bibitem[Duan et~al., 2020]{duan2020minimax}
Duan, Y., Jia, Z., and Wang, M. (2020).
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  2701--2709. PMLR.

\bibitem[Eykholt et~al., 2018]{eykholt2018robust}
Eykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Xiao, C.,
  Prakash, A., Kohno, T., and Song, D. (2018).
\newblock Robust physical-world attacks on deep learning visual classification.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1625--1634.

\bibitem[Foster et~al., 2020]{foster2020adapting}
Foster, D.~J., Gentile, C., Mohri, M., and Zimmert, J. (2020).
\newblock Adapting to misspecification in contextual bandits.
\newblock {\em Advances in Neural Information Processing Systems},
  33:11478--11489.

\bibitem[Foster et~al., 2021]{foster2021statistical}
Foster, D.~J., Kakade, S.~M., Qian, J., and Rakhlin, A. (2021).
\newblock The statistical complexity of interactive decision making.
\newblock {\em arXiv preprint arXiv:2112.13487}.

\bibitem[Fu et~al., 2020]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. (2020).
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock {\em arXiv preprint arXiv:2004.07219}.

\bibitem[Ghasemipour et~al., 2022]{ghasemipour2022so}
Ghasemipour, K., Gu, S.~S., and Nachum, O. (2022).
\newblock Why so pessimistic? estimating uncertainties for offline rl through
  ensembles, and why their independence matters.
\newblock {\em Advances in Neural Information Processing Systems},
  35:18267--18281.

\bibitem[Gupta et~al., 2019]{gupta2019better}
Gupta, A., Koren, T., and Talwar, K. (2019).
\newblock Better algorithms for stochastic bandits with adversarial
  corruptions.
\newblock In {\em Conference on Learning Theory}, pages 1562--1578. PMLR.

\bibitem[He et~al., 2022]{he2022nearly}
He, J., Zhou, D., Zhang, T., and Gu, Q. (2022).
\newblock Nearly optimal algorithms for linear contextual bandits with
  adversarial corruptions.
\newblock {\em arXiv preprint arXiv:2205.06811}.

\bibitem[Jiang et~al., 2017]{jiang2017contextual}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
  (2017).
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In {\em International Conference on Machine Learning}, pages
  1704--1713. PMLR.

\bibitem[Jin et~al., 2020a]{jin2020learning}
Jin, C., Jin, T., Luo, H., Sra, S., and Yu, T. (2020a).
\newblock Learning adversarial markov decision processes with bandit feedback
  and unknown transition.
\newblock In {\em International Conference on Machine Learning}, pages
  4860--4869. PMLR.

\bibitem[Jin et~al., 2021a]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S. (2021a).
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Jin et~al., 2020b]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I. (2020b).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143. PMLR.

\bibitem[Jin and Luo, 2020]{jin2020simultaneously}
Jin, T. and Luo, H. (2020).
\newblock Simultaneously learning stochastic and adversarial episodic mdps with
  known transition.
\newblock {\em Advances in neural information processing systems},
  33:16557--16566.

\bibitem[Jin et~al., 2021b]{jin2021pessimism}
Jin, Y., Yang, Z., and Wang, Z. (2021b).
\newblock Is pessimism provably efficient for offline rl?
\newblock In {\em International Conference on Machine Learning}, pages
  5084--5096. PMLR.

\bibitem[Kang et~al., 2023]{kang2023robust}
Kang, Y., Hsieh, C.-J., and Lee, T. (2023).
\newblock Robust lipschitz bandits to adversarial corruptions.
\newblock {\em arXiv preprint arXiv:2305.18543}.

\bibitem[Kober et~al., 2013]{kober2013reinforcement}
Kober, J., Bagnell, J.~A., and Peters, J. (2013).
\newblock Reinforcement learning in robotics: A survey.
\newblock {\em The International Journal of Robotics Research},
  32(11):1238--1274.

\bibitem[Lee et~al., 2021]{lee2021achieving}
Lee, C.-W., Luo, H., Wei, C.-Y., Zhang, M., and Zhang, X. (2021).
\newblock Achieving near instance-optimality and minimax-optimality in
  stochastic and adversarial linear bandits simultaneously.
\newblock In {\em International Conference on Machine Learning}, pages
  6142--6151. PMLR.

\bibitem[Liu et~al., 2017]{liu2017deep}
Liu, Y., Logan, B., Liu, N., Xu, Z., Tang, J., and Wang, Y. (2017).
\newblock Deep reinforcement learning for dynamic treatment regimes on medical
  registry data.
\newblock In {\em 2017 IEEE international conference on healthcare informatics
  (ICHI)}, pages 380--385. IEEE.

\bibitem[Luo et~al., 2021]{luo2021policy}
Luo, H., Wei, C.-Y., and Lee, C.-W. (2021).
\newblock Policy optimization in adversarial mdps: Improved exploration via
  dilated bonuses.
\newblock {\em Advances in Neural Information Processing Systems},
  34:22931--22942.

\bibitem[Lykouris et~al., 2018]{lykouris2018stochastic}
Lykouris, T., Mirrokni, V., and Paes~Leme, R. (2018).
\newblock Stochastic bandits robust to adversarial corruptions.
\newblock In {\em Proceedings of the 50th Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 114--122.

\bibitem[Ma et~al., 2019]{ma2019policy}
Ma, Y., Zhang, X., Sun, W., and Zhu, J. (2019).
\newblock Policy poisoning in batch reinforcement learning and control.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Neff, 2016]{neff2016talking}
Neff, G. (2016).
\newblock Talking to bots: Symbiotic agency and the case of tay.
\newblock {\em International Journal of Communication}.

\bibitem[Neu et~al., 2010]{neu2010online}
Neu, G., Gy{\"o}rgy, A., Szepesv{\'a}ri, C., et~al. (2010).
\newblock The online loop-free stochastic shortest-path problem.
\newblock In {\em COLT}, volume 2010, pages 231--243. Citeseer.

\bibitem[Osband et~al., 2016]{osband2016deep}
Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B. (2016).
\newblock Deep exploration via bootstrapped dqn.
\newblock {\em Advances in neural information processing systems}, 29.

\bibitem[Pan et~al., 2017]{pan2017agile}
Pan, Y., Cheng, C.-A., Saigol, K., Lee, K., Yan, X., Theodorou, E., and Boots,
  B. (2017).
\newblock Agile autonomous driving using end-to-end deep imitation learning.
\newblock {\em arXiv preprint arXiv:1709.07174}.

\bibitem[Rashidinejad et~al., 2021]{rashidinejad2021bridging}
Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. (2021).
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock {\em Advances in Neural Information Processing Systems},
  34:11702--11716.

\bibitem[Rosenberg and Mansour, 2019]{rosenberg2019online}
Rosenberg, A. and Mansour, Y. (2019).
\newblock Online convex optimization in adversarial markov decision processes.
\newblock In {\em International Conference on Machine Learning}, pages
  5478--5486. PMLR.

\bibitem[Rosenberg and Mansour, 2020]{rosenberg2020stochastic}
Rosenberg, A. and Mansour, Y. (2020).
\newblock Stochastic shortest path with adversarially changing costs.
\newblock {\em arXiv preprint arXiv:2006.11561}.

\bibitem[Russo and Van~Roy, 2013]{russo2013eluder}
Russo, D. and Van~Roy, B. (2013).
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock {\em Advances in Neural Information Processing Systems}, 26.

\bibitem[Sun et~al., 2022a]{sun2022exploit}
Sun, H., Han, L., Yang, R., Ma, X., Guo, J., and Zhou, B. (2022a).
\newblock Exploit reward shifting in value-based deep-rl: Optimistic
  curiosity-based exploration and conservative exploitation via linear reward
  shaping.
\newblock {\em Advances in Neural Information Processing Systems},
  35:37719--37734.

\bibitem[Sun et~al., 2022b]{sun2022daux}
Sun, H., van Breugel, B., Crabbe, J., Seedat, N., and van~der Schaar, M.
  (2022b).
\newblock Daux: a density-based approach for uncertainty explanations.
\newblock {\em arXiv preprint arXiv:2207.05161}.

\bibitem[Tarasov et~al., 2022]{tarasov2022corl}
Tarasov, D., Nikulin, A., Akimov, D., Kurenkov, V., and Kolesnikov, S. (2022).
\newblock {CORL}: Research-oriented deep offline reinforcement learning
  library.
\newblock In {\em 3rd Offline RL Workshop: Offline RL as a ''Launchpad''}.

\bibitem[Tropp, 2012]{tropp2012user}
Tropp, J.~A. (2012).
\newblock User-friendly tail bounds for sums of random matrices.
\newblock {\em Foundations of computational mathematics}, 12:389--434.

\bibitem[Uehara and Sun, 2021]{uehara2021pessimistic}
Uehara, M. and Sun, W. (2021).
\newblock Pessimistic model-based offline reinforcement learning under partial
  coverage.
\newblock {\em arXiv preprint arXiv:2107.06226}.

\bibitem[Wang et~al., 2018]{wang2018supervised}
Wang, L., Zhang, W., He, X., and Zha, H. (2018).
\newblock Supervised reinforcement learning with recurrent neural network for
  dynamic treatment recommendation.
\newblock In {\em Proceedings of the 24th ACM SIGKDD international conference
  on knowledge discovery \& data mining}, pages 2447--2456.

\bibitem[Wang et~al., 2020a]{wang2020statistical}
Wang, R., Foster, D.~P., and Kakade, S.~M. (2020a).
\newblock What are the statistical limits of offline rl with linear function
  approximation?
\newblock {\em arXiv preprint arXiv:2010.11895}.

\bibitem[Wang et~al., 2020b]{wang2020reinforcement}
Wang, R., Salakhutdinov, R.~R., and Yang, L. (2020b).
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6123--6135.

\bibitem[Wei et~al., 2022]{wei2022model}
Wei, C.-Y., Dann, C., and Zimmert, J. (2022).
\newblock A model selection approach for corruption robust reinforcement
  learning.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 1043--1096. PMLR.

\bibitem[Wu et~al., 2022]{wucopa}
Wu, F., Li, L., Zhang, H., Kailkhura, B., Kenthapadi, K., Zhao, D., and Li, B.
  (2022).
\newblock Copa: Certifying robust policies for offline reinforcement learning
  against poisoning attacks.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Wu et~al., 2021]{wu2021reinforcement}
Wu, T., Yang, Y., Du, S., and Wang, L. (2021).
\newblock On reinforcement learning with adversarial corruption and its
  application to block mdp.
\newblock In {\em International Conference on Machine Learning}, pages
  11296--11306. PMLR.

\bibitem[Xie et~al., 2021]{xie2021bellman}
Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2021).
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock {\em Advances in neural information processing systems},
  34:6683--6694.

\bibitem[Xiong et~al., 2022]{xiong2022nearly}
Xiong, W., Zhong, H., Shi, C., Shen, C., Wang, L., and Zhang, T. (2022).
\newblock Nearly minimax optimal offline reinforcement learning with linear
  function approximation: Single-agent mdp and markov game.
\newblock {\em arXiv preprint arXiv:2205.15512}.

\bibitem[Yang et~al., 2022]{yangrorl}
Yang, R., Bai, C., Ma, X., Wang, Z., Zhang, C., and Han, L. (2022).
\newblock Rorl: Robust offline reinforcement learning via conservative
  smoothing.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Yang et~al., 2023]{yang2023essential}
Yang, R., Yong, L., Ma, X., Hu, H., Zhang, C., and Zhang, T. (2023).
\newblock What is essential for unseen goal generalization of offline
  goal-conditioned rl?
\newblock In {\em International Conference on Machine Learning}, pages
  39543--39571. PMLR.

\bibitem[Ye et~al., 2022]{ye2022corruptionrobust}
Ye, C., Xiong, W., Gu, Q., and Zhang, T. (2022).
\newblock Corruption-robust algorithms with uncertainty weighting for nonlinear
  contextual bandits and markov decision processes.

\bibitem[Yin and Wang, 2021]{yin2021towards}
Yin, M. and Wang, Y.-X. (2021).
\newblock Towards instance-optimal offline reinforcement learning with
  pessimism.
\newblock {\em Advances in neural information processing systems},
  34:4065--4078.

\bibitem[Zanette et~al., 2021]{zanette2021provable}
Zanette, A., Wainwright, M.~J., and Brunskill, E. (2021).
\newblock Provable benefits of actor-critic methods for offline reinforcement
  learning.
\newblock {\em Advances in neural information processing systems},
  34:13626--13640.

\bibitem[Zhang et~al., 2020a]{zhang2020robust}
Zhang, H., Chen, H., Xiao, C., Li, B., Liu, M., Boning, D., and Hsieh, C.-J.
  (2020a).
\newblock Robust deep reinforcement learning against adversarial perturbations
  on state observations.
\newblock {\em Advances in Neural Information Processing Systems},
  33:21024--21037.

\bibitem[Zhang, 2023]{TZ23-lt}
Zhang, T. (2023).
\newblock {\em Mathematical Analysis of Machine Learning Algorithms}.
\newblock Cambridge University Press.
\newblock in press, also available as
  \url{http://tongzhang-ml.org/lt-book.html}.

\bibitem[Zhang et~al., 2022]{zhang2022corruption}
Zhang, X., Chen, Y., Zhu, X., and Sun, W. (2022).
\newblock Corruption-robust offline reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 5757--5773. PMLR.

\bibitem[Zhang et~al., 2020b]{zhang2020adaptive}
Zhang, X., Ma, Y., Singla, A., and Zhu, X. (2020b).
\newblock Adaptive reward-poisoning attacks against reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  11225--11234. PMLR.

\bibitem[Zhao et~al., 2021]{zhao2021linear}
Zhao, H., Zhou, D., and Gu, Q. (2021).
\newblock Linear contextual bandits with adversarial corruptions.
\newblock {\em arXiv preprint arXiv:2110.12615}.

\bibitem[Zhong et~al., 2022a]{zhong2022pessimistic}
Zhong, H., Xiong, W., Tan, J., Wang, L., Zhang, T., Wang, Z., and Yang, Z.
  (2022a).
\newblock Pessimistic minimax value iteration: Provably efficient equilibrium
  learning from offline datasets.
\newblock In {\em International Conference on Machine Learning}, pages
  27117--27142. PMLR.

\bibitem[Zhong et~al., 2022b]{zhong2022posterior}
Zhong, H., Xiong, W., Zheng, S., Wang, L., Wang, Z., Yang, Z., and Zhang, T.
  (2022b).
\newblock A posterior sampling framework for interactive decision making.
\newblock {\em arXiv preprint arXiv:2211.01962}.

\bibitem[Zhou and Gu, 2022]{zhou2022computationally}
Zhou, D. and Gu, Q. (2022).
\newblock Computationally efficient horizon-free reinforcement learning for
  linear mixture mdps.
\newblock {\em Advances in neural information processing systems},
  35:36337--36349.

\end{thebibliography}
