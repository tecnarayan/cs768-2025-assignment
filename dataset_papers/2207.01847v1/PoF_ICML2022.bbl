\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Botev et~al.(2017)Botev, Ritter, and Barber]{paper:Botev17}
Botev, A., Ritter, H., and Barber, D.
\newblock Practical gauss-newton optimisation for deep learning.
\newblock In \emph{ICML}, 2017.

\bibitem[Chaudhari \& Soatto(2018)Chaudhari and
  Soatto]{chaudhari2018stochastic}
Chaudhari, P. and Soatto, S.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Chaudhari et~al.(2019)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{chaudhari2019entropy}
Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C.,
  Chayes, J., Sagun, L., and Zecchina, R.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2019\penalty0 (12):\penalty0 124018, 2019.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Zhou, Tang, Yang, Cao, and
  Gu]{10.5555/3491440.3491892}
Chen, J., Zhou, D., Tang, Y., Yang, Z., Cao, Y., and Gu, Q.
\newblock Closing the generalization gap of adaptive gradient methods in
  training deep neural networks.
\newblock In \emph{IJCAI}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Xiao, Li, Ye, Sun, and
  Deng]{paper:Chen20}
Chen, Z., Xiao, R., Li, C., Ye, G., Sun, H., and Deng, H.
\newblock Esam: Discriminative domain adaptation with non-displayed items to
  improve long-tail performance.
\newblock In \emph{SIGIR}, 2020{\natexlab{b}}.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{paper:dinh17}
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{ICML}, 2017.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{PAC_flat}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock In \emph{UAI}, 2017.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and Neyshabur]{paper:sam}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{ICLR}, 2021.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Vinyals, and Saxe]{Goodfellow2015}
Goodfellow, I.~J., Vinyals, O., and Saxe, A.~M.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock In \emph{ICLR}, 2015.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Goyal, P., Doll{\'a}r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv:1706.02677}, 2017.

\bibitem[Grosse \& Martens(2016)Grosse and Martens]{paper:Grosse16}
Grosse, R. and Martens, J.
\newblock A kronecker-factored approximate fisher matrix for convolution
  layers.
\newblock In \emph{ICML}, 2016.

\bibitem[Hinton \& Camp(1993)Hinton and Camp]{paper:Hinton93}
Hinton, G.~E. and Camp, D.~v.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{Annual Conference on Computational Learning Theory (COLT)},
  pp.\  5–13, 1993.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, Krizhevsky, Sutskever, and
  Salakhutdinov]{Hinton2012}
Hinton, G.~E., Srivastava, N., Krizhevsky, A., Sutskever, I., and
  Salakhutdinov, R.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock \emph{arXiv:1207.0580}, 2012.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{paper:Hochreiter97}
Hochreiter, S. and Schmidhuber, J.
\newblock Flat minima.
\newblock In \emph{Neural Computation}, volume 9(1), pp.\  1–42, 1997.

\bibitem[Honkela \& Valpola(2004)Honkela and Valpola]{paper:Honkela04}
Honkela, A. and Valpola, H.~V.
\newblock Variational learning and bits-back coding: An information-theoretic
  view to bayesian learning.
\newblock In \emph{IEEE Trans. on Neural Networks, 15(4): 800–810}, 2004.

\bibitem[Im et~al.(2016)Im, Tao, and Branson]{DBLP:journals/corr/ImTB16}
Im, D.~J., Tao, M., and Branson, K.
\newblock An empirical analysis of deep network loss surfaces.
\newblock \emph{arXiv:1612.04010}, 2016.

\bibitem[Izmailov et~al.(2018)Izmailov, ad~Timur~Garipov, Vetrov, and
  Wilson]{paper:Izmailov}
Izmailov, P., ad~Timur~Garipov, D.~P., Vetrov, D., and Wilson, A.~G.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In \emph{UAI}, 2018.

\bibitem[Jastrzebski et~al.(2018)Jastrzebski, Kenton, Arpit, Ballas, Fischer,
  Bengio, and Storkey]{paper:Jastrzebski18}
Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y.,
  and Storkey, A.~J.
\newblock Finding flatter minima with sgd.
\newblock In \emph{ICLR Workshop}, 2018.

\bibitem[Jiang* et~al.(2020)Jiang*, Neyshabur*, Mobahi, Krishnan, and
  Bengio]{paper:Jiang20}
Jiang*, Y., Neyshabur*, B., Mobahi, H., Krishnan, D., and Bengio, S.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{ICLR}, 2020.

\bibitem[Keskar \& Socher(2017)Keskar and Socher]{Keskar2017ImprovingGP}
Keskar, N.~S. and Socher, R.
\newblock Improving generalization performance by switching from adam to sgd.
\newblock \emph{arXiv:1712.07628}, 2017.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{paper:keskar17}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{ICLR}, 2017.

\bibitem[Krizhevskyf \& Hinton(2009)Krizhevskyf and
  Hinton]{Krizhevsky09learningmultiple}
Krizhevskyf, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical Report~4, 2009.

\bibitem[Kwon et~al.(2021)Kwon, Kim, Park, and Choi]{paper:Jungmin21}
Kwon, J., Kim, J., Park, H., and Choi, I.~K.
\newblock Asam: Adaptive sharpness-aware minimization for scale-invariant
  learning of deep neural networks.
\newblock In \emph{ICML}, 2021.

\bibitem[Langford \& Shawe-Taylor(2003)Langford and
  Shawe-Taylor]{paper:Becker03}
Langford, J. and Shawe-Taylor, J.
\newblock Pac-bayes \& margins.
\newblock In \emph{NeurIPS}, 2003.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{NEURIPS2018_a41b3bb3}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Lin et~al.(2014)Lin, Chen, and Yan]{Lin2013network}
Lin, M., Chen, Q., and Yan, S.
\newblock Network in network.
\newblock \emph{ICLR}, 2014.

\bibitem[Maddox et~al.(2020)Maddox, Benton, and Wilson]{paper:Maddox20}
Maddox, W.~J., Benton, G., and Wilson, A.~G.
\newblock Rethinking parameter counting in deep models: Effective
  dimensionality revisited.
\newblock \emph{arXiv:2003.02139}, 2020.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{paper:Martens15}
Martens, J. and Grosse, R.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{ICML}, pp.\  2408–2417, 2015.

\bibitem[Moayed \& Mansoori(2020)Moayed and Mansoori]{paper:Hojjat20}
Moayed, H. and Mansoori, E.~G.
\newblock Regularization of neural network using dropcoadapt.
\newblock In \emph{International Conference on Computer and Knowledge
  Engineering (ICCKE)}, 2020.

\bibitem[Nesterov(1998)]{nesterov1998introductory}
Nesterov, Y.
\newblock Introductory lectures on convex programming volume i: Basic course.
\newblock \emph{Lecture notes}, 3\penalty0 (4):\penalty0 5, 1998.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and Ng]{SVHN}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NeurIPS Workshop}, 2011.

\bibitem[Pauloski et~al.(2021)Pauloski, Huang, Huang, Venkataraman, Chard,
  Foster, and Zhang]{paper:Pauloski21}
Pauloski, J.~G., Huang, Q., Huang, L., Venkataraman, S., Chard, K., Foster, I.,
  and Zhang, Z.
\newblock Kaisa: An adaptive second-order optimizer framework for deep neural
  networks.
\newblock In \emph{International Conference for High Performance Computing,
  Networking, Storage, and Analysis (SC)}, 2021.

\bibitem[Rissanen(1978)]{paper:Rissanen78}
Rissanen, J.
\newblock Modeling by shortest data description.
\newblock In \emph{Automatica, 14(5):465–471}, 1978.

\bibitem[Roux et~al.(2008)Roux, Manzagol, and Bengio]{paper:Roux}
Roux, N.~L., Manzagol, P.~A., and Bengio, Y.
\newblock Topmoumoute online natural gradient algorithm.
\newblock In \emph{NeurIPS}, 2008.

\bibitem[Sagun et~al.(2016)Sagun, Bottou, and LeCun]{paper:Sagun16}
Sagun, L., Bottou, L., and LeCun, Y.
\newblock Eigenvalues of the hessian in deep learning: Singularity and beyond.
\newblock In \emph{arXiv:1611.07476}, 2016.

\bibitem[Sato et~al.(2019)Sato, Ishikawa, Liu, and Tanaka]{paper:foca}
Sato, I., Ishikawa, K., Liu, G., and Tanaka, M.
\newblock Breaking inter-layer co-adaptation by classifier anonymization.
\newblock In \emph{ICML}, 2019.

\bibitem[Sato et~al.(2021)Sato, Ishikawa, Liu, and Tanaka]{paper:foca2}
Sato, I., Ishikawa, K., Liu, G., and Tanaka, M.
\newblock Does end-to-end trained deep model always perform better than
  non-end-to-end counterpart?
\newblock In \emph{Electronic Imaging}, 2021.

\bibitem[Wei et~al.(2020)Wei, Wei, Jin, Wei, Huang, Hua, Cai, and
  He]{paper:Wei20}
Wei, L., Wei, Z., Jin, Z., Wei, Q., Huang, J., Hua, X.-S., Cai, D., and He, X.
\newblock Decouple co-adaptation: Classifier randomization for person
  re-identification.
\newblock In \emph{Elsevier Neurocomputing}, volume 383, pp.\  1--9, 2020.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{AshiaWilson}
Wilson, A.~C., Roelofs, R., Stern, M., Srebro, N., and Recht, B.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{paper:Donxian}
Wu, D., Xia, S.-T., and Wang, Y.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Wu et~al.(2017)Wu, Zhu, and E]{paper:Wu17}
Wu, L., Zhu, Z., and E, W.
\newblock Towards understanding generalization of deep learning: Perspective of
  loss landscapes.
\newblock In \emph{ICML PADL Workshop}, 2017.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{paper:fmnist}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv1708.07747}, 2017.

\bibitem[Yao et~al.(2018)Yao, Gholami, Lei, Keutzer, and Mahoney]{paper:Yao18}
Yao, Z., Gholami, A., Lei, Q., Keutzer, K., and Mahoney, M.~W.
\newblock Hessian-based analysis of large batch training and robustness to
  adversaries.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and Komodakis]{paper:wideresnet}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{British Machine Vision Virtual Conference (BMVC)}, 2016.

\bibitem[Zhang et~al.(2018)Zhang, Saxe, Advani, and Lee]{paper:Zhang18}
Zhang, Y., Saxe, A.~M., Advani, M.~S., and Lee, A.~A.
\newblock Energy–entropy competition and the effectiveness of stochastic
  gradient descent in machine learning.
\newblock In \emph{Molecular Physics}, volume 116(21-22), pp.\  3214–3223,
  2018.

\bibitem[Zheng et~al.(2021)Zheng, Zhang, and Mao]{paper:AMP}
Zheng, Y., Zhang, R., and Mao, Y.
\newblock Regularizing neural networks via adversarial model perturbation.
\newblock In \emph{CVPR}, 2021.

\bibitem[Zhou et~al.(2020)Zhou, Karimi, Yu, Xu, and Li]{NEURIPS2020_08fb104b}
Zhou, Y., Karimi, B., Yu, J., Xu, Z., and Li, P.
\newblock Towards better generalization of adaptive gradient methods.
\newblock In \emph{NeurIPS}, 2020.

\end{thebibliography}
