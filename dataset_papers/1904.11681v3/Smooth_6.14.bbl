\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2008)Abernethy, Bartlett, Rakhlin, and
  Tewari]{Minimax:Online}
Jacob Abernethy, Peter~L. Bartlett, Alexander Rakhlin, and Ambuj Tewari.
\newblock Optimal stragies and minimax lower bounds for online convex games.
\newblock In \emph{Proceedings of the 21st Annual Conference on Learning
  Theory}, pages 415--423, 2008.

\bibitem[Adamskiy et~al.(2012)Adamskiy, Koolen, Chernov, and
  Vovk]{Adamskiy2012}
Dmitry Adamskiy, Wouter~M. Koolen, Alexey Chernov, and Vladimir Vovk.
\newblock A closer look at adaptive regret.
\newblock In \emph{Proceedings of the 23rd International Conference on
  Algorithmic Learning Theory}, pages 290--304, 2012.

\bibitem[Arora et~al.(2012)Arora, Hazan, and Kale]{v008a006}
Sanjeev Arora, Elad Hazan, and Satyen Kale.
\newblock The multiplicative weights update method: a meta-algorithm and
  applications.
\newblock \emph{Theory of Computing}, 8\penalty0 (6):\penalty0 121--164, 2012.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Gentile]{AUER200248}
Peter Auer, Nicol\`{o} Cesa-Bianchi, and Claudio Gentile.
\newblock Adaptive and self-confident on-line learning algorithms.
\newblock \emph{Journal of Computer and System Sciences}, 64\penalty0
  (1):\penalty0 48--75, 2002.

\bibitem[Boyd and Vandenberghe(2004)]{Convex-Optimization}
Stephen Boyd and Lieven Vandenberghe.
\newblock \emph{Convex Optimization}.
\newblock Cambridge University Press, 2004.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{bianchi-2006-prediction}
Nicol\`{o} Cesa-Bianchi and G{\'a}bor Lugosi.
\newblock \emph{Prediction, Learning, and Games}.
\newblock Cambridge University Press, 2006.

\bibitem[Cesa-bianchi et~al.(2012)Cesa-bianchi, Gaillard, Lugosi, and
  Stoltz]{Fixed:Share:NIPS12}
Nicol\`{o} Cesa-bianchi, Pierre Gaillard, Gabor Lugosi, and Gilles Stoltz.
\newblock Mirror descent meets fixed share (and feels no regret).
\newblock In \emph{Advances in Neural Information Processing Systems 25}, pages
  980--988, 2012.

\bibitem[Chernov and Vovk(2010)]{Chernov:2010:PAU}
Alexey Chernov and Vladimir Vovk.
\newblock Prediction with advice of unknown number of experts.
\newblock In \emph{Proceedings of the 26th Conference on Uncertainty in
  Artificial Intelligence}, pages 117--125, 2010.

\bibitem[Daniely et~al.(2015)Daniely, Gonen, and
  Shalev-Shwartz]{Adaptive:ICML:15}
Amit Daniely, Alon Gonen, and Shai Shalev-Shwartz.
\newblock Strongly adaptive online learning.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, pages 1405--1411, 2015.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{JMLR:Adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121--2159,
  2011.

\bibitem[Gaillard et~al.(2014)Gaillard, Stoltz, and van
  Erven]{pmlr-v35-gaillard14}
Pierre Gaillard, Gilles Stoltz, and Tim van Erven.
\newblock A second-order bound with excess losses.
\newblock In \emph{Proceedings of The 27th Conference on Learning Theory},
  pages 176--196, 2014.

\bibitem[Gy\"{o}rgy et~al.(2012)Gy\"{o}rgy, Linder, and
  Lugosi]{Track_Large_Expert}
Andr\'{a}s Gy\"{o}rgy, Tam\'{a}s Linder, and G\'{a}bor Lugosi.
\newblock Efficient tracking of large classes of experts.
\newblock \emph{IEEE Transactions on Information Theory}, 58\penalty0
  (11):\penalty0 6709--6725, 2012.

\bibitem[Hall and Willett(2013)]{Dynamic:ICML:13}
Eric~C. Hall and Rebecca~M. Willett.
\newblock Dynamical models and tracking regret in online convex programming.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning}, pages 579--587, 2013.

\bibitem[Hazan(2016)]{Intro:Online:Convex}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends in Optimization}, 2\penalty0
  (3-4):\penalty0 157--325, 2016.

\bibitem[Hazan and Seshadhri(2007)]{Adaptive:Hazan}
Elad Hazan and C.~Seshadhri.
\newblock Adaptive algorithms for online decision problems.
\newblock \emph{Electronic Colloquium on Computational Complexity}, 88, 2007.

\bibitem[Hazan and Seshadhri(2009)]{Hazan:2009:ELA}
Elad Hazan and C.~Seshadhri.
\newblock Efficient learning algorithms for changing environments.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 393--400, 2009.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{ML:Hazan:2007}
Elad Hazan, Amit Agarwal, and Satyen Kale.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 169--192, 2007.

\bibitem[Herbster and Warmuth(1998)]{Herbster1998}
Mark Herbster and Manfred~K. Warmuth.
\newblock Tracking the best expert.
\newblock \emph{Machine Learning}, 32\penalty0 (2):\penalty0 151--178, 1998.

\bibitem[Jadbabaie et~al.(2015)Jadbabaie, Rakhlin, Shahrampour, and
  Sridharan]{Dynamic:AISTATS:15}
Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan.
\newblock Online optimization: Competing with dynamic comparators.
\newblock In \emph{Proceedings of the 18th International Conference on
  Artificial Intelligence and Statistics}, pages 398--406, 2015.

\bibitem[Jun et~al.(2017{\natexlab{a}})Jun, Orabona, Wright, and
  Willett]{Improved:Strongly:Adaptive}
Kwang-Sung Jun, Francesco Orabona, Stephen Wright, and Rebecca Willett.
\newblock Improved strongly adaptive online learning using coin betting.
\newblock In \emph{Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics}, pages 943--951, 2017{\natexlab{a}}.

\bibitem[Jun et~al.(2017{\natexlab{b}})Jun, Orabona, Wright, and
  Willett]{jun2017}
Kwang-Sung Jun, Francesco Orabona, Stephen Wright, and Rebecca Willett.
\newblock Online learning for changing environments using coin betting.
\newblock \emph{Electronic Journal of Statistics}, 11\penalty0 (2):\penalty0
  5282--5310, 2017{\natexlab{b}}.

\bibitem[Littlestone and Warmuth(1994)]{LITTLESTONE1994212}
Nick Littlestone and Manfred~K. Warmuth.
\newblock The weighted majority algorithm.
\newblock \emph{Information and Computation}, 108\penalty0 (2):\penalty0
  212--261, 1994.

\bibitem[Luo and Schapire(2015)]{pmlr-v40-Luo15}
Haipeng Luo and Robert~E. Schapire.
\newblock Achieving all with no parameters: Adanormalhedge.
\newblock In \emph{Proceedings of The 28th Conference on Learning Theory},
  pages 1286--1304, 2015.

\bibitem[Mokhtari et~al.(2016)Mokhtari, Shahrampour, Jadbabaie, and
  Ribeiro]{Dynamic:Strongly}
Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro.
\newblock Online optimization in dynamic environments: Improved regret rates
  for strongly convex problems.
\newblock In \emph{Proceedings of the 55th IEEE Conference on Decision and
  Control}, pages 7195--7201, 2016.

\bibitem[Orabona and P\'{a}l(2018)]{Scale:Free:Online}
Francesco Orabona and D\'{a}vid P\'{a}l.
\newblock Scale-free online learning.
\newblock \emph{Theoretical Computer Science}, 716:\penalty0 50--69, 2018.

\bibitem[Orabona and Tommasi(2017)]{NIPS2017_6811}
Francesco Orabona and Tatiana Tommasi.
\newblock Training deep networks without learning rates through coin betting.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pages
  2160--2170, 2017.

\bibitem[Orabona et~al.(2012)Orabona, Cesa-Bianchi, and
  Gentile]{Beyond:Logarithmic}
Francesco Orabona, Nicolo Cesa-Bianchi, and Claudio Gentile.
\newblock Beyond logarithmic bounds in online learning.
\newblock In \emph{Proceedings of the 15th International Conference on
  Artificial Intelligence and Statistics}, pages 823--831, 2012.

\bibitem[Shalev-Shwartz(2007)]{Shai:thesis}
Shai Shalev-Shwartz.
\newblock \emph{Online Learning: Theory, Algorithms, and Applications}.
\newblock PhD thesis, The Hebrew University of Jerusalem, 2007.

\bibitem[Shalev-Shwartz(2011)]{Online:suvery}
Shai Shalev-Shwartz.
\newblock Online learning and online convex optimization.
\newblock \emph{Foundations and Trends in Machine Learning}, 4\penalty0
  (2):\penalty0 107--194, 2011.

\bibitem[Srebro et~al.(2010)Srebro, Sridharan, and Tewari]{NIPS2010_Smooth}
Nathan Srebro, Karthik Sridharan, and Ambuj Tewari.
\newblock Smoothness, low-noise and fast rates.
\newblock In \emph{Advances in Neural Information Processing Systems 23}, pages
  2199--2207, 2010.

\bibitem[Wang et~al.(2018)Wang, Zhao, and Zhang]{Adaptive:One:Gradient}
Guanghui Wang, Dakuan Zhao, and Lijun Zhang.
\newblock Minimizing adaptive regret with one gradient per iteration.
\newblock In \emph{Proceedings of the 27th International Joint Conference on
  Artificial Intelligence}, pages 2762--2768, 2018.

\bibitem[Yang et~al.(2016)Yang, Zhang, Jin, and Yi]{Dynamic:2016}
Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi.
\newblock Tracking slowly moving clairvoyant: Optimal dynamic regret of online
  learning with true and noisy gradient.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning}, pages 449--457, 2016.

\bibitem[Zhang et~al.(2017)Zhang, Yang, Yi, Jin, and
  Zhou]{Dynamic:Regret:Squared}
Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, and Zhi-Hua Zhou.
\newblock Improved dynamic regret for non-degenerate functions.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pages
  732--741, 2017.

\bibitem[Zhang et~al.(2018{\natexlab{a}})Zhang, Lu, and
  Zhou]{Adaptive:Dynamic:Regret:NIPS}
Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou.
\newblock Adaptive online learning in dynamic environments.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pages
  1330--1340, 2018{\natexlab{a}}.

\bibitem[Zhang et~al.(2018{\natexlab{b}})Zhang, Yang, Jin, and
  Zhou]{Dynamic:Regret:Adaptive}
Lijun Zhang, Tianbao Yang, Rong Jin, and Zhi-Hua Zhou.
\newblock Dynamic regret of strongly adaptive methods.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, 2018{\natexlab{b}}.

\bibitem[Zinkevich(2003)]{zinkevich-2003-online}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning}, pages 928--936, 2003.

\end{thebibliography}
