
@incollection{10.5555/3454287.3455010,
  title = {A Debiased {{MDI}} Feature Importance Measure for Random Forests},
  booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  author = {Li, Xiao and Wang, Yu and Basu, Sumanta and Kumbier, Karl and Yu, Bin},
  year = {2019},
  publisher = {{Curran Associates Inc.}},
  location = {{Red Hook, NY, USA}},
  abstract = {Tree ensembles such as Random Forests have achieved impressive empirical success across a wide variety of applications. To understand how these models make predictions, people routinely turn to feature importance measures calculated from tree ensembles. It has long been known that Mean Decrease Impurity (MDI), one of the most widely used measures of feature importance, incorrectly assigns high importance to noisy features, leading to systematic bias in feature selection. In this paper, we address the feature selection bias of MDI from both theoretical and methodological perspectives. Based on the original definition of MDI by Breiman et al. [3] for a single tree, we derive a tight non-asymptotic bound on the expected bias of MDI importance of noisy features, showing that deep trees have higher (expected) feature selection bias than shallow ones. However, it is not clear how to reduce the bias of MDI using its existing analytical expression. We derive a new analytical expression for MDI, and based on this new expression, we are able to propose a new MDI feature importance measure using out-of-bag samples, called MDI-oob. For both the simulated data and a genomic ChIP dataset, MDI-oob achieves state-of-the-art performance in feature selection from Random Forests for both deep and shallow trees.},
  articleno = {723},
  pagetotal = {11}
}

@article{aasExplainingIndividualPredictions2020,
title = {Explaining individual predictions when features are dependent: More accurate approximations to Shapley values},
journal = {Artificial Intelligence},
volume = {298},
pages = {103502},
year = {2021},
_issn = {0004-3702},
_doi = {https://doi.org/10.1016/j.artint.2021.103502},
__url = {https://www.sciencedirect.com/science/article/pii/S0004370221000539},
author = {Kjersti Aas and Martin Jullum and Anders Løland},
keywords = {Feature attribution, Shapley values, Kernel SHAP, Dependence},
abstract = {Explaining complex or seemingly simple machine learning models is an important practical problem. We want to explain individual predictions from such models by learning simple, interpretable explanations. Shapley value is a game theoretic concept that can be used for this purpose. The Shapley value framework has a series of desirable theoretical properties, and can in principle handle any predictive model. Kernel SHAP is a computationally efficient approximation to Shapley values in higher dimensions. Like several other existing methods, this approach assumes that the features are independent. Since Shapley values currently suffer from inclusion of unrealistic data instances when features are correlated, the explanations may be very misleading. This is the case even if a simple linear model is used for predictions. In this paper, we extend the Kernel SHAP method to handle dependent features. We provide several examples of linear and non-linear models with various degrees of feature dependence, where our method gives more accurate approximations to the true Shapley values.}
}

@inproceedings{abadiTensorflowSystemLargescale2016,
  title = {Tensorflow: {{A}} System for Large-Scale Machine Learning},
  booktitle = {12th \$\{\$\vphantom\}{{USENIX}}\$\vphantom\{\}\$ Symposium on Operating Systems Design and Implementation (\$\{\$\vphantom\}{{OSDI}}\$\vphantom\{\}\$ 16)},
  author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  year = {2016},
  pages = {265--283}
}

@article{grompingModelagnosticEffectsPlots2020,
  journal = {Reports in Mathematics, Physics and Chemistry},
  title = {Model-Agnostic Effects Plots for Interpreting Machine Learning Models},
  author = {Grömping, Ulrike},
  year = {2020},
  number = {1/2020}
}

@inproceedings{abadiTensorflowSystemLargescale2016a,
  title = {Tensorflow: {{A}} System for Large-Scale Machine Learning},
  booktitle = {12th \$\{\$\vphantom\}{{USENIX}}\$\vphantom\{\}\$ Symposium on Operating Systems Design and Implementation (\$\{\$\vphantom\}{{OSDI}}\$\vphantom\{\}\$ 16)},
  author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  year = {2016},
  pages = {265--283}
}

@inproceedings{abadiTensorflowSystemLargescale2016b,
  title = {Tensorflow: {{A}} System for Large-Scale Machine Learning},
  booktitle = {12th \$\{\$\vphantom\}{{USENIX}}\$\vphantom\{\}\$ Symposium on Operating Systems Design and Implementation (\$\{\$\vphantom\}{{OSDI}}\$\vphantom\{\}\$ 16)},
  author = {Abadi, Martín and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  year = {2016},
  pages = {265--283}
}

@article{abelsonVarianceExplanationParadox1985,
  title = {A Variance Explanation Paradox: {{When}} a Little Is a Lot.},
  shorttitle = {A Variance Explanation Paradox},
  author = {Abelson, Robert P.},
  year = {1985},
  journaltitle = {Psychological Bulletin},
  shortjournal = {Psychological Bulletin},
  volume = {97},
  number = {1},
  pages = {129--133},
  _issn = {1939-1455, 0033-2909},
  _doi = {10.1037/0033-2909.97.1.129},
  __url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.97.1.129},
  _urldate = {2021-11-22},
  langid = {english}
}

@article{adadiPeekingBlackBoxSurvey2018,
  title = {Peeking {{Inside}} the {{Black-Box}}: {{A Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  shorttitle = {Peeking {{Inside}} the {{Black-Box}}},
  author = {Adadi, Amina and Berrada, Mohammed},
  year = {2018},
  journaltitle = {IEEE Access},
  volume = {6},
  pages = {52138--52160},
  _issn = {2169-3536},
  _doi = {10.1109/ACCESS.2018.2870052},
  abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  keywords = {AI-based systems,artificial intelligence,Biological system modeling,black-box models,black-box nature,Conferences,explainable AI,explainable artificial intelligence,Explainable artificial intelligence,fourth industrial revolution,interpretable machine learning,Machine learning,Machine learning algorithms,Market research,Prediction algorithms,XAI}
}

@unpublished{adamczewskiQFITQuantifiableFeature2020,
  title = {Q-{{FIT}}: {{The Quantifiable Feature Importance Technique}} for {{Explainable Machine Learning}}},
  shorttitle = {Q-{{FIT}}},
  author = {Adamczewski, Kamil and Harder, Frederik and Park, Mijung},
  year = {2020-10-26},
  eprint = {2010.13872},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2010.13872},
  _urldate = {2020-11-30},
  abstract = {We introduce a novel framework to quantify the importance of each input feature for model explainability. A user of our framework can choose between two modes: (a) global explanation: providing feature importance globally across all the data points; and (b) local explanation: providing feature importance locally for each individual data point. The core idea of our method comes from utilizing the Dirichlet distribution to define a distribution over the importance of input features. This particular distribution is useful in ranking the importance of the input features as a sample from this distribution is a probability vector (i.e., the vector components sum to 1), Thus, the ranking uncovered by our framework which provides a \textbackslash textit\{quantifiable explanation\} of how significant each input feature is to a model's output. This quantifiable explainability differentiates our method from existing feature-selection methods, which simply determine whether a feature is relevant or not. Furthermore, a distribution over the explanation allows to define a closed-form divergence to measure the similarity between learned feature importance under different models. We use this divergence to study how the feature importance trade-offs with essential notions in modern machine learning, such as privacy and fairness. We show the effectiveness of our method on a variety of synthetic and real datasets, taking into account both tabular and image datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{adebayoSanityChecksSaliency2018,
  title = {Sanity Checks for Saliency Maps},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  year = {2018},
  series = {{{NIPS}}'18},
  pages = {9525--9536},
  publisher = {{Curran Associates Inc.}},
  location = {{Montréal, Canada}},
  abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.}
}



@online{aguiarOverviewModelExplainability2019,
  title = {An Overview of Model Explainability in Modern Machine Learning},
  author = {Aguiar, Rui},
  year = {2019-12-05T01:50:27},
  __url = {https://towardsdatascience.com/an-overview-of-model-explainability-in-modern-machine-learning-fc0f22c8c29a},
  _urldate = {2020-04-20},
  abstract = {How we can understand black box machine learning models, and why it matters},
  langid = {english},
  organization = {{Medium}}
}

@unpublished{aiBeneficialHarmfulExplanatory2020,
  title = {Beneficial and {{Harmful Explanatory Machine Learning}}},
  author = {Ai, Lun and Muggleton, Stephen H. and Hocquette, Céline and Gromowski, Mark and Schmid, Ute},
  year = {2020-09-09},
  eprint = {2009.06410},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2009.06410},
  _urldate = {2020-11-30},
  abstract = {Given the recent successes of Deep Learning in AI there has been increased interest in the role and need for explanations in machine learned theories. A distinct notion in this context is that of Michie's definition of Ultra-Strong Machine Learning (USML). USML is demonstrated by a measurable increase in human performance of a task following provision to the human of a symbolic machine learned theory for task performance. A recent paper demonstrates the beneficial effect of a machine learned logic theory for a classification task, yet no existing work has examined the potential harmfulness of machine's involvement in human learning. This paper investigates the explanatory effects of a machine learned theory in the context of simple two person games and proposes a framework for identifying the harmfulness of machine explanations based on the Cognitive Science literature. The approach involves a cognitive window consisting of two quantifiable bounds and it is supported by empirical evidence collected from human trials. Our quantitative and qualitative results indicate that human learning aided by a symbolic machine learned theory which satisfies a cognitive window has achieved significantly higher performance than human self learning. Results also demonstrate that human learning aided by a symbolic machine learned theory that fails to satisfy this window leads to significantly worse performance than unaided human learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@unpublished{aivodjiFairwashingRiskRationalization2019,
  title = {Fairwashing: The Risk of Rationalization},
  shorttitle = {Fairwashing},
  author = {Aïvodji, Ulrich and Arai, Hiromi and Fortineau, Olivier and Gambs, Sébastien and Hara, Satoshi and Tapp, Alain},
  year = {2019-05-15},
  eprint = {1901.09749},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1901.09749},
  _urldate = {2020-10-19},
  abstract = {Black-box explanation is the problem of explaining how a machine learning model -- whose internal logic is hidden to the auditor and generally complex -- produces its outcomes. Current approaches for solving this problem include model explanation, outcome explanation as well as model inspection. While these techniques can be beneficial by providing interpretability, they can be used in a negative manner to perform fairwashing, which we define as promoting the false perception that a machine learning model respects some ethical values. In particular, we demonstrate that it is possible to systematically rationalize decisions taken by an unfair black-box model using the model explanation as well as the outcome explanation approaches with a given fairness metric. Our solution, LaundryML, is based on a regularized rule list enumeration algorithm whose objective is to search for fair rule lists approximating an unfair black-box model. We empirically evaluate our rationalization technique on black-box models trained on real-world datasets and show that one can obtain rule lists with high fidelity to the black-box model while being considerably less unfair at the same time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{alberINNvestigateNeuralNetworks2018,
  title = {{{iNNvestigate}} Neural Networks!},
  author = {Alber, Maximilian and Lapuschkin, Sebastian and Seegerer, Philipp and Hägele, Miriam and Schütt, Kristof T. and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert and Dähne, Sven and Kindermans, Pieter-Jan},
  year = {2018-08-13},
  eprint = {1808.04260},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1808.04260},
  _urldate = {2020-10-19},
  abstract = {In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and pre- dictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this short- coming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-the- box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{alberINNvestigateNeuralNetworks2018a,
  title = {{{iNNvestigate}} Neural Networks!},
  author = {Alber, Maximilian and Lapuschkin, Sebastian and Seegerer, Philipp and Hägele, Miriam and Schütt, Kristof T. and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert and Dähne, Sven and Kindermans, Pieter-Jan},
  year = {2018-08},
  eprint = {1808.04260},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1808.04260},
  _urldate = {2020-10-19},
  abstract = {In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and pre- dictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this short- coming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-the- box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{alvarez-melisRobustInterpretabilitySelfExplaining2018,
 author = {Alvarez Melis, David and Jaakkola, Tommi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Towards Robust Interpretability with Self-Explaining Neural Networks},
 _url = {https://proceedings.neurips.cc/paper/2018/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf},
 volume = {31},
 year = {2018}
}


@unpublished{alvarez-melisRobustnessInterpretabilityMethods2018,
  title = {On the {{Robustness}} of {{Interpretability Methods}}},
  author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
  year = {2018},
  eprint = {1806.08049},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1806.08049},
  __urldate = {2020-10-19},
  abstract = {We argue that robustness of explanations---i.e., that similar inputs should give rise to similar explanations---is a key desideratum for interpretability. We introduce metrics to quantify robustness and demonstrate that current methods do not perform well according to these metrics. Finally, we propose ways that robustness can be enforced on existing interpretability approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{alvesMakingMLModels2020,
  title = {Making {{ML}} Models Fairer through Explanations: The Case of {{LimeOut}}},
  shorttitle = {Making {{ML}} Models Fairer through Explanations},
  author = {Alves, Guilherme and Bhargava, Vaishnavi and Couceiro, Miguel and Napoli, Amedeo},
  year = {2020-11-01},
  eprint = {2011.00603},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2011.00603},
  _urldate = {2020-11-30},
  abstract = {Algorithmic decisions are now being used on a daily basis, and based on Machine Learning (ML) processes that may be complex and biased. This raises several concerns given the critical impact that biased decisions may have on individuals or on society as a whole. Not only unfair outcomes affect human rights, they also undermine public trust in ML and AI. In this paper we address fairness issues of ML models based on decision outcomes, and we show how the simple idea of "feature dropout" followed by an "ensemble approach" can improve model fairness. To illustrate, we will revisit the case of "LimeOut" that was proposed to tackle "process fairness", which measures a model's reliance on sensitive or discriminatory features. Given a classifier, a dataset and a set of sensitive features, LimeOut first assesses whether the classifier is fair by checking its reliance on sensitive features using "Lime explanations". If deemed unfair, LimeOut then applies feature dropout to obtain a pool of classifiers. These are then combined into an ensemble classifier that was empirically shown to be less dependent on sensitive features without compromising the classifier's accuracy. We present different experiments on multiple datasets and several state of the art classifiers, which show that LimeOut's classifiers improve (or at least maintain) not only process fairness but also other fairness metrics such as individual and group fairness, equal opportunity, and demographic parity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning}
}

@unpublished{amarasingheExplainableMachineLearning2020,
  title = {Explainable {{Machine Learning}} for {{Public Policy}}: {{Use Cases}}, {{Gaps}}, and {{Research Directions}}},
  shorttitle = {Explainable {{Machine Learning}} for {{Public Policy}}},
  author = {Amarasinghe, Kasun and Rodolfa, Kit and Lamba, Hemank and Ghani, Rayid},
  year = {2020-10-27},
  eprint = {2010.14374},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2010.14374},
  _urldate = {2020-11-30},
  abstract = {In Machine Learning (ML) models used for supporting decisions in high-stakes domains such as public policy, explainability is crucial for adoption and effectiveness. While the field of explainable ML has expanded in recent years, much of this work does not take real-world needs into account. A majority of proposed methods use benchmark ML problems with generic explainability goals without clear use-cases or intended end-users. As a result, the effectiveness of this large body of theoretical and methodological work on real-world applications is unclear. This paper focuses on filling this void for the domain of public policy. We develop a taxonomy of explainability use-cases within public policy problems; for each use-case, we define the end-users of explanations and the specific goals explainability has to fulfill; third, we map existing work to these use-cases, identify gaps, and propose research directions to fill those gaps in order to have practical policy impact through ML.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@unpublished{amiriDataRepresentingGroundTruth2020,
  title = {Data {{Representing Ground-Truth Explanations}} to {{Evaluate XAI Methods}}},
  author = {Amiri, Shideh Shams and Weber, Rosina O. and Goel, Prateek and Brooks, Owen and Gandley, Archer and Kitchell, Brian and Zehm, Aaron},
  year = {2020-11-18},
  eprint = {2011.09892},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2011.09892},
  _urldate = {2020-11-30},
  abstract = {Explainable artificial intelligence (XAI) methods are currently evaluated with approaches mostly originated in interpretable machine learning (IML) research that focus on understanding models such as comparison against existing attribution approaches, sensitivity analyses, gold set of features, axioms, or through demonstration of images. There are problems with these methods such as that they do not indicate where current XAI approaches fail to guide investigations towards consistent progress of the field. They do not measure accuracy in support of accountable decisions, and it is practically impossible to determine whether one XAI method is better than the other or what the weaknesses of existing models are, leaving researchers without guidance on which research questions will advance the field. Other fields usually utilize ground-truth data and create benchmarks. Data representing ground-truth explanations is not typically used in XAI or IML. One reason is that explanations are subjective, in the sense that an explanation that satisfies one user may not satisfy another. To overcome these problems, we propose to represent explanations with canonical equations that can be used to evaluate the accuracy of XAI methods. The contributions of this paper include a methodology to create synthetic data representing ground-truth explanations, three data sets, an evaluation of LIME using these data sets, and a preliminary analysis of the challenges and potential benefits in using these data to evaluate existing XAI approaches. Evaluation methods based on human-centric studies are outside the scope of this paper.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@unpublished{amiriDataRepresentingGroundTruth2020a,
  title = {Data {{Representing Ground-Truth Explanations}} to {{Evaluate XAI Methods}}},
  author = {Amiri, Shideh Shams and Weber, Rosina O. and Goel, Prateek and Brooks, Owen and Gandley, Archer and Kitchell, Brian and Zehm, Aaron},
  year = {2020-11-18},
  eprint = {2011.09892},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2011.09892},
  _urldate = {2021-09-29},
  abstract = {Explainable artificial intelligence (XAI) methods are currently evaluated with approaches mostly originated in interpretable machine learning (IML) research that focus on understanding models such as comparison against existing attribution approaches, sensitivity analyses, gold set of features, axioms, or through demonstration of images. There are problems with these methods such as that they do not indicate where current XAI approaches fail to guide investigations towards consistent progress of the field. They do not measure accuracy in support of accountable decisions, and it is practically impossible to determine whether one XAI method is better than the other or what the weaknesses of existing models are, leaving researchers without guidance on which research questions will advance the field. Other fields usually utilize ground-truth data and create benchmarks. Data representing ground-truth explanations is not typically used in XAI or IML. One reason is that explanations are subjective, in the sense that an explanation that satisfies one user may not satisfy another. To overcome these problems, we propose to represent explanations with canonical equations that can be used to evaluate the accuracy of XAI methods. The contributions of this paper include a methodology to create synthetic data representing ground-truth explanations, three data sets, an evaluation of LIME using these data sets, and a preliminary analysis of the challenges and potential benefits in using these data to evaluate existing XAI approaches. Evaluation methods based on human-centric studies are outside the scope of this paper.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{anconaUnifiedViewGradientbased2017,
  title = {A Unified View of Gradient-Based Attribution Methods for {{Deep Neural Networks}}},
  author = {Ancona, M. and Ceolini, Enea and Öztireli, A. C. and Gross, M.},
  year = {2017},
  journaltitle = {NIPS 2017},
  _doi = {10.3929/ethz-b-000237705},
  abstract = {Understanding the flow of information in Deep Neural Networks is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, only few attempts to analyze them from a theoretical perspective have been made in the past. In this work we analyze various state-of-the-art attribution methods and prove unexplored connections between them. We also show how some methods can be reformulated and more conveniently implemented. Finally, we perform an empirical evaluation with six attribution methods on a variety of tasks and architectures and discuss their strengths and limitations.}
}

@article{anconaUnifiedViewGradientbased2017a,
  title = {A Unified View of Gradient-Based Attribution Methods for {{Deep Neural Networks}}},
  author = {Ancona, M. and Ceolini, Enea and Öztireli, A. C. and Gross, M.},
  year = {2017},
  journaltitle = {NIPS 2017},
  _doi = {10.3929/ethz-b-000237705},
  abstract = {Understanding the flow of information in Deep Neural Networks is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, only few attempts to analyze them from a theoretical perspective have been made in the past. In this work we analyze various state-of-the-art attribution methods and prove unexplored connections between them. We also show how some methods can be reformulated and more conveniently implemented. Finally, we perform an empirical evaluation with six attribution methods on a variety of tasks and architectures and discuss their strengths and limitations.}
}

@unpublished{andersFairwashingExplanationsOffManifold2020,
  title = {Fairwashing {{Explanations}} with {{Off-Manifold Detergent}}},
  author = {Anders, Christopher J. and Pasliev, Plamen and Dombrowski, Ann-Kathrin and Müller, Klaus-Robert and Kessel, Pan},
  year = {2020-07-20},
  eprint = {2007.09969},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.09969},
  _urldate = {2021-11-16},
  abstract = {Explanation methods promise to make black-box classifiers more transparent. As a result, it is hoped that they can act as proof for a sensible, fair and trustworthy decision-making process of the algorithm and thereby increase its acceptance by the end-users. In this paper, we show both theoretically and experimentally that these hopes are presently unfounded. Specifically, we show that, for any classifier \$g\$, one can always construct another classifier \$\textbackslash tilde\{g\}\$ which has the same behavior on the data (same train, validation, and test error) but has arbitrarily manipulated explanation maps. We derive this statement theoretically using differential geometry and demonstrate it experimentally for various explanation methods, architectures, and datasets. Motivated by our theoretical insights, we then propose a modification of existing explanation methods which makes them significantly more robust.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/7LZETYCE/Anders et al. - 2020 - Fairwashing Explanations with Off-Manifold Deterge.pdf;/home/rick/Zotero/storage/E637EZDK/2007.html}
}

@unpublished{andersFairwashingExplanationsOffManifold2020a,
  title = {Fairwashing {{Explanations}} with {{Off-Manifold Detergent}}},
  author = {Anders, Christopher J. and Pasliev, Plamen and Dombrowski, Ann-Kathrin and Müller, Klaus-Robert and Kessel, Pan},
  year = {2020-07},
  eprint = {2007.09969},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.09969},
  _urldate = {2021-11-16},
  abstract = {Explanation methods promise to make black-box classifiers more transparent. As a result, it is hoped that they can act as proof for a sensible, fair and trustworthy decision-making process of the algorithm and thereby increase its acceptance by the end-users. In this paper, we show both theoretically and experimentally that these hopes are presently unfounded. Specifically, we show that, for any classifier \$g\$, one can always construct another classifier \$\textbackslash textbackslashtilde\{g\}\$ which has the same behavior on the data (same train, validation, and test error) but has arbitrarily manipulated explanation maps. We derive this statement theoretically using differential geometry and demonstrate it experimentally for various explanation methods, architectures, and datasets. Motivated by our theoretical insights, we then propose a modification of existing explanation methods which makes them significantly more robust.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/4DMYJ3W7/Anders et al. - 2020 - Fairwashing Explanations with Off-Manifold Deterge.pdf;/home/rick/Zotero/storage/FQN3J4XY/2007.html}
}

@article{apleyVisualizingEffectsPredictor2019,
  author={Daniel W. Apley and Jingyu Zhu},
  title={{Visualizing the effects of predictor variables in black box supervised learning models}},
  journal={Journal of the Royal Statistical Society Series B},
  year=2020,
  volume={82},
  number={4},
  pages={1059-1086},
  month={September},
  keywords={},
  _doi={10.1111/rssb.12377},
  abstract={In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel‐weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method.},
  __url={https://ideas.repec.org/a/bla/jorssb/v82y2020i4p1059-1086.html}
}

@article{archerEmpiricalCharacterizationRandom2008,
  title = {Empirical Characterization of Random Forest Variable Importance Measures},
  author = {Archer, Kellie J. and Kimes, Ryan V.},
  year = {2008-01-10},
  journaltitle = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  volume = {52},
  number = {4},
  pages = {2249--2260},
  __issn = {0167-9473},
  _doi = {10.1016/j.csda.2007.08.015},
  __url = {http://www.sciencedirect.com/science/article/pii/S0167947307003076},
  _urldate = {2020-10-07},
  abstract = {Microarray studies yield data sets consisting of a large number of candidate predictors (genes) on a small number of observations (samples). When interest lies in predicting phenotypic class using gene expression data, often the goals are both to produce an accurate classifier and to uncover the predictive structure of the problem. Most machine learning methods, such as k-nearest neighbors, support vector machines, and neural networks, are useful for classification. However, these methods provide no insight regarding the covariates that best contribute to the predictive structure. Other methods, such as linear discriminant analysis, require the predictor space be substantially reduced prior to deriving the classifier. A recently developed method, random forests (RF), does not require reduction of the predictor space prior to classification. Additionally, RF yield variable importance measures for each candidate predictor. This study examined the effectiveness of RF variable importance measures in identifying the true predictor among a large number of candidate predictors. An extensive simulation study was conducted using 20 levels of correlation among the predictor variables and 7 levels of association between the true predictor and the dichotomous response. We conclude that the RF methodology is attractive for use in classification problems when the goals of the study are to produce an accurate classifier and to provide insight regarding the discriminative ability of individual predictor variables. Such goals are common among microarray studies, and therefore application of the RF methodology for the purpose of obtaining variable importance measures is demonstrated on a microarray data set.},
  langid = {english},
  keywords = {Bootstrap aggregating,Classification tree,Random forest,Variable importance}
}

@unpublished{ardizzoneAnalyzingInverseProblems2019,
  title = {Analyzing {{Inverse Problems}} with {{Invertible Neural Networks}}},
  author = {Ardizzone, Lynton and Kruse, Jakob and Wirkert, Sebastian and Rahner, Daniel and Pellegrini, Eric W. and Klessen, Ralf S. and Maier-Hein, Lena and Rother, Carsten and Köthe, Ullrich},
  year = {2019-02-06},
  eprint = {1808.04730},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1808.04730},
  _urldate = {2021-09-07},
  abstract = {In many tasks, in particular in natural science, the goal is to determine hidden system parameters from a set of measurements. Often, the forward process from parameter- to measurement-space is a well-defined function, whereas the inverse problem is ambiguous: one measurement may map to multiple different sets of parameters. In this setting, the posterior parameter distribution, conditioned on an input measurement, has to be determined. We argue that a particular class of neural networks is well suited for this task -- so-called Invertible Neural Networks (INNs). Although INNs are not new, they have, so far, received little attention in literature. While classical neural networks attempt to solve the ambiguous inverse problem directly, INNs are able to learn it jointly with the well-defined forward process, using additional latent output variables to capture the information otherwise lost. Given a specific measurement and sampled latent variables, the inverse pass of the INN provides a full distribution over parameter space. We verify experimentally, on artificial data and real-world problems from astrophysics and medicine, that INNs are a powerful analysis tool to find multi-modalities in parameter space, to uncover parameter correlations, and to identify unrecoverable parameters.},
  archiveprefix = {arXiv},
  keywords = {68T01,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{arjovskyInvariantRiskMinimization2020,
  title = {Invariant {{Risk Minimization}}},
  author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
  year = {2020-03-27},
  eprint = {1907.02893},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1907.02893},
  _urldate = {2021-05-12},
  abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/C6RT5ZB3/Arjovsky et al. - 2020 - Invariant Risk Minimization.pdf;/home/rick/Zotero/storage/6J37M2BM/1907.html}
}

@article{arrasWhatRelevantText2017,
  title = {"{{What}} Is Relevant in a Text Document?": {{An}} Interpretable Machine Learning Approach},
  shorttitle = {"{{What}} Is Relevant in a Text Document?},
  author = {Arras, Leila and Horn, Franziska and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
  year = {2017-08-11},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {12},
  number = {8},
  pages = {e0181142},
  __issn = {1932-6203},
  _doi = {10.1371/journal.pone.0181142},
  __url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181142},
  _urldate = {2020-10-19},
  abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text’s category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
  langid = {english},
  keywords = {Machine learning,Neural networks,Neurons,Recurrent neural networks,Semantics,Support vector machines,Vector spaces,Word embedding}
}

@article{arrasWhatRelevantText2017a,
  title = {"{{What}} Is Relevant in a Text Document?": {{An}} Interpretable Machine Learning Approach},
  shorttitle = {"{{What}} Is Relevant in a Text Document?},
  author = {Arras, Leila and Horn, Franziska and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
  year = {2017-08-11},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {12},
  number = {8},
  pages = {e0181142},
  publisher = {{Public Library of Science}},
  __issn = {1932-6203},
  _doi = {10.1371/journal.pone.0181142},
  __url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181142},
  _urldate = {2021-05-12},
  abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text’s category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
  langid = {english},
  keywords = {Machine learning,Neural networks,Neurons,Recurrent neural networks,Semantics,Support vector machines,Vector spaces,Word embedding},
  file = {/home/rick/Zotero/storage/VWFHENDD/Arras et al. - 2017 - What is relevant in a text document An interpr.pdf;/home/rick/Zotero/storage/KJN9V4ZR/article.html}
}

@article{arrasWhatRelevantText2017b,
  title = {" {{What}} Is Relevant in a Text Document?": {{An}} Interpretable Machine Learning Approach},
  author = {Arras, Leila and Horn, Franziska and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
  year = {2017},
  journaltitle = {PloS one},
  volume = {12},
  number = {8},
  pages = {e0181142},
  publisher = {{Public Library of Science San Francisco, CA USA}}
}

@article{arrasWhatRelevantText2017c,
  title = {" {{What}} Is Relevant in a Text Document?": {{An}} Interpretable Machine Learning Approach},
  author = {Arras, Leila and Horn, Franziska and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
  year = {2017},
  journaltitle = {PloS one},
  volume = {12},
  number = {8},
  pages = {e0181142},
  publisher = {{Public Library of Science San Francisco, CA USA}}
}

@article{arrasWhatRelevantText2017d,
  title = {"{{What}} Is Relevant in a Text Document?": {{An}} Interpretable Machine Learning Approach},
  shorttitle = {"{{What}} Is Relevant in a Text Document?},
  author = {Arras, Leila and Horn, Franziska and Montavon, Grégoire and Müller, Klaus-Robert and Samek, Wojciech},
  year = {2017-08},
  journaltitle = {PLOS ONE},
  volume = {12},
  number = {8},
  pages = {e0181142},
  __issn = {1932-6203},
  _doi = {10.1371/journal.pone.0181142},
  __url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0181142},
  _urldate = {2021-05-12},
  abstract = {Text documents can be described by a number of abstract concepts such as semantic category, writing style, or sentiment. Machine learning (ML) models have been trained to automatically map documents to these abstract concepts, allowing to annotate very large text collections, more than could be processed by a human in a lifetime. Besides predicting the text’s category very accurately, it is also highly desirable to understand how and why the categorization process takes place. In this paper, we demonstrate that such understanding can be achieved by tracing the classification decision back to individual words using layer-wise relevance propagation (LRP), a recently developed technique for explaining predictions of complex non-linear classifiers. We train two word-based ML models, a convolutional neural network (CNN) and a bag-of-words SVM classifier, on a topic categorization task and adapt the LRP method to decompose the predictions of these models onto words. Resulting scores indicate how much individual words contribute to the overall classification decision. This enables one to distill relevant information from text documents without an explicit semantic information extraction step. We further use the word-wise relevance scores for generating novel vector-based document representations which capture semantic information. Based on these document vectors, we introduce a measure of model explanatory power and show that, although the SVM and CNN models perform similarly in terms of classification accuracy, the latter exhibits a higher level of explainability which makes it more comprehensible for humans and potentially more useful for other applications.},
  langid = {english},
  keywords = {Machine learning,Neural networks,Neurons,Recurrent neural networks,Semantics,Support vector machines,Vector spaces,Word embedding},
  file = {/home/rick/Zotero/storage/KZU6DN8M/Arras et al. - 2017 - What is relevant in a text document An interpr.pdf;/home/rick/Zotero/storage/8ADCA5EF/article.html}
}

@article{arrietaExplainableArtificialIntelligence2019,
title = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
journal = {Information Fusion},
volume = {58},
pages = {82-115},
year = {2020},
author = {Alejandro {Arrieta} and Natalia Díaz-Rodríguez and Javier {Del Ser} and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
}

@inproceedings{agarwal2022openxai,
title={Open{XAI}: Towards a Transparent Evaluation of Model Explanations},
author={Chirag Agarwal and Satyapriya Krishna and Eshika Saxena and Martin Pawelczyk and Nari Johnson and Isha Puri and Marinka Zitnik and Himabindu Lakkaraju},
booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2022},
}

@unpublished{aryaOneExplanationDoes2019,
  title = {One {{Explanation Does Not Fit All}}: {{A Toolkit}} and {{Taxonomy}} of {{AI Explainability Techniques}}},
  shorttitle = {One {{Explanation Does Not Fit All}}},
  author = {Arya, Vijay and Bellamy, Rachel K. E. and Chen, Pin-Yu and Dhurandhar, Amit and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Liao, Q. Vera and Luss, Ronny and Mojsilović, Aleksandra and Mourad, Sami and Pedemonte, Pablo and Raghavendra, Ramya and Richards, John and Sattigeri, Prasanna and Shanmugam, Karthikeyan and Singh, Moninder and Varshney, Kush R. and Wei, Dennis and Zhang, Yunfeng},
  year = {2019-09-14},
  eprint = {1909.03012},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1909.03012},
  _urldate = {2020-10-26},
  abstract = {As artificial intelligence and machine learning algorithms make further inroads into society, calls are increasing from multiple stakeholders for these algorithms to explain their outputs. At the same time, these stakeholders, whether they be affected citizens, government regulators, domain experts, or system developers, present different requirements for explanations. Toward addressing these needs, we introduce AI Explainability 360 (http://aix360.mybluemix.net/), an open-source software toolkit featuring eight diverse and state-of-the-art explainability methods and two evaluation metrics. Equally important, we provide a taxonomy to help entities requiring explanations to navigate the space of explanation methods, not only those in the toolkit but also in the broader literature on explainability. For data scientists and other users of the toolkit, we have implemented an extensible software architecture that organizes methods according to their place in the AI modeling pipeline. We also discuss enhancements to bring research innovations closer to consumers of explanations, ranging from simplified, more accessible versions of algorithms, to tutorials and an interactive web demo to introduce AI explainability to different audiences and application domains. Together, our toolkit and taxonomy can help identify gaps where more explainability methods are needed and provide a platform to incorporate them as they are developed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Statistics - Machine Learning}
}

@unpublished{atanasovaDiagnosticsGuidedExplanationGeneration2021,
  title = {Diagnostics-{{Guided Explanation Generation}}},
  author = {Atanasova, Pepa and Simonsen, Jakob Grue and Lioma, Christina and Augenstein, Isabelle},
  year = {2021-09-08},
  eprint = {2109.03756},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2109.03756},
  _urldate = {2021-09-17},
  abstract = {Explanations shed light on a machine learning model's rationales and can aid in identifying deficiencies in its reasoning process. Explanation generation models are typically trained in a supervised way given human explanations. When such annotations are not available, explanations are often selected as those portions of the input that maximise a downstream task's performance, which corresponds to optimising an explanation's Faithfulness to a given model. Faithfulness is one of several so-called diagnostic properties, which prior work has identified as useful for gauging the quality of an explanation without requiring annotations. Other diagnostic properties are Data Consistency, which measures how similar explanations are for similar input instances, and Confidence Indication, which shows whether the explanation reflects the confidence of the model. In this work, we show how to directly optimise for these diagnostic properties when training a model to generate sentence-level explanations, which markedly improves explanation quality, agreement with human rationales, and downstream task performance on three complex reasoning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,I.2.7}
}

@unpublished{atanasovaDiagnosticStudyExplainability2020,
  title = {A {{Diagnostic Study}} of {{Explainability Techniques}} for {{Text Classification}}},
  author = {Atanasova, Pepa and Simonsen, Jakob Grue and Lioma, Christina and Augenstein, Isabelle},
  year = {2020-09-25},
  eprint = {2009.13295},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2009.13295},
  _urldate = {2020-11-30},
  abstract = {Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model's performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,cs.CL; cs.AI,I.2.7}
}

@unpublished{atesCounterfactualExplanationsMachine2020,
  title = {Counterfactual {{Explanations}} for {{Machine Learning}} on {{Multivariate Time Series Data}}},
  author = {Ates, Emre and Aksar, Burak and Leung, Vitus J. and Coskun, Ayse K.},
  year = {2020-08-24},
  eprint = {2008.10781},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2008.10781},
  _urldate = {2020-09-16},
  abstract = {Applying machine learning (ML) on multivariate time series data has growing popularity in many application domains, including in computer system management. For example, recent high performance computing (HPC) research proposes a variety of ML frameworks that use system telemetry data in the form of multivariate time series so as to detect performance variations, perform intelligent scheduling or node allocation, and improve system security. Common barriers for adoption for these ML frameworks include the lack of user trust and the difficulty of debugging. These barriers need to be overcome to enable the widespread adoption of ML frameworks in production systems. To address this challenge, this paper proposes a novel explainability technique for providing counterfactual explanations for supervised ML frameworks that use multivariate time series data. The proposed method outperforms state-of-the-art explainability methods on several different ML frameworks and data sets in metrics such as faithfulness and robustness. The paper also demonstrates how the proposed method can be used to debug ML frameworks and gain a better understanding of HPC system telemetry data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{bachPixelWiseExplanationsNonLinear2015,
    _doi = {10.1371/journal.pone.0130140},
    author = {Bach, Sebastian AND Binder, Alexander AND Montavon, Grégoire AND Klauschen, Frederick AND Müller, Klaus-Robert AND Samek, Wojciech},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
    year = {2015},
    month = {07},
    volume = {10},
    __url = {https://doi.org/10.1371/journal.pone.0130140},
    pages = {1-46},
    abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
    number = {7},

}

@article{baehrensHowExplainIndividual2010,
    author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and M\"{u}ller, Klaus-Robert},
    title = {How to Explain Individual Classification Decisions},
    year = {2010},
    issue_date = {3/1/2010},
    publisher = {JMLR.org},
    volume = {11},
    __issn = {1532-4435},
    abstract = {After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.},
    journal = {J. Mach. Learn. Res.},
    month = {aug},
    pages = {1803–1831},
    numpages = {29}
}

@unpublished{banegas-lunaWhenWillMist2020,
  title = {When Will the Mist Clear? {{On}} the {{Interpretability}} of {{Machine Learning}} for {{Medical Applications}}: A Survey},
  shorttitle = {When Will the Mist Clear?},
  author = {Banegas-Luna, Antonio-Jesús and Peña-García, Jorge and Iftene, Adrian and Guadagni, Fiorella and Ferroni, Patrizia and Scarpato, Noemi and Zanzotto, Fabio Massimo and Bueno-Crespo, Andrés and Pérez-Sánchez, Horacio},
  year = {2020-10-01},
  eprint = {2010.00353},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  __url = {http://arxiv.org/abs/2010.00353},
  _urldate = {2020-11-30},
  abstract = {Artificial Intelligence is providing astonishing results, with medicine being one of its favourite playgrounds. In a few decades, computers may be capable of formulating diagnoses and choosing the correct treatment, while robots may perform surgical operations, and conversational agents could interact with patients as virtual coaches. Machine Learning and, in particular, Deep Neural Networks are behind this revolution. In this scenario, important decisions will be controlled by standalone machines that have learned predictive models from provided data. Among the most challenging targets of interest in medicine are cancer diagnosis and therapies but, to start this revolution, software tools need to be adapted to cover the new requirements. In this sense, learning tools are becoming a commodity in Python and Matlab libraries, just to name two, but to exploit all their possibilities, it is essential to fully understand how models are interpreted and which models are more interpretable than others. In this survey, we analyse current machine learning models, frameworks, databases and other related tools as applied to medicine - specifically, to cancer research - and we discuss their interpretability, performance and the necessary input data. From the evidence available, ANN, LR and SVM have been observed to be the preferred models. Besides, CNNs, supported by the rapid development of GPUs and tensor-oriented programming libraries, are gaining in importance. However, the interpretability of results by doctors is rarely considered which is a factor that needs to be improved. We therefore consider this study to be a timely contribution to the issue.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods}
}

@article{barachantMulticlassBrainComputer2012,
  title = {Multiclass {{Brain}}–{{Computer Interface Classification}} by {{Riemannian Geometry}}},
  author = {Barachant, A. and Bonnet, S. and Congedo, M. and Jutten, C.},
  year = {2012-04},
  journaltitle = {IEEE Transactions on Biomedical Engineering},
  volume = {59},
  number = {4},
  pages = {920--928},
  _issn = {1558-2531},
  _doi = {10.1109/TBME.2011.2172210},
  abstract = {This paper presents a new classification framework for brain-computer interface (BCI) based on motor imagery. This framework involves the concept of Riemannian geometry in the manifold of covariance matrices. The main idea is to use spatial covariance matrices as EEG signal descriptors and to rely on Riemannian geometry to directly classify these matrices using the topology of the manifold of symmetric and positive definite (SPD) matrices. This framework allows to extract the spatial information contained in EEG signals without using spatial filtering. Two methods are proposed and compared with a reference method [multiclass Common Spatial Pattern (CSP) and Linear Discriminant Analysis (LDA)] on the multiclass dataset IIa from the BCI Competition IV. The first method, named minimum distance to Riemannian mean (MDRM), is an implementation of the minimum distance to mean (MDM) classification algorithm using Riemannian distance and Riemannian mean. This simple method shows comparable results with the reference method. The second method, named tangent space LDA (TSLDA), maps the covariance matrices onto the Riemannian tangent space where matrices can be vectorized and treated as Euclidean objects. Then, a variable selection procedure is applied in order to decrease dimensionality and a classification by LDA is performed. This latter method outperforms the reference method increasing the mean classification accuracy from 65.1\% to 70.2\%.},
  keywords = {Algorithms,Brain computer interfaces,brain-computer interface,brain-computer interfaces,classification algorithms,classification framework,covariance matrix,Covariance matrix,covariance matrix manifold,CSP,EEG signal descriptors,electroencephalography,Electroencephalography,Evoked Potentials; Motor,Geometry,Humans,Imagination,information geometry,linear discriminant analysis,Manifolds,matrix algebra,Matrix decomposition,MDM classification algorithm,MDRM classification algorithm,medical signal processing,minimum distance to mean classification,minimum distance to Riemannian mean classification,Motor Cortex,motor imagery,Movement,multiclass BCI classification,multiclass common spatial pattern,Pattern Recognition; Automated,positive definite matrix manifold topology,Reproducibility of Results,Riemannian distance,Riemannian geometry,Sensitivity and Specificity,signal classification,Silicon,spatial covariance matrices,spatial information,statistical analysis,Symmetric matrices,symmetric matrix manifold topology,tangent space LDA,TSLDA,User-Computer Interface},
  file = {/home/rick/Zotero/storage/K4RMM8F9/Barachant et al. - 2012 - Multiclass Brain–Computer Interface Classification.pdf}
}

@unpublished{barceloModelInterpretabilityLens2020,
  title = {Model {{Interpretability}} through the {{Lens}} of {{Computational Complexity}}},
  author = {Barceló, Pablo and Monet, Mikaël and Pérez, Jorge and Subercaseaux, Bernardo},
  year = {2020-11-12},
  eprint = {2010.12265},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2010.12265},
  _urldate = {2020-11-30},
  abstract = {In spite of several claims stating that some models are more interpretable than others -- e.g., "linear models are more interpretable than deep neural networks" -- we still lack a principled notion of interpretability to formally compare among different classes of models. We make a step towards such a notion by studying whether folklore interpretability claims have a correlate in terms of computational complexity theory. We focus on local post-hoc explainability queries that, intuitively, attempt to answer why individual inputs are classified in a certain way by a given model. In a nutshell, we say that a class \$\textbackslash mathcal\{C\}\_1\$ of models is more interpretable than another class \$\textbackslash mathcal\{C\}\_2\$, if the computational complexity of answering post-hoc queries for models in \$\textbackslash mathcal\{C\}\_2\$ is higher than for those in \$\textbackslash mathcal\{C\}\_1\$. We prove that this notion provides a good theoretical counterpart to current beliefs on the interpretability of models; in particular, we show that under our definition and assuming standard complexity-theoretical assumptions (such as P\$\textbackslash neq\$NP), both linear and tree-based models are strictly more interpretable than neural networks. Our complexity analysis, however, does not provide a clear-cut difference between linear and tree-based models, as we obtain different results depending on the particular post-hoc explanations considered. Finally, by applying a finer complexity analysis based on parameterized complexity, we are able to prove a theoretical result suggesting that shallow neural networks are more interpretable than deeper ones.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Complexity,Computer Science - Machine Learning}
}

@unpublished{barrGroundTruthExplainability2020,
  title = {Towards {{Ground Truth Explainability}} on {{Tabular Data}}},
  author = {Barr, Brian and Xu, Ke and Silva, Claudio and Bertini, Enrico and Reilly, Robert and Bruss, C. Bayan and Wittenbach, Jason D.},
  year = {2020-07-20},
  eprint = {2007.10532},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.10532},
  _urldate = {2020-08-05},
  abstract = {In data science, there is a long history of using synthetic data for method development, feature selection and feature engineering. Our current interest in synthetic data comes from recent work in explainability. Today's datasets are typically larger and more complex - requiring less interpretable models. In the setting of \textbackslash textit\{post hoc\} explainability, there is no ground truth for explanations. Inspired by recent work in explaining image classifiers that does provide ground truth, we propose a similar solution for tabular data. Using copulas, a concise specification of the desired statistical properties of a dataset, users can build intuition around explainability using controlled data sets and experimentation. The current capabilities are demonstrated on three use cases: one dimensional logistic regression, impact of correlation from informative features, impact of correlation from redundant variables.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{barrGroundTruthExplainability2020a,
  title = {Towards {{Ground Truth Explainability}} on {{Tabular Data}}},
  author = {Barr, Brian and Xu, Ke and Silva, Claudio and Bertini, Enrico and Reilly, Robert and Bruss, C. Bayan and Wittenbach, Jason D.},
  year = {2020-07-20},
  eprint = {2007.10532},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.10532},
  _urldate = {2021-09-29},
  abstract = {In data science, there is a long history of using synthetic data for method development, feature selection and feature engineering. Our current interest in synthetic data comes from recent work in explainability. Today's datasets are typically larger and more complex - requiring less interpretable models. In the setting of \textbackslash textit\{post hoc\} explainability, there is no ground truth for explanations. Inspired by recent work in explaining image classifiers that does provide ground truth, we propose a similar solution for tabular data. Using copulas, a concise specification of the desired statistical properties of a dataset, users can build intuition around explainability using controlled data sets and experimentation. The current capabilities are demonstrated on three use cases: one dimensional logistic regression, impact of correlation from informative features, impact of correlation from redundant variables.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{barrGroundTruthExplainability2020b,
  title = {Towards {{Ground Truth Explainability}} on {{Tabular Data}}},
  author = {Barr, Brian and Xu, Ke and Silva, Claudio and Bertini, Enrico and Reilly, Robert and Bruss, C. Bayan and Wittenbach, Jason D.},
  year = {2020-07-20},
  eprint = {2007.10532},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.10532},
  _urldate = {2021-11-02},
  abstract = {In data science, there is a long history of using synthetic data for method development, feature selection and feature engineering. Our current interest in synthetic data comes from recent work in explainability. Today's datasets are typically larger and more complex - requiring less interpretable models. In the setting of \textbackslash textit\{post hoc\} explainability, there is no ground truth for explanations. Inspired by recent work in explaining image classifiers that does provide ground truth, we propose a similar solution for tabular data. Using copulas, a concise specification of the desired statistical properties of a dataset, users can build intuition around explainability using controlled data sets and experimentation. The current capabilities are demonstrated on three use cases: one dimensional logistic regression, impact of correlation from informative features, impact of correlation from redundant variables.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/QE75DUEM/Barr et al. - 2020 - Towards Ground Truth Explainability on Tabular Dat.pdf;/home/rick/Zotero/storage/35RWIXH3/2007.html}
}

@unpublished{begleyExplainabilityFairMachine2020,
  title = {Explainability for Fair Machine Learning},
  author = {Begley, Tom and Schwedes, Tobias and Frye, Christopher and Feige, Ilya},
  year = {2020-10-14},
  eprint = {2010.07389},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2010.07389},
  _urldate = {2020-11-30},
  abstract = {As the decisions made or influenced by machine learning models increasingly impact our lives, it is crucial to detect, understand, and mitigate unfairness. But even simply determining what "unfairness" should mean in a given context is non-trivial: there are many competing definitions, and choosing between them often requires a deep understanding of the underlying task. It is thus tempting to use model explainability to gain insights into model fairness, however existing explainability tools do not reliably indicate whether a model is indeed fair. In this work we present a new approach to explaining fairness in machine learning, based on the Shapley value paradigm. Our fairness explanations attribute a model's overall unfairness to individual input features, even in cases where the model does not operate on sensitive attributes directly. Moreover, motivated by the linearity of Shapley explainability, we propose a meta algorithm for applying existing training-time fairness interventions, wherein one trains a perturbation to the original model, rather than a new model entirely. By explaining the original model, the perturbation, and the fair-corrected model, we gain insight into the accuracy-fairness trade-off that is being made by the intervention. We further show that this meta algorithm enjoys both flexibility and stability benefits with no loss in performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{bellePrinciplesPracticeExplainable2020,
  title = {Principles and {{Practice}} of {{Explainable Machine Learning}}},
  author = {Belle, Vaishak and Papantonis, Ioannis},
  year = {2020-09-18},
  eprint = {2009.11698},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2009.11698},
  _urldate = {2020-11-30},
  abstract = {Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with significant challenges: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods -- machine learning (ML) and pattern recognition models in particular -- so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature, or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{benchekrounNeedStandardizedExplainability2020,
  title = {The {{Need}} for {{Standardized Explainability}}},
  author = {Benchekroun, Othman and Rahimi, Adel and Zhang, Qini and Kodliuk, Tetiana},
  year = {2020-10-22},
  eprint = {2010.11273},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2010.11273},
  _urldate = {2020-11-30},
  abstract = {Explainable AI (XAI) is paramount in industry-grade AI; however existing methods fail to address this necessity, in part due to a lack of standardisation of explainability methods. The purpose of this paper is to offer a perspective on the current state of the area of explainability, and to provide novel definitions for Explainability and Interpretability to begin standardising this area of research. To do so, we provide an overview of the literature on explainability, and of the existing methods that are already implemented. Finally, we offer a tentative taxonomy of the different explainability methods, opening the door to future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@unpublished{benchekrounNeedStandardizedExplainability2020a,
  title = {The {{Need}} for {{Standardized Explainability}}},
  author = {Benchekroun, Othman and Rahimi, Adel and Zhang, Qini and Kodliuk, Tetiana},
  year = {2020-10-22},
  eprint = {2010.11273},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2010.11273},
  _urldate = {2021-05-12},
  abstract = {Explainable AI (XAI) is paramount in industry-grade AI; however existing methods fail to address this necessity, in part due to a lack of standardisation of explainability methods. The purpose of this paper is to offer a perspective on the current state of the area of explainability, and to provide novel definitions for Explainability and Interpretability to begin standardising this area of research. To do so, we provide an overview of the literature on explainability, and of the existing methods that are already implemented. Finally, we offer a tentative taxonomy of the different explainability methods, opening the door to future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/rick/Zotero/storage/AWEIZD47/Benchekroun et al. - 2020 - The Need for Standardized Explainability.pdf;/home/rick/Zotero/storage/XVF7TXJW/2010.html}
}

@unpublished{bertsimasPriceInterpretability2019,
  title = {The {{Price}} of {{Interpretability}}},
  author = {Bertsimas, Dimitris and Delarue, Arthur and Jaillet, Patrick and Martin, Sebastien},
  year = {2019-07-08},
  eprint = {1907.03419},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1907.03419},
  _urldate = {2020-10-26},
  abstract = {When quantitative models are used to support decision-making on complex and important topics, understanding a model's ``reasoning'' can increase trust in its predictions, expose hidden biases, or reduce vulnerability to adversarial attacks. However, the concept of interpretability remains loosely defined and application-specific. In this paper, we introduce a mathematical framework in which machine learning models are constructed in a sequence of interpretable steps. We show that for a variety of models, a natural choice of interpretable steps recovers standard interpretability proxies (e.g., sparsity in linear models). We then generalize these proxies to yield a parametrized family of consistent measures of model interpretability. This formal definition allows us to quantify the ``price'' of interpretability, i.e., the tradeoff with predictive accuracy. We demonstrate practical algorithms to apply our framework on real and synthetic datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{bhattExplainableMachineLearning2020,
  title = {Explainable {{Machine Learning}} in {{Deployment}}},
  author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, José M. F. and Eckersley, Peter},
  year = {2020-07-10},
  eprint = {1909.06342},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1909.06342},
  _urldate = {2020-10-13},
  abstract = {Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{bhattExplainableMachineLearning2020a,
  title = {Explainable {{Machine Learning}} in {{Deployment}}},
  author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, José M. F. and Eckersley, Peter},
  year = {2020-07-10},
  eprint = {1909.06342},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1909.06342},
  _urldate = {2020-10-13},
  abstract = {Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{bibalImpactLegalRequirements2020,
  title = {Impact of {{Legal Requirements}} on {{Explainability}} in {{Machine Learning}}},
  author = {Bibal, Adrien and Lognoul, Michael and de Streel, Alexandre and Frénay, Benoît},
  options = {useprefix=true},
  year = {2020-07-10},
  eprint = {2007.05479},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2007.05479},
  _urldate = {2020-08-05},
  abstract = {The requirements on explainability imposed by European laws and their implications for machine learning (ML) models are not always clear. In that perspective, our research analyzes explanation obligations imposed for private and public decision-making, and how they can be implemented by machine learning techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning}
}

@article{bickel2009discriminative,
  title = {Discriminative Learning under Covariate Shift.},
  author = {Bickel, Steffen and Brückner, Michael and Scheffer, Tobias},
  year = {2009},
  journaltitle = {Journal of Machine Learning Research},
  volume = {10},
  number = {9}
}

@inproceedings{binderLayerWiseRelevancePropagation2016,
  title = {Layer-{{Wise Relevance Propagation}} for {{Deep Neural Network Architectures}}},
  booktitle = {Information {{Science}} and {{Applications}} ({{ICISA}}) 2016},
  author = {Binder, Alexander and Bach, Sebastian and Montavon, Gregoire and Müller, Klaus-Robert and Samek, Wojciech},
  editor = {Kim, Kuinam J. and Joukov, Nikolai},
  year = {2016},
  series = {Lecture {{Notes}} in {{Electrical Engineering}}},
  pages = {913--922},
  publisher = {{Springer}},
  location = {{Singapore}},
  _doi = {10.1007/978-981-10-0557-2_87},
  abstract = {We present the application of layer-wise relevance propagation to several deep neural networks such as the BVLC reference neural net and googlenet trained on ImageNet and MIT Places datasets. Layer-wise relevance propagation is a method to compute scores for image pixels and image regions denoting the impact of the particular image region on the prediction of the classifier for one particular test image. We demonstrate the impact of different parameter settings on the resulting explanation.},
  _isbn = {978-981-10-0557-2},
  langid = {english},
  keywords = {Deep neural networks,Non-linear explanations}
}


@online{biranExplanationJustificationMachine2017,
  title = {Explanation and {{Justification}} in {{Machine Learning}} : {{A Survey Or}}},
  shorttitle = {Explanation and {{Justification}} in {{Machine Learning}}},
  author = {Biran, Or and Cotton, Courtenay V.},
  year = {2017},
  abstract = {We present a survey of the research concerning explanation and justification in the Machine Learning literature and several adjacent fields. Within Machine Learning, we differentiate between two main branches of current research: interpretable models, and prediction interpretation and justification.},
  langid = {english}
}

@unpublished{bodriaBenchmarkingSurveyExplanation2021,
  title = {Benchmarking and {{Survey}} of {{Explanation Methods}} for {{Black Box Models}}},
  author = {Bodria, Francesco and Giannotti, Fosca and Guidotti, Riccardo and Naretto, Francesca and Pedreschi, Dino and Rinzivillo, Salvatore},
  year = {2021-02-25},
  eprint = {2102.13076},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2102.13076},
  _urldate = {2021-05-12},
  abstract = {The widespread adoption of black-box models in Artificial Intelligence has enhanced the need for explanation methods to reveal how these obscure models reach specific decisions. Retrieving explanations is fundamental to unveil possible biases and to resolve practical or ethical issues. Nowadays, the literature is full of methods with different explanations. We provide a categorization of explanation methods based on the type of explanation returned. We present the most recent and widely used explainers, and we show a visual comparison among explanations and a quantitative benchmarking.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/home/rick/Zotero/storage/8QHTIE7J/Bodria et al. - 2021 - Benchmarking and Survey of Explanation Methods for.pdf;/home/rick/Zotero/storage/DWVV2E2T/2102.html}
}

@article{bogdanovicIndepthInsightsAlzheimer2022,
  title = {In-Depth Insights into {{Alzheimer}}’s Disease by Using Explainable Machine Learning Approach},
  author = {Bogdanovic, Bojan and Eftimov, Tome and Simjanoska, Monika},
  year = {2022-12},
  journaltitle = {Scientific Reports},
  journal = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {12},
  number = {1},
  pages = {6508},
  _issn = {2045-2322},
  _doi = {10.1038/s41598-022-10202-2},
  __url = {https://www.nature.com/articles/s41598-022-10202-2},
  _urldate = {2022-04-22},
  abstract = {Abstract             Alzheimer’s disease is still a field of research with lots of open questions. The complexity of the disease prevents the early diagnosis before visible symptoms regarding the individual’s cognitive capabilities occur. This research presents an in-depth analysis of a huge data set encompassing medical, cognitive and lifestyle’s measurements from more than 12,000 individuals. Several hypothesis were established whose validity has been questioned considering the obtained results. The importance of appropriate experimental design is highly stressed in the research. Thus, a sequence of methods for handling missing data, redundancy, data imbalance, and correlation analysis have been applied for appropriate preprocessing of the data set, and consequently XGBoost model has been trained and evaluated with special attention to the hyperparameters tuning. The model was explained by using the Shapley values produced by the SHAP method. XGBoost produced a f1-score of 0.84 and as such is considered to be highly competitive among those published in the literature. This achievement, however, was not the main contribution of this paper. This research’s goal was to perform global and local interpretability of the intelligent model and derive valuable conclusions over the established hypothesis. Those methods led to a single scheme which presents either positive, or, negative influence of the values of each of the features whose importance has been confirmed by means of Shapley values. This scheme might be considered as additional source of knowledge for the physicians and other experts whose concern is the exact diagnosis of early stage of Alzheimer’s disease. The conclusions derived from the intelligent model’s data-driven interpretability confronted all the established hypotheses. This research clearly showed the importance of explainable Machine learning approach that opens the black box and clearly unveils the relationships among the features and the diagnoses.},
  langid = {english}
}

@article{bohanecDecisionmakingFrameworkDoubleloop2017,
  title = {Decision-Making Framework with Double-Loop Learning through Interpretable Black-Box Machine Learning Models},
  author = {Bohanec, Marko and Robnik-Šikonja, Marko and Kljajić Borštnar, Mirjana},
  year = {2017-01-01},
  journaltitle = {Industrial Management \& Data Systems},
  volume = {117},
  number = {7},
  pages = {1389--1406},
  _issn = {0263-5577},
  _doi = {10.1108/IMDS-09-2016-0409},
  __url = {https://doi.org/10.1108/IMDS-09-2016-0409},
  _urldate = {2021-08-13},
  abstract = {Purpose The purpose of this paper is to address the problem of weak acceptance of machine learning (ML) models in business. The proposed framework of top-performing ML models coupled with general explanation methods provides additional information to the decision-making process. This builds a foundation for sustainable organizational learning. Design/methodology/approach To address user acceptance, participatory approach of action design research (ADR) was chosen. The proposed framework is demonstrated on a B2B sales forecasting process in an organizational setting, following cross-industry standard process for data mining (CRISP-DM) methodology. Findings The provided ML model explanations efficiently support business decision makers, reduce forecasting error for new sales opportunities, and facilitate discussion about the context of opportunities in the sales team. Research limitations/implications The quality and quantity of available data affect the performance of models and explanations. Practical implications The application in the real-world company demonstrates the utility of the approach and provides evidence that transparent explanations of ML models contribute to individual and organizational learning. Social implications All used methods are available as an open-source software and can improve the acceptance of ML in data-driven decision making. Originality/value The proposed framework incorporates existing ML models and general explanation methodology into a decision-making process. To the authors’ knowledge, this is the first attempt to support organizational learning with a framework combining ML explanations, ADR, and data mining methodology based on the CRISP-DM industry standard.},
  keywords = {B2B sales forecasting,Double-loop learning,Explanation of black-box models,Machine learning}
}

@unpublished{botariMeLIMEMeaningfulLocal2020,
  title = {{{MeLIME}}: {{Meaningful Local Explanation}} for {{Machine Learning Models}}},
  shorttitle = {{{MeLIME}}},
  author = {Botari, Tiago and Hvilshøj, Frederik and Izbicki, Rafael and de Carvalho, Andre C. P. L. F.},
  options = {useprefix=true},
  year = {2020-09-12},
  eprint = {2009.05818},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2009.05818},
  _urldate = {2020-11-30},
  abstract = {Most state-of-the-art machine learning algorithms induce black-box models, preventing their application in many sensitive domains. Hence, many methodologies for explaining machine learning models have been proposed to address this problem. In this work, we introduce strategies to improve local explanations taking into account the distribution of the data used to train the black-box models. We show that our approach, MeLIME, produces more meaningful explanations compared to other techniques over different ML models, operating on various types of data. MeLIME generalizes the LIME method, allowing more flexible perturbation sampling and the use of different local interpretable models. Additionally, we introduce modifications to standard training algorithms of local interpretable models fostering more robust explanations, even allowing the production of counterfactual examples. To show the strengths of the proposed approach, we include experiments on tabular data, images, and text; all showing improved explanations. In particular, MeLIME generated more meaningful explanations on the MNIST dataset than methods such as GuidedBackprop, SmoothGrad, and Layer-wise Relevance Propagation. MeLIME is available on https://github.com/tiagobotari/melime.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@incollection{bousquetIntroductionStatisticalLearning2004,
  title = {Introduction to {{Statistical Learning Theory}}},
  booktitle = {Advanced {{Lectures}} on {{Machine Learning}}},
  author = {Bousquet, Olivier and Boucheron, Stéphane and Lugosi, Gábor},
  editor = {Bousquet, Olivier and von Luxburg, Ulrike and Rätsch, Gunnar},
  options = {useprefix=true},
  year = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {3176},
  pages = {169--207},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  _doi = {10.1007/978-3-540-28650-9_8},
  __url = {http://link.springer.com/10.1007/978-3-540-28650-9_8},
  _urldate = {2022-01-19},
  _isbn = {978-3-540-23122-6 978-3-540-28650-9}
}

@book{breimanClassificationRegressionTrees2017,
  title = {Classification {{And Regression Trees}}},
  author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
  year = {2017-10-19},
  edition = {1},
  publisher = {{Routledge}},
  _doi = {10.1201/9781315139470},
  __url = {https://www.taylorfrancis.com/books/9781351460491},
  _urldate = {2022},
  _isbn = {978-1-315-13947-0},
  langid = {english}
}


@article{breimanRandomForests2001,
  title = {Random {{Forests}}},
  author = {Breiman, Leo},
  year = {2001},
  journaltitle = {Machine Learning},
  journal = {Machine Learning},
  shortjournal = {Machine Learning},
  volume = {45},
  number = {1},
  pages = {5--32},
  _issn = {1573-0565},
  _doi = {10.1023/A:1010933404324},
  __url = {https://doi.org/10.1023/A:1010933404324},
  _urldate = {2020-08-19},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  langid = {english}
}

@article{breimanStatisticalModelingTwo2001,
  title = {Statistical {{Modeling}}: {{The Two Cultures}} (with Comments and a Rejoinder by the Author)},
  shorttitle = {Statistical {{Modeling}}},
  author = {Breiman, Leo},
  year = {2001},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {16},
  number = {3},
  pages = {199--231},
  _issn = {0883-4237, 2168-8745},
  _doi = {10.1214/ss/1009213726},
  __url = {https://projecteuclid.org/euclid.ss/1009213726},
  _urldate = {2020-08-25},
  abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
  langid = {english},
  mrnumber = {MR1874152},
  zmnumber = {1059.62505}
}

@article{bringGeometricApproachCompare1996,
  title = {A {{Geometric Approach}} to {{Compare Variables}} in a {{Regression Model}}},
  author = {Bring, Johan},
  year = {1996-02},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {50},
  number = {1},
  eprint = {2685045},
  eprinttype = {jstor},
  pages = {57},
  _issn = {00031305},
  _doi = {10.2307/2685045}
}

@article{bringGeometricApproachCompare1996a,
  title = {A {{Geometric Approach}} to {{Compare Variables}} in a {{Regression Model}}},
  author = {Bring, Johan},
  year = {1996},
  journaltitle = {The American Statistician},
  volume = {50},
  number = {1},
  eprint = {2685045},
  eprinttype = {jstor},
  pages = {57--62},
  _issn = {0003-1305},
  _doi = {10.2307/2685045},
  abstract = {Geometry is a very useful tool for illustrating regression analysis. Despite its merits the geometric approach is seldom used. One reason for this might be that there are very few applications at an elementary level. This article gives a brief introduction to the geometric approach in regression analysis, and then geometry is used to shed some light on the problem of comparing the "importance" of the independent variables in a multiple regression model. Even though no final answer of how to assess variable importance is given, it is still useful to illustrate the different measures geometrically to gain a better understanding of their properties.}
}

@article{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {33},
  __url = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  _urldate = {2020-12-07},
  langid = {english}
}

@unpublished{brundageTrustworthyAIDevelopment2020,
  title = {Toward {{Trustworthy AI Development}}: {{Mechanisms}} for {{Supporting Verifiable Claims}}},
  shorttitle = {Toward {{Trustworthy AI Development}}},
  author = {Brundage, Miles and Avin, Shahar and Wang, Jasmine and Belfield, Haydn and Krueger, Gretchen and Hadfield, Gillian and Khlaaf, Heidy and Yang, Jingying and Toner, Helen and Fong, Ruth and Maharaj, Tegan and Koh, Pang Wei and Hooker, Sara and Leung, Jade and Trask, Andrew and Bluemke, Emma and Lebensold, Jonathan and O'Keefe, Cullen and Koren, Mark and Ryffel, Théo and Rubinovitz, J. B. and Besiroglu, Tamay and Carugati, Federica and Clark, Jack and Eckersley, Peter and de Haas, Sarah and Johnson, Maritza and Laurie, Ben and Ingerman, Alex and Krawczuk, Igor and Askell, Amanda and Cammarota, Rosario and Lohn, Andrew and Krueger, David and Stix, Charlotte and Henderson, Peter and Graham, Logan and Prunkl, Carina and Martin, Bianca and Seger, Elizabeth and Zilberman, Noa and {hÉigeartaigh}, Seán Ó and Kroeger, Frens and Sastry, Girish and Kagan, Rebecca and Weller, Adrian and Tse, Brian and Barnes, Elizabeth and Dafoe, Allan and Scharre, Paul and Herbert-Voss, Ariel and Rasser, Martijn and Sodhani, Shagun and Flynn, Carrick and Gilbert, Thomas Krendl and Dyer, Lisa and Khan, Saif and Bengio, Yoshua and Anderljung, Markus},
  options = {useprefix=true},
  year = {2020-04-20},
  eprint = {2004.07213},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2004.07213},
  _urldate = {2020-10-22},
  abstract = {With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society}
}

@article{bruntonDiscoveringGoverningEquations2016,
  title = {Discovering Governing Equations from Data by Sparse Identification of Nonlinear Dynamical Systems},
  author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
  year = {2016-04-12},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {113},
  number = {15},
  pages = {3932--3937},
  _issn = {0027-8424, 1091-6490},
  _doi = {10.1073/pnas.1517384113},
  __url = {https://pnas.org/doi/full/10.1073/pnas.1517384113},
  _urldate = {2022-03-31},
  abstract = {Significance             Understanding dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled technology, including aircraft, combustion engines, satellites, and electrical power. This work develops a novel framework to discover governing equations underlying a dynamical system simply from data measurements, leveraging advances in sparsity techniques and machine learning. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. There are many critical data-driven problems, such as understanding cognition from neural recordings, inferring climate patterns, determining stability of financial markets, predicting and suppressing the spread of disease, and controlling turbulence for greener transportation and energy. With abundant data and elusive laws, data-driven discovery of dynamics will continue to play an important role in these efforts.           ,              Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
  langid = {english},
  file = {/home/rick/Zotero/storage/4HSZGRZU/Brunton et al. - 2016 - Discovering governing equations from data by spars.pdf}
}

@inproceedings{bucincaProxyTasksSubjective2020,
  title = {Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable {{AI}} Systems},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Buçinca, Zana and Lin, Phoebe and Gajos, Krzysztof Z. and Glassman, Elena L.},
  year = {2020-03-17},
  series = {{{IUI}} '20},
  pages = {454--464},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  _doi = {10.1145/3377325.3377498},
  __url = {https://doi.org/10.1145/3377325.3377498},
  _urldate = {2021-05-12},
  abstract = {Explainable artificially intelligent (XAI) systems form part of sociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet, current XAI systems are rarely evaluated by measuring the performance of human+AI teams on actual decision-making tasks. We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, artificial tasks such as how well humans predict the AI's decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance. The results of our experiments demonstrate that evaluations with proxy tasks did not predict the results of the evaluations with the actual decision-making tasks. Further, the subjective measures on evaluations with actual decision-making tasks did not predict the objective performance on those same tasks. Our results suggest that by employing misleading evaluation methods, our field may be inadvertently slowing its progress toward developing human+AI teams that can reliably perform better than humans or AIs alone.},
  _isbn = {978-1-4503-7118-6},
  keywords = {artificial intelligence,explanations,trust},
  file = {/home/rick/Zotero/storage/2QU76PCC/Buçinca et al. - 2020 - Proxy tasks and subjective measures can be mislead.pdf}
}

@unpublished{buckerTransparencyAuditabilityEXplainability2020,
  title = {Transparency, {{Auditability}} and {{eXplainability}} of {{Machine Learning Models}} in {{Credit Scoring}}},
  author = {Bücker, Michael and Szepannek, Gero and Gosiewska, Alicja and Biecek, Przemyslaw},
  year = {2020-09-28},
  eprint = {2009.13384},
  eprinttype = {arxiv},
  primaryclass = {cs, econ, q-fin, stat},
  __url = {http://arxiv.org/abs/2009.13384},
  _urldate = {2020-11-30},
  abstract = {A major requirement for credit scoring models is to provide a maximally accurate risk prediction. Additionally, regulators demand these models to be transparent and auditable. Thus, in credit scoring, very simple predictive models such as logistic regression or decision trees are still widely used and the superior predictive power of modern machine learning algorithms cannot be fully leveraged. Significant potential is therefore missed, leading to higher reserves or more credit defaults. This paper works out different dimensions that have to be considered for making credit scoring models understandable and presents a framework for making ``black box'' machine learning models transparent, auditable and explainable. Following this framework, we present an overview of techniques, demonstrate how they can be applied in credit scoring and how results compare to the interpretability of score cards. A real world case study shows that a comparable degree of interpretability can be achieved while machine learning techniques keep their ability to improve predictive power.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Economics - General Economics,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology}
}

@article{budescuDominanceAnalysisNew1993,
  title = {Dominance Analysis: {{A}} New Approach to the Problem of Relative Importance of Predictors in Multiple Regression.},
  shorttitle = {Dominance Analysis},
  author = {Budescu, David V.},
  year = {1993},
  journaltitle = {Psychological Bulletin},
  shortjournal = {Psychological Bulletin},
  volume = {114},
  number = {3},
  pages = {542--551},
  _issn = {1939-1455, 0033-2909},
  _doi = {10.1037/0033-2909.114.3.542},
  __url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.114.3.542},
  _urldate = {2021-11-26},
  langid = {english}
}

@unpublished{burkartSurveyExplainabilitySupervised2020,
  title = {A {{Survey}} on the {{Explainability}} of {{Supervised Machine Learning}}},
  author = {Burkart, Nadia and Huber, Marco F.},
  year = {2020-11-16},
  eprint = {2011.07876},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2011.07876},
  _urldate = {2020-11-30},
  abstract = {Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or fifinance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{candesPanningGoldModelX2017,
  title = {Panning for {{Gold}}: {{Model-X Knockoffs}} for {{High-dimensional Controlled Variable Selection}}},
  shorttitle = {Panning for {{Gold}}},
  author = {Candes, Emmanuel and Fan, Yingying and Janson, Lucas and Lv, Jinchi},
  year = {2017-12-12},
  eprint = {1610.02351},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  __url = {http://arxiv.org/abs/1610.02351},
  _urldate = {2020-12-22},
  abstract = {Many contemporary large-scale applications involve building interpretable models linking a large set of potential covariates to a response in a nonlinear fashion, such as when the response is binary. Although this modeling problem has been extensively studied, it remains unclear how to effectively control the fraction of false discoveries even in high-dimensional logistic regression, not to mention general high-dimensional nonlinear models. To address such a practical problem, we propose a new framework of \$model\$-\$X\$ knockoffs, which reads from a different perspective the knockoff procedure (Barber and Cand\textbackslash `es, 2015) originally designed for controlling the false discovery rate in linear models. Whereas the knockoffs procedure is constrained to homoscedastic linear models with \$n\textbackslash ge p\$, the key innovation here is that model-X knockoffs provide valid inference from finite samples in settings in which the conditional distribution of the response is arbitrary and completely unknown. Furthermore, this holds no matter the number of covariates. Correct inference in such a broad setting is achieved by constructing knockoff variables probabilistically instead of geometrically. To do this, our approach requires the covariates be random (independent and identically distributed rows) with a distribution that is known, although we provide preliminary experimental evidence that our procedure is robust to unknown/estimated distributions. To our knowledge, no other procedure solves the \$controlled\$ variable selection problem in such generality, but in the restricted settings where competitors exist, we demonstrate the superior power of knockoffs through simulations. Finally, we apply our procedure to data from a case-control study of Crohn's disease in the United Kingdom, making twice as many discoveries as the original analysis of the same data.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Applications,Statistics - Methodology},
  file = {/home/rick/Zotero/storage/VM29R7U7/Candes et al. - 2017 - Panning for Gold Model-X Knockoffs for High-dimen.pdf;/home/rick/Zotero/storage/VBPFZUTQ/1610.html}
}

@unpublished{carmichaelObjectiveEvaluationPost2021,
  title = {On the {{Objective Evaluation}} of {{Post Hoc Explainers}}},
  author = {Carmichael, Zachariah and Scheirer, Walter J.},
  year = {2021-06-15},
  eprint = {2106.08376},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2106.08376},
  _urldate = {2022-01-22},
  abstract = {Many applications of data-driven models demand transparency of decisions, especially in health care, criminal justice, and other high-stakes environments. Modern trends in machine learning research have led to algorithms that are increasingly intricate to the degree that they are considered to be black boxes. In an effort to reduce the opacity of decisions, methods have been proposed to construe the inner workings of such models in a human-comprehensible manner. These post hoc techniques are described as being universal explainers - capable of faithfully augmenting decisions with algorithmic insight. Unfortunately, there is little agreement about what constitutes a "good" explanation. Moreover, current methods of explanation evaluation are derived from either subjective or proxy means. In this work, we propose a framework for the evaluation of post hoc explainers on ground truth that is directly derived from the additive structure of a model. We demonstrate the efficacy of the framework in understanding explainers by evaluating popular explainers on thousands of synthetic and several real-world tasks. The framework unveils that explanations may be accurate but misattribute the importance of individual features.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@unpublished{carrilloIndividualExplanationsMachine2021,
  title = {Individual {{Explanations}} in {{Machine Learning Models}}: {{A Survey}} for {{Practitioners}}},
  shorttitle = {Individual {{Explanations}} in {{Machine Learning Models}}},
  author = {Carrillo, Alfredo and Cantú, Luis F. and Noriega, Alejandro},
  year = {2021-04-11},
  eprint = {2104.04144},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2104.04144},
  _urldate = {2021-05-12},
  abstract = {In recent years, the use of sophisticated statistical models that influence decisions in domains of high societal relevance is on the rise. Although these models can often bring substantial improvements in the accuracy and efficiency of organizations, many governments, institutions, and companies are reluctant to their adoption as their output is often difficult to explain in human-interpretable ways. Hence, these models are often regarded as black-boxes, in the sense that their internal mechanisms can be opaque to human audit. In real-world applications, particularly in domains where decisions can have a sensitive impact--e.g., criminal justice, estimating credit scores, insurance risk, health risks, etc.--model interpretability is desired. Recently, the academic literature has proposed a substantial amount of methods for providing interpretable explanations to machine learning models. This survey reviews the most relevant and novel methods that form the state-of-the-art for addressing the particular problem of explaining individual instances in machine learning. It seeks to provide a succinct review that can guide data science and machine learning practitioners in the search for appropriate methods to their problem domain.},
  archiveprefix = {arXiv},
  version = {2},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Applications},
  file = {/home/rick/Zotero/storage/DAYDT3AM/Carrillo et al. - 2021 - Individual Explanations in Machine Learning Models.pdf;/home/rick/Zotero/storage/X6JUTATN/2104.html}
}

@inproceedings{carterDimensionalityReductionStatistical2009,
  title = {Dimensionality Reduction on Statistical Manifolds},
  author = {Carter, Kev},
  year = {2009}
}

@inproceedings{caruanaIntelligibleModelsHealthCare2015,
  title = {Intelligible {{Models}} for {{HealthCare}}: {{Predicting Pneumonia Risk}} and {{Hospital}} 30-Day {{Readmission}}},
  shorttitle = {Intelligible {{Models}} for {{HealthCare}}},
  booktitle = {Proceedings of the 21th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
  year = {2015-08-10},
  series = {{{KDD}} '15},
  pages = {1721--1730},
  publisher = {{Association for Computing Machinery}},
  location = {{Sydney, NSW, Australia}},
  _doi = {10.1145/2783258.2788613},
  __url = {https://doi.org/10.1145/2783258.2788613},
  _urldate = {2020-12-07},
  abstract = {In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.},
  _isbn = {978-1-4503-3664-2},
  keywords = {additive models,classification,healthcare,intelligibility,interaction detection,logistic regression,risk prediction}
}

@article{carvalhoMachineLearningInterpretability2019,
  title = {Machine {{Learning Interpretability}}: {{A Survey}} on {{Methods}} and {{Metrics}}},
  shorttitle = {Machine {{Learning Interpretability}}},
  author = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
  year = {2019-08},
  journaltitle = {Electronics},
  volume = {8},
  number = {8},
  pages = {832},
  _doi = {10.3390/electronics8080832},
  __url = {https://www.mdpi.com/2079-9292/8/8/832},
  _urldate = {2020-10-19},
  abstract = {Machine learning systems are becoming increasingly ubiquitous. These systems\&rsquo;s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.},
  langid = {english},
  keywords = {explainability,interpretability,machine learning,XAI}
}

@article{castroPolynomialCalculationShapley2009,
  title = {Polynomial Calculation of the {{Shapley}} Value Based on Sampling},
  author = {Castro, Javier and Gómez, Daniel and Tejada, Juan},
  year = {2009-05},
  journaltitle = {Computers \& Operations Research},
  shortjournal = {Computers \& Operations Research},
  volume = {36},
  number = {5},
  pages = {1726--1730},
  _issn = {03050548},
  _doi = {10.1016/j.cor.2008.04.004},
  __url = {https://linkinghub.elsevier.com/retrieve/pii/S0305054808000804},
  _urldate = {2022-01-31},
  langid = {english}
}

@unpublished{chajewskaDefiningExplanationProbabilistic2013,
  title = {Defining {{Explanation}} in {{Probabilistic Systems}}},
  author = {Chajewska, Urszula and Halpern, Joseph Y.},
  year = {2013-02-06},
  eprint = {1302.1526},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1302.1526},
  _urldate = {2021-08-11},
  abstract = {As probabilistic systems gain popularity and are coming into wider use, the need for a mechanism that explains the system's findings and recommendations becomes more critical. The system will also need a mechanism for ordering competing explanations. We examine two representative approaches to explanation in the literature - one due to G\textbackslash "ardenfors and one due to Pearl - and show that both suffer from significant problems. We propose an approach to defining a notion of "better explanation" that combines some of the features of both together with more recent work by Pearl and others on causality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@unpublished{chatterjeeNewCoefficientCorrelation2020,
  title = {A New Coefficient of Correlation},
  author = {Chatterjee, Sourav},
  year = {2020-04-28},
  eprint = {1909.10140},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  __url = {http://arxiv.org/abs/1909.10140},
  _urldate = {2022-01-24},
  abstract = {Is it possible to define a coefficient of correlation which is (a) as simple as the classical coefficients like Pearson's correlation or Spearman's correlation, and yet (b) consistently estimates some simple and interpretable measure of the degree of dependence between the variables, which is 0 if and only if the variables are independent and 1 if and only if one is a measurable function of the other, and (c) has a simple asymptotic theory under the hypothesis of independence, like the classical coefficients? This article answers this question in the affirmative, by producing such a coefficient. No assumptions are needed on the distributions of the variables. There are several coefficients in the literature that converge to 0 if and only if the variables are independent, but none that satisfy any of the other properties mentioned above.},
  archiveprefix = {arXiv},
  keywords = {62H20; 62H15,Mathematics - Probability,Mathematics - Statistics Theory},
  file = {/home/rick/Zotero/storage/JHEFUXMH/Chatterjee - 2020 - A new coefficient of correlation.pdf;/home/rick/Zotero/storage/6UEZEDDB/1909.html}
}

@article{chenConceptWhiteningInterpretable2020,
  title = {Concept Whitening for Interpretable Image Recognition},
  author = {Chen, Zhi and Bei, Yijie and Rudin, Cynthia},
  year = {2020-12},
  journaltitle = {Nature Machine Intelligence},
  volume = {2},
  number = {12},
  pages = {772--782},
  _issn = {2522-5839},
  _doi = {10.1038/s42256-020-00265-z},
  __url = {https://www.nature.com/articles/s42256-020-00265-z},
  _urldate = {2021-01-13},
  abstract = {What does a neural network encode about a concept as we traverse through the layers? Interpretability in machine learning is undoubtedly important, but the calculations of neural networks are very challenging to understand. Attempts to see inside their hidden layers can be misleading, unusable or rely on the latent space to possess properties that it may not have. Here, rather than attempting to analyse a neural network post hoc, we introduce a mechanism, called concept whitening (CW), to alter a given layer of the network to allow us to better understand the computation leading up to that layer. When a concept whitening module is added to a convolutional neural network, the latent space is whitened (that is, decorrelated and normalized) and the axes of the latent space are aligned with known concepts of interest. By experiment, we show that CW can provide us with a much clearer understanding of how the network gradually learns concepts over layers. CW is an alternative to a batch normalization layer in that it normalizes, and also decorrelates (whitens), the latent space. CW can be used in any layer of the network without hurting predictive performance.},
  langid = {english}
}

@unpublished{chengDECEDecisionExplorer2020,
  title = {{{DECE}}: {{Decision Explorer}} with {{Counterfactual Explanations}} for {{Machine Learning Models}}},
  shorttitle = {{{DECE}}},
  author = {Cheng, Furui and Ming, Yao and Qu, Huamin},
  year = {2020-08-19},
  eprint = {2008.08353},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2008.08353},
  _urldate = {2020-09-16},
  abstract = {With machine learning models being increasingly applied to various decision-making scenarios, people have spent growing efforts to make machine learning models more transparent and explainable. Among various explanation techniques, counterfactual explanations have the advantages of being human-friendly and actionable -- a counterfactual explanation tells the user how to gain the desired prediction with minimal changes to the input. Besides, counterfactual explanations can also serve as efficient probes to the models' decisions. In this work, we exploit the potential of counterfactual explanations to understand and explore the behavior of machine learning models. We design DECE, an interactive visualization system that helps understand and explore a model's decisions on individual instances and data subsets, supporting users ranging from decision-subjects to model developers. DECE supports exploratory analysis of model decisions by combining the strengths of counterfactual explanations at instance- and subgroup-levels. We also introduce a set of interactions that enable users to customize the generation of counterfactual explanations to find more actionable ones that can suit their needs. Through three use cases and an expert interview, we demonstrate the effectiveness of DECE in supporting decision exploration tasks and instance explanations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,H.5.2,I.2.0,Statistics - Machine Learning}
}

@unpublished{chenLearningExplainInformationTheoretic2018,
  title = {Learning to {{Explain}}: {{An Information-Theoretic Perspective}} on {{Model Interpretation}}},
  shorttitle = {Learning to {{Explain}}},
  author = {Chen, Jianbo and Song, Le and Wainwright, Martin J. and Jordan, Michael I.},
  year = {2018-06-13},
  eprint = {1802.07814},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1802.07814},
  _urldate = {2020-10-19},
  abstract = {We introduce instancewise feature selection as a methodology for model interpretation. Our method is based on learning a function to extract a subset of features that are most informative for each given example. This feature selector is trained to maximize the mutual information between selected features and the response variable, where the conditional distribution of the response variable given the input is the model to be explained. We develop an efficient variational approximation to the mutual information, and show the effectiveness of our method on a variety of synthetic and real data sets using both quantitative metrics and human evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{chenLShapleyCShapleyEfficient2018,
  title = {L-{{Shapley}} and {{C-Shapley}}: {{Efficient Model Interpretation}} for {{Structured Data}}},
  shorttitle = {L-{{Shapley}} and {{C-Shapley}}},
  author = {Chen, Jianbo and Song, Le and Wainwright, Martin J. and Jordan, Michael I.},
  year = {2018-08-07},
  eprint = {1808.02610},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1808.02610},
  _urldate = {2020-10-19},
  abstract = {We study instancewise feature importance scoring as a method for model interpretation. Any such method yields, for each predicted instance, a vector of importance scores associated with the feature vector. Methods based on the Shapley score have been proposed as a fair way of computing feature attributions of this kind, but incur an exponential complexity in the number of features. This combinatorial explosion arises from the definition of the Shapley value and prevents these methods from being scalable to large data sets and complex models. We focus on settings in which the data have a graph structure, and the contribution of features to the target variable is well-approximated by a graph-structured factorization. In such settings, we develop two algorithms with linear complexity for instancewise feature importance scoring. We establish the relationship of our methods to the Shapley value and another closely related concept known as the Myerson value from cooperative game theory. We demonstrate on both language and image data that our algorithms compare favorably with other methods for model interpretation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{chenTrueModelTrue2020,
  title = {True to the {{Model}} or {{True}} to the {{Data}}?},
  author = {Chen, Hugh and Janizek, Joseph D. and Lundberg, Scott and Lee, Su-In},
  year = {2020-06-29},
  eprint = {2006.16234},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2006.16234},
  _urldate = {2021-11-16},
  abstract = {A variety of recent papers discuss the application of Shapley values, a concept for explaining coalitional games, for feature attribution in machine learning. However, the correct way to connect a machine learning model to a coalitional game has been a source of controversy. The two main approaches that have been proposed differ in the way that they condition on known features, using either (1) an interventional or (2) an observational conditional expectation. While previous work has argued that one of the two approaches is preferable in general, we argue that the choice is application dependent. Furthermore, we argue that the choice comes down to whether it is desirable to be true to the model or true to the data. We use linear models to investigate this choice. After deriving an efficient method for calculating observational conditional expectation Shapley values for linear models, we investigate how correlation in simulated data impacts the convergence of observational conditional expectation Shapley values. Finally, we present two real data examples that we consider to be representative of possible use cases for feature attribution -- (1) credit risk modeling and (2) biological discovery. We show how a different choice of value function performs better in each scenario, and how possible attributions are impacted by modeling choices.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/3SI2R4LY/Chen et al. - 2020 - True to the Model or True to the Data.pdf;/home/rick/Zotero/storage/BB7PKBKW/2006.html}
}

@unpublished{cheungDiscoveringHiddenFactors2015,
  title = {Discovering {{Hidden Factors}} of {{Variation}} in {{Deep Networks}}},
  author = {Cheung, Brian and Livezey, Jesse A. and Bansal, Arjun K. and Olshausen, Bruno A.},
  year = {2015-06-17},
  eprint = {1412.6583},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1412.6583},
  _urldate = {2021-09-07},
  abstract = {Deep learning has enjoyed a great deal of success because of its ability to learn useful features for tasks such as classification. But there has been less exploration in learning the factors of variation apart from the classification signal. By augmenting autoencoders with simple regularization terms during training, we demonstrate that standard deep architectures can discover and explicitly represent factors of variation beyond those relevant for categorization. We introduce a cross-covariance penalty (XCov) as a method to disentangle factors like handwriting style for digits and subject identity in faces. We demonstrate this on the MNIST handwritten digit database, the Toronto Faces Database (TFD) and the Multi-PIE dataset by generating manipulated instances of the data. Furthermore, we demonstrate these deep networks can extrapolate `hidden' variation in the supervised signal.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{chevanHierarchicalPartitioning1991,
  title = {Hierarchical {{Partitioning}}},
  author = {Chevan, Albert and Sutherland, Michael},
  year = {1991-05},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {45},
  number = {2},
  pages = {90--96},
  _issn = {0003-1305, 1537-2731},
  _doi = {10.1080/00031305.1991.10475776},
  __url = {http://www.tandfonline.com/doi/abs/10.1080/00031305.1991.10475776},
  _urldate = {2022-02-22},
  langid = {english}
}

@article{cholletfranccoisKeras2015,
  title = {Keras},
  author = {{Chollet, Fran\textbackslash c\{c\}ois} and {Others}},
  year = {2015},
  journaltitle = {https://keras.io}
}

@book{cholletKeras2015,
  title = {Keras},
  author = {Chollet, François and others},
  year = {2015},
  __url = {https://keras.io}
}

@book{cholletKeras2015a,
  title = {Keras},
  author = {Chollet, François and others},
  year = {2015},
  __url = {https://keras.io}
}

@book{cholletKeras2015b,
  title = {Keras},
  author = {Chollet, François and others},
  year = {2015},
  __url = {https://keras.io}
}

@unpublished{chongDejaVuSVM2020,
  title = {Deja vu from the {{SVM Era}}: {{Example-based Explanations}} with {{Outlier Detection}}},
  shorttitle = {Deja vu from the {{SVM Era}}},
  author = {Chong, Penny and Elovici, Yuval and Binder, Alexander},
  year = {2020-11-11},
  eprint = {2011.05577},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2011.05577},
  _urldate = {2020-11-30},
  abstract = {Understanding the features that contributed to a prediction is important for high-stake tasks. In this work, we revisit the idea of a student network to provide an example-based explanation for its prediction in two forms: i) identify top-k most relevant prototype examples and ii) show evidence of similarity between the prediction sample and each of the top-k prototypes. We compare the prediction performance and the explanation performance for the second type of explanation with the teacher network. In addition, we evaluate the outlier detection performance of the network. We show that using prototype-based students beyond similarity kernels deliver meaningful explanations and promising outlier detection results, without compromising on classification accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@online{choudhuryExplainabilityInterpretabilityArtificial2019,
  title = {Explainability {{V}}/s {{Interpretability In Artificial Intelligence}}},
  author = {Choudhury, Ambika},
  year = {2019-01-14T04:37:24+00:00},
  __url = {https://analyticsindiamag.com/explainability-vs-interpretability-in-artificial-intelligence-and-machine-learning/},
  _urldate = {2020-04-20},
  abstract = {Over the last few years, there have been several innovations in the field of artificial intelligence and machine learning.},
  langid = {american},
  organization = {{Analytics India Magazine}}
}

@article{chuAreVisualExplanations2020,
  title = {Are {{Visual Explanations Useful}}? {{A Case Study}} in {{Model-in-the-Loop Prediction}}},
  shorttitle = {Are {{Visual Explanations Useful}}?},
  author = {Chu, Eric and Roy, Deb and Andreas, Jacob},
  year = {2020-07-23},
  __url = {https://arxiv.org/abs/2007.12248v1},
  _urldate = {2020-08-05},
  abstract = {We present a randomized controlled trial for a model-in-the-loop regression task, with the goal of measuring the extent to which (1) good explanations of model predictions increase human accuracy, and (2) faulty explanations decrease human trust in the model. We study explanations based on visual saliency in an image-based age prediction task for which humans and learned models are individually capable but not highly proficient and frequently disagree. Our experimental design separates model quality from explanation quality, and makes it possible to compare treatments involving a variety of explanations of varying levels of quality. We find that presenting model predictions improves human accuracy. However, visual explanations of various kinds fail to significantly alter human accuracy or trust in the model - regardless of whether explanations characterize an accurate model, an inaccurate one, or are generated randomly and independently of the input image. These findings suggest the need for greater evaluation of explanations in downstream decision making tasks, better design-based tools for presenting explanations to users, and better approaches for generating explanations.},
  langid = {english}
}

@article{clarkComputationalMethodsProbabilistic1997,
  title = {Computational {{Methods}} for {{Probabilistic Decision Trees}}},
  author = {Clark, David E.},
  year = {1997-02},
  journaltitle = {Computers and Biomedical Research},
  shortjournal = {Computers and Biomedical Research},
  volume = {30},
  number = {1},
  pages = {19--33},
  _issn = {00104809},
  _doi = {10.1006/cbmr.1997.1438},
  __url = {https://linkinghub.elsevier.com/retrieve/pii/S0010480997914385},
  _urldate = {2022-04-12},
  langid = {english}
}

@inproceedings{clinciuSurveyExplainableAI2019,
  title = {A {{Survey}} of {{Explainable AI Terminology}}},
  booktitle = {Proceedings of the 1st {{Workshop}} on {{Interactive Natural Language Technology}} for {{Explainable Artificial Intelligence}} ({{NL4XAI}} 2019)},
  author = {Clinciu, Miruna-Adriana and Hastie, Helen},
  year = {2019},
  pages = {8--13},
  publisher = {{Association for Computational Linguistics}},
  _doi = {10.18653/v1/W19-8403},
  __url = {https://www.aclweb.org/anthology/W19-8403},
  _urldate = {2020-10-22}
}

@article{congedoRiemannianGeometryEEGbased2017,
  title = {Riemannian Geometry for {{EEG-based}} Brain-Computer Interfaces; a Primer and a Review},
  author = {Congedo, Marco and Barachant, Alexandre and Bhatia, Rajendra},
  year = {2017-07-03},
  journaltitle = {Brain-Computer Interfaces},
  volume = {4},
  number = {3},
  pages = {155--174},
  _issn = {2326-263X},
  _doi = {10.1080/2326263X.2017.1297192},
  __url = {https://doi.org/10.1080/2326263X.2017.1297192},
  _urldate = {2020-10-28},
  abstract = {Despite its short history, the use of Riemannian geometry in brain-computer interface (BCI) decoding is currently attracting increasing attention, due to accumulating documentation of its simplicity, accuracy, robustness and transfer learning capabilities, including the winning score obtained in five recent international predictive modeling BCI data competitions. The Riemannian framework is sharp from a mathematical perspective, yet in practice it is simple, both algorithmically and computationally. This allows the conception of online decoding machines suiting real-world operation in adverse conditions. We provide here a review on the use of Riemannian geometry for BCI and a primer on the classification frameworks based on it. While the theoretical research on Riemannian geometry is technical, our aim here is to show the appeal of the framework on an intuitive geometrical ground. In particular, we provide a rationale for its robustness and transfer learning capabilities and we elucidate the link between a simple Riemannian classifier and a state-of-the-art spatial filtering approach. We conclude by reporting details on the construction of data points to be manipulated in the Riemannian framework in the context of BCI and by providing links to available open-source Matlab and Python code libraries for designing BCI decoders.},
  keywords = {classification,covariance matrix,decoding,electroencephalography,geometric mean,Riemannian geometry,signal processing}
}

@article{congerRevisedDefinitionSuppressor1974,
  title = {A {{Revised Definition}} for {{Suppressor Variables}}: A {{Guide To Their Identification}} and {{Interpretation}} ,  {{A Revised Definition}} for {{Suppressor Variables}}: A {{Guide To Their Identification}} and {{Interpretation}}},
  shorttitle = {A {{Revised Definition}} for {{Suppressor Variables}}},
  author = {Conger, Anthony J.},
  year = {1974},
  journaltitle = {Educational and Psychological Measurement},
  journal = {Educational and Psychological Measurement},
  shortjournal = {Educational and Psychological Measurement},
  volume = {34},
  number = {1},
  pages = {35--46},
  _issn = {0013-1644},
  _doi = {10.1177/001316447403400105},
  __url = {https://doi.org/10.1177/001316447403400105},
  _urldate = {2020-11-30},
  abstract = {In the two-predictor situation it is shown that traditional and negative suppressors increase the predictive value of a standard predictor beyond that suggested by the predictor's zero order validity. This effect of suppression is used to provide a revised definition of suppression and completely accounts for traditional and negative suppression. The revised definition, in conjunction with a two factor model, is shown to lead to a previously undetected type of suppression (reciprocal suppression) which occurs when predictors with positive zero order validities are negatively correlated with one another. In terms of the definition and parameters of the model, limits are determined in which the types of suppression can occur. Furthermore, it is shown how suppressors can be identified in multiple regression equations and a procedure is given for interpreting whether the variables are contributing directly (by predicting relevant variance in the criterion) or indirectly (by removing irrelevant variance in another predictor) or both., In the two-predictor situation it is shown that traditional and negative suppressors increase the predictive value of a standard predictor beyond that suggested by the predictor's zero order validity. This effect of suppression is used to provide a revised definition of suppression and completely accounts for traditional and negative suppression. The revised definition, in conjunction with a two factor model, is shown to lead to a previously undetected type of suppression (reciprocal suppression) which occurs when predictors with positive zero order validities are negatively correlated with one another. In terms of the definition and parameters of the model, limits are determined in which the types of suppression can occur. Furthermore, it is shown how suppressors can be identified in multiple regression equations and a procedure is given for interpreting whether the variables are contributing directly (by predicting relevant variance in the criterion) or indirectly (by removing irrelevant variance in another predictor) or both.},
  langid = {english}
}

@unpublished{covertExplainingRemovingUnified2020,
  title = {Explaining by {{Removing}}: {{A Unified Framework}} for {{Model Explanation}}},
  shorttitle = {Explaining by {{Removing}}},
  author = {Covert, Ian and Lundberg, Scott and Lee, Su-In},
  year = {2020-11-20},
  eprint = {2011.14878},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2011.14878},
  _urldate = {2020-12-01},
  abstract = {Researchers have proposed a wide variety of model explanation approaches, but it remains unclear how most methods are related or when one method is preferable to another. We establish a new class of methods, removal-based explanations, that are based on the principle of simulating feature removal to quantify each feature's influence. These methods vary in several respects, so we develop a framework that characterizes each method along three dimensions: 1) how the method removes features, 2) what model behavior the method explains, and 3) how the method summarizes each feature's influence. Our framework unifies 25 existing methods, including several of the most widely used approaches (SHAP, LIME, Meaningful Perturbations, permutation tests). This new class of explanation methods has rich connections that we examine using tools that have been largely overlooked by the explainability literature. To anchor removal-based explanations in cognitive psychology, we show that feature removal is a simple application of subtractive counterfactual reasoning. Ideas from cooperative game theory shed light on the relationships and trade-offs among different methods, and we derive conditions under which all removal-based explanations have information-theoretic interpretations. Through this analysis, we develop a unified framework that helps practitioners better understand model explanation tools, and that offers a strong theoretical foundation upon which future explainability research can build.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{covertFeatureRemovalUnifying2020,
  title = {Feature {{Removal Is}} a {{Unifying Principle}} for {{Model Explanation Methods}}},
  author = {Covert, Ian and Lundberg, Scott and Lee, Su-In},
  year = {2020-11-06},
  eprint = {2011.03623},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2011.03623},
  _urldate = {2020-11-30},
  abstract = {Researchers have proposed a wide variety of model explanation approaches, but it remains unclear how most methods are related or when one method is preferable to another. We examine the literature and find that many methods are based on a shared principle of explaining by removing - essentially, measuring the impact of removing sets of features from a model. These methods vary in several respects, so we develop a framework for removal-based explanations that characterizes each method along three dimensions: 1) how the method removes features, 2) what model behavior the method explains, and 3) how the method summarizes each feature's influence. Our framework unifies 25 existing methods, including several of the most widely used approaches (SHAP, LIME, Meaningful Perturbations, permutation tests). Exposing the fundamental similarities between these methods empowers users to reason about which tools to use and suggests promising directions for ongoing research in model explainability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{crabbeLearningOutsideBlackBox2020,
  title = {Learning Outside the {{Black-Box}}: {{The}} Pursuit of Interpretable Models},
  shorttitle = {Learning Outside the {{Black-Box}}},
  author = {Crabbé, Jonathan and Zhang, Yao and Zame, William and van der Schaar, Mihaela},
  options = {useprefix=true},
  year = {2020-11-17},
  eprint = {2011.08596},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2011.08596},
  _urldate = {2020-11-30},
  abstract = {Machine Learning has proved its ability to produce accurate models but the deployment of these models outside the machine learning community has been hindered by the difficulties of interpreting these models. This paper proposes an algorithm that produces a continuous global interpretation of any given continuous black-box function. Our algorithm employs a variation of projection pursuit in which the ridge functions are chosen to be Meijer G-functions, rather than the usual polynomial splines. Because Meijer G-functions are differentiable in their parameters, we can tune the parameters of the representation by gradient descent; as a consequence, our algorithm is efficient. Using five familiar data sets from the UCI repository and two familiar machine learning algorithms, we demonstrate that our algorithm produces global interpretations that are both highly accurate and parsimonious (involve a small number of terms). Our interpretations permit easy understanding of the relative importance of features and feature interactions. Our interpretation algorithm represents a leap forward from the previous state of the art.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@inproceedings{cravenExtractingTreestructuredRepresentations1995,
  title = {Extracting Tree-Structured Representations of Trained Networks},
  booktitle = {Proceedings of the 8th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Craven, Mark W. and Shavlik, Jude W.},
  year = {1995-11-27},
  series = {{{NIPS}}'95},
  pages = {24--30},
  publisher = {{MIT Press}},
  location = {{Denver, Colorado}},
  abstract = {A significant limitation of neural networks is that the representations they learn are usually incomprehensible to humans. We present a novel algorithm, TREPAN, for extracting comprehensible, symbolic representations from trained neural networks. Our algorithm uses queries to induce a decision tree that approximates the concept represented by a given network. Our experiments demonstrate that TREPAN is able to produce decision trees that maintain a high level of fidelity to their respective networks while being comprehensible and accurate. Unlike previous work in this area, our algorithm is general in its applicability and scales well to large networks and problems with high-dimensional input spaces.}
}

@unpublished{cremerInferenceSuboptimalityVariational2018,
  title = {Inference {{Suboptimality}} in {{Variational Autoencoders}}},
  author = {Cremer, Chris and Li, Xuechen and Duvenaud, David},
  year = {2018-05-27},
  eprint = {1801.03558},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1801.03558},
  _urldate = {2022-01-17},
  abstract = {Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true posterior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate inference in variational autoencoders in terms of these factors. We find that divergence from the true posterior is often due to imperfect recognition networks, rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/C6M6HVII/Cremer et al. - 2018 - Inference Suboptimality in Variational Autoencoder.pdf;/home/rick/Zotero/storage/CYHFNTDK/1801.html}
}

@article{Cucker2001OnTM,
  title = {On the Mathematical Foundations of Learning},
  author = {Cucker, Felipe and Smale, Stephen},
  year = {2001},
  journaltitle = {Bulletin of the American Mathematical Society},
  journal = {Bulletin of the American Mathematical Society},
  volume = {39},
  pages = {1--49}
}

@unpublished{dandlMultiObjectiveCounterfactualExplanations2020,
  title = {Multi-{{Objective Counterfactual Explanations}}},
  author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
  year = {2020},
  volume = {12269},
  eprint = {2004.11165},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {448--469},
  _doi = {10.1007/978-3-030-58112-1_31},
  __url = {http://arxiv.org/abs/2004.11165},
  _urldate = {2021-10-19},
  abstract = {Counterfactual explanations are one of the most popular methods to make predictions of black box machine learning models interpretable by providing explanations in the form of `what-if scenarios'. Most current approaches optimize a collapsed, weighted sum of multiple objectives, which are naturally difficult to balance a-priori. We propose the Multi-Objective Counterfactuals (MOC) method, which translates the counterfactual search into a multi-objective optimization problem. Our approach not only returns a diverse set of counterfactuals with different trade-offs between the proposed objectives, but also maintains diversity in feature space. This enables a more detailed post-hoc analysis to facilitate better understanding and also more options for actionable user responses to change the predicted outcome. Our approach is also model-agnostic and works for numerical and categorical input features. We show the usefulness of MOC in concrete cases and compare our approach with state-of-the-art methods for counterfactual explanations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/FIIG6CI6/Dandl et al. - 2020 - Multi-Objective Counterfactual Explanations.pdf;/home/rick/Zotero/storage/H4QMCBZ2/2004.html}
}

@article{darlingtonMultipleRegressionPsychological1968,
  title = {Multiple Regression in Psychological Research and Practice.},
  author = {Darlington, Richard B.},
  year = {1968},
  journaltitle = {Psychological Bulletin},
  shortjournal = {Psychological Bulletin},
  volume = {69},
  number = {3},
  pages = {161--182},
  _issn = {1939-1455, 0033-2909},
  _doi = {10.1037/h0025471},
  __url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0025471},
  _urldate = {2022-02-01},
  langid = {english}
}

@inproceedings{dattaAlgorithmicTransparencyQuantitative2016,
  title = {Algorithmic {{Transparency}} via {{Quantitative Input Influence}}: {{Theory}} and {{Experiments}} with {{Learning Systems}}},
  booktitle = {2016 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Datta, Anupam and Sen, Shayak and Zick, Yair},
  year = {2016},
  pages = {598--617},
  _doi = {10.1109/SP.2016.42}
}

@inproceedings{davisRelationshipPrecisionRecallROC2006,
  title = {The Relationship between {{Precision-Recall}} and {{ROC}} Curves},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning},
  author = {Davis, Jesse and Goadrich, Mark},
  year = {2006},
  pages = {233--240}
}

@online{DblpAnchorsHighPrecision,
  title = {Dblp: {{Anchors}}: {{High-Precision Model-Agnostic Explanations}}.},
  __url = {https://dblp.org/rec/conf/aaai/Ribeiro0G18.html},
  _urldate = {2021-01-15},
  abstract = {Bibliographic details on Anchors: High-Precision Model-Agnostic Explanations.}
}

@unpublished{demijollaHumaninterpretableModelExplainability2020,
  title = {Human-Interpretable Model Explainability on High-Dimensional Data},
  author = {de Mijolla, Damien and Frye, Christopher and Kunesch, Markus and Mansir, John and Feige, Ilya},
  options = {useprefix=true},
  year = {2020-10-14},
  eprint = {2010.07384},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2010.07384},
  _urldate = {2020-11-30},
  abstract = {The importance of explainability in machine learning continues to grow, as both neural-network architectures and the data they model become increasingly complex. Unique challenges arise when a model's input features become high dimensional: on one hand, principled model-agnostic approaches to explainability become too computationally expensive; on the other, more efficient explainability algorithms lack natural interpretations for general users. In this work, we introduce a framework for human-interpretable explainability on high-dimensional data, consisting of two modules. First, we apply a semantically meaningful latent representation, both to reduce the raw dimensionality of the data, and to ensure its human interpretability. These latent features can be learnt, e.g. explicitly as disentangled representations or implicitly through image-to-image translation, or they can be based on any computable quantities the user chooses. Second, we adapt the Shapley paradigm for model-agnostic explainability to operate on these latent features. This leads to interpretable model explanations that are both theoretically controlled and computationally tractable. We benchmark our approach on synthetic data and demonstrate its effectiveness on several image-classification tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, J. and Dong, W. and Socher, R. and Li, L. and {Kai Li} and {Li Fei-Fei}},
  year = {2009-06},
  pages = {248--255},
  _issn = {1063-6919},
  _doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  eventtitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {computer vision,Explosions,Image databases,image resolution,image retrieval,Image retrieval,ImageNet database,Information retrieval,Internet,large-scale hierarchical image database,large-scale ontology,Large-scale systems,multimedia computing,multimedia data,Multimedia databases,Ontologies,ontologies (artificial intelligence),Robustness,Spine,subtree,trees (mathematics),very large databases,visual databases,wordNet structure}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019-06},
  pages = {4171--4186},
  publisher = {{Association for Computational Linguistics}},
  location = {{Minneapolis, Minnesota}},
  _doi = {10.18653/v1/N19-1423},
  __url = {https://www.aclweb.org/anthology/N19-1423},
  _urldate = {2020-12-07},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  eventtitle = {{{NAACL-HLT}} 2019}
}

@book{devroyeProbabilisticTheoryPattern1996,
  title = {A {{Probabilistic Theory}} of {{Pattern Recognition}}},
  author = {Devroye, Luc and Györfi, László and Lugosi, Gábor},
  year = {1996},
  series = {Stochastic {{Modelling}} and {{Applied Probability}}},
  volume = {31},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  _doi = {10.1007/978-1-4612-0711-5},
  __url = {http://link.springer.com/10.1007/978-1-4612-0711-5},
  _urldate = {2022-04-14},
  editorb = {Karatzas, I. and Yor, M.},
  editorbtype = {redactor},
  _isbn = {978-1-4612-6877-2 978-1-4612-0711-5}
}

@unpublished{deyoungERASERBenchmarkEvaluate2020,
  title = {{{ERASER}}: {{A Benchmark}} to {{Evaluate Rationalized NLP Models}}},
  shorttitle = {{{ERASER}}},
  author = {DeYoung, Jay and Jain, Sarthak and Rajani, Nazneen Fatema and Lehman, Eric and Xiong, Caiming and Socher, Richard and Wallace, Byron C.},
  year = {2020-04-24},
  eprint = {1911.03429},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1911.03429},
  _urldate = {2022-03-23},
  abstract = {State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the `reasoning' behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER) benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of "rationales" (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/rick/Zotero/storage/5A84UQR5/DeYoung et al. - 2020 - ERASER A Benchmark to Evaluate Rationalized NLP M.pdf;/home/rick/Zotero/storage/XUTT2ZTD/1911.html}
}

@incollection{dhurandharExplanationsBasedMissing2018,
  title = {Explanations Based on the {{Missing}}: {{Towards Contrastive Explanations}} with {{Pertinent Negatives}}},
  shorttitle = {Explanations Based on the {{Missing}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Dhurandhar, Amit and Chen, Pin-Yu and Luss, Ronny and Tu, Chun-Chen and Ting, Paishun and Shanmugam, Karthikeyan and Das, Payel},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  year = {2018},
  pages = {592--603},
  publisher = {{Curran Associates, Inc.}},
  __url = {http://papers.nips.cc/paper/7340-explanations-based-on-the-missing-towards-contrastive-explanations-with-pertinent-negatives.pdf},
  _urldate = {2020-04-20}
}

@unpublished{dhurandharFormalFrameworkCharacterize2017,
  title = {A {{Formal Framework}} to {{Characterize Interpretability}} of {{Procedures}}},
  author = {Dhurandhar, Amit and Iyengar, Vijay and Luss, Ronny and Shanmugam, Karthikeyan},
  year = {2017-07-12},
  eprint = {1707.03886},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1707.03886},
  _urldate = {2021-01-10},
  abstract = {We provide a novel notion of what it means to be interpretable, looking past the usual association with human understanding. Our key insight is that interpretability is not an absolute concept and so we define it relative to a target model, which may or may not be a human. We define a framework that allows for comparing interpretable procedures by linking it to important practical aspects such as accuracy and robustness. We characterize many of the current state-of-the-art interpretable methods in our framework portraying its general applicability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@unpublished{dhurandharModelAgnosticContrastive2019,
  title = {Model {{Agnostic Contrastive Explanations}} for {{Structured Data}}},
  author = {Dhurandhar, Amit and Pedapati, Tejaswini and Balakrishnan, Avinash and Chen, Pin-Yu and Shanmugam, Karthikeyan and Puri, Ruchir},
  year = {2019-05-31},
  eprint = {1906.00117},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1906.00117},
  _urldate = {2020-04-20},
  abstract = {Recently, a method [7] was proposed to generate contrastive explanations for differentiable models such as deep neural networks, where one has complete access to the model. In this work, we propose a method, Model Agnostic Contrastive Explanations Method (MACEM), to generate contrastive explanations for \textbackslash emph\{any\} classification model where one is able to \textbackslash emph\{only\} query the class probabilities for a desired input. This allows us to generate contrastive explanations for not only neural networks, but models such as random forests, boosted trees and even arbitrary ensembles that are still amongst the state-of-the-art when learning on structured data [13]. Moreover, to obtain meaningful explanations we propose a principled approach to handle real and categorical features leading to novel formulations for computing pertinent positives and negatives that form the essence of a contrastive explanation. A detailed treatment of the different data types of this nature was not performed in the previous work, which assumed all features to be positive real valued with zero being indicative of the least interesting value. We part with this strong implicit assumption and generalize these methods so as to be applicable across a much wider range of problem settings. We quantitatively and qualitatively validate our approach over 5 public datasets covering diverse domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@online{dietterichWhatDoesIt2019,
  title = {What Does It Mean for a Machine to “Understand”?},
  author = {Dietterich, Thomas G.},
  year = {2019-10-27T20:01:17},
  __url = {https://medium.com/@tdietterich/what-does-it-mean-for-a-machine-to-understand-555485f3ad40},
  _urldate = {2020-04-20},
  abstract = {Critics of recent advances in artificial intelligence complain that although these advances have produced remarkable improvements in AI…},
  langid = {english},
  organization = {{Medium}}
}

@unpublished{dinhDensityEstimationUsing2017,
  title = {Density Estimation Using {{Real NVP}}},
  author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  year = {2017-02-27},
  eprint = {1605.08803},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1605.08803},
  _urldate = {2021-09-07},
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@incollection{dombrowskiExplanationsCanBe2019,
  title = {Explanations Can Be Manipulated and Geometry Is to Blame},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Dombrowski, Ann-Kathrin and Alber, Maximillian and Anders, Christopher and Ackermann, Marcel and Müller, Klaus-Robert and Kessel, Pan},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d\textbackslash textquotesingle Alché-Buc, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {13589--13600},
  publisher = {{Curran Associates, Inc.}},
  __url = {http://papers.nips.cc/paper/9511-explanations-can-be-manipulated-and-geometry-is-to-blame.pdf},
  _urldate = {2020-10-19}
}

@incollection{dombrowskiExplanationsCanBe2019a,
  title = {Explanations Can Be Manipulated and Geometry Is to Blame},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Dombrowski, Ann-Kathrin and Alber, Maximillian and Anders, Christopher and Ackermann, Marcel and Müller, Klaus-Robert and Kessel, Pan},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d\textbackslash textbackslashtextquotesingle Alché-Buc, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {13589--13600},
  publisher = {{Curran Associates, Inc.}},
  __url = {http://papers.nips.cc/paper/9511-explanations-can-be-manipulated-and-geometry-is-to-blame.pdf},
  _urldate = {2020-10-19}
}

@article{donnatGenericRepresentationRandom2016,
  title = {Toward a Generic Representation of Random Variables for Machine Learning},
  author = {Donnat, Philippe and Marti, Gautier and Very, Philippe},
  year = {2016-01},
  journaltitle = {Pattern Recognition Letters},
  shortjournal = {Pattern Recognition Letters},
  volume = {70},
  pages = {24--31},
  _issn = {01678655},
  _doi = {10.1016/j.patrec.2015.11.004},
  __url = {https://linkinghub.elsevier.com/retrieve/pii/S0167865515003906},
  _urldate = {2022-03-13},
  langid = {english},
  file = {/home/rick/Zotero/storage/NWL5LRF8/Donnat et al. - 2016 - Toward a generic representation of random variable.pdf}
}

@unpublished{doranWhatDoesExplainable2017,
  title = {What {{Does Explainable AI Really Mean}}? {{A New Conceptualization}} of {{Perspectives}}},
  shorttitle = {What {{Does Explainable AI Really Mean}}?},
  author = {Doran, Derek and Schulz, Sarah and Besold, Tarek R.},
  year = {2017-10-02},
  eprint = {1710.00794},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1710.00794},
  _urldate = {2020-04-20},
  abstract = {We characterize three notions of explainable AI that cut across research fields: opaque systems that offer no insight into its algo- rithmic mechanisms; interpretable systems where users can mathemat- ically analyze its algorithmic mechanisms; and comprehensible systems that emit symbols enabling user-driven explanations of how a conclusion is reached. The paper is motivated by a corpus analysis of NIPS, ACL, COGSCI, and ICCV/ECCV paper titles showing differences in how work on explainable AI is positioned in various fields. We close by introducing a fourth notion: truly explainable systems, where automated reasoning is central to output crafted explanations without requiring human post processing as final step of the generative process.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@unpublished{doshi-velezAccountabilityAILaw2019,
  title = {Accountability of {{AI Under}} the {{Law}}: {{The Role}} of {{Explanation}}},
  shorttitle = {Accountability of {{AI Under}} the {{Law}}},
  author = {Doshi-Velez, Finale and Kortz, Mason and Budish, Ryan and Bavitz, Chris and Gershman, Sam and O'Brien, David and Scott, Kate and Schieber, Stuart and Waldo, James and Weinberger, David and Weller, Adrian and Wood, Alexandra},
  year = {2019-12-20},
  eprint = {1711.01134},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1711.01134},
  _urldate = {2020-10-19},
  abstract = {The ubiquity of systems using artificial intelligence or "AI" has brought increasing attention to how those systems should be regulated. The choice of how to regulate AI systems will require care. AI systems have the potential to synthesize large amounts of data, allowing for greater levels of personalization and precision than ever before---applications range from clinical decision support to autonomous driving and predictive policing. That said, there exist legitimate concerns about the intentional and unintentional negative consequences of AI systems. There are many ways to hold AI systems accountable. In this work, we focus on one: explanation. Questions about a legal right to explanation from AI systems was recently debated in the EU General Data Protection Regulation, and thus thinking carefully about when and how explanation from AI systems might improve accountability is timely. In this work, we review contexts in which explanation is currently required under the law, and then list the technical considerations that must be considered if we desired AI systems that could provide kinds of explanations that are currently required of humans.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Statistics - Machine Learning}
}

@article{doshi-velezRigorousScienceInterpretable2017,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  author = {Doshi-Velez, Finale and Kim, Been},
  year = {2017},
  journal={arXiv preprint arXiv:1702.08608},
  eprint = {1702.08608},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1702.08608},
  __urldate = {2020-04-21},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{dosilovicExplainableArtificialIntelligence2018,
  title = {Explainable Artificial Intelligence: {{A}} Survey},
  shorttitle = {Explainable Artificial Intelligence},
  booktitle = {2018 41st {{International Convention}} on {{Information}} and {{Communication Technology}}, {{Electronics}} and {{Microelectronics}} ({{MIPRO}})},
  author = {Došilović, Filip Karlo and Brčić, Mario and Hlupić, Nikica},
  year = {2018-05},
  pages = {0210--0215},
  _doi = {10.23919/MIPRO.2018.8400040},
  abstract = {In the last decade, with availability of large datasets and more computing power, machine learning systems have achieved (super)human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis, strategic game planning and many more. The problem with many state-of-the-art models is a lack of transparency and interpretability. The lack of thereof is a major drawback in many applications, e.g. healthcare and finance, where rationale for model's decision is a requirement for trust. In the light of these issues, explainable artificial intelligence (XAI) has become an area of interest in research community. This paper summarizes recent developments in XAI in supervised learning, starts a discussion on its connection with artificial general intelligence, and gives proposals for further research directions.},
  eventtitle = {2018 41st {{International Convention}} on {{Information}} and {{Communication Technology}}, {{Electronics}} and {{Microelectronics}} ({{MIPRO}})},
  keywords = {(super)human performance,artificial general intelligence,comprehensibility,computing power,datasets,Decision trees,explainability,explainable artificial intelligence,finance,healthcare,image recognition,interpretability,learning (artificial intelligence),Machine learning,machine learning systems,Optimization,Predictive models,recent developments,speech analysis,state-of-the-art models,strategic game planning,supervised learning,Supervised learning,Support vector machines,transparency,XAI}
}

@unpublished{dziugaiteEnforcingInterpretabilityIts2020,
  title = {Enforcing {{Interpretability}} and Its {{Statistical Impacts}}: {{Trade-offs}} between {{Accuracy}} and {{Interpretability}}},
  shorttitle = {Enforcing {{Interpretability}} and Its {{Statistical Impacts}}},
  author = {Dziugaite, Gintare Karolina and Ben-David, Shai and Roy, Daniel M.},
  year = {2020-10-28},
  eprint = {2010.13764},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2010.13764},
  _urldate = {2020-11-30},
  abstract = {To date, there has been no formal study of the statistical cost of interpretability in machine learning. As such, the discourse around potential trade-offs is often informal and misconceptions abound. In this work, we aim to initiate a formal study of these trade-offs. A seemingly insurmountable roadblock is the lack of any agreed upon definition of interpretability. Instead, we propose a shift in perspective. Rather than attempt to define interpretability, we propose to model the \textbackslash emph\{act\} of \textbackslash emph\{enforcing\} interpretability. As a starting point, we focus on the setting of empirical risk minimization for binary classification, and view interpretability as a constraint placed on learning. That is, we assume we are given a subset of hypothesis that are deemed to be interpretable, possibly depending on the data distribution and other aspects of the context. We then model the act of enforcing interpretability as that of performing empirical risk minimization over the set of interpretable hypotheses. This model allows us to reason about the statistical implications of enforcing interpretability, using known results in statistical learning theory. Focusing on accuracy, we perform a case analysis, explaining why one may or may not observe a trade-off between accuracy and interpretability when the restriction to interpretable classifiers does or does not come at the cost of some excess statistical risk. We close with some worked examples and some open problems, which we hope will spur further theoretical development around the tradeoffs involved in interpretability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{ebrahimiRememberingRightReasons2020,
  title = {Remembering for the {{Right Reasons}}: {{Explanations Reduce Catastrophic Forgetting}}},
  shorttitle = {Remembering for the {{Right Reasons}}},
  author = {Ebrahimi, Sayna and Petryk, Suzanne and Gokul, Akash and Gan, William and Gonzalez, Joseph E. and Rohrbach, Marcus and Darrell, Trevor},
  year = {2020-10-04},
  eprint = {2010.01528},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2010.01528},
  _urldate = {2020-11-30},
  abstract = {The goal of continual learning (CL) is to learn a sequence of tasks without suffering from the phenomenon of catastrophic forgetting. Previous work has shown that leveraging memory in the form of a replay buffer can reduce performance degradation on prior tasks. We hypothesize that forgetting can be further reduced when the model is encouraged to remember the \textbackslash textit\{evidence\} for previously made decisions. As a first step towards exploring this hypothesis, we propose a simple novel training paradigm, called Remembering for the Right Reasons (RRR), that additionally stores visual model explanations for each example in the buffer and ensures the model has "the right reasons" for its predictions by encouraging its explanations to remain consistent with those used to make decisions at training time. Without this constraint, there is a drift in explanations and increase in forgetting as conventional continual learning algorithms learn new tasks. We demonstrate how RRR can be easily added to any memory or regularization-based approach and results in reduced forgetting, and more importantly, improved model explanations. We have evaluated our approach in the standard and few-shot settings and observed a consistent improvement across various CL approaches using different architectures and techniques to generate model explanations and demonstrated our approach showing a promising connection between explainability and continual learning. Our code is available at https://github.com/SaynaEbrahimi/Remembering-for-the-Right-Reasons.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@inproceedings{elbekriStudyTrustBlack2020,
  title = {A {{Study}} on {{Trust}} in {{Black Box Models}} and {{Post-hoc Explanations}}},
  booktitle = {14th {{International Conference}} on {{Soft Computing Models}} in {{Industrial}} and {{Environmental Applications}} ({{SOCO}} 2019)},
  author = {El Bekri, Nadia and Kling, Jasmin and Huber, Marco F.},
  editor = {Martínez Álvarez, Francisco and Troncoso Lora, Alicia and Sáez Muñoz, José António and Quintián, Héctor and Corchado, Emilio},
  year = {2020},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {35--46},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  _doi = {10.1007/978-3-030-20055-8_4},
  abstract = {Machine learning algorithms that construct complex prediction models are increasingly used for decision-making due to their high accuracy, e.g., to decide whether a bank customer should receive a loan or not. Due to the complexity, the models are perceived as black boxes. One approach is to augment the models with post-hoc explainability. In this work, we evaluate three different explanation approaches based on the users’ initial trust, the users’ trust in the provided explanation, and the established trust in the black box by a within-subject design study.},
  _isbn = {978-3-030-20055-8},
  langid = {english},
  keywords = {Black box,Explainability,Interpretability,Machine learning,Trust}
}

@unpublished{federCausalInferenceNatural2021,
  title = {Causal {{Inference}} in {{Natural Language Processing}}: {{Estimation}}, {{Prediction}}, {{Interpretation}} and {{Beyond}}},
  shorttitle = {Causal {{Inference}} in {{Natural Language Processing}}},
  author = {Feder, Amir and Keith, Katherine A. and Manzoor, Emaad and Pryzant, Reid and Sridhar, Dhanya and Wood-Doughty, Zach and Eisenstein, Jacob and Grimmer, Justin and Reichart, Roi and Roberts, Margaret E. and Stewart, Brandon M. and Veitch, Victor and Yang, Diyi},
  year = {2021-09-02},
  eprint = {2109.00725},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2109.00725},
  _urldate = {2021-09-17},
  abstract = {A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the remaining challenges. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects, encompassing settings where text is used as an outcome, treatment, or as a means to address confounding. In addition, we explore potential uses of causal inference to improve the performance, robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the computational linguistics community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{fengAutoEncoderForest2018,
  title = {{{AutoEncoder}} by {{Forest}}},
  author = {Feng, Ji and Zhou, Zhi-Hua},
  year = {2018},
  journaltitle = {AAAI},
  eprint = {1709.09018},
  eprinttype = {arxiv},
  __url = {http://arxiv.org/abs/1709.09018},
  _urldate = {2020-06-10},
  abstract = {Auto-encoding is an important task which is typically realized by deep neural networks (DNNs) such as convolutional neural networks (CNN). In this paper, we propose EncoderForest (abbrv. eForest), the first tree ensemble based auto-encoder. We present a procedure for enabling forests to do backward reconstruction by utilizing the equivalent classes defined by decision paths of the trees, and demonstrate its usage in both supervised and unsupervised setting. Experiments show that, compared with DNN autoencoders, eForest is able to obtain lower reconstruction error with fast training speed, while the model itself is reusable and damage-tolerable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{fengWhatCanAI2019,
  title = {What Can {{AI}} Do for Me: {{Evaluating Machine Learning Interpretations}} in {{Cooperative Play}}},
  shorttitle = {What Can {{AI}} Do for Me},
  author = {Feng, Shi and Boyd-Graber, Jordan},
  year = {2019-06-09},
  eprint = {1810.09648},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1810.09648},
  _urldate = {2020-10-13},
  abstract = {Machine learning is an important tool for decision making, but its ethical and responsible application requires rigorous vetting of its interpretability and utility: an understudied problem, particularly for natural language processing models. We propose an evaluation of interpretation on a real task with real human users, where the effectiveness of interpretation is measured by how much it improves human performance. We design a grounded, realistic human-computer cooperative setting using a question answering task, Quizbowl. We recruit both trivia experts and novices to play this game with computer as their teammate, who communicates its prediction via three different interpretations. We also provide design guidance for natural language processing human-in-the-loop settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@article{fisherAllModelsAre2019,
  title = {All {{Models}} Are {{Wrong}}, but {{Many}} Are {{Useful}}: {{Learning}} a {{Variable}}'s {{Importance}} by {{Studying}} an {{Entire Class}} of {{Prediction Models Simultaneously}}},
  shorttitle = {All {{Models}} Are {{Wrong}}, but {{Many}} Are {{Useful}}},
  author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
  year = {2019},
  journaltitle = {Journal of Machine Learning Research},
  journal = {Journal of Machine Learning Research},
  volume = {20},
  number = {177},
  pages = {1--81},
  _issn = {1533-7928},
  __url = {http://jmlr.org/papers/v20/18-760.html},
  _urldate = {2021-01-15}
}

@article{flachPrecisionrecallgainCurvesPR2015,
  title = {Precision-Recall-Gain Curves: {{PR}} Analysis Done Right},
  author = {Flach, Peter and Kull, Meelis},
  year = {2015},
  journaltitle = {Advances in neural information processing systems},
  volume = {28},
  pages = {838--846}
}

@unpublished{fordPlayMNISTMe2020,
  title = {Play {{MNIST For Me}}! {{User Studies}} on the {{Effects}} of {{Post-Hoc}}, {{Example-Based Explanations}} \& {{Error Rates}} on {{Debugging}} a {{Deep Learning}}, {{Black-Box Classifier}}},
  author = {Ford, Courtney and Kenny, Eoin M. and Keane, Mark T.},
  year = {2020-09-10},
  eprint = {2009.06349},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2009.06349},
  _urldate = {2020-11-30},
  abstract = {This paper reports two experiments (N=349) on the impact of post hoc explanations by example and error rates on peoples perceptions of a black box classifier. Both experiments show that when people are given case based explanations, from an implemented ANN CBR twin system, they perceive miss classifications to be more correct. They also show that as error rates increase above 4\%, people trust the classifier less and view it as being less correct, less reasonable and less trustworthy. The implications of these results for XAI are discussed.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,F.2.2,I.2.8}
}

@article{friedmanGraphicalViewsSuppression2005,
  title = {Graphical {{Views}} of {{Suppression}} and {{Multicollinearity}} in {{Multiple Linear Regression}}},
  author = {Friedman, Lynn and Wall, Melanie},
  year = {2005},
  journaltitle = {The American Statistician},
  journal = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {59},
  number = {2},
  pages = {127--136},
  _issn = {0003-1305, 1537-2731},
  _doi = {10.1198/000313005X41337},
  __url = {http://www.tandfonline.com/doi/abs/10.1198/000313005X41337},
  _urldate = {2022-04-22},
  langid = {english}
}

@article{friedmanGreedyFunctionApproximation2001,
  title = {Greedy {{Function Approximation}}: {{A Gradient Boosting Machine}}},
  shorttitle = {Greedy {{Function Approximation}}},
  author = {Friedman, Jerome H.},
  year = {2001},
  journaltitle = {The Annals of Statistics},
  journal = {The Annals of Statistics},
  volume = {29},
  number = {5},
  eprint = {2699986},
  eprinttype = {jstor},
  pages = {1189--1232},
  _issn = {0090-5364},
  abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.}
}

@article{friedmanMultivariateAdaptiveRegression1991,
  title = {Multivariate Adaptive Regression Splines},
  author = {Friedman, Jerome H},
  year = {1991},
  journaltitle = {The annals of statistics},
  pages = {1--67},
  publisher = {{JSTOR}}
}

@article{friedmanPathsConsistencyAdditive2004,
  title = {Paths and Consistency in Additive Cost Sharing},
  author = {Friedman, Eric J.},
  year = {2004},
  journaltitle = {International Journal of Games Theory},
  journal = {International Journal of Games Theory},
  shortjournal = {Int J Game Theory},
  volume = {32},
  number = {4},
  _issn = {0020-7276, 1432-1270},
  _doi = {10.1007/s001820400173},
  __url = {http://link.springer.com/10.1007/s001820400173},
  _urldate = {2022-04-25},
  langid = {english},
  file = {/home/rick/Zotero/storage/4A86X2TU/Friedman - 2004 - Paths and consistency in additive cost sharing.pdf}
}

@article{friedmanPredictiveLearningRule2008,
  title = {Predictive Learning via Rule Ensembles},
  author = {Friedman, Jerome H. and Popescu, Bogdan E.},
  year = {2008-09},
  journaltitle = {The Annals of Applied Statistics},
  shortjournal = {Ann. Appl. Stat.},
  volume = {2},
  number = {3},
  eprint = {0811.1679},
  eprinttype = {arxiv},
  pages = {916--954},
  _issn = {1932-6157},
  _doi = {10.1214/07-AOAS148},
  __url = {http://arxiv.org/abs/0811.1679},
  _urldate = {2020-05-18},
  abstract = {General regression and classification models are constructed as linear combinations of simple rules derived from the data. Each rule consists of a conjunction of a small number of simple statements concerning the values of individual input variables. These rule ensembles are shown to produce predictive accuracy comparable to the best methods. However, their principal advantage lies in interpretation. Because of its simple form, each rule is easy to understand, as is its influence on individual predictions, selected subsets of predictions, or globally over the entire space of joint input variable values. Similarly, the degree of relevance of the respective input variables can be assessed globally, locally in different regions of the input space, or at individual prediction points. Techniques are presented for automatically identifying those variables that are involved in interactions with other variables, the strength and degree of those interactions, as well as the identities of the other variables with which they interact. Graphical representations are used to visualize both main and interaction effects.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications}
}

@unpublished{fryeAsymmetricShapleyValues2020,
  title = {Asymmetric {{Shapley}} Values: Incorporating Causal Knowledge into Model-Agnostic Explainability},
  shorttitle = {Asymmetric {{Shapley}} Values},
  author = {Frye, Christopher and Rowat, Colin and Feige, Ilya},
  year = {2020-10-21},
  eprint = {1910.06358},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1910.06358},
  _urldate = {2021-05-12},
  abstract = {Explaining AI systems is fundamental both to the development of high performing models and to the trust placed in them by their users. The Shapley framework for explainability has strength in its general applicability combined with its precise, rigorous foundation: it provides a common, model-agnostic language for AI explainability and uniquely satisfies a set of intuitive mathematical axioms. However, Shapley values are too restrictive in one significant regard: they ignore all causal structure in the data. We introduce a less restrictive framework, Asymmetric Shapley values (ASVs), which are rigorously founded on a set of axioms, applicable to any AI system, and flexible enough to incorporate any causal structure known to be respected by the data. We demonstrate that ASVs can (i) improve model explanations by incorporating causal information, (ii) provide an unambiguous test for unfair discrimination in model predictions, (iii) enable sequentially incremental explanations in time-series models, and (iv) support feature-selection studies without the need for model retraining.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/742YJWFQ/Frye et al. - 2020 - Asymmetric Shapley values incorporating causal kn.pdf;/home/rick/Zotero/storage/5C2PR934/1910.html}
}

@unpublished{fryerExplainingDataExplaining2020,
  title = {Explaining the Data or Explaining a Model? {{Shapley}} Values That Uncover Non-Linear Dependencies},
  shorttitle = {Explaining the Data or Explaining a Model?},
  author = {Fryer, Daniel Vidali and Strümke, Inga and Nguyen, Hien},
  year = {2020-07-28},
  eprint = {2007.06011},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.06011},
  _urldate = {2020-08-05},
  abstract = {Shapley values have become increasingly popular in the machine learning literature thanks to their attractive axiomatisation, flexibility, and uniqueness in satisfying certain notions of `fairness'. The flexibility arises from the myriad potential forms of the Shapley value \textbackslash textit\{game formulation\}. Amongst the consequences of this flexibility is that there are now many types of Shapley values being discussed, with such variety being a source of potential misunderstanding. To the best of our knowledge, all existing game formulations in the machine learning and statistics literature fall into a category which we name the model-dependent category of game formulations. In this work, we consider an alternative and novel formulation which leads to the first instance of what we call model-independent Shapley values. These Shapley values use a (non-parametric) measure of non-linear dependence as the characteristic function. The strength of these Shapley values is in their ability to uncover and attribute non-linear dependencies amongst features. We introduce and demonstrate the use of the energy distance correlations, affine-invariant distance correlation, and Hilbert-Shmidt independence criterion as Shapley value characteristic functions. In particular, we demonstrate their potential value for exploratory data analysis and model diagnostics. We conclude with an interesting expository application to a classical medical survey data set.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{fryerExplainingDataExplaining2021,
  title = {Explaining the Data or Explaining a Model? {{Shapley}} Values That Uncover Non-Linear Dependencies},
  shorttitle = {Explaining the Data or Explaining a Model?},
  author = {Fryer, Daniel Vidali and Strümke, Inga and Nguyen, Hien},
  year = {2021-03-06},
  eprint = {2007.06011},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.06011},
  _urldate = {2021-05-12},
  abstract = {Shapley values have become increasingly popular in the machine learning literature thanks to their attractive axiomatisation, flexibility, and uniqueness in satisfying certain notions of `fairness'. The flexibility arises from the myriad potential forms of the Shapley value \textbackslash textit\{game formulation\}. Amongst the consequences of this flexibility is that there are now many types of Shapley values being discussed, with such variety being a source of potential misunderstanding. To the best of our knowledge, all existing game formulations in the machine learning and statistics literature fall into a category which we name the model-dependent category of game formulations. In this work, we consider an alternative and novel formulation which leads to the first instance of what we call model-independent Shapley values. These Shapley values use a (non-parametric) measure of non-linear dependence as the characteristic function. The strength of these Shapley values is in their ability to uncover and attribute non-linear dependencies amongst features. We introduce and demonstrate the use of the energy distance correlations, affine-invariant distance correlation, and Hilbert-Shmidt independence criterion as Shapley value characteristic functions. In particular, we demonstrate their potential value for exploratory data analysis and model diagnostics. We conclude with an interesting expository application to a classical medical survey data set.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/A7JAQMMG/Fryer et al. - 2021 - Explaining the data or explaining a model Shapley.pdf;/home/rick/Zotero/storage/55RIBUCI/2007.html}
}

@unpublished{fryeShapleyExplainabilityData2021,
  title = {Shapley Explainability on the Data Manifold},
  author = {Frye, Christopher and de Mijolla, Damien and Begley, Tom and Cowton, Laurence and Stanley, Megan and Feige, Ilya},
  options = {useprefix=true},
  year = {2021-02-22},
  eprint = {2006.01272},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2006.01272},
  _urldate = {2021-11-16},
  abstract = {Explainability in AI is crucial for model development, compliance with regulation, and providing operational nuance to predictions. The Shapley framework for explainability attributes a model's predictions to its input features in a mathematically principled and model-agnostic way. However, general implementations of Shapley explainability make an untenable assumption: that the model's features are uncorrelated. In this work, we demonstrate unambiguous drawbacks of this assumption and develop two solutions to Shapley explainability that respect the data manifold. One solution, based on generative modelling, provides flexible access to data imputations; the other directly learns the Shapley value-function, providing performance and stability at the cost of flexibility. While "off-manifold" Shapley values can (i) give rise to incorrect explanations, (ii) hide implicit model dependence on sensitive attributes, and (iii) lead to unintelligible explanations in higher-dimensional data, on-manifold explainability overcomes these problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/YLCRHHYI/Frye et al. - 2021 - Shapley explainability on the data manifold.pdf;/home/rick/Zotero/storage/UZDIWETT/2006.html}
}

@unpublished{fuAxiombasedGradCAMAccurate2020,
  title = {Axiom-Based {{Grad-CAM}}: {{Towards Accurate Visualization}} and {{Explanation}} of {{CNNs}}},
  shorttitle = {Axiom-Based {{Grad-CAM}}},
  author = {Fu, Ruigang and Hu, Qingyong and Dong, Xiaohu and Guo, Yulan and Gao, Yinghui and Li, Biao},
  year = {2020-08-19},
  eprint = {2008.02312},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  __url = {http://arxiv.org/abs/2008.02312},
  _urldate = {2020-09-16},
  abstract = {To have a better understanding and usage of Convolution Neural Networks (CNNs), the visualization and interpretation of CNNs has attracted increasing attention in recent years. In particular, several Class Activation Mapping (CAM) methods have been proposed to discover the connection between CNN's decision and image regions. In spite of the reasonable visualization, lack of clear and sufficient theoretical support is the main limitation of these methods. In this paper, we introduce two axioms -- Conservation and Sensitivity -- to the visualization paradigm of the CAM methods. Meanwhile, a dedicated Axiom-based Grad-CAM (XGrad-CAM) is proposed to satisfy these axioms as much as possible. Experiments demonstrate that XGrad-CAM is an enhanced version of Grad-CAM in terms of conservation and sensitivity. It is able to achieve better visualization performance than Grad-CAM, while also be class-discriminative and easy-to-implement compared with Grad-CAM++ and Ablation-CAM. The code is available at https://github.com/Fu0511/XGrad-CAM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing}
}

@unpublished{fuxinHeatmapsStructuralExplanations2021,
  title = {From {{Heatmaps}} to {{Structural Explanations}} of {{Image Classifiers}}},
  author = {Fuxin, Li and Qi, Zhongang and Khorram, Saeed and Shitole, Vivswan and Tadepalli, Prasad and Kahng, Minsuk and Fern, Alan},
  year = {2021-09-13},
  eprint = {2109.06365},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2109.06365},
  _urldate = {2021-09-17},
  abstract = {This paper summarizes our endeavors in the past few years in terms of explaining image classifiers, with the aim of including negative results and insights we have gained. The paper starts with describing the explainable neural network (XNN), which attempts to extract and visualize several high-level concepts purely from the deep network, without relying on human linguistic concepts. This helps users understand network classifications that are less intuitive and substantially improves user performance on a difficult fine-grained classification task of discriminating among different species of seagulls. Realizing that an important missing piece is a reliable heatmap visualization tool, we have developed I-GOS and iGOS++ utilizing integrated gradients to avoid local optima in heatmap generation, which improved the performance across all resolutions. During the development of those visualizations, we realized that for a significant number of images, the classifier has multiple different paths to reach a confident prediction. This has lead to our recent development of structured attention graphs (SAGs), an approach that utilizes beam search to locate multiple coarse heatmaps for a single image, and compactly visualizes a set of heatmaps by capturing how different combinations of image regions impact the confidence of a classifier. Through the research process, we have learned much about insights in building deep network explanations, the existence and frequency of multiple explanations, and various tricks of the trade that make explanations work. In this paper, we attempt to share those insights and opinions with the readers with the hope that some of them will be informative for future researchers on explainable deep learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@unpublished{ganeshCWarsUnfoldingArgument2020,
  title = {C-{{Wars}}: {{The Unfolding Argument Strikes Back}} -- {{A Reply}} to '{{Falsification}} \& {{Consciousness}}'},
  shorttitle = {C-{{Wars}}},
  author = {Ganesh, Natesh},
  year = {2020-05-28},
  eprint = {2006.13664},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2006.13664},
  _urldate = {2020-10-08},
  abstract = {The 'unfolding argument' was presented by Doerig et.al. [1] as an argument to show that causal structure theories (CST) like IIT are either falsified or outside the realm of science. In their recent paper [2],[3], the authors mathematically formalized the process of generating observable data from experiments and using that data to generate inferences and predictions onto an experience space. The resulting `substitution argument built on this formal framework was used to show that all existing theories of consciousness were 'pre-falsified' if the inference reports are valid. If this argument is indeed correct, it would have a profound effect on the field of consciousness as a whole indicating extremely fundamental problems that would require radical changes to how consciousness science is performed. However in this note the author identifies the shortcomings in the formulation of the substitution argument and explains why it's claims about functionalist theories are wrong.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Other Computer Science}
}

@inproceedings{garcia-torresScatterSearchHighdimensional2021,
  title = {Scatter Search for High-Dimensional Feature Selection Using Feature Grouping},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference Companion}}},
  author = {García-Torres, Miguel and Gómez-Vela, Francisco and Divina, Federico and Pinto-Roa, Diego P. and Noguera, José Luis Vázquez and Román, Julio C. Mello},
  year = {2021-07-07},
  pages = {149--150},
  publisher = {{ACM}},
  location = {{Lille France}},
  _doi = {10.1145/3449726.3459481},
  __url = {https://dl.acm.org/doi/10.1145/3449726.3459481},
  _urldate = {2022-04-04},
  eventtitle = {{{GECCO}} '21: {{Genetic}} and {{Evolutionary Computation Conference}}},
  _isbn = {978-1-4503-8351-6},
  langid = {english}
}

@conference{garreauExplainingExplainerFirst2020,
  title = {Explaining the Explainer: A First Theoretical Analysis of LIME},
  author = {Garreau, D. and von Luxburg, U.},
  booktitle = {Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS)},
  volume = {108},
  pages = {1287--1296},
  series = {Proceedings of Machine Learning Research},
  editors = {Silvia Chiappa and Roberto Calandra},
  publisher = {PMLR},
  month = aug,
  year = {2020},
  _doi = {},
  __url = {http://proceedings.mlr.press/v108/garreau20a.html},
  month_numeric = {8}
}

@unpublished{garreauLookingDeeperTabular2020,
  title = {Looking {{Deeper}} into {{Tabular LIME}}},
  author = {Garreau, Damien and von Luxburg, Ulrike},
  options = {useprefix=true},
  year = {2020-09-18},
  eprint = {2008.11092},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2008.11092},
  _urldate = {2020-10-30},
  abstract = {Interpretability of machine learning algorithms is an urgent need. Numerous methods appeared in recent years, but do their explanations make sense? In this paper, we present a thorough theoretical analysis of one of these methods, LIME, in the case of tabular data. We prove that in the large sample limit, the interpretable coefficients provided by Tabular LIME can be computed in an explicit way as a function of the algorithm parameters and some expectation computations related to the black-box model. When the function to explain has some nice algebraic structure (linear, multiplicative, or sparsely depending on a subset of the coordinates), our analysis provides interesting insights into the explanations provided by LIME. These can be applied to a range of machine learning models including Gaussian kernels or CART random forests. As an example, for linear functions we show that LIME has the desirable property to provide explanations that are proportional to the coefficients of the function to explain and to ignore coordinates that are not used by the function to explain. For partition-based regressors, on the other side, we show that LIME produces undesired artifacts that may provide misleading explanations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{geirhosShortcutLearningDeep2020,
  title = {Shortcut {{Learning}} in {{Deep Neural Networks}}},
  author = {Geirhos, Robert and Jacobsen, Jörn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
  year = {2020-11},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {2},
  number = {11},
  eprint = {2004.07780},
  eprinttype = {arxiv},
  pages = {665--673},
  _issn = {2522-5839},
  _doi = {10.1038/s42256-020-00257-z},
  __url = {http://arxiv.org/abs/2004.07780},
  _urldate = {2021-12-03},
  abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distil how many of deep learning's problem can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/home/rick/Zotero/storage/PVTFD99E/Geirhos et al. - 2020 - Shortcut Learning in Deep Neural Networks.pdf;/home/rick/Zotero/storage/Z25TFIHE/2004.html}
}

@article{GeneratingRandomCorrelation2009,
  title = {Generating Random Correlation Matrices Based on Vines and Extended Onion Method},
  year = {2009-10-01},
  journaltitle = {Journal of Multivariate Analysis},
  volume = {100},
  number = {9},
  pages = {1989--2001},
  _issn = {0047-259X},
  _doi = {10.1016/j.jmva.2009.04.008},
  __url = {https://www.sciencedirect.com/science/article/pii/S0047259X09000876},
  _urldate = {2020-10-28},
  abstract = {We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderso…},
  langid = {english}
}

@unpublished{genzelMismatchPrincipleGeneralized2019,
  title = {The {{Mismatch Principle}}: {{The Generalized Lasso Under Large Model Uncertainties}}},
  shorttitle = {The {{Mismatch Principle}}},
  author = {Genzel, Martin and Kutyniok, Gitta},
  year = {2019-09-11},
  eprint = {1808.06329},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  __url = {http://arxiv.org/abs/1808.06329},
  _urldate = {2020-04-20},
  abstract = {We study the estimation capacity of the generalized Lasso, i.e., least squares minimization combined with a (convex) structural constraint. While Lasso-type estimators were originally designed for noisy linear regression problems, it has recently turned out that they are in fact robust against various types of model uncertainties and misspecifications, most notably, non-linearly distorted observation models. This work provides more theoretical evidence for this somewhat astonishing phenomenon. At the heart of our analysis stands the mismatch principle, which is a simple recipe to establish theoretical error bounds for the generalized Lasso. The associated estimation guarantees are of independent interest and are formulated in a fairly general setup, permitting arbitrary sub-Gaussian data, possibly with strongly correlated feature designs; in particular, we do not assume a specific observation model which connects the input and output variables. Although the mismatch principle is conceived based on ideas from statistical learning theory, its actual application area are (high-dimensional) estimation tasks for semi-parametric models. In this context, the benefits of the mismatch principle are demonstrated for a variety of popular problem classes, such as single-index models, generalized linear models, and variable selection. Apart from that, our findings are also relevant to recent advances in quantized and distributed compressed sensing.},
  archiveprefix = {arXiv},
  version = {2},
  keywords = {68T37; 60D05; 90C25; 62F30; 62F35,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@article{gettierJustifiedTrueBelief1963,
  title = {Is {{Justified True Belief Knowledge}}?},
  author = {Gettier, Edmund L.},
  year = {1963},
  journaltitle = {Analysis},
  journal = {Analysis},
  volume = {23},
  number = {6},
  eprint = {3326922},
  eprinttype = {jstor},
  pages = {121--123},
  publisher = {{[Analysis Committee, Oxford University Press]}},
  _issn = {00032638, 14678284}
}

@unpublished{ghojoghUniformManifoldApproximation2021,
  title = {Uniform {{Manifold Approximation}} and {{Projection}} ({{UMAP}}) and Its {{Variants}}: {{Tutorial}} and {{Survey}}},
  shorttitle = {Uniform {{Manifold Approximation}} and {{Projection}} ({{UMAP}}) and Its {{Variants}}},
  author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
  year = {2021-08-24},
  eprint = {2109.02508},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2109.02508},
  _urldate = {2021-11-06},
  abstract = {Uniform Manifold Approximation and Projection (UMAP) is one of the state-of-the-art methods for dimensionality reduction and data visualization. This is a tutorial and survey paper on UMAP and its variants. We start with UMAP algorithm where we explain probabilities of neighborhood in the input and embedding spaces, optimization of cost function, training algorithm, derivation of gradients, and supervised and semi-supervised embedding by UMAP. Then, we introduce the theory behind UMAP by algebraic topology and category theory. Then, we introduce UMAP as a neighbor embedding method and compare it with t-SNE and LargeVis algorithms. We discuss negative sampling and repulsive forces in UMAP's cost function. DensMAP is then explained for density-preserving embedding. We then introduce parametric UMAP for embedding by deep learning and progressive UMAP for streaming and out-of-sample data embedding.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/home/rick/Zotero/storage/MBR4KJV5/Ghojogh et al. - 2021 - Uniform Manifold Approximation and Projection (UMA.pdf;/home/rick/Zotero/storage/A9CWN6VB/2109.html}
}

@incollection{ghorbaniAutomaticConceptbasedExplanations2019,
  title = {Towards {{Automatic Concept-based Explanations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Ghorbani, Amirata and Wexler, James and Zou, James Y and Kim, Been},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d\textbackslash textquotesingle Alché-Buc, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {9277--9286},
  publisher = {{Curran Associates, Inc.}},
  __url = {http://papers.nips.cc/paper/9126-towards-automatic-concept-based-explanations.pdf},
  _urldate = {2020-10-22}
}

@unpublished{ghorbaniDataShapleyEquitable2019,
  title = {Data {{Shapley}}: {{Equitable Valuation}} of {{Data}} for {{Machine Learning}}},
  shorttitle = {Data {{Shapley}}},
  author = {Ghorbani, Amirata and Zou, James},
  year = {2019-06-10},
  eprint = {1904.02868},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1904.02868},
  _urldate = {2020-10-19},
  abstract = {As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on \$n\$ data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{ghorbaniInterpretationNeuralNetworks2019,
  title = {Interpretation of {{Neural Networks Is Fragile}}},
  author = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
  year = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  number = {01},
  pages = {3681--3688},
  _issn = {2374-3468},
  _doi = {10.1609/aaai.v33i01.33013681},
  __url = {https://ojs.aaai.org/index.php/AAAI/article/view/4252},
  _urldate = {2020-12-21},
  langid = {english}
}

@article{ghorbaniInterpretationNeuralNetworks2019a,
  title = {Interpretation of {{Neural Networks Is Fragile}}},
  author = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
  year = {2019-07},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {3681--3688},
  _issn = {2374-3468},
  _doi = {10.1609/aaai.v33i01.33013681},
  __url = {https://ojs.aaai.org/index.php/AAAI/article/view/4252},
  _urldate = {2020-12-21},
  langid = {english}
}

@unpublished{gilpinExplainingExplanationsOverview2019,
  title = {Explaining {{Explanations}}: {{An Overview}} of {{Interpretability}} of {{Machine Learning}}},
  shorttitle = {Explaining {{Explanations}}},
  author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  year = {2019-02-03},
  eprint = {1806.00069},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1806.00069},
  _urldate = {2020-04-20},
  abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
  archiveprefix = {arXiv},
  version = {3},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{gimenezKnockoffsMassNew2019,
  title = {Knockoffs for the Mass: New Feature Importance Statistics with False Discovery Guarantees},
  shorttitle = {Knockoffs for the Mass},
  author = {Gimenez, Jaime Roquero and Ghorbani, Amirata and Zou, James},
  year = {2019-05-28},
  eprint = {1807.06214},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1807.06214},
  _urldate = {2020-12-22},
  abstract = {An important problem in machine learning and statistics is to identify features that causally affect the outcome. This is often impossible to do from purely observational data, and a natural relaxation is to identify features that are correlated with the outcome even conditioned on all other observed features. For example, we want to identify that smoking really is correlated with cancer conditioned on demographics. The knockoff procedure is a recent breakthrough in statistics that, in theory, can identify truly correlated features while guaranteeing that the false discovery is limited. The idea is to create synthetic data -- knockoffs -- that captures correlations amongst the features. However there are substantial computational and practical challenges to generating and using knockoffs. This paper makes several key advances that enable knockoff application to be more efficient and powerful. We develop an efficient algorithm to generate valid knockoffs from Bayesian Networks. Then we systematically evaluate knockoff test statistics and develop new statistics with improved power. The paper combines new mathematical guarantees with systematic experiments on real and synthetic data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{goebelExplainableAINew2018,
  title = {Explainable {{AI}}: {{The New}} 42?},
  shorttitle = {Explainable {{AI}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  author = {Goebel, Randy and Chander, Ajay and Holzinger, Katharina and Lecue, Freddy and Akata, Zeynep and Stumpf, Simone and Kieseberg, Peter and Holzinger, Andreas},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {295--303},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  _doi = {10.1007/978-3-319-99740-7_21},
  abstract = {Explainable AI is not a new field. Since at least the early exploitation of C.S. Pierce’s abductive reasoning in expert systems of the 1980s, there were reasoning architectures to support an explanation function for complex AI systems, including applications in medical diagnosis, complex multi-component design, and reasoning about the real world. So explainability is at least as old as early AI, and a natural consequence of the design of AI systems. While early expert systems consisted of handcrafted knowledge bases that enabled reasoning over narrowly well-defined domains (e.g., INTERNIST, MYCIN), such systems had no learning capabilities and had only primitive uncertainty handling. But the evolution of formal reasoning architectures to incorporate principled probabilistic reasoning helped address the capture and use of uncertain knowledge.There has been recent and relatively rapid success of AI/machine learning solutions arises from neural network architectures. A new generation of neural methods now scale to exploit the practical applicability of statistical and algebraic learning approaches in arbitrarily high dimensional spaces. But despite their huge successes, largely in problems which can be cast as classification problems, their effectiveness is still limited by their un-debuggability, and their inability to “explain” their decisions in a human understandable and reconstructable way. So while AlphaGo or DeepStack can crush the best humans at Go or Poker, neither program has any internal model of its task; its representations defy interpretation by humans, there is no mechanism to explain their actions and behaviour, and furthermore, there is no obvious instructional value ... the high performance systems can not help humans improve.Even when we understand the underlying mathematical scaffolding of current machine learning architectures, it is often impossible to get insight into the internal working of the models; we need explicit modeling and reasoning tools to explain how and why a result was achieved. We also know that a significant challenge for future AI is contextual adaptation, i.e., systems that incrementally help to construct explanatory models for solving real-world problems. Here it would be beneficial not to exclude human expertise, but to augment human intelligence with artificial intelligence.},
  _isbn = {978-3-319-99740-7},
  langid = {english},
  keywords = {Artificial intelligence,Explainability,Explainable AI,Machine learning}
}

@article{goldsteinPeekingBlackBox2014,
author = {Alex Goldstein and Adam Kapelner and Justin Bleich and Emil Pitkin},
title = {Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation},
journal = {Journal of Computational and Graphical Statistics},
volume = {24},
number = {1},
pages = {44-65},
year  = {2015},
publisher = {Taylor & Francis},
_doi = {10.1080/10618600.2014.907095},

__url = { 
        https://doi.org/10.1080/10618600.2014.907095
    
},
eprint = { 
        https://doi.org/10.1080/10618600.2014.907095
    
}

}

@article{greenlandStatisticalTestsValues2016,
  title = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  year = {2016-04-01},
  journaltitle = {European Journal of Epidemiology},
  shortjournal = {Eur J Epidemiol},
  volume = {31},
  number = {4},
  pages = {337--350},
  _issn = {1573-7284},
  _doi = {10.1007/s10654-016-0149-3},
  __url = {https://doi.org/10.1007/s10654-016-0149-3},
  _urldate = {2020-10-28},
  abstract = {Misinterpretation and abuse of statistical tests, confidence intervals, and statistical power have been decried for decades, yet remain rampant. A key problem is that there are no interpretations of these concepts that are at once simple, intuitive, correct, and foolproof. Instead, correct use and interpretation of these statistics requires an attention to detail which seems to tax the patience of working scientists. This high cognitive demand has led to an epidemic of shortcut definitions and interpretations that are simply wrong, sometimes disastrously so—and yet these misinterpretations dominate much of the scientific literature. In light of this problem, we provide definitions and a discussion of basic statistics that are more general and critical than typically found in traditional introductory expositions. Our goal is to provide a resource for instructors, researchers, and consumers of statistics whose knowledge of statistical theory and technique may be limited but who wish to avoid and spot misinterpretations. We emphasize how violation of often unstated analysis protocols (such as selecting analyses for presentation based on the P values they produce) can lead to small P values even if the declared test hypothesis is correct, and can lead to large P values even if that hypothesis is incorrect. We then provide an explanatory list of 25 misinterpretations of P values, confidence intervals, and power. We conclude with guidelines for improving statistical interpretation and reporting.},
  langid = {english}
}

@article{gregoruttiCorrelationVariableImportance2017,
  title = {Correlation and Variable Importance in Random Forests},
  author = {Gregorutti, Baptiste and Michel, B. and Saint-Pierre, P.},
  year = {2017},
  journaltitle = {Stat. Comput.},
  _doi = {10.1007/s11222-016-9646-1},
  abstract = {This paper is about variable selection with the random forests algorithm in presence of correlated predictors. In high-dimensional regression or classification frameworks, variable selection is a difficult task, that becomes even more challenging in the presence of highly correlated predictors. Firstly we provide a theoretical study of the permutation importance measure for an additive regression model. This allows us to describe how the correlation between predictors impacts the permutation importance. Our results motivate the use of the recursive feature elimination (RFE) algorithm for variable selection in this context. This algorithm recursively eliminates the variables using permutation importance measure as a ranking criterion. Next various simulation experiments illustrate the efficiency of the RFE algorithm for selecting a small number of variables together with a good prediction error. Finally, this selection algorithm is tested on the Landsat Satellite data from the UCI Machine Learning Repository.}
}

@article{gregoruttiCorrelationVariableImportance2017a,
  title = {Correlation and Variable Importance in Random Forests},
  author = {Gregorutti, Baptiste and Michel, Bertrand and Saint-Pierre, Philippe},
  year = {2017-05},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {27},
  number = {3},
  pages = {659--678},
  _issn = {0960-3174, 1573-1375},
  _doi = {10.1007/s11222-016-9646-1},
  __url = {http://link.springer.com/10.1007/s11222-016-9646-1},
  _urldate = {2022-01-19},
  langid = {english},
  file = {/home/rick/Zotero/storage/TPXAJVX3/Gregorutti et al. - 2017 - Correlation and variable importance in random fore.pdf}
}

@article{sixt2022RigorousStudyDeepTylor,
title={A Rigorous Study Of The Deep Taylor Decomposition},
author={Leon Sixt and Tim Landgraf},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
}

@inproceedings{kohlbrenner2020TowardsBestPraticeExplNN,
  author={Kohlbrenner, Maximilian and Bauer, Alexander and Nakajima, Shinichi and Binder, Alexander and Samek, Wojciech and Lapuschkin, Sebastian},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Towards Best Practice in Explaining Neural Network Decisions with LRP}, 
  year={2020},
  volume={},
  number={},
  pages={1-7},
}

@article{montavon2018MethodsInterpretingNN,
title = {Methods for interpreting and understanding deep neural networks},
journal = {Digital Signal Processing},
volume = {73},
pages = {1-15},
year = {2018},
issn = {1051-2004},
author = {Grégoire Montavon and Wojciech Samek and Klaus-Robert Müller},
}

@article{gretton2009covariate,
  title = {Covariate Shift by Kernel Mean Matching},
  author = {Gretton, Arthur and Smola, Alex and Huang, Jiayuan and Schmittfull, Marcel and Borgwardt, Karsten and Schölkopf, Bernhard},
  year = {2009},
  journaltitle = {Dataset shift in machine learning},
  volume = {3},
  number = {4},
  pages = {5}
}

@article{grompingEstimatorsRelativeImportance2007,
  title = {Estimators of {{Relative Importance}} in {{Linear Regression Based}} on {{Variance Decomposition}}},
  author = {Grömping, Ulrike},
  year = {2007-05},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {61},
  number = {2},
  pages = {139--147},
  _issn = {0003-1305, 1537-2731},
  _doi = {10.1198/000313007X188252},
  __url = {http://www.tandfonline.com/doi/abs/10.1198/000313007X188252},
  _urldate = {2022-02-01},
  langid = {english}
}

@article{grompingNotAdjustCoefficients2010,
  title = {Do Not Adjust Coefficients in {{Shapley}} Value Regression},
  author = {Grömping, Ulrike and Landau, Sabine},
  year = {2010-03},
  journaltitle = {Applied Stochastic Models in Business and Industry},
  journal = {Applied Stochastic Models in Business and Industry},
  shortjournal = {Appl. Stochastic Models Bus. Ind.},
  volume = {26},
  number = {2},
  pages = {194--202},
  _issn = {15241904, 15264025},
  _doi = {10.1002/asmb.773},
  __url = {https://onlinelibrary.wiley.com/doi/10.1002/asmb.773},
  _urldate = {2022-02-01},
  langid = {english}
}

@article{grompingVariableImportanceRegression2015,
  title = {Variable Importance in Regression Models},
  author = {Grömping, Ulrike},
  year = {2015-03},
  journaltitle = {Wiley Interdisciplinary Reviews: Computational Statistics},
  shortjournal = {WIREs Comput Stat},
  volume = {7},
  number = {2},
  pages = {137--152},
  _issn = {19395108},
  _doi = {10.1002/wics.1346},
  __url = {https://onlinelibrary.wiley.com/doi/10.1002/wics.1346},
  _urldate = {2021-10-06},
  langid = {english}
}

@unpublished{guidottiSurveyMethodsExplaining2018,
  title = {A {{Survey Of Methods For Explaining Black Box Models}}},
  author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Pedreschi, Dino and Giannotti, Fosca},
  year = {2018-06-21},
  eprint = {1802.01933},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1802.01933},
  _urldate = {2020-04-20},
  abstract = {In the last years many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness sometimes at the cost of scarifying accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, delineating explicitly or implicitly its own definition of interpretability and explanation. The aim of this paper is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
  archiveprefix = {arXiv},
  version = {3},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning}
}

@article{guidottiSurveyMethodsExplaining2019,
  title = {A {{Survey}} of {{Methods}} for {{Explaining Black Box Models}}},
  author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
  year = {2019-01-23},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {51},
  number = {5},
  pages = {1--42},
  _issn = {0360-0300, 1557-7341},
  _doi = {10.1145/3236009},
  __url = {https://dl.acm.org/doi/10.1145/3236009},
  _urldate = {2021-01-09},
  langid = {english},
  file = {/home/rick/Zotero/storage/LMUU57QZ/Guidotti et al. - 2019 - A Survey of Methods for Explaining Black Box Model.pdf}
}

@inproceedings{guyonDesignExperimentsNIPS2003,
  title = {Design of Experiments of the {{NIPS}} 2003 Variable Selection Benchmark},
  booktitle = {{{NIPS}} 2003 Workshop on Feature Extraction and Feature Selection},
  author = {Guyon, Isabelle},
  year = {2003},
  volume = {253}
}

@inproceedings{hanAutoencoderInspiredUnsupervised2018,
  title = {Autoencoder {{Inspired Unsupervised Feature Selection}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Han, Kai and Wang, Yunhe and Zhang, Chao and Li, Chao and Xu, Chao},
  year = {2018-04},
  pages = {2941--2945},
  publisher = {{IEEE}},
  location = {{Calgary, AB}},
  _doi = {10.1109/ICASSP.2018.8462261},
  __url = {https://ieeexplore.ieee.org/document/8462261/},
  _urldate = {2022-04-04},
  eventtitle = {{{ICASSP}} 2018 - 2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  _isbn = {978-1-5386-4658-8},
  file = {/home/rick/Zotero/storage/JHEKJP7M/Han et al. - 2018 - Autoencoder Inspired Unsupervised Feature Selectio.pdf}
}

@article{hanTransparentInterpretationKnockouts2020,
  title = {Transparent {{Interpretation}} with {{Knockouts}}},
  author = {Han, Xing},
  year = {2020-11-01},
  __url = {https://arxiv.org/abs/2011.00639v2},
  _urldate = {2020-11-30},
  abstract = {How can we find a subset of training samples that are most responsible for a complicated black-box machine learning model prediction? More generally, how can we explain the model decision to end-users in a transparent way? We propose a new model-agnostic algorithm to identify a minimum number of training samples that are indispensable for a given model decision at a particular test point, as the model decision would otherwise change upon the removal of these training samples. In line with the counterfactual explanation, our algorithm identifies such a set of indispensable samples iteratively by solving a constrained optimization problem. Further, we efficiently speed up the algorithm through approximation. To demonstrate the effectiveness of the algorithm, we apply it to a variety of tasks including data poisoning detection, training set debugging, and understanding loan decisions. Results show that our algorithm is an effective and easy to comprehend tool to help better understand local model behaviors and therefore facilitate the application of machine learning in domains where such understanding is a requisite and where end-users do not have a machine learning background.},
  langid = {english}
}

@article{haufeInterpretationWeightVectors2014,
  title = {On the Interpretation of Weight Vectors of Linear Models in Multivariate Neuroimaging},
  author = {Haufe, Stefan and Meinecke, Frank and Görgen, Kai and Dähne, Sven and Haynes, John-Dylan and Blankertz, Benjamin and Bießmann, Felix},
  year = {2014},
  journaltitle = {NeuroImage},
  journal = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {87},
  pages = {96--110},
  _issn = {1053-8119},
  _doi = {10.1016/j.neuroimage.2013.10.067},
  __url = {http://www.sciencedirect.com/science/article/pii/S1053811913010914},
  _urldate = {2020-04-20},
  abstract = {The increase in spatiotemporal resolution of neuroimaging devices is accompanied by a trend towards more powerful multivariate analysis methods. Often it is desired to interpret the outcome of these methods with respect to the cognitive processes under study. Here we discuss which methods allow for such interpretations, and provide guidelines for choosing an appropriate analysis for a given experimental goal: For a surgeon who needs to decide where to remove brain tissue it is most important to determine the origin of cognitive functions and associated neural processes. In contrast, when communicating with paralyzed or comatose patients via brain–computer interfaces, it is most important to accurately extract the neural processes specific to a certain mental state. These equally important but complementary objectives require different analysis methods. Determining the origin of neural processes in time or space from the parameters of a data-driven model requires what we call a forward model of the data; such a model explains how the measured data was generated from the neural sources. Examples are general linear models (GLMs). Methods for the extraction of neural information from data can be considered as backward models, as they attempt to reverse the data generating process. Examples are multivariate classifiers. Here we demonstrate that the parameters of forward models are neurophysiologically interpretable in the sense that significant nonzero weights are only observed at channels the activity of which is related to the brain process under study. In contrast, the interpretation of backward model parameters can lead to wrong conclusions regarding the spatial or temporal origin of the neural signals of interest, since significant nonzero weights may also be observed at channels the activity of which is statistically independent of the brain process under study. As a remedy for the linear case, we propose a procedure for transforming backward models into forward models. This procedure enables the neurophysiological interpretation of the parameters of linear backward models. We hope that this work raises awareness for an often encountered problem and provides a theoretical basis for conducting better interpretable multivariate neuroimaging analyses.},
  langid = {english},
  keywords = {Activation patterns,Decoding,EEG,Encoding,Extraction filters,fMRI,Forward/backward models,Generative/discriminative models,Interpretability,Multivariate,Neuroimaging,Regularization,Sparsity,Univariate}
}

@unpublished{haunschmidAudioLIMEListenableExplanations2020,
  title = {{{audioLIME}}: {{Listenable Explanations Using Source Separation}}},
  shorttitle = {{{audioLIME}}},
  author = {Haunschmid, Verena and Manilow, Ethan and Widmer, Gerhard},
  year = {2020-09-07},
  eprint = {2008.00582},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  __url = {http://arxiv.org/abs/2008.00582},
  _urldate = {2020-09-16},
  abstract = {Deep neural networks (DNNs) are successfully applied in a wide variety of music information retrieval (MIR) tasks but their predictions are usually not interpretable. We propose audioLIME, a method based on Local Interpretable Model-agnostic Explanations (LIME) extended by a musical definition of locality. The perturbations used in LIME are created by switching on/off components extracted by source separation which makes our explanations listenable. We validate audioLIME on two different music tagging systems and show that it produces sensible explanations in situations where a competing method cannot.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@unpublished{haunschmidMusicallyMeaningfulExplanations2020,
  title = {Towards {{Musically Meaningful Explanations Using Source Separation}}},
  author = {Haunschmid, Verena and Manilow, Ethan and Widmer, Gerhard},
  year = {2020-09-04},
  eprint = {2009.02051},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  __url = {http://arxiv.org/abs/2009.02051},
  _urldate = {2020-11-30},
  abstract = {Deep neural networks (DNNs) are successfully applied in a wide variety of music information retrieval (MIR) tasks. Such models are usually considered "black boxes", meaning that their predictions are not interpretable. Prior work on explainable models in MIR has generally used image processing tools to produce explanations for DNN predictions, but these are not necessarily musically meaningful, or can be listened to (which, arguably, is important in music). We propose audioLIME, a method based on Local Interpretable Model-agnostic Explanation (LIME), extended by a musical definition of locality. LIME learns locally linear models on perturbations of an example that we want to explain. Instead of extracting components of the spectrogram using image segmentation as part of the LIME pipeline, we propose using source separation. The perturbations are created by switching on/off sources which makes our explanations listenable. We first validate audioLIME on a classifier that was deliberately trained to confuse the true target with a spurious signal, and show that this can easily be detected using our method. We then show that it passes a sanity check that many available explanation methods fail. Finally, we demonstrate the general applicability of our (model-agnostic) method on a third-party music tagger.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, K. and Zhang, X. and Ren, S. and Sun, J.},
  year = {2016-06},
  pages = {770--778},
  _issn = {1063-6919},
  _doi = {10.1109/CVPR.2016.90},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {CIFAR-10,COCO object detection dataset,COCO segmentation,Complexity theory,deep residual learning,deep residual nets,deeper neural network training,Degradation,ILSVRC & COCO 2015 competitions,ILSVRC 2015 classification task,image classification,image recognition,Image recognition,Image segmentation,ImageNet dataset,ImageNet localization,ImageNet test set,learning (artificial intelligence),neural nets,Neural networks,object detection,residual function learning,residual nets,Training,VGG nets,visual recognition tasks,Visualization}
}

@unpublished{henckaertsModelAgnosticInterpretableDatadriven2020,
  title = {Model-{{Agnostic Interpretable}} and {{Data-driven suRRogates}} Suited for Highly Regulated Industries},
  author = {Henckaerts, Roel and Antonio, Katrien and Côté, Marie-Pier},
  year = {2020-07-14},
  eprint = {2007.06894},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.06894},
  _urldate = {2020-08-05},
  abstract = {Highly regulated industries, like banking and insurance, ask for transparent decision-making algorithms. At the same time, competitive markets push for sophisticated black box models. We therefore present a procedure to develop a Model-Agnostic Interpretable Data-driven suRRogate, suited for structured tabular data. Insights are extracted from a black box via partial dependence effects. These are used to group feature values, resulting in a segmentation of the feature space with automatic feature selection. A transparent generalized linear model (GLM) is fit to the features in categorical format and their relevant interactions. We demonstrate our R package maidrr with a case study on general insurance claim frequency modeling for six public datasets. Our maidrr GLM closely approximates a gradient boosting machine (GBM) and outperforms both a linear and tree surrogate as benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{heoFoolingNeuralNetwork2019,
  title = {Fooling {{Neural Network Interpretations}} via {{Adversarial Model Manipulation}}},
  author = {Heo, Juyeon and Joo, Sunghwan and Moon, Taesup},
  year = {2019-10-31},
  eprint = {1902.02041},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1902.02041},
  _urldate = {2020-12-21},
  abstract = {We ask whether the neural network interpretation methods can be fooled via adversarial model manipulation, which is defined as a model fine-tuning step that aims to radically alter the explanations without hurting the accuracy of the original models, e.g., VGG19, ResNet50, and DenseNet121. By incorporating the interpretation results directly in the penalty term of the objective function for fine-tuning, we show that the state-of-the-art saliency map based interpreters, e.g., LRP, Grad-CAM, and SimpleGrad, can be easily fooled with our model manipulation. We propose two types of fooling, Passive and Active, and demonstrate such foolings generalize well to the entire validation set as well as transfer to other interpretation methods. Our results are validated by both visually showing the fooled explanations and reporting quantitative metrics that measure the deviations from the original explanations. We claim that the stability of neural network interpretation method with respect to our adversarial model manipulation is an important criterion to check for developing robust and reliable neural network interpretation method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{heoFoolingNeuralNetwork2019a,
  title = {Fooling {{Neural Network Interpretations}} via {{Adversarial Model Manipulation}}},
  author = {Heo, Juyeon and Joo, Sunghwan and Moon, Taesup},
  year = {2019-10},
  eprint = {1902.02041},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1902.02041},
  _urldate = {2020-12-21},
  abstract = {We ask whether the neural network interpretation methods can be fooled via adversarial model manipulation, which is defined as a model fine-tuning step that aims to radically alter the explanations without hurting the accuracy of the original models, e.g., VGG19, ResNet50, and DenseNet121. By incorporating the interpretation results directly in the penalty term of the objective function for fine-tuning, we show that the state-of-the-art saliency map based interpreters, e.g., LRP, Grad-CAM, and SimpleGrad, can be easily fooled with our model manipulation. We propose two types of fooling, Passive and Active, and demonstrate such foolings generalize well to the entire validation set as well as transfer to other interpretation methods. Our results are validated by both visually showing the fooled explanations and reporting quantitative metrics that measure the deviations from the original explanations. We claim that the stability of neural network interpretation method with respect to our adversarial model manipulation is an important criterion to check for developing robust and reliable neural network interpretation method.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{heskesCausalShapleyValues2020,
  title = {Causal {{Shapley Values}}: {{Exploiting Causal Knowledge}} to {{Explain Individual Predictions}} of {{Complex Models}}},
  shorttitle = {Causal {{Shapley Values}}},
  author = {Heskes, Tom and Sijben, Evi and Bucur, Ioan Gabriel and Claassen, Tom},
  year = {2020-11-03},
  eprint = {2011.01625},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2011.01625},
  _urldate = {2020-11-30},
  abstract = {Shapley values underlie one of the most popular model-agnostic methods within explainable artificial intelligence. These values are designed to attribute the difference between a model's prediction and an average baseline to the different features used as input to the model. Being based on solid game-theoretic principles, Shapley values uniquely satisfy several desirable properties, which is why they are increasingly used to explain the predictions of possibly complex and highly non-linear machine learning models. Shapley values are well calibrated to a user's intuition when features are independent, but may lead to undesirable, counterintuitive explanations when the independence assumption is violated. In this paper, we propose a novel framework for computing Shapley values that generalizes recent work that aims to circumvent the independence assumption. By employing Pearl's do-calculus, we show how these 'causal' Shapley values can be derived for general causal graphs without sacrificing any of their desirable properties. Moreover, causal Shapley values enable us to separate the contribution of direct and indirect effects. We provide a practical implementation for computing causal Shapley values based on causal chain graphs when only partial information is available and illustrate their utility on a real-world example.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.6}
}

@unpublished{hoffmanMetricsExplainableAI2019,
  title = {Metrics for {{Explainable AI}}: {{Challenges}} and {{Prospects}}},
  shorttitle = {Metrics for {{Explainable AI}}},
  author = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
  year = {2019-02-01},
  eprint = {1812.04608},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1812.04608},
  _urldate = {2020-12-07},
  abstract = {The question addressed in this paper is: If we present to a user an AI system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the AI? In other words, how do we know that an explanainable AI system (XAI) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the AI systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the AI are appropriate, and finally, (6) how the human-XAI work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@article{hoffmanParamorphicRepresentationClinical1960,
  title = {The Paramorphic Representation of Clinical Judgment.},
  author = {Hoffman, Paul J.},
  year = {1960},
  journaltitle = {Psychological Bulletin},
  journal = {Psychological Bulletin},
  shortjournal = {Psychological Bulletin},
  volume = {57},
  number = {2},
  pages = {116--131},
  _issn = {0033-2909},
  _doi = {10.1037/h0047807},
  __url = {http://content.apa.org/journals/bul/57/2/116},
  _urldate = {2021-10-06},
  langid = {english}
}


@inproceedings{hohmanGamutDesignProbe2019,
  title = {Gamut: {{A Design Probe}} to {{Understand How Data Scientists Understand Machine Learning Models}}},
  shorttitle = {Gamut},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Hohman, Fred and Head, Andrew and Caruana, Rich and DeLine, Robert and Drucker, Steven M.},
  year = {2019-05-02},
  series = {{{CHI}} '19},
  pages = {1--13},
  publisher = {{Association for Computing Machinery}},
  location = {{Glasgow, Scotland Uk}},
  _doi = {10.1145/3290605.3300809},
  __url = {https://doi.org/10.1145/3290605.3300809},
  _urldate = {2020-10-22},
  abstract = {Without good models and the right tools to interpret them, data scientists risk making decisions based on hidden biases, spurious correlations, and false generalizations. This has led to a rallying cry for model interpretability. Yet the concept of interpretability remains nebulous, such that researchers and tool designers lack actionable guidelines for how to incorporate interpretability into models and accompanying tools. Through an iterative design process with expert machine learning researchers and practitioners, we designed a visual analytics system, Gamut, to explore how interactive interfaces could better support model interpretation. Using Gamut as a probe, we investigated why and how professional data scientists interpret models, and how interface affordances can support data scientists in answering questions about model interpretability. Our investigation showed that interpretability is not a monolithic concept: data scientists have different reasons to interpret models and tailor explanations for specific audiences, often balancing competing concerns of simplicity and completeness. Participants also asked to use Gamut in their work, highlighting its potential to help data scientists understand their own data.},
  _isbn = {978-1-4503-5970-2},
  keywords = {data visualization,design probe,interactive interfaces,machine learning interpretability,visual analytics}
}

@article{holzingerCausabilityExplainabilityArtificial2019,
  title = {Causability and Explainability of Artificial Intelligence in Medicine},
  author = {Holzinger, Andreas and Langs, Georg and Denk, Helmut and Zatloukal, Kurt and Müller, Heimo},
  year = {2019},
  journaltitle = {WIREs Data Mining and Knowledge Discovery},
  volume = {9},
  number = {4},
  pages = {e1312},
  _issn = {1942-4795},
  _doi = {10.1002/widm.1312},
  __url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1312},
  _urldate = {2020-10-19},
  abstract = {Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black-box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use-case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system This article is categorized under: Fundamental Concepts of Data and Knowledge {$>$} Human Centricity and User Interaction},
  langid = {english},
  keywords = {artificial intelligence,causability,explainability,explainable AI,histopathology,medicine}
}

@unpublished{honeggerSheddingLightBlack2018,
  title = {Shedding {{Light}} on {{Black Box Machine Learning Algorithms}}: {{Development}} of an {{Axiomatic Framework}} to {{Assess}} the {{Quality}} of {{Methods}} That {{Explain Individual Predictions}}},
  shorttitle = {Shedding {{Light}} on {{Black Box Machine Learning Algorithms}}},
  author = {Honegger, Milo},
  year = {2018-08-15},
  eprint = {1808.05054},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1808.05054},
  _urldate = {2020-10-19},
  abstract = {From self-driving vehicles and back-flipping robots to virtual assistants who book our next appointment at the hair salon or at that restaurant for dinner - machine learning systems are becoming increasingly ubiquitous. The main reason for this is that these methods boast remarkable predictive capabilities. However, most of these models remain black boxes, meaning that it is very challenging for humans to follow and understand their intricate inner workings. Consequently, interpretability has suffered under this ever-increasing complexity of machine learning models. Especially with regards to new regulations, such as the General Data Protection Regulation (GDPR), the necessity for plausibility and verifiability of predictions made by these black boxes is indispensable. Driven by the needs of industry and practice, the research community has recognised this interpretability problem and focussed on developing a growing number of so-called explanation methods over the past few years. These methods explain individual predictions made by black box machine learning models and help to recover some of the lost interpretability. With the proliferation of these explanation methods, it is, however, often unclear, which explanation method offers a higher explanation quality, or is generally better-suited for the situation at hand. In this thesis, we thus propose an axiomatic framework, which allows comparing the quality of different explanation methods amongst each other. Through experimental validation, we find that the developed framework is useful to assess the explanation quality of different explanation methods and reach conclusions that are consistent with independent research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{hookerBenchmarkInterpretabilityMethods2019,
  title = {A {{Benchmark}} for {{Interpretability Methods}} in {{Deep Neural Networks}}},
  author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
  year = {2019-11-04},
  eprint = {1806.10758},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1806.10758},
  _urldate = {2020-12-22},
  abstract = {We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches---VarGrad and SmoothGrad-Squared---outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{hookerBenchmarkInterpretabilityMethods2019a,
  title = {A {{Benchmark}} for {{Interpretability Methods}} in {{Deep Neural Networks}}},
  author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
  year = {2019-11},
  eprint = {1806.10758},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1806.10758},
  _urldate = {2020-12-22},
  abstract = {We propose an empirical measure of the approximate accuracy of feature importance estimates in deep neural networks. Our results across several large-scale image classification datasets show that many popular interpretability methods produce estimates of feature importance that are not better than a random designation of feature importance. Only certain ensemble based approaches—VarGrad and SmoothGrad-Squared—outperform such a random assignment of importance. The manner of ensembling remains critical, we show that some approaches do no better then the underlying method but carry a far higher computational burden.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{hookerPleaseStopPermuting2019,
  title = {Please {{Stop Permuting Features}}: {{An Explanation}} and {{Alternatives}}},
  shorttitle = {Please {{Stop Permuting Features}}},
  author = {Hooker, Giles and Mentch, Lucas},
  year = {2019-05-01},
  eprint = {1905.03151},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1905.03151},
  _urldate = {2020-10-07},
  abstract = {This paper advocates against permute-and-predict (PaP) methods for interpreting black box functions. Methods such as the variable importance measures proposed for random forests, partial dependence plots, and individual conditional expectation plots remain popular because of their ability to provide model-agnostic measures that depend only on the pre-trained model output. However, numerous studies have found that these tools can produce diagnostics that are highly misleading, particularly when there is strong dependence among features. Rather than simply add to this growing literature by further demonstrating such issues, here we seek to provide an explanation for the observed behavior. In particular, we argue that breaking dependencies between features in hold-out data places undue emphasis on sparse regions of the feature space by forcing the original model to extrapolate to regions where there is little to no data. We explore these effects through various settings where a ground-truth is understood and find support for previous claims in the literature that PaP metrics tend to over-emphasize correlated features both in variable importance and partial dependence plots, even though applying permutation methods to the ground-truth models do not. As an alternative, we recommend more direct approaches that have proven successful in other settings: explicitly removing features, conditional permutations, or model distillation methods.},
  archiveprefix = {arXiv},
  keywords = {62G08,Computer Science - Machine Learning,I.5.1,Statistics - Machine Learning,Statistics - Methodology}
}

@book{horstPredictionPersonalAdjustment1941,
  title = {The Prediction of Personal Adjustment: {{A}} Survey of Logical Problems and Research Techniques, with Illustrative Application to Problems of Vocational Selection, School Success, Marriage, and Crime.},
  shorttitle = {The Prediction of Personal Adjustment},
  author = {Horst, Paul and Wallin, Paul and Guttman, Louis and Wallin, Frieda Brim and Clausen, John A. and Reed, Robert and Rosenthal, Erich},
  year = {1941},
  publisher = {{Social Science Research Council}},
  location = {{New York}},
  _doi = {10.1037/11521-000},
  __url = {http://content.apa.org/books/11521-000},
  _urldate = {2022-04-22},
  langid = {english}
}

@online{hSelectiveLabelsProblem2017,
  title = {The {{Selective Labels Problem}}: {{Evaluating Algorithmic Predictions}} in the {{Presence}} of {{Unobservables}}},
  shorttitle = {The {{Selective Labels Problem}}},
  author = {H, Lakkaraju and J, Kleinberg and J, Leskovec and J, Ludwig and S, Mullainathan},
  year = {2017-08},
  eprint = {29780658},
  eprinttype = {pmid},
  _doi = {10.1145/3097983.3098066},
  __url = {https://pubmed.ncbi.nlm.nih.gov/29780658/},
  _urldate = {2020-06-01},
  abstract = {Evaluating whether machines improve on human performance is one of the central questions of machine learning. However, there are many domains where the data is \emph{selectively labeled} in the sense that the observed outcomes are themselves a consequence of the existing choices of the human decisio …},
  langid = {english},
  organization = {{KDD : proceedings. International Conference on Knowledge Discovery \& Data Mining}}
}

@article{huangPermuteNotPermute2006,
  title = {To Permute or Not to Permute},
  author = {Huang, Yifan and Xu, Haiyan and Calian, Violeta and Hsu, Jason C.},
  year = {2006-09-15},
  journaltitle = {Bioinformatics},
  volume = {22},
  number = {18},
  pages = {2244--2248},
  _issn = {1460-2059, 1367-4803},
  _doi = {10.1093/bioinformatics/btl383},
  __url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btl383},
  _urldate = {2022-04-04},
  langid = {english}
}

@article{huangSurveySafetyTrustworthiness2020,
  title = {A Survey of Safety and Trustworthiness of Deep Neural Networks: {{Verification}}, Testing, Adversarial Attack and Defence, and Interpretability},
  shorttitle = {A Survey of Safety and Trustworthiness of Deep Neural Networks},
  author = {Huang, Xiaowei and Kroening, Daniel and Ruan, Wenjie and Sharp, James and Sun, Youcheng and Thamo, Emese and Wu, Min and Yi, Xinping},
  year = {2020-08-01},
  journaltitle = {Computer Science Review},
  shortjournal = {Computer Science Review},
  volume = {37},
  pages = {100270},
  _issn = {1574-0137},
  _doi = {10.1016/j.cosrev.2020.100270},
  __url = {http://www.sciencedirect.com/science/article/pii/S1574013719302527},
  _urldate = {2020-10-22},
  abstract = {In the past few years, significant progress has been made on deep neural networks (DNNs) in achieving human-level performance on several long-standing tasks. With the broader deployment of DNNs on various applications, the concerns over their safety and trustworthiness have been raised in public, especially after the widely reported fatal incidents involving self-driving cars. Research to address these concerns is particularly active, with a significant number of papers released in the past few years. This survey paper conducts a review of the current research effort into making DNNs safe and trustworthy, by focusing on four aspects: verification, testing, adversarial attack and defence, and interpretability. In total, we survey 202 papers, most of which were published after 2017.},
  langid = {english}
}

@unpublished{huSurrogateLocallyInterpretableModels2020,
  title = {Surrogate {{Locally-Interpretable Models}} with {{Supervised Machine Learning Algorithms}}},
  author = {Hu, Linwei and Chen, Jie and Nair, Vijayan N. and Sudjianto, Agus},
  year = {2020-07-28},
  eprint = {2007.14528},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.14528},
  _urldate = {2020-08-05},
  abstract = {Supervised Machine Learning (SML) algorithms, such as Gradient Boosting, Random Forest, and Neural Networks, have become popular in recent years due to their superior predictive performance over traditional statistical methods. However, their complexity makes the results hard to interpret without additional tools. There has been a lot of recent work in developing global and local diagnostics for interpreting SML models. In this paper, we propose a locally-interpretable model that takes the fitted ML response surface, partitions the predictor space using model-based regression trees, and fits interpretable main-effects models at each of the nodes. We adapt the algorithm to be efficient in dealing with high-dimensional predictors. While the main focus is on interpretability, the resulting surrogate model also has reasonably good predictive performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{iandolaSqueezeNetAlexNetlevelAccuracy2016,
  title = {{{SqueezeNet}}: {{AlexNet-level}} Accuracy with 50x Fewer Parameters and {$<$}0.{{5MB}} Model Size},
  shorttitle = {{{SqueezeNet}}},
  author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
  year = {2016-11-04},
  eprint = {1602.07360},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1602.07360},
  _urldate = {2020-11-04},
  abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition}
}

@unpublished{islamQuantificationExplainabilityExplainable2019,
  title = {Towards {{Quantification}} of {{Explainability}} in {{Explainable Artificial Intelligence Methods}}},
  author = {Islam, Sheikh Rabiul and Eberle, William and Ghafoor, Sheikh K.},
  year = {2019-11-22},
  eprint = {1911.10104},
  eprinttype = {arxiv},
  primaryclass = {cs, q-fin},
  __url = {http://arxiv.org/abs/1911.10104},
  _urldate = {2020-10-22},
  abstract = {Artificial Intelligence (AI) has become an integral part of domains such as security, finance, healthcare, medicine, and criminal justice. Explaining the decisions of AI systems in human terms is a key challenge--due to the high complexity of the model, as well as the potential implications on human interests, rights, and lives . While Explainable AI is an emerging field of research, there is no consensus on the definition, quantification, and formalization of explainability. In fact, the quantification of explainability is an open challenge. In our previous work, we incorporated domain knowledge for better explainability, however, we were unable to quantify the extent of explainability. In this work, we (1) briefly analyze the definitions of explainability from the perspective of different disciplines (e.g., psychology, social science), properties of explanation, explanation methods, and human-friendly explanations; and (2) propose and formulate an approach to quantify the extent of explainability. Our experimental result suggests a reasonable and model-agnostic way to quantify explainability},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Finance - Risk Management}
}

@unpublished{ismailBenchmarkingDeepLearning2020,
  title = {Benchmarking {{Deep Learning Interpretability}} in {{Time Series Predictions}}},
  author = {Ismail, Aya Abdelsalam and Gunady, Mohamed and Bravo, Héctor Corrada and Feizi, Soheil},
  year = {2020-10-26},
  eprint = {2010.13924},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2010.13924},
  _urldate = {2021-01-10},
  abstract = {Saliency methods are used extensively to highlight the importance of input features in model predictions. These methods are mostly used in vision and language tasks, and their applications to time series data is relatively unexplored. In this paper, we set out to extensively compare the performance of various saliency-based interpretability methods across diverse neural architectures, including Recurrent Neural Network, Temporal Convolutional Networks, and Transformers in a new benchmark of synthetic time series data. We propose and report multiple metrics to empirically evaluate the performance of saliency methods for detecting feature importance over time using both precision (i.e., whether identified features contain meaningful signals) and recall (i.e., the number of features with signal identified as important). Through several experiments, we show that (i) in general, network architectures and saliency methods fail to reliably and accurately identify feature importance over time in time series data, (ii) this failure is mainly due to the conflation of time and feature domains, and (iii) the quality of saliency maps can be improved substantially by using our proposed two-step temporal saliency rescaling (TSR) approach that first calculates the importance of each time step before calculating the importance of each feature at a time step.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{ismailBenchmarkingDeepLearning2020a,
  title = {Benchmarking {{Deep Learning Interpretability}} in {{Time Series Predictions}}},
  author = {Ismail, Aya Abdelsalam and Gunady, Mohamed and Bravo, Héctor Corrada and Feizi, Soheil},
  year = {2020-10},
  eprint = {2010.13924},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2010.13924},
  _urldate = {2021-01-10},
  abstract = {Saliency methods are used extensively to highlight the importance of input features in model predictions. These methods are mostly used in vision and language tasks, and their applications to time series data is relatively unexplored. In this paper, we set out to extensively compare the performance of various saliency-based interpretability methods across diverse neural architectures, including Recurrent Neural Network, Temporal Convolutional Networks, and Transformers in a new benchmark of synthetic time series data. We propose and report multiple metrics to empirically evaluate the performance of saliency methods for detecting feature importance over time using both precision (i.e., whether identified features contain meaningful signals) and recall (i.e., the number of features with signal identified as important). Through several experiments, we show that (i) in general, network architectures and saliency methods fail to reliably and accurately identify feature importance over time in time series data, (ii) this failure is mainly due to the conflation of time and feature domains, and (iii) the quality of saliency maps can be improved substantially by using our proposed two-step temporal saliency rescaling (TSR) approach that first calculates the importance of each time step before calculating the importance of each feature at a time step.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{ismailInputCellAttentionReduces2019,
  title = {Input-{{Cell Attention Reduces Vanishing Saliency}} of {{Recurrent Neural Networks}}},
  author = {Ismail, Aya Abdelsalam and Gunady, Mohamed and Pessoa, Luiz and Bravo, Hector Corrada and Feizi, Soheil},
  year = {2019},
  pages = {10814--10824},
  __url = {http://papers.neurips.cc/paper/9264-input-cell-attention-reduces-vanishing-saliency-of-recurrent-neural-networks},
  _urldate = {2020-12-02},
  abstract = {Electronic Proceedings of Neural Information Processing Systems},
  eventtitle = {Advances in {{Neural Information Processing Systems}}}
}

@inproceedings{ismailInputCellAttentionReduces2019a,
  title = {Input-{{Cell Attention Reduces Vanishing Saliency}} of {{Recurrent Neural Networks}}},
  author = {Ismail, Aya Abdelsalam and Gunady, Mohamed and Pessoa, Luiz and Bravo, Hector Corrada and Feizi, Soheil},
  year = {2019},
  pages = {10814--10824},
  __url = {http://papers.neurips.cc/paper/9264-input-cell-attention-reduces-vanishing-saliency-of-recurrent-neural-networks},
  _urldate = {2020-12-02},
  abstract = {Electronic Proceedings of Neural Information Processing Systems}
}

@unpublished{izzaExplainingDecisionTrees2020,
  title = {On {{Explaining Decision Trees}}},
  author = {Izza, Yacine and Ignatiev, Alexey and Marques-Silva, Joao},
  year = {2020-10-21},
  eprint = {2010.11034},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2010.11034},
  _urldate = {2020-11-30},
  abstract = {Decision trees (DTs) epitomize what have become to be known as interpretable machine learning (ML) models. This is informally motivated by paths in DTs being often much smaller than the total number of features. This paper shows that in some settings DTs can hardly be deemed interpretable, with paths in a DT being arbitrarily larger than a PI-explanation, i.e. a subset-minimal set of feature values that entails the prediction. As a result, the paper proposes a novel model for computing PI-explanations of DTs, which enables computing one PI-explanation in polynomial time. Moreover, it is shown that enumeration of PI-explanations can be reduced to the enumeration of minimal hitting sets. Experimental results were obtained on a wide range of publicly available datasets with well-known DT-learning tools, and confirm that in most cases DTs have paths that are proper supersets of PI-explanations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@unpublished{jacobsMeasurementFairness2019,
  title = {Measurement and {{Fairness}}},
  author = {Jacobs, Abigail Z. and Wallach, Hanna},
  year = {2019-12-11},
  eprint = {1912.05511},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1912.05511},
  _urldate = {2020-10-22},
  abstract = {We introduce the language of measurement modeling from the quantitative social sciences as a framework for understanding fairness in computational systems. Computational systems often involve unobservable theoretical constructs, such as "creditworthiness," "teacher quality," or "risk to society," that cannot be measured directly and must instead be inferred from observable properties thought to be related to them---i.e., operationalized via a measurement model. This process introduces the potential for mismatch between the theoretical understanding of the construct purported to be measured and its operationalization. Indeed, we argue that many of the harms discussed in the literature on fairness in computational systems are direct results of such mismatches. Further complicating these discussions is the fact that fairness itself is an unobservable theoretical construct. Moreover, it is an essentially contested construct---i.e., it has many different theoretical understandings depending on the context. We argue that this contestedness underlies recent debates about fairness definitions: disagreements that appear to be about contradictory operationalizations are, in fact, disagreements about different theoretical understandings of the construct itself. By introducing the language of measurement modeling, we provide the computer science community with a process for making explicit and testing assumptions about unobservable theoretical constructs, thereby making it easier to identify, characterize, and even mitigate fairness-related harms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning}
}

@inproceedings{jacoviFaithfullyInterpretableNLP2020,
    title = "Towards Faithfully Interpretable {NLP} Systems: How Should We Define and Evaluate Faithfulness?",
    author = "Jacovi, Alon  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    pages = "4198--4205",
    abstract = "With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is {``}defined{''} by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.",
}

@article{yang2019benchmarking,
  title={Benchmarking attribution methods with relative feature importance},
  author={Yang, Mengjiao and Kim, Been},
  journal={arXiv preprint arXiv:1907.09701},
  year={2019}
}

@inproceedings{dandl2020multi,
  title = {Multi-Objective Counterfactual Explanations},
  booktitle = {International Conference on Parallel Problem Solving from Nature},
  author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
  date = {2020},
  year = {2020},
  pages = {448--469},
  organization = {{Springer}}
}

@inproceedings{mothilalExplainingMachineLearning2020,
  title = {Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
  date = {2020},
  year = {2020},
  series = {{{FAT}}* '20},
  pages = {607--617},
  publisher = {{Association for Computing Machinery}},
  location = {{Barcelona, Spain}},

}

@unpublished{jainAttentionNotExplanation2019,
  title = {Attention Is Not {{Explanation}}},
  author = {Jain, Sarthak and Wallace, Byron C.},
  year = {2019-05-08},
  eprint = {1902.10186},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1902.10186},
  _urldate = {2020-10-13},
  abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful `explanations' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/successar/AttentionExplanation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@unpublished{jainAttentionNotExplanation2019a,
  title = {Attention Is Not {{Explanation}}},
  author = {Jain, Sarthak and Wallace, Byron C.},
  year = {2019-05},
  eprint = {1902.10186},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1902.10186},
  _urldate = {2020-10-13},
  abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful `explanations' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/successar/AttentionExplanation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@unpublished{jakovacUnderstandingUnderstandingRenormalization2020,
  title = {Understanding Understanding: A Renormalization Group Inspired Model of (Artificial) Intelligence},
  shorttitle = {Understanding Understanding},
  author = {Jakovac, A. and Berenyi, D. and Posfay, P.},
  year = {2020-10-26},
  eprint = {2010.13482},
  eprinttype = {arxiv},
  primaryclass = {hep-th},
  __url = {http://arxiv.org/abs/2010.13482},
  _urldate = {2020-11-30},
  abstract = {This paper is about the meaning of understanding in scientific and in artificial intelligent systems. We give a mathematical definition of the understanding, where, contrary to the common wisdom, we define the probability space on the input set, and we treat the transformation made by an intelligent actor not as a loss of information, but instead a reorganization of the information in the framework of a new coordinate system. We introduce, following the ideas of physical renormalization group, the notions of relevant and irrelevant parameters, and discuss, how the different AI tasks can be interpreted along these concepts, and how the process of learning can be described. We show, how scientific understanding fits into this framework, and demonstrate, what is the difference between a scientific task and pattern recognition. We also introduce a measure of relevance, which is useful for performing lossy compression.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,High Energy Physics - Theory}
}

@article{jamesWhatPragmatismMeans1907,
  title = {"{{What Pragmatism Means}}". {{Lecture}} 2 in {{Pragmatism}}: {{A}} New Name for Some Old Ways of Thinking.},
  author = {James, William},
  year = {1907},
  journaltitle = {New York: Longman Green and Co},
  pages = {17--32},
  __url = {https://brocku.ca/MeadProject/James/James_1907/James_1907_02.html},
  _urldate = {2020-12-07}
}

@article{janzingAlgorithmicIndependenceInitial2016,
  title = {Algorithmic Independence of Initial Condition and Dynamical Law in Thermodynamics and Causal Inference},
  author = {Janzing, Dominik and Chaves, Rafael and Schoelkopf, Bernhard},
  year = {2016-09-27},
  journaltitle = {New Journal of Physics},
  shortjournal = {New J. Phys.},
  volume = {18},
  number = {9},
  eprint = {1512.02057},
  eprinttype = {arxiv},
  pages = {093052},
  _issn = {1367-2630},
  _doi = {10.1088/1367-2630/18/9/093052},
  __url = {http://arxiv.org/abs/1512.02057},
  _urldate = {2021-09-27},
  abstract = {We postulate a principle stating that the initial condition of a physical system is typically algorithmically independent of the dynamical law. We argue that this links thermodynamics and causal inference. On the one hand, it entails behaviour that is similar to the usual arrow of time. On the other hand, it motivates a statistical asymmetry between cause and effect that has recently postulated in the field of causal inference, namely, that the probability distribution P(cause) contains no information about the conditional distribution P(effect|cause) and vice versa, while P(effect) may contain information about P(cause|effect).},
  archiveprefix = {arXiv},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematics - Statistics Theory,Quantum Physics}
}


@inproceedings{janzingFeatureRelevanceQuantification2020,
  title = {Feature Relevance Quantification in Explainable {{AI}}: {{A}} Causal Problem},
  shorttitle = {Feature Relevance Quantification in Explainable {{AI}}},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Janzing, Dominik and Minorics, Lenon and Bloebaum, Patrick},
  year = {2020},
  pages = {2907--2916},
  publisher = {{PMLR}},
  __url = {http://proceedings.mlr.press/v108/janzing20a.html},
  _urldate = {2020},
  abstract = {We discuss promising recent contributions on quantifying feature relevance using Shapley values, where we observed some confusion on which probability distribution is the right one for dropped feat...},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english}
}

@article{johnsonPredictiveViewDetection1983,
  title = {A {{Predictive View}} of the {{Detection}} and {{Characterization}} of {{Influential Observations}} in {{Regression Analysis}}},
  author = {Johnson, Wesley and Geisser, Seymour},
  year = {1983},
  journaltitle = {Journal of the American Statistical Association},
  volume = {78},
  number = {381},
  pages = {137--144},
  publisher = {{Taylor \& Francis}},
  _doi = {10.1080/01621459.1983.10477942},
  __url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10477942}
}

@unpublished{jungExplainableEmpiricalRisk2020,
  title = {Explainable {{Empirical Risk Minimization}}},
  author = {Jung, A.},
  year = {2020-09-03},
  eprint = {2009.01492},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2009.01492},
  _urldate = {2020-11-30},
  abstract = {The widespread use of modern machine learning methods in decision making crucially depends on their interpretability or explainability. The human users (decision makers) of machine learning methods are often not only interested in getting accurate predictions or projections. Rather, as a decision-maker, the user also needs a convincing answer (or explanation) to the question of why a particular prediction was delivered. Explainable machine learning might be a legal requirement when used for decision making with an immediate effect on the health of human beings. As an example consider the computer vision of a self-driving car whose predictions are used to decide if to stop the car. We have recently proposed an information-theoretic approach to construct personalized explanations for predictions obtained from ML. This method was model-agnostic and only required some training samples of the model to be explained along with a user feedback signal. This paper uses an information-theoretic measure for the quality of an explanation to learn predictors that are intrinsically explainable to a specific user. Our approach is not restricted to a particular hypothesis space, such as linear maps or shallow decision trees, whose predictor maps are considered as explainable by definition. Rather, we regularize an arbitrary hypothesis space using a personalized measure for the explainability of a particular predictor.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{kakasAbductionArgumentationExplainable2020,
  title = {Abduction and {{Argumentation}} for {{Explainable Machine Learning}}: {{A Position Survey}}},
  shorttitle = {Abduction and {{Argumentation}} for {{Explainable Machine Learning}}},
  author = {Kakas, Antonis and Michael, Loizos},
  year = {2020-10-24},
  __url = {https://arxiv.org/abs/2010.12896v1},
  _urldate = {2020-11-30},
  abstract = {This paper presents Abduction and Argumentation as two principled forms for reasoning, and fleshes out the fundamental role that they can play within Machine Learning. It reviews the state-of-the-art work over the past few decades on the link of these two reasoning forms with machine learning work, and from this it elaborates on how the explanation-generating role of Abduction and Argumentation makes them naturally-fitting mechanisms for the development of Explainable Machine Learning and AI systems. Abduction contributes towards this goal by facilitating learning through the transformation, preparation, and homogenization of data. Argumentation, as a conservative extension of classical deductive reasoning, offers a flexible prediction and coverage mechanism for learning -- an associated target language for learned knowledge -- that explicitly acknowledges the need to deal, in the context of learning, with uncertain, incomplete and inconsistent data that are incompatible with any classically-represented logical theory.},
  langid = {english}
}

@unpublished{kamathDoesInvariantRisk2021,
  title = {Does {{Invariant Risk Minimization Capture Invariance}}?},
  author = {Kamath, Pritish and Tangella, Akilesh and Sutherland, Danica J. and Srebro, Nathan},
  year = {2021-02-26},
  eprint = {2101.01134},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2101.01134},
  _urldate = {2021-05-12},
  abstract = {We show that the Invariant Risk Minimization (IRM) formulation of Arjovsky et al. (2019) can fail to capture "natural" invariances, at least when used in its practical "linear" form, and even on very simple problems which directly follow the motivating examples for IRM. This can lead to worse generalization on new environments, even when compared to unconstrained ERM. The issue stems from a significant gap between the linear variant (as in their concrete method IRMv1) and the full non-linear IRM formulation. Additionally, even when capturing the "right" invariances, we show that it is possible for IRM to learn a sub-optimal predictor, due to the loss function not being invariant across environments. The issues arise even when measuring invariance on the population distributions, but are exacerbated by the fact that IRM is extremely fragile to sampling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/8D5P2LNA/Kamath et al. - 2021 - Does Invariant Risk Minimization Capture Invarianc.pdf;/home/rick/Zotero/storage/CNRP7LUF/2101.html}
}

@inproceedings{kaurInterpretingInterpretabilityUnderstanding2020,
  title = {Interpreting {{Interpretability}}: {{Understanding Data Scientists}}' {{Use}} of {{Interpretability Tools}} for {{Machine Learning}}},
  shorttitle = {Interpreting {{Interpretability}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Wortman Vaughan, Jennifer},
  year = {2020-04-21},
  series = {{{CHI}} '20},
  pages = {1--14},
  publisher = {{Association for Computing Machinery}},
  location = {{Honolulu, HI, USA}},
  _doi = {10.1145/3313831.3376219},
  __url = {https://doi.org/10.1145/3313831.3376219},
  _urldate = {2020-10-22},
  abstract = {Machine learning (ML) models are now routinely deployed in domains ranging from criminal justice to healthcare. With this newfound ubiquity, ML has moved beyond academia and grown into an engineering discipline. To that end, interpretability tools have been designed to help data scientists and machine learning practitioners better understand how ML models work. However, there has been little evaluation of the extent to which these tools achieve this goal. We study data scientists' use of two existing interpretability tools, the InterpretML implementation of GAMs and the SHAP Python package. We conduct a contextual inquiry (N=11) and a survey (N=197) of data scientists to observe how they use interpretability tools to uncover common issues that arise when building and evaluating ML models. Our results indicate that data scientists over-trust and misuse interpretability tools. Furthermore, few of our participants were able to accurately describe the visualizations output by these tools. We highlight qualitative themes for data scientists' mental models of interpretability tools. We conclude with implications for researchers and tool designers, and contextualize our findings in the social science literature.},
  _isbn = {978-1-4503-6708-0},
  keywords = {interpretability,machine learning,user-centric evaluation}
}

@unpublished{keaneHowCaseBased2019,
  title = {How {{Case Based Reasoning Explained Neural Networks}}: {{An XAI Survey}} of {{Post-Hoc Explanation-by-Example}} in {{ANN-CBR Twins}}},
  shorttitle = {How {{Case Based Reasoning Explained Neural Networks}}},
  author = {Keane, Mark T. and Kenny, Eoin M.},
  year = {2019-05-17},
  eprint = {1905.07186},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1905.07186},
  _urldate = {2020-10-26},
  abstract = {This paper surveys an approach to the XAI problem, using post-hoc explanation by example, that hinges on twinning Artificial Neural Networks (ANNs) with Case-Based Reasoning (CBR) systems, so-called ANN-CBR twins. A systematic survey of 1100+ papers was carried out to identify the fragmented literature on this topic and to trace it influence through to more recent work involving Deep Neural Networks (DNNs). The paper argues that this twin-system approach, especially using ANN-CBR twins, presents one possible coherent, generic solution to the XAI problem (and, indeed, XCBR problem). The paper concludes by road-mapping some future directions for this XAI solution involving (i) further tests of feature-weighting techniques, (iii) explorations of how explanatory cases might best be deployed (e.g., in counterfactuals, near-miss cases, a fortori cases), and (iii) the raising of the unwelcome and, much ignored, issue of human user evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@unpublished{kehrenbergNullsamplingInterpretableFair2020,
  title = {Null-Sampling for {{Interpretable}} and {{Fair Representations}}},
  author = {Kehrenberg, Thomas and Bartlett, Myles and Thomas, Oliver and Quadrianto, Novi},
  year = {2020-08-12},
  eprint = {2008.05248},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2008.05248},
  _urldate = {2020-09-16},
  abstract = {We propose to learn invariant representations, in the data domain, to achieve interpretability in algorithmic fairness. Invariance implies a selectivity for high level, relevant correlations w.r.t. class label annotations, and a robustness to irrelevant correlations with protected characteristics such as race or gender. We introduce a non-trivial setup in which the training set exhibits a strong bias such that class label annotations are irrelevant and spurious correlations cannot be distinguished. To address this problem, we introduce an adversarially trained model with a null-sampling procedure to produce invariant representations in the data domain. To enable disentanglement, a partially-labelled representative set is used. By placing the representations into the data domain, the changes made by the model are easily examinable by human auditors. We show the effectiveness of our method on both image and tabular datasets: Coloured MNIST, the CelebA and the Adult dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{kessyOptimalWhiteningDecorrelation2018,
  title = {Optimal Whitening and Decorrelation},
  author = {Kessy, Agnan and Lewin, Alex and Strimmer, Korbinian},
  year = {2018-10-02},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {72},
  number = {4},
  eprint = {1512.00809},
  eprinttype = {arxiv},
  pages = {309--314},
  _issn = {0003-1305, 1537-2731},
  _doi = {10.1080/00031305.2016.1277159},
  __url = {http://arxiv.org/abs/1512.00809},
  _urldate = {2021-08-13},
  abstract = {Whitening, or sphering, is a common preprocessing step in statistical analysis to transform random variables to orthogonality. However, due to rotational freedom there are infinitely many possible whitening procedures. Consequently, there is a diverse range of sphering methods in use, for example based on principal component analysis (PCA), Cholesky matrix decomposition and zero-phase component analysis (ZCA), among others. Here we provide an overview of the underlying theory and discuss five natural whitening procedures. Subsequently, we demonstrate that investigating the cross-covariance and the cross-correlation matrix between sphered and original variables allows to break the rotational invariance and to identify optimal whitening transformations. As a result we recommend two particular approaches: ZCA-cor whitening to produce sphered variables that are maximally similar to the original variables, and PCA-cor whitening to obtain sphered variables that maximally compress the original variables.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning,Statistics - Methodology}
}

@unpublished{khademiCausalLensPeeking2020,
  title = {A {{Causal Lens}} for {{Peeking}} into {{Black Box Predictive Models}}: {{Predictive Model Interpretation}} via {{Causal Attribution}}},
  shorttitle = {A {{Causal Lens}} for {{Peeking}} into {{Black Box Predictive Models}}},
  author = {Khademi, Aria and Honavar, Vasant},
  year = {2020-08-01},
  eprint = {2008.00357},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2008.00357},
  _urldate = {2020-09-16},
  abstract = {With the increasing adoption of predictive models trained using machine learning across a wide range of high-stakes applications, e.g., health care, security, criminal justice, finance, and education, there is a growing need for effective techniques for explaining such models and their predictions. We aim to address this problem in settings where the predictive model is a black box; That is, we can only observe the response of the model to various inputs, but have no knowledge about the internal structure of the predictive model, its parameters, the objective function, and the algorithm used to optimize the model. We reduce the problem of interpreting a black box predictive model to that of estimating the causal effects of each of the model inputs on the model output, from observations of the model inputs and the corresponding outputs. We estimate the causal effects of model inputs on model output using variants of the Rubin Neyman potential outcomes framework for estimating causal effects from observational data. We show how the resulting causal attribution of responsibility for model output to the different model inputs can be used to interpret the predictive model and to explain its predictions. We present results of experiments that demonstrate the effectiveness of our approach to the interpretation of black box predictive models via causal attribution in the case of deep neural network models trained on one synthetic data set (where the input variables that impact the output variable are known by design) and two real-world data sets: Handwritten digit classification, and Parkinson's disease severity prediction. Because our approach does not require knowledge about the predictive model algorithm and is free of assumptions regarding the black box predictive model except that its input-output responses be observable, it can be applied, in principle, to any black box predictive model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{kimInterpretabilityFeatureAttribution2018,
  title = {Interpretability {{Beyond Feature Attribution}}: {{Quantitative Testing}} with {{Concept Activation Vectors}} ({{TCAV}})},
  shorttitle = {Interpretability {{Beyond Feature Attribution}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  year = {2018},
  pages = {2668--2677},
  publisher = {{PMLR}},
  __url = {http://proceedings.mlr.press/v80/kim18d.html},
  _urldate = {2020-12-07},
  abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level ...},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english}
}


@inproceedings{kindermansLearningHowExplain2017,
title={Learning how to explain neural networks: PatternNet and PatternAttribution},
author={Pieter-Jan Kindermans and Kristof T. Schütt and Maximilian Alber and Klaus-Robert Müller and Dumitru Erhan and Been Kim and Sven Dähne},
booktitle={International Conference on Learning Representations},
year={2018},
__url={https://openreview.net/forum?id=Hkn7CBaTW},
}

@incollection{kindermans2019reliability,
  title={The (un) reliability of saliency methods},
  author={Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Sch{\"u}tt, Kristof T and D{\"a}hne, Sven and Erhan, Dumitru and Kim, Been},
  booktitle={Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
  pages={267--280},
  year={2019},
  publisher={Springer}
}

@unpublished{k_issnerHackingNeuralNetworks2019,
  title = {Hacking {{Neural Networks}}: {{A Short Introduction}}},
  shorttitle = {Hacking {{Neural Networks}}},
  author = {K_issner, Michael},
  year = {2019-12-01},
  eprint = {1911.07658},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1911.07658},
  _urldate = {2020-04-20},
  abstract = {A large chunk of research on the security issues of neural networks is focused on adversarial attacks. However, there exists a vast sea of simpler attacks one can perform both against and with neural networks. In this article, we give a quick introduction on how deep learning in security works and explore the basic methods of exploitation, but also look at the offensive capabilities deep learning enabled tools provide. All presented attacks, such as backdooring, GPU-based buffer overflows or automated bug hunting, are accompanied by short open-source exercises for anyone to try out.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@unpublished{klaiseMonitoringExplainabilityModels2020,
  title = {Monitoring and Explainability of Models in Production},
  author = {Klaise, Janis and Van Looveren, Arnaud and Cox, Clive and Vacanti, Giovanni and Coca, Alexandru},
  year = {2020-07-13},
  eprint = {2007.06299},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.06299},
  _urldate = {2020-08-05},
  abstract = {The machine learning lifecycle extends beyond the deployment stage. Monitoring deployed models is crucial for continued provision of high quality machine learning enabled services. Key areas include model performance and data monitoring, detecting outliers and data drift using statistical techniques, and providing explanations of historic predictions. We discuss the challenges to successful implementation of solutions in each of these areas with some recent examples of production ready solutions using open source tools.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{kleinerFalsificationConsciousness2020,
  title = {Falsification and Consciousness},
  author = {Kleiner, Johannes and Hoel, Erik},
  year = {2020-07-09},
  eprint = {2004.03541},
  eprinttype = {arxiv},
  primaryclass = {q-bio},
  __url = {http://arxiv.org/abs/2004.03541},
  _urldate = {2020-10-08},
  abstract = {The search for a scientific theory of consciousness should result in theories that are falsifiable. However, here we show that falsification is especially problematic for theories of consciousness. We formally describe the standard experimental setup for testing these theories. Based on a theory's application to some physical system, such as the brain, testing requires comparing a theory's predicted experience (given some internal observables of the system like brain imaging data) with an inferred experience (using report or behavior). If there is a mismatch between inference and prediction, a theory is falsified. We show that if inference and prediction are independent, it follows that any minimally informative theory of consciousness is automatically falsified. This is deeply problematic since the field's reliance on report or behavior to infer conscious experiences implies such independence, so this fragility affects many contemporary theories of consciousness. Furthermore, we show that if inference and prediction are strictly dependent, it follows that a theory is unfalsifiable. This affects theories which claim consciousness to be determined by report or behavior. Finally, we explore possible ways out of this dilemma.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Biology - Neurons and Cognition}
}

@unpublished{kokhlikyanCaptumUnifiedGeneric2020,
  title = {Captum: {{A}} Unified and Generic Model Interpretability Library for {{PyTorch}}},
  shorttitle = {Captum},
  author = {Kokhlikyan, Narine and Miglani, Vivek and Martin, Miguel and Wang, Edward and Alsallakh, Bilal and Reynolds, Jonathan and Melnikov, Alexander and Kliushkina, Natalia and Araya, Carlos and Yan, Siqi and Reblitz-Richardson, Orion},
  year = {2020-09-16},
  eprint = {2009.07896},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2009.07896},
  _urldate = {2020-11-30},
  abstract = {In this paper we introduce a novel, unified, open-source model interpretability library for PyTorch [12]. The library contains generic implementations of a number of gradient and perturbation-based attribution algorithms, also known as feature, neuron and layer importance algorithms, as well as a set of evaluation metrics for these algorithms. It can be used for both classification and non-classification models including graph-structured models built on Neural Networks (NN). In this paper we give a high-level overview of supported attribution algorithms and show how to perform memory-efficient and scalable computations. We emphasize that the three main characteristics of the library are multimodality, extensibility and ease of use. Multimodality supports different modality of inputs such as image, text, audio or video. Extensibility allows adding new algorithms and features. The library is also designed for easy understanding and use. Besides, we also introduce an interactive visualization tool called Captum Insights that is built on top of Captum library and allows sample-based model debugging and visualization using feature importance metrics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{konigRelativeFeatureImportance2020,
  title = {Relative {{Feature Importance}}},
  author = {König, Gunnar and Molnar, Christoph and Bischl, Bernd and Grosse-Wentrup, Moritz},
  year = {2020-07-16},
  eprint = {2007.08283},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.08283},
  _urldate = {2020-08-05},
  abstract = {Interpretable Machine Learning (IML) methods are used to gain insight into the relevance of a feature of interest for the performance of a model. Commonly used IML methods differ in whether they consider features of interest in isolation, e.g., Permutation Feature Importance (PFI), or in relation to all remaining feature variables, e.g., Conditional Feature Importance (CFI). As such, the perturbation mechanisms inherent to PFI and CFI represent extreme reference points. We introduce Relative Feature Importance (RFI), a generalization of PFI and CFI that allows for a more nuanced feature importance computation beyond the PFI versus CFI dichotomy. With RFI, the importance of a feature relative to any other subset of features can be assessed, including variables that were not available at training time. We derive general interpretation rules for RFI based on a detailed theoretical analysis of the implications of relative feature relevance, and demonstrate the method's usefulness on simulated examples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{krauseInteractingPredictionsVisual2016,
  title = {Interacting with {{Predictions}}: {{Visual Inspection}} of {{Black-box Machine Learning Models}}},
  shorttitle = {Interacting with {{Predictions}}},
  booktitle = {Proceedings of the 2016 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Krause, Josua and Perer, Adam and Ng, Kenney},
  year = {2016-05-07},
  series = {{{CHI}} '16},
  pages = {5686--5697},
  publisher = {{Association for Computing Machinery}},
  location = {{San Jose, California, USA}},
  _doi = {10.1145/2858036.2858529},
  __url = {https://doi.org/10.1145/2858036.2858529},
  _urldate = {2020-11-04},
  abstract = {Understanding predictive models, in terms of interpreting and identifying actionable insights, is a challenging task. Often the importance of a feature in a model is only a rough estimate condensed into one number. However, our research goes beyond these naïve estimates through the design and implementation of an interactive visual analytics system, Prospector. By providing interactive partial dependence diagnostics, data scientists can understand how features affect the prediction overall. In addition, our support for localized inspection allows data scientists to understand how and why specific datapoints are predicted as they are, as well as support for tweaking feature values and seeing how the prediction responds. Our system is then evaluated using a case study involving a team of data scientists improving predictive models for detecting the onset of diabetes from electronic medical records.},
  _isbn = {978-1-4503-3362-7},
  keywords = {interactive machine learning,partial dependence,predictive modeling}
}

@incollection{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  pages = {1097--1105},
  publisher = {{Curran Associates, Inc.}},
  __url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  _urldate = {2020-11-04}
}

@inproceedings{kumarNILENaturalLanguage2020,
  title = {{{NILE}} : {{Natural Language Inference}} with {{Faithful Natural Language Explanations}}},
  shorttitle = {{{NILE}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Kumar, Sawan and Talukdar, Partha},
  year = {2020},
  pages = {8730--8742},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  _doi = {10.18653/v1/2020.acl-main.771},
  __url = {https://www.aclweb.org/anthology/2020.acl-main.771},
  _urldate = {2022-03-23},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  file = {/home/rick/Zotero/storage/YVP6ZQXZ/Kumar and Talukdar - 2020 - NILE  Natural Language Inference with Faithful Na.pdf}
}

@unpublished{kumarProblemsShapleyvaluebasedExplanations2020,
  title = {Problems with {{Shapley-value-based}} Explanations as Feature Importance Measures},
  author = {Kumar, I. Elizabeth and Venkatasubramanian, Suresh and Scheidegger, Carlos and Friedler, Sorelle},
  year = {2020-06-30},
  eprint = {2002.11097},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2002.11097},
  _urldate = {2020-10-15},
  abstract = {Game-theoretic formulations of feature importance have become popular as a way to "explain" machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game's unique Shapley values. Justification for these methods rests on two pillars: their desirable mathematical properties, and their applicability to specific motivations for explanations. We show that mathematical problems arise when Shapley values are used for feature importance and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values do not provide explanations which suit human-centric goals of explainability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{lageEvaluationHumanInterpretabilityExplanation2019,
  title = {An {{Evaluation}} of the {{Human-Interpretability}} of {{Explanation}}},
  author = {Lage, Isaac and Chen, Emily and He, Jeffrey and Narayanan, Menaka and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
  year = {2019-08-28},
  eprint = {1902.00006},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1902.00006},
  _urldate = {2020-10-22},
  abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable under three specific tasks that users may perform with machine learning systems: simulation of the response, verification of a suggested response, and determining whether the correctness of a suggested response changes under a change to the inputs. Through carefully controlled human-subject experiments, we identify regularizers that can be used to optimize for the interpretability of machine learning systems. Our results show that the type of complexity matters: cognitive chunks (newly defined concepts) affect performance more than variable repetitions, and these trends are consistent across tasks and domains. This suggests that there may exist some common design principles for explanation systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@incollection{lageHumanintheLoopInterpretabilityPrior2018,
  title = {Human-in-the-{{Loop Interpretability Prior}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Lage, Isaac and Ross, Andrew and Gershman, Samuel J and Kim, Been and Doshi-Velez, Finale},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
  year = {2018},
  pages = {10159--10168},
  publisher = {{Curran Associates, Inc.}},
  __url = {http://papers.nips.cc/paper/8219-human-in-the-loop-interpretability-prior.pdf},
  _urldate = {2020-10-22}
}

@article{lakkarajuRobustStableBlack2020,
  title = {Robust and {{Stable Black Box Explanations}}},
  author = {Lakkaraju, Himabindu and Arsov, Nino and Bastani, Osbert},
  year = {2020-11-12},
  __url = {https://arxiv.org/abs/2011.06169v1},
  _urldate = {2020-11-30},
  abstract = {As machine learning black boxes are increasingly being deployed in real-world applications, there has been a growing interest in developing post hoc explanations that summarize the behaviors of these black boxes. However, existing algorithms for generating such explanations have been shown to lack stability and robustness to distribution shifts. We propose a novel framework for generating robust and stable explanations of black box models based on adversarial training. Our framework optimizes a minimax objective that aims to construct the highest fidelity explanation with respect to the worst-case over a set of adversarial perturbations. We instantiate this algorithm for explanations in the form of linear models and decision sets by devising the required optimization procedures. To the best of our knowledge, this work makes the first attempt at generating post hoc explanations that are robust to a general class of adversarial perturbations that are of practical interest. Experimental evaluation with real-world and synthetic datasets demonstrates that our approach substantially improves robustness of explanations without sacrificing their fidelity on the original data distribution.},
  langid = {english}
}

@unpublished{lampleDeepLearningSymbolic2019,
  title = {Deep {{Learning}} for {{Symbolic Mathematics}}},
  author = {Lample, Guillaume and Charton, François},
  year = {2019-12-02},
  eprint = {1912.01412},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1912.01412},
  _urldate = {2020-06-01},
  abstract = {Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Symbolic Computation}
}

@unpublished{lanProbabilisticRepresentationDeep2020,
  title = {A {{Probabilistic Representation}} of {{Deep Learning}} for {{Improving The Information Theoretic Interpretability}}},
  author = {Lan, Xinjie and Barner, Kenneth E.},
  year = {2020-10-27},
  eprint = {2010.14054},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  __url = {http://arxiv.org/abs/2010.14054},
  _urldate = {2020-11-30},
  abstract = {In this paper, we propose a probabilistic representation of MultiLayer Perceptrons (MLPs) to improve the information-theoretic interpretability. Above all, we demonstrate that the activations being i.i.d. is not valid for all the hidden layers of MLPs, thus the existing mutual information estimators based on non-parametric inference methods, e.g., empirical distributions and Kernel Density Estimate (KDE), are invalid for measuring the information flow in MLPs. Moreover, we introduce explicit probabilistic explanations for MLPs: (i) we define the probability space (Omega\_F, t, P\_F) for a fully connected layer f and demonstrate the great effect of an activation function on the probability measure P\_F ; (ii) we prove the entire architecture of MLPs as a Gibbs distribution P; and (iii) the back-propagation aims to optimize the sample space Omega\_F of all the fully connected layers of MLPs for learning an optimal Gibbs distribution P* to express the statistical connection between the input and the label. Based on the probabilistic explanations for MLPs, we improve the information-theoretic interpretability of MLPs in three aspects: (i) the random variable of f is discrete and the corresponding entropy is finite; (ii) the information bottleneck theory cannot correctly explain the information flow in MLPs if we take into account the back-propagation; and (iii) we propose novel information-theoretic explanations for the generalization of MLPs. Finally, we demonstrate the proposed probabilistic representation and information-theoretic explanations for MLPs in a synthetic dataset and benchmark datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning}
}

@article{lapuschkinUnmaskingCleverHans2019,
  title = {Unmasking {{Clever Hans}} Predictors and Assessing What Machines Really Learn},
  author = {Lapuschkin, Sebastian and Wäldchen, Stephan and Binder, Alexander and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
  year = {2019},
  journaltitle = {Nature Communications},
  journal = {Nature Communications},
  shortjournal = {Nature Communications},
  volume = {10},
  number = {1},
  pages = {1096},
}

@unpublished{leavittFalsifiableInterpretabilityResearch2020,
  title = {Towards Falsifiable Interpretability Research},
  author = {Leavitt, Matthew L. and Morcos, Ari},
  year = {2020-10-22},
  eprint = {2010.12016},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2010.12016},
  _urldate = {2020-11-30},
  abstract = {Methods for understanding the decisions of and mechanisms underlying deep neural networks (DNNs) typically rely on building intuition by emphasizing sensory or semantic features of individual examples. For instance, methods aim to visualize the components of an input which are "important" to a network's decision, or to measure the semantic properties of single neurons. Here, we argue that interpretability research suffers from an over-reliance on intuition-based approaches that risk-and in some cases have caused-illusory progress and misleading conclusions. We identify a set of limitations that we argue impede meaningful progress in interpretability research, and examine two popular classes of interpretability methods-saliency and single-neuron-based approaches-that serve as case studies for how overreliance on intuition and lack of falsifiability can undermine interpretability research. To address these concerns, we propose a strategy to address these impediments in the form of a framework for strongly falsifiable interpretability research. We encourage researchers to use their intuitions as a starting point to develop and test clear, falsifiable hypotheses, and hope that our framework yields robust, evidence-based interpretability methods that generate meaningful advances in our understanding of DNNs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{leeDimensionalityReductionClustering2007,
  title = {Dimensionality Reduction and Clustering on Statistical Manifolds},
  booktitle = {2007 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Lee, Sang-Mook and Abbott, A Lynn and Araman, Philip A},
  year = {2007},
  pages = {1--7},
  publisher = {{IEEE}}
}

@article{lethamInterpretableClassifiersUsing2015,
  title = {Interpretable Classifiers Using Rules and {{Bayesian}} Analysis: {{Building}} a Better Stroke Prediction Model},
  shorttitle = {Interpretable Classifiers Using Rules and {{Bayesian}} Analysis},
  author = {Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H. and Madigan, David},
  year = {2015-09},
  journaltitle = {Annals of Applied Statistics},
  shortjournal = {Ann. Appl. Stat.},
  volume = {9},
  number = {3},
  pages = {1350--1371},
  _issn = {1932-6157, 1941-7330},
  _doi = {10.1214/15-AOAS848},
  __url = {https://projecteuclid.org/euclid.aoas/1446488742},
  _urldate = {2020-11-04},
  abstract = {We aim to produce predictive models that are not only accurate, but are also interpretable to human experts. Our models are decision lists, which consist of a series of if…then…statements (e.g., if high blood pressure, then stroke) that discretize a high-dimensional, multivariate feature space into a series of simple, readily interpretable decision statements. We introduce a generative model called Bayesian Rule Lists that yields a posterior distribution over possible decision lists. It employs a novel prior structure to encourage sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy on par with the current top algorithms for prediction in machine learning. Our method is motivated by recent developments in personalized medicine, and can be used to produce highly accurate and interpretable medical scoring systems. We demonstrate this by producing an alternative to the CHADS\$\_\{2\}\$ score, actively used in clinical practice for estimating the risk of stroke in patients that have atrial fibrillation. Our model is as interpretable as CHADS\$\_\{2\}\$, but more accurate.},
  langid = {english},
  mrnumber = {MR3418726},
  zmnumber = {06525989},
  keywords = {Bayesian analysis,classification,interpretability}
}

@article{lewisSuppressionEnhancementBivariate1986,
  title = {Suppression and {{Enhancement}} in {{Bivariate Regression}}},
  author = {Lewis, Jerry W. and Escobar, Luis A.},
  year = {1986},
  journaltitle = {The Statistician},
  shortjournal = {The Statistician},
  volume = {35},
  number = {1},
  eprint = {10.2307/2988294},
  eprinttype = {jstor},
  pages = {17},
  _issn = {00390526},
  _doi = {10.2307/2988294}
}

@inproceedings{liaoQuestioningAIInforming2020,
  title = {Questioning the {{AI}}: {{Informing Design Practices}} for {{Explainable AI User Experiences}}},
  shorttitle = {Questioning the {{AI}}},
  booktitle = {Proceedings of the 2020 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Liao, Q. Vera and Gruen, Daniel and Miller, Sarah},
  year = {2020-04-21},
  series = {{{CHI}} '20},
  pages = {1--15},
  publisher = {{Association for Computing Machinery}},
  location = {{Honolulu, HI, USA}},
  _doi = {10.1145/3313831.3376590},
  __url = {https://doi.org/10.1145/3313831.3376590},
  _urldate = {2020-10-22},
  abstract = {A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI.},
  _isbn = {978-1-4503-6708-0},
  keywords = {explainable AI,human-AI interaction,user experience}
}

@unpublished{liDeepLearningCaseBased2018,
  title = {Deep {{Learning}} for {{Case-Based Reasoning}} through {{Prototypes}}: {{A Neural Network}} That {{Explains Its Predictions}}},
  shorttitle = {Deep {{Learning}} for {{Case-Based Reasoning}} through {{Prototypes}}},
  author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
  year = {2018},
  eprint = {1710.04806},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1710.04806},
  _urldate = {2020-12-07},
  abstract = {Deep neural networks are widely used for classification. These deep models often suffer from a lack of interpretability -- they are particularly difficult to understand because of their non-linear nature. As a result, neural networks are often treated as "black box" models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used as part of the classification process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{liDeepLearningCaseBased2018a,
  title = {Deep {{Learning}} for {{Case-Based Reasoning}} through {{Prototypes}}: {{A Neural Network}} That {{Explains Its Predictions}}},
  shorttitle = {Deep {{Learning}} for {{Case-Based Reasoning}} through {{Prototypes}}},
  author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
  year = {2018},
  eprint = {1710.04806},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1710.04806},
  _urldate = {2020-12-07},
  abstract = {Deep neural networks are widely used for classification. These deep models often suffer from a lack of interpretability – they are particularly difficult to understand because of their non-linear nature. As a result, neural networks are often treated as "black box" models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used as part of the classification process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{liLearningTheoreticPerspective2020,
  title = {A {{Learning Theoretic Perspective}} on {{Local Explainability}}},
  author = {Li, Jeffrey and Nagarajan, Vaishnavh and Plumb, Gregory and Talwalkar, Ameet},
  year = {2020-11-02},
  __url = {https://arxiv.org/abs/2011.01205v1},
  _urldate = {2020-11-30},
  abstract = {In this paper, we explore connections between interpretable machine learning and learning theory through the lens of local approximation explanations. First, we tackle the traditional problem of performance generalization and bound the test-time accuracy of a model using a notion of how locally explainable it is. Second, we explore the novel problem of explanation generalization which is an important concern for a growing class of finite sample-based local approximation explanations. Finally, we validate our theoretical results empirically and show that they reflect what can be seen in practice.},
  langid = {english}
}

@unpublished{lillicrapWhatDoesIt2019,
  title = {What Does It Mean to Understand a Neural Network?},
  author = {Lillicrap, Timothy P. and Kording, Konrad P.},
  year = {2019-07-15},
  eprint = {1907.06374},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio, stat},
  __url = {http://arxiv.org/abs/1907.06374},
  _urldate = {2020-04-20},
  abstract = {We can define a neural network that can learn to recognize objects in less than 100 lines of code. However, after training, it is characterized by millions of weights that contain the knowledge about many object types across visual scenes. Such networks are thus dramatically easier to understand in terms of the code that makes them than the resulting properties, such as tuning or connections. In analogy, we conjecture that rules for development and learning in brains may be far easier to understand than their resulting properties. The analogy suggests that neuroscience would benefit from a focus on learning and development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning}
}

@unpublished{linWhatYouSee2020,
  title = {What {{Do You See}}? {{Evaluation}} of {{Explainable Artificial Intelligence}} ({{XAI}}) {{Interpretability}} through {{Neural Backdoors}}},
  shorttitle = {What {{Do You See}}?},
  author = {Lin, Yi-Shan and Lee, Wen-Chuan and Celik, Z. Berkay},
  year = {2020-09-22},
  eprint = {2009.10639},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2009.10639},
  _urldate = {2020-11-30},
  abstract = {EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the parts of the inputs deemed important to arrive a decision at a specific target. However, it remains challenging to quantify correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for systematic evaluation of explanations that an XAI method generates and evaluate seven state-of-the-art model-free and model-specific posthoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We discovered six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{lipovetskyAnalysisRegressionGame2001,
  title = {Analysis of Regression in Game Theory Approach},
  author = {Lipovetsky, Stan and Conklin, Michael},
  year = {2001},
  journaltitle = {Applied Stochastic Models in Business and Industry},
  journal = {Applied Stochastic Models in Business and Industry},
  volume = {17},
  number = {4},
  pages = {319--330},
  _issn = {1526-4025},
  _doi = {10.1002/asmb.446},
  __url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.446},
  _urldate = {2021-05-12},
  abstract = {Working with multiple regression analysis a researcher usually wants to know a comparative importance of predictors in the model. However, the analysis can be made difficult because of multicollinearity among regressors, which produces biased coefficients and negative inputs to multiple determination from presum ably useful regressors. To solve this problem we apply a tool from the co-operative games theory, the Shapley Value imputation. We demonstrate the theoretical and practical advantages of the Shapley Value and show that it provides consistent results in the presence of multicollinearity. Copyright © 2001 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {co-operative games,multicollinearity,regressors net effects,Shapley Value},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asmb.446},
  file = {/home/rick/Zotero/storage/BEVJEAZE/asmb.html}
}


@unpublished{liptonMythosModelInterpretability2017,
  title = {The {{Mythos}} of {{Model Interpretability}}},
  author = {Lipton, Zachary C.},
  year = {2017-03-06},
  eprint = {1606.03490},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1606.03490},
  _urldate = {2020-04-20},
  abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@unpublished{liuAbsoluteShapleyValue2020,
  title = {Absolute {{Shapley Value}}},
  author = {Liu, Jinfei},
  year = {2020-03-23},
  eprint = {2003.10076},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2003.10076},
  _urldate = {2020-07-09},
  abstract = {Shapley value is a concept in cooperative game theory for measuring the contribution of each participant, which was named in honor of Lloyd Shapley. Shapley value has been recently applied in data marketplaces for compensation allocation based on their contribution to the models. Shapley value is the only value division scheme used for compensation allocation that meets three desirable criteria: group rationality, fairness, and additivity. In cooperative game theory, the marginal contribution of each contributor to each coalition is a nonnegative value. However, in machine learning model training, the marginal contribution of each contributor (data tuple) to each coalition (a set of data tuples) can be a negative value, i.e., the accuracy of the model trained by a dataset with an additional data tuple can be lower than the accuracy of the model trained by the dataset only. In this paper, we investigate the problem of how to handle the negative marginal contribution when computing Shapley value. We explore three philosophies: 1) taking the original value (Original Shapley Value); 2) taking the larger of the original value and zero (Zero Shapley Value); and 3) taking the absolute value of the original value (Absolute Shapley Value). Experiments on Iris dataset demonstrate that the definition of Absolute Shapley Value significantly outperforms the other two definitions in terms of evaluating data importance (the contribution of each data tuple to the trained model).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{liuAreInterpretationsFairly2020,
  title = {Are {{Interpretations Fairly Evaluated}}? {{A Definition Driven Pipeline}} for {{Post-Hoc Interpretability}}},
  shorttitle = {Are {{Interpretations Fairly Evaluated}}?},
  author = {Liu, Ninghao and Meng, Yunsong and Hu, Xia and Wang, Tie and Long, Bo},
  year = {2020-09-16},
  eprint = {2009.07494},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2009.07494},
  _urldate = {2020-11-30},
  abstract = {Recent years have witnessed an increasing number of interpretation methods being developed for improving transparency of NLP models. Meanwhile, researchers also try to answer the question that whether the obtained interpretation is faithful in explaining mechanisms behind model prediction? Specifically, (Jain and Wallace, 2019) proposes that "attention is not explanation" by comparing attention interpretation with gradient alternatives. However, it raises a new question that can we safely pick one interpretation method as the ground-truth? If not, on what basis can we compare different interpretation methods? In this work, we propose that it is crucial to have a concrete definition of interpretation before we could evaluate faithfulness of an interpretation. The definition will affect both the algorithm to obtain interpretation and, more importantly, the metric used in evaluation. Through both theoretical and experimental analysis, we find that although interpretation methods perform differently under a certain evaluation metric, such a difference may not result from interpretation quality or faithfulness, but rather the inherent bias of the evaluation metric.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@unpublished{liuImpactAccuracyModel2020,
  title = {Impact of {{Accuracy}} on {{Model Interpretations}}},
  author = {Liu, Brian and Udell, Madeleine},
  year = {2020-11-17},
  eprint = {2011.09903},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2011.09903},
  _urldate = {2020-11-30},
  abstract = {Model interpretations are often used in practice to extract real world insights from machine learning models. These interpretations have a wide range of applications; they can be presented as business recommendations or used to evaluate model bias. It is vital for a data scientist to choose trustworthy interpretations to drive real world impact. Doing so requires an understanding of how the accuracy of a model impacts the quality of standard interpretation tools. In this paper, we will explore how a model's predictive accuracy affects interpretation quality. We propose two metrics to quantify the quality of an interpretation and design an experiment to test how these metrics vary with model accuracy. We find that for datasets that can be modeled accurately by a variety of methods, simpler methods yield higher quality interpretations. We also identify which interpretation method works the best for lower levels of model accuracy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@unpublished{lomonacoAvalancheEndtoEndLibrary2021,
  title = {Avalanche: An {{End-to-End Library}} for {{Continual Learning}}},
  shorttitle = {Avalanche},
  author = {Lomonaco, Vincenzo and Pellegrini, Lorenzo and Cossu, Andrea and Carta, Antonio and Graffieti, Gabriele and Hayes, Tyler L. and De Lange, Matthias and Masana, Marc and Pomponi, Jary and van de Ven, Gido and Mundt, Martin and She, Qi and Cooper, Keiland and Forest, Jeremy and Belouadah, Eden and Calderara, Simone and Parisi, German I. and Cuzzolin, Fabio and Tolias, Andreas and Scardapane, Simone and Antiga, Luca and Amhad, Subutai and Popescu, Adrian and Kanan, Christopher and van de Weijer, Joost and Tuytelaars, Tinne and Bacciu, Davide and Maltoni, Davide},
  options = {useprefix=true},
  year = {2021-04-01},
  eprint = {2104.00405},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2104.00405},
  _urldate = {2022-03-28},
  abstract = {Learning continually from non-stationary data streams is a long-standing goal and a challenging problem in machine learning. Recently, we have witnessed a renewed and fast-growing interest in continual learning, especially within the deep learning community. However, algorithmic solutions are often difficult to re-implement, evaluate and port across different settings, where even results on standard benchmarks are hard to reproduce. In this work, we propose Avalanche, an open-source end-to-end library for continual learning research based on PyTorch. Avalanche is designed to provide a shared and collaborative codebase for fast prototyping, training, and reproducible evaluation of continual learning algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/rick/Zotero/storage/C7HB6RA6/Lomonaco et al. - 2021 - Avalanche an End-to-End Library for Continual Lea.pdf;/home/rick/Zotero/storage/L9CF9TFC/2104.html}
}

@unpublished{lucieriExAIDMultimodalExplanation2022,
  title = {{{ExAID}}: {{A Multimodal Explanation Framework}} for {{Computer-Aided Diagnosis}} of {{Skin Lesions}}},
  shorttitle = {{{ExAID}}},
  author = {Lucieri, Adriano and Bajwa, Muhammad Naseer and Braun, Stephan Alexander and Malik, Muhammad Imran and Dengel, Andreas and Ahmed, Sheraz},
  year = {2022-01-04},
  eprint = {2201.01249},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  __url = {http://arxiv.org/abs/2201.01249},
  _urldate = {2022-01-17},
  abstract = {One principal impediment in the successful deployment of AI-based Computer-Aided Diagnosis (CAD) systems in clinical workflows is their lack of transparent decision making. Although commonly used eXplainable AI methods provide some insight into opaque algorithms, such explanations are usually convoluted and not readily comprehensible except by highly trained experts. The explanation of decisions regarding the malignancy of skin lesions from dermoscopic images demands particular clarity, as the underlying medical problem definition is itself ambiguous. This work presents ExAID (Explainable AI for Dermatology), a novel framework for biomedical image analysis, providing multi-modal concept-based explanations consisting of easy-to-understand textual explanations supplemented by visual maps justifying the predictions. ExAID relies on Concept Activation Vectors to map human concepts to those learnt by arbitrary Deep Learning models in latent space, and Concept Localization Maps to highlight concepts in the input space. This identification of relevant concepts is then used to construct fine-grained textual explanations supplemented by concept-wise location information to provide comprehensive and coherent multi-modal explanations. All information is comprehensively presented in a diagnostic interface for use in clinical routines. An educational mode provides dataset-level explanation statistics and tools for data and model exploration to aid medical research and education. Through rigorous quantitative and qualitative evaluation of ExAID, we show the utility of multi-modal explanations for CAD-assisted scenarios even in case of wrong predictions. We believe that ExAID will provide dermatologists an effective screening tool that they both understand and trust. Moreover, it will be the basis for similar applications in other biomedical imaging fields.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/home/rick/Zotero/storage/6EUVI45H/Lucieri et al. - 2022 - ExAID A Multimodal Explanation Framework for Comp.pdf;/home/rick/Zotero/storage/5SFSMJ4K/2201.html}
}

@article{lundbergLocalExplanationsGlobal2020,
  title = {From Local Explanations to Global Understanding with Explainable {{AI}} for Trees},
  author = {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
  year = {2020-01},
  journaltitle = {Nature Machine Intelligence},
  volume = {2},
  number = {1},
  pages = {56--67},
  _issn = {2522-5839},
  _doi = {10.1038/s42256-019-0138-9},
  __url = {https://www.nature.com/articles/s42256-019-0138-9},
  _urldate = {2020-10-19},
  abstract = {Tree-based machine learning models are widely used in domains such as healthcare, finance and public services. The authors present an explanation method for trees that enables the computation of optimal local explanations for individual predictions, and demonstrate their method on three medical datasets.},
  langid = {english}
}

@incollection{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Lundberg, Scott M and Lee, Su-In},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {4765--4774},
  publisher = {{Curran Associates, Inc.}},
  __url = {http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf},
  _urldate = {2020-10-07}
}

@unpublished{lussBetterModelUnderstanding2021,
  title = {Towards {{Better Model Understanding}} with {{Path-Sufficient Explanations}}},
  author = {Luss, Ronny and Dhurandhar, Amit},
  year = {2021-09-13},
  eprint = {2109.06181},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2109.06181},
  _urldate = {2021-09-17},
  abstract = {Feature based local attribution methods are amongst the most prevalent in explainable artificial intelligence (XAI) literature. Going beyond standard correlation, recently, methods have been proposed that highlight what should be minimally sufficient to justify the classification of an input (viz. pertinent positives). While minimal sufficiency is an attractive property, the resulting explanations are often too sparse for a human to understand and evaluate the local behavior of the model, thus making it difficult to judge its overall quality. To overcome these limitations, we propose a novel method called Path-Sufficient Explanations Method (PSEM) that outputs a sequence of sufficient explanations for a given input of strictly decreasing size (or value) -- from original input to a minimally sufficient explanation -- which can be thought to trace the local boundary of the model in a smooth manner, thus providing better intuition about the local model behavior for the specific input. We validate these claims, both qualitatively and quantitatively, with experiments that show the benefit of PSEM across all three modalities (image, tabular and text). A user study depicts the strength of the method in communicating the local behavior, where (many) users are able to correctly determine the prediction made by a model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@unpublished{macdonaldRateDistortionFrameworkExplaining2019,
  title = {A {{Rate-Distortion Framework}} for {{Explaining Neural Network Decisions}}},
  author = {Macdonald, Jan and Wäldchen, Stephan and Hauch, Sascha and Kutyniok, Gitta},
  year = {2019-05-27},
  eprint = {1905.11092},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  __url = {http://arxiv.org/abs/1905.11092},
  _urldate = {2020-04-20},
  abstract = {We formalise the widespread idea of interpreting neural network decisions as an explicit optimisation problem in a rate-distortion framework. A set of input features is deemed relevant for a classification decision if the expected classifier score remains nearly constant when randomising the remaining features. We discuss the computational complexity of finding small sets of relevant features and show that the problem is complete for \$\textbackslash mathsf\{NP\}\^\textbackslash mathsf\{PP\}\$, an important class of computational problems frequently arising in AI tasks. Furthermore, we show that it even remains \$\textbackslash mathsf\{NP\}\$-hard to only approximate the optimal solution to within any non-trivial approximation factor. Finally, we consider a continuous problem relaxation and develop a heuristic solution strategy based on assumed density filtering for deep ReLU neural networks. We present numerical experiments for two image classification data sets where we outperform established methods in particular for sparse explanations of neural network decisions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Complexity,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{macdonaldRateDistortionFrameworkExplaining2019a,
  title = {A {{Rate-Distortion Framework}} for {{Explaining Neural Network Decisions}}},
  author = {Macdonald, Jan and Wäldchen, Stephan and Hauch, Sascha and Kutyniok, Gitta},
  year = {2019-05},
  __url = {http://arxiv.org/abs/1905.11092},
  abstract = {We formalise the widespread idea of interpreting neural network decisions as an explicit optimisation problem in a rate-distortion framework. A set of input features is deemed relevant for a classification decision if the expected classifier score remains nearly constant when randomising the remaining features. We discuss the computational complexity of finding small sets of relevant features and show that the problem is complete for \$\textbackslash mathsfNP\^\textbackslash mathsfPP\$, an important class of computational problems frequently arising in AI tasks. Furthermore, we show that it even remains \$\textbackslash mathsfNP\$-hard to only approximate the optimal solution to within any non-trivial approximation factor. Finally, we consider a continuous problem relaxation and develop a heuristic solution strategy based on assumed density filtering for deep ReLU neural networks. We present numerical experiments for two image classification data sets where we outperform established methods in particular for sparse explanations of neural network decisions.},
  keywords = {Deep Neural Networks,Explainable Neural Networks}
}

@online{MachineLearningExplainability,
  title = {Machine {{Learning Explainability}} vs {{Interpretability}}: {{Two}} Concepts That Could Help Restore Trust in {{AI}}},
  shorttitle = {Machine {{Learning Explainability}} vs {{Interpretability}}},
  __url = {https://www.kdnuggets.com/machine-learning-explainability-vs-interpretability-two-concepts-that-could-help-restore-trust-in-ai.html/},
  _urldate = {2020-04-20},
  abstract = {We explain the key differences between explainability and interpretability and why they're so important for machine learning and AI, before taking a look at several techniques and methods for improving machine learning interpretability.},
  langid = {american},
  organization = {{KDnuggets}}
}

@article{mackinnonEquivalenceMediationConfounding2000,
  title = {Equivalence of the {{Mediation}}, {{Confounding}} and {{Suppression Effect}}},
  author = {MacKinnon, David P. and Krull, Jennifer L. and Lockwood, Chondra M.},
  year = {2000-12-01},
  journaltitle = {Prevention Science},
  shortjournal = {Prev Sci},
  volume = {1},
  number = {4},
  pages = {173--181},
  _issn = {1573-6695},
  _doi = {10.1023/A:1026595011371},
  __url = {https://doi.org/10.1023/A:1026595011371},
  _urldate = {2020-11-30},
  abstract = {This paper describes the statistical similarities among mediation, confounding, and suppression. Each is quantified by measuring the change in the relationship between an independent and a dependent variable after adding a third variable to the analysis. Mediation and confounding are identical statistically and can be distinguished only on conceptual grounds. Methods to determine the confidence intervals for confounding and suppression effects are proposed based on methods developed for mediated effects. Although the statistical estimation of effects and standard errors is the same, there are important conceptual differences among the three types of effects.},
  langid = {english}
}

@unpublished{maclarenWhatCanBe2020,
  title = {What Can Be Estimated? {{Identifiability}}, Estimability, Causal Inference and Ill-Posed Inverse Problems},
  shorttitle = {What Can Be Estimated?},
  author = {Maclaren, Oliver J. and Nicholson, Ruanui},
  year = {2020-07-20},
  eprint = {1904.02826},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  __url = {http://arxiv.org/abs/1904.02826},
  _urldate = {2022-04-04},
  abstract = {We consider basic conceptual questions concerning the relationship between statistical estimation and causal inference. Firstly, we show how to translate causal inference problems into an abstract statistical formalism without requiring any structure beyond an arbitrarily-indexed family of probability models. The formalism is simple but can incorporate a variety of causal modelling frameworks, including 'structural causal models', but also models expressed in terms of, e.g., differential equations. We focus primarily on the structural/graphical causal modelling literature, however. Secondly, we consider the extent to which causal and statistical concerns can be cleanly separated, examining the fundamental question: 'What can be estimated from data?'. We call this the problem of estimability. We approach this by analysing a standard formal definition of 'can be estimated' commonly adopted in the causal inference literature -- identifiability -- in our abstract statistical formalism. We use elementary category theory to show that identifiability implies the existence of a Fisher-consistent estimator, but also show that this estimator may be discontinuous, and thus unstable, in general. This difficulty arises because the causal inference problem is, in general, an ill-posed inverse problem. Inverse problems have three conditions which must be satisfied to be considered well-posed: existence, uniqueness, and stability of solutions. Here identifiability corresponds to the question of uniqueness; in contrast, we take estimability to mean satisfaction of all three conditions, i.e. well-posedness. Lack of stability implies that naive translation of a causally identifiable quantity into an achievable statistical estimation target may prove impossible. Our article is primarily expository and aimed at unifying ideas from multiple fields, though we provide new constructions and proofs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/BR8LLW7P/Maclaren and Nicholson - 2020 - What can be estimated Identifiability, estimabili.pdf;/home/rick/Zotero/storage/P4NPMGMX/1904.html}
}

@unpublished{madsenPosthocInterpretabilityNeural2021,
  title = {Post-Hoc {{Interpretability}} for {{Neural NLP}}: {{A Survey}}},
  shorttitle = {Post-Hoc {{Interpretability}} for {{Neural NLP}}},
  author = {Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
  year = {2021-08-13},
  eprint = {2108.04840},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2108.04840},
  _urldate = {2021-09-08},
  abstract = {Natural Language Processing (NLP) models have become increasingly more complex and widespread. With recent developments in neural networks, a growing concern is whether it is responsible to use these models. Concerns such as safety and ethics can be partially addressed by providing explanations. Furthermore, when models do fail, providing explanations is paramount for accountability purposes. To this end, interpretability serves to provide these explanations in terms that are understandable to humans. Central to what is understandable is how explanations are communicated. Therefore, this survey provides a categorization of how recent interpretability methods communicate explanations and discusses the methods in depth. Furthermore, the survey focuses on post-hoc methods, which provide explanations after a model is learned and generally model-agnostic. A common concern for this class of methods is whether they accurately reflect the model. Hence, how these post-hoc methods are evaluated is discussed throughout the paper.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@unpublished{majiExclusionInclusionModel2020,
  title = {Exclusion and {{Inclusion}} -- {{A}} Model Agnostic Approach to Feature Importance in {{DNNs}}},
  author = {Maji, Subhadip and Chowdhury, Arijit Ghosh and Bali, Raghav and Bhandaru, Vamsi M.},
  year = {2020-07-13},
  eprint = {2007.16010},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.16010},
  _urldate = {2020-08-05},
  abstract = {Deep Neural Networks in NLP have enabled systems to learn complex non-linear relationships. One of the major bottlenecks towards being able to use DNNs for real world applications is their characterization as black boxes. To solve this problem, we introduce a model agnostic algorithm which calculates phrase-wise importance of input features. We contend that our method is generalizable to a diverse set of tasks, by carrying out experiments for both Regression and Classification. We also observe that our approach is robust to outliers, implying that it only captures the essential aspects of the input.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning}
}

@unpublished{malladiDerivingExplanationDeep2021,
  title = {Deriving {{Explanation}} of {{Deep Visual Saliency Models}}},
  author = {Malladi, Sai Phani Kumar and Mukhopadhyay, Jayanta and Larabi, Chaker and Chaudhury, Santanu},
  year = {2021-09-08},
  eprint = {2109.03575},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2109.03575},
  _urldate = {2021-09-17},
  abstract = {Deep neural networks have shown their profound impact on achieving human level performance in visual saliency prediction. However, it is still unclear how they learn the task and what it means in terms of understanding human visual system. In this work, we develop a technique to derive explainable saliency models from their corresponding deep neural architecture based saliency models by applying human perception theories and the conventional concepts of saliency. This technique helps us understand the learning pattern of the deep network at its intermediate layers through their activation maps. Initially, we consider two state-of-the-art deep saliency models, namely UNISAL and MSI-Net for our interpretation. We use a set of biologically plausible log-gabor filters for identifying and reconstructing the activation maps of them using our explainable saliency model. The final saliency map is generated using these reconstructed activation maps. We also build our own deep saliency model named cross-concatenated multi-scale residual block based network (CMRNet) for saliency prediction. Then, we evaluate and compare the performance of the explainable models derived from UNISAL, MSI-Net and CMRNet on three benchmark datasets with other state-of-the-art methods. Hence, we propose that this approach of explainability can be applied to any deep visual saliency model for interpretation which makes it a generic one.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@online{manglaDesigningFasterNeural2020,
  title = {Designing {{Faster Neural Networks}}},
  author = {Mangla, Deepak},
  year = {2020-02-28T16:53:36},
  __url = {https://blog.usejournal.com/designing-faster-neural-networks-e1f1dc026533},
  _urldate = {2020-04-20},
  abstract = {4 tips to make smaller and faster neural networks.},
  langid = {english},
  organization = {{Medium}}
}

@unpublished{maPredictiveCausalImplications2020,
  title = {Predictive and {{Causal Implications}} of Using {{Shapley Value}} for {{Model Interpretation}}},
  author = {Ma, Sisi and Tourani, Roshan},
  year = {2020-08-11},
  eprint = {2008.05052},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  __url = {http://arxiv.org/abs/2008.05052},
  _urldate = {2020-09-16},
  abstract = {Shapley value is a concept from game theory. Recently, it has been used for explaining complex models produced by machine learning techniques. Although the mathematical definition of Shapley value is straight-forward, the implication of using it as a model interpretation tool is yet to be described. In the current paper, we analyzed Shapley value in the Bayesian network framework. We established the relationship between Shapley value and conditional independence, a key concept in both predictive and causal modeling. Our results indicate that, eliminating a variable with high Shapley value from a model do not necessarily impair predictive performance, whereas eliminating a variable with low Shapley value from a model could impair performance. Therefore, using Shapley value for feature selection do not result in the most parsimonious and predictively optimal model in the general case. More importantly, Shapley value of a variable do not reflect their causal relationship with the target of interest.},
  archiveprefix = {arXiv},
  keywords = {62H22; 62H30,Computer Science - Machine Learning,G.3,I.2.6,Mathematics - Statistics Theory,Statistics - Machine Learning}
}

@unpublished{markusRoleExplainabilityCreating2020,
  title = {The Role of Explainability in Creating Trustworthy Artificial Intelligence for Health Care: A Comprehensive Survey of the Terminology, Design Choices, and Evaluation Strategies},
  shorttitle = {The Role of Explainability in Creating Trustworthy Artificial Intelligence for Health Care},
  author = {Markus, Aniek F. and Kors, Jan A. and Rijnbeek, Peter R.},
  year = {2020-07-31},
  eprint = {2007.15911},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.15911},
  _urldate = {2020-08-05},
  abstract = {Artificial intelligence (AI) has huge potential to improve the health and well-being of people, but adoption in clinical practice is still limited. Lack of transparency is identified as one of the main barriers to implementation, as clinicians should be confident the AI system can be trusted. Explainable AI has the potential to overcome this issue and can be a step towards trustworthy AI. In this paper we review the recent literature to provide guidance to researchers and practitioners on the design of explainable AI systems for the health-care domain and contribute to formalization of the field of explainable AI. We argue the reason to demand explainability determines what should be explained as this determines the relative importance of the properties of explainability (i.e. interpretability and fidelity). Based on this, we give concrete recommendations to choose between classes of explainable AI methods (explainable modelling versus post-hoc explanation; model-based, attribution-based, or example-based explanations; global and local explanations). Furthermore, we find that quantitative evaluation metrics, which are important for objective standardized evaluation, are still lacking for some properties (e.g. clarity) and types of explanators (e.g. example-based methods). We conclude that explainable modelling can contribute to trustworthy AI, but recognize that complementary measures might be needed to create trustworthy AI (e.g. reporting data quality, performing extensive (external) validation, and regulation).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{marques-silvaExplainingNaiveBayes2020,
  title = {Explaining {{Naive Bayes}} and {{Other Linear Classifiers}} with {{Polynomial Time}} and {{Delay}}},
  author = {Marques-Silva, Joao and Gerspacher, Thomas and Cooper, Martin C. and Ignatiev, Alexey and Narodytska, Nina},
  year = {2020-08-13},
  eprint = {2008.05803},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2008.05803},
  _urldate = {2020-09-16},
  abstract = {Recent work proposed the computation of so-called PI-explanations of Naive Bayes Classifiers (NBCs). PI-explanations are subset-minimal sets of feature-value pairs that are sufficient for the prediction, and have been computed with state-of-the-art exact algorithms that are worst-case exponential in time and space. In contrast, we show that the computation of one PI-explanation for an NBC can be achieved in log-linear time, and that the same result also applies to the more general class of linear classifiers. Furthermore, we show that the enumeration of PI-explanations can be obtained with polynomial delay. Experimental results demonstrate the performance gains of the new algorithms when compared with earlier work. The experimental results also investigate ways to measure the quality of heuristic explanations},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{martinInterpretableMachineLearning2019,
  title = {Interpretable {{Machine Learning}}},
  author = {Martin, Tyler B.},
  year = {2019},
  __url = {https://www.semanticscholar.org/paper/Interpretable-Machine-Learning-Martin/b0c34618ffd1154f35863e2ce7250ac6b6f2c424},
  abstract = {Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.}
}

@article{mccoyBelievingBlackBoxes2022,
  title = {Believing in Black Boxes: Machine Learning for Healthcare Does Not Need Explainability to Be Evidence-Based},
  shorttitle = {Believing in Black Boxes},
  author = {McCoy, Liam G. and Brenna, Connor T.A. and Chen, Stacy S. and Vold, Karina and Das, Sunit},
  year = {2022-02},
  journaltitle = {Journal of Clinical Epidemiology},
  shortjournal = {Journal of Clinical Epidemiology},
  volume = {142},
  pages = {252--257},
  _issn = {08954356},
  _doi = {10.1016/j.jclinepi.2021.11.001},
  __url = {https://linkinghub.elsevier.com/retrieve/pii/S0895435621003541},
  _urldate = {2022-03-26},
  langid = {english}
}

@unpublished{mcinnesUMAPUniformManifold2020,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2020-09-17},
  eprint = {1802.03426},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1802.03426},
  _urldate = {2021-11-06},
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Geometry,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/6KPQI7JX/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf;/home/rick/Zotero/storage/AXMMMTDZ/1802.html}
}

@unpublished{merrickExplanationGameExplaining2020,
  title = {The {{Explanation Game}}: {{Explaining Machine Learning Models Using Shapley Values}}},
  shorttitle = {The {{Explanation Game}}},
  author = {Merrick, Luke and Taly, Ankur},
  year = {2020-06-25},
  eprint = {1909.08128},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1909.08128},
  _urldate = {2020-09-16},
  abstract = {A number of techniques have been proposed to explain a machine learning model's prediction by attributing it to the corresponding input features. Popular among these are techniques that apply the Shapley value method from cooperative game theory. While existing papers focus on the axiomatic motivation of Shapley values, and efficient techniques for computing them, they offer little justification for the game formulations used, and do not address the uncertainty implicit in their methods' outputs. For instance, the popular SHAP algorithm's formulation may give substantial attributions to features that play no role in the model. In this work, we illustrate how subtle differences in the underlying game formulations of existing methods can cause large differences in the attributions for a prediction. We then present a general game formulation that unifies existing methods, and enables straightforward confidence intervals on their attributions. Furthermore, it allows us to interpret the attributions as contrastive explanations of an input relative to a distribution of reference inputs. We tie this idea to classic research in cognitive psychology on contrastive explanations, and propose a conceptual framework for generating and interpreting explanations for ML models, called formulate, approximate, explain (FAE). We apply this framework to explain black-box models trained on two UCI datasets and a Lending Club dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{merrickExplanationGameExplaining2020a,
  title = {The {{Explanation Game}}: {{Explaining Machine Learning Models Using Shapley Values}}},
  shorttitle = {The {{Explanation Game}}},
  author = {Merrick, Luke and Taly, Ankur},
  year = {2020-06-25},
  eprint = {1909.08128},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1909.08128},
  _urldate = {2021-11-16},
  abstract = {A number of techniques have been proposed to explain a machine learning model's prediction by attributing it to the corresponding input features. Popular among these are techniques that apply the Shapley value method from cooperative game theory. While existing papers focus on the axiomatic motivation of Shapley values, and efficient techniques for computing them, they offer little justification for the game formulations used, and do not address the uncertainty implicit in their methods' outputs. For instance, the popular SHAP algorithm's formulation may give substantial attributions to features that play no role in the model. In this work, we illustrate how subtle differences in the underlying game formulations of existing methods can cause large differences in the attributions for a prediction. We then present a general game formulation that unifies existing methods, and enables straightforward confidence intervals on their attributions. Furthermore, it allows us to interpret the attributions as contrastive explanations of an input relative to a distribution of reference inputs. We tie this idea to classic research in cognitive psychology on contrastive explanations, and propose a conceptual framework for generating and interpreting explanations for ML models, called formulate, approximate, explain (FAE). We apply this framework to explain black-box models trained on two UCI datasets and a Lending Club dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/QGGKDAIT/Merrick and Taly - 2020 - The Explanation Game Explaining Machine Learning .pdf;/home/rick/Zotero/storage/6TMXY77X/1909.html}
}

@unpublished{millerExplainableAIBeware2017,
  title = {Explainable {{AI}}: {{Beware}} of {{Inmates Running}} the {{Asylum Or}}: {{How I Learnt}} to {{Stop Worrying}} and {{Love}} the {{Social}} and {{Behavioural Sciences}}},
  shorttitle = {Explainable {{AI}}},
  author = {Miller, Tim and Howe, Piers and Sonenberg, Liz},
  year = {2017-12-04},
  eprint = {1712.00547},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1712.00547},
  _urldate = {2020-10-19},
  abstract = {In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'. This paper argues that explainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But explainable AI is more likely to succeed if researchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science, and if evaluation of these models is focused more on people than on technology. From a light scan of literature, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@article{millerExplanationArtificialIntelligence2019,
  title = {Explanation in Artificial Intelligence: {{Insights}} from the Social Sciences},
  shorttitle = {Explanation in Artificial Intelligence},
  author = {Miller, Tim},
  year = {2019-02-01},
  journaltitle = {Artificial Intelligence},
  shortjournal = {Artificial Intelligence},
  volume = {267},
  pages = {1--38},
  _issn = {0004-3702},
  _doi = {10.1016/j.artint.2018.07.007},
  __url = {http://www.sciencedirect.com/science/article/pii/S0004370218305988},
  _urldate = {2020-10-19},
  abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  langid = {english},
  keywords = {Explainability,Explainable AI,Explanation,Interpretability,Transparency}
}

@inproceedings{mittelstadtExplainingExplanationsAI2019,
  title = {Explaining {{Explanations}} in {{AI}}},
  booktitle = {Proceedings of the {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Mittelstadt, Brent and Russell, Chris and Wachter, Sandra},
  year = {2019-01-29},
  series = {{{FAT}}* '19},
  pages = {279--288},
  publisher = {{Association for Computing Machinery}},
  location = {{Atlanta, GA, USA}},
  _doi = {10.1145/3287560.3287574},
  __url = {https://doi.org/10.1145/3287560.3287574},
  _urldate = {2020-10-19},
  abstract = {Recent work on interpretability in machine learning and AI has focused on the building of simplified models that approximate the true criteria used to make decisions. These models are a useful pedagogical device for teaching trained professionals how to predict what decisions will be made by the complex system, and most importantly how the system might break. However, when considering any such model it's important to remember Box's maxim that "All models are wrong but some are useful." We focus on the distinction between these models and explanations in philosophy and sociology. These models can be understood as a "do it yourself kit" for explanations, allowing a practitioner to directly answer "what if questions" or generate contrastive explanations without external assistance. Although a valuable ability, giving these models as explanations appears more difficult than necessary, and other forms of explanation may not have the same trade-offs. We contrast the different schools of thought on what makes an explanation, and suggest that machine learning might benefit from viewing the problem more broadly.},
  _isbn = {978-1-4503-6125-5},
  keywords = {Accountability,Explanations,Interpretability,Philosophy of Science}
}

@unpublished{mohseniMultidisciplinarySurveyFramework2020,
  title = {A {{Multidisciplinary Survey}} and {{Framework}} for {{Design}} and {{Evaluation}} of {{Explainable AI Systems}}},
  author = {Mohseni, Sina and Zarei, Niloofar and Ragan, Eric D.},
  year = {2020-08-05},
  eprint = {1811.11839},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1811.11839},
  _urldate = {2020-12-07},
  abstract = {The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence applications used in everyday life. Explainable intelligent systems are designed to self-explain the reasoning behind system decisions and predictions, and researchers from different disciplines work together to define, design, and evaluate interpretable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of interpretable machine learning research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this paper presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of interpretable machine learning design goals and evaluation methods to show a mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction}
}

@unpublished{mollasAltruistArgumentativeExplanations2020,
  title = {Altruist: {{Argumentative Explanations}} through {{Local Interpretations}} of {{Predictive Models}}},
  shorttitle = {Altruist},
  author = {Mollas, Ioannis and Bassiliades, Nick and Tsoumakas, Grigorios},
  year = {2020-10-15},
  eprint = {2010.07650},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2010.07650},
  _urldate = {2020-11-30},
  abstract = {Interpretable machine learning is an emerging field providing solutions on acquiring insights into machine learning models' rationale. It has been put in the map of machine learning by suggesting ways to tackle key ethical and societal issues. However, existing techniques of interpretable machine learning are far from being comprehensible and explainable to the end user. Another key issue in this field is the lack of evaluation and selection criteria, making it difficult for the end user to choose the most appropriate interpretation technique for its use. In this study, we introduce a meta-explanation methodology that will provide truthful interpretations, in terms of feature importance, to the end user through argumentation. At the same time, this methodology can be used as an evaluation or selection tool for multiple interpretation techniques based on feature importance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning,I.2.0,I.2.6}
}

@book{molnarInterpretableMachineLearning,
  title = {Interpretable {{Machine Learning}}},
  publisher = {Independently published},
  author = {Molnar, Christoph},
  __url = {https://christophm.github.io/interpretable-ml-book/},
  _urldate = {2020-04-20},
  year = {2020},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.}
}

@unpublished{molnarInterpretableMachineLearning2020,
  title = {Interpretable {{Machine Learning}} -- {{A Brief History}}, {{State-of-the-Art}} and {{Challenges}}},
  author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
  year = {2020-10-19},
  eprint = {2010.09337},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2010.09337},
  _urldate = {2020-11-30},
  year = {2020},
  abstract = {We present a brief history of the field of interpretable machine learning (IML), give an overview of state-of-the-art interpretation methods, and discuss challenges. Research in IML has boomed in recent years. As young as the field is, it has over 200 years old roots in regression modeling and rule-based machine learning, starting in the 1960s. Recently, many new IML methods have been proposed, many of them model-agnostic, but also interpretation techniques specific to deep learning and tree-based ensembles. IML methods either directly analyze model components, study sensitivity to input perturbations, or analyze local or global surrogate approximations of the ML model. The field approaches a state of readiness and stability, with many methods not only proposed in research, but also implemented in open-source software. But many important challenges remain for IML, such as dealing with dependent features, causal interpretation, and uncertainty estimation, which need to be resolved for its successful application to scientific problems. A further challenge is a missing rigorous definition of interpretability, which is accepted by the community. To address the challenges and advance the field, we urge to recall our roots of interpretable, data-driven modeling in statistics and (rule-based) ML, but also to consider other areas such as sensitivity analysis, causal inference, and the social sciences.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{molnarPitfallsAvoidWhen2020,
  title = {Pitfalls to {{Avoid}} When {{Interpreting Machine Learning Models}}},
  author = {Molnar, Christoph and König, Gunnar and Herbinger, Julia and Freiesleben, Timo and Dandl, Susanne and Scholbeck, Christian A. and Casalicchio, Giuseppe and Grosse-Wentrup, Moritz and Bischl, Bernd},
  year = {2020-07-08},
  eprint = {2007.04131},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.04131},
  _urldate = {2020-08-05},
  abstract = {Modern requirements for machine learning (ML) models include both high predictive performance and model interpretability. A growing number of techniques provide model interpretations, but can lead to wrong conclusions if applied incorrectly. We illustrate pitfalls of ML model interpretation such as bad model generalization, dependent features, feature interactions or unjustified causal interpretations. Our paper addresses ML practitioners by raising awareness of pitfalls and pointing out solutions for correct model interpretation, as well as ML researchers by discussing open issues for further research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{mongaAlgorithmUnrollingInterpretable2019,
  title = {Algorithm {{Unrolling}}: {{Interpretable}}, {{Efficient Deep Learning}} for {{Signal}} and {{Image Processing}}},
  shorttitle = {Algorithm {{Unrolling}}},
  author = {Monga, Vishal and Li, Yuelong and Eldar, Yonina C.},
  year = {2019-12-22},
  eprint = {1912.10557},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  __url = {http://arxiv.org/abs/1912.10557},
  _urldate = {2020-04-20},
  abstract = {Deep neural networks provide unprecedented performance gains in many real world problems in signal and image processing. Despite these gains, future development and practical deployment of deep networks is hindered by their blackbox nature, i.e., lack of interpretability, and by the need for very large training sets. An emerging technique called algorithm unrolling or unfolding offers promise in eliminating these issues by providing a concrete and systematic connection between iterative algorithms that are used widely in signal processing and deep neural networks. Unrolling methods were first proposed to develop fast neural network approximations for sparse coding. More recently, this direction has attracted enormous attention and is rapidly growing both in theoretic investigations and practical applications. The growing popularity of unrolled deep networks is due in part to their potential in developing efficient, high-performance and yet interpretable network architectures from reasonable size training sets. In this article, we review algorithm unrolling for signal and image processing. We extensively cover popular techniques for algorithm unrolling in various domains of signal and image processing including imaging, vision and recognition, and speech processing. By reviewing previous works, we reveal the connections between iterative algorithms and neural networks and present recent theoretical results. Finally, we provide a discussion on current limitations of unrolling and suggest possible future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Electrical Engineering and Systems Science - Signal Processing}
}

@article{montavonExplainingNonLinearClassification2017,
  title = {Explaining {{NonLinear Classification Decisions}} with {{Deep Taylor Decomposition}}},
  author = {Montavon, Grégoire and Bach, Sebastian and Binder, Alexander and Samek, Wojciech and Müller, Klaus-Robert},
  year = {2017},
  journaltitle = {Pattern Recognition},
  journal = {Pattern Recognition},
  volume = {65},
  pages = {211--222},
  _doi = {10.1016/j.patcog.2016.11.008},
  __url = {https://www.sciencedirect.com/science/article/pii/S0031320316303582},
  _urldate = {2020-12-07},
  abstract = {Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard for various challenging machine learning problems, e.g., image classification, natural language processing or human action recognition. Although these methods perform impressively well, they have a significant disadvantage, the lack of transparency, limiting the interpretability of the solution and thus the scope of application in practice. Especially DNNs act as black boxes due to their multilayer nonlinear structure. In this paper we introduce a novel methodology for interpreting generic multilayer neural networks by decomposing the network classification decision into contributions of its input elements. Although our focus is on image classification, the method is applicable to a broad set of input data, learning tasks and network architectures. Our method is based on deep Taylor decomposition and efficiently utilizes the structure of the network by backpropagating the explanations from the output to the input layer. We evaluate the proposed method empirically on the MNIST and ILSVRC data sets.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}


@unpublished{moreiraInterpretableProbabilisticApproach2020,
  title = {An {{Interpretable Probabilistic Approach}} for {{Demystifying Black-box Predictive Models}}},
  author = {Moreira, Catarina and Chou, Yu-Liang and Velmurugan, Mythreyi and Ouyang, Chun and Sindhgatta, Renuka and Bruza, Peter},
  year = {2020-07-21},
  eprint = {2007.10668},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2007.10668},
  _urldate = {2020-07-30},
  abstract = {The use of sophisticated machine learning models for critical decision making is faced with a challenge that these models are often applied as a "black-box". This has led to an increased interest in interpretable machine learning, where post hoc interpretation presents a useful mechanism for generating interpretations of complex learning models. In this paper, we propose a novel approach underpinned by an extended framework of Bayesian networks for generating post hoc interpretations of a black-box predictive model. The framework supports extracting a Bayesian network as an approximation of the black-box model for a specific prediction. Compared to the existing post hoc interpretation methods, the contribution of our approach is three-fold. Firstly, the extracted Bayesian network, as a probabilistic graphical model, can provide interpretations about not only what input features but also why these features contributed to a prediction. Secondly, for complex decision problems with many features, a Markov blanket can be generated from the extracted Bayesian network to provide interpretations with a focused view on those input features that directly contributed to a prediction. Thirdly, the extracted Bayesian network enables the identification of four different rules which can inform the decision-maker about the confidence level in a prediction, thus helping the decision-maker assess the reliability of predictions learned by a black-box model. We implemented the proposed approach, applied it in the context of two well-known public datasets and analysed the results, which are made available in an open-source repository.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{morleyWhatHowInitial2020,
  title = {From {{What}} to {{How}}: {{An Initial Review}} of {{Publicly Available AI Ethics Tools}}, {{Methods}} and {{Research}} to {{Translate Principles}} into {{Practices}}},
  shorttitle = {From {{What}} to {{How}}},
  author = {Morley, Jessica and Floridi, Luciano and Kinsey, Libby and Elhalal, Anat},
  year = {2020-08-01},
  journaltitle = {Science and Engineering Ethics},
  shortjournal = {Sci Eng Ethics},
  volume = {26},
  number = {4},
  pages = {2141--2168},
  _issn = {1471-5546},
  _doi = {10.1007/s11948-019-00165-5},
  __url = {https://doi.org/10.1007/s11948-019-00165-5},
  _urldate = {2020-10-22},
  abstract = {The debate about the ethical implications of Artificial Intelligence dates from the 1960s (Samuel in Science, 132(3429):741–742, 1960. https://doi.org/10.1126/science.132.3429.741; Wiener in Cybernetics: or control and communication in the animal and the machine, MIT Press, New York, 1961). However, in recent years symbolic AI has been complemented and sometimes replaced by (Deep) Neural Networks and Machine Learning (ML) techniques. This has vastly increased its potential utility and impact on society, with the consequence that the ethical debate has gone mainstream. Such a debate has primarily focused on principles—the ‘what’ of AI ethics (beneficence, non-maleficence, autonomy, justice and explicability)—rather than on practices, the ‘how.’ Awareness of the potential issues is increasing at a fast rate, but the AI community’s ability to take action to mitigate the associated risks is still at its infancy. Our intention in presenting this research is to contribute to closing the gap between principles and practices by constructing a typology that may help practically-minded developers apply ethics at each stage of the Machine Learning development pipeline, and to signal to researchers where further work is needed. The focus is exclusively on Machine Learning, but it is hoped that the results of this research may be easily applicable to other branches of AI. The article outlines the research method for creating this typology, the initial findings, and provides a summary of future research needs.},
  langid = {english}
}


@unpublished{mothilalUnifyingFeatureAttribution2020,
  title = {Towards {{Unifying Feature Attribution}} and {{Counterfactual Explanations}}: {{Different Means}} to the {{Same End}}},
  shorttitle = {Towards {{Unifying Feature Attribution}} and {{Counterfactual Explanations}}},
  author = {Mothilal, Ramaravind K. and Mahajan, Divyat and Tan, Chenhao and Sharma, Amit},
  year = {2020-11-10},
  eprint = {2011.04917},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2011.04917},
  _urldate = {2020-11-30},
  abstract = {To explain a machine learning model, there are two main approaches: feature attributions that assign an importance score to each input feature, and counterfactual explanations that provide input examples with minimal changes to alter the model's prediction. We provide two key results towards unifying these approaches in terms of their interpretation and use. First, we present a method to generate feature attribution explanations from a set of counterfactual examples. These feature attributions convey how important a feature is to changing the classification outcome of a model, especially on whether a subset of features is necessary and/or sufficient for that change, which feature attribution methods are unable to provide. Second, we show how counterfactual examples can be used to evaluate the goodness of an attribution-based explanation in terms of its necessity and sufficiency. As a result, we highlight the complementarity of these two approaches and provide an interpretation based on a causal inference framework. Our evaluation on three benchmark datasets -- Adult Income, LendingClub, and GermanCredit -- confirm the complementarity. Feature attribution methods like LIME and SHAP and counterfactual explanation methods like DiCE often do not agree on feature importance rankings. In addition, by restricting the features that can be modified for generating counterfactual examples, we find that the top-k features from LIME or SHAP are neither necessary nor sufficient explanations of a model's prediction. Finally, we present a case study of different explanation methods on a real-world hospital triage problem.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning}
}

@unpublished{muCompositionalExplanationsNeurons2021,
  title = {Compositional {{Explanations}} of {{Neurons}}},
  author = {Mu, Jesse and Andreas, Jacob},
  year = {2021-02-02},
  eprint = {2006.14032},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2006.14032},
  _urldate = {2021-12-15},
  abstract = {We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple "copy-paste" adversarial examples that change model behavior in predictable ways.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{muCompositionalExplanationsNeurons2021a,
  title = {Compositional {{Explanations}} of {{Neurons}}},
  author = {Mu, Jesse and Andreas, Jacob},
  year = {2021-02},
  eprint = {2006.14032},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2006.14032},
  _urldate = {2021-12-15},
  abstract = {We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple "copy-paste" adversarial examples that change model behavior in predictable ways.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{mujkanovicTimeXplainFrameworkExplaining2020,
  title = {{{timeXplain}} -- {{A Framework}} for {{Explaining}} the {{Predictions}} of {{Time Series Classifiers}}},
  author = {Mujkanovic, Felix and Doskoč, Vanja and Schirneck, Martin and Schäfer, Patrick and Friedrich, Tobias},
  year = {2020-07-15},
  eprint = {2007.07606},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.07606},
  _urldate = {2020-07-30},
  abstract = {Modern time series classifiers display impressive predictive capabilities, yet their decision-making processes mostly remain black boxes to the user. At the same time, model-agnostic explainers, such as the recently proposed SHAP, promise to make the predictions of machine learning models interpretable, provided there are well-designed domain mappings. We bring both worlds together in our timeXplain framework, extending the reach of explainable artificial intelligence to time series classification and value prediction. We present novel domain mappings for the time and the frequency domain as well as series statistics and analyze their explicative power as well as their limits. We employ timeXplain in a large-scale experimental comparison of several state-of-the-art time series classifiers and discover similarities between seemingly distinct classification concepts such as residual neural networks and elastic ensembles.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{murdochDefinitionsMethodsApplications2019,
  title = {Definitions, Methods, and Applications in Interpretable Machine Learning},
  author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and Abbasi-Asl, Reza and Yu, Bin},
  year = {2019},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {116},
  number = {44},
  eprint = {31619572},
  eprinttype = {pmid},
  pages = {22071--22080},
  _issn = {0027-8424, 1091-6490},
  _doi = {10.1073/pnas.1900654116},
  __url = {https://www.pnas.org/content/116/44/22071},
  _urldate = {2020-10-22},
  abstract = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
  langid = {english},
  keywords = {explainability,interpretability,machine learning,relevancy}
}

@unpublished{nakkiranDeepDoubleDescent2019,
  title = {Deep {{Double Descent}}: {{Where Bigger Models}} and {{More Data Hurt}}},
  shorttitle = {Deep {{Double Descent}}},
  author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  year = {2019-12-04},
  eprint = {1912.02292},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1912.02292},
  _urldate = {2020-11-03},
  abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@unpublished{nandaUnifyingModelExplainability2020,
  title = {Unifying {{Model Explainability}} and {{Robustness}} via {{Machine-Checkable Concepts}}},
  author = {Nanda, Vedant and Speicher, Till and Dickerson, John P. and Gummadi, Krishna P. and Zafar, Muhammad Bilal},
  year = {2020-07-02},
  eprint = {2007.00251},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2007.00251},
  _urldate = {2020-08-05},
  abstract = {As deep neural networks (DNNs) get adopted in an ever-increasing number of applications, explainability has emerged as a crucial desideratum for these models. In many real-world tasks, one of the principal reasons for requiring explainability is to in turn assess prediction robustness, where predictions (i.e., class labels) that do not conform to their respective explanations (e.g., presence or absence of a concept in the input) are deemed to be unreliable. However, most, if not all, prior methods for checking explanation-conformity (e.g., LIME, TCAV, saliency maps) require significant manual intervention, which hinders their large-scale deployability. In this paper, we propose a robustness-assessment framework, at the core of which is the idea of using machine-checkable concepts. Our framework defines a large number of concepts that the DNN explanations could be based on and performs the explanation-conformity check at test time to assess prediction robustness. Both steps are executed in an automated manner without requiring any human intervention and are easily scaled to datasets with a very large number of classes. Experiments on real-world datasets and human surveys show that our framework is able to enhance prediction robustness significantly: the predictions marked to be robust by our framework have significantly higher accuracy and are more robust to adversarial perturbations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning}
}

@unpublished{narayananHowHumansUnderstand2018,
  title = {How Do {{Humans Understand Explanations}} from {{Machine Learning Systems}}? {{An Evaluation}} of the {{Human-Interpretability}} of {{Explanation}}},
  shorttitle = {How Do {{Humans Understand Explanations}} from {{Machine Learning Systems}}?},
  author = {Narayanan, Menaka and Chen, Emily and He, Jeffrey and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
  year = {2018-02-02},
  eprint = {1802.00682},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1802.00682},
  _urldate = {2020-10-19},
  abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@unpublished{narayananHowHumansUnderstand2018a,
  title = {How Do {{Humans Understand Explanations}} from {{Machine Learning Systems}}? {{An Evaluation}} of the {{Human-Interpretability}} of {{Explanation}}},
  shorttitle = {How Do {{Humans Understand Explanations}} from {{Machine Learning Systems}}?},
  author = {Narayanan, Menaka and Chen, Emily and He, Jeffrey and Kim, Been and Gershman, Sam and Doshi-Velez, Finale},
  year = {2018-02-02},
  eprint = {1802.00682},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1802.00682},
  _urldate = {2021-01-10},
  abstract = {Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@unpublished{nautaThisLooksThat2020,
  title = {This {{Looks Like That}}, {{Because}} ... {{Explaining Prototypes}} for {{Interpretable Image Recognition}}},
  author = {Nauta, Meike and Jutte, Annemarie and Provoost, Jesper and Seifert, Christin},
  year = {2020-11-05},
  eprint = {2011.02863},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2011.02863},
  _urldate = {2020-11-30},
  abstract = {Image recognition with prototypes is considered an interpretable alternative for black box deep learning models. Classification depends on the extent to which a test image "looks like" a prototype. However, perceptual similarity for humans can be different from the similarity learnt by the model. A user is unaware of the underlying classification strategy and does not know which image characteristics (e.g., color or shape) is the dominant characteristic for the decision. We address this ambiguity and argue that prototypes should be explained. Only visualizing prototypes can be insufficient for understanding what a prototype exactly represents, and why a prototype and an image are considered similar. We improve interpretability by automatically enhancing prototypes with extra information about visual characteristics considered important by the model. Specifically, our method quantifies the influence of color hue, shape, texture, contrast and saturation in a prototype. We apply our method to the existing Prototypical Part Network (ProtoPNet) and show that our explanations clarify the meaning of a prototype which might have been interpreted incorrectly otherwise. We also reveal that visually similar prototypes can have the same explanations, indicating redundancy. Because of the generality of our approach, it can improve the interpretability of any similarity-based method for prototypical image recognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@online{NewModelDataset,
  title = {"A A new model and dataset for long-range memory},
  __url = {/blog/article/A_new_model_and_dataset_for_long-range_memory},
  _urldate = {2020-04-20},
  abstract = {This blog introduces a new long-range memory model, the Compressive Transformer, alongside a new benchmark for book-level language modelling, PG19. We provide the conceptual tools needed to understand this new research in the context of recent developments in memory models and language modelling.},
  langid = {ALL},
  organization = {{Deepmind}}
}

@unpublished{nguyenQuantitativeAspectsModel2020,
  title = {On Quantitative Aspects of Model Interpretability},
  author = {Nguyen, An-phi and Martínez, María Rodríguez},
  year = {2020-07-15},
  eprint = {2007.07584},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.07584},
  _urldate = {2020-07-30},
  abstract = {Despite the growing body of work in interpretable machine learning, it remains unclear how to evaluate different explainability methods without resorting to qualitative assessment and user-studies. While interpretability is an inherently subjective matter, previous works in cognitive science and epistemology have shown that good explanations do possess aspects that can be objectively judged apart from fidelity), such assimplicity and broadness. In this paper we propose a set of metrics to programmatically evaluate interpretability methods along these dimensions. In particular, we argue that the performance of methods along these dimensions can be orthogonally imputed to two conceptual parts, namely the feature extractor and the actual explainability method. We experimentally validate our metrics on different benchmark tasks and show how they can be used to guide a practitioner in the selection of the most appropriate method for the task at hand.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{nicodemusBehaviourRandomForest2010,
  title = {The Behaviour of Random Forest Permutation-Based Variable Importance Measures under Predictor Correlation},
  author = {Nicodemus, Kristin K. and Malley, James D. and Strobl, Carolin and Ziegler, Andreas},
  year = {2010-02-27},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  volume = {11},
  number = {1},
  pages = {110},
  _issn = {1471-2105},
  _doi = {10.1186/1471-2105-11-110},
  __url = {https://doi.org/10.1186/1471-2105-11-110},
  _urldate = {2020-10-07},
  abstract = {Random forests (RF) have been increasingly used in applications such as genome-wide association and microarray studies where predictor correlation is frequently observed. Recent works on permutation-based variable importance measures (VIMs) used in RF have come to apparently contradictory conclusions. We present an extended simulation study to synthesize results.}
}

@inproceedings{NIPS2013_e3796ae8,
  title = {Understanding Variable Importances in Forests of Randomized Trees},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Louppe, Gilles and Wehenkel, Louis and Sutera, Antonio and Geurts, Pierre},
  editor = {Burges, C.J. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K.Q.},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  __url = {https://proceedings.neurips.cc/paper/2013/file/e3796ae838835da0b6f6ea37bcf8bcb7-Paper.pdf}
}

@unpublished{nutiBayesianDecisionTree2020,
  title = {A {{Bayesian Decision Tree Algorithm}}},
  author = {Nuti, Giuseppe and Rugama, Lluís Antoni Jiménez and Cross, Andreea-Ingrid},
  year = {2020-09-22},
  eprint = {1901.03214},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1901.03214},
  _urldate = {2022-04-10},
  abstract = {Bayesian Decision Trees are known for their probabilistic interpretability. However, their construction can sometimes be costly. In this article we present a general Bayesian Decision Tree algorithm applicable to both regression and classification problems. The algorithm does not apply Markov Chain Monte Carlo and does not require a pruning step. While it is possible to construct a weighted probability tree space we find that one particular tree, the greedy-modal tree (GMT), explains most of the information contained in the numerical examples. This approach seems to perform similarly to Random Forests.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/39LPTGA2/Nuti et al. - 2020 - A Bayesian Decision Tree Algorithm.pdf;/home/rick/Zotero/storage/UMC9VUYK/1901.html}
}

@unpublished{nutiBayesianDecisionTree2020a,
  title = {A {{Bayesian Decision Tree Algorithm}}},
  author = {Nuti, Giuseppe and Rugama, Lluís Antoni Jiménez and Cross, Andreea-Ingrid},
  year = {2020-09-22},
  eprint = {1901.03214},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1901.03214},
  _urldate = {2022-04-12},
  abstract = {Bayesian Decision Trees are known for their probabilistic interpretability. However, their construction can sometimes be costly. In this article we present a general Bayesian Decision Tree algorithm applicable to both regression and classification problems. The algorithm does not apply Markov Chain Monte Carlo and does not require a pruning step. While it is possible to construct a weighted probability tree space we find that one particular tree, the greedy-modal tree (GMT), explains most of the information contained in the numerical examples. This approach seems to perform similarly to Random Forests.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/NTHRIV67/Nuti et al. - 2020 - A Bayesian Decision Tree Algorithm.pdf;/home/rick/Zotero/storage/6GPW2E9H/1901.html}
}

@unpublished{ozyegenEvaluationLocalExplanation2020,
  title = {Evaluation of {{Local Explanation Methods}} for {{Multivariate Time Series Forecasting}}},
  author = {Ozyegen, Ozan and Ilic, Igor and Cevik, Mucahit},
  year = {2020-09-18},
  eprint = {2009.09092},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2009.09092},
  _urldate = {2020-11-30},
  abstract = {Being able to interpret a machine learning model is a crucial task in many applications of machine learning. Specifically, local interpretability is important in determining why a model makes particular predictions. Despite the recent focus on AI interpretability, there has been a lack of research in local interpretability methods for time series forecasting while the few interpretable methods that exist mainly focus on time series classification tasks. In this study, we propose two novel evaluation metrics for time series forecasting: Area Over the Perturbation Curve for Regression and Ablation Percentage Threshold. These two metrics can measure the local fidelity of local explanation models. We extend the theoretical foundation to collect experimental results on two popular datasets, \textbackslash textit\{Rossmann sales\} and \textbackslash textit\{electricity\}. Both metrics enable a comprehensive comparison of numerous local explanation models and find which metrics are more sensitive. Lastly, we provide heuristical reasoning for this analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{parascandoloLearningExplanationsThat2020,
  title = {Learning Explanations That Are Hard to Vary},
  author = {Parascandolo, Giambattista and Neitz, Alexander and Orvieto, Antonio and Gresele, Luigi and Schölkopf, Bernhard},
  year = {2020-10-24},
  eprint = {2009.00329},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2009.00329},
  _urldate = {2020-11-30},
  abstract = {In this paper, we investigate the principle that `good explanations are hard to vary' in the context of deep learning. We show that averaging gradients across examples -- akin to a logical OR of patterns -- can favor memorization and `patchwork' solutions that sew together different strategies, instead of identifying invariances. To inspect this, we first formalize a notion of consistency for minima of the loss surface, which measures to what extent a minimum appears only when examples are pooled. We then propose and experimentally validate a simple alternative algorithm based on a logical AND, that focuses on invariances and prevents memorization in a set of real-world tasks. Finally, using a synthetic dataset with a clear distinction between invariant and spurious mechanisms, we dissect learning signals and compare this approach to well-established regularizers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{parekhFrameworkLearnInterpretation2020,
  title = {A {{Framework}} to {{Learn}} with {{Interpretation}}},
  author = {Parekh, Jayneel and Mozharovskyi, Pavlo and d'Alche- Buc, Florence},
  options = {useprefix=true},
  year = {2020-10-19},
  eprint = {2010.09345},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2010.09345},
  _urldate = {2020-11-30},
  abstract = {With increasingly widespread use of deep neural networks in critical decision-making applications, interpretability of these models is becoming imperative. We consider the problem of jointly learning a predictive model and its associated interpretation model. The task of the interpreter is to provide both local and global interpretability about the predictive model in terms of human-understandable high level attribute functions, without any loss of accuracy. This is achieved by a dedicated architecture and well chosen regularization penalties. We seek for a small-size dictionary of attribute functions that take as inputs the outputs of selected hidden layers and whose outputs feed a linear classifier. We impose a high level of conciseness by constraining the activation of a very few attributes for a given input with a real-entropy-based criterion while enforcing fidelity to both inputs and outputs of the predictive model. A major advantage of simultaneous learning is that the predictive neural network benefits from the interpretability constraint as well. We also develop a more detailed pipeline based on some common and novel simple tools to develop understanding about the learnt features. We show on two datasets, MNIST and QuickDraw, their relevance for both global and local interpretability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{parkNeuralDivergenceExploringUnderstanding2019,
  title = {{{NeuralDivergence}}: {{Exploring}} and {{Understanding Neural Networks}} by {{Comparing Activation Distributions}}},
  shorttitle = {{{NeuralDivergence}}},
  author = {Park, Haekyu and Hohman, Fred and Chau, Duen Horng},
  year = {2019-06-01},
  eprint = {1906.00332},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1906.00332},
  _urldate = {2020-04-20},
  abstract = {As deep neural networks are increasingly used in solving high-stake problems, there is a pressing need to understand their internal decision mechanisms. Visualization has helped address this problem by assisting with interpreting complex deep neural networks. However, current tools often support only single data instances, or visualize layers in isolation. We present NeuralDivergence, an interactive visualization system that uses activation distributions as a high-level summary of what a model has learned. NeuralDivergence enables users to interactively summarize and compare activation distributions across layers, classes, and instances (e.g., pairs of adversarial attacked and benign images), helping them gain better understanding of neural network models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{parrPartialDependenceStratification2021,
  title = {Partial Dependence through Stratification},
  author = {Parr, Terence and Wilson, James D.},
  year = {2021-12},
  journaltitle = {Machine Learning with Applications},
  shortjournal = {Machine Learning with Applications},
  volume = {6},
  pages = {100146},
  _issn = {26668270},
  _doi = {10.1016/j.mlwa.2021.100146},
  __url = {https://linkinghub.elsevier.com/retrieve/pii/S2666827021000736},
  _urldate = {2021-11-19},
  langid = {english}
}

@unpublished{patelHighDimensionalModel2021,
  title = {High {{Dimensional Model Explanations}}: An {{Axiomatic Approach}}},
  shorttitle = {High {{Dimensional Model Explanations}}},
  author = {Patel, Neel and Strobel, Martin and Zick, Yair},
  year = {2021-03-29},
  eprint = {2006.08969},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2006.08969},
  _urldate = {2021-11-16},
  abstract = {Complex black-box machine learning models are regularly used in critical decision-making domains. This has given rise to several calls for algorithmic explainability. Many explanation algorithms proposed in literature assign importance to each feature individually. However, such explanations fail to capture the joint effects of sets of features. Indeed, few works so far formally analyze high-dimensional model explanations. In this paper, we propose a novel high dimension model explanation method that captures the joint effect of feature subsets. We propose a new axiomatization for a generalization of the Banzhaf index; our method can also be thought of as an approximation of a black-box model by a higher-order polynomial. In other words, this work justifies the use of the generalized Banzhaf index as a model explanation by showing that it uniquely satisfies a set of natural desiderata and that it is the optimal local approximation of a black-box model. Our empirical evaluation of our measure highlights how it manages to capture desirable behavior, whereas other measures that do not satisfy our axioms behave in an unpredictable manner.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/ZC4U4XLU/Patel et al. - 2021 - High Dimensional Model Explanations an Axiomatic .pdf;/home/rick/Zotero/storage/6G7HYNMR/2006.html}
}

@unpublished{paulusEffectsDataAmbiguity2019,
  title = {Effects of Data Ambiguity and Cognitive Biases on the Interpretability of Machine Learning Models in Humanitarian Decision Making},
  author = {Paulus, David and de Vries, Gerdien and Van de Walle, Bartel},
  options = {useprefix=true},
  year = {2019-11-12},
  eprint = {1911.04787},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1911.04787},
  _urldate = {2020-10-22},
  abstract = {The effectiveness of machine learning algorithms depends on the quality and amount of data and the operationalization and interpretation by the human analyst. In humanitarian response, data is often lacking or overburdening, thus ambiguous, and the time-scarce, volatile, insecure environments of humanitarian activities are likely to inflict cognitive biases. This paper proposes to research the effects of data ambiguity and cognitive biases on the interpretability of machine learning algorithms in humanitarian decision making.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{pearlBayesianAnalysisExpert1993,
  title = {[{{Bayesian Analysis}} in {{Expert Systems}}]: {{Comment}}: {{Graphical Models}}, {{Causality}} and {{Intervention}}},
  shorttitle = {[{{Bayesian Analysis}} in {{Expert Systems}}]},
  author = {Pearl, Judea},
  year = {1993-08},
  journaltitle = {Statistical Science},
  volume = {8},
  number = {3},
  pages = {266--269},
  _issn = {0883-4237, 2168-8745},
  _doi = {10.1214/ss/1177010894},
  __url = {https://projecteuclid.org/journals/statistical-science/volume-8/issue-3/Bayesian-Analysis-in-Expert-Systems--Comment--Graphical-Models/10.1214/ss/1177010894.full},
  _urldate = {2021-08-21},
  abstract = {Statistical Science}
}

@misc{pearlSevenToolsCausal2019,
  title = {The Seven Tools of Causal Inference, with Reflections on Machine Learning},
  author = {Pearl, Judea},
  year = {2019-02-21},
  publisher = {{Association for Computing Machinery}},
  __url = {https://doi.org/10.1145/3241036},
  _urldate = {2020-10-19},
  abstract = {The kind of causal inference seen in natural human thought can be "algorithmitized" to help produce human-level machine intelligence.}
}

@article{pedregosaScikitlearnMachineLearning2011,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}}},
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  year = {2011},
  journaltitle = {the Journal of machine Learning research},
  volume = {12},
  pages = {2825--2830},
  publisher = {{JMLR. org}}
}

@article{pedregosaScikitlearnMachineLearning2011a,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}}},
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  year = {2011},
  journaltitle = {the Journal of machine Learning research},
  volume = {12},
  pages = {2825--2830},
  publisher = {{JMLR. org}}
}

@article{pedregosaScikitlearnMachineLearning2011b,
  title = {Scikit-Learn: {{Machine}} Learning in {{Python}}},
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  year = {2011},
  journaltitle = {the Journal of machine Learning research},
  volume = {12},
  pages = {2825--2830},
  publisher = {{JMLR. org}}
}

@article{pedreschiMeaningfulExplanationsBlack2019,
  title = {Meaningful {{Explanations}} of {{Black Box AI Decision Systems}}},
  author = {Pedreschi, Dino and Giannotti, Fosca and Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco},
  year = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {9780--9784},
  _issn = {2374-3468},
  _doi = {10.1609/aaai.v33i01.33019780},
  __url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5050},
  _urldate = {2020-10-19},
  abstract = {Black box AI systems for automated decision making, often based on machine learning over (big) data, map a user’s features into a class or a score without exposing the reasons why. This is problematic not only for lack of transparency, but also for possible biases inherited by the algorithms from human prejudices and collection artifacts hidden in the training data, which may lead to unfair or wrong decisions. We focus on the urgent open challenge of how to construct meaningful explanations of opaque AI/ML systems, introducing the local-toglobal framework for black box explanation, articulated along three lines: (i) the language for expressing explanations in terms of logic rules, with statistical and causal interpretation; (ii) the inference of local explanations for revealing the decision rationale for a specific case, by auditing the black box in the vicinity of the target instance; (iii), the bottom-up generalization of many local explanations into simple global ones, with algorithms that optimize for quality and comprehensibility. We argue that the local-first approach opens the door to a wide variety of alternative solutions along different dimensions: a variety of data sources (relational, text, images, etc.), a variety of learning problems (multi-label classification, regression, scoring, ranking), a variety of languages for expressing meaningful explanations, a variety of means to audit a black box.},
  langid = {english}
}

@book{petersElementsCausalInference2017,
  title = {Elements of {{Causal Inference}}: {{Foundations}} and {{Learning Algorithms}}},
  shorttitle = {Elements of {{Causal Inference}}},
  author = {Peters, Jonas and Janzing, Dominik and Schölkopf, Bernhard},
  year = {2017-11-29},
  series = {Adaptive {{Computation}} and {{Machine Learning}} Series},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA, USA}},
  abstract = {A concise and self-contained introduction to causal inference, increasingly important in data science and machine learning.},
  editorb = {Bach, Francis},
  editorbtype = {redactor},
  _isbn = {978-0-262-03731-0},
  langid = {english},
  pagetotal = {288}
}

@unpublished{petsiukRISERandomizedInput2018,
  title = {{{RISE}}: {{Randomized Input Sampling}} for {{Explanation}} of {{Black-box Models}}},
  shorttitle = {{{RISE}}},
  author = {Petsiuk, Vitali and Das, Abir and Saenko, Kate},
  year = {2018-09-25},
  eprint = {1806.07421},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1806.07421},
  _urldate = {2020-10-22},
  abstract = {Deep neural networks are being used increasingly to automate data analysis and decision making, yet their decision-making process is largely unclear and is difficult to explain to the end users. In this paper, we address the problem of Explainable AI for deep neural networks that take images as input and output a class probability. We propose an approach called RISE that generates an importance map indicating how salient each pixel is for the model's prediction. In contrast to white-box approaches that estimate pixel importance using gradients or other internal network state, RISE works on black-box models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs. We compare our approach to state-of-the-art importance extraction methods using both an automatic deletion/insertion metric and a pointing metric based on human-annotated object segments. Extensive experiments on several benchmark datasets show that our approach matches or exceeds the performance of other methods, including white-box approaches. Project page: http://cs-people.bu.edu/vpetsiuk/rise/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@unpublished{petsiukRISERandomizedInput2018a,
  title = {{{RISE}}: {{Randomized Input Sampling}} for {{Explanation}} of {{Black-box Models}}},
  shorttitle = {{{RISE}}},
  author = {Petsiuk, Vitali and Das, Abir and Saenko, Kate},
  year = {2018-09},
  eprint = {1806.07421},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1806.07421},
  _urldate = {2020-10-22},
  abstract = {Deep neural networks are being used increasingly to automate data analysis and decision making, yet their decision-making process is largely unclear and is difficult to explain to the end users. In this paper, we address the problem of Explainable AI for deep neural networks that take images as input and output a class probability. We propose an approach called RISE that generates an importance map indicating how salient each pixel is for the model's prediction. In contrast to white-box approaches that estimate pixel importance using gradients or other internal network state, RISE works on black-box models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs. We compare our approach to state-of-the-art importance extraction methods using both an automatic deletion/insertion metric and a pointing metric based on human-annotated object segments. Extensive experiments on several benchmark datasets show that our approach matches or exceeds the performance of other methods, including white-box approaches. Project page: http://cs-people.bu.edu/vpetsiuk/rise/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@unpublished{phillipsExplanatoryMasksNeural2019,
  title = {Explanatory {{Masks}} for {{Neural Network Interpretability}}},
  author = {Phillips, Lawrence and Goh, Garrett and Hodas, Nathan},
  year = {2019-11-15},
  eprint = {1911.06876},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1911.06876},
  _urldate = {2020-04-20},
  abstract = {Neural network interpretability is a vital component for applications across a wide variety of domains. In such cases it is often useful to analyze a network which has already been trained for its specific purpose. In this work, we develop a method to produce explanation masks for pre-trained networks. The mask localizes the most important aspects of each input for prediction of the original network. Masks are created by a secondary network whose goal is to create as small an explanation as possible while still preserving the predictive accuracy of the original network. We demonstrate the applicability of our method for image classification with CNNs, sentiment analysis with RNNs, and chemical property prediction with mixed CNN/RNN architectures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{phillipsFourPrinciplesExplainable2020,
  title = {Four {{Principles}} of {{Explainable AI}} as {{Applied}} to {{Biometrics}} and {{Facial Forensic Algorithms}}},
  author = {Phillips, P. Jonathon and Przybocki, Mark},
  year = {2020-02-03},
  eprint = {2002.01014},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2002.01014},
  _urldate = {2020-04-20},
  abstract = {Traditionally, researchers in automatic face recognition and biometric technologies have focused on developing accurate algorithms. With this technology being integrated into operational systems, engineers and scientists are being asked, do these systems meet societal norms? The origin of this line of inquiry is `trust' of artificial intelligence (AI) systems. In this paper, we concentrate on adapting explainable AI to face recognition and biometrics, and we present four principles of explainable AI to face recognition and biometrics. The principles are illustrated by \$\textbackslash it\{four\}\$ case studies, which show the challenges and issues in developing algorithms that can produce explanations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition}
}

@unpublished{plumbFindingFixingSpurious2021,
  title = {Finding and {{Fixing Spurious Patterns}} with {{Explanations}}},
  author = {Plumb, Gregory and Ribeiro, Marco Tulio and Talwalkar, Ameet},
  year = {2021-12-06},
  eprint = {2106.02112},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2106.02112},
  _urldate = {2022-01-21},
  abstract = {Machine learning models often use spurious patterns such as "relying on the presence of a person to detect a tennis racket," which do not generalize. In this work, we present an end-to-end pipeline for identifying and mitigating spurious patterns for image classifiers. We start by finding patterns such as "the model's prediction for tennis racket changes 63\% of the time if we hide the people." Then, if a pattern is spurious, we mitigate it via a novel form of data augmentation. We demonstrate that this approach identifies a diverse set of spurious patterns and that it mitigates them by producing a model that is both more accurate on a distribution where the spurious pattern is not helpful and more robust to distribution shift.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/rick/Zotero/storage/4GEQPA98/Plumb et al. - 2021 - Finding and Fixing Spurious Patterns with Explanat.pdf;/home/rick/Zotero/storage/XF9BYK2Z/2106.html}
}

@unpublished{poceviciuteSurveyXAIDigital2020,
  title = {Survey of {{XAI}} in Digital Pathology},
  author = {Pocevičiūtė, Milda and Eilertsen, Gabriel and Lundström, Claes},
  year = {2020},
  volume = {12090},
  eprint = {2008.06353},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  pages = {56--88},
  _doi = {10.1007/978-3-030-50402-1_4},
  __url = {http://arxiv.org/abs/2008.06353},
  _urldate = {2020-09-16},
  abstract = {Artificial intelligence (AI) has shown great promise for diagnostic imaging assessments. However, the application of AI to support medical diagnostics in clinical routine comes with many challenges. The algorithms should have high prediction accuracy but also be transparent, understandable and reliable. Thus, explainable artificial intelligence (XAI) is highly relevant for this domain. We present a survey on XAI within digital pathology, a medical imaging sub-discipline with particular characteristics and needs. The review includes several contributions. Firstly, we give a thorough overview of current XAI techniques of potential relevance for deep learning methods in pathology imaging, and categorise them from three different aspects. In doing so, we incorporate uncertainty estimation methods as an integral part of the XAI landscape. We also connect the technical methods to the specific prerequisites in digital pathology and present findings to guide future research efforts. The survey is intended for both technical researchers and medical professionals, one of the objectives being to establish a common ground for cross-disciplinary discussions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing}
}

@unpublished{popordanoskaMachineGuidesHuman2020,
  title = {Machine {{Guides}}, {{Human Supervises}}: {{Interactive Learning}} with {{Global Explanations}}},
  shorttitle = {Machine {{Guides}}, {{Human Supervises}}},
  author = {Popordanoska, Teodora and Kumar, Mohit and Teso, Stefano},
  year = {2020-09-21},
  eprint = {2009.09723},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2009.09723},
  _urldate = {2020-11-30},
  abstract = {We introduce explanatory guided learning (XGL), a novel interactive learning strategy in which a machine guides a human supervisor toward selecting informative examples for a classifier. The guidance is provided by means of global explanations, which summarize the classifier's behavior on different regions of the instance space and expose its flaws. Compared to other explanatory interactive learning strategies, which are machine-initiated and rely on local explanations, XGL is designed to be robust against cases in which the explanations supplied by the machine oversell the classifier's quality. Moreover, XGL leverages global explanations to open up the black-box of human-initiated interaction, enabling supervisors to select informative examples that challenge the learned model. By drawing a link to interactive machine teaching, we show theoretically that global explanations are a viable approach for guiding supervisors. Our simulations show that explanatory guided learning avoids overselling the model's quality and performs comparably or better than machine- and human-initiated interactive learning strategies in terms of model quality.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{poursabzi-sangdehManipulatingMeasuringModel2019,
  title = {Manipulating and {{Measuring Model Interpretability}}},
  author = {Poursabzi-Sangdeh, Forough and Goldstein, Daniel G. and Hofman, Jake M. and Vaughan, Jennifer Wortman and Wallach, Hanna},
  year = {2019-11-08},
  eprint = {1802.07810},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1802.07810},
  _urldate = {2020-10-19},
  abstract = {With the increased use of machine learning in decision-making scenarios, there has been a growing interest in creating human-interpretable machine learning models. While many such models have been proposed, there have been relatively few experimental studies of whether these models achieve their intended effects, such as encouraging people to follow the model's predictions when the model is correct and to deviate when it makes a mistake. We present a series of randomized, pre-registered experiments comprising 3,800 participants in which people were shown functionally identical models that varied only in two factors thought to influence interpretability: the number of input features and the model transparency (clear or black-box). Predictably, participants who were shown a clear model with a small number of features were better able to simulate the model's predictions. However, contrary to what one might expect when manipulating interpretability, we found no improvements in the degree to which participants followed the model's predictions when it was beneficial to do so. Even more surprisingly, increased transparency hampered people's ability to detect when the model makes a sizable mistake and correct for it, seemingly due to information overload. These counterintuitive results suggest that decision scientists creating interpretable models should harbor a healthy skepticism of their intuitions and empirically verify that interpretable models achieve their intended effects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society}
}

@unpublished{prabhushankarContrastiveExplanationsNeural2020,
  title = {Contrastive {{Explanations}} in {{Neural Networks}}},
  author = {Prabhushankar, Mohit and Kwon, Gukyeong and Temel, Dogancan and AlRegib, Ghassan},
  year = {2020-08-01},
  eprint = {2008.00178},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2008.00178},
  _urldate = {2020-09-16},
  abstract = {Visual explanations are logical arguments based on visual features that justify the predictions made by neural networks. Current modes of visual explanations answer questions of the form \$`Why \textbackslash text\{ \} P?'\$. These \$Why\$ questions operate under broad contexts thereby providing answers that are irrelevant in some cases. We propose to constrain these \$Why\$ questions based on some context \$Q\$ so that our explanations answer contrastive questions of the form \$`Why \textbackslash text\{ \} P, \textbackslash text\{\} rather \textbackslash text\{ \} than \textbackslash text\{ \} Q?'\$. In this paper, we formalize the structure of contrastive visual explanations for neural networks. We define contrast based on neural networks and propose a methodology to extract defined contrasts. We then use the extracted contrasts as a plug-in on top of existing \$`Why \textbackslash text\{ \} P?'\$ techniques, specifically Grad-CAM. We demonstrate their value in analyzing both networks and data in applications of large-scale recognition, fine-grained recognition, subsurface seismic analysis, and image quality assessment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@incollection{quinlanPROBABILISTICDECISIONTREES1990,
  title = {{{PROBABILISTIC DECISION TREES}}},
  booktitle = {Machine {{Learning}}},
  author = {Quinlan, J.R.},
  year = {1990},
  pages = {140--152},
  publisher = {{Elsevier}},
  _doi = {10.1016/B978-0-08-051055-2.50011-0},
  __url = {https://linkinghub.elsevier.com/retrieve/pii/B9780080510552500110},
  _urldate = {2022-04-10},
  _isbn = {978-0-08-051055-2},
  langid = {english}
}

@unpublished{rackauckasUniversalDifferentialEquations2020,
  title = {Universal {{Differential Equations}} for {{Scientific Machine Learning}}},
  author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali},
  year = {2020-01-13},
  eprint = {2001.04385},
  eprinttype = {arxiv},
  primaryclass = {cs, math, q-bio, stat},
  __url = {http://arxiv.org/abs/2001.04385},
  _urldate = {2020-04-20},
  abstract = {In the context of science, the well-known adage "a picture is worth a thousand words" might well be "a model is worth a thousand datasets." Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring "big data". In this work we develop a new methodology, universal differential equations (UDEs), which augments scientific models with machine-learnable structures for scientifically-based learning. We show how UDEs can be utilized to discover previously unknown governing equations, accurately extrapolate beyond the original data, and accelerate model simulation, all in a time and data-efficient manner. This advance is coupled with open-source software that allows for training UDEs which incorporate physical constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity in the model. Our examples show how a diverse set of computationally-difficult modeling issues across scientific disciplines, from automatically discovering biological mechanisms to accelerating climate simulations by 15,000x, can be handled by training UDEs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Dynamical Systems,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning}
}

@incollection{raderExplanationsMechanismsSupporting2018,
  title = {Explanations as {{Mechanisms}} for {{Supporting Algorithmic Transparency}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Rader, Emilee and Cotter, Kelley and Cho, Janghee},
  year = {2018-04-19},
  pages = {1--13},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  __url = {https://doi.org/10.1145/3173574.3173677},
  _urldate = {2020-10-19},
  abstract = {Transparency can empower users to make informed choices about how they use an algorithmic decision-making system and judge its potential consequences. However, transparency is often conceptualized by the outcomes it is intended to bring about, not the specifics of mechanisms to achieve those outcomes. We conducted an online experiment focusing on how different ways of explaining Facebook's News Feed algorithm might affect participants' beliefs and judgments about the News Feed. We found that all explanations caused participants to become more aware of how the system works, and helped them to determine whether the system is biased and if they can control what they see. The explanations were less effective for helping participants evaluate the correctness of the system's output, and form opinions about how sensible and consistent its behavior is. We present implications for the design of transparency mechanisms in algorithmic decision-making systems based on these results.},
  _isbn = {978-1-4503-5620-6},
  keywords = {algorithmic decision-making,explanations,transparency}
}

@unpublished{rahnamaEvaluationLocalModelAgnostic2021,
  title = {Evaluation of {{Local Model-Agnostic Explanations Using Ground Truth}}},
  author = {Rahnama, Amir Hossein Akhavan and Butepage, Judith and Geurts, Pierre and Bostrom, Henrik},
  year = {2021-06-04},
  eprint = {2106.02488},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2106.02488},
  _urldate = {2021-09-29},
  abstract = {Explanation techniques are commonly evaluated using human-grounded methods, limiting the possibilities for large-scale evaluations and rapid progress in the development of new techniques. We propose a functionally-grounded evaluation procedure for local model-agnostic explanation techniques. In our approach, we generate ground truth for explanations when the black-box model is Logistic Regression and Gaussian Naive Bayes and compare how similar each explanation is to the extracted ground truth. In our empirical study, explanations of Local Interpretable Model-agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), and Local Permutation Importance (LPI) are compared in terms of how similar they are to the extracted ground truth. In the case of Logistic Regression, we find that the performance of the explanation techniques is highly dependent on the normalization of the data. In contrast, Local Permutation Importance outperforms the other techniques on Naive Bayes, irrespective of normalization. We hope that this work lays the foundation for further research into functionally-grounded evaluation methods for explanation techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{rasulMultivariateProbabilisticTime2021,
  title = {Multivariate {{Probabilistic Time Series Forecasting}} via {{Conditioned Normalizing Flows}}},
  author = {Rasul, Kashif and Sheikh, Abdul-Saboor and Schuster, Ingmar and Bergmann, Urs and Vollgraf, Roland},
  year = {2021-01-14},
  eprint = {2002.06103},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2002.06103},
  _urldate = {2021-09-03},
  abstract = {Time series forecasting is often fundamental to scientific and engineering problems and enables decision making. With ever increasing data set sizes, a trivial solution to scale up predictions is to assume independence between interacting time series. However, modeling statistical dependencies can improve accuracy and enable analysis of interaction effects. Deep learning methods are well suited for this problem, but multivariate models often assume a simple parametric distribution and do not scale to high dimensions. In this work we model the multivariate temporal dynamics of time series via an autoregressive deep learning model, where the data distribution is represented by a conditioned normalizing flow. This combination retains the power of autoregressive models, such as good performance in extrapolation into the future, with the flexibility of flows as a general purpose high-dimensional distribution model, while remaining computationally tractable. We show that it improves over the state-of-the-art for standard metrics on many real-world data sets with several thousand interacting time-series.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{reddi2015doubly,
  title = {Doubly Robust Covariate Shift Correction},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Reddi, Sashank and Poczos, Barnabas and Smola, Alex},
  year = {2015},
  volume = {29},
  number = {1}
}

@unpublished{redelmeierExplainingPredictiveModels2020,
  title = {Explaining Predictive Models with Mixed Features Using {{Shapley}} Values and Conditional Inference Trees},
  author = {Redelmeier, Annabelle and Jullum, Martin and Aas, Kjersti},
  year = {2020-07-02},
  eprint = {2007.01027},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.01027},
  _urldate = {2020-08-05},
  abstract = {It is becoming increasingly important to explain complex, black-box machine learning models. Although there is an expanding literature on this topic, Shapley values stand out as a sound method to explain predictions from any type of machine learning model. The original development of Shapley values for prediction explanation relied on the assumption that the features being described were independent. This methodology was then extended to explain dependent features with an underlying continuous distribution. In this paper, we propose a method to explain mixed (i.e. continuous, discrete, ordinal, and categorical) dependent features by modeling the dependence structure of the features using conditional inference trees. We demonstrate our proposed method against the current industry standards in various simulation studies and find that our method often outperforms the other approaches. Finally, we apply our method to a real financial data set used in the 2018 FICO Explainable Machine Learning Challenge and show how our explanations compare to the FICO challenge Recognition Award winning team.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@online{ReleasesSneakerstheratZoteroscihub,
  title = {Releases · Sneakers-the-Rat/Zotero-Scihub},
  __url = {/sneakers-the-rat/zotero-scihub/releases},
  _urldate = {2021-05-12},
  abstract = {A plugin that will automatically download PDFs of zotero items from sci-hub - sneakers-the-rat/zotero-scihub},
  langid = {english},
  organization = {{GitHub}},
  file = {/home/rick/Zotero/storage/JJKM9NJI/Releases · sneakers-the-rat/zotero-scihub.html}
}

@unpublished{rengasamyMoreReliableInterpretation2020,
  title = {Towards a {{More Reliable Interpretation}} of {{Machine Learning Outputs}} for {{Safety-Critical Systems}} Using {{Feature Importance Fusion}}},
  author = {Rengasamy, Divish and Rothwell, Benjamin and Figueredo, Grazziela},
  year = {2020-09-11},
  eprint = {2009.05501},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2009.05501},
  _urldate = {2020-11-30},
  abstract = {When machine learning supports decision-making in safety-critical systems, it is important to verify and understand the reasons why a particular output is produced. Although feature importance calculation approaches assist in interpretation, there is a lack of consensus regarding how features' importance is quantified, which makes the explanations offered for the outcomes mostly unreliable. A possible solution to address the lack of agreement is to combine the results from multiple feature importance quantifiers to reduce the variance of estimates. Our hypothesis is that this will lead to more robust and trustworthy interpretations of the contribution of each feature to machine learning predictions. To assist test this hypothesis, we propose an extensible Framework divided in four main parts: (i) traditional data pre-processing and preparation for predictive machine learning models; (ii) predictive machine learning; (iii) feature importance quantification and (iv) feature importance decision fusion using an ensemble strategy. We also introduce a novel fusion metric and compare it to the state-of-the-art. Our approach is tested on synthetic data, where the ground truth is known. We compare different fusion approaches and their results for both training and test sets. We also investigate how different characteristics within the datasets affect the feature importance ensembles studied. Results show that our feature importance ensemble Framework overall produces 15\% less feature importance error compared to existing methods. Additionally, results reveal that different levels of noise in the datasets do not affect the feature importance ensembles' ability to accurately quantify feature importance, whereas the feature importance quantification error increases with the number of features and number of orthogonal informative features.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{rennieLossFunctionsPreference2005,
  title = {Loss Functions for Preference Levels: {{Regression}} with Discrete Ordered Labels},
  shorttitle = {Loss Functions for Preference Levels},
  booktitle = {Proceedings of the {{IJCAI Multidisciplinary Workshop}} on {{Advances}} in {{Preference Handling}}},
  author = {Rennie, Jason D. M.},
  year = {2005},
  pages = {180--186},
  abstract = {We consider different types of loss functions for discrete ordinal regression, i.e. fitting labels that may take one of several discrete, but ordered, values. These types of labels arise when preferences are specified by selecting, for each item, one of several rating “levels”, e.g. one through five stars. We present two general threshold-based constructions which can be used to generalize loss functions for binary labels, such as the logistic and hinge loss, and another generalization of the logistic loss based on a probabilistic model for discrete ordered labels. Experiments on the 1 Million MovieLens data set indicate that one of our construction is a significant improvement over previous classification- and regression-based approaches. 1},
  file = {/home/rick/Zotero/storage/3DFV65ER/Rennie - 2005 - Loss functions for preference levels Regression w.pdf;/home/rick/Zotero/storage/J2JMTUFU/summary.html}
}

@inproceedings{ribeiroAnchorsHighprecisionModelagnostic2018,
  title = {Anchors: {{High-precision}} Model-Agnostic Explanations},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2018},
  volume = {32},
  number = {1}
}

@unpublished{ribeiroModelAgnosticInterpretabilityMachine2016,
  title = {Model-{{Agnostic Interpretability}} of {{Machine Learning}}},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016-06-16},
  eprint = {1606.05386},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1606.05386},
  _urldate = {2020-10-19},
  abstract = {Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{ribeiroWhyShouldTrust2016,
  title = {" {{Why}} Should {{I}} Trust You?" {{Explaining}} the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD}} International Conference on Knowledge Discovery and Data Mining},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  pages = {1135--1144}
}

@book{RM-670-PR,
  title = {Notes on the N-Person Game \&mdash; {{II}}: {{The}} Value of an n-Person Game},
  author = {Shapley, Lloyd S},
  year = {1951},
  publisher = {{RAND Corporation}},
  location = {{Santa Monica, CA}},
  _doi = {10.7249/RM0670}
}

@unpublished{robinsonCanContrastiveLearning2021,
  title = {Can Contrastive Learning Avoid Shortcut Solutions?},
  author = {Robinson, Joshua and Sun, Li and Yu, Ke and Batmanghelich, Kayhan and Jegelka, Stefanie and Sra, Suvrit},
  year = {2021-06-21},
  eprint = {2106.11230},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2106.11230},
  _urldate = {2021-11-07},
  abstract = {The generalization of representations learned via contrastive learning depends crucially on what features of the data are extracted. However, we observe that the contrastive loss does not always sufficiently guide which features are extracted, a behavior that can negatively impact the performance on downstream tasks via "shortcuts", i.e., by inadvertently suppressing important predictive features. We find that feature extraction is influenced by the difficulty of the so-called instance discrimination task (i.e., the task of discriminating pairs of similar points from pairs of dissimilar ones). Although harder pairs improve the representation of some features, the improvement comes at the cost of suppressing previously well represented features. In response, we propose implicit feature modification (IFM), a method for altering positive and negative samples in order to guide contrastive models towards capturing a wider variety of predictive features. Empirically, we observe that IFM reduces feature suppression, and as a result improves performance on vision and medical imaging tasks. The code is available at: \textbackslash _url\{https://github.com/joshr17/IFM\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/rick/Zotero/storage/PDID2BC8/Robinson et al. - 2021 - Can contrastive learning avoid shortcut solutions.pdf;/home/rick/Zotero/storage/T5U83UAI/2106.html}
}

@unpublished{robinsonCanContrastiveLearning2021a,
  title = {Can Contrastive Learning Avoid Shortcut Solutions?},
  author = {Robinson, Joshua and Sun, Li and Yu, Ke and Batmanghelich, Kayhan and Jegelka, Stefanie and Sra, Suvrit},
  year = {2021-11-11},
  eprint = {2106.11230},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2106.11230},
  _urldate = {2021-11-15},
  abstract = {The generalization of representations learned via contrastive learning depends crucially on what features of the data are extracted. However, we observe that the contrastive loss does not always sufficiently guide which features are extracted, a behavior that can negatively impact the performance on downstream tasks via "shortcuts", i.e., by inadvertently suppressing important predictive features. We find that feature extraction is influenced by the difficulty of the so-called instance discrimination task (i.e., the task of discriminating pairs of similar points from pairs of dissimilar ones). Although harder pairs improve the representation of some features, the improvement comes at the cost of suppressing previously well represented features. In response, we propose implicit feature modification (IFM), a method for altering positive and negative samples in order to guide contrastive models towards capturing a wider variety of predictive features. Empirically, we observe that IFM reduces feature suppression, and as a result improves performance on vision and medical imaging tasks. The code is available at: \textbackslash _url\{https://github.com/joshr17/IFM\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/rick/Zotero/storage/462PQHFR/Robinson et al. - 2021 - Can contrastive learning avoid shortcut solutions.pdf;/home/rick/Zotero/storage/F2EPSV24/2106.html}
}

@unpublished{robinsonCanContrastiveLearning2021b,
  title = {Can Contrastive Learning Avoid Shortcut Solutions?},
  author = {Robinson, Joshua and Sun, Li and Yu, Ke and Batmanghelich, Kayhan and Jegelka, Stefanie and Sra, Suvrit},
  year = {2021-11-11},
  eprint = {2106.11230},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2106.11230},
  _urldate = {2021-12-03},
  abstract = {The generalization of representations learned via contrastive learning depends crucially on what features of the data are extracted. However, we observe that the contrastive loss does not always sufficiently guide which features are extracted, a behavior that can negatively impact the performance on downstream tasks via "shortcuts", i.e., by inadvertently suppressing important predictive features. We find that feature extraction is influenced by the difficulty of the so-called instance discrimination task (i.e., the task of discriminating pairs of similar points from pairs of dissimilar ones). Although harder pairs improve the representation of some features, the improvement comes at the cost of suppressing previously well represented features. In response, we propose implicit feature modification (IFM), a method for altering positive and negative samples in order to guide contrastive models towards capturing a wider variety of predictive features. Empirically, we observe that IFM reduces feature suppression, and as a result improves performance on vision and medical imaging tasks. The code is available at: \textbackslash _url\{https://github.com/joshr17/IFM\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/rick/Zotero/storage/GZUF3XH3/Robinson et al. - 2021 - Can contrastive learning avoid shortcut solutions.pdf;/home/rick/Zotero/storage/Z6QDA6AB/2106.html}
}

@unpublished{robinsonDissectingDeepNeural2020,
  title = {Dissecting {{Deep Neural Networks}}},
  author = {Robinson, Haakon and Rasheed, Adil and San, Omer},
  year = {2020-01-19},
  eprint = {1910.03879},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1910.03879},
  _urldate = {2020-04-20},
  abstract = {In exchange for large quantities of data and processing power, deep neural networks have yielded models that provide state of the art predication capabilities in many fields. However, a lack of strong guarantees on their behaviour have raised concerns over their use in safety-critical applications. A first step to understanding these networks is to develop alternate representations that allow for further analysis. It has been shown that neural networks with piecewise affine activation functions are themselves piecewise affine, with their domains consisting of a vast number of linear regions. So far, the research on this topic has focused on counting the number of linear regions, rather than obtaining explicit piecewise affine representations. This work presents a novel algorithm that can compute the piecewise affine form of any fully connected neural network with rectified linear unit activations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@unpublished{roffoInfiniteLatentFeature2017,
  title = {Infinite {{Latent Feature Selection}}: {{A Probabilistic Latent Graph-Based Ranking Approach}}},
  shorttitle = {Infinite {{Latent Feature Selection}}},
  author = {Roffo, Giorgio and Melzi, Simone and Castellani, Umberto and Vinciarelli, Alessandro},
  year = {2017-07-24},
  eprint = {1707.07538},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1707.07538},
  _urldate = {2022-04-04},
  abstract = {Feature selection is playing an increasingly significant role with respect to many computer vision applications spanning from object recognition to visual object tracking. However, most of the recent solutions in feature selection are not robust across different and heterogeneous set of data. In this paper, we address this issue proposing a robust probabilistic latent graph-based feature selection algorithm that performs the ranking step while considering all the possible subsets of features, as paths on a graph, bypassing the combinatorial problem analytically. An appealing characteristic of the approach is that it aims to discover an abstraction behind low-level sensory data, that is, relevancy. Relevancy is modelled as a latent variable in a PLSA-inspired generative process that allows the investigation of the importance of a feature when injected into an arbitrary set of cues. The proposed method has been tested on ten diverse benchmarks, and compared against eleven state of the art feature selection methods. Results show that the proposed approach attains the highest performance levels across many different scenarios and difficulties, thereby confirming its strong robustness while setting a new state of the art in feature selection domain.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/rick/Zotero/storage/PDPGK3LN/Roffo et al. - 2017 - Infinite Latent Feature Selection A Probabilistic.pdf;/home/rick/Zotero/storage/ADH6QFAB/1707.html}
}

@unpublished{rojatExplainableArtificialIntelligence2021,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}) on {{TimeSeries Data}}: {{A Survey}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}}) on {{TimeSeries Data}}},
  author = {Rojat, Thomas and Puget, Raphaël and Filliat, David and Del Ser, Javier and Gelin, Rodolphe and Díaz-Rodríguez, Natalia},
  year = {2021-04-02},
  eprint = {2104.00950},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2104.00950},
  _urldate = {2021-10-06},
  abstract = {Most of state of the art methods applied on time series consist of deep learning methods that are too complex to be interpreted. This lack of interpretability is a major drawback, as several applications in the real world are critical tasks, such as the medical field or the autonomous driving field. The explainability of models applied on time series has not gather much attention compared to the computer vision or the natural language processing fields. In this paper, we present an overview of existing explainable AI (XAI) methods applied on time series and illustrate the type of explanations they produce. We also provide a reflection on the impact of these explanation methods to provide confidence and trust in the AI systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/rick/Zotero/storage/ZXU34CG7/Rojat et al. - 2021 - Explainable Artificial Intelligence (XAI) on TimeS.pdf;/home/rick/Zotero/storage/TV7D5JTS/2104.html}
}

@article{roscherExplainableMachineLearning2020,
  title = {Explainable {{Machine Learning}} for {{Scientific Insights}} and {{Discoveries}}},
  author = {Roscher, Ribana and Bohn, Bastian and Duarte, Marco F. and Garcke, Jochen},
  year = {2020},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {8},
  pages = {42200--42216},
  _issn = {2169-3536},
  _doi = {10.1109/ACCESS.2020.2976199},
  __url = {https://ieeexplore.ieee.org/document/9007737/},
  _urldate = {2022-04-09},
  file = {/home/rick/Zotero/storage/V6G6C5JH/Roscher et al. - 2020 - Explainable Machine Learning for Scientific Insigh.pdf}
}

@unpublished{rosenfeldRisksInvariantRisk2021,
  title = {The {{Risks}} of {{Invariant Risk Minimization}}},
  author = {Rosenfeld, Elan and Ravikumar, Pradeep and Risteski, Andrej},
  year = {2021-03-27},
  eprint = {2010.05761},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2010.05761},
  _urldate = {2021-05-12},
  abstract = {Invariant Causal Prediction (Peters et al., 2016) is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. (2019) proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested. However, formal guarantees for all of these works are severely lacking. In this paper, we present the first analysis of classification under the IRM objective--as well as these recently proposed alternatives--under a fairly natural and general model. In the linear case, we show simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very first results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data are sufficiently similar to the training distribution--this is precisely the issue that it was intended to solve. Thus, in this setting we find that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/BSDWDVQV/Rosenfeld et al. - 2021 - The Risks of Invariant Risk Minimization.pdf;/home/rick/Zotero/storage/IJCIICCU/2010.html}
}

@article{rudinStopExplainingBlack2019,
  title = {Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead},
  author = {Rudin, Cynthia},
  year = {2019},
  journaltitle = {Nature Machine Intelligence},
  journal = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {1},
  number = {5},
  pages = {206--215},
  langid = {english},
}

@unpublished{rychenerQUACKIENLPClassification2020,
  title = {{{QUACKIE}}: {{A NLP Classification Task With Ground Truth Explanations}}},
  shorttitle = {{{QUACKIE}}},
  author = {Rychener, Yves and Renard, Xavier and Seddah, Djamé and Frossard, Pascal and Detyniecki, Marcin},
  year = {2020-12-27},
  eprint = {2012.13190},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2012.13190},
  _urldate = {2021-09-29},
  abstract = {NLP Interpretability aims to increase trust in model predictions. This makes evaluating interpretability approaches a pressing issue. There are multiple datasets for evaluating NLP Interpretability, but their dependence on human provided ground truths raises questions about their unbiasedness. In this work, we take a different approach and formulate a specific classification task by diverting question-answering datasets. For this custom classification task, the interpretability ground-truth arises directly from the definition of the classification problem. We use this method to propose a benchmark and lay the groundwork for future research in NLP interpretability by evaluating a wide range of current state of the art methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Statistics - Machine Learning}
}

@article{saitoPrecisionrecallPlotMore2015,
  title = {The Precision-Recall Plot Is More Informative than the {{ROC}} Plot When Evaluating Binary Classifiers on Imbalanced Datasets},
  author = {Saito, Takaya and Rehmsmeier, Marc},
  year = {2015},
  journaltitle = {PloS one},
  volume = {10},
  number = {3},
  pages = {e0118432},
  publisher = {{Public Library of Science}}
}

@article{samekEvaluatingVisualizationWhat2015,
    title = "Evaluating the visualization of what a deep neural network has learned",
    abstract = "Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the 'importance' of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.",
    keywords = "Convolutional neural networks, Explaining classification, Image classification, Interpretable machine learning, Relevance models",
    author = "Wojciech Samek and Alexander Binder and Gr{\'e}goire Montavon and Sebastian Lapuschkin and M{\"u}ller, {Klaus Robert}",
    note = "",
    year = "2017",
    month = nov,
    _doi = "10.1109/TNNLS.2016.2599820",
    language = "English",
    volume = "28",
    pages = "2660--2673",
    journal = "IEEE Transactions on Neural Networks and Learning Systems",
    _issn = "2162-237X",
    publisher = "IEEE Computational Intelligence Society",
    number = "11",
}

@book{samekExplainableAIInterpreting2019,
  title = {Explainable {{AI}}: {{Interpreting}}, {{Explaining}} and {{Visualizing Deep Learning}}},
  shorttitle = {Explainable {{AI}}},
  author = {Samek, Wojciech and Montavon, Grégoire and Vedaldi, Andrea and Hansen, Lars Kai and Muller, Klaus-Robert},
  year = {2019},
  __url = {https://doi.org/10.1007/978-3-030-28954-6},
  _urldate = {2020-04-20},
  _isbn = {978-3-030-28954-6},
  langid = {english},
  annotation = {OCLC: 1123168393}
}

@unpublished{samekExplainableArtificialIntelligence2017,
  title = {Explainable {{Artificial Intelligence}}: {{Understanding}}, {{Visualizing}} and {{Interpreting Deep Learning Models}}},
  shorttitle = {Explainable {{Artificial Intelligence}}},
  author = {Samek, Wojciech and Wiegand, Thomas and Müller, Klaus-Robert},
  year = {2017-08-28},
  eprint = {1708.08296},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1708.08296},
  _urldate = {2020-10-19},
  abstract = {With the availability of large databases and recent improvements in deep learning methodology, the performance of AI systems is reaching or even exceeding the human level on an increasing number of complex tasks. Impressive examples of this development can be found in domains such as image classification, sentiment analysis, speech understanding or strategic game playing. However, because of their nested non-linear structure, these highly successful machine learning and artificial intelligence models are usually applied in a black box manner, i.e., no information is provided about what exactly makes them arrive at their predictions. Since this lack of transparency can be a major drawback, e.g., in medical applications, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This paper summarizes recent developments in this field and makes a plea for more interpretability in artificial intelligence. Furthermore, it presents two approaches to explaining predictions of deep learning models, one method which computes the sensitivity of the prediction with respect to changes in the input and one approach which meaningfully decomposes the decision in terms of the input variables. These methods are evaluated on three classification tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@article{samekExplainingDeepNeural2021,
  title = {Explaining {{Deep Neural Networks}} and {{Beyond}}: {{A Review}} of {{Methods}} and {{Applications}}},
  shorttitle = {Explaining {{Deep Neural Networks}} and {{Beyond}}},
  author = {Samek, Wojciech and Montavon, Grégoire and Lapuschkin, Sebastian and Anders, Christopher J. and Müller, Klaus-Robert},
  year = {2021},
  journaltitle = {Proceedings of the IEEE},
  journal = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {109},
  number = {3},
  eprint = {2003.07631},
  eprinttype = {arxiv},
  pages = {247--278},
  _issn = {0018-9219, 1558-2256},
  _doi = {10.1109/JPROC.2021.3060483},
  __url = {http://arxiv.org/abs/2003.07631},
  _urldate = {2022-04-03},
  abstract = {With the broader and highly successful usage of machine learning in industry and the sciences, there has been a growing demand for Explainable AI. Interpretability and explanation methods for gaining a better understanding about the problem solving abilities and strategies of nonlinear Machine Learning, in particular, deep neural networks, are therefore receiving increased attention. In this work we aim to (1) provide a timely overview of this active emerging field, with a focus on 'post-hoc' explanations, and explain its theoretical foundations, (2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations, (3) outline best practice aspects i.e. how to best include interpretation methods into the standard usage of machine learning and (4) demonstrate successful usage of explainable AI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/MBHZV9K5/Samek et al. - 2021 - Explaining Deep Neural Networks and Beyond A Revi.pdf;/home/rick/Zotero/storage/ADNPA7I8/2003.html}
}

@article{samekExplainingDeepNeural2021a,
  title = {Explaining {{Deep Neural Networks}} and {{Beyond}}: {{A Review}} of {{Methods}} and {{Applications}}},
  shorttitle = {Explaining {{Deep Neural Networks}} and {{Beyond}}},
  author = {Samek, Wojciech and Montavon, Grégoire and Lapuschkin, Sebastian and Anders, Christopher J. and Müller, Klaus-Robert},
  year = {2021-03},
  journaltitle = {Proceedings of the IEEE},
  volume = {109},
  number = {3},
  pages = {247--278},
  _issn = {0018-9219, 1558-2256},
  _doi = {10.1109/JPROC.2021.3060483},
  __url = {http://arxiv.org/abs/2003.07631},
  _urldate = {2022-04-03},
  abstract = {With the broader and highly successful usage of machine learning in industry and the sciences, there has been a growing demand for Explainable AI. Interpretability and explanation methods for gaining a better understanding about the problem solving abilities and strategies of nonlinear Machine Learning, in particular, deep neural networks, are therefore receiving increased attention. In this work we aim to (1) provide a timely overview of this active emerging field, with a focus on 'post-hoc' explanations, and explain its theoretical foundations, (2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations, (3) outline best practice aspects i.e. how to best include interpretation methods into the standard usage of machine learning and (4) demonstrate successful usage of explainable AI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of machine learning.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/NS5TG5RJ/Samek et al. - 2021 - Explaining Deep Neural Networks and Beyond A Revi.pdf;/home/rick/Zotero/storage/ZKG9UMWZ/2003.html}
}

@unpublished{scherrerLearningNeuralCausal2021,
  title = {Learning {{Neural Causal Models}} with {{Active Interventions}}},
  author = {Scherrer, Nino and Bilaniuk, Olexa and Annadani, Yashas and Goyal, Anirudh and Schwab, Patrick and Schölkopf, Bernhard and Mozer, Michael C. and Bengio, Yoshua and Bauer, Stefan and Ke, Nan Rosemary},
  year = {2021-09-06},
  eprint = {2109.02429},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2109.02429},
  _urldate = {2021-09-25},
  abstract = {Discovering causal structures from data is a challenging inference problem of fundamental importance in all areas of science. The appealing scaling properties of neural networks have recently led to a surge of interest in differentiable neural network-based methods for learning causal structures from data. So far differentiable causal discovery has focused on static datasets of observational or interventional origin. In this work, we introduce an active intervention-targeting mechanism which enables a quick identification of the underlying causal structure of the data-generating process. Our method significantly reduces the required number of interactions compared with random intervention targeting and is applicable for both discrete and continuous optimization formulations of learning the underlying directed acyclic graph (DAG) from data. We examine the proposed method across a wide range of settings and demonstrate superior performance on multiple benchmarks from simulated to real-world data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{schmidtQuantifyingInterpretabilityTrust2019,
  title = {Quantifying {{Interpretability}} and {{Trust}} in {{Machine Learning Systems}}},
  author = {Schmidt, Philipp and Biessmann, Felix},
  year = {2019-01-20},
  eprint = {1901.08558},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1901.08558},
  _urldate = {2020-05-31},
  abstract = {Decisions by Machine Learning (ML) models have become ubiquitous. Trusting these decisions requires understanding how algorithms take them. Hence interpretability methods for ML are an active focus of research. A central problem in this context is that both the quality of interpretability methods as well as trust in ML predictions are difficult to measure. Yet evaluations, comparisons and improvements of trust and interpretability require quantifiable measures. Here we propose a quantitative measure for the quality of interpretability methods. Based on that we derive a quantitative measure of trust in ML decisions. Building on previous work we propose to measure intuitive understanding of algorithmic decisions using the information transfer rate at which humans replicate ML model predictions. We provide empirical evidence from crowdsourcing experiments that the proposed metric robustly differentiates interpretability methods. The proposed metric also demonstrates the value of interpretability for ML assisted human decision making: in our experiments providing explanations more than doubled productivity in annotation tasks. However unbiased human judgement is critical for doctors, judges, policy makers and others. Here we derive a trust metric that identifies when human decisions are overly biased towards ML predictions. Our results complement existing qualitative work on trust and interpretability by quantifiable measures that can serve as objectives for further improving methods in this field of research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{schneiderReflectiveNetLearningExplanations2020,
  title = {Reflective-{{Net}}: {{Learning}} from {{Explanations}}},
  shorttitle = {Reflective-{{Net}}},
  author = {Schneider, Johannes and Vlachos, Michalis},
  year = {2020-11-27},
  eprint = {2011.13986},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2011.13986},
  _urldate = {2020-12-01},
  abstract = {Humans possess a remarkable capability to make fast, intuitive decisions, but also to self-reflect, i.e., to explain to oneself, and to efficiently learn from explanations by others. This work provides the first steps toward mimicking this process by capitalizing on the explanations generated based on existing explanation methods, i.e. Grad-CAM. Learning from explanations combined with conventional labeled data yields significant improvements for classification in terms of accuracy and training time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@unpublished{schockaertVAELIMEDeepGenerative2020,
  title = {{{VAE-LIME}}: {{Deep Generative Model Based Approach}} for {{Local Data-Driven Model Interpretability Applied}} to the {{Ironmaking Industry}}},
  shorttitle = {{{VAE-LIME}}},
  author = {Schockaert, Cedric and Macher, Vadim and Schmitz, Alexander},
  year = {2020-07-15},
  eprint = {2007.10256},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.10256},
  _urldate = {2020-07-30},
  abstract = {Machine learning applied to generate data-driven models are lacking of transparency leading the process engineer to lose confidence in relying on the model predictions to optimize his industrial process. Bringing processes in the industry to a certain level of autonomy using data-driven models is particularly challenging as the first user of those models, is the expert in the process with often decades of experience. It is necessary to expose to the process engineer, not solely the model predictions, but also their interpretability. To that end, several approaches have been proposed in the literature. The Local Interpretable Model-agnostic Explanations (LIME) method has gained a lot of interest from the research community recently. The principle of this method is to train a linear model that is locally approximating the black-box model, by generating randomly artificial data points locally. Model-agnostic local interpretability solutions based on LIME have recently emerged to improve the original method. We present in this paper a novel approach, VAE-LIME, for local interpretability of data-driven models forecasting the temperature of the hot metal produced by a blast furnace. Such ironmaking process data is characterized by multivariate time series with high inter-correlation representing the underlying process in a blast furnace. Our contribution is to use a Variational Autoencoder (VAE) to learn the complex blast furnace process characteristics from the data. The VAE is aiming at generating optimal artificial samples to train a local interpretable model better representing the black-box model in the neighborhood of the input sample processed by the black-box model to make a prediction. In comparison with LIME, VAE-LIME is showing a significantly improved local fidelity of the local interpretable linear model with the black-box model resulting in robust model interpretability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{scholkopfCausalityMachineLearning2019,
  title = {Causality for {{Machine Learning}}},
  author = {Schölkopf, Bernhard},
  year = {2019-12-23},
  eprint = {1911.10500},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1911.10500},
  _urldate = {2021-09-25},
  abstract = {Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning. This article discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the field is beginning to understand them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2,I.2; I.5; K.4,I.5,K.4,Statistics - Machine Learning}
}

@unpublished{scornetTreesForestsImpuritybased2020,
  title = {Trees, Forests, and Impurity-Based Variable Importance},
  author = {Scornet, Erwan},
  year = {2020-11-10},
  eprint = {2001.04295},
  eprinttype = {arxiv},
  primaryclass = {math, stat},
  __url = {http://arxiv.org/abs/2001.04295},
  _urldate = {2021-01-02},
  abstract = {Tree ensemble methods such as random forests [Breiman, 2001] are very popular to handle high-dimensional tabular data sets, notably because of their good predictive accuracy. However, when machine learning is used for decision-making problems, settling for the best predictive procedures may not be reasonable since enlightened decisions require an in-depth comprehension of the algorithm prediction process. Unfortunately, random forests are not intrinsically interpretable since their prediction results from averaging several hundreds of decision trees. A classic approach to gain knowledge on this so-called black-box algorithm is to compute variable importances, that are employed to assess the predictive impact of each input variable. Variable importances are then used to rank or select variables and thus play a great role in data analysis. Nevertheless, there is no justification to use random forest variable importances in such way: we do not even know what these quantities estimate. In this paper, we analyze one of the two well-known random forest variable importances, the Mean Decrease Impurity (MDI). We prove that if input variables are independent and in absence of interactions, MDI provides a variance decomposition of the output, where the contribution of each variable is clearly identified. We also study models exhibiting dependence between input variables or interaction, for which the variable importance is intrinsically ill-defined. Our analysis shows that there may exist some benefits to use a forest compared to a single tree.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/3MBI67AR/Scornet - 2020 - Trees, forests, and impurity-based variable import.pdf;/home/rick/Zotero/storage/LVFDRJ4L/2001.html}
}

@inproceedings{tomsett2020sanity,
  title={Sanity checks for saliency metrics},
  author={Tomsett, Richard and Harborne, Dan and Chakraborty, Supriyo and Gurram, Prudhvi and Preece, Alun},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={04},
  pages={6021--6029},
  year={2020}
}

@inproceedings{selvarajuGradCAMVisualExplanations2017,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Selvaraju, R. R. and Cogswell, M. and Das, A. and Vedantam, R. and Parikh, D. and Batra, D.},
  year = {2017},
  pages = {618--626},
  _issn = {2380-7504},
  _doi = {10.1109/ICCV.2017.74},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for `dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a `stronger' deep network from a `weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] and video at youtu.be/COjUB9Izk6E.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  keywords = {Cats,CNN,CNN model-families,coarse localization map,Computer architecture,convolution,Convolutional Neural Network,data visualisation,deep networks,Dogs,fine-grained visualizations,Grad- CAM,Grad-CAM explanations,gradient methods,gradient-based localization,Gradient-weighted Class Activation Mapping,Guided Grad-CAM,high-resolution class-discriminative visualization,image captioning,image classification,image classification models,image representation,inference mechanisms,Knowledge discovery,learning (artificial intelligence),neural nets,nonattention based models,object detection,object recognition,reinforcement learning,visual explanations,visual question answering models,Visualization}
}

@inproceedings{selvarajuGradCAMVisualExplanations2017a,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-Based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Selvaraju, R. R. and Cogswell, M. and Das, A. and Vedantam, R. and Parikh, D. and Batra, D.},
  year = {2017-10},
  pages = {618--626},
  _doi = {10.1109/ICCV.2017.74},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for `dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad- CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a `stronger' deep network from a `weaker' one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2] and video at youtu.be/COjUB9Izk6E.},
  keywords = {Cats,CNN,CNN model-families,coarse localization map,Computer architecture,convolution,Convolutional Neural Network,data visualisation,deep networks,Dogs,fine-grained visualizations,Grad- CAM,Grad-CAM explanations,gradient methods,gradient-based localization,Gradient-weighted Class Activation Mapping,Guided Grad-CAM,high-resolution class-discriminative visualization,image captioning,image classification,image classification models,image representation,inference mechanisms,Knowledge discovery,learning (artificial intelligence),neural nets,nonattention based models,object detection,object recognition,reinforcement learning,visual explanations,visual question answering models,Visualization}
}

@incollection{shapleyValueNPersonGames1953,
  title = {A {{Value}} for N-{{Person Games}}},
  booktitle = {Contributions to the {{Theory}} of {{Games}} ({{AM-28}}), {{Volume II}}},
  author = {Shapley, L. S.},
  editor = {Kuhn, Harold William and Tucker, Albert William},
  year = {1953},
  pages = {307--318},
  publisher = {{Princeton University Press}},
  _doi = {10.1515/9781400881970-018},
  __url = {https://www.degruyter.com/document/doi/10.1515/9781400881970-018/html},
  _urldate = {2022-02-02},
  _isbn = {978-1-4008-8197-0}
}

@article{shmueliCommentBreimanTwo2021,
  title = {Comment on {{Breiman}}'s "{{Two Cultures}}" (2002): {{From Two Cultures}} to {{Multicultural}}},
  shorttitle = {Comment on {{Breiman}}'s "{{Two Cultures}}" (2002)},
  author = {Shmueli, Galit},
  year = {2021},
  journaltitle = {Observational Studies},
  shortjournal = {Observational Studies},
  volume = {7},
  number = {1},
  pages = {197--201},
  _issn = {2767-3324},
  _doi = {10.1353/obs.2021.0010},
  __url = {https://muse.jhu.edu/article/799735},
  _urldate = {2022-02-14},
  langid = {english},
  file = {/home/rick/Zotero/storage/2IIFFULJ/Shmueli - 2021 - Comment on Breiman's Two Cultures (2002) From T.pdf}
}

@article{shmueliExplainPredict2010,
  title = {To {{Explain}} or to {{Predict}}?},
  author = {Shmueli, Galit},
  year = {2010},
  journaltitle = {Statistical Science},
  journal = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {25},
  number = {3},
  eprint = {1101.0891},
  eprinttype = {arxiv},
  _issn = {0883-4237},
  _doi = {10.1214/10-STS330},
  __url = {http://arxiv.org/abs/1101.0891},
  _urldate = {2022-02-10},
  abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {/home/rick/Zotero/storage/82RDH9TY/Shmueli - 2010 - To Explain or to Predict.pdf;/home/rick/Zotero/storage/BXVMF856/1101.html}
}

@article{shmueliPredictiveAnalyticsInformation2011,
  title = {Predictive {{Analytics}} in {{Information Systems Research}}},
  author = {Shmueli, Galit and Koppius, Otto R.},
  year = {2011},
  journaltitle = {MIS Quarterly},
  volume = {35},
  number = {3},
  eprint = {23042796},
  eprinttype = {jstor},
  pages = {553--572},
  publisher = {{Management Information Systems Research Center, University of Minnesota}},
  _issn = {02767783},
  abstract = {This research essay highlights the need to integrate predictive analytics into information systems research and shows several concrete ways in which this goal can be accomplished. Predictive analytics include empirical methods (statistical and other) that generate data predictions as well as methods for assessing predictive power. Predictive analytics not only assist in creating practically useful models, they also play an important role alongside explanatory modeling in theory building and theory testing. We describe six roles for predictive analytics: new theory generation, measurement development, comparison of competing theories, improvement of existing models, relevance assessment, and assessment of the predictability of empirical phenomena. Despite the importance of predictive analytics, we find that they are rare in the empirical IS literature. Extant IS literature relies nearly exclusively on explanatory statistical modeling, where statistical inference is used to test and evaluate the explanatory power of underlying causal models, and predictive power is assumed to follow automatically from the explanatory model. However, explanatory power does not imply predictive power and thus predictive analytics are necessary for assessing predictive power and for building empirical models that predict well. To show that predictive analytics and explanatory statistical modeling are fundamentally disparate, we show that they are different in each step of the modeling process. These differences translate into different final models, so that a pure explanatory statistical model is best tuned for testing causal hypotheses and a pure predictive model is best in terms of predictive power. We convert a well-known explanatory paper on TAM to a predictive context to illustrate these differences and show how predictive analytics can add theoretical and practical value to IS research.}
}

@article{shmueliPredictiveModelAssessment2019,
  title = {Predictive Model Assessment in {{PLS-SEM}}: Guidelines for Using {{PLSpredict}}},
  shorttitle = {Predictive Model Assessment in {{PLS-SEM}}},
  author = {Shmueli, Galit and Sarstedt, Marko and Hair, Joseph F. and Cheah, Jun-Hwa and Ting, Hiram and Vaithilingam, Santha and Ringle, Christian M.},
  year = {2019-11-11},
  journaltitle = {European Journal of Marketing},
  shortjournal = {EJM},
  volume = {53},
  number = {11},
  pages = {2322--2347},
  _issn = {0309-0566, 0309-0566},
  _doi = {10.1108/EJM-02-2019-0189},
  __url = {https://www.emerald.com/insight/content/doi/10.1108/EJM-02-2019-0189/full/html},
  _urldate = {2022-02-14},
  abstract = {Purpose               Partial least squares (PLS) has been introduced as a “causal-predictive” approach to structural equation modeling (SEM), designed to overcome the apparent dichotomy between explanation and prediction. However, while researchers using PLS-SEM routinely stress the predictive nature of their analyses, model evaluation assessment relies exclusively on metrics designed to assess the path model’s explanatory power. Recent research has proposed PLSpredict, a holdout sample-based procedure that generates case-level predictions on an item or a construct level. This paper offers guidelines for applying PLSpredict and explains the key choices researchers need to make using the procedure.                                         Design/methodology/approach               The authors discuss the need for prediction-oriented model evaluations in PLS-SEM and conceptually explain and further advance the PLSpredict method. In addition, they illustrate the PLSpredict procedure’s use with a tourism marketing model and provide recommendations on how the results should be interpreted. While the focus of the paper is on the PLSpredict procedure, the overarching aim is to encourage the routine prediction-oriented assessment in PLS-SEM analyses.                                         Findings               The paper advances PLSpredict and offers guidance on how to use this prediction-oriented model evaluation approach. Researchers should routinely consider the assessment of the predictive power of their PLS path models. PLSpredict is a useful and straightforward approach to evaluate the out-of-sample predictive capabilities of PLS path models that researchers can apply in their studies.                                         Research limitations/implications               Future research should seek to extend PLSpredict’s capabilities, for example, by developing more benchmarks for comparing PLS-SEM results and empirically contrasting the earliest antecedent and the direct antecedent approaches to predictive power assessment.                                         Practical implications               This paper offers clear guidelines for using PLSpredict, which researchers and practitioners should routinely apply as part of their PLS-SEM analyses.                                         Originality/value               This research substantiates the use of PLSpredict. It provides marketing researchers and practitioners with the knowledge they need to properly assess, report and interpret PLS-SEM results. Thereby, this research contributes to safeguarding the rigor of marketing studies using PLS-SEM.},
  langid = {english}
}

@article{shrikumarLearningImportantFeatures2017,
  title = {Learning {{Important Features Through Propagating Activation Differences}}},
  author = {Shrikumar, A. and Greenside, P. and Kundaje, A.},
  year = {2017},
  journaltitle = {ICML}
}


@article{silverMasteringGameGo2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  options = {useprefix=true},
  year = {2017-10},
  journaltitle = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  _issn = {1476-4687},
  _doi = {10.1038/nature24270},
  __url = {https://www.nature.com/articles/nature24270},
  _urldate = {2020-12-07},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.},
  langid = {english}
}

@inproceedings{Simonyan14a,
  title = {Deep inside Convolutional Networks: {{Visualising}} Image Classification Models and Saliency Maps},
  booktitle = {Workshop at International Conference on Learning Representations},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2014}
}

@unpublished{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015-04-10},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1409.1556},
  _urldate = {2020-11-04},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@InProceedings{sixtWhenExplanationsLie2020,
  title = 	 {When Explanations Lie: Why Many Modified {BP} Attributions Fail},
  author =       {Sixt, Leon and Granz, Maximilian and Landgraf, Tim},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9046--9057},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/sixt20a/sixt20a.pdf},
  _url = 	 {https://proceedings.mlr.press/v119/sixt20a.html},
  abstract = 	 {Attribution methods aim to explain a neural network’s prediction by highlighting the most relevant image areas. A popular approach is to backpropagate (BP) a custom relevance score using modified rules, rather than the gradient. We analyze an extensive set of modified BP methods: Deep Taylor Decomposition, Layer-wise Relevance Propagation (LRP), Excitation BP, PatternAttribution, DeepLIFT, Deconv, RectGrad, and Guided BP. We find empirically that the explanations of all mentioned methods, except for DeepLIFT, are independent of the parameters of later layers. We provide theoretical insights for this surprising behavior and also analyze why DeepLIFT does not suffer from this limitation. Empirically, we measure how information of later layers is ignored by using our new metric, cosine similarity convergence (CSC). The paper provides a framework to assess the faithfulness of new and existing modified BP methods theoretically and empirically.}
}



@unpublished{slackAssessingLocalInterpretability2019,
  title = {Assessing the {{Local Interpretability}} of {{Machine Learning Models}}},
  author = {Slack, Dylan and Friedler, Sorelle A. and Scheidegger, Carlos and Roy, Chitradeep Dutta},
  year = {2019-08-02},
  eprint = {1902.03501},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1902.03501},
  _urldate = {2020-10-22},
  abstract = {The increasing adoption of machine learning tools has led to calls for accountability via model interpretability. But what does it mean for a machine learning model to be interpretable by humans, and how can this be assessed? We focus on two definitions of interpretability that have been introduced in the machine learning literature: simulatability (a user's ability to run a model on a given input) and "what if" local explainability (a user's ability to correctly determine a model's prediction under local changes to the input, given knowledge of the model's original prediction). Through a user study with 1,000 participants, we test whether humans perform well on tasks that mimic the definitions of simulatability and "what if" local explainability on models that are typically considered locally interpretable. To track the relative interpretability of models, we employ a simple metric, the runtime operation count on the simulatability task. We find evidence that as the number of operations increases, participant accuracy on the local interpretability tasks decreases. In addition, this evidence is consistent with the common intuition that decision trees and logistic regression models are interpretable and are more interpretable than neural networks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{slackHowMuchShould2020,
  title = {How {{Much Should I Trust You}}? {{Modeling Uncertainty}} of {{Black Box Explanations}}},
  shorttitle = {How {{Much Should I Trust You}}?},
  author = {Slack, Dylan and Hilgard, Sophie and Singh, Sameer and Lakkaraju, Himabindu},
  year = {2020-08-11},
  eprint = {2008.05030},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2008.05030},
  _urldate = {2020-09-16},
  abstract = {As local explanations of black box models are increasingly being employed to establish model credibility in high stakes settings, it is important to ensure that these explanations are accurate and reliable. However, local explanations generated by existing techniques are often prone to high variance. Further, these techniques are computationally inefficient, require significant hyper-parameter tuning, and provide little insight into the quality of the resulting explanations. By identifying lack of uncertainty modeling as the main cause of these challenges, we propose a novel Bayesian framework that produces explanations that go beyond point-wise estimates of feature importance. We instantiate this framework to generate Bayesian versions of LIME and KernelSHAP. In particular, we estimate credible intervals (CIs) that capture the uncertainty associated with each feature importance in local explanations. These credible intervals are tight when we have high confidence in the feature importances of a local explanation. The CIs are also informative both for estimating how many perturbations we need to sample -- sampling can proceed until the CIs are sufficiently narrow -- and where to sample -- sampling in regions with high predictive uncertainty leads to faster convergence. Experimental evaluation with multiple real world datasets and user studies demonstrate the efficacy of our framework and the resulting explanations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{sokolFaithfulMeaningfulInterpretable2020,
  title = {Towards {{Faithful}} and {{Meaningful Interpretable Representations}}},
  author = {Sokol, Kacper and Flach, Peter},
  year = {2020-08-16},
  eprint = {2008.07007},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2008.07007},
  _urldate = {2020-09-16},
  abstract = {Interpretable representations are the backbone of many black-box explainers. They translate the low-level data representation necessary for good predictive performance into high-level human-intelligible concepts used to convey the explanation. Notably, the explanation type and its cognitive complexity are directly controlled by the interpretable representation, allowing to target a particular audience and use case. However, many explainers that rely on interpretable representations overlook their merit and fall back on default solutions, which may introduce implicit assumptions, thereby degrading the explanatory power of such techniques. To address this problem, we study properties of interpretable representations that encode presence and absence of human-comprehensible concepts. We show how they are operationalised for tabular, image and text data, discussing their strengths and weaknesses. Finally, we analyse their explanatory properties in the context of tabular data, where a linear model is used to quantify the importance of interpretable concepts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{sokolOneExplanationDoes2020,
  title = {One {{Explanation Does Not Fit All}}},
  author = {Sokol, Kacper and Flach, Peter},
  year = {2020-06-01},
  journaltitle = {KI - Künstliche Intelligenz},
  shortjournal = {Künstl Intell},
  volume = {34},
  number = {2},
  pages = {235--250},
  _issn = {1610-1987},
  _doi = {10.1007/s13218-020-00637-y},
  __url = {https://doi.org/10.1007/s13218-020-00637-y},
  _urldate = {2020-10-22},
  abstract = {The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system’s operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the promises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations—a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up “What if?” questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee’s mental model, which is the main building block of intelligible human–machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend “Wizard of Oz” studies as a proxy for testing and evaluating standalone interactive explainability algorithms.},
  langid = {english}
}

@inproceedings{speicherUnifiedApproachQuantifying2018,
  title = {A {{Unified Approach}} to {{Quantifying Algorithmic Unfairness}}: {{Measuring Individual}} \&amp;{{Group Unfairness}} via {{Inequality Indices}}},
  shorttitle = {A {{Unified Approach}} to {{Quantifying Algorithmic Unfairness}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Speicher, Till and Heidari, Hoda and Grgic-Hlaca, Nina and Gummadi, Krishna P. and Singla, Adish and Weller, Adrian and Zafar, Muhammad Bilal},
  year = {2018-07-19},
  series = {{{KDD}} '18},
  pages = {2239--2248},
  publisher = {{Association for Computing Machinery}},
  location = {{London, United Kingdom}},
  _doi = {10.1145/3219819.3220046},
  __url = {https://doi.org/10.1145/3219819.3220046},
  _urldate = {2020-10-19},
  abstract = {Discrimination via algorithmic decision making has received considerable attention. Prior work largely focuses on defining conditions for fairness, but does not define satisfactory measures of algorithmic unfairness. In this paper, we focus on the following question: Given two unfair algorithms, how should we determine which of the two is more unfair? Our core idea is to use existing inequality indices from economics to measure how unequally the outcomes of an algorithm benefit different individuals or groups in a population. Our work offers a justified and general framework to compare and contrast the (un)fairness of algorithmic predictors. This unifying approach enables us to quantify unfairness both at the individual and the group level. Further, our work reveals overlooked tradeoffs between different fairness notions: using our proposed measures, the overall individual-level unfairness of an algorithm can be decomposed into a between-group and a within-group component. Earlier methods are typically designed to tackle only between-group un- fairness, which may be justified for legal or other reasons. However, we demonstrate that minimizing exclusively the between-group component may, in fact, increase the within-group, and hence the overall unfairness. We characterize and illustrate the tradeoffs between our measures of (un)fairness and the prediction accuracy.},
  _isbn = {978-1-4503-5552-0},
  keywords = {algorithmic decision making,fairness in machine learning,fairness measures,generalized entropy,group fairness,individual fairness,inequality indices,subgroup decomposability}
}

@inproceedings{springenbergStrivingSimplicityAll2015,
  title = {Striving for Simplicity: The All Convolutional Net},
  shorttitle = {Striving for Simplicity},
  author = {Springenberg, J. and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, M.},
  year = {2015},
  __url = {https://lmb.informatik.uni-freiburg.de/Publications/2015/DB15a/},
  _urldate = {2020-12-07},
  eventtitle = {ICLR (workshop track)},
  booktitle  = {ICLR (workshop track)},
  langid = {german}
}

@article{stroblBiasRandomForest2007,
  title = {Bias in Random Forest Variable Importance Measures: {{Illustrations}}, Sources and a Solution},
  shorttitle = {Bias in Random Forest Variable Importance Measures},
  author = {Strobl, Carolin and Boulesteix, Anne-Laure and Zeileis, Achim and Hothorn, Torsten},
  year = {2007-01-25},
  journaltitle = {BMC Bioinformatics},
  shortjournal = {BMC Bioinformatics},
  volume = {8},
  number = {1},
  pages = {25},
  _issn = {1471-2105},
  _doi = {10.1186/1471-2105-8-25},
  __url = {https://doi.org/10.1186/1471-2105-8-25},
  _urldate = {2020-10-07},
  abstract = {Variable importance measures for random forests have been receiving increased attention as a means of variable selection in many classification tasks in bioinformatics and related scientific fields, for instance to select a subset of genetic markers relevant for the prediction of a certain disease. We show that random forest variable importance measures are a sensible means for variable selection in many applications, but are not reliable in situations where potential predictor variables vary in their scale of measurement or their number of categories. This is particularly important in genomics and computational biology, where predictors often include variables of different types, for example when predictors include both sequence data and continuous variables such as folding energy, or when amino acid sequence data show different numbers of categories.}
}

@article{strumbeljEfficientExplanationIndividual2010,
  title = {An {{Efficient Explanation}} of {{Individual Classifications}} Using {{Game Theory}}},
  author = {Strumbelj, Erik and Kononenko, Igor},
  year = {2010-03-01},
  journaltitle = {The Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {11},
  pages = {1--18},
  _issn = {1532-4435},
  abstract = {We present a general method for explaining individual predictions of classification models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method's initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efficient and that the explanations are intuitive and useful.}
}

@article{gevrey2003reviewVariableContr,
title = {Review and comparison of methods to study the contribution of variables in artificial neural network models},
journal = {Ecological Modelling},
volume = {160},
number = {3},
pages = {249-264},
year = {2003},
author = {Muriel Gevrey and Ioannis Dimopoulos and Sovan Lek},
}

@article{strumbeljExplainingPredictionModels2014,
  title = {Explaining Prediction Models and Individual Predictions with Feature Contributions},
  author = {Štrumbelj, Erik and Kononenko, Igor},
  year = {2014},
  journaltitle = {Knowledge and Information Systems},
  journal = {Knowledge and Information Systems},
  shortjournal = {Knowl Inf Syst},
  volume = {41},
  number = {3},
  pages = {647--665},
  _issn = {0219-3116},
  _doi = {10.1007/s10115-013-0679-x},
  __url = {https://doi.org/10.1007/s10115-013-0679-x},
  _urldate = {2020-07-09},
  abstract = {We present a sensitivity analysis-based method for explaining prediction models that can be applied to any type of classification or regression model. Its advantage over existing general methods is that all subsets of input features are perturbed, so interactions and redundancies between features are taken into account. Furthermore, when explaining an additive model, the method is equivalent to commonly used additive model-specific methods. We illustrate the method’s usefulness with examples from artificial and real-world data sets and an empirical analysis of running times. Results from a controlled experiment with 122 participants suggest that the method’s explanations improved the participants’ understanding of the model.},
  langid = {english}
}

@unpublished{subramaniSurveyDeepLearning2020,
  title = {A {{Survey}} of {{Deep Learning Approaches}} for {{OCR}} and {{Document Understanding}}},
  author = {Subramani, Nishant and Matton, Alexandre and Greaves, Malcolm and Lam, Adrian},
  year = {2020-11-26},
  eprint = {2011.13534},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2011.13534},
  _urldate = {2020-11-30},
  abstract = {Documents are a core part of many businesses in many fields such as law, finance, and technology among others. Automatic understanding of documents such as invoices, contracts, and resumes is lucrative, opening up many new avenues of business. The fields of natural language processing and computer vision have seen tremendous progress through the development of deep learning such that these methods have started to become infused in contemporary document understanding systems. In this survey paper, we review different techniques for document understanding for documents written in English and consolidate methodologies present in literature to act as a jumping-off point for researchers exploring this area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning}
}

@book{sugiyama2012machine,
  title = {Machine Learning in Non-Stationary Environments: {{Introduction}} to Covariate Shift Adaptation},
  author = {Sugiyama, Masashi and Kawanabe, Motoaki},
  year = {2012},
  publisher = {{MIT press}}
}

@inproceedings{sundararajanAxiomaticAttributionDeep2017,
  title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  booktitle = {{{ICML}}},
  author = {Sundararajan, M. and Taly, Ankur and Yan, Qiqi},
  year = {2017},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms— Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.}
}

@unpublished{sundararajanAxiomaticAttributionDeep2017a,
  title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  year = {2017-06-12},
  eprint = {1703.01365},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1703.01365},
  _urldate = {2020-11-03},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{sundararajanAxiomaticAttributionDeep2017b,
  title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  booktitle = {{{ICML}}},
  author = {Sundararajan, M. and Taly, Ankur and Yan, Qiqi},
  year = {2017},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms— Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.}
}

@unpublished{sundararajanManyShapleyValues2020,
  title = {The Many {{Shapley}} Values for Model Explanation},
  author = {Sundararajan, Mukund and Najmi, Amir},
  year = {2020-02-07},
  eprint = {1908.08474},
  eprinttype = {arxiv},
  primaryclass = {cs, econ},
  __url = {http://arxiv.org/abs/1908.08474},
  _urldate = {2020-10-15},
  abstract = {The Shapley value has become a popular method to attribute the prediction of a machine-learning model on an input to its base features. The use of the Shapley value is justified by citing [16] showing that it is the \textbackslash emph\{unique\} method that satisfies certain good properties (\textbackslash emph\{axioms\}). There are, however, a multiplicity of ways in which the Shapley value is operationalized in the attribution problem. These differ in how they reference the model, the training data, and the explanation context. These give very different results, rendering the uniqueness result meaningless. Furthermore, we find that previously proposed approaches can produce counterintuitive attributions in theory and in practice---for instance, they can assign non-zero attributions to features that are not even referenced by the model. In this paper, we use the axiomatic approach to study the differences between some of the many operationalizations of the Shapley value for attribution, and propose a technique called Baseline Shapley (BShap) that is backed by a proper uniqueness result. We also contrast BShap with Integrated Gradients, another extension of Shapley value to the continuous setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Economics - Theoretical Economics}
}

@unpublished{suPixelAdaptiveConvolutionalNeural2019,
  title = {Pixel-{{Adaptive Convolutional Neural Networks}}},
  author = {Su, Hang and Jampani, Varun and Sun, Deqing and Gallo, Orazio and Learned-Miller, Erik and Kautz, Jan},
  year = {2019-04-10},
  eprint = {1904.05373},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1904.05373},
  _urldate = {2020-04-20},
  abstract = {Convolutions are the fundamental building block of CNNs. The fact that their weights are spatially shared is one of the main reasons for their widespread use, but it also is a major limitation, as it makes convolutions content agnostic. We propose a pixel-adaptive convolution (PAC) operation, a simple yet effective modification of standard convolutions, in which the filter weights are multiplied with a spatially-varying kernel that depends on learnable, local pixel features. PAC is a generalization of several popular filtering techniques and thus can be used for a wide range of use cases. Specifically, we demonstrate state-of-the-art performance when PAC is used for deep joint image upsampling. PAC also offers an effective alternative to fully-connected CRF (Full-CRF), called PAC-CRF, which performs competitively, while being considerably faster. In addition, we also demonstrate that PAC can be used as a drop-in replacement for convolution layers in pre-trained networks, resulting in consistent performance improvements.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Machine Learning}
}

@inproceedings{szegedyGoingDeeperConvolutions2015,
  title = {Going Deeper with Convolutions},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, C. and {Wei Liu} and {Yangqing Jia} and Sermanet, P. and Reed, S. and Anguelov, D. and Erhan, D. and Vanhoucke, V. and Rabinovich, A.},
  year = {2015-06},
  pages = {1--9},
  _issn = {1063-6919},
  _doi = {10.1109/CVPR.2015.7298594},
  abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  eventtitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  keywords = {architectural decision,Computer architecture,Computer vision,convolution,Convolutional codes,convolutional neural network architecture,decision making,feature extraction,Hebbian learning,Hebbian principle,image classification,neural net architecture,Neural networks,object classification,object detection,Object detection,resource allocation,resource utilization,Sparse matrices,Visualization}
}

@unpublished{taverniersMutualInformationExplainable2020,
  title = {Mutual {{Information}} for {{Explainable Deep Learning}} of {{Multiscale Systems}}},
  author = {Taverniers, Søren and Hall, Eric J. and Katsoulakis, Markos A. and Tartakovsky, Daniel M.},
  year = {2020-09-07},
  eprint = {2009.04570},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  __url = {http://arxiv.org/abs/2009.04570},
  _urldate = {2020-11-30},
  abstract = {Timely completion of design cycles for multiscale and multiphysics systems ranging from consumer electronics to hypersonic vehicles relies on rapid simulation-based prototyping. The latter typically involves high-dimensional spaces of possibly correlated control variables (CVs) and quantities of interest (QoIs) with non-Gaussian and/or multimodal distributions. We develop a model-agnostic, moment-independent global sensitivity analysis (GSA) that relies on differential mutual information to rank the effects of CVs on QoIs. Large amounts of data, which are necessary to rank CVs with confidence, are cheaply generated by a deep neural network (DNN) surrogate model of the underlying process. The DNN predictions are made explainable by the GSA so that the DNN can be deployed to close design loops. Our information-theoretic framework is compatible with a wide variety of black-box models. Its application to multiscale supercapacitor design demonstrates that the CV rankings facilitated by a domain-aware Graph-Informed Neural Network are better resolved than their counterparts obtained with a physics-based model for a fixed computational budget. Consequently, our information-theoretic GSA provides an "outer loop" for accelerated product design by identifying the most and least sensitive input directions and performing subsequent optimization over appropriately reduced parameter subspaces.},
  archiveprefix = {arXiv},
  keywords = {93B35 (Primary) 68T07; 62R07 (Secondary),Computer Science - Machine Learning,Mathematics - Numerical Analysis,Statistics - Machine Learning}
}

@article{thomasDefensePrattVariable2018,
  title = {In Defense of {{Pratt}}'s Variable Importance Axioms: {{A}} Response to {{Gromping}}},
  shorttitle = {In Defense of {{Pratt}}'s Variable Importance Axioms},
  author = {Thomas, D. Roland and Kwan, Ernest and Zumbo, Bruno D.},
  year = {2018-07},
  journaltitle = {WIREs Computational Statistics},
  shortjournal = {WIREs Comp Stat},
  volume = {10},
  number = {4},
  _issn = {1939-5108, 1939-0068},
  _doi = {10.1002/wics.1433},
  __url = {https://onlinelibrary.wiley.com/doi/10.1002/wics.1433},
  _urldate = {2021-11-26},
  langid = {english}
}

@article{thomasNoTitleFound1998,
  title = {[{{No}} Title Found]},
  author = {Thomas, D. Roland and Hughes, Edward and Zumbo, Bruno D.},
  year = {1998},
  journaltitle = {Social Indicators Research},
  volume = {45},
  number = {1/3},
  pages = {253--275},
  _issn = {03038300},
  _doi = {10.1023/A:1006954016433},
  __url = {http://link.springer.com/10.1023/A:1006954016433},
  _urldate = {2021-11-26}
}

@article{thomasVariableImportanceLinear1998,
  title = {On {{Variable Importance}} in {{Linear Regression}}},
  author = {Thomas, D. Roland and Hughes, Edward and Zumbo, Bruno D.},
  year = {1998},
  journaltitle = {Social Indicators Research},
  volume = {45},
  number = {1/3},
  pages = {253--275},
  _issn = {03038300},
  _doi = {10.1023/A:1006954016433},
  __url = {http://link.springer.com/10.1023/A:1006954016433},
  _urldate = {2021-11-26}
}

@article{tjoa2022quantifying,
  title={Quantifying Explainability of Saliency Methods in Deep Neural Networks With a Synthetic Dataset},
  author={Tjoa, Erico and Cuntai, Guan},
  journal={IEEE Transactions on Artificial Intelligence},
  year={2022},
  publisher={IEEE}
}

@unpublished{tjoaQuantifyingExplainabilitySaliency2020a,
  title = {Quantifying {{Explainability}} of {{Saliency Methods}} in {{Deep Neural Networks}}},
  author = {Tjoa, Erico and Guan, Cuntai},
  year = {2020-09},
  eprint = {2009.02899},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2009.02899},
  _urldate = {2021-01-02},
  abstract = {One way to achieve eXplainable artificial intelligence (XAI) is through the use of post-hoc analysis methods. In particular, methods that generate heatmaps have been used to explain black-box models, such as deep neural network. In some cases, heatmaps are appealing due to the intuitive and visual ways to understand them. However, quantitative analysis that demonstrates the actual potential of heatmaps have been lacking, and comparison between different methods are not standardized as well. In this paper, we introduce a synthetic data that can be generated adhoc along with the ground-truth heatmaps for better quantitative assessment. Each sample data is an image of a cell with easily distinguishable features, facilitating a more transparent assessment of different XAI methods. Comparison and recommendations are made, shortcomings are clarified along with suggestions for future research directions to handle the finer details of select post-hoc analysis methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/rick/Zotero/storage/32V66S66/Tjoa and Guan - 2020 - Quantifying Explainability of Saliency Methods in .pdf;/home/rick/Zotero/storage/JWMUZBX9/2009.html}
}

@unpublished{tjoaSurveyExplainableArtificial2020,
  title = {A {{Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}}): {{Towards Medical XAI}}},
  shorttitle = {A {{Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  author = {Tjoa, Erico and Guan, Cuntai},
  year = {2020-08-10},
  eprint = {1907.07374},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1907.07374},
  _urldate = {2020-10-19},
  abstract = {Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning. Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the deep learning is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide "obviously" interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that (1) clinicians and practitioners can subsequently approach these methods with caution, (2) insights into interpretability will be born with more considerations for medical practices, and (3) initiatives to push forward data-based, mathematically- and technically-grounded medical education are encouraged.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@unpublished{tomsettInterpretableWhomRolebased2018,
  title = {Interpretable to {{Whom}}? {{A Role-based Model}} for {{Analyzing Interpretable Machine Learning Systems}}},
  shorttitle = {Interpretable to {{Whom}}?},
  author = {Tomsett, Richard and Braines, Dave and Harborne, Dan and Preece, Alun and Chakraborty, Supriyo},
  year = {2018-06-20},
  eprint = {1806.07552},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1806.07552},
  _urldate = {2020-10-22},
  abstract = {Several researchers have argued that a machine learning system's interpretability should be defined in relation to a specific agent or task: we should not ask if the system is interpretable, but to whom is it interpretable. We describe a model intended to help answer this question, by identifying different roles that agents can fulfill in relation to the machine learning system. We illustrate the use of our model in a variety of scenarios, exploring how an agent's role influences its goals, and the implications for defining interpretability. Finally, we make suggestions for how our model could be useful to interpretability researchers, system developers, and regulatory bodies auditing machine learning systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence}
}

@unpublished{tonekaboniWhatCliniciansWant2019,
  title = {What {{Clinicians Want}}: {{Contextualizing Explainable Machine Learning}} for {{Clinical End Use}}},
  shorttitle = {What {{Clinicians Want}}},
  author = {Tonekaboni, Sana and Joshi, Shalmali and McCradden, Melissa D. and Goldenberg, Anna},
  year = {2019-08-07},
  eprint = {1905.05134},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1905.05134},
  _urldate = {2020-10-22},
  abstract = {Translating machine learning (ML) models effectively to clinical practice requires establishing clinicians' trust. Explainability, or the ability of an ML model to justify its outcomes and assist clinicians in rationalizing the model prediction, has been generally understood to be critical to establishing trust. However, the field suffers from the lack of concrete definitions for usable explanations in different settings. To identify specific aspects of explainability that may catalyze building trust in ML models, we surveyed clinicians from two distinct acute care specialties (Intenstive Care Unit and Emergency Department). We use their feedback to characterize when explainability helps to improve clinicians' trust in ML models. We further identify the classes of explanations that clinicians identified as most relevant and crucial for effective translation to clinical practice. Finally, we discern concrete metrics for rigorous evaluation of clinical explainability methods. By integrating perceptions of explainability between clinicians and ML researchers we hope to facilitate the endorsement and broader adoption and sustained use of ML systems in healthcare.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{tranUnsupervisedCausalBinary2021,
  title = {Unsupervised {{Causal Binary Concepts Discovery}} with {{VAE}} for {{Black-box Model Explanation}}},
  author = {Tran, Thien Q. and Fukuchi, Kazuto and Akimoto, Youhei and Sakuma, Jun},
  year = {2021-09-09},
  eprint = {2109.04518},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2109.04518},
  _urldate = {2021-09-17},
  abstract = {We aim to explain a black-box classifier with the form: `data X is classified as class Y because X \textbackslash textit\{has\} A, B and \textbackslash textit\{does not have\} C' in which A, B, and C are high-level concepts. The challenge is that we have to discover in an unsupervised manner a set of concepts, i.e., A, B and C, that is useful for the explaining the classifier. We first introduce a structural generative model that is suitable to express and discover such concepts. We then propose a learning process that simultaneously learns the data distribution and encourages certain concepts to have a large causal influence on the classifier output. Our method also allows easy integration of user's prior knowledge to induce high interpretability of concepts. Using multiple datasets, we demonstrate that our method can discover useful binary concepts for explanation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{tuSimpsonParadoxLord2008,
  title = {Simpson's {{Paradox}}, {{Lord}}'s {{Paradox}}, and {{Suppression Effects}} Are the Same Phenomenon – the Reversal Paradox},
  author = {Tu, Yu-Kang and Gunnell, David and Gilthorpe, Mark S},
  year = {2008-12},
  journaltitle = {Emerging Themes in Epidemiology},
  shortjournal = {Emerg Themes Epidemiol},
  volume = {5},
  number = {1},
  pages = {2},
  _issn = {1742-7622},
  _doi = {10.1186/1742-7622-5-2},
  __url = {https://ete-online.biomedcentral.com/articles/10.1186/1742-7622-5-2},
  _urldate = {2022-02-01},
  langid = {english},
  file = {/home/rick/Zotero/storage/HIMCPQSR/Tu et al. - 2008 - Simpson's Paradox, Lord's Paradox, and Suppression.pdf}
}

@article{vanderlaanStatisticalInferenceVariable2006,
  title = {Statistical {{Inference}} for {{Variable Importance}}},
  author = {van der Laan, Mark J.},
  options = {useprefix=true},
  year = {2006-01-20},
  journaltitle = {The International Journal of Biostatistics},
  volume = {2},
  number = {1},
  _issn = {1557-4679},
  _doi = {10.2202/1557-4679.1008},
  __url = {https://www.degruyter.com/document/doi/10.2202/1557-4679.1008/html},
  _urldate = {2021-07-21},
  file = {/home/rick/Zotero/storage/CLW2ML7S/van der Laan - 2006 - Statistical Inference for Variable Importance.pdf}
}

@article{vanderlaanStatisticalInferenceVariable2006a,
  title = {Statistical {{Inference}} for {{Variable Importance}}},
  author = {van der Laan, Mark J.},
  options = {useprefix=true},
  year = {2006-01-20},
  journaltitle = {The International Journal of Biostatistics},
  volume = {2},
  number = {1},
  _issn = {1557-4679},
  _doi = {10.2202/1557-4679.1008},
  __url = {https://www.degruyter.com/document/doi/10.2202/1557-4679.1008/html},
  _urldate = {2022-04-22},
  file = {/home/rick/Zotero/storage/E8TMPF5F/van der Laan - 2006 - Statistical Inference for Variable Importance.pdf}
}

@article{vapnikRethinkingStatisticalLearning2019,
  title = {Rethinking Statistical Learning Theory: Learning Using Statistical Invariants},
  shorttitle = {Rethinking Statistical Learning Theory},
  author = {Vapnik, Vladimir and Izmailov, Rauf},
  year = {2019-03-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {108},
  number = {3},
  pages = {381--423},
  _issn = {1573-0565},
  _doi = {10.1007/s10994-018-5742-0},
  __url = {https://doi.org/10.1007/s10994-018-5742-0},
  _urldate = {2020-04-20},
  abstract = {This paper introduces a new learning paradigm, called Learning Using Statistical Invariants (LUSI), which is different from the classical one. In a classical paradigm, the learning machine constructs a classification rule that minimizes the probability of expected error; it is data-driven model of learning. In the LUSI paradigm, in order to construct the desired classification function, a learning machine computes statistical invariants that are specific for the problem, and then minimizes the expected error in a way that preserves these invariants; it is thus both data- and invariant-driven learning. From a mathematical point of view, methods of the classical paradigm employ mechanisms of strong convergence of approximations to the desired function, whereas methods of the new paradigm employ both strong and weak convergence mechanisms. This can significantly increase the rate of convergence.},
  langid = {english}
}

@incollection{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {5998--6008},
  publisher = {{Curran Associates, Inc.}},
  __url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf},
  _urldate = {2020-11-04}
}

@unpublished{vermaCounterfactualExplanationsMachine2020,
  title = {Counterfactual {{Explanations}} for {{Machine Learning}}: {{A Review}}},
  shorttitle = {Counterfactual {{Explanations}} for {{Machine Learning}}},
  author = {Verma, Sahil and Dickerson, John and Hines, Keegan},
  year = {2020-10-20},
  eprint = {2010.10596},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2010.10596},
  _urldate = {2020-11-30},
  abstract = {Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{vidovicFeatureImportanceMeasure2016,
  title = {Feature {{Importance Measure}} for {{Non-linear Learning Algorithms}}},
  author = {Vidovic, Marina M.-C. and Görnitz, Nico and Müller, Klaus-Robert and Kloft, Marius},
  year = {2016-11-22},
  eprint = {1611.07567},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1611.07567},
  _urldate = {2020-10-07},
  abstract = {Complex problems may require sophisticated, non-linear learning methods such as kernel machines or deep neural networks to achieve state of the art prediction accuracies. However, high prediction accuracies are not the only objective to consider when solving problems using machine learning. Instead, particular scientific applications require some explanation of the learned prediction function. Unfortunately, most methods do not come with out of the box straight forward interpretation. Even linear prediction functions are not straight forward to explain if features exhibit complex correlation structure. In this paper, we propose the Measure of Feature Importance (MFI). MFI is general and can be applied to any arbitrary learning machine (including kernel machines and deep learning). MFI is intrinsically non-linear and can detect features that by itself are inconspicuous and only impact the prediction function through their interaction with other features. Lastly, MFI can be used for both --- model-based feature importance and instance-based feature importance (i.e, measuring the importance of a feature for a particular data point).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{viloneExplainableArtificialIntelligence2020,
  title = {Explainable {{Artificial Intelligence}}: A {{Systematic Review}}},
  shorttitle = {Explainable {{Artificial Intelligence}}},
  author = {Vilone, Giulia and Longo, Luca},
  year = {2020-10-12},
  eprint = {2006.00093},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2006.00093},
  _urldate = {2020-10-22},
  abstract = {Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.0,I.2.6,I.2.m}
}

@unpublished{viswanathanModelExplanationsAxiomatic2021,
  title = {Model {{Explanations}} via the {{Axiomatic Causal Lens}}},
  author = {Viswanathan, Vignesh and Zick, Yair},
  year = {2021-09-08},
  eprint = {2109.03890},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2109.03890},
  _urldate = {2021-09-17},
  abstract = {Explaining the decisions of black-box models has been a central theme in the study of trustworthy ML. Numerous measures have been proposed in the literature; however, none of them have been able to adopt a provably causal take on explainability. Building upon Halpern and Pearl's formal definition of a causal explanation, we derive an analogous set of axioms for the classification setting, and use them to derive three explanation measures. Our first measure is a natural adaptation of Chockler and Halpern's notion of causal responsibility, whereas the other two correspond to existing game-theoretic influence measures. We present an axiomatic treatment for our proposed indices, showing that they can be uniquely characterized by a set of desirable properties. We compliment this with computational analysis, providing probabilistic approximation schemes for all of our proposed measures. Thus, our work is the first to formally bridge the gap between model explanations, game-theoretic influence, and causal analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@unpublished{viswanathanModelExplanationsAxiomatic2021a,
  title = {Model {{Explanations}} via the {{Axiomatic Causal Lens}}},
  author = {Viswanathan, Vignesh and Zick, Yair},
  year = {2021-09-17},
  eprint = {2109.03890},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2109.03890},
  _urldate = {2021-10-06},
  abstract = {Explaining the decisions of black-box models has been a central theme in the study of trustworthy ML. Numerous measures have been proposed in the literature; however, none of them have been able to adopt a provably causal take on explainability. Building upon Halpern and Pearl's formal definition of a causal explanation, we derive an analogous set of axioms for the classification setting, and use them to derive three explanation measures. Our first measure is a natural adaptation of Chockler and Halpern's notion of causal responsibility, whereas the other two correspond to existing game-theoretic influence measures. We present an axiomatic treatment for our proposed indices, showing that they can be uniquely characterized by a set of desirable properties. We compliment this with computational analysis, providing probabilistic approximation schemes for all of our proposed measures. Thus, our work is the first to formally bridge the gap between model explanations, game-theoretic influence, and causal analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/rick/Zotero/storage/YGWTXGGJ/Viswanathan and Zick - 2021 - Model Explanations via the Axiomatic Causal Lens.pdf;/home/rick/Zotero/storage/FN54VLRA/2109.html}
}

@unpublished{vuPGMExplainerProbabilisticGraphical2020,
  title = {{{PGM-Explainer}}: {{Probabilistic Graphical Model Explanations}} for {{Graph Neural Networks}}},
  shorttitle = {{{PGM-Explainer}}},
  author = {Vu, Minh N. and Thai, My T.},
  year = {2020-10-12},
  eprint = {2010.05788},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2010.05788},
  _urldate = {2020-11-30},
  abstract = {In Graph Neural Networks (GNNs), the graph structure is incorporated into the learning of node representations. This complex structure makes explaining GNNs' predictions become much more challenging. In this paper, we propose PGM-Explainer, a Probabilistic Graphical Model (PGM) model-agnostic explainer for GNNs. Given a prediction to be explained, PGM-Explainer identifies crucial graph components and generates an explanation in form of a PGM approximating that prediction. Different from existing explainers for GNNs where the explanations are drawn from a set of linear functions of explained features, PGM-Explainer is able to demonstrate the dependencies of explained features in form of conditional probabilities. Our theoretical analysis shows that the PGM generated by PGM-Explainer includes the Markov-blanket of the target prediction, i.e. including all its statistical information. We also show that the explanation returned by PGM-Explainer contains the same set of independence statements in the perfect map. Our experiments on both synthetic and real-world datasets show that PGM-Explainer achieves better performance than existing explainers in many benchmark tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@article{waaInterpretableConfidenceMeasures2020,
  title = {Interpretable Confidence Measures for Decision Support Systems},
  author = {Waa, J. V. D. and Schoonderwoerd, Tjeerd and Diggelen, J. and Neerincx, M.},
  year = {2020-12-01},
  journaltitle = {International Journal of Human-Computer Studies},
  volume = {144},
  pages = {102493},
  _issn = {1071-5819},
  _doi = {10.1016/j.ijhcs.2020.102493},
  __url = {https://www.sciencedirect.com/science/article/pii/S1071581920300951},
  _urldate = {2020-12-02},
  abstract = {Decision support systems (DSS) have improved significantly but are more complex due to recent advances in Artificial Intelligence. Current XAI methods…},
  langid = {english}
}

@article{wachterCounterfactualExplanationsOpening2017,
  title = {Counterfactual Explanations without Opening the Black Box: {{Automated}} Decisions and the {{GDPR}}},
  author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  year = {2017},
  journaltitle = {Harv. JL \& Tech.},
  journal = {Harv. JL \& Tech.},
  volume = {31},
  pages = {841},
  publisher = {{HeinOnline}}
}

@unpublished{waldchenComputationalComplexityUnderstanding2019,
  title = {The {{Computational Complexity}} of {{Understanding Network Decisions}}},
  author = {Wäldchen, Stephan and Macdonald, Jan and Hauch, Sascha and Kutyniok, Gitta},
  year = {2019-06-18},
  eprint = {1905.09163},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1905.09163},
  _urldate = {2020-04-20},
  abstract = {For a Boolean function \$\textbackslash Phi\textbackslash colon\textbackslash\{0,1\textbackslash\}\^d\textbackslash to\textbackslash\{0,1\textbackslash\}\$ and an assignment to its variables \$\textbackslash mathbf\{x\}=(x\_1, x\_2, \textbackslash dots, x\_d)\$ we consider the problem of finding the subsets of the variables that are sufficient to determine the function value with a given probability \$\textbackslash delta\$. This is motivated by the task of interpreting predictions of binary classifiers described as Boolean circuits (which can be seen as special cases of neural networks). We show that the problem of deciding whether such subsets of relevant variables of limited size \$k\textbackslash leq d\$ exist is complete for the complexity class \$\textbackslash mathsf\{NP\}\^\{\textbackslash mathsf\{PP\}\}\$ and thus generally unfeasible to solve. We introduce a variant where it suffices to check whether a subset determines the function value with probability at least \$\textbackslash delta\$ or at most \$\textbackslash delta-\textbackslash gamma\$ for \$0{$<\backslash$}gamma{$<\backslash$}delta\$. This reduces the complexity to the class \$\textbackslash mathsf\{NP\}\^\{\textbackslash mathsf\{BPP\}\}\$. Finally, we show that finding the minimal set of relevant variables can not be reasonably approximated, i.e. with an approximation factor \$d\^\{1-\textbackslash alpha\}\$ for \$\textbackslash alpha {$>$} 0\$, by a polynomial time algorithm unless \$\textbackslash mathsf\{P\} = \textbackslash mathsf\{NP\}\$ (this holds even with the probability gap).},
  archiveprefix = {arXiv},
  keywords = {68Q25; 68Q17,Computer Science - Computational Complexity,F.2.0}
}

@inproceedings{wangFallingRuleLists2015,
  title = {Falling {{Rule Lists}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Wang, Fulton and Rudin, Cynthia},
  year = {2015-02-21},
  pages = {1013--1022},
  publisher = {{PMLR}},
  __url = {http://proceedings.mlr.press/v38/wang15a.html},
  _urldate = {2020-12-07},
  abstract = {Falling rule lists are classification models consisting of an ordered list of if-then rules, where (i) the order of rules determines which example should be classified by each rule, and (ii) the es...},
  eventtitle = {Artificial {{Intelligence}} and {{Statistics}}},
  langid = {english}
}

@unpublished{wangShapleyFlowGraphbased2020,
  title = {Shapley {{Flow}}: {{A Graph-based Approach}} to {{Interpreting Model Predictions}}},
  shorttitle = {Shapley {{Flow}}},
  author = {Wang, Jiaxuan and Wiens, Jenna and Lundberg, Scott},
  year = {2020-11-13},
  eprint = {2010.14592},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2010.14592},
  _urldate = {2020-11-30},
  abstract = {Many existing approaches for estimating feature importance are problematic because they ignore or hide dependencies among features. A causal graph, which encodes the relationships among input variables, can aid in assigning feature importance. However, current approaches that assign credit to nodes in the causal graph fail to explain the entire graph. In light of these limitations, we propose Shapley Flow, a novel approach to interpreting machine learning models. It considers the entire causal graph, and assigns credit to \textbackslash textit\{edges\} instead of treating nodes as the fundamental unit of credit assignment. Shapley Flow is the unique solution to a generalization of the Shapley value axioms to directed acyclic graphs. We demonstrate the benefit of using Shapley Flow to reason about the impact of a model's input on its output. In addition to maintaining insights from existing approaches, Shapley Flow extends the flat, set-based, view prevalent in game theory based explanation methods to a deeper, \textbackslash textit\{graph-based\}, view. This graph-based view enables users to understand the flow of importance through a system, and reason about potential interventions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{wardExplainableArtificialIntelligence2021,
  title = {Explainable {{Artificial Intelligence}} for {{Pharmacovigilance}}: {{What Features Are Important When Predicting Adverse Outcomes}}?},
  shorttitle = {Explainable {{Artificial Intelligence}} for {{Pharmacovigilance}}},
  author = {Ward, Isaac Ronald and Wang, Ling and {lu}, Juan and Bennamoun, Mohammed and Dwivedi, Girish and Sanfilippo, Frank M.},
  year = {2021-11},
  journaltitle = {Computer Methods and Programs in Biomedicine},
  shortjournal = {Computer Methods and Programs in Biomedicine},
  volume = {212},
  eprint = {2112.13210},
  eprinttype = {arxiv},
  pages = {106415},
  _issn = {01692607},
  _doi = {10.1016/j.cmpb.2021.106415},
  __url = {http://arxiv.org/abs/2112.13210},
  _urldate = {2022-01-17},
  abstract = {Explainable Artificial Intelligence (XAI) has been identified as a viable method for determining the importance of features when making predictions using Machine Learning (ML) models. In this study, we created models that take an individual's health information (e.g. their drug history and comorbidities) as inputs, and predict the probability that the individual will have an Acute Coronary Syndrome (ACS) adverse outcome. Using XAI, we quantified the contribution that specific drugs had on these ACS predictions, thus creating an XAI-based technique for pharmacovigilance monitoring, using ACS as an example of the adverse outcome to detect. Individuals aged over 65 who were supplied Musculo-skeletal system (anatomical therapeutic chemical (ATC) class M) or Cardiovascular system (ATC class C) drugs between 1993 and 2009 were identified, and their drug histories, comorbidities, and other key features were extracted from linked Western Australian datasets. Multiple ML models were trained to predict if these individuals would have an ACS related adverse outcome (i.e., death or hospitalisation with a discharge diagnosis of ACS), and a variety of ML and XAI techniques were used to calculate which features -- specifically which drugs -- led to these predictions. The drug dispensing features for rofecoxib and celecoxib were found to have a greater than zero contribution to ACS related adverse outcome predictions (on average), and it was found that ACS related adverse outcomes can be predicted with 72\% accuracy. Furthermore, the XAI libraries LIME and SHAP were found to successfully identify both important and unimportant features, with SHAP slightly outperforming LIME. ML models trained on linked administrative health datasets in tandem with XAI algorithms can successfully quantify feature importance, and with further development, could potentially be used as pharmacovigilance monitoring techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods},
  file = {/home/rick/Zotero/storage/77RGUWDJ/Ward et al. - 2021 - Explainable Artificial Intelligence for Pharmacovi.pdf;/home/rick/Zotero/storage/8LVNNY2R/2112.html}
}

@unpublished{weertsHumanGroundedEvaluationSHAP2019,
  title = {A {{Human-Grounded Evaluation}} of {{SHAP}} for {{Alert Processing}}},
  author = {Weerts, Hilde J. P. and van Ipenburg, Werner and Pechenizkiy, Mykola},
  options = {useprefix=true},
  year = {2019-07-07},
  eprint = {1907.03324},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1907.03324},
  _urldate = {2020-10-22},
  abstract = {In the past years, many new explanation methods have been proposed to achieve interpretability of machine learning predictions. However, the utility of these methods in practical applications has not been researched extensively. In this paper we present the results of a human-grounded evaluation of SHAP, an explanation method that has been well-received in the XAI and related communities. In particular, we study whether this local model-agnostic explanation method can be useful for real human domain experts to assess the correctness of positive predictions, i.e. alerts generated by a classifier. We performed experimentation with three different groups of participants (159 in total), who had basic knowledge of explainable machine learning. We performed a qualitative analysis of recorded reflections of experiment participants performing alert processing with and without SHAP information. The results suggest that the SHAP explanations do impact the decision-making process, although the model's confidence score remains to be a leading source of evidence. We statistically test whether there is a significant difference in task utility metrics between tasks for which an explanation was available and tasks in which it was not provided. As opposed to common intuitions, we did not find a significant difference in alert processing performance when a SHAP explanation is available compared to when it is not.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{wen2014robust,
  title = {Robust Learning under Uncertain Test Distributions: {{Relating}} Covariate Shift to Model Misspecification},
  booktitle = {International Conference on Machine Learning},
  author = {Wen, Junfeng and Yu, Chun-Nam and Greiner, Russell},
  year = {2014},
  pages = {631--639},
  organization = {{PMLR}}
}

@unpublished{wickstromRELAXRepresentationLearning2021,
  title = {{{RELAX}}: {{Representation Learning Explainability}}},
  shorttitle = {{{RELAX}}},
  author = {Wickstrøm, Kristoffer K. and Trosten, Daniel J. and Løkse, Sigurd and Mikalsen, Karl Øyvind and Kampffmeyer, Michael C. and Jenssen, Robert},
  year = {2021-12-19},
  eprint = {2112.10161},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2112.10161},
  _urldate = {2022-01-17},
  abstract = {Despite the significant improvements that representation learning via self-supervision has led to when learning from unlabeled data, no methods exist that explain what influences the learned representation. We address this need through our proposed approach, RELAX, which is the first approach for attribution-based explanations of representations. Our approach can also model the uncertainty in its explanations, which is essential to produce trustworthy explanations. RELAX explains representations by measuring similarities in the representation space between an input and masked out versions of itself, providing intuitive explanations and significantly outperforming the gradient-based baseline. We provide theoretical interpretations of RELAX and conduct a novel analysis of feature extractors trained using supervised and unsupervised learning, providing insights into different learning strategies. Finally, we illustrate the usability of RELAX in multi-view clustering and highlight that incorporating uncertainty can be essential for providing low-complexity explanations, taking a crucial step towards explaining representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/5TCD8EAD/Wickstrøm et al. - 2021 - RELAX Representation Learning Explainability.pdf;/home/rick/Zotero/storage/4GD9LGME/2112.html}
}

@article{wilmingScrutinizingXAIUsing2022,
  title={Scrutinizing XAI using linear ground-truth data with suppressor variables},
  author={Wilming, Rick and Budding, C{\'e}line and M{\"u}ller, Klaus-Robert and Haufe, Stefan},
  journal={Machine learning},
  volume={111},
  number={5},
  pages={1903--1923},
  year={2022},
  publisher={Springer}
}

@unpublished{wolpertWhatImportantNo2020,
  title = {What Is Important about the {{No Free Lunch}} Theorems?},
  author = {Wolpert, David H.},
  year = {2020-07-21},
  eprint = {2007.10928},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2007.10928},
  _urldate = {2020-08-05},
  abstract = {The No Free Lunch theorems prove that under a uniform distribution over induction problems (search problems or learning problems), all induction algorithms perform equally. As I discuss in this chapter, the importance of the theorems arises by using them to analyze scenarios involving \{non-uniform\} distributions, and to compare different algorithms, without any assumption about the distribution over problems at all. In particular, the theorems prove that \{anti\}-cross-validation (choosing among a set of candidate algorithms based on which has \{worst\} out-of-sample behavior) performs as well as cross-validation, unless one makes an assumption -- which has never been formalized -- about how the distribution over induction problems, on the one hand, is related to the set of algorithms one is choosing among using (anti-)cross validation, on the other. In addition, they establish strong caveats concerning the significance of the many results in the literature which establish the strength of a particular algorithm without assuming a particular distribution. They also motivate a ``dictionary'' between supervised learning and improve blackbox optimization, which allows one to ``translate'' techniques from supervised learning into the domain of blackbox optimization, thereby strengthening blackbox optimization algorithms. In addition to these topics, I also briefly discuss their implications for philosophy of science.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
}

@unpublished{xenopoulosTopologicalRepresentationsLocal2022,
  title = {Topological {{Representations}} of {{Local Explanations}}},
  author = {Xenopoulos, Peter and Chan, Gromit and Doraiswamy, Harish and Nonato, Luis Gustavo and Barr, Brian and Silva, Claudio},
  year = {2022-01-06},
  eprint = {2201.02155},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2201.02155},
  _urldate = {2022-01-17},
  abstract = {Local explainability methods -- those which seek to generate an explanation for each prediction -- are becoming increasingly prevalent due to the need for practitioners to rationalize their model outputs. However, comparing local explainability methods is difficult since they each generate outputs in various scales and dimensions. Furthermore, due to the stochastic nature of some explainability methods, it is possible for different runs of a method to produce contradictory explanations for a given observation. In this paper, we propose a topology-based framework to extract a simplified representation from a set of local explanations. We do so by first modeling the relationship between the explanation space and the model predictions as a scalar function. Then, we compute the topological skeleton of this function. This topological skeleton acts as a signature for such functions, which we use to compare different explanation methods. We demonstrate that our framework can not only reliably identify differences between explainability techniques but also provides stable representations. Then, we show how our framework can be used to identify appropriate parameters for local explainability methods. Our framework is simple, does not require complex optimizations, and can be broadly applied to most local explanation methods. We believe the practicality and versatility of our approach will help promote topology-based approaches as a tool for understanding and comparing explanation methods.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/rick/Zotero/storage/PQEA49W3/Xenopoulos et al. - 2022 - Topological Representations of Local Explanations.pdf;/home/rick/Zotero/storage/L3CH2CCI/2201.html}
}

@inproceedings{xiongRegularizingDeepConvolutional2016,
  title = {Regularizing {{Deep Convolutional Neural Networks}} with a {{Structured Decorrelation Constraint}}},
  booktitle = {2016 {{IEEE}} 16th {{International Conference}} on {{Data Mining}} ({{ICDM}})},
  author = {Xiong, Wei and Du, Bo and Zhang, Lefei and Hu, Ruimin and Tao, Dacheng},
  year = {2016-12},
  pages = {519--528},
  _issn = {2374-8486},
  _doi = {10.1109/ICDM.2016.0063},
  abstract = {Deep convolutional networks have achieved successful performance in data mining field. However, training large networks still remains a challenge, as the training data may be insufficient and the model can easily get overfitted. Hence the training process is usually combined with a model regularization. Typical regularizers include weight decay, Dropout, etc. In this paper, we propose a novel regularizer, named Structured Decorrelation Constraint (SDC), which is applied to the activations of the hidden layers to prevent overfitting and achieve better generalization. SDC impels the network to learn structured representations by grouping the hidden units and encouraging the units within the same group to have strong connections during the training procedure. Meanwhile, it forces the units in different groups to learn non-redundant representations by minimizing the cross-covariance between them. Compared with Dropout, SDC reduces the co-adaptions between the hidden units in an explicit way. Besides, we propose a novel approach called Reg-Conv that can help SDC to regularize the complex convolutional layers. Experiments on extensive datasets show that SDC significantly reduces overfitting and yields very meaningful improvements on classification performance (on CIFAR-10 6.22\% accuracy promotion and on CIFAR-100 9.63\% promotion).},
  eventtitle = {2016 {{IEEE}} 16th {{International Conference}} on {{Data Mining}} ({{ICDM}})},
  keywords = {Biological neural networks,Convolutional Networks,Correlation,Data models,Decorrelation,Electronic mail,Neurons,Overfitting,Training}
}

@unpublished{yampolskiyUnexplainabilityIncomprehensibilityArtificial2019,
  title = {Unexplainability and {{Incomprehensibility}} of {{Artificial Intelligence}}},
  author = {Yampolskiy, Roman V.},
  year = {2019-06-20},
  eprint = {1907.03869},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/1907.03869},
  _urldate = {2020-10-22},
  abstract = {Explainability and comprehensibility of AI are important requirements for intelligent systems deployed in real-world domains. Users want and frequently need to understand how decisions impacting them are made. Similarly it is important to understand how an intelligent system functions for safety and security reasons. In this paper, we describe two complementary impossibility results (Unexplainability and Incomprehensibility), essentially showing that advanced AIs would not be able to accurately explain some of their decisions and for the decisions they could explain people would not understand some of those explanations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society}
}

@unpublished{yangBenchmarkingAttributionMethods2019,
  title = {Benchmarking {{Attribution Methods}} with {{Relative Feature Importance}}},
  author = {Yang, Mengjiao and Kim, Been},
  year = {2019-11-04},
  eprint = {1907.09701},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1907.09701},
  _urldate = {2021-01-08},
  abstract = {Interpretability is an important area of research for safe deployment of machine learning systems. One particular type of interpretability method attributes model decisions to input features. Despite active development, quantitative evaluation of feature attribution methods remains difficult due to the lack of ground truth: we do not know which input features are in fact important to a model. In this work, we propose a framework for Benchmarking Attribution Methods (BAM) with a priori knowledge of relative feature importance. BAM includes 1) a carefully crafted dataset and models trained with known relative feature importance and 2) three complementary metrics to quantitatively evaluate attribution methods by comparing feature attributions between pairs of models and pairs of inputs. Our evaluation on several widely-used attribution methods suggests that certain methods are more likely to produce false positive explanations---features that are incorrectly attributed as more important to model prediction. We open source our dataset, models, and metrics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{yangBenchmarkingAttributionMethods2019a,
  title = {Benchmarking {{Attribution Methods}} with {{Relative Feature Importance}}},
  author = {Yang, Mengjiao and Kim, Been},
  year = {2019-11},
  eprint = {1907.09701},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1907.09701},
  _urldate = {2021-01-08},
  abstract = {Interpretability is an important area of research for safe deployment of machine learning systems. One particular type of interpretability method attributes model decisions to input features. Despite active development, quantitative evaluation of feature attribution methods remains difficult due to the lack of ground truth: we do not know which input features are in fact important to a model. In this work, we propose a framework for Benchmarking Attribution Methods (BAM) with a priori knowledge of relative feature importance. BAM includes 1) a carefully crafted dataset and models trained with known relative feature importance and 2) three complementary metrics to quantitatively evaluate attribution methods by comparing feature attributions between pairs of models and pairs of inputs. Our evaluation on several widely-used attribution methods suggests that certain methods are more likely to produce false positive explanations—features that are incorrectly attributed as more important to model prediction. We open source our dataset, models, and metrics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{yangEvaluatingExplanationGround2019,
  title = {Evaluating {{Explanation Without Ground Truth}} in {{Interpretable Machine Learning}}},
  author = {Yang, Fan and Du, Mengnan and Hu, Xia},
  year = {2019-08-15},
  eprint = {1907.06831},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1907.06831},
  _urldate = {2021-09-30},
  abstract = {Interpretable Machine Learning (IML) has become increasingly important in many real-world applications, such as autonomous cars and medical diagnosis, where explanations are significantly preferred to help people better understand how machine learning systems work and further enhance their trust towards systems. However, due to the diversified scenarios and subjective nature of explanations, we rarely have the ground truth for benchmark evaluation in IML on the quality of generated explanations. Having a sense of explanation quality not only matters for assessing system boundaries, but also helps to realize the true benefits to human users in practical settings. To benchmark the evaluation in IML, in this article, we rigorously define the problem of evaluating explanations, and systematically review the existing efforts from state-of-the-arts. Specifically, we summarize three general aspects of explanation (i.e., generalizability, fidelity and persuasibility) with formal definitions, and respectively review the representative methodologies for each of them under different tasks. Further, a unified evaluation framework is designed according to the hierarchical needs from developers and end-users, which could be easily adopted for different scenarios in practice. In the end, open problems are discussed, and several limitations of current evaluation techniques are raised for future explorations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{yangXDeepInterpretationTool2019,
  title = {{{XDeep}}: {{An Interpretation Tool}} for {{Deep Neural Networks}}},
  shorttitle = {{{XDeep}}},
  author = {Yang, Fan and Zhang, Zijian and Wang, Haofan and Li, Yuening and Hu, Xia},
  year = {2019-11-03},
  eprint = {1911.01005},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1911.01005},
  _urldate = {2020-04-20},
  abstract = {XDeep is an open-source Python package developed to interpret deep models for both practitioners and researchers. Overall, XDeep takes a trained deep neural network (DNN) as the input, and generates relevant interpretations as the output with the post-hoc manner. From the functionality perspective, XDeep integrates a wide range of interpretation algorithms from the state-of-the-arts, covering different types of methodologies, and is capable of providing both local explanation and global explanation for DNN when interpreting model behaviours. With the well-documented API designed in XDeep, end-users can easily obtain the interpretations for their deep models at hand with several lines of codes, and compare the results among different algorithms. XDeep is generally compatible with Python 3, and can be installed through Python Package Index (PyPI). The source codes are available at: https://github.com/datamllab/xdeep.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@unpublished{yehBringingRulerBlack2021,
  title = {Bringing a {{Ruler Into}} the {{Black Box}}: {{Uncovering Feature Impact}} from {{Individual Conditional Expectation Plots}}},
  shorttitle = {Bringing a {{Ruler Into}} the {{Black Box}}},
  author = {Yeh, Andrew and Ngo, Anhthy},
  year = {2021-09-06},
  eprint = {2109.02724},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2109.02724},
  _urldate = {2021-09-17},
  abstract = {As machine learning systems become more ubiquitous, methods for understanding and interpreting these models become increasingly important. In particular, practitioners are often interested both in what features the model relies on and how the model relies on them--the feature's impact on model predictions. Prior work on feature impact including partial dependence plots (PDPs) and Individual Conditional Expectation (ICE) plots has focused on a visual interpretation of feature impact. We propose a natural extension to ICE plots with ICE feature impact, a model-agnostic, performance-agnostic feature impact metric drawn out from ICE plots that can be interpreted as a close analogy to linear regression coefficients. Additionally, we introduce an in-distribution variant of ICE feature impact to vary the influence of out-of-distribution points as well as heterogeneity and non-linearity measures to characterize feature impact. Lastly, we demonstrate ICE feature impact's utility in several tasks using real-world data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@incollection{yehFidelitySensitivityExplanations2019,
  title = {On the ({{In}})Fidelity and {{Sensitivity}} of {{Explanations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Yeh, Chih-Kuan and Hsieh, Cheng-Yu and Suggala, Arun and Inouye, David I and Ravikumar, Pradeep K},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d\textbackslash textquotesingle Alché-Buc, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {10967--10978},
  publisher = {{Curran Associates, Inc.}},
  __url = {http://papers.nips.cc/paper/9278-on-the-infidelity-and-sensitivity-of-explanations.pdf},
  _urldate = {2020-10-19}
}

@inproceedings{zeilerVisualizingUnderstandingConvolutional2014,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {818--833},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  _doi = {10.1007/978-3-319-10590-1_53},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  _isbn = {978-3-319-10590-1},
  langid = {english},
  keywords = {Convolutional Neural Network,Input Image,Pixel Space,Stochastic Gradient Descent,Training Image}
}

@inproceedings{zeilerVisualizingUnderstandingConvolutional2014a,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {818--833},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  _doi = {10.1007/978-3-319-10590-1_53},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  _isbn = {978-3-319-10590-1},
  langid = {english},
  keywords = {Convolutional Neural Network,Input Image,Pixel Space,Stochastic Gradient Descent,Training Image}
}

@unpublished{zeltnerSquashingActivationFunctions2020,
  title = {Squashing Activation Functions in Benchmark Tests: Towards {{eXplainable Artificial Intelligence}} Using Continuous-Valued Logic},
  shorttitle = {Squashing Activation Functions in Benchmark Tests},
  author = {Zeltner, Daniel and Schmid, Benedikt and Csiszar, Gabor and Csiszar, Orsolya},
  year = {2020-10-17},
  eprint = {2010.08760},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2010.08760},
  _urldate = {2020-11-30},
  abstract = {Over the past few years, deep neural networks have shown excellent results in multiple tasks, however, there is still an increasing need to address the problem of interpretability to improve model transparency, performance, and safety. Achieving eXplainable Artificial Intelligence (XAI) by combining neural networks with continuous logic and multi-criteria decision-making tools is one of the most promising ways to approach this problem: by this combination, the black-box nature of neural models can be reduced. The continuous logic-based neural model uses so-called Squashing activation functions, a parametric family of functions that satisfy natural invariance requirements and contain rectified linear units as a particular case. This work demonstrates the first benchmark tests that measure the performance of Squashing functions in neural networks. Three experiments were carried out to examine their usability and a comparison with the most popular activation functions was made for five different network types. The performance was determined by measuring the accuracy, loss, and time per epoch. These experiments and the conducted benchmarks have proven that the use of Squashing functions is possible and similar in performance to conventional activation functions. Moreover, a further experiment was conducted by implementing nilpotent logical gates to demonstrate how simple classification tasks can be solved successfully and with high performance. The results indicate that due to the embedded nilpotent logical operators and the differentiability of the Squashing function, it is possible to solve classification problems, where other commonly used activation functions fail.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@unpublished{zhangFineGrainedNeuralNetwork2021,
  title = {Fine-{{Grained Neural Network Explanation}} by {{Identifying Input Features}} with {{Predictive Information}}},
  author = {Zhang, Yang and Khakzar, Ashkan and Li, Yawei and Farshad, Azade and Kim, Seong Tae and Navab, Nassir},
  year = {2021-10-04},
  eprint = {2110.01471},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2110.01471},
  _urldate = {2021-10-06},
  abstract = {One principal approach for illuminating a black-box neural network is feature attribution, i.e. identifying the importance of input features for the network's prediction. The predictive information of features is recently proposed as a proxy for the measure of their importance. So far, the predictive information is only identified for latent features by placing an information bottleneck within the network. We propose a method to identify features with predictive information in the input domain. The method results in fine-grained identification of input features' information and is agnostic to network architecture. The core idea of our method is leveraging a bottleneck on the input that only lets input features associated with predictive latent features pass through. We compare our method with several feature attribution methods using mainstream feature attribution evaluation experiments. The code is publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/rick/Zotero/storage/YTE73BI8/Zhang et al. - 2021 - Fine-Grained Neural Network Explanation by Identif.pdf;/home/rick/Zotero/storage/FDR5MWWS/2110.html}
}

@unpublished{zhangInterpretingBoostingDropout2020,
  title = {Interpreting and {{Boosting Dropout}} from a {{Game-Theoretic View}}},
  author = {Zhang, Hao and Li, Sen and Ma, Yinchao and Li, Mingjie and Xie, Yichen and Zhang, Quanshi},
  year = {2020-10-10},
  eprint = {2009.11729},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/2009.11729},
  _urldate = {2020-11-30},
  abstract = {This paper aims to understand and improve the utility of the dropout operation from the perspective of game-theoretic interactions. We prove that dropout can suppress the strength of interactions between input variables of deep neural networks (DNNs). The theoretic proof is also verified by various experiments. Furthermore, we find that such interactions were strongly related to the over-fitting problem in deep learning. Thus, the utility of dropout can be regarded as decreasing interactions to alleviate the significance of over-fitting. Based on this understanding, we propose an interaction loss to further improve the utility of dropout. Experimental results have shown that the interaction loss can effectively improve the utility of dropout and boost the performance of DNNs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{zhangMachineLearningTesting2020,
  title = {Machine {{Learning Testing}}: {{Survey}}, {{Landscapes}} and {{Horizons}}},
  shorttitle = {Machine {{Learning Testing}}},
  author = {Zhang, Jie M. and Harman, Mark and Ma, Lei and Liu, Yang},
  year = {2020},
  journaltitle = {IEEE Transactions on Software Engineering},
  pages = {1--1},
  _issn = {1939-3520},
  _doi = {10.1109/TSE.2019.2962027},
  abstract = {This paper provides a comprehensive survey of Machine Learning Testing (ML testing) research. It covers 138 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in machine learning testing.},
  keywords = {Data models,deep neural network,machine learning,Machine learning,Robustness,Software engineering,software testing,Software testing,Training data}
}

@unpublished{zhangWhyShouldYou2019,
  title = {"{{Why Should You Trust My Explanation}}?" {{Understanding Uncertainty}} in {{LIME Explanations}}},
  shorttitle = {"{{Why Should You Trust My Explanation}}?},
  author = {Zhang, Yujia and Song, Kuangyan and Sun, Yiming and Tan, Sarah and Udell, Madeleine},
  year = {2019-06-04},
  eprint = {1904.12991},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1904.12991},
  _urldate = {2020-10-13},
  abstract = {Methods for interpreting machine learning black-box models increase the outcomes' transparency and in turn generates insight into the reliability and fairness of the algorithms. However, the interpretations themselves could contain significant uncertainty that undermines the trust in the outcomes and raises concern about the model's reliability. Focusing on the method "Local Interpretable Model-agnostic Explanations" (LIME), we demonstrate the presence of two sources of uncertainty, namely the randomness in its sampling procedure and the variation of interpretation quality across different input data points. Such uncertainty is present even in models with high training and test accuracy. We apply LIME to synthetic data and two public data sets, text classification in 20 Newsgroup and recidivism risk-scoring in COMPAS, to support our argument.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,drawback,LIME,Statistics - Machine Learning}
}

@unpublished{zhangWhyShouldYou2019a,
  title = {"{{Why Should You Trust My Explanation}}?" {{Understanding Uncertainty}} in {{LIME Explanations}}},
  shorttitle = {"{{Why Should You Trust My Explanation}}?},
  author = {Zhang, Yujia and Song, Kuangyan and Sun, Yiming and Tan, Sarah and Udell, Madeleine},
  year = {2019-06-04},
  eprint = {1904.12991},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  __url = {http://arxiv.org/abs/1904.12991},
  _urldate = {2021-05-12},
  abstract = {Methods for interpreting machine learning black-box models increase the outcomes' transparency and in turn generates insight into the reliability and fairness of the algorithms. However, the interpretations themselves could contain significant uncertainty that undermines the trust in the outcomes and raises concern about the model's reliability. Focusing on the method "Local Interpretable Model-agnostic Explanations" (LIME), we demonstrate the presence of two sources of uncertainty, namely the randomness in its sampling procedure and the variation of interpretation quality across different input data points. Such uncertainty is present even in models with high training and test accuracy. We apply LIME to synthetic data and two public data sets, text classification in 20 Newsgroup and recidivism risk-scoring in COMPAS, to support our argument.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/rick/Zotero/storage/VQ55KP6W/Zhang et al. - 2019 - Why Should You Trust My Explanation Understandi.pdf;/home/rick/Zotero/storage/NAA94CF4/1904.html}
}

@article{zhaoCausalInterpretationsBlackBox2019,
  title = {Causal {{Interpretations}} of {{Black-Box Models}}},
  author = {Zhao, Qingyuan and Hastie, Trevor},
  year = {2019-06-03},
  journaltitle = {Journal of Business \& Economic Statistics},
  volume = {0},
  number = {0},
  pages = {1--10},
  _issn = {0735-0015},
  _doi = {10.1080/07350015.2019.1624293},
  __url = {https://doi.org/10.1080/07350015.2019.1624293},
  _urldate = {2020-06-04},
  abstract = {The fields of machine learning and causal inference have developed many concepts, tools, and theory that are potentially useful for each other. Through exploring the possibility of extracting causal interpretations from black-box machine-trained models, we briefly review the languages and concepts in causal inference that may be interesting to machine learning researchers. We start with the curious observation that Friedman’s partial dependence plot has exactly the same formula as Pearl’s back-door adjustment and discuss three requirements to make causal interpretations: a model with good predictive performance, some domain knowledge in the form of a causal diagram and suitable visualization tools. We provide several illustrative examples and find some interesting and potentially causal relations using visualization tools for black-box models.},
  keywords = {Back-door adjustment,Data visualization,Machine learning,Mediation analysis,Partial dependence plot}
}

@unpublished{zhouFeatureAttributionMethods2021,
  title = {Do {{Feature Attribution Methods Correctly Attribute Features}}?},
  author = {Zhou, Yilun and Booth, Serena and Ribeiro, Marco Tulio and Shah, Julie},
  year = {2021-12-15},
  eprint = {2104.14403},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2104.14403},
  _urldate = {2022-01-21},
  abstract = {Feature attribution methods are popular in interpretable machine learning. These methods compute the attribution of each input feature to represent its importance, but there is no consensus on the definition of "attribution", leading to many competing methods with little systematic evaluation, complicated in particular by the lack of ground truth attribution. To address this, we propose a dataset modification procedure to induce such ground truth. Using this procedure, we evaluate three common methods: saliency maps, rationales, and attentions. We identify several deficiencies and add new perspectives to the growing body of evidence questioning the correctness and reliability of these methods applied on datasets in the wild. We further discuss possible avenues for remedy and recommend new attribution methods to be tested against ground truth before deployment. The code is available at https://github.com/YilunZhou/feature-attribution-evaluation},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/rick/Zotero/storage/CK2Q34HT/Zhou et al. - 2021 - Do Feature Attribution Methods Correctly Attribute.pdf;/home/rick/Zotero/storage/P8KJPCDY/2104.html}
}

@unpublished{zhouInterpretableNaturalLanguage2020,
  title = {Towards {{Interpretable Natural Language Understanding}} with {{Explanations}} as {{Latent Variables}}},
  author = {Zhou, Wangchunshu and Hu, Jinyi and Zhang, Hanlin and Liang, Xiaodan and Sun, Maosong and Xiong, Chenyan and Tang, Jian},
  year = {2020-10-23},
  eprint = {2011.05268},
  eprinttype = {arxiv},
  primaryclass = {cs},
  __url = {http://arxiv.org/abs/2011.05268},
  _urldate = {2020-11-30},
  abstract = {Recently generating natural language explanations has shown very promising results in not only offering interpretable explanations but also providing additional information and supervision for prediction. However, existing approaches usually require a large set of human annotated explanations for training while collecting a large set of explanations is not only time consuming but also expensive. In this paper, we develop a general framework for interpretable natural language understanding that requires only a small set of human annotated explanations for training. Our framework treats natural language explanations as latent variables that model the underlying reasoning process of a neural model. We develop a variational EM framework for optimization where an explanation generation module and an explanation-augmented prediction module are alternatively optimized and mutually enhance each other. Moreover, we further propose an explanation-based self-training method under this framework for semi-supervised learning. It alternates between assigning pseudo-labels to unlabeled data and generating new explanations to iteratively improve each other. Experiments on two natural language understanding tasks demonstrate that our framework can not only make effective predictions in both supervised and semi-supervised settings, but also generate good natural language explanation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{zienFeatureImportanceRanking2009,
  title = {The {{Feature Importance Ranking Measure}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Zien, Alexander and Krämer, Nicole and Sonnenburg, Sören and Rätsch, Gunnar},
  editor = {Buntine, Wray and Grobelnik, Marko and Mladenić, Dunja and Shawe-Taylor, John},
  year = {2009},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {694--709},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  _doi = {10.1007/978-3-642-04174-7_45},
  abstract = {Most accurate predictions are typically obtained by learning machines with complex feature spaces (as e.g. induced by kernels). Unfortunately, such decision rules are hardly accessible to humans and cannot easily be used to gain insights about the application domain. Therefore, one often resorts to linear models in combination with variable selection, thereby sacrificing some predictive power for presumptive interpretability. Here, we introduce the Feature Importance Ranking Measure (FIRM), which by retrospective analysis of arbitrary learning machines allows to achieve both excellent predictive performance and superior interpretation. In contrast to standard raw feature weighting, FIRM takes the underlying correlation structure of the features into account. Thereby, it is able to discover the most relevant features, even if their appearance in the training data is entirely prevented by noise. The desirable properties of FIRM are investigated analytically and illustrated in simulations.},
  _isbn = {978-3-642-04174-7},
  langid = {english},
  keywords = {Binary Feature,Feature Weighting,Importance Measure,Multiple Kernel Learn,Random Forest}
}

@article{zouRegularizationVariableSelection2005,
  title = {Regularization and Variable Selection via the Elastic Net},
  author = {Zou, Hui and Hastie, Trevor},
  year = {2005},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {67},
  number = {2},
  pages = {301--320},
  _issn = {1467-9868},
  _doi = {10.1111/j.1467-9868.2005.00503.x},
  __url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00503.x},
  _urldate = {2020-05-08},
  abstract = {Summary. We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
  langid = {english},
  keywords = {Grouping effect,LARS algorithm,Lasso,p≫n problem,Penalization,Variable selection}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

