@article{maini2024tofu,
  title={Tofu: A task of fictitious unlearning for llms},
  author={Maini, Pratyush and Feng, Zhili and Schwarzschild, Avi and Lipton, Zachary C and Kolter, J Zico},
  journal={arXiv preprint arXiv:2401.06121},
  year={2024}
}

@article{thaker2024guardrail,
  title={Guardrail Baselines for Unlearning in LLMs},
  author={Thaker, Pratiksha and Maurya, Yash and Smith, Virginia},
  journal={arXiv preprint arXiv:2403.03329
},
  year={2024}
}

@article{pawelczyk2023context,
  title={In-context unlearning: Language models as few shot unlearners},
  author={Pawelczyk, Martin and Neel, Seth and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2310.07579},
  year={2023}
}

@inproceedings{li2024latesteval,
  title={Latesteval: Addressing data contamination in language model evaluation through dynamic and time-sensitive test construction},
  author={Li, Yucheng and Guerin, Frank and Lin, Chenghua},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={17},
  pages={18600--18607},
  year={2024}
}

@article{eldan2023s,
  title={Who's Harry Potter? Approximate Unlearning in LLMs},
  author={Eldan, Ronen and Russinovich, Mark},
  journal={arXiv preprint arXiv:2310.02238},
  year={2023}
}

@article{fort2023scaling,
  title={Scaling Laws for Adversarial Attacks on Language Model Activations},
  author={Fort, Stanislav},
  journal={arXiv preprint arXiv:2312.02780},
  year={2023}
}

@article{yao2023large,
  title={Large Language Model Unlearning},
  author={Yao, Yuanshun and Xu, Xiaojun and Liu, Yang},
  journal={arXiv preprint arXiv:2310.10683},
  year={2023}
}

@article{shi2023detecting,
  title={Detecting pretraining data from large language models},
  author={Shi, Weijia and Ajith, Anirudh and Xia, Mengzhou and Huang, Yangsibo and Liu, Daogao and Blevins, Terra and Chen, Danqi and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2310.16789},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{wu2023depn,
  title={Depn: Detecting and editing privacy neurons in pretrained language models},
  author={Wu, Xinwei and Li, Junzhuo and Xu, Minghui and Dong, Weilong and Wu, Shuangzhi and Bian, Chao and Xiong, Deyi},
  journal={arXiv preprint arXiv:2310.20138},
  year={2023}
}

@article{wang2023kga,
  title={KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment},
  author={Wang, Lingzhi and Chen, Tong and Yuan, Wei and Zeng, Xingshan and Wong, Kam-Fai and Yin, Hongzhi},
  journal={arXiv preprint arXiv:2305.06535},
  year={2023}
}

@article{chen2023unlearn,
  title={Unlearn what you want to forget: Efficient unlearning for llms},
  author={Chen, Jiaao and Yang, Diyi},
  journal={arXiv preprint arXiv:2310.20150},
  year={2023}
}

@article{zhou2023making,
  title={Making Harmful Behaviors Unlearnable for Large Language Models},
  author={Zhou, Xin and Lu, Yi and Ma, Ruotian and Gui, Tao and Zhang, Qi and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2311.02105},
  year={2023}
}


@article{zhang2023composing,
  title={Composing parameter-efficient modules with arithmetic operations},
  author={Zhang, Jinghan and Chen, Shiqi and Liu, Junteng and He, Junxian},
  journal={arXiv preprint arXiv:2306.14870},
  year={2023}
}

@article{belrose2023leace,
  title={LEACE: Perfect linear concept erasure in closed form},
  author={Belrose, Nora and Schneider-Joseph, David and Ravfogel, Shauli and Cotterell, Ryan and Raff, Edward and Biderman, Stella},
  journal={arXiv preprint arXiv:2306.03819},
  year={2023}
}

@article{ilharco2022editing,
  title={Editing models with task arithmetic},
  author={Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  journal={arXiv preprint arXiv:2212.04089},
  year={2022}
}

@article{adolphs2022cringe,
  title={The cringe loss: Learning what language not to model},
  author={Adolphs, Leonard and Gao, Tianyu and Xu, Jing and Shuster, Kurt and Sukhbaatar, Sainbayar and Weston, Jason},
  journal={arXiv preprint arXiv:2211.05826},
  year={2022}
}

@article{gu2024model,
  title={Model Editing Can Hurt General Abilities of Large Language Models},
  author={Gu, Jia-Chen and Xu, Hao-Xiang and Ma, Jun-Yu and Lu, Pan and Ling, Zhen-Hua and Chang, Kai-Wei and Peng, Nanyun},
  journal={arXiv preprint arXiv:2401.04700},
  year={2024}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@article{turner2023activation,
  title={Activation addition: Steering language models without optimization},
  author={Turner, Alex and Thiergart, Lisa and Udell, David and Leech, Gavin and Mini, Ulisse and MacDiarmid, Monte},
  journal={arXiv preprint arXiv:2308.10248},
  year={2023}
}

@article{wang2023backdoor,
  title={Backdoor activation attack: Attack large language models using activation steering for safety-alignment},
  author={Wang, Haoran and Shu, Kai},
  journal={arXiv preprint arXiv:2311.09433},
  year={2023}
}

@article{rimsky2023steering,
  title={Steering Llama 2 via Contrastive Activation Addition},
  author={Rimsky, Nina and Gabrieli, Nick and Schulz, Julian and Tong, Meg and Hubinger, Evan and Turner, Alexander Matt},
  journal={arXiv preprint arXiv:2312.06681},
  year={2023}
}

@article{li2024open,
  title={Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering},
  author={Li, Tianlong and Zheng, Xiaoqing and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2401.06824},
  year={2024}
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={arXiv preprint arXiv:2311.05232},
  year={2023}
}

@inproceedings{
niu2024what,
title={What does the Knowledge Neuron Thesis Have to do with Knowledge?},
author={Jingcheng Niu and Andrew Liu and Zining Zhu and Gerald Penn},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=2HJRwwbV3G}
}

@article{hase2024does,
  title={Does localization inform editing? surprising differences in causality-based localization vs. knowledge editing in language models},
  author={Hase, Peter and Bansal, Mohit and Kim, Been and Ghandeharioun, Asma},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{gehman2020realtoxicityprompts,
  title={Realtoxicityprompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  journal={arXiv preprint arXiv:2009.11462},
  year={2020}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{jang2022knowledge,
  title={Knowledge unlearning for mitigating privacy risks in language models},
  author={Jang, Joel and Yoon, Dongkeun and Yang, Sohee and Cha, Sungmin and Lee, Moontae and Logeswaran, Lajanugen and Seo, Minjoon},
  journal={arXiv preprint arXiv:2210.01504},
  year={2022}
}

@article{dong2024unmemorization,
  title={Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination},
  author={Dong, Yijiang River and Lin, Hongzhou and Belkin, Mikhail and Huerta, Ramon and Vuli{\'c}, Ivan},
  journal={arXiv preprint arXiv:2402.10052},
  year={2024}
}


@article{geiping2024coercing,
  title={Coercing LLMs to do and reveal (almost) anything},
  author={Geiping, Jonas and Stein, Alex and Shu, Manli and Saifullah, Khalid and Wen, Yuxin and Goldstein, Tom},
  journal={arXiv preprint arXiv:2402.14020},
  year={2024}
}

@article{dong2024building,
  title={Building Guardrails for Large Language Models},
  author={Dong, Yi and Mu, Ronghui and Jin, Gaojie and Qi, Yi and Hu, Jinwei and Zhao, Xingyu and Meng, Jie and Ruan, Wenjie and Huang, Xiaowei},
  journal={arXiv preprint arXiv:2402.01822},
  year={2024}
}

@article{rebedea2023nemo,
  title={Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails},
  author={Rebedea, Traian and Dinu, Razvan and Sreedhar, Makesh and Parisien, Christopher and Cohen, Jonathan},
  journal={arXiv preprint arXiv:2310.10501},
  year={2023}
}

@article{wang2023adding,
  title={Adding guardrails to advanced chatbots},
  author={Wang, Yanchen and Singh, Lisa},
  journal={arXiv preprint arXiv:2306.07500},
  year={2023}
}

@article{li2023textbooks,
  title={Textbooks are all you need ii: phi-1.5 technical report},
  author={Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
  journal={arXiv preprint arXiv:2309.05463},
  year={2023}
}

@article{nguyen2022survey,
  title={A survey of machine unlearning},
  author={Nguyen, Thanh Tam and Huynh, Thanh Trung and Nguyen, Phi Le and Liew, Alan Wee-Chung and Yin, Hongzhi and Nguyen, Quoc Viet Hung},
  journal={arXiv preprint arXiv:2209.02299},
  year={2022}
}

@inproceedings{bourtoule2021machine,
  title={Machine unlearning},
  author={Bourtoule, Lucas and Chandrasekaran, Varun and Choquette-Choo, Christopher A and Jia, Hengrui and Travers, Adelin and Zhang, Baiwu and Lie, David and Papernot, Nicolas},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  pages={141--159},
  year={2021},
  organization={IEEE}
}

@article{shaik2023exploring,
  title={Exploring the landscape of machine unlearning: A survey and taxonomy},
  author={Shaik, Thanveer and Tao, Xiaohui and Xie, Haoran and Li, Lin and Zhu, Xiaofeng and Li, Qing},
  journal={arXiv preprint arXiv:2305.06360},
  year={2023}
}

@article{dai2021knowledge,
  title={Knowledge neurons in pretrained transformers},
  author={Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  journal={arXiv preprint arXiv:2104.08696},
  year={2021}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@article{mitchell2021fast,
  title={Fast model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D},
  journal={arXiv preprint arXiv:2110.11309},
  year={2021}
}

@inproceedings{mitchell2022memory,
  title={Memory-based model editing at scale},
  author={Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={15817--15831},
  year={2022},
  organization={PMLR}
}

@article{zhang2024comprehensive,
  title={A comprehensive study of knowledge editing for large language models},
  author={Zhang, Ningyu and Yao, Yunzhi and Tian, Bozhong and Wang, Peng and Deng, Shumin and Wang, Mengru and Xi, Zekun and Mao, Shengyu and Zhang, Jintian and Ni, Yuansheng and others},
  journal={arXiv preprint arXiv:2401.01286},
  year={2024}
}

@article{inan2023llama,
  title={Llama guard: Llm-based input-output safeguard for human-ai conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023}
}

@article{yuan2024rigorllm,
  title={RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content},
  author={Yuan, Zhuowen and Xiong, Zidi and Zeng, Yi and Yu, Ning and Jia, Ruoxi and Song, Dawn and Li, Bo},
  journal={arXiv preprint arXiv:2403.13031},
  year={2024}
}

@inproceedings{markov2023holistic,
  title={A holistic approach to undesired content detection in the real world},
  author={Markov, Todor and Zhang, Chong and Agarwal, Sandhini and Nekoul, Florentine Eloundou and Lee, Theodore and Adler, Steven and Jiang, Angela and Weng, Lilian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={15009--15018},
  year={2023}
}

@inproceedings{lees2022new,
  title={A new generation of perspective api: Efficient multilingual character-level transformers},
  author={Lees, Alyssa and Tran, Vinh Q and Tay, Yi and Sorensen, Jeffrey and Gupta, Jai and Metzler, Donald and Vasserman, Lucy},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={3197--3207},
  year={2022}
}

@article{li2024wmdp,
  title={The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning},
  author={Li, Nathaniel and Pan, Alexander and Gopal, Anjali and Yue, Summer and Berrios, Daniel and Gatti, Alice and Li, Justin D and Dombrowski, Ann-Kathrin and Goel, Shashwat and Phan, Long and others},
  journal={arXiv preprint arXiv:2403.03218},
  year={2024}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  pages={7432--7439},
  year={2020}
}

@article{sap2019socialiqa,
  title={Socialiqa: Commonsense reasoning about social interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09728},
  year={2019}
}

@article{clark2019boolq,
  title={BoolQ: Exploring the surprising difficulty of natural yes/no questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1905.10044},
  year={2019}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{talmor2018commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}

@article{mihaylov2018can,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@inproceedings{ren2021zero,
  title={$\{$Zero-offload$\}$: Democratizing $\{$billion-scale$\}$ model training},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages={551--564},
  year={2021}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}

@article{mosbach2020stability,
  title={On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines},
  author={Mosbach, Marius and Andriushchenko, Maksym and Klakow, Dietrich},
  journal={arXiv preprint arXiv:2006.04884},
  year={2020}
}

@article{elhage2021mathematical,
  title={A mathematical framework for transformer circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and others},
  journal={Transformer Circuits Thread},
  volume={1},
  pages={1},
  year={2021}
}

@article{liu2024rethinking,
  title={Rethinking machine unlearning for large language models},
  author={Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Yao, Yuguang and Liu, Chris Yuhao and Xu, Xiaojun and Li, Hang and Varshney, Kush R. and Bansal, Mohit and Koyejo, Sanmi and Liu, Yang},
  journal={arXiv preprint arXiv:2402.08787},
  year={2024}
}

@article{lynch2024eight,
  title={Eight Methods to Evaluate Robust Unlearning in LLMs},
  author={Lynch, Aengus and Guo, Phillip and Ewart, Aidan and Casper, Stephen and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2402.16835},
  year={2024}
}

@InProceedings{hamborg2017news,
  author     = {Hamborg, Felix and Meuschke, Norman and Breitinger, Corinna and Gipp, Bela},
  title      = {news-please: A Generic News Crawler and Extractor},
  year       = {2017},
  booktitle  = {Proceedings of the 15th International Symposium of Information Science},
  location   = {Berlin},
  doi        = {10.5281/zenodo.4120316},
  pages      = {218--223},
  month      = {March}
}

@article{yao2024machine,
  title={Machine Unlearning of Pre-trained Large Language Models},
  author={Yao, Jin and Chien, Eli and Du, Minxin and Niu, Xinyao and Wang, Tianhao and Cheng, Zezhou and Yue, Xiang},
  journal={arXiv preprint arXiv:2402.15159},
  year={2024}
}


@article{duan2024membership,
  title={Do membership inference attacks work on large language models?},
  author={Duan, Michael and Suri, Anshuman and Mireshghallah, Niloofar and Min, Sewon and Shi, Weijia and Zettlemoyer, Luke and Tsvetkov, Yulia and Choi, Yejin and Evans, David and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2402.07841},
  year={2024}
}

@article{casper2024defending,
  title={Defending Against Unforeseen Failure Modes with Latent Adversarial Training},
  author={Casper, Stephen and Schulze, Lennart and Patel, Oam and Hadfield-Menell, Dylan},
  journal={arXiv preprint arXiv:2403.05030},
  year={2024}
}

@article{schwinn2024soft,
  title={Soft Prompt Threats: Attacking Safety Alignment and Unlearning in Open-Source LLMs through the Embedding Space},
  author={Schwinn, Leo and Dobre, David and Xhonneux, Sophie and Gidel, Gauthier and Gunnemann, Stephan},
  journal={arXiv preprint arXiv:2402.09063},
  year={2024}
}

@article{jain2023neftune,
  title={Neftune: Noisy embeddings improve instruction finetuning},
  author={Jain, Neel and Chiang, Ping-yeh and Wen, Yuxin and Kirchenbauer, John and Chu, Hong-Min and Somepalli, Gowthami and Bartoldson, Brian R and Kailkhura, Bhavya and Schwarzschild, Avi and Saha, Aniruddha and others},
  journal={arXiv preprint arXiv:2310.05914},
  year={2023}
}

@article{cho2014properties,
  title={On the properties of neural machine translation: Encoder-decoder approaches},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.1259},
  year={2014}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@article{zhang2024negative,
  title={Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning},
  author={Zhang, Ruiqi and Lin, Licong and Bai, Yu and Mei, Song},
  journal={arXiv preprint arXiv:2404.05868},
  year={2024}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv: 230605685},
  author={Zheng, L and Chiang, WL and Sheng, Y and Zhuang, S and Wu, Z and Zhuang, Y and Lin, Z and Li, Z and Li, D and Xing, E},
  year={2023}
}

@article{young2024yi,
  title={Yi: Open foundation models by 01. ai},
  author={Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and others},
  journal={arXiv preprint arXiv:2403.04652},
  year={2024}
}

@article{tunstall2023zephyr,
  title={Zephyr: Direct distillation of lm alignment},
  author={Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Cl{\'e}mentine and Habib, Nathan and others},
  journal={arXiv preprint arXiv:2310.16944},
  year={2023}
}

@article{kurmanji2024towards,
  title={Towards unbounded machine unlearning},
  author={Kurmanji, Meghdad and Triantafillou, Peter and Hayes, Jamie and Triantafillou, Eleni},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{foster2024fast,
  title={Fast machine unlearning without retraining through selective synaptic dampening},
  author={Foster, Jack and Schoepf, Stefan and Brintrup, Alexandra},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={12043--12051},
  year={2024}
}

@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}

@article{post2018call,
  title={A call for clarity in reporting BLEU scores},
  author={Post, Matt},
  journal={arXiv preprint arXiv:1804.08771},
  year={2018}
}

@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}

@article{liu2020adversarial,
  title={Adversarial training for large neural language models},
  author={Liu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2004.08994},
  year={2020}
}

@article{kim2023robust,
  title={Robust safety classifier for large language models: Adversarial prompt shield},
  author={Kim, Jinhwa and Derakhshan, Ali and Harris, Ian G},
  journal={arXiv preprint arXiv:2311.00172},
  year={2023}
}

@article{ebrahimi2021does,
  title={How does adversarial fine-tuning benefit bert?},
  author={Ebrahimi, Javid and Yang, Hao and Zhang, Wei},
  journal={arXiv preprint arXiv:2108.13602},
  year={2021}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{shevlane2022structured,
  title={Structured access: an emerging paradigm for safe AI deployment},
  author={Shevlane, Toby},
  journal={arXiv preprint arXiv:2201.05159},
  year={2022}
}

@article{liu2024model,
  title={Model sparsity can simplify machine unlearning},
  author={Liu, Jiancheng and Ram, Parikshit and Yao, Yuguang and Liu, Gaowen and Liu, Yang and SHARMA, PRANAY and Liu, Sijia and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{thudi2022unrolling,
  title={Unrolling sgd: Understanding factors influencing machine unlearning},
  author={Thudi, Anvith and Deza, Gabriel and Chandrasekaran, Varun and Papernot, Nicolas},
  booktitle={2022 IEEE 7th European Symposium on Security and Privacy (EuroS\&P)},
  pages={303--319},
  year={2022},
  organization={IEEE}
}

@article{fan2023salun,
  title={Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation},
  author={Fan, Chongyu and Liu, Jiancheng and Zhang, Yihua and Wei, Dennis and Wong, Eric and Liu, Sijia},
  journal={arXiv preprint arXiv:2310.12508},
  year={2023}
}

@inproceedings{hu2024separate,
  title={Separate the wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation},
  author={Hu, Xinshuo and Li, Dongfang and Hu, Baotian and Zheng, Zihao and Liu, Zhenyu and Zhang, Min},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={18252--18260},
  year={2024}
}

@article{grynbaum2023times,
  title={The Times Sues OpenAI and Microsoft Over A.I. Use of Copyrighted Work},
  author={Grynbaum, Michael M. and Mac, Ryan},
  journal={The New York Times},
  url={https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html},
  year={2023},
  month={12},
  day={27}
}

@inproceedings{chen2023boundary,
  title={Boundary unlearning: Rapid forgetting of deep networks via shifting the decision boundary},
  author={Chen, Min and Gao, Weizhuo and Liu, Gaoyang and Peng, Kai and Wang, Chen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7766--7775},
  year={2023}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}


@article{liao2024amplegcg,
  title={AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs},
  author={Liao, Zeyi and Sun, Huan},
  journal={arXiv preprint arXiv:2404.07921},
  year={2024}
}

@inproceedings{golatkar2020eternal,
  title={Eternal sunshine of the spotless net: Selective forgetting in deep networks},
  author={Golatkar, Aditya and Achille, Alessandro and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9304--9312},
  year={2020}
}

@article{jia2023model,
  title={Model sparsification can simplify machine unlearning},
  author={Jia, Jinghan and Liu, Jiancheng and Ram, Parikshit and Yao, Yuguang and Liu, Gaowen and Liu, Yang and Sharma, Pranay and Liu, Sijia},
  journal={arXiv preprint arXiv:2304.04934},
  year={2023}
}

@article{guo2019certified,
  title={Certified data removal from machine learning models},
  author={Guo, Chuan and Goldstein, Tom and Hannun, Awni and Van Der Maaten, Laurens},
  journal={arXiv preprint arXiv:1911.03030},
  year={2019}
}

@inproceedings{carlini2022membership,
  title={Membership inference attacks from first principles},
  author={Carlini, Nicholas and Chien, Steve and Nasr, Milad and Song, Shuang and Terzis, Andreas and Tramer, Florian},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
  pages={1897--1914},
  year={2022},
  organization={IEEE}
}

@article{hayes2024inexact,
  title={Inexact Unlearning Needs More Careful Evaluations to Avoid a False Sense of Privacy},
  author={Hayes, Jamie and Shumailov, Ilia and Triantafillou, Eleni and Khalifa, Amr and Papernot, Nicolas},
  journal={arXiv preprint arXiv:2403.01218},
  year={2024}
}

@article{paulus2024advprompter,
  title={AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs},
  author={Paulus, Anselm and Zharmagambetov, Arman and Guo, Chuan and Amos, Brandon and Tian, Yuandong},
  journal={arXiv preprint arXiv:2404.16873},
  year={2024}
}

@article{karamolegkou2023copyright,
  title={Copyright violations and large language models},
  author={Karamolegkou, Antonia and Li, Jiaang and Zhou, Li and S{\o}gaard, Anders},
  journal={arXiv preprint arXiv:2310.13771},
  year={2023}
}

@article{li2024digger,
  title={Digger: Detecting Copyright Content Mis-usage in Large Language Model Training},
  author={Li, Haodong and Deng, Gelei and Liu, Yi and Wang, Kailong and Li, Yuekang and Zhang, Tianwei and Liu, Yang and Xu, Guoai and Xu, Guosheng and Wang, Haoyu},
  journal={arXiv preprint arXiv:2401.00676},
  year={2024}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Gemini},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@misc{anthropic2024claude,
  title = {Introducing the next generation of Claude},
  author = {Anthropic},
  year = {2024},
  month = {Mar},
  howpublished = {\url{https://www.anthropic.com/news/claude-3-family}}
}

@article{harandizadeh2024risk,
  title={Risk and Response in Large Language Models: Evaluating Key Threat Categories},
  author={Harandizadeh, Bahareh and Salinas, Abel and Morstatter, Fred},
  journal={arXiv preprint arXiv:2403.14988},
  year={2024}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}


@article{caramancion2024large,
  title={Large Language Models vs. Search Engines: Evaluating User Preferences Across Varied Information Retrieval Scenarios},
  author={Caramancion, Kevin Matthe},
  journal={arXiv preprint arXiv:2401.05761},
  year={2024}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@misc{openai2022chatgpt,
  title = {Introducing ChatGPT},
  author = {OpenAI},
  year = {2022},
  month = {Nov},
  day = {30},
}

@article{fang2024llm,
  title={LLM Agents can Autonomously Hack Websites},
  author={Fang, Richard and Bindu, Rohan and Gupta, Akul and Zhan, Qiusi and Kang, Daniel},
  journal={arXiv preprint arXiv:2402.06664},
  year={2024}
}

@article{sandbrink2023artificial,
  title={Artificial intelligence and biological misuse: Differentiating risks of language models and biological design tools},
  author={Sandbrink, Jonas B},
  journal={arXiv preprint arXiv:2306.13952},
  year={2023}
}

@article{staab2023beyond,
  title={Beyond memorization: Violating privacy via inference with large language models},
  author={Staab, Robin and Vero, Mark and Balunovi{\'c}, Mislav and Vechev, Martin},
  journal={arXiv preprint arXiv:2310.07298},
  year={2023}
}

@article{mireshghallah2023can,
  title={Can llms keep a secret? testing privacy implications of language models via contextual integrity theory},
  author={Mireshghallah, Niloofar and Kim, Hyunwoo and Zhou, Xuhui and Tsvetkov, Yulia and Sap, Maarten and Shokri, Reza and Choi, Yejin},
  journal={arXiv preprint arXiv:2310.17884},
  year={2023}
}

@article{neel2023privacy,
  title={Privacy issues in large language models: A survey},
  author={Neel, Seth and Chang, Peter},
  journal={arXiv preprint arXiv:2312.06717},
  year={2023}
}

@misc{gdpr,
  title = {General Data Protection Regulation (GDPR)},
  author = {{European Union}},
  year = {2016},
  howpublished = {\url{https://gdpr-info.eu/}},
}

@article{goel2024corrective,
  title={Corrective Machine Unlearning},
  author={Goel, Shashwat and Prabhu, Ameya and Torr, Philip and Kumaraguru, Ponnurangam and Sanyal, Amartya},
  journal={arXiv preprint arXiv:2402.14015},
  year={2024}
}

@article{jia2024soul,
  title={SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning},
  author={Jia, Jinghan and Zhang, Yihua and Zhang, Yimeng and Liu, Jiancheng and Runwal, Bharat and Diffenderfer, James and Kailkhura, Bhavya and Liu, Sijia},
  journal={arXiv preprint arXiv:2404.18239},
  year={2024}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{xu2024machine,
  title={Machine unlearning: Solutions and challenges},
  author={Xu, Jie and Wu, Zihan and Wang, Cong and Jia, Xiaohua},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  year={2024},
  publisher={IEEE}
}

@article{liu2024towards,
  title={Towards Safer Large Language Models through Machine Unlearning},
  author={Liu, Zheyuan and Dou, Guangyao and Tan, Zhaoxuan and Tian, Yijun and Jiang, Meng},
  journal={arXiv preprint arXiv:2402.10058},
  year={2024}
}

@article{ni2023forgetting,
  title={Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models},
  author={Ni, Shiwen and Chen, Dingwei and Li, Chengming and Hu, Xiping and Xu, Ruifeng and Yang, Min},
  journal={arXiv preprint arXiv:2311.08011},
  year={2023}
}

@article{muresanu2024unlearnable,
  title={Unlearnable algorithms for in-context learning},
  author={Muresanu, Andrei and Thudi, Anvith and Zhang, Michael R and Papernot, Nicolas},
  journal={arXiv preprint arXiv:2402.00751},
  year={2024}
}

@article{kim2024propile,
  title={Propile: Probing privacy leakage in large language models},
  author={Kim, Siwon and Yun, Sangdoo and Lee, Hwaran and Gubri, Martin and Yoon, Sungroh and Oh, Seong Joon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{borkar2023can,
  title={What can we learn from Data Leakage and Unlearning for Law?},
  author={Borkar, Jaydeep},
  journal={arXiv preprint arXiv:2307.10476},
  year={2023}
}

@inproceedings{lukas2023analyzing,
  title={Analyzing leakage of personally identifiable information in language models},
  author={Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and Zanella-B{\'e}guelin, Santiago},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)},
  pages={346--363},
  year={2023},
  organization={IEEE}
}

@article{kumar2022privacy,
  title={Privacy adhering machine un-learning in nlp},
  author={Kumar, Vinayshekhar Bannihatti and Gangadharaiah, Rashmi and Roth, Dan},
  journal={arXiv preprint arXiv:2212.09573},
  year={2022}
}

@article{huang2024offset,
  title={Offset Unlearning for Large Language Models},
  author={Huang, James Y and Zhou, Wenxuan and Wang, Fei and Morstatter, Fred and Zhang, Sheng and Poon, Hoifung and Chen, Muhao},
  journal={arXiv preprint arXiv:2404.11045},
  year={2024}
}

@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@article{groeneveld2024olmo,
  title={Olmo: Accelerating the science of language models},
  author={Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and others},
  journal={arXiv preprint arXiv:2402.00838},
  year={2024}
}

@article{chen2023deepzero,
  title={Deepzero: Scaling up zeroth-order optimization for deep model training},
  author={Chen, Aochuan and Zhang, Yimeng and Jia, Jinghan and Diffenderfer, James and Liu, Jiancheng and Parasyris, Konstantinos and Zhang, Yihua and Zhang, Zheng and Kailkhura, Bhavya and Liu, Sijia},
  journal={arXiv preprint arXiv:2310.02025},
  year={2023}
}

@article{liu2020primer,
  title={A primer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances, and applications},
  author={Liu, Sijia and Chen, Pin-Yu and Kailkhura, Bhavya and Zhang, Gaoyuan and Hero III, Alfred O and Varshney, Pramod K},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={5},
  pages={43--54},
  year={2020},
  publisher={IEEE}
}

@article{campos2024conformal,
  title={Conformal Prediction for Natural Language Processing: A Survey},
  author={Campos, Margarida M and Farinhas, Ant{\'o}nio and Zerva, Chrysoula and Figueiredo, M{\'a}rio AT and Martins, Andr{\'e} FT},
  journal={arXiv preprint arXiv:2405.01976},
  year={2024}
}

@inproceedings{maltoudoglou2020bert,
  title={BERT-based conformal predictor for sentiment analysis},
  author={Maltoudoglou, Lysimachos and Paisios, Andreas and Papadopoulos, Harris},
  booktitle={Conformal and Probabilistic Prediction and Applications},
  pages={269--284},
  year={2020},
  organization={PMLR}
}

@inproceedings{messoudi2020deep,
  title={Deep conformal prediction for robust models},
  author={Messoudi, Soundouss and Rousseau, Sylvain and Destercke, S{\'e}bastien},
  booktitle={Information Processing and Management of Uncertainty in Knowledge-Based Systems: 18th International Conference, IPMU 2020, Lisbon, Portugal, June 15--19, 2020, Proceedings, Part I 18},
  pages={528--540},
  year={2020},
  organization={Springer}
}

@inproceedings{giovannotti2022calibration,
  title={Calibration of Natural Language Understanding Models with Venn--ABERS Predictors},
  author={Giovannotti, Patrizio},
  booktitle={Conformal and Probabilistic Prediction with Applications},
  pages={55--71},
  year={2022},
  organization={PMLR}
}

@book{vovk2005algorithmic,
  title={Algorithmic learning in a random world},
  author={Vovk, Vladimir and Gammerman, Alexander and Shafer, Glenn},
  volume={29},
  year={2005},
  publisher={Springer}
}

@article{baumhauer2022machine,
  title={Machine unlearning: Linear filtration for logit-based classifiers},
  author={Baumhauer, Thomas and Sch{\"o}ttle, Pascal and Zeppelzauer, Matthias},
  journal={Machine Learning},
  volume={111},
  number={9},
  pages={3203--3226},
  year={2022},
  publisher={Springer}
}

@article{spall1992multivariate,
  title={Multivariate stochastic approximation using a simultaneous perturbation gradient approximation},
  author={Spall, James C},
  journal={IEEE transactions on automatic control},
  volume={37},
  number={3},
  pages={332--341},
  year={1992},
  publisher={IEEE}
}

@book{spall2005introduction,
  title={Introduction to stochastic search and optimization: estimation, simulation, and control},
  author={Spall, James C},
  year={2005},
  publisher={John Wiley \& Sons}
}

@article{deepseekai2023deepseek,
  title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
  author={DeepSeek-AI},
  journal={arXiv preprint arXiv:2405.04434},
  year={2024}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{cai2024internlm2,
  title={Internlm2 technical report},
  author={Cai, Zheng and Cao, Maosong and Chen, Haojiong and Chen, Kai and Chen, Keyu and Chen, Xin and Chen, Xun and Chen, Zehui and Chen, Zhi and Chu, Pei and others},
  journal={arXiv preprint arXiv:2403.17297},
  year={2024}
}

@article{bellagente2024stable,
  title={Stable LM 2 1.6 B Technical Report},
  author={Bellagente, Marco and Tow, Jonathan and Mahan, Dakota and Phung, Duy and Zhuravinskyi, Maksym and Adithyan, Reshinth and Baicoianu, James and Brooks, Ben and Cooper, Nathan and Datta, Ashish and others},
  journal={arXiv preprint arXiv:2402.17834},
  year={2024}
}

@article{luo2023biomedgpt,
  title={Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine},
  author={Luo, Yizhen and Zhang, Jiahuan and Fan, Siqi and Yang, Kai and Wu, Yushuai and Qiao, Mu and Nie, Zaiqing},
  journal={arXiv preprint arXiv:2308.09442},
  year={2023}
}

@misc{labrak2024biomistral,
      title={BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains}, 
      author={Yanis Labrak and Adrien Bazoge and Emmanuel Morin and Pierre-Antoine Gourraud and Mickael Rouvier and Richard Dufour},
      year={2024},
      eprint={2402.10373},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{OpenBioLLMs,
  author = {Ankit Pal, Malaikannan Sankarasubbu},
  title = {OpenBioLLMs: Advancing Open-Source Large Language Models for Healthcare and Life Sciences},
  year = {2024},
  publisher = {Hugging Face},
  journal = {Hugging Face repository},
  howpublished = {\url{https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B}}
}

@misc{zhao2024chemdfm,
    title={ChemDFM: Dialogue Foundation Model for Chemistry},
    author={Zihan Zhao and Da Ma and Lu Chen and Liangtai Sun and Zihao Li and Hongshen Xu and Zichen Zhu and Su Zhu and Shuai Fan and Guodong Shen and Xin Chen and Kai Yu},
    year={2024},
    eprint={2401.14818},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{zhang2024chemllm,
      title={ChemLLM: A Chemical Large Language Model}, 
      author={Di Zhang and Wei Liu and Qian Tan and Jingdan Chen and Hang Yan and Yuliang Yan and Jiatong Li and Weiran Huang and Xiangyu Yue and Dongzhan Zhou and Shufei Zhang and Mao Su and Hansen Zhong and Yuqiang Li and Wanli Ouyang},
      year={2024},
      eprint={2402.06852},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{mishra2024granite,
  title={Granite Code Models: A Family of Open Foundation Models for Code Intelligence},
  author={Mishra, Mayank and Stallone, Matt and Zhang, Gaoyuan and Shen, Yikang and Prasad, Aditya and Soria, Adriana Meza and Merler, Michele and Selvam, Parameswaran and Surendran, Saptha and Singh, Shivdeep and others},
  journal={arXiv preprint arXiv:2405.04324},
  year={2024}
}

@article{codegemma_2024,
    title={CodeGemma: Open Code Models Based on Gemma},
    url={https://goo.gle/codegemma},
    author={{CodeGemma Team} and Hartman, Ale Jakse and Hu, Andrea and Choquette-Choo, Christopher A. and Zhao, Heri and Fine, Jane and Hui, Jeffrey and Shen, Jingyue and Kelley, Joe and Howland, Joshua and Bansal, Kshitij and Vilnis, Luke and Wirth, Mateo and Nguyen, Nam and Michel, Paul and Choy, Peter and Joshi, Pratik and Kumar, Ravin and Hashmi, Sarmad and Agrawal, Shubham and Zuo, Siqi and Warkentin, Tris and Gong, Zhitao et al.},
    year={2024}
}


@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{guo2024deepseek,
  title={DeepSeek-Coder: When the Large Language Model Meets Programming--The Rise of Code Intelligence},
  author={Guo, Daya and Zhu, Qihao and Yang, Dejian and Xie, Zhenda and Dong, Kai and Zhang, Wentao and Chen, Guanting and Bi, Xiao and Wu, Y and Li, YK and others},
  journal={arXiv preprint arXiv:2401.14196},
  year={2024}
}

@misc{lozhkov2024starcoder,
      title={StarCoder 2 and The Stack v2: The Next Generation}, 
      author={Anton Lozhkov and Raymond Li and Loubna Ben Allal and Federico Cassano and Joel Lamy-Poirier and Nouamane Tazi and Ao Tang and Dmytro Pykhtar and Jiawei Liu and Yuxiang Wei and Tianyang Liu and Max Tian and Denis Kocetkov and Arthur Zucker and Younes Belkada and Zijian Wang and Qian Liu and Dmitry Abulkhanov and Indraneil Paul and Zhuang Li and Wen-Ding Li and Megan Risdal and Jia Li and Jian Zhu and Terry Yue Zhuo and Evgenii Zheltonozhskii and Nii Osae Osae Dade and Wenhao Yu and Lucas Krauß and Naman Jain and Yixuan Su and Xuanli He and Manan Dey and Edoardo Abati and Yekun Chai and Niklas Muennighoff and Xiangru Tang and Muhtasham Oblokulov and Christopher Akiki and Marc Marone and Chenghao Mou and Mayank Mishra and Alex Gu and Binyuan Hui and Tri Dao and Armel Zebaze and Olivier Dehaene and Nicolas Patry and Canwen Xu and Julian McAuley and Han Hu and Torsten Scholak and Sebastien Paquet and Jennifer Robinson and Carolyn Jane Anderson and Nicolas Chapados and Mostofa Patwary and Nima Tajbakhsh and Yacine Jernite and Carlos Muñoz Ferrandis and Lingming Zhang and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
      year={2024},
      eprint={2402.19173},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@article{pinnaparaju2024stable,
  title={Stable Code Technical Report},
  author={Pinnaparaju, Nikhil and Adithyan, Reshinth and Phung, Duy and Tow, Jonathan and Baicoianu, James and Datta, Ashish and Zhuravinskyi, Maksym and Mahan, Dakota and Bellagente, Marco and Riquelme, Carlos and others},
  journal={arXiv preprint arXiv:2404.01226},
  year={2024}
}

@article{yang2023baichuan,
  title={Baichuan 2: Open large-scale language models},
  author={Yang, Aiyuan and Xiao, Bin and Wang, Bingning and Zhang, Borong and Bian, Ce and Yin, Chao and Lv, Chenxu and Pan, Da and Wang, Dian and Yan, Dong and others},
  journal={arXiv preprint arXiv:2309.10305},
  year={2023}
}

@article{llama3modelcard,
    title={Llama 3 Model Card},
    author={AI@Meta},
    year={2024},
    url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}

@article{falcon,
  title={The Falcon Series of Language Models:Towards Open Frontier Models},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year={2023}
}

@article{liu2024chatqa,
  title={ChatQA: Surpassing GPT-4 on Conversational QA and RAG},
  author={Liu, Zihan and Ping, Wei and Roy, Rajarshi and Xu, Peng and Lee, Chankyu and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2401.10225},
  year={2024}
}

@article{wang2023openchat,
  title={OpenChat: Advancing Open-source Language Models with Mixed-Quality Data},
  author={Wang, Guan and Cheng, Sijie and Zhan, Xianyuan and Li, Xiangang and Song, Sen and Liu, Yang},
  journal={arXiv preprint arXiv:2309.11235},
  year={2023}
}

@article{mukherjee2023orca,
  title={Orca: Progressive learning from complex explanation traces of gpt-4},
  author={Mukherjee, Subhabrata and Mitra, Arindam and Jawahar, Ganesh and Agarwal, Sahaj and Palangi, Hamid and Awadallah, Ahmed},
  journal={arXiv preprint arXiv:2306.02707},
  year={2023}
}

@article{abdin2024phi,
  title={Phi-3 technical report: A highly capable language model locally on your phone},
  author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024}
}

@article{botev2024recurrentgemma,
  title={RecurrentGemma: Moving Past Transformers for Efficient Open Language Models},
  author={Botev, Aleksandar and De, Soham and Smith, Samuel L and Fernando, Anushan and Muraru, George-Cristian and Haroun, Ruba and Berrada, Leonard and Pascanu, Razvan and Sessa, Pier Giuseppe and Dadashi, Robert and others},
  journal={arXiv preprint arXiv:2404.07839},
  year={2024}
}

@misc{StableBelugaModels, 
      url={[https://huggingface.co/stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)}, 
      title={Stable Beluga models}, 
      author={Mahan, Dakota and Carlow, Ryan and Castricato, Louis and Cooper, Nathan and Laforte, Christian}
}

@misc{starling2023,
    title = {Starling-7B: Improving LLM Helpfulness \& Harmlessness with RLAIF},
    url = {},
    author = {Zhu, Banghua and Frick, Evan and Wu, Tianhao and Zhu, Hanlin and Ganesan, Karthik and Chiang, Wei-Lin and Zhang, Jian and Jiao, Jiantao},
    month = {November},
    year = {2023}
}

@article{zheng2024judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}

@misc{zephyr_141b,
  author = {Alvaro Bartolome and Jiwoo Hong and Noah Lee and Kashif Rasul and Lewis Tunstall},
  title = {Zephyr 141B A39B},
  year = {2024},
  publisher = {Hugging Face},
  journal = {Hugging Face repository},
  howpublished = {\url{https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1}}
}

@misc{zephyr_7b_gemma,
  author = {Lewis Tunstall and Philipp Schmid},
  title = {Zephyr 7B Gemma},
  year = {2024},
  publisher = {Hugging Face},
  journal = {Hugging Face repository},
  howpublished = {\url{https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1}}
}

@misc{cohere2024,
    title={Command R: Retrieval-Augmented Generation at Production Scale},
    author={{Cohere Team}},
    year={2024},
    url={https://cohere.com/blog/command-r},
}

@misc{databricks2024,
    title={Introducing DBRx: A New State-of-the-Art Open LLM},
    author={{Databricks Team}},
    year={2024},
    url={https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm},
}

@article{shen2024jetmoe,
  title={JetMoE: Reaching Llama2 Performance with 0.1 M Dollars},
  author={Shen, Yikang and Guo, Zhen and Cai, Tianle and Qin, Zengyi},
  journal={arXiv preprint arXiv:2404.07413},
  year={2024}
}

@article{chu2024causal,
  title={A Causal Explainable Guardrails for Large Language Models},
  author={Chu, Zhixuan and Wang, Yan and Li, Longfei and Wang, Zhibo and Qin, Zhan and Ren, Kui},
  journal={arXiv preprint arXiv:2405.04160},
  year={2024}
}

@inproceedings{goyal2024llmguard,
  title={LLMGuard: Guarding against Unsafe LLM Behavior},
  author={Goyal, Shubh and Hira, Medha and Mishra, Shubham and Goyal, Sukriti and Goel, Arnav and Dadu, Niharika and Kirushikesh, DB and Mehta, Sameep and Madaan, Nishtha},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={21},
  pages={23790--23792},
  year={2024}
}

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model. arXiv 2023},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@article{sepahvand2024data,
  title={Data Selection for Transfer Unlearning. arXiv 2024},
  author={Sepahvand, Nazanin Mohammadi and Dumoulin, Vincent and Triantafillou, Eleni and Dziugaite, Gintare Karolina},
  journal={arXiv preprint arXiv:2405.10425},
  year={2024}
}

@book{rowling1997harry,
  title={Harry Potter and the Sorcerer's Stone},
  author={J.K. Rowling},
  year={1997},
  publisher={Scholastic},
  address={New York},
  isbn={978-0-590-35340-3}
}

@misc{aryabumi2024aya,
      title={Aya 23: Open Weight Releases to Further Multilingual Progress}, 
      author={Viraat Aryabumi and John Dang and Dwarak Talupuru and Saurabh Dash and David Cairuz and Hangyu Lin and Bharat Venkitesh and Madeline Smith and Kelly Marchisio and Sebastian Ruder and Acyr Locatelli and Julia Kreutzer and Nick Frosst and Phil Blunsom and Marzieh Fadaee and Ahmet Üstün and Sara Hooker},
      year={2024},
      eprint={2405.15032},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{maini2024llm,
  title={LLM Dataset Inference: Did you train on my dataset?},
  author={Maini, Pratyush and Jia, Hengrui and Papernot, Nicolas and Dziedzic, Adam},
  journal={arXiv preprint arXiv:2406.06443},
  year={2024}
}

@article{qiu2024pistol,
  title={PISTOL: Dataset Compilation Pipeline for Structural Unlearning of LLMs},
  author={Qiu, Xinchi and Shen, William F and Chen, Yihong and Cancedda, Nicola and Stenetorp, Pontus and Lane, Nicholas D},
  journal={arXiv preprint arXiv:2406.16810},
  year={2024}
}

@incollection{mccloskey1989catastrophic,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  volume={24},
  pages={109--165},
  year={1989},
  publisher={Elsevier}
}

@article{thaker2024position,
  title={Position: LLM Unlearning Benchmarks are Weak Measures of Progress},
  author={Thaker, Pratiksha and Hu, Shengyuan and Kale, Neil and Maurya, Yash and Wu, Zhiwei Steven and Smith, Virginia},
  journal={arXiv preprint arXiv:2410.02879},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{gan2023model,
  title={Model-as-a-service (MaaS): A survey},
  author={Gan, Wensheng and Wan, Shicheng and Philip, S Yu},
  booktitle={2023 IEEE International Conference on Big Data (BigData)},
  pages={4636--4645},
  year={2023},
  organization={IEEE}
}

@article{si2023knowledge,
  title={Knowledge unlearning for llms: Tasks, methods, and challenges},
  author={Si, Nianwen and Zhang, Hao and Chang, Heyu and Zhang, Wenlin and Qu, Dan and Zhang, Weiqiang},
  journal={arXiv preprint arXiv:2311.15766},
  year={2023}
}

@article{blanco2024digital,
  title={Digital Forgetting in Large Language Models: A Survey of Unlearning Methods},
  author={Blanco-Justicia, Alberto and Jebreel, Najeeb and Manzanares, Benet and S{\'a}nchez, David and Domingo-Ferrer, Josep and Collell, Guillem and Tan, Kuan Eeik},
  journal={arXiv preprint arXiv:2404.02062},
  year={2024}
}