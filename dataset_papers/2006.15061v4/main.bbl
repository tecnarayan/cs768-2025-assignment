\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel \& Ng(2004)Abbeel and Ng]{abbeel2004apprenticeship}
Abbeel, P. and Ng, A.~Y.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\ ~1,
  2004.

\bibitem[Alemi et~al.(2017)Alemi, Fischer, Dillon, and Murphy]{alemi2016deep}
Alemi, A.~A., Fischer, I., Dillon, J.~V., and Murphy, K.
\newblock Deep variational information bottleneck.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Boularias et~al.(2011)Boularias, Kober, and
  Peters]{boularias2011relative}
Boularias, A., Kober, J., and Peters, J.
\newblock Relative entropy inverse reinforcement learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  182--189, 2011.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Brown et~al.(2019{\natexlab{a}})Brown, Goo, Nagarajan, and
  Niekum]{brown2019extrapolating}
Brown, D.~S., Goo, W., Nagarajan, P., and Niekum, S.
\newblock Extrapolating beyond suboptimal demonstrations via inverse
  reinforcement learning from observations.
\newblock In \emph{International Conference on Machine Learning},
  2019{\natexlab{a}}.

\bibitem[Brown et~al.(2019{\natexlab{b}})Brown, Goo, and
  Niekum]{brown2019ranking}
Brown, D.~S., Goo, W., and Niekum, S.
\newblock Ranking-based reward extrapolation without rankings.
\newblock In \emph{Conference on Robot Learning}, 2019{\natexlab{b}}.

\bibitem[Burda et~al.(2019)Burda, Edwards, Pathak, Storkey, Darrell, and
  Efros]{burda2018large}
Burda, Y., Edwards, H., Pathak, D., Storkey, A., Darrell, T., and Efros, A.~A.
\newblock Large-scale study of curiosity-driven learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4299--4307, 2017.

\bibitem[Coumans \& Bai(2016--2019)Coumans and Bai]{coumans2019}
Coumans, E. and Bai, Y.
\newblock Pybullet, a python module for physics simulation for games, robotics
  and machine learning.
\newblock \url{http://pybullet.org}, 2016--2019.

\bibitem[Finn et~al.(2016)Finn, Levine, and Abbeel]{finn2016guided}
Finn, C., Levine, S., and Abbeel, P.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  49--58,
  2016.

\bibitem[Fu et~al.(2018)Fu, Luo, and Levine]{fu2017learning}
Fu, J., Luo, K., and Levine, S.
\newblock Learning robust rewards with adversarial inverse reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{NIPS2014_5423}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2672--2680, 2014.

\bibitem[Hester et~al.(2018)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Horgan, Quan, Sendonaris, Osband, et~al.]{hester2018deep}
Hester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B.,
  Horgan, D., Quan, J., Sendonaris, A., Osband, I., et~al.
\newblock Deep q-learning from demonstrations.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2018.

\bibitem[Ho \& Ermon(2016)Ho and Ermon]{ho2016generative}
Ho, J. and Ermon, S.
\newblock Generative adversarial imitation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4565--4573, 2016.

\bibitem[Ibarz et~al.(2018)Ibarz, Leike, Pohlen, Irving, Legg, and
  Amodei]{ibarz2018reward}
Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D.
\newblock Reward learning from human preferences and demonstrations in atari.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8011--8023, 2018.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, 2015.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{jang2016categorical}
Jang, E., Gu, S., and Poole, B.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Karnewar(2018)]{karnewar2018repo}
Karnewar, A.
\newblock Pytorch implementations of variational discriminator bottleneck,
  2018.
\newblock URL
  \url{https://github.com/akanimax/Variational_Discriminator_Bottleneck}.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kostrikov(2018)]{kostrikov2018repo}
Kostrikov, I.
\newblock Pytorch implementations of reinforcement learning algorithms, 2018.
\newblock URL \url{https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail}.

\bibitem[Maas et~al.(2013)Maas, Hannun, and Ng]{maas2013rectifier}
Maas, A.~L., Hannun, A.~Y., and Ng, A.~Y.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In \emph{International Conference on Machine Learning}, 2013.

\bibitem[Ng \& Russell(2000)Ng and Russell]{ng2000algorithms}
Ng, A.~Y. and Russell, S.~J.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, volume~1,
  pp.\  663--670, 2000.

\bibitem[Papamakarios et~al.(2017)Papamakarios, Pavlakou, and
  Murray]{papamakarios2017masked}
Papamakarios, G., Pavlakou, T., and Murray, I.
\newblock Masked autoregressive flow for density estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2338--2347, 2017.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
Pathak, D., Agrawal, P., Efros, A.~A., and Darrell, T.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2778--2787, 2017.

\bibitem[Peng et~al.(2019)Peng, Kanazawa, Toyer, Abbeel, and
  Levine]{peng2018variational}
Peng, X.~B., Kanazawa, A., Toyer, S., Abbeel, P., and Levine, S.
\newblock Variational discriminator bottleneck: Improving imitation learning,
  inverse rl, and gans by constraining information flow.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Pomerleau(1991)]{pomerleau1991efficient}
Pomerleau, D.~A.
\newblock Efficient training of artificial neural networks for autonomous
  navigation.
\newblock \emph{Neural Computation}, 3\penalty0 (1):\penalty0 88--97, 1991.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Qureshi et~al.(2019)Qureshi, Boots, and Yip]{qureshi2018adversarial}
Qureshi, A.~H., Boots, B., and Yip, M.~C.
\newblock Adversarial imitation via variational inverse reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
Ross, S., Gordon, G., and Bagnell, D.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  627--635, 2011.

\bibitem[Schroecker et~al.(2019)Schroecker, Vecerik, and
  Scholz]{schroecker2019generative}
Schroecker, Y., Vecerik, M., and Scholz, J.
\newblock Generative predecessor models for sample-efficient imitation
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{schulman2015high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sohn et~al.(2015)Sohn, Lee, and Yan]{sohn2015learning}
Sohn, K., Lee, H., and Yan, X.
\newblock Learning structured output representation using deep conditional
  generative models.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3483--3491, 2015.

\bibitem[Sutton et~al.(1998)Sutton, Barto, et~al.]{sutton1998introduction}
Sutton, R.~S., Barto, A.~G., et~al.
\newblock \emph{Introduction to reinforcement learning}.
\newblock MIT press Cambridge, 1998.

\bibitem[Tishby \& Zaslavsky(2015)Tishby and Zaslavsky]{tishby2015deep}
Tishby, N. and Zaslavsky, N.
\newblock Deep learning and the information bottleneck principle.
\newblock In \emph{2015 IEEE Information Theory Workshop (ITW)}, pp.\  1--5.
  IEEE, 2015.

\bibitem[Tucker et~al.(2018)Tucker, Gleave, and Russell]{tucker2018inverse}
Tucker, A., Gleave, A., and Russell, S.
\newblock Inverse reinforcement learning for video games.
\newblock \emph{Workshop on Deep Reinforcement Learning at NeurIPS}, 2018.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Ziebart, B.~D., Maas, A., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2008.

\bibitem[Ziebart et~al.(2010)Ziebart, Bagnell, and Dey]{ziebart2010modeling}
Ziebart, B.~D., Bagnell, J.~A., and Dey, A.~K.
\newblock Modeling interaction via the principle of maximum causal entropy.
\newblock In \emph{International Conference on Machine Learning}, 2010.

\end{thebibliography}
