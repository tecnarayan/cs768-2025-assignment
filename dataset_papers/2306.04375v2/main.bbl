\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{PORPH{\etalchar{+}}21}

\bibitem[ACB17]{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock {Wasserstein Generative Adversarial Networks}.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2017.

\bibitem[AEMM22]{amit2022integral}
Ron Amit, Baruch Epstein, Shay Moran, and Ron Meir.
\newblock {Integral Probability Metrics PAC-Bayes Bounds}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2022.

\bibitem[AG18]{alquier2018simpler}
Pierre Alquier and Benjamin Guedj.
\newblock {Simpler PAC-Bayesian bounds for hostile data}.
\newblock {\em Machine Learning}, 107(5), 2018.

\bibitem[AM18]{amit2018meta}
Ron Amit and Ron Meir.
\newblock {Meta-Learning by Adjusting Priors Based on Extended PAC-Bayes
  Theory}.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2018.

\bibitem[ARC16]{alquier2016properties}
Pierre Alquier, James Ridgway, and Nicolas Chopin.
\newblock {On the properties of variational approximations of {G}ibbs
  posteriors}.
\newblock {\em Journal of Machine Learning Research}, 2016.

\bibitem[BG21]{biggs2020differentiable}
Felix Biggs and Benjamin Guedj.
\newblock {Differentiable PAC-Bayes Objectives with Partially Aggregated Neural
  Networks}.
\newblock {\em Entropy}, 23(10), 2021.

\bibitem[BG22a]{biggs2022vacuous}
Felix Biggs and Benjamin Guedj.
\newblock {Non-Vacuous Generalisation Bounds for Shallow Neural Networks}.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2022.

\bibitem[BG22b]{biggs2021margins}
Felix Biggs and Benjamin Guedj.
\newblock {On Margins and Derandomisation in PAC-Bayes}.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2022.

\bibitem[BG23]{biggs2022tighter}
Felix Biggs and Benjamin Guedj.
\newblock {Tighter PAC-Bayes Generalisation Bounds by Leveraging Example
  Difficulty}.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2023.

\bibitem[BM01]{bartlett2001rademacher}
Peter Bartlett and Shahar Mendelson.
\newblock {Rademacher and Gaussian Complexities: Risk Bounds and Structural
  Results}.
\newblock In {\em Conference on Computational Learning Theory ({COLT})}, 2001.

\bibitem[BM02]{bartlett2002rademacher}
Peter Bartlett and Shahar Mendelson.
\newblock {Rademacher and Gaussian Complexities: Risk Bounds and Structural
  Results}.
\newblock {\em Journal of Machine Learning Research}, 2002.

\bibitem[BT08]{bercu2008exponential}
Bernard Bercu and Abderrahmen Touati.
\newblock {Exponential inequalities for self-normalized martingales with
  applications}.
\newblock {\em The Annals of Applied Probability}, 2008.

\bibitem[BV04]{boyd2004convex}
Stephen Boyd and Lieven Vandenberghe.
\newblock {\em {Convex optimization}}.
\newblock Cambridge University Press, 2004.

\bibitem[BZG22]{biggs2022margins}
Felix Biggs, Valentina Zantedeschi, and Benjamin Guedj.
\newblock {On Margins and Generalisation for Voting Classifiers}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2022.

\bibitem[Cat07]{catoni2007pac}
Olivier Catoni.
\newblock {\em {PAC-Bayesian supervised classification: the thermodynamics of
  statistical learning}}.
\newblock Institute of Mathematical Statistics, 2007.

\bibitem[Cat16]{catoni2016pac}
Olivier Catoni.
\newblock {PAC-Bayesian bounds for the Gram matrix and least squares regression
  with a random design}.
\newblock {\em arXiv}, abs/1603.05229, 2016.

\bibitem[CDE{\etalchar{+}}21]{camuto2021fractal}
Alexander Camuto, George Deligiannidis, Murat~A. Erdogdu, Mert
  G{\"{u}}rb{\"{u}}zbalaban, Umut {\c{S}}im{\c{s}}ekli, and Lingjiong Zhu.
\newblock {Fractal Structure and Generalization Properties of Stochastic
  Optimization Algorithms}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[CG17]{catoni2017dimension}
Olivier Catoni and Ilaria Giulini.
\newblock {Dimension-free PAC-Bayesian bounds for matrices, vectors, and linear
  least squares regression}.
\newblock {\em arXiv}, abs/1712.02747, 2017.

\bibitem[CL21]{chee2021learning}
Andrew Chee and S{\'e}bastien Loustau.
\newblock {Learning with BOT - Bregman and Optimal Transport divergences}.
\newblock \, 2021.

\bibitem[CSDG22]{cherief2021pac}
Badr{-}Eddine Ch{\'{e}}rief{-}Abdellatif, Yuyang Shi, Arnaud Doucet, and
  Benjamin Guedj.
\newblock {On PAC-Bayesian reconstruction guarantees for VAEs}.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2022.

\bibitem[Csi75]{csizar1975divergence}
Imre Csiszár.
\newblock {$I$-Divergence Geometry of Probability Distributions and
  Minimization Problems}.
\newblock {\em The Annals of Probability}, 3(1), 1975.

\bibitem[CWR23]{chugg2023unified}
Ben Chugg, Hongjian Wang, and Aaditya Ramdas.
\newblock {A unified recipe for deriving (time-uniform) {PAC-B}ayes bounds}.
\newblock {\em arXiv}, abs/2302.03421, 2023.

\bibitem[DCL{\etalchar{+}}21]{ding2021bridging}
Nan Ding, Xi~Chen, Tomer Levinboim, Sebastian Goodman, and Radu Soricut.
\newblock {Bridging the Gap Between Practice and {PAC-B}ayes Theory in Few-Shot
  Meta-Learning}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[DG17]{dua2017uci}
Dheeru Dua and Casey Graff.
\newblock {{UCI} Machine Learning Repository}, 2017.

\bibitem[DR17]{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel Roy.
\newblock {Computing Nonvacuous Generalization Bounds for Deep (Stochastic)
  Neural Networks with Many More Parameters than Training Data}.
\newblock In {\em Conference on Uncertainty in Artificial Intelligence (UAI)},
  2017.

\bibitem[DV76]{donsker1976asymptotic}
Monroe~David Donsker and Srinivasa Varadhan.
\newblock {Asymptotic evaluation of certain Markov process expectations for
  large time—{III}}.
\newblock {\em Communications on Pure and Applied Mathematics}, 29(4), 1976.

\bibitem[FM21]{farid2021generalization}
Alec Farid and Anirudha Majumdar.
\newblock {Generalization Bounds for Meta-Learning via PAC-Bayes and Uniform
  Stability}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[FP10]{fard2010pac}
Mahdi~Milani Fard and Joelle Pineau.
\newblock {PAC-Bayesian Model Selection for Reinforcement Learning}.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2010.

\bibitem[FRKP22]{flynn2022pac}
Hamish Flynn, David Reeb, Melih Kandemir, and Jan Peters.
\newblock {PAC-Bayesian lifelong learning for multi-armed bandits}.
\newblock {\em Data Mining and Knowledge Discovery}, 2022.

\bibitem[GBTS21]{rodriguez2021tighter}
Borja~Rodr{\'{i}}guez G{\'{a}}lvez, Germ{\'{a}}n Bassi, Ragnar Thobaben, and
  Mikael Skoglund.
\newblock {Tighter Expected Generalization Error Bounds via Wasserstein
  Distance}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\bibitem[GMGA17]{goyal2017pac}
Anil Goyal, Emilie Morvant, Pascal Germain, and Massih{-}Reza Amini.
\newblock {PAC-Bayesian Analysis for a Two-Step Hierarchical Multiview Learning
  Approach}.
\newblock In {\em Machine Learning and Knowledge Discovery in Databases -
  European Conference (ECML-PKDD)}, 2017.

\bibitem[Gue19]{guedj2019primer}
Benjamin Guedj.
\newblock {A Primer on PAC-Bayesian Learning}.
\newblock {\em arXiv}, abs/1901.05353, 2019.

\bibitem[HG22]{haddouche2022online}
Maxime Haddouche and Benjamin Guedj.
\newblock {Online PAC-Bayes Learning}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2022.

\bibitem[HG23a]{haddouche2023pac}
Maxime Haddouche and Benjamin Guedj.
\newblock {PAC-Bayes Generalisation Bounds for Heavy-Tailed Losses through
  Supermartingales}.
\newblock {\em Transactions on Machine Learning Research}, 2023.

\bibitem[HG23b]{haddouche2023wasserstein}
Maxime Haddouche and Benjamin Guedj.
\newblock {Wasserstein PAC-Bayes Learning: A Bridge Between Generalisation and
  Optimisation}.
\newblock {\em arXiv}, abs/2304.07048, 2023.

\bibitem[HGRS21]{haddouche2021pac}
Maxime Haddouche, Benjamin Guedj, Omar Rivasplata, and John Shawe{-}Taylor.
\newblock {PAC-Bayes Unleashed: Generalisation Bounds with Unbounded Losses}.
\newblock {\em Entropy}, 23(10), 2021.

\bibitem[JJKO23]{jang2023tighter}
Kyoungseok Jang, Kwang{-}Sung Jun, Ilja Kuzborskij, and Francesco Orabona.
\newblock {Tighter PAC-Bayes Bounds Through Coin-Betting}.
\newblock In {\em Conference on Learning Theory (COLT)}, 2023.

\bibitem[Kan60]{kantorovich1960mathematical}
Leonid~Vitalievitch Kantorovitch.
\newblock {Mathematical Methods of Organizing and Planning Production}.
\newblock {\em Management Science}, 1960.

\bibitem[KDY{\etalchar{+}}22]{kervadec2022constrained}
Hoel Kervadec, Jose Dolz, Jing Yuan, Christian Desrosiers, Eric Granger, and
  Ismail~Ben Ayed.
\newblock {Constrained deep networks: Lagrangian optimization via log-barrier
  extensions}.
\newblock In {\em European Signal Processing Conference (EUSIPCO)}, 2022.

\bibitem[KP00]{koltchinskii2000rademacher}
Vladimir Koltchinskii and Dmitriy Panchenko.
\newblock {Rademacher processes and bounding the risk of function learning}.
\newblock In {\em High dimensional probability II}, 2000.

\bibitem[LeC98]{lecun1998mnist}
Yann LeCun.
\newblock {The MNIST database of handwritten digits}, 1998.

\bibitem[LFK{\etalchar{+}}22]{lotfi2022pac}
Sanae Lotfi, Marc Finzi, Sanyam Kapoor, Andres Potapczynski, Micah Goldblum,
  and Andrew Wilson.
\newblock {PAC-Bayes Compression Bounds So Tight That They Can Explain
  Generalization}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2022.

\bibitem[LGGL19]{letarte2019dichotomize}
Ga{\"{e}}l Letarte, Pascal Germain, Benjamin Guedj, and Fran{\c{c}}ois
  Laviolette.
\newblock {Dichotomize and Generalize: PAC-Bayesian Binary Activated Deep
  Neural Networks}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem[LN22]{lugosi2022generalization}
G{\'{a}}bor Lugosi and Gergely Neu.
\newblock {Generalization Bounds via Convex Analysis}.
\newblock In {\em Conference on Learning Theory (COLT)}, 2022.

\bibitem[LWHZ19]{lu2019optimal}
Shiyin Lu, Guanghui Wang, Yao Hu, and Lijun Zhang.
\newblock {Optimal Algorithms for Lipschitz Bandits with Heavy-tailed Rewards}.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2019.

\bibitem[LZ11]{liu2011multi}
Keqin Liu and Qing Zhao.
\newblock {Multi-Armed Bandit Problems with Heavy Tail Reward Distributions}.
\newblock {\em Allerton Conference on Communication, Control, and Computing},
  2011.

\bibitem[Mau04]{maurer2004note}
Andreas Maurer.
\newblock {A note on the PAC-Bayesian theorem}.
\newblock {\em arXiv}, cs/0411099, 2004.

\bibitem[MGG19]{mhammedi2019pac}
Zakaria Mhammedi, Peter Gr{\"{u}}nwald, and Benjamin Guedj.
\newblock {PAC-Bayes Un-Expected Bernstein Inequality}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem[MGW20]{mhammedi2020pac}
Zakaria Mhammedi, Benjamin Guedj, and Robert Williamson.
\newblock {PAC-Bayesian Bound for the Conditional Value at Risk}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem[Mon81]{monge1781memoire}
Gaspard Monge.
\newblock {M{\'e}moire sur la th{\'e}orie des d{\'e}blais et des remblais}.
\newblock {\em Histoire de l’Académie Royale des Sciences de Paris}, 1781.

\bibitem[Mü97]{muller1997integral}
Alfred Müller.
\newblock {Integral Probability Metrics and Their Generating Classes of
  Functions}.
\newblock {\em Advances in Applied Probability}, 29(2), 1997.

\bibitem[NGG20]{nozawa2019pac}
Kento Nozawa, Pascal Germain, and Benjamin Guedj.
\newblock {PAC-Bayesian Contrastive Unsupervised Representation Learning}.
\newblock In {\em Conference on Uncertainty in Artificial Intelligence (UAI)},
  2020.

\bibitem[OH21]{ohnishi2021novel}
Yuki Ohnishi and Jean Honorio.
\newblock {Novel Change of Measure Inequalities with Applications to
  PAC-Bayesian Bounds and Monte Carlo Estimation}.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2021.

\bibitem[OT17]{orabona2017training}
Francesco Orabona and Tatiana Tommasi.
\newblock {Training Deep Networks without Learning Rates Through Coin Betting}.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[PC19]{peyre2019computational}
Gabriel Peyr{\'{e}} and Marco Cuturi.
\newblock {Computational Optimal Transport}.
\newblock {\em Foundations and Trends in Machine Learning}, 11(5-6), 2019.

\bibitem[PORPH{\etalchar{+}}21]{perez2021progress}
Maria Perez-Ortiz, Omar Rivasplata, Emilio Parrado-Hernandez, Benjamin Guedj,
  and John Shawe-Taylor.
\newblock {Progress in Self-Certified Neural Networks}.
\newblock In {\em {NeurIPS 2021 Workshop on Bayesian Deep Learning}}, 2021.

\bibitem[PRSS21]{perez2021tighter}
Mar{\'{\i}}a P{\'{e}}rez{-}Ortiz, Omar Rivasplata, John Shawe{-}Taylor, and
  Csaba Szepesv{\'{a}}ri.
\newblock Tighter risk certificates for neural networks.
\newblock {\em Journal of Machine Learning Research}, 22, 2021.

\bibitem[PWG22]{picard2022change}
Antoine Picard-Weibel and Benjamin Guedj.
\newblock {On change of measure inequalities for $f$-divergences}.
\newblock {\em arXiv}, abs/2202.05568, 2022.

\bibitem[RACA23]{riou2023bayes}
Charles Riou, Pierre Alquier, and Badr-Eddine Ch{\'e}rief-Abdellatif.
\newblock {Bayes meets Bernstein at the Meta Level: an Analysis of Fast Rates
  in Meta-Learning with PAC-Bayes}.
\newblock {\em arXiv}, abs/2302.11709, 2023.

\bibitem[RFJK21]{rothfuss2021pacoh}
Jonas Rothfuss, Vincent Fortuin, Martin Josifoski, and Andreas Krause.
\newblock {PACOH: Bayes-optimal meta-learning with PAC-guarantees}.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2021.

\bibitem[RJFK22]{rothfuss2022pac}
Jonas Rothfuss, Martin Josifoski, Vincent Fortuin, and Andreas Krause.
\newblock {PAC-Bayesian Meta-Learning: From Theory to Practice}.
\newblock {\em arXiv}, abs/2211.07206, 2022.

\bibitem[RZ20]{russo2020how}
Daniel Russo and James Zou.
\newblock {How Much Does Your Data Exploration Overfit? Controlling Bias via
  Information Usage}.
\newblock {\em {IEEE} Transactions on Information Theory}, 66(1), 2020.

\bibitem[SAC23]{sakhi2022pac}
Otmane Sakhi, Pierre Alquier, and Nicolas Chopin.
\newblock {PAC-Bayesian Offline Contextual Bandits With Guarantees}.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2023.

\bibitem[SLCB{\etalchar{+}}12]{seldin2012pac}
Yevgeny Seldin, Fran{\c{c}}ois Laviolette, Nicolò Cesa-Bianchi, John
  Shawe-Taylor, and Peter Auer.
\newblock {PAC-Bayesian Inequalities for Martingales}.
\newblock {\em IEEE Transactions on Information Theory}, 58(12), 2012.

\bibitem[Sli19]{slivkins19intro}
Aleksandrs Slivkins.
\newblock {Introduction to Multi-Armed Bandits}.
\newblock {\em Foundations and Trends in Machine Learning}, 2019.

\bibitem[SLST{\etalchar{+}}11]{seldin2011pac}
Yevgeny Seldin, Fran{\c{c}}ois Laviolette, John Shawe-Taylor, Jan Peters, and
  Peter Auer.
\newblock {PAC-Bayesian Analysis of Martingales and Multiarmed Bandits}.
\newblock {\em arXiv}, abs/1105.2416, 2011.

\bibitem[VC68]{vapnik1968uniform}
Vladimir Vapnik and Alexey Chervonenkis.
\newblock {On the uniform convergence of relative frequencies of events to
  their probabilities}.
\newblock In {\em Doklady Akademii Nauk USSR}, 1968.

\bibitem[VC74]{vapnik1974theory}
Vladimir Vapnik and Alexey Chervonenkis.
\newblock {Theory of pattern recognition}, 1974.

\bibitem[Vil09]{villani2009optimal}
C{\'e}dric Villani.
\newblock {\em {Optimal transport: old and new}}.
\newblock Number 338 in Grundlehren der mathematischen {Wissenschaften}.
  Springer, 2009.

\bibitem[WDFC19]{wang2019information}
Hao Wang, Mario D{\'{i}}az, Jos{\'{e}} C{\^{a}}ndido Silveira~Santos Filho, and
  Fl{\'{a}}vio~P. Calmon.
\newblock {An Information-Theoretic View of Generalization via Wasserstein
  Distance}.
\newblock In {\em {IEEE} International Symposium on Information Theory
  ({ISIT})}, 2019.

\bibitem[XR17]{xu2017information}
Aolin Xu and Maxim Raginsky.
\newblock {Information-theoretic analysis of generalization capability of
  learning algorithms}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2017.

\bibitem[XRV17]{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine
  Learning Algorithms}, 2017.

\bibitem[Zin03]{zinkevich2003online}
Martin Zinkevich.
\newblock {Online Convex Programming and Generalized Infinitesimal Gradient
  Ascent}.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2003.

\bibitem[ZLT18]{zhang2018optimal}
Jingwei Zhang, Tongliang Liu, and Dacheng Tao.
\newblock {An Optimal Transport View on Generalization}.
\newblock {\em arXiv}, abs/1811.03270, 2018.

\bibitem[ZS21]{zhuang2021regret}
Vincent Zhuang and Yanan Sui.
\newblock {No-Regret Reinforcement Learning with Heavy-Tailed Rewards}.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2021.

\bibitem[ZVM{\etalchar{+}}21]{zantedeschi2021learning}
Valentina Zantedeschi, Paul Viallard, Emilie Morvant, R\'{e}mi Emonet, Amaury
  Habrard, Pascal Germain, and Benjamin Guedj.
\newblock {Learning Stochastic Majority Votes by Minimizing a PAC-Bayes
  Generalization Bound}.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2021.

\end{thebibliography}
