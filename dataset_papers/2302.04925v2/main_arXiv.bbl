\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aldous(1983)]{aldous1983random}
D.~Aldous.
\newblock Random walks on finite groups and rapidly mixing markov chains.
\newblock In \emph{S{\'e}minaire de Probabilit{\'e}s XVII 1981/82}, pages
  243--297. Springer, 1983.

\bibitem[Alon et~al.(1997)Alon, Ben-David, Cesa-Bianchi, and
  Haussler]{alon1997scale}
N.~Alon, S.~Ben-David, N.~Cesa-Bianchi, and D.~Haussler.
\newblock Scale-sensitive dimensions, uniform convergence, and learnability.
\newblock \emph{Journal of the ACM (JACM)}, 44\penalty0 (4):\penalty0 615--631,
  1997.

\bibitem[Aminian et~al.(2021)Aminian, Bu, Toni, Rodrigues, and
  Wornell]{aminian2021exact}
G.~Aminian, Y.~Bu, L.~Toni, M.~Rodrigues, and G.~Wornell.
\newblock An exact characterization of the generalization error for the gibbs
  algorithm.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 8106--8118, 2021.

\bibitem[Amir et~al.(2021{\natexlab{a}})Amir, Carmon, Koren, and
  Livni]{amir2021never}
I.~Amir, Y.~Carmon, T.~Koren, and R.~Livni.
\newblock Never go full batch (in stochastic convex optimization).
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 25033--25043, 2021{\natexlab{a}}.

\bibitem[Amir et~al.(2021{\natexlab{b}})Amir, Koren, and Livni]{amir2021sgd}
I.~Amir, T.~Koren, and R.~Livni.
\newblock Sgd generalizes better than gd (and regularization doesnâ€™t help).
\newblock In \emph{Conference on Learning Theory}, pages 63--92. PMLR,
  2021{\natexlab{b}}.

\bibitem[Bassily et~al.(2018)Bassily, Moran, Nachum, Shafer, and
  Yehudayoff]{bassily2018learners}
R.~Bassily, S.~Moran, I.~Nachum, J.~Shafer, and A.~Yehudayoff.
\newblock Learners that use little information.
\newblock In \emph{Algorithmic Learning Theory}, pages 25--55. PMLR, 2018.

\bibitem[Bassily et~al.(2020)Bassily, Feldman, Guzm{\'a}n, and
  Talwar]{bassily2020stability}
R.~Bassily, V.~Feldman, C.~Guzm{\'a}n, and K.~Talwar.
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4381--4391, 2020.

\bibitem[Blumer et~al.(1989)Blumer, Ehrenfeucht, Haussler, and
  Warmuth]{blumer1989learnability}
A.~Blumer, A.~Ehrenfeucht, D.~Haussler, and M.~K. Warmuth.
\newblock Learnability and the vapnik-chervonenkis dimension.
\newblock \emph{Journal of the ACM (JACM)}, 36\penalty0 (4):\penalty0 929--965,
  1989.

\bibitem[Boneh and Shaw(1998)]{boneh1998collusion}
D.~Boneh and J.~Shaw.
\newblock Collusion-secure fingerprinting for digital data.
\newblock \emph{IEEE Transactions on Information Theory}, 44\penalty0
  (5):\penalty0 1897--1905, 1998.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
O.~Bousquet and A.~Elisseeff.
\newblock Stability and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 2:\penalty0
  499--526, 2002.

\bibitem[Bu et~al.(2020)Bu, Zou, and Veeravalli]{bu2020tightening}
Y.~Bu, S.~Zou, and V.~V. Veeravalli.
\newblock Tightening mutual information-based bounds on generalization error.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 121--130, 2020.

\bibitem[Bun et~al.(2014)Bun, Ullman, and Vadhan]{bun2014fingerprinting}
M.~Bun, J.~Ullman, and S.~Vadhan.
\newblock Fingerprinting codes and the price of approximate differential
  privacy.
\newblock In \emph{Proceedings of the forty-sixth annual ACM symposium on
  Theory of computing}, pages 1--10, 2014.

\bibitem[Bun et~al.(2020)Bun, Livni, and Moran]{bun2020equivalence}
M.~Bun, R.~Livni, and S.~Moran.
\newblock An equivalence between private classification and online prediction.
\newblock In \emph{2020 IEEE 61st Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 389--402. IEEE, 2020.

\bibitem[Carlini et~al.(2021)Carlini, Tramer, Wallace, Jagielski, Herbert-Voss,
  Lee, Roberts, Brown, Song, Erlingsson, et~al.]{carlini2021extracting}
N.~Carlini, F.~Tramer, E.~Wallace, M.~Jagielski, A.~Herbert-Voss, K.~Lee,
  A.~Roberts, T.~Brown, D.~Song, U.~Erlingsson, et~al.
\newblock Extracting training data from large language models.
\newblock In \emph{30th USENIX Security Symposium (USENIX Security 21)}, pages
  2633--2650, 2021.

\bibitem[Duchi(2016)]{duchi2016lecture}
J.~Duchi.
\newblock Lecture notes for statistics 311/electrical engineering 377.
\newblock \emph{URL: https://stanford. edu/class/stats311/Lectures/full notes.
  pdf. Last visited on}, 2:\penalty0 23, 2016.

\bibitem[Feldman(2016)]{feldman2016generalization}
V.~Feldman.
\newblock Generalization of erm in stochastic convex optimization: The
  dimension strikes back.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Feldman(2020)]{feldman2020does}
V.~Feldman.
\newblock Does learning require memorization? a short tale about a long tail.
\newblock In \emph{Proceedings of the 52nd Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 954--959, 2020.

\bibitem[Haghifam et~al.(2020)Haghifam, Negrea, Khisti, Roy, and
  Dziugaite]{haghifam2020sharpened}
M.~Haghifam, J.~Negrea, A.~Khisti, D.~M. Roy, and G.~K. Dziugaite.
\newblock Sharpened generalization bounds based on conditional mutual
  information and an application to noisy, iterative algorithms.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9925--9935, 2020.

\bibitem[Haghifam et~al.(2022)Haghifam, Rodr{\'\i}guez-G{\'a}lvez, Thobaben,
  Skoglund, Roy, and Dziugaite]{haghifam2022limitations}
M.~Haghifam, B.~Rodr{\'\i}guez-G{\'a}lvez, R.~Thobaben, M.~Skoglund, D.~M. Roy,
  and G.~K. Dziugaite.
\newblock Limitations of information-theoretic generalization bounds for
  gradient descent methods in stochastic convex optimization.
\newblock \emph{arXiv preprint arXiv:2212.13556}, 2022.

\bibitem[Hazan et~al.(2016)]{hazan2016introduction}
E.~Hazan et~al.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Optimization},
  2\penalty0 (3-4):\penalty0 157--325, 2016.

\bibitem[Kamath et~al.(2019)Kamath, Li, Singhal, and
  Ullman]{kamath2019privately}
G.~Kamath, J.~Li, V.~Singhal, and J.~Ullman.
\newblock Privately learning high-dimensional distributions.
\newblock In \emph{Conference on Learning Theory}, pages 1853--1902. PMLR,
  2019.

\bibitem[Langford and Shawe-Taylor(2002)]{langford2002pac}
J.~Langford and J.~Shawe-Taylor.
\newblock Pac-bayes \& margins.
\newblock \emph{Advances in neural information processing systems}, 15, 2002.

\bibitem[Livni and Moran(2020)]{livni2020limitation}
R.~Livni and S.~Moran.
\newblock A limitation of the pac-bayes framework.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 20543--20553, 2020.

\bibitem[McAllester(1998)]{mcallester1998some}
D.~A. McAllester.
\newblock Some pac-bayesian theorems.
\newblock In \emph{Proceedings of the eleventh annual conference on
  Computational learning theory}, pages 230--234, 1998.

\bibitem[McAllester(1999)]{mcallester1999pac}
D.~A. McAllester.
\newblock Pac-bayesian model averaging.
\newblock In \emph{Proceedings of the twelfth annual conference on
  Computational learning theory}, pages 164--170, 1999.

\bibitem[Negrea et~al.(2019)Negrea, Haghifam, Dziugaite, Khisti, and
  Roy]{negrea2019information}
J.~Negrea, M.~Haghifam, G.~K. Dziugaite, A.~Khisti, and D.~M. Roy.
\newblock Information-theoretic generalization bounds for sgld via
  data-dependent estimates.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Neu et~al.(2021)Neu, Dziugaite, Haghifam, and Roy]{neu2021information}
G.~Neu, G.~K. Dziugaite, M.~Haghifam, and D.~M. Roy.
\newblock Information-theoretic generalization bounds for stochastic gradient
  descent.
\newblock In \emph{Conference on Learning Theory}, pages 3526--3545. PMLR,
  2021.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and
  Srebro]{neyshabur2014search}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Paley and Zygmund(1932)]{paley1932note}
R.~Paley and A.~Zygmund.
\newblock A note on analytic functions in the unit circle.
\newblock In \emph{Mathematical Proceedings of the Cambridge Philosophical
  Society}, volume~28, pages 266--272. Cambridge University Press, 1932.

\bibitem[Pensia et~al.(2018)Pensia, Jog, and Loh]{pensia2018generalization}
A.~Pensia, V.~Jog, and P.-L. Loh.
\newblock Generalization error bounds for noisy, iterative algorithms.
\newblock In \emph{2018 IEEE International Symposium on Information Theory
  (ISIT)}, pages 546--550. IEEE, 2018.

\bibitem[Pradeep et~al.(2022)Pradeep, Nachum, and Gastpar]{pradeep2022finite}
A.~Pradeep, I.~Nachum, and M.~Gastpar.
\newblock Finite littlestone dimension implies finite information complexity.
\newblock In \emph{2022 IEEE International Symposium on Information Theory
  (ISIT)}, pages 3055--3060. IEEE, 2022.

\bibitem[Rigollet and H{\"u}tter(2015)]{rigollet2015high}
P.~Rigollet and J.-C. H{\"u}tter.
\newblock High dimensional statistics.
\newblock \emph{Lecture notes for course 18S997}, 813\penalty0 (814):\penalty0
  46, 2015.

\bibitem[Rodr{\'\i}guez-G{\'a}lvez et~al.(2021)Rodr{\'\i}guez-G{\'a}lvez,
  Bassi, Thobaben, and Skoglund]{rodriguez2021random}
B.~Rodr{\'\i}guez-G{\'a}lvez, G.~Bassi, R.~Thobaben, and M.~Skoglund.
\newblock On random subset generalization error bounds and the stochastic
  gradient langevin dynamics algorithm.
\newblock In \emph{2020 IEEE Information Theory Workshop (ITW)}, pages 1--5.
  IEEE, 2021.

\bibitem[Russo and Zou(2019)]{russo2019much}
D.~Russo and J.~Zou.
\newblock How much does your data exploration overfit? controlling bias via
  information usage.
\newblock \emph{IEEE Transactions on Information Theory}, 66\penalty0
  (1):\penalty0 302--323, 2019.

\bibitem[Schapire(1990)]{schapire1990strength}
R.~E. Schapire.
\newblock The strength of weak learnability.
\newblock \emph{Machine learning}, 5\penalty0 (2):\penalty0 197--227, 1990.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalev2014understanding}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Shalev-Shwartz et~al.(2009)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{shalev2009stochastic}
S.~Shalev-Shwartz, O.~Shamir, N.~Srebro, and K.~Sridharan.
\newblock Stochastic convex optimization.
\newblock In \emph{COLT}, volume~2, page~5, 2009.

\bibitem[Sridharan et~al.(2008)Sridharan, Shalev-Shwartz, and
  Srebro]{sridharan2008fast}
K.~Sridharan, S.~Shalev-Shwartz, and N.~Srebro.
\newblock Fast rates for regularized objectives.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem[Steinke and Ullman(2015)]{steinke2015interactive}
T.~Steinke and J.~Ullman.
\newblock Interactive fingerprinting codes and the hardness of preventing false
  discovery.
\newblock In \emph{Conference on learning theory}, pages 1588--1628. PMLR,
  2015.

\bibitem[Steinke and Zakynthinou(2020)]{steinke2020reasoning}
T.~Steinke and L.~Zakynthinou.
\newblock Reasoning about generalization via conditional mutual information.
\newblock In \emph{Conference on Learning Theory}, pages 3437--3452. PMLR,
  2020.

\bibitem[Tardos(2008)]{tardos2008optimal}
G.~Tardos.
\newblock Optimal probabilistic fingerprint codes.
\newblock \emph{Journal of the ACM (JACM)}, 55\penalty0 (2):\penalty0 1--24,
  2008.

\bibitem[Valiant(1984)]{valiant1984theory}
L.~G. Valiant.
\newblock A theory of the learnable.
\newblock \emph{Communications of the ACM}, 27\penalty0 (11):\penalty0
  1134--1142, 1984.

\bibitem[Vapnik and Chervonenkis(2015)]{vapnik2015uniform}
V.~N. Vapnik and A.~Y. Chervonenkis.
\newblock On the uniform convergence of relative frequencies of events to their
  probabilities.
\newblock In \emph{Measures of complexity}, pages 11--30. Springer, 2015.

\bibitem[Wu(2017)]{wu2017lecture}
Y.~Wu.
\newblock Lecture notes on information-theoretic methods for high-dimensional
  statistics.
\newblock \emph{Lecture Notes for ECE598YW (UIUC)}, 16, 2017.

\bibitem[Xu and Raginsky(2017)]{xu2017information}
A.~Xu and M.~Raginsky.
\newblock Information-theoretic analysis of generalization capability of
  learning algorithms.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021.

\bibitem[Zhou et~al.(2022)Zhou, Tian, and Liu]{zhou2022individually}
R.~Zhou, C.~Tian, and T.~Liu.
\newblock Individually conditional individual mutual information bound on
  generalization error.
\newblock \emph{IEEE Transactions on Information Theory}, 68\penalty0
  (5):\penalty0 3304--3316, 2022.

\end{thebibliography}
