% Stochastic Optimisation

% Deterministic Coordinate Descent

@article{Tseng2001,
author = {Tseng, P},
doi = {10.1023/A:1017501703105},
issn = {00223239},
journal = {Journal of Optimization Theory and Applications},
keywords = {Block coordinate descent,Convergence,Gauss-seidel method,Nondifferentiable minimization,Pseudoconvex functions,Quasiconvex functions,Stationary point},
mendeley-groups = {Stochastic Optimisation},
number = {3},
pages = {475--494},
title = {{Convergence of a block coordinate descent method for nondifferentiable minimization}},
volume = {109},
year = {2001}
}


@article{Luo1992,
abstract = {The coordinate descent method enjoys a long history in convex differentiable minimization. Surprisingly, very little is known about the convergence of the iterates generated by this method. Convergence typically requires restrictive assumptions such as that the cost function has bounded level sets and is in some sense strictly convex. In a recent work, Luo and Tseng showed that the iterates are convergent for the symmetric monotone linear complementarity problem, for which the cost function is convex quadratic, but not necessarily strictly convex, and does not necessarily have bounded level sets. In this paper, we extend these results to problems for which the cost function is the composition of an affine mapping with a strictly convex function which is twice differentiable in its effective domain. In addition, we show that the convergence is at least linear. As a consequence of this result, we obtain, for the first time, that the dual iterates generated by a number of existing methods for matrix balancing and entropy optimization are linearly convergent.},
author = {Luo, Z Q and Tseng, P},
journal = {Journal of Optimization Theory and Applications},
keywords = {Coordinate descent,convex differentiable optimization,symmetric linear complementarity problems},
number = {1},
pages = {7--35},
title = {{On the Convergence of the Coordinate Descent Method for Convex Differentiable Minimization}},
volume = {72},
year = {1992}
}


@article{Saha2013,
abstract = {Cyclic coordinate descent is a classic optimization method that has witnessed a resurgence of interest in machine learning. Reasons for this include its simplicity, speed and stability, as well as its competitive performance on {\$}\backslashell{\_}1{\$} regularized smooth optimization problems. Surprisingly, very little is known about its finite time convergence behavior on these problems. Most existing results either just prove convergence or provide asymptotic rates. We fill this gap in the literature by proving {\$}O(1/k){\$} convergence rates (where {\$}k{\$} is the iteration counter) for two variants of cyclic coordinate descent under an isotonicity assumption. Our analysis proceeds by comparing the objective values attained by the two variants with each other, as well as with the gradient descent algorithm. We show that the iterates generated by the cyclic coordinate descent methods remain better than those of gradient descent uniformly over time.},
author = {Saha, Ankan and Tewari, Ambuj},
doi = {10.1137/110840054},
issn = {10526234},
journal = {SIAM Journal on Optimization},
keywords = {Convergence rates,Convex optimization,Cyclic coordinate descent,Sparsity},
number = {1},
pages = {576--601},
title = {{On the finite time convergence of cyclic coordinate descent methods}},
volume = {23},
year = {2013}
}


@article{Hsieh2008a,
abstract = {Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classification and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while fixing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efficient and stable than state of the art methods such as Pegasos and TRON.},
author = {Hsieh, Cho-Jui and Chang, Kai-Wei and Lin, Chih-Jen and Keerthi, Sathiya S. and Sundararajan, S.},
journal = {Journal of Machine Learning Research},
keywords = {coordinate descent,document classification,linear support vector machines},
pages = {1369--1398},
title = {{Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines}},
volume = {9},
year = {2008}
}


@inproceedings{Gurbuzbalaban2017,
abstract = {The coordinate descent (CD) method is a classical optimization algorithm that has seen a revival of interest because of its competitive performance in machine learning applications. A number of recent papers provided convergence rate estimates for their deterministic (cyclic) and randomized variants that differ in the selection of update coordinates. These estimates suggest randomized coordinate descent (RCD) performs better than cyclic coordinate descent (CCD), although numerical experiments do not provide clear justification for this comparison. In this paper, we provide examples and more generally problem classes for which CCD (or CD with any deterministic order) is faster than RCD in terms of asymptotic worst-case convergence. Furthermore, we provide lower and upper bounds on the amount of improvement on the rate of CCD relative to RCD, which depends on the deterministic order used. We also provide a characterization of the best deterministic order (that leads to the maximum improvement in convergence rate) in terms of the combinatorial properties of the Hessian matrix of the objective function.},
author = {G{\"{u}}rb{\"{u}}zbalaban, Mert and Ozdaglar, Asuman and Parrilo, Pablo A and Vanli, N. Denizcan},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/sudeep/Library/Application Support/Mendeley Desktop/Downloaded/G{\"{u}}rb{\"{u}}zbalaban et al. - Unknown - When Cyclic Coordinate Descent Outperforms Randomized Coordinate Descent.pdf:pdf},
issn = {10495258},
pages = {7000--7008},
title = {{When cyclic coordinate descent outperforms randomized coordinate descent}},
volume = {2017-Decem},
year = {2017}
}



@article{Friedman2007,
abstract = {We consider ``one-at-a-time'' coordinate-wise descent algorithms for a class of convex optimization problems. An algorithm of this kind has been proposed for the {\$}L{\_}1{\$}-penalized regression (lasso) in the literature, but it seems to have been largely ignored. Indeed, it seems that coordinate-wise algorithms are not often used in convex optimization. We show that this algorithm is very competitive with the well-known LARS (or homotopy) procedure in large lasso problems, and that it can be applied to related methods such as the garotte and elastic net. It turns out that coordinate-wise descent does not work in the ``fused lasso,'' however, so we derive a generalized algorithm that yields the solution in much less time that a standard convex optimizer. Finally, we generalize the procedure to the two-dimensional fused lasso, and demonstrate its performance on some image smoothing problems.},
archivePrefix = {arXiv},
arxivId = {0708.1485v2},
author = {Friedman, Jerome and Hastie, Trevor and H{\"{o}}fling, Holger and Tibshirani, Robert},
doi = {10.1214/07-aoas131},
issn = {1932-6157},
journal = {The Annals of Applied Statistics},
number = {2},
pages = {302--332},
title = {{Pathwise coordinate optimization}},
volume = {1},
year = {2007}
}


@inproceedings{Hsieh2008b,
abstract = {In many applications, data appear with a huge number of instances as well as features. Linear Support Vector Machines (SVM) is one of the most popular tools to deal with such large-scale sparse data. This paper presents a novel dual coordinate descent method for linear SVM with L1-and L2-loss functions. The proposed method is simple and reaches an $\epsilon$-accurate solution in O(log(1/$\epsilon$)) iterations. Experiments indicate that our method is much faster than state of the art solvers such as Pegasos, TRON, SVMperf, and a recent primal coordinate descent implementation.},
author = {Hsieh, Cho Jui and Chang, Kai Wei and Lin, Chih Jen and Keerthi, S Sathiya and Sundararajan, S},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
file = {:Users/sudeep/Library/Application Support/Mendeley Desktop/Downloaded/Hsieh et al. - Unknown - A Dual Coordinate Descent Method for Large-scale Linear SVM.pdf:pdf},
isbn = {9781605582054},
pages = {408--415},
title = {{A dual coordinate descent method for large-scale linear SVM}},
year = {2008}
}



@article{Tseng2009,
abstract = {We consider the problem of minimizing the sum of a smooth function and a separable convex function. This problem includes as special cases bound-constrained optimization and smooth optimization with l 1-regularization. We propose a (block) coordinate gradient descent method for solving this class of nonsmooth separable problems. We establish global convergence and, under a local Lipschitzian error bound assumption, linear convergence for this method. The local Lipschitzian error bound holds under assumptions analogous to those for constrained smooth optimization, e.g., the convex function is polyhedral and the smooth function is (nonconvex) quadratic or is the composition of a strongly convex function with a linear mapping. We report numerical experience with solving the l1- regularization of unconstrained optimization problems from Mor{\'{e}} et al. in ACM Trans. Math. Softw. 7, 17-41, 1981 and from the CUTEr set (Gould and Orban in ACM Trans. Math. Softw. 29, 373-394, 2003). Comparison with L-BFGS-B and MINOS, applied to a reformulation of the l1-regularized problem as a bound-constrained optimization problem, is also reported. {\textcopyright} 2007 Springer-Verlag.},
author = {Tseng, Paul and Yun, Sangwoon},
doi = {10.1007/s10107-007-0170-0},
isbn = {1010700701700},
issn = {00255610},
journal = {Mathematical Programming},
keywords = {Coordinate descent,Error bound,Global convergence,Linear convergence rate,Nonsmooth optimization},
number = {1-2},
pages = {387--423},
title = {{A coordinate gradient descent method for nonsmooth separable minimization}},
volume = {117},
year = {2009}
}



@Article{Tseng2008,
author="Tseng, P.
and Yun, S.",
title="Block-Coordinate Gradient Descent Method for Linearly Constrained Nonsmooth Separable Optimization",
journal="Journal of Optimization Theory and Applications",
year="2008",
month="Sep",
day="30",
volume="140",
number="3",
pages="513",
abstract="We consider the problem of minimizing the weighted sum of a smooth function f and a convex function P of n real variables subject to m linear equality constraints. We propose a block-coordinate gradient descent method for solving this problem, with the coordinate block chosen by a Gauss-Southwell-q rule based on sufficient predicted descent. We establish global convergence to first-order stationarity for this method and, under a local error bound assumption, linear rate of convergence. If f is convex with Lipschitz continuous gradient, then the method terminates in O(n2/$\epsilon$) iterations with an $\epsilon$-optimal solution. If P is separable, then the Gauss-Southwell-q rule is implementable in O(n) operations when m=1 and in O(n2) operations when m>1. In the special case of support vector machines training, for which f is convex quadratic, P is separable, and m=1, this complexity bound is comparable to the best known bound for decomposition methods. If f is convex, then, by gradually reducing the weight on P to zero, the method can be adapted to solve the bilevel problem of minimizing P over the set of minima of f+$\delta$X, where X denotes the closure of the feasible set. This has application in the least 1-norm solution of maximum-likelihood estimation.",
issn="1573-2878",
doi="10.1007/s10957-008-9458-3"
}




% Randomised Coordinate Descent


@inproceedings{Deng2013,
author = {Deng, Q. and Ho, Jeffrey and Rangarajan, Anand},
year = {2013},
month = {12},
pages = {},
title = {Stochastic Coordinate Descent for Nonsmooth Convex Optimization}
}

@article{Ma2016,
abstract = {In this paper we generalize the framework of the Feasible Descent Method (FDM) to a Randomized (R-FDM) and a Randomized Coordinate-wise Feasible Descent Method (RCFDM) framework. We show that many machine learning algorithms, including the famous SDCA algorithm for optimizing the SVM dual problem, or the stochastic coordinate descent method for the LASSO problem, fits into the framework of RC-FDM. We prove linear convergence for both R-FDM and RC-FDM under the weak strong convexity assumption. Moreover, we show that the duality gap converges linearly for RC-FDM, which implies that the duality gap also converges linearly for SDCA applied to the SVM dual problem.},
author = {Ma, Chenxin and Tappenden, Rachael and Takac, Martin},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Convergence theory,Feasible descent method,Iteration complexity,Stochastic methods,Weak strong convexity},
number = {228},
pages = {1--24},
title = {{Linear convergence of randomized feasible descent methods under the weak strong convexity assumption}},
volume = {17},
year = {2016}
}


@inproceedings{Karimi2016,
abstract = {In 1963, Polyak proposed a simple condition that is sufficient to show a global linear convergence rate for gradient descent. This condition is a special case of the {\L}ojasiewicz inequality proposed in the same year, and it does not require strong convexity (or even convexity). In this work, we show that this much-older Polyak-{\L}ojasiewicz (PL) inequality is actually weaker than the main conditions that have been explored to show linear convergence rates without strong convexity over the last 25 years. We also use the PL inequality to give new analyses of coordinate descent and stochastic gradient for many non-strongly-convex (and some non-convex) functions. We further propose a generalization that applies to proximal-gradient methods for non-smooth optimization, leading to simple proofs of linear convergence for support vector machines and L1- regularized least squares without additional assumptions.},
archivePrefix = {arXiv},
arxivId = {1608.04636},
author = {Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46128-1_50},
isbn = {9783319461274},
issn = {16113349},
keywords = {Boosting,Coordinate descent,Gradient descent,L1-regularization,Stochastic gradient,Support vector machines,Variance-reduction},
pages = {795--811},
title = {{Linear convergence of gradient and proximal-gradient methods under the Polyak-{\L}ojasiewicz condition}},
volume = {9851 LNAI},
year = {2016}
}



@article{Richtarik2014,
abstract = {In this paper we develop a randomized block-coordinate descent method for minimizing the sum of a smooth and a simple nonsmooth block-separable convex function and prove that it obtains an e-Accurate solution with probability at least 1-p in at most O((n/e) log (1/p)) iterations, where n is the number of blocks. This extends recent results of Nesterov (SIAM J Optim 22(2): 341-362, 2012), which cover the smooth case, to composite minimization, while at the same time improving the complexity by the factor of 4 and removing efrom the logarithmic term. More importantly, in contrast with the aforementioned work in which the author achieves the results by applying the method to a regularized version of the objective function with an unknown scaling factor, we show that this is not necessary, thus achieving first true iteration complexity bounds. For strongly convex functions the method converges linearly. In the smooth case we also allow for arbitrary probability vectors and non-Euclidean norms. Finally, we demonstrate numerically that the algorithm is able to solve huge-scale l1-regularized least squares problems with a billion variables. {\textcopyright} 2012 Springer-Verlag Berlin Heidelberg and Mathematical Optimization Society.},
archivePrefix = {arXiv},
arxivId = {1107.2848},
author = {Richt{\'{a}}rik, Peter and Tak{\'{a}}{\v{c}}, Martin},
doi = {10.1007/s10107-012-0614-z},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {Block coordinate descent,Composite minimization,Convex optimization,Coordinate relaxation,Gauss-Seidel method,Gradient descent,Huge-scale optimization,Iteration complexity,LASSO,Sparse regression},
number = {1-2},
pages = {1--38},
title = {{Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function}},
volume = {144},
year = {2014}
}



@inproceedings{Csiba2015,
abstract = {This paper introduces AdaSDCA: an adaptive variant of stochastic dual coordinate ascent (SDCA) for solving the regularized empirical risk minimization problems. Our modification consists in allowing the method to adaptively change the probability distribution over the dual variables throughout the iterative process. AdaSDCA achieves provably better complexity bound than SDCA with the best fixed probability distribution, known as importance sampling. However, it is of a theoretical character as it is expensive to implement. We also propose AdaSDCA+: a practical variant which in our experiments outperforms existing non-adaptive methods.},
archivePrefix = {arXiv},
arxivId = {1502.08053},
author = {Csiba, Dominik and Qu, Zheng and Richtarik, Peter},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
file = {:Users/sudeep/Library/Application Support/Mendeley Desktop/Downloaded/Csiba, Qu ZHENGQU, Richt{\'{a}}rik PETERRICHTARIK - Unknown - Stochastic Dual Coordinate Ascent with Adaptive Probabilities.pdf:pdf},
isbn = {9781510810587},
pages = {674--683},
title = {{Stochastic dual coordinate ascent with adaptive probabilities}},
volume = {1},
year = {2015}
}



@inproceedings{Salehi2018,
abstract = {Coordinate descent methods usually minimize a cost function by updating a random decision variable (corresponding to one coordinate) at a time. Ideally, we would update the decision variable that yields the largest decrease in the cost function. However, finding this coordinate would require checking all of them, which would effectively negate the improvement in computational tractability that coordinate descent is intended to afford. To address this, we propose a new adaptive method for selecting a coordinate. First, we find a lower bound on the amount the cost function decreases when a coordinate is updated. We then use a multi-armed bandit algorithm to learn which coordinates result in the largest lower bound by interleaving this learning with conventional coordinate descent updates except that the coordinate is selected proportionately to the expected decrease. We show that our approach improves the convergence of coordinate descent methods both theoretically and experimentally.},
archivePrefix = {arXiv},
arxivId = {1712.03010v2},
author = {Salehi, Farnood and Thiran, Patrick and {Elisa Celis}, L.},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1712.03010v2},
issn = {10495258},
mendeley-groups = {Stochastic Optimisation},
pages = {9247--9257},
publisher = {Curran Associates, Inc.},
title = {{Coordinate descent with bandit sampling}},
year = {2018}
}



@article{Tewari2011,
abstract = {We describe and analyze two stochastic methods for 1 regularized loss minimization problems, such as the Lasso. The first method updates the weight of a single feature at each iteration while the second method updates the entire weight vector but only uses a single training example at each iteration. In both methods, the choice of feature or example is uniformly at random. Our theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art deterministic approaches, including their deterministic counterparts, when the size of the problem is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and natural data sets. 1},
author = {Tewari, Ambuj and Shalev-Shwartz, Shai},
journal = {Journal of Machine Learning Research},
keywords = {L1 regularization,coordinate descent,mirror descent,optimization,sparsity},
pages = {1865--1892},
title = {{Stochastic Methods for l1-regularized Loss Minimization}},
volume = {12},
year = {2011}
}



@inproceedings{Fercoq2019,
abstract = {We study the performance of a family of randomized parallel coordinate descent methods for minimizing a nonsmooth nonseparable convex function. The problem class includes as a special case L1-regularized L1 regression and the minimization of the exponential loss (“AdaBoost problem”). We assume that the input data defining the loss function is contained in a sparse {\&}{\#}x0024;{\&}{\#}x0024;m$\backslash$times n{\&}{\#}x0024;{\&}{\#}x0024; matrix A with at most {\&}{\#}x0024;{\&}{\#}x0024;$\backslash$omega {\&}{\#}x0024;{\&}{\#}x0024; nonzeros in each row and that the objective function has a “max structure”, allowing us to smooth it. Our main contribution consists in identifying parameters with a closed-form expression that guarantees a parallelization speedup that depends on basic quantities of the problem (like its size and the number of processors). The theory relies on a fine study of the Lipschitz constant of the smoothed objective restricted to low dimensional subspaces and shows an increased acceleration for sparser problems.},
archivePrefix = {arXiv},
arxivId = {1309.5885},
author = {Fercoq, Olivier and Richt{\'{a}}rik, Peter},
booktitle = {Springer Proceedings in Mathematics and Statistics},
doi = {10.1007/978-3-030-12119-8_4},
isbn = {9783030121181},
issn = {21941017},
keywords = {Coordinate descent,Lipschitz constant,Parallel computing,Smoothing},
pages = {57--96},
title = {{Smooth Minimization of Nonsmooth Functions with Parallel Coordinate Descent Methods}},
volume = {279},
year = {2019}
}



@article{Wright2015,
abstract = {Coordinate descent algorithms solve optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes. They have been used in applications for many years, and their popularity continues to grow because of their usefulness in data analysis, machine learning, and other areas of current interest. This paper describes the fundamentals of the coordinate descent approach, together with variants and extensions and their convergence properties, mostly with reference to convex objectives. We pay particular attention to a certain problem structure that arises frequently in machine learning applications, showing that efficient implementations of accelerated coordinate descent algorithms are possible for problems of this type. We also present some parallel variants and discuss their convergence properties under several models of parallel execution.},
author = {Wright, Stephen J},
doi = {10.1007/s10107-015-0892-3},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {Coordinate descent,Parallel numerical computing,Randomized algorithms},
number = {1},
pages = {3--34},
title = {{Coordinate descent algorithms}},
volume = {151},
year = {2015}
}



@article{Nesterov2012,
abstract = {In this paper we propose new methods for solving huge-scale optimization problems. For problems of this size, even the simplest full-dimensional vector operations are very expensive. Hence, we propose to apply an optimization technique based on random partial update of decision variables. For these methods, we prove the global estimates for the rate of convergence. Surprisingly, for certain classes of objective functions, our results are better than the standard worst-case bounds for deterministic algorithms. We present constrained and unconstrained versions of the method and its accelerated variant. Our numerical test confirms a high efficiency of this technique on problems of very big size. Read More: http://epubs.siam.org/doi/abs/10.1137/100802001},
author = {Nesterov, Yu},
doi = {10.1137/100802001},
issn = {10526234},
journal = {SIAM Journal on Optimization},
keywords = {Convex optimization,Coordinate relaxation,Fast gradient schemes,Google problem,Worst-case efficiency estimates},
number = {2},
pages = {341--362},
title = {{Efficiency of coordinate descent methods on huge-scale optimization problems}},
volume = {22},
year = {2012}
}


@article{Leventhal2010,
abstract = {We study randomized variants of two classical algorithms: coordinate descent for systems of linear equations and iterated projections for systems of linear inequalities. Expanding on a recent randomized iterated projection algorithm of Strohmer and Vershynin (Strohmer, T., R. Vershynin. 2009. A randomized Kaczmarz algorithm with exponential convergence. J. Fourier Anal. Appl. 15 262-278) for systems of linear equations, we show that, under appropriate probability distributions, the linear rates of convergence (in expectation) can be bounded in terms of natural linear-algebraic condition numbers for the problems. We relate these condition measures to distances to ill-posedness and discuss generalizations to convex systems under metric regularity assumptions. Copyright {\textcopyright} 2010 INFORMS.},
archivePrefix = {arXiv},
arxivId = {0806.3015},
author = {Leventhal, D and Lewis, A S},
doi = {10.1287/moor.1100.0456},
issn = {0364765X},
journal = {Mathematics of Operations Research},
keywords = {Averaged projections,Condition number,Coordinate descent,Distance to ill-posedness,Error bound,Iterated projections,Linear constraint,Metric regularity,Randomization},
number = {3},
pages = {641--654},
title = {{Randomized methods for linear constraints: Convergence rates and conditioning}},
volume = {35},
year = {2010}
}


@InProceedings{ShalevShwartz14,
  title = 	 {Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization},
  author = 	 {Shai Shalev-Shwartz and Tong Zhang},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {64--72},
  year = 	 {2014},
  editor = 	 {Eric P. Xing and Tony Jebara},
  volume = 	 {32},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Beijing, China},
  month = 	 {22--24 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/shalev-shwartz14.pdf},
  abstract = 	 {We introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the framework and obtain rates that improve state-of-the-art results for various key machine learning optimization problems including SVM,   logistic regression, ridge regression, Lasso, and multiclass SVM. Experiments validate our theoretical findings.}
}


@inproceedings{Tao2012,
abstract = {Stochastic Coordinate Descent (SCD) methods are among the first optimization schemes suggested for efficiently solving large scale problems. However, until now, there exists a gap between the convergence rate analysis and practical SCD algorithms for general smooth losses and there is no primal SCD algorithm for nonsmooth losses. In this paper, we discuss these issues using the recently developed structural optimization techniques. In particular, we first present a principled and practical SCD algorithm for regularized smooth losses, in which the one-variable subproblem is solved using the proximal gradient method and the adaptive componentwise Lipschitz constant is obtained employing the line search strategy. When the loss is nonsmooth, we present a novel SCD algorithm, in which the one-variable subproblem is solved using the dual averaging method. We show that our algorithms exploit the regularization structure and achieve several optimal convergence rates that are standard in the literature. The experiments demonstrate the expected efficiency of our SCD algorithms in both smooth and nonsmooth cases. {\textcopyright} 2012 Springer-Verlag.},
author = {Tao, Qing and Kong, Kang and Chu, Dejun and Wu, Gaowei},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33460-3_40},
isbn = {9783642334597},
issn = {03029743},
keywords = {Coordinate Descent Algorithms,Large-Scale Learning,Nonsmooth and smooth Losses,Optimization Algorithms},
mendeley-groups = {Stochastic Optimisation},
pages = {537--552},
volume = {7523},
title = {{Stochastic coordinate descent methods for regularized smooth and nonsmooth losses}},
year = {2012}
}

@article{Lu2015,
abstract = {In this paper we analyze the randomized block-coordinate descent (RBCD) methods proposed in Nesterov (SIAM J Optim 22(2):341–362, 2012), Richt{\'{a}}rik and Tak{\'{a}}{\v{c}} (Math Program 144(1–2):1–38, 2014) for minimizing the sum of a smooth convex function and a block-separable convex function, and derive improved bounds on their convergence rates. In particular, we extend Nesterov's technique developed in Nesterov (SIAM J Optim 22(2):341–362, 2012) for analyzing the RBCD method for minimizing a smooth convex function over a block-separable closed convex set to the aforementioned more general problem and obtain a sharper expected-value type of convergence rate than the one implied in Richt{\'{a}}rik and Tak{\'{a}}{\v{c}} (Math Program 144(1–2):1–38, 2014). As a result, we also obtain a better high-probability type of iteration complexity. In addition, for unconstrained smooth convex minimization, we develop a new technique called randomized estimate sequence to analyze the accelerated RBCD method proposed by Nesterov (SIAM J Optim 22(2):341–362, 2012) and establish a sharper expected-value type of convergence rate than the one given in Nesterov (SIAM J Optim 22(2):341–362, 2012).},
archivePrefix = {arXiv},
arxivId = {1305.4723},
author = {Lu, Zhaosong and Xiao, Lin},
doi = {10.1007/s10107-014-0800-2},
eprint = {1305.4723},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {Accelerated coordinate descent,Composite minimization,Convergence rate,Iteration complexity,Randomized block-coordinate descent},
number = {1-2},
pages = {615--642},
title = {{On the complexity analysis of randomized block-coordinate descent methods}},
volume = {152},
year = {2015}
}





%%%%%%%%% Accelerated Coordinate Descent with arbitrary sampling and best rates for minibatch

% Inexact/Stochastic Coordinate Descent

@inproceedings{Zhao2014,
abstract = {We consider regularized empirical risk minimization problems. In particular, we minimize the sum of a smooth empirical risk function and a nonsmooth regularization function. When the regularization function is block separable, we can solve the minimization problems in a randomized block coordinate descent (RBCD) manner. Existing RBCD methods usually decrease the objective value by exploiting the partial gradient of a randomly selected block of coordinates in each iteration. Thus they need all data to be accessible so that the partial gradient of the block gradient can be exactly obtained. However, such a "batch" setting may be computationally expensive in practice. In this paper, we propose a mini-batch randomized block coordinate descent (MRBCD) method, which estimates the partial gradient of the selected block based on a mini-batch of randomly sampled data in each iteration. We further accelerate the MRBCD method by exploiting the semi-stochastic optimization scheme, which effectively reduces the variance of the partial gradient estimators. Theoretically, we show that for strongly convex functions, the MRBCD method attains lower overall iteration complexity than existing RBCD methods. As an application, we further trim the MRBCD method to solve the regularized sparse learning problems. Our numerical experiments shows that the MRBCD method naturally exploits the sparsity structure and achieves better computational performance than existing methods.},
author = {Zhao, Tuo and Yu, Mo and Wang, Yiming and Arora, Raman and Liu, Han},
booktitle = {Advances in Neural Information Processing Systems},
issn = {10495258},
pages = {3329--3337},
title = {{Accelerated mini-batch randomized block coordinate descent method}},
volume = {4},
year = {2014}
}


@article{Wang2014,
abstract = {Two types of low cost-per-iteration gradient descent methods have been extensively studied in parallel. One is online or stochastic gradient descent (OGD/SGD), and the other is randomzied coordinate descent (RBCD). In this paper, we combine the two types of methods together and propose online randomized block coordinate descent (ORBCD). At each iteration, ORBCD only computes the partial gradient of one block coordinate of one mini-batch samples. ORBCD is well suited for the composite minimization problem where one function is the average of the losses of a large number of samples and the other is a simple regularizer defined on high dimensional variables. We show that the iteration complexity of ORBCD has the same order as OGD or SGD. For strongly convex functions, by reducing the variance of stochastic gradients, we show that ORBCD can converge at a geometric rate in expectation, matching the convergence rate of SGD with variance reduction and RBCD.},
archivePrefix = {arXiv},
arxivId = {1407.0107},
author = {Wang, Huahua and Banerjee, Arindam},
eprint = {1407.0107},
title = {{Randomized Block Coordinate Descent for Online and Stochastic Optimization}},
year = {2014}
}


@inproceedings{Zhang2016,
abstract = {We study the composite minimization problem where the objective function is the sum of two convex functions: one is the sum of a finite number of strongly convex and smooth functions, and the other is a general convex function that is non-differentiable. Specifically, we consider the case where the non-differentiable function is block separable and admits a simple proximal mapping for each block. This type of composite optimization is common in many data mining and machine learning problems, and can be solved by block coordinate descent algorithms. We propose an accelerated stochastic block coordinate descent (ASBCD) algorithm, which incorporates the incrementally averaged partial derivative into the stochastic partial derivative and exploits optimal sampling. We prove that ASBCD attains a linear rate of convergence. In contrast to uniform sampling, we reveal that the optimal non-uniform sampling can be employed to achieve a lower iteration complexity. Experimental results on different large-scale real data sets support our theory.},
author = {Zhang, Aston and Gu, Quanquan},
booktitle = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/2939672.2939819},
isbn = {9781450342322},
keywords = {Sampling,Stochastic block coordinate descent},
pages = {2035--2044},
title = {{Accelerated stochastic block coordinate descent with optimal sampling}},
volume = {13-17-Augu},
year = {2016}
}


@article{Xu2015,
abstract = {The stochastic gradient (SG) method can quickly solve a problem with a large number of components in the objective, or a stochastic optimization problem, to a moderate accuracy. The block coordinate descent/update (BCD) method, on the other hand, can quickly solve problems with multiple (blocks of) variables. This paper introduces a method that combines the great features of SG and BCD for problems with many components in the objective and with multiple (blocks of) variables. This paper proposes a block SG (BSG) method for both convex and nonconvex programs. BSG generalizes SG by updating all the blocks of variables in the Gauss-Seidel type (updating the current block depends on the previously updated block), in either a fixed or randomly shuffled order. Although BSG has slightly more work at each iteration, it typically outperforms SG because of BSG's Gauss-Seidel updates and larger step sizes, the latter of which are determined by the smaller per-block Lipschitz constants. The convergence of BSG is established for both convex and nonconvex cases. In the convex case, BSG has the same order of convergence rate as SG. In the nonconvex case, its convergence is established in terms of the expected violation of a first-order optimality condition. In both cases our analysis is nontrivial since the typical unbiasedness assumption no longer holds. BSG is numerically evaluated on the following problems: stochastic least squares and logistic regression, which are convex, and low-rank tensor recovery and bilinear logistic regression, which are nonconvex. On the convex problems, BSG performed significantly better than SG. On the nonconvex problems, BSG significantly outperformed the deterministic BCD method because the latter tends to stagnate early near local minimizers. Overall, BSG inherits the benefits of both SG approximation and block coordinate updates and is especially useful for solving large-scale nonconvex problems.},
archivePrefix = {arXiv},
arxivId = {1408.2597},
author = {Xu, Yangyang and Yin, Wotao},
doi = {10.1137/140983938},
eprint = {1408.2597},
issn = {10526234},
journal = {SIAM Journal on Optimization},
keywords = {Block coordinate update,Convex optimization,Nonconvex optimization,Nonsmooth optimization,Stochastic gradient},
number = {3},
pages = {1686--1716},
title = {{Block stochastic gradient iteration for convex and nonconvex optimization}},
volume = {25},
year = {2015}
}


@inproceedings{Reddi2015,
abstract = {We develop randomized block coordinate descent (CD) methods for linearly constrained convex optimization. Unlike other large-scale CD methods, we do not assume the constraints to be separable, but allow them be coupled linearly. To our knowledge, ours is the first CD method that allows linear coupling constraints, without making the global iteration complexity have an exponential dependence on the number of constraints. We present algorithms and theoretical analysis for four key (convex) scenarios: (i) smooth; (ii) smooth + separable nonsmooth; (iii) asynchronous parallel; and (iv) stochastic. We discuss some architectural details of our methods and present preliminary results to illustrate the behavior of our algorithms.},
archivePrefix = {arXiv},
arxivId = {1409.2617},
author = {Reddi, Sashank J and Hefny, Ahmed and Downey, Carlton and Dubey, Avinava and Sra, Suvrit},
booktitle = {Uncertainty in Artificial Intelligence - Proceedings of the 31st Conference, UAI 2015},
eprint = {1409.2617},
pages = {762--771},
title = {{Large-scale randomized-coordinate descent methods with non-separable linear constraints}},
year = {2015}
}


@article{Konecny2016,
author={J. {Konečný} and J. {Liu} and P. {Richtárik} and M. {Takáč}},
journal={IEEE Journal of Selected Topics in Signal Processing},
title={Mini-Batch Semi-Stochastic Gradient Descent in the Proximal Setting},
year={2016},
volume={10},
number={2},
pages={242-255},
keywords={convex programming;gradient methods;minibatch semistochastic gradient descent;proximal setting;mS2GD;smooth convex functions;nonsmooth convex regularizer;deterministic step;objective function;starting point;speedup effects;parallel implementation;Radio frequency;Complexity theory;Signal processing algorithms;Linear systems;Stochastic processes;Indexes;Optimization;Empirical risk minimization;mini-batches;proximal methods;semi-stochastic gradient descent;sparse data;stochastic gradient descent;variance reduction},
doi={10.1109/JSTSP.2015.2505682},
ISSN={1941-0484},
month={March},}

@article{Dang2015,
abstract = {In this paper, we present a new stochastic algorithm, namely, the stochastic block mirror descent (SBMD) method for solving large-scale nonsmooth and stochastic optimization problems. The basic idea of this algorithm is to incorporate block coordinate decomposition and an incremental block averaging scheme into the classic (stochastic) mirror descent method, in order to significantly reduce the cost per iteration of the latter algorithm. We establish the rate of convergence of the SBMD method along with its associated large-deviation results for solving general nonsmooth and stochastic optimization problems. We also introduce variants of this method and establish their rate of convergence for solving strongly convex, smooth, and composite optimization problems, as well as certain nonconvex optimization problems. To the best of our knowledge, all these developments related to the SBMD methods are new in the stochastic optimization literature. Moreover, some of our results seem to be new for block coordinate descent methods for deterministic optimization.},
archivePrefix = {arXiv},
arxivId = {1309.2249},
author = {Dang, Cong D and Lan, Guanghui},
doi = {10.1137/130936361},
issn = {10526234},
journal = {SIAM Journal on Optimization},
keywords = {Block coordinate descent,Metric learning,Mirror descent,Nonsmooth optimization,Stochastic composite optimization,Stochastic optimization},
number = {2},
pages = {856--881},
title = {{Stochastic block mirror descent methods for nonsmooth and stochastic optimization}},
volume = {25},
year = {2015}
}





@article{Grippo1999,
abstract = {In this paper we define new classes of globally convergent block-coordinate techniques for the unconstrained minimization of a continuously differentiable function. More specifically, we first describe conceptual models of decomposition algorithms based on the interconnection of elementary operations performed on the block components of the variable vector. Then we characterize the elementary operations defined through a suitable line search or the global minimization in a component subspace. Using these models, we establish new results on the convergence of the nonlinear Gauss-Seidel method and we prove that this method with a two-block decomposition is globally convergent towards stationary points, even in the absence of convexity or uniqueness assumptions. In the general case of nonconvex objective function and arbitrary decomposition we define new globally convergent line-search-based schemes that may also include partial global minimizations with respect to some component. Computational aspects are discussed and, in particular, an application to a learning problem in a Radial Basis Function neural network is illustrated.},
author = {Grippo, Luigi and Sciandrone, Marco},
doi = {10.1080/10556789908805730},
issn = {10556788},
journal = {Optimization Methods and Software},
mendeley-groups = {Stochastic Optimisation},
number = {4},
pages = {587--637},
title = {{Globally convergent block-coordinate techniques for unconstrained optimization}},
volume = {10},
year = {1999}
}



@article{Mahsereci2017,
abstract = {In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.},
author = {Mahsereci, Maren and Hennig, Philipp},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Bayesian optimization,Gaussian processes,Learning rates,Line searches,Stochastic optimization},
mendeley-groups = {Stochastic Optimisation},
pages = {1--59},
title = {{Probabilistic line searches for stochastic optimization}},
volume = {18},
year = {2017}
}


@article{Razaviyayn2013,
abstract = {The block coordinate descent (BCD) method is widely used for minimizing a continuous function f of several block variables. At each iteration of this method, a single block of variables is optimized, while the remaining variables are held fixed. To ensure the convergence of the BCD method, the subproblem of each block variable needs to be solved to its unique global optimal. Unfortunately, this requirement is often too restrictive for many practical scenarios. In this paper, we study an alternative inexact BCD approach which updates the variable blocks by successively minimizing a sequence of approximations of f which are either locally tight upper bounds of f or strictly convex local approximations of f. The main contributions of this work include the characterizations of the convergence conditions for a fairly wide class of such methods, especially for the cases where the objective functions are either nondifferentiable or nonconvex. Our results unify and extend the existing convergence results for many classical algorithms such as the BCD method, the difference of convex functions (DC) method, the expectation maximization (EM) algorithm, as well as the block forward-backward splitting algorithm, all of which are popular for large scale optimization problems involving big data. {\textcopyright} 2013 Society for Industrial and Applied Mathematics.},
archivePrefix = {arXiv},
arxivId = {1209.2385},
author = {Razaviyayn, Meisam and Hong, Mingyi and Luo, Zhi Quan},
doi = {10.1137/120891009},
eprint = {1209.2385},
issn = {10526234},
journal = {SIAM Journal on Optimization},
keywords = {Block coordinate descent,Block successive upper-bound minimization,Successive convex approximation,Successive inner approximation},
mendeley-groups = {Stochastic Optimisation},
number = {2},
pages = {1126--1153},
title = {{A unified convergence analysis of block successive minimization methods for nonsmooth optimization}},
volume = {23},
year = {2013}
}



@article{Tappenden2016,
abstract = {One of the key steps at each iteration of a randomized block coordinate descent method consists in determining the update to a block of variables. Existing algorithms assume that in order to compute the update, a particular subproblem is solved exactly. In this work, we relax this requirement and allow for the subproblem to be solved inexactly, leading to an inexact block coordinate descent method. Our approach incorporates the best known results for exact updates as a special case. Moreover, these theoretical guarantees are complemented by practical considerations: the use of iterative techniques to determine the update and the use of preconditioning for further acceleration.},
archivePrefix = {arXiv},
arxivId = {1304.5530},
author = {Tappenden, Rachael and Richt{\'{a}}rik, Peter and Gondzio, Jacek},
doi = {10.1007/s10957-016-0867-4},
issn = {15732878},
journal = {Journal of Optimization Theory and Applications},
keywords = {Block coordinate descent,Conjugate gradients,Convex optimization,Inexact methods,Iteration complexity,Preconditioning},
number = {1},
pages = {144--176},
title = {{Inexact Coordinate Descent: Complexity and Preconditioning}},
volume = {170},
year = {2016}
}





% Parallel and Distributed methods

@article{Richtarik2016a,
abstract = {In this work we show that randomized (block) coordinate descent methods can be accelerated by parallelization when applied to the problem of minimizing the sum of a partially separable smooth convex function and a simple separable convex function. The theoretical speedup, as compared to the serial method, and referring to the number of iterations needed to approximately solve the problem with high probability, is a simple expression depending on the number of parallel processors and a natural and easily computable measure of separability of the smooth component of the objective function. In the worst case, when no degree of separability is present, there may be no speedup; in the best case, when the problem is separable, the speedup is equal to the number of processors. Our analysis also works in the mode when the number of blocks being updated at each iteration is random, which allows for modeling situations with busy or unreliable processors. We show that our algorithm is able to solve a LASSO problem involving a matrix with 20 billion nonzeros in 2 h on a large memory node with 24 cores.},
archivePrefix = {arXiv},
arxivId = {1212.0873},
author = {Richt{\'{a}}rik, Peter and Tak{\'{a}}{\v{c}}, Martin},
doi = {10.1007/s10107-015-0901-6},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {Big data optimization,Composite objective,Convex optimization,Expected separable over-approximation,Huge-scale optimization,Iteration complexity,LASSO,Parallel coordinate descent,Partial separability},
mendeley-groups = {Stochastic Optimisation},
number = {1-2},
pages = {433--484},
title = {{Parallel coordinate descent methods for big data optimization}},
volume = {156},
year = {2016}
}


@incollection{Bezdek2007,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 220906070 Some Conference DOI : 10 . 1007 / 3 - 540 - 45631 - 7{\_}39 : DBLP CITATIONS 108 READS 193 2 , including : James . Bezdek University 366 , 082 SEE Available : James . Bezdek Retrieved : 13 Abstract . Let f : ℜ s a ℜ be a real - valued scalar field , and let x = (x 1 , {\ldots} , x s) T ∈ ℜ s be partitioned into t subsets of non - overlapping variables as x = (X 1 , {\ldots} , X t) T , with X i ∈ ℜ p i , for i = 1 , {\ldots} , t , p i i=1 t ∑ = s . Alternating optimization (AO) is an iterative procedure for minimizing (or maximizing) the function f (x) = f (X1 , X2 , {\ldots} , Xt) jointly over all variables by alternating restricted minimizations over the individual subsets of variables X1 , {\ldots} , Xt . AO is the basis for the c - means clustering algorithms (t=2) , many forms of vector quantization (t = 2 , 3 and 4) , and the expectation - maximization (EM) algorithm (t = 4) for normal mixture decomposition . First we review where and how AO fits into the overall optimization landscape . Then we discuss the important theoretical issues connected with the AO approach . Finally , we state (without proofs) two new theorems that give very general local and global convergence and rate of convergence results which hold for all partitionings of x .},
author = {Bezdek, James C and Hathaway, Richard J},
doi = {10.1007/3-540-45631-7_39},
pages = {288--300},
title = {{Some Notes on Alternating Optimization}},
year = {2007}
}


@inproceedings{Marecek2015,
abstract = {In this work we propose a distributed randomized block coordinate descent method for minimizing a convex function with a huge number of variables/coordinates. We analyze its complexity under the assumption that the smooth part of the objective function is partially block separable, and show that the degree of separability directly influences the complexity. This extends the results in [Richtarik, Takac: Parallel coordinate descent methods for big data optimization] to a distributed environment. We first show that partially block separable functions admit an expected separable overapproximation (ESO) with respect to a distributed sampling, compute the ESO parameters, and then specialize complexity results from recent literature that hold under the generic ESO assumption. We describe several approaches to distribution and synchronization of the computation across a cluster of multi-core computers and provide promising computational results.},
author = {Mare{\v{c}}ek, Jakub and Richt{\'{a}}rik, Peter and Tak{\'{a}}{\v{c}}, Martin},
booktitle = {Springer Proceedings in Mathematics and Statistics},
doi = {10.1007/978-3-319-17689-5_11},
isbn = {9783319176888},
issn = {21941017},
keywords = {Big data optimization,Communication complexity,Composite objective,Convex optimization,Distributed coordinate descent,Empirical risk minimization,Expected separable over-approximation,Huge-scale optimization,Iteration complexity,Partial separability,Support vector machine},
mendeley-groups = {Stochastic Optimisation},
pages = {261--288},
title = {{Distributed block coordinate descent for minimizing partially separable functions}},
volume = {134},
year = {2015}
}


@article{Richtarik2016b,
abstract = {In this paper we develop and analyze Hydra: HYbriD cooRdinAte descent method for solving loss minimization problems with big data. We initially partition the coordinates (features) and assign each partition to a different node of a cluster. At every iteration, each node picks a random subset of the coordinates from those it owns, independently from the other computers, and in parallel computes and applies updates to the selected coordinates based on a simple closed-form formula. We give bounds on the number of iterations sufficient to approximately solve the problem with high probability, and show how it depends on the data and on the partitioning. We perform numerical experiments with a LASSO instance described by a 3TB matrix.},
archivePrefix = {arXiv},
arxivId = {1310.2059},
author = {Richt{\'{a}}rik, Peter and Tak{\'{a}}{\v{c}}, Martin},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Boosting,Distributed algorithms,Parallel coordinate descent,Stochastic methods},
pages = {1--25},
title = {{Distributed coordinate descent method for learning with big data}},
volume = {17},
year = {2016}
}


@Article{Richtarik2016c,
author="Richt{\'a}rik, Peter
and Tak{\'a}{\v{c}}, Martin",
title="On optimal probabilities in stochastic coordinate descent methods",
journal="Optimization Letters",
year="2016",
month="Aug",
day="01",
volume="10",
number="6",
pages="1233--1243",
abstract="We propose and analyze a new parallel coordinate descent method---NSync---in which at each iteration a random subset of coordinates is updated, in parallel, allowing for the subsets to be chosen using an arbitrary probability law. This is the first method of this type. We derive convergence rates under a strong convexity assumption, and comment on how to assign probabilities to the sets to optimize the bound. The complexity and practical performance of the method can outperform its uniform variant by an order of magnitude. Surprisingly, the strategy of updating a single randomly selected coordinate per iteration---with optimal probabilities---may require less iterations, both in theory and practice, than the strategy of updating all coordinates at every iteration.",
issn="1862-4480",
doi="10.1007/s11590-015-0916-1",
}



@inproceedings{Peng2013,
abstract = {This paper proposes parallel and distributed algorithms for solving very large-scale sparse optimization problems on computer clusters and clouds. Modern datasets usually have a large number of features or training samples, and they are usually stored in a distributed manner. Motivated by the need of solving sparse optimization problems with large datasets, we propose two approaches including (i) distributed implementations of prox-linear algorithms and (ii) GRock, a parallel greedy block coordinate descent method. Different separability properties of the objective terms in the problem enable different data distributed schemes along with their corresponding algorithm implementations. We also establish the convergence of GRock and explain why it often performs exceptionally well for sparse optimization. Numerical results on a computer cluster and Amazon EC2 demonstrate the efficiency and elasticity of our algorithms. {\textcopyright} 2013 IEEE.},
author = {Peng, Zhimin and Yan, Ming and Yin, Wotao},
booktitle = {Asilomar Conference on Signals, Systems and Computers},
doi = {10.1109/ACSSC.2013.6810364},
isbn = {9781479923908},
issn = {10586393},
keywords = {GRock,LASSO,parallel and distributed computing,sparse optimization},
pages = {659--664},
title = {{Parallel and distributed sparse optimization}},
year = {2013}
}



@article{Ferris1994,
abstract = {An approach is presented here for solving optimization problems in which the variables are distributed among p processors. Each processor has primary responsibility for updating its own block of variables in parallel while allowing the remaining variables to change in a restricted fashion, e.g., along a steepest descent, quasi-Newton, or any arbitrary direction. This “forget-me-not” approach is a distinctive feature of the algorithm that has not been analyzed before. The parallelization step is followed by a fast synchronization step wherein the affine hull of the points computed by the parallel processors and the current point is searched for an optimal point. Convergence to a stationary point under continuous differentiability is established for the unconstrained case, as well as a linear convergence rate under the additional assumption of a Lipschitzian gradient and strong convexity. For problems constrained to lie in the Cartesian product of closed convex sets, convergence is established to a point sa...},
author = {Ferris, M C and Mangasarian, O. L.},
doi = {10.1137/0804047},
issn = {1052-6234},
journal = {SIAM Journal on Optimization},
number = {4},
pages = {815--832},
title = {{Parallel Variable Distribution}},
volume = {4},
year = {1994}
}


@inproceedings{Scherrer2012,
abstract = {Large-scale ℓ1-regularized loss minimization problems arise in high-dimensional applications such as compressed sensing and high-dimensional supervised learning, including classification and regression problems. High-performance algorithms and implementations are critical to efficiently solving these problems. Building upon previous work on coordinate descent algorithms for ℓ1-regularized problems, we introduce a novel family of algorithms called block-greedy coordinate descent that includes, as special cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and Thread-Greedy. We give a unified convergence analysis for the family of block-greedy algorithms. The analysis suggests that block-greedy coordinate descent can better exploit parallelism if features are clustered so that the maximum inner product between features in different blocks is small. Our theoretical convergence analysis is supported with experimental results using data from diverse real-world applications. We hope that algorithmic approaches and convergence analysis we provide will not only advance the field, but will also encourage researchers to systematically explore the design space of algorithms for solving large-scale ℓ1-regularization problems.},
archivePrefix = {arXiv},
arxivId = {1212.4174},
author = {Scherrer, Chad and Tewari, Ambuj and Halappanavar, Mahantesh and Haglin, David J},
booktitle = {Advances in Neural Information Processing Systems},
isbn = {9781627480031},
issn = {10495258},
pages = {28--36},
title = {{Feature clustering for accelerating parallel coordinate descent}},
volume = {1},
year = {2012}
}


@article{Necoara2013,
abstract = {In this paper we propose a parallel coordinate descent algorithm for solving smooth convex optimization problems with separable constraints that may arise, e.g. in distributed model predictive control (MPC) for linear network systems. Our algorithm is based on block coordinate descent updates in parallel and has a very simple iteration. We prove (sub)linear rate of convergence for the new algorithm under standard assumptions for smooth convex optimization. Further, our algorithm uses local information and thus is suitable for distributed implementations. Moreover, it has low iteration complexity, which makes it appropriate for embedded control. An MPC scheme based on this new parallel algorithm is derived, for which every subsystem in the network can compute feasible and stabilizing control inputs using distributed and cheap computations. For ensuring stability of the MPC scheme, we use a terminal cost formulation derived from a distributed synthesis. Preliminary numerical tests show better performance for our optimization algorithm than other existing methods. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1302.3092},
author = {Necoara, Ion and Clipici, Dragos},
doi = {10.1016/j.jprocont.2012.12.012},
issn = {09591524},
journal = {Journal of Process Control},
keywords = {(Sub)linear convergence rate,Coordinate descent optimization,Distributed model predictive control,Embedded control,Parallel algorithm},
number = {3},
pages = {243--253},
title = {{Efficient parallel coordinate descent algorithm for convex optimization problems with separable constraints: Application to distributed MPC}},
volume = {23},
year = {2013}
}



@inproceedings{Bradley2011,
abstract = {We propose Shotgun, a parallel coordinate descent algorithm for minimizing L1-regularized losses. Though coordinate descent seems inherently sequential, we prove convergence bounds for Shotgun which predict linear speedups, up to a problem-dependent limit. We present a comprehensive empirical study of Shotgun for Lasso and sparse logistic regression. Our theoretical predictions on the potential for parallelism closely match behavior on real data. Shotgun outperforms other published solvers on a range of large problems, proving to be one of the most scalable algorithms for L1. Copyright 2011 by the author(s)/owner(s).},
archivePrefix = {arXiv},
arxivId = {1105.5379},
author = {Bradley, Joseph K and Kyrola, Aapo and Bickson, Danny and Guestrin, Carlos},
booktitle = {Proceedings of the 28th International Conference on Machine Learning, ICML 2011},
isbn = {9781450306195},
pages = {321--328},
title = {{Parallel coordinate descent for L1-regularized loss minimization}},
year = {2011}
}


@article{Liu2015,
abstract = {We describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on functions that satisfy an essential strong convexity property and a sublinear rate (1/K) on general convex functions. Near-linear speedup on a multicore system can be expected if the number of processors is O(n 1/2) in unconstrained optimization and O(n 1/4) in the separable-constrained case, where n is the number of variables. We describe results from implementation on 40-core processors.},
author = {Liu, Ji and Wright, Stephen J and R{\'{e}}, Christopher and Sridhar, Srikrishna},
journal = {Journal of Machine Learning Research},
keywords = {asynchronous parallel optimization,stochastic coordinate descent},
pages = {285--322},
title = {{An Asynchronous Parallel Stochastic Coordinate Descent Algorithm}},
volume = {16},
year = {2015}
}


% Additional References 

@article{Beck2013,
abstract = {In this paper we study smooth convex programming problems where the decision variables vector is split into several blocks of variables. We analyze the block coordinate gradient projection method in which each iteration consists of performing a gradient projection step with respect to a certain block taken in a cyclic order. Global sublinear rate of convergence of this method is established and it is shown that it can be acceler- ated when the problem is unconstrained. In the unconstrained setting we also prove a sublinear rate of convergence result for the so-called alternating minimization method when the number of blocks is two. When the objective function is also assumed to be strongly convex, linear rate of convergence is established.},
author = {Beck, Amir and Tetruashvili, Luba},
doi = {10.1137/120887679},
issn = {10526234},
journal = {SIAM Journal on Optimization},
keywords = {Alternating minimization,Block descent methods,Convex optimization,Rate of convergence},
number = {4},
pages = {2037--2060},
title = {{On the convergence of block coordinate descent type methods}},
volume = {23},
year = {2013}
}

@article{Tibshirani2013,
abstract = {Coordinate descent is a non-derivative optimization algorithm. To find a local minimum of a function, one does line search along one coordinate direction at the current point in each iteration. One uses different coordinate directions cyclically throughout the procedure.},
author = {Tibshirani, Ryan},
pages = {1--28},
title = {{Coordinate descent}},
url = {https://www.stat.cmu.edu/~ryantibs/convexopt/lectures/coord-desc.pdf},
year = {2013}
}


@article{Nesterov2014,
author = {Nesterov, Yu},
doi = {10.1007/s10107-013-0686-4},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {Complexity bounds,Huge-scale problems,Nonsmooth convex optimization,Subgradient methods},
number = {1-2},
pages = {275--297},
title = {{Subgradient methods for huge-scale optimization problems}},
volume = {146},
year = {2014}
}

@InProceedings{Julien2013,
  title = 	 {Block-Coordinate {Frank-Wolfe} Optimization for Structural {SVMs}},
  author = 	 {Simon Lacoste-Julien and Martin Jaggi and Mark Schmidt and Patrick Pletscher},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {53--61},
  year = 	 {2013},
  editor = 	 {Sanjoy Dasgupta and David McAllester},
  volume = 	 {28},
  number =       {1},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/lacoste-julien13.pdf},
  abstract = 	 {We propose a randomized block-coordinate variant of the classic Frank-Wolfe algorithm for convex optimization with block-separable constraints. Despite its lower iteration cost, we show that it achieves a similar convergence rate in duality gap as the full Frank-Wolfe algorithm. We also show that, when applied to the dual structural support vector machine (SVM) objective, this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods. However, unlike stochastic subgradient methods, the block-coordinate Frank-Wolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee. Our experiments indicate that this simple algorithm outperforms competing structural SVM solvers.}
}

@article{Hannan1957,
title = {Approximation to Rayes risk in repeated play},
author = {J. Hannan},
pages= {97-139},
year = {1957},
volume = {3},
journal = {Contributions to the Theory of Games}
}

@inproceedings{Vakili2019b,
abstract = {Online minimization of an unknown convex function over the interval [0, 1] is considered under first-order stochastic bandit feedback, which returns a random realization of the gradient of the function at each query point. Without knowing the distribution of the random gradients, a learning algorithm sequentially chooses query points with the objective of minimizing regret defined as the expected cumulative loss of the function values at the query points in excess to the minimum value of the function. An approach based on devising a biased random walk on an infinite-depth binary tree constructed through successive partitioning of the domain of the function is developed. Each move of the random walk is guided by a sequential test based on confidence bounds on the empirical mean constructed using the law of the iterated logarithm. With no tuning parameters, this learning algorithm is robust to heavy-tailed noise with infinite variance and adaptive to unknown function characteristics (specifically, convex, strongly convex, and nonsmooth). It achieves the corresponding optimal regret orders (up to a $\backslash$sqrt{\{}$\backslash$log T{\}} or a $\backslash$log $\backslash$log T factor) in each class of functions and offers better or matching regret orders than the classical stochastic gradient descent approach which requires the knowledge of the function characteristics for tuning the sequence of step-sizes.},
archivePrefix = {arXiv},
arxivId = {1901.05947},
author = {Vakili, Sattar and Salgia, Sudeep and Zhao, Qing},
booktitle = {2019 57th Annual Allerton Conference on Communication, Control, and Computing, Allerton 2019},
doi = {10.1109/ALLERTON.2019.8919740},
isbn = {9781728131511},
pages = {432--438},
title = {{Stochastic Gradient Descent on a Tree: An Adaptive and Robust Approach to Stochastic Convex Optimization}},
year = {2019}
}

@article{Zhang2019,
abstract = {While stochastic gradient descent (SGD) is still the de facto algorithm in deep learning, adaptive methods like Adam have been observed to outperform SGD across important tasks, such as attention models. The settings under which SGD performs poorly in comparison to Adam are not well understood yet. In this paper, we provide empirical and theoretical evidence that a heavy-tailed distribution of the noise in stochastic gradients is a root cause of SGD's poor performance. Based on this observation, we study clipped variants of SGD that circumvent this issue; we then analyze their convergence under heavy-tailed noise. Furthermore, we develop a new adaptive coordinate-wise clipping algorithm (ACClip) tailored to such settings. Subsequently, we show how adaptive methods like Adam can be viewed through the lens of clipping, which helps us explain Adam's strong performance under heavy-tail noise settings. Finally, we show that the proposed ACClip outperforms Adam for both BERT pretraining and finetuning tasks.},
archivePrefix = {arXiv},
arxivId = {1912.03194},
author = {Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank J and Kumar, Sanjiv and Sra, Suvrit},
title = {{Why ADAM Beats SGD for Attention Models}},
year = {2019}
}

@article{Robbins1951,
author = {Robbins, H and Monro, S},
journal = {The Annals of Statistics},
number = {3},
pages = {400--407},
title = {{A stochastic approximation method}},
volume = {22},
year = {1951}
}


@article{Pasupathy2011,
abstract = {The stochastic root-finding problem (SRFP) is that of finding the zero(s) of a vector function, i.e., solving a nonlinear system of equations, when the function is expressed implicitly through a stochastic simulation. SRFPs are equivalently expressed as stochastic fixed-point problems where the underlying function is expressed implicitly, via a noisy simulation. After motivating SRFPs using a few examples, we review available methods to solve such problems on constrained Euclidean spaces. We present the current literature as three broad categories, and detail the basic theoretical results that are currently known in each of the categories. With a view towards helping the practitioner, we discuss specific variations in their implementable form, and provide references to computer code when easily available. Finally, we list various questions that are worthwhile research pursuits from the standpoint of advancing our knowledge of the theoretical underpinnings and the implementation aspects of solutions to SRFPs.},
author = {Pasupathy, Raghu and Tech, Virginia and Kim, Sujin},
journal = {ACM Transactions on Modeling and Computational Simulations},
keywords = {Design,G15 [Numerical Analysis]: Roots of Nonlinear Equa-,G3 [Probability and Statistics]: Probabilistic alg,I6 [Simulation Modeling]: Simulation Theory Genera,Theory Additional Key Words and Phrases: stochasti,sample-average approximation,stochastic approximation},
number = {3},
pages = {19},
title = {{The Stochastic Root Finding Problem: Overview, Solutions, and Open Questions}},
volume = {21},
year = {2011}
}

@article{Bottou2018,
abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
archivePrefix = {arXiv},
arxivId = {1606.04838},
author = {Bottou, L{\'{e}}on and Curtis, Frank E and Nocedal, Jorge},
doi = {10.1137/16M1080173},
issn = {00361445},
journal = {SIAM Review},
keywords = {Algorithm complexity analysis,Machine learning,Noise reduction methods,Numerical optimization,Second-order methods,Stochastic gradient methods},
number = {2},
pages = {223--311},
title = {{Optimization methods for large-scale machine learning}},
volume = {60},
year = {2018}
}

@article{Ruder2016,
  title={An overview of gradient descent optimization algorithms},
  author={Sebastian Ruder},
  journal={ArXiv},
  year={2016},
  volume={abs/1609.04747}
}

@INPROCEEDINGS{Vakili2019a,
author={S. {Vakili} and Q. {Zhao}},
booktitle={2019 IEEE International Symposium on Information Theory (ISIT)},
title={A Random Walk Approach to First-Order Stochastic Convex Optimization},
year={2019},
volume={},
number={},
pages={395-399},
keywords={approximation theory;convex programming;decision making;feedback;learning (artificial intelligence);query processing;random processes;sampling methods;stochastic processes;random walk approach;first-order stochastic convex optimization;finite-time regret analysis;random-walk based strategy;order-optimal regret performance;optimal point;biased random walk moves;infinite-depth tree;active search strategy;function values;expected cumulative loss;minimizing regret;learning algorithm;random gradients;query point;random realization;first-order stochastic bandit feedback;compact set;unknown convex function;online minimization;Stochastic processes;Convex functions;Loss measurement;Computational modeling;Complexity theory;Convergence;Search problems},
doi={10.1109/ISIT.2019.8849396},
ISSN={2157-8095},
month={July},}


@ARTICLE{LeCunn1998,
author={Y. {Lecun} and L. {Bottou} and Y. {Bengio} and P. {Haffner}},
journal={Proceedings of the IEEE},
title={Gradient-based learning applied to document recognition},
year={1998},
volume={86},
number={11},
pages={2278-2324},
keywords={optical character recognition;multilayer perceptrons;backpropagation;convolution;gradient-based learning;document recognition;multilayer neural networks;back-propagation;gradient based learning technique;complex decision surface synthesis;high-dimensional patterns;handwritten character recognition;handwritten digit recognition task;2D shape variability;document recognition systems;field extraction;segmentation recognition;language modeling;graph transformer networks;GTN;multimodule systems;performance measure minimization;cheque reading;convolutional neural network character recognizers;Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
doi={10.1109/5.726791},
ISSN={1558-2256},
month={Nov},}

@article{Wang2018,
abstract = {The problem of detecting a few anomalous processes among a large number of data streams is considered. At each time, aggregated observations can be taken from a chosen subset of the processes, where the chosen subset conforms to a given tree structure. The random observations are drawn from a general distribution that may depend on the size of the chosen subset and the number of anomalous processes in the subset. We propose a sequential search strategy by devising an information-directed random walk (IRW) on the tree-structured observation hierarchy. Subject to a reliability constraint, the proposed policy is shown to be asymptotically optimal with respect to the detection accuracy. Furthermore, it achieves the optimal logarithmic-order sample complexity with respect to the size of the search space provided that the Kullback-Liebler divergence between aggregated observations in the presence and the absence of anomalous processes are bounded away from zero at all levels of the tree structure as the size of the search space approaches infinity. Sufficient conditions on the decaying rate of the aggregated observations to pure noise under which a sublinear scaling in the size of the search space is preserved are also identified for the Bernoulli case.},
archivePrefix = {arXiv},
arxivId = {1612.09067v4},
author = {Wang, Chao and Cohen, Kobi and Zhao, Qing},
eprint = {1612.09067v4},
title = {{Information-Directed Random Walk for Rare Event Detection in Hierarchical Processes}},
year = {2018}
}





