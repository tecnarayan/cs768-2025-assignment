\begin{thebibliography}{84}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Munos, Heess, and Riedmiller]{abdolmaleki2018maximum}
Abbas Abdolmaleki, Jost~Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller.
\newblock Maximum a posteriori policy optimisation.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=S1ANxQW0b}.

\bibitem[Adkins et~al.()Adkins, Bowling, and White]{adkinsmethod}
Jacob Adkins, Michael Bowling, and Adam White.
\newblock A method for evaluating hyperparameter sensitivity in reinforcement learning.
\newblock In \emph{Finding the Frame: An RLC Workshop for Examining Conceptual Frameworks}.

\bibitem[Agarwal et~al.(2021)Agarwal, Schwarzer, Castro, Courville, and Bellemare]{agarwal2021deep}
Rishabh Agarwal, Max Schwarzer, Pablo~Samuel Castro, Aaron~C Courville, and Marc Bellemare.
\newblock Deep reinforcement learning at the edge of the statistical precipice.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 29304--29320, 2021.

\bibitem[Amos et~al.(2023)]{amos2023tutorial}
Brandon Amos et~al.
\newblock Tutorial on amortized optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 16\penalty0 (5):\penalty0 592--732, 2023.

\bibitem[Anderson and Moore(2007)]{anderson2007optimal}
Brian~DO Anderson and John~B Moore.
\newblock \emph{Optimal control: linear quadratic methods}.
\newblock Courier Corporation, 2007.

\bibitem[Andrieu et~al.(2004)Andrieu, Doucet, Singh, and Tadic]{andrieu2004particle}
Christophe Andrieu, Arnaud Doucet, Sumeetpal~S Singh, and Vladislav~B Tadic.
\newblock Particle methods for change detection, system identification, and control.
\newblock \emph{Proceedings of the IEEE}, 92\penalty0 (3):\penalty0 423--438, 2004.

\bibitem[Anthony et~al.(2017)Anthony, Tian, and Barber]{anthony2017thinking}
Thomas Anthony, Zheng Tian, and David Barber.
\newblock Thinking fast and slow with deep learning and tree search.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Antonoglou et~al.(2021)Antonoglou, Schrittwieser, Ozair, Hubert, and Silver]{antonoglou2021planning}
Ioannis Antonoglou, Julian Schrittwieser, Sherjil Ozair, Thomas~K Hubert, and David Silver.
\newblock Planning in stochastic environments with a learned model.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, and Fischer]{auer2002finite}
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine learning}, 47\penalty0 (2):\penalty0 235--256, 2002.

\bibitem[Bain and Crisan(2009)]{bain2009fundamentals}
Alan Bain and Dan Crisan.
\newblock \emph{Fundamentals of stochastic filtering}, volume~3.
\newblock Springer, 2009.

\bibitem[Beck and Teboulle(2003)]{beck2003mirror}
Amir Beck and Marc Teboulle.
\newblock Mirror descent and nonlinear projected subgradient methods for convex optimization.
\newblock \emph{Operations Research Letters}, 31\penalty0 (3):\penalty0 167--175, 2003.

\bibitem[Bonnet et~al.(2023)Bonnet, Luo, Byrne, Surana, Duckworth, Coyette, Midgley, Abramowitz, Tegegn, Kalloniatis, et~al.]{bonnet2023jumanji}
Cl{\'e}ment Bonnet, Daniel Luo, Donal~John Byrne, Shikha Surana, Paul Duckworth, Vincent Coyette, Laurence~Illing Midgley, Sasha Abramowitz, Elshadai Tegegn, Tristan Kalloniatis, et~al.
\newblock Jumanji: a diverse suite of scalable reinforcement learning environments in jax.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Bouthillier et~al.(2021)Bouthillier, Delaunay, Bronzi, Trofimov, Nichyporuk, Szeto, Mohammadi~Sepahvand, Raff, Madan, Voleti, et~al.]{bouthillier2021accounting}
Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin Szeto, Nazanin Mohammadi~Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti, et~al.
\newblock Accounting for variance in machine learning benchmarks.
\newblock \emph{Proceedings of Machine Learning and Systems}, 3:\penalty0 747--769, 2021.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary, Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and Zhang]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs, 2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Colas et~al.(2018)Colas, Sigaud, and Oudeyer]{colas2018gep}
C{\'e}dric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer.
\newblock Gep-pg: Decoupling exploration and exploitation in deep reinforcement learning algorithms.
\newblock In \emph{International conference on machine learning}, pages 1039--1048. PMLR, 2018.

\bibitem[Coulom(2006)]{coulom2006efficient}
R{\'e}mi Coulom.
\newblock Efficient selectivity and backup operators in {Monte-Carlo} tree search.
\newblock In \emph{International conference on computers and games}, pages 72--83. Springer, 2006.

\bibitem[Dalal et~al.(2021)Dalal, Hallak, Dalton, Mannor, Chechik, et~al.]{dalal2021improve}
Gal Dalal, Assaf Hallak, Steven Dalton, Shie Mannor, Gal Chechik, et~al.
\newblock Improve agents without retraining: Parallel tree search with off-policy correction.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 5518--5530, 2021.

\bibitem[Danihelka et~al.(2021)Danihelka, Guez, Schrittwieser, and Silver]{danihelka2021policy}
Ivo Danihelka, Arthur Guez, Julian Schrittwieser, and David Silver.
\newblock Policy improvement by planning with gumbel.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Dayan and Hinton(1997)]{dayan1997using}
Peter Dayan and Geoffrey~E Hinton.
\newblock Using expectation-maximization for reinforcement learning.
\newblock \emph{Neural Computation}, 9\penalty0 (2):\penalty0 271--278, 1997.

\bibitem[Dempster et~al.(1977)Dempster, Laird, and Rubin]{dempster1977maximum}
Arthur~P Dempster, Nan~M Laird, and Donald~B Rubin.
\newblock Maximum likelihood from incomplete data via the em algorithm.
\newblock \emph{Journal of the royal statistical society: series B (methodological)}, 39\penalty0 (1):\penalty0 1--22, 1977.

\bibitem[Doucet et~al.(2000)Doucet, Godsill, and Andrieu]{doucet2000sequential}
Arnaud Doucet, Simon Godsill, and Christophe Andrieu.
\newblock On sequential monte carlo sampling methods for bayesian filtering.
\newblock \emph{Statistics and computing}, 10:\penalty0 197--208, 2000.

\bibitem[Doucet et~al.(2001)Doucet, De~Freitas, Gordon, et~al.]{doucet2001sequential}
Arnaud Doucet, Nando De~Freitas, Neil~James Gordon, et~al.
\newblock \emph{Sequential Monte Carlo methods in practice}, volume~1.
\newblock Springer, 2001.

\bibitem[Dror et~al.(2019)Dror, Shlomov, and Reichart]{dror2019deep}
Rotem Dror, Segev Shlomov, and Roi Reichart.
\newblock Deep dominance-how to properly compare deep neural models.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 2773--2785, 2019.

\bibitem[Fawzi et~al.(2022)Fawzi, Balog, Huang, Hubert, Romera-Paredes, Barekatain, Novikov, Ruiz, Schrittwieser, Swirszcz, Silver, Hassabis, and Kohli]{AlphaTensor2022}
Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Francisco J.~R. Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, David Silver, Demis Hassabis, and Pushmeet Kohli.
\newblock Discovering faster matrix multiplication algorithms with reinforcement learning.
\newblock \emph{Nature}, 610\penalty0 (7930):\penalty0 47--53, 2022.
\newblock \doi{10.1038/s41586-022-05172-4}.

\bibitem[Fountas et~al.(2020)Fountas, Sajid, Mediano, and Friston]{fountas2020deep}
Zafeirios Fountas, Noor Sajid, Pedro Mediano, and Karl Friston.
\newblock Deep active inference agents using monte-carlo methods.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 11662--11675, 2020.

\bibitem[Freeman et~al.(2021)Freeman, Frey, Raichuk, Girgin, Mordatch, and Bachem]{brax2021github}
C.~Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem.
\newblock Brax - a differentiable physics engine for large scale rigid body simulation, 2021.
\newblock URL \url{http://github.com/google/brax}.

\bibitem[Friston(2010)]{friston2010free}
Karl Friston.
\newblock The free-energy principle: a unified brain theory?
\newblock \emph{Nature reviews neuroscience}, 11\penalty0 (2):\penalty0 127--138, 2010.

\bibitem[Furmston and Barber(2010)]{furmston2010variational}
Thomas Furmston and David Barber.
\newblock Variational methods for reinforcement learning.
\newblock In \emph{Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics}, pages 241--248. JMLR Workshop and Conference Proceedings, 2010.

\bibitem[Furuta et~al.(2021)Furuta, Kozuno, Matsushima, Matsuo, and Gu]{furuta2021co}
Hiroki Furuta, Tadashi Kozuno, Tatsuya Matsushima, Yutaka Matsuo, and Shixiang~Shane Gu.
\newblock Co-adaptation of algorithmic and implementational innovations in inference-based deep reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 9828--9842, 2021.

\bibitem[Gilks et~al.(1995)Gilks, Richardson, and Spiegelhalter]{gilks1995markov}
Walter~R Gilks, Sylvia Richardson, and David Spiegelhalter.
\newblock \emph{Markov chain Monte Carlo in practice}.
\newblock CRC press, 1995.

\bibitem[Gordon et~al.(1993)Gordon, Salmond, and Smith]{gordon1993novel}
Neil~J Gordon, David~J Salmond, and Adrian~FM Smith.
\newblock Novel approach to nonlinear/non-gaussian bayesian state estimation.
\newblock In \emph{IEE proceedings F (radar and signal processing)}, volume 140, pages 107--113. IET, 1993.

\bibitem[Gorsane et~al.(2022)Gorsane, Mahjoub, de~Kock, Dubb, Singh, and Pretorius]{gorsane2022towards}
Rihab Gorsane, Omayma Mahjoub, Ruan~John de~Kock, Roland Dubb, Siddarth Singh, and Arnu Pretorius.
\newblock Towards a standardised performance evaluation protocol for cooperative marl.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 5510--5521, 2022.

\bibitem[Grill et~al.(2020)Grill, Altch{\'e}, Tang, Hubert, Valko, Antonoglou, and Munos]{grill2020monte}
Jean-Bastien Grill, Florent Altch{\'e}, Yunhao Tang, Thomas Hubert, Michal Valko, Ioannis Antonoglou, and R{\'e}mi Munos.
\newblock Monte-carlo tree search as regularized policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages 3769--3778. PMLR, 2020.

\bibitem[Gu et~al.(2015)Gu, Ghahramani, and Turner]{gu2015neural}
Shixiang~Shane Gu, Zoubin Ghahramani, and Richard~E Turner.
\newblock Neural adaptive sequential monte carlo.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Guez et~al.(2018)Guez, Mirza, Gregor, Kabra, Racaniere, Weber, Raposo, Santoro, Orseau, Eccles, Wayne, Silver, Lillicrap, and Valdes]{boxobanlevels}
Arthur Guez, Mehdi Mirza, Karol Gregor, Rishabh Kabra, Sebastien Racaniere, Theophane Weber, David Raposo, Adam Santoro, Laurent Orseau, Tom Eccles, Greg Wayne, David Silver, Timothy Lillicrap, and Victor Valdes.
\newblock An investigation of model-free planning: boxoban levels.
\newblock https://github.com/deepmind/boxoban-levels/, 2018.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and Levine]{haarnoja2017reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International conference on machine learning}, pages 1352--1361. PMLR, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pages 1861--1870. PMLR, 2018.

\bibitem[Hachiya et~al.(2009)Hachiya, Peters, and Sugiyama]{hachiya2009efficient}
Hirotaka Hachiya, Jan Peters, and Masashi Sugiyama.
\newblock Efficient sample reuse in em-based policy search.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2009, Bled, Slovenia, September 7-11, 2009, Proceedings, Part I 20}, pages 469--484. Springer, 2009.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hubert et~al.(2021)Hubert, Schrittwieser, Antonoglou, Barekatain, Schmitt, and Silver]{hubert2021learning}
Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, and David Silver.
\newblock Learning and planning in complex action spaces.
\newblock In \emph{International Conference on Machine Learning}, pages 4476--4486. PMLR, 2021.

\bibitem[Kappen et~al.(2012)Kappen, G{\'o}mez, and Opper]{kappen2012optimal}
Hilbert~J Kappen, Vicen{\c{c}} G{\'o}mez, and Manfred Opper.
\newblock Optimal control as a graphical model inference problem.
\newblock \emph{Machine learning}, 87:\penalty0 159--182, 2012.

\bibitem[Kitagawa(1996)]{kitagawa1996monte}
Genshiro Kitagawa.
\newblock Monte carlo filter and smoother for non-gaussian nonlinear state space models.
\newblock \emph{Journal of computational and graphical statistics}, 5\penalty0 (1):\penalty0 1--25, 1996.

\bibitem[Lazaric et~al.(2007)Lazaric, Restelli, and Bonarini]{lazaric2007reinforcement}
Alessandro Lazaric, Marcello Restelli, and Andrea Bonarini.
\newblock Reinforcement learning in continuous action spaces through sequential monte carlo methods.
\newblock \emph{Advances in neural information processing systems}, 20, 2007.

\bibitem[Levine(2018)]{levine2018reinforcement}
Sergey Levine.
\newblock Reinforcement learning and control as probabilistic inference: Tutorial and review.
\newblock \emph{arXiv preprint arXiv:1805.00909}, 2018.

\bibitem[Levine and Koltun(2013)]{levine2013guided}
Sergey Levine and Vladlen Koltun.
\newblock Guided policy search.
\newblock In \emph{International conference on machine learning}, pages 1--9. PMLR, 2013.

\bibitem[Levy(1992)]{levy1992stochastic}
Haim Levy.
\newblock Stochastic dominance and expected utility: Survey and analysis.
\newblock \emph{Management Science}, pages 555--593, 1992.

\bibitem[Li and Todorov(2004)]{li2004iterative}
Weiwei Li and Emanuel Todorov.
\newblock Iterative linear quadratic regulator design for nonlinear biological movement systems.
\newblock In \emph{First International Conference on Informatics in Control, Automation and Robotics}, volume~2, pages 222--229. SciTePress, 2004.

\bibitem[Li et~al.(2017)Li, Turner, and Liu]{li2017approximate}
Yingzhen Li, Richard~E Turner, and Qiang Liu.
\newblock Approximate inference with amortised mcmc.
\newblock \emph{arXiv preprint arXiv:1702.08343}, 2017.

\bibitem[Lioutas et~al.(2022)Lioutas, Lavington, Sefas, Niedoba, Liu, Zwartsenberg, Dabiri, Wood, and Scibior]{lioutas2022critic}
Vasileios Lioutas, Jonathan~Wilder Lavington, Justice Sefas, Matthew Niedoba, Yunpeng Liu, Berend Zwartsenberg, Setareh Dabiri, Frank Wood, and Adam Scibior.
\newblock Critic sequential monte carlo.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Liu et~al.(2019)Liu, Chen, Yu, Zhai, Zhou, and Liu]{liu2018watch}
Anji Liu, Jianshu Chen, Mingze Yu, Yu~Zhai, Xuewen Zhou, and Ji~Liu.
\newblock Watch the unobserved: A simple approach to parallelizing monte carlo tree search.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Liu and Chen(1998)]{liu1998sequential}
Jun~S Liu and Rong Chen.
\newblock Sequential monte carlo methods for dynamic systems.
\newblock \emph{Journal of the American statistical association}, 93\penalty0 (443):\penalty0 1032--1044, 1998.

\bibitem[Liu et~al.(2001)Liu, Chen, and Logvinenko]{liu2001theoretical}
Jun~S Liu, Rong Chen, and Tanya Logvinenko.
\newblock A theoretical framework for sequential importance sampling with resampling.
\newblock In \emph{Sequential Monte Carlo methods in practice}, pages 225--246. Springer, 2001.

\bibitem[Liu et~al.(2022)Liu, Cen, Isenbaev, Liu, Wu, Li, and Zhao]{liu2022constrained}
Zuxin Liu, Zhepeng Cen, Vladislav Isenbaev, Wei Liu, Steven Wu, Bo~Li, and Ding Zhao.
\newblock Constrained variational policy optimization for safe reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages 13644--13668. PMLR, 2022.

\bibitem[Mann and Whitney(1947)]{mann1947test}
Henry~B Mann and Donald~R Whitney.
\newblock On a test of whether one of two random variables is stochastically larger than the other.
\newblock \emph{The annals of mathematical statistics}, pages 50--60, 1947.

\bibitem[Maskell and Gordon(2001)]{maskell2001tutorial}
Simon Maskell and Neil Gordon.
\newblock A tutorial on particle filters for on-line nonlinear/non-gaussian bayesian tracking.
\newblock \emph{IEE Target Tracking: Algorithms and Applications (Ref. No. 2001/174)}, pages 2--1, 2001.

\bibitem[Moerland et~al.(2018)Moerland, Broekens, Plaat, and Jonker]{moerland2018a0c}
Thomas~M Moerland, Joost Broekens, Aske Plaat, and Catholijn~M Jonker.
\newblock A0c: Alpha zero in continuous action space.
\newblock \emph{arXiv preprint arXiv:1805.09613}, 2018.

\bibitem[Montgomery and Levine(2016)]{montgomery2016guided}
William Montgomery and Sergey Levine.
\newblock Guided policy search as approximate mirror descent.
\newblock \emph{arXiv preprint arXiv:1607.04614}, 2016.

\bibitem[Nair et~al.(2020)Nair, Gupta, Dalal, and Levine]{nair2020awac}
Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine.
\newblock Awac: Accelerating online reinforcement learning with offline datasets.
\newblock \emph{arXiv preprint arXiv:2006.09359}, 2020.

\bibitem[Neumann et~al.(2011)]{neumann2011variational}
Gerhard Neumann et~al.
\newblock Variational inference for policy search in changing situations.
\newblock In \emph{Proceedings of the 28th International Conference on Machine Learning, ICML 2011}, pages 817--824, 2011.

\bibitem[Pang et~al.(2023)Pang, Wang, Li, Chen, Xu, Zhang, and Yu]{pang2023language}
Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu, Zongzhang Zhang, and Yang Yu.
\newblock Language model self-improvement by reinforcement learning contemplation.
\newblock \emph{arXiv preprint arXiv:2305.14483}, 2023.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{peng2019advantage}
Xue~Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
\newblock Advantage-weighted regression: Simple and scalable off-policy reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.00177}, 2019.

\bibitem[Peters and Schaal(2007)]{peters2007reinforcement}
Jan Peters and Stefan Schaal.
\newblock Reinforcement learning by reward-weighted regression for operational space control.
\newblock In \emph{Proceedings of the 24th international conference on Machine learning}, pages 745--750, 2007.

\bibitem[Peters et~al.(2010)Peters, Mulling, and Altun]{peters2010relative}
Jan Peters, Katharina Mulling, and Yasemin Altun.
\newblock Relative entropy policy search.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~24, pages 1607--1612, 2010.

\bibitem[Pich{\'e} et~al.(2018)Pich{\'e}, Thomas, Ibrahim, Bengio, and Pal]{piche2018probabilistic}
Alexandre Pich{\'e}, Valentin Thomas, Cyril Ibrahim, Yoshua Bengio, and Chris Pal.
\newblock Probabilistic planning with sequential monte carlo methods.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Pitt and Shephard(2001)]{pitt2001auxiliary}
Michael~K Pitt and Neil Shephard.
\newblock Auxiliary variable based particle filters.
\newblock \emph{Sequential Monte Carlo methods in practice}, pages 273--293, 2001.

\bibitem[Puterman(2014)]{puterman2014markov}
Martin~L Puterman.
\newblock \emph{Markov decision processes: discrete stochastic dynamic programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan, Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel, et~al.]{schrittwieser2020mastering}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et~al.
\newblock Mastering atari, go, chess and shogi by planning with a learned model.
\newblock \emph{Nature}, 588\penalty0 (7839):\penalty0 604--609, 2020.

\bibitem[Schulman(2015)]{schulman2015trust}
John Schulman.
\newblock Trust region policy optimization.
\newblock \emph{arXiv preprint arXiv:1502.05477}, 2015.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and Abbeel]{schulman2016high}
John Schulman, Philipp Moritz, Sergey Levine, Michael~I. Jordan, and Pieter Abbeel.
\newblock High-dimensional continuous control using generalized advantage estimation.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{4th International Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings}, 2016.
\newblock URL \url{http://arxiv.org/abs/1506.02438}.

\bibitem[Schulman et~al.(2017{\natexlab{a}})Schulman, Chen, and Abbeel]{schulman2017equivalence}
John Schulman, Xi~Chen, and Pieter Abbeel.
\newblock Equivalence between policy gradients and soft q-learning.
\newblock \emph{arXiv preprint arXiv:1704.06440}, 2017{\natexlab{a}}.

\bibitem[Schulman et~al.(2017{\natexlab{b}})Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017{\natexlab{b}}.

\bibitem[Segal(2010)]{segal2010scalability}
Richard~B Segal.
\newblock On the scalability of parallel uct.
\newblock In \emph{International Conference on Computers and Games}, pages 36--47. Springer, 2010.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang, Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai, Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2018general}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi, and go through self-play.
\newblock \emph{Science}, 362\penalty0 (6419):\penalty0 1140--1144, 2018.

\bibitem[Song et~al.(2019)Song, Abdolmaleki, Springenberg, Clark, Soyer, Rae, Noury, Ahuja, Liu, Tirumala, et~al.]{song2019v}
H~Francis Song, Abbas Abdolmaleki, Jost~Tobias Springenberg, Aidan Clark, Hubert Soyer, Jack~W Rae, Seb Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala, et~al.
\newblock V-mpo: On-policy maximum a posteriori policy optimization for discrete and continuous control.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Sun et~al.(2018)Sun, Gordon, Boots, and Bagnell]{sun2018dual}
Wen Sun, Geoffrey~J Gordon, Byron Boots, and J~Bagnell.
\newblock Dual policy iteration.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, pages 5026--5033. IEEE, 2012.
\newblock \doi{10.1109/IROS.2012.6386109}.

\bibitem[Toledo(2024)]{toledo2024stoix}
Edan Toledo.
\newblock Stoix: Distributed single-agent reinforcement learning end-to-end in jax, April 2024.
\newblock URL \url{https://github.com/EdanToledo/Stoix}.

\bibitem[Toledo et~al.(2023)Toledo, Midgley, Byrne, Tilbury, Macfarlane, Courtot, and Laterre]{flashbax}
Edan Toledo, Laurence Midgley, Donal Byrne, Callum~Rhys Tilbury, Matthew Macfarlane, Cyprien Courtot, and Alexandre Laterre.
\newblock Flashbax: Streamlining experience replay buffers for reinforcement learning with jax, 2023.
\newblock URL \url{https://github.com/instadeepai/flashbax/}.

\bibitem[Toussaint and Storkey(2006)]{toussaint2006probabilistic}
Marc Toussaint and Amos Storkey.
\newblock Probabilistic inference for solving discrete and continuous state markov decision processes.
\newblock In \emph{Proceedings of the 23rd international conference on Machine learning}, pages 945--952, 2006.

\bibitem[Vieillard et~al.(2020)Vieillard, Pietquin, and Geist]{vieillard2020munchausen}
Nino Vieillard, Olivier Pietquin, and Matthieu Geist.
\newblock Munchausen reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 4235--4246, 2020.

\bibitem[Wirth et~al.(2016)Wirth, F{\"u}rnkranz, and Neumann]{wirth2016model}
Christian Wirth, Johannes F{\"u}rnkranz, and Gerhard Neumann.
\newblock Model-free preference-based reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~30, 2016.

\end{thebibliography}
