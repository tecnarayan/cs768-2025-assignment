\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal \& Bottou(2014)Agarwal and Bottou]{AgarwalBottou14}
Agarwal, Alekh and Bottou, Leon.
\newblock A lower bound for the optimization of finite sums.
\newblock \emph{arXiv:1410.0723}, 2014.

\bibitem[Banks-Watson(2012)]{NewClassesCD}
Banks-Watson, Alexander.
\newblock New classes of coordinate descent methods.
\newblock Master's thesis, University of Edinburgh, 2012.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{SAGA}
Defazio, Aaron, Bach, Francis, and Lacoste-Julien, Simon.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock \emph{arXiv:1407.0202}, 2014.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{AdaGrad}
Duchi, John, Hazan, Elad, and Singer, Yoram.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (1):\penalty0 2121--2159, 2011.

\bibitem[Fercoq \& Richt\'{a}rik(2013)Fercoq and Richt\'{a}rik]{APPROX}
Fercoq, Olivier and Richt\'{a}rik, Peter.
\newblock Accelerated, parallel and proximal coordinate descent.
\newblock \emph{SIAM Journal on Optimization (after minor revision),
  arXiv:1312.5799}, 2013.

\bibitem[Fercoq \& Richt\'arik(2013)Fercoq and Richt\'arik]{SPCDM}
Fercoq, Olivier and Richt\'arik, Peter.
\newblock Smooth minimization of nonsmooth functions by parallel coordinate
  descent.
\newblock \emph{arXiv:1309.5885}, 2013.

\bibitem[Fercoq et~al.(2014)Fercoq, Qu, Richt\'{a}rik, and
  Tak{\'a}{\v{c}}]{Hydra2}
Fercoq, Olivier, Qu, Zheng, Richt\'{a}rik, Peter, and Tak{\'a}{\v{c}}, Martin.
\newblock Fast distributed coordinate descent for minimizing non-strongly
  convex losses.
\newblock \emph{IEEE International Workshop on Machine Learning for Signal
  Processing}, 2014.

\bibitem[Fountoulakis \& Tappenden(2014)Fountoulakis and Tappenden]{RBCDFT}
Fountoulakis, Kimon and Tappenden, Rachael.
\newblock Robust block coordinate descent.
\newblock \emph{arXiv:1407.7573}, 2014.

\bibitem[Glasmachers \& Dogan(2013)Glasmachers and
  Dogan]{glasmachers2013accelerated}
Glasmachers, Tobias and Dogan, Urun.
\newblock Accelerated coordinate descent with adaptive coordinate frequencies.
\newblock In \emph{Asian Conference on Machine Learning}, pp.\  72--86, 2013.

\bibitem[Jaggi et~al.(2014)Jaggi, Smith, Takac, Terhorst, Krishnan, Hofmann,
  and Jordan]{CoCoa}
Jaggi, Martin, Smith, Virginia, Takac, Martin, Terhorst, Jonathan, Krishnan,
  Sanjay, Hofmann, Thomas, and Jordan, Michael~I.
\newblock Communication-efficient distributed dual coordinate ascent.
\newblock In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., and
  Weinberger, K.Q. (eds.), \emph{Advances in Neural Information Processing
  Systems 27}, pp.\  3068--3076. Curran Associates, Inc., 2014.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{SVRG}
Johnson, Rie and Zhang, Tong.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{NIPS}, 2013.

\bibitem[Kone\v{c}n\'{y} \& Richt\'{a}rik(2014)Kone\v{c}n\'{y} and
  Richt\'{a}rik]{S2GD}
Kone\v{c}n\'{y}, Jakub and Richt\'{a}rik, Peter.
\newblock {S2GD}: {S}emi-stochastic gradient descent methods.
\newblock \emph{arXiv:1312.1666}, 2014.

\bibitem[Kone\v{c}n\'{y} et~al.(2014{\natexlab{a}})Kone\v{c}n\'{y}, Lu,
  Richt\'{a}rik, and Tak\'{a}\v{c}]{mS2GD}
Kone\v{c}n\'{y}, Jakub, Lu, Jie, Richt\'{a}rik, Peter, and Tak\'{a}\v{c},
  Martin.
\newblock m{S2GD}: {M}ini-batch semi-stochastic gradient descent in the
  proximal setting.
\newblock \emph{arXiv:1410.4744}, 2014{\natexlab{a}}.

\bibitem[Kone\v{c}n\'{y} et~al.(2014{\natexlab{b}})Kone\v{c}n\'{y}, Qu, and
  Richt\'{a}rik]{S2CD}
Kone\v{c}n\'{y}, Jakub, Qu, Zheng, and Richt\'{a}rik, Peter.
\newblock Semi-stochastic coordinate descent.
\newblock \emph{arXiv:1412.6293}, 2014{\natexlab{b}}.

\bibitem[Lin et~al.(2014)Lin, Lu, and Xiao]{APCG}
Lin, Qihang, Lu, Zhaosong, and Xiao, Lin.
\newblock An accelerated proximal coordinate gradient method and its
  application to regularized empirical risk minimization.
\newblock Technical Report MSR-TR-2014-94, July 2014.

\bibitem[Liu \& Wright(2014)Liu and Wright]{WrightAsynchrous14}
Liu, Ji and Wright, Stephen~J.
\newblock Asynchronous stochastic coordinate descent: Parallelism and
  convergence properties.
\newblock \emph{arXiv:1403.3862}, 2014.

\bibitem[Loshchilov et~al.(2011)Loshchilov, Schoenauer, and Sebag]{Adaptivecd}
Loshchilov, I., Schoenauer, M., and Sebag, M.
\newblock {A}daptive {C}oordinate {D}escent.
\newblock In et~al., N.~Krasnogor (ed.), \emph{Genetic and Evolutionary
  Computation Conference (GECCO)}, pp.\  885--892. ACM Press, July 2011.

\bibitem[Lukasewitz(2013)]{randomFrankWolfe}
Lukasewitz, Isabella.
\newblock Block-coordinate frank-wolfe optimization.
\newblock A study on randomized sampling methods, 2013.

\bibitem[Mahajan et~al.(2014)Mahajan, Keerthi, and
  Sundararajan]{MahajanKeerthiSunda}
Mahajan, Dhruv, Keerthi, S.~Sathiya, and Sundararajan, S.
\newblock A distributed block coordinate descent method for training l1
  regularized linear classifiers.
\newblock \emph{arXiv:1405.4544}, 2014.

\bibitem[Mairal(2014)]{MISO}
Mairal, Julien.
\newblock Incremental majorization-minimization optimization with application
  to large-scale machine learning.
\newblock Technical report, 2014.

\bibitem[Mare\v{c}ek et~al.(2014)Mare\v{c}ek, Richt\'{a}rik, and
  T{a}k\'{a}\v{c}]{DisBCDM}
Mare\v{c}ek, Jakub, Richt\'{a}rik, Peter, and T{a}k\'{a}\v{c}, Martin.
\newblock Distributed block coordinate descent for minimizing partially
  separable functions.
\newblock \emph{arXiv:1406.0328}, 2014.

\bibitem[Necoara \& Patrascu(2014)Necoara and Patrascu]{Necoara:rcdm-coupled}
Necoara, Ion and Patrascu, Andrei.
\newblock A random coordinate descent algorithm for optimization problems with
  composite objective function and linear coupled constraints.
\newblock \emph{Computational Optimization and Applications}, 57:\penalty0
  307--337, 2014.

\bibitem[Necoara et~al.(2012)Necoara, Nesterov, and Glineur]{Necoara:Coupled}
Necoara, Ion, Nesterov, Yurii, and Glineur, Francois.
\newblock Efficiency of randomized coordinate descent methods on optimization
  problems with linearly coupled constraints.
\newblock Technical report, Politehnica University of Bucharest, 2012.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
Nemirovski, Arkadi, Juditsky, Anatoli, Lan, Guanghui, and Shapiro, Alexander.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Nesterov(2012)]{Nesterov:2010RCDM}
Nesterov, Yurii.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  341--362, 2012.

\bibitem[Qu \& Richt\'{a}rik(2014)Qu and Richt\'{a}rik]{ALPHA}
Qu, Zheng and Richt\'{a}rik, Peter.
\newblock Coordinate descent methods with arbitrary sampling {I}: Algorithms
  and complexity.
\newblock \emph{arXiv:1412.8060}, 2014.

\bibitem[{Qu} \& {Richt{\'a}rik}(2014){Qu} and {Richt{\'a}rik}]{ESO}
{Qu}, Zheng and {Richt{\'a}rik}, Peter.
\newblock {Coordinate Descent with Arbitrary Sampling {II}: Expected Separable
  Overapproximation}.
\newblock \emph{ArXiv e-prints}, 2014.

\bibitem[{Qu} et~al.(2014){Qu}, {Richt{\'a}rik}, and {Zhang}]{Quartz}
{Qu}, Zheng, {Richt{\'a}rik}, Peter, and {Zhang}, Tong.
\newblock {Randomized Dual Coordinate Ascent with Arbitrary Sampling}.
\newblock \emph{arXiv:1411.5873}, 2014.

\bibitem[Richt{\'a}rik \& Tak{\'a}{\v{c}}(2013{\natexlab{a}})Richt{\'a}rik and
  Tak{\'a}{\v{c}}]{Hydra}
Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin.
\newblock Distributed coordinate descent method for learning with big data.
\newblock \emph{arXiv:1310.2059}, 2013{\natexlab{a}}.

\bibitem[Richt{\'a}rik \& Tak{\'a}{\v{c}}(2013{\natexlab{b}})Richt{\'a}rik and
  Tak{\'a}{\v{c}}]{NSync}
Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin.
\newblock On optimal probabilities in stochastic coordinate descent methods.
\newblock \emph{arXiv:1310.3438}, 2013{\natexlab{b}}.

\bibitem[Richt\'{a}rik \& Tak\'{a}\v{c}(2012)Richt\'{a}rik and
  Tak\'{a}\v{c}]{PCDM}
Richt\'{a}rik, Peter and Tak\'{a}\v{c}, Martin.
\newblock Parallel coordinate descent methods for big data optimization
  problems.
\newblock \emph{Mathematical Programming (after minor revision),
  arXiv:1212.0873}, 2012.

\bibitem[Richt\'{a}rik \& Tak\'{a}\v{c}(2014)Richt\'{a}rik and
  Tak\'{a}\v{c}]{UCDC}
Richt\'{a}rik, Peter and Tak\'{a}\v{c}, Martin.
\newblock Iteration complexity of randomized block-coordinate descent methods
  for minimizing a composite function.
\newblock \emph{Mathematical Programming}, 144\penalty0 (2):\penalty0 1--38,
  2014.

\bibitem[Schaul et~al.(2013)Schaul, Zhang, and LeCun]{Nopesky}
Schaul, Tom, Zhang, Sixin, and LeCun, Yann.
\newblock No more pesky learning rates.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (28):\penalty0 343--351, 2013.

\bibitem[Schmidt et~al.(2013)Schmidt, Le~Roux, and Bach]{SAG}
Schmidt, Mark, Le~Roux, Nicolas, and Bach, Francis.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{arXiv:1309.2388}, 2013.

\bibitem[Shalev-Shwartz \& Tewari(2011)Shalev-Shwartz and
  Tewari]{ShalevTewari09}
Shalev-Shwartz, Shai and Tewari, Ambuj.
\newblock Stochastic methods for $\ell_1$-regularized loss minimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 1865--1892,
  2011.

\bibitem[Shalev-Shwartz \& Zhang(2012)Shalev-Shwartz and Zhang]{ProxSDCA}
Shalev-Shwartz, Shai and Zhang, Tong.
\newblock Proximal stochastic dual coordinate ascent.
\newblock \emph{arXiv:1211.2717}, 2012.

\bibitem[Shalev-Shwartz \& Zhang(2013{\natexlab{a}})Shalev-Shwartz and
  Zhang]{ASDCA}
Shalev-Shwartz, Shai and Zhang, Tong.
\newblock Accelerated mini-batch stochastic dual coordinate ascent.
\newblock In \emph{Advances in Neural Information Processing Systems 26}, pp.\
  378--385. 2013{\natexlab{a}}.

\bibitem[Shalev-Shwartz \& Zhang(2013{\natexlab{b}})Shalev-Shwartz and
  Zhang]{SDCA}
Shalev-Shwartz, Shai and Zhang, Tong.
\newblock Stochastic dual coordinate ascent methods for regularized loss.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 567--599, 2013{\natexlab{b}}.

\bibitem[Tak{\'{a}}\v{c} et~al.(2013)Tak{\'{a}}\v{c}, Bijral, Richt{\'{a}}rik,
  and Srebro]{pegasos2}
Tak{\'{a}}\v{c}, Martin, Bijral, Avleen~Singh, Richt{\'{a}}rik, Peter, and
  Srebro, Nathan.
\newblock Mini-batch primal and dual methods for svms.
\newblock \emph{CoRR}, abs/1303.2314, 2013.

\bibitem[Tappenden et~al.(2013)Tappenden, Richt\'{a}rik, and Gondzio]{ICD}
Tappenden, Rachael, Richt\'{a}rik, Peter, and Gondzio, Jacek.
\newblock Inexact block coordinate descent method: complexity and
  preconditioning.
\newblock \emph{arXiv:1304.5530}, 2013.

\bibitem[Tappenden et~al.(2014)Tappenden, Richt\'{a}rik, and B\"{u}ke]{DQA}
Tappenden, Rachael, Richt\'{a}rik, Peter, and B\"{u}ke, Burak.
\newblock Separable approximations and decomposition methods for the augmented
  lagrangian.
\newblock \emph{Optimization Methods and Software}, 2014.

\bibitem[Trofimov \& Genkin(2014)Trofimov and Genkin]{TrofimovGenkin14}
Trofimov, Ilya and Genkin, Alexander.
\newblock Distributed coordinate descent for l1-regularized logistic
  regression.
\newblock \emph{arXiv:1411.6520}, 2014.

\bibitem[Wright(2014)]{ReviewWright}
Wright, Stephen~J.
\newblock Coordinate descent algorithms.
\newblock Technical report, 2014.
\newblock URL
  \url{http://www.optimization-online.org/DB_FILE/2014/12/4679.pdf}.

\bibitem[Xiao \& Zhang(2014)Xiao and Zhang]{proxSVRG}
Xiao, Lin and Zhang, Tong.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{arXiv:1403.4699}, 2014.

\bibitem[Zhang \& Xiao(2014)Zhang and Xiao]{SPDC}
Zhang, Yuchen and Xiao, Lin.
\newblock Stochastic primal-dual coordinate method for regularized empirical
  risk minimization.
\newblock Technical Report MSR-TR-2014-123, September 2014.

\bibitem[Zhao \& Zhang(2014)Zhao and Zhang]{IProx-SDCA}
Zhao, Peilin and Zhang, Tong.
\newblock Stochastic optimization with importance sampling.
\newblock \emph{arXiv:1401.2753}, 2014.

\bibitem[Zhao et~al.(2014{\natexlab{a}})Zhao, Liu, and Zhang]{Pathwisecd}
Zhao, Tuo, Liu, Han, and Zhang, Tong.
\newblock A general theory of pathwise coordinate optimization.
\newblock \emph{arXiv:1412.7477}, 2014{\natexlab{a}}.

\bibitem[Zhao et~al.(2014{\natexlab{b}})Zhao, Yu, Wang, Arora, and
  Liu]{AccMiniBatchRBCDNIPS14}
Zhao, Tuo, Yu, Mo, Wang, Yiming, Arora, Raman, and Liu, Han.
\newblock Accelerated mini-batch randomized block coordinate descent method.
\newblock In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N.D., and
  Weinberger, K.Q. (eds.), \emph{Advances in Neural Information Processing
  Systems 27}, pp.\  3329--3337. Curran Associates, Inc., 2014{\natexlab{b}}.

\end{thebibliography}
