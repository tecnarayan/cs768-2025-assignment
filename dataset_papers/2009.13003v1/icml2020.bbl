\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, et~al.]{abadi2016tensorflow}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., et~al.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
\newblock \emph{arXiv preprint arXiv:1603.04467}, 2016.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1709--1720, 2017.

\bibitem[Baylor et~al.(2017)Baylor, Breck, Cheng, Fiedel, Foo, Haque, Haykal,
  Ispir, Jain, Koc, et~al.]{baylor2017tfx}
Baylor, D., Breck, E., Cheng, H.-T., Fiedel, N., Foo, C.~Y., Haque, Z., Haykal,
  S., Ispir, M., Jain, V., Koc, L., et~al.
\newblock Tfx: A tensorflow-based production-scale machine learning platform.
\newblock In \emph{Proceedings of the 23rd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pp.\  1387--1395, 2017.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{boyd2004convex}
Boyd, S. and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Courbariaux et~al.(2015)Courbariaux, Bengio, and
  David]{courbariaux2015binaryconnect}
Courbariaux, M., Bengio, Y., and David, J.-P.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3123--3131, 2015.

\bibitem[Courbariaux et~al.(2016)Courbariaux, Hubara, Soudry, El-Yaniv, and
  Bengio]{courbariaux2016binarized}
Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized neural networks: Training deep neural networks with weights
  and activations constrained to+ 1 or-1.
\newblock \emph{arXiv preprint arXiv:1602.02830}, 2016.

\bibitem[Dai et~al.(2019)Dai, Yin, and Jha]{dai2019nest}
Dai, X., Yin, H., and Jha, N.~K.
\newblock Nest: A neural network synthesis tool based on a grow-and-prune
  paradigm.
\newblock \emph{IEEE Transactions on Computers}, 68\penalty0 (10):\penalty0
  1487--1497, 2019.

\bibitem[Goldberg et~al.(2001)Goldberg, Roeder, Gupta, and
  Perkins]{goldberg2001eigentaste}
Goldberg, K., Roeder, T., Gupta, D., and Perkins, C.
\newblock Eigentaste: A constant time collaborative filtering algorithm.
\newblock \emph{information retrieval}, 4\penalty0 (2):\penalty0 133--151,
  2001.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., and Courville, A.
\newblock \emph{Deep learning}.
\newblock MIT press, 2016.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{guo2016dynamic}
Guo, Y., Yao, A., and Chen, Y.
\newblock Dynamic network surgery for efficient dnns.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1379--1387, 2016.

\bibitem[Han et~al.(2015{\natexlab{a}})Han, Mao, and Dally]{han2015deep}
Han, S., Mao, H., and Dally, W.~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015{\natexlab{a}}.

\bibitem[Han et~al.(2015{\natexlab{b}})Han, Pool, Tran, and
  Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1135--1143, 2015{\natexlab{b}}.

\bibitem[Harlap et~al.(2017)Harlap, Tumanov, Chung, Ganger, and
  Gibbons]{harlap2017proteus}
Harlap, A., Tumanov, A., Chung, A., Ganger, G.~R., and Gibbons, P.~B.
\newblock Proteus: agile ml elasticity through tiered reliability in dynamic
  resource markets.
\newblock In \emph{Proceedings of the Twelfth European Conference on Computer
  Systems}, pp.\  589--604, 2017.

\bibitem[Harper \& Konstan(2015)Harper and Konstan]{harper2015movielens}
Harper, F.~M. and Konstan, J.~A.
\newblock The movielens datasets: History and context.
\newblock \emph{Acm transactions on interactive intelligent systems (tiis)},
  5\penalty0 (4):\penalty0 1--19, 2015.

\bibitem[Hong et~al.(2016)Hong, Luo, and Razaviyayn]{hong2016convergence}
Hong, M., Luo, Z.-Q., and Razaviyayn, M.
\newblock Convergence analysis of alternating direction method of multipliers
  for a family of nonconvex problems.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (1):\penalty0
  337--364, 2016.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167}, 2015.

\bibitem[Ivkin et~al.(2019)Ivkin, Rothchild, Ullah, Stoica, Arora,
  et~al.]{ivkin2019communication}
Ivkin, N., Rothchild, D., Ullah, E., Stoica, I., Arora, R., et~al.
\newblock Communication-efficient distributed sgd with sketching.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  13144--13154, 2019.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[Kushner \& Yin(2003)Kushner and Yin]{kushner2003stochastic}
Kushner, H. and Yin, G.~G.
\newblock \emph{Stochastic approximation and recursive algorithms and
  applications}, volume~35.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Lai(2009)]{lai2009martingales}
Lai, T.~L.
\newblock Martingales in sequential analysis and time series, 1945--1985.
\newblock \emph{Electronic Journal for history of probability and statistics},
  5\penalty0 (1), 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Leng et~al.(2018)Leng, Dou, Li, Zhu, and Jin]{leng2018extremely}
Leng, C., Dou, Z., Li, H., Zhu, S., and Jin, R.
\newblock Extremely low bit neural network: Squeeze the last bit out with admm.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Li et~al.(2014)Li, Andersen, Park, Smola, Ahmed, Josifovski, Long,
  Shekita, and Su]{li2014scaling}
Li, M., Andersen, D.~G., Park, J.~W., Smola, A.~J., Ahmed, A., Josifovski, V.,
  Long, J., Shekita, E.~J., and Su, B.-Y.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{11th $\{$USENIX$\}$ Symposium on Operating Systems Design
  and Implementation ($\{$OSDI$\}$ 14)}, pp.\  583--598, 2014.

\bibitem[Lin et~al.(2016)Lin, Talathi, and Annapureddy]{lin2016fixed}
Lin, D., Talathi, S., and Annapureddy, S.
\newblock Fixed point quantization of deep convolutional networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2849--2858, 2016.

\bibitem[Low et~al.(2012)Low, Gonzalez, Kyrola, Bickson, Guestrin, and
  Hellerstein]{low2012distributed}
Low, Y., Gonzalez, J., Kyrola, A., Bickson, D., Guestrin, C., and Hellerstein,
  J.~M.
\newblock Distributed graphlab: A framework for machine learning in the cloud.
\newblock \emph{arXiv preprint arXiv:1204.6078}, 2012.

\bibitem[Mao et~al.(2017)Mao, Han, Pool, Li, Liu, Wang, and
  Dally]{mao2017exploring}
Mao, H., Han, S., Pool, J., Li, W., Liu, X., Wang, Y., and Dally, W.~J.
\newblock Exploring the regularity of sparse structure in convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1705.08922}, 2017.

\bibitem[Mogul et~al.(1997)Mogul, Douglis, Feldmann, and
  Krishnamurthy]{mogul1997potential}
Mogul, J.~C., Douglis, F., Feldmann, A., and Krishnamurthy, B.
\newblock Potential benefits of delta encoding and data compression for http.
\newblock In \emph{Proceedings of the ACM SIGCOMM'97 conference on
  Applications, technologies, architectures, and protocols for computer
  communication}, pp.\  181--194, 1997.

\bibitem[Park et~al.(2017)Park, Ahn, and Yoo]{park2017weighted}
Park, E., Ahn, J., and Yoo, S.
\newblock Weighted-entropy-based quantization for deep neural networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  5456--5464, 2017.

\bibitem[Qiao et~al.(2018{\natexlab{a}})Qiao, Aghayev, Yu, Chen, Ho, Gibson,
  and Xing]{qiao2018litz}
Qiao, A., Aghayev, A., Yu, W., Chen, H., Ho, Q., Gibson, G.~A., and Xing, E.~P.
\newblock Litz: Elastic framework for high-performance distributed machine
  learning.
\newblock In \emph{2018 $\{$USENIX$\}$ Annual Technical Conference
  ($\{$USENIX$\}$$\{$ATC$\}$ 18)}, pp.\  631--644, 2018{\natexlab{a}}.

\bibitem[Qiao et~al.(2018{\natexlab{b}})Qiao, Aragam, Zhang, and
  Xing]{qiao2018fault}
Qiao, A., Aragam, B., Zhang, B., and Xing, E.~P.
\newblock Fault tolerance in iterative-convergent machine learning.
\newblock \emph{arXiv preprint arXiv:1810.07354}, 2018{\natexlab{b}}.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi]{rastegari2016xnor}
Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In \emph{European conference on computer vision}, pp.\  525--542.
  Springer, 2016.

\bibitem[Van~Leeuwen(1976)]{van1976construction}
Van~Leeuwen, J.
\newblock On the construction of huffman trees.
\newblock In \emph{ICALP}, pp.\  382--410, 1976.

\bibitem[Woodruff et~al.(2014)]{woodruff2014sketching}
Woodruff, D.~P. et~al.
\newblock Sketching as a tool for numerical linear algebra.
\newblock \emph{Foundations and Trends{\textregistered} in Theoretical Computer
  Science}, 10\penalty0 (1--2):\penalty0 1--157, 2014.

\bibitem[Wu et~al.(2016)Wu, Leng, Wang, Hu, and Cheng]{wu2016quantized}
Wu, J., Leng, C., Wang, Y., Hu, Q., and Cheng, J.
\newblock Quantized convolutional neural networks for mobile devices.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  4820--4828, 2016.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Zhou et~al.(2017)Zhou, Yao, Guo, Xu, and Chen]{zhou2017incremental}
Zhou, A., Yao, A., Guo, Y., Xu, L., and Chen, Y.
\newblock Incremental network quantization: Towards lossless cnns with
  low-precision weights.
\newblock \emph{arXiv preprint arXiv:1702.03044}, 2017.

\end{thebibliography}
