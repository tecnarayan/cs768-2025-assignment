\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson(1982)]{anderson1982reverse}
Anderson, B.~D.
\newblock Reverse-time diffusion equation models.
\newblock \emph{Stochastic Processes and their Applications}, 12\penalty0 (3):\penalty0 313--326, 1982.

\bibitem[Atkinson et~al.(2009)Atkinson, Han, and Stewart]{atkinson2009numerical}
Atkinson, K., Han, W., and Stewart, D.~E.
\newblock \emph{Numerical solution of ordinary differential equations}, volume~81.
\newblock John Wiley \& Sons, 2009.

\bibitem[Austin et~al.(2023)Austin, Johnson, Ho, Tarlow, and van~den Berg]{austin2023structured}
Austin, J., Johnson, D.~D., Ho, J., Tarlow, D., and van~den Berg, R.
\newblock Structured denoising diffusion models in discrete state-spaces, 2023.

\bibitem[Balaji et~al.(2023)Balaji, Nah, Huang, Vahdat, Song, Zhang, Kreis, Aittala, Aila, Laine, Catanzaro, Karras, and Liu]{balaji2023ediffi}
Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Zhang, Q., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., Karras, T., and Liu, M.-Y.
\newblock ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers, 2023.

\bibitem[Bao et~al.(2022{\natexlab{a}})Bao, Li, Sun, Zhu, and Zhang]{bao2022estimating}
Bao, F., Li, C., Sun, J., Zhu, J., and Zhang, B.
\newblock Estimating the optimal covariance with imperfect mean in diffusion probabilistic models.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1555--1584. PMLR, 2022{\natexlab{a}}.

\bibitem[Bao et~al.(2022{\natexlab{b}})Bao, Li, Zhu, and Zhang]{bao2022analytic}
Bao, F., Li, C., Zhu, J., and Zhang, B.
\newblock Analytic-{DPM}: an analytic estimate of the optimal reverse variance in diffusion probabilistic models.
\newblock In \emph{International Conference on Learning Representations}, 2022{\natexlab{b}}.

\bibitem[Bao et~al.(2023)Bao, Nie, Xue, Li, Pu, Wang, Yue, Cao, Su, and Zhu]{bao2023one}
Bao, F., Nie, S., Xue, K., Li, C., Pu, S., Wang, Y., Yue, G., Cao, Y., Su, H., and Zhu, J.
\newblock One transformer fits all distributions in multi-modal diffusion at scale.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1692--1717. PMLR, 2023.

\bibitem[Bar-Tal et~al.(2022)Bar-Tal, Ofri-Amar, Fridman, Kasten, and Dekel]{bar2022text2live}
Bar-Tal, O., Ofri-Amar, D., Fridman, R., Kasten, Y., and Dekel, T.
\newblock Text2live: Text-driven layered image and video editing.
\newblock In \emph{European conference on computer vision}, pp.\  707--723. Springer, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Campbell et~al.(2022)Campbell, Benton, Bortoli, Rainforth, Deligiannidis, and Doucet]{campbell2022continuous}
Campbell, A., Benton, J., Bortoli, V.~D., Rainforth, T., Deligiannidis, G., and Doucet, A.
\newblock A continuous time framework for discrete denoising models, 2022.

\bibitem[Chen et~al.(2020)Chen, Zhang, Zen, Weiss, Norouzi, and Chan]{chen2020wavegrad}
Chen, N., Zhang, Y., Zen, H., Weiss, R.~J., Norouzi, M., and Chan, W.
\newblock Wavegrad: Estimating gradients for waveform generation.
\newblock \emph{arXiv preprint arXiv:2009.00713}, 2020.

\bibitem[Chen et~al.(2022)Chen, Zhang, and Hinton]{chen2023analog}
Chen, T., Zhang, R., and Hinton, G.
\newblock Analog bits: Generating discrete data using diffusion models with self-conditioning.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Dhariwal \& Nichol(2021)Dhariwal and Nichol]{dhariwal2021diffusion}
Dhariwal, P. and Nichol, A.
\newblock Diffusion models beat gans on image synthesis.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 8780--8794, 2021.

\bibitem[Dieleman et~al.(2022)Dieleman, Sartran, Roshannai, Savinov, Ganin, Richemond, Doucet, Strudel, Dyer, Durkan, Hawthorne, Leblond, Grathwohl, and Adler]{dieleman2022continuous}
Dieleman, S., Sartran, L., Roshannai, A., Savinov, N., Ganin, Y., Richemond, P.~H., Doucet, A., Strudel, R., Dyer, C., Durkan, C., Hawthorne, C., Leblond, R., Grathwohl, W., and Adler, J.
\newblock Continuous diffusion for categorical data, 2022.

\bibitem[Graves et~al.(2023)Graves, Srivastava, Atkinson, and Gomez]{graves2023bayesian}
Graves, A., Srivastava, R.~K., Atkinson, T., and Gomez, F.
\newblock Bayesian flow networks, 2023.

\bibitem[Guo et~al.(2023)Guo, Lu, Bao, Pang, Yan, Du, and Li]{guo2023gaussian}
Guo, H., Lu, C., Bao, F., Pang, T., Yan, S., Du, C., and Li, C.
\newblock Gaussian mixture solvers for diffusion models.
\newblock \emph{arXiv preprint arXiv:2311.00941}, 2023.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and Hochreiter]{heusel2017gans}
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S.
\newblock Gans trained by a two time-scale update rule converge to a local nash equilibrium.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6840--6851, 2020.

\bibitem[Ho et~al.(2022)Ho, Chan, Saharia, Whang, Gao, Gritsenko, Kingma, Poole, Norouzi, Fleet, et~al.]{ho2022imagen}
Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D.~P., Poole, B., Norouzi, M., Fleet, D.~J., et~al.
\newblock Imagen video: High definition video generation with diffusion models.
\newblock \emph{arXiv preprint arXiv:2210.02303}, 2022.

\bibitem[Hoogeboom et~al.(2021)Hoogeboom, Nielsen, Jaini, Forré, and Welling]{hoogeboom2021argmax}
Hoogeboom, E., Nielsen, D., Jaini, P., Forré, P., and Welling, M.
\newblock Argmax flows and multinomial diffusion: Learning categorical distributions, 2021.

\bibitem[Hoogeboom et~al.(2022)Hoogeboom, Gritsenko, Bastings, Poole, van~den Berg, and Salimans]{HoogeboomGBPBS22}
Hoogeboom, E., Gritsenko, A.~A., Bastings, J., Poole, B., van~den Berg, R., and Salimans, T.
\newblock Autoregressive diffusion models.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=Lm8T39vLDTE}.

\bibitem[Hyv{\"a}rinen(2005)]{hyvarinen2005estimation}
Hyv{\"a}rinen, A.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 6\penalty0 (Apr):\penalty0 695--709, 2005.

\bibitem[Jolicoeur-Martineau et~al.(2021)Jolicoeur-Martineau, Li, Pich{\'e}-Taillefer, Kachman, and Mitliagkas]{jolicoeur2021gotta}
Jolicoeur-Martineau, A., Li, K., Pich{\'e}-Taillefer, R., Kachman, T., and Mitliagkas, I.
\newblock Gotta go fast when generating data with score-based models.
\newblock \emph{arXiv preprint arXiv:2105.14080}, 2021.

\bibitem[Karras et~al.(2022)Karras, Aittala, Aila, and Laine]{karras2022elucidating}
Karras, T., Aittala, M., Aila, T., and Laine, S.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 26565--26577, 2022.

\bibitem[Kawar et~al.(2023)Kawar, Zada, Lang, Tov, Chang, Dekel, Mosseri, and Irani]{kawar2023imagic}
Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I., and Irani, M.
\newblock Imagic: Text-based real image editing with diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  6007--6017, 2023.

\bibitem[Kingma et~al.(2021)Kingma, Salimans, Poole, and Ho]{kingma2021variational}
Kingma, D., Salimans, T., Poole, B., and Ho, J.
\newblock Variational diffusion models.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 21696--21707, 2021.

\bibitem[Kloeden et~al.(1992)Kloeden, Platen, Kloeden, and Platen]{kloeden1992stochastic}
Kloeden, P.~E., Platen, E., Kloeden, P.~E., and Platen, E.
\newblock \emph{Stochastic differential equations}.
\newblock Springer, 1992.

\bibitem[Kong et~al.(2020)Kong, Ping, Huang, Zhao, and Catanzaro]{kong2020diffwave}
Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B.
\newblock Diffwave: A versatile diffusion model for audio synthesis.
\newblock \emph{arXiv preprint arXiv:2009.09761}, 2020.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical Report~0, University of Toronto, Toronto, Ontario, 2009.
\newblock URL \url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}.

\bibitem[Li et~al.(2022)Li, Thickstun, Gulrajani, Liang, and Hashimoto]{li2022diffusionlm}
Li, X.~L., Thickstun, J., Gulrajani, I., Liang, P., and Hashimoto, T.~B.
\newblock Diffusion-lm improves controllable text generation, 2022.

\bibitem[Liu et~al.(2022)Liu, Ren, Lin, and Zhao]{liu2022pseudo}
Liu, L., Ren, Y., Lin, Z., and Zhao, Z.
\newblock Pseudo numerical methods for diffusion models on manifolds.
\newblock \emph{arXiv preprint arXiv:2202.09778}, 2022.

\bibitem[Lou \& Ermon(2023)Lou and Ermon]{lou2023reflected}
Lou, A. and Ermon, S.
\newblock Reflected diffusion models.
\newblock \emph{arXiv preprint arXiv:2304.04740}, 2023.

\bibitem[Lou et~al.(2023)Lou, Meng, and Ermon]{lou2023discrete}
Lou, A., Meng, C., and Ermon, S.
\newblock Discrete diffusion language modeling by estimating the ratios of the data distribution, 2023.

\bibitem[Lu et~al.(2022{\natexlab{a}})Lu, Zheng, Bao, Chen, Li, and Zhu]{DBLP:conf/icml/LuZB0LZ22}
Lu, C., Zheng, K., Bao, F., Chen, J., Li, C., and Zhu, J.
\newblock Maximum likelihood training for score-based diffusion odes by high order denoising score matching.
\newblock In \emph{International Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  14429--14460. {PMLR}, 2022{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v162/lu22f.html}.

\bibitem[Lu et~al.(2022{\natexlab{b}})Lu, Zhou, Bao, Chen, Li, and Zhu]{lu2022dpm}
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J.
\newblock Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 5775--5787, 2022{\natexlab{b}}.

\bibitem[Lu et~al.(2022{\natexlab{c}})Lu, Zhou, Bao, Chen, Li, and Zhu]{lu2022dpm++}
Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J.
\newblock Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models.
\newblock \emph{arXiv preprint arXiv:2211.01095}, 2022{\natexlab{c}}.

\bibitem[Mahabadi et~al.(2023)Mahabadi, Tae, Ivison, Henderson, Beltagy, Peters, and Cohan]{mahabadi2023tess}
Mahabadi, R.~K., Tae, J., Ivison, H., Henderson, J., Beltagy, I., Peters, M.~E., and Cohan, A.
\newblock Tess: Text-to-text self-conditioned simplex diffusion.
\newblock \emph{arXiv preprint arXiv:2305.08379}, 2023.

\bibitem[Mahoney(2011)]{mahoney2011large}
Mahoney, M.
\newblock Large text compression benchmark, 2011.

\bibitem[Meng et~al.(2023)Meng, Choi, Song, and Ermon]{meng2023concrete}
Meng, C., Choi, K., Song, J., and Ermon, S.
\newblock Concrete score matching: Generalized score matching for discrete data, 2023.

\bibitem[Nie et~al.(2023)Nie, Guo, Lu, Zhou, Zheng, and Li]{nie2023blessing}
Nie, S., Guo, H.~A., Lu, C., Zhou, Y., Zheng, C., and Li, C.
\newblock The blessing of randomness: Sde beats ode in general diffusion-based image editing.
\newblock \emph{arXiv preprint arXiv:2311.01410}, 2023.

\bibitem[{\O}ksendal(2003)]{oksendal2003stochastic}
{\O}ksendal, B.
\newblock \emph{Stochastic differential equations}.
\newblock Springer, 2003.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Pang et~al.(2020)Pang, Xu, Li, Song, Ermon, and Zhu]{pang2020efficient}
Pang, T., Xu, K., Li, C., Song, Y., Ermon, S., and Zhu, J.
\newblock Efficient learning of generative models via finite-difference score matching.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 19175--19188, 2020.

\bibitem[Park et~al.(2020)Park, Zhu, Wang, Lu, Shechtman, Efros, and Zhang]{park2020swapping}
Park, T., Zhu, J.-Y., Wang, O., Lu, J., Shechtman, E., Efros, A., and Zhang, R.
\newblock Swapping autoencoder for deep image manipulation.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 7198--7211, 2020.

\bibitem[Podell et~al.(2023)Podell, English, Lacey, Blattmann, Dockhorn, M{\"u}ller, Penna, and Rombach]{podell2023sdxl}
Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M{\"u}ller, J., Penna, J., and Rombach, R.
\newblock Sdxl: improving latent diffusion models for high-resolution image synthesis.
\newblock \emph{arXiv preprint arXiv:2307.01952}, 2023.

\bibitem[Poole et~al.(2022)Poole, Jain, Barron, and Mildenhall]{poole2022dreamfusion}
Poole, B., Jain, A., Barron, J.~T., and Mildenhall, B.
\newblock Dreamfusion: Text-to-3d using 2d diffusion.
\newblock \emph{arXiv preprint arXiv:2209.14988}, 2022.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and Chen]{ramesh2022hierarchical}
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 1\penalty0 (2):\penalty0 3, 2022.

\bibitem[Richemond et~al.(2022)Richemond, Dieleman, and Doucet]{richemond2022categorical}
Richemond, P.~H., Dieleman, S., and Doucet, A.
\newblock Categorical sdes with simplex diffusion.
\newblock \emph{arXiv preprint arXiv:2210.14784}, 2022.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pp.\  10684--10695, 2022.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton, Ghasemipour, Gontijo~Lopes, Karagol~Ayan, Salimans, et~al.]{saharia2022photorealistic}
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.~L., Ghasemipour, K., Gontijo~Lopes, R., Karagol~Ayan, B., Salimans, T., et~al.
\newblock Photorealistic text-to-image diffusion models with deep language understanding.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 36479--36494, 2022.

\bibitem[Singer et~al.(2022)Singer, Polyak, Hayes, Yin, An, Zhang, Hu, Yang, Ashual, Gafni, et~al.]{singer2022make}
Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., et~al.
\newblock Make-a-video: Text-to-video generation without text-video data.
\newblock \emph{arXiv preprint arXiv:2209.14792}, 2022.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and Ganguli]{sohl2015deep}
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{International conference on machine learning}, pp.\  2256--2265. PMLR, 2015.

\bibitem[Song et~al.(2020)Song, Meng, and Ermon]{song2020denoising}
Song, J., Meng, C., and Ermon, S.
\newblock Denoising diffusion implicit models.
\newblock \emph{arXiv preprint arXiv:2010.02502}, 2020.

\bibitem[Song et~al.(2019)Song, Garg, Shi, and Ermon]{song2019sliced}
Song, Y., Garg, S., Shi, J., and Ermon, S.
\newblock Sliced score matching: A scalable approach to density and score estimation.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence (UAI)}, 2019.

\bibitem[Song et~al.(2021)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole]{song2021scorebased}
Song, Y., Sohl-Dickstein, J., Kingma, D.~P., Kumar, A., Ermon, S., and Poole, B.
\newblock Score-based generative modeling through stochastic differential equations, 2021.

\bibitem[Sun et~al.(2023)Sun, Yu, Dai, Schuurmans, and Dai]{sun2023scorebased}
Sun, H., Yu, L., Dai, B., Schuurmans, D., and Dai, H.
\newblock Score-based continuous-time discrete diffusion models, 2023.

\bibitem[Vincent(2011)]{vincent2011connection}
Vincent, P.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural computation}, 23\penalty0 (7):\penalty0 1661--1674, 2011.

\bibitem[Wang et~al.(2023)Wang, Lu, Wang, Bao, Li, Su, and Zhu]{wang2023prolificdreamer}
Wang, Z., Lu, C., Wang, Y., Bao, F., Li, C., Su, H., and Zhu, J.
\newblock Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation.
\newblock \emph{arXiv preprint arXiv:2305.16213}, 2023.

\bibitem[Xue et~al.(2023{\natexlab{a}})Xue, Yi, Luo, Zhang, Sun, Li, and Ma]{xue2023sa}
Xue, S., Yi, M., Luo, W., Zhang, S., Sun, J., Li, Z., and Ma, Z.-M.
\newblock Sa-solver: Stochastic adams solver for fast sampling of diffusion models.
\newblock \emph{arXiv preprint arXiv:2309.05019}, 2023{\natexlab{a}}.

\bibitem[Xue et~al.(2023{\natexlab{b}})Xue, Song, Guo, Liu, Zong, Liu, and Luo]{xue2023raphael}
Xue, Z., Song, G., Guo, Q., Liu, B., Zong, Z., Liu, Y., and Luo, P.
\newblock Raphael: Text-to-image generation via large mixture of diffusion paths.
\newblock \emph{arXiv preprint arXiv:2305.18295}, 2023{\natexlab{b}}.

\bibitem[Ye et~al.(2023)Ye, Zheng, Bao, Qian, and Wang]{ye2023dinoiser}
Ye, J., Zheng, Z., Bao, Y., Qian, L., and Wang, M.
\newblock Dinoiser: Diffused conditional sequence learning by manipulating noises, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Tao, and Chen]{zhang2022gddim}
Zhang, Q., Tao, M., and Chen, Y.
\newblock gddim: Generalized denoising diffusion implicit models.
\newblock \emph{arXiv preprint arXiv:2206.05564}, 2022.

\bibitem[Zhao et~al.(2023)Zhao, Bai, Rao, Zhou, and Lu]{zhao2023unipc}
Zhao, W., Bai, L., Rao, Y., Zhou, J., and Lu, J.
\newblock Unipc: A unified predictor-corrector framework for fast sampling of diffusion models.
\newblock \emph{arXiv preprint arXiv:2302.04867}, 2023.

\end{thebibliography}
