\begin{thebibliography}{10}

\bibitem{afshar2018copa}
Ardavan Afshar, Ioakeim Perros, Evangelos~E Papalexakis, Elizabeth Searles,
  Joyce Ho, and Jimeng Sun.
\newblock Copa: Constrained parafac2 for sparse \& large datasets.
\newblock In {\em Proceedings of the 27th ACM International Conference on
  Information and Knowledge Management}, pages 793--802, 2018.

\bibitem{bai2020binarybert}
Haoli Bai, Wei Zhang, Lu~Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu,
  Michael Lyu, and Irwin King.
\newblock Binarybert: Pushing the limit of bert quantization.
\newblock {\em arXiv preprint arXiv:2012.15701}, 2020.

\bibitem{berant-etal-2013-semantic}
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.
\newblock Semantic parsing on {F}reebase from question-answer pairs.
\newblock In {\em Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing}, pages 1533--1544, Seattle, Washington, USA,
  October 2013. Association for Computational Linguistics.

\bibitem{gpt-neox}
Sid Black, Stella Biderman, Alex Andonian, Quentin Anthony, Preetham Gali, Leo
  Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip
  Parker, Jason Phang, Michael Pieler, Shivanshu Purohit, Tri Songz, Phil Wang,
  and Samuel Weinbach.
\newblock {GPT-NeoX}: Large scale autoregressive language modeling in pytorch,
  2021.

\bibitem{cutlass}
NVIDIA blog.
\newblock {CUTLASS: Fast Linear Algebra in CUDA C++}.
\newblock \url{https://developer.nvidia.com/blog/cutlass-linear-algebra-cuda/},
  December 2017.

\bibitem{bondarenko2021understanding}
Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
\newblock Understanding and overcoming the challenges of efficient transformer
  quantization.
\newblock {\em arXiv preprint arXiv:2109.12948}, 2021.

\bibitem{boratko2018systematic}
Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj,
  Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan
  Kapanipathi, Nicholas Mattei, et~al.
\newblock A systematic classification of knowledge, reasoning, and context
  within the arc dataset.
\newblock {\em arXiv preprint arXiv:1806.00358}, 2018.

\bibitem{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}, 2020.

\bibitem{cai2020zeroq}
Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael~W Mahoney, and Kurt
  Keutzer.
\newblock Zeroq: A novel zero shot quantization framework.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13169--13178, 2020.

\bibitem{cer2017semeval}
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia.
\newblock Semeval-2017 task 1: Semantic textual similarity-multilingual and
  cross-lingual focused evaluation.
\newblock {\em arXiv preprint arXiv:1708.00055}, 2017.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
  Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no
  questions.
\newblock {\em arXiv preprint arXiv:1905.10044}, 2019.

\bibitem{dagan2013recognizing}
Ido Dagan, Dan Roth, Mark Sammons, and Fabio~Massimo Zanzotto.
\newblock Recognizing textual entailment: Models and applications.
\newblock {\em Synthesis Lectures on Human Language Technologies}, 6(4):1--220,
  2013.

\bibitem{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and {\L}ukasz
  Kaiser.
\newblock Universal transformers.
\newblock {\em arXiv preprint arXiv:1807.03819}, 2018.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{dodge2020fine}
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi,
  and Noah Smith.
\newblock Fine-tuning pretrained language models: Weight initializations, data
  orders, and early stopping.
\newblock {\em arXiv preprint arXiv:2002.06305}, 2020.

\bibitem{dolan2005automatically}
William~B Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In {\em Proceedings of the Third International Workshop on
  Paraphrasing (IWP2005)}, 2005.

\bibitem{dong2019hawq}
Zhen Dong, Zhewei Yao, Amir Gholami, Michael~W Mahoney, and Kurt Keutzer.
\newblock {HAWQ}: Hessian aware quantization of neural networks with
  mixed-precision.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 293--302, 2019.

\bibitem{esser2019learned}
Steven~K Esser, Jeffrey~L McKinstry, Deepika Bablani, Rathinakumar Appuswamy,
  and Dharmendra~S Modha.
\newblock Learned step size quantization.
\newblock {\em arXiv preprint arXiv:1902.08153}, 2019.

\bibitem{fan2019reducing}
Angela Fan, Edouard Grave, and Armand Joulin.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock {\em arXiv preprint arXiv:1909.11556}, 2019.

\bibitem{fan2020training}
Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval, Herve
  Jegou, and Armand Joulin.
\newblock Training with quantization noise for extreme fixed-point compression.
\newblock {\em arXiv preprint arXiv:2004.07320}, 2020.

\bibitem{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{gholami2021survey}
Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael~W Mahoney, and Kurt
  Keutzer.
\newblock A survey of quantization methods for efficient neural network
  inference.
\newblock {\em arXiv preprint arXiv:2103.13630}, 2021.

\bibitem{gordon2020compressing}
Mitchell~A Gordon, Kevin Duh, and Nicholas Andrews.
\newblock Compressing bert: Studying the effects of weight pruning on transfer
  learning.
\newblock {\em arXiv preprint arXiv:2002.08307}, 2020.

\bibitem{han2015learning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In {\em Advances in neural information processing systems}, pages
  1135--1143, 2015.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em Workshop paper in NIPS}, 2014.

\bibitem{iyer2017first}
Shankar Iyer, Nikhil Dandekar, and Kornl Csernai.
\newblock First quora dataset release: Question pairs.(2017).
\newblock {\em URL https://data. quora.
  com/First-Quora-Dataset-Release-Question-Pairs}, 2017.

\bibitem{jiao2019tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu.
\newblock Tinybert: Distilling bert for natural language understanding.
\newblock {\em arXiv preprint arXiv:1909.10351}, 2019.

\bibitem{jin2021kdlsq}
Jing Jin, Cai Liang, Tiancheng Wu, Liqin Zou, and Zhiliang Gan.
\newblock Kdlsq-bert: A quantized bert combining knowledge distillation with
  learned step size quantization.
\newblock {\em arXiv preprint arXiv:2101.05938}, 2021.

\bibitem{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel~S Weld, and Luke Zettlemoyer.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for
  reading comprehension.
\newblock {\em arXiv preprint arXiv:1705.03551}, 2017.

\bibitem{kim2021bert}
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael~W Mahoney, and Kurt Keutzer.
\newblock I-bert: Integer-only bert quantization.
\newblock In {\em International conference on machine learning}, pages
  5506--5518. PMLR, 2021.

\bibitem{lai2017race}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.
\newblock Race: Large-scale reading comprehension dataset from examinations.
\newblock {\em arXiv preprint arXiv:1704.04683}, 2017.

\bibitem{lan2019albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock {\em arXiv preprint arXiv:1909.11942}, 2019.

\bibitem{lecun1990optimal}
Yann LeCun, John~S Denker, and Sara~A Solla.
\newblock Optimal brain damage.
\newblock In {\em Advances in neural information processing systems}, pages
  598--605, 1990.

\bibitem{levesque2012winograd}
Hector Levesque, Ernest Davis, and Leora Morgenstern.
\newblock The winograd schema challenge.
\newblock In {\em Thirteenth International Conference on the Principles of
  Knowledge Representation and Reasoning}. Citeseer, 2012.

\bibitem{li2016ternary}
Fengfu Li, Bo~Zhang, and Bin Liu.
\newblock Ternary weight networks.
\newblock {\em arXiv preprint arXiv:1605.04711}, 2016.

\bibitem{li2016pruning}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock {\em arXiv preprint arXiv:1608.08710}, 2016.

\bibitem{liu2021post}
Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao.
\newblock Post-training quantization for vision transformer.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{mao2017exploring}
Huizi Mao, Song Han, Jeff Pool, Wenshuo Li, Xingyu Liu, Yu~Wang, and William~J
  Dally.
\newblock Exploring the regularity of sparse structure in convolutional neural
  networks.
\newblock {\em Workshop paper in CVPR}, 2017.

\bibitem{mao2020ladabert}
Yihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Yaming Yang, Quanlu
  Zhang, Yunhai Tong, and Jing Bai.
\newblock Ladabert: Lightweight adaptation of bert through hybrid model
  compression.
\newblock {\em arXiv preprint arXiv:2004.04124}, 2020.

\bibitem{marcinkiewicz1994building}
Mary~Ann Marcinkiewicz.
\newblock Building a large annotated corpus of english: The penn treebank.
\newblock {\em Using Large Corpora}, page 273, 1994.

\bibitem{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{michel2019sixteen}
Paul Michel, Omer Levy, and Graham Neubig.
\newblock Are sixteen heads really better than one?
\newblock {\em arXiv preprint arXiv:1905.10650}, 2019.

\bibitem{mihaylov2018can}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock {\em arXiv preprint arXiv:1809.02789}, 2018.

\bibitem{nagel2020up}
Markus Nagel, Rana~Ali Amjad, Mart Van~Baalen, Christos Louizos, and Tijmen
  Blankevoort.
\newblock Up or down? adaptive rounding for post-training quantization.
\newblock In {\em International Conference on Machine Learning}, pages
  7197--7206. PMLR, 2020.

\bibitem{nagel2019data}
Markus Nagel, Mart~van Baalen, Tijmen Blankevoort, and Max Welling.
\newblock Data-free quantization through weight equalization and bias
  correction.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1325--1334, 2019.

\bibitem{paperno2016lambada}
Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham,
  Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
  Fern{\'a}ndez.
\newblock The lambada dataset: Word prediction requiring a broad discourse
  context.
\newblock {\em arXiv preprint arXiv:1606.06031}, 2016.

\bibitem{radford2019gpt}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem{colin2019t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer, 2019.

\bibitem{raganato2020fixed}
Alessandro Raganato, Yves Scherrer, and J{\"o}rg Tiedemann.
\newblock Fixed encoder self-attention patterns in transformer-based machine
  translation.
\newblock {\em arXiv preprint arXiv:2002.10260}, 2020.

\bibitem{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQuAD}: 100,000+ questions for machine comprehension of text.
\newblock {\em arXiv preprint arXiv:1606.05250}, 2016.

\bibitem{rasley2020deepspeed}
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.
\newblock Deepspeed: System optimizations enable training deep learning models
  with over 100 billion parameters.
\newblock In {\em Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 3505--3506, 2020.

\bibitem{nvidia_wmma}
Greg Ruetsch.
\newblock Using tensor cores in cuda fortran.
\newblock {\em Nvidia Blog}, 2021.

\bibitem{sakaguchi2020winogrande}
Keisuke Sakaguchi, Ronan Le~Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 8732--8740, 2020.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{shen2020q}
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
  Michael~W Mahoney, and Kurt Keutzer.
\newblock {Q-BERT}: Hessian based ultra low precision quantization of bert.
\newblock In {\em AAAI}, pages 8815--8821, 2020.

\bibitem{smith2022using}
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
  Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
  Vijay Korthikanti, et~al.
\newblock Using deepspeed and megatron to train megatron-turing nlg 530b, a
  large-scale generative language model.
\newblock {\em arXiv preprint arXiv:2201.11990}, 2022.

\bibitem{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew~Y Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In {\em Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1631--1642, 2013.

\bibitem{sun2019patient}
Siqi Sun, Yu~Cheng, Zhe Gan, and Jingjing Liu.
\newblock Patient knowledge distillation for bert model compression.
\newblock {\em arXiv preprint arXiv:1908.09355}, 2019.

\bibitem{sun2020mobilebert}
Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou.
\newblock Mobilebert: a compact task-agnostic bert for resource-limited
  devices.
\newblock {\em arXiv preprint arXiv:2004.02984}, 2020.

\bibitem{tao2022compression}
Chaofan Tao, Lu~Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and
  Ngai Wong.
\newblock Compression of generative pre-trained language models via
  quantization.
\newblock {\em arXiv preprint arXiv:2203.10705}, 2022.

\bibitem{tata2003piqa}
Sandeep Tata and Jignesh~M Patel.
\newblock Piqa: An algebra for querying protein data sets.
\newblock In {\em 15th International Conference on Scientific and Statistical
  Database Management, 2003.}, pages 141--150. IEEE, 2003.

\bibitem{tenney2019bert}
Ian Tenney, Dipanjan Das, and Ellie Pavlick.
\newblock Bert rediscovers the classical nlp pipeline.
\newblock {\em arXiv:1905.05950}, 2019.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock {\em arXiv preprint arXiv:1804.07461}, 2018.

\bibitem{gpt-j}
Ben Wang and Aran Komatsuzaki.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem{kernel-fusion}
Guibin Wang, Yisong Lin, and Wei Yi.
\newblock Kernel fusion: An effective method for better power efficiency on
  multithreaded {GPU}.
\newblock In Peidong Zhu, Lizhe Wang, Feng Xia, Huajun Chen, Ian McLoughlin,
  Shiao{-}Li Tsao, Mitsuhisa Sato, Sun{-}Ki Chai, and Irwin King, editors, {\em
  2010 {IEEE/ACM} Int'l Conference on Green Computing and Communications,
  GreenCom 2010, {\&} Int'l Conference on Cyber, Physical and Social Computing,
  CPSCom 2010, Hangzhou, China, December 18-20, 2010}, pages 344--350. {IEEE}
  Computer Society, 2010.

\bibitem{wang2020minilm}
Wenhui Wang, Furu Wei, Li~Dong, Hangbo Bao, Nan Yang, and Ming Zhou.
\newblock Minilm: Deep self-attention distillation for task-agnostic
  compression of pre-trained transformers.
\newblock {\em arXiv preprint arXiv:2002.10957}, 2020.

\bibitem{warstadt2018neural}
Alex Warstadt, Amanpreet Singh, and Samuel~R Bowman.
\newblock Neural network acceptability judgments.
\newblock {\em arXiv preprint arXiv:1805.12471}, 2018.

\bibitem{williams2018broad}
Adina Williams, Nikita Nangia, and Samuel Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In {\em Proceedings of the 2018 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long Papers)}, pages 1112--1122, 2018.

\bibitem{williams2020anlizing}
Adina Williams, Tristan Thrush, and Douwe Kiela.
\newblock Anlizing the adversarial natural language inference dataset.
\newblock {\em arXiv preprint arXiv:2010.12729}, 2020.

\bibitem{wolf2019huggingface}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,
  et~al.
\newblock {HuggingFace's Transformers}: State-of-the-art natural language
  processing.
\newblock {\em ArXiv}, pages arXiv--1910, 2019.

\bibitem{yadav2019quick}
Vikas Yadav, Steven Bethard, and Mihai Surdeanu.
\newblock Quick and (not so) dirty: Unsupervised selection of justification
  sentences for multi-hop question answering.
\newblock {\em arXiv preprint arXiv:1911.07176}, 2019.

\bibitem{yao2021mlpruning}
Zhewei Yao, Linjian Ma, Sheng Shen, Kurt Keutzer, and Michael~W Mahoney.
\newblock Mlpruning: A multilevel structured pruning framework for
  transformer-based models.
\newblock {\em arXiv preprint arXiv:2105.14636}, 2021.

\bibitem{zadeh2020gobo}
Ali~Hadi Zadeh, Isak Edo, Omar~Mohamed Awad, and Andreas Moshovos.
\newblock Gobo: Quantizing attention-based nlp models for low latency and
  energy efficient inference.
\newblock In {\em 2020 53rd Annual IEEE/ACM International Symposium on
  Microarchitecture (MICRO)}, pages 811--824. IEEE, 2020.

\bibitem{zafrir2019q8bert}
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
\newblock {Q8BERT}: Quantized 8bit bert.
\newblock {\em arXiv preprint arXiv:1910.06188}, 2019.

\bibitem{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock {\em arXiv preprint arXiv:1905.07830}, 2019.

\bibitem{zhang2018record}
Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin
  Van~Durme.
\newblock Record: Bridging the gap between human and machine commonsense
  reading comprehension.
\newblock {\em arXiv preprint arXiv:1810.12885}, 2018.

\bibitem{zhang2020ternarybert}
Wei Zhang, Lu~Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu.
\newblock Ternarybert: Distillation-aware ultra-low bit bert.
\newblock {\em arXiv preprint arXiv:2009.12812}, 2020.

\end{thebibliography}
