\begin{thebibliography}{62}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2012)Agarwal, Negahban, Wainwright,
  et~al.]{agarwal2012fast}
Agarwal, A., Negahban, S., Wainwright, M.~J., et~al.
\newblock Fast global convergence of gradient methods for high-dimensional
  statistical recovery.
\newblock \emph{The Annals of Statistics}, 40\penalty0 (5):\penalty0
  2452--2482, 2012.

\bibitem[Aksoylar et~al.(2017)Aksoylar, Orecchia, and
  Saligrama]{aksoylar2017connected}
Aksoylar, C., Orecchia, L., and Saligrama, V.
\newblock Connected subgraph detection with mirror descent on sdps.
\newblock In \emph{International Conference on Machine Learning}, pp.\  51--59,
  2017.

\bibitem[Arias-Castro et~al.(2011)Arias-Castro, Cand{\`e}s, and
  Durand]{arias2011detection}
Arias-Castro, E., Cand{\`e}s, E.~J., and Durand, A.
\newblock Detection of an anomalous cluster in a network.
\newblock \emph{The Annals of Statistics}, pp.\  278--304, 2011.

\bibitem[Bach \& Moulines(2013)Bach and Moulines]{Bach}
Bach, F. and Moulines, E.
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate o (1/n).
\newblock In \emph{Advances in neural information processing systems}, pp.\
  773--781, 2013.

\bibitem[Bach et~al.(2012{\natexlab{a}})Bach, Jenatton, Mairal, Obozinski,
  et~al.]{bach2012optimization}
Bach, F., Jenatton, R., Mairal, J., Obozinski, G., et~al.
\newblock Optimization with sparsity-inducing penalties.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  4\penalty0 (1):\penalty0 1--106, 2012{\natexlab{a}}.

\bibitem[Bach et~al.(2012{\natexlab{b}})Bach, Jenatton, Mairal, Obozinski,
  et~al.]{bach2012structured}
Bach, F., Jenatton, R., Mairal, J., Obozinski, G., et~al.
\newblock Structured sparsity through convex optimization.
\newblock \emph{Statistical Science}, 27\penalty0 (4):\penalty0 450--468,
  2012{\natexlab{b}}.

\bibitem[Bahmani et~al.(2013)Bahmani, Raj, and Boufounos]{bahmani2013greedy}
Bahmani, S., Raj, B., and Boufounos, P.~T.
\newblock Greedy sparsity-constrained optimization.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0
  (Mar):\penalty0 807--841, 2013.

\bibitem[Baraniuk et~al.(2010)Baraniuk, Cevher, Duarte, and
  Hegde]{baraniuk2010model}
Baraniuk, R.~G., Cevher, V., Duarte, M.~F., and Hegde, C.
\newblock Model-based compressive sensing.
\newblock \emph{IEEE Transactions on Information Theory}, 56\penalty0
  (4):\penalty0 1982--2001, 2010.

\bibitem[Blumensath \& Davies(2009)Blumensath and
  Davies]{blumensath2009iterative}
Blumensath, T. and Davies, M.~E.
\newblock Iterative hard thresholding for compressed sensing.
\newblock \emph{Applied and computational harmonic analysis}, 27\penalty0
  (3):\penalty0 265--274, 2009.

\bibitem[Blumensath \& Davies(2010)Blumensath and
  Davies]{blumensath2010normalized}
Blumensath, T. and Davies, M.~E.
\newblock Normalized iterative hard thresholding: Guaranteed stability and
  performance.
\newblock \emph{IEEE Journal of selected topics in signal processing},
  4\penalty0 (2):\penalty0 298--309, 2010.

\bibitem[Bousquet \& Bottou(2008)Bousquet and Bottou]{bousquet2008tradeoffs}
Bousquet, O. and Bottou, L.
\newblock The tradeoffs of large scale learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  161--168, 2008.

\bibitem[Candes \& Tao(2005)Candes and Tao]{candes2005decoding}
Candes, E.~J. and Tao, T.
\newblock Decoding by linear programming.
\newblock \emph{IEEE transactions on information theory}, 51\penalty0
  (12):\penalty0 4203--4215, 2005.

\bibitem[Charles \& Papailiopoulos(2018)Charles and
  Papailiopoulos]{charles2018stability}
Charles, Z. and Papailiopoulos, D.
\newblock Stability and generalization of learning algorithms that converge to
  global optima.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  744--753, 2018.

\bibitem[Chen et~al.(2001)Chen, Donoho, and Saunders]{chen2001atomic}
Chen, S.~S., Donoho, D.~L., and Saunders, M.~A.
\newblock Atomic decomposition by basis pursuit.
\newblock \emph{SIAM review}, 43\penalty0 (1):\penalty0 129--159, 2001.

\bibitem[Chuang et~al.(2007)Chuang, Lee, Liu, Lee, and
  Ideker]{chuang2007network}
Chuang, H.-Y., Lee, E., Liu, Y.-T., Lee, D., and Ideker, T.
\newblock Network-based classification of breast cancer metastasis.
\newblock \emph{Molecular systems biology}, 3\penalty0 (1):\penalty0 140, 2007.

\bibitem[Couch et~al.(2017)Couch, Shimelis, Hu, Hart, Polley, Na, Hallberg,
  Moore, Thomas, Lilyquist, et~al.]{couch2017associations}
Couch, F.~J., Shimelis, H., Hu, C., Hart, S.~N., Polley, E.~C., Na, J.,
  Hallberg, E., Moore, R., Thomas, A., Lilyquist, J., et~al.
\newblock Associations between cancer predisposition testing panel genes and
  breast cancer.
\newblock \emph{JAMA oncology}, 3\penalty0 (9):\penalty0 1190--1196, 2017.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1646--1654, 2014.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Elenberg et~al.(2018)Elenberg, Khanna, Dimakis, Negahban,
  et~al.]{elenberg2018restricted}
Elenberg, E.~R., Khanna, R., Dimakis, A.~G., Negahban, S., et~al.
\newblock Restricted strong convexity implies weak submodularity.
\newblock \emph{The Annals of Statistics}, 46\penalty0 (6B):\penalty0
  3539--3568, 2018.

\bibitem[Gy{\"o}rffy et~al.(2010)Gy{\"o}rffy, Lanczky, Eklund, Denkert,
  Budczies, Li, and Szallasi]{gyorffy2010online}
Gy{\"o}rffy, B., Lanczky, A., Eklund, A.~C., Denkert, C., Budczies, J., Li, Q.,
  and Szallasi, Z.
\newblock An online survival analysis tool to rapidly assess the effect of
  22,277 genes on breast cancer prognosis using microarray data of 1,809
  patients.
\newblock \emph{Breast cancer research and treatment}, 123\penalty0
  (3):\penalty0 725--731, 2010.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
Hardt, M., Recht, B., and Singer, Y.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1225--1234, 2016.

\bibitem[Hegde et~al.(2014)Hegde, Indyk, and Schmidt]{hegde2014fast}
Hegde, C., Indyk, P., and Schmidt, L.
\newblock A fast approximation algorithm for tree-sparse recovery.
\newblock In \emph{Information Theory (ISIT), 2014 IEEE International Symposium
  on}, pp.\  1842--1846. IEEE, 2014.

\bibitem[Hegde et~al.(2015{\natexlab{a}})Hegde, Indyk, and
  Schmidt]{hegde2015approximation}
Hegde, C., Indyk, P., and Schmidt, L.
\newblock Approximation algorithms for model-based compressive sensing.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (9):\penalty0 5129--5147, 2015{\natexlab{a}}.

\bibitem[Hegde et~al.(2015{\natexlab{b}})Hegde, Indyk, and
  Schmidt]{hegde2015nearly}
Hegde, C., Indyk, P., and Schmidt, L.
\newblock {A} nearly-linear time framework for graph-structured sparsity.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning (ICML-15)}, pp.\  928--937, 2015{\natexlab{b}}.

\bibitem[Hegde et~al.(2016)Hegde, Indyk, and Schmidt]{hegde2016fast}
Hegde, C., Indyk, P., and Schmidt, L.
\newblock Fast recovery from a union of subspaces.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4394--4402, 2016.

\bibitem[Jacob et~al.(2009)Jacob, Obozinski, and Vert]{jacob2009group}
Jacob, L., Obozinski, G., and Vert, J.-P.
\newblock Group lasso with overlap and graph lasso.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pp.\  433--440. ACM, 2009.

\bibitem[Jain et~al.(2014)Jain, Tewari, and Kar]{jain2014iterative}
Jain, P., Tewari, A., and Kar, P.
\newblock On iterative hard thresholding methods for high-dimensional
  m-estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  685--693, 2014.

\bibitem[Jenatton et~al.(2011)Jenatton, Audibert, and
  Bach]{jenatton2011structured}
Jenatton, R., Audibert, J.-Y., and Bach, F.
\newblock Structured variable selection with sparsity-inducing norms.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Oct):\penalty0 2777--2824, 2011.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and
  Jordan]{jin2017escape}
Jin, C., Ge, R., Netrapalli, P., Kakade, S.~M., and Jordan, M.~I.
\newblock How to escape saddle points efficiently.
\newblock \emph{arXiv preprint arXiv:1703.00887}, 2017.

\bibitem[Johnson et~al.(2000)Johnson, Minkoff, and Phillips]{johnson2000prize}
Johnson, D.~S., Minkoff, M., and Phillips, S.
\newblock The prize collecting steiner tree problem: theory and practice.
\newblock In \emph{Proceedings of the eleventh annual ACM-SIAM symposium on
  Discrete algorithms}, pp.\  760--769. Society for Industrial and Applied
  Mathematics, 2000.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson2013accelerating}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  315--323, 2013.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Li \& Orabona(2018)Li and Orabona]{li2018convergence}
Li, X. and Orabona, F.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock \emph{arXiv preprint arXiv:1805.08114}, 2018.

\bibitem[Liu et~al.(2017)Liu, Yuan, Wang, Liu, and Metaxas]{liu2017dual}
Liu, B., Yuan, X.-T., Wang, L., Liu, Q., and Metaxas, D.~N.
\newblock Dual iterative hard thresholding: From non-convex sparse minimization
  to non-smooth concave maximization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2179--2187, 2017.

\bibitem[Lov{\'a}sz et~al.(1993)]{lovasz1993random}
Lov{\'a}sz, L. et~al.
\newblock Random walks on graphs: A survey.
\newblock \emph{Combinatorics}, 2(1):\penalty0 1--46, 1993.

\bibitem[Morales et~al.(2010)Morales, Micchelli, and Pontil]{morales2010family}
Morales, J., Micchelli, C.~A., and Pontil, M.
\newblock A family of penalty functions for structured sparsity.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1612--1623, 2010.

\bibitem[Murata \& Suzuki(2018)Murata and Suzuki]{murata2018sample}
Murata, T. and Suzuki, T.
\newblock Sample efficient stochastic gradient iterative hard thresholding
  method for stochastic sparse linear regression with limited attribute
  observation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5313--5322, 2018.

\bibitem[Needell \& Tropp(2009)Needell and Tropp]{needell2009cosamp}
Needell, D. and Tropp, J.~A.
\newblock Cosamp: Iterative signal recovery from incomplete and inaccurate
  samples.
\newblock \emph{Applied and computational harmonic analysis}, 26\penalty0
  (3):\penalty0 301--321, 2009.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Nguyen et~al.(2017)Nguyen, Needell, and Woolf]{nguyen2017linear}
Nguyen, N., Needell, D., and Woolf, T.
\newblock Linear convergence of stochastic iterative greedy algorithms with
  sparse constraints.
\newblock \emph{IEEE Transactions on Information Theory}, 63\penalty0
  (11):\penalty0 6869--6895, 2017.

\bibitem[Obozinski \& Bach(2016)Obozinski and Bach]{obozinski2016unified}
Obozinski, G. and Bach, F.
\newblock A unified perspective on convex structured sparsity: Hierarchical,
  symmetric, submodular norms and beyond.
\newblock \emph{HAL}, 2016.

\bibitem[Phaedra et~al.(2009)Phaedra, Yiming, and Colin]{phaedra2009bayesian}
Phaedra, A., Yiming, Y., and Colin, C.
\newblock Bayesian unsupervised learning with multiple data types.
\newblock \emph{Statistical Applications in Genetics and Molecular Biology},
  8\penalty0 (1):\penalty0 1--27, 2009.

\bibitem[Qian et~al.(2014)Qian, Saligrama, and Chen]{qian2014connected}
Qian, J., Saligrama, V., and Chen, Y.
\newblock Connected sub-graph detection.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  796--804,
  2014.

\bibitem[Rakhlin et~al.(2012)Rakhlin, Shamir, and Sridharan]{rakhlin2012making}
Rakhlin, A., Shamir, O., and Sridharan, K.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In \emph{Proceedings of the 29th International Conference on Machine
  Learning}, pp.\  449--456, 2012.

\bibitem[Rheinbay et~al.(2017)Rheinbay, Parasuraman, Grimsby, Tiao, Engreitz,
  Kim, Lawrence, Taylor-Weiner, Rodriguez-Cuevas, Rosenberg,
  et~al.]{rheinbay2017recurrent}
Rheinbay, E., Parasuraman, P., Grimsby, J., Tiao, G., Engreitz, J.~M., Kim, J.,
  Lawrence, M.~S., Taylor-Weiner, A., Rodriguez-Cuevas, S., Rosenberg, M.,
  et~al.
\newblock Recurrent and functional regulatory mutations in breast cancer.
\newblock \emph{Nature}, 547\penalty0 (7661):\penalty0 55--60, 2017.

\bibitem[Rosasco et~al.(2014)Rosasco, Villa, and
  V{\~u}]{rosasco2014convergence}
Rosasco, L., Villa, S., and V{\~u}, B.~C.
\newblock Convergence of stochastic proximal gradient algorithm.
\newblock \emph{arXiv preprint arXiv:1403.5074}, 2014.

\bibitem[Rozenshtein et~al.(2014)Rozenshtein, Anagnostopoulos, Gionis, and
  Tatti]{rozenshtein2014event}
Rozenshtein, P., Anagnostopoulos, A., Gionis, A., and Tatti, N.
\newblock Event detection in activity networks.
\newblock In \emph{Proceedings of the 20th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pp.\  1176--1185. ACM, 2014.

\bibitem[Shamir \& Zhang(2013)Shamir and Zhang]{shamir2013stochastic}
Shamir, O. and Zhang, T.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In \emph{International Conference on Machine Learning}, pp.\  71--79,
  2013.

\bibitem[Shen \& Li(2017)Shen and Li]{shen2017tight}
Shen, J. and Li, P.
\newblock A tight bound of hard thresholding.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 7650--7691, 2017.

\bibitem[Tibshirani(1996)]{tibshirani1996regression}
Tibshirani, R.
\newblock Regression shrinkage and selection via the lasso.
\newblock \emph{Journal of the Royal Statistical Society. Series B
  (Methodological)}, pp.\  267--288, 1996.

\bibitem[Tropp(2012)]{tropp2012user}
Tropp, J.~A.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock \emph{Foundations of computational mathematics}, 12\penalty0
  (4):\penalty0 389--434, 2012.

\bibitem[Turlach et~al.(2005)Turlach, Venables, and
  Wright]{turlach2005simultaneous}
Turlach, B.~A., Venables, W.~N., and Wright, S.~J.
\newblock Simultaneous variable selection.
\newblock \emph{Technometrics}, 47\penalty0 (3):\penalty0 349--363, 2005.

\bibitem[Van De~Vijver et~al.(2002)Van De~Vijver, He, Van't~Veer, Dai, Hart,
  Voskuil, Schreiber, Peterse, Roberts, Marton, et~al.]{van2002gene}
Van De~Vijver, M.~J., He, Y.~D., Van't~Veer, L.~J., Dai, H., Hart, A.~A.,
  Voskuil, D.~W., Schreiber, G.~J., Peterse, J.~L., Roberts, C., Marton, M.~J.,
  et~al.
\newblock A gene-expression signature as a predictor of survival in breast
  cancer.
\newblock \emph{New England Journal of Medicine}, 347\penalty0 (25):\penalty0
  1999--2009, 2002.

\bibitem[Van~Ngai \& Penot(2008)Van~Ngai and Penot]{van2008paraconvex}
Van~Ngai, H. and Penot, J.-P.
\newblock Paraconvex functions and paraconvex sets.
\newblock \emph{Studia Mathematica}, 184:\penalty0 1--29, 2008.

\bibitem[Yang \& Lin(2015)Yang and Lin]{yang2015rsg}
Yang, T. and Lin, Q.
\newblock Rsg: Beating subgradient method without smoothness and strong
  convexity.
\newblock \emph{arXiv preprint arXiv:1512.03107}, 2015.

\bibitem[Ying \& Zhou(2006)Ying and Zhou]{ying2006online}
Ying, Y. and Zhou, D.-X.
\newblock Online regularized classification algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 52\penalty0
  (11):\penalty0 4775--4788, 2006.

\bibitem[Ying \& Zhou(2017)Ying and Zhou]{ying2017unregularized}
Ying, Y. and Zhou, D.-X.
\newblock Unregularized online learning algorithms with general loss functions.
\newblock \emph{Applied and Computational Harmonic Analysis}, 42\penalty0
  (2):\penalty0 224--244, 2017.

\bibitem[Yu et~al.(2016)Yu, Qiu, Wen, Lin, and Liu]{yu2016survey}
Yu, R., Qiu, H., Wen, Z., Lin, C., and Liu, Y.
\newblock A survey on social media anomaly detection.
\newblock \emph{ACM SIGKDD Explorations Newsletter}, 18\penalty0 (1):\penalty0
  1--14, 2016.

\bibitem[Yuan \& Lin(2006)Yuan and Lin]{yuan2006model}
Yuan, M. and Lin, Y.
\newblock Model selection and estimation in regression with grouped variables.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 68\penalty0 (1):\penalty0 49--67, 2006.

\bibitem[Yuan et~al.(2018)Yuan, Li, and Zhang]{yuan2018gradient}
Yuan, X.-T., Li, P., and Zhang, T.
\newblock Gradient hard thresholding pursuit.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (166):\penalty0 1--43, 2018.

\bibitem[Zhou et~al.(2018{\natexlab{a}})Zhou, Yuan, and
  Feng]{zhou2018efficient}
Zhou, P., Yuan, X., and Feng, J.
\newblock Efficient stochastic gradient hard thresholding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1985--1994, 2018{\natexlab{a}}.

\bibitem[Zhou et~al.(2018{\natexlab{b}})Zhou, Yuan, and Feng]{zhou2018new}
Zhou, P., Yuan, X., and Feng, J.
\newblock New insight into hybrid stochastic gradient descent: Beyond
  with-replacement sampling and convexity.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1242--1251, 2018{\natexlab{b}}.

\end{thebibliography}
