
@inproceedings{aladagoSlotMachinesDiscovering2021,
  title = {Slot {{Machines}}: {{Discovering Winning Combinations}} of {{Random Weights}} in {{Neural Networks}}},
  shorttitle = {Slot {{Machines}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Aladago, Maxwell M. and Torresani, Lorenzo},
  year = {2021},
  month = jul,
  pages = {163--174},
  publisher = {{PMLR}},
  abstract = {In contrast to traditional weight optimization in a continuous space, we demonstrate the existence of effective random networks whose weights are never updated. By selecting a weight among a fixed ...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/Q27C6X7E/Aladago and Torresani - 2021 - Slot Machines Discovering Winning Combinations of Random Weights in Neural Networks.pdf}
}

@article{appalarajuGoodPracticesSelfsupervised2020,
  title = {Towards {{Good Practices}} in {{Self-supervised Representation Learning}}},
  author = {Appalaraju, Srikar and Zhu, Yi and Xie, Yusheng and Feh{\'e}rv{\'a}ri, Istv{\'a}n},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.00868 [cs]},
  eprint = {2012.00868},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Self-supervised representation learning has seen remarkable progress in the last few years. More recently, contrastive instance learning has shown impressive results compared to its supervised learning counterparts. However, even with the ever increased interest in contrastive instance learning, it is still largely unclear why these methods work so well. In this paper, we aim to unravel some of the mysteries behind their success, which are the good practices. Through an extensive empirical analysis, we hope to not only provide insights but also lay out a set of best practices that led to the success of recent work in self-supervised representation learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/FKQN85VQ/Appalaraju et al. - 2020 - Towards Good Practices in Self-supervised Represen.pdf}
}

@article{arpitHowInitializeYour2019,
  title = {How to {{Initialize}} Your {{Network}}? {{Robust Initialization}} for {{WeightNorm}} \& {{ResNets}}},
  shorttitle = {How to {{Initialize}} Your {{Network}}?},
  author = {Arpit, Devansh and Campos, Victor and Bengio, Yoshua},
  year = {2019},
  month = oct,
  journal = {arXiv:1906.02341 [cs, stat]},
  eprint = {1906.02341},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Residual networks (ResNet) and weight normalization play an important role in various deep learning applications. However, parameter initialization strategies have not been studied previously for weight normalized networks and, in practice, initialization methods designed for un-normalized networks are used as a proxy. Similarly, initialization for ResNets have also been studied for un-normalized networks and often under simplified settings ignoring the shortcut connection. To address these issues, we propose a novel parameter initialization strategy that avoids explosion/vanishment of information across layers for weight normalized networks with and without residual connections. The proposed strategy is based on a theoretical analysis using mean field approximation. We run over 2,500 experiments and evaluate our proposal on image datasets showing that the proposed initialization outperforms existing initialization methods in terms of generalization performance, robustness to hyper-parameter values and variance between seeds, especially when networks get deeper in which case existing methods fail to even start training. Finally, we show that using our initialization in conjunction with learning rate warmup is able to reduce the gap between the performance of weight normalized and batch normalized networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/X7XI5IMM/Arpit et al. - 2019 - How to Initialize your Network Robust Initializat.pdf}
}

@article{baehrensHowExplainIndividual,
  title = {How to {{Explain Individual Classification Decisions}}},
  author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja},
  pages = {29},
  abstract = {After building a classifier with modern tools of machine learning we typically have a black box at hand that is able to predict well for unseen data. Thus, we get an answer to the question what is the most likely label of a given unseen data point. However, most methods will provide no answer why the model predicted a particular label for a single instance and what features were most influential for that particular instance. The only method that is currently able to provide such explanations are decision trees. This paper proposes a procedure which (based on a set of assumptions) allows to explain the decisions of any classification method.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/MRMCU6GH/Baehrens et al. - How to Explain Individual ClassiÔ¨Åcation Decisions.pdf}
}

@article{baevskiData2vecGeneralFramework,
  title = {Data2vec: {{A General Framework}} for {{Self-supervised Learning}} in {{Speech}}, {{Vision}} and {{Language}}},
  author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
  pages = {13},
  abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a selfdistillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches. Models and code are available at www.github.com/pytorch/fairseq/ tree/master/examples/data2vec.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/49VKCV2C/Baevski et al. - data2vec A General Framework for Self-supervised .pdf}
}

@inproceedings{bartanTrainingQuantizedNeural2021,
  title = {Training {{Quantized Neural Networks}} to {{Global Optimality}} via {{Semidefinite Programming}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Bartan, Burak and Pilanci, Mert},
  year = {2021},
  month = jul,
  pages = {694--704},
  publisher = {{PMLR}},
  abstract = {Neural networks (NNs) have been extremely successful across many tasks in machine learning. Quantization of NN weights has become an important topic due to its impact on their energy efficiency, in...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/8DI2EXXV/Bartan and Pilanci - 2021 - Training Quantized Neural Networks to Global Optimality via Semidefinite Programming.pdf}
}

@article{bauNetworkDissectionQuantifying2017,
  title = {Network {{Dissection}}: {{Quantifying Interpretability}} of {{Deep Visual Representations}}},
  shorttitle = {Network {{Dissection}}},
  author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  year = {2017},
  month = apr,
  journal = {arXiv:1704.05796 [cs]},
  eprint = {1704.05796},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,I.2.10},
  file = {/Users/konstantinschurholt/Zotero/storage/GSH3EMU8/Bau et al. - 2017 - Network Dissection Quantifying Interpretability o.pdf}
}

@article{belkinReconcilingModernMachinelearning2019,
  title = {Reconciling Modern Machine-Learning Practice and the Classical Bias\textendash Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Mandal, Soumik},
  year = {2019},
  journal = {PNAS},
  volume = {116},
  number = {32},
  doi = {10.1073/pnas.1903070116},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/62QW5RZN/Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf;/Users/konstantinschurholt/Zotero/storage/GIBM8RLN/Reconciling modern machine-learning practice and t.pdf;/Users/konstantinschurholt/Zotero/storage/MLVVLRDI/pnas.html}
}

@article{bengioRepresentationLearningReview2014,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  shorttitle = {Representation {{Learning}}},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year = {2014},
  month = apr,
  journal = {arXiv:1206.5538 [cs]},
  eprint = {1206.5538},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/MAGKT24S/Bengio et al. - 2014 - Representation Learning A Review and New Perspect.pdf;/Users/konstantinschurholt/Zotero/storage/UATMHV9T/Bengio et al. - 2014 - Representation Learning A Review and New Perspect.pdf}
}

@inproceedings{bentonLossSurfaceSimplexes2021,
  title = {Loss {{Surface Simplexes}} for {{Mode Connecting Volumes}} and {{Fast Ensembling}}},
  booktitle = {{{PMLR}}},
  author = {Benton, Gregory W. and Maddox, Wesley J. and Lotfi, Sanae and Wilson, Andrew Gordon},
  year = {2021},
  eprint = {2102.13042},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {With a better understanding of the loss surfaces for multilayer networks, we can build more robust and accurate training procedures. Recently it was discovered that independently trained SGD solutions can be connected along one-dimensional paths of near-constant training loss. In this paper, we show that there are in fact mode-connecting simplicial complexes that form multi-dimensional manifolds of low loss, connecting many independently trained models. Inspired by this discovery, we show how to efficiently build simplicial complexes for fast ensembling, outperforming independently trained deep ensembles in accuracy, calibration, and robustness to dataset shift. Notably, our approach only requires a few training epochs to discover a low-loss simplex, starting from a pre-trained solution. Code is available at https://github.com/g-benton/ loss-surface-simplexes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/PCPJIKY9/Benton et al. - 2021 - Loss Surface Simplexes for Mode Connecting Volumes.pdf}
}

@misc{berardiLearningSpaceDeep2022,
  title = {Learning the {{Space}} of {{Deep Models}}},
  author = {Berardi, Gianluca and De Luigi, Luca and Salti, Samuele and Di Stefano, Luigi},
  year = {2022},
  month = jun,
  number = {arXiv:2206.05194},
  eprint = {2206.05194},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Embedding of large but redundant data, such as images or text, in a hierarchy of lower-dimensional spaces is one of the key features of representation learning approaches, which nowadays provide state-of-the-art solutions to problems once believed hard or impossible to solve. In this work1, in a plot twist with a strong meta aftertaste, we show how trained deep models are as redundant as the data they are optimized to process, and how it is therefore possible to use deep learning models to embed deep learning models. In particular, we show that it is possible to use representation learning to learn a fixed-size, lowdimensional embedding space of trained deep models and that such space can be explored by interpolation or optimization to attain ready-to-use models. We find that it is possible to learn an embedding space of multiple instances of the same architecture and of multiple architectures. We address image classification and neural representation of signals, showing how our embedding space can be learnt so as to capture the notions of performance and 3D shape, respectively. In the Multi-Architecture setting we also show how an embedding trained only on a subset of architectures can learn to generate already-trained instances of architectures it never sees instantiated at training time.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/KRCRXRR6/Berardi et al. - 2022 - Learning the Space of Deep Models.pdf}
}

@article{birdalIntrinsicDimensionPersistent2021,
  title = {Intrinsic {{Dimension}}, {{Persistent Homology}} and {{Generalization}} in {{Neural Networks}}},
  author = {Birdal, Tolga and Lou, Aaron and Guibas, Leonidas and {\c S}im{\c s}ekli, Umut},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.13171 [cs, math, stat]},
  eprint = {2111.13171},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Disobeying the classical wisdom of statistical learning theory, modern deep neural networks generalize well even though they typically contain millions of parameters. Recently, it has been shown that the trajectories of iterative optimization algorithms can possess fractal structures, and their generalization error can be formally linked to the complexity of such fractals. This complexity is measured by the fractal's intrinsic dimension, a quantity usually much smaller than the number of parameters in the network. Even though this perspective provides an explanation for why overparametrized networks would not overfit, computing the intrinsic dimension (e.g., for monitoring generalization during training) is a notoriously difficult task, where existing methods typically fail even in moderate ambient dimensions. In this study, we consider this problem from the lens of topological data analysis (TDA) and develop a generic computational tool that is built on rigorous mathematical foundations. By making a novel connection between learning theory and TDA, we first illustrate that the generalization error can be equivalently bounded in terms of a notion called the 'persistent homology dimension' (PHD), where, compared with prior work, our approach does not require any additional geometrical or statistical assumptions on the training dynamics. Then, by utilizing recently established theoretical results and TDA tools, we develop an efficient algorithm to estimate PHD in the scale of modern deep neural networks and further provide visualization tools to help understand generalization in deep learning. Our experiments show that the proposed approach can efficiently compute a network's intrinsic dimension in a variety of settings, which is predictive of the generalization error.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Mathematics - General Topology,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/UW3B7Q4H/Birdal et al. - 2021 - Intrinsic Dimension, Persistent Homology and Gener.pdf}
}

@inproceedings{blumTraining3NodeNeural1988,
  title = {Training a 3-{{Node Neural Network}} Is {{NP-Complete}}},
  booktitle = {{{NIPS}}},
  author = {Blum, Avrim and Rivest, Ronald L},
  year = {1988},
  pages = {8},
  abstract = {We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for the three nodes of this network so that it will produce output consistent with a given set of training examples. We extend the result to other simple networks. This result suggests that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. It also suggests the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with non-linear functions such as sigmoids.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/MCVF9WMU/Blum and Rivest - Training a 3-Node Neural Network is NP-Complete.pdf}
}

@article{blundellWeightUncertaintyNeural2015,
  title = {Weight {{Uncertainty}} in {{Neural Networks}}},
  author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
  year = {2015},
  month = may,
  journal = {arXiv:1505.05424 [cs, stat]},
  eprint = {1505.05424},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/WVHRNRFR/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf}
}

@article{bond-taylorDeepGenerativeModelling2021,
  title = {Deep {{Generative Modelling}}: {{A Comparative Review}} of {{VAEs}}, {{GANs}}, {{Normalizing Flows}}, {{Energy-Based}} and {{Autoregressive Models}}},
  shorttitle = {Deep {{Generative Modelling}}},
  author = {{Bond-Taylor}, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.04922 [cs, stat]},
  eprint = {2103.04922},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep generative modelling is a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which making trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are drawn under a single cohesive framework, comparing and contrasting to explain the premises behind each, while reviewing current state-of-the-art advances and implementations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T01 (Primary); 68T07 (Secondary),Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,G.3,I.4.0,I.5.0,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/XRDYV2EJ/Bond-Taylor et al. - 2021 - Deep Generative Modelling A Comparative Review of.pdf}
}

@inproceedings{borthLargescaleVisualSentiment2013,
  title = {Large-Scale Visual Sentiment Ontology and Detectors Using Adjective Noun Pairs},
  booktitle = {Proceedings of the 21st {{ACM}} International Conference on {{Multimedia}}},
  author = {Borth, Damian and Ji, Rongrong and Chen, Tao and Breuel, Thomas and Chang, Shih-Fu},
  year = {2013},
  month = oct,
  series = {{{MM}} '13},
  pages = {223--232},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2502081.2502282},
  abstract = {We address the challenge of sentiment analysis from visual content. In contrast to existing methods which infer sentiment or emotion directly from visual low-level features, we propose a novel approach based on understanding of the visual concepts that are strongly related to sentiments. Our key contribution is two-fold: first, we present a method built upon psychological theories and web mining to automatically construct a large-scale Visual Sentiment Ontology (VSO) consisting of more than 3,000 Adjective Noun Pairs (ANP). Second, we propose SentiBank, a novel visual concept detector library that can be used to detect the presence of 1,200 ANPs in an image. The VSO and SentiBank are distinct from existing work and will open a gate towards various applications enabled by automatic sentiment analysis. Experiments on detecting sentiment of image tweets demonstrate significant improvement in detection accuracy when comparing the proposed SentiBank based predictors with the text-based approaches. The effort also leads to a large publicly available resource consisting of a visual sentiment ontology, a large detector library, and the training/testing benchmark for visual sentiment analysis.},
  isbn = {978-1-4503-2404-5},
  keywords = {concept detection,ontology,sentiment prediction,social multimedia},
  file = {/Users/konstantinschurholt/Zotero/storage/XDL7PDKU/Borth et al. - 2013 - Large-scale visual sentiment ontology and detector.pdf}
}

@article{bousmalisDomainSeparationNetworks2016,
  title = {Domain {{Separation Networks}}},
  author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
  year = {2016},
  month = aug,
  journal = {arXiv:1608.06019 [cs]},
  eprint = {1608.06019},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The cost of large scale data collection and annotation often makes the application of machine learning algorithms to new tasks or datasets prohibitively expensive. One approach circumventing this cost is training models on synthetic data where annotations are provided automatically. Despite their appeal, such models often fail to generalize from synthetic to real images, necessitating domain adaptation algorithms to manipulate these models before they can be successfully applied. Existing approaches focus either on mapping representations from one domain to the other, or on learning to extract features that are invariant to the domain from which they were extracted. However, by focusing only on creating a mapping or shared representation between the two domains, they ignore the individual characteristics of each domain. We suggest that explicitly modeling what is unique to each domain can improve a model's ability to extract domain\textendash invariant features. Inspired by work on private\textendash shared component analysis, we explicitly learn to extract image representations that are partitioned into two subspaces: one component which is private to each domain and one which is shared across domains. Our model is trained not only to perform the task we care about in the source domain, but also to use the partitioned representation to reconstruct the images from both domains. Our novel architecture results in a model that outperforms the state\textendash of\textendash the\textendash art on a range of unsupervised domain adaptation scenarios and additionally produces visualizations of the private and shared representations enabling interpretation of the domain adaptation process.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/MDETGR8A/Bousmalis et al. - 2016 - Domain Separation Networks.pdf}
}

@article{bousmalisUnsupervisedPixelLevelDomain2016,
  title = {Unsupervised {{Pixel-Level Domain Adaptation}} with {{Generative Adversarial Networks}}},
  author = {Bousmalis, Konstantinos and Silberman, Nathan and Dohan, David and Erhan, Dumitru and Krishnan, Dilip},
  year = {2016},
  month = dec,
  journal = {arXiv:1612.05424 [cs]},
  eprint = {1612.05424},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. An appealing alternative is to render synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images often fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that attempt to map representations between the two domains or learn to extract features that are domain\textendash invariant. In this work, we present a new approach that learns, in an unsupervised manner, a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)\textendash based model adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/JCHL4EDC/Bousmalis et al. - 2016 - Unsupervised Pixel-Level Domain Adaptation with Ge.pdf}
}

@article{breaWeightspaceSymmetryDeep2019,
  title = {Weight-Space Symmetry in Deep Networks Gives Rise to Permutation Saddles, Connected by Equal-Loss Valleys across the Loss Landscape},
  author = {Brea, Johanni and Simsek, Berfin and Illing, Bernd and Gerstner, Wulfram},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.02911 [cs, stat]},
  eprint = {1907.02911},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The permutation symmetry of neurons in each layer of a deep neural network gives rise not only to multiple equivalent global minima of the loss function, but also to first-order saddle points located on the path between the global minima. In a network of \$d-1\$ hidden layers with \$n\_k\$ neurons in layers \$k = 1, \textbackslash ldots, d\$, we construct smooth paths between equivalent global minima that lead through a `permutation point' where the input and output weight vectors of two neurons in the same hidden layer \$k\$ collide and interchange. We show that such permutation points are critical points with at least \$n\_\{k+1\}\$ vanishing eigenvalues of the Hessian matrix of second derivatives indicating a local plateau of the loss function. We find that a permutation point for the exchange of neurons \$i\$ and \$j\$ transits into a flat valley (or generally, an extended plateau of \$n\_\{k+1\}\$ flat dimensions) that enables all \$n\_k!\$ permutations of neurons in a given layer \$k\$ at the same loss value. Moreover, we introduce high-order permutation points by exploiting the recursive structure in neural network functions, and find that the number of \$K\^\{\textbackslash text\{th\}\}\$-order permutation points is at least by a factor \$\textbackslash sum\_\{k=1\}\^\{d-1\}\textbackslash frac\{1\}\{2!\^K\}\{n\_k-K \textbackslash choose K\}\$ larger than the (already huge) number of equivalent global minima. In two tasks, we illustrate numerically that some of the permutation points correspond to first-order saddles (`permutation saddles'): first, in a toy network with a single hidden layer on a function approximation task and, second, in a multilayer network on the MNIST task. Our geometric approach yields a lower bound on the number of critical points generated by weight-space symmetries and provides a simple intuitive link between previous mathematical results and numerical observations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/PCNC2244/Brea et al. - 2019 - Weight-space symmetry in deep networks gives rise .pdf}
}

@article{bressonTwoStepGraphConvolutional2019,
  title = {A {{Two-Step Graph Convolutional Decoder}} for {{Molecule Generation}}},
  author = {Bresson, Xavier and Laurent, Thomas},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.03412 [cs, stat]},
  eprint = {1906.03412},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a simple auto-encoder framework for molecule generation. The molecular graph is first encoded into a continuous latent representation z, which is then decoded back to a molecule. The encoding process is easy, but the decoding process remains challenging. In this work, we introduce a simple two-step decoding process. In a first step, a fully connected neural network uses the latent vector z to produce a molecular formula, for example CO2 (one carbon and two oxygen atoms). In a second step, a graph convolutional neural network uses the same latent vector z to place bonds between the atoms that were produced in the first step (for example a double bond will be placed between the carbon and each of the oxygens). This two-step process, in which a bag of atoms is first generated, and then assembled, provides a simple framework that allows us to develop an efficient molecule auto-encoder. Numerical experiments on basic tasks such as novelty, uniqueness, validity and optimized chemical property for the 250k ZINC molecules demonstrate the performances of the proposed system. Particularly, we achieve the highest reconstruction rate of 90.5\%, improving the previous rate of 76.7\%. We also report the best property improvement results when optimization is constrained by the molecular distance between the original and generated molecules.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/8PJB8R9V/Bresson and Laurent - 2019 - A Two-Step Graph Convolutional Decoder for Molecul.pdf}
}

@article{bronsteinGeometricDeepLearning2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veli{\v c}kovi{\'c}, Petar},
  year = {2021},
  month = may,
  journal = {arXiv:2104.13478 [cs, stat]},
  eprint = {2104.13478},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/HW9CMTR2/Bronstein et al. - 2021 - Geometric Deep Learning Grids, Groups, Graphs, Ge.pdf}
}

@misc{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  number = {arXiv:2005.14165},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions \textendash{} something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/konstantinschurholt/Zotero/storage/2YU64NLZ/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@article{caronEmergingPropertiesSelfSupervised2021,
  title = {Emerging {{Properties}} in {{Self-Supervised Vision Transformers}}},
  author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  year = {2021},
  month = may,
  journal = {arXiv:2104.14294 [cs]},
  eprint = {2104.14294},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/W9E6L65D/Caron et al. - 2021 - Emerging Properties in Self-Supervised Vision Tran.pdf}
}

@article{caronUnsupervisedLearningVisual2021,
  title = {Unsupervised {{Learning}} of {{Visual Features}} by {{Contrasting Cluster Assignments}}},
  author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  year = {2021},
  month = jan,
  journal = {arXiv:2006.09882 [cs]},
  eprint = {2006.09882},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or ``views'') of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a ``swapped'' prediction mechanism where we predict the code of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/DNMT2UY7/Caron et al. - 2021 - Unsupervised Learning of Visual Features by Contra.pdf}
}

@article{caruanaCaseBasedExplanationNonCaseBased,
  title = {Case-{{Based Explanation}} of {{Non-Case-Based Learning Methods}}},
  author = {Caruana, Rich and Kangarloo, Hooshang and David, John and Dionisio, N and Sinha, Usha and Johnson, David},
  pages = {4},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/3J6VQYTT/Caruana et al. - Case-Based Explanation of Non-Case-Based Learning .pdf}
}

@inproceedings{caruanaEnsembleSelectionLibraries2004,
  title = {Ensemble Selection from Libraries of Models},
  booktitle = {Twenty-First International Conference on {{Machine}} Learning  - {{ICML}} '04},
  author = {Caruana, Rich and {Niculescu-Mizil}, Alexandru and Crew, Geoff and Ksikes, Alex},
  year = {2004},
  pages = {18},
  publisher = {{ACM Press}},
  address = {{Banff, Alberta, Canada}},
  doi = {10.1145/1015330.1015432},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/C7JXWNYG/1015330.pdf}
}

@article{caruanaLearningManyRelated,
  title = {Learning {{Many Related Tasks}} at the {{Same Time}} with {{Backpropagation}}},
  author = {Caruana, Rich},
  pages = {8},
  abstract = {Hinton [6] proposed that generalization in artificial neural nets should improve if nets learn to represent the domain's underlying regularities . Abu-Mustafa's hints work [1] shows that the outputs of a backprop net can be used as inputs through which domainspecific information can be given to the net . We extend these ideas by showing that a backprop net learning many related tasks at the same time can use these tasks as inductive bias for each other and thus learn better. We identify five mechanisms by which multitask backprop improves generalization and give empirical evidence that multitask backprop generalizes better in real domains.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/NGGIJKFH/Caruana - Learning Many Related Tasks at the Same Time with .pdf}
}

@inproceedings{cazenavetteDatasetDistillationMatching2022,
  title = {Dataset {{Distillation}} by {{Matching Training Trajectories}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Cazenavette, George and Wang, Tongzhou and Torralba, Antonio and Efros, Alexei A and Zhu, Jun-Yan},
  year = {2022},
  pages = {10},
  abstract = {Dataset distillation is the task of synthesizing a small dataset such that a model trained on the synthetic set will match the test accuracy of the model trained on the full dataset. In this paper, we propose a new formulation that optimizes our distilled data to guide networks to a similar state as those trained on real data across many training steps. Given a network, we train it for several iterations on our distilled data and optimize the distilled data with respect to the distance between the synthetically trained parameters and the parameters trained on real data. To efficiently obtain the initial and target network parameters for large-scale datasets, we pre-compute and store training trajectories of expert networks trained on the real dataset. Our method handily outperforms existing methods and also allows us to distill higher-resolution visual data.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/HC7F3VXZ/Cazenavette et al. - Dataset Distillation by Matching Training Trajecto.pdf}
}

@article{chakrabortyBiasMachineLearning2021,
  title = {Bias in {{Machine Learning Software}}: {{Why}}? {{How}}? {{What}} to Do?},
  shorttitle = {Bias in {{Machine Learning Software}}},
  author = {Chakraborty, Joymallya and Majumder, Suvodeep and Menzies, Tim},
  year = {2021},
  month = may,
  journal = {arXiv:2105.12195 [cs]},
  eprint = {2105.12195},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.1145/3468264.3468537},
  abstract = {Increasingly, software is making autonomous decisions in case of criminal sentencing, approving credit cards, hiring employees, and so on. Some of these decisions show bias and adversely affect certain social groups (e.g. those defined by sex, race, age, marital status). Many prior works on bias mitigation take the following form: change the data or learners in multiple ways, then see if any of that improves fairness. Perhaps a better approach is to postulate root causes of bias and then applying some resolution strategy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/konstantinschurholt/Zotero/storage/WQNKWJM4/Chakraborty et al. - 2021 - Bias in Machine Learning Software Why How What .pdf}
}

@article{chenCLOSERLOOKFEWSHOT2019,
  title = {A {{CLOSER LOOK AT FEW-SHOT CLASSIFICATION}}},
  author = {Chen, Wei-Yu and Wang, Yu-Chiang Frank and Liu, Yen-Cheng and Kira, Zsolt and Huang, Jia-Bin},
  year = {2019},
  pages = {16},
  abstract = {Few-shot classification aims to learn a classifier to recognize unseen classes during training with limited labeled examples. While significant progress has been made, the growing complexity of network designs, meta-learning algorithms, and differences in implementation details make a fair comparison difficult. In this paper, we present 1) a consistent comparative analysis of several representative few-shot classification algorithms, with results showing that deeper backbones significantly reduce the performance differences among methods on datasets with limited domain differences, 2) a modified baseline method that surprisingly achieves competitive performance when compared with the state-of-the-art on both the miniImageNet and the CUB datasets, and 3) a new experimental setting for evaluating the cross-domain generalization ability for few-shot classification algorithms. Our results reveal that reducing intra-class variation is an important factor when the feature backbone is shallow, but not as critical when using deeper backbones. In a realistic cross-domain evaluation setting, we show that a baseline method with a standard fine-tuning practice compares favorably against other state-of-the-art few-shot learning algorithms.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/BHGZ2VTW/Chen et al. - 2019 - A CLOSER LOOK AT FEW-SHOT CLASSIFICATION.pdf}
}

@article{chenExploringSimpleSiamese2021,
  title = {Exploring {{Simple Siamese Representation Learning}}},
  author = {Chen, Xinlei and He, Kaiming},
  year = {2021},
  journal = {CVPR},
  pages = {9},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/FQ6RNIL3/Chen and He - Exploring Simple Siamese Representation Learning.pdf}
}

@article{chenGraphRepresentationLearning2020,
  title = {Graph {{Representation Learning}}: {{A Survey}}},
  shorttitle = {Graph {{Representation Learning}}},
  author = {Chen, Fenxiao and Wang, Yuncheng and Wang, Bin and Kuo, C.-C. Jay},
  year = {2020},
  journal = {APSIPA Transactions on Signal and Information Processing},
  volume = {9},
  eprint = {1909.00958},
  eprinttype = {arxiv},
  pages = {e15},
  issn = {2048-7703},
  doi = {10.1017/ATSIP.2020.13},
  abstract = {Research on graph representation learning has received a lot of attention in recent years since many data in real-world applications come in form of graphs. High-dimensional graph data are often in irregular form, which makes them more difficult to analyze than image/video/audio data defined on regular lattices. Various graph embedding techniques have been developed to convert the raw graph data into a low-dimensional vector representation while preserving the intrinsic graph properties. In this review, we first explain the graph embedding task and its challenges. Next, we review a wide range of graph embedding techniques with insights. Then, we evaluate several state-of-the-art methods against small and large datasets and compare their performance. Finally, potential applications and future directions are presented.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/QCRMIRQ2/Chen et al. - 2020 - Graph Representation Learning A Survey.pdf}
}

@article{chenImprovedBaselinesMomentum2020,
  title = {Improved {{Baselines}} with {{Momentum Contrastive Learning}}},
  author = {Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.04297 [cs]},
  eprint = {2003.04297},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Contrastive unsupervised learning has recently shown encouraging progress, e.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the effectiveness of two of SimCLR's design improvements by implementing them in the MoCo framework. With simple modifications to MoCo\textemdash namely, using an MLP projection head and more data augmentation\textemdash we establish stronger baselines that outperform SimCLR and do not require large training batches. We hope this will make state-of-the-art unsupervised learning research more accessible. Code will be made public.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/MJ8HAX5A/Chen et al. - 2020 - Improved Baselines with Momentum Contrastive Learn.pdf}
}

@article{chenIntriguingPropertiesContrastive2020,
  title = {Intriguing {{Properties}} of {{Contrastive Losses}}},
  author = {Chen, Ting and Li, Lala},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.02803 [cs, stat]},
  eprint = {2011.02803},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Contrastive loss and its variants have become very popular recently for learning visual representations without supervision. In this work, we first generalize the standard contrastive loss based on cross entropy to a broader family of losses that share an abstract form of Lalignment + {$\lambda$}Ldistribution, where hidden representations are encouraged to (1) be aligned under some transformations/augmentations, and (2) match a prior distribution of high entropy. We show that various instantiations of the generalized loss perform similarly under the presence of a multi-layer nonlinear projection head, and the temperature scaling ({$\tau$} ) widely used in the standard contrastive loss is (within a range) inversely related to the weighting ({$\lambda$}) between the two loss terms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/J4Y49AG9/Chen and Li - 2020 - Intriguing Properties of Contrastive Losses.pdf}
}

@inproceedings{chenNet2NetAcceleratingLearning2016,
  title = {{{Net2Net}}: {{Accelerating Learning}} via {{Knowledge Transfer}}},
  shorttitle = {{{Net2Net}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  year = {2016},
  month = apr,
  doi = {10.48550/arXiv.1511.05641},
  abstract = {We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/FDBJSMNE/Chen et al. - 2016 - Net2Net Accelerating Learning via Knowledge Trans.pdf;/Users/konstantinschurholt/Zotero/storage/N9L8K3NJ/1511.html}
}

@article{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = jun,
  journal = {arXiv:2002.05709 [cs, stat]},
  eprint = {2002.05709},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/VVG7PWAT/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf}
}

@article{choromanskiRethinkingAttentionPerformers2021,
  title = {Rethinking {{Attention}} with {{Performers}}},
  author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  year = {2021},
  month = mar,
  journal = {arXiv:2009.14794 [cs, stat]},
  eprint = {2009.14794},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attentionkernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can also be used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/3GZ6D8DZ/Choromanski et al. - 2021 - Rethinking Attention with Performers.pdf}
}

@inproceedings{coatesAnalysisSingleLayerNetworks2011,
  title = {An {{Analysis}} of {{Single-Layer Networks}} in {{Unsupervised Feature Learning}}},
  booktitle = {Proceedings of the 14th {{International Con-}} Ference on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Coates, Adam and Lee, Honglak and Ng, Andrew Y},
  year = {2011},
  pages = {9},
  abstract = {A great deal of research has focused on algorithms for learning features from unlabeled data. Indeed, much progress has been made on benchmark datasets like NORB and CIFAR by employing increasingly complex unsupervised learning algorithms and deep models. In this paper, however, we show that several simple factors, such as the number of hidden nodes in the model, may be more important to achieving high performance than the learning algorithm or the depth of the model. Specifically, we will apply several offthe-shelf feature learning algorithms (sparse auto-encoders, sparse RBMs, K-means clustering, and Gaussian mixtures) to CIFAR, NORB, and STL datasets using only singlelayer networks. We then present a detailed analysis of the effect of changes in the model setup: the receptive field size, number of hidden nodes (features), the step-size (``stride'') between extracted features, and the effect of whitening. Our results show that large numbers of hidden nodes and dense feature extraction are critical to achieving high performance\textemdash so critical, in fact, that when these parameters are pushed to their limits, we achieve state-of-the-art performance on both CIFAR-10 and NORB using only a single layer of features. More surprisingly, our best performance is based on K-means clustering, which is extremely fast, has no hyperparameters to tune beyond the model structure itself, and is very easy to implement. Despite the simplicity of our system, we achieve accuracy beyond all previously published results on the CIFAR-10 and NORB datasets (79.6\% and 97.2\% respectively).},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/ANFNABX3/Coates et al. - An Analysis of Single-Layer Networks in Unsupervis.pdf}
}

@inproceedings{corneanuComputingTestingError2020,
  title = {Computing the {{Testing Error Without}} a {{Testing Set}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Corneanu, Ciprian A. and Escalera, Sergio and Martinez, Aleix M.},
  year = {2020},
  month = jun,
  pages = {2674--2682},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00275},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/2UPC8346/Corneanu et al. - 2020 - Computing the Testing Error Without a Testing Set.pdf}
}

@article{criscitielloEfficientlyEscapingSaddle2019,
  title = {Efficiently Escaping Saddle Points on Manifolds},
  author = {Criscitiello, Chris and Boumal, Nicolas},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.04321 [cs, math]},
  eprint = {1906.04321},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Smooth, non-convex optimization problems on Riemannian manifolds occur in machine learning as a result of orthonormality, rank or positivity constraints. First- and second-order necessary optimality conditions state that the Riemannian gradient must be zero, and the Riemannian Hessian must be positive semidefinite. Generalizing Jin et al.'s recent work on perturbed gradient descent (PGD) for optimization on linear spaces [How to Escape Saddle Points Efficiently (2017), Stochastic Gradient Descent Escapes Saddle Points Efficiently (2019)], we propose a version of perturbed Riemannian gradient descent (PRGD) to show that necessary optimality conditions can be met approximately with high probability, without evaluating the Hessian. Specifically, for an arbitrary Riemannian manifold \$\textbackslash mathcal\{M\}\$ of dimension \$d\$, a sufficiently smooth (possibly non-convex) objective function \$f\$, and under weak conditions on the retraction chosen to move on the manifold, with high probability, our version of PRGD produces a point with gradient smaller than \$\textbackslash epsilon\$ and Hessian within \$\textbackslash sqrt\{\textbackslash epsilon\}\$ of being positive semidefinite in \$O((\textbackslash log\{d\})\^4 / \textbackslash epsilon\^\{2\})\$ gradient queries. This matches the complexity of PGD in the Euclidean case. Crucially, the dependence on dimension is low. This matters for large-scale applications including PCA and low-rank matrix completion, which both admit natural formulations on manifolds. The key technical idea is to generalize PRGD with a distinction between two types of gradient steps: "steps on the manifold" and "perturbed steps in a tangent space of the manifold." Ultimately, this distinction makes it possible to extend Jin et al.'s analysis seamlessly.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computational Complexity,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/konstantinschurholt/Zotero/storage/TYUMGDWJ/Criscitiello and Boumal - 2019 - Efficiently escaping saddle points on manifolds.pdf}
}

@article{dabkowskiRealTimeImage2017,
  title = {Real {{Time Image Saliency}} for {{Black Box Classifiers}}},
  author = {Dabkowski, Piotr and Gal, Yarin},
  year = {2017},
  month = may,
  journal = {arXiv:1705.07857 [stat]},
  eprint = {1705.07857},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {In this work we develop a fast saliency detection method that can be applied to any differentiable image classifier. We train a masking model to manipulate the scores of the classifier by masking salient parts of the input image. Our model generalises well to unseen images and requires a single forward pass to perform saliency detection, therefore suitable for use in real-time systems. We test our approach on CIFAR-10 and ImageNet datasets and show that the produced saliency maps are easily interpretable, sharp, and free of artifacts. We suggest a new metric for saliency and test our method on the ImageNet object localisation task. We achieve results outperforming other weakly supervised methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/MN9JHRGR/Dabkowski and Gal - 2017 - Real Time Image Saliency for Black Box Classifiers.pdf}
}

@inproceedings{daiScalableDeepGenerative2020,
  title = {Scalable {{Deep Generative Modeling}} for {{Sparse Graphs}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Dai, Hanjun and Nazi, Azade and Li, Yujia and Dai, Bo and Schuurmans, Dale},
  year = {2020},
  month = nov,
  pages = {2302--2312},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Learning graph generative models is a challenging task for deep learning and has wide applicability to a range of domains like chemistry, biology and social science. However current deep neural methods suffer from limited scalability: for a graph with n nodes and m edges, existing deep neural methods require Omega(n\^2) complexity by building up the adjacency matrix. On the other hand, many real world graphs are actually sparse in the sense that m {$<<$} n\^2. Based on this, we develop a novel autoregressive model, named BiGG, that utilizes this sparsity to avoid generating the full adjacency matrix, and importantly reduces the graph generation time complexity to O((n + m) log n). Furthermore, during training this autoregressive model can be parallelized with O(log n) synchronization stages, which makes it much more efficient than other autoregressive models that require Omega(n). Experiments on several benchmarks show that the proposed approach not only scales to orders of magnitude larger graphs than previously possible with deep autoregressive graph generative models, but also yields better graph generation quality.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/7UGLUB3F/Dai et al. - 2020 - Scalable Deep Generative Modeling for Sparse Graph.pdf;/Users/konstantinschurholt/Zotero/storage/G6XHUC6H/Dai et al. - 2020 - Scalable Deep Generative Modeling for Sparse Graph.pdf}
}

@article{dascoliDeepSymbolicRegression2022,
  title = {Deep {{Symbolic Regression}} for {{Recurrent Sequences}}},
  author = {{d'Ascoli}, St{\'e}phane and Kamienny, Pierre-Alexandre and Lample, Guillaume and Charton, Fran{\c c}ois},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.04600 [cs]},
  eprint = {2201.04600},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Symbolic regression, i.e. predicting a function from the observation of its values, is well-known to be a challenging task. In this paper, we train Transformers to infer the function or recurrence relation underlying sequences of integers or floats, a typical task in human IQ tests which has hardly been tackled in the machine learning literature. We evaluate our integer model on a subset of OEIS sequences, and show that it outperforms built-in Mathematica functions for recurrence prediction. We also demonstrate that our float model is able to yield informative approximations of out-of-vocabulary functions and constants, e.g. \$\textbackslash operatorname\{bessel0\}(x)\textbackslash approx \textbackslash frac\{\textbackslash sin(x)+\textbackslash cos(x)\}\{\textbackslash sqrt\{\textbackslash pi x\}\}\$ and \$1.644934\textbackslash approx \textbackslash pi\^2/6\$. An interactive demonstration of our models is provided at https://bit.ly/3niE5FS.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/3P4APD9X/d'Ascoli et al. - 2022 - Deep Symbolic Regression for Recurrent Sequences.pdf}
}

@inproceedings{dauphinIdentifyingAttackingSaddle2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  booktitle = {{{NIPS}}},
  author = {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  year = {2014},
  pages = {9},
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/VA5RGHY9/Dauphin et al. - Identifying and attacking the saddle point problem.pdf}
}

@inproceedings{dauphinMetaInitInitializingLearning2019,
  title = {{{MetaInit}}: {{Initializing}} Learning by Learning to Initialize},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Dauphin, Yann N and Schoenholz, Samuel},
  year = {2019},
  pages = {13},
  abstract = {Deep learning models frequently trade handcrafted features for deep features learned with much less human intervention using gradient descent. While this paradigm has been enormously successful, deep networks are often difficult to train and performance can depend crucially on the initial choice of parameters. In this work, we introduce an algorithm called MetaInit as a step towards automating the search for good initializations using meta-learning. Our approach is based on a hypothesis that good initializations make gradient descent easier by starting in regions that look locally linear with minimal second order effects. We formalize this notion via a quantity that we call the gradient quotient, which can be computed with any architecture or dataset. MetaInit minimizes this quantity efficiently by using gradient descent to tune the norms of the initial weight matrices. We conduct experiments on plain and residual networks and show that the algorithm can automatically recover from a class of bad initializations. MetaInit allows us to train networks and achieve performance competitive with the state-of-the-art without batch normalization or residual connections. In particular, we find that this approach outperforms normalization for networks without skip connections on CIFAR-10 and can scale to Resnet-50 models on Imagenet.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/2DIVTVEV/Dauphin and Schoenholz - MetaInit Initializing learning by learning to ini.pdf;/Users/konstantinschurholt/Zotero/storage/FLPCR9TH/Dauphin and Schoenholz - MetaInit Initializing learning by learning to ini.pdf}
}

@inproceedings{dechantPredictingAccuracyNeural2019,
  title = {Predicting the Accuracy of Neural Networks from Final and Intermediate Layer Outputs},
  booktitle = {{{ICML}}},
  author = {DeChant, Chad and Han, Seungwook and Lipson, Hod},
  year = {2019},
  abstract = {We show that information about whether a neural network's output will be correct or incorrect is present in the outputs of the network's intermedi- ate layers. To demonstrate this effect, we train a new ''meta'' network to predict from either the fi- nal output of the underlying ''base'' network or the output of one of the base network's intermediate layers whether the base network will be correct or incorrect for a particular input. We find that, over a wide range of tasks and base networks, the meta network can achieve accuracies ranging from 65\% - 85\% in making this determination.},
  file = {/Users/konstantinschurholt/Zotero/storage/X6LEAFT9/DeChant et al. - 2019 - Predicting the accuracy of neural networks from fi.pdf}
}

@article{dengImageNetLargeScaleHierarchical,
  title = {{{ImageNet}}: {{A Large-Scale Hierarchical Image Database}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  pages = {8},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/F2PF2LLJ/Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf}
}

@inproceedings{denilPredictingParametersDeep2013,
  title = {Predicting {{Parameters}} in {{Deep Learning}}},
  booktitle = {Neural {{Information Processing Systems}} ({{NeurIPS}})},
  author = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'Aurelio},
  year = {2013},
  pages = {9},
  abstract = {We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95\% of the weights of a network without any drop in accuracy.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/JB35HU72/Denil et al. - Predicting Parameters in Deep Learning.pdf}
}

@inproceedings{desousaOverviewWeightInitialization2016,
  title = {An Overview on Weight Initialization Methods for Feedforward Neural Networks},
  booktitle = {2016 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {{de Sousa}, Celso A. R.},
  year = {2016},
  month = jul,
  pages = {52--59},
  publisher = {{IEEE}},
  address = {{Vancouver, BC, Canada}},
  doi = {10.1109/IJCNN.2016.7727180},
  abstract = {Feedforward neural networks are neural networks with (possibly) multiple layers of neurons such that each layer is fully connected to the next one. They have been widely studied in the past partially due to their universal approximation capabilities and empirical effectiveness on a variety of application domains for both regression and classification tasks. In this paper, we provide an overview on feedforward neural networks, focusing on weight initialization methods.},
  isbn = {978-1-5090-0620-5},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/YGRS6A7C/de Sousa - 2016 - An overview on weight initialization methods for f.pdf}
}

@article{deutschGeneratingNeuralNetworks2018,
  title = {Generating {{Neural Networks}} with {{Neural Networks}}},
  author = {Deutsch, Lior},
  year = {2018},
  month = apr,
  journal = {arXiv:1801.01952 [cs, stat]},
  eprint = {1801.01952},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Hypernetworks are neural networks that generate weights for another neural network. We formulate the hypernetwork training objective as a compromise between accuracy and diversity, where the diversity takes into account trivial symmetry transformations of the target network. We explain how this simple formulation generalizes variational inference. We use multi-layered perceptrons to form the mapping from the low dimensional input random vector to the high dimensional weight space, and demonstrate how to reduce the number of parameters in this mapping by parameter sharing. We perform experiments and show that the generated weights are diverse and lie on a non-trivial manifold.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/J8DE4EAR/Deutsch - 2018 - Generating Neural Networks with Neural Networks.pdf}
}

@article{doerschUnsupervisedVisualRepresentation2016,
  title = {Unsupervised {{Visual Representation Learning}} by {{Context Prediction}}},
  author = {Doersch, Carl and Gupta, Abhinav and Efros, Alexei A.},
  year = {2016},
  month = jan,
  journal = {arXiv:1505.05192 [cs]},
  eprint = {1505.05192},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image and train a convolutional neural net to predict the position of the second patch relative to the first. We argue that doing well on this task requires the model to learn to recognize objects and their parts. We demonstrate that the feature representation learned using this within-image context indeed captures visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned ConvNet can be used in the RCNN framework [21] and provides a significant boost over a randomly-initialized ConvNet, resulting in state-of-theart performance among algorithms which use only Pascalprovided training set annotations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/Z8S5E5PT/Doersch et al. - 2016 - Unsupervised Visual Representation Learning by Con.pdf}
}

@article{dosovitskiyImageWorth16x162020,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.11929 [cs]},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/2524YU6N/Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf}
}

@article{eilertsenClassifyingClassifierDissecting2020,
  title = {Classifying the Classifier: Dissecting the Weight Space of Neural Networks},
  shorttitle = {Classifying the Classifier},
  author = {Eilertsen, Gabriel and J{\"o}nsson, Daniel and Ropinski, Timo and Unger, Jonas and Ynnerman, Anders},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.05688 [cs]},
  eprint = {2002.05688},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper presents an empirical study on the weights of neural networks, where we interpret each model as a point in a high-dimensional space \textendash{} the neural weight space. To explore the complex structure of this space, we sample from a diverse selection of training variations (dataset, optimization procedure, architecture, etc.) of neural network classifiers, and train a large number of models to represent the weight space. Then, we use a machine learning approach for analyzing and extracting information from this space. Most centrally, we train a number of novel deep meta-classifiers with the objective of classifying different properties of the training setup by identifying their footprints in the weight space. Thus, the metaclassifiers probe for patterns induced by hyper-parameters, so that we can quantify how much, where, and when these are encoded through the optimization process. This provides a novel and complementary view for explainable AI, and we show how meta-classifiers can reveal a great deal of information about the training setup and optimization, by only considering a small subset of randomly selected consecutive weights. To promote further research on the weight space, we release the neural weight space (NWS) dataset \textendash{} a collection of 320K weight snapshots from 16K individually trained deep neural networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/D8NIEHPN/Eilertsen et al. - 2020 - Classifying the classifier dissecting the weight .pdf;/Users/konstantinschurholt/Zotero/storage/HTH5MLV4/Eilertsen et al. - 2020 - Classifying the classifier dissecting the weight .pdf;/Users/konstantinschurholt/Zotero/storage/Z7NJBJ3D/Eilertsen et al. - 2020 - Classifying the classifier dissecting the weight .pdf}
}

@inproceedings{ergenRevealingStructureDeep2021,
  title = {Revealing the {{Structure}} of {{Deep Neural Networks}} via {{Convex Duality}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Ergen, Tolga and Pilanci, Mert},
  year = {2021},
  month = jul,
  pages = {3004--3014},
  publisher = {{PMLR}},
  abstract = {We study regularized deep neural networks (DNNs) and introduce a convex analytic framework to characterize the structure of the hidden layers. We show that a set of optimal hidden layer weights for...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/3W97VHWU/Ergen and Pilanci - 2021 - Revealing the Structure of Deep Neural Networks via Convex Duality.pdf}
}

@inproceedings{ermolovWhiteningSelfSupervisedRepresentation2021,
  title = {Whitening for {{Self-Supervised Representation Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Ermolov, Aleksandr and Siarohin, Aliaksandr and Sangineto, Enver and Sebe, Nicu},
  year = {2021},
  month = jul,
  pages = {3015--3024},
  publisher = {{PMLR}},
  abstract = {Most of the current self-supervised representation learning (SSL) methods are based on the contrastive loss and the instance-discrimination task, where augmented versions of the same image instance...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/VDETNU6M/Ermolov et al. - 2021 - Whitening for Self-Supervised Representation Learning.pdf}
}

@inproceedings{esserTamingTransformersHighResolution2021b,
  title = {Taming {{Transformers}} for {{High-Resolution Image Synthesis}}},
  booktitle = {Conference on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Esser, Patrick and Rombach, Robin and Ommer, Bj{\"o}rn},
  year = {2021},
  publisher = {{arXiv}},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/7R5I3U2R/Esser et al. - 2021 - Taming Transformers for High-Resolution Image Synt.pdf}
}

@article{feiConstructionAnalysisLarge,
  title = {Construction and {{Analysis}} of a {{Large Scale Image Ontology}}},
  author = {Fei, Li Fei- and Deng, Jia and Do, Minh and Su, Hao and Li, Kai},
  pages = {1},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/QFZ4WH57/Fei et al. - Construction and Analysis of a Large Scale Image O.pdf}
}

@article{fengTransferredDiscrepancyQuantifying2020,
  title = {Transferred {{Discrepancy}}: {{Quantifying}} the {{Difference Between Representations}}},
  shorttitle = {Transferred {{Discrepancy}}},
  author = {Feng, Yunzhen and Zhai, Runtian and He, Di and Wang, Liwei and Dong, Bin},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.12446 [cs, stat]},
  eprint = {2007.12446},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Understanding what information neural networks capture is an essential problem in deep learning, and studying whether different models capture similar features is an initial step to achieve this goal. Previous works sought to define metrics over the feature matrices to measure the difference between two models. However, different metrics sometimes lead to contradictory conclusions, and there has been no consensus on which metric is suitable to use in practice. In this work, we propose a novel metric that goes beyond previous approaches. Recall that one of the most practical scenarios of using the learned representations is to apply them to downstream tasks. We argue that we should design the metric based on a similar principle. For that, we introduce the transferred discrepancy (TD), a new metric that defines the difference between two representations based on their downstream-task performance. Through an asymptotic analysis, we show how TD correlates with downstream tasks and the necessity to define metrics in such a task-dependent fashion. In particular, we also show that under specific conditions, the TD metric is closely related to previous metrics. Our experiments show that TD can provide fine-grained information for varied downstream tasks, and for the models trained from different initializations, the learned features are not the same in terms of downstream-task predictions. We find that TD may also be used to evaluate the effectiveness of different training strategies. For example, we demonstrate that the models trained with proper data augmentations that improve the generalization capture more similar features in terms of TD, while those with data augmentations that hurt the generalization will not. This suggests a training strategy that leads to more robust representation also trains models that generalize better.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/FVJ3GTT4/Feng et al. - 2020 - Transferred Discrepancy Quantifying the Differenc.pdf;/Users/konstantinschurholt/Zotero/storage/TELH6QNB/Feng et al. - 2020 - Transferred Discrepancy Quantifying the Differenc.pdf}
}

@inproceedings{finnModelAgnosticMetaLearningFast2017,
  title = {Model-{{Agnostic Meta-Learning}} for {{Fast Adaptation}} of {{Deep Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  year = {2017},
  month = jul,
  pages = {1126--1135},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/EXJGY8MC/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf;/Users/konstantinschurholt/Zotero/storage/P4LWFI6E/Finn et al. - 2017 - Model-Agnostic Meta-Learning for Fast Adaptation o.pdf}
}

@article{finnoffImprovingModelSelection1993,
  title = {Improving Model Selection by Nonconvergent Methods},
  author = {Finnoff, William and Hergert, Ferdinand and Zimmermann, Hans Georg},
  year = {1993},
  month = jan,
  journal = {Neural Networks},
  volume = {6},
  number = {6},
  pages = {771--783},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(05)80122-4},
  abstract = {Many techniques for model selection in the field of neural networks correspond to well established statistical methods. For example, architecture modifications based on test variables calculated after convergence of the training process can be viewed as part of a hypothesis testing search, and the use of complexity penalty terms is essentially a type of regularization or biased regression. The method of ``stopped'' or ``cross-validation'' training, on the other hand, in which an oversized network is trained until the error on a further validation set of examples deteriorates, then training is stopped, is a true innovation since model selection doesn't require convergence of the training process. Here, the training process is used to perform a directed search of the parameter space for a model which doesn't overfit the data and thus demonstrates superior generalization performance. In this paper we show that this performance can be significantly enhanced by expanding the ``nonconvergent method'' of stopped training to include dynamic topology modifications (dynamic weight pruning) and modified complexity penalty term methods in which the weighting of the penalty term is adjusted during the training process. On an extensive sequence of simulation examples we demonstrate the general superiority of the ``extended'' nonconvergent methods compared to classical penalty term methods, simple stopped training, and methods which only vary the number of hidden units.},
  langid = {english},
  keywords = {Cross-validation,Dynamic topology modifications,Generalization,Model selection,Nonconvergent training,Penalty terms,Weight pruning},
  file = {/Users/konstantinschurholt/Zotero/storage/PNEC3T4R/S0893608005801224.html}
}

@article{fischerDL2TrainingQuerying,
  title = {{{DL2}}: {{Training}} and {{Querying Neural Networks}} with {{Logic}}},
  author = {Fischer, Marc and Balunovic, Mislav and {Drachsler-Cohen}, Dana and Gehr, Timon and Zhang, Ce and Vechev, Martin},
  pages = {17},
  abstract = {We present DL2, a system for training and querying neural networks with logical constraints. Using DL2, one can declaratively specify domain knowledge constraints to be enforced during training, as well as pose queries on the model to find inputs that satisfy a set of constraints. DL2 works by translating logical constraints into a loss function with desirable mathematical properties. The loss is then minimized with standard gradientbased methods. We evaluate DL2 by training networks with interesting constraints in unsupervised, semi-supervised and supervised settings. Our experimental evaluation demonstrates that DL2 is more expressive than prior approaches combining logic and neural networks, and its loss functions are better suited for optimization. Further, we show that for a number of queries, DL2 can find the desired inputs in seconds (even for large models such as ResNet-50 on ImageNet).},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/XMBFT6PQ/Fischer et al. - DL2 Training and Querying Neural Networks with Lo.pdf}
}

@inproceedings{fischerWhatBoxExploring2021,
  title = {What's in the {{Box}}? {{Exploring}} the {{Inner Life}} of {{Neural Networks}} with {{Robust Rules}}},
  shorttitle = {What's in the {{Box}}?},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Fischer, Jonas and Olah, Anna and Vreeken, Jilles},
  year = {2021},
  month = jul,
  pages = {3352--3362},
  publisher = {{PMLR}},
  abstract = {We propose a novel method for exploring how neurons within neural networks interact. In particular, we consider activation values of a network for given data, and propose to mine noise-robust rules...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/KGZXXKIK/Fischer et al. - 2021 - What‚Äôs in the Box Exploring the Inner Life of Neural Networks with Robust Rules.pdf}
}

@article{fongInterpretableExplanationsBlack2017,
  title = {Interpretable {{Explanations}} of {{Black Boxes}} by {{Meaningful Perturbation}}},
  author = {Fong, Ruth and Vedaldi, Andrea},
  year = {2017},
  month = oct,
  journal = {2017 IEEE International Conference on Computer Vision (ICCV)},
  eprint = {1704.03296},
  eprinttype = {arxiv},
  pages = {3449--3457},
  doi = {10.1109/ICCV.2017.371},
  abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks ``look'' in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/YXU7V6RA/Fong and Vedaldi - 2017 - Interpretable Explanations of Black Boxes by Meani.pdf}
}

@article{fongNet2VecQuantifyingExplaining2018,
  title = {{{Net2Vec}}: {{Quantifying}} and {{Explaining}} How {{Concepts}} Are {{Encoded}} by {{Filters}} in {{Deep Neural Networks}}},
  shorttitle = {{{Net2Vec}}},
  author = {Fong, Ruth and Vedaldi, Andrea},
  year = {2018},
  month = mar,
  journal = {arXiv:1801.03454 [cs, stat]},
  eprint = {1801.03454},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In an effort to understand the meaning of the intermediate representations captured by deep networks, recent papers have tried to associate specific semantic concepts to individual neural network filter responses, where interesting correlations are often found, largely by focusing on extremal filter responses. In this paper, we show that this approach can favor easy-to-interpret cases that are not necessarily representative of the average behavior of a representation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/FDIGKUA3/Fong and Vedaldi - 2018 - Net2Vec Quantifying and Explaining how Concepts a.pdf}
}

@article{fortLargeScaleStructure2019,
  title = {Large {{Scale Structure}} of {{Neural Network Loss Landscapes}}},
  author = {Fort, Stanislav and Jastrzebski, Stanislaw},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.04724 [cs, stat]},
  eprint = {1906.04724},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {There are many surprising and perhaps counter-intuitive properties of optimization of deep neural networks. We propose and experimentally verify a unified phenomenological model of the loss landscape that incorporates many of them. High dimensionality plays a key role in our model. Our core idea is to model the loss landscape as a set of high dimensional wedges that together form a large-scale, inter-connected structure and towards which optimization is drawn. We first show that hyperparameter choices such as learning rate, network width and L2 regularization, affect the path optimizer takes through the landscape in a similar ways, influencing the large scale curvature of the regions the optimizer explores. Finally, we predict and demonstrate new counter-intuitive properties of the loss-landscape. We show an existence of low loss subspaces connecting a set (not only a pair) of solutions, and verify it experimentally. Finally, we analyze recently popular ensembling techniques for deep networks in the light of our model.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/23RU4KMB/Fort and Jastrzebski - 2019 - Large Scale Structure of Neural Network Loss Lands.pdf}
}

@article{franchiTRADITrackingDeep2021,
  title = {{{TRADI}}: {{Tracking}} Deep Neural Network Weight Distributions for Uncertainty Estimation},
  shorttitle = {{{TRADI}}},
  author = {Franchi, Gianni and Bursuc, Andrei and Aldea, Emanuel and Dubuisson, Severine and Bloch, Isabelle},
  year = {2021},
  month = mar,
  journal = {arXiv:1912.11316 [cs, stat]},
  eprint = {1912.11316},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {During training, the weights of a Deep Neural Network (DNN) are optimized from a random initialization towards a nearly optimum value minimizing a loss function. Only this final state of the weights is typically kept for testing, while the wealth of information on the geometry of the weight space, accumulated over the descent towards the minimum is discarded. In this work we propose to make use of this knowledge and leverage it for computing the distributions of the weights of the DNN. This can be further used for estimating the epistemic uncertainty of the DNN by aggregating predictions from an ensemble of networks sampled from these distributions. To this end we introduce a method for tracking the trajectory of the weights during optimization, that does neither require any change in the architecture, nor in the training procedure. We evaluate our method, TRADI, on standard classification and regression benchmarks, and on out-of-distribution detection for classification and semantic segmentation. We achieve competitive results, while preserving computational efficiency in comparison to ensemble approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/QB6SGG7Z/Franchi et al. - 2021 - TRADI Tracking deep neural network weight distrib.pdf}
}

@misc{frankleLotteryTicketHypothesis2019,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2019},
  month = mar,
  number = {arXiv:1803.03635},
  eprint = {1803.03635},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the lottery ticket hypothesis, proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. Larger networks have more of these ``lottery tickets,'' meaning they are more likely to luck out with a subcomponent initialized in a configuration amenable to successful optimization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/konstantinschurholt/Zotero/storage/28SPL3VV/Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf}
}

@book{fredStructuralSyntacticStatistical2004,
  title = {Structural, Syntactic, and Statistical Pattern Recognition: Joint {{IAPR}} International Workshops, {{SSPR}} 2004 and {{SPR}} 2004, {{Lisbon}}, {{Portugal}}, {{August}} 18-20, 2004: Proceedings},
  shorttitle = {Structural, Syntactic, and Statistical Pattern Recognition},
  editor = {Fred, Ana and {International Association for Pattern Recognition}},
  year = {2004},
  series = {Lecture Notes in Computer Science},
  number = {3138},
  publisher = {{Springer}},
  address = {{Berlin ; New York}},
  isbn = {978-3-540-22570-6},
  langid = {english},
  lccn = {TK7882.P3 I57 2004},
  keywords = {Congresses,Pattern recognition systems},
  annotation = {OCLC: ocm56334037},
  file = {/Users/konstantinschurholt/Zotero/storage/I4VBLVUK/Fred and International Association for Pattern Recognition - 2004 - Structural, syntactic, and statistical pattern rec.pdf}
}

@article{gabellaTopologyLearningArtificial2019,
  title = {Topology of {{Learning}} in {{Artificial Neural Networks}}},
  author = {Gabella, Maxime},
  year = {2019},
  month = feb,
  journal = {arXiv:1902.08160 [cs, stat]},
  eprint = {1902.08160},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Understanding how neural networks learn remains one of the central challenges in machine learning research. From random at the start of training, the weights of a neural network evolve in such a way as to be able to perform a variety of tasks, like classifying images. Here we study the emergence of structure in the weights by applying methods from topological data analysis. We train simple feedforward neural networks on the MNIST dataset and monitor the evolution of the weights. When initialized to zero, the weights follow trajectories that branch off recurrently, thus generating trees that describe the growth of the effective capacity of each layer. When initialized to tiny random values, the weights evolve smoothly along two-dimensional surfaces. We show that natural coordinates on these learning surfaces correspond to important factors of variation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/S2JIIUEF/Gabella - 2019 - Topology of Learning in Artificial Neural Networks.pdf}
}

@inproceedings{gaborSelfReplicationNeuralNetworks2019,
  title = {Self-{{Replication}} in {{Neural Networks}}},
  booktitle = {Artificial {{Life Conference Proceedings}}},
  author = {Gabor, Thomas and Illium, Steffen and Mattausch, Andy and Belzner, Lenz and {Linnhoff-Popien}, Claudia},
  year = {2019},
  pages = {8},
  abstract = {The foundation of biological structures is self-replication. Neural networks are the prime structure used for the emergent construction of complex behavior in computers. We analyze how various network types lend themselves to selfreplication. We argue that backpropagation is the natural way to navigate the space of network weights and show how it allows non-trivial self-replicators to arise naturally. We then extend the setting to construct an artificial chemistry environment of several neural networks.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/GHS4YF9W/Gabor et al. - Self-Replication in Neural Networks.pdf}
}

@inproceedings{galDropoutBayesianApproximation2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = jun,
  pages = {1050--1059},
  publisher = {{PMLR}},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs \textendash{} extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/6HJM6AVN/Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning.pdf}
}

@article{gargLeveragingUnlabeledData2022,
  title = {Leveraging {{Unlabeled Data}} to {{Predict Out-of-Distribution Performance}}},
  author = {Garg, Saurabh and Balakrishnan, Sivaraman and Lipton, Zachary C. and Neyshabur, Behnam and Sedghi, Hanie},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.04234 [cs, stat]},
  eprint = {2201.04234},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Real-world machine learning deployments are characterized by mismatches between the source (training) and target (test) distributions that may cause performance drops. In this work, we investigate methods for predicting the target domain accuracy using only labeled source data and unlabeled target data. We propose Average Thresholded Confidence (ATC), a practical method that learns a threshold on the model's confidence, predicting accuracy as the fraction of unlabeled examples for which model confidence exceeds that threshold. ATC outperforms previous methods across several model architectures, types of distribution shifts (e.g., due to synthetic corruptions, dataset reproduction, or novel subpopulations), and datasets (WILDS, ImageNet, BREEDS, CIFAR, and MNIST). In our experiments, ATC estimates target performance 2\textendash 4\textasciicircum{} more accurately than prior methods. We also explore the theoretical foundations of the problem, proving that, in general, identifying the accuracy is just as hard as identifying the optimal predictor and thus, the efficacy of any method rests upon (perhaps unstated) assumptions on the nature of the shift. Finally, analyzing our method on some toy distributions, we provide insights concerning when it works.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/6I4VP6RE/Garg et al. - 2022 - Leveraging Unlabeled Data to Predict Out-of-Distri.pdf}
}

@inproceedings{gavrikovCNNFilterDB2022,
  title = {{{CNN Filter DB}}: {{An Empirical Investigation}} of {{Trained Convolutional Filters}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Gavrikov, Paul and Keuper, Janis},
  year = {2022},
  pages = {11},
  abstract = {Currently, many theoretical as well as practically relevant questions towards the transferability and robustness of Convolutional Neural Networks (CNNs) remain unsolved. While ongoing research efforts are engaging these problems from various angles, in most computer vision related cases these approaches can be generalized to investigations of the effects of distribution shifts in image data.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/UZEGNI6Y/Gavrikov and Keuper - CNN Filter DB An Empirical Investigation of Train.pdf}
}

@inproceedings{gehrAI2SafetyRobustness2018,
  title = {{{AI2}}: {{Safety}} and {{Robustness Certification}} of {{Neural Networks}} with {{Abstract Interpretation}}},
  shorttitle = {{{AI2}}},
  booktitle = {2018 {{IEEE Symposium}} on {{Security}} and {{Privacy}} ({{SP}})},
  author = {Gehr, Timon and Mirman, Matthew and {Drachsler-Cohen}, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
  year = {2018},
  month = may,
  pages = {3--18},
  publisher = {{IEEE}},
  address = {{San Francisco, CA}},
  doi = {10.1109/SP.2018.00058},
  abstract = {We present AI2, the first sound and scalable analyzer for deep neural networks. Based on overapproximation, AI2 can automatically prove safety properties (e.g., robustness) of realistic neural networks (e.g., convolutional neural networks). The key insight behind AI2 is to phrase reasoning about safety and robustness of neural networks in terms of classic abstract interpretation, enabling us to leverage decades of advances in that area. Concretely, we introduce abstract transformers that capture the behavior of fully connected and convolutional neural network layers with rectified linear unit activations (ReLU), as well as max pooling layers. This allows us to handle real-world neural networks, which are often built out of those types of layers. We present a complete implementation of AI2 together with an extensive evaluation on 20 neural networks. Our results demonstrate that: (i) AI2 is precise enough to prove useful specifications (e.g., robustness), (ii) AI2 can be used to certify the effectiveness of state-of-the-art defenses for neural networks, (iii) AI2 is significantly faster than existing analyzers based on symbolic analysis, which often take hours to verify simple fully connected networks, and (iv) AI2 can handle deep convolutional networks, which are beyond the reach of existing methods.},
  isbn = {978-1-5386-4353-2},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/HEQWFF5A/Gehr et al. - 2018 - AI2 Safety and Robustness Certification of Neural.pdf}
}

@inproceedings{ghoshVariationalDeterministicAutoencoders2020,
  title = {From {{Variational}} to {{Deterministic Autoencoders}}},
  booktitle = {{{arXiv}}:1903.12436 [Cs, Stat]},
  author = {Ghosh, Partha and Sajjadi, Mehdi S. M. and Vergari, Antonio and Black, Michael and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = may,
  eprint = {1903.12436},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of VAEs. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without forcing it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data, we introduce an ex-post density estimation step that can be readily applied also to existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules. \textbackslash footnote\{An implementation is available at: \textbackslash url\{https://github.com/ParthaEth/Regularized\_autoencoders-RAE-\}\}},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/TYIMYEF6/Ghosh et al. - 2020 - From Variational to Deterministic Autoencoders.pdf}
}

@article{gidarisUnsupervisedRepresentationLearning2018,
  title = {Unsupervised {{Representation Learning}} by {{Predicting Image Rotations}}},
  author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  year = {2018},
  month = mar,
  journal = {arXiv:1803.07728 [cs]},
  eprint = {1803.07728},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4\% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/9DS3RM6G/Gidaris et al. - 2018 - Unsupervised Representation Learning by Predicting.pdf}
}

@misc{giordengiorgioHowDoesGuerrilla2019,
  title = {How Does {{Guerrilla Gravity Make Affordable Carbon Bikes}} in the {{USA}}?},
  author = {{Giorden Giorgio}},
  year = {2019},
  month = jan,
  abstract = {Guerrilla Gravity's new all-carbon, all-American line up costs significantly less than a lot of the competition. We sat down with Guerrilla Gravity co-founder Will Montague to find out how. Pinkbike Merch  -  https://pinkbike.click/merch\hspace{0pt} Additional photography by - Justin VanAlstyne -- Subscribe for more content from the pulse of mountain biking. Website - https://pinkbike.com\hspace{0pt} Facebook - https://facebook.com/pinkbikecom\hspace{0pt} Instagram - https://instagram.com/pinkbike\hspace{0pt} Youtube - https://youtube.com/pinkbike\hspace{0pt}}
}

@article{giryesDeepNeuralNetworks2016,
  title = {Deep {{Neural Networks}} with {{Random Gaussian Weights}}: {{A Universal Classification Strategy}}?},
  shorttitle = {Deep {{Neural Networks}} with {{Random Gaussian Weights}}},
  author = {Giryes, Raja and Sapiro, Guillermo and Bronstein, Alex M.},
  year = {2016},
  month = jul,
  journal = {IEEE Transactions on Signal Processing},
  volume = {64},
  number = {13},
  eprint = {1504.08291},
  eprinttype = {arxiv},
  pages = {3444--3457},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2016.2546221},
  abstract = {Three important properties of a classification machinery are: (i) the system preserves the core information of the input data; (ii) the training examples convey information about unseen data; and (iii) the system is able to treat differently points from different classes. In this work we show that these fundamental properties are satisfied by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure, as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {62M45,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.5.1,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/ABKADYJU/Giryes et al. - 2016 - Deep Neural Networks with Random Gaussian Weights.pdf}
}

@inproceedings{glorotUnderstandingDifÔ¨ÅcultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}, {{PMLR}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  pages = {8},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/K3R4A2U6/Glorot and Bengio - Understanding the difÔ¨Åculty of training deep feedf.pdf}
}

@book{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}}
}

@inproceedings{goodfellowGenerativeAdversarialNets2014,
  title = {Generative {{Adversarial Nets}}},
  booktitle = {Conference on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Goodfellow, Ian and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  pages = {9},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/9SFXYEF5/Goodfellow et al. - Generative Adversarial Nets.pdf}
}

@article{goodfellowQualitativelyCharacterizingNeural2015,
  title = {Qualitatively Characterizing Neural Network Optimization Problems},
  author = {Goodfellow, Ian J. and Vinyals, Oriol and Saxe, Andrew M.},
  year = {2015},
  month = may,
  journal = {arXiv:1412.6544 [cs, stat]},
  eprint = {1412.6544},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/RZFTQ2SE/Goodfellow et al. - 2015 - Qualitatively characterizing neural network optimi.pdf}
}

@article{goyalSelfsupervisedPretrainingVisual2021,
  title = {Self-Supervised {{Pretraining}} of {{Visual Features}} in the {{Wild}}},
  author = {Goyal, Priya and Caron, Mathilde and Lefaudeux, Benjamin and Xu, Min and Wang, Pengchao and Pai, Vivek and Singh, Mannat and Liptchinsky, Vitaliy and Misra, Ishan and Joulin, Armand and Bojanowski, Piotr},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.01988 [cs]},
  eprint = {2103.01988},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recently, self-supervised learning methods like MoCo [22], SimCLR [8], BYOL [20] and SwAV [7] have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our final SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2\% top-1 accuracy, surpassing the best self-supervised pretrained model by 1\% and confirming that self-supervised learning works in a real world setting. Interestingly, we also observe that selfsupervised models are good few-shot learners achieving 77.9\% top-1 with access to only 10\% of ImageNet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/BAQNE235/Goyal et al. - 2021 - Self-supervised Pretraining of Visual Features in .pdf}
}

@inproceedings{grafDissectingSupervisedConstrastive2021a,
  title = {Dissecting {{Supervised Constrastive Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Graf, Florian and Hofer, Christoph and Niethammer, Marc and Kwitt, Roland},
  year = {2021},
  month = jul,
  pages = {3821--3830},
  publisher = {{PMLR}},
  abstract = {Minimizing cross-entropy over the softmax scores of a linear map composed with a high-capacity encoder is arguably the most popular choice for training neural networks on supervised learning tasks....},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/63W3DUKN/Graf et al. - 2021 - Dissecting Supervised Constrastive Learning.pdf}
}

@article{grillBootstrapYourOwn2020,
  title = {Bootstrap Your Own Latent: {{A}} New Approach to Self-Supervised {{Learning}}},
  shorttitle = {Bootstrap Your Own Latent},
  author = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, R{\'e}mi and Valko, Michal},
  year = {2020},
  month = sep,
  journal = {arXiv:2006.07733 [cs, stat]},
  eprint = {2006.07733},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3\textbackslash\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6\textbackslash\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/DSL9PKZT/Grill et al. - 2020 - Bootstrap your own latent A new approach to self-.pdf}
}

@misc{guEfficientlyModelingLong2022,
  title = {Efficiently {{Modeling Long Sequences}} with {{Structured State Spaces}}},
  author = {Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  year = {2022},
  month = aug,
  number = {arXiv:2111.00396},
  eprint = {2111.00396},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of \$10000\$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \textbackslash ( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \textbackslash ), and showed that for appropriate choices of the state matrix \textbackslash ( A \textbackslash ), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \textbackslash ( A \textbackslash ) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\textbackslash\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation \$60\textbackslash times\$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/BATPHHJB/Gu et al. - 2022 - Efficiently Modeling Long Sequences with Structure.pdf}
}

@inproceedings{haHyperNetworks2016,
  title = {{{HyperNetworks}}},
  booktitle = {{{arXiv}}:1609.09106 [Cs]},
  author = {Ha, David and Dai, Andrew and Le, Quoc V.},
  year = {2016},
  eprint = {1609.09106},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype \textendash{} the hypernetwork \textendash{} and a phenotype \textendash{} the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/3JUUUYCA/Ha et al. - 2016 - HyperNetworks.pdf;/Users/konstantinschurholt/Zotero/storage/K55A57DP/Ha et al. - 2016 - HyperNetworks.pdf;/Users/konstantinschurholt/Zotero/storage/R2BJKKSV/Ha et al. - 2016 - HyperNetworks.pdf}
}

@article{haninHowStartTraining2018,
  title = {How to {{Start Training}}: {{The Effect}} of {{Initialization}} and {{Architecture}}},
  shorttitle = {How to {{Start Training}}},
  author = {Hanin, Boris and Rolnick, David},
  year = {2018},
  month = nov,
  journal = {arXiv:1803.01719 [cs, stat]},
  eprint = {1803.01719},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We identify and study two common failure modes for early training in deep ReLU nets. For each, we give a rigorous proof of when it occurs and how to avoid it, for fully connected, convolutional, and residual architectures. We show that the first failure mode, exploding or vanishing mean activation length, can be avoided by initializing weights from a symmetric distribution with variance 2/fan-in and, for ResNets, by correctly scaling the residual modules. We prove that the second failure mode, exponentially large variance of activation length, never occurs in residual nets once the first failure mode is avoided. In contrast, for fully connected nets, we prove that this failure mode can happen and is avoided by keeping constant the sum of the reciprocals of layer widths. We demonstrate empirically the effectiveness of our theoretical results in predicting when networks are able to start training. In particular, we note that many popular initializations fail our criteria, whereas correct initialization and architecture allows much deeper networks to be trained.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/WKDLSQAW/Hanin and Rolnick - 2018 - How to Start Training The Effect of Initializatio.pdf}
}

@article{hansenNeuralNetworkEnsembles1990,
  title = {Neural Network Ensembles},
  author = {Hansen, L.K. and Salamon, P.},
  year = {1990},
  month = oct,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {12},
  number = {10},
  pages = {993--1001},
  issn = {1939-3539},
  doi = {10.1109/34.58871},
  abstract = {Several means for improving the performance and training of neural networks for classification are proposed. Crossvalidation is used as a tool for optimizing network parameters and architecture. It is shown that the remaining residual generalization error can be reduced by invoking ensembles of similar networks.{$<>$}},
  keywords = {Computer architecture,Data mining,Databases,Fault tolerance,Feedforward systems,Neural networks,Neurons,Pattern recognition,Performance analysis,Supervised learning},
  file = {/Users/konstantinschurholt/Zotero/storage/TAFR7TPB/Hansen and Salamon - 1990 - Neural network ensembles.pdf}
}

@article{hardoonCanonicalCorrelationAnalysis2004,
  title = {Canonical {{Correlation Analysis}}: {{An Overview}} with {{Application}} to {{Learning Methods}}},
  shorttitle = {Canonical {{Correlation Analysis}}},
  author = {Hardoon, David R. and Szedmak, Sandor and {Shawe-Taylor}, John},
  year = {2004},
  month = dec,
  journal = {Neural Computation},
  volume = {16},
  number = {12},
  pages = {2639--2664},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/0899766042321814},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/HPSML4YS/Hardoon et al. - 2004 - Canonical Correlation Analysis An Overview with A.pdf}
}

@article{havivTransformerLanguageModels2022,
  title = {Transformer {{Language Models}} without {{Positional Encodings Still Learn Positional Information}}},
  author = {Haviv, Adi and Ram, Ori and Press, Ofir and Izsak, Peter and Levy, Omer},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.16634 [cs]},
  eprint = {2203.16634},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Transformers typically require some form of positional encoding, such as positional embeddings, to process natural language sequences. Surprisingly, we find that transformer language models without any explicit positional encoding are still competitive with standard models, and that this phenomenon is robust across different datasets, model sizes, and sequence lengths. Probing experiments reveal that such models acquire an implicit notion of absolute positions throughout the network, effectively compensating for the missing information. We conjecture that causal attention enables the model to infer the number of predecessors that each token can attend to, thereby approximating its absolute position.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/J37NC294/Haviv et al. - 2022 - Transformer Language Models without Positional Enc.pdf}
}

@article{heDeBERTaDecodingenhancedBERT2021,
  title = {{{DeBERTa}}: {{Decoding-enhanced BERT}} with {{Disentangled Attention}}},
  shorttitle = {{{DeBERTa}}},
  author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  year = {2021},
  month = mar,
  journal = {arXiv:2006.03654 [cs]},
  eprint = {2006.03654},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understand (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9\% (90.2\% vs. 91.1\%), on SQuAD v2.0 by +2.3\% (88.4\% vs. 90.7\%) and RACE by +3.6\% (83.2\% vs. 86.8\%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, outperforming the human baseline by a decent margin (90.3 versus 89.8). The pre-trained DeBERTa models and the source code were released at: https://github.com/microsoft/DeBERTa1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,cs.CL; cs.GL,I.2,I.7},
  file = {/Users/konstantinschurholt/Zotero/storage/2CEN9RN5/He et al. - 2021 - DeBERTa Decoding-enhanced BERT with Disentangled .pdf;/Users/konstantinschurholt/Zotero/storage/XKV9XN3P/He et al. - 2021 - DeBERTa Decoding-enhanced BERT with Disentangled .pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {{{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers\textemdash 8\texttimes{} deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/LFQIDM4Y/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf}
}

@inproceedings{heDelvingDeepRectifiers2015,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human-Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  booktitle = {{{arXiv}}:1502.01852 [Cs]},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  eprint = {1502.01852},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [29]). To our knowledge, our result is the first to surpass human-level performance (5.1\%, [22]) on this visual recognition challenge.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/JWWBP2L6/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf;/Users/konstantinschurholt/Zotero/storage/MYBNEQ2T/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf}
}

@article{heMomentumContrastUnsupervised2020,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  year = {2020},
  month = mar,
  journal = {arXiv:1911.05722 [cs]},
  eprint = {1911.05722},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning [29] as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/ES4XDH3B/He et al. - 2020 - Momentum Contrast for Unsupervised Visual Represen.pdf}
}

@inproceedings{hoDenoisingDiffusionProbabilistic2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  booktitle = {Conference on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/7AXKFBLC/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@misc{hoeflerSparsityDeepLearning2021,
  title = {Sparsity in {{Deep Learning}}: {{Pruning}} and Growth for Efficient Inference and Training in Neural Networks},
  shorttitle = {Sparsity in {{Deep Learning}}},
  author = {Hoefler, Torsten and Alistarh, Dan and {Ben-Nun}, Tal and Dryden, Nikoli and Peste, Alexandra},
  year = {2021},
  month = jan,
  number = {arXiv:2102.00554},
  eprint = {2102.00554},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2102.00554},
  abstract = {The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, if not better than, the original dense networks. Sparsity can reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Hardware Architecture,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/konstantinschurholt/Zotero/storage/8BYZQL6Y/Hoefler et al. - 2021 - Sparsity in Deep Learning Pruning and growth for .pdf;/Users/konstantinschurholt/Zotero/storage/2IEXKITS/2102.html}
}

@inproceedings{hoiemLearningCurvesAnalysis2021,
  title = {Learning {{Curves}} for {{Analysis}} of {{Deep Networks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Hoiem, Derek and Gupta, Tanmay and Li, Zhizhong and {Shlapentokh-Rothman}, Michal},
  year = {2021},
  month = jul,
  pages = {4287--4296},
  publisher = {{PMLR}},
  abstract = {Learning curves model a classifier's test error as a function of the number of training samples. Prior works show that learning curves can be used to select model parameters and extrapolate perform...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/S4694AMI/Hoiem et al. - 2021 - Learning Curves for Analysis of Deep Networks.pdf}
}

@article{hosnyModelHubAIDissemination,
  title = {{{ModelHub}}.{{AI}}: {{Dissemination Platform}} for {{Deep Learning Models}}},
  author = {Hosny, Ahmed and Schwier, Michael and Berger, Christoph and {\"O}rnek, Evin P and Turan, Mehmet and Tran, Phi V and Isensee, Fabian and {Maier-Hein}, Klaus H and McKinley, Richard and Lu, Michael T and Hoffmann, Udo and Menze, Bjoern and Bakas, Spyridon and Fedorov, Andriy and Aerts, Hugo JWL},
  pages = {12},
  abstract = {Recent advances in artificial intelligence research have led to a profusion of studies that apply deep learning to problems in image analysis and natural language processing among others. Additionally, the availability of open-source computational frameworks has lowered the barriers to implementing state-of-the-art methods across multiple domains. Albeit leading to major performance breakthroughs in some tasks, effective dissemination of deep learning algorithms remains challenging, inhibiting reproducibility and benchmarking studies, impeding further validation, and ultimately hindering their effectiveness in the cumulative scientific progress. In developing a platform for sharing research outputs, we present ModelHub.AI (www.modelhub.ai), a community-driven container-based software engine and platform for the structured dissemination of deep learning models. For contributors, the engine controls data flow throughout the inference cycle, while the contributor-facing standard template exposes model-specific functions including inference, as well as pre- and post-processing. Python and RESTful Application programming interfaces (APIs) enable users to interact with models hosted on ModelHub.AI and allows both researchers and developers to utilize models out-of-the-box. ModelHub.AI is domain-, data-, and framework-agnostic, catering to different workflows and contributors' preferences.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/EV5ZNQCU/Hosny et al. - ModelHub.AI Dissemination Platform for Deep Learn.pdf}
}

@article{hospedalesMetaLearningNeuralNetworks2020,
  title = {Meta-{{Learning}} in {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}} in {{Neural Networks}}},
  author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  year = {2020},
  month = nov,
  journal = {arXiv:2004.05439 [cs, stat]},
  eprint = {2004.05439},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/QBWKBUWM/Hospedales et al. - 2020 - Meta-Learning in Neural Networks A Survey.pdf}
}

@article{huismanSurveyDeepMetaLearning2021,
  title = {A {{Survey}} of {{Deep Meta-Learning}}},
  author = {Huisman, Mike and {van Rijn}, Jan N. and Plaat, Aske},
  year = {2021},
  month = apr,
  journal = {Artificial Intelligence Review},
  eprint = {2010.03522},
  eprinttype = {arxiv},
  issn = {0269-2821, 1573-7462},
  doi = {10.1007/s10462-021-10004-4},
  abstract = {Deep neural networks can achieve great successes when presented with large data sets and sufficient computational resources. However, their ability to learn new concepts quickly is limited. Meta-learning is one approach to address this issue, by enabling the network to learn how to learn. The field of Deep Meta-Learning advances at great speed, but lacks a unified, indepth overview of current techniques. With this work, we aim to bridge this gap. After providing the reader with a theoretical foundation, we investigate and summarize key methods, which are categorized into i) metric-, ii) model-, and iii) optimization-based techniques. In addition, we identify the main open challenges, such as performance evaluations on heterogeneous benchmarks, and reduction of the computational costs of meta-learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/HKG8FY9U/Huisman et al. - 2021 - A Survey of Deep Meta-Learning.pdf}
}

@article{hullDatabaseHandwrittenText1994,
  title = {A Database for Handwritten Text Recognition Research},
  author = {Hull, J.J.},
  year = {1994},
  month = may,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {16},
  number = {5},
  pages = {550--554},
  issn = {1939-3539},
  doi = {10.1109/34.291440},
  abstract = {An image database for handwritten text recognition research is described. Digital images of approximately 5000 city names, 5000 state names, 10000 ZIP Codes, and 50000 alphanumeric characters are included. Each image was scanned from mail in a working post office at 300 pixels/in in 8-bit gray scale on a high-quality flat bed digitizer. The data were unconstrained for the writer, style, and method of preparation. These characteristics help overcome the limitations of earlier databases that contained only isolated characters or were prepared in a laboratory setting under prescribed circumstances. Also, the database is divided into explicit training and testing sets to facilitate the sharing of results among researchers as well as performance comparisons.{$<>$}},
  keywords = {Cities and towns,Digital images,Gray-scale,Handwriting recognition,Image databases,Performance analysis,Postal services,Testing,Text recognition,Writing},
  file = {/Users/konstantinschurholt/Zotero/storage/M3R4PVTL/Hull - 1994 - A database for handwritten text recognition resear.pdf;/Users/konstantinschurholt/Zotero/storage/NWW76D9Z/291440.html}
}

@article{jaderbergPopulationBasedTraining2017,
  title = {Population {{Based Training}} of {{Neural Networks}}},
  author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.09846 [cs]},
  eprint = {1711.09846},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present Population Based Training (PBT), a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/konstantinschurholt/Zotero/storage/HC4UF9QQ/Jaderberg et al. - 2017 - Population Based Training of Neural Networks.pdf}
}

@article{jaeglePerceiverGeneralPerception2021,
  title = {Perceiver: {{General Perception}} with {{Iterative Attention}}},
  shorttitle = {Perceiver},
  author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.03206 [cs, eess]},
  eprint = {2103.03206},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver \textendash{} a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture performs competitively or beyond strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 on ImageNet without convolutions and by directly attending to 50,000 pixels. It also surpasses state-of-the-art results for all modalities in AudioSet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/konstantinschurholt/Zotero/storage/2V96B4KT/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf;/Users/konstantinschurholt/Zotero/storage/3ZR5Z9FD/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf}
}

@article{jaeglePerceiverIOGeneral2021,
  title = {Perceiver {{IO}}: {{A General Architecture}} for {{Structured Inputs}} \& {{Outputs}}},
  shorttitle = {Perceiver {{IO}}},
  author = {Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and H{\'e}naff, Olivier and Botvinick, Matthew M. and Zisserman, Andrew and Vinyals, Oriol and Carreira, Jo{\~a}o},
  year = {2021},
  month = aug,
  journal = {arXiv:2107.14795 [cs, eess]},
  eprint = {2107.14795},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {The recently-proposed Perceiver model obtains good results on several domains (images, audio, multimodal, point clouds) while scaling linearly in compute and memory with the input size. While the Perceiver supports many kinds of inputs, it can only produce very simple outputs such as class scores. Perceiver IO overcomes this limitation without sacrificing the original's appealing properties by learning to flexibly query the model's latent space to produce outputs of arbitrary size and semantics. Perceiver IO still decouples model depth from data size and still scales linearly with data size, but now with respect to both input and output sizes. The full Perceiver IO model achieves strong results on tasks with highly structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-task and multi-modal domains. As highlights, Perceiver IO matches a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/konstantinschurholt/Zotero/storage/NTYCAXRJ/Jaegle et al. - 2021 - Perceiver IO A General Architecture for Structure.pdf}
}

@article{jiaGeometricStructureActivation2019,
  title = {On {{Geometric Structure}} of {{Activation Spaces}} in {{Neural Networks}}},
  author = {Jia, Yuting and Wang, Haiwen and Shao, Shuo and Long, Huan and Zhou, Yunsong and Wang, Xinbing},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.01399 [cs, stat]},
  eprint = {1904.01399},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this paper, we investigate the geometric structure of activation spaces of fully connected layers in neural networks and then show applications of this study. We propose an efficient approximation algorithm to characterize the convex hull of massive points in high dimensional space. Based on this new algorithm, four common geometric properties shared by the activation spaces are concluded, which gives a rather clear description of the activation spaces. We then propose an alternative classification method grounding on the geometric structure description, which works better than neural networks alone. Surprisingly, this data classification method can be an indicator of overfitting in neural networks. We believe our work reveals several critical intrinsic properties of modern neural networks and further gives a new metric for evaluating them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/AMP8H553/Jia et al. - 2019 - On Geometric Structure of Activation Spaces in Neu.pdf}
}

@article{jiangPredictingGeneralizationGap2019,
  title = {Predicting the {{Generalization Gap}} in {{Deep Networks}} with {{Margin Distributions}}},
  author = {Jiang, Yiding and Krishnan, Dilip and Mobahi, Hossein and Bengio, Samy},
  year = {2019},
  month = jun,
  journal = {arXiv:1810.00113 [cs, stat]},
  eprint = {1810.00113},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Recent research has demonstrated that deep neural networks can perfectly fit randomly labeled data, but with very poor accuracy on held out data. This phenomenon indicates that loss functions such as cross-entropy are not a reliable indicator of generalization. This leads to the crucial question of how generalization gap can be predicted from training data and network parameters. In this paper, we propose such a measure, and conduct extensive empirical studies on how well it can predict the generalization gap. Our measure is based on the concept of margin distribution, which are the distances of training points to the decision boundary. We find that it is necessary to use margin distributions at multiple layers of a deep network. On the CIFAR-10 and the CIFAR-100 datasets, our proposed measure correlates very strongly with the generalization gap. In addition, we find the following other factors to be of importance: normalizing margin values for scale independence, using characterizations of margin distribution rather than just the margin (closest distance to decision boundary), and working in log space instead of linear space (effectively using a product of margins rather than a sum). Our measure can be easily applied to feedforward deep networks with any architecture and may point towards new training loss functions that could enable better generalization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/CA7EMLGE/Jiang et al. - 2019 - Predicting the Generalization Gap in Deep Networks.pdf}
}

@inproceedings{jiangSelfDamagingContrastiveLearning2021,
  title = {Self-{{Damaging Contrastive Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Jiang, Ziyu and Chen, Tianlong and Mortazavi, Bobak J. and Wang, Zhangyang},
  year = {2021},
  month = jul,
  pages = {4927--4939},
  publisher = {{PMLR}},
  abstract = {The recent breakthrough achieved by contrastive learning accelerates the pace for deploying unsupervised training on real-world data applications. However, unlabeled data in reality is commonly imb...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/NQD2J3PZ/Jiang et al. - 2021 - Self-Damaging Contrastive Learning.pdf}
}

@inproceedings{jingUnderstandingDimensionalCollapse2021,
  title = {Understanding {{Dimensional Collapse}} in {{Contrastive Self-supervised Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Jing, Li and Vincent, Pascal and LeCun, Yann and Tian, Yuandong},
  year = {2021},
  month = sep,
  abstract = {Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between...},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Complexity,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/ZHQB92FI/Jing et al. - 2021 - Understanding Dimensional Collapse in Contrastive Self-supervised Learning.pdf}
}

@article{johnsonSubspaceMatchProbably2019,
  title = {Subspace {{Match Probably Does Not Accurately Assess}} the {{Similarity}} of {{Learned Representations}}},
  author = {Johnson, Jeremiah},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.00884 [cs, math, stat]},
  eprint = {1901.00884},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Learning informative representations of data is one of the primary goals of deep learning, but there is still little understanding as to what representations a neural network actually learns. To better understand this, subspace match was recently proposed as a method for assessing the similarity of the representations learned by neural networks. It has been shown that two networks with the same architecture trained from different initializations learn representations that at hidden layers show low similarity when assessed with subspace match, even when the output layers show high similarity and the networks largely exhibit similar performance on classification tasks. In this note, we present a simple example motivated by standard results in commutative algebra to illustrate how this can happen, and show that although the subspace match at a hidden layer may be 0, the representations learned may be isomorphic as vector spaces. This leads us to conclude that a subspace match comparison of learned representations may well be uninformative, and it points to the need for better methods of understanding learned representations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Commutative Algebra,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/PPG4SSVH/Johnson - 2019 - Subspace Match Probably Does Not Accurately Assess.pdf}
}

@article{kalantidisHardNegativeMixing2020,
  title = {Hard {{Negative Mixing}} for {{Contrastive Learning}}},
  author = {Kalantidis, Yannis and Sariyildiz, Mert Bulent and Pion, Noe and Weinzaepfel, Philippe and Larlus, Diane},
  year = {2020},
  month = dec,
  journal = {arXiv:2010.01028 [cs]},
  eprint = {2010.01028},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Contrastive learning has become a key component of self-supervised learning approaches for computer vision. By learning to embed two augmented versions of the same image close to each other and to push the embeddings of different images apart, one can train highly transferable visual representations. As revealed by recent studies, heavy data augmentation and large sets of negatives are both crucial in learning such representations. At the same time, data mixing strategies either at the image or the feature level improve both supervised and semi-supervised learning by synthesizing novel examples, forcing networks to learn more robust features. In this paper, we argue that an important aspect of contrastive learning, i.e., the effect of hard negatives, has so far been neglected. To get more meaningful negative samples, current top contrastive self-supervised learning approaches either substantially increase the batch sizes, or keep very large memory banks; increasing the memory size, however, leads to diminishing returns in terms of performance. We therefore start by delving deeper into a top-performing framework and show evidence that harder negatives are needed to facilitate better and faster learning. Based on these observations, and motivated by the success of data mixing, we propose hard negative mixing strategies at the feature level, that can be computed on-the-fly with a minimal computational overhead. We exhaustively ablate our approach on linear classification, object detection and instance segmentation and show that employing our hard negative mixing procedure improves the quality of visual representations learned by a state-of-the-art self-supervised learning method.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/FJCTSSWI/Kalantidis et al. - 2020 - Hard Negative Mixing for Contrastive Learning.pdf}
}

@article{karpathyVisualizingUnderstandingRecurrent2015,
  title = {Visualizing and {{Understanding Recurrent Networks}}},
  author = {Karpathy, Andrej and Johnson, Justin and {Fei-Fei}, Li},
  year = {2015},
  month = jun,
  journal = {arXiv:1506.02078 [cs]},
  eprint = {1506.02078},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long ShortTerm Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/konstantinschurholt/Zotero/storage/I3Z45Y7K/Karpathy et al. - 2015 - Visualizing and Understanding Recurrent Networks.pdf}
}

@article{kazemiRepresentationLearningDynamic2020,
  title = {Representation {{Learning}} for {{Dynamic Graphs}}: {{A Survey}}},
  shorttitle = {Representation {{Learning}} for {{Dynamic Graphs}}},
  author = {Kazemi, Seyed Mehran and Goel, Rishab and Jain, Kshitij and Kobyzev, Ivan and Sethi, Akshay and Forsyth, Peter and Poupart, Pascal},
  year = {2020},
  month = apr,
  journal = {arXiv:1905.11485 [cs, stat]},
  eprint = {1905.11485},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. Traditionally, machine learning models for graphs have been mostly designed for static graphs. However, many applications involve evolving graphs. This introduces important challenges for learning and inference since nodes, attributes, and edges change over time. In this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. We describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. We also review several prominent applications and widely used datasets and highlight directions for future research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/XS4UENHV/Kazemi et al. - 2020 - Representation Learning for Dynamic Graphs A Surv.pdf}
}

@article{kellerTopographicVAEsLearn2021,
  title = {Topographic {{VAEs}} Learn {{Equivariant Capsules}}},
  author = {Keller, T. Anderson and Welling, Max},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.01394 [cs]},
  eprint = {2109.01394},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this work we seek to bridge the concepts of topographic organization and equivariance in neural networks. To accomplish this, we introduce the Topographic VAE: a novel method for efficiently training deep generative models with topographically organized latent variables. We show that such a model indeed learns to organize its activations according to salient characteristics such as digit class, width, and style on MNIST. Furthermore, through topographic organization over time (i.e. temporal coherence), we demonstrate how predefined latent space transformation operators can be encouraged for observed transformed input sequences \textendash{} a primitive form of unsupervised learned equivariance. We demonstrate that this model successfully learns sets of approximately equivariant features (i.e. "capsules") directly from sequences and achieves higher likelihood on correspondingly transforming test sequences. Equivariance is verified quantitatively by measuring the approximate commutativity of the inference network and the sequence transformations. Finally, we demonstrate approximate equivariance to complex transformations, expanding upon the capabilities of existing group equivariant neural networks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/konstantinschurholt/Zotero/storage/KLB6E49C/Keller and Welling - 2021 - Topographic VAEs learn Equivariant Capsules.pdf;/Users/konstantinschurholt/Zotero/storage/P5WYBXMB/2109.html}
}

@misc{kimPureTransformersAre2022,
  title = {Pure {{Transformers}} Are {{Powerful Graph Learners}}},
  author = {Kim, Jinwoo and Nguyen, Tien Dat and Min, Seonwoo and Cho, Sungjun and Lee, Moontae and Lee, Honglak and Hong, Seunghoon},
  year = {2022},
  month = jul,
  number = {arXiv:2207.02505},
  eprint = {2207.02505},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We show that standard Transformers without graph-specific modifications can lead to promising results in graph learning both in theory and practice. Given a graph, we simply treat all nodes and edges as independent tokens, augment them with token embeddings, and feed them to a Transformer. With an appropriate choice of token embeddings, we prove that this approach is theoretically at least as expressive as an invariant graph network (2-IGN) composed of equivariant linear layers, which is already more expressive than all message-passing Graph Neural Networks (GNN). When trained on a large-scale graph dataset (PCQM4Mv2), our method coined Tokenized Graph Transformer (TokenGT) achieves significantly better results compared to GNN baselines and competitive results compared to Transformer variants with sophisticated graph-specific inductive bias. Our implementation is available at https://github.com/jw9730/tokengt.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/B6YUDLFQ/Kim et al. - 2022 - Pure Transformers are Powerful Graph Learners.pdf}
}

@inproceedings{kingmaAutoEncodingVariationalBayes2013,
  title = {Auto-{{Encoding Variational Bayes}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2013},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/95R5NKU7/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf}
}

@article{kipfVariationalGraphAutoEncoders2016,
  title = {Variational {{Graph Auto-Encoders}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2016},
  month = nov,
  journal = {arXiv:1611.07308 [cs, stat]},
  eprint = {1611.07308},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/9CQCVNR3/Kipf and Welling - 2016 - Variational Graph Auto-Encoders.pdf}
}

@article{kirichenkoLastLayerReTraining2022,
  title = {Last {{Layer Re-Training}} Is {{Sufficient}} for {{Robustness}} to {{Spurious Correlations}}},
  author = {Kirichenko, Polina and Izmailov, Pavel and Wilson, Andrew Gordon},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.02937 [cs, stat]},
  eprint = {2204.02937},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Neural network classifiers can largely rely on simple spurious features, such as backgrounds, to make predictions. However, even in these cases, we show that they still often learn core features associated with the desired attributes of the data, contrary to recent findings. Inspired by this insight, we demonstrate that simple last layer retraining can match or outperform state-of-the-art approaches on spurious correlation benchmarks, but with profoundly lower complexity and computational expenses. Moreover, we show that last layer retraining on large ImageNet-trained models can also significantly reduce reliance on background and texture information, improving robustness to covariate shift, after only minutes of training on a single GPU.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/QCVQ4G25/Kirichenko et al. - 2022 - Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations.pdf}
}

@article{klocekHypernetworkFunctionalImage2019,
  title = {Hypernetwork Functional Image Representation},
  author = {Klocek, Sylwester and Maziarka, Lukasz and Wo{\l}czyk, Maciej and Tabor, Jacek and Nowak, Jakub and {\'S}mieja, Marek},
  year = {2019},
  journal = {arXiv:1902.10404 [cs, stat]},
  volume = {11731},
  eprint = {1902.10404},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {496--510},
  doi = {10.1007/978-3-030-30493-5_48},
  abstract = {Hypernetworks mechanism allows to generate and train neural networks (target networks) with use of other neural network (hypernetwork). In this paper, we extend this idea and show that hypernetworks are able to generate target networks, which can be customized to serve different purposes. In particular, we apply this mechanism to create a continuous functional representation of images. Namely, the hypernetwork takes an image and at test time produces weights to a target network, which approximates its RGB pixel intensities. Due to the continuity of representation, we may look at the image at different scales or fill missing regions. Second, we demonstrate how to design a hypernetwork, which produces a generative model for a new data set at test time. Experimental results demonstrate that the proposed mechanism can be successfully used in super-resolution and 2D object modeling.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/I9EER4QH/Klocek et al. - 2019 - Hypernetwork functional image representation.pdf;/Users/konstantinschurholt/Zotero/storage/LZUF7DLN/Klocek et al. - 2019 - Hypernetwork functional image representation.pdf;/Users/konstantinschurholt/Zotero/storage/T34MAMA6/Klocek et al. - 2019 - Hypernetwork functional image representation.pdf}
}

@inproceedings{knyazevParameterPredictionUnseen2021,
  title = {Parameter {{Prediction}} for {{Unseen Deep Architectures}}},
  booktitle = {Conference on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Knyazev, Boris and Drozdzal, Michal and Taylor, Graham W. and {Romero-Soriano}, Adriana},
  year = {2021},
  eprint = {2110.13100},
  eprinttype = {arxiv},
  abstract = {Deep learning has been successful in automating the design of features in machine learning pipelines. However, the algorithms optimizing neural network parameters remain largely hand-designed and computationally inefficient. We study if we can use deep learning to directly predict these parameters by exploiting the past knowledge of training other networks. We introduce a large-scale dataset of diverse computational graphs of neural architectures \textendash{} DEEPNETS-1M\textendash{} and use it to explore parameter prediction on CIFAR-10 and ImageNet. By leveraging advances in graph neural networks, we propose a hypernetwork that can predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good performance on unseen and diverse networks. For example, it is able to predict all 24 million parameters of a ResNet-50 achieving a 60\% accuracy on CIFAR-10. On ImageNet, top-5 accuracy of some of our networks approaches 50\%. Our task along with the model and results can potentially lead to a new, more computationally efficient paradigm of training networks. Our model also learns a strong representation of neural architectures enabling their analysis.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/HXBKSBXT/Knyazev et al. - 2021 - Parameter Prediction for Unseen Deep Architectures.pdf}
}

@article{kolesnikovRevisitingSelfSupervisedVisual2019,
  title = {Revisiting {{Self-Supervised Visual Representation Learning}}},
  author = {Kolesnikov, Alexander and Zhai, Xiaohua and Beyer, Lucas},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.09005 [cs]},
  eprint = {1901.09005},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Unsupervised visual representation learning remains a largely unsolved problem in computer vision research. Among a big body of recently proposed approaches for unsupervised learning of visual representations, a class of self-supervised techniques achieves superior performance on many challenging benchmarks. A large number of the pretext tasks for self-supervised learning have been studied, but other important aspects, such as the choice of convolutional neural networks (CNN), has not received equal attention. Therefore, we revisit numerous previously proposed self-supervised models, conduct a thorough large scale study and, as a result, uncover multiple crucial insights. We challenge a number of common practices in selfsupervised visual representation learning and observe that standard recipes for CNN design do not always translate to self-supervised representation learning. As part of our study, we drastically boost the performance of previously proposed techniques and outperform previously published state-of-the-art results by a large margin.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/EKUL4FSP/Kolesnikov et al. - 2019 - Revisiting Self-Supervised Visual Representation L.pdf}
}

@article{kornblithSimilarityNeuralNetwork2019,
  title = {Similarity of {{Neural Network Representations Revisited}}},
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  year = {2019},
  month = may,
  journal = {arXiv:1905.00414 [cs, q-bio, stat]},
  eprint = {1905.00414},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio, stat},
  abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Similarity Measures,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/ILE7M6PH/Kornblith et al. - 2019 - Similarity of Neural Network Representations Revis.pdf}
}

@inproceedings{koutnikEvolvingNeuralNetworks2010,
  title = {Evolving Neural Networks in Compressed Weight Space},
  booktitle = {Proceedings of the 12th Annual Conference on {{Genetic}} and Evolutionary Computation - {{GECCO}} '10},
  author = {Koutnik, Jan and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2010},
  pages = {619},
  publisher = {{ACM Press}},
  address = {{Portland, Oregon, USA}},
  doi = {10.1145/1830483.1830596},
  abstract = {We propose a new indirect encoding scheme for neural networks in which the weight matrices are represented in the frequency domain by sets of Fourier coefficients. This scheme exploits spatial regularities in the matrix to reduce the dimensionality of the representation by ignoring high-frequency coefficients, as is done in lossy image compression. We compare the efficiency of searching in this ``compressed'' network space to searching in the space of directly encoded networks, using the CoSyNE neuroevolution algorithm on three benchmark problems: pole-balancing, ball throwing and octopusarm control. The results show that this encoding can dramatically reduce the search space dimensionality such that solutions can be found in significantly fewer evaluations.},
  isbn = {978-1-4503-0072-8},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/ZFHYHW5G/Koutnik et al. - 2010 - Evolving neural networks in compressed weight spac.pdf}
}

@article{kriegeskorteRepresentationalSimilarityAnalysis2008,
  title = {Representational Similarity Analysis \textendash{} Connecting the Branches of Systems Neuroscience},
  author = {Kriegeskorte, Nikolaus},
  year = {2008},
  journal = {Frontiers in Systems Neuroscience},
  issn = {16625137},
  doi = {10.3389/neuro.06.004.2008},
  abstract = {A fundamental challenge for systems neuroscience is to quantitatively relate its three major branches of research: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to define the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities.The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/7VEYHTEV/Kriegeskorte - 2008 - Representational similarity analysis ‚Äì connecting .pdf}
}

@article{krizhevskyLearningMultipleLayers2009,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  year = {2009},
  pages = {60},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/F79YMCQW/Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf}
}

@article{kuditipudiExplainingLandscapeConnectivity2020,
  title = {Explaining {{Landscape Connectivity}} of {{Low-cost Solutions}} for {{Multilayer Nets}}},
  author = {Kuditipudi, Rohith and Wang, Xiang and Lee, Holden and Zhang, Yi and Li, Zhiyuan and Hu, Wei and Arora, Sanjeev and Ge, Rong},
  year = {2020},
  month = jan,
  journal = {arXiv:1906.06247 [cs, stat]},
  eprint = {1906.06247},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Mode connectivity (Garipov et al., 2018; Draxler et al., 2018) is a surprising phenomenon in the loss landscape of deep nets. Optima\textemdash at least those discovered by gradient-based optimization\textemdash turn out to be connected by simple paths on which the loss function is almost constant. Often, these paths can be chosen to be piece-wise linear, with as few as two segments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/4NJKPPH2/Kuditipudi et al. - 2020 - Explaining Landscape Connectivity of Low-cost Solu.pdf}
}

@article{laaksoContentClusterAnalysis2000,
  title = {Content and Cluster Analysis: {{Assessing}} Representational Similarity in Neural Systems},
  shorttitle = {Content and Cluster Analysis},
  author = {Laakso, Aarre and Cottrell, Garrison},
  year = {2000},
  month = mar,
  journal = {Philosophical Psychology},
  volume = {13},
  number = {1},
  pages = {47--76},
  issn = {0951-5089, 1465-394X},
  doi = {10.1080/09515080050002726},
  abstract = {If connectionism is to be an adequate theory of mind, we must have a theory of representation for neural networks that allows for individual differences in weighting and architecture while preserving sameness, or at least similarity, of content. In this paper we propose a procedure for measuring sameness of content of neural representations. We argue that the correct way to compare neural representations is through analysis of the distances between neural activations, and we present a method for doing so. We then use the technique to demonstrate empirically that different arti cial neural networks trained by backpropagation on the same categorization task, even with different representational encodings of the input patterns and different numbers of hidden units, reach states in which representations at the hidden units are similar. We discuss how this work provides a rebuttal to Fodor and Lepore's critique of Paul Churchland's state space semantics.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/UR9G7I32/Laakso and Cottrell - 2000 - Content and cluster analysis Assessing representa.pdf}
}

@article{larsenHowManyDegrees2021,
  title = {How Many Degrees of Freedom Do We Need to Train Deep Networks: A Loss Landscape Perspective},
  shorttitle = {How Many Degrees of Freedom Do We Need to Train Deep Networks},
  author = {Larsen, Brett W. and Fort, Stanislav and Becker, Nic and Ganguli, Surya},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.05802 [cs, stat]},
  eprint = {2107.05802},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {A variety of recent works, spanning pruning, lottery tickets, and training within random subspaces, have shown that deep neural networks can be trained using far fewer degrees of freedom than the total number of parameters. We explain this phenomenon by first examining the success probability of hitting a training loss sublevel set when training within a random subspace of a given training dimensionality. We find a sharp phase transition in the success probability from 0 to 1 as the training dimension surpasses a threshold. This threshold training dimension increases as the desired final loss decreases, but decreases as the initial loss decreases. We then theoretically explain the origin of this phase transition, and its dependence on initialization and final desired loss, in terms of precise properties of the high dimensional geometry of the loss landscape. In particular, we show via Gordon's escape theorem, that the training dimension plus the Gaussian width of the desired loss sub-level set, projected onto a unit sphere surrounding the initialization, must exceed the total number of parameters for the success probability to be large. In several architectures and datasets, we measure the threshold training dimension as a function of initialization and demonstrate that it is a small fraction of the total number of parameters, thereby implying, by our theory, that successful training with so few dimensions is possible precisely because the Gaussian width of low loss sub-level sets is very large. Moreover, this threshold training dimension provides a strong null model for assessing the efficacy of more sophisticated ways to reduce training degrees of freedom, including lottery tickets as well a more optimal method we introduce: lottery subspaces.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/2QWA3B5E/Larsen et al. - 2021 - How many degrees of freedom do we need to train de.pdf}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/PN6MME6P/LeCun et al. - 2015 - Deep learning.pdf}
}

@article{lecunGradientBasedLearningApplied1998,
  title = {Gradient-{{Based Learning Applied}} to {{Document Recognition}}},
  author = {LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {/Users/konstantinschurholt/Zotero/storage/JDH7TQC2/Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf;/Users/konstantinschurholt/Zotero/storage/ZPWMN56F/726791.html}
}

@misc{lecunISSCC2019Deep2019,
  title = {{{ISSCC}} 2019: {{Deep Learning Hardware}}: {{Past}}, {{Present}}, and {{Future}} - {{Yann LeCun}}},
  shorttitle = {{{ISSCC}} 2019},
  author = {LeCun, Yann},
  year = {2019},
  month = may,
  abstract = {Yann LeCun, Facebook AI Research \& New York University, New York, NY Deep learning has caused revolutions in computer understanding of images, audio, and text, enabling new applications such as information search and filtering, autonomous driving, radiology screening, real-time language translation, and virtual assistants. But almost all these successes largely use supervised learning, which requires human-annotated data, or reinforcement learning, which requires too many trials to be practical in most real-world situations. In contrast, animals and humans seem to learn vast amounts of background knowledge about the world through mere observation and occasional actions in a selfsupervised manner. Making progress in self-supervised learning is the main challenge of AI for the next decade. Success may result in machines with some level of common sense. But they will be built around deep learning architectures that are considerably larger than current ones, requiring vastly more powerful hardware than what we have today.}
}

@misc{lecunSelfsupervisedLearningDark2021,
  title = {Self-Supervised Learning: {{The}} Dark Matter of Intelligence},
  shorttitle = {Self-Supervised Learning},
  author = {LeCun, Yann and Misra, Ishan},
  year = {2021},
  month = apr,
  abstract = {How can we build machines with human-level intelligence? There's a limit to how far the field of AI can go with supervised learning alone. Here's why self-supervised learning is one of the most promising ways to make significant progress in AI.},
  howpublished = {https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/SHS4PD3I/self-supervised-learning-the-dark-matter-of-intelligence.html}
}

@inproceedings{leeLearningSharedKnowledge2019,
  title = {Learning {{Shared Knowledge}} for {{Deep Lifelong Learning}} Using {{Deconvolutional Networks}}},
  booktitle = {Proceedings of the {{Twenty-Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Lee, Seungwon and Stokes, James and Eaton, Eric},
  year = {2019},
  month = aug,
  pages = {2837--2844},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Macao, China}},
  doi = {10.24963/ijcai.2019/393},
  abstract = {Current mechanisms for knowledge transfer in deep networks tend to either share the lower layers between tasks, or build upon representations trained on other tasks. However, existing work in non-deep multi-task and lifelong learning has shown success with using factorized representations of the model parameter space for transfer, permitting more flexible construction of task models. Inspired by this idea, we introduce a novel architecture for sharing latent factorized representations in convolutional neural networks (CNNs). The proposed approach, called a deconvolutional factorized CNN, uses a combination of deconvolutional factorization and tensor contraction to perform flexible transfer between tasks. Experiments on two computer vision data sets show that the DF-CNN achieves superior performance in challenging lifelong learning settings, resists catastrophic forgetting, and exhibits reverse transfer to improve previously learned tasks from subsequent experience without retraining.},
  isbn = {978-0-9992411-4-1},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/6U8QV46N/Lee et al. - 2019 - Learning Shared Knowledge for Deep Lifelong Learni.pdf;/Users/konstantinschurholt/Zotero/storage/A6ELH4S2/Lee et al. - 2019 - Learning Shared Knowledge for Deep Lifelong Learni.pdf;/Users/konstantinschurholt/Zotero/storage/LEQDME4M/Lee et al. - 2019 - Learning Shared Knowledge for Deep Lifelong Learni.pdf}
}

@article{leeNeuralComplexityMeasures2020,
  title = {Neural {{Complexity Measures}}},
  author = {Lee, Yoonho and Lee, Juho and Hwang, Sung Ju and Yang, Eunho and Choi, Seungjin},
  year = {2020},
  month = oct,
  journal = {arXiv:2008.02953 [cs, stat]},
  eprint = {2008.02953},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {While various complexity measures for deep neural networks exist, specifying an appropriate measure capable of predicting and explaining generalization in deep networks has proven challenging. We propose Neural Complexity (NC), a meta-learning framework for predicting generalization. Our model learns a scalar complexity measure through interactions with many heterogeneous tasks in a datadriven way. The trained NC model can be added to the standard training loss to regularize any task learner in a standard supervised learning scenario. We contrast NC's approach against existing manually-designed complexity measures and other meta-learning models, and we validate NC's performance on multiple regression and classification tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/JDR7SWH5/2008.02953.pdf}
}

@article{leeSetTransformerFramework2019,
  title = {Set {{Transformer}}: {{A Framework}} for {{Attention-based Permutation-Invariant Neural Networks}}},
  shorttitle = {Set {{Transformer}}},
  author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam R. and Choi, Seungjin and Teh, Yee Whye},
  year = {2019},
  month = may,
  journal = {arXiv:1810.00825 [cs, stat]},
  eprint = {1810.00825},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Many machine learning tasks such as multiple instance learning, 3D shape recognition and fewshot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/MG7S9CHU/Lee et al. - 2019 - Set Transformer A Framework for Attention-based P.pdf}
}

@article{leeSetTransformerFramework2019a,
  title = {Set {{Transformer}}: {{A Framework}} for {{Attention-based Permutation-Invariant Neural Networks}}},
  shorttitle = {Set {{Transformer}}},
  author = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam R. and Choi, Seungjin and Teh, Yee Whye},
  year = {2019},
  month = may,
  journal = {arXiv:1810.00825 [cs, stat]},
  eprint = {1810.00825},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Many machine learning tasks such as multiple instance learning, 3D shape recognition and fewshot image classification are defined on sets of instances. Since solutions to such problems do not depend on the order of elements of the set, models used to address them should be permutation invariant. We present an attention-based neural network module, the Set Transformer, specifically designed to model interactions among elements in the input set. The model consists of an encoder and a decoder, both of which rely on attention mechanisms. In an effort to reduce computational complexity, we introduce an attention scheme inspired by inducing point methods from sparse Gaussian process literature. It reduces computation time of self-attention from quadratic to linear in the number of elements in the set. We show that our model is theoretically attractive and we evaluate it on a range of tasks, demonstrating increased performance compared to recent methods for set-structured data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/J4J6FNVD/Lee et al. - 2019 - Set Transformer A Framework for Attention-based P.pdf}
}

@article{leTinyImageNetVisual,
  title = {Tiny {{ImageNet Visual Recognition Challenge}}},
  author = {Le, Ya and Yang, Xuan},
  pages = {6},
  abstract = {In this work, we investigate the effect of convolutional network depth, receptive field size, dropout layers, rectified activation unit type and dataset noise on its accuracy in Tiny-ImageNet Challenge settings. In order to make a thorough evaluation of the cause of the peformance improvement, we start with a basic 5 layer model with 5\texttimes 5 convolutional receptive fields. We keep increasing network depth or reducing receptive field size, and continue applying modern techniques, such as PReLu and dropout, to the model. Our model achieves excellent performance even compared to state-of-the-art results, with 0.444 final error rate on the test set.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/Y5JYCTFT/Le and Yang - Tiny ImageNet Visual Recognition Challenge.pdf}
}

@article{liawTuneResearchPlatform2018,
  title = {Tune: {{A Research Platform}} for {{Distributed Model Selection}} and {{Training}}},
  shorttitle = {Tune},
  author = {Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.05118 [cs, stat]},
  eprint = {1807.05118},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Modern machine learning algorithms are increasingly computationally demanding, requiring specialized hardware and distributed computation to achieve high performance in a reasonable time frame. Many hyperparameter search algorithms have been proposed for improving the efficiency of model selection, however their adaptation to the distributed compute environment is often ad-hoc. We propose Tune, a unified framework for model selection and training that provides a narrow-waist interface between training scripts and search algorithms. We show that this interface meets the requirements for a broad range of hyperparameter search algorithms, allows straightforward scaling of search to large clusters, and simplifies algorithm implementation. We demonstrate the implementation of several state-of-the-art hyperparameter search algorithms in Tune. Tune is available at http://ray.readthedocs.io/en/latest/tune.html.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/NRDTP4VS/Liaw et al. - 2018 - Tune A Research Platform for Distributed Model Se.pdf}
}

@article{liConvergentLearningDifferent2015,
  title = {Convergent {{Learning}}: {{Do}} Different Neural Networks Learn the Same Representations?},
  shorttitle = {Convergent {{Learning}}},
  author = {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
  year = {2015},
  month = nov,
  journal = {arXiv:1511.07543 [cs]},
  eprint = {1511.07543},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of parameters, but valuable because it increases our ability to understand current models and create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low-dimensional spaces. We propose a specific method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level: a bipartite matching approach that makes one-to-one assignments between neurons, a sparse prediction approach that finds one-to-many mappings, and a spectral clustering approach that finds many-to-many mappings. This initial investigation reveals a few previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the representation codes show evidence of being a mix between a local code and slightly, but not fully, distributed codes across multiple units.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/konstantinschurholt/Zotero/storage/B9F3K63Z/Li et al. - 2015 - Convergent Learning Do different neural networks .pdf}
}

@article{liDELTADEepLearning2020,
  title = {{{DELTA}}: {{DEep Learning Transfer}} Using {{Feature Map}} with {{Attention}} for {{Convolutional Networks}}},
  shorttitle = {{{DELTA}}},
  author = {Li, Xingjian and Xiong, Haoyi and Wang, Hanchao and Rao, Yuxuan and Liu, Liping and Chen, Zeyu and Huan, Jun},
  year = {2020},
  month = may,
  journal = {arXiv:1901.09229 [cs, stat]},
  eprint = {1901.09229},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Transfer learning through fine-tuning a pre-trained neural network with an extremely large dataset, such as ImageNet, can significantly accelerate training while the accuracy is frequently bottlenecked by the limited dataset size of the new target task. To solve the problem, some regularization methods, constraining the outer layer weights of the target network using the starting point as references (SPAR), have been studied. In this paper, we propose a novel regularized transfer learning framework DELTA, namely DEep Learning Transfer using Feature Map with Attention. Instead of constraining the weights of neural network, DELTA aims to preserve the outer layer outputs of the target network. Specifically, in addition to minimizing the empirical loss, DELTA aligns the outer layer outputs of two networks, through constraining a subset of feature maps that are precisely selected by attention that has been learned in a supervised learning manner. We evaluate DELTA with the state-of-the-art algorithms, including L2 and L2-SP . The experiment results show that our method outperforms these baselines with higher accuracy for new tasks. Code has been made publicly available for PaddlePaddle1 and Pytorch2.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/ER6T6IGM/Li et al. - 2020 - DELTA DEep Learning Transfer using Feature Map wi.pdf}
}

@article{liDHPDifferentiableMeta2020,
  title = {{{DHP}}: {{Differentiable Meta Pruning}} via {{HyperNetworks}}},
  shorttitle = {{{DHP}}},
  author = {Li, Yawei and Gu, Shuhang and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.13683 [cs, eess]},
  eprint = {2003.13683},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  abstract = {Network pruning has been the driving force for the efficient inference of neural networks and the alleviation of model storage and transmission burden. Traditional network pruning methods focus on the per-filter influence on the network accuracy by analyzing the filter distribution. With the advent of AutoML and neural architecture search (NAS), pruning has become topical with automatic mechanism and searching based architecture optimization. However, current automatic designs rely on either reinforcement learning or evolutionary algorithm, which often do not have a theoretical convergence guarantee or do not converge in a meaningful time limit.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/konstantinschurholt/Zotero/storage/I46Z9T4U/Li et al. - 2020 - DHP Differentiable Meta Pruning via HyperNetworks.pdf;/Users/konstantinschurholt/Zotero/storage/Q84PLQ2T/Li et al. - 2020 - DHP Differentiable Meta Pruning via HyperNetworks.pdf;/Users/konstantinschurholt/Zotero/storage/Z43QBN7X/Li et al. - 2020 - DHP Differentiable Meta Pruning via HyperNetworks.pdf}
}

@article{liEpisodicTrainingDomain2019,
  title = {Episodic {{Training}} for {{Domain Generalization}}},
  author = {Li, Da and Zhang, Jianshu and Yang, Yongxin and Liu, Cong and Song, Yi-Zhe and Hospedales, Timothy M.},
  year = {2019},
  month = jan,
  journal = {arXiv:1902.00113 [cs]},
  eprint = {1902.00113},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Domain generalization (DG) is the challenging and topical problem of learning models that generalize to novel testing domain with different statistics than a set of known training domains. The simple approach of aggregating data from all source domains and training a single deep neural network end-to-end on all the data provides a surprisingly strong baseline that surpasses many prior published methods. In this paper we build on this strong baseline by designing an episodic training procedure that trains a single deep network in a way that exposes it to the domain shift that characterises a novel domain at runtime. Specifically, we decompose a deep network into feature extractor and classifier components, and then train each component by simulating it interacting with a partner who is badly tuned for the current domain. This makes both components more robust, ultimately leading to our networks producing state-of-the-art performance on three DG benchmarks. As a demonstration, we consider the pervasive workflow of using an ImageNet trained CNN as a fixed feature extractor for downstream recognition tasks. Using the Visual Decathlon benchmark, we demonstrate that our episodic-DG training improves the performance of such a general purpose feature extractor by explicitly training it for robustness to novel problems. This provides the largest-scale demonstration of heterogeneous DG to date.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/6GBYB7RZ/Li et al. - 2019 - Episodic Training for Domain Generalization.pdf}
}

@article{liExplicitInductiveBias,
  title = {Explicit {{Inductive Bias}} for {{Transfer Learning}} with {{Convolutional Networks}}},
  author = {Li, Xuhong and Grandvalet, Yves and Davoine, Franck},
  pages = {10},
  abstract = {In inductive transfer learning, fine-tuning pretrained convolutional networks substantially outperforms training from scratch. When using finetuning, the underlying assumption is that the pretrained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task. However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task. In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model. We show the benefit of having an explicit inductive bias towards the initial model, and we eventually recommend a simple L2 penalty with the pre-trained model being a reference as the baseline of penalty for transfer learning tasks.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/6A4IG9H9/Li et al. - Explicit Inductive Bias for Transfer Learning with.pdf}
}

@article{liHyperbandNovelBanditBased2018,
  title = {Hyperband: {{A Novel Bandit-Based Approach}} to {{Hyperparameter Optimization}}},
  shorttitle = {Hyperband},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2018},
  month = jun,
  journal = {Journal of Machine Learning Research (JMLR)},
  volume = {18},
  eprint = {1603.06560},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/X3N7946N/Li et al. - 2018 - Hyperband A Novel Bandit-Based Approach to Hyperp.pdf}
}

@article{liLearningGeneralizeMetaLearning2017,
  title = {Learning to {{Generalize}}: {{Meta-Learning}} for {{Domain Generalization}}},
  shorttitle = {Learning to {{Generalize}}},
  author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M.},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.03463 [cs]},
  eprint = {1710.03463},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Domain shift refers to the well known problem that a model trained in one source domain performs poorly when applied to a target domain with different statistics. Domain Generalization (DG) techniques attempt to alleviate this issue by producing models which by design generalize well to novel testing domains. We propose a novel meta-learning method for domain generalization. Rather than designing a specific model that is robust to domain shift as in most previous DG work, we propose a model agnostic training procedure for DG. Our algorithm simulates train/test domain shift during training by synthesizing virtual testing domains within each mini-batch. The meta-optimization objective requires that steps to improve training domain performance should also improve testing domain performance. This meta-learning procedure trains models with good generalization ability to novel domains. We evaluate our method and achieve state of the art results on a recent cross-domain image classification benchmark, as well demonstrating its potential on two classic reinforcement learning tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/Q57ZLK9Y/Li et al. - 2017 - Learning to Generalize Meta-Learning for Domain G.pdf}
}

@inproceedings{liSystemMassivelyParallel2020,
  title = {A {{System}} for {{Massively Parallel Hyperparameter Tuning}}},
  booktitle = {Proceedings of the 3 Rd {{MLSys Conference}}},
  author = {Li, Liam and Jamieson, Kevin and Rostamizadeh, Afshin and Gonina, Ekaterina and Hardt, Moritz and Recht, Benjamin and Talwalkar, Ameet},
  year = {2020},
  eprint = {1810.05934},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  address = {{Austin, TX, USA}},
  abstract = {Modern learning models are characterized by large hyperparameter spaces and long training times. These properties, coupled with the rise of parallel computing and the growing demand to productionize machine learning workloads, motivate the need to develop mature hyperparameter optimization functionality in distributed computing settings. We address this challenge by first introducing a simple and robust hyperparameter optimization algorithm called ASHA, which exploits parallelism and aggressive early-stopping to tackle large-scale hyperparameter optimization problems. Our extensive empirical results show that ASHA outperforms existing state-of-the-art hyperparameter optimization methods; scales linearly with the number of workers in distributed settings; and is suitable for massive parallelism, as demonstrated on a task with 500 workers. We then describe several design decisions we encountered, along with our associated solutions, when integrating ASHA in Determined AI's end-to-end production-quality machine learning system that offers hyperparameter tuning as a service.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/QYWKS6WA/Li et al. - 2020 - A System for Massively Parallel Hyperparameter Tun.pdf}
}

@article{liuEnsembleLearningNegative1999,
  title = {Ensemble Learning via Negative Correlation},
  author = {Liu, Y. and Yao, X.},
  year = {1999},
  month = dec,
  journal = {Neural Networks},
  volume = {12},
  number = {10},
  pages = {1399--1404},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(99)00073-8},
  abstract = {This paper presents a learning approach, i.e. negative correlation learning, for neural network ensembles. Unlike previous learning approaches for neural network ensembles, negative correlation learning attempts to train individual networks in an ensemble and combines them in the same learning process. In negative correlation learning, all the individual networks in the ensemble are trained simultaneously and interactively through the correlation penalty terms in their error functions. Rather than producing unbiased individual networks whose errors are uncorrelated, negative correlation learning can create negatively correlated networks to encourage specialisation and cooperation among the individual networks. Empirical studies have been carried out to show why and how negative correlation learning works. The experimental results show that negative correlation learning can produce neural network ensembles with good generalisation ability.},
  langid = {english},
  keywords = {Combination method,Correct response set,Correlation,Generalisation,Negative correlation learning,Neural network ensembles},
  file = {/Users/konstantinschurholt/Zotero/storage/VZ79DRIX/Liu and Yao - 1999 - Ensemble learning via negative correlation.pdf;/Users/konstantinschurholt/Zotero/storage/ALNPJNAD/S0893608099000738.html}
}

@inproceedings{liuKnowledgeFlowImprove2019,
  title = {Knowledge {{Flow}}: {{Improve Upon Your Teachers}}},
  shorttitle = {Knowledge {{Flow}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Liu, Iou-Jen and Peng, Jian and Schwing, Alexander G.},
  year = {2019},
  month = apr,
  eprint = {1904.05878},
  eprinttype = {arxiv},
  abstract = {A zoo of deep nets is available these days for almost any given task, and it is increasingly unclear which net to start with when addressing a new task, or which net to use as an initialization for fine-tuning a new model. To address this issue, in this paper, we develop knowledge flow which moves `knowledge' from multiple deep nets, referred to as teachers, to a new deep net model, called the student. The structure of the teachers and the student can differ arbitrarily and they can be trained on entirely different tasks with different output spaces too. Upon training with knowledge flow the student is independent of the teachers. We demonstrate our approach on a variety of supervised and reinforcement learning tasks, outperforming fine-tuning and other `knowledge exchange' methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/FN33EU4M/Liu et al. - 2019 - Knowledge Flow Improve Upon Your Teachers.pdf}
}

@inproceedings{liuLearningTurningNeural2021,
  title = {Learning by {{Turning}}: {{Neural Architecture Aware Optimisation}}},
  shorttitle = {Learning by {{Turning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Liu, Yang and Bernstein, Jeremy and Meister, Markus and Yue, Yisong},
  year = {2021},
  month = jul,
  pages = {6748--6758},
  publisher = {{PMLR}},
  abstract = {Descent methods for deep networks are notoriously capricious: they require careful tuning of step size, momentum and weight decay, and which method will work best on a new benchmark is a priori unc...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/A743PFNY/Liu et al. - 2021 - Learning by Turning Neural Architecture Aware Optimisation.pdf}
}

@inproceedings{liuWeActuallyNeed2021,
  title = {Do {{We Actually Need Dense Over-Parameterization}}? {{In-Time Over-Parameterization}} in {{Sparse Training}}},
  shorttitle = {Do {{We Actually Need Dense Over-Parameterization}}?},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Liu, Shiwei and Yin, Lu and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  year = {2021},
  month = jul,
  pages = {6989--7000},
  publisher = {{PMLR}},
  abstract = {In this paper, we introduce a new perspective on training deep neural networks capable of state-of-the-art performance without the need for the expensive over-parameterization by proposing the conc...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/F5ELC2HF/Liu et al. - 2021 - Do We Actually Need Dense Over-Parameterization In-Time Over-Parameterization in Sparse Training.pdf}
}

@inproceedings{liVisualizingLossLandscape2018,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  booktitle = {{{NIPS}}},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  year = {2018},
  pages = {11},
  abstract = {Neural network training relies on our ability to find ``good'' minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple ``filter normalization'' method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/JLTW4A52/Li et al. - Visualizing the Loss Landscape of Neural Nets.pdf}
}

@article{lucasAnalyzingMonotonicLinear,
  title = {Analyzing {{Monotonic Linear Interpolation}} in {{Neural Network Loss Landscapes}}},
  author = {Lucas, James and Bae, Juhan and Zhang, Michael R and Fort, Stanislav and Zemel, Richard and Grosse, Roger},
  pages = {12},
  abstract = {Linear interpolation between initial neural network parameters and converged parameters after training with stochastic gradient descent (SGD) typically leads to a monotonic decrease in the training objective. This Monotonic Linear Interpolation (MLI) property, first observed by Goodfellow et al. (2014), persists in spite of the nonconvex objectives and highly non-linear training dynamics of neural networks. Extending this work, we evaluate several hypotheses for this property that, to our knowledge, have not yet been explored. Using tools from differential geometry, we draw connections between the interpolated paths in function space and the monotonicity of the network \textemdash{} providing sufficient conditions for the MLI property under mean squared error. While the MLI property holds under various settings (e.g. network architectures and learning problems), we show in practice that networks violating the MLI property can be produced systematically, by encouraging the weights to move far from initialization. The MLI property raises important questions about the loss landscape geometry of neural networks and highlights the need to further study their global properties.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/CKGWWE2S/Lucas et al. - Analyzing Monotonic Linear Interpolation in Neural.pdf}
}

@inproceedings{lucasMonotonicLinearInterpolation2021,
  title = {On {{Monotonic Linear Interpolation}} of {{Neural Network Parameters}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Lucas, James R. and Bae, Juhan and Zhang, Michael R. and Fort, Stanislav and Zemel, Richard and Grosse, Roger B.},
  year = {2021},
  month = jul,
  pages = {7168--7179},
  publisher = {{PMLR}},
  abstract = {Linear interpolation between initial neural network parameters and converged parameters after training with stochastic gradient descent (SGD) typically leads to a monotonic decrease in the training...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/BYQ5EWM3/Lucas et al. - 2021 - On Monotonic Linear Interpolation of Neural Network Parameters.pdf}
}

@article{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  author = {Lundberg, Scott and Lee, Su-In},
  year = {2017},
  month = may,
  journal = {arXiv:1705.07874 [cs, stat]},
  eprint = {1705.07874},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/A6PXNVMN/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf}
}

@article{luoDataAugmentationStructured2020,
  title = {Data {{Augmentation}} via {{Structured Adversarial Perturbations}}},
  author = {Luo, Calvin and Mobahi, Hossein and Bengio, Samy},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.03010 [cs]},
  eprint = {2011.03010},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Data augmentation is a major component of many machine learning methods with state-ofthe-art performance. Common augmentation strategies work by drawing random samples from a space of transformations. Unfortunately, such sampling approaches are limited in expressivity, as they are unable to scale to rich transformations that depend on numerous parameters due to the curse of dimensionality. Adversarial examples can be considered as an alternative scheme for data augmentation. By being trained on the most difficult modifications of the inputs, the resulting models are then hopefully able to handle other, presumably easier, modifications as well. The advantage of adversarial augmentation is that it replaces sampling with the use of a single, calculated perturbation that maximally increases the loss. The downside, however, is that these raw adversarial perturbations appear rather unstructured; applying them often does not produce a natural transformation, contrary to a desirable data augmentation technique. To address this, we propose a method to generate adversarial examples that maintain some desired natural structure. We first construct a subspace that only contains perturbations with the desired structure. We then project the raw adversarial gradient onto this space to select a structured transformation that would maximally increase the loss when applied. We demonstrate this approach through two types of image transformations: photometric and geometric. Furthermore, we show that training on such structured adversarial images improves generalization.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/B4XSEQ9N/Luo et al. - 2020 - Data Augmentation via Structured Adversarial Pertu.pdf}
}

@article{maheswaranathanUniversalityIndividualityNeural2019,
  title = {Universality and Individuality in Neural Dynamics across Large Populations of Recurrent Networks},
  author = {Maheswaranathan, Niru and Williams, Alex H. and Golub, Matthew D. and Ganguli, Surya and Sussillo, David},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.08549 [cs, q-bio]},
  eprint = {1907.08549},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {Task-based modeling with recurrent neural networks (RNNs) has emerged as a popular way to infer the computational function of different brain regions. These models are quantitatively assessed by comparing the low-dimensional neural representations of the model with the brain, for example using canonical correlation analysis (CCA). However, the nature of the detailed neurobiological inferences one can draw from such efforts remains elusive. For example, to what extent does training neural networks to solve common tasks uniquely determine the network dynamics, independent of modeling architectural choices? Or alternatively, are the learned dynamics highly sensitive to different model choices? Knowing the answer to these questions has strong implications for whether and how we should use task-based RNN modeling to understand brain dynamics. To address these foundational questions, we study populations of thousands of networks, with commonly used RNN architectures, trained to solve neuroscientifically motivated tasks and characterize their nonlinear dynamics. We find the geometry of the RNN representations can be highly sensitive to different network architectures, yielding a cautionary tale for measures of similarity that rely on representational geometry, such as CCA. Moreover, we find that while the geometry of neural dynamics can vary greatly across architectures, the underlying computational scaffold\textemdash the topological structure of fixed points, transitions between them, limit cycles, and linearized dynamics\textemdash often appears universal across all architectures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/konstantinschurholt/Zotero/storage/YW744YTM/Maheswaranathan et al. - 2019 - Universality and individuality in neural dynamics .pdf}
}

@article{martinTraditionalHeavyTailedSelf2019,
  title = {Traditional and {{Heavy-Tailed Self Regularization}} in {{Neural Network Models}}},
  author = {Martin, Charles H. and Mahoney, Michael W.},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.08276 [cs, stat]},
  eprint = {1901.08276},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniatureAlexNet. Empirical and theoretical results clearly indicate that the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of regularization, such as Dropout or Weight Norm constraints. Building on recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization. For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a ``size scale'' separating signal from noise. For state-of-theart DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems. This implicit SelfRegularization can depend strongly on the many knobs of the training process. By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/89QP2ZCU/Martin and Mahoney - 2019 - Traditional and Heavy-Tailed Self Regularization i.pdf}
}

@article{maWeightNetRevisitingDesign2020,
  title = {{{WeightNet}}: {{Revisiting}} the {{Design Space}} of {{Weight Networks}}},
  shorttitle = {{{WeightNet}}},
  author = {Ma, Ningning and Zhang, Xiangyu and Huang, Jiawei and Sun, Jian},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.11823 [cs]},
  eprint = {2007.11823},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present a conceptually simple, flexible and effective framework for weight generating networks. Our approach is general that unifies two current distinct and extremely effective SENet and CondConv into the same framework on weight space. The method, called W eightN et, generalizes the two methods by simply adding one more grouped fullyconnected layer to the attention activation layer. We use the WeightNet, composed entirely of (grouped) fully-connected layers, to directly output the convolutional weight. WeightNet is easy and memory-conserving to train, on the kernel space instead of the feature space. Because of the flexibility, our method outperforms existing approaches on both ImageNet and COCO detection tasks, achieving better Accuracy-FLOPs and Accuracy-Parameter trade-offs. The framework on the flexible weight space has the potential to further improve the performance. Code is available at https://github.com/megvii-model/WeightNet.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/NXBBQSRP/Ma et al. - 2020 - WeightNet Revisiting the Design Space of Weight N.pdf}
}

@article{mcinnesUMAPUniformManifold2018a,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}}},
  author = {McInnes, Leland and Healy, John and Saul, Nathaniel},
  year = {2018},
  file = {/Users/konstantinschurholt/Zotero/storage/6PC57WXZ/McInnes et al. - 2018 - UMAP Uniform Manifold Approximation and Projectio.pdf}
}

@article{mehrerIndividualDifferencesDeep2020,
  title = {Individual Differences among Deep Neural Network Models},
  author = {Mehrer, Johannes and Spoerer, Courtney J. and Kriegeskorte, Nikolaus and Kietzmann, Tim C.},
  year = {2020},
  month = dec,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {5725},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-19632-w},
  abstract = {Abstract             Deep neural networks (DNNs) excel at visual recognition tasks and are increasingly used as a modeling framework for neural computations in the primate brain. Just like individual brains, each DNN has a unique connectivity and representational profile. Here, we investigate individual differences among DNN instances that arise from varying only the random initialization of the network weights. Using tools typically employed in systems neuroscience, we show that this minimal change in initial conditions prior to training leads to substantial differences in intermediate and higher-level network representations despite similar network-level classification performance. We locate the origins of the effects in an under-constrained alignment of category exemplars, rather than misaligned category centroids. These results call into question the common practice of using single networks to derive insights into neural information processing and rather suggest that computational neuroscientists working with DNNs may need to base their inferences on groups of multiple network instances.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/6TYY3MMB/Mehrer et al. - 2020 - Individual differences among deep neural network m.pdf;/Users/konstantinschurholt/Zotero/storage/KXMPP7GB/Mehrer et al. - 2020 - Individual differences among deep neural network m.pdf}
}

@inproceedings{mellorNeuralArchitectureSearch2021a,
  title = {Neural {{Architecture Search}} without {{Training}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Mellor, Joe and Turner, Jack and Storkey, Amos and Crowley, Elliot J.},
  year = {2021},
  month = jul,
  pages = {7588--7598},
  publisher = {{PMLR}},
  abstract = {The time and effort involved in hand-designing deep neural networks is immense. This has prompted the development of Neural Architecture Search (NAS) techniques to automate this design. However, NA...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/BFKGWEQH/Mellor et al. - 2021 - Neural Architecture Search without Training.pdf}
}

@article{mensinkFactorsInfluenceTransfer2021,
  title = {Factors of {{Influence}} for {{Transfer Learning}} across {{Diverse Appearance Domains}} and {{Task Types}}},
  author = {Mensink, Thomas and Uijlings, Jasper and Kuznetsova, Alina and Gygli, Michael and Ferrari, Vittorio},
  year = {2021},
  month = nov,
  journal = {arXiv:2103.13318 [cs]},
  eprint = {2103.13318},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Transfer learning enables to re-use knowledge learned on a source task to help learning a target task. A simple form of transfer learning is common in current state-of-the-art computer vision models, i.e. pre-training a model for image classification on the ILSVRC dataset, and then fine-tune on any target task. However, previous systematic studies of transfer learning have been limited and the circumstances in which it is expected to work are not fully understood. In this paper we carry out an extensive experimental exploration of transfer learning across vastly different image domains (consumer photos, autonomous driving, aerial imagery, underwater, indoor scenes, synthetic, close-ups) and task types (semantic segmentation, object detection, depth estimation, keypoint detection). Importantly, these are all complex, structured output tasks types relevant to modern computer vision applications. In total we carry out over 2000 transfer learning experiments, including many where the source and target come from different image domains, task types, or both. We systematically analyze these experiments to understand the impact of image domain, task type, and dataset size on transfer learning performance. Our study leads to several insights and concrete recommendations: (1) for most tasks there exists a source which significantly outperforms ILSVRC'12 pre-training; (2) the image domain is the most important factor for achieving positive transfer; (3) the source dataset should include the image domain of the target dataset to achieve best results; (4) at the same time, we observe only small negative effects when the image domain of the source task is much broader than that of the target; (5) transfer across task types can be beneficial, but its success is heavily dependent on both the source and target task types.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/6WL7FY48/Mensink et al. - 2021 - Factors of Influence for Transfer Learning across .pdf}
}

@article{mialonGraphiTEncodingGraph2021,
  title = {{{GraphiT}}: {{Encoding Graph Structure}} in {{Transformers}}},
  shorttitle = {{{GraphiT}}},
  author = {Mialon, Gr{\'e}goire and Chen, Dexiong and Selosse, Margot and Mairal, Julien},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.05667 [cs]},
  eprint = {2106.05667},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We show that viewing graphs as sets of node features and incorporating structural and positional information into a transformer architecture is able to outperform representations learned with classical graph neural networks (GNNs). Our model, GraphiT, encodes such information by (i) leveraging relative positional encoding strategies in self-attention scores based on positive definite kernels on graphs, and (ii) enumerating and encoding local sub-structures such as paths of short length. We thoroughly evaluate these two ideas on many classification and regression tasks, demonstrating the effectiveness of each of them independently, as well as their combination. In addition to performing well on standard benchmarks, our model also admits natural visualization mechanisms for interpreting graph motifs explaining the predictions, making it a potentially strong candidate for scientific applications where interpretation is important. Code available at https://github.com/inria-thoth/GraphiT.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/J3MUYH4M/Mialon et al. - 2021 - GraphiT Encoding Graph Structure in Transformers.pdf}
}

@article{mirmanDifferentiableAbstractInterpretation,
  title = {Differentiable {{Abstract Interpretation}} for {{Provably Robust Neural Networks}}},
  author = {Mirman, Matthew and Gehr, Timon and Vechev, Martin},
  pages = {13},
  abstract = {We introduce a scalable method for training robust neural networks based on abstract interpretation. We present several abstract transformers which balance efficiency with precision and show these can be used to train large neural networks that are certifiably robust to adversarial perturbations.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/8UVHM8Y3/Mirman et al. - Differentiable Abstract Interpretation for Provabl.pdf}
}

@inproceedings{mishkinAllYouNeed2016,
  title = {All You Need Is a Good Init},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Mishkin, Dmytro and Matas, Jiri},
  year = {2016},
  eprint = {1511.06422},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1511.06422},
  abstract = {Layer-sequential unit-variance (LSUV) initialization - a simple method for weight initialization for deep net learning - is proposed. The method consists of the two steps. First, pre-initialize weights of each convolution or inner-product layer with orthonormal matrices. Second, proceed from the first to the final layer, normalizing the variance of the output of each layer to be equal to one. Experiment with different activation functions (maxout, ReLU-family, tanh) show that the proposed initialization leads to learning of very deep nets that (i) produces networks with test accuracy better or equal to standard methods and (ii) is at least as fast as the complex schemes proposed specifically for very deep nets such as FitNets (Romero et al. (2015)) and Highway (Srivastava et al. (2015)). Performance is evaluated on GoogLeNet, CaffeNet, FitNets and Residual nets and the state-of-the-art, or very close to it, is achieved on the MNIST, CIFAR-10/100 and ImageNet datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/6Z5JSGZF/Mishkin and Matas - 2016 - All you need is a good init.pdf;/Users/konstantinschurholt/Zotero/storage/UU29JR37/1511.html}
}

@article{misraSelfSupervisedLearningPretextInvariant2019,
  title = {Self-{{Supervised Learning}} of {{Pretext-Invariant Representations}}},
  author = {Misra, Ishan and {van der Maaten}, Laurens},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.01991 [cs]},
  eprint = {1912.01991},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The goal of self-supervised learning from images is to construct image representations that are semantically meaningful via pretext tasks that do not require semantic annotations for a large training set of images. Many pretext tasks lead to representations that are covariant with image transformations. We argue that, instead, semantic representations ought to be invariant under such transformations. Specifically, we develop Pretext-Invariant Representation Learning (PIRL, pronounced as ``pearl'') that learns invariant representations based on pretext tasks. We use PIRL with a commonly used pretext task that involves solving jigsaw puzzles. We find that PIRL substantially improves the semantic quality of the learned image representations. Our approach sets a new state-of-the-art in selfsupervised learning from images on several popular benchmarks for self-supervised learning. Despite being unsupervised, PIRL outperforms supervised pre-training in learning image representations for object detection. Altogether, our results demonstrate the potential of self-supervised learning of image representations with good invariance properties.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/T53SMBPZ/Misra and van der Maaten - 2019 - Self-Supervised Learning of Pretext-Invariant Repr.pdf}
}

@inproceedings{miyatoSpectralNormalizationGenerative2018,
  title = {Spectral {{Normalization}} for {{Generative Adversarial Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  year = {2018},
  abstract = {We propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator of GANs.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/TVDMJY3P/Miyato et al. - 2022 - Spectral Normalization for Generative Adversarial .pdf;/Users/konstantinschurholt/Zotero/storage/4HVIQ5Z5/forum.html}
}

@misc{ModelHubAI,
  title = {{{ModelHub}}.{{AI}}},
  howpublished = {https://app.modelhub.ai/},
  file = {/Users/konstantinschurholt/Zotero/storage/ATMLPMI2/app.modelhub.ai.html}
}

@misc{ModelsHuggingFace,
  title = {Models - {{Hugging Face}}},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/models},
  file = {/Users/konstantinschurholt/Zotero/storage/LCFNV7AD/models.html}
}

@inproceedings{molchanovVariationalDropoutSparsifies2017,
  title = {Variational {{Dropout Sparsifies Deep Neural Networks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  year = {2017},
  pages = {10},
  abstract = {We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fullyconnected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/V7VQ8KYX/Molchanov et al. - Variational Dropout Sparsifies Deep Neural Network.pdf}
}

@article{molnarInterpretableMachineLearning2020,
  title = {Interpretable {{Machine Learning}}},
  author = {Molnar, Christoph},
  year = {2020},
  pages = {251},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/TCDJSW7L/Molnar - Interpretable Machine Learning.pdf}
}

@article{morcosInsightsRepresentationalSimilarity2018,
  title = {Insights on Representational Similarity in Neural Networks with Canonical Correlation},
  author = {Morcos, Ari S. and Raghu, Maithra and Bengio, Samy},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.05759 [cs, stat]},
  eprint = {1806.05759},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method [22]. We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, finding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Similarity Measures,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/MDB32MPC/Morcos et al. - 2018 - Insights on representational similarity in neural .pdf}
}

@article{morgadoAudioVisualInstanceDiscrimination2021,
  title = {Audio-{{Visual Instance Discrimination}} with {{Cross-Modal Agreement}}},
  author = {Morgado, Pedro and Vasconcelos, Nuno and Misra, Ishan},
  year = {2021},
  month = mar,
  journal = {arXiv:2004.12943 [cs]},
  eprint = {2004.12943},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present a self-supervised learning approach to learn audio-visual representations from video and audio. Our method uses contrastive learning for cross-modal discrimination of video from audio and vice-versa. We show that optimizing for cross-modal discrimination, rather than withinmodal discrimination, is important to learn good representations from video and audio. With this simple but powerful insight, our method achieves highly competitive performance when finetuned on action recognition tasks. Furthermore, while recent work in contrastive learning defines positive and negative samples as individual instances, we generalize this definition by exploring cross-modal agreement. We group together multiple instances as positives by measuring their similarity in both the video and audio feature spaces. Cross-modal agreement creates better positive and negative sets, which allows us to calibrate visual similarities by seeking within-modal discrimination of positive instances, and achieve significant gains on downstream tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/PNTBS528/Morgado et al. - 2021 - Audio-Visual Instance Discrimination with Cross-Mo.pdf}
}

@article{munkhdalaiMetaNetworks2017,
  title = {Meta {{Networks}}},
  author = {Munkhdalai, Tsendsuren and Yu, Hong},
  year = {2017},
  month = jun,
  journal = {arXiv:1703.00837 [cs, stat]},
  eprint = {1703.00837},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Neural networks have been successfully applied in applications with a large amount of labeled data. However, the task of rapid generalization on new concepts with small training data while preserving performances on previously learned ones still presents a significant challenge to neural network models. In this work, we introduce a novel meta learning method, Meta Networks (MetaNet), that learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. When evaluated on Omniglot and Mini-ImageNet benchmarks, our MetaNet models achieve a near human-level performance and outperform the baseline approaches by up to 6\% accuracy. We demonstrate several appealing properties of MetaNet relating to generalization and continual learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/9PNF7K5X/Munkhdalai and Yu - 2017 - Meta Networks.pdf}
}

@article{murdochAutomaticRuleExtraction2017,
  title = {Automatic {{Rule Extraction}} from {{Long Short Term Memory Networks}}},
  author = {Murdoch, W. James and Szlam, Arthur},
  year = {2017},
  month = feb,
  journal = {arXiv:1702.02540 [cs, stat]},
  eprint = {1702.02540},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Although deep learning models have proven effective at solving problems in natural language processing, the mechanism by which they come to their conclusions is often unclear. As a result, these models are generally treated as black boxes, yielding no insight of the underlying learned patterns. In this paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new approach for tracking the importance of a given input to the LSTM for a given output. By identifying consistently important patterns of words, we are able to distill state of the art LSTMs on sentiment analysis and question answering into a set of representative phrases. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/W8E8ENFF/Murdoch and Szlam - 2017 - Automatic Rule Extraction from Long Short Term Mem.pdf}
}

@article{murdochInterpretableMachineLearning2019,
  title = {Interpretable Machine Learning: Definitions, Methods, and Applications},
  shorttitle = {Interpretable Machine Learning},
  author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and {Abbasi-Asl}, Reza and Yu, Bin},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.04592 [cs, stat]},
  eprint = {1901.04592},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related, and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the Predictive, Descriptive, Relevant (PDR) framework for discussing interpretations. The PDR framework provides three overarching desiderata for evaluation: predictive accuracy, descriptive accuracy and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post-hoc categories, with sub-groups including sparsity, modularity and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often under-appreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/2CVQWGCR/Murdoch et al. - 2019 - Interpretable machine learning definitions, metho.pdf}
}

@misc{nakkiranDeepDoubleDescent2019,
  title = {Deep {{Double Descent}}: {{Where Bigger Models}} and {{More Data Hurt}}},
  shorttitle = {Deep {{Double Descent}}},
  author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  year = {2019},
  month = dec,
  number = {arXiv:1912.02292},
  eprint = {1912.02292},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {We show that a variety of modern deep learning tasks exhibit a ``double-descent'' phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/3X8F6IGV/Nakkiran et al. - 2019 - Deep Double Descent Where Bigger Models and More .pdf}
}

@inproceedings{netzerReadingDigitsNatural2011,
  title = {Reading {{Digits}} in {{Natural Images}} with {{Unsupervised Feature Learning}}},
  booktitle = {{{NIPS Workshop}} on {{Deep Learning}} and {{Unsupervised Feature Learning}} 2011},
  author = {Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year = {2011},
  pages = {9},
  abstract = {Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/82CYQLB7/Netzer et al. - Reading Digits in Natural Images with Unsupervised.pdf}
}

@inproceedings{NEURIPS2021_4f3d7d38,
  title = {Permutation-Invariant Variational Autoencoder for Graph-Level Representation Learning},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Winter, Robin and Noe, Frank and Clevert, Djork-Arn{\'e}},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  year = {2021},
  volume = {34},
  pages = {9559--9573},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/konstantinschurholt/Zotero/storage/QSJCTMG3/Winter et al. - 2021 - Permutation-invariant variational autoencoder for .pdf}
}

@misc{NeurIPSReviewEvaluation,
  title = {{{NeurIPS Review Evaluation Experiment}}},
  howpublished = {https://cmu.ca1.qualtrics.com/jfe/form/SV\_4U5n3fvWNbjqMxE?Q\_CHL=gl\&Q\_DL=hvIxKCHU90b85YV\_4U5n3fvWNbjqMxE\_CGC\_nSi9QiZitTMkRpW},
  file = {/Users/konstantinschurholt/Zotero/storage/CFBC6MMA/SV_4U5n3fvWNbjqMxE.html}
}

@article{neyshaburWhatBeingTransferred2020,
  title = {What Is Being Transferred in Transfer Learning?},
  author = {Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  year = {2020},
  month = aug,
  journal = {arXiv:2008.11687 [cs, stat]},
  eprint = {2008.11687},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {One desired capability for machines is the ability to transfer their knowledge of one domain to another where data is (usually) scarce. Despite ample adaptation of transfer learning in various deep learning applications, we yet do not understand what enables a successful transfer and which part of the network is responsible for that. In this paper, we provide new tools and analyses to address these fundamental questions. Through a series of analyses on transferring to block-shuffled images, we separate the effect of feature reuse from learning low-level statistics of data and show that some benefit of transfer learning comes from the latter. We present that when training from pre-trained weights, the model stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/AKGX3AEJ/Neyshabur et al. - 2020 - What is being transferred in transfer learning.pdf}
}

@article{nguyenHyperVAEMinimumDescription2019,
  title = {{{HyperVAE}}: {{A Minimum Description Length Variational Hyper-Encoding Network}}},
  author = {Nguyen, Phuoc and Tran, Truyen and Gupta, Sunil and Rana, Santu and Dam, Hieu-Chi},
  year = {2019},
  pages = {7},
  abstract = {We propose a framework called HyperVAE for encoding distributions of distributions. When a target distribution is modeled by a VAE, its neural network parameters \texttheta{} is drawn from a distribution p(\texttheta ) which is modeled by a hyper-level VAE. We propose a variational inference using Gaussian mixture models to implicitly encode the parameters \texttheta{} into a low dimensional Gaussian distribution. Given a target distribution, we predict the posterior distribution of the latent code, then use a matrix-network decoder to generate a posterior distribution q(\texttheta ). HyperVAE can encode the parameters \texttheta{} in full in contrast to common hyper-networks practices, which generate only the scale and bias vectors as target-network parameters. Thus HyperVAE preserves much more information about the model for each task in the latent space. We discuss HyperVAE using the minimum description length (MDL) principle and show that it helps HyperVAE to generalize. We evaluate HyperVAE in density estimation tasks, outlier detection and discovery of novel design classes, demonstrating its efficacy.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/SN8U996W/Nguyen et al. - HyperVAE A Minimum Description Length Variational.pdf}
}

@article{nguyenLEEPNewMeasure,
  title = {{{LEEP}}: {{A New Measure}} to {{Evaluate Transferability}} of {{Learned Representations}}},
  author = {Nguyen, Cuong V and Hassner, Tal and Seeger, Matthias and Archambeau, Cedric},
  pages = {12},
  abstract = {We introduce a new measure to evaluate the transferability of representations learned by classifiers. Our measure, the Log Expected Empirical Prediction (LEEP), is simple and easy to compute: when given a classifier trained on a source data set, it only requires running the target data set through this classifier once. We analyze the properties of LEEP theoretically and demonstrate its effectiveness empirically. Our analysis shows that LEEP can predict the performance and convergence speed of both transfer and meta-transfer learning methods, even for small or imbalanced data. Moreover, LEEP outperforms recently proposed transferability measures such as negative conditional entropy and H scores. Notably, when transferring from ImageNet to CIFAR100, LEEP can achieve up to 30\% improvement compared to the best competing method in terms of the correlations with actual transfer accuracy.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/SUTKEF3D/Nguyen et al. - LEEP A New Measure to Evaluate Transferability of.pdf}
}

@article{nguyenWideDeepNetworks2020,
  title = {Do {{Wide}} and {{Deep Networks Learn}} the {{Same Things}}? {{Uncovering How Neural Network Representations Vary}} with {{Width}} and {{Depth}}},
  shorttitle = {Do {{Wide}} and {{Deep Networks Learn}} the {{Same Things}}?},
  author = {Nguyen, Thao and Raghu, Maithra and Kornblith, Simon},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.15327 [cs]},
  eprint = {2010.15327},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {A key factor in the success of deep neural networks is the ability to scale models to improve performance by varying the architecture depth and width. This simple property of neural network design has resulted in highly effective architectures for a variety of tasks. Nevertheless, there is limited understanding of effects of depth and width on the learned representations. In this paper, we study this fundamental question. We begin by investigating how varying depth and width affects model hidden representations, finding a characteristic block structure in the hidden representations of larger capacity (wider or deeper) models. We demonstrate that this block structure arises when model capacity is large relative to the size of the training set, and is indicative of the underlying layers preserving and propagating the dominant principal component of their representations. This discovery has important ramifications for features learned by different models, namely, representations outside the block structure are often similar across architectures with varying widths and depths, but the block structure is unique to each model. We analyze the output predictions of different model architectures, finding that even when the overall accuracy is similar, wide and deep models exhibit distinctive error patterns and variations across classes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/HBQ5IZUD/Nguyen et al. - 2020 - Do Wide and Deep Networks Learn the Same Things U.pdf}
}

@article{norooziUnsupervisedLearningVisual2017,
  title = {Unsupervised {{Learning}} of {{Visual Representations}} by {{Solving Jigsaw Puzzles}}},
  author = {Noroozi, Mehdi and Favaro, Paolo},
  year = {2017},
  month = aug,
  journal = {arXiv:1603.09246 [cs]},
  eprint = {1603.09246},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper we study the problem of image representation learning without human annotation. By following the principles of selfsupervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN includes fewer parameters than AlexNet while preserving the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their correct spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. Our proposed method for learning visual representations outperforms state of the art methods in several transfer learning benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/EDSMHMLB/Noroozi and Favaro - 2017 - Unsupervised Learning of Visual Representations by.pdf}
}

@article{oordNeuralDiscreteRepresentation2018,
  title = {Neural {{Discrete Representation Learning}}},
  author = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
  year = {2018},
  month = may,
  journal = {arXiv:1711.00937 [cs]},
  eprint = {1711.00937},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector QuantisedVariational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of ``posterior collapse'' -\textemdash{} where the latents are ignored when they are paired with a powerful autoregressive decoder -\textemdash{} typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/7MUJBXKA/Oord et al. - 2018 - Neural Discrete Representation Learning.pdf}
}

@article{oordRepresentationLearningContrastive2019,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2019},
  month = jan,
  journal = {arXiv:1807.03748 [cs, stat]},
  eprint = {1807.03748},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/96X5T7HE/Oord et al. - 2019 - Representation Learning with Contrastive Predictiv.pdf}
}

@inproceedings{parkUnsupervisedRepresentationLearning2021,
  title = {Unsupervised {{Representation Learning}} via {{Neural Activation Coding}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Park, Yookoon and Lee, Sangho and Kim, Gunhee and Blei, David},
  year = {2021},
  month = jul,
  pages = {8391--8400},
  publisher = {{PMLR}},
  abstract = {We present neural activation coding (NAC) as a novel approach for learning deep representations from unlabeled data for downstream applications. We argue that the deep encoder should maximize its n...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/C23XIXCS/Park et al. - 2021 - Unsupervised Representation Learning via Neural Activation Coding.pdf}
}

@inproceedings{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  year = {2019},
  pages = {12},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/3SV677XC/Paszke et al. - PyTorch An Imperative Style, High-Performance Dee.pdf}
}

@misc{PathAutonomousMachine,
  title = {A {{Path Towards Autonomous Machine Intelligence}}},
  abstract = {How could machines learn as efficiently as humans and animals?  How could machines learn to reason and plan?  How could machines learn representations of percepts and action plans at multiple...},
  howpublished = {https://openreview.net/forum?id=BZ5a1r-kVsf}
}

@misc{peeblesLearningLearnGenerative2022,
  title = {Learning to {{Learn}} with {{Generative Models}} of {{Neural Network Checkpoints}}},
  author = {Peebles, William and Radosavovic, Ilija and Brooks, Tim and Efros, Alexei A. and Malik, Jitendra},
  year = {2022},
  month = sep,
  number = {arXiv:2209.12892},
  eprint = {2209.12892},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {We explore a data-driven approach for learning to optimize neural networks. We construct a dataset of neural network checkpoints and train a generative model on the parameters. In particular, our model is a conditional diffusion transformer that, given an initial input parameter vector and a prompted loss, error, or return, predicts the distribution over parameter updates that achieve the desired metric. At test time, it can optimize neural networks with unseen parameters for downstream tasks in just one update. We find that our approach successfully generates parameters for a wide range of loss prompts. Moreover, it can sample multimodal parameter solutions and has favorable scaling properties. We apply our method to different neural network architectures and tasks in supervised and reinforcement learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/6US7UPU4/Peebles et al. - 2022 - Learning to Learn with Generative Models of Neural.pdf}
}

@inproceedings{poklukarGeomCAGeometricEvaluation2021,
  title = {{{GeomCA}}: {{Geometric Evaluation}} of {{Data Representations}}},
  shorttitle = {{{GeomCA}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Poklukar, Petra and Varava, Anastasiia and Kragic, Danica},
  year = {2021},
  month = jul,
  pages = {8588--8598},
  publisher = {{PMLR}},
  abstract = {Evaluating the quality of learned representations without relying on a downstream task remains one of the challenges in representation learning. In this work, we present Geometric Component Analysi...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/JC54SJ3S/Poklukar et al. - 2021 - GeomCA Geometric Evaluation of Data Representations.pdf}
}

@inproceedings{preechakulDiffusionAutoencodersMeaningful2022,
  title = {Diffusion {{Autoencoders}}: {{Toward}} a {{Meaningful}} and {{Decodable Representation}}},
  shorttitle = {Diffusion {{Autoencoders}}},
  booktitle = {Conference on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Preechakul, Konpat and Chatthee, Nattanat and Wizadwongsa, Suttisak and Suwajanakorn, Supasorn},
  year = {2022},
  abstract = {Diffusion probabilistic models (DPMs) have achieved remarkable quality in image generation that rivals GANs'. But unlike GANs, DPMs use a set of latent variables that lack semantic meaning and cannot serve as a useful representation for other tasks. This paper explores the possibility of using DPMs for representation learning and seeks to extract a meaningful and decodable representation of an input image via autoencoding. Our key idea is to use a learnable encoder for discovering the high-level semantics, and a DPM as the decoder for modeling the remaining stochastic variations. Our method can encode any image into a two-part latent code, where the first part is semantically meaningful and linear, and the second part captures stochastic details, allowing near-exact reconstruction. This capability enables challenging applications that currently foil GAN-based methods, such as attribute manipulation on real images. We also show that this two-level encoding improves denoising efficiency and naturally facilitates various downstream tasks including few-shot conditional sampling. Please visit our project page: https://Diff-AE.github.io/},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/A5GTCR9W/Preechakul et al. - 2022 - Diffusion Autoencoders Toward a Meaningful and De.pdf}
}

@article{qianSpatiotemporalContrastiveVideo2021,
  title = {Spatiotemporal {{Contrastive Video Representation Learning}}},
  author = {Qian, Rui and Meng, Tianjian and Gong, Boqing and Yang, Ming-Hsuan and Wang, Huisheng and Belongie, Serge and Cui, Yin},
  year = {2021},
  month = apr,
  journal = {arXiv:2008.03800 [cs]},
  eprint = {2008.03800},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present a self-supervised Contrastive Video Representation Learning (CVRL) method to learn spatiotemporal visual representations from unlabeled videos. Our representations are learned using a contrastive loss, where two augmented clips from the same short video are pulled together in the embedding space, while clips from different videos are pushed away. We study what makes for good data augmentations for video self-supervised learning and find that both spatial and temporal information are crucial. We carefully design data augmentations involving spatial and temporal cues. Concretely, we propose a temporally consistent spatial augmentation method to impose strong spatial augmentations on each frame of the video while maintaining the temporal consistency across frames. We also propose a sampling-based temporal augmentation method to avoid overly enforcing invariance on clips that are distant in time. On Kinetics-600, a linear classifier trained on the representations learned by CVRL achieves 70.4\% top-1 accuracy with a 3D-ResNet-50 (R3D-50) backbone, outperforming ImageNet supervised pre-training by 15.7\% and SimCLR unsupervised pre-training by 18.8\% using the same inflated R3D-50. The performance of CVRL can be further improved to 72.9\% with a larger R3D-152 (2\texttimes{} filters) backbone, significantly closing the gap between unsupervised and supervised video representation learning. Our code and models will be available at https://github.com/tensorflow/models/tree/master/official/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/NAEWJIVF/Qian et al. - 2021 - Spatiotemporal Contrastive Video Representation Le.pdf}
}

@article{qiaoMicroBatchTrainingBatchChannel2020,
  title = {Micro-{{Batch Training}} with {{Batch-Channel Normalization}} and {{Weight Standardization}}},
  author = {Qiao, Siyuan and Wang, Huiyu and Liu, Chenxi and Shen, Wei and Yuille, Alan},
  year = {2020},
  month = aug,
  journal = {arXiv:1903.10520 [cs]},
  eprint = {1903.10520},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Batch Normalization (BN) has become an out-of-box technique to improve deep network training. However, its effectiveness is limited for micro-batch training, i.e., each GPU typically has only 1-2 images for training, which is inevitable for many computer vision tasks, e.g., object detection and semantic segmentation, constrained by memory consumption. To address this issue, we propose Weight Standardization (WS) and Batch-Channel Normalization (BCN) to bring two success factors of BN into micro-batch training: 1) the smoothing effects on the loss landscape and 2) the ability to avoid harmful elimination singularities along the training trajectory. WS standardizes the weights in convolutional layers to smooth the loss landscape by reducing the Lipschitz constants of the loss and the gradients; BCN combines batch and channel normalizations and leverages estimated statistics of the activations in convolutional layers to keep networks away from elimination singularities. We validate WS and BCN on comprehensive computer vision tasks, including image classification, object detection, instance segmentation, video recognition and semantic segmentation. All experimental results consistently show that WS and BCN improve micro-batch training significantly. Moreover, using WS and BCN with micro-batch training is even able to match or outperform the performances of BN with large-batch training.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/YHVNL67T/Qiao et al. - 2020 - Micro-Batch Training with Batch-Channel Normalizat.pdf}
}

@inproceedings{qiuNeuralTransformationLearning2021,
  title = {Neural {{Transformation Learning}} for {{Deep Anomaly Detection Beyond Images}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Qiu, Chen and Pfrommer, Timo and Kloft, Marius and Mandt, Stephan and Rudolph, Maja},
  year = {2021},
  month = jul,
  pages = {8703--8714},
  publisher = {{PMLR}},
  abstract = {Data transformations (e.g. rotations, reflections, and cropping) play an important role in self-supervised learning. Typically, images are transformed into different views, and neural networks trai...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/YJ6DS3NC/Qiu et al. - 2021 - Neural Transformation Learning for Deep Anomaly Detection Beyond Images.pdf}
}

@article{radenovicDeepShapeMatching2017,
  title = {Deep {{Shape Matching}}},
  author = {Radenovi{\'c}, Filip and Tolias, Giorgos and Chum, Ond{\v r}ej},
  year = {2017},
  month = sep,
  journal = {arXiv:1709.03409 [cs]},
  eprint = {1709.03409},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We cast shape matching as metric learning with convolutional networks. We break the end-to-end process of image representation into two parts. Firstly, well established efficient methods are chosen to turn the images into edge maps. Secondly, the network is trained with edge maps of landmark images, which are automatically obtained by a structure-from-motion pipeline. The learned representation is evaluated on a range of different tasks, providing improvements on challenging cases of domain generalization, generic sketch-based image retrieval or its fine-grained counterpart. In contrast to other methods that learn a different model per task, object category, or domain, we use the same network throughout all our experiments, achieving state-of-the-art results in multiple benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/LIIDJATG/Radenoviƒá et al. - 2017 - Deep Shape Matching.pdf}
}

@article{raghuSVCCASingularVector2017,
  title = {{{SVCCA}}: {{Singular Vector Canonical Correlation Analysis}} for {{Deep Learning Dynamics}} and {{Interpretability}}},
  shorttitle = {{{SVCCA}}},
  author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and {Sohl-Dickstein}, Jascha},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.05806 [cs, stat]},
  eprint = {1706.05806},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/9W58SD7J/Raghu et al. - 2017 - SVCCA Singular Vector Canonical Correlation Analy.pdf}
}

@misc{rameshHierarchicalTextConditionalImage2022,
  title = {Hierarchical {{Text-Conditional Image Generation}} with {{CLIP Latents}}},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  year = {2022},
  month = apr,
  number = {arXiv:2204.06125},
  eprint = {2204.06125},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/4M5YSK22/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf}
}

@inproceedings{rameshModelZooGrowing2022,
  title = {Model {{Zoo}}: {{A Growing}} "{{Brain}}" {{That Learns Continually}}},
  shorttitle = {Model {{Zoo}}},
  booktitle = {International {{Conference}} on {{Learning Representations ICLR}}},
  author = {Ramesh, Rahul and Chaudhari, Pratik},
  year = {2022},
  eprint = {2106.03027},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper argues that continual learning methods can benefit by splitting the capacity of the learner across multiple models. We use statistical learning theory and experimental analysis to show how multiple tasks can interact with each other in a non-trivial fashion when a single model is trained on them. The generalization error on a particular task can improve when it is trained with synergistic tasks, but can also deteriorate when trained with competing tasks. This theory motivates our method named Model Zoo which, inspired from the boosting literature, grows an ensemble of small models, each of which is trained during one episode of continual learning. We demonstrate that Model Zoo obtains large gains in accuracy on a variety of continual learning benchmark problems. Code is available at https://github.com/grasp-lyrl/modelzoo\_continual.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/UAE6YAJ7/Ramesh and Chaudhari - 2022 - Model Zoo A Growing Brain That Learns Continual.pdf}
}

@inproceedings{ratzlaffHyperGANGenerativeModel2019,
  title = {{{HyperGAN}}: {{A Generative Model}} for {{Diverse}}, {{Performant Neural Networks}}},
  shorttitle = {{{HyperGAN}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Ratzlaff, Neale and Fuxin, Li},
  year = {2019},
  month = may,
  pages = {5361--5369},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We introduce HyperGAN, a generative model that learns to generate all the parameters of a deep neural network. HyperGAN first transforms low dimensional noise into a latent space, which can be sampled from to obtain diverse, performant sets of parameters for a target architecture. We utilize an architecture that bears resemblance to generative adversarial networks, but we evaluate the likelihood of generated samples with a classification loss. This is equivalent to minimizing the KL-divergence between the distribution of generated parameters, and the unknown true parameter distribution. We apply HyperGAN to classification, showing that HyperGAN can learn to generate parameters which solve the MNIST and CIFAR-10 datasets with competitive performance to fully supervised learning, while also generating a rich distribution of effective parameters. We also show that HyperGAN can also provide better uncertainty estimates than standard ensembles. This is evidenced by the ability of HyperGAN-generated ensembles to detect out of distribution data as well as adversarial examples.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/342JPYDT/Ratzlaff and Fuxin - 2019 - HyperGAN A Generative Model for Diverse, Performa.pdf;/Users/konstantinschurholt/Zotero/storage/F9TR63MR/Ratzlaff and Fuxin - 2019 - HyperGAN A Generative Model for Diverse, Performa.pdf}
}

@article{rechtImageNetClassifiersGeneralize,
  title = {Do {{ImageNet Classifiers Generalize}} to {{ImageNet}}?},
  author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  pages = {78},
  abstract = {We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used test sets. By closely following the original dataset creation processes, we test to what extent current classification models generalize to new data. We evaluate a broad range of models and find accuracy drops of 3\% \textendash{} 15\% on CIFAR-10 and 11\% \textendash{} 14\% on ImageNet. However, accuracy gains on the original test sets translate to larger gains on the new test sets. Our results suggest that the accuracy drops are not caused by adaptivity, but by the models' inability to generalize to slightly ``harder'' images than those found in the original test sets.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/L23F46YG/Recht et al. - Do ImageNet ClassiÔ¨Åers Generalize to ImageNet.pdf}
}

@article{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = feb,
  journal = {arXiv:1602.04938 [cs, stat]},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/TYU4DKFS/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@article{rizveExploringComplementaryStrengths2021,
  title = {Exploring {{Complementary Strengths}} of {{Invariant}} and {{Equivariant Representations}} for {{Few-Shot Learning}}},
  author = {Rizve, Mamshad Nayeem and Khan, Salman and Khan, Fahad Shahbaz and Shah, Mubarak},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.01315 [cs]},
  eprint = {2103.01315},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In many real-world problems, collecting a large number of labeled samples is infeasible. Few-shot learning (FSL) is the dominant approach to address this issue, where the objective is to quickly adapt to novel categories in presence of a limited number of samples. FSL tasks have been predominantly solved by leveraging the ideas from gradient-based meta-learning and metric learning approaches. However, recent works have demonstrated the significance of powerful feature representations with a simple embedding network that can outperform existing sophisticated FSL algorithms. In this work, we build on this insight and propose a novel training mechanism that simultaneously enforces equivariance and invariance to a general set of geometric transformations. Equivariance or invariance has been employed standalone in the previous works; however, to the best of our knowledge, they have not been used jointly. Simultaneous optimization for both of these contrasting objectives allows the model to jointly learn features that are not only independent of the input transformation but also the features that encode the structure of geometric transformations. These complementary sets of features help generalize well to novel classes with only a few data samples. We achieve additional improvements by incorporating a novel self-supervised distillation objective. Our extensive experimentation shows that even without knowledge distillation our proposed method can outperform current state-of-theart FSL methods on five popular benchmark datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/XXVWHL4I/Rizve et al. - 2021 - Exploring Complementary Strengths of Invariant and.pdf}
}

@article{rohrbachGroundingTextualPhrases2016,
  title = {Grounding of {{Textual Phrases}} in {{Images}} by {{Reconstruction}}},
  author = {Rohrbach, Anna and Rohrbach, Marcus and Hu, Ronghang and Darrell, Trevor and Schiele, Bernt},
  year = {2016},
  journal = {arXiv:1511.03745 [cs]},
  volume = {9905},
  eprint = {1511.03745},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {817--834},
  doi = {10.1007/978-3-319-46448-0_49},
  abstract = {Grounding (i.e. localizing) arbitrary, free-form textual phrases in visual content is a challenging problem with many applications for human-computer interaction and image-text reference resolution. Few datasets provide the ground truth spatial localization of phrases, thus it is desirable to learn from data with no or little grounding supervision. We propose a novel approach which learns grounding by reconstructing a given phrase using an attention mechanism, which can be either latent or optimized directly. During training our approach encodes the phrase using a recurrent network language model and then learns to attend to the relevant image region in order to reconstruct the input phrase. At test time, the correct attention, i.e., the grounding, is evaluated. If grounding supervision is available it can be directly applied via a loss over the attention mechanism. We demonstrate the effectiveness of our approach on the Flickr 30k Entities [35] and ReferItGame [26] datasets with different levels of supervision, ranging from no supervision over partial supervision to full supervision. Our supervised variant improves by a large margin over the state-of-the-art on both datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/2JQXDFAE/Rohrbach et al. - 2016 - Grounding of Textual Phrases in Images by Reconstr.pdf}
}

@inproceedings{rolfRepresentationMattersAssessing2021a,
  title = {Representation {{Matters}}: {{Assessing}} the {{Importance}} of {{Subgroup Allocations}} in {{Training Data}}},
  shorttitle = {Representation {{Matters}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Rolf, Esther and Worledge, Theodora T. and Recht, Benjamin and Jordan, Michael},
  year = {2021},
  month = jul,
  pages = {9040--9051},
  publisher = {{PMLR}},
  abstract = {Collecting more diverse and representative training data is often touted as a remedy for the disparate performance of machine learning predictors across subpopulations. However, a precise framework...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/GLVJEZR9/Rolf et al. - 2021 - Representation Matters Assessing the Importance of Subgroup Allocations in Training Data.pdf}
}

@inproceedings{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  booktitle = {Conference on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/FD4BKR98/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf}
}

@inproceedings{rossBenchmarksAlgorithmsMetrics2021,
  title = {Benchmarks, {{Algorithms}}, and {{Metrics}} for {{Hierarchical Disentanglement}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Ross, Andrew and {Doshi-Velez}, Finale},
  year = {2021},
  month = jul,
  pages = {9084--9094},
  publisher = {{PMLR}},
  abstract = {In representation learning, there has been recent interest in developing algorithms to disentangle the ground-truth generative factors behind a dataset, and metrics to quantify how fully this occur...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/AKV4KBDC/Ross and Doshi-Velez - 2021 - Benchmarks, Algorithms, and Metrics for Hierarchical Disentanglement.pdf}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year = {1986},
  journal = {nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {{Nature Publishing Group}}
}

@article{salimansWeightNormalizationSimple2016,
  title = {Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  shorttitle = {Weight {{Normalization}}},
  author = {Salimans, Tim and Kingma, Diederik P.},
  year = {2016},
  month = jun,
  journal = {arXiv:1602.07868 [cs]},
  eprint = {1602.07868},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/konstantinschurholt/Zotero/storage/Q592PDP6/Salimans and Kingma - 2016 - Weight Normalization A Simple Reparameterization .pdf}
}

@article{samekExplainableArtificialIntelligence2019,
  title = {Towards {{Explainable Artificial Intelligence}}},
  author = {Samek, Wojciech and M{\"u}ller, Klaus-Robert},
  year = {2019},
  journal = {arXiv:1909.12072 [cs]},
  volume = {11700},
  eprint = {1909.12072},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {5--22},
  doi = {10.1007/978-3-030-28954-6_1},
  abstract = {In recent years, machine learning (ML) has become a key enabling technology for the sciences and industry. Especially through improvements in methodology, the availability of large databases and increased computational power, today's ML algorithms are able to achieve excellent performance (at times even exceeding the human level) on an increasing number of complex tasks. Deep learning models are at the forefront of this development. However, due to their nested non-linear structure, these powerful models have been generally considered ``black boxes'', not providing any information about what exactly makes them arrive at their predictions. Since in many applications, e.g., in the medical domain, such lack of transparency may be not acceptable, the development of methods for visualizing, explaining and interpreting deep learning models has recently attracted increasing attention. This introductory paper presents recent developments and applications in this field and makes a plea for a wider use of explainable learning algorithms in practice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/konstantinschurholt/Zotero/storage/R6UUJ92G/Samek and M√ºller - 2019 - Towards Explainable Artificial Intelligence.pdf}
}

@article{saphraUnderstandingLearningDynamics2018,
  title = {Understanding {{Learning Dynamics Of Language Models}} with {{SVCCA}}},
  author = {Saphra, Naomi and Lopez, Adam},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.00225 [cs]},
  eprint = {1811.00225},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Research has shown that neural models implicitly encode linguistic features, but there has been no research showing how these encodings arise as the models are trained. We present the first study on the learning dynamics of neural language models, using a simple and flexible analysis method called Singular Vector Canonical Correlation Analysis (SVCCA), which enables us to compare learned representations across time and across models, without the need to evaluate directly on annotated data. We probe the evolution of syntactic, semantic, and topic representations and find that part-of-speech is learned earlier than topic; that recurrent layers become more similar to those of a tagger during training; and embedding layers less similar. Our results and methods could inform better learning algorithms for NLP models, possibly to incorporate linguistic information more effectively.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,Similarity Measures},
  file = {/Users/konstantinschurholt/Zotero/storage/P65W397J/Saphra and Lopez - 2018 - Understanding Learning Dynamics Of Language Models.pdf}
}

@inproceedings{schurholtHyperRepresentationsGenerativeModels2022,
  title = {Hyper-{{Representations}} as {{Generative Models}}: {{Sampling Unseen Neural Network Weights}}},
  booktitle = {Thirty-Sixth {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Sch{\"u}rholt, Konstantin and Knyazev, Boris and {Gir{\'o}-i-Nieto}, Xavier and Borth, Damian},
  year = {2022},
  month = sep,
  abstract = {We extend hyper-representations for generative use to sample neural network weights for initialization, ensembling and transfer learning.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/MPXWHMK2/Anonymous - 2022 - Sampling from Hyper-Representations to Generate Ne.pdf;/Users/konstantinschurholt/Zotero/storage/QLM7XFS7/forum.html}
}

@inproceedings{schurholtHyperRepresentationsPreTrainingTransfer2022,
  title = {Hyper-{{Representations}} for {{Pre-Training}} and {{Transfer Learning}}},
  booktitle = {First {{Workshop}} of {{Pre-training}}: {{Perspectives}}, {{Pitfalls}}, and {{Paths Forward}} at {{ICML}} 2022},
  author = {Sch{\"u}rholt, Konstantin and Knyazev, Boris and {Gir{\'o}-i-Nieto}, Xavier and Borth, Damian},
  year = {2022},
  abstract = {Learning representations of neural network weights given a model zoo is an emerging and challenging area with many potential applications from model inspection, to neural architecture search or knowledge distillation. Recently, an autoencoder trained on a model zoo was able to learn a hyper-representation, which captures intrinsic and extrinsic properties of the models in the zoo. In this work, we extend hyperrepresentations for generative use to sample new model weights as pre-training. We propose layerwise loss normalization which we demonstrate is key to generate high-performing models and a sampling method based on the empirical density of hyper-representations. The models generated using our methods are diverse, performant and capable to outperform conventional baselines for transfer learning. Our results indicate the potential of knowledge aggregation from model zoos to new models via hyper-representations thereby paving the avenue for novel research directions.},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/NJ6XYW7U/Sch√ºrholt et al. - 2022 - Hyper-Representations for Pre-Training and Transfe.pdf}
}

@misc{schurholtInvestigationWeightSpace2021,
  title = {An {{Investigation}} of the {{Weight Space}} to {{Monitor}} the {{Training Progress}} of {{Neural Networks}}},
  author = {Sch{\"u}rholt, Konstantin and Borth, Damian},
  year = {2021},
  month = mar,
  number = {arXiv:2006.10424},
  eprint = {2006.10424},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2006.10424},
  abstract = {Safe use of Deep Neural Networks (DNNs) requires careful testing. However, deployed models are often trained further to improve in performance. As rigorous testing and evaluation is expensive, triggers are in need to determine the degree of change of a model. In this paper we investigate the weight space of DNN models for structure that can be exploited to that end. Our results show that DNN models evolve on unique, smooth trajectories in weight space which can be used to track DNN training progress. We hypothesize that curvature and smoothness of the trajectories as well as step length along it may contain information on the state of training as well as potential domain shifts. We show that the model trajectories can be separated and the order of checkpoints on the trajectories recovered, which may serve as a first step towards DNN model versioning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/SLRE9XKX/Sch√ºrholt and Borth - 2021 - An Investigation of the Weight Space to Monitor th.pdf;/Users/konstantinschurholt/Zotero/storage/LN4S9IUA/2006.html}
}

@inproceedings{schurholtModelZoosDataset2022,
  title = {Model {{Zoos}}: {{A Dataset}} of {{Diverse Populations}} of {{Neural Network Models}}},
  shorttitle = {Model {{Zoo}}},
  booktitle = {Thirty-Sixth {{Conference}} on {{Neural Information Processing Systems}} ({{NeurIPS}}) {{Datasets}} and {{Benchmarks Track}}},
  author = {Sch{\"u}rholt, Konstantin and Taskiran, Diyar and Knyazev, Boris and {Gir{\'o}-i-Nieto}, Xavier and Borth, Damian},
  year = {2022},
  month = sep,
  abstract = {To enable the investigation of populations of neural network models, we release a novel dataset of diverse model zoos with this work.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/36MU3LKY/Sch√ºrholt et al. - 2022 - Model Zoo A Dataset of Diverse Populations of Neu.pdf;/Users/konstantinschurholt/Zotero/storage/LH9QPBL8/forum.html}
}

@inproceedings{schurholtSelfSupervisedRepresentationLearning2021,
  title = {Self-{{Supervised Representation Learning}} on {{Neural Network Weights}} for {{Model Characteristic Prediction}}},
  booktitle = {Conference on {{Neural Information Processing Systems}} ({{NeurIPS}})},
  author = {Sch{\"u}rholt, Konstantin and Kostadinov, Dimche and Borth, Damian},
  year = {2021},
  volume = {35},
  abstract = {Self-Supervised Learning (SSL) has been shown to learn useful and informationpreserving representations. Neural Networks (NNs) are widely applied, yet their weight space is still not fully understood. Therefore, we propose to use SSL to learn hyper-representations of the weights of populations of NNs. To that end, we introduce domain specific data augmentations and an adapted attention architecture. Our empirical evaluation demonstrates that self-supervised representation learning in this domain is able to recover diverse NN model characteristics. Further, we show that the proposed learned representations outperform prior work for predicting hyper-parameters, test accuracy, and generalization gap as well as transfer to out-of-distribution settings. Code and datasets are publicly available1.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/A72VKD2P/Sch√ºrholt et al. - Self-Supervised Representation Learning on Neural .pdf}
}

@article{schwarzerDataEfficientReinforcementLearning2021,
  title = {Data-{{Efficient Reinforcement Learning}} with {{Self-Predictive Representations}}},
  author = {Schwarzer, Max and Anand, Ankesh and Goel, Rishab and Hjelm, R. Devon and Courville, Aaron and Bachman, Philip},
  year = {2021},
  month = may,
  journal = {ICLR},
  eprint = {2007.05929},
  eprinttype = {arxiv},
  abstract = {While deep reinforcement learning excels at solving tasks where large amounts of data can be collected through virtually unlimited interaction with the environment, learning from limited interaction remains a key challenge. We posit that an agent can learn more efficiently if we augment reward maximization with self-supervised objectives based on structure in its visual input and sequential interaction with the environment. Our method, Self-Predictive Representations (SPR), trains an agent to predict its own latent state representations multiple steps into the future. We compute target representations for future states using an encoder which is an exponential moving average of the agent's parameters and we make predictions using a learned transition model. On its own, this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. We further improve performance by adding data augmentation to the future prediction loss, which forces the agent's representations to be consistent across multiple views of an observation. Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.415 on Atari in a setting limited to 100k steps of environment interaction, which represents a 55\% relative improvement over the previous state-of-the-art. Notably, even in this limited data regime, SPR exceeds expert human scores on 7 out of 26 games. We've made the code associated with this work available at https://github.com/mila-iqia/spr.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/XE9G39I3/Schwarzer et al. - 2021 - Data-Efficient Reinforcement Learning with Self-Pr.pdf}
}

@article{selvarajuGradCAMVisualExplanations2016,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2016},
  month = oct,
  journal = {arXiv:1610.02391 [cs]},
  eprint = {1610.02391},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose a technique for producing `visual explanations' for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach \textendash{} Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for `dog' or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. VQA) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a `stronger' deep network from a `weaker' one. Our code is available at https://github.com/ramprs/grad-cam/ and a demo is available on CloudCV [2]1. Video of the demo can be found at youtu.be/COjUB9Izk6E.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/Z6D54QGC/Selvaraju et al. - 2016 - Grad-CAM Visual Explanations from Deep Networks v.pdf}
}

@inproceedings{shamsianPersonalizedFederatedLearning2021,
  title = {Personalized {{Federated Learning}} Using {{Hypernetworks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Shamsian, Aviv and Navon, Aviv and Fetaya, Ethan and Chechik, Gal},
  year = {2021},
  month = jul,
  pages = {9489--9502},
  publisher = {{PMLR}},
  abstract = {Personalized federated learning is tasked with training machine learning models for multiple clients, each with its own data distribution. The goal is to train personalized models collaboratively w...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/QLEB9DU6/Shamsian et al. - 2021 - Personalized Federated Learning using Hypernetworks.pdf}
}

@article{shawSelfAttentionRelativePosition2018,
  title = {Self-{{Attention}} with {{Relative Position Representations}}},
  author = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  year = {2018},
  month = apr,
  journal = {arXiv:1803.02155 [cs]},
  eprint = {1803.02155},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graphlabeled inputs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/konstantinschurholt/Zotero/storage/3C67CWSK/Shaw et al. - 2018 - Self-Attention with Relative Position Representati.pdf}
}

@article{shrikumarLearningImportantFeatures2017,
  title = {Learning {{Important Features Through Propagating Activation Differences}}},
  author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  year = {2017},
  month = apr,
  journal = {arXiv:1704.02685 [cs]},
  eprint = {1704.02685},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The purported ``black box'' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its `reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at http://goo.gl/qKb7pL and code is at http://goo.gl/RM8jvH.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/konstantinschurholt/Zotero/storage/HSRCD3NZ/Shrikumar et al. - 2017 - Learning Important Features Through Propagating Ac.pdf}
}

@inproceedings{shuZooTuningAdaptiveTransfer2021,
  title = {Zoo-{{Tuning}}: {{Adaptive Transfer}} from a {{Zoo}} of {{Models}}},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Shu, Yang and Kou, Zhi and Cao, Zhangjie and Wang, Jianmin and Long, Mingsheng},
  year = {2021},
  pages = {12},
  abstract = {With the development of deep networks on various large-scale datasets, a large zoo of pretrained models are available. When transferring from a model zoo, applying classic single-model-based transfer learning methods to each source model suffers from high computational cost and cannot fully utilize the rich knowledge in the zoo. We propose Zoo-Tuning to address these challenges, which learns to adaptively transfer the parameters of pretrained models to the target task. With the learnable channel alignment layer and adaptive aggregation layer, Zoo-Tuning adaptively aggregates channel aligned pretrained parameters to derive the target model, which simultaneously promotes knowledge transfer and adapts source models to downstream tasks. The adaptive aggregation substantially reduces the computation cost at both training and inference. We further propose lite Zoo-Tuning with the temporal ensemble of batch average gating values to reduce the storage cost at the inference time. We evaluate our approach on a variety of tasks, including reinforcement learning, image classification, and facial landmark detection. Experiment results demonstrate that the proposed adaptive transfer learning approach can more effectively and efficiently transfer knowledge from a zoo of models.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/IYP329TT/Shu et al. - Zoo-Tuning Adaptive Transfer from a Zoo of Models.pdf}
}

@article{simonovskyGraphVAEGenerationSmall2018,
  title = {{{GraphVAE}}: {{Towards Generation}} of {{Small Graphs Using Variational Autoencoders}}},
  shorttitle = {{{GraphVAE}}},
  author = {Simonovsky, Martin and Komodakis, Nikos},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.03480 [cs]},
  eprint = {1802.03480},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fullyconnected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of molecule generation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/konstantinschurholt/Zotero/storage/MBGGAA8V/Simonovsky and Komodakis - 2018 - GraphVAE Towards Generation of Small Graphs Using.pdf}
}

@article{simsekGeometryLossLandscape,
  title = {Geometry of the {{Loss Landscape}} in {{Overparameterized Neural Networks}}: {{Symmetries}} and {{Invariances}}},
  author = {Simsek, Berfin and Ged, Fran{\c c}ois and Jacot, Arthur and Spadaro, Francesco and Hongler, Cl{\'e}ment and Gerstner, Wulfram and Brea, Johanni},
  pages = {11},
  abstract = {We study how permutation symmetries in overparameterized multi-layer neural networks generate `symmetry-induced' critical points. Assuming a network with L layers of minimal widths r1{${_\ast}$}, . . . , rL{${_\ast}$} -1 reaches a zero-loss minimum at r1{${_\ast}$}! {$\cdot$} {$\cdot$} {$\cdot$} rL{${_\ast}$} -1! isolated points that are permutations of one another, we show that adding one extra neuron to each layer is sufficient to connect all these previously discrete minima into a single manifold. For a two-layer overparameterized network of width r{${_\ast}$} + h =: m we explicitly describe the manifold of global minima: it consists of T (r{${_\ast}$}, m) affine subspaces of dimension at least h that are connected to one another. For a network of width m, we identify the number G(r, m) of affine subspaces containing only symmetry-induced critical points that are related to the critical points of a smaller network of width r {$<$} r{${_\ast}$}. Via a combinatorial analysis, we derive closed-form formulas for T and G and show that the number of symmetry-induced critical subspaces dominates the number of affine subspaces forming the global minima manifold in the mildly overparameterized regime (small h) and vice versa in the vastly overparameterized regime (h r{${_\ast}$}). Our results provide new insights into the minimization of the non-convex loss function of overparameterized neural networks.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/BNJLZ96C/Simsek et al. - Geometry of the Loss Landscape in Overparameterize.pdf}
}

@inproceedings{simsekGeometryLossLandscape2021,
  title = {Geometry of the {{Loss Landscape}} in {{Overparameterized Neural Networks}}: {{Symmetries}} and {{Invariances}}},
  shorttitle = {Geometry of the {{Loss Landscape}} in {{Overparameterized Neural Networks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Simsek, Berfin and Ged, Fran{\c c}ois and Jacot, Arthur and Spadaro, Francesco and Hongler, Clement and Gerstner, Wulfram and Brea, Johanni},
  year = {2021},
  month = jul,
  pages = {9722--9732},
  publisher = {{PMLR}},
  abstract = {We study how permutation symmetries in overparameterized multi-layer neural networks generate `symmetry-induced' critical points. Assuming a network with \$ L \$ layers of minimal widths \$ r\_1\^*, \textbackslash ld...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/FFQHAFY9/Simsek et al. - 2021 - Geometry of the Loss Landscape in Overparameterized Neural Networks Symmetries and Invariances.pdf}
}

@article{singhAbstractDomainCertifying2019,
  title = {An Abstract Domain for Certifying Neural Networks},
  author = {Singh, Gagandeep and Gehr, Timon and P{\"u}schel, Markus and Vechev, Martin},
  year = {2019},
  month = jan,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  number = {POPL},
  pages = {1--30},
  issn = {24751421},
  doi = {10.1145/3290354},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/2YFPR9V8/Singh et al. - 2019 - An abstract domain for certifying neural networks.pdf}
}

@article{singhBOOSTINGROBUSTNESSCERTIFICATION2019,
  title = {{{BOOSTING ROBUSTNESS CERTIFICATION OF NEURAL NETWORKS}}},
  author = {Singh, Gagandeep and Gehr, Timon},
  year = {2019},
  pages = {12},
  abstract = {We present a novel approach for the certification of neural networks against adversarial perturbations which combines scalable overapproximation methods with precise (mixed integer) linear programming. This results in significantly better precision than state-of-the-art verifiers on challenging feedforward and convolutional neural networks with piecewise linear activation functions.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/3ET52CUY/Singh and Gehr - 2019 - BOOSTING ROBUSTNESS CERTIFICATION OF NEURAL NETWOR.pdf}
}

@article{singhHierarchicalInterpretationsNeural2018,
  title = {Hierarchical Interpretations for Neural Network Predictions},
  author = {Singh, Chandan and Murdoch, W. James and Yu, Bin},
  year = {2018},
  month = jun,
  journal = {arXiv:1806.05337 [cs, stat]},
  eprint = {1806.05337},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. We introduce ACD using examples from Stanford Sentiment Treebank and ImageNet, in order to diagnose incorrect predictions, identify dataset bias, and extract polarizing phrases of varying lengths. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/C85VC3AT/Singh et al. - 2018 - Hierarchical interpretations for neural network pr.pdf}
}

@article{smilkovSmoothGradRemovingNoise2017,
  title = {{{SmoothGrad}}: Removing Noise by Adding Noise},
  shorttitle = {{{SmoothGrad}}},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  year = {2017},
  month = jun,
  journal = {arXiv:1706.03825 [cs, stat]},
  eprint = {1706.03825},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SMOOTHGRAD, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/GC7F8LVR/Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf}
}

@inproceedings{sohl-dicksteinDeepUnsupervisedLearning2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/LAMQ6XJU/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf}
}

@article{songScoreBasedGenerativeModeling2021,
  title = {Score-{{Based Generative Modeling}} through {{Stochastic Differential Equations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2021},
  month = feb,
  journal = {arXiv:2011.13456 [cs, stat]},
  eprint = {2011.13456},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 \textasciicircum{} 1024 images for the first time from a score-based generative model.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/B973BL3B/Song et al. - 2021 - Score-Based Generative Modeling through Stochastic.pdf}
}

@article{stanleyHypercubeBasedEncodingEvolving2009,
  title = {A {{Hypercube-Based Encoding}} for {{Evolving Large-Scale Neural Networks}}},
  author = {Stanley, Kenneth O. and D'Ambrosio, David B. and Gauci, Jason},
  year = {2009},
  month = apr,
  journal = {Artificial Life},
  volume = {15},
  number = {2},
  pages = {185--212},
  issn = {1064-5462, 1530-9185},
  doi = {10.1162/artl.2009.15.2.15202},
  abstract = {Research in neuroevolution \textemdash{} that is, evolving artificial neural networks (ANNs) through evolutionary algorithms \textemdash{} is inspired by the evolution of biological brains, which can contain trillions of connections. Yet while neuroevolution has produced successful results, the scale of natural brains remains far beyond reach. This article presents a method called hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) that aims to narrow this gap. HyperNEAT employs an indirect encoding called connective compositional pattern-producing networks (CPPNs) that can produce connectivity patterns with symmetries and repeating motifs by interpreting spatial patterns generated within a hypercube as connectivity patterns in a lower-dimensional space. This approach can exploit the geometry of the task by mapping its regularities onto the topology of the network, thereby shifting problem difficulty away from dimensionality to the underlying problem structure. Furthermore, connective CPPNs can represent the same connectivity pattern at any resolution, allowing ANNs to scale to new numbers of inputs and outputs without further evolution. HyperNEAT is demonstrated through visual discrimination and food-gathering tasks, including successful visual discrimination networks containing over eight million connections. The main conclusion is that the ability to explore the space of regular connectivity patterns opens up a new class of complex high-dimensional tasks to neuroevolution.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/D9NIIPVX/Stanley et al. - 2009 - A Hypercube-Based Encoding for Evolving Large-Scal.pdf}
}

@article{steinerHowTrainYour2021,
  title = {How to Train Your {{ViT}}? {{Data}}, {{Augmentation}}, and {{Regularization}} in {{Vision Transformers}}},
  shorttitle = {How to Train Your {{ViT}}?},
  author = {Steiner, Andreas and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.10270 [cs]},
  eprint = {2106.10270},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (``AugReg'' for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. 1 As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/ALCELSAY/Steiner et al. - 2021 - How to train your ViT Data, Augmentation, and Reg.pdf}
}

@article{suarezGANYouGAN2019,
  title = {{{GAN You Do}} the {{GAN GAN}}?},
  author = {Suarez, Joseph},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.00724 [cs]},
  eprint = {1904.00724},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Generative Adversarial Networks (GANs) have become a dominant class of generative models. In recent years, GAN variants have yielded especially impressive results in the synthesis of a variety of forms of data. Examples include compelling natural and artistic images, textures, musical sequences, and 3D object files. However, one obvious synthesis candidate is missing. In this work, we answer one of deep learning's most pressing questions: GAN you do the GAN GAN? That is, is it possible to train a GAN to model a distribution of GANs? We release the full source code for this project under the MIT license.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/BU27KBF9/Suarez - 2019 - GAN You Do the GAN GAN.pdf}
}

@misc{suchAtariModelZoo2019,
  title = {An {{Atari Model Zoo}} for {{Analyzing}}, {{Visualizing}}, and {{Comparing Deep Reinforcement Learning Agents}}},
  author = {Such, Felipe Petroski and Madhavan, Vashisht and Liu, Rosanne and Wang, Rui and Castro, Pablo Samuel and Li, Yulun and Zhi, Jiale and Schubert, Ludwig and Bellemare, Marc G. and Clune, Jeff and Lehman, Joel},
  year = {2019},
  month = may,
  number = {arXiv:1812.07069},
  eprint = {1812.07069},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Much human and computational effort has aimed to improve how deep reinforcement learning (DRL) algorithms perform on benchmarks such as the Atari Learning Environment. Comparatively less effort has focused on understanding what has been learned by such methods, and investigating and comparing the representations learned by different families of DRL algorithms. Sources of friction include the onerous computational requirements, and general logistical and architectural complications for running DRL algorithms at scale. We lessen this friction, by (1) training several algorithms at scale and releasing trained models, (2) integrating with a previous DRL model release, and (3) releasing code that makes it easy for anyone to load, visualize, and analyze such models. This paper introduces the Atari Zoo framework, which contains models trained across benchmark Atari games, in an easy-to-use format, as well as code that implements common modes of analysis and connects such models to a popular neural network visualization library. Further, to demonstrate the potential of this dataset and software package, we show initial quantitative and qualitative comparisons between the performance and representations of several DRL algorithms, highlighting interesting and previously unknown distinctions between them.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/konstantinschurholt/Zotero/storage/DUURE8UI/Such et al. - 2019 - An Atari Model Zoo for Analyzing, Visualizing, and.pdf}
}

@inproceedings{summersNondeterminismInstabilityNeural2021,
  title = {Nondeterminism and {{Instability}} in {{Neural Network Optimization}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Summers, Cecilia and Dinneen, Michael J.},
  year = {2021},
  month = jul,
  pages = {9913--9922},
  publisher = {{PMLR}},
  abstract = {Nondeterminism in neural network optimization produces uncertainty in performance, making small improvements difficult to discern from run-to-run variability. While uncertainty can be reduced by tr...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/7QWBS44J/Summers and Dinneen - 2021 - Nondeterminism and Instability in Neural Network Optimization.pdf}
}

@article{sundararajanAxiomaticAttributionDeep2017,
  title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.01365 [cs]},
  eprint = {1703.01365},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms\textemdash Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/AS2XBEVG/Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf}
}

@article{sunTestTimeTrainingSelfSupervision,
  title = {Test-{{Time Training}} with {{Self-Supervision}} for {{Generalization}} under {{Distribution Shifts}}},
  author = {Sun, Yu and Wang, Xiaolong and Liu, Zhuang and Miller, John and Efros, Alexei A and Hardt, Moritz},
  pages = {20},
  abstract = {In this paper, we propose Test-Time Training, a general approach for improving the performance of predictive models when training and test data come from different distributions. We turn a single unlabeled test sample into a self-supervised learning problem, on which we update the model parameters before making a prediction. This also extends naturally to data in an online stream. Our simple approach leads to improvements on diverse image classification benchmarks aimed at evaluating robustness to distribution shifts.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/BZJTPNS6/Sun et al. - Test-Time Training with Self-Supervision for Gener.pdf}
}

@inproceedings{tanEfficientNetV2SmallerModels2021,
  title = {{{EfficientNetV2}}: {{Smaller Models}} and {{Faster Training}}},
  shorttitle = {{{EfficientNetV2}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Tan, Mingxing and Le, Quoc},
  year = {2021},
  month = jul,
  pages = {10096--10106},
  publisher = {{PMLR}},
  abstract = {This paper introduces EfficientNetV2, a new family of convolutional networks that have faster training speed and better parameter efficiency than previous models. To develop these models, we use a ...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/C828M9KS/Tan and Le - 2021 - EfficientNetV2 Smaller Models and Faster Training.pdf}
}

@inproceedings{tayOmniNetOmnidirectionalRepresentations2021a,
  title = {{{OmniNet}}: {{Omnidirectional Representations}} from {{Transformers}}},
  shorttitle = {{{OmniNet}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Tay, Yi and Dehghani, Mostafa and Aribandi, Vamsi and Gupta, Jai and Pham, Philip M. and Qin, Zhen and Bahri, Dara and Juan, Da-Cheng and Metzler, Donald},
  year = {2021},
  month = jul,
  pages = {10193--10202},
  publisher = {{PMLR}},
  abstract = {This paper proposes Omnidirectional Representations from Transformers (OMNINET). In OmniNet, instead of maintaining a strictly horizon-tal receptive field, each token is allowed to attend to all to...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/38WJGKDH/Tay et al. - 2021 - OmniNet Omnidirectional Representations from Transformers.pdf}
}

@inproceedings{tianUnderstandingSelfsupervisedLearning2021a,
  title = {Understanding Self-Supervised Learning Dynamics without Contrastive Pairs},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Tian, Yuandong and Chen, Xinlei and Ganguli, Surya},
  year = {2021},
  month = jul,
  pages = {10268--10278},
  publisher = {{PMLR}},
  abstract = {While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data point (positive pairs) and maximizing vi...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/PR8677DL/Tian et al. - 2021 - Understanding self-supervised learning dynamics without contrastive pairs.pdf}
}

@article{tianWhatMakesGood2020,
  title = {What {{Makes}} for {{Good Views}} for {{Contrastive Learning}}?},
  author = {Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip},
  year = {2020},
  month = dec,
  journal = {arXiv:2005.10243 [cs]},
  eprint = {2005.10243},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Contrastive learning between multiple views of the data has recently achieved state of the art performance in the field of self-supervised representation learning. Despite its success, the influence of different view choices has been less studied. In this paper, we use theoretical and empirical analysis to better understand the importance of view selection, and argue that we should reduce the mutual information (MI) between views while keeping task-relevant information intact. To verify this hypothesis, we devise unsupervised and semi-supervised frameworks that learn effective views by aiming to reduce their MI. We also consider data augmentation as a way to reduce MI, and show that increasing data augmentation indeed leads to decreasing MI and improves downstream classification accuracy. As a byproduct, we achieve a new state-of-the-art accuracy on unsupervised pre-training for ImageNet classification (73\% top-1 linear readout with a ResNet-50)1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/FJCNLQAB/Tian et al. - 2020 - What Makes for Good Views for Contrastive Learning.pdf}
}

@article{tolstikhinMLPMixerAllMLPArchitecture2021,
  title = {{{MLP-Mixer}}: {{An}} All-{{MLP Architecture}} for {{Vision}}},
  shorttitle = {{{MLP-Mixer}}},
  author = {Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
  year = {2021},
  month = jun,
  journal = {arXiv:2105.01601 [cs]},
  eprint = {2105.01601},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/BVNHCU8V/Tolstikhin et al. - 2021 - MLP-Mixer An all-MLP Architecture for Vision.pdf}
}

@inproceedings{tranTransferabilityHardnessSupervised2019,
  title = {Transferability and {{Hardness}} of {{Supervised Classification Tasks}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Tran, Anh and Nguyen, Cuong and Hassner, Tal},
  year = {2019},
  month = oct,
  pages = {1395--1405},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00148},
  abstract = {We propose a novel approach for estimating the difficulty and transferability of supervised classification tasks. Unlike previous work, our approach is solution agnostic and does not require or assume trained models. Instead, we estimate these values using an information theoretic approach: treating training labels as random variables and exploring their statistics. When transferring from a source to a target task, we consider the conditional entropy between two such variables (i.e., label assignments of the two tasks). We show analytically and empirically that this value is related to the loss of the transferred model. We further show how to use this value to estimate task hardness. We test our claims extensively on three large scale data sets\textemdash CelebA (40 tasks), Animals with Attributes 2 (85 tasks), and Caltech-UCSD Birds 200 (312 tasks)\textemdash together representing 437 classification tasks. We provide results showing that our hardness and transferability estimates are strongly correlated with empirical hardness and transferability. As a case study, we transfer a learned face recognition model to CelebA attribute classification tasks, showing state of the art accuracy for tasks estimated to be highly transferable.},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/RNIXBI79/Tran et al. - 2019 - Transferability and Hardness of Supervised Classif.pdf}
}

@article{tsangDetectingStatisticalInteractions2017,
  title = {Detecting {{Statistical Interactions}} from {{Neural Network Weights}}},
  author = {Tsang, Michael and Cheng, Dehua and Liu, Yan},
  year = {2017},
  month = may,
  journal = {arXiv:1705.04977 [cs, stat]},
  eprint = {1705.04977},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Interpreting neural networks is a crucial and challenging task in machine learning. In this paper, we develop a novel framework for detecting statistical interactions captured by a feedforward multilayer neural network by directly interpreting its learned weights. Depending on the desired interactions, our method can achieve significantly better or similar interaction detection performance compared to the state-of-the-art without searching an exponential solution space of possible interactions. We obtain this accuracy and efficiency by observing that interactions between input features are created by the non-additive effect of nonlinear activation functions, and that interacting paths are encoded in weight matrices. We demonstrate the performance of our method and the importance of discovered interactions via experimental results on both synthetic datasets and real-world application datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/GQN5SC4J/Tsang et al. - 2017 - Detecting Statistical Interactions from Neural Net.pdf}
}

@article{unterthinerPredictingNeuralNetwork2020,
  title = {Predicting {{Neural Network Accuracy}} from {{Weights}}},
  author = {Unterthiner, Thomas and Keysers, Daniel and Gelly, Sylvain and Bousquet, Olivier and Tolstikhin, Ilya},
  year = {2020},
  month = feb,
  journal = {arXiv:2002.11448 [cs, stat]},
  eprint = {2002.11448},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We study the prediction of the accuracy of a neural network given only its weights with the goal of better understanding network training and performance. To do so, we propose a formal setting which frames this task and connects to previous work in this area. We collect (and release) a large dataset of almost 80k convolutional neural networks trained on four image datasets. We demonstrate that strong predictors of accuracy exist. Moreover, they can achieve good predictions while only using simple statistics of the weights. Surprisingly, these predictors are able to rank networks trained on unobserved datasets or using different architectures.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/9M7JLT5P/Unterthiner et al. - 2020 - Predicting Neural Network Accuracy from Weights.pdf;/Users/konstantinschurholt/Zotero/storage/GLW2HTU3/Unterthiner et al. - 2020 - Predicting Neural Network Accuracy from Weights.pdf;/Users/konstantinschurholt/Zotero/storage/LL952Q6B/2002.11448.pdf;/Users/konstantinschurholt/Zotero/storage/YVACYPVE/Unterthiner et al. - 2020 - Predicting Neural Network Accuracy from Weights.pdf}
}

@article{vallatPingouinStatisticsPython2018,
  title = {Pingouin: Statistics in {{Python}}},
  shorttitle = {Pingouin},
  author = {Vallat, Raphael},
  year = {2018},
  month = nov,
  journal = {Journal of Open Source Software},
  volume = {3},
  number = {31},
  pages = {1026},
  issn = {2475-9066},
  doi = {10.21105/joss.01026},
  abstract = {Python is currently the fastest growing programming language in the world, thanks to its ease-of-use, fast learning curve and its numerous high quality packages for data science and machine-learning. Surprisingly however, Python is far behind the R programming language when it comes to general statistics and for this reason many scientists still rely heavily on R to perform their statistical analyses.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/ZATYNB57/Vallat - 2018 - Pingouin statistics in Python.pdf}
}

@article{vapnikOverviewStatisticalLearning1999,
  title = {An Overview of Statistical Learning Theory},
  author = {Vapnik, V.N.},
  year = {Sept./1999},
  journal = {IEEE Transactions on Neural Networks},
  volume = {10},
  number = {5},
  pages = {988--999},
  issn = {10459227},
  doi = {10.1109/72.788640},
  abstract = {Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more detailed overview of the theory (without proofs) can be found in Vapnik (1995). In Vapnik (1998) one can find detailed description of the theory (including proofs).},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/8CT354F9/Vapnik - 1999 - An overview of statistical learning theory.pdf}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  booktitle = {{{arXiv}}:1706.03762 [Cs]},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/P6AV2UW3/Vaswani et al. - 2017 - Attention Is All You Need.pdf;/Users/konstantinschurholt/Zotero/storage/UHYERFRZ/Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}

@inproceedings{vermaDomainAgnosticContrastiveLearning2021,
  title = {Towards {{Domain-Agnostic Contrastive Learning}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Verma, Vikas and Luong, Thang and Kawaguchi, Kenji and Pham, Hieu and Le, Quoc},
  year = {2021},
  month = jul,
  pages = {10530--10541},
  publisher = {{PMLR}},
  abstract = {Despite recent successes, most contrastive self-supervised learning methods are domain-specific, relying heavily on data augmentation techniques that require knowledge about a particular domain, su...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/NJHZIVTS/Verma et al. - 2021 - Towards Domain-Agnostic Contrastive Learning.pdf}
}

@article{vidalMathematicsDeepLearning2017,
  title = {Mathematics of {{Deep Learning}}},
  author = {Vidal, Rene and Bruna, Joan and Giryes, Raja and Soatto, Stefano},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.04741 [cs]},
  eprint = {1712.04741},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recently there has been a dramatic increase in the performance of recognition systems due to the introduction of deep architectures for representation learning and classification. However, the mathematical reasons for this success remain elusive. This tutorial will review recent work that aims to provide a mathematical justification for several properties of deep networks, such as global optimality, geometric stability, and invariance of the learned representations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/5ZB4JJTK/Vidal et al. - 2017 - Mathematics of Deep Learning.pdf}
}

@article{wangAxialDeepLabStandAloneAxialAttention2020,
  title = {Axial-{{DeepLab}}: {{Stand-Alone Axial-Attention}} for {{Panoptic Segmentation}}},
  shorttitle = {Axial-{{DeepLab}}},
  author = {Wang, Huiyu and Zhu, Yukun and Green, Bradley and Adam, Hartwig and Yuille, Alan and Chen, Liang-Chieh},
  year = {2020},
  month = aug,
  journal = {arXiv:2003.07853 [cs]},
  eprint = {2003.07853},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Convolution exploits locality for efficiency at a cost of missing long range context. Self-attention has been adopted to augment CNNs with non-local interactions. Recent works prove it possible to stack self-attention layers to obtain a fully attentional network by restricting the attention to a local region. In this paper, we attempt to remove this constraint by factorizing 2D self-attention into two 1D selfattentions. This reduces computation complexity and allows performing attention within a larger or even global region. In companion, we also propose a position-sensitive self-attention design. Combining both yields our position-sensitive axial-attention layer, a novel building block that one could stack to form axial-attention models for image classification and dense prediction. We demonstrate the effectiveness of our model on four large-scale datasets. In particular, our model outperforms all existing stand-alone self-attention models on ImageNet. Our Axial-DeepLab improves 2.8\% PQ over bottom-up state-of-the-art on COCO test-dev. This previous state-of-the-art is attained by our small variant that is 3.8\texttimes{} parameter-efficient and 27\texttimes{} computation-efficient. Axial-DeepLab also achieves state-of-the-art results on Mapillary Vistas and Cityscapes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/UD7PGKYM/Wang et al. - 2020 - Axial-DeepLab Stand-Alone Axial-Attention for Pan.pdf}
}

@article{wangDiscriminativeFeatureAlignment2020,
  title = {Discriminative {{Feature Alignment}}: {{Improving Transferability}} of {{Unsupervised Domain Adaptation}} by {{Gaussian-guided Latent Alignment}}},
  shorttitle = {Discriminative {{Feature Alignment}}},
  author = {Wang, Jing and Chen, Jiahong and Lin, Jianzhe and Sigal, Leonid and {de Silva}, Clarence W.},
  year = {2020},
  month = aug,
  journal = {arXiv:2006.12770 [cs]},
  eprint = {2006.12770},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper, we focus on the unsupervised domain adaptation problem where an approximate inference model is to be learned from a labeled data domain and expected to generalize well to an unlabeled data domain. The success of unsupervised domain adaptation largely relies on the cross-domain feature alignment. Previous work has attempted to directly align latent features by the classifier-induced discrepancies. Nevertheless, a common feature space cannot always be learned via this direct feature alignment especially when a large domain gap exists. To solve this problem, we introduce a Gaussianguided latent alignment approach to align the latent feature distributions of the two domains under the guidance of the prior distribution. In such an indirect way, the distributions over the samples from the two domains will be constructed on a common feature space, i.e., the space of the prior, which promotes better feature alignment. To effectively align the target latent distribution with this prior distribution, we also propose a novel unpaired L1-distance by taking advantage of the formulation of the encoder-decoder. The extensive evaluations on nine benchmark datasets validate the superior knowledge transferability through outperforming state-of-the-art methods and the versatility of the proposed method by improving the existing work significantly.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/NWNQ3VGL/Wang et al. - 2020 - Discriminative Feature Alignment Improving Transf.pdf}
}

@article{wangEnsemblesGenerativeAdversarial2016,
  title = {Ensembles of {{Generative Adversarial Networks}}},
  author = {Wang, Yaxing and Zhang, Lichao and {van de Weijer}, Joost},
  year = {2016},
  month = dec,
  journal = {arXiv:1612.00991 [cs]},
  eprint = {1612.00991},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Ensembles are a popular way to improve results of discriminative CNNs. The combination of several networks trained starting from different initializations improves results significantly. In this paper we investigate the usage of ensembles of GANs. The specific nature of GANs opens up several new ways to construct ensembles. The first one is based on the fact that in the minimax game which is played to optimize the GAN objective the generator network keeps on changing even after the network can be considered optimal. As such ensembles of GANs can be constructed based on the same network initialization but just taking models which have different amount of iterations. These so-called self ensembles are much faster to train than traditional ensembles. The second method, called cascade GANs, redirects part of the training data which is badly modeled by the first GAN to another GAN. In experiments on the CIFAR10 dataset we show that ensembles of GANs obtain model probability distributions which better model the data distribution. In addition, we show that these improved results can be obtained at little additional computational cost.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/A7PKL53T/Wang et al. - 2016 - Ensembles of Generative Adversarial Networks.pdf}
}

@article{wangGainingFreeLowCost2018,
  title = {Gaining {{Free}} or {{Low-Cost Transparency}} with {{Interpretable Partial Substitute}}},
  author = {Wang, Tong},
  year = {2018},
  month = feb,
  journal = {arXiv:1802.04346 [cs, stat]},
  eprint = {1802.04346},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This work addresses the situation where a blackbox model with good predictive performance is chosen over its interpretable competitors, and we show interpretability is still achievable in this case. Our solution is to find an interpretable substitute on a subset of data where the black-box model is overkill or nearly overkill while leaving the rest to the black-box. This transparency is obtained at minimal cost or no cost of the predictive performance. Under this framework, we develop a Hybrid Rule Sets (HyRS) model that uses decision rules to capture the subspace of data where the rules are as accurate or almost as accurate as the black-box provided. To train a HyRS, we devise an efficient search algorithm that iteratively finds the optimal model and exploits theoretically grounded strategies to reduce computation. Our framework is agnostic to the black-box during training. Experiments on structured and text data show that HyRS obtains an effective trade-off between transparency and interpretability.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/AWIWWCA4/Wang - 2018 - Gaining Free or Low-Cost Transparency with Interpr.pdf}
}

@article{wangLinformerSelfAttentionLinear2020,
  title = {Linformer: {{Self-Attention}} with {{Linear Complexity}}},
  shorttitle = {Linformer},
  author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.04768 [cs, stat]},
  eprint = {2006.04768},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Large transformer models have shown extraordinary success in achieving state-ofthe-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses O(n2) time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from O(n2) to O(n) in both time and space. The resulting linear transformer, the Linformer, performs on par with standard Transformer models, while being much more memory- and time-efficient.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/75WNKT3S/Wang et al. - 2020 - Linformer Self-Attention with Linear Complexity.pdf}
}

@inproceedings{wangRealTimeWorkloadClassification2018,
  title = {Real-{{Time Workload Classification}} during {{Driving}} Using {{HyperNetworks}}},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Wang, Ruohan and Amadori, Pierluigi V. and Demiris, Yiannis},
  year = {2018},
  month = oct,
  pages = {3060--3065},
  publisher = {{IEEE}},
  address = {{Madrid}},
  doi = {10.1109/IROS.2018.8594305},
  abstract = {Classifying human cognitive states from behavioral and physiological signals is a challenging problem with important applications in robotics. The problem is challenging due to the data variability among individual users, and sensor artefacts. In this work, we propose an end-to-end framework for real-time cognitive workload classification with mixture Hyper Long Short Term Memory Networks (m-HyperLSTM), a novel variant of HyperNetworks. Evaluating the proposed approach on an eye-gaze pattern dataset collected from simulated driving scenarios of different cognitive demands, we show that the proposed framework outperforms previous baseline methods and achieves 83.9\% precision and 87.8\% recall during test. We also demonstrate the merit of our proposed architecture by showing improved performance over other LSTM-based methods.},
  isbn = {978-1-5386-8094-0},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/33ATNBVK/Wang et al. - 2018 - Real-Time Workload Classification during Driving u.pdf;/Users/konstantinschurholt/Zotero/storage/JN2TBNQF/Wang et al. - 2018 - Real-Time Workload Classification during Driving u.pdf;/Users/konstantinschurholt/Zotero/storage/KSHXSQC5/Wang et al. - 2018 - Real-Time Workload Classification during Driving u.pdf}
}

@misc{wangRecurrentParameterGenerators2021,
  title = {Recurrent {{Parameter Generators}}},
  author = {Wang, Jiayun and Chen, Yubei and Yu, Stella X. and Cheung, Brian and LeCun, Yann},
  year = {2021},
  month = jul,
  number = {arXiv:2107.07110},
  eprint = {2107.07110},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.07110},
  abstract = {We present a generic method for recurrently using the same parameters for many different convolution layers to build a deep network. Specifically, for a network, we create a recurrent parameter generator (RPG), from which the parameters of each convolution layer are generated. Though using recurrent models to build a deep convolutional neural network (CNN) is not entirely new, our method achieves significant performance gain compared to the existing works. We demonstrate how to build a one-layer neural network to achieve similar performance compared to other traditional CNN models on various applications and datasets. Such a method allows us to build an arbitrarily complex neural network with any amount of parameters. For example, we build a ResNet34 with model parameters reduced by more than \$400\$ times, which still achieves \$41.6\textbackslash\%\$ ImageNet top-1 accuracy. Furthermore, we demonstrate the RPG can be applied at different scales, such as layers, blocks, or even sub-networks. Specifically, we use the RPG to build a ResNet18 network with the number of weights equivalent to one convolutional layer of a conventional ResNet and show this model can achieve \$67.2\textbackslash\%\$ ImageNet top-1 accuracy. The proposed method can be viewed as an inverse approach to model compression. Rather than removing the unused parameters from a large model, it aims to squeeze more information into a small number of parameters. Extensive experiment results are provided to demonstrate the power of the proposed recurrent parameter generator.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/DX8PV8FI/Wang et al. - 2021 - Recurrent Parameter Generators.pdf;/Users/konstantinschurholt/Zotero/storage/JB3TS7BW/2107.html}
}

@article{wangUnderstandingLearningRepresentations2018,
  title = {Towards {{Understanding Learning Representations}}: {{To What Extent Do Different Neural Networks Learn}} the {{Same Representation}}},
  shorttitle = {Towards {{Understanding Learning Representations}}},
  author = {Wang, Liwei and Hu, Lunjia and Gu, Jiayuan and Wu, Yue and Hu, Zhiqiang and He, Kun and Hopcroft, John},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.11750 [cs, stat]},
  eprint = {1810.11750},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks. Although highly intuitive, there is a lack of theory and systematic approach quantitatively characterizing what representations do deep neural networks learn. In this work, we move a tiny step towards a theory and better understanding of the representations. Specifically, we study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations. We develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches, where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches. Finally, we conduct extensive experiments using our algorithms. Experimental results suggest that, surprisingly, representations learned by the same convolutional layers of networks trained from different initializations are not as similar as prevalently expected, at least in terms of subspace match.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/VMKBC49J/Wang et al. - 2018 - Towards Understanding Learning Representations To.pdf}
}

@article{wangWisdomEnsembleImproving2020,
  title = {Wisdom of the {{Ensemble}}: {{Improving Consistency}} of {{Deep Learning Models}}},
  shorttitle = {Wisdom of the {{Ensemble}}},
  author = {Wang, Lijing and Ghosh, Dipanjan and Diaz, Maria Teresa Gonzalez and Farahat, Ahmed and Alam, Mahbubul and Gupta, Chetan and Chen, Jiangzhuo and Marathe, Madhav},
  year = {2020},
  month = nov,
  journal = {arXiv:2011.06796 [cs, stat]},
  eprint = {2011.06796},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep learning classifiers are assisting humans in making decisions and hence the user's trust in these models is of paramount importance. Trust is often a function of constant behavior. From an AI model perspective it means given the same input the user would expect the same output, especially for correct outputs, or in other words consistently correct outputs. This paper studies a model behavior in the context of periodic retraining of deployed models where the outputs from successive generations of the models might not agree on the correct labels assigned to the same input. We formally define consistency and correct-consistency of a learning model. We prove that consistency and correct-consistency of an ensemble learner is not less than the average consistency and correct-consistency of individual learners and correct-consistency can be improved with a probability by combining learners with accuracy not less than the average accuracy of ensemble component learners. To validate the theory using three datasets and two state-ofthe-art deep learning classifiers we also propose an efficient dynamic snapshot ensemble method and demonstrate its value. Code for our algorithm is available at https://github.com/christa60/dynens.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/RIIUTLUG/Wang et al. - 2020 - Wisdom of the Ensemble Improving Consistency of D.pdf}
}

@article{wenzelHyperparameterEnsemblesRobustness2021,
  title = {Hyperparameter {{Ensembles}} for {{Robustness}} and {{Uncertainty Quantification}}},
  author = {Wenzel, Florian and Snoek, Jasper and Tran, Dustin and Jenatton, Rodolphe},
  year = {2021},
  month = jan,
  journal = {arXiv:2006.13570 [cs, stat]},
  eprint = {2006.13570},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Ensembles over neural network weights trained from different random initialization, known as deep ensembles, achieve state-of-the-art accuracy and calibration. The recently introduced batch ensembles provide a drop-in replacement that is more parameter efficient. In this paper, we design ensembles not only over weights, but over hyperparameters to improve the state of the art in both settings. For best performance independent of budget, we propose hyper-deep ensembles, a simple procedure that involves a random search over different hyperparameters, themselves stratified across multiple random initializations. Its strong performance highlights the benefit of combining models with both weight and hyperparameter diversity. We further propose a parameter efficient version, hyper-batch ensembles, which builds on the layer structure of batch ensembles and self-tuning networks. The computational and memory costs of our method are notably lower than typical ensembles. On image classification tasks, with MLP, LeNet, ResNet 20 and Wide ResNet 28-10 architectures, we improve upon both deep and batch ensembles.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/3ANWJV7Z/Wenzel et al. - 2021 - Hyperparameter Ensembles for Robustness and Uncert.pdf;/Users/konstantinschurholt/Zotero/storage/SAYKRTAA/Wenzel et al. - 2020 - Hyperparameter Ensembles for Robustness and Uncert.pdf}
}

@inproceedings{winterPermutationInvariantVariationalAutoencoder2021,
  title = {Permutation-{{Invariant Variational Autoencoder}} for {{Graph-Level Representation Learning}}},
  booktitle = {{{arXiv}}:2104.09856 [Cs]},
  author = {Winter, Robin and No{\'e}, Frank and Clevert, Djork-Arn{\'e}},
  year = {2021},
  month = dec,
  eprint = {2104.09856},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recently, there has been great success in applying deep neural networks on graph structured data. Most work, however, focuses on either node- or graph-level supervised learning, such as node, link or graph classification or node-level unsupervised learning (e.g., node clustering). Despite its wide range of possible applications, graph-level unsupervised representation learning has not received much attention yet. This might be mainly attributed to the high representation complexity of graphs, which can be represented by n! equivalent adjacency matrices, where n is the number of nodes. In this work we address this issue by proposing a permutation-invariant variational autoencoder for graph structured data. Our proposed model indirectly learns to match the node order of input and output graph, without imposing a particular node order or performing expensive graph matching. We demonstrate the effectiveness of our proposed model for graph reconstruction, generation and interpolation and evaluate the expressive power of extracted representations for downstream graph-level classification and regression.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/S3Q6FQTE/Winter et al. - 2021 - Permutation-Invariant Variational Autoencoder for .pdf}
}

@inproceedings{wortsmanLearningNeuralNetwork2021,
  title = {Learning {{Neural Network Subspaces}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Wortsman, Mitchell and Horton, Maxwell C. and Guestrin, Carlos and Farhadi, Ali and Rastegari, Mohammad},
  year = {2021},
  month = jul,
  pages = {11217--11227},
  publisher = {{PMLR}},
  abstract = {Recent observations have advanced our understanding of the neural network optimization landscape, revealing the existence of (1) paths of high accuracy containing diverse solutions and (2) wider mi...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/FXEAR3FE/Wortsman et al. - 2021 - Learning Neural Network Subspaces.pdf}
}

@misc{xiaoFashionMNISTNovelImage2017,
  title = {Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking Machine Learning Algorithms}}},
  shorttitle = {Fashion-{{MNIST}}},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  year = {2017},
  month = sep,
  number = {arXiv:1708.07747},
  eprint = {1708.07747},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/4JNVMT54/Xiao et al. - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmark.pdf}
}

@article{xiaoWhatShouldNot2021,
  title = {What {{Should Not Be Contrastive}} in {{Contrastive Learning}}},
  author = {Xiao, Tete and Wang, Xiaolong and Efros, Alexei A. and Darrell, Trevor},
  year = {2021},
  month = mar,
  journal = {arXiv:2008.05659 [cs]},
  eprint = {2008.05659},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent self-supervised contrastive methods have been able to produce impressive transferable visual representations by learning to be invariant to different data augmentations. However, these methods implicitly assume a particular set of representational invariances (e.g., invariance to color), and can perform poorly when a downstream task violates this assumption (e.g., distinguishing red vs. yellow cars). We introduce a contrastive learning framework which does not require prior knowledge of specific, task-dependent invariances. Our model learns to capture varying and invariant factors for visual representations by constructing separate embedding spaces, each of which is invariant to all but one augmentation. We use a multi-head network with a shared backbone which captures information across each augmentation and alone outperforms all baselines on downstream tasks. We further find that the concatenation of the invariant and varying spaces performs best across all tasks we investigate, including coarse-grained, fine-grained, and few-shot downstream classification tasks, and various data corruptions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/35E99TBF/Xiao et al. - 2021 - What Should Not Be Contrastive in Contrastive Lear.pdf}
}

@article{yakTaskArchitectureIndependentGeneralization2019,
  title = {Towards {{Task}} and {{Architecture-Independent Generalization Gap Predictors}}},
  author = {Yak, Scott and Gonzalvo, Javier and Mazzawi, Hanna},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.01550 [cs, stat]},
  eprint = {1906.01550},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Can we use deep learning to predict when deep learning works? Our results suggest the affirmative. We created a dataset by training 13,500 neural networks with different architectures, on different variations of spiral datasets, and using different optimization parameters. We used this dataset to train task-independent and architectureindependent generalization gap predictors for those neural networks. We extend Jiang et al. (2018) to also use DNNs and RNNs and show that they outperform the linear model, obtaining R2 = 0.965. We also show results for architecture-independent, task-independent, and out-of-distribution generalization gap prediction tasks. Both DNNs and RNNs consistently and significantly outperform linear models, with RNNs obtaining R2 = 0.584.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/Q6B3JS4R/Yak et al. - 2019 - Towards Task and Architecture-Independent Generali.pdf}
}

@article{yangHeterogeneousNetworkRepresentation2020,
  title = {Heterogeneous {{Network Representation Learning}}: {{Survey}}, {{Benchmark}}, {{Evaluation}}, and {{Beyond}}},
  shorttitle = {Heterogeneous {{Network Representation Learning}}},
  author = {Yang, Carl and Xiao, Yuxin and Zhang, Yu and Sun, Yizhou and Han, Jiawei},
  year = {2020},
  month = mar,
  journal = {arXiv:2004.00216 [cs]},
  eprint = {2004.00216},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Since real-world objects and their interactions are often multimodal and multi-typed, heterogeneous networks have been widely used as a more powerful, realistic, and generic superclass of traditional homogeneous networks (graphs). Meanwhile, representation learning (a.k.a. embedding) has recently been intensively studied and shown effective for various network mining and analytical tasks. Since there has already been a broad body of heterogeneous network embedding (HNE) algorithms but no dedicated survey, as the first contribution of this work, we pioneer in providing a unified paradigm for the systematic categorization and analysis over the merits of various existing HNE algorithms. Moreover, existing HNE algorithms, though mostly claimed generic, are often evaluated on different datasets. Understandable due to the natural application favor of HNE, such indirect comparisons largely hinder the proper attribution of improved task performance towards effective data preprocessing and novel technical design, especially considering the various ways possible to construct a heterogeneous network from real-world application data. Therefore, as the second contribution, we create four benchmark datasets with various properties regarding scale, structure, attribute/label availability, and etc. from different sources, towards the comprehensive evaluation of HNE algorithms. As the third contribution, we carefully refactor and amend the implementations of and create friendly interfaces for ten popular HNE algorithms, and provide all-around comparisons among them over multiple tasks and experimental settings.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/konstantinschurholt/Zotero/storage/PXXR2IAF/Yang et al. - 2020 - Heterogeneous Network Representation Learning Sur.pdf}
}

@article{yinAdaViTAdaptiveTokens2021,
  title = {{{AdaViT}}: {{Adaptive Tokens}} for {{Efficient Vision Transformer}}},
  shorttitle = {{{AdaViT}}},
  author = {Yin, Hongxu and Vahdat, Arash and Alvarez, Jose and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.07658 [cs]},
  eprint = {2112.07658},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce AdaViT, a method that adaptively adjusts the inference cost of vision transformer (ViT) for images of different complexity. AdaViT achieves this by automatically reducing the number of tokens in vision transformers that are processed in the network as inference proceeds. We reformulate Adaptive Computation Time (ACT) for this task, extending halting to discard redundant spatial tokens. The appealing architectural properties of vision transformers enables our adaptive token reduction mechanism to speed up inference without modifying the network architecture or inference hardware. We demonstrate that AdaViT requires no extra parameters or sub-network for halting, as we base the learning of adaptive halting on the original network parameters. We further introduce distributional prior regularization that stabilizes training compared to prior ACT approaches. On the image classification task (ImageNet1K), we show that our proposed AdaViT yields high efficacy in filtering informative spatial features and cutting down on the overall compute. The proposed method improves the throughput of DeiT-Tiny by 62\% and DeiT-Small by 38\% with only 0.3\% accuracy drop, outperforming prior art by a large margin.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/YF4G7KYH/Yin et al. - 2021 - AdaViT Adaptive Tokens for Efficient Vision Trans.pdf}
}

@inproceedings{yingNASBench101ReproducibleNeural2019,
  title = {{{NAS-Bench-101}}: {{Towards Reproducible Neural Architecture Search}}},
  shorttitle = {{{NAS-Bench-101}}},
  booktitle = {{{PMLR}}},
  author = {Ying, Chris and Klein, Aaron and Real, Esteban and Christiansen, Eric and Murphy, Kevin and Hutter, Frank},
  year = {2019},
  volume = {97},
  eprint = {1902.09635},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Recent advances in neural architecture search (NAS) demand tremendous computational resources, which makes it difficult to reproduce experiments and imposes a barrier-to-entry to researchers without access to large-scale computation. We aim to ameliorate these problems by introducing NAS-Bench-101, the first public architecture dataset for NAS research. To build NASBench-101, we carefully constructed a compact, yet expressive, search space, exploiting graph isomorphisms to identify 423k unique convolutional architectures. We trained and evaluated all of these architectures multiple times on CIFAR-10 and compiled the results into a large dataset of over 5 million trained models. This allows researchers to evaluate the quality of a diverse range of models in milliseconds by querying the precomputed dataset. We demonstrate its utility by analyzing the dataset as a whole and by benchmarking a range of architecture optimization algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/XHBSFYAR/Ying et al. - 2019 - NAS-Bench-101 Towards Reproducible Neural Archite.pdf}
}

@inproceedings{yosinskiHowTransferableAre2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  booktitle = {Neural {{Information Processing Systems}} ({{NeurIPS}})},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  year = {2014},
  month = nov,
  eprint = {1411.1792},
  eprinttype = {arxiv},
  abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/konstantinschurholt/Zotero/storage/XJG9LYMW/Yosinski et al. - How transferable are features in deep neural netwo.pdf;/Users/konstantinschurholt/Zotero/storage/ZUSDRG4N/Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf}
}

@article{yosinskiUnderstandingNeuralNetworks2015,
  title = {Understanding {{Neural Networks Through Deep Visualization}}},
  author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  year = {2015},
  month = jun,
  journal = {arXiv:1506.06579 [cs]},
  eprint = {1506.06579},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recent years have produced great advances in training large, deep neural networks (DNNs), including notable successes in training convolutional neural networks (convnets) to recognize natural images. However, our understanding of how these models work, especially what computations they perform at intermediate layers, has lagged behind. Progress in the field will be further accelerated by the development of better tools for visualizing and interpreting neural nets. We introduce two such tools here. The first is a tool that visualizes the activations produced on each layer of a trained convnet as it processes an image or video (e.g. a live webcam stream). We have found that looking at live activations that change in response to user input helps build valuable intuitions about how convnets work. The second tool enables visualizing features at each layer of a DNN via regularized optimization in image space. Because previous versions of this idea produced less recognizable images, here we introduce several new regularization methods that combine to produce qualitatively clearer, more interpretable visualizations. Both tools are open source and work on a pretrained convnet with minimal setup.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/konstantinschurholt/Zotero/storage/5EZPILNK/Yosinski et al. - 2015 - Understanding Neural Networks Through Deep Visuali.pdf}
}

@article{youDesignSpaceGraph2021,
  title = {Design {{Space}} for {{Graph Neural Networks}}},
  author = {You, Jiaxuan and Ying, Rex and Leskovec, Jure},
  year = {2021},
  pages = {19},
  abstract = {The rapid evolution of Graph Neural Networks (GNNs) has led to a growing number of new architectures as well as novel applications. However, current research focuses on proposing and evaluating specific architectural designs of GNNs, such as GCN, GIN, or GAT, as opposed to studying the more general design space of GNNs that consists of a Cartesian product of different design dimensions, such as the number of layers or the type of the aggregation function. Additionally, GNN designs are often specialized to a single task, yet few efforts have been made to understand how to quickly find the best GNN design for a novel task or a novel dataset. Here we define and systematically study the architectural design space for GNNs which consists of 315,000 different designs over 32 different predictive tasks. Our approach features three key innovations: (1) A general GNN design space; (2) a GNN task space with a similarity metric, so that for a given novel task/dataset, we can quickly identify/transfer the best performing architecture; (3) an efficient and effective design space evaluation method which allows insights to be distilled from a huge number of model-task combinations. Our key results include: (1) A comprehensive set of guidelines for designing well-performing GNNs; (2) while best GNN designs for different tasks vary significantly, the GNN task space allows for transferring the best designs across different tasks; (3) models discovered using our design space achieve state-of-the-art performance. Overall, our work offers a principled and scalable approach to transition from studying individual GNN designs for specific tasks, to systematically studying the GNN design space and the task space. Finally, we release GraphGym, a powerful platform for exploring different GNN designs and tasks. GraphGym features modularized GNN implementation, standardized GNN evaluation, and reproducible and scalable experiment management.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/NDULB4TR/You et al. - Design Space for Graph Neural Networks.pdf}
}

@article{youGraphConvolutionalPolicy2018,
  title = {Graph {{Convolutional Policy Network}} for {{Goal-Directed Molecular Graph Generation}}},
  author = {You, Jiaxuan and Liu, Bowen},
  year = {2018},
  pages = {12},
  abstract = {Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. This is especially important in the task of molecular graph generation, whose goal is to discover novel molecules with desired properties such as drug-likeness and synthetic accessibility, while obeying physical laws such as chemical valency. However, designing models to find molecules that optimize desired properties while incorporating highly complex and non-differentiable rules remains to be a challenging task. Here we propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goaldirected graph generation through reinforcement learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules. Experimental results show that GCPN can achieve 61\% improvement on chemical property optimization over state-of-the-art baselines while resembling known molecules, and achieve 184\% improvement on the constrained property optimization task.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/PCMZSPYA/You and Liu - Graph Convolutional Policy Network for Goal-Direct.pdf}
}

@article{youGraphRNNGeneratingRealistic2018,
  title = {{{GraphRNN}}: {{Generating Realistic Graphs}} with {{Deep Auto-regressive Models}}},
  shorttitle = {{{GraphRNN}}},
  author = {You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William L. and Leskovec, Jure},
  year = {2018},
  month = jun,
  journal = {arXiv:1802.08773 [cs]},
  eprint = {1802.08773},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. In order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50\texttimes{} larger than previous deep models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,I.2.6},
  file = {/Users/konstantinschurholt/Zotero/storage/UGWDLIAR/You et al. - 2018 - GraphRNN Generating Realistic Graphs with Deep Au.pdf}
}

@article{youGraphStructureNeural2020,
  title = {Graph {{Structure}} of {{Neural Networks}}},
  author = {You, Jiaxuan and Leskovec, Jure and He, Kaiming and Xie, Saining},
  year = {2020},
  month = aug,
  journal = {arXiv:2007.06559 [cs, stat]},
  eprint = {2007.06559},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Neural networks are often represented as graphs of connections between neurons. However, despite their wide use, there is currently little understanding of the relationship between the graph structure of the neural network and its predictive performance. Here we systematically investigate how does the graph structure of neural networks affect their predictive performance. To this end, we develop a novel graph-based representation of neural networks called relational graph, where layers of neural network computation correspond to rounds of message exchange along the graph structure. Using this representation we show that: (1) a sweet spot of relational graphs leads to neural networks with significantly improved predictive performance; (2) neural networks performance is approximately a smooth function of the clustering coefficient and average path length of its relational graph; (3) our findings are consistent across many different tasks and datasets; (4) the sweet spot can be identified efficiently; (5) top-performing neural networks have graph structure surprisingly similar to those of real biological neural networks. Our work opens new directions for the design of neural architectures and the understanding on neural networks in general.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/TQYJGRL7/You et al. - 2020 - Graph Structure of Neural Networks.pdf}
}

@article{yuAutoGraphEncoderDecoder2020,
  title = {Auto {{Graph Encoder-Decoder}} for {{Model Compression}} and {{Network Acceleration}}},
  author = {Yu, Sixing and Mazaheri, Arya and Jannesari, Ali},
  year = {2020},
  month = dec,
  journal = {arXiv:2011.12641 [cs]},
  eprint = {2011.12641},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Model compression aims to deploy deep neural networks (DNN) to mobile devices with limited computing power and storage resource. However, most of the existing model compression methods rely on manually defined rules, which requires domain expertise. In this paper, we propose an auto graph encoder-decoder model compression (AGMC) method combined with graph neural networks (GNN) and reinforcement learning (RL) to find the best compression policy. We model the target DNN as a graph and use GNN to learn the embeddings of the DNN automatically. In the experiments, we first compared our method with rule-based DNN embedding methods to show the graph auto encoder-decoder's effectiveness. Our learning-based DNN embedding achieved better performance and a higher compression ratio with fewer search steps. Moreover, we evaluated the AGMC on CIFAR10 and ILSVRC-2012 datasets and compared handcrafted and learning-based model compression approaches. Our method outperformed handcrafted and learning-based methods on ResNet-56 with 3.6\% and 1.8\% higher accuracy, respectively. Furthermore, we achieved a higher compression ratio than state-of-the-art methods on MobileNet-V2 with just 0.93\% accuracy loss.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/XTZKKAUZ/Yu et al. - 2020 - Auto Graph Encoder-Decoder for Model Compression a.pdf}
}

@article{zbontarBarlowTwinsSelfSupervised2021,
  title = {Barlow {{Twins}}: {{Self-Supervised Learning}} via {{Redundancy Reduction}}},
  shorttitle = {Barlow {{Twins}}},
  author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, St{\'e}phane},
  year = {2021},
  month = jun,
  journal = {arXiv:2103.03230 [cs, q-bio]},
  eprint = {2103.03230},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  abstract = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/Users/konstantinschurholt/Zotero/storage/ERKVM52Z/Zbontar et al. - 2021 - Barlow Twins Self-Supervised Learning via Redunda.pdf}
}

@incollection{zeilerVisualizingUnderstandingConvolutional2014,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2014},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  year = {2014},
  volume = {8689},
  pages = {818--833},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-10590-1_53},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al. on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-ofthe-art results on Caltech-101 and Caltech-256 datasets.},
  isbn = {978-3-319-10589-5 978-3-319-10590-1},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/P8RMHYNG/Zeiler and Fergus - 2014 - Visualizing and Understanding Convolutional Networ.pdf}
}

@article{zhaiLiTZeroShotTransfer2022,
  title = {{{LiT}}: {{Zero-Shot Transfer}} with {{Locked-image}} Text {{Tuning}}},
  shorttitle = {{{LiT}}},
  author = {Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
  year = {2022},
  month = mar,
  journal = {arXiv:2111.07991 [cs]},
  eprint = {2111.07991},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This paper presents contrastive-tuning, a simple method employing contrastive training to align image and text models while still taking advantage of their pre-training. In our empirical study we find that locked pre-trained image models with unlocked text models work best. We call this instance of contrastive-tuning ``Locked-image Tuning'' (LiT), which just teaches a text model to read out good representations from a pre-trained image model for new tasks. A LiT model gains the capability of zero-shot transfer to new vision tasks, such as image classification or retrieval. The proposed LiT is widely applicable; it works reliably with multiple pre-training methods (supervised and unsupervised) and across diverse architectures (ResNet, Vision Transformers and MLP-Mixer) using three different imagetext datasets. With the transformer-based pre-trained ViTg/14 model, the LiT model achieves 84.5\% zero-shot transfer accuracy on the ImageNet test set, and 81.1\% on the challenging out-of-distribution ObjectNet test set.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/CG6ET6X4/Zhai et al. - 2022 - LiT Zero-Shot Transfer with Locked-image text Tun.pdf}
}

@inproceedings{zhangCanSubnetworkStructure2021,
  title = {Can {{Subnetwork Structure Be}} the {{Key}} to {{Out-of-Distribution Generalization}}?},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Zhang, Dinghuai and Ahuja, Kartik and Xu, Yilun and Wang, Yisen and Courville, Aaron},
  year = {2021},
  month = jul,
  pages = {12356--12367},
  publisher = {{PMLR}},
  abstract = {Can models with particular structure avoid being biased towards spurious correlation in out-of-distribution (OOD) generalization? Peters et al. (2016) provides a positive answer for linear cases. I...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/FZ3SFNF2/Zhang et al. - 2021 - Can Subnetwork Structure Be the Key to Out-of-Distribution Generalization.pdf}
}

@article{zhangColorfulImageColorization2016,
  title = {Colorful {{Image Colorization}}},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
  year = {2016},
  month = oct,
  journal = {arXiv:1603.08511 [cs]},
  eprint = {1603.08511},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a ``colorization Turing test,'' asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32\% of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/ACD8MBJW/Zhang et al. - 2016 - Colorful Image Colorization.pdf}
}

@inproceedings{zhangGraphHyperNetworksNeural2019,
  title = {Graph {{HyperNetworks}} for {{Neural Architecture Search}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Zhang, Chris and Ren, Mengye and Urtasun, Raquel},
  year = {2019},
  eprint = {1810.05749},
  eprinttype = {arxiv},
  abstract = {Neural architecture search (NAS) automatically finds the best task-specific neural network topology, outperforming many manual architecture designs. However, it can be prohibitively expensive as the search requires training thousands of different networks, while each can last for hours. In this work, we propose the Graph HyperNetwork (GHN) to amortize the search cost: given an architecture, it directly generates the weights by running inference on a graph neural network. GHNs model the topology of an architecture and therefore can predict network performance more accurately than regular hypernetworks and premature early stopping. To perform NAS, we randomly sample architectures and use the validation accuracy of networks with GHN generated weights as the surrogate search signal. GHNs are fast \textendash{} they can search nearly 10\texttimes{} faster than other random search methods on CIFAR-10 and ImageNet. GHNs can be further extended to the anytime prediction setting, where they have found networks with better speed-accuracy tradeoff than the state-of-the-art manual designs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/IHJUXCZY/Zhang et al. - 2020 - Graph HyperNetworks for Neural Architecture Search.pdf}
}

@inproceedings{zhmoginovHyperTransformerModelGeneration2022,
  title = {{{HyperTransformer}}: {{Model Generation}} for {{Supervised}} and {{Semi-Supervised Few-Shot Learning}}},
  shorttitle = {{{HyperTransformer}}},
  booktitle = {International {{Conference}} on {{Machine Learning}} ({{ICML}})},
  author = {Zhmoginov, Andrey and Sandler, Mark and Vladymyrov, Max},
  year = {2022},
  month = jan,
  eprint = {2201.04182},
  eprinttype = {arxiv},
  abstract = {In this work we propose a HyperTransformer, a transformer-based model for fewshot learning that generates weights of a convolutional neural network (CNN) directly from support samples. Since the dependence of a small generated CNN model on a specific task is encoded by a high-capacity transformer model, we effectively decouple the complexity of the large task space from the complexity of individual tasks. Our method is particularly effective for small target CNN architectures where learning a fixed universal task-independent embedding is not optimal and better performance is attained when the information about the task can modulate all model parameters. For larger models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while being end-to-end differentiable. Finally, we extend our approach to a semi-supervised regime utilizing unlabeled samples in the support set and further improving few-shot performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/QCKSB88C/Zhmoginov et al. - 2022 - HyperTransformer Model Generation for Supervised .pdf}
}

@inproceedings{zhouDiverseEnsembleEvolution2018,
  title = {Diverse {{Ensemble Evolution}}: {{Curriculum Data-Model Marriage}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Zhou, Tianyi and Wang, Shengjie and Bilmes, Jeff A},
  year = {2018},
  pages = {12},
  abstract = {We study a new method ``Diverse Ensemble Evolution (DivE2)'' to train an ensemble of machine learning models that assigns data to models at each training epoch based on each model's current expertise and an intra- and inter-model diversity reward. DivE2 schedules, over the course of training epochs, the relative importance of these characteristics; it starts by selecting easy samples for each model, and then gradually adjusts towards the models having specialized and complementary expertise on subsets of the training data, thereby encouraging high accuracy of the ensemble. We utilize an intra-model diversity term on data assigned to each model, and an inter-model diversity term on data assigned to pairs of models, to penalize both within-model and cross-model redundancy. We formulate the data-model marriage problem as a generalized bipartite matching, represented as submodular maximization subject to two matroid constraints. DivE2 solves a sequence of continuous-combinatorial optimizations with slowly varying objectives and constraints. The combinatorial part handles the data-model marriage while the continuous part updates model parameters based on the assignments. In experiments, DivE2 outperforms other ensemble training methods under a variety of model aggregation techniques, while also maintaining competitive efficiency.},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/M55FE86T/Zhou et al. - Diverse Ensemble Evolution Curriculum Data-Model .pdf}
}

@article{zhouJittorGANFasttrainingGenerative2021,
  title = {Jittor-{{GAN}}: {{A}} Fast-Training Generative Adversarial Network Model Zoo Based on {{Jittor}}},
  shorttitle = {Jittor-{{GAN}}},
  author = {Zhou, Wen-Yang and Yang, Guo-Wei and Hu, Shi-Min},
  year = {2021},
  month = mar,
  journal = {Computational Visual Media},
  volume = {7},
  number = {1},
  pages = {153--157},
  issn = {2096-0433, 2096-0662},
  doi = {10.1007/s41095-021-0203-2},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/A8DEWS9C/Zhou et al. - 2021 - Jittor-GAN A fast-training generative adversarial.pdf}
}

@inproceedings{zhuangComprehensiveSurveyTransfer2020,
  title = {A {{Comprehensive Survey}} on {{Transfer Learning}}},
  booktitle = {Proceedings of {{IEEE}}},
  author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  year = {2020},
  eprint = {1911.02685},
  eprinttype = {arxiv},
  abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/konstantinschurholt/Zotero/storage/92A2TTWY/Zhuang et al. - 2020 - A Comprehensive Survey on Transfer Learning.pdf}
}

@inproceedings{zimmermannContrastiveLearningInverts2021a,
  title = {Contrastive {{Learning Inverts}} the {{Data Generating Process}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
  year = {2021},
  month = jul,
  pages = {12979--12990},
  publisher = {{PMLR}},
  abstract = {Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large va...},
  langid = {english},
  file = {/Users/konstantinschurholt/Zotero/storage/ZYUK59VH/Zimmermann et al. - 2021 - Contrastive Learning Inverts the Data Generating Process.pdf}
}

@article{zintgrafVisualizingDeepNeural2017,
  title = {Visualizing {{Deep Neural Network Decisions}}: {{Prediction Difference Analysis}}},
  shorttitle = {Visualizing {{Deep Neural Network Decisions}}},
  author = {Zintgraf, Luisa M. and Cohen, Taco S. and Adel, Tameem and Welling, Max},
  year = {2017},
  month = feb,
  journal = {arXiv:1702.04595 [cs]},
  eprint = {1702.04595},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/konstantinschurholt/Zotero/storage/PRW9A7P7/Zintgraf et al. - 2017 - Visualizing Deep Neural Network Decisions Predict.pdf}
}


