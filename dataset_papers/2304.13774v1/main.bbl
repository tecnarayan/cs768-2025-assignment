\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akgun et~al.(2012)Akgun, Cakmak, Jiang, and Thomaz]{akgun2012keyframe}
Akgun, B., Cakmak, M., Jiang, K., and Thomaz, A.~L.
\newblock Keyframe-based learning from demonstration.
\newblock \emph{International Journal of Social Robotics}, 4\penalty0
  (4):\penalty0 343--355, 2012.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Pieter~Abbeel, and
  Zaremba]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Pieter~Abbeel, O., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Argall et~al.(2009)Argall, Chernova, Veloso, and
  Browning]{argall2009survey}
Argall, B.~D., Chernova, S., Veloso, M., and Browning, B.
\newblock A survey of robot learning from demonstration.
\newblock \emph{Robotics and autonomous systems}, 57\penalty0 (5):\penalty0
  469--483, 2009.

\bibitem[Beliaev et~al.(2022)Beliaev, Shih, Ermon, Sadigh, and
  Pedarsani]{pmlr-v162-beliaev22a}
Beliaev, M., Shih, A., Ermon, S., Sadigh, D., and Pedarsani, R.
\newblock Imitation learning by estimating expertise of demonstrators.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and
  Sabato, S. (eds.), \emph{Proceedings of the 39th International Conference on
  Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pp.\  1732--1748. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/beliaev22a.html}.

\bibitem[Belkhale \& Sadigh(2022)Belkhale and Sadigh]{belkhale2022plato}
Belkhale, S. and Sadigh, D.
\newblock {PLATO}: Predicting latent affordances through object-centric play.
\newblock In \emph{6th Annual Conference on Robot Learning}, 2022.
\newblock URL \url{https://openreview.net/forum?id=UAA5bNospA0}.

\bibitem[Bertsekas \& Tsitsiklis(1991)Bertsekas and
  Tsitsiklis]{bertsekas1991analysis}
Bertsekas, D.~P. and Tsitsiklis, J.~N.
\newblock An analysis of stochastic shortest path problems.
\newblock \emph{Mathematics of Operations Research}, 16\penalty0 (3):\penalty0
  580--595, 1991.

\bibitem[Brohan et~al.(2022)Brohan, Brown, Carbajal, Chebotar, Dabis, Finn,
  Gopalakrishnan, Hausman, Herzog, Hsu, Ibarz, Ichter, Irpan, Jackson,
  Jesmonth, Joshi, Julian, Kalashnikov, Kuang, Leal, Lee, Levine, Lu, Malla,
  Manjunath, Mordatch, Nachum, Parada, Peralta, Perez, Pertsch, Quiambao, Rao,
  Ryoo, Salazar, Sanketi, Sayed, Singh, Sontakke, Stone, Tan, Tran, Vanhoucke,
  Vega, Vuong, Xia, Xiao, Xu, Xu, Yu, and Zitkovich]{rt12022arxiv}
Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C.,
  Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., Ibarz, J., Ichter, B.,
  Irpan, A., Jackson, T., Jesmonth, S., Joshi, N., Julian, R., Kalashnikov, D.,
  Kuang, Y., Leal, I., Lee, K.-H., Levine, S., Lu, Y., Malla, U., Manjunath,
  D., Mordatch, I., Nachum, O., Parada, C., Peralta, J., Perez, E., Pertsch,
  K., Quiambao, J., Rao, K., Ryoo, M., Salazar, G., Sanketi, P., Sayed, K.,
  Singh, J., Sontakke, S., Stone, A., Tan, C., Tran, H., Vanhoucke, V., Vega,
  S., Vuong, Q., Xia, F., Xiao, T., Xu, P., Xu, S., Yu, T., and Zitkovich, B.
\newblock Rt-1: Robotics transformer for real-world control at scale.
\newblock In \emph{arXiv preprint arXiv:2212.06817}, 2022.

\bibitem[Cao \& Sadigh(2021)Cao and Sadigh]{cao2021learning}
Cao, Z. and Sadigh, D.
\newblock Learning from imperfect demonstrations from agents with varying
  dynamics.
\newblock \emph{IEEE Robotics and Automation Letters}, 6\penalty0 (3):\penalty0
  5231--5238, 2021.

\bibitem[Chebotar et~al.(2021)Chebotar, Hausman, Lu, Xiao, Kalashnikov, Varley,
  Irpan, Eysenbach, Julian, Finn, et~al.]{chebotar2021actionable}
Chebotar, Y., Hausman, K., Lu, Y., Xiao, T., Kalashnikov, D., Varley, J.,
  Irpan, A., Eysenbach, B., Julian, R.~C., Finn, C., et~al.
\newblock Actionable models: Unsupervised offline reinforcement learning of
  robotic skills.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1518--1528. PMLR, 2021.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{chen2021decision}
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P.,
  Srinivas, A., and Mordatch, I.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 15084--15097, 2021.

\bibitem[Cui et~al.(2022)Cui, Wang, Muhammad, Pinto, et~al.]{cui2022play}
Cui, Z.~J., Wang, Y., Muhammad, N., Pinto, L., et~al.
\newblock From play to policy: Conditional behavior generation from uncurated
  robot data.
\newblock \emph{arXiv preprint arXiv:2210.10047}, 2022.

\bibitem[Ding et~al.(2019)Ding, Florensa, Abbeel, and Phielipp]{ding2019goal}
Ding, Y., Florensa, C., Abbeel, P., and Phielipp, M.
\newblock Goal-conditioned imitation learning.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Duan et~al.(2017)Duan, Andrychowicz, Stadie, Jonathan~Ho, Schneider,
  Sutskever, Abbeel, and Zaremba]{duan2017one}
Duan, Y., Andrychowicz, M., Stadie, B., Jonathan~Ho, O., Schneider, J.,
  Sutskever, I., Abbeel, P., and Zaremba, W.
\newblock One-shot imitation learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Emmons et~al.(2022)Emmons, Eysenbach, Kostrikov, and
  Levine]{emmons2022rvs}
Emmons, S., Eysenbach, B., Kostrikov, I., and Levine, S.
\newblock Rvs: What is essential for offline {RL} via supervised learning?
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=S874XAIpkR-}.

\bibitem[Eysenbach et~al.(2019)Eysenbach, Salakhutdinov, and
  Levine]{eysenbach2019search}
Eysenbach, B., Salakhutdinov, R.~R., and Levine, S.
\newblock Search on the replay buffer: Bridging planning and reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Eysenbach et~al.(2020{\natexlab{a}})Eysenbach, Geng, Levine, and
  Salakhutdinov]{eysenbach2020rewriting}
Eysenbach, B., Geng, X., Levine, S., and Salakhutdinov, R.~R.
\newblock Rewriting history with inverse rl: Hindsight inference for policy
  improvement.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 14783--14795, 2020{\natexlab{a}}.

\bibitem[Eysenbach et~al.(2020{\natexlab{b}})Eysenbach, Salakhutdinov, and
  Levine]{eysenbach2020c}
Eysenbach, B., Salakhutdinov, R., and Levine, S.
\newblock C-learning: Learning to achieve goals via recursive classification.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.

\bibitem[Fang et~al.()Fang, Yin, Nair, Walke, Yan, and
  Levine]{fanggeneralization}
Fang, K., Yin, P., Nair, A., Walke, H.~R., Yan, G., and Levine, S.
\newblock Generalization with lossy affordances: Leveraging broad offline data
  for learning visuomotor tasks.
\newblock In \emph{6th Annual Conference on Robot Learning}.

\bibitem[Finn et~al.(2016)Finn, Tan, Duan, Darrell, Levine, and
  Abbeel]{finn2016deep}
Finn, C., Tan, X.~Y., Duan, Y., Darrell, T., Levine, S., and Abbeel, P.
\newblock Deep spatial autoencoders for visuomotor learning.
\newblock In \emph{2016 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  512--519. IEEE, 2016.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock D4rl: Datasets for deep data-driven reinforcement learning, 2020.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{fujimoto2021a}
Fujimoto, S. and Gu, S.
\newblock A minimalist approach to offline reinforcement learning.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Q32U7dzWXpc}.

\bibitem[Garg et~al.(2023)Garg, Hejna, Geist, and Ermon]{garg2023extreme}
Garg, D., Hejna, J., Geist, M., and Ermon, S.
\newblock Extreme q-learning: Maxent reinforcement learning without entropy.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://arxiv.org/abs/2301.02328}.

\bibitem[Ghosh et~al.(2021)Ghosh, Gupta, Reddy, Fu, Devin, Eysenbach, and
  Levine]{ghosh2021learning}
Ghosh, D., Gupta, A., Reddy, A., Fu, J., Devin, C.~M., Eysenbach, B., and
  Levine, S.
\newblock Learning to reach goals via iterated supervised learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=rALA0Xo6yNJ}.

\bibitem[Gupta et~al.(2020)Gupta, Kumar, Lynch, Levine, and
  Hausman]{gupta2020relay}
Gupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman, K.
\newblock Relay policy learning: Solving long-horizon tasks via imitation and
  reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pp.\  1025--1037. PMLR, 2020.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International conference on machine learning}, pp.\
  1352--1361. PMLR, 2017.

\bibitem[Hartikainen et~al.(2020)Hartikainen, Geng, Haarnoja, and
  Levine]{Hartikainen2020Dynamical}
Hartikainen, K., Geng, X., Haarnoja, T., and Levine, S.
\newblock Dynamical distance learning for semi-supervised and unsupervised
  skill discovery.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=H1lmhaVtvr}.

\bibitem[Jang et~al.(2022)Jang, Irpan, Khansari, Kappler, Ebert, Lynch, Levine,
  and Finn]{jang2022bc}
Jang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine,
  S., and Finn, C.
\newblock Bc-z: Zero-shot task generalization with robotic imitation learning.
\newblock In \emph{Conference on Robot Learning}, pp.\  991--1002. PMLR, 2022.

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{janner2021offline}
Janner, M., Li, Q., and Levine, S.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 1273--1286, 2021.

\bibitem[Kaelbling(1993)]{kaelbling1993learning}
Kaelbling, L.~P.
\newblock Learning to achieve goals.
\newblock In \emph{IJCAI}, volume~2, pp.\  1094--8. Citeseer, 1993.

\bibitem[Kostrikov et~al.(2022)Kostrikov, Nair, and
  Levine]{kostrikov2022offline}
Kostrikov, I., Nair, A., and Levine, S.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=68n2s9ZJWF8}.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020.

\bibitem[Li et~al.(2020)Li, Pinto, and Abbeel]{li2020generalized}
Li, A., Pinto, L., and Abbeel, P.
\newblock Generalized hindsight for reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 7754--7767, 2020.

\bibitem[Lynch et~al.(2019)Lynch, Khansari, Xiao, Kumar, Tompson, Levine, and
  Sermanet]{lynch2019play}
Lynch, C., Khansari, M., Xiao, T., Kumar, V., Tompson, J., Levine, S., and
  Sermanet, P.
\newblock Learning latent plans from play.
\newblock \emph{Conference on Robot Learning (CoRL)}, 2019.
\newblock URL \url{https://arxiv.org/abs/1903.01973}.

\bibitem[Ma et~al.(2022)Ma, Yan, Jayaraman, and Bastani]{ma2022offline}
Ma, Y.~J., Yan, J., Jayaraman, D., and Bastani, O.
\newblock Offline goal-conditioned reinforcement learning via \$f\$-advantage
  regression.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=_h29VprPHD}.

\bibitem[Mandlekar et~al.(2021)Mandlekar, Xu, Wong, Nasiriany, Wang, Kulkarni,
  Fei-Fei, Savarese, Zhu, and Mart\'{i}n-Mart\'{i}n]{robomimic2021}
Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C., Kulkarni, R.,
  Fei-Fei, L., Savarese, S., Zhu, Y., and Mart\'{i}n-Mart\'{i}n, R.
\newblock What matters in learning from offline human demonstrations for robot
  manipulation.
\newblock In \emph{arXiv preprint arXiv:2108.03298}, 2021.

\bibitem[Nair et~al.(2020)Nair, Gupta, Dalal, and Levine]{nair2020awac}
Nair, A., Gupta, A., Dalal, M., and Levine, S.
\newblock Awac: Accelerating online reinforcement learning with offline
  datasets.
\newblock \emph{arXiv preprint arXiv:2006.09359}, 2020.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{AWRPeng19}
Peng, X.~B., Kumar, A., Zhang, G., and Levine, S.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock \emph{CoRR}, abs/1910.00177, 2019.
\newblock URL \url{https://arxiv.org/abs/1910.00177}.

\bibitem[Pertsch et~al.(2021)Pertsch, Lee, and Lim]{pertsch2021accelerating}
Pertsch, K., Lee, Y., and Lim, J.
\newblock Accelerating reinforcement learning with learned skill priors.
\newblock In \emph{Conference on robot learning}, pp.\  188--204. PMLR, 2021.

\bibitem[Peters \& Schaal(2007)Peters and Schaal]{peters2007reinforcement}
Peters, J. and Schaal, S.
\newblock Reinforcement learning by reward-weighted regression for operational
  space control.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pp.\  745--750, 2007.

\bibitem[Plappert et~al.(2018)Plappert, Andrychowicz, Ray, McGrew, Baker,
  Powell, Schneider, Tobin, Chociej, Welinder, Kumar, and
  Zaremba]{DBLP:journals/corr/abs-1802-09464}
Plappert, M., Andrychowicz, M., Ray, A., McGrew, B., Baker, B., Powell, G.,
  Schneider, J., Tobin, J., Chociej, M., Welinder, P., Kumar, V., and Zaremba,
  W.
\newblock Multi-goal reinforcement learning: Challenging robotics environments
  and request for research.
\newblock \emph{CoRR}, abs/1802.09464, 2018.
\newblock URL \url{http://arxiv.org/abs/1802.09464}.

\bibitem[Rosete-Beas et~al.()Rosete-Beas, Mees, Kalweit, Boedecker, and
  Burgard]{rosetelatent}
Rosete-Beas, E., Mees, O., Kalweit, G., Boedecker, J., and Burgard, W.
\newblock Latent plans for task-agnostic offline reinforcement learning.
\newblock In \emph{6th Annual Conference on Robot Learning}.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor, and
  Silver]{pmlr-v37-schaul15}
Schaul, T., Horgan, D., Gregor, K., and Silver, D.
\newblock Universal value function approximators.
\newblock In Bach, F. and Blei, D. (eds.), \emph{Proceedings of the 32nd
  International Conference on Machine Learning}, volume~37 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1312--1320, Lille, France, 07--09 Jul
  2015. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v37/schaul15.html}.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897. PMLR, 2015.

\bibitem[Srivastava et~al.(2019)Srivastava, Shyam, Mutz, Ja{\'s}kowski, and
  Schmidhuber]{srivastava2019training}
Srivastava, R.~K., Shyam, P., Mutz, F., Ja{\'s}kowski, W., and Schmidhuber, J.
\newblock Training agents using upside-down reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.02877}, 2019.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tian et~al.(2020)Tian, Nair, Ebert, Dasari, Eysenbach, Finn, and
  Levine]{tian2020model}
Tian, S., Nair, S., Ebert, F., Dasari, S., Eysenbach, B., Finn, C., and Levine,
  S.
\newblock Model-based visual planning with self-supervised functional
  distances.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Van~Hasselt et~al.(2018)Van~Hasselt, Doron, Strub, Hessel, Sonnerat,
  and Modayil]{van2018deep}
Van~Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil,
  J.
\newblock Deep reinforcement learning and the deadly triad.
\newblock \emph{arXiv preprint arXiv:1812.02648}, 2018.

\bibitem[Wang et~al.(2018)Wang, Xiong, Han, Liu, Zhang,
  et~al.]{wang2018exponentially}
Wang, Q., Xiong, J., Han, L., Liu, H., Zhang, T., et~al.
\newblock Exponentially weighted imitation learning for batched historical
  data.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Yang et~al.(2022)Yang, Lu, Li, Sun, Fang, Du, Li, Han, and
  Zhang]{yang2022rethinking}
Yang, R., Lu, Y., Li, W., Sun, H., Fang, M., Du, Y., Li, X., Han, L., and
  Zhang, C.
\newblock Rethinking goal-conditioned supervised learning and its connection to
  offline {RL}.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=KJztlfGPdwW}.

\bibitem[Yarats et~al.(2021)Yarats, Zhang, Kostrikov, Amos, Pineau, and
  Fergus]{yarats2021improving}
Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R.
\newblock Improving sample efficiency in model-free reinforcement learning from
  images.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  10674--10681, 2021.

\bibitem[Yarats et~al.(2022)Yarats, Brandfonbrener, Liu, Laskin, Abbeel,
  Lazaric, and Pinto]{yarats2022exorl}
Yarats, D., Brandfonbrener, D., Liu, H., Laskin, M., Abbeel, P., Lazaric, A.,
  and Pinto, L.
\newblock Don't change the algorithm, change the data: Exploratory data for
  offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2201.13425}, 2022.

\bibitem[Zhang et~al.(2021)Zhang, Cao, Sadigh, and Sui]{NEURIPS2021_670e8a43}
Zhang, S., Cao, Z., Sadigh, D., and Sui, Y.
\newblock Confidence-aware imitation learning from demonstrations with varying
  optimality.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
  J.~W. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  12340--12350. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/670e8a43b246801ca1eaca97b3e19189-Paper.pdf}.

\bibitem[Ziebart(2010)]{ziebart2010modeling}
Ziebart, B.~D.
\newblock \emph{Modeling purposeful adaptive behavior with the principle of
  maximum causal entropy}.
\newblock Carnegie Mellon University, 2010.

\end{thebibliography}
