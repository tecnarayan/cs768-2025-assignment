\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alacaoglu et~al.(2017)Alacaoglu, Dinh, Fercoq, and
  Cevher]{alacaoglu2017smooth}
Alacaoglu, A., Dinh, Q.~T., Fercoq, O., and Cevher, V.
\newblock Smooth primal-dual coordinate descent algorithms for nonsmooth convex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5852--5861, 2017.

\bibitem[Alacaoglu et~al.(2019)Alacaoglu, Fercoq, and
  Cevher]{alacaoglu2019convergence}
Alacaoglu, A., Fercoq, O., and Cevher, V.
\newblock On the convergence of stochastic primal-dual hybrid gradient.
\newblock \emph{arXiv preprint arXiv:1911.00799}, 2019.

\bibitem[Arrow et~al.(1958)Arrow, Azawa, Hurwicz, and Uzawa]{arrow1958studies}
Arrow, K.~J., Azawa, H., Hurwicz, L., and Uzawa, H.
\newblock \emph{Studies in linear and non-linear programming}, volume~2.
\newblock Stanford University Press, 1958.

\bibitem[Bauschke \& Combettes(2011)Bauschke and Combettes]{bauschke2011convex}
Bauschke, H.~H. and Combettes, P.~L.
\newblock \emph{Convex analysis and monotone operator theory in Hilbert
  spaces}.
\newblock Springer, 2011.

\bibitem[Chambolle \& Pock(2011)Chambolle and Pock]{chambolle2011first}
Chambolle, A. and Pock, T.
\newblock A first-order primal-dual algorithm for convex problems with
  applications to imaging.
\newblock \emph{Journal of mathematical imaging and vision}, 40\penalty0
  (1):\penalty0 120--145, 2011.

\bibitem[Chambolle et~al.(2018)Chambolle, Ehrhardt, Richt{\'a}rik, and
  Schonlieb]{chambolle2018stochastic}
Chambolle, A., Ehrhardt, M.~J., Richt{\'a}rik, P., and Schonlieb, C.-B.
\newblock Stochastic primal-dual hybrid gradient algorithm with arbitrary
  sampling and imaging applications.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (4):\penalty0
  2783--2808, 2018.

\bibitem[Chang \& Lin(2011)Chang and Lin]{CC01a}
Chang, C.-C. and Lin, C.-J.
\newblock {LIBSVM}: A library for support vector machines.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
  2:\penalty0 27:1--27:27, 2011.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem[Combettes \& Pesquet(2015)Combettes and
  Pesquet]{combettes2015stochastic}
Combettes, P.~L. and Pesquet, J.-C.
\newblock Stochastic quasi-fej{\'e}r block-coordinate fixed point iterations
  with random sweeping.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  1221--1248, 2015.

\bibitem[Combettes \& Pesquet(2019)Combettes and
  Pesquet]{combettes2019stochastic}
Combettes, P.~L. and Pesquet, J.-C.
\newblock Stochastic quasi-fej{\'e}r block-coordinate fixed point iterations
  with random sweeping ii: mean-square and linear convergence.
\newblock \emph{Mathematical Programming}, 174\penalty0 (1-2):\penalty0
  433--451, 2019.

\bibitem[Condat(2013)]{condat2013primal}
Condat, L.
\newblock A primal--dual splitting method for convex optimization involving
  lipschitzian, proximable and linear composite terms.
\newblock \emph{Journal of Optimization Theory and Applications}, 158\penalty0
  (2):\penalty0 460--479, 2013.

\bibitem[Dang \& Lan(2014)Dang and Lan]{dang2014randomized}
Dang, C. and Lan, G.
\newblock Randomized methods for saddle point computation.
\newblock \emph{arXiv preprint arXiv:1409.8625}, 3\penalty0 (4), 2014.

\bibitem[Dang \& Lan(2015)Dang and Lan]{dang2015stochastic}
Dang, C.~D. and Lan, G.
\newblock Stochastic block mirror descent methods for nonsmooth and stochastic
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  856--881, 2015.

\bibitem[Fercoq(2019)]{fercoq2019generic}
Fercoq, O.
\newblock A generic coordinate descent solver for non-smooth convex
  optimisation.
\newblock \emph{Optimization Methods and Software}, pp.\  1--21, 2019.

\bibitem[Fercoq \& Bianchi(2019)Fercoq and Bianchi]{fercoq2019coordinate}
Fercoq, O. and Bianchi, P.
\newblock A coordinate-descent primal-dual algorithm with large step size and
  possibly nonseparable functions.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (1):\penalty0
  100--134, 2019.

\bibitem[Fercoq \& Richt{\'a}rik(2015)Fercoq and
  Richt{\'a}rik]{fercoq2015accelerated}
Fercoq, O. and Richt{\'a}rik, P.
\newblock Accelerated, parallel, and proximal coordinate descent.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (4):\penalty0
  1997--2023, 2015.

\bibitem[Fercoq et~al.(2019)Fercoq, Alacaoglu, Necoara, and
  Cevher]{fercoq2019almost}
Fercoq, O., Alacaoglu, A., Necoara, I., and Cevher, V.
\newblock Almost surely constrained convex optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1910--1919, 2019.

\bibitem[Gao et~al.(2019)Gao, Xu, and Zhang]{gao2019randomized}
Gao, X., Xu, Y.-Y., and Zhang, S.-Z.
\newblock Randomized primal--dual proximal block coordinate updates.
\newblock \emph{Journal of the Operations Research Society of China},
  7\penalty0 (2):\penalty0 205--250, 2019.

\bibitem[Iutzeler et~al.(2013)Iutzeler, Bianchi, Ciblat, and
  Hachem]{iutzeler2013asynchronous}
Iutzeler, F., Bianchi, P., Ciblat, P., and Hachem, W.
\newblock Asynchronous distributed optimization using a randomized alternating
  direction method of multipliers.
\newblock In \emph{52nd IEEE conference on decision and control}, pp.\
  3671--3676. IEEE, 2013.

\bibitem[Latafat et~al.(2019)Latafat, Freris, and Patrinos]{latafat2019new}
Latafat, P., Freris, N.~M., and Patrinos, P.
\newblock A new randomized block-coordinate primal-dual proximal algorithm for
  distributed optimization.
\newblock \emph{arXiv preprint arXiv:1706.02882v4}, 2019.

\bibitem[Liang et~al.(2016)Liang, Fadili, and Peyr{\'e}]{liang2016convergence}
Liang, J., Fadili, J., and Peyr{\'e}, G.
\newblock Convergence rates with inexact non-expansive operators.
\newblock \emph{Mathematical Programming}, 159\penalty0 (1-2):\penalty0
  403--434, 2016.

\bibitem[Luke \& Malitsky(2018)Luke and Malitsky]{luke2018block}
Luke, D.~R. and Malitsky, Y.
\newblock Block-coordinate primal-dual method for nonsmooth minimization over
  linear constraints.
\newblock In \emph{Large-Scale and Distributed Optimization}, pp.\  121--147.
  Springer, 2018.

\bibitem[Nesterov(2005)]{nesterov2005smooth}
Nesterov, Y.
\newblock Smooth minimization of non-smooth functions.
\newblock \emph{Mathematical programming}, 103\penalty0 (1):\penalty0 127--152,
  2005.

\bibitem[Nesterov(2012)]{nesterov2012efficiency}
Nesterov, Y.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  341--362, 2012.

\bibitem[Patrascu \& Necoara(2017)Patrascu and
  Necoara]{patrascu2017nonasymptotic}
Patrascu, A. and Necoara, I.
\newblock Nonasymptotic convergence of stochastic proximal point methods for
  constrained convex optimization.
\newblock \emph{Journal of Machine Learning Research}, 18:\penalty0 198--1,
  2017.

\bibitem[Pesquet \& Repetti(2015)Pesquet and Repetti]{pesquet2015class}
Pesquet, J.-C. and Repetti, A.
\newblock A class of randomized primal-dual algorithms for distributed
  optimization.
\newblock \emph{Journal of Nonlinear and Convex Analysis}, 16\penalty0
  (12):\penalty0 2453--2490, 2015.

\bibitem[Richt{\'a}rik \& Tak{\'a}{\v{c}}(2014)Richt{\'a}rik and
  Tak{\'a}{\v{c}}]{richtarik2014iteration}
Richt{\'a}rik, P. and Tak{\'a}{\v{c}}, M.
\newblock Iteration complexity of randomized block-coordinate descent methods
  for minimizing a composite function.
\newblock \emph{Mathematical Programming}, 144\penalty0 (1-2):\penalty0 1--38,
  2014.

\bibitem[Shalev-Shwartz \& Zhang(2013)Shalev-Shwartz and
  Zhang]{shalev2013stochastic}
Shalev-Shwartz, S. and Zhang, T.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0
  (Feb):\penalty0 567--599, 2013.

\bibitem[Shalev-Shwartz \& Zhang(2014)Shalev-Shwartz and
  Zhang]{shalev2014accelerated}
Shalev-Shwartz, S. and Zhang, T.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  64--72,
  2014.

\bibitem[Tan et~al.(2020)Tan, Qian, Ma, and Zhang]{tan2020accelerated}
Tan, C., Qian, Y., Ma, S., and Zhang, T.
\newblock Accelerated dual-averaging primal-dual method for composite convex
  minimization.
\newblock \emph{Optimization Methods and Software}, 0\penalty0 (0):\penalty0
  1--26, 2020.
\newblock \doi{10.1080/10556788.2020.1713779}.

\bibitem[Tran-Dinh et~al.(2018)Tran-Dinh, Fercoq, and Cevher]{tran2018smooth}
Tran-Dinh, Q., Fercoq, O., and Cevher, V.
\newblock A smooth primal-dual optimization framework for nonsmooth composite
  convex minimization.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (1):\penalty0
  96--134, 2018.

\bibitem[V{\~u}(2013)]{vu2013splitting}
V{\~u}, B.~C.
\newblock A splitting algorithm for dual monotone inclusions involving
  cocoercive operators.
\newblock \emph{Advances in Computational Mathematics}, 38\penalty0
  (3):\penalty0 667--681, 2013.

\bibitem[Xiao \& Zhang(2014)Xiao and Zhang]{xiao2014proximal}
Xiao, L. and Zhang, T.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (4):\penalty0
  2057--2075, 2014.

\bibitem[Xu \& Zhang(2018)Xu and Zhang]{xu2018accelerated}
Xu, Y. and Zhang, S.
\newblock Accelerated primal--dual proximal block coordinate updating methods
  for constrained convex optimization.
\newblock \emph{Computational Optimization and Applications}, 70\penalty0
  (1):\penalty0 91--128, 2018.

\bibitem[Zhang \& Xiao(2017)Zhang and Xiao]{zhang2017stochastic}
Zhang, Y. and Xiao, L.
\newblock Stochastic primal-dual coordinate method for regularized empirical
  risk minimization.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 2939--2980, 2017.

\bibitem[Zhou et~al.(2018)Zhou, Shang, and Cheng]{zhou2018simple}
Zhou, K., Shang, F., and Cheng, J.
\newblock A simple stochastic variance reduced algorithm with fast convergence
  rates.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5975--5984, 2018.

\end{thebibliography}
