\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Addepalli et~al.(2022)Addepalli, Jain, Sriramanan, and
  Venkatesh~Babu]{addepalli2022scaling}
Sravanti Addepalli, Samyak Jain, Gaurang Sriramanan, and R~Venkatesh~Babu.
\newblock Scaling adversarial training to large perturbation bounds.
\newblock In \emph{ECCV}, 2022.

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and
  Wagner]{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In \emph{ICML}, 2018.

\bibitem[Bai et~al.(2021)Bai, Zeng, Jiang, Xia, Ma, and Wang]{bai2021improving}
Yang Bai, Yuyuan Zeng, Yong Jiang, Shu-Tao Xia, Xingjun Ma, and Yisen Wang.
\newblock Improving adversarial robustness via channel-wise activation
  suppressing.
\newblock In \emph{ICLR}, 2021.

\bibitem[Balcan et~al.(2023)Balcan, Pukdee, Ravikumar, and
  Zhang]{balcan2023nash}
Maria-Florina Balcan, Rattana Pukdee, Pradeep Ravikumar, and Hongyang Zhang.
\newblock Nash equilibria and pitfalls of adversarial training in adversarial
  robustness games.
\newblock In \emph{AISTATS}, 2023.

\bibitem[Bose et~al.(2020)Bose, Gidel, Berard, Cianflone, Vincent,
  Lacoste-Julien, and Hamilton]{bose2020adversarial}
Joey Bose, Gauthier Gidel, Hugo Berard, Andre Cianflone, Pascal Vincent, Simon
  Lacoste-Julien, and Will Hamilton.
\newblock Adversarial example games.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Bul{\`o} et~al.(2016)Bul{\`o}, Biggio, Pillai, Pelillo, and
  Roli]{bulo2016randomized}
Samuel~Rota Bul{\`o}, Battista Biggio, Ignazio Pillai, Marcello Pelillo, and
  Fabio Roli.
\newblock Randomized prediction games for adversarial machine learning.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  28\penalty0 (11):\penalty0 2466--2478, 2016.

\bibitem[Cai et~al.(2018)Cai, Du, Liu, and Song]{cai2018curriculum}
Qi-Zhi Cai, Min Du, Chang Liu, and Dawn Song.
\newblock Curriculum adversarial training.
\newblock In \emph{IJCAI}, 2018.

\bibitem[Chen et~al.(2021)Chen, Zhang, Liu, Chang, and Wang]{chen2020robust}
Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang.
\newblock Robust overfitting may be mitigated by properly learned smoothening.
\newblock In \emph{ICLR}, 2021.

\bibitem[Croce and Hein(2020)]{croce2020reliable}
Francesco Croce and Matthias Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In \emph{ICML}, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Dong et~al.(2021)Dong, Liu, and Shang]{dong2021double}
Chengyu Dong, Liyuan Liu, and Jingbo Shang.
\newblock Double descent in adversarial training: An implicit label noise
  perspective.
\newblock \emph{arXiv preprint arXiv:2110.03135}, 2021.

\bibitem[Dong et~al.(2022)Dong, Xu, Yang, Pang, Deng, Su, and
  Zhu]{dong2021exploring}
Yinpeng Dong, Ke~Xu, Xiao Yang, Tianyu Pang, Zhijie Deng, Hang Su, and Jun Zhu.
\newblock Exploring memorization in adversarial training.
\newblock In \emph{ICLR}, 2022.

\bibitem[Engstrom et~al.(2019)Engstrom, Ilyas, Santurkar, Tsipras, Tran, and
  Madry]{engstrom2019adversarial}
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial robustness as a prior for learned representations.
\newblock \emph{arXiv preprint arXiv:1906.00945}, 2019.

\bibitem[Fowl et~al.(2021)Fowl, Goldblum, Chiang, Geiping, Czaja, and
  Goldstein]{fowl2021adversarial}
Liam~H Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojciech Czaja,
  and Tom Goldstein.
\newblock Adversarial examples make strong poisons.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{ICLR}, 2015.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar, et~al.]{byol}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec, Pierre
  Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila~Pires, Zhaohan
  Guo, Mohammad Gheshlaghi~Azar, et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{ECCV}, 2016.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer2017train}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Huang et~al.(2021)Huang, Ma, Erfani, Bailey, and
  Wang]{huang2021unlearnable}
Hanxun Huang, Xingjun Ma, Sarah~Monazam Erfani, James Bailey, and Yisen Wang.
\newblock Unlearnable examples: Making personal data unexploitable.
\newblock In \emph{ICLR}, 2021.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Tsipras, Engstrom, Tran, and
  Madry]{ilyas2019adversarial}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averaging}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock \emph{arXiv preprint arXiv:1803.05407}, 2018.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Li et~al.(2023)Li, Wang, and Wang]{li2023adversarial}
Ang Li, Yifei Wang, and Yisen Wang.
\newblock Adversarial examples are not real features.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Li and Spratling(2023{\natexlab{a}})]{li2023data}
Lin Li and Michael Spratling.
\newblock Data augmentation alone can improve adversarial training.
\newblock In \emph{ICLR}, 2023{\natexlab{a}}.

\bibitem[Li and Spratling(2023{\natexlab{b}})]{li2023understanding}
Lin Li and Michael Spratling.
\newblock Understanding and combating robust overfitting via input loss
  landscape analysis and regularization.
\newblock \emph{Pattern Recognition}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2019)Li, Wei, and Ma]{li2019towards}
Yuanzhi Li, Colin Wei, and Tengyu Ma.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Luo et~al.(2023)Luo, Wang, and Wang]{luo2023rethinking}
Rundong Luo, Yifei Wang, and Yisen Wang.
\newblock Rethinking the effect of data augmentation in adversarial contrastive
  learning.
\newblock In \emph{ICLR}, 2023.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Mo et~al.(2022)Mo, Wu, Wang, Guo, and Wang]{mo2022adversarial}
Yichuan Mo, Dongxian Wu, Yifei Wang, Yiwen Guo, and Yisen Wang.
\newblock When adversarial training meets vision transformers: Recipes from
  training to architecture.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Pal and Vidal(2020)]{pal2020game}
Ambar Pal and Ren{\'e} Vidal.
\newblock A game theoretic analysis of additive adversarial attacks and
  defenses.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Pang et~al.(2020)Pang, Yang, Dong, Su, and Zhu]{pang2020bag}
Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu.
\newblock Bag of tricks for adversarial training.
\newblock In \emph{ICLR}, 2020.

\bibitem[Pinot et~al.(2020)Pinot, Ettedgui, Rizk, Chevaleyre, and
  Atif]{pinot2020randomization}
Rafael Pinot, Raphael Ettedgui, Geovani Rizk, Yann Chevaleyre, and Jamal Atif.
\newblock Randomization matters how to defend against strong adversarial
  attacks.
\newblock In \emph{ICML}, 2020.

\bibitem[Qin et~al.(2019)Qin, Martens, Gowal, Krishnan, Dvijotham, Fawzi, De,
  Stanforth, and Kohli]{qin2019adversarial}
Chongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy
  Dvijotham, Alhussein Fawzi, Soham De, Robert Stanforth, and Pushmeet Kohli.
\newblock Adversarial robustness through local linearization.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Rebuffi et~al.(2021)Rebuffi, Gowal, Calian, Stimberg, Wiles, and
  Mann]{rebuffi2021data}
Sylvestre-Alvise Rebuffi, Sven Gowal, Dan~Andrei Calian, Florian Stimberg,
  Olivia Wiles, and Timothy~A Mann.
\newblock Data augmentation can improve robustness.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Ren et~al.(2021)Ren, Zhang, Wang, Chen, Zhou, Chen, Cheng, Wang, Zhou,
  Shi, et~al.]{ren2021unified}
Jie Ren, Die Zhang, Yisen Wang, Lu~Chen, Zhanpeng Zhou, Yiting Chen, Xu~Cheng,
  Xin Wang, Meng Zhou, Jie Shi, et~al.
\newblock A unified game-theoretic interpretation of adversarial robustness.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Rice et~al.(2020)Rice, Wong, and Kolter]{rice2020overfitting}
Leslie Rice, Eric Wong, and Zico Kolter.
\newblock Overfitting in adversarially robust deep learning.
\newblock In \emph{ICML}, 2020.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{CVPR}, 2018.

\bibitem[Santurkar et~al.(2019)Santurkar, Ilyas, Tsipras, Engstrom, Tran, and
  Madry]{santurkar2019image}
Shibani Santurkar, Andrew Ilyas, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Image synthesis with a single (robust) classifier.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Simonyan and Zisserman(2015)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{ICLR}, 2015.

\bibitem[Stutz et~al.(2021)Stutz, Hein, and Schiele]{stutz2021relating}
David Stutz, Matthias Hein, and Bernt Schiele.
\newblock Relating adversarially robust generalization to flat minima.
\newblock In \emph{ICCV}, 2021.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2014intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{ICLR}, 2014.

\bibitem[Tsipras et~al.(2018)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{ICLR}, 2018.

\bibitem[Wang and Wang(2022)]{wang2022self}
Hongjun Wang and Yisen Wang.
\newblock Self-ensemble adversarial training for improved robustness.
\newblock In \emph{ICLR}, 2022.

\bibitem[Wang and Wang(2023)]{wang2023simple}
Hongjun Wang and Yisen Wang.
\newblock Generalist: Decoupling natural and robust generalization.
\newblock In \emph{CVPR}, 2023.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Peng, Liu, Li, Chen, and
  Yang]{wang2020decoder}
Yifei Wang, Dan Peng, Furui Liu, Zhenguo Li, Zhitang Chen, and Jiansheng Yang.
\newblock Decoder-free robustness disentanglement without (additional)
  supervision.
\newblock \emph{arXiv preprint arXiv:2007.01356}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2022)Wang, Wang, Yang, and Lin]{wang2022unified}
Yifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin.
\newblock A unified contrastive energy-based model for understanding the
  generative ability of adversarial training.
\newblock In \emph{ICLR}, 2022.

\bibitem[Wang et~al.(2019)Wang, Ma, Bailey, Yi, Zhou, and
  Gu]{wang2021convergence}
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu.
\newblock On the convergence and robustness of adversarial training.
\newblock In \emph{ICML}, 2019.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Zou, Yi, Bailey, Ma, and
  Gu]{wang2020improving}
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu.
\newblock Improving adversarial robustness requires revisiting misclassified
  examples.
\newblock In \emph{ICLR}, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2021)Wang, Wang, and Wang]{wang2021fooling}
Zhirui Wang, Yifei Wang, and Yisen Wang.
\newblock Fooling adversarial training with inducing noise.
\newblock \emph{arXiv preprint arXiv:2111.10130}, 2021.

\bibitem[Wei et~al.(2023)Wei, Wang, Guo, and Wang]{wei2023cfa}
Zeming Wei, Yifei Wang, Yiwen Guo, and Yisen Wang.
\newblock Cfa: Class-wise calibrated fair adversarial training.
\newblock In \emph{CVPR}, 2023.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{wu2020adversarial}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Yu et~al.(2022{\natexlab{a}})Yu, Han, Shen, Yu, Gong, Gong, and
  Liu]{yu2022understanding}
Chaojian Yu, Bo~Han, Li~Shen, Jun Yu, Chen Gong, Mingming Gong, and Tongliang
  Liu.
\newblock Understanding robust overfitting of adversarial training and beyond.
\newblock In \emph{ICML}, 2022{\natexlab{a}}.

\bibitem[Yu et~al.(2022{\natexlab{b}})Yu, Zhou, Shen, Yu, Han, Gong, Wang, and
  Liu]{yu2022strength}
Chaojian Yu, Dawei Zhou, Li~Shen, Jun Yu, Bo~Han, Mingming Gong, Nannan Wang,
  and Tongliang Liu.
\newblock Strength-adaptive adversarial training.
\newblock \emph{arXiv preprint arXiv:2210.01288}, 2022{\natexlab{b}}.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{yun2019CutMix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{ICCV}, 2019.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In \emph{BMVC}, 2016.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, Xing, El~Ghaoui, and
  Jordan]{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El~Ghaoui, and
  Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In \emph{ICML}, 2019.

\end{thebibliography}
