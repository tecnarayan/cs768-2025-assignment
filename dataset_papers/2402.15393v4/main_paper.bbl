\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{He2015SurpassHumanImageNetPrelu}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.
\newblock In \emph{2015 {IEEE} International Conference on Computer Vision, {ICCV} 2015, Santiago, Chile, December 7-13, 2015}, pages 1026--1034. {IEEE} Computer Society, 2015.
\newblock \doi{10.1109/ICCV.2015.123}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep_resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou, Wierstra, and Riedmiller]{mnih2013playing}
V.\ Mnih, K.\ Kavukcuoglu, D.\ Silver, A.\ Graves, I.\ Antonoglou, D.\ Wierstra, and M.\ Riedmiller.
\newblock {Playing Atari with deep reinforcement learning}.
\newblock \emph{CoRR}, abs/1312.5602, 2013.

\bibitem[Jumper et~al.(2021)Jumper, Evans, Pritzel, Green, Figurnov, Ronneberger, Tunyasuvunakool, Bates, {\v{Z}}{\'\i}dek, Potapenko, et~al.]{jumper2021AlphaFoldProteinPrediction}
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin {\v{Z}}{\'\i}dek, Anna Potapenko, et~al.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock \emph{Nature}, 596\penalty0 (7873):\penalty0 583--589, 2021.

\bibitem[Schwarzschild et~al.(2021)Schwarzschild, Borgnia, Gupta, Huang, Vishkin, Goldblum, and Goldstein]{Schwarzschild2021CanYouLearnAlgorithmEasytoHard}
Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein.
\newblock Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks.
\newblock In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann~N. Dauphin, Percy Liang, and Jennifer~Wortman Vaughan, editors, \emph{Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual}, pages 6695--6706, 2021.

\bibitem[Bansal et~al.(2022)Bansal, Schwarzschild, Borgnia, Emam, Huang, Goldblum, and Goldstein]{Bansal2022endtoendalgorithmsynthesis}
Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum, and Tom Goldstein.
\newblock End-to-end algorithm synthesis with recurrent networks: Extrapolation without overthinking.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Salle and Malmasi(2023)]{Salle2023delta_loss_algo_synth}
Alexandre Salle and Shervin Malmasi.
\newblock A simple loss function for convergent algorithm synthesis using rnns.
\newblock In Krystal Maughan, Rosanne Liu, and Thomas~F. Burns, editors, \emph{The First Tiny Papers Track at {ICLR} 2023, Tiny Papers @ {ICLR} 2023, Kigali, Rwanda, May 5, 2023}. OpenReview.net, 2023.

\bibitem[Salle and Prates(2019)]{Salle2019delta_loss}
Alexandre Salle and Marcelo O.~R. Prates.
\newblock Think again networks and the delta loss.
\newblock \emph{CoRR}, abs/1904.11816, 2019.

\bibitem[Hochreiter and Schmidhuber(1997)]{Hochreiter1997LSTM}
Sepp Hochreiter and JÃ¼rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.
\newblock ISSN 0899-7667.
\newblock \doi{10.1162/neco.1997.9.8.1735}.

\bibitem[Shi et~al.(2015)Shi, Chen, Wang, Yeung, Wong, and Woo]{Shi2015ConvLSTM}
Xingjian Shi, Zhourong Chen, Hao Wang, Dit{-}Yan Yeung, Wai{-}Kin Wong, and Wang{-}chun Woo.
\newblock Convolutional {LSTM} network: {A} machine learning approach for precipitation nowcasting.
\newblock In Corinna Cortes, Neil~D. Lawrence, Daniel~D. Lee, Masashi Sugiyama, and Roman Garnett, editors, \emph{Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada}, pages 802--810, 2015.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{Ba2016LayerNormalization}
Lei~Jimmy Ba, Jamie~Ryan Kiros, and Geoffrey~E. Hinton.
\newblock Layer normalization.
\newblock \emph{CoRR}, abs/1607.06450, 2016.
\newblock \doi{10.48550/arxiv.1607.06450}.

\bibitem[Kaiser and Sutskever(2016)]{Kaiser2016NeuralGPU}
Lukasz Kaiser and Ilya Sutskever.
\newblock Neural gpus learn algorithms.
\newblock In Yoshua Bengio and Yann LeCun, editors, \emph{4th International Conference on Learning Representations, {ICLR} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings}, 2016.

\bibitem[Zaremba and Sutskever(2014)]{Zaremba2014LearningtoExecute}
Wojciech Zaremba and Ilya Sutskever.
\newblock Learning to execute.
\newblock \emph{CoRR}, abs/1410.4615, 2014.
\newblock \doi{10.48550/arxiv.1410.4615}.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement_book_barto}
R.\ Sutton and A.\ Barto.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT press, 2018.

\bibitem[Dror et~al.(2019)Dror, Shlomov, and Reichart]{Dror2019DeepDominanceAso}
Rotem Dror, Segev Shlomov, and Roi Reichart.
\newblock Deep dominance - how to properly compare deep neural models.
\newblock In Anna Korhonen, David~R. Traum, and Llu{\'{\i}}s M{\`{a}}rquez, editors, \emph{Proceedings of the 57th Conference of the Association for Computational Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers}, pages 2773--2785. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/V1/P19-1266}.

\bibitem[Del~Barrio et~al.(2018)Del~Barrio, Cuesta-Albertos, and Matr{\'a}n]{del2018optimalAso}
Eustasio Del~Barrio, Juan~A Cuesta-Albertos, and Carlos Matr{\'a}n.
\newblock An optimal transportation approach for assessing almost stochastic order.
\newblock \emph{The Mathematics of the Uncertain: A Tribute to Pedro Gil}, pages 33--44, 2018.

\bibitem[Bonferroni(1936)]{bonferroni1936teoriaprobabilitaBonferroniCorrection}
C.E. Bonferroni.
\newblock \emph{Teoria statistica delle classi e calcolo delle probabilit{\`a}}.
\newblock Pubblicazioni del R. Istituto superiore di scienze economiche e commerciali di Firenze. Seeber, 1936.

\bibitem[Chevalier-Boisvert et~al.(2023)Chevalier-Boisvert, Dai, Towers, de~Lazcano, Willems, Lahlou, Pal, Castro, and Terry]{MinigridMiniworld23minigrid}
Maxime Chevalier-Boisvert, Bolun Dai, Mark Towers, Rodrigo de~Lazcano, Lucas Willems, Salem Lahlou, Suman Pal, Pablo~Samuel Castro, and Jordan Terry.
\newblock Minigrid \& miniworld: Modular \& customizable reinforcement learning environments for goal-oriented tasks.
\newblock \emph{CoRR}, abs/2306.13831, 2023.

\bibitem[Price et~al.(2016)Price, Zaremba, and Sutskever]{Price2016ExtensionsLimitationsNGPU}
Eric Price, Wojciech Zaremba, and Ilya Sutskever.
\newblock Extensions and limitations of the neural {GPU}.
\newblock \emph{CoRR}, abs/1611.00736, 2016.
\newblock \doi{10.48550/arxiv.1611.00736}.

\bibitem[Wurman et~al.(2022)Wurman, Barrett, Kawamoto, MacGlashan, Subramanian, Walsh, Capobianco, Devlic, Eckert, Fuchs, et~al.]{wurman2022outracing}
Peter~R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian, Thomas~J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et~al.
\newblock Outracing champion gran turismo drivers with deep reinforcement learning.
\newblock \emph{Nature}, 602\penalty0 (7896):\penalty0 223--228, 2022.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Hartikainen, Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, et~al.]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018.

\bibitem[Mnih et~al.(2015)]{mnih2015reinforcement}
Volodymyr Mnih et~al.
\newblock reinforcement learning." nature 518.7540 (2015): 529-533.
\newblock \emph{Nature}, 518:\penalty0 529--533, 2015.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan, Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel, et~al.]{schrittwieser2020mastering}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et~al.
\newblock Mastering atari, go, chess and shogi by planning with a learned model.
\newblock \emph{Nature}, 588\penalty0 (7839):\penalty0 604--609, 2020.

\bibitem[Wang et~al.(2022)Wang, Wang, Liang, Zhao, Huang, Xu, Dai, and Miao]{wang2022deep}
Xu~Wang, Sen Wang, Xingxing Liang, Dawei Zhao, Jincai Huang, Xin Xu, Bin Dai, and Qiguang Miao.
\newblock Deep reinforcement learning: A survey.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}, 2022.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel, Srinivas, and Mordatch]{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 15084--15097, 2021.

\bibitem[Wu et~al.(2024)Wu, Wang, and Hamaya]{wu2024elastic}
Yueh-Hua Wu, Xiaolong Wang, and Masashi Hamaya.
\newblock Elastic decision transformer.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Gal and Ghahramani(2016)]{Gal2016RnnGalDropoutGrounded}
Yarin Gal and Zoubin Ghahramani.
\newblock A theoretically grounded application of dropout in recurrent neural networks.
\newblock In Daniel~D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, \emph{Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain}, pages 1019--1027, 2016.

\bibitem[Cho et~al.(2014)Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau, Bougares, Schwenk, and Bengio]{cho2014learning}
Kyunghyun Cho, Bart Van~Merri{\"e}nboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using rnn encoder-decoder for statistical machine translation.
\newblock \emph{arXiv preprint arXiv:1406.1078}, 2014.

\bibitem[Veerabadran et~al.(2023)Veerabadran, Ravishankar, Tang, Raina, and de~Sa]{Veerabadran2023locrnn_adrnn}
Vijay Veerabadran, Srinivas Ravishankar, Yuan Tang, Ritik Raina, and Virginia de~Sa.
\newblock Adaptive recurrent vision performs zero-shot computation scaling to unseen difficulty levels.
\newblock In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, \emph{Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023}, 2023.
\newblock URL \url{http://papers.nips.cc/paper_files/paper/2023/hash/3a40e042c66e84659249f3254460c123-Abstract-Conference.html}.

\bibitem[Ulmer et~al.(2022)Ulmer, Hardmeier, and Frellsen]{Ulmer2022deepSignificance}
Dennis Ulmer, Christian Hardmeier, and Jes Frellsen.
\newblock deep-significance - easy and meaningful statistical significance testing in the age of neural networks.
\newblock \emph{CoRR}, abs/2204.06815, 2022.
\newblock \doi{10.48550/ARXIV.2204.06815}.

\end{thebibliography}
