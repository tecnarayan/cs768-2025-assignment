\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[ARC(2021)]{ARC}
The acl anthology reference corpus.
\newblock https://aclanthology.org/, Nov. 2021.

\bibitem[Abu-El-Haija et~al.(2019)Abu-El-Haija, Perozzi, Kapoor, Alipourfard,
  Lerman, Harutyunyan, Ver~Steeg, and Galstyan]{abu2019mixhop}
Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina
  Lerman, Hrayr Harutyunyan, Greg Ver~Steeg, and Aram Galstyan.
\newblock Mixhop: Higher-order graph convolutional architectures via sparsified
  neighborhood mixing.
\newblock In \emph{international conference on machine learning}, pages 21--29.
  PMLR, 2019.

\bibitem[Allamanis et~al.(2018)Allamanis, Barr, Devanbu, and
  Sutton]{allamanis2018survey}
Miltiadis Allamanis, Earl~T Barr, Premkumar Devanbu, and Charles Sutton.
\newblock A survey of machine learning for big code and naturalness.
\newblock \emph{ACM Computing Surveys (CSUR)}, 51\penalty0 (4):\penalty0 1--37,
  2018.

\bibitem[Alon et~al.(2018)Alon, Brody, Levy, and Yahav]{alon2018code2seq}
Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav.
\newblock code2seq: Generating sequences from structured representations of
  code.
\newblock \emph{arXiv preprint arXiv:1808.01400}, 2018.

\bibitem[Brin(1998)]{brin1998pagerank}
Sergey Brin.
\newblock The pagerank citation ranking: bringing order to the web.
\newblock \emph{Proceedings of ASIS, 1998}, 98:\penalty0 161--172, 1998.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, O’Bray, and
  Borgwardt]{chen2022structure}
Dexiong Chen, Leslie O’Bray, and Karsten Borgwardt.
\newblock Structure-aware transformer for graph representation learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3469--3489. PMLR, 2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Gao, Li, and
  He]{chen2022nagphormer}
Jinsong Chen, Kaiyuan Gao, Gaichao Li, and Kun He.
\newblock Nagphormer: Neighborhood aggregation graph transformer for node
  classification in large graphs.
\newblock \emph{arXiv preprint arXiv:2206.04910}, 2022{\natexlab{b}}.

\bibitem[Chen et~al.(2020)Chen, Wu, and Zaki]{chen2020iterative}
Yu~Chen, Lingfei Wu, and Mohammed Zaki.
\newblock Iterative deep graph learning for graph neural networks: Better and
  robust node embeddings.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 19314--19326, 2020.

\bibitem[Corso et~al.(2020)Corso, Cavalleri, Beaini, Li{\`o}, and
  Veli{\v{c}}kovi{\'c}]{corso2020principal}
Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li{\`o}, and Petar
  Veli{\v{c}}kovi{\'c}.
\newblock Principal neighbourhood aggregation for graph nets.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 13260--13271, 2020.

\bibitem[Crouse et~al.(2019)Crouse, Abdelaziz, Cornelio, Thost, Wu, Forbus, and
  Fokoue]{crouse2019improving}
Maxwell Crouse, Ibrahim Abdelaziz, Cristina Cornelio, Veronika Thost, Lingfei
  Wu, Kenneth Forbus, and Achille Fokoue.
\newblock Improving graph neural network representations of logical formulae
  with subgraph pooling.
\newblock \emph{arXiv preprint arXiv:1911.06904}, 2019.

\bibitem[Dong et~al.(2022)Dong, Zhang, Li, and Chen]{dong2022pace}
Zehao Dong, Muhan Zhang, Fuhai Li, and Yixin Chen.
\newblock Pace: A parallelizable computation encoder for directed acyclic
  graphs.
\newblock In \emph{International Conference on Machine Learning}, pages
  5360--5377. PMLR, 2022.

\bibitem[Dwivedi and Bresson(2020)]{dwivedi2020generalization}
Vijay~Prakash Dwivedi and Xavier Bresson.
\newblock A generalization of transformer networks to graphs.
\newblock \emph{arXiv preprint arXiv:2012.09699}, 2020.

\bibitem[Dwivedi et~al.(2021)Dwivedi, Luu, Laurent, Bengio, and
  Bresson]{dwivedi2021graph}
Vijay~Prakash Dwivedi, Anh~Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier
  Bresson.
\newblock Graph neural networks with learnable structural and positional
  representations.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Dwivedi et~al.(2022)Dwivedi, Ramp{\'a}{\v{s}}ek, Galkin, Parviz, Wolf,
  Luu, and Beaini]{dwivedi2022long}
Vijay~Prakash Dwivedi, Ladislav Ramp{\'a}{\v{s}}ek, Mikhail Galkin, Ali Parviz,
  Guy Wolf, Anh~Tuan Luu, and Dominique Beaini.
\newblock Long range graph benchmark.
\newblock \emph{arXiv preprint arXiv:2206.08164}, 2022.

\bibitem[Fey and Lenssen(2019)]{fey2019fast}
Matthias Fey and Jan~Eric Lenssen.
\newblock Fast graph representation learning with pytorch geometric.
\newblock \emph{arXiv preprint arXiv:1903.02428}, 2019.

\bibitem[Franceschi et~al.(2019)Franceschi, Niepert, Pontil, and
  He]{franceschi2019learning}
Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He.
\newblock Learning discrete structures for graph neural networks.
\newblock In \emph{International conference on machine learning}, pages
  1972--1982. PMLR, 2019.

\bibitem[Frasconi et~al.(1998)Frasconi, Gori, and
  Sperduti]{frasconi1998general}
Paolo Frasconi, Marco Gori, and Alessandro Sperduti.
\newblock A general framework for adaptive processing of data structures.
\newblock \emph{IEEE transactions on Neural Networks}, 9\penalty0 (5):\penalty0
  768--786, 1998.

\bibitem[Freitas and Dong(2021)]{freitas2021large}
Scott Freitas and Yuxiao Dong.
\newblock A large-scale database for graph representation learning.
\newblock \emph{Advances in neural information processing systems}, 2021.

\bibitem[Gagrani et~al.(2022)Gagrani, Rainone, Yang, Teague, Jeon, Bondesan,
  van Hoof, Lott, Zeng, and Zappi]{gagrani2022neural}
Mukul Gagrani, Corrado Rainone, Yang Yang, Harris Teague, Wonseok Jeon, Roberto
  Bondesan, Herke van Hoof, Christopher Lott, Weiliang Zeng, and Piero Zappi.
\newblock Neural topological ordering for computation graphs.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 17327--17339, 2022.

\bibitem[Gasteiger et~al.()Gasteiger, Bojchevski, and
  G{\"u}nnemann]{gasteigerpredict}
Johannes Gasteiger, Aleksandar Bojchevski, and Stephan G{\"u}nnemann.
\newblock Predict then propagate: Graph neural networks meet personalized
  pagerank.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[Geisler et~al.(2023)Geisler, Li, Mankowitz, Cemgil, G{\"u}nnemann, and
  Paduraru]{geisler2023transformers}
Simon Geisler, Yujia Li, Daniel Mankowitz, Ali~Taylan Cemgil, Stephan
  G{\"u}nnemann, and Cosmin Paduraru.
\newblock Transformers meet directed graphs.
\newblock \emph{arXiv preprint arXiv:2302.00049}, 2023.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{gilmer2017neural}
Justin Gilmer, Samuel~S Schoenholz, Patrick~F Riley, Oriol Vinyals, and
  George~E Dahl.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International conference on machine learning}, pages
  1263--1272. PMLR, 2017.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and
  Leskovec]{hu2020open}
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
  Michele Catasta, and Jure Leskovec.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 22118--22133, 2020.

\bibitem[Huang et~al.(2022)Huang, Tao, Zhou, Li, and Huang]{huang2022learning}
Fei Huang, Tianhua Tao, Hao Zhou, Lei Li, and Minlie Huang.
\newblock On the learning of non-autoregressive transformers.
\newblock In \emph{International Conference on Machine Learning}, pages
  9356--9376. PMLR, 2022.

\bibitem[Kim et~al.(2021)Kim, Zhao, Tian, and Chandra]{kim2021code}
Seohyun Kim, Jinman Zhao, Yuchi Tian, and Satish Chandra.
\newblock Code prediction by feeding trees to transformers.
\newblock In \emph{2021 IEEE/ACM 43rd International Conference on Software
  Engineering (ICSE)}, pages 150--162. IEEE, 2021.

\bibitem[Kipf and Welling(2017)]{kipf2017semisupervised}
Thomas~N. Kipf and Max Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=SJU4ayYgl}.

\bibitem[Knyazev et~al.(2021)Knyazev, Drozdzal, Taylor, and
  Romero~Soriano]{knyazev2021parameter}
Boris Knyazev, Michal Drozdzal, Graham~W Taylor, and Adriana Romero~Soriano.
\newblock Parameter prediction for unseen deep architectures.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 29433--29448, 2021.

\bibitem[Kotnis et~al.(2021)Kotnis, Lawrence, and Niepert]{kotnis2021answering}
Bhushan Kotnis, Carolin Lawrence, and Mathias Niepert.
\newblock Answering complex queries in knowledge graphs with bidirectional
  sequence encoders.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 4968--4977, 2021.

\bibitem[Kreuzer et~al.(2021)Kreuzer, Beaini, Hamilton, L{\'e}tourneau, and
  Tossou]{kreuzer2021rethinking}
Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent L{\'e}tourneau, and
  Prudencio Tossou.
\newblock Rethinking graph transformers with spectral attention.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 21618--21629, 2021.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Loshchilov and Hutter(2016)]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Luo et~al.(2023)Luo, Shi, Xu, Ji, Xiao, Hu, and Shan]{luo2023impact}
Yuankai Luo, Lei Shi, Mufan Xu, Yuwen Ji, Fengli Xiao, Chunming Hu, and
  Zhiguang Shan.
\newblock Impact-oriented contextual scholar profiling using self-citation
  graphs.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining (KDD)}, pages 4572--4583, 2023.

\bibitem[Ma et~al.(2019)Ma, Hao, Yang, Li, Jin, and Chen]{ma2019spectral}
Yi~Ma, Jianye Hao, Yaodong Yang, Han Li, Junqi Jin, and Guangyong Chen.
\newblock Spectral-based graph convolutional network for directed graphs.
\newblock \emph{arXiv preprint arXiv:1907.08990}, 2019.

\bibitem[Mialon et~al.(2021)Mialon, Chen, Selosse, and
  Mairal]{mialon2021graphit}
Gr{\'e}goire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal.
\newblock Graphit: Encoding graph structure in transformers.
\newblock \emph{arXiv preprint arXiv:2106.05667}, 2021.

\bibitem[Min et~al.(2022)Min, Chen, Bian, Xu, Zhao, Huang, Zhao, Huang,
  Ananiadou, and Rong]{min2022transformer}
Erxue Min, Runfa Chen, Yatao Bian, Tingyang Xu, Kangfei Zhao, Wenbing Huang,
  Peilin Zhao, Junzhou Huang, Sophia Ananiadou, and Yu~Rong.
\newblock Transformer for graphs: An overview from architecture perspective.
\newblock \emph{arXiv preprint arXiv:2202.08455}, 2022.

\bibitem[Monti et~al.(2018)Monti, Otness, and Bronstein]{monti2018motifnet}
Federico Monti, Karl Otness, and Michael~M Bronstein.
\newblock Motifnet: a motif-based graph convolutional network for directed
  graphs.
\newblock In \emph{2018 IEEE Data Science Workshop (DSW)}, pages 225--228.
  IEEE, 2018.

\bibitem[Peng et~al.(2022)Peng, Li, Zhao, and Jin]{peng2022rethinking}
Han Peng, Ge~Li, Yunfei Zhao, and Zhi Jin.
\newblock Rethinking positional encoding in tree transformer for code
  representation.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 3204--3214, 2022.

\bibitem[Pham et~al.(2018)Pham, Guan, Zoph, Le, and Dean]{pham2018efficient}
Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean.
\newblock Efficient neural architecture search via parameters sharing.
\newblock In \emph{International conference on machine learning}, pages
  4095--4104. PMLR, 2018.

\bibitem[Ramp{\'a}{\v{s}}ek et~al.(2022)Ramp{\'a}{\v{s}}ek, Galkin, Dwivedi,
  Luu, Wolf, and Beaini]{rampavsek2022recipe}
Ladislav Ramp{\'a}{\v{s}}ek, Mikhail Galkin, Vijay~Prakash Dwivedi, Anh~Tuan
  Luu, Guy Wolf, and Dominique Beaini.
\newblock Recipe for a general, powerful, scalable graph transformer.
\newblock \emph{arXiv preprint arXiv:2205.12454}, 2022.

\bibitem[Ross et~al.(2022)Ross, Belgodere, Chenthamarakshan, Padhi, Mroueh, and
  Das]{ross2022molformer}
Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef
  Mroueh, and Payel Das.
\newblock Molformer: Large scale chemical language representations capture
  molecular structure and properties.
\newblock 2022.

\bibitem[Sanchez{-}Gonzalez et~al.(2020)Sanchez{-}Gonzalez, Godwin, Pfaff,
  Ying, Leskovec, and Battaglia]{physics}
Alvaro Sanchez{-}Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure
  Leskovec, and Peter~W. Battaglia.
\newblock Learning to simulate complex physics with graph networks.
\newblock \emph{CoRR}, abs/2002.09405, 2020.
\newblock URL \url{https://arxiv.org/abs/2002.09405}.

\bibitem[Sen et~al.(2008)Sen, Namata, Bilgic, Getoor, Galligher, and
  Eliassi-Rad]{sen2008collective}
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher,
  and Tina Eliassi-Rad.
\newblock Collective classification in network data.
\newblock \emph{AI magazine}, 29\penalty0 (3):\penalty0 93--93, 2008.

\bibitem[Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani]{shaw2018self}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
\newblock Self-attention with relative position representations.
\newblock \emph{arXiv preprint arXiv:1803.02155}, 2018.

\bibitem[Shen et~al.(2021)Shen, Wu, Yang, and Quan]{shen2021directed}
Weizhou Shen, Siyue Wu, Yunyi Yang, and Xiaojun Quan.
\newblock Directed acyclic graph network for conversational emotion
  recognition.
\newblock \emph{arXiv preprint arXiv:2105.12907}, 2021.

\bibitem[Shuai et~al.(2016)Shuai, Zuo, Wang, and Wang]{shuai2016dag}
Bing Shuai, Zhen Zuo, Bing Wang, and Gang Wang.
\newblock Dag-recurrent neural networks for scene labeling.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3620--3629, 2016.

\bibitem[Snelson and Ghahramani(2005)]{snelson2005sparse}
Edward Snelson and Zoubin Ghahramani.
\newblock Sparse gaussian processes using pseudo-inputs.
\newblock \emph{Advances in neural information processing systems}, 18, 2005.

\bibitem[Sperduti and Starita(1997)]{sperduti1997supervised}
Alessandro Sperduti and Antonina Starita.
\newblock Supervised neural networks for the classification of structures.
\newblock \emph{IEEE Transactions on Neural Networks}, 8\penalty0 (3):\penalty0
  714--735, 1997.

\bibitem[Sun et~al.(2020)Sun, Zhu, Xiong, Sun, Mou, and Zhang]{sun2020treegen}
Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu~Zhang.
\newblock Treegen: A tree-based transformer architecture for code generation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 8984--8991, 2020.

\bibitem[Tai et~al.(2015)Tai, Socher, and Manning]{tai2015improved}
Kai~Sheng Tai, Richard Socher, and Christopher~D Manning.
\newblock Improved semantic representations from tree-structured long
  short-term memory networks.
\newblock In \emph{Proceedings of the 53rd Annual Meeting of the Association
  for Computational Linguistics and the 7th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 1556--1566, 2015.

\bibitem[Thost and Chen(2021)]{thost2021directed}
Veronika Thost and Jie Chen.
\newblock Directed acyclic graph neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=JbuYF437WB6}.

\bibitem[Tong et~al.(2020)Tong, Liang, Sun, Li, Rosenblum, and
  Lim]{tong2020digraph}
Zekun Tong, Yuxuan Liang, Changsheng Sun, Xinke Li, David Rosenblum, and Andrew
  Lim.
\newblock Digraph inception convolutional networks.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 17907--17918, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2018)Veli{\v{c}}kovi{\'c}, Cucurull,
  Casanova, Romero, Li{\`o}, and Bengio]{velivckovic2018graph}
Petar Veli{\v{c}}kovi{\'c}, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
  Pietro Li{\`o}, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Wu et~al.(2022)Wu, Zhao, Li, Wipf, and Yan]{wu2022nodeformer}
Qitian Wu, Wentao Zhao, Zenan Li, David~P Wipf, and Junchi Yan.
\newblock Nodeformer: A scalable graph structure learning transformer for node
  classification.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27387--27401, 2022.

\bibitem[Wu et~al.(2023)Wu, Yang, Zhao, He, Wipf, and Yan]{wu2023difformer}
Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, and Junchi Yan.
\newblock {DIFF}ormer: Scalable (graph) transformers induced by energy
  constrained diffusion.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=j6zUzrapY3L}.

\bibitem[Wu et~al.(2021)Wu, Jain, Wright, Mirhoseini, Gonzalez, and
  Stoica]{wu2021representing}
Zhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph~E Gonzalez,
  and Ion Stoica.
\newblock Representing long-range context for graph neural networks with global
  attention.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 13266--13279, 2021.

\bibitem[Wu et~al.(2020)Wu, Pan, Chen, Long, Zhang, and
  Philip]{wu2020comprehensive}
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S~Yu
  Philip.
\newblock A comprehensive survey on graph neural networks.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  32\penalty0 (1):\penalty0 4--24, 2020.

\bibitem[Xu et~al.(2018)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful}
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
\newblock How powerful are graph neural networks?
\newblock \emph{arXiv preprint arXiv:1810.00826}, 2018.

\bibitem[Ying et~al.(2021)Ying, Cai, Luo, Zheng, Ke, He, Shen, and
  Liu]{ying2021transformers}
Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di~He,
  Yanming Shen, and Tie-Yan Liu.
\newblock Do transformers really perform badly for graph representation?
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 28877--28888, 2021.

\bibitem[Zhang et~al.(2018)Zhang, Ren, and Urtasun]{zhang2018graph}
Chris Zhang, Mengye Ren, and Raquel Urtasun.
\newblock Graph hypernetworks for neural architecture search.
\newblock \emph{arXiv preprint arXiv:1810.05749}, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Jiang, Cui, Garnett, and Chen]{zhang2019d}
Muhan Zhang, Shali Jiang, Zhicheng Cui, Roman Garnett, and Yixin Chen.
\newblock D-vae: A variational autoencoder for directed acyclic graphs.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zhang et~al.(2021)Zhang, He, Brugnone, Perlmutter, and
  Hirn]{zhang2021magnet}
Xitong Zhang, Yixuan He, Nathan Brugnone, Michael Perlmutter, and Matthew Hirn.
\newblock Magnet: A neural network for directed graphs.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 27003--27015, 2021.

\bibitem[Zitnik et~al.(2018)Zitnik, Agrawal, and Leskovec]{ZitnikAL18}
Marinka Zitnik, Monica Agrawal, and Jure Leskovec.
\newblock Modeling polypharmacy side effects with graph convolutional networks.
\newblock \emph{Bioinform.}, 34\penalty0 (13):\penalty0 i457--i466, 2018.

\bibitem[Z{\"u}gner et~al.()Z{\"u}gner, Kirschstein, Catasta, Leskovec, and
  G{\"u}nnemann]{zugnerlanguage}
Daniel Z{\"u}gner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and
  Stephan G{\"u}nnemann.
\newblock Language-agnostic representation learning of source code from
  structure and context.
\newblock In \emph{International Conference on Learning Representations}.

\end{thebibliography}
