@misc{gupta2023behavior,
      title={Behavior Alignment via Reward Function Optimization}, 
      author={Dhawal Gupta and Yash Chandak and Scott M. Jordan and Philip S. Thomas and Bruno Castro da Silva},
      year={2023},
      eprint={2310.19007},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Buyukkoc1985TheCR,
  title={The c$\mu$ rule revisited},
  author={C. Buyukkoc and Pravin Pratap Varaiya and Jean C. Walrand},
  journal={Advances in Applied Probability},
  year={1985},
  volume={17},
  pages={237 - 238},
  url={https://api.semanticscholar.org/CorpusID:125747190}
}

@article{huang2022cleanrl,
  author  = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga and Dipam Chakraborty and Kinal Mehta and João G.M. Araújo},
  title   = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {274},
  pages   = {1--18},
  url     = {http://jmlr.org/papers/v23/21-1342.html}
}

@inproceedings{
dohare2023overcoming,
title={Overcoming Policy Collapse in Deep Reinforcement Learning},
author={Shibhansh Dohare and Qingfeng Lan and A. Rupam Mahmood},
booktitle={Sixteenth European Workshop on Reinforcement Learning},
year={2023},
url={https://openreview.net/forum?id=m9Jfdz4ymO}
}

@article{fayolle1993lyplinear,
author = {Fayolle, G. and Malyshev, V. A. and Menshikov, M. V. and Sidorenko, A. F.},
title = {Lyapunov functions for Jackson networks},
year = {1993},
issue_date = {Nov. 1993},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {18},
number = {4},
issn = {0364-765X},
journal = {Math. Oper. Res.},
month = {nov},
pages = {916–927},
numpages = {12},
keywords = {queues, ergodicity, Lyapunov functions, Jackson networks}
}

@article{sanjay2006opt_vf_quad,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/27595737},
 abstract = {We consider a single-server queueing system at which customers arrive according to a Poisson process. The service times of the customers are independent and follow a Coxian distribution of order r. The system is subject to costs per unit time for holding a customer in the system. We give a closed-form expression for the average cost and the corresponding value function. The result can be used to derive nearly optimal policies in controlled queueing systems in which the service times are not necessarily Markovian, by performing a single step of policy iteration. We illustrate this in the model where a controller has to route to several single-server queues. Numerical experiments show that the improved policy has a close-to-optimal value.},
 author = {Sandjai Bhulai},
 journal = {Journal of Applied Probability},
 number = {2},
 pages = {363--376},
 publisher = {Applied Probability Trust},
 title = {On the Value Function of the M/Cox(r)/1 Queue},
 urldate = {2024-05-08},
 volume = {43},
 year = {2006}
}


@misc{harutyunyan2019hindsight,
      title={Hindsight Credit Assignment}, 
      author={Anna Harutyunyan and Will Dabney and Thomas Mesnard and Mohammad Azar and Bilal Piot and Nicolas Heess and Hado van Hasselt and Greg Wayne and Satinder Singh and Doina Precup and Remi Munos},
      year={2019},
      eprint={1912.02503},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{maguluri2016heavy,
  title={Heavy traffic queue length behavior in a switch under the MaxWeight algorithm},
  author={Maguluri, Siva Theja and Srikant, R},
  journal={Stochastic Systems},
  volume={6},
  number={1},
  pages={211--250},
  year={2016},
  publisher={INFORMS}
}


@misc{wei2023mixedsystem,
      title={Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks}, 
      author={Honghao Wei and Xin Liu and Weina Wang and Lei Ying},
      year={2023},
      eprint={2305.16483},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{sharma2022statedistribution,
      title={A State-Distribution Matching Approach to Non-Episodic Reinforcement Learning}, 
      author={Archit Sharma and Rehaan Ahmad and Chelsea Finn},
      year={2022},
      eprint={2205.05212},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{meynCSRL2022, place={Cambridge}, title={Control Systems and Reinforcement Learning}, publisher={Cambridge University Press}, author={Meyn, Sean}, year={2022}}

@article{taylorLearningLyp2019,
  author       = {Andrew J. Taylor and
                  Victor D. Dorobantu and
                  Hoang Minh Le and
                  Yisong Yue and
                  Aaron D. Ames},
  title        = {Episodic Learning with Control Lyapunov Functions for Uncertain Robotic
                  Systems},
  journal      = {CoRR},
  volume       = {abs/1903.01577},
  year         = {2019},
  url          = {http://arxiv.org/abs/1903.01577},
  eprinttype    = {arXiv},
  eprint       = {1903.01577},
  timestamp    = {Thu, 06 Jun 2019 18:03:26 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1903-01577.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{NEURIPS2019_2647c1db,
 author = {Chang, Ya-Chien and Roohi, Nima and Gao, Sicun},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Lyapunov Control},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/2647c1dba23bc0e0f9cdf75339e120d2-Paper.pdf},
 volume = {32},
 year = {2019}
}


@misc{dai2021lyapunovstable,
      title={Lyapunov-stable neural-network control}, 
      author={Hongkai Dai and Benoit Landry and Lujie Yang and Marco Pavone and Russ Tedrake},
      year={2021},
      eprint={2109.14152},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@article{harrisonNmodel1998,
author = {J. Michael Harrison},
title = {{Heavy traffic analysis of a system with parallel servers: asymptotic optimality of discrete-review policies}},
volume = {8},
journal = {The Annals of Applied Probability},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {822 -- 848},
keywords = {BIGSTEP method, heavy traffic, Queueing theory, resource pooling},
year = {1998},
doi = {10.1214/aoap/1028903452},
URL = {https://doi.org/10.1214/aoap/1028903452}
}


@article{agarwal2021deep,
  title={Deep reinforcement learning at the edge of the statistical precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{grimmreceding2005,
author = {Grimm, Gene and Messina, Michael and Tuna, Sezai and Teel, Andrew},
year = {2005},
month = {06},
pages = {546 - 558},
title = {Model predictive control: For want of a local control Lyapunov function, all is not lost},
volume = {50},
journal = {Automatic Control, IEEE Transactions on},
doi = {10.1109/TAC.2005.847055}
}

@article{jadbabaiereceding2005,
author = {Jadbabaie, A. and Hauser, John},
year = {2005},
month = {06},
pages = {674 - 678},
title = {On the stability of receding horizon control with a general terminal cost},
volume = {50},
journal = {Automatic Control, IEEE Transactions on},
doi = {10.1109/TAC.2005.846597}
}

@inproceedings{ngpotential1999,
author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
title = {Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
year = {1999},
isbn = {1558606122},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
pages = {278–287},
numpages = {10},
series = {ICML '99}
}

@INPROCEEDINGS{todorovmujoco2012,
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems}, 
  title={MuJoCo: A physics engine for model-based control}, 
  year={2012},
  volume={},
  number={},
  pages={5026-5033},
  doi={10.1109/IROS.2012.6386109}}


@article{bellemareatari2012,
  author       = {Marc G. Bellemare and
                  Yavar Naddaf and
                  Joel Veness and
                  Michael Bowling},
  title        = {The Arcade Learning Environment: An Evaluation Platform for General
                  Agents},
  journal      = {CoRR},
  volume       = {abs/1207.4708},
  year         = {2012},
  url          = {http://arxiv.org/abs/1207.4708},
  eprinttype    = {arXiv},
  eprint       = {1207.4708},
  timestamp    = {Mon, 13 Aug 2018 16:46:44 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1207-4708.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{pignatelli2023CAPsurvey,
      title={A Survey of Temporal Credit Assignment in Deep Reinforcement Learning}, 
      author={Eduardo Pignatelli and Johan Ferret and Matthieu Geist and Thomas Mesnard and Hado van Hasselt and Laura Toni},
      year={2023},
      eprint={2312.01072},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{kelletstability2003,
  author={Kellett, C.M. and Teel, A.R.},
  booktitle={42nd IEEE International Conference on Decision and Control (IEEE Cat. No.03CH37475)}, 
  title={Results on discrete-time control-Lyapunov functions}, 
  year={2003},
  volume={6},
  number={},
  pages={5961-5966 Vol.6},
  doi={10.1109/CDC.2003.1271964}}


@inproceedings{mao2019TowardsSO,
  title={Towards Safe Online Reinforcement Learning in Computer Systems},
  author={Hongzi Mao and Malte Schwarzkopf and Hao He and Mohammad Alizadeh},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:204978262}
}

@book{garcia_little_2008,
  added-at = {2014-07-11T21:28:31.000+0200},
  address = {Upper Saddle River, NJ},
  author = {Leon-Garcia, Alberto},
  biburl = {https://www.bibsonomy.org/bibtex/28318355a9fe01615ac9cf2865fbc2ab1/ytyoun},
  edition = {Third},
  interhash = {7a6afe6a26ae8f7551fe054d3cf7d9e4},
  intrahash = {8318355a9fe01615ac9cf2865fbc2ab1},
  isbn = {9780131471221 0131471228},
  keywords = {leon-garcia probability queueing.theory textbook},
  publisher = {Pearson/Prentice Hall},
  refid = {181079252},
  timestamp = {2015-09-19T12:47:00.000+0200},
  title = {Probability, Statistics, and Random Processes for Electrical Engineering},
  year = 2008
}



@article{hessel_rainbow_2017,
  abstract = {The deep reinforcement learning community has made several independent
improvements to the DQN algorithm. However, it is unclear which of these
extensions are complementary and can be fruitfully combined. This paper
examines six extensions to the DQN algorithm and empirically studies their
combination. Our experiments show that the combination provides
state-of-the-art performance on the Atari 2600 benchmark, both in terms of data
efficiency and final performance. We also provide results from a detailed
ablation study that shows the contribution of each component to overall
performance.},
  added-at = {2020-03-03T23:40:35.000+0100},
  author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  biburl = {https://www.bibsonomy.org/bibtex/223e587e36693531fa1a5bd1f8d2a1bdd/kirk86},
  description = {[1710.02298] Rainbow: Combining Improvements in Deep Reinforcement Learning},
  interhash = {f4fb4d30fac6e6290d70a94e7420777a},
  intrahash = {23e587e36693531fa1a5bd1f8d2a1bdd},
  keywords = {reinforcement-learning},
  note = {cite arxiv:1710.02298Comment: Under review as a conference paper at AAAI 2018},
  timestamp = {2020-03-03T23:40:35.000+0100},
  title = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  url = {http://arxiv.org/abs/1710.02298},
  year = 2017
}



@article{arumugam_creditinfo_2021,
  author       = {Dilip Arumugam and
                  Peter Henderson and
                  Pierre{-}Luc Bacon},
  title        = {An Information-Theoretic Perspective on Credit Assignment in Reinforcement
                  Learning},
  journal      = {CoRR},
  volume       = {abs/2103.06224},
  year         = {2021},
  url          = {https://arxiv.org/abs/2103.06224},
  eprinttype    = {arXiv},
  eprint       = {2103.06224},
  timestamp    = {Tue, 16 Mar 2021 11:26:59 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2103-06224.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@book{neely_networkcomm_2010,
author = {Neely, Michael J.},
title = {Stochastic Network Optimization with Application to Communication and Queueing Systems},
year = {2010},
isbn = {160845455X},
publisher = {Morgan and Claypool Publishers},
abstract = {This text presents a modern theory of analysis, control, and optimization for dynamic networks. Mathematical techniques of Lyapunov drift and Lyapunov optimization are developed and shown to enable constrained optimization of time averages in general stochastic systems. The focus is on communication and queueing systems, including wireless networks with time-varying channels, mobility, and randomly arriving traffic. A simple drift-plus-penalty framework is used to optimize time averages such as throughput, throughput-utility, power, and distortion. Explicit performance-delay tradeoffs are provided to illustrate the cost of approaching optimality. This theory is also applicable to problems in operations research and economics, where energy-efficient and profit-maximizing decisions must be made without knowing the future. Topics in the text include the following: - Queue stability theory - Backpressure, max-weight, and virtual queue methods - Primal-dual methods for non-convex stochastic utility maximization - Universal scheduling theory for arbitrary sample paths - Approximate and randomized scheduling theory - Optimization of renewal systems and Markov decision systems Detailed examples and numerous problem set questions are provided to reinforce the main concepts. Table of Contents: Introduction / Introduction to Queues / Dynamic Scheduling Example / Optimizing Time Averages / Optimizing Functions of Time Averages / Approximate Scheduling / Optimization of Renewal Systems / Conclusions}
}

@book{srikant_comm_2014,
author = {Srikant, R. and Ying, Lei},
title = {Communication Networks: An Optimization, Control and Stochastic Networks Perspective},
year = {2014},
isbn = {1107036054},
publisher = {Cambridge University Press},
address = {USA},
abstract = {Provides a modern mathematical approach to the design of communication networks for graduate students, blending control, optimization, and stochastic network theories. A broad range of performance analysis tools are discussed, including important advanced topics that have been made accessible to students for the first time. Taking a top-down approach to network protocol design, the authors begin with the deterministic model and progress to more sophisticated models. Network algorithms and protocols are tied closely to the theory, illustrating the practical engineering applications of each topic. The background behind the mathematical analyses is given before the formal proofs and is supported by worked examples, enabling students to understand the big picture before going into the detailed theory. End-of-chapter problems cover a range of difficulties, with complex problems broken into several parts, and hints to many problems are provided to guide students. Full solutions are available online for instructors.}
}

@article{stolyar_mw_2004,
 ISSN = {10505164},
 URL = {http://www.jstor.org/stable/4140489},
 abstract = {We consider a generalized switch model, which includes as special cases the model of multiuser data scheduling over a wireless medium, the input-queued cross-bar switch model and a discrete time version of a parallel server queueing system. Input flows n = 1, ..., N are served in discrete time by a switch. The switch state follows a finite state, discrete time Markov chain. In each state m, the switch chooses a scheduling decision k from a finite set K(m), which has the associated service rate vector ($\mu_1^m(k), ..., \mu_N^m(k)$). We consider a heavy traffic regime, and assume a Resource Pooling (RP) condition. Associated with this condition is a notion of workload $X = \sum_n \zeta_n Q_n$, where $\zeta = (\zeta_1, ..., \zeta_N)$ is some fixed nonzero vector with nonnegative components, and $Q_1, ..., Q_N$ are the queue lengths. We study the MaxWeight discipline which always chooses a decision k maximizing $\sum_n/\gamma_n[Q_n]^\beta\mu_n^m(k)$, that is, $k\in argmax_{_{_{\!\!\!\!\!\!\!_i}}}\sum\limits_n\gamma_n[Q_n]^\beta \mu_n^m(i)$ where $\beta > 0, \gamma_1 > 0, ..., \gamma_N > 0$ are arbitrary parameters. We prove that under MaxWeight scheduling and the RP condition, in the heavy traffic limit, the queue length process has the following properties: (a) The vector ($\gamma_1Q_1^\beta, ..., \gamma_NQ_N^\beta$) is always proportional to ζ (this is "State Space Collapse"), (b) the workload process converges to a Reflected Brownian Motion, (c) MaxWeight minimizes the workload among all disciplines. As a corollary of these properties, MaxWeight asymptotically minimizes the holding cost rate $\sum\limits_n\gamma_nQ_n^{\beta+1}$ at all times, and cumulative cost (with this rate) over finite intervals.},
 author = {Alexander L. Stolyar},
 journal = {The Annals of Applied Probability},
 number = {1},
 pages = {1--53},
 publisher = {Institute of Mathematical Statistics},
 title = {Maxweight Scheduling in a Generalized Switch: State Space Collapse and Workload Minimization in Heavy Traffic},
 urldate = {2023-09-21},
 volume = {14},
 year = {2004}
}



@INPROCEEDINGS{tassiulas_mw_1990,
  author={Tassiulas, L. and Ephremides, A.},
  booktitle={29th IEEE Conference on Decision and Control}, 
  title={Stability properties of constrained queueing systems and scheduling policies for maximum throughput in multihop radio networks}, 
  year={1990},
  volume={},
  number={},
  pages={2130-2132 vol.4},
  doi={10.1109/CDC.1990.204000}
}


@book{challot_deep_2017,
author = {Chollet, Francois},
title = {Deep Learning with Python},
year = {2017},
isbn = {1617294438},
publisher = {Manning Publications Co.},
address = {USA},
edition = {1st},
abstract = {Summary Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Franois Chollet, this book builds your understanding through intuitive explanations and practical examples. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology Machine learning has made remarkable progress in recent years. We went from near-unusable speech and image recognition, to near-human accuracy. We went from machines that couldn't beat a serious Go player, to defeating a world champion. Behind this progress is deep learninga combination of engineering advances, best practices, and theory that enables a wealth of previously impossible smart applications. About the Book Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Franois Chollet, this book builds your understanding through intuitive explanations and practical examples. You'll explore challenging concepts and practice with applications in computer vision, natural-language processing, and generative models. By the time you finish, you'll have the knowledge and hands-on skills to apply deep learning in your own projects. What's Inside Deep learning from first principles Setting up your own deep-learning environment Image-classification models Deep learning for text and sequences Neural style transfer, text generation, and image generation About the Reader Readers need intermediate Python skills. No previous experience with Keras, TensorFlow, or machine learning is required. About the Author Franois Chollet works on deep learning at Google in Mountain View, CA. He is the creator of the Keras deep-learning library, as well as a contributor to the TensorFlow machine-learning framework. He also does deep-learning research, with a focus on computer vision and the application of machine learning to formal reasoning. His papers have been published at major conferences in the field, including the Conference on Computer Vision and Pattern Recognition (CVPR), the Conference and Workshop on Neural Information Processing Systems (NIPS), the International Conference on Learning Representations (ICLR), and others.}
}


@inproceedings{russell_qdecomp_2003,
author = {Russell, Stuart and Zimdars, Andrew L.},
title = {Q-Decomposition for Reinforcement Learning Agents},
year = {2003},
isbn = {1577351894},
publisher = {AAAI Press},
abstract = {The paper explores a very simple agent design method called Q-decomposition, wherein a complex agent is built from simpler subagents. Each subagent has its own reward function and runs its own reinforcement learning process. It supplies to a central arbitrator the Q-values (according to its own reward function) for each possible action. The arbitrator selects an action maximizing the sum of Q-values from all the subagents. This approach has advantages over designs in which subagents recommend actions. It also has the property that if each subagent runs the Sarsa reinforcement learning algorithm to learn its local Q-function, then a globally optimal policy is achieved. (On the other hand, local Q-learning leads to globally suboptimal behavior.) In some cases, this form of agent decomposition allows the local Q-functions to be expressed by much-reduced state and action spaces. These results are illustrated in two domains that require effective coordination of behaviors.},
booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
pages = {656–663},
numpages = {8},
location = {Washington, DC, USA},
series = {ICML'03}
}

@InProceedings{wan_arqlearning_2021,
  title = 	 {Learning and Planning in Average-Reward Markov Decision Processes},
  author =       {Wan, Yi and Naik, Abhishek and Sutton, Richard S},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10653--10662},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/wan21a/wan21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/wan21a.html},
  abstract = 	 {We introduce learning and planning algorithms for average-reward MDPs, including 1) the first general proven-convergent off-policy model-free control algorithm without reference states, 2) the first proven-convergent off-policy model-free prediction algorithm, and 3) the first off-policy learning algorithm that converges to the actual value function rather than to the value function plus an offset. All of our algorithms are based on using the temporal-difference error rather than the conventional error when updating the estimate of the average reward. Our proof techniques are a slight generalization of those by Abounadi, Bertsekas, and Borkar (2001). In experiments with an Access-Control Queuing Task, we show some of the difficulties that can arise when using methods that rely on reference states and argue that our new algorithms are significantly easier to use.}
}


@INPROCEEDINGS{liu_rlqueueing_2019,
  author={Liu, Bai and Xie, Qiaomin and Modiano, Eytan},
  booktitle={2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
  title={Reinforcement Learning for Optimal Control of Queueing Systems}, 
  year={2019},
  volume={},
  number={},
  pages={663-670},
  doi={10.1109/ALLERTON.2019.8919665}
}
  
@InProceedings{nikishin_primacy_2022,
  title = 	 {The Primacy Bias in Deep Reinforcement Learning},
  author =       {Nikishin, Evgenii and Schwarzer, Max and D'Oro, Pierluca and Bacon, Pierre-Luc and Courville, Aaron},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {16828--16847},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/nikishin22a/nikishin22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/nikishin22a.html},
  abstract = 	 {This work identifies a common flaw of deep reinforcement learning (RL) algorithms: a tendency to rely on early interactions and ignore useful evidence encountered later. Because of training on progressively growing datasets, deep RL agents incur a risk of overfitting to earlier experiences, negatively affecting the rest of the learning process. Inspired by cognitive science, we refer to this effect as the primacy bias. Through a series of experiments, we dissect the algorithmic aspects of deep RL that exacerbate this bias. We then propose a simple yet generally-applicable mechanism that tackles the primacy bias by periodically resetting a part of the agent. We apply this mechanism to algorithms in both discrete (Atari 100k) and continuous action (DeepMind Control Suite) domains, consistently improving their performance.}
}

@misc{dohare_continual_2022,
title={Continual Backprop: Stochastic Gradient Descent with Persistent Randomness},
author={Shibhansh Dohare and Richard S. Sutton and A. Rupam Mahmood},
year={2022},
url={https://openreview.net/forum?id=86sEVRfeGYS}
}

@article{mccloskey_catastrophic_1989,
  title={Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
  author={Michael McCloskey and Neal J. Cohen},
  journal={Psychology of Learning and Motivation},
  year={1989},
  volume={24},
  pages={109-165}
}

@ARTICLE{ganti_qopen-prob_2007,
  author={Ganti, Anand and Modiano, Eytan and Tsitsiklis, John N.},
  journal={IEEE Transactions on Information Theory}, 
  title={Optimal Transmission Scheduling in Symmetric Communication Models With Intermittent Connectivity}, 
  year={2007},
  volume={53},
  number={3},
  pages={998-1008},
  abstract={We consider a slotted system with N queues, and independent and identically distributed (i.i.d.) Bernoulli arrivals at each queue during each slot. Each queue is associated with a channel that changes between "on" and "off" states according to i.i.d. Bernoulli processes. We assume that the system has K identical transmitters ("servers"). Each server, during each slot, can transmit up to C packets from each queue associated with an "on" channel. We show that a policy that assigns the servers to the longest queues whose channel is "on" minimizes the total queue size, as well as a broad class of other performance criteria. We provide several extensions, as well as some qualitative results for the limiting case where N is very large. Finally, we consider a "fluid" model under which fractional packets can be served, and subject to a constraint that at most C packets can be served in total from all of the N queues. We show that when K=N, there is an optimal policy which serves the queues so that the resulting vector of queue lengths is "Most Balanced" (MB)},
  keywords={},
  doi={10.1109/TIT.2006.890695},
  ISSN={1557-9654},
  month={March},}


@misc{arumugam_informationtheoretic_2021,
      title={An Information-Theoretic Perspective on Credit Assignment in Reinforcement Learning}, 
      author={Dilip Arumugam and Peter Henderson and Pierre-Luc Bacon},
      year={2021},
      eprint={2103.06224},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{naik_continuing_2021,
  title={Towards Reinforcement Learning in the Continuing Setting},
  author={Naik, Abhishek and Abbas, Zaheer and White, Adam and Sutton, Richard S.},
  url={https://drive.google.com/file/d/1xh7WjGP2VI_QdpjVWygRC1BuH6WB_gqi/view},
  year={2021}
}

@article{garcia2015comprehensive,
  title={A comprehensive survey on safe reinforcement learning},
  author={Garc{\i}a, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}

@inproceedings{berkenkamp2017safe,
  title={Safe model-based reinforcement learning with stability guarantees},
  author={Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela and Krause, Andreas},
  booktitle={Advances in neural information processing systems},
  pages={908--918},
  year={2017}
}


@book{altman1999constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}
@article{dalal2018safe,
  title={Safe exploration in continuous action spaces},
  author={Dalal, Gal and Dvijotham, Krishnamurthy and Vecerik, Matej and Hester, Todd and Paduraru, Cosmin and Tassa, Yuval},
  journal={arXiv preprint arXiv:1801.08757},
  year={2018}
}
@inproceedings{hans2008safe,
  title={Safe exploration for reinforcement learning.},
  author={Hans, Alexander and Schneega{\ss}, Daniel and Sch{\"a}fer, Anton Maximilian and Udluft, Steffen},
  booktitle={ESANN},
  pages={143--148},
  year={2008}
}
@article{garcia2012safe,
  title={Safe exploration of state and action spaces in reinforcement learning},
  author={Garcia, Javier and Fern{\'a}ndez, Fernando},
  journal={Journal of Artificial Intelligence Research},
  volume={45},
  pages={515--564},
  year={2012}
}
@inproceedings{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={22--31},
  year={2017},
  organization={JMLR. org}
}
@article{moldovan2012safe,
  title={Safe exploration in Markov decision processes},
  author={Moldovan, Teodor Mihai and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1205.4810},
  year={2012}
}
@inproceedings{turchetta2016safe,
  title={Safe exploration in finite markov decision processes with gaussian processes},
  author={Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4312--4320},
  year={2016}
}

@misc{krishnasamy2019learning,
      title={Learning Unknown Service Rates in Queues: A Multi-Armed Bandit Approach}, 
      author={Subhashini Krishnasamy and Rajat Sen and Ramesh Johari and Sanjay Shakkottai},
      year={2019},
      eprint={1604.06377},
      archivePrefix={arXiv},
      primaryClass={cs.SY}
}

@inproceedings{krishnasamy2018learning,
  title={On Learning the c$\mu$ Rule in Single and Parallel Server Networks},
  author={Krishnasamy, Subhashini and Arapostathis, Ari and Johari, Ramesh and Shakkottai, Sanjay},
  booktitle={2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  pages={153--154},
  year={2018},
  organization={IEEE}
}


@inproceedings{dean2019safely,
  title={Safely learning to control the constrained linear quadratic regulator},
  author={Dean, Sarah and Tu, Stephen and Matni, Nikolai and Recht, Benjamin},
  booktitle={2019 American Control Conference (ACC)},
  pages={5582--5588},
  year={2019},
  organization={IEEE}
}

@inproceedings{koller2018learning,
  title={Learning-based Model Predictive Control for Safe Exploration},
  author={Koller, Torsten and Berkenkamp, Felix and Turchetta, Matteo and Krause, Andreas},
  booktitle={2018 IEEE Conference on Decision and Control (CDC)},
  pages={6059--6066},
  year={2018},
  organization={IEEE}
}

@article{wabersich2018safe,
  title={Safe exploration of nonlinear dynamical systems: A predictive safety filter for reinforcement learning},
  author={Wabersich, Kim P and Zeilinger, Melanie N},
  journal={arXiv preprint arXiv:1812.05506},
  year={2018}
  }

@inproceedings{chow2018lyapunov,
  title={A lyapunov-based approach to safe reinforcement learning},
  author={Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8092--8101},
  year={2018}
}

@inproceedings{vinogradska2016stability,
  title={Stability of controllers for Gaussian process forward models},
  author={Vinogradska, Julia and Bischoff, Bastian and Nguyen-Tuong, Duy and Romer, Anne and Schmidt, Henner and Peters, Jan},
  booktitle={International Conference on Machine Learning},
  pages={545--554},
  year={2016}
}

@inproceedings{yu2019convergent,
  title={Convergent policy optimization for safe reinforcement learning},
  author={Yu, Ming and Yang, Zhuoran and Kolar, Mladen and Wang, Zhaoran},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3121--3133},
  year={2019}
}

  


@book{puterman_mdp_2014,
  added-at = {2017-04-07T12:13:11.000+0200},
  author = {Puterman, Martin L},
  biburl = {https://www.bibsonomy.org/bibtex/22e7ac99cd30c4892171e5a7cef1bc7a7/becker},
  interhash = {6cec8f775a265d8741171d17e4a4e7d0},
  intrahash = {2e7ac99cd30c4892171e5a7cef1bc7a7},
  keywords = {inthesis diss markov chain decision process citedby:scholar:count:9594 citedby:scholar:timestamp:2017-4-7},
  publisher = {John Wiley \& Sons},
  timestamp = {2017-04-07T12:13:11.000+0200},
  title = {Markov decision processes: discrete stochastic dynamic programming},
  year = 2014
}



@misc{brockman_gym_2016,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}
@article{raffin_sb3_2021,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@misc{alegre_sumorl_2019,
    author = {Lucas N. Alegre},
    title = {{SUMO-RL}},
    year = {2019},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/LucasAlegre/sumo-rl}},
}

@inproceedings{behrisch_sumo_2011,
author = {Behrisch, Michael and Bieker-Walz, Laura and Erdmann, Jakob and Krajzewicz, Daniel},
year = {2011},
month = {10},
pages = {},
title = {SUMO – Simulation of Urban MObility: An Overview},
volume = {2011},
isbn = {978-1-61208-169-4},
journal = {Proceedings of SIMUL}
}

@inproceedings{ault_resco_2021,
  title={Reinforcement Learning Benchmarks for Traffic Signal Control},
  author={James Ault and Guni Sharon},
  booktitle={Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS 2021) Datasets and Benchmarks Track},
  month={December},
  year={2021}
}

@inproceedings{
macglashan_valuedecom_2022,
title={Value Function Decomposition for Iterative Design of Reinforcement Learning Agents},
author={James MacGlashan and Evan Archer and Alisa Devlic and Takuma Seno and Craig Sherstan and Peter R. Wurman and Peter Stone},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=pNEisJqGuei}
}

@inproceedings{
fatemi_orchestrated_2022,
title={Orchestrated Value Mapping for Reinforcement Learning},
author={Mehdi Fatemi and Arash Tavakoli},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=c87d0TS4yX}
}

@article{khetarpal_crl_2020,
  title={Towards Continual Reinforcement Learning: A Review and Perspectives},
  author={Khimya Khetarpal and Matthew Riemer and Irina Rish and Doina Precup},
  journal={J. Artif. Intell. Res.},
  year={2020},
  volume={75},
  pages={1401-1476}
}

@article{sharma_autorl_2021,
  title={Autonomous Reinforcement Learning: Formalism and Benchmarking},
  author={Archit Sharma and Kelvin Xu and Nikhil Sardana and Abhishek Gupta and Karol Hausman and Sergey Levine and Chelsea Finn},
  journal={ArXiv},
  year={2021},
  volume={abs/2112.09605}
}

@book{sutton_rlbook_2018,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Reinforcement Learning: An Introduction},
year = {2018},
isbn = {0262039249},
publisher = {A Bradford Book},
address = {Cambridge, MA, USA},
abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.}
}

@inproceedings{
kapturowski_sqrt_2018,
title={Recurrent Experience Replay in Distributed Reinforcement Learning},
author={Steven Kapturowski and Georg Ostrovski and Will Dabney and John Quan and Remi Munos},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=r1lyTjAqYX},
}

@misc{hafner_dreamerv3_2023,
      title={Mastering Diverse Domains through World Models}, 
      author={Danijar Hafner and Jurgis Pasukonis and Jimmy Ba and Timothy Lillicrap},
      year={2023},
      eprint={2301.04104},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@article{bai_rlqn_2022,
author = {Liu, Bai and Xie, Qiaomin and Modiano, Eytan},
title = {RL-QN: A Reinforcement Learning Framework for Optimal Control of Queueing Systems},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {1},
issn = {2376-3639},
url = {https://doi.org/10.1145/3529375},
doi = {10.1145/3529375},
abstract = {With the rapid advance of information technology, network systems have become increasingly complex and hence the underlying system dynamics are often unknown or difficult to characterize. Finding a good network control policy is of significant importance to achieve desirable network performance (e.g., high throughput or low delay). In this work, we consider using model-based reinforcement learning (RL) to learn the optimal control policy for queueing networks so that the average job delay (or equivalently the average queue backlog) is minimized. Traditional approaches in RL, however, cannot handle the unbounded state spaces of the network control problem. To overcome this difficulty, we propose a new algorithm, called RL for Queueing Networks (RL-QN), which applies model-based RL methods over a finite subset of the state space while applying a known stabilizing policy for the rest of the states. We establish that the average queue backlog under RL-QN with an appropriately constructed subset can be arbitrarily close to the optimal result. We evaluate RL-QN in dynamic server allocation, routing, and switching problems. Simulation results show that RL-QN minimizes the average queue backlog effectively.},
journal = {ACM Trans. Model. Perform. Eval. Comput. Syst.},
month = {aug},
articleno = {2},
numpages = {35},
keywords = {reinforcement learning, Queueing networks}
}

@article{dai_ppo_2022,
author = {Dai, J. G. and Gluzman, Mark},
title = {Queueing Network Controls via Deep Reinforcement Learning},
journal = {Stochastic Systems},
volume = {12},
number = {1},
pages = {30-67},
year = {2022},
doi = {10.1287/stsy.2021.0081},
URL = {
        https://doi.org/10.1287/stsy.2021.0081
},
eprint = { 
        https://doi.org/10.1287/stsy.2021.0081
}
,
    abstract = { Novel advanced policy gradient (APG) methods, such as trust region policy optimization and proximal policy optimization (PPO), have become the dominant reinforcement learning algorithms because of their ease of implementation and good practical performance. A conventional setup for notoriously difficult queueing network control problems is a Markov decision problem (MDP) that has three features: infinite state space, unbounded costs, and long-run average cost objective. We extend the theoretical framework of these APG methods for such MDP problems. The resulting PPO algorithm is tested on a parallel-server system and large-size multiclass queueing networks. The algorithm consistently generates control policies that outperform state-of-art heuristics in literature in a variety of load conditions from light to heavy traffic. These policies are demonstrated to be near optimal when the optimal policy can be computed. A key to the successes of our PPO algorithm is the use of three variance reduction techniques in estimating the relative value function via sampling. First, we use a discounted relative value function as an approximation of the relative value function. Second, we propose regenerative simulation to estimate the discounted relative value function. Finally, we incorporate the approximating martingale-process method into the regenerative estimator. }
}




@article{naik_discnotopt_2019,
  author       = {Abhishek Naik and
                  Roshan Shariff and
                  Niko Yasui and
                  Richard S. Sutton},
  title        = {Discounted Reinforcement Learning is Not an Optimization Problem},
  journal      = {CoRR},
  volume       = {abs/1910.02140},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.02140},
  eprinttype    = {arXiv},
  eprint       = {1910.02140},
  timestamp    = {Wed, 09 Oct 2019 14:07:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-02140.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{schulman_ppo_2017,
  author       = {John Schulman and
                  Filip Wolski and
                  Prafulla Dhariwal and
                  Alec Radford and
                  Oleg Klimov},
  title        = {Proximal Policy Optimization Algorithms},
  journal      = {CoRR},
  volume       = {abs/1707.06347},
  year         = {2017},
  url          = {http://arxiv.org/abs/1707.06347},
  eprinttype    = {arXiv},
  eprint       = {1707.06347},
  timestamp    = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{schulman_trpo_2015,
  title = 	 {Trust Region Policy Optimization},
  author = 	 {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1889--1897},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/schulman15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/schulman15.html},
  abstract = 	 {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.}
}


@misc{fu_d4rl_2020,
    title={D4RL: Datasets for Deep Data-Driven Reinforcement Learning},
    author={Justin Fu and Aviral Kumar and Ofir Nachum and George Tucker and Sergey Levine},
    year={2020},
    eprint={2004.07219},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@ARTICLE{barnard_nnextra_1992,
  author={Barnard, E. and Wessels, L.F.A.},
  journal={IEEE Control Systems Magazine}, 
  title={Extrapolation and interpolation in neural network classifiers}, 
  year={1992},
  volume={12},
  number={5},
  pages={50-53},
  doi={10.1109/37.158898}}


@INPROCEEDINGS{haley_nnextra_1992,
  author={Haley, P.J. and Soloway, D.},
  booktitle={[Proceedings 1992] IJCNN International Joint Conference on Neural Networks}, 
  title={Extrapolation limitations of multilayer feedforward neural networks}, 
  year={1992},
  volume={4},
  number={},
  pages={25-30 vol.4},
  doi={10.1109/IJCNN.1992.227294}}


@inproceedings{
xu_nnextra_2021,
title={How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks},
author={Keyulu Xu and Mozhi Zhang and Jingling Li and Simon Shaolei Du and Ken-Ichi Kawarabayashi and Stefanie Jegelka},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=UH-cmocLJC}
}

@InProceedings{shah_unbounded_2020,
  title = 	 {Stable Reinforcement Learning with Unbounded State Space},
  author =       {Shah, Devavrat and Xie, Qiaomin and Xu, Zhi},
  booktitle = 	 {Proceedings of the 2nd Conference on Learning for Dynamics and Control},
  pages = 	 {581--581},
  year = 	 {2020},
  editor = 	 {Bayen, Alexandre M. and Jadbabaie, Ali and Pappas, George and Parrilo, Pablo A. and Recht, Benjamin and Tomlin, Claire and Zeilinger, Melanie},
  volume = 	 {120},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--11 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v120/shah20a/shah20a.pdf},
  url = 	 {https://proceedings.mlr.press/v120/shah20a.html},
  abstract = 	 {We consider the problem of reinforcement learning (RL) with unbounded state space motivated by the classical problem of scheduling in a queueing network. We argue that a reasonable RL policy for such settings must be based on online training, since any policy based only on finite samples cannot perform well in the entire unbounded state space. We introduce such an online RL policy using Sparse-Sampling-based Monte Carlo Oracle. To analyze this policy, we propose an appropriate notion of desirable performance in terms of stability: the state dynamics under the policy should remain in a bounded region with high probability. We show that if the system dynamics under optimal policy respects a Lyapunov function, then our policy is stable. Our policy does not need to know the Lyapunov function. Moreover, the assumption of existence Lyapunov function is not restrictive as this assumption is equivalent to the positive recurrence or stability property of any Markov chain, i.e., if there is any policy that can stabilize the system then it must posses a Lyapunov function.}
}


@InProceedings{zhang_artrpo_2021,
  title = 	 {On-Policy Deep Reinforcement Learning for the Average-Reward Criterion},
  author =       {Zhang, Yiming and Ross, Keith W},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12535--12545},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zhang21q/zhang21q.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zhang21q.html},
  abstract = 	 {We develop theory and algorithms for average-reward on-policy Reinforcement Learning (RL). We first consider bounding the difference of the long-term average reward for two policies. We show that previous work based on the discounted return (Schulman et al. 2015, Achiam et al. 2017) results in a non-meaningful lower bound in the average reward setting. By addressing the average-reward criterion directly, we then derive a novel bound which depends on the average divergence between the policies and on Kemeny’s constant. Based on this bound, we develop an iterative procedure which produces a sequence of monotonically improved policies for the average reward criterion. This iterative procedure can then be combined with classic Deep Reinforcement Learning (DRL) methods, resulting in practical DRL algorithms that target the long-run average reward criterion. In particular, we demonstrate that Average-Reward TRPO (ATRPO), which adapts the on-policy TRPO algorithm to the average-reward criterion, significantly outperforms TRPO in the most challenging MuJuCo environments.}
}


@inproceedings{
engstrom_implementation_2020,
title={Implementation Matters in Deep RL: A Case Study on PPO and TRPO},
author={Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1etN1rtPB}
}


@inproceedings{eysenbach_leave_2018,
	title = {Leave no {Trace}: {Learning} to {Reset} for {Safe} and {Autonomous} {Reinforcement} {Learning}},
	shorttitle = {Leave no {Trace}},
	url = {https://openreview.net/forum?id=S1vuO-bCW},
	abstract = {Deep reinforcement learning algorithms can learn complex behavioral skills, but real-world application of these methods requires a considerable amount of experience to be collected by the agent. In practical settings, such as robotics, this involves repeatedly attempting a task, resetting the environment between each attempt. However, not all tasks are easily or automatically reversible. In practice, this learning process requires considerable human intervention. In this work, we propose an autonomous method for safe and efficient reinforcement learning that simultaneously learns a forward and backward policy, with the backward policy resetting the environment for a subsequent attempt. By learning a value function for the backward policy, we can automatically determine when the forward policy is about to enter a non-reversible state, providing for uncertainty-aware safety aborts. Our experiments illustrate that proper use of the backward policy can greatly reduce the number of manual resets required to learn a task and can reduce the number of unsafe actions that lead to non-reversible states.},
	language = {en},
	urldate = {2023-04-26},
	author = {Eysenbach, Benjamin and Gu, Shixiang and Ibarz, Julian and Levine, Sergey},
	month = feb,
	year = {2018},
}

@inproceedings{han_learning_2015,
	title = {Learning compound multi-step controllers under unknown dynamics},
	doi = {10.1109/IROS.2015.7354297},
	abstract = {Applications of reinforcement learning for robotic manipulation often assume an episodic setting. However, controllers trained with reinforcement learning are often situated in the context of a more complex compound task, where multiple controllers might be invoked in sequence to accomplish a higher-level goal. Furthermore, training such controllers typically requires resetting the environment between episodes, which is typically handled manually. We describe an approach for training chains of controllers with reinforcement learning. This requires taking into account the state distributions induced by preceding controllers in the chain, as well as automatically training reset controllers that can reset the task between episodes. The initial state of each controller is determined by the controller that precedes it, resulting in a non-stationary learning problem. We demonstrate that a recently developed method that optimizes linear-Gaussian controllers under learned local linear models can tackle this sort of non-stationary problem, and that training controllers concurrently with a corresponding reset controller only minimally increases training time. We also demonstrate this method on a complex tool use task that consists of seven stages and requires using a toy wrench to screw in a bolt. This compound task requires grasping and handling complex contact dynamics. After training, the controllers can execute the entire task quickly and efficiently. Finally, we show that this method can be combined with guided policy search to automatically train nonlinear neural network controllers for a grasping task with considerable variation in target position.},
	booktitle = {2015 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	author = {Han, Weiqiao and Levine, Sergey and Abbeel, Pieter},
	month = sep,
	year = {2015},
	keywords = {Compounds, Heuristic algorithms, Learning (artificial intelligence), Neural networks, Robots, Training, Trajectory},
	pages = {6435--6442},
}

@misc{gupta_reset-free_2021,
	title = {Reset-{Free} {Reinforcement} {Learning} via {Multi}-{Task} {Learning}: {Learning} {Dexterous} {Manipulation} {Behaviors} without {Human} {Intervention}},
	shorttitle = {Reset-{Free} {Reinforcement} {Learning} via {Multi}-{Task} {Learning}},
	url = {http://arxiv.org/abs/2104.11203},
	doi = {10.48550/arXiv.2104.11203},
	abstract = {Reinforcement Learning (RL) algorithms can in principle acquire complex robotic skills by learning from large amounts of data in the real world, collected via trial and error. However, most RL algorithms use a carefully engineered setup in order to collect data, requiring human supervision and intervention to provide episodic resets. This is particularly evident in challenging robotics problems, such as dexterous manipulation. To make data collection scalable, such applications require reset-free algorithms that are able to learn autonomously, without explicit instrumentation or human intervention. Most prior work in this area handles single-task learning. However, we might also want robots that can perform large repertoires of skills. At first, this would appear to only make the problem harder. However, the key observation we make in this work is that an appropriately chosen multi-task RL setting actually alleviates the reset-free learning challenge, with minimal additional machinery required. In effect, solving a multi-task problem can directly solve the reset-free problem since different combinations of tasks can serve to perform resets for other tasks. By learning multiple tasks together and appropriately sequencing them, we can effectively learn all of the tasks together reset-free. This type of multi-task learning can effectively scale reset-free learning schemes to much more complex problems, as we demonstrate in our experiments. We propose a simple scheme for multi-task learning that tackles the reset-free learning problem, and show its effectiveness at learning to solve complex dexterous manipulation tasks in both hardware and simulation without any explicit resets. This work shows the ability to learn dexterous manipulation behaviors in the real world with RL without any human intervention.},
	urldate = {2023-04-26},
	publisher = {arXiv},
	author = {Gupta, Abhishek and Yu, Justin and Zhao, Tony Z. and Kumar, Vikash and Rovinsky, Aaron and Xu, Kelvin and Devlin, Thomas and Levine, Sergey},
	month = apr,
	year = {2021},
	note = {arXiv:2104.11203 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	annote = {Comment: Published at ICRA 2021. First four authors contributed equally},
}

@misc{zhu_ingredients_2020,
	title = {The {Ingredients} of {Real}-{World} {Robotic} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2004.12570},
	doi = {10.48550/arXiv.2004.12570},
	abstract = {The success of reinforcement learning for real world robotics has been, in many cases limited to instrumented laboratory scenarios, often requiring arduous human effort and oversight to enable continuous learning. In this work, we discuss the elements that are needed for a robotic learning system that can continually and autonomously improve with data collected in the real world. We propose a particular instantiation of such a system, using dexterous manipulation as our case study. Subsequently, we investigate a number of challenges that come up when learning without instrumentation. In such settings, learning must be feasible without manually designed resets, using only on-board perception, and without hand-engineered reward functions. We propose simple and scalable solutions to these challenges, and then demonstrate the efficacy of our proposed system on a set of dexterous robotic manipulation tasks, providing an in-depth analysis of the challenges associated with this learning paradigm. We demonstrate that our complete system can learn without any human intervention, acquiring a variety of vision-based skills with a real-world three-fingered hand. Results and videos can be found at https://sites.google.com/view/realworld-rl/},
	urldate = {2023-04-26},
	publisher = {arXiv},
	author = {Zhu, Henry and Yu, Justin and Gupta, Abhishek and Shah, Dhruv and Hartikainen, Kristian and Singh, Avi and Kumar, Vikash and Levine, Sergey},
	month = apr,
	year = {2020},
	note = {arXiv:2004.12570 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
	annote = {Comment: First three authors contributed equally. Accepted as a spotlight presentation at ICLR 2020},
}



@misc{sharma_autonomous_2021,
	title = {Autonomous {Reinforcement} {Learning}: {Formalism} and {Benchmarking}},
	shorttitle = {Autonomous {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2112.09605},
	abstract = {Reinforcement learning (RL) provides a naturalistic framing for learning through trial and error, which is appealing both because of its simplicity and effectiveness and because of its resemblance to how humans and animals acquire skills through experience. However, real-world embodied learning, such as that performed by humans and animals, is situated in a continual, non-episodic world, whereas common benchmark tasks in RL are episodic, with the environment resetting between trials to provide the agent with multiple attempts. This discrepancy presents a major challenge when attempting to take RL algorithms developed for episodic simulated environments and run them on real-world platforms, such as robots. In this paper, we aim to address this discrepancy by laying out a framework for Autonomous Reinforcement Learning (ARL): reinforcement learning where the agent not only learns through its own experience, but also contends with lack of human supervision to reset between trials. We introduce a simulated benchmark EARL around this framework, containing a set of diverse and challenging simulated tasks reflective of the hurdles introduced to learning when only a minimal reliance on extrinsic intervention can be assumed. We show that standard approaches to episodic RL and existing approaches struggle as interventions are minimized, underscoring the need for developing new algorithms for reinforcement learning with a greater focus on autonomy.},
	urldate = {2022-06-20},
	publisher = {arXiv},
	author = {Sharma, Archit and Xu, Kelvin and Sardana, Nikhil and Gupta, Abhishek and Hausman, Karol and Levine, Sergey and Finn, Chelsea},
	month = dec,
	year = {2021},
	note = {Number: arXiv:2112.09605
arXiv:2112.09605 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
}

@misc{chen_you_2022,
	title = {You {Only} {Live} {Once}: {Single}-{Life} {Reinforcement} {Learning}},
	shorttitle = {You {Only} {Live} {Once}},
	url = {http://arxiv.org/abs/2210.08863},
	abstract = {Reinforcement learning algorithms are typically designed to learn a performant policy that can repeatedly and autonomously complete a task, usually starting from scratch. However, in many real-world situations, the goal might not be to learn a policy that can do the task repeatedly, but simply to perform a new task successfully once in a single trial. For example, imagine a disaster relief robot tasked with retrieving an item from a fallen building, where it cannot get direct supervision from humans. It must retrieve this object within one test-time trial, and must do so while tackling unknown obstacles, though it may leverage knowledge it has of the building before the disaster. We formalize this problem setting, which we call single-life reinforcement learning (SLRL), where an agent must complete a task within a single episode without interventions, utilizing its prior experience while contending with some form of novelty. SLRL provides a natural setting to study the challenge of autonomously adapting to unfamiliar situations, and we find that algorithms designed for standard episodic reinforcement learning often struggle to recover from out-of-distribution states in this setting. Motivated by this observation, we propose an algorithm, \$Q\$-weighted adversarial learning (QWALE), which employs a distribution matching strategy that leverages the agent's prior experience as guidance in novel situations. Our experiments on several single-life continuous control problems indicate that methods based on our distribution matching formulation are 20-60\% more successful because they can more quickly recover from novel states.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Chen, Annie S. and Sharma, Archit and Levine, Sergey and Finn, Chelsea},
	month = oct,
	year = {2022},
	note = {arXiv:2210.08863 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: 17 pages},
}


@article{mahadevan_average_1996,
	title = {Average reward reinforcement learning: {Foundations}, algorithms, and empirical results},
	volume = {22},
	issn = {1573-0565},
	shorttitle = {Average reward reinforcement learning},
	url = {https://doi.org/10.1007/BF00114727},
	doi = {10.1007/BF00114727},
	abstract = {This paper presents a detailed study of average reward reinforcement learning, an undiscounted optimality framework that is more appropriate for cyclical tasks than the much better studied discounted framework. A wide spectrum of average reward algorithms are described, ranging from synchronous dynamic programming methods to several (provably convergent) asynchronous algorithms from optimal control and learning automata. A general sensitive discount optimality metric calledn-discount-optimality is introduced, and used to compare the various algorithms. The overview identifies a key similarity across several asynchronous algorithms that is crucial to their convergence, namely independent estimation of the average reward and the relative values. The overview also uncovers a surprising limitation shared by the different algorithms while several algorithms can provably generategain-optimal policies that maximize average reward, none of them can reliably filter these to producebias-optimal (orT-optimal) policies that also maximize the finite reward to absorbing goal states. This paper also presents a detailed empirical study of R-learning, an average reward reinforcement learning method, using two empirical testbeds: a stochastic grid world domain and a simulated robot environment. A detailed sensitivity analysis of R-learning is carried out to test its dependence on learning rates and exploration levels. The results suggest that R-learning is quite sensitive to exploration strategies and can fall into sub-optimal limit cycles. The performance of R-learning is also compared with that of Q-learning, the best studied discounted RL method. Here, the results suggest that R-learning can be fine-tuned to give better performance than Q-learning in both domains.},
	language = {en},
	number = {1},
	urldate = {2023-04-26},
	journal = {Machine Learning},
	author = {Mahadevan, Sridhar},
	month = mar,
	year = {1996},
	keywords = {Markov decision processes, Reinforcement learning},
	pages = {159--195},
}


@inproceedings{schwartz_reinforcement_1993,
	title = {A {Reinforcement} {Learning} {Method} for {Maximizing} {Undiscounted} {Rewards}},
	isbn = {978-1-55860-307-3},
	doi = {10.1016/B978-1-55860-307-3.50045-9},
	abstract = {While most Reinforcement Learning work utilizes temporal discounting to evaluate performance, the reasons for this are unclear. Is it out of desire or necessity? We argue that it is not out of desire, and seek to dispel the notion that temporal discounting is necessary by proposing a framework for undiscounted optimization. We present a metric of undiscounted performance and an algorithm for finding action policies that maximize that measure. The technique, which we call R-learning, is modelled after the popular Q-learning algorithm [17]. Initial experimental results are presented which attest to a great improvement over Q-learning in some simple cases.},
	author = {Schwartz, Anton},
	month = dec,
	year = {1993},
	pages = {298--305},
}



@misc{zhang_-policy_2021,
	title = {On-{Policy} {Deep} {Reinforcement} {Learning} for the {Average}-{Reward} {Criterion}},
	url = {http://arxiv.org/abs/2106.07329},
	abstract = {We develop theory and algorithms for average-reward on-policy Reinforcement Learning (RL). We first consider bounding the difference of the long-term average reward for two policies. We show that previous work based on the discounted return (Schulman et al., 2015; Achiam et al., 2017) results in a non-meaningful bound in the average-reward setting. By addressing the average-reward criterion directly, we then derive a novel bound which depends on the average divergence between the two policies and Kemeny's constant. Based on this bound, we develop an iterative procedure which produces a sequence of monotonically improved policies for the average reward criterion. This iterative procedure can then be combined with classic DRL (Deep Reinforcement Learning) methods, resulting in practical DRL algorithms that target the long-run average reward criterion. In particular, we demonstrate that Average-Reward TRPO (ATRPO), which adapts the on-policy TRPO algorithm to the average-reward criterion, significantly outperforms TRPO in the most challenging MuJuCo environments.},
	urldate = {2022-11-09},
	publisher = {arXiv},
	author = {Zhang, Yiming and Ross, Keith W.},
	month = jun,
	year = {2021},
	note = {arXiv:2106.07329 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: International Conference on Machine Learning (ICML) 2021},
}

@misc{wei_model-free_2020,
	title = {Model-free {Reinforcement} {Learning} in {Infinite}-horizon {Average}-reward {Markov} {Decision} {Processes}},
	url = {http://arxiv.org/abs/1910.07072},
	doi = {10.48550/arXiv.1910.07072},
	abstract = {Model-free reinforcement learning is known to be memory and computation efficient and more amendable to large scale problems. In this paper, two model-free algorithms are introduced for learning infinite-horizon average-reward Markov Decision Processes (MDPs). The first algorithm reduces the problem to the discounted-reward version and achieves \${\textbackslash}mathcal\{O\}(T{\textasciicircum}\{2/3\})\$ regret after \$T\$ steps, under the minimal assumption of weakly communicating MDPs. To our knowledge, this is the first model-free algorithm for general MDPs in this setting. The second algorithm makes use of recent advances in adaptive algorithms for adversarial multi-armed bandits and improves the regret to \${\textbackslash}mathcal\{O\}({\textbackslash}sqrt\{T\})\$, albeit with a stronger ergodic assumption. This result significantly improves over the \${\textbackslash}mathcal\{O\}(T{\textasciicircum}\{3/4\})\$ regret achieved by the only existing model-free algorithm by Abbasi-Yadkori et al. (2019a) for ergodic MDPs in the infinite-horizon average-reward setting.},
	urldate = {2023-04-26},
	publisher = {arXiv},
	author = {Wei, Chen-Yu and Jafarnia-Jahromi, Mehdi and Luo, Haipeng and Sharma, Hiteshi and Jain, Rahul},
	month = feb,
	year = {2020},
	note = {arXiv:1910.07072 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{zhang_average-reward_2021,
	title = {Average-{Reward} {Off}-{Policy} {Policy} {Evaluation} with {Function} {Approximation}},
	url = {https://proceedings.mlr.press/v139/zhang21u.html},
	abstract = {We consider off-policy policy evaluation with function approximation (FA) in average-reward MDPs, where the goal is to estimate both the reward rate and the differential value function. For this problem, bootstrapping is necessary and, along with off-policy learning and FA, results in the deadly triad (Sutton \& Barto, 2018). To address the deadly triad, we propose two novel algorithms, reproducing the celebrated success of Gradient TD algorithms in the average-reward setting. In terms of estimating the differential value function, the algorithms are the first convergent off-policy linear function approximation algorithms. In terms of estimating the reward rate, the algorithms are the first convergent off-policy linear function approximation algorithms that do not require estimating the density ratio. We demonstrate empirically the advantage of the proposed algorithms, as well as their nonlinear variants, over a competitive density-ratio-based approach, in a simple domain as well as challenging robot simulation tasks.},
	language = {en},
	urldate = {2023-04-26},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhang, Shangtong and Wan, Yi and Sutton, Richard S. and Whiteson, Shimon},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12578--12588},
}

@inproceedings{wan_learning_2021,
	title = {Learning and {Planning} in {Average}-{Reward} {Markov} {Decision} {Processes}},
	url = {https://proceedings.mlr.press/v139/wan21a.html},
	abstract = {We introduce learning and planning algorithms for average-reward MDPs, including 1) the first general proven-convergent off-policy model-free control algorithm without reference states, 2) the first proven-convergent off-policy model-free prediction algorithm, and 3) the first off-policy learning algorithm that converges to the actual value function rather than to the value function plus an offset. All of our algorithms are based on using the temporal-difference error rather than the conventional error when updating the estimate of the average reward. Our proof techniques are a slight generalization of those by Abounadi, Bertsekas, and Borkar (2001). In experiments with an Access-Control Queuing Task, we show some of the difficulties that can arise when using methods that rely on reference states and argue that our new algorithms are significantly easier to use.},
	language = {en},
	urldate = {2023-04-26},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wan, Yi and Naik, Abhishek and Sutton, Richard S.},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {10653--10662},
}


@inproceedings{westenbroek_lyapunov_2022,
  title={Lyapunov Design for Robust and Efficient Robotic Reinforcement Learning},
  author={Westenbroek, Tyler and Castaneda, Fernando and Agrawal, Ayush and Sastry, Shankar and Sreenath, Koushil},
  booktitle={6th Annual Conference on Robot Learning},
  year = {2022}
}

@article{bertsekas2015dynamic_vii,
  title={Dynamic Programming and Optimal Control, 4th Edition, Volume {II}},
  author={Bertsekas, Dimitri P},
  journal={Athena Scientific},
  year={2015}
}

@book{xudrift,
  title={Drift Method: from Stochastic Networks to Machine Learning},
  author={Xu, Kuang}
}

@book{meyn2012markov,
  title={Markov chains and stochastic stability},
  author={Meyn, Sean P and Tweedie, Richard L},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{sutton1999policy_grad,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@book{ajks,
  title={Reinforcement Learning: Theory and Algorithms},
  author={Agarwal, Alekh and Jiang, Nan and  Kakade,  Sham M and Sun,  Wen},
  year={2021}
}

@article{bhandari2019global,
  title={Global optimality guarantees for policy gradient methods},
  author={Bhandari, Jalaj and Russo, Daniel},
  journal={arXiv preprint arXiv:1906.01786},
  year={2019}
}

@book{el2012rate_stable,
  title={Sample-Path Analysis of Queueing Systems},
  author={El-Taha, Muhammad and Stidham Jr, Shaler},
  volume={11},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{dai2004rate_stable,
  title={Stabilizing queueing networks with setups},
  author={Dai, JG and Jennings, Otis B},
  journal={Mathematics of Operations Research},
  volume={29},
  number={4},
  pages={891--922},
  year={2004},
  publisher={INFORMS}
}

@article{hajek1982hitting,
  title={Hitting-time and occupation-time bounds implied by drift analysis with applications},
  author={Hajek, Bruce},
  journal={Advances in Applied probability},
  volume={14},
  number={3},
  pages={502--525},
  year={1982},
  publisher={Cambridge University Press}
}