@article{tay2021pre,
  title={Are Pre-trained Convolutions Better than Pre-trained Transformers?},
  author={Tay, Yi and Dehghani, Mostafa and Gupta, Jai and Bahri, Dara and Aribandi, Vamsi and Qin, Zhen and Metzler, Donald},
  journal={arXiv preprint arXiv:2105.03322},
  year={2021}
}

@inproceedings{peters2018dissecting,
  title={Dissecting contextual word embeddings: Architecture and representation},
  author={Peters, Matthew E and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
  booktitle={EMNLP},
  year={2018}
}
@inproceedings{conneau2018you,
  title={What you can cram into a single vector: Probing sentence embeddings for linguistic properties},
  author={Conneau, Alexis and Kruszewski, Germ{\'a}n and Lample, Guillaume and Barrault, Lo{\"\i}c and Baroni, Marco},
  booktitle={ACL},
  year={2018}
}
@inproceedings{voita2019bottom,
  title={The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives},
  author={Voita, Elena and Sennrich, Rico and Titov, Ivan},
  booktitle={EMNLP},
  year={2019}
}
@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}

@misc{naseer2021intriguing,
      title={Intriguing Properties of Vision Transformers}, 
      author={Muzammal Naseer and Kanchana Ranasinghe and Salman Khan and Munawar Hayat and Fahad Shahbaz Khan and Ming-Hsuan Yang},
      year={2021},
      eprint={2105.10497},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{nguyen2020wide,
  title={Do Wide and Deep Networks Learn the Same Things? Uncovering How Neural Network Representations Vary with Width and Depth},
  author={Nguyen, Thao and Raghu, Maithra and Kornblith, Simon},
  journal={arXiv preprint arXiv:2010.15327},
  year={2020}
}

@inproceedings{kornblith2019similarity,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={ICML},
  year={2019}
}

@article{howard2013some,
  title={Some improvements on deep convolutional neural network based image classification},
  author={Howard, Andrew G},
  journal={arXiv preprint arXiv:1312.5402},
  year={2013}
}

@article{zhai2019visual,
  title={The visual task adaptation benchmark},
  author={Zhai, Xiaohua and Puigcerver, Joan and Kolesnikov, Alexander and Ruyssen, Pierre and Riquelme, Carlos and Lucic, Mario and Djolonga, Josip and Pinto, Andre Susano and Neumann, Maxim and Dosovitskiy, Alexey and others},
  year={2019}
}

@article{luo2017understanding,
  title={Understanding the effective receptive field in deep convolutional neural networks},
  author={Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
  journal={arXiv preprint arXiv:1701.04128},
  year={2017}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={arXiv preprint arXiv:1706.03762},
  year={2017}
}

@inproceedings{parmar2018image,
  title={Image transformer},
  author={Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  booktitle={International Conference on Machine Learning},
  pages={4055--4064},
  year={2018},
  organization={PMLR}
}

@article{ramachandran2019stand,
  title={Stand-alone self-attention in vision models},
  author={Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jonathon},
  journal={arXiv preprint arXiv:1906.05909},
  year={2019}
}

@article{cordonnier2019relationship,
  title={On the relationship between self-attention and convolutional layers},
  author={Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  journal={arXiv preprint arXiv:1911.03584},
  year={2019}
}

@inproceedings{carion2020end,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European Conference on Computer Vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@inproceedings{bello2019attention,
  title={Attention augmented convolutional networks},
  author={Bello, Irwan and Zoph, Barret and Vaswani, Ashish and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3286--3295},
  year={2019}
}

@article{wu2020visual,
  title={Visual transformers: Token-based image representation and processing for computer vision},
  author={Wu, Bichen and Xu, Chenfeng and Dai, Xiaoliang and Wan, Alvin and Zhang, Peizhao and Yan, Zhicheng and Tomizuka, Masayoshi and Gonzalez, Joseph and Keutzer, Kurt and Vajda, Peter},
  journal={arXiv preprint arXiv:2006.03677},
  year={2020}
}

@inproceedings{chen2020generative,
  title={Generative pretraining from pixels},
  author={Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={1691--1703},
  year={2020},
  organization={PMLR}
}

@article{bhojanapalli2021understanding,
  title={Understanding robustness of transformers for image classification},
  author={Bhojanapalli, Srinadh and Chakrabarti, Ayan and Glasner, Daniel and Li, Daliang and Unterthiner, Thomas and Veit, Andreas},
  journal={arXiv preprint arXiv:2103.14586},
  year={2021}
}

@article{paul2021vision,
  title={Vision Transformers are Robust Learners},
  author={Paul, Sayak and Chen, Pin-Yu},
  journal={arXiv preprint arXiv:2105.07581},
  year={2021}
}

@article{chen2021empirical,
  title={An Empirical Study of Training Self-Supervised Vision Transformers},
  author={Chen, Xinlei and Xie, Saining and He, Kaiming},
  journal={arXiv preprint arXiv:2104.02057},
  year={2021}
}

@article{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  journal={arXiv preprint arXiv:2104.14294},
  year={2021}
}

@article{tolstikhin2021mlp,
  title={MLP-Mixer: An all-MLP architecture for vision},
  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and others},
  journal={arXiv preprint arXiv:2105.01601},
  year={2021}
}

@article{touvron2021resmlp,
  title={ResMLP: Feedforward networks for image classification with data-efficient training},
  author={Touvron, Hugo and Bojanowski, Piotr and Caron, Mathilde and Cord, Matthieu and El-Nouby, Alaaeldin and Grave, Edouard and Joulin, Armand and Synnaeve, Gabriel and Verbeek, Jakob and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:2105.03404},
  year={2021}
}

@article{raghu2017svcca,
  title={Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability},
  author={Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1706.05806},
  year={2017}
}

@article{morcos2018insights,
  title={Insights on representational similarity in neural networks with canonical correlation},
  author={Morcos, Ari S and Raghu, Maithra and Bengio, Samy},
  journal={arXiv preprint arXiv:1806.05759},
  year={2018}
}

@article{mustafa2021supervised,
  title={Supervised transfer learning at scale for medical imaging},
  author={Mustafa, Basil and Loh, Aaron and Freyberg, Jan and MacWilliams, Patricia and Wilson, Megan and McKinney, Scott Mayer and Sieniek, Marcin and Winkens, Jim and Liu, Yuan and Bui, Peggy and others},
  journal={arXiv preprint arXiv:2101.05913},
  year={2021}
}

@article{lindsay2020convolutional,
  title={Convolutional neural networks as a model of the visual system: past, present, and future},
  author={Lindsay, Grace W},
  journal={Journal of cognitive neuroscience},
  pages={1--15},
  year={2020},
  publisher={MIT Press}
}

@article{wu2019emerging,
  title={Emerging cross-lingual structure in pretrained language models},
  author={Wu, Shijie and Conneau, Alexis and Li, Haoran and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.01464},
  year={2019}
}

@article{raghu2019rapid,
  title={Rapid learning or feature reuse? towards understanding the effectiveness of maml},
  author={Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1909.09157},
  year={2019}
}

@article{kriegeskorte2008representational,
  title={Representational similarity analysis-connecting the branches of systems neuroscience},
  author={Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter A},
  journal={Frontiers in systems neuroscience},
  volume={2},
  pages={4},
  year={2008},
  publisher={Frontiers}
}

@article{merchant2020happens,
  title={What Happens To BERT Embeddings During Fine-tuning?},
  author={Merchant, Amil and Rahimtoroghi, Elahe and Pavlick, Ellie and Tenney, Ian},
  journal={arXiv preprint arXiv:2004.14448},
  year={2020}
}

@article{raghu2019transfusion,
  title={Transfusion: Understanding transfer learning for medical imaging},
  author={Raghu, Maithra and Zhang, Chiyuan and Kleinberg, Jon and Bengio, Samy},
  journal={arXiv preprint arXiv:1902.07208},
  year={2019}
}

@article{wu2020similarity,
  title={Similarity analysis of contextual word representation models},
  author={Wu, John M and Belinkov, Yonatan and Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim and Glass, James},
  journal={arXiv preprint arXiv:2005.01172},
  year={2020}
}

@article{kudugunta2019investigating,
  title={Investigating multilingual nmt representations at scale},
  author={Kudugunta, Sneha Reddy and Bapna, Ankur and Caswell, Isaac and Arivazhagan, Naveen and Firat, Orhan},
  journal={arXiv preprint arXiv:1909.02197},
  year={2019}
}

@article{kornblith2020s,
  title={What's in a Loss Function for Image Classification?},
  author={Kornblith, Simon and Lee, Honglak and Chen, Ting and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:2010.16402},
  year={2020}
}

@article{shi2019comparison,
  title={Comparison against task driven artificial neural networks reveals functional properties in mouse visual cortex},
  author={Shi, Jianghong and Shea-Brown, Eric and Buice, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={5764--5774},
  year={2019}
}

@article{maheswaranathan2019universality,
  title={Universality and individuality in neural dynamics across large populations of recurrent networks},
  author={Maheswaranathan, Niru and Williams, Alex H and Golub, Matthew D and Ganguli, Surya and Sussillo, David},
  journal={Advances in neural information processing systems},
  volume={2019},
  pages={15629},
  year={2019},
  publisher={NIH Public Access}
}

@inproceedings{sun2017revisiting,
  title={Revisiting unreasonable effectiveness of data in deep learning era},
  author={Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={843--852},
  year={2017}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{zhai2019large,
  title={A large-scale study of representation learning with the visual task adaptation benchmark},
  author={Zhai, Xiaohua and Puigcerver, Joan and Kolesnikov, Alexander and Ruyssen, Pierre and Riquelme, Carlos and Lucic, Mario and Djolonga, Josip and Pinto, Andre Susano and Neumann, Maxim and Dosovitskiy, Alexey and others},
  journal={arXiv preprint arXiv:1910.04867},
  year={2019}
}

@article{kolesnikov2019big,
  title={Big transfer (bit): General visual representation learning},
  author={Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
  journal={arXiv preprint arXiv:1912.11370},
  volume={6},
  number={2},
  pages={8},
  year={2019}
}

@article{song2012feature,
  title={Feature selection via dependence maximization.},
  author={Song, Le and Smola, Alex and Gretton, Arthur and Bedo, Justin and Borgwardt, Karsten},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={5},
  year={2012}
}

@article{cortes2012algorithms,
  title={Algorithms for learning kernels based on centered alignment},
  author={Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
  journal={The Journal of Machine Learning Research},
  volume={13},
  number={1},
  pages={795--828},
  year={2012},
  publisher={JMLR. org}
}

@inproceedings{gretton2007kernel,
  title={A kernel statistical test of independence},
  author={Gretton, Arthur and Fukumizu, Kenji and Teo, Choon Hui and Song, Le and Sch{\"o}lkopf, Bernhard and Smola, Alexander J and others},
  booktitle={Nips},
  volume={20},
  pages={585--592},
  year={2007},
  organization={Citeseer}
}

@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  pages={1097--1105},
  year={2012}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@article{yuan2021tokens,
  title={Tokens-to-token vit: Training vision transformers from scratch on imagenet},
  author={Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zihang and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2101.11986},
  year={2021}
}

@article{zhou2021deepvit,
  title={Deepvit: Towards deeper vision transformer},
  author={Zhou, Daquan and Kang, Bingyi and Jin, Xiaojie and Yang, Linjie and Lian, Xiaochen and Jiang, Zihang and Hou, Qibin and Feng, Jiashi},
  journal={arXiv preprint arXiv:2103.11886},
  year={2021}
}

@article{d2021convit,
  title={Convit: Improving vision transformers with soft convolutional inductive biases},
  author={d'Ascoli, St{\'e}phane and Touvron, Hugo and Leavitt, Matthew and Morcos, Ari and Biroli, Giulio and Sagun, Levent},
  journal={arXiv preprint arXiv:2103.10697},
  year={2021}
}