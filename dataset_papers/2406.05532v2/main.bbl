\begin{thebibliography}{10}

\bibitem{RangapuramSGSWJ18}
Syama~Sundar Rangapuram, Matthias~W. Seeger, Jan Gasthaus, Lorenzo Stella, Yuyang Wang, and Tim Januschowski.
\newblock Deep state space models for time series forecasting.
\newblock In {\em NeurIPS}, 2018.

\bibitem{GuDERR20}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R{\'{e}}.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock In {\em NeurIPS}, 2020.

\bibitem{GuGR22}
Albert Gu, Karan Goel, and Christopher R{\'{e}}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In {\em ICLR}, 2022.

\bibitem{qi2024smr}
Biqing Qi, Junqi Gao, Kaiyan Zhang, Dong Li, Jianxing Liu, Ligang Wu, and Bowen Zhou.
\newblock Smr: State memory replay for long sequence modeling, 2024.

\bibitem{GuJGSDRR21}
Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R{\'{e}}.
\newblock Combining recurrent, convolutional, and continuous-time models with linear state space layers.
\newblock In {\em NeurIPS}, 2021.

\bibitem{Mehta0CN23}
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur.
\newblock Long range language modeling via gated state spaces.
\newblock In {\em {ICLR}}, 2023.

\bibitem{GoelGDR22}
Karan Goel, Albert Gu, Chris Donahue, and Christopher R{\'{e}}.
\newblock It's raw! audio generation with state-space models.
\newblock In {\em {ICML}}, 2022.

\bibitem{abs-2210-06583}
Eric Nguyen, Karan Goel, Albert Gu, Gordon~W. Downs, Preey Shah, Tri Dao, Stephen~A. Baccus, and Christopher R{\'{e}}.
\newblock {S4ND:} modeling images and videos as multidimensional signals using state spaces.
\newblock {\em CoRR}, abs/2210.06583, 2022.

\bibitem{SzegedyZSBEGF13}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian~J. Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In {\em ICLR}, 2014.

\bibitem{GoodfellowSS14}
Ian~J. Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In {\em ICLR}, 2015.

\bibitem{GaoQLGLXZ23}
Junqi Gao, Biqing Qi, Yao Li, Zhichang Guo, Dong Li, Yuming Xing, and Dazhi Zhang.
\newblock Perturbation towards easy samples improves targeted adversarial transferability.
\newblock In {\em NeurIPS 2023}, 2023.

\bibitem{QiGLWZ24}
Biqing Qi, Junqi Gao, Jianxing Liu, Ligang Wu, and Bowen Zhou.
\newblock Enhancing adversarial transferability via information bottleneck constraints.
\newblock {\em {IEEE} Signal Process. Lett.}, 31:1414--1418, 2024.

\bibitem{abs-2402-16397}
Biqing Qi, Junqi Gao, Yiang Luo, Jianxing Liu, Ligang Wu, and Bowen Zhou.
\newblock Investigating deep watermark security: An adversarial transferability perspective.
\newblock {\em CoRR}, abs/2402.16397, 2024.

\bibitem{abs-2403-10935}
Chengbin Du, Yanxi Li, and Chang Xu.
\newblock Understanding robustness of visual state space models for image classification.
\newblock {\em CoRR}, abs/2403.10935, 2024.

\bibitem{kurakin2016adversarial}
Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
\newblock Adversarial machine learning at scale.
\newblock {\em arXiv preprint arXiv:1611.01236}, 2016.

\bibitem{MadryMSTV18}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em ICLR}, 2018.

\bibitem{MiyatoDG17}
Takeru Miyato, Andrew~M. Dai, and Ian~J. Goodfellow.
\newblock Adversarial training methods for semi-supervised text classification.
\newblock In {\em ICLR}, 2017.

\bibitem{QiZZLW24}
Biqing Qi, Bowen Zhou, Weinan Zhang, Jianxing Liu, and Ligang Wu.
\newblock Improving robustness of intent detection under adversarial attacks: {A} geometric constraint perspective.
\newblock {\em {IEEE} Trans. Neural Networks Learn. Syst.}, 35(5):6133--6144, 2024.

\bibitem{MaZKHGNMZ23}
Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer.
\newblock Mega: Moving average equipped gated attention.
\newblock In {\em ICLR}, 2023.

\bibitem{abs-2312-00752}
Albert Gu and Tri Dao.
\newblock Mamba: Linear-time sequence modeling with selective state spaces.
\newblock {\em CoRR}, abs/2312.00752, 2023.

\bibitem{ZhangYJXGJ19}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric~P. Xing, Laurent~El Ghaoui, and Michael~I. Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In {\em ICML}, 2019.

\bibitem{TsiprasSETM19}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In {\em ICLR}, 2019.

\bibitem{PinotMAKYGA19}
Rafael Pinot, Laurent Meunier, Alexandre Araujo, Hisashi Kashima, Florian Yger, C{\'{e}}dric Gouy{-}Pailler, and Jamal Atif.
\newblock Theoretical evidence for adversarial robustness through randomization.
\newblock In {\em NeurIPS}, 2019.

\bibitem{RiceWK20}
Leslie Rice, Eric Wong, and J.~Zico Kolter.
\newblock Overfitting in adversarially robust deep learning.
\newblock In {\em ICML}, 2020.

\bibitem{DongXYPDSZ22}
Yinpeng Dong, Ke~Xu, Xiao Yang, Tianyu Pang, Zhijie Deng, Hang Su, and Jun Zhu.
\newblock Exploring memorization in adversarial training.
\newblock In {\em ICLR}, 2022.

\bibitem{GuG0R22}
Albert Gu, Karan Goel, Ankit Gupta, and Christopher R{\'{e}}.
\newblock On the parameterization and initialization of diagonal state space models.
\newblock In {\em NeurIPS}, 2022.

\bibitem{SmithWL23}
Jimmy T.~H. Smith, Andrew Warrington, and Scott~W. Linderman.
\newblock Simplified state space layers for sequence modeling.
\newblock In {\em ICLR}, 2023.

\bibitem{0001GB22}
Ankit Gupta, Albert Gu, and Jonathan Berant.
\newblock Diagonal state spaces are as effective as structured state spaces.
\newblock In {\em NeurIPS}, 2022.

\bibitem{Tustin1947AMO}
Arnold Tustin.
\newblock A method of analysing the behaviour of linear systems in terms of time series.
\newblock 1947.

\bibitem{ShafahiNG0DSDTG19}
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John~P. Dickerson, Christoph Studer, Larry~S. Davis, Gavin Taylor, and Tom Goldstein.
\newblock Adversarial training for free!
\newblock In {\em Advances in Neural Information Processing Systems}, pages 3353--3364, 2019.

\bibitem{ZhangZLZ019}
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong.
\newblock You only propagate once: Accelerating adversarial training via maximal principle.
\newblock In {\em Advances in Neural Information Processing Systems}, pages 227--238, 2019.

\bibitem{DBLP:conf/nips/SchmidtSTTM18}
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry.
\newblock Adversarially robust generalization requires more data.
\newblock In {\em NeurIPS}, 2018.

\bibitem{WangPDL0Y23}
Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan.
\newblock Better diffusion models further improve adversarial training.
\newblock In {\em ICML}, 2023.

\bibitem{Croce020a}
Francesco Croce and Matthias Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.
\newblock In {\em ICML}, 2020.

\bibitem{Croce020}
Francesco Croce and Matthias Hein.
\newblock Minimally distorted adversarial examples with a fast adaptive boundary attack.
\newblock In {\em ICML}, 2020.

\bibitem{AndriushchenkoC20}
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein.
\newblock Square attack: {A} query-efficient black-box adversarial attack via random search.
\newblock In {\em ECCV}, 2020.

\bibitem{SchottRBB19}
Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel.
\newblock Towards the first adversarially robust neural network model on {MNIST}.
\newblock In {\em ICLR}, 2019.

\bibitem{KLdiv}
S.~Kullback and R.~A. Leibler.
\newblock On information and sufficiency.
\newblock {\em The Annals of Mathematical Statistics}, (1):79--86, 1951.

\bibitem{TuZT19}
Zhuozhuo Tu, Jingwei Zhang, and Dacheng Tao.
\newblock Theoretical analysis of adversarial learning: {A} minimax approach.
\newblock In {\em NeurIPS}, 2019.

\bibitem{kurakin2018adversarial}
Alexey Kurakin, Ian~J Goodfellow, and Samy Bengio.
\newblock Adversarial examples in the physical world.
\newblock In {\em Artificial intelligence safety and security}. 2018.

\bibitem{athalye2018synthesizing}
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok.
\newblock Synthesizing robust adversarial examples.
\newblock In {\em ICML}, 2018.

\bibitem{mao2019metric}
Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, and Baishakhi Ray.
\newblock Metric learning for adversarial robustness.
\newblock In {\em NeurIPS}, 2019.

\bibitem{pang2019rethinking}
Tianyu Pang, Kun Xu, Yinpeng Dong, Chao Du, Ning Chen, and Jun Zhu.
\newblock Rethinking softmax cross-entropy loss for adversarial robustness.
\newblock {\em arXiv preprint arXiv:1905.10626}, 2019.

\bibitem{wang2022robustness}
Zekai Wang and Weiwei Liu.
\newblock Robustness verification for contrastive learning.
\newblock In {\em ICML}, 2022.

\bibitem{naseer2020self}
Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad~Shahbaz Khan, and Fatih Porikli.
\newblock A self-supervised approach for adversarial robustness.
\newblock In {\em CVPR}, 2020.

\bibitem{pang2019improving}
Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu.
\newblock Improving adversarial robustness via promoting ensemble diversity.
\newblock In {\em ICML}, 2019.

\bibitem{tramer2017ensemble}
Florian Tram{\`e}r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock {\em arXiv preprint arXiv:1705.07204}, 2017.

\bibitem{andriushchenko2020understanding}
Maksym Andriushchenko and Nicolas Flammarion.
\newblock Understanding and improving fast adversarial training.
\newblock In {\em NeurIPS}, 2020.

\bibitem{li2020towards}
Bai Li, Shiqi Wang, Suman Jana, and Lawrence Carin.
\newblock Towards understanding fast adversarial training.
\newblock {\em arXiv preprint arXiv:2006.03089}, 2020.

\bibitem{zhu2024vision}
Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang.
\newblock Vision mamba: Efficient visual representation learning with bidirectional state space model.
\newblock {\em arXiv preprint arXiv:2401.09417}, 2024.

\bibitem{liu2024vmamba}
Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu.
\newblock Vmamba: Visual state space model.
\newblock {\em arXiv preprint arXiv:2401.10166}, 2024.

\bibitem{wang2024state}
Shida Wang and Beichen Xue.
\newblock State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory.
\newblock In {\em NeurIPS}, 2024.

\bibitem{ren2023sparse}
Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, and Cheng~Xiang Zhai.
\newblock Sparse modular activation for efficient sequence modeling.
\newblock In {\em NeurIPS}, 2023.

\bibitem{liang2021uncovering}
Kaizhao Liang, Jacky~Y Zhang, Boxin Wang, Zhuolin Yang, Sanmi Koyejo, and Bo~Li.
\newblock Uncovering the connections between adversarial transferability and knowledge transferability.
\newblock In {\em ICML}, 2021.

\end{thebibliography}
