\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL}, 2019.

\bibitem[Ding et~al.(2021{\natexlab{a}})Ding, Wang, Liu, Wong, Tao, and
  Tu]{Ding:2021:ACL}
Ding, L., Wang, L., Liu, X., Wong, D.~F., Tao, D., and Tu, Z.
\newblock Rejuvenating low-frequency words: Making the most of parallel data in
  non-autoregressive translation.
\newblock In \emph{ACL}, 2021{\natexlab{a}}.

\bibitem[Ding et~al.(2021{\natexlab{b}})Ding, Wang, Liu, Wong, Tao, and
  Tu]{Ding:2021:ICLR}
Ding, L., Wang, L., Liu, X., Wong, D.~F., Tao, D., and Tu, Z.
\newblock Understanding and improving lexical choice in non-autoregressive
  translation.
\newblock In \emph{ICLR}, 2021{\natexlab{b}}.

\bibitem[Ghazvininejad et~al.(2019)Ghazvininejad, Levy, Liu, and
  Zettloyer]{maskp}
Ghazvininejad, M., Levy, O., Liu, Y., and Zettloyer, L.
\newblock Mask-predict: Parallel decoding of conditional masked language
  models.
\newblock In \emph{EMNLP}, 2019.

\bibitem[Ghazvininejad et~al.(2020)Ghazvininejad, Karpukhin, Zettlemoyer, and
  Levy]{axe}
Ghazvininejad, M., Karpukhin, V., Zettlemoyer, L., and Levy, O.
\newblock Aligned cross entropy for non-autoregressive machine translation.
\newblock In \emph{ICML}, 2020.

\bibitem[Gu et~al.(2018)Gu, Bradbury, Xiong, Li, and Socher]{NAT}
Gu, J., Bradbury, J., Xiong, C., Li, V.~O., and Socher, R.
\newblock Non-autoregressive neural machine translation.
\newblock In \emph{ICLR}, 2018.

\bibitem[Gu et~al.(2019)Gu, Wang, and Zhao]{gu2019levenshtein}
Gu, J., Wang, C., and Zhao, J.
\newblock Levenshtein transformer.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Huang et~al.(2009)Huang, Guestrin, Jiang, and Guibas]{permutations}
Huang, J., Guestrin, C., Jiang, X., and Guibas, L.
\newblock Exploiting probabilistic independence for permutations.
\newblock In \emph{AISTATS}, 2009.

\bibitem[Kang \& Hashimoto(2020)Kang and Hashimoto]{losstruncation}
Kang, D. and Hashimoto, T.
\newblock Improved natural language generation via loss truncation.
\newblock In \emph{ACL}, 2020.

\bibitem[Kasai et~al.(2020)Kasai, Cross, Ghazvininejad, and Gu]{Kasai2020DisCo}
Kasai, J., Cross, J., Ghazvininejad, M., and Gu, J.
\newblock Non-autoregressive machine translation with disentangled context
  transformer.
\newblock In \emph{ICML}, 2020.

\bibitem[Kasai et~al.(2021)Kasai, Pappas, Peng, Cross, and
  Smith]{kasai2021deep}
Kasai, J., Pappas, N., Peng, H., Cross, J., and Smith, N.
\newblock Deep encoder, shallow decoder: Reevaluating non-autoregressive
  machine translation.
\newblock In \emph{ICLR}, 2021.

\bibitem[Kim \& Rush(2016)Kim and Rush]{kim2016sequence}
Kim, Y. and Rush, A.~M.
\newblock Sequence-level knowledge distillation.
\newblock In \emph{EMNLP}, 2016.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2015adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Kong et~al.(2019)Kong, Tu, Shi, Hovy, and Zhang]{Kong:2019:AAAI}
Kong, X., Tu, Z., Shi, S., Hovy, E., and Zhang, T.
\newblock Neural machine translation with adequacy-oriented learning.
\newblock In \emph{AAAI}, 2019.

\bibitem[Kreutzer et~al.(2020)Kreutzer, Foster, and Cherry]{maskstrategy}
Kreutzer, J., Foster, G., and Cherry, C.
\newblock Inference strategies for sequence generation with conditional
  masking.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Kuhn(1955)]{kuhn1955hungarian}
Kuhn, H.~W.
\newblock The hungarian method for the assignment problem.
\newblock \emph{Naval research logistics quarterly}, 1955.

\bibitem[Lee et~al.(2018)Lee, Mansimov, and Cho]{iterativerefine}
Lee, J., Mansimov, E., and Cho, K.
\newblock Deterministic non-autoregressive neural sequence modeling by
  iterative refinement.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Li et~al.(2019)Li, Lin, He, Tian, Qin, Wang, and Liu]{nathint}
Li, Z., Lin, Z., He, D., Tian, F., Qin, T., Wang, L., and Liu, T.
\newblock Hint-based training for non-autoregressive machine translation.
\newblock In \emph{EMNLP-IJCNLP}, 2019.

\bibitem[Libovick{\'y} \& Helcl(2018)Libovick{\'y} and
  Helcl]{libovicky-helcl-2018-end}
Libovick{\'y}, J. and Helcl, J.
\newblock End-to-end non-autoregressive neural machine translation with
  connectionist temporal classification.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Lopes et~al.(2019)Lopes, Yadav, Ilic, and Patra]{lopes2019fast}
Lopes, P.~A., Yadav, S.~S., Ilic, A., and Patra, S.~K.
\newblock Fast block distributed cuda implementation of the hungarian
  algorithm.
\newblock \emph{Journal of Parallel and Distributed Computing}, 130:\penalty0
  50--62, 2019.

\bibitem[Ma et~al.(2018)Ma, Sun, Wang, and Lin]{bowastarget}
Ma, S., Sun, X., Wang, Y., and Lin, J.
\newblock Bag-of-words as target for neural machine translation.
\newblock In \emph{ACL}, 2018.

\bibitem[Ma et~al.(2019)Ma, Zhou, Li, Neubig, and Hovy]{flowseq}
Ma, X., Zhou, C., Li, X., Neubig, G., and Hovy, E.
\newblock Flowseq: Non-autoregressive conditional sequence generation with
  generative flow.
\newblock In \emph{EMNLP-IJCNLP}, 2019.

\bibitem[Ott et~al.(2018)Ott, Auli, Grangier, and Ranzato]{ott2018analyzing}
Ott, M., Auli, M., Grangier, D., and Ranzato, M.
\newblock Analyzing uncertainty in neural machine translation.
\newblock In \emph{ICML}, 2018.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu]{papineni2002bleu}
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{ACL}, 2002.

\bibitem[Post(2018)]{post2018call}
Post, M.
\newblock A call for clarity in reporting bleu scores.
\newblock In \emph{WMT}, 2018.

\bibitem[Qian et~al.(2020)Qian, Zhou, Bao, Wang, Qiu, Zhang, Yu, and Li]{glat}
Qian, L., Zhou, H., Bao, Y., Wang, M., Qiu, L., Zhang, W., Yu, Y., and Li, L.
\newblock Glancing transformer for non-autoregressive neural machine
  translation.
\newblock \emph{ArXiv}, abs/2008.07905, 2020.

\bibitem[{Ranzato} et~al.(2016){Ranzato}, {Chopra}, {Auli}, and
  {Zaremba}]{Ranzato:2016:ICLR}
{Ranzato}, M., {Chopra}, S., {Auli}, M., and {Zaremba}, W.
\newblock {Sequence Level Training with Recurrent Neural Networks}.
\newblock In \emph{ICLR}, 2016.

\bibitem[Ren et~al.(2020)Ren, Liu, Tan, Zhao, Zhao, and Liu]{ren2020astudy}
Ren, Y., Liu, J., Tan, X., Zhao, Z., Zhao, S., and Liu, T.-Y.
\newblock A study of non-autoregressive model for sequence generation.
\newblock In \emph{ACL}, 2020.

\bibitem[Saharia et~al.(2020)Saharia, Chan, Saxena, and Norouzi]{imputer}
Saharia, C., Chan, W., Saxena, S., and Norouzi, M.
\newblock Non-autoregressive machine translation with latent alignments.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and Birch]{Sennrich:BPE}
Sennrich, R., Haddow, B., and Birch, A.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{ACL}, 2016.

\bibitem[Shao et~al.(2020)Shao, Zhang, Feng, Meng, and Zhou]{natbow}
Shao, C., Zhang, J., Feng, Y., Meng, F., and Zhou, J.
\newblock Minimizing the bag-of-ngrams difference for non-autoregressive neural
  machine translation.
\newblock In \emph{AAAI}, 2020.

\bibitem[Shen et~al.(2016)Shen, Cheng, He, He, Wu, Sun, and Liu]{Shen:2016:MRT}
Shen, S., Cheng, Y., He, Z., He, W., Wu, H., Sun, M., and Liu, Y.
\newblock Minimum risk training for neural machine translation.
\newblock In \emph{ACL}, 2016.

\bibitem[Shu et~al.(2020)Shu, Lee, Nakayama, and Cho]{Shu2020LaNMT}
Shu, R., Lee, J., Nakayama, H., and Cho, K.
\newblock Latent-variable non-autoregressive neural machine translation with
  deterministic inference using a delta posterior.
\newblock In \emph{AAAI}, 2020.

\bibitem[Sun \& Yang(2020)Sun and Yang]{em}
Sun, Z. and Yang, Y.
\newblock An em approach to non-autoregressive conditional sequence generation.
\newblock In \emph{ICML}, 2020.

\bibitem[Sun et~al.(2019)Sun, Li, Wang, Lin, He, and Deng]{natcrf}
Sun, Z., Li, Z., Wang, H., Lin, Z., He, D., and Deng, Z.-H.
\newblock Fast structured decoding for sequence models.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{transformer}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Zhou et~al.(2020)Zhou, Neubig, and Gu]{zhou2019understanding}
Zhou, C., Neubig, G., and Gu, J.
\newblock Understanding knowledge distillation in non-autoregressive machine
  translation.
\newblock In \emph{ICLR}, 2020.

\end{thebibliography}
