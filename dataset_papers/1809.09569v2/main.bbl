\begin{thebibliography}{23}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Mart{\'\i}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{OSDI}, volume~16, pages 265--283, 2016.

\bibitem[Al-Rfou et~al.(2016)Al-Rfou, Alain, Almahairi, Angermueller, Bahdanau,
  Ballas, Bastien, Bayer, Belikov, Belopolsky, et~al.]{al2016theano}
Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermueller, Dzmitry
  Bahdanau, Nicolas Ballas, Fr{\'e}d{\'e}ric Bastien, Justin Bayer, Anatoly
  Belikov, Alexander Belopolsky, et~al.
\newblock Theano: A python framework for fast computation of mathematical
  expressions.
\newblock \emph{arXiv preprint arXiv:1605.02688}, 472:\penalty0 473, 2016.

\bibitem[Baydin et~al.(2015)Baydin, Pearlmutter, Radul, and
  Siskind]{baydin2015automatic}
Atilim~Gunes Baydin, Barak~A Pearlmutter, Alexey~Andreyevich Radul, and
  Jeffrey~Mark Siskind.
\newblock Automatic differentiation in machine learning: a survey.
\newblock \emph{arXiv preprint arXiv:1502.05767}, 2015.

\bibitem[Bengio et~al.(2013)Bengio, L{\'e}onard, and
  Courville]{bengio2013estimating}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{arXiv preprint arXiv:1308.3432}, 2013.

\bibitem[Bischof and B{\"u}cker(2000)]{bischof2000computing}
Christian~H Bischof and H~Martin B{\"u}cker.
\newblock Computing derivatives of computer programs.
\newblock Technical report, Argonne National Lab., IL (US), 2000.

\bibitem[Buckman and Neubig(2018)]{buckman2018neural}
Jacob Buckman and Graham Neubig.
\newblock Neural lattice language models.
\newblock \emph{arXiv preprint arXiv:1803.05071}, 2018.

\bibitem[Driscoll et~al.(1989)Driscoll, Sarnak, Sleator, and
  Tarjan]{driscoll1989making}
James~R Driscoll, Neil Sarnak, Daniel~D Sleator, and Robert~E Tarjan.
\newblock Making data structures persistent.
\newblock \emph{Journal of computer and system sciences}, 38\penalty0
  (1):\penalty0 86--124, 1989.

\bibitem[Ganin et~al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle,
  Laviolette, Marchand, and Lempitsky]{ganin2016domain}
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
  Larochelle, Fran{\c{c}}ois Laviolette, Mario Marchand, and Victor Lempitsky.
\newblock Domain-adversarial training of neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2096--2030, 2016.

\bibitem[Griewank and Walther(2008)]{griewank2008evaluating}
Andreas Griewank and Andrea Walther.
\newblock \emph{Evaluating derivatives: principles and techniques of
  algorithmic differentiation}, volume 105.
\newblock Siam, 2008.

\bibitem[Heess et~al.(2015)Heess, Wayne, Silver, Lillicrap, Erez, and
  Tassa]{heess2015learning}
Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval
  Tassa.
\newblock Learning continuous control policies by stochastic value gradients.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2944--2952, 2015.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{jang2016categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock \emph{arXiv preprint arXiv:1611.01144}, 2016.

\bibitem[Lillicrap et~al.(2014)Lillicrap, Cownden, Tweed, and
  Akerman]{lillicrap2014random}
Timothy~P Lillicrap, Daniel Cownden, Douglas~B Tweed, and Colin~J Akerman.
\newblock Random feedback weights support learning in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1411.0247}, 2014.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{maclaurin2015autograd}
Dougal Maclaurin, David Duvenaud, and Ryan~P Adams.
\newblock Autograd: Effortless gradients in numpy.
\newblock In \emph{ICML 2015 AutoML Workshop}, 2015.

\bibitem[Naumann(2012)]{naumann2012art}
Uwe Naumann.
\newblock \emph{The art of differentiating computer programs: an introduction
  to algorithmic differentiation}, volume~24.
\newblock Siam, 2012.

\bibitem[N{\o}kland(2016)]{nokland2016direct}
Arild N{\o}kland.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1037--1045, 2016.

\bibitem[Oliphant(2006)]{oliphant2006guide}
Travis~E Oliphant.
\newblock \emph{A guide to NumPy}, volume~1.
\newblock Trelgol Publishing USA, 2006.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1310--1318, 2013.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock \emph{NIPS 2017 Autodiff Workshop}, 2017.

\bibitem[Rae et~al.(2016)Rae, Hunt, Danihelka, Harley, Senior, Wayne, Graves,
  and Lillicrap]{rae2016scaling}
Jack Rae, Jonathan~J Hunt, Ivo Danihelka, Timothy Harley, Andrew~W Senior,
  Gregory Wayne, Alex Graves, and Tim Lillicrap.
\newblock Scaling memory-augmented neural networks with sparse reads and
  writes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3621--3629, 2016.

\bibitem[Siskind and Pearlmutter(2005)]{siskind2005perturbation}
Jeffrey~Mark Siskind and Barak~A Pearlmutter.
\newblock Perturbation confusion and referential transparency: Correct
  functional implementation of forward-mode ad.
\newblock 2005.

\bibitem[Tokui et~al.(2015)Tokui, Oono, Hido, and Clayton]{tokui2015chainer}
Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton.
\newblock Chainer: a next-generation open source framework for deep learning.
\newblock In \emph{NIPS 2015 LearningSys Workshop}, volume~5, 2015.

\bibitem[van~den Oord et~al.(2017)van~den Oord, Vinyals, and
  Kavukcuoglu]{oord2017neural}
A{\"{a}}ron van~den Oord, Oriol Vinyals, and Koray Kavukcuoglu.
\newblock Neural discrete representation learning.
\newblock \emph{CoRR}, abs/1711.00937, 2017.
\newblock URL \url{http://arxiv.org/abs/1711.00937}.

\bibitem[Williams and Peng(1990)]{williams1990efficient}
Ronald~J Williams and Jing Peng.
\newblock An efficient gradient-based algorithm for on-line training of
  recurrent network trajectories.
\newblock \emph{Neural computation}, 2\penalty0 (4):\penalty0 490--501, 1990.

\end{thebibliography}
