\providecommand{\AC}{A.-C}\providecommand{\CA}{C.-A}\providecommand{\CH}{C.-H}\providecommand{\CN}{C.-N}\providecommand{\CC}{C.-C}\providecommand{\CJ}{C.-J}\providecommand{\HJ}{H.-J}\providecommand{\HY}{H.-Y}\providecommand{\JC}{J.-C}\providecommand{\JP}{J.-P}\providecommand{\JB}{J.-B}\providecommand{\JF}{J.-F}\providecommand{\JJ}{J.-J}\providecommand{\JM}{J.-M}\providecommand{\KW}{K.-W}\providecommand{\KR}{K.-R}\providecommand{\PL}{P.-L}\providecommand{\RE}{R.-E}\providecommand{\SJ}{S.-J}\providecommand{\XR}{X.-R}\providecommand{\WX}{W.-X}\providecommand{\PL}{P.-L}\providecommand{\YX}{Y.-X}
\begin{thebibliography}{85}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson(1965)]{Anderson65}
D.~G. Anderson.
\newblock Iterative procedures for nonlinear integral equations.
\newblock \emph{Journal of the ACM}, 12\penalty0 (4):\penalty0 547--560, 1965.

\bibitem[Attouch and Bolte(2009)]{Attouch_Bolte2009}
H.~Attouch and J.~Bolte.
\newblock On the convergence of the proximal algorithm for nonsmooth functions
  involving analytic features.
\newblock \emph{Mathematical Programming}, 116\penalty0 (1):\penalty0 5--16,
  2009.

\bibitem[Attouch et~al.(2013)Attouch, Bolte, and
  Svaiter]{attouch2013convergence}
H.~Attouch, J.~Bolte, and B.~F. Svaiter.
\newblock Convergence of descent methods for semi-algebraic and tame problems:
  proximal algorithms, forward--backward splitting, and regularized
  {Gauss}--{Seidel} methods.
\newblock \emph{Mathematical Programming}, 137\penalty0 (1):\penalty0 91--129,
  2013.

\bibitem[Bach(2008)]{Bach08}
F.~Bach.
\newblock Consistency of the group {Lasso} and multiple kernel learning.
\newblock \emph{J. Mach. Learn. Res.}, 9:\penalty0 1179--1225, 2008.

\bibitem[Bertrand and Massias(2021)]{Bertrand_Massias2020}
Q.~Bertrand and M.~Massias.
\newblock Anderson acceleration of coordinate descent.
\newblock In \emph{AISTATS}, 2021.

\bibitem[Bertrand et~al.(2020)Bertrand, Klopfenstein, Blondel, Vaiter,
  Gramfort, and Salmon]{bertrand2020implicit}
Q.~Bertrand, Q.~Klopfenstein, M.~Blondel, S.~Vaiter, A.~Gramfort, and
  J.~Salmon.
\newblock Implicit differentiation of lasso-type models for hyperparameter
  optimization.
\newblock \emph{ICML}, 2020.

\bibitem[Blondel and Pedregosa(2016)]{Blondel_Pedregosa2016}
M.~Blondel and F.~Pedregosa.
\newblock Lightning: large-scale linear classification, regression and ranking
  in python, 2016.

\bibitem[Boisbunon et~al.(2014)Boisbunon, Flamary, and
  Rakotomamonjy]{Boisbunon_Flamary_Rakotomamonjy}
A.~Boisbunon, R.~Flamary, and A.~Rakotomamonjy.
\newblock Active set strategy for high-dimensional non-convex sparse
  optimization problems.
\newblock In \emph{2014 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 1517--1521. IEEE, 2014.

\bibitem[Bolte et~al.(2014)Bolte, Sabach, and
  Teboulle]{Bolte_Sabach_Teboulle2014}
J.~Bolte, S.~Sabach, and M.~Teboulle.
\newblock Proximal alternating linearized minimization for nonconvex and
  nonsmooth problems.
\newblock \emph{Mathematical Programming}, 146\penalty0 (1):\penalty0 459--494,
  2014.

\bibitem[Bonnefoy et~al.(2015)Bonnefoy, Emiya, Ralaivola, and
  Gribonval]{Bonnefoy_Emiya_Ralaivola_Gribonval15}
A.~Bonnefoy, V.~Emiya, L.~Ralaivola, and R.~Gribonval.
\newblock {Dynamic screening: accelerating first-order algorithms for the Lasso
  and Group-Lasso}.
\newblock \emph{{IEEE} Trans. Signal Process.}, 63\penalty0 (19):\penalty0 20,
  2015.

\bibitem[Boyd et~al.(2011)Boyd, Parikh, and Chu]{Boyd_Parikh_Chu2011}
S.~Boyd, N.~Parikh, and E.~Chu.
\newblock \emph{Distributed optimization and statistical learning via the
  alternating direction method of multipliers}.
\newblock Now Publishers Inc, 2011.

\bibitem[Breheny and Huang(2011)]{Breheny_Huang2011}
P.~Breheny and J.~Huang.
\newblock Coordinate descent algorithms for nonconvex penalized regression,
  with applications to biological feature selection.
\newblock \emph{The annals of applied statistics}, 5\penalty0 (1):\penalty0
  232, 2011.

\bibitem[Brezinski et~al.(2018)Brezinski, Redivo-Zaglia, and
  Saad]{Brezinski2018}
C.~Brezinski, M.~Redivo-Zaglia, and Y.~Saad.
\newblock Shanks sequence transformations and anderson acceleration.
\newblock \emph{SIAM Review}, 60\penalty0 (3):\penalty0 646--669, 2018.

\bibitem[Candes and Tao(2005)]{Candes_Tao2005}
E.~J. Candes and T.~Tao.
\newblock Decoding by linear programming.
\newblock \emph{IEEE transactions on information theory}, 51\penalty0
  (12):\penalty0 4203--4215, 2005.

\bibitem[Candes et~al.(2008)Candes, Wakin, and Boyd]{Candes_Wakin_Boyd2008}
E.~J. Candes, M.~B. Wakin, and S.~P. Boyd.
\newblock Enhancing sparsity by reweighted $\ell_1$ minimization.
\newblock \emph{Journal of Fourier analysis and applications}, 14\penalty0
  (5):\penalty0 877--905, 2008.

\bibitem[Davis and Drusvyatskiy(2019)]{Davis_Drusvyatskiy2019}
D.~Davis and D.~Drusvyatskiy.
\newblock Stochastic model-based minimization of weakly convex functions.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (1):\penalty0
  207--239, 2019.

\bibitem[Deng and Lan(2019)]{Deng_Lan2019}
Q.~Deng and C.~Lan.
\newblock Efficiency of coordinate descent methods for structured nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:1909.00918}, 2019.

\bibitem[{El Ghaoui} et~al.(2010){El Ghaoui}, Viallon, and
  Rabbani]{Ghaoui_Viallon_Rabbani2010}
L.~{El Ghaoui}, V.~Viallon, and T.~Rabbani.
\newblock Safe feature elimination for the lasso and sparse supervised learning
  problems.
\newblock \emph{arXiv preprint arXiv:1009.4219}, 2010.

\bibitem[Fan et~al.(2008)Fan, Chang, Hsieh, Wang, and
  Lin]{Fan_Chang_Hsieh_Wang_Lin08}
R.~E. Fan, K.~W. Chang, C.~J. Hsieh, X.~R. Wang, and C.~J. Lin.
\newblock Liblinear: A library for large linear classification.
\newblock \emph{JMLR}, 9:\penalty0 1871--1874, 2008.

\bibitem[Fang et~al.(2020)Fang, Zhe, Lee, Zhang, and Neville]{Fang2020}
S.~Fang, S.~Zhe, K.-C. Lee, K.~Zhang, and J.~Neville.
\newblock Online bayesian sparse learning with spike and slab priors.
\newblock In \emph{2020 IEEE International Conference on Data Mining (ICDM)},
  pages 142--151. IEEE, 2020.

\bibitem[Fercoq and Richt{\'a}rik(2015)]{Fercoq_Richtarik2015}
O.~Fercoq and P.~Richt{\'a}rik.
\newblock Accelerated, parallel, and proximal coordinate descent.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (4):\penalty0
  1997--2023, 2015.

\bibitem[Fercoq et~al.(2015)Fercoq, Gramfort, and
  Salmon]{Fercoq_Gramfort_Salmon2015}
O.~Fercoq, A.~Gramfort, and J.~Salmon.
\newblock Mind the duality gap: safer rules for the lasso.
\newblock In \emph{ICML}, pages 333--342. PMLR, 2015.

\bibitem[Foucart and Lai(2009)]{Foucart_Lai2009}
S.~Foucart and M.-J. Lai.
\newblock Sparsest solutions of underdetermined linear systems via
  $\ell_q$-minimization for {$0 < q \leq 1$}.
\newblock \emph{Applied and Computational Harmonic Analysis}, 26\penalty0
  (3):\penalty0 395--407, 2009.

\bibitem[Friedman et~al.(2007)Friedman, Hastie, H{\"o}fling, and
  Tibshirani]{Friedman_Hastie_Hofling_Tibshirani07}
J.~Friedman, T.~J. Hastie, H.~H{\"o}fling, and R.~Tibshirani.
\newblock Pathwise coordinate optimization.
\newblock \emph{Ann. Appl. Stat.}, 1\penalty0 (2):\penalty0 302--332, 2007.

\bibitem[Friedman et~al.(2010)Friedman, Hastie, and
  Tibshirani]{Friedman_Hastie_Tibshirani10}
J.~Friedman, T.~J. Hastie, and R.~Tibshirani.
\newblock Regularization paths for generalized linear models via coordinate
  descent.
\newblock \emph{J. Stat. Softw.}, 33\penalty0 (1):\penalty0 1, 2010.

\bibitem[Ge et~al.(2019)Ge, Li, Jiang, Liu, Zhang, Wang, and Zhao]{picasso}
J.~Ge, X.~Li, H.~Jiang, H.~Liu, T.~Zhang, M.~Wang, and T.~Zhao.
\newblock Picasso: A sparse learning library for high dimensional data analysis
  in r and python.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 1692--1696, 2019.

\bibitem[Ghosh and Chinnaiyan(2005)]{ghosh2005classification}
D.~Ghosh and A.~M. Chinnaiyan.
\newblock Classification and selection of biomarkers in genomic data using
  lasso.
\newblock \emph{Journal of Biomedicine and Biotechnology}, 2005\penalty0
  (2):\penalty0 147, 2005.

\bibitem[Ghosh and Doshi-Velez(2017)]{Ghosh2017}
S.~Ghosh and F.~Doshi-Velez.
\newblock Model selection in bayesian neural networks via horseshoe priors.
\newblock \emph{arXiv preprint arXiv:1705.10388}, 2017.

\bibitem[Golub and Varga(1961)]{Golub_Varga1961}
G.~H. Golub and R.~S. Varga.
\newblock Chebyshev semi-iterative methods, successive overrelaxation iterative
  methods, and second order richardson iterative methods.
\newblock \emph{Numerische Mathematik}, 3\penalty0 (1):\penalty0 147--156,
  1961.

\bibitem[Gramfort et~al.(2013)Gramfort, Strohmeier, Haueisen,
  H{\"a}m{\"a}l{\"a}inen, and
  Kowalski]{Gramfort_Strohmeier_Haueisen_Hamalainen_Kowalski2013}
A.~Gramfort, D.~Strohmeier, J.~Haueisen, M.~S. H{\"a}m{\"a}l{\"a}inen, and
  M.~Kowalski.
\newblock Time-frequency mixed-norm estimates: Sparse {M/EEG} imaging with
  non-stationary source activations.
\newblock \emph{NeuroImage}, 70:\penalty0 410--422, 2013.

\bibitem[Gramfort et~al.(2014)Gramfort, Luessi, Larson, Engemann, Strohmeier,
  Brodbeck, Parkkonen, and H{\"a}m{\"a}l{\"a}inen]{mne}
A.~Gramfort, M.~Luessi, E.~Larson, D.~A. Engemann, D.~Strohmeier, C.~Brodbeck,
  L.~Parkkonen, and M.~S. H{\"a}m{\"a}l{\"a}inen.
\newblock {MNE} software for processing {MEG} and {EEG} data.
\newblock \emph{NeuroImage}, 86:\penalty0 446 -- 460, 2014.

\bibitem[Hare and Lewis(2007)]{Hare_Lewis2007}
W.~L. Hare and A.~S. Lewis.
\newblock Identifying active manifolds.
\newblock \emph{Algorithmic Operations Research}, 2\penalty0 (2):\penalty0
  75--75, 2007.

\bibitem[Harris et~al.(2020)Harris, Millman, van~der Walt, Gommers, Virtanen,
  Cournapeau, Wieser, Taylor, Berg, Smith, et~al.]{harris2020array}
C.~R. Harris, K.~J. Millman, S.~J. van~der Walt, R.~Gommers, P.~Virtanen,
  D.~Cournapeau, E.~Wieser, J.~Taylor, S.~Berg, N.~J. Smith, et~al.
\newblock Array programming with numpy.
\newblock \emph{arXiv preprint arXiv:2006.10256}, 2020.

\bibitem[Jas et~al.(2020)Jas, Achakulvisut, Idrizović, Acuna, Antalek,
  Marques, Odland, Garg, Agrawal, Umegaki, Foley, Fernandes, Harris, Li,
  Pieters, Otterson, Toni, Rodgers, Dyer, Hamalainen, Kording, and
  Ramkumar]{pyGMLnet}
M.~Jas, T.~Achakulvisut, A.~Idrizović, D.~Acuna, M.~Antalek, V.~Marques,
  T.~Odland, R.~Garg, M.~Agrawal, Y.~Umegaki, P.~Foley, H.~Fernandes,
  D.~Harris, B.~Li, O.~Pieters, S.~Otterson, G.~De Toni, C.~Rodgers, E.~Dyer,
  M.~Hamalainen, K.~Kording, and P.~Ramkumar.
\newblock {P}yglmnet: {P}ython implementation of elastic-net regularized
  generalized linear models.
\newblock \emph{Journal of Open Source Software}, 5\penalty0 (47):\penalty0
  1959, 2020.

\bibitem[Johnson and Guestrin(2015)]{Johnson_Guestrin15}
T.~B. Johnson and C.~Guestrin.
\newblock Blitz: A principled meta-algorithm for scaling sparse optimization.
\newblock In \emph{ICML}, volume~37, pages 1171--1179, 2015.

\bibitem[Kim et~al.(2021)Kim, Brackbill, Batty, Lee, Mitelut, Tong,
  Chichilnisky, and Paninski]{kim2021nonlinear}
Y.~J. Kim, N.~Brackbill, E.~Batty, J.~Lee, C.~Mitelut, W.~Tong,
  EJ~Chichilnisky, and L.~Paninski.
\newblock Nonlinear decoding of natural images from large-scale primate retinal
  ganglion recordings.
\newblock \emph{Neural Computation}, 33\penalty0 (7):\penalty0 1719--1750,
  2021.

\bibitem[Klopfenstein et~al.(2020)Klopfenstein, Bertrand, Gramfort, Salmon, and
  Vaiter]{Klopfenstein2020}
Q.~Klopfenstein, Q.~Bertrand, A.~Gramfort, J.~Salmon, and S.~Vaiter.
\newblock Model identification and local linear convergence of coordinate
  descent.
\newblock \emph{arXiv preprint arXiv:2010.11825}, 2020.

\bibitem[Kruger(2003)]{Kruger2003}
A.~Y. Kruger.
\newblock On {F}r{\'e}chet subdifferentials.
\newblock \emph{Journal of Mathematical Sciences}, 116\penalty0 (3):\penalty0
  3325--3358, 2003.

\bibitem[Lam et~al.(2015)Lam, Pitrou, and Seibert]{numba}
S.~K. Lam, A.~Pitrou, and S.~Seibert.
\newblock Numba: A llvm-based python jit compiler.
\newblock In \emph{Proceedings of the Second Workshop on the {LLVM} Compiler
  Infrastructure in {HPC}}, pages 1--6, 2015.

\bibitem[Le~Morvan and Vert(2018)]{lemorvan2018whinter}
M.~Le~Morvan and J.-P. Vert.
\newblock {WHInter}: A working set algorithm for high-dimensional sparse second
  order interaction models.
\newblock In \emph{ICML}, pages 3635--3644. PMLR, 2018.

\bibitem[Lee et~al.(2012)Lee, Sun, and Saunders]{Lee_Sun_Saunders2012}
J.~D. Lee, Y.~Sun, and M.~Saunders.
\newblock Proximal {N}ewton-type methods for convex optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  25:\penalty0 827--835, 2012.

\bibitem[Lewis(2002)]{Lewis_2002}
A.~S. Lewis.
\newblock Active sets, nonsmoothness, and sensitivity.
\newblock \emph{SIAM Journal on Optimization}, 13\penalty0 (3):\penalty0
  702--725, 2002.

\bibitem[Liang et~al.(2016)Liang, Fadili, and
  Peyr{\'e}]{Liang_Faidli_Peyre_2016}
J.~Liang, J.~Fadili, and G.~Peyr{\'e}.
\newblock A multi-step inertial forward-backward splitting method for
  non-convex optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  29:\penalty0 4035--4043, 2016.

\bibitem[Lin et~al.(2014)Lin, Lu, and Xiao]{Lin_Lu_Xiao_2014}
Q.~Lin, Z.~Lu, and L.~Xiao.
\newblock An accelerated proximal coordinate gradient method.
\newblock In \emph{NeurIPS}, pages 3059--3067. 2014.

\bibitem[Liu and Nocedal(1989)]{Liu_Nocedal1989}
D.~C. Liu and J.~Nocedal.
\newblock On the limited memory {BFGS} method for large scale optimization.
\newblock \emph{Mathematical programming}, 45\penalty0 (1):\penalty0 503--528,
  1989.

\bibitem[Mai and Johansson(2019)]{Mai_Johansson_19}
V.~V. Mai and M.~Johansson.
\newblock Anderson acceleration of proximal gradient methods.
\newblock In \emph{ICML}. 2019.

\bibitem[Mairal(2010)]{Mairal}
J.~Mairal.
\newblock \emph{Sparse coding for machine learning, image processing and
  computer vision}.
\newblock PhD thesis, {\'E}cole normale sup{\'e}rieure de Cachan, 2010.

\bibitem[Massias et~al.(2018)Massias, Gramfort, and
  Salmon]{Massias_Gramfort_Salmon2018}
M.~Massias, A.~Gramfort, and J.~Salmon.
\newblock Celer: a fast solver for the lasso with dual extrapolation.
\newblock 2018.

\bibitem[Massias et~al.(2020)Massias, Vaiter, Gramfort, and
  Salmon]{Massias_Vaiter_Salmon_Gramfort2019}
M.~Massias, S.~Vaiter, A.~Gramfort, and J.~Salmon.
\newblock Dual extrapolation for sparse generalized linear models.
\newblock \emph{J. Mach. Learn. Res.}, 2020.

\bibitem[Mazumder et~al.(2011)Mazumder, Friedman, and
  Hastie]{Mazumder_Friedman_Hastie2011}
R.~Mazumder, J.~H. Friedman, and T.~Hastie.
\newblock Sparsenet: Coordinate descent with nonconvex penalties.
\newblock \emph{Journal of the American Statistical Association}, 106\penalty0
  (495):\penalty0 1125--1138, 2011.

\bibitem[Moreau et~al.(2022)Moreau, Massias, Gramfort, Ablin, Charlier,
  Bannier, Dagr{\'e}ou, la~Tour, Durif, Dantas, Klopfenstein, et~al.]{benchopt}
T.~Moreau, M.~Massias, A.~Gramfort, P.~Ablin, B.~Charlier, P.-A. Bannier,
  M.~Dagr{\'e}ou, T.~Dupr{\'e} la~Tour, G.~Durif, C.~F. Dantas,
  Q.~Klopfenstein, et~al.
\newblock Benchopt: Reproducible, efficient and collaborative optimization
  benchmarks.
\newblock \emph{arXiv preprint arXiv:2206.13424}, 2022.

\bibitem[Muir and Zhan(2021)]{Muir_Zhan2021}
J.~B. Muir and Z.~Zhan.
\newblock Seismic wavefield reconstruction using a pre-conditioned
  wavelet--curvelet compressive sensing approach.
\newblock \emph{Geophysical Journal International}, 227\penalty0 (1):\penalty0
  303--315, 2021.

\bibitem[Ndiaye and Takeuchi(2021)]{ndiaye2021continuation}
E.~Ndiaye and I.~Takeuchi.
\newblock Continuation path with linear convergence rate.
\newblock \emph{arXiv preprint arXiv:2112.05104}, 2021.

\bibitem[Ndiaye et~al.(2017)Ndiaye, Fercoq, Gramfort, and
  Salmon]{Ndiaye_Fercoq_Gramfort_Salmon17}
E.~Ndiaye, O.~Fercoq, A.~Gramfort, and J.~Salmon.
\newblock Gap safe screening rules for sparsity enforcing penalties.
\newblock \emph{J. Mach. Learn. Res.}, 18\penalty0 (128):\penalty0 1--33, 2017.

\bibitem[Ndiaye et~al.(2020)Ndiaye, Fercoq, and Salmon]{Ndiaye_Fercoq_Salmon20}
E.~Ndiaye, O.~Fercoq, and J.~Salmon.
\newblock Screening rules and its complexity for active set identification.
\newblock \emph{Journal of Convex Analysis}, 2020.

\bibitem[Nesterov(2012)]{Nesterov2012}
Y.~Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  341--362, 2012.

\bibitem[Ng(2004)]{Ng04}
A.~Y. Ng.
\newblock Feature selection, l1 vs. l2 regularization, and rotational
  invariance.
\newblock In \emph{ICML}, page~78, 2004.

\bibitem[Nutini(2018)]{Nutini2018}
J.~Nutini.
\newblock \emph{Greed is good: greedy optimization methods for large-scale
  structured problems}.
\newblock PhD thesis, University of British Columbia, 2018.

\bibitem[Nutini et~al.(2015)Nutini, Schmidt, Laradji, Friedlander, and
  Koepke]{Nutini_Schmidt_Laradji_Friedlander_Koepke15}
J.~Nutini, M.~W. Schmidt, I.~H. Laradji, M.~P. Friedlander, and H.~A. Koepke.
\newblock Coordinate descent converges faster with the {Gauss-Southwell} rule
  than random selection.
\newblock In \emph{ICML}, pages 1632--1641, 2015.

\bibitem[Ouyang et~al.(2020)Ouyang, Peng, Yao, Zhang, and
  Deng]{Ouyang_Peng_Yao_Zhang_Deng2020}
W.~Ouyang, Y.~Peng, Y.~Yao, J.~Zhang, and B.~Deng.
\newblock Anderson acceleration for nonconvex {ADMM} based on
  {Douglas-Rachford} splitting.
\newblock In \emph{Computer Graphics Forum}, volume~39, pages 221--239. Wiley
  Online Library, 2020.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{Pedregosa_etal11}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{JMLR}, 12:\penalty0 2825--2830, 2011.

\bibitem[Poon and Liang(2019)]{Poon_Liang2019}
C.~Poon and J.~Liang.
\newblock Trajectory of alternating direction method of multipliers and
  adaptive acceleration.
\newblock In \emph{NeurIPS}, pages 7357--7365, 2019.

\bibitem[Quemener and Corvellec(2013)]{quemener2013sidus}
E.~Quemener and M.~Corvellec.
\newblock Sidus—the solution for extreme deduplication of an operating
  system.
\newblock \emph{Linux Journal}, 2013\penalty0 (235):\penalty0 3, 2013.

\bibitem[Rakotomamonjy et~al.(2019)Rakotomamonjy, Gasso, and
  Salmon]{Rakotomamonjy_Gasso_Salmon19}
A.~Rakotomamonjy, G.~Gasso, and J.~Salmon.
\newblock Screening rules for lasso with non-convex sparse regularizers.
\newblock In \emph{ICML}, pages 5341--5350, 2019.

\bibitem[Rakotomamonjy et~al.(2022)Rakotomamonjy, Flamary, Gasso, and
  Salmon]{Rakotomamonjy_Flamary_Gasso_Salmon20}
A.~Rakotomamonjy, R.~Flamary, G.~Gasso, and J.~Salmon.
\newblock Provably convergent working set algorithm for non-convex regularized
  regression.
\newblock In \emph{AISTATS}, 2022.

\bibitem[Reidenbach et~al.(2021)Reidenbach, Lal, Slim, Mosafi, and
  Israeli]{Reidenbach_Lal_Slim_Mosafi_Israeli2021}
D.~A. Reidenbach, A.~Lal, L.~Slim, O.~Mosafi, and J.~Israeli.
\newblock Gepsi: A python library to simulate gwas phenotype data.
\newblock \emph{bioRxiv}, 2021.

\bibitem[Richt{\'a}rik and Tak{\'a}{\v{c}}(2014)]{Richtarik_Takac2014}
P.~Richt{\'a}rik and M.~Tak{\'a}{\v{c}}.
\newblock Iteration complexity of randomized block-coordinate descent methods
  for minimizing a composite function.
\newblock \emph{Mathematical Programming}, 144\penalty0 (1-2):\penalty0 1--38,
  2014.

\bibitem[Rudelson and Vershynin(2008)]{Rudelson_Vershynin2008}
M.~Rudelson and R.~Vershynin.
\newblock On sparse reconstruction from fourier and gaussian measurements.
\newblock \emph{Communications on Pure and Applied Mathematics: A Journal
  Issued by the Courant Institute of Mathematical Sciences}, 61\penalty0
  (8):\penalty0 1025--1045, 2008.

\bibitem[Scieur et~al.(2016)Scieur, d'Aspremont, and
  Bach]{Scieur_dAspremont_Bach2016}
D.~Scieur, A.~d'Aspremont, and F.~Bach.
\newblock Regularized nonlinear acceleration.
\newblock In \emph{Advances In Neural Information Processing Systems}, pages
  712--720, 2016.

\bibitem[Scieur et~al.(2020)Scieur, d’Aspremont, and
  Bach]{Scieur_dAspremont_Bach2020}
D.~Scieur, A.~d’Aspremont, and F.~Bach.
\newblock Regularized nonlinear acceleration.
\newblock \emph{Mathematical Programming}, 179\penalty0 (1):\penalty0 47--83,
  2020.

\bibitem[Sidi(2017)]{Sidi17}
A.~Sidi.
\newblock \emph{Vector extrapolation methods with applications}.
\newblock SIAM, 2017.

\bibitem[Simon et~al.(2013)Simon, Friedman, Hastie, and
  Tibshirani]{Simon_Friedman_Hastie_Tibshirani12}
N.~Simon, J.~Friedman, T.~J. Hastie, and R.~Tibshirani.
\newblock A sparse-group lasso.
\newblock \emph{J. Comput. Graph. Statist.}, 22\penalty0 (2):\penalty0
  231--245, 2013.

\bibitem[Soubies et~al.(2015)Soubies, Blanc-F{\'e}raud, and
  Aubert]{Soubies_BlancFeraud_Aubert2015}
E.~Soubies, L.~Blanc-F{\'e}raud, and G.~Aubert.
\newblock A continuous exact $\ell_0$ penalty (cel0) for least squares
  regularized problem.
\newblock \emph{SIAM Journal on Imaging Sciences}, 8\penalty0 (3):\penalty0
  1607--1639, 2015.

\bibitem[Strohmeier et~al.(2015)Strohmeier, Gramfort, and
  Haueisen]{Strohmeier_Gramfort_Haueisen2015}
D.~Strohmeier, A.~Gramfort, and J.~Haueisen.
\newblock {MEG/EEG source imaging with a non-convex penalty in the
  time-frequency domain}.
\newblock In \emph{{Pattern Recognition in Neuroimaging, 2015 International
  Workshop on}}, 2015.

\bibitem[Strohmeier et~al.(2016)Strohmeier, Bekhti, Haueisen, and
  Gramfort]{Strohmeier_Bekhti_Haueisen_Gramfort2016}
D.~Strohmeier, Y.~Bekhti, J.~Haueisen, and A.~Gramfort.
\newblock The iterative reweighted mixed-norm estimate for spatio-temporal
  {MEG/EEG} source reconstruction.
\newblock \emph{IEEE transactions on medical imaging}, 35\penalty0
  (10):\penalty0 2218--2228, 2016.

\bibitem[Tibshirani(1996)]{Tibshirani96}
R.~Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock \emph{J. R. Stat. Soc. Ser. B Stat. Methodol.}, 58\penalty0
  (1):\penalty0 267--288, 1996.

\bibitem[Tibshirani et~al.(2012)Tibshirani, Bien, Friedman, Hastie, Simon,
  Taylor, and Tibshirani]{Tibshirani2012}
R.~Tibshirani, J.~Bien, J.~Friedman, T.~Hastie, N.~Simon, J.~Taylor, and R.~J.
  Tibshirani.
\newblock Strong rules for discarding predictors in lasso-type problems.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 74\penalty0 (2):\penalty0 245--266, 2012.

\bibitem[Tibshirani(2013)]{Tibshirani2013}
R.~J. Tibshirani.
\newblock The lasso problem and uniqueness.
\newblock \emph{Electronic Journal of statistics}, 7:\penalty0 1456--1490,
  2013.

\bibitem[Tseng and S.Yun(2009)]{tseng2009coordinate}
P.~Tseng and S.Yun.
\newblock A coordinate gradient descent method for nonsmooth separable
  minimization.
\newblock \emph{Mathematical Programming}, 117\penalty0 (1):\penalty0 387--423,
  2009.

\bibitem[Vaiter et~al.(2015)Vaiter, Peyr{\'e}, and
  Fadili]{Vaiter_Peyre_Fadili2015}
S.~Vaiter, G.~Peyr{\'e}, and J.~Fadili.
\newblock Low complexity regularization of linear inverse problems.
\newblock In \emph{Sampling Theory, a Renaissance}, pages 103--153. Springer,
  2015.

\bibitem[Wei et~al.(2021)Wei, Bao, and Liu]{Wei_Bao_Liu2021}
F.~Wei, C.~Bao, and Y.~Liu.
\newblock Stochastic {Anderson} mixing for nonconvex stochastic optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Wen et~al.(2018)Wen, Chu, Liu, and Qiu]{wen2018survey}
F.~Wen, L.~Chu, P.~Liu, and R.~Qiu.
\newblock A survey on nonconvex regularization-based sparse and low-rank
  recovery in signal processing, statistics, and machine learning.
\newblock \emph{IEEE Access}, 6:\penalty0 69883--69906, 2018.

\bibitem[Zhang(2010)]{Zhang2010}
C.-H. Zhang.
\newblock Nearly unbiased variable selection under minimax concave penalty.
\newblock \emph{The Annals of statistics}, 38\penalty0 (2):\penalty0 894--942,
  2010.

\bibitem[Zhao and Yu(2006)]{Zhao_Yu2006}
P.~Zhao and B.~Yu.
\newblock On model selection consistency of lasso.
\newblock \emph{J. Mach. Learn. Res.}, 7:\penalty0 2541--2563, 2006.

\bibitem[Zou and Hastie(2005)]{Zou_Hastie05}
H.~Zou and T.~J. Hastie.
\newblock Regularization and variable selection via the elastic net.
\newblock \emph{J. R. Stat. Soc. Ser. B Stat. Methodol.}, 67\penalty0
  (2):\penalty0 301--320, 2005.

\end{thebibliography}
