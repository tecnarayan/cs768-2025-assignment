\begin{thebibliography}{}

\bibitem[Agarwal et~al., 2012]{6142067}
Agarwal, A., Bartlett, P.~L., Ravikumar, P., and Wainwright, M.~J. (2012).
\newblock Information-theoretic lower bounds on the oracle complexity of
  stochastic convex optimization.
\newblock {\em IEEE Transactions on Information Theory}, 58(5):3235--3249.

\bibitem[Amir et~al., 2020]{amir2020prediction}
Amir, I., Attias, I., Koren, T., Mansour, Y., and Livni, R. (2020).
\newblock Prediction with corrupted expert advice.
\newblock In {\em Advances in Neural Information Processing Systems 33
  (NeurIPS)}, pages 14315--14325.

\bibitem[Cesa-Bianchi et~al., 2002]{CesaBianchi:2002}
Cesa-Bianchi, N., Conconi, A., and Gentile, C. (2002).
\newblock On the generalization ability of online learning algorithms.
\newblock In {\em Advances in Neural Information Processing Systems 14
  (NeurIPS)}, volume~14.

\bibitem[Chiang et~al., 2012]{pmlr-v23-chiang12}
Chiang, C.-K., Yang, T., Lee, C.-J., Mahdavi, M., Lu, C.-J., Jin, R., and Zhu,
  S. (2012).
\newblock Online optimization with gradual variations.
\newblock In {\em Proceedings of the 25th Annual Conference on Learning
  Theory}, volume~23 of {\em Proceedings of Machine Learning Research}, pages
  6.1--6.20. PMLR.

\bibitem[Cutkosky, 2019]{Cutkosky2019AnytimeOC}
Cutkosky, A. (2019).
\newblock Anytime online-to-batch, optimism and acceleration.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of {\em Proceedings of Machine Learning Research}, pages
  1446--1454. PMLR.

\bibitem[Garber et~al., 2020]{pmlr-v119-garber20a}
Garber, D., Korcia, G., and Levy, K. (2020).
\newblock Online convex optimization in the random order model.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, volume 119 of {\em Proceedings of Machine Learning Research},
  pages 3387--3396. PMLR.

\bibitem[Ghadimi and Lan, 2012]{ghadimi2012optimal}
Ghadimi, S. and Lan, G. (2012).
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization i: A generic algorithmic framework.
\newblock {\em SIAM Journal on Optimization}, 22(4):1469--1492.

\bibitem[Ghadimi and Lan, 2013]{Ghadimi2013StochasticFA}
Ghadimi, S. and Lan, G. (2013).
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM J. Optim.}, 23:2341--2368.

\bibitem[Haghtalab et~al., 2022]{haghtalab2022smoothed}
Haghtalab, N., Roughgarden, T., and Shetty, A. (2022).
\newblock Smoothed analysis with adaptive adversaries.
\newblock In {\em 2021 IEEE 62nd Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 942--953. IEEE.

\bibitem[Hazan, 2016]{HazanOCO:2016}
Hazan, E. (2016).
\newblock Introduction to online convex optimization.
\newblock {\em Found. Trends Optim.}, 2(3?4):157?325.

\bibitem[Hazan and Kale, 2010]{Hazan2010ExtractingCF}
Hazan, E. and Kale, S. (2010).
\newblock Extracting certainty from uncertainty: regret bounded by variation in
  costs.
\newblock {\em Machine Learning}, 80:165--188.

\bibitem[Hazan and Kale, 2011]{pmlr-v19-hazan11a}
Hazan, E. and Kale, S. (2011).
\newblock Beyond the regret minimization barrier: an optimal algorithm for
  stochastic strongly-convex optimization.
\newblock In {\em Proceedings of the 24th Annual Conference on Learning
  Theory}, volume~19 of {\em Proceedings of Machine Learning Research}, pages
  421--436. PMLR.

\bibitem[Ito, 2021]{ito2021on-optimal}
Ito, S. (2021).
\newblock On optimal robustness to adversarial corruption in online decision
  problems.
\newblock In {\em Advances in Neural Information Processing Systems 34
  (NeurIPS)}, volume~34, pages 7409--7420.

\bibitem[Jain et~al., 2018]{jain2018accelerating}
Jain, P., Kakade, S.~M., Kidambi, R., Netrapalli, P., and Sidford, A. (2018).
\newblock Accelerating stochastic gradient descent for least squares
  regression.
\newblock In {\em Conference On Learning Theory}, pages 545--604. PMLR.

\bibitem[Joulani et~al., 2017]{joulani2017modular}
Joulani, P., Gy{\"o}rgy, A., and Szepesv{\'a}ri, C. (2017).
\newblock A modular analysis of adaptive (non-) convex optimization: Optimism,
  composite objectives, and variational bounds.
\newblock In {\em International Conference on Algorithmic Learning Theory
  (ALT)}, pages 681--720. PMLR.

\bibitem[Joulani et~al., 2020]{AccStochOptDM}
Joulani, P., Raj, A., Gyorgy, A., and Szepesv{\'a}ri, C. (2020).
\newblock A simpler approach to accelerated optimization: iterative averaging
  meets optimism.
\newblock In {\em International Conference on Machine Learning}, pages
  4984--4993. PMLR.

\bibitem[Kenyon, 1997]{Kenyon97best-fitbin-packing}
Kenyon, C. (1997).
\newblock Best-fit bin-packing with random order.
\newblock In {\em In 7th Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages 359--364.

\bibitem[McMahan, 2011]{DBLP:journals/corr/abs-1301-0534}
McMahan, B. (2011).
\newblock Follow-the-regularized-leader and mirror descent: Equivalence
  theorems and l1 regularization.
\newblock In {\em Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, volume~15 of {\em Proceedings of
  Machine Learning Research}, pages 525--533. PMLR.

\bibitem[Nemirovski, 2005]{Nemirovski:2004}
Nemirovski, A. (2005).
\newblock Prox-method with rate of convergence o(1/t) for variational
  inequalities with lipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock {\em SIAM J. on Optimization}, 15(1):229--251.

\bibitem[Nemirovsky and Yudin, 1985]{doi:10.1137/1027074}
Nemirovsky, A. and Yudin, D. (1985).
\newblock Problem complexity and method efficiency in optimization.
\newblock {\em SIAM Review}, 27(2):264--265.

\bibitem[Orabona, 2021]{Orabona2019AMI}
Orabona, F. (2021).
\newblock A modern introduction to online learning.
\newblock arXiv preprint: 1912.13213.

\bibitem[Orabona and P{\'a}l, 2018]{Orabona2018ScalefreeOL}
Orabona, F. and P{\'a}l, D. (2018).
\newblock Scale-free online learning.
\newblock {\em Theor. Comput. Sci.}, 716:50--69.

\bibitem[Polyak and Juditsky, 1992]{Polyak:1992}
Polyak, B.~T. and Juditsky, A.~B. (1992).
\newblock Acceleration of stochastic approximation by averaging.
\newblock {\em SIAM journal on control and optimization}, 30(4):838--855.

\bibitem[Rakhlin and Sridharan, 2013a]{rakhlin2013online}
Rakhlin, A. and Sridharan, K. (2013a).
\newblock Online learning with predictable sequences.
\newblock In {\em Conference on Learning Theory}, pages 993--1019. PMLR.

\bibitem[Rakhlin and Sridharan, 2013b]{10.5555/2999792.2999954}
Rakhlin, A. and Sridharan, K. (2013b).
\newblock Optimization, learning, and games with predictable sequences.
\newblock In {\em Advances in Neural Information Processing Systems 26
  (NeurIPS)}, pages 3066--3074.

\bibitem[Rakhlin et~al., 2011]{rakhlin2011online}
Rakhlin, A., Sridharan, K., and Tewari, A. (2011).
\newblock Online learning: stochastic, constrained, and smoothed adversaries.
\newblock In {\em Advances in Neural Information Processing Systems 24
  (NeurIPS)}, pages 1764--1772.

\bibitem[Robbins and Monro, 1951]{RobbinsMonro:1951}
Robbins, H. and Monro, S. (1951).
\newblock A stochastic approximation method.
\newblock {\em Annals of Mathematical Statistics}, 22:400--407.

\bibitem[Seldin and Slivkins, 2014]{pmlr-v32-seldinb14}
Seldin, Y. and Slivkins, A. (2014).
\newblock One practical algorithm for both stochastic and adversarial bandits.
\newblock In {\em Proceedings of the 31st International Conference on Machine
  Learning}, volume~32 of {\em Proceedings of Machine Learning Research}, pages
  1287--1295. PMLR.

\bibitem[Shapiro et~al., 2014]{10.5555/2678054}
Shapiro, A., Dentcheva, D., and Ruszczynski, A. (2014).
\newblock {\em Lectures on Stochastic Programming: Modeling and Theory, Second
  Edition}.
\newblock Society for Industrial and Applied Mathematics, USA.

\bibitem[Sherman et~al., 2021]{sherman2021optimal}
Sherman, U., Koren, T., and Mansour, Y. (2021).
\newblock Optimal rates for random order online optimization.
\newblock In {\em Advances in Neural Information Processing Systems 34
  (NeurIPS)}, volume~34.

\bibitem[Spielman and Teng, 2004]{spielman2004smoothed}
Spielman, D.~A. and Teng, S.-H. (2004).
\newblock Smoothed analysis of algorithms: Why the simplex algorithm usually
  takes polynomial time.
\newblock {\em Journal of the ACM (JACM)}, 51(3):385--463.

\bibitem[Yang et~al., 2013]{Yang_2013}
Yang, T., Mahdavi, M., Jin, R., and Zhu, S. (2013).
\newblock Regret bounded by gradual variation for online convex optimization.
\newblock {\em Machine Learning}, 95(2):183--223.

\bibitem[Zhao et~al., 2020]{NEURIPS2020_93931410}
Zhao, P., Zhang, Y.-J., Zhang, L., and Zhou, Z.-H. (2020).
\newblock Dynamic regret of convex and smooth functions.
\newblock In {\em Advances in Neural Information Processing Systems 33
  (NeurIPS)}, pages 12510--12520.

\bibitem[Zhao et~al., 2021]{zhao2021adaptivity}
Zhao, P., Zhang, Y.-J., Zhang, L., and Zhou, Z.-H. (2021).
\newblock Adaptivity and non-stationarity: Problem-dependent dynamic regret for
  online convex optimization.
\newblock arXiv preprint arXiv:2112.14368.

\bibitem[Zimmert and Seldin, 2019]{pmlr-v89-zimmert19a}
Zimmert, J. and Seldin, Y. (2019).
\newblock An optimal algorithm for stochastic and adversarial bandits.
\newblock In {\em Proceedings of the Twenty-Second International Conference on
  Artificial Intelligence and Statistics}, volume~89 of {\em Proceedings of
  Machine Learning Research}, pages 467--475. PMLR.

\bibitem[Zinkevich, 2003]{zinkevich2003online}
Zinkevich, M. (2003).
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In {\em Proceedings of the Twentieth International Conference on
  International Conference on Machine Learning}, ICML'03, pages 928--935. AAAI
  Press.

\end{thebibliography}
