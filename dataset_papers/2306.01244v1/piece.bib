@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{strubell2019energy,
  title={Energy and Policy Considerations for Deep Learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3645--3650},
  year={2019}
}

@article{asi2019importance,
  title={The importance of better models in stochastic optimization},
  author={Asi, Hilal and Duchi, John C},
  journal={arXiv preprint arXiv:1903.08619},
  year={2019}
}


@article{BEKAS20071214,
title = {An estimator for the diagonal of a matrix},
journal = {Applied Numerical Mathematics},
volume = {57},
number = {11},
pages = {1214-1229},
year = {2007},
note = {Numerical Algorithms, Parallelism and Applications (2)},
issn = {0168-9274},
doi = {https://doi.org/10.1016/j.apnum.2007.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0168927407000244},
author = {C. Bekas and E. Kokiopoulou and Y. Saad},
keywords = {Stochastic estimator, Hadamard matrices, Grassmannian spaces, Electronic structure calculations, Density Functional Theory},
}

@article{birodkar2019semantic,
  title={Semantic Redundancies in Image-Classification Datasets: The 10\% You Don't Need},
  author={Birodkar, Vighnesh and Mobahi, Hossein and Bengio, Samy},
  journal={arXiv preprint arXiv:1901.11409},
  year={2019}
}

@inproceedings{coleman2020selection,
  title={Selection via Proxy: Efficient Data Selection for Deep Learning},
  author={Coleman, C and Yeh, C and Mussmann, S and Mirzasoleiman, B and Bailis, P and Liang, P and Leskovec, J and Zaharia, M},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@article{schwartz2019green,
  title={Green ai},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={arXiv preprint arXiv:1907.10597},
  year={2019}
}

@article{zhu2016we,
  title={Do we need more training data?},
  author={Zhu, Xiangxin and Vondrick, Carl and Fowlkes, Charless C and Ramanan, Deva},
  journal={International Journal of Computer Vision},
  volume={119},
  number={1},
  pages={76--92},
  year={2016},
  publisher={Springer}
}

@inproceedings{toneva2018empirical,
  title={An Empirical Study of Example Forgetting during Deep Neural Network Learning},
  author={Toneva, Mariya and Sordoni, Alessandro and des Combes, Remi Tachet and Trischler, Adam and Bengio, Yoshua and Gordon, Geoffrey J},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{halevy2009unreasonable,
  title={The unreasonable effectiveness of data},
  author={Halevy, Alon and Norvig, Peter and Pereira, Fernando},
  journal={IEEE Intelligent Systems},
  volume={24},
  number={2},
  pages={8--12},
  year={2009},
  publisher={IEEE}
}

@inproceedings{sun2017revisiting,
  title={Revisiting unreasonable effectiveness of data in deep learning era},
  author={Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={843--852},
  year={2017}
}

@article{vodrahalli2018all,
  title={Are all training examples created equal? an empirical study},
  author={Vodrahalli, Kailas and Li, Ke and Malik, Jitendra},
  journal={arXiv preprint arXiv:1811.12569},
  year={2018}
}

@inproceedings{ghorbani2019data,
  title={Data shapley: Equitable valuation of data for machine learning},
  author={Ghorbani, Amirata and Zou, James},
  booktitle={International Conference on Machine Learning},
  pages={2242--2251},
  year={2019},
  organization={PMLR}
}

@inproceedings{katharopoulos2018not,
  title={Not all samples are created equal: Deep learning with importance sampling},
  author={Katharopoulos, Angelos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={2525--2534},
  year={2018},
  organization={PMLR}
}

@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@article{loshchilov2015online,
  title={Online batch selection for faster training of neural networks},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1511.06343},
  year={2015}
}

@article{alain2015variance,
  title={Variance reduction in sgd by distributed importance sampling},
  author={Alain, Guillaume and Lamb, Alex and Sankar, Chinnadhurai and Courville, Aaron and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1511.06481},
  year={2015}
}

@inproceedings{huang2021novel,
  title={A Novel Sequential Coreset Method for Gradient Descent Algorithms},
  author={Huang, Jiawei and Huang, Ruomin and Liu, Wenjie and Freris, Nikolaos and Ding, Hu},
  booktitle={International Conference on Machine Learning},
  pages={4412--4422},
  year={2021},
  organization={PMLR}
}

@article{defazio2019ineffectiveness,
  title={On the Ineffectiveness of Variance Reduced Optimization for Deep Learning},
  author={Defazio, Aaron and Bottou, Leon},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={1755--1765},
  year={2019}
}

@article{marquardt1963algorithm,
  title={An algorithm for least-squares estimation of nonlinear parameters},
  author={Marquardt, Donald W},
  journal={Journal of the society for Industrial and Applied Mathematics},
  volume={11},
  number={2},
  pages={431--441},
  year={1963},
  publisher={SIAM}
}

@article{hutchinson1989stochastic,
  title={A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines},
  author={Hutchinson, Michael F},
  journal={Communications in Statistics-Simulation and Computation},
  volume={18},
  number={3},
  pages={1059--1076},
  year={1989},
  publisher={Taylor \& Francis}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{xiao2015learning,
  title={Learning from massive noisy labeled data for image classification},
  author={Xiao, Tong and Xia, Tian and Yang, Yi and Huang, Chang and Wang, Xiaogang},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2691--2699},
  year={2015}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@inproceedings{wang2019stochastic,
  title={Stochastic nonconvex optimization with large minibatches},
  author={Wang, Weiran and Srebro, Nathan},
  booktitle={Algorithmic Learning Theory},
  pages={857--882},
  year={2019},
  organization={PMLR}
}

@article{carmon2018accelerated,
  title={Accelerated methods for nonconvex optimization},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={SIAM Journal on Optimization},
  volume={28},
  number={2},
  pages={1751--1772},
  year={2018},
  publisher={SIAM}
}

@article{bertsekas1979convexification,
  title={Convexification procedures and decomposition methods for nonconvex optimization problems},
  author={Bertsekas, Dimitri P},
  journal={Journal of Optimization Theory and Applications},
  volume={29},
  number={2},
  pages={169--197},
  year={1979},
  publisher={Springer}
}

@article{jin2021nonconvex,
  title={On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points},
  author={Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M and Jordan, Michael I},
  journal={Journal of the ACM (JACM)},
  volume={68},
  number={2},
  pages={1--29},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{mindermann2022prioritized,
  title={Prioritized training on points that are learnable, worth learning, and not yet learnt},
  author={Mindermann, S{\"o}ren and Brauner, Jan M and Razzak, Muhammed T and Sharma, Mrinank and Kirsch, Andreas and Xu, Winnie and H{\"o}ltgen, Benedikt and Gomez, Aidan N and Morisot, Adrien and Farquhar, Sebastian and others},
  booktitle={International Conference on Machine Learning},
  pages={15630--15649},
  year={2022},
  organization={PMLR}
}

@article{guo2022deepcore,
  title={DeepCore: A Comprehensive Library for Coreset Selection in Deep Learning},
  author={Guo, Chengcheng and Zhao, Bo and Bai, Yanbing},
  journal={arXiv preprint arXiv:2204.08499},
  year={2022}
}

@article{paul2021deep,
  title={Deep learning on a data diet: Finding important examples early in training},
  author={Paul, Mansheej and Ganguli, Surya and Dziugaite, Gintare Karolina},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20596--20607},
  year={2021}
}

@inproceedings{sener2018active,
  title={Active Learning for Convolutional Neural Networks: A Core-Set Approach},
  author={Sener, Ozan and Savarese, Silvio},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{killamsetty2021glister,
  title={Glister: Generalization based data subset selection for efficient and robust learning},
  author={Killamsetty, Krishnateja and Sivasubramanian, Durga and Ramakrishnan, Ganesh and Iyer, Rishabh},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={9},
  pages={8110--8118},
  year={2021}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{zhai2022scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12104--12113},
  year={2022}
}

@inproceedings{pooladzandi2022adaptive,
  title={Adaptive second order coresets for data-efficient machine learning},
  author={Pooladzandi, Omead and Davini, David and Mirzasoleiman, Baharan},
  booktitle={International Conference on Machine Learning},
  pages={17848--17869},
  year={2022},
  organization={PMLR}
}

@article{kaushal2021prism,
  title={PRISM: A Unified Framework of Parameterized Submodular Information Measures for Targeted Data Subset Selection and Summarization},
  author={Kaushal, Vishal and Kothawade, Suraj and Ramakrishnan, Ganesh and Bilmes, Jeff and Iyer, Rishabh},
  journal={arXiv preprint arXiv:2103.00128},
  year={2021}
}

@article{sivasubramanian2021training,
  title={Training Data Subset Selection for Regression with Controlled Generalization Error},
  author={Sivasubramanian, Durga and Iyer, Rishabh and Ramakrishnan, Ganesh and De, Abir},
  journal={arXiv preprint arXiv:2106.12491},
  year={2021}
}

@article{maheshwari2020data,
  title={Data programming using semi-supervision and subset selection},
  author={Maheshwari, Ayush and Chatterjee, Oishik and Killamsetty, KrishnaTeja and Iyer, Rishabh and Ramakrishnan, Ganesh},
  journal={arXiv preprint arXiv:2008.09887},
  year={2020}
}

@inproceedings{mirzasoleiman2020coresets,
  title={Coresets for data-efficient training of machine learning models},
  author={Mirzasoleiman, Baharan and Bilmes, Jeff and Leskovec, Jure},
  booktitle={International Conference on Machine Learning},
  pages={6950--6960},
  year={2020},
  organization={PMLR}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{hinton2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  journal={Cited on},
  volume={14},
  number={8},
  year={2012}
}

@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}

@inproceedings{sutton1986two,
  title={Two problems with back propagation and other steepest descent learning procedures for networks},
  author={Sutton, Richard},
  booktitle={Proceedings of the Eighth Annual Conference of the Cognitive Science Society, 1986},
  pages={823--832},
  year={1986}
}

@article{qian1999momentum,
  title={On the momentum term in gradient descent learning algorithms},
  author={Qian, Ning},
  journal={Neural networks},
  volume={12},
  number={1},
  pages={145--151},
  year={1999},
  publisher={Elsevier}
}

@article{bertsekas1982projected,
  title={Projected Newton methods for optimization problems with simple constraints},
  author={Bertsekas, Dimitri P},
  journal={SIAM Journal on control and Optimization},
  volume={20},
  number={2},
  pages={221--246},
  year={1982},
  publisher={SIAM}
}

@article{Nocedal,
 ISSN = {00255718, 10886842},
 URL = {http://www.jstor.org/stable/2006193},
 abstract = {We study how to use the BFGS quasi-Newton matrices to precondition minimization methods for problems where the storage is critical. We give an update formula which generates matrices using information from the last $m$ iterations, where $m$ is any number supplied by the user. The quasi-Newton matrix is updated at every iteration by dropping the oldest information and replacing it by the newest information. It is shown that the matrices generated have some desirable properties. The resulting algorithms are tested numerically and compared with several well-known methods.},
 author = {Jorge Nocedal},
 journal = {Mathematics of Computation},
 number = {151},
 pages = {773--782},
 publisher = {American Mathematical Society},
 title = {Updating Quasi-Newton Matrices with Limited Storage},
 volume = {35},
 year = {1980}
}

@inproceedings{martens2015optimizing,
  title={Optimizing neural networks with kronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={International conference on machine learning},
  pages={2408--2417},
  year={2015},
  organization={PMLR}
}

@inproceedings{gupta2018shampoo,
  title={Shampoo: Preconditioned stochastic tensor optimization},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2018},
  organization={PMLR}
}

@inproceedings{schaul2013no,
  title={No more pesky learning rates},
  author={Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  booktitle={International Conference on Machine Learning},
  pages={343--351},
  year={2013},
  organization={PMLR}
}

@article{yao2020adahessian,
  title={ADAHESSIAN: An adaptive second order optimizer for machine learning},
  author={Yao, Zhewei and Gholami, Amir and Shen, Sheng and Keutzer, Kurt and Mahoney, Michael W},
  journal={arXiv preprint arXiv:2006.00719},
  year={2020}
}

@inproceedings{xu2020second,
  title={Second-order optimization for non-convex machine learning: An empirical study},
  author={Xu, Peng and Roosta, Fred and Mahoney, Michael W},
  booktitle={Proceedings of the 2020 SIAM International Conference on Data Mining},
  pages={199--207},
  year={2020},
  organization={SIAM}
}

@article{dembo1982inexact,
  title={Inexact newton methods},
  author={Dembo, Ron S and Eisenstat, Stanley C and Steihaug, Trond},
  journal={SIAM Journal on Numerical analysis},
  volume={19},
  number={2},
  pages={400--408},
  year={1982},
  publisher={SIAM}
}

@article{schraudolph2002fast,
  title={Fast curvature matrix-vector products for second-order gradient descent},
  author={Schraudolph, Nicol N},
  journal={Neural computation},
  volume={14},
  number={7},
  pages={1723--1738},
  year={2002},
  publisher={MIT Press}
}

@article{bollapragada2019exact,
  title={Exact and inexact subsampled Newton methods for optimization},
  author={Bollapragada, Raghu and Byrd, Richard H and Nocedal, Jorge},
  journal={IMA Journal of Numerical Analysis},
  volume={39},
  number={2},
  pages={545--578},
  year={2019},
  publisher={Oxford University Press}
}

@article{xu2020newton,
  title={Newton-type methods for non-convex optimization under inexact hessian information},
  author={Xu, Peng and Roosta, Fred and Mahoney, Michael W},
  journal={Mathematical Programming},
  volume={184},
  number={1},
  pages={35--70},
  year={2020},
  publisher={Springer}
}

@article{yao2018inexact,
  title={Inexact non-convex Newton-type methods},
  author={Yao, Zhewei and Xu, Peng and Roosta-Khorasani, Farbod and Mahoney, Michael W},
  journal={arXiv preprint arXiv:1802.06925},
  year={2018}
}

@article{natarajan1995sparse,
  title={Sparse approximate solutions to linear systems},
  author={Natarajan, Balas Kausik},
  journal={SIAM journal on computing},
  volume={24},
  number={2},
  pages={227--234},
  year={1995},
  publisher={SIAM}
}

@article{elenberg2018restricted,
  title={Restricted strong convexity implies weak submodularity},
  author={Elenberg, Ethan R and Khanna, Rajiv and Dimakis, Alexandros G and Negahban, Sahand and others},
  journal={Annals of Statistics},
  volume={46},
  number={6B},
  pages={3539--3568},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}

@article{chen2001atomic,
  title={Atomic decomposition by basis pursuit},
  author={Chen, Scott Shaobing and Donoho, David L and Saunders, Michael A},
  journal={SIAM review},
  volume={43},
  number={1},
  pages={129--159},
  year={2001},
  publisher={SIAM}
}

@article{tibshirani1996regression,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Wiley Online Library}
}

@article{candes2007dantzig,
  title={The Dantzig selector: Statistical estimation when p is much larger than n},
  author={Candes, Emmanuel and Tao, Terence and others},
  journal={Annals of statistics},
  volume={35},
  number={6},
  pages={2313--2351},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}

@article{pilanci2012recovery,
  title={Recovery of sparse probability measures via convex programming},
  author={Pilanci, Mert and El Ghaoui, Laurent and Chandrasekaran, Venkat},
  year={2012}
}

@inproceedings{kyrillidis2013sparse,
  title={Sparse projections onto the simplex},
  author={Kyrillidis, Anastasios and Becker, Stephen and Cevher, Volkan and Koch, Christoph},
  booktitle={International Conference on Machine Learning},
  pages={235--243},
  year={2013},
  organization={PMLR}
}

@article{donoho2006compressed,
  title={Compressed sensing},
  author={Donoho, David L},
  journal={IEEE Transactions on information theory},
  volume={52},
  number={4},
  pages={1289--1306},
  year={2006},
  publisher={IEEE}
}

@article{wolsey1982analysis,
	title={An analysis of the greedy algorithm for the submodular set covering problem},
	author={Wolsey, Laurence A},
	journal={Combinatorica},
	volume={2},
	number={4},
	pages={385--393},
	year={1982},
	publisher={Springer}
}

@inproceedings{mirzasoleiman2015lazier,
  title={Lazier than lazy greedy},
  author={Mirzasoleiman, Baharan and Badanidiyuru, Ashwinkumar and Karbasi, Amin and Vondr{\'a}k, Jan and Krause, Andreas},
  booktitle={Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year={2015}
}

@incollection{minoux1978accelerated,
	title={Accelerated greedy algorithms for maximizing submodular set functions},
	author={Minoux, Michel},
	booktitle={Optimization techniques},
	pages={234--243},
	year={1978},
	publisher={Springer}
}

@inproceedings{mirzasoleiman2013distributed,
	title={Distributed submodular maximization: Identifying representative elements in massive data},
	author={Mirzasoleiman, Baharan and Karbasi, Amin and Sarkar, Rik and Krause, Andreas},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2049--2057},
	year={2013}
}

@article{liu2020toward,
  title={Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2003.00307},
  year={2020}
}

@book{10.5555/993483,
author = {Boyd, Stephen and Vandenberghe, Lieven},
title = {Convex Optimization},
year = {2004},
isbn = {0521833787},
publisher = {Cambridge University Press},
address = {USA}
}

@inproceedings{killamsetty2021grad,
  title={Grad-match: Gradient matching based data subset selection for efficient deep model training},
  author={Killamsetty, Krishnateja and Durga, S and Ramakrishnan, Ganesh and De, Abir and Iyer, Rishabh},
  booktitle={International Conference on Machine Learning},
  pages={5464--5474},
  year={2021},
  organization={PMLR}
}

@inproceedings{allen2016exploiting,
	title={Exploiting the structure: Stochastic gradient methods using raw clusters},
	author={Allen-Zhu, Zeyuan and Yuan, Yang and Sridharan, Karthik},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1642--1650},
	year={2016}
}

@inproceedings{hofmann2015variance,
	title={Variance reduced stochastic gradient descent with neighbors},
	author={Hofmann, Thomas and Lucchi, Aurelien and Lacoste-Julien, Simon and McWilliams, Brian},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2305--2313},
	year={2015}
}

@InProceedings{bdd100k,
    author = {Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and Chen,
              Yingying and Liu, Fangchen and Madhavan, Vashisht and Darrell, Trevor},
    title = {BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning},
    booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2020}
}

@article{deng2012mnist,
  title={The mnist database of handwritten digit images for machine learning research},
  author={Deng, Li},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  number={6},
  pages={141--142},
  year={2012},
  publisher={IEEE}
}
@article{cifar10,
title= {CIFAR-10 (Canadian Institute for Advanced Research)},
journal= {},
author= {Alex Krizhevsky and Vinod Nair and Geoffrey Hinton},
year= {2009},
url= {http://www.cs.toronto.edu/~kriz/cifar.html},
abstract= {The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. 

The dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. },
keywords= {Dataset},
terms= {}
}

@inproceedings{bowman-etal-2015-large,
    title = "A large annotated corpus for learning natural language inference",
    author = "Bowman, Samuel R.  and
      Angeli, Gabor  and
      Potts, Christopher  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1075",
    doi = "10.18653/v1/D15-1075",
    pages = "632--642",
}

@article{imagenet15russakovsky,
    Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
    Title = { {ImageNet Large Scale Visual Recognition Challenge} },
    Year = {2015},
    journal   = {International Journal of Computer Vision (IJCV)},
    doi = {10.1007/s11263-015-0816-y},
    volume={115},
    number={3},
    pages={211-252}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}