\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abramowitz et~al.(1988)Abramowitz, Stegun, and Romer]{AS88}
Abramowitz, M., Stegun, I.~A., and Romer, R.~H.
\newblock Handbook of mathematical functions with formulas, graphs, and
  mathematical tables, 1988.

\bibitem[Berger(1985)]{Berger85}
Berger, J.~O.
\newblock \emph{Statistical Decision Theory and Bayesian Analysis}.
\newblock Springer Series in Statistics. Springer New York, New York, NY,
  January 1985.
\newblock URL \url{http://link.springer.com/10.1007/978-1-4757-4286-2}.

\bibitem[Buckman et~al.(2020)Buckman, Gelada, and Bellemare]{BuGeBe20}
Buckman, J., Gelada, C., and Bellemare, M.~G.
\newblock The importance of pessimism in {Fixed-Dataset} policy optimization.
\newblock September 2020.
\newblock URL \url{http://arxiv.org/abs/2009.06799}.

\bibitem[Chen et~al.(2019)Chen, Yu, and Haskell]{chen2019distributionally}
Chen, Z., Yu, P., and Haskell, W.~B.
\newblock Distributionally robust optimization for sequential decision-making.
\newblock \emph{Optimization}, 68\penalty0 (12):\penalty0 2397--2426, 2019.

\bibitem[Covington et~al.(2016)Covington, Adams, and Sargin]{covington2016deep}
Covington, P., Adams, J., and Sargin, E.
\newblock Deep neural networks for {Youtube} recommendations.
\newblock In \emph{Proceedings of the 10th ACM conference on recommender
  systems}, pp.\  191--198, 2016.

\bibitem[Dai et~al.(2020)Dai, Nachum, Chow, Li, Szepesv{\'a}ri, and
  Schuurmans]{dai2020coindice}
Dai, B., Nachum, O., Chow, Y., Li, L., Szepesv{\'a}ri, C., and Schuurmans, D.
\newblock Coindice: Off-policy confidence interval estimation.
\newblock \emph{arXiv preprint arXiv:2010.11652}, 2020.

\bibitem[Delage et~al.(2019)Delage, Kuhn, and Wiesemann]{DeKiWi19}
Delage, E., Kuhn, D., and Wiesemann, W.
\newblock ``dice''-sion--making under uncertainty: When can a random decision
  reduce risk?
\newblock \emph{Management Science}, 65\penalty0 (7):\penalty0 3282--3301, July
  2019.

\bibitem[Derman \& Mannor(2020)Derman and Mannor]{DeSh20}
Derman, E. and Mannor, S.
\newblock Distributional robustness and regularization in reinforcement
  learning.
\newblock March 2020.
\newblock URL \url{http://arxiv.org/abs/2003.02894}.

\bibitem[Duchi et~al.(2016)Duchi, Glynn, and Namkoong]{DuGlNam16}
Duchi, J., Glynn, P., and Namkoong, H.
\newblock Statistics of robust optimization: A generalized empirical likelihood
  approach.
\newblock October 2016.
\newblock URL \url{https://arxiv.org/abs/1610.03425v3}.

\bibitem[Faury et~al.(2019)Faury, Tanielian, Vasile, Smirnova, and
  Dohmatob]{FaTaVaSmDo19}
Faury, L., Tanielian, U., Vasile, F., Smirnova, E., and Dohmatob, E.
\newblock Distributionally robust counterfactual risk minimization.
\newblock June 2019.
\newblock URL \url{http://arxiv.org/abs/1906.06211}.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2052--2062. PMLR, 2019.

\bibitem[Gilboa \& Schmeidler(1989)Gilboa and Schmeidler]{GiSch89}
Gilboa, I. and Schmeidler, D.
\newblock Maxmin expected utility with non-unique prior.
\newblock \emph{Journal of Mathematical Economics}, 18\penalty0 (2):\penalty0
  141--153, 1989.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2019way}
Jaques, N., Ghandeharioun, A., Shen, J.~H., Ferguson, C., Lapedriza, A., Jones,
  N., Gu, S., and Picard, R.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock 2019.

\bibitem[Jin et~al.(2020)Jin, Yang, and Wang]{jin2020pessimism}
Jin, Y., Yang, Z., and Wang, Z.
\newblock Is pessimism provably efficient for offline {RL}?
\newblock 2020.

\bibitem[Karampatziakis et~al.(2019)Karampatziakis, Langford, and
  Mineiro]{karampatziakis2019empirical}
Karampatziakis, N., Langford, J., and Mineiro, P.
\newblock Empirical likelihood for contextual bandits.
\newblock \emph{arXiv preprint arXiv:1906.03323}, 2019.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{KiRaNeJo20}
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T.
\newblock {MOReL}: Model-based offline reinforcement learning.
\newblock In \emph{{NeurIPS}}, 2020.
\newblock URL
  \url{https://papers.nips.cc/paper/2020/hash/f7efa4f864ae9b88d43527f4b14f750f-Abstract.html}.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Tucker, and Levine]{kumar2019stabilizing}
Kumar, A., Fu, J., Tucker, G., and Levine, S.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock 2020.

\bibitem[Lai \& Robbins(1985)Lai and Robbins]{lai1985asymptotically}
Lai, T.~L. and Robbins, H.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock \emph{Advances in applied mathematics}, 6\penalty0 (1):\penalty0
  4--22, 1985.

\bibitem[Lam(2019)]{Lam2019}
Lam, H.
\newblock Recovering best statistical guarantees via the empirical
  {Divergence-Based} distributionally robust optimization.
\newblock \emph{Oper. Res.}, 67\penalty0 (4):\penalty0 1090--1105, July 2019.
\newblock URL \url{https://doi.org/10.1287/opre.2018.1786}.

\bibitem[Lattimore \& Szepesv{\'a}ri(2020)Lattimore and
  Szepesv{\'a}ri]{lattimore2020bandit}
Lattimore, T. and Szepesv{\'a}ri, C.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock 2020.

\bibitem[Liu et~al.(2020)Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E.
\newblock Provably good batch reinforcement learning without great exploration.
\newblock 2020.

\bibitem[Neyman \& Pearson(1933)Neyman and Pearson]{neyman1933ix}
Neyman, J. and Pearson, E.~S.
\newblock Ix. on the problem of the most efficient tests of statistical
  hypotheses.
\newblock \emph{Philosophical Transactions of the Royal Society of London.
  Series A, Containing Papers of a Mathematical or Physical Character},
  231\penalty0 (694-706):\penalty0 289--337, 1933.

\bibitem[Siegel et~al.(2020)Siegel, Springenberg, Berkenkamp, Abdolmaleki,
  Neunert, Lampe, Hafner, and Riedmiller]{siegel2020keep}
Siegel, N.~Y., Springenberg, J.~T., Berkenkamp, F., Abdolmaleki, A., Neunert,
  M., Lampe, T., Hafner, R., and Riedmiller, M.
\newblock Keep doing what worked: Behavioral modelling priors for offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.08396}, 2020.

\bibitem[Smith \& Winkler(2006{\natexlab{a}})Smith and Winkler]{SmiWi06}
Smith, J.~E. and Winkler, R.~L.
\newblock The optimizer's curse: Skepticism and postdecision surprise in
  decision analysis.
\newblock \emph{Manage. Sci.}, 52\penalty0 (3):\penalty0 311--322, March
  2006{\natexlab{a}}.
\newblock URL \url{https://doi.org/10.1287/mnsc.1050.0451}.

\bibitem[Smith \& Winkler(2006{\natexlab{b}})Smith and
  Winkler]{smith2006optimizer}
Smith, J.~E. and Winkler, R.~L.
\newblock The optimizerâ€™s curse: Skepticism and postdecision surprise in
  decision analysis.
\newblock \emph{Management Science}, 52\penalty0 (3):\penalty0 311--322,
  2006{\natexlab{b}}.

\bibitem[Strehl et~al.(2011)Strehl, Langford, Li, and Kakade]{strehl11learning}
Strehl, A.~L., Langford, J., Li, L., and Kakade, S.~M.
\newblock Learning from logged implicit exploration data.
\newblock In \emph{Advances in Neural Information Processing Systems 23}, pp.\
  2217--2225, 2011.

\bibitem[Sutter et~al.(2020)Sutter, Van~Parys, and Kuhn]{SuVPKuhn20}
Sutter, T., Van~Parys, B. P.~G., and Kuhn, D.
\newblock A general framework for optimal {Data-Driven} optimization.
\newblock October 2020.
\newblock URL \url{http://arxiv.org/abs/2010.06606}.

\bibitem[Swaminathan \& Joachims(2015)Swaminathan and
  Joachims]{swaminathan2015batch}
Swaminathan, A. and Joachims, T.
\newblock Batch learning from logged bandit feedback through counterfactual
  risk minimization.
\newblock \emph{The Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 1731--1755, 2015.

\bibitem[Van~Parys et~al.(2017)Van~Parys, Esfahani, and Kuhn]{VPEKuhn17}
Van~Parys, B. P.~G., Esfahani, P.~M., and Kuhn, D.
\newblock From data to decisions: Distributionally robust optimization is
  optimal.
\newblock April 2017.
\newblock URL \url{http://arxiv.org/abs/1704.04118}.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning.
\newblock 2019.

\bibitem[Xu \& Mannor(2010)Xu and Mannor]{xu2010distributionally}
Xu, H. and Mannor, S.
\newblock Distributionally robust markov decision processes.
\newblock In \emph{Proceedings of the 23rd International Conference on Neural
  Information Processing Systems-Volume 2}, pp.\  2505--2513, 2010.

\bibitem[Yang(2017)]{yang2017convex}
Yang, I.
\newblock A convex optimization approach to distributionally robust markov
  decision processes with wasserstein distance.
\newblock \emph{IEEE control systems letters}, 1\penalty0 (1):\penalty0
  164--169, 2017.

\bibitem[Yin et~al.(2021)Yin, Bai, and Wang]{yin2021near}
Yin, M., Bai, Y., and Wang, Y.-X.
\newblock Near-optimal offline reinforcement learning via double variance
  reduction.
\newblock \emph{arXiv preprint arXiv:2102.01748}, 2021.

\bibitem[Yu \& Xu(2015)Yu and Xu]{yu2015distributionally}
Yu, P. and Xu, H.
\newblock Distributionally robust counterpart in markov decision processes.
\newblock \emph{IEEE Transactions on Automatic Control}, 61\penalty0
  (9):\penalty0 2538--2543, 2015.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine, S., Finn, C., and Ma,
  T.
\newblock Mopo: Model-based offline policy optimization.
\newblock 2020.

\end{thebibliography}
