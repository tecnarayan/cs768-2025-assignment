@inproceedings{bhatia2018gen,
  title={{Gen-Oja}: Simple \& Efficient Algorithm for Streaming Generalized Eigenvector Computation},
  author={Bhatia, Kush and Pacchiano, Aldo and Flammarion, Nicolas and Bartlett, Peter L and Jordan, Michael I},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2018}
}


@inproceedings{jain2016streaming,
  title={Streaming {PCA}: Matching matrix Bernstein and near-optimal finite sample guarantees for Oja’s algorithm},
  author={Jain, Prateek and Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth and Sidford, Aaron},
  booktitle={Conference on Learning Theory (COLT)},
  year={2016}
}


@article{oja1982simplified,
  title={Simplified neuron model as a principal component analyzer},
  author={Oja, Erkki},
  journal={Journal of Mathematical Biology},
  volume={15},
  number={3},
  pages={267--273},
  year={1982},
  publisher={Springer}
}


@article{stewart1975methods,
  title={Methods of simultaneous iteration for calculating eigenvectors of matrices},
  author={Stewart, GW and Miller, JH},
  journal={Topics in Numerical Analysis II},
  pages={169--185},
  year={1975},
  publisher={New York: Academic}
}

@article{arbenz2012lecture,
  title={Lecture notes on solving large scale eigenvalue problems},
  author={Arbenz, Peter},
  journal={D-MATH, ETH Z{\"u}rich},
  volume={2},
  year={2016}
}

@article{stewart1976simultaneous,
  title={Simultaneous iteration for computing invariant subspaces of non-{Hermitian} matrices},
  author={Stewart, GW},
  journal={Numerische Mathematik},
  volume={25},
  number={2},
  pages={123--136},
  year={1976},
  publisher={Springer}
}


@article{ghadimi2016accelerated,
  title={Accelerated gradient methods for nonconvex nonlinear and stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={Mathematical Programming},
  volume={156},
  number={1-2},
  pages={59--99},
  year={2016},
  publisher={Springer}
}


@article{collins2018memorization,
  author={Edo Collins and
               Siavash Arjomand Bigdeli and
               Sabine S{\"{u}}sstrunk},
  title={Detecting Memorization in {ReLU} Networks},
  journal={arXiv},
  volume={abs/1810.03372},
  year={2018}
}

@article{stich2019errorfeedback,
  author    = {Sebastian U. Stich and
               Sai Praneeth Karimireddy},
  title     = {The Error-Feedback Framework: Better Rates for SGD with Delayed Gradients and Compressed Communication},
  journal   = {arXiv},
  volume    = {abs/1909.05350},
  year      = {2019}
}

@article{calas2018expanding,
  author    = {Sebastian Caldas and
               Jakub Konecn{\'{y}} and
               H. Brendan McMahan and
               Ameet Talwalkar},
  title     = {Expanding the Reach of Federated Learning by Reducing Client Resource
               Requirements},
  journal   = {arXiv},
  volume    = {abs/1812.07210},
  year      = {2018},
}


@InProceedings{Carlson:2015to,
  author={Carlson, David and Cevher, Volkan and Carin, Lawrence},
  title={{Stochastic Spectral Descent for Restricted {Boltzmann} Machines}},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2015},
  annote={has signSGD convergence for large batch. algo 1, but unclear and for a specialized case.}
}

@InProceedings{Bottou2010:sgd,
  author={Bottou, L{\'e}on},
  title={Large-Scale Machine Learning with Stochastic Gradient Descent},
  booktitle={International Conference on Computational Statistics (COMPSTAT)},
  year={2010},
}

@Misc{anonymous,
  title={Suppressed for Anonymity},
  author={Author, N. N.},
  year={2018}
}

@InProceedings{zaheer2018adaptive,
  title={Adaptive methods for nonconvex optimization},
  author={Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2018}
}

@inproceedings{yu2018gradiveq,
  author    = {Mingchao Yu and
               Zhifeng Lin and
               Krishna Narra and
               Songze Li and
               Youjie Li and
               Nam Sung Kim and
               Alexander G. Schwing and
               Murali Annavaram and
               Salman Avestimehr},
  title     = {GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation
               in Distributed {CNN} Training},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2018},
}

@Article{kamilov2018signprox,
  title={{signProx}: One-Bit Proximal Algorithm for Nonconvex Stochastic Optimization},
  author={Kamilov, Ulugbek S},
  journal={arXiv},
  volume={abs/1807.08023},
  year={2018}
}

@InProceedings{bernstein2018signsgd,
  title={{signSGD}: compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2018}
}

@InProceedings{bernstein2019iclr,
  title={{signSGD} with Majority Vote is Communication Efficient and Fault Tolerant},
  author={Jeremy Bernstein and Jiawei Zhao and Kamyar Azizzadenesheli and Anima Anandkumar},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}

@inproceedings{baevski2019adaptive,
  author    = {Alexei Baevski and
               Michael Auli},
  title     = {Adaptive Input Representations for Neural Language Modeling},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2019}
}

@InProceedings{liu2018signsgd,
  title={{signSGD} via Zeroth-Order Oracle},
  author={Liu, Sijia and Chen, Pin-Yu and Chen, Xiangyi and Hong, Mingyi},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@InProceedings{balles2017dissecting,
  title={Dissecting {Adam}: The Sign, Magnitude and Variance of Stochastic Gradients},
  author={Balles, Lukas and Hennig, Philipp},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2018}
}

@InProceedings{wang2018atomo,
  title={{ATOMO}: Communication-efficient Learning via Atomic Sparsification},
  author={Wang, Hongyi and Sievert, Scott and Liu, Shengchao and Charles, Zachary and Papailiopoulos, Dimitris and Wright, Stephen},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2018}
}

@InProceedings{stich2018sparsified,
  title={Sparsified {SGD} with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2018}
}

@InProceedings{alistarh2017quantized,
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  title={{QSGD}: Communication-Efficient SGD via Gradient Quantization and Encoding},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2017}
}

@InProceedings{wangni2018gradient,
  title={Gradient sparsification for communication-efficient distributed optimization},
  author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2018}
}

@InProceedings{wen2017terngrad,
  title={Terngrad: Ternary gradients to reduce communication in distributed deep learning},
  author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  pages={1509--1519},
  year={2017}
}

@InProceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Annual Conference of the International Speech Communication Association (INTERSPEECH)},
  year={2014}
}
@InProceedings{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2015}
}
@InProceedings{strom2015scalable,
  title={Scalable distributed {DNN} training using commodity {GPU} cloud computing},
  author={Strom, Nikko},
  booktitle={Annual Conference of the International Speech Communication Association (INTERSPEECH)},
  year={2015}
}

@Article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={Nature},
  volume={521},
  number={7553},
  pages={436},
  year={2015},
  publisher={Nature Publishing Group}
}

@Article{schmidhuber2015deep,
  title={Deep learning in neural networks: An overview},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural Networks},
  volume={61},
  pages={85--117},
  year={2015},
  publisher={Elsevier}
}

@Article{wainwright2008graphical,
  title={Graphical models, exponential families, and variational inference},
  author={Wainwright, Martin J and Jordan, Michael I and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={1},
  number={1--2},
  pages={1--305},
  year={2008},
  publisher={Now Publishers, Inc.}
}

@InProceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European Conference on Computer Vision (ECCV)},
  year={2016},
}

@InProceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2012}
}

@Article{goyal2017accurate,
  title={Accurate, large minibatch {SGD}: training imagenet in 1 hour},
  author={Goyal, Priya and Dollar, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv},
  volume={abs/1706.02677},
  year={2017}
}

@InProceedings{huang2017densely,
  title={Densely connected convolutional networks.},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017}
}

@InProceedings{chen2018neural,
  title={Neural Ordinary Differential Equations},
  author={Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2018}
}

@InProceedings{dean2012large,
  title={Large scale distributed deep networks},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc V and others},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2012}
}

@InProceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2015}
}

@InProceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2014}
}

@Article{robbins1951stochastic,
  author={Robbins, Herbert and Monro, Sutton},
  title={{A Stochastic Approximation Method}},
  journal={The Annals of Mathematical Statistics},
  year={1951},
  volume={22},
  number={3},
  pages={400--407},
  month=sep,
}

@InProceedings{riedmiller1993direct,
  title={A direct adaptive method for faster backpropagation learning: The {RPROP} algorithm},
  author={Riedmiller, Martin and Braun, Heinrich},
  booktitle={International Conference on Neural Networks (ICNN)},
  year={1993},
}

@Article{tieleman2012lecture,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}

@Article{lacoste2012simpler,
  title={A simpler approach to obtaining an O(1/t) convergence rate for the projected stochastic subgradient method},
  author={Lacoste-Julien, Simon and Schmidt, Mark and Bach, Francis},
  journal={arXiv},
  volume={abs/1212.2002},
  year={2012}
}

@InProceedings{li2014scaling,
  title={Scaling Distributed Machine Learning with the Parameter Server},
  author={Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-Yiing},
  booktitle={USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year={2014}
}

@InProceedings{chilimbi2014project,
  title={Project {Adam}: Building an Efficient and Scalable Deep Learning Training System},
  author={Chilimbi, Trishul M and Suzue, Yutaka and Apacible, Johnson and Kalyanaraman, Karthik},
  booktitle={USENIX Symposium on Operating Systems Design and Implementation (OSDI)},
  year={2014}
}

@InProceedings{lin2017deep,
  title={Deep gradient compression: Reducing the communication bandwidth for distributed training},
  author={Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

#################### Generalization ####################


@Article{soltanolkotabi2018theoretical,
  title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author={Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D},
  journal={Transactions on Information Theory},
  year={2018},
  publisher={IEEE}
}

@Article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={70},
  year={2018}
}

@Article{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  journal={arXiv},
  volume={abs/1703.04933},
  year={2017}
}

@InProceedings{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2017}
}

@Article{zhang2018study,
  title={A Study on Overfitting in Deep Reinforcement Learning},
  author={Zhang, Chiyuan and Vinyals, Oriol and Munos, Remi and Bengio, Samy},
  journal={arXiv},
  volume={abs/1804.06893},
  year={2018}
}

@InProceedings{arpit2017closer,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and Jastrz{\k{e}}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2017}
}

@Article{kawaguchi2017generalization,
  title={Generalization in deep learning},
  author={Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  journal={arXiv},
  volume={abs/1710.05468},
  year={2017}
}

@InProceedings{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2018}
}

@Article{im2016empirical,
  title={An empirical analysis of the optimization of deep network loss surfaces},
  author={Im, Daniel Jiwoong and Tao, Michael and Branson, Kristin},
  journal={arXiv},
  volume={abs/1612.04010},
  year={2016}
}

@InProceedings{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2018}
}

@Misc{paszke2017pytorch,
  title={{PyTorch}},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory},
  year={2017}
}

@techreport{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey},
  year={2009},
  institution={University of Toronto}
}

@Article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv},
  volume={abs/1409.1556},
  year={2014}
}

@InProceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016}
}

@Article{cortes1995support,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@Article{valiant1984theory,
  title={A theory of the learnable},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={27},
  number={11},
  pages={1134--1142},
  year={1984},
  publisher={ACM}
}

@article{martin2018implicit,
  title={Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning},
  author={Martin, Charles H and Mahoney, Michael W},
  journal={arXiv},
  volume={abs/1810.01075},
  year={2018}
}


@article{yoshida2017spectral,
  title={Spectral norm regularization for improving the generalizability of deep learning},
  author={Yoshida, Yuichi and Miyato, Takeru},
  journal={arXiv},
  volume={abs/1705.10941},
  year={2017}
}

@article{mazumder2010spectral,
  title={Spectral regularization algorithms for learning large incomplete matrices},
  author={Mazumder, Rahul and Hastie, Trevor and Tibshirani, Robert},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Aug},
  pages={2287--2322},
  year={2010}
}

@InProceedings{li2017algorithmic,
  title={Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle={Conference on Learning Theory (COLT)},
  year={2018}
}

@InProceedings{arora2018stronger,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2018}
}

@article{wangni2019trajectory,
  title={Trajectory Normalized Gradients for Distributed Optimization},
  author={Wangni, Jianqiao and Li, Ke and Shi, Jianbo and Malik, Jitendra},
  journal={arXiv},
  volume={abs/1901.08227},
  year={2019}
}
@InProceedings{karimireddy2019error,
  title={Error Feedback Fixes {SignSGD} and other Gradient Compression Schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian U and Jaggi, Martin},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2019}
}

@inproceedings{awan2018optimized,
  title={Optimized broadcast for deep learning workloads on dense-{GPU} infiniband clusters: {MPI} or {NCCL}?},
  author={Awan, Ammar Ahmad and Chu, Ching-Hsiang and Subramoni, Hari and Panda, Dhabaleswar K},
  booktitle={European MPI Users' Group Meeting (EuroMPI)},
  year={2018},
}

@misc{nvidia2019,
  author={NVIDIA},
  title={{NVIDIA} Collective Communications Library ({NCCL})},
  howpublished={\url{https://developer.nvidia.com/nccl}},
  year={2019},
  note={[Online; accessed 21-May-2019]},
}

@misc{github2019signsgd,
  author={Jiawei Zhao},
  title={{signSGD} with Majority Vote},
  howpublished={\url{github.com/PermiJW/signSGD-with-Majority-Vote}},
  year={2019},
  note={[Online; accessed 12-May-2019]},
}

@misc{nvidia2019speeding,
  author={Sylvain Jeaugey},
  title={Massively Scale Your Deep Learning Training with {NCCL} 2.4},
  howpublished={\url{https://devblogs.nvidia.com/massively-scale-deep-learning-training-nccl-2-4/}},
  year={2019},
  note={[Online; accessed 21-May-2019]},
}

@misc{iandola2015firecaffe,
  title={{FireCaffe}: near-linear acceleration of deep neural network training on compute clusters. CoRR abs/1511.00175 (2015)},
  author={Iandola, Forrest N and Ashraf, Khalid and Moskewicz, Matthew W and Keutzer, Kurt},
  year={2015}
}

@inproceedings{panda2019high,
  title={High Performance Distributed Deep Learning: A Beginner’s Guide},
  author={Panda, Dhabaleswar K DK and Subramoni, Hari and Awan, Ammar Ahmad},
  booktitle={Symposium on Principles and Practice of Parallel Programming (PPoPP)},
  year={2019},
}

@inproceedings{yurtsever2017sketchy,
  title={Sketchy decisions: Convex low-rank matrix optimization with optimal storage},
  author={Yurtsever, Alp and Udell, Madeleine and Tropp, Joel A and Cevher, Volkan},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2017},
}

@article{lin2018don,
  title={Don't Use Large Mini-Batches, Use Local {SGD}},
  author={Lin, Tao and Stich, Sebastian U and Jaggi, Martin},
  journal={arXiv},
  volume={abs/1808.07217},
  year={2018}
}

@article{stich2018local,
  title={Local {SGD} converges fast and communicates little},
  author={Stich, Sebastian U},
  journal={arXiv},
  volume={abs/1805.09767},
  year={2018}
}

@article{konevcny2016afederated,
  title={Federated optimization: Distributed machine learning for on-device intelligence},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Ramage, Daniel and Richt{\'a}rik, Peter},
  journal={arXiv},
  volume={abs/1610.02527},
  year={2016}
}

@article{konevcny2016bfederated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv},
  volume={abs/1610.05492},
  year={2016}
}

@article{shallue2018measuring,
  title={Measuring the effects of data parallelism on neural network training},
  author={Shallue, Christopher J and Lee, Jaehoon and Antognini, Joe and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E},
  journal={arXiv},
  volume={abs/1811.03600},
  year={2018}
}

@inproceedings{ott2019fairseq,
  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
}
