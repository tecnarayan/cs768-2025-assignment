\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alemi et~al.(2018)Alemi, Fischer, and Dillon]{alemi2018uncertainty}
Alexander~A Alemi, Ian Fischer, and Joshua~V Dillon.
\newblock Uncertainty in the variational information bottleneck.
\newblock In \emph{UAI}, 2018.

\bibitem[Nguyen et~al.(2015)Nguyen, Yosinski, and Clune]{nguyen2015deep}
Anh Nguyen, Jason Yosinski, and Jeff Clune.
\newblock Deep neural networks are easily fooled: High confidence predictions
  for unrecognizable images.
\newblock In \emph{CVPR}, 2015.

\bibitem[Hein et~al.(2019)Hein, Andriushchenko, and Bitterwolf]{hein2019relu}
Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf.
\newblock Why {ReLU} networks yield high-confidence predictions far away from
  the training data and how to mitigate the problem.
\newblock In \emph{CVPR}, 2019.

\bibitem[Kristiadi et~al.(2020)Kristiadi, Hein, and Hennig]{kristiadi2020being}
Agustinus Kristiadi, Matthias Hein, and Philipp Hennig.
\newblock Being {B}ayesian, even just a bit, fixes overconfidence in {ReLU}
  networks.
\newblock In \emph{ICML}, 2020.

\bibitem[MacKay(1992{\natexlab{a}})]{mackay1992practical}
David~JC MacKay.
\newblock A practical {B}ayesian framework for backpropagation networks.
\newblock \emph{Neural Computation}, 4\penalty0 (3), 1992{\natexlab{a}}.

\bibitem[Daxberger et~al.(2021{\natexlab{a}})Daxberger, Kristiadi, Immer,
  Eschenhagen, Bauer, and Hennig]{daxberger2021laplace}
Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen,
  Matthias Bauer, and Philipp Hennig.
\newblock {L}aplace redux--effortless {B}ayesian deep learning.
\newblock In \emph{NeurIPS}, 2021{\natexlab{a}}.

\bibitem[Immer et~al.(2021{\natexlab{a}})Immer, Korzepa, and
  Bauer]{immer2021improving}
Alexander Immer, Maciej Korzepa, and Matthias Bauer.
\newblock Improving predictions of {B}ayesian neural nets via local
  linearization.
\newblock In \emph{AISTATS}, 2021{\natexlab{a}}.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell_weight_2015}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural networks.
\newblock In \emph{ICML}, 2015.

\bibitem[Louizos and Welling(2016)]{louizos2016structured}
Christos Louizos and Max Welling.
\newblock Structured and efficient variational deep learning with matrix
  {G}aussian posteriors.
\newblock In \emph{ICML}, 2016.

\bibitem[Louizos and Welling(2017)]{louizos2017multiplicative}
Christos Louizos and Max Welling.
\newblock Multiplicative normalizing flows for variational {B}ayesian neural
  networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Ritter et~al.(2018{\natexlab{a}})Ritter, Botev, and
  Barber]{ritter2018laplace}
Hippolyt Ritter, Aleksandar Botev, and David Barber.
\newblock A scalable {L}aplace approximation for neural networks.
\newblock In \emph{ICLR}, 2018{\natexlab{a}}.

\bibitem[Osawa et~al.(2019)Osawa, Swaroop, Khan, Jain, Eschenhagen, Turner, and
  Yokota]{osawa2019practical}
Kazuki Osawa, Siddharth Swaroop, Mohammad Emtiyaz~E Khan, Anirudh Jain, Runa
  Eschenhagen, Richard~E Turner, and Rio Yokota.
\newblock Practical deep learning with {B}ayesian principles.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Maddox et~al.(2019)Maddox, Izmailov, Garipov, Vetrov, and
  Wilson]{maddox2019simple}
Wesley~J Maddox, Pavel Izmailov, Timur Garipov, Dmitry~P Vetrov, and
  Andrew~Gordon Wilson.
\newblock A simple baseline for {B}ayesian uncertainty in deep learning.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Dusenberry et~al.(2020)Dusenberry, Jerfel, Wen, Ma, Snoek, Heller,
  Lakshminarayanan, and Tran]{dusenberry2020rankone}
Michael Dusenberry, Ghassen Jerfel, Yeming Wen, Yian Ma, Jasper Snoek,
  Katherine Heller, Balaji Lakshminarayanan, and Dustin Tran.
\newblock Efficient and scalable {B}ayesian neural nets with rank-1 factors.
\newblock In \emph{ICML}, 2020.

\bibitem[MacKay(1992{\natexlab{b}})]{mackay1992evidence}
David~JC MacKay.
\newblock The evidence framework applied to classification networks.
\newblock \emph{Neural Computation}, 4\penalty0 (5), 1992{\natexlab{b}}.

\bibitem[Foong et~al.(2019)Foong, Li, Hern{\'a}ndez-Lobato, and
  Turner]{foong2019between}
Andrew~YK Foong, Yingzhen Li, Jos{\'e}~Miguel Hern{\'a}ndez-Lobato, and
  Richard~E Turner.
\newblock In-between uncertainty in {B}ayesian neural networks.
\newblock \emph{arXiv}, 2019.

\bibitem[Kristiadi et~al.(2021)Kristiadi, Hein, and
  Hennig]{kristiadi2021infinite}
Agustinus Kristiadi, Matthias Hein, and Philipp Hennig.
\newblock An infinite-feature extension for {B}ayesian {ReLU} nets that fixes
  their asymptotic overconfidence.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
Radford~M Neal et~al.
\newblock {MCMC} using {H}amiltonian dynamics.
\newblock \emph{Handbook of Markov Chain Monte Carlo}, 2\penalty0 (11), 2011.

\bibitem[Hoffman et~al.(2014)Hoffman, Gelman, et~al.]{hoffman2014nuts}
Matthew~D Hoffman, Andrew Gelman, et~al.
\newblock The {No-U-Turn} sampler: Adaptively setting path lengths in
  {H}amiltonian {M}onte {C}arlo.
\newblock \emph{JMLR}, 15\penalty0 (1), 2014.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{NIPS}, 2017.

\bibitem[Zhang et~al.(2020)Zhang, Li, Zhang, Chen, and
  Wilson]{zhang2020csgmcmc}
Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew~Gordon Wilson.
\newblock Cyclical stochastic gradient mcmc for {B}ayesian deep learning.
\newblock In \emph{ICLR}, 2020.

\bibitem[Rezende and Mohamed(2015)]{rezende2015variational}
Danilo Rezende and Shakir Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In \emph{ICML}, 2015.

\bibitem[Dangel et~al.(2020)Dangel, Kunstner, and Hennig]{dangel2020backpack}
Felix Dangel, Frederik Kunstner, and Philipp Hennig.
\newblock {BackPACK}: Packing more into backprop.
\newblock In \emph{ICLR}, 2020.

\bibitem[Osawa(2021)]{osawa2021asdl}
Kazuki Osawa.
\newblock {ASDL}: Automatic second-order differentiation (for {F}isher,
  gradient covariance, {H}essian, {J}acobian, and kernel) library.
\newblock \url{https://github.com/kazukiosawa/asdfghjkl}, 2021.

\bibitem[Hinton and Van~Camp(1993)]{hinton1993keeping}
Geoffrey~E Hinton and Drew Van~Camp.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{COLT}, 1993.

\bibitem[Murphy(2012)]{Murphy:2012:MLP:2380985}
Kevin~P. Murphy.
\newblock \emph{Machine Learning: A Probabilistic Perspective}.
\newblock The MIT Press, 2012.

\bibitem[Spiegelhalter and Lauritzen(1990)]{spiegelhalter1990sequential}
David~J Spiegelhalter and Steffen~L Lauritzen.
\newblock Sequential updating of conditional probabilities on directed
  graphical structures.
\newblock \emph{Networks}, 20\penalty0 (5), 1990.

\bibitem[Gibbs(1998)]{gibbs1998bayesian}
Mark Gibbs.
\newblock \emph{{B}ayesian Gaussian Processes for Regression and
  Classification}.
\newblock PhD thesis, University of Cambridge, 1998.

\bibitem[Hobbhahn et~al.(2020)Hobbhahn, Kristiadi, and
  Hennig]{hobbhahn2020laplacebridge}
Marius Hobbhahn, Agustinus Kristiadi, and Philipp Hennig.
\newblock Fast predictive uncertainty for classification with {B}ayesian deep
  networks.
\newblock \emph{arXiv preprint arXiv:2003.01227}, 2020.

\bibitem[Rippel and Adams(2013)]{rippel2013density}
Oren Rippel and Ryan~Prescott Adams.
\newblock High-dimensional probability estimation with deep density models.
\newblock \emph{arXiv preprint arXiv:1302.5125}, 2013.

\bibitem[Dinh et~al.(2015)Dinh, Krueger, and Bengio]{dinh2015nice}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock {NICE}: Non-linear independent components estimation.
\newblock In \emph{ICLR Workshop}, 2015.

\bibitem[Eschenhagen et~al.(2021)Eschenhagen, Daxberger, Hennig, and
  Kristiadi]{eschenhagen2021mixtures}
Runa Eschenhagen, Erik Daxberger, Philipp Hennig, and Agustinus Kristiadi.
\newblock Mixtures of {L}aplace approximations for improved post-hoc
  uncertainty in deep learning.
\newblock In \emph{NeurIPS Workshop of {B}ayesian Deep Learning}, 2021.

\bibitem[Sun et~al.(2019)Sun, Zhang, Shi, and Grosse]{sun2018functional}
Shengyang Sun, Guodong Zhang, Jiaxin Shi, and Roger Grosse.
\newblock Functional variational {B}ayesian neural networks.
\newblock In \emph{ICLR}, 2019.

\bibitem[Lotfi et~al.(2022)Lotfi, Izmailov, Benton, Goldblum, and
  Wilson]{lotfi2022marglik}
Sanae Lotfi, Pavel Izmailov, Gregory Benton, Micah Goldblum, and Andrew~Gordon
  Wilson.
\newblock {B}ayesian model selection, the marginal likelihood, and
  generalization.
\newblock \emph{arXiv preprint arXiv:2202.11678}, 2022.

\bibitem[Izmailov et~al.(2021)Izmailov, Vikram, Hoffman, and
  Wilson]{izmailov2021hmc}
Pavel Izmailov, Sharad Vikram, Matthew~D Hoffman, and Andrew~Gordon Wilson.
\newblock What are {B}ayesian neural network posteriors really like?
\newblock In \emph{ICML}, 2021.

\bibitem[Papamakarios et~al.(2021)Papamakarios, Nalisnick, Rezende, Mohamed,
  and Lakshminarayanan]{papamakarios2021normalizing}
George Papamakarios, Eric Nalisnick, Danilo~Jimenez Rezende, Shakir Mohamed,
  and Balaji Lakshminarayanan.
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock \emph{JMLR}, 22\penalty0 (57), 2021.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In \emph{BMVC}, 2016.

\bibitem[Izmailov et~al.(2019)Izmailov, Maddox, Kirichenko, Garipov, Vetrov,
  and Wilson]{izmailov2019subspace}
Pavel Izmailov, Wesley~J Maddox, Polina Kirichenko, Timur Garipov, Dmitry
  Vetrov, and Andrew~Gordon Wilson.
\newblock Subspace inference for {B}ayesian deep learning.
\newblock In \emph{UAI}, 2019.

\bibitem[Bingham et~al.(2019)Bingham, Chen, Jankowiak, Obermeyer, Pradhan,
  Karaletsos, Singh, Szerlip, Horsfall, and Goodman]{bingham2019pyro}
Eli Bingham, Jonathan~P Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj
  Pradhan, Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and
  Noah~D Goodman.
\newblock Pyro: Deep universal probabilistic programming.
\newblock \emph{JMLR}, 20\penalty0 (1), 2019.

\bibitem[Maro{\~n}as et~al.(2021)Maro{\~n}as, Hamelijnck, Knoblauch, and
  Damoulas]{maronas2021gpnf}
Juan Maro{\~n}as, Oliver Hamelijnck, Jeremias Knoblauch, and Theodoros
  Damoulas.
\newblock Transforming {G}aussian processes with normalizing flows.
\newblock In \emph{AISTATS}, 2021.

\bibitem[Miller et~al.(2017)Miller, Foti, and Adams]{miller2017vboost}
Andrew~C Miller, Nicholas~J Foti, and Ryan~P Adams.
\newblock Variational boosting: Iteratively refining posterior approximations.
\newblock In \emph{ICML}, 2017.

\bibitem[Havasi et~al.(2021)Havasi, Snoek, Tran, Gordon, and
  Hern{\'a}ndez-Lobato]{havasi2021refining}
Marton Havasi, Jasper Snoek, Dustin Tran, Jonathan Gordon, and Jos{\'e}~Miguel
  Hern{\'a}ndez-Lobato.
\newblock Refining the variational posterior through iterative optimization.
\newblock \emph{Entropy}, 23\penalty0 (1475), 2021.

\bibitem[Wilson and Izmailov(2020)]{wilson2020bayesian}
Andrew~Gordon Wilson and Pavel Izmailov.
\newblock {B}ayesian deep learning and a probabilistic perspective of
  generalization.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11), 1998.

\bibitem[Gelman and Rubin(1992)]{gelman1992inference}
Andrew Gelman and Donald~B Rubin.
\newblock Inference from iterative simulation using multiple sequences.
\newblock \emph{Statistical Science}, 7\penalty0 (4), 1992.

\bibitem[Wen et~al.(2018)Wen, Vicol, Ba, Tran, and Grosse]{wen2018flipout}
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse.
\newblock Flipout: Efficient pseudo-independent weight perturbations on
  mini-batches.
\newblock In \emph{ICLR}, 2018.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and
  Hauskrecht]{naeini2015obtaining}
Mahdi~Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht.
\newblock Obtaining well calibrated probabilities using {B}ayesian binning.
\newblock In \emph{AAAI}, 2015.

\bibitem[Gretton et~al.(2012)Gretton, Borgwardt, Rasch, Sch{\"o}lkopf, and
  Smola]{gretton2012mmd}
Arthur Gretton, Karsten~M Borgwardt, Malte~J Rasch, Bernhard Sch{\"o}lkopf, and
  Alexander Smola.
\newblock A kernel two-sample test.
\newblock \emph{JMLR}, 13\penalty0 (1), 2012.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo17calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q. Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Daxberger et~al.(2021{\natexlab{b}})Daxberger, Nalisnick, Allingham,
  Antor{\'a}n, and Hern{\'a}ndez-Lobato]{daxberger2021subnetwork}
Erik Daxberger, Eric Nalisnick, James~U Allingham, Javier Antor{\'a}n, and
  Jos{\'e}~Miguel Hern{\'a}ndez-Lobato.
\newblock {B}ayesian deep learning via subnetwork inference.
\newblock In \emph{ICML}, 2021{\natexlab{b}}.

\bibitem[Ritter et~al.(2018{\natexlab{b}})Ritter, Botev, and
  Barber]{ritter2018online}
Hippolyt Ritter, Aleksandar Botev, and David Barber.
\newblock Online structured {L}aplace approximations for overcoming
  catastrophic forgetting.
\newblock In \emph{NIPS}, 2018{\natexlab{b}}.

\bibitem[Immer et~al.(2021{\natexlab{b}})Immer, Bauer, Fortuin, R{\"a}tsch, and
  Khan]{immer2021scalable}
Alexander Immer, Matthias Bauer, Vincent Fortuin, Gunnar R{\"a}tsch, and
  Mohammad~Emtiyaz Khan.
\newblock Scalable marginal likelihood estimation for model selection in deep
  learning.
\newblock In \emph{ICML}, 2021{\natexlab{b}}.

\bibitem[mpa()]{mpa_derivation}
Convolution of {G}aussians and the {P}robit integral.
\newblock \url{https://agustinus.kristia.de/techblog/2022/06/25/conv-probit/}.
\newblock Accessed: 2022-09-26.

\bibitem[Lu et~al.(2020)Lu, Ie, and Sha]{lu2020mfsoftmax}
Zhiyun Lu, Eugene Ie, and Fei Sha.
\newblock Mean-field approximation to {G}aussian-softmax integral with
  application to uncertainty estimation.
\newblock \emph{arXiv preprint arXiv:2006.07584}, 2020.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017cosine}
Ilya Loshchilov and Frank Hutter.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock In \emph{ICLR}, 2017.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In \emph{ICLR}, 2019.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019can}
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian
  Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
\newblock Can you trust your model's uncertainty? {E}valuating predictive
  uncertainty under dataset shift.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Mazeika, and
  Dietterich]{hendrycks2018deep}
Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich.
\newblock Deep anomaly detection with outlier exposure.
\newblock In \emph{ICLR}, 2019.

\bibitem[Kristiadi et~al.(2022)Kristiadi, Hein, and Hennig]{kristiadi2022freq}
Agustinus Kristiadi, Matthias Hein, and Philipp Hennig.
\newblock Being a bit frequentist improves {B}ayesian neural networks.
\newblock In \emph{AISTATS}, 2022.

\bibitem[Cho et~al.(2014)Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau,
  Bougares, Schwenk, and Bengio]{cho2014learning}
Kyunghyun Cho, Bart Van~Merri{\"e}nboer, Caglar Gulcehre, Dzmitry Bahdanau,
  Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using {RNN} encoder-decoder for
  statistical machine translation.
\newblock In \emph{EMNLP}, 2014.

\end{thebibliography}
