@article{bigbird,
  author    = {Manzil Zaheer and
               Guru Guruganesh and
               Avinava Dubey and
               Joshua Ainslie and
               Chris Alberti and
               Santiago Onta{\~{n}}{\'{o}}n and
               Philip Pham and
               Anirudh Ravula and
               Qifan Wang and
               Li Yang and
               Amr Ahmed},
  title     = {Big Bird: Transformers for Longer Sequences},
  journal   = {CoRR},
  volume    = {abs/2007.14062},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.14062},
  eprinttype = {arXiv},
  eprint    = {2007.14062},
  timestamp = {Mon, 03 Aug 2020 14:32:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-14062.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{fastrg,
  title={A note on quickly sampling a sparse matrix with low rank expectation},
  author={Rohe, Karl and Tao, Jun and Han, Xintian and Binkiewicz, Norbert},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={3040--3052},
  year={2018},
  publisher={JMLR. org}
}

@article{linformer,
  author    = {Sinong Wang and
               Belinda Z. Li and
               Madian Khabsa and
               Han Fang and
               Hao Ma},
  title     = {Linformer: Self-Attention with Linear Complexity},
  journal   = {CoRR},
  volume    = {abs/2006.04768},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.04768},
  eprinttype = {arXiv},
  eprint    = {2006.04768},
  timestamp = {Fri, 12 Jun 2020 14:02:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-04768.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ste,
  author    = {Yoshua Bengio and
               Nicholas L{\'{e}}onard and
               Aaron C. Courville},
  title     = {Estimating or Propagating Gradients Through Stochastic Neurons for
               Conditional Computation},
  journal   = {CoRR},
  volume    = {abs/1308.3432},
  year      = {2013},
  url       = {http://arxiv.org/abs/1308.3432},
  eprinttype = {arXiv},
  eprint    = {1308.3432},
  timestamp = {Mon, 13 Aug 2018 16:47:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BengioLC13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{dgm,
  author    = {Anees Kazi and
               Luca Cosmo and
               Nassir Navab and
               Michael M. Bronstein},
  title     = {Differentiable Graph Module {(DGM)} Graph Convolutional Networks},
  journal   = {CoRR},
  volume    = {abs/2002.04999},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.04999},
  eprinttype = {arXiv},
  eprint    = {2002.04999},
  timestamp = {Fri, 14 Feb 2020 12:07:41 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-04999.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{correia2019,
  author    = {Gon{\c{c}}alo M. Correia and
               Vlad Niculae and
               Andr{\'{e}} F. T. Martins},
  title     = {Adaptively Sparse Transformers},
  journal   = {CoRR},
  volume    = {abs/1909.00015},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.00015},
  eprinttype = {arXiv},
  eprint    = {1909.00015},
  timestamp = {Mon, 16 Sep 2019 17:27:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-00015.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{nystromformer,
  author    = {Yunyang Xiong and
               Zhanpeng Zeng and
               Rudrasis Chakraborty and
               Mingxing Tan and
               Glenn Fung and
               Yin Li and
               Vikas Singh},
  title     = {Nystr{\"{o}}mformer: {A} Nystr{\"{o}}m-Based Algorithm for
               Approximating Self-Attention},
  journal   = {CoRR},
  volume    = {abs/2102.03902},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.03902},
  eprinttype = {arXiv},
  eprint    = {2102.03902},
  timestamp = {Thu, 16 Dec 2021 17:36:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-03902.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lra,
  author    = {Yi Tay and
               Mostafa Dehghani and
               Samira Abnar and
               Yikang Shen and
               Dara Bahri and
               Philip Pham and
               Jinfeng Rao and
               Liu Yang and
               Sebastian Ruder and
               Donald Metzler},
  title     = {Long Range Arena: {A} Benchmark for Efficient Transformers},
  journal   = {CoRR},
  volume    = {abs/2011.04006},
  year      = {2020},
  url       = {https://arxiv.org/abs/2011.04006},
  eprinttype = {arXiv},
  eprint    = {2011.04006},
  timestamp = {Thu, 12 Nov 2020 15:14:56 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2011-04006.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{etsurvey,
  author    = {Yi Tay and
               Mostafa Dehghani and
               Dara Bahri and
               Donald Metzler},
  title     = {Efficient Transformers: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2009.06732},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.06732},
  eprinttype = {arXiv},
  eprint    = {2009.06732},
  timestamp = {Fri, 18 Sep 2020 15:17:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-06732.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{reformer,
  author    = {Nikita Kitaev and
               Lukasz Kaiser and
               Anselm Levskaya},
  title     = {Reformer: The Efficient Transformer},
  journal   = {CoRR},
  volume    = {abs/2001.04451},
  year      = {2020},
  url       = {https://arxiv.org/abs/2001.04451},
  eprinttype = {arXiv},
  eprint    = {2001.04451},
  timestamp = {Sat, 23 Jan 2021 01:20:41 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2001-04451.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lineartransformer,
  author    = {Angelos Katharopoulos and
               Apoorv Vyas and
               Nikolaos Pappas and
               Fran{\c{c}}ois Fleuret},
  title     = {Transformers are RNNs: Fast Autoregressive Transformers with Linear
               Attention},
  journal   = {CoRR},
  volume    = {abs/2006.16236},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.16236},
  eprinttype = {arXiv},
  eprint    = {2006.16236},
  timestamp = {Wed, 01 Jul 2020 15:21:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-16236.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sparsetransformer,
  author    = {Rewon Child and
               Scott Gray and
               Alec Radford and
               Ilya Sutskever},
  title     = {Generating Long Sequences with Sparse Transformers},
  journal   = {CoRR},
  volume    = {abs/1904.10509},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.10509},
  eprinttype = {arXiv},
  eprint    = {1904.10509},
  timestamp = {Thu, 02 May 2019 15:13:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-10509.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{performer,
  author    = {Krzysztof Choromanski and
               Valerii Likhosherstov and
               David Dohan and
               Xingyou Song and
               Andreea Gane and
               Tam{\'{a}}s Sarl{\'{o}}s and
               Peter Hawkins and
               Jared Davis and
               Afroz Mohiuddin and
               Lukasz Kaiser and
               David Belanger and
               Lucy J. Colwell and
               Adrian Weller},
  title     = {Rethinking Attention with Performers},
  journal   = {CoRR},
  volume    = {abs/2009.14794},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.14794},
  eprinttype = {arXiv},
  eprint    = {2009.14794},
  timestamp = {Wed, 23 Jun 2021 10:58:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-14794.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{luna,
  author    = {Xuezhe Ma and
               Xiang Kong and
               Sinong Wang and
               Chunting Zhou and
               Jonathan May and
               Hao Ma and
               Luke Zettlemoyer},
  title     = {Luna: Linear Unified Nested Attention},
  journal   = {CoRR},
  volume    = {abs/2106.01540},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.01540},
  eprinttype = {arXiv},
  eprint    = {2106.01540},
  timestamp = {Thu, 10 Jun 2021 16:34:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-01540.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{transformer,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  eprinttype = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{clark2019,
  author    = {Kevin Clark and
               Urvashi Khandelwal and
               Omer Levy and
               Christopher D. Manning},
  title     = {What Does {BERT} Look At? An Analysis of BERT's Attention},
  journal   = {CoRR},
  volume    = {abs/1906.04341},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.04341},
  eprinttype = {arXiv},
  eprint    = {1906.04341},
  timestamp = {Fri, 14 Jun 2019 09:38:24 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-04341.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sparsebert,
  author    = {Han Shi and
               Jiahui Gao and
               Xiaozhe Ren and
               Hang Xu and
               Xiaodan Liang and
               Zhenguo Li and
               James T. Kwok},
  title     = {SparseBERT: Rethinking the Importance Analysis in Self-attention},
  journal   = {CoRR},
  volume    = {abs/2102.12871},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.12871},
  eprinttype = {arXiv},
  eprint    = {2102.12871},
  timestamp = {Tue, 02 Mar 2021 12:11:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-12871.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gpt-3,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
  title     = {Language Models are Few-Shot Learners},
  journal   = {CoRR},
  volume    = {abs/2005.14165},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.14165},
  eprinttype = {arXiv},
  eprint    = {2005.14165},
  timestamp = {Wed, 03 Jun 2020 11:36:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-14165.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lotteryticket,
  author    = {Jonathan Frankle and
               Michael Carbin},
  title     = {The Lottery Ticket Hypothesis: Training Pruned Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1803.03635},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.03635},
  eprinttype = {arXiv},
  eprint    = {1803.03635},
  timestamp = {Mon, 13 Aug 2018 16:48:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-03635.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cosformer,
title={cosFormer: Rethinking Softmax In Attention},
author={Zhen Qin and Weixuan Sun and Hui Deng and Dongxu Li and Yunshen Wei and Baohong Lv and Junjie Yan and Lingpeng Kong and Yiran Zhong},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=Bl8CQrx2Up4}
}

@article{narang2021,
  author    = {Sharan Narang and
               Hyung Won Chung and
               Yi Tay and
               William Fedus and
               Thibault F{\'{e}}vry and
               Michael Matena and
               Karishma Malkan and
               Noah Fiedel and
               Noam Shazeer and
               Zhenzhong Lan and
               Yanqi Zhou and
               Wei Li and
               Nan Ding and
               Jake Marcus and
               Adam Roberts and
               Colin Raffel},
  title     = {Do Transformer Modifications Transfer Across Implementations and Applications?},
  journal   = {CoRR},
  volume    = {abs/2102.11972},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.11972},
  eprinttype = {arXiv},
  eprint    = {2102.11972},
  timestamp = {Wed, 24 Nov 2021 07:55:15 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-11972.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mmsbm,
  title={Mixed membership stochastic blockmodels},
  author={Airoldi, Edo M and Blei, David and Fienberg, Stephen and Xing, Eric},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@article{osbm,
  title={Overlapping stochastic block models with application to the french political blogosphere},
  author={Latouche, Pierre and Birmel{\'e}, Etienne and Ambroise, Christophe},
  journal={The Annals of Applied Statistics},
  volume={5},
  number={1},
  pages={309--336},
  year={2011},
  publisher={Institute of Mathematical Statistics}
}

@article{abbe2017community,
  title={Community detection and stochastic block models: recent developments},
  author={Abbe, Emmanuel},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6446--6531},
  year={2017},
  publisher={JMLR. org}
}

@article{dcsbm,
  title={Stochastic blockmodels and community structure in networks},
  author={Karrer, Brian and Newman, Mark EJ},
  journal={Physical review E},
  volume={83},
  number={1},
  pages={016107},
  year={2011},
  publisher={APS}
}

@article{hsbm,
  title={Hierarchical block structures and high-resolution model selection in large networks},
  author={Peixoto, Tiago P},
  journal={Physical Review X},
  volume={4},
  number={1},
  pages={011047},
  year={2014},
  publisher={APS}
}

@article{synthesizer,
  author    = {Yi Tay and
               Dara Bahri and
               Donald Metzler and
               Da{-}Cheng Juan and
               Zhe Zhao and
               Che Zheng},
  title     = {Synthesizer: Rethinking Self-Attention in Transformer Models},
  journal   = {CoRR},
  volume    = {abs/2005.00743},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.00743},
  eprinttype = {arXiv},
  eprint    = {2005.00743},
  timestamp = {Fri, 08 May 2020 15:04:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-00743.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{clusterformer,
  author    = {Shuohang Wang and
               Luowei Zhou and
               Zhe Gan and
               Yen{-}Chun Chen and
               Yuwei Fang and
               Siqi Sun and
               Yu Cheng and
               Jingjing Liu},
  title     = {Cluster-Former: Clustering-based Sparse Transformer for Long-Range
               Dependency Encoding},
  journal   = {CoRR},
  volume    = {abs/2009.06097},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.06097},
  eprinttype = {arXiv},
  eprint    = {2009.06097},
  timestamp = {Wed, 18 Nov 2020 16:11:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-06097.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{smyrf,
  author    = {Giannis Daras and
               Nikita Kitaev and
               Augustus Odena and
               Alexandros G. Dimakis},
  title     = {{SMYRF:} Efficient Attention using Asymmetric Clustering},
  journal   = {CoRR},
  volume    = {abs/2010.05315},
  year      = {2020},
  url       = {https://arxiv.org/abs/2010.05315},
  eprinttype = {arXiv},
  eprint    = {2010.05315},
  timestamp = {Tue, 20 Oct 2020 15:08:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2010-05315.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yun2020,
  author    = {Chulhee Yun and
               Yin{-}Wen Chang and
               Srinadh Bhojanapalli and
               Ankit Singh Rawat and
               Sashank J. Reddi and
               Sanjiv Kumar},
  title     = {O(n) Connections are Expressive Enough:
               Universal Approximability of Sparse Transformers},
  journal   = {CoRR},
  volume    = {abs/2006.04862},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.04862},
  eprinttype = {arXiv},
  eprint    = {2006.04862},
  timestamp = {Fri, 12 Jun 2020 14:02:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-04862.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sukhbaatar2019,
  author    = {Sainbayar Sukhbaatar and
               Edouard Grave and
               Piotr Bojanowski and
               Armand Joulin},
  title     = {Adaptive Attention Span in Transformers},
  journal   = {CoRR},
  volume    = {abs/1905.07799},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.07799},
  eprinttype = {arXiv},
  eprint    = {1905.07799},
  timestamp = {Tue, 28 May 2019 12:48:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-07799.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{glue,
  author    = {Alex Wang and
               Amanpreet Singh and
               Julian Michael and
               Felix Hill and
               Omer Levy and
               Samuel R. Bowman},
  title     = {{GLUE:} {A} Multi-Task Benchmark and Analysis Platform for Natural
               Language Understanding},
  journal   = {CoRR},
  volume    = {abs/1804.07461},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.07461},
  eprinttype = {arXiv},
  eprint    = {1804.07461},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-07461.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{liu2020,
  author    = {Junjie Liu and
               Zhe Xu and
               Runbin Shi and
               Ray C. C. Cheung and
               Hayden Kwok{-}Hay So},
  title     = {Dynamic Sparse Training: Find Efficient Sparse Network From Scratch
               With Trainable Masked Layers},
  journal   = {CoRR},
  volume    = {abs/2005.06870},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.06870},
  eprinttype = {arXiv},
  eprint    = {2005.06870},
  timestamp = {Fri, 22 May 2020 16:21:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-06870.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{martins2020,
  author    = {Andr{\'{e}} F. T. Martins and
               Marcos V. Treviso and
               Ant{\'{o}}nio Farinhas and
               Vlad Niculae and
               M{\'{a}}rio A. T. Figueiredo and
               Pedro M. Q. Aguiar},
  title     = {Sparse and Continuous Attention Mechanisms},
  journal   = {CoRR},
  volume    = {abs/2006.07214},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.07214},
  eprinttype = {arXiv},
  eprint    = {2006.07214},
  timestamp = {Wed, 17 Jun 2020 14:28:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-07214.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gat,
  title="{Graph Attention Networks}",
  author={Veli{\v{c}}kovi{\'{c}}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`{o}}, Pietro and Bengio, Yoshua},
  journal={International Conference on Learning Representations},
  year={2018},
  url={https://openreview.net/forum?id=rJXMpikCZ},
  note={accepted as poster},
}

@article{fan2021,
  author    = {Zhihao Fan and
               Yeyun Gong and
               Dayiheng Liu and
               Zhongyu Wei and
               Siyuan Wang and
               Jian Jiao and
               Nan Duan and
               Ruofei Zhang and
               Xuanjing Huang},
  title     = {Mask Attention Networks: Rethinking and Strengthen Transformer},
  journal   = {CoRR},
  volume    = {abs/2103.13597},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.13597},
  eprinttype = {arXiv},
  eprint    = {2103.13597},
  timestamp = {Wed, 07 Apr 2021 15:31:46 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-13597.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{routingtransformer,
  author    = {Aurko Roy and
               Mohammad Saffar and
               Ashish Vaswani and
               David Grangier},
  title     = {Efficient Content-Based Sparse Attention with Routing Transformers},
  journal   = {CoRR},
  volume    = {abs/2003.05997},
  year      = {2020},
  url       = {https://arxiv.org/abs/2003.05997},
  eprinttype = {arXiv},
  eprint    = {2003.05997},
  timestamp = {Tue, 17 Mar 2020 14:18:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2003-05997.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{ViT,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{Transformer-NMT,
    title = "Scaling Neural Machine Translation",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6301",
    doi = "10.18653/v1/W18-6301",
    pages = "1--9",
    abstract = "Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT{'}14 English-German translation, we match the accuracy of Vaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT{'}14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",
}

@InProceedings{MSATranformer,
  title = 	 {MSA Transformer},
  author =       {Rao, Roshan M and Liu, Jason and Verkuil, Robert and Meier, Joshua and Canny, John and Abbeel, Pieter and Sercu, Tom and Rives, Alexander},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8844--8856},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/rao21a/rao21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/rao21a.html},
  abstract = 	 {Unsupervised protein language models trained across millions of diverse sequences learn structure and function of proteins. Protein language models studied to date have been trained to perform inference from individual sequences. The longstanding approach in computational biology has been to make inferences from a family of evolutionarily related sequences by fitting a model to each family independently. In this work we combine the two paradigms. We introduce a protein language model which takes as input a set of sequences in the form of a multiple sequence alignment. The model interleaves row and column attention across the input sequences and is trained with a variant of the masked language modeling objective across many protein families. The performance of the model surpasses current state-of-the-art unsupervised structure learning methods by a wide margin, with far greater parameter efficiency than prior state-of-the-art protein language models.}
}

@article{LSTM,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{huang2021efficient,
  title={Efficient Attentions for Long Document Summarization},
  author={Huang, Luyang and Cao, Shuyang and Parulian, Nikolaus and Ji, Heng and Wang, Lu},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1419--1436},
  year={2021}
}

@inproceedings{zhang2021multi,
  title={Multi-scale vision longformer: A new vision transformer for high-resolution image encoding},
  author={Zhang, Pengchuan and Dai, Xiyang and Yang, Jianwei and Xiao, Bin and Yuan, Lu and Zhang, Lei and Gao, Jianfeng},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2998--3008},
  year={2021}
}

@article{wang2019dgl,
    title={Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks},
    author={Minjie Wang and Da Zheng and Zihao Ye and Quan Gan and Mufei Li and Xiang Song and Jinjing Zhou and Chao Ma and Lingfan Yu and Yu Gai and Tianjun Xiao and Tong He and George Karypis and Jinyang Li and Zheng Zhang},
    year={2019},
    journal={arXiv preprint arXiv:1909.01315}
}

@article{funke2019sbm,
    doi = {10.1371/journal.pone.0215296},
    author = {Funke, Thorben AND Becker, Till},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Stochastic block models: A comparison of variants and inference methods},
    year = {2019},
    month = {04},
    volume = {14},
    url = {https://doi.org/10.1371/journal.pone.0215296},
    pages = {1-40},
    abstract = {Finding communities in complex networks is a challenging task and one promising approach is the Stochastic Block Model (SBM). But the influences from various fields led to a diversity of variants and inference methods. Therefore, a comparison of the existing techniques and an independent analysis of their capabilities and weaknesses is needed. As a first step, we review the development of different SBM variants such as the degree-corrected SBM of Karrer and Newman or Peixoto’s hierarchical SBM. Beside stating all these variants in a uniform notation, we show the reasons for their development. Knowing the variants, we discuss a variety of approaches to infer the optimal partition like the Metropolis-Hastings algorithm. We perform our analysis based on our extension of the Girvan-Newman test and the Lancichinetti-Fortunato-Radicchi benchmark as well as a selection of some real world networks. Using these results, we give some guidance to the challenging task of selecting an inference method and SBM variant. In addition, we give a simple heuristic to determine the number of steps for the Metropolis-Hastings algorithms that lack a usual stop criterion. With our comparison, we hope to guide researches in the field of SBM and highlight the problem of existing techniques to focus future research. Finally, by making our code freely available, we want to promote a faster development, integration and exchange of new ideas.},
    number = {4},

}

@article{listops,
  author    = {Nikita Nangia and
               Samuel R. Bowman},
  title     = {ListOps: {A} Diagnostic Dataset for Latent Tree Learning},
  journal   = {CoRR},
  volume    = {abs/1804.06028},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.06028},
  eprinttype = {arXiv},
  eprint    = {1804.06028},
  timestamp = {Mon, 13 Aug 2018 16:48:56 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-06028.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{text,
    title = "Learning Word Vectors for Sentiment Analysis",
    author = "Maas, Andrew L.  and
      Daly, Raymond E.  and
      Pham, Peter T.  and
      Huang, Dan  and
      Ng, Andrew Y.  and
      Potts, Christopher",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P11-1015",
    pages = "142--150",
}

@inproceedings{retrieval,
    title = "The {ACL} {A}nthology Network",
    author = "Radev, Dragomir R.  and
      Muthukrishnan, Pradeep  and
      Qazvinian, Vahed",
    booktitle = "Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries ({NLPIR}4{DL})",
    month = aug,
    year = "2009",
    address = "Suntec City, Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W09-3607",
    pages = "54--61",
}

@inproceedings{image,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009}
}

@article{pathfinder,
  author    = {Drew Linsley and
               Junkyung Kim and
               Vijay Veerabadran and
               Thomas Serre},
  title     = {Learning long-range spatial dependencies with horizontal gated-recurrent
               units},
  journal   = {CoRR},
  volume    = {abs/1805.08315},
  year      = {2018},
  url       = {http://arxiv.org/abs/1805.08315},
  eprinttype = {arXiv},
  eprint    = {1805.08315},
  timestamp = {Mon, 13 Aug 2018 16:48:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1805-08315.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bhojanapalli2020,
  author    = {Srinadh Bhojanapalli and
               Chulhee Yun and
               Ankit Singh Rawat and
               Sashank J. Reddi and
               Sanjiv Kumar},
  title     = {Low-Rank Bottleneck in Multi-head Attention Models},
  journal   = {CoRR},
  volume    = {abs/2002.07028},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.07028},
  eprinttype = {arXiv},
  eprint    = {2002.07028},
  timestamp = {Mon, 02 Mar 2020 16:46:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-07028.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{berger-tal2014,
    doi = {10.1371/journal.pone.0095693},
    author = {Berger-Tal, Oded AND Nathan, Jonathan AND Meron, Ehud AND Saltz, David},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {The Exploration-Exploitation Dilemma: A Multidisciplinary Framework},
    year = {2014},
    month = {04},
    volume = {9},
    url = {https://doi.org/10.1371/journal.pone.0095693},
    pages = {1-8},
    abstract = {The trade-off between the need to obtain new knowledge and the need to use that knowledge to improve performance is one of the most basic trade-offs in nature, and optimal performance usually requires some balance between exploratory and exploitative behaviors. Researchers in many disciplines have been searching for the optimal solution to this dilemma. Here we present a novel model in which the exploration strategy itself is dynamic and varies with time in order to optimize a definite goal, such as the acquisition of energy, money, or prestige. Our model produced four very distinct phases: Knowledge establishment, Knowledge accumulation, Knowledge maintenance, and Knowledge exploitation, giving rise to a multidisciplinary framework that applies equally to humans, animals, and organizations. The framework can be used to explain a multitude of phenomena in various disciplines, such as the movement of animals in novel landscapes, the most efficient resource allocation for a start-up company, or the effects of old age on knowledge acquisition in humans.},
    number = {4},
}

@article{longformer,
  author    = {Iz Beltagy and
               Matthew E. Peters and
               Arman Cohan},
  title     = {Longformer: The Long-Document Transformer},
  journal   = {CoRR},
  volume    = {abs/2004.05150},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.05150},
  eprinttype = {arXiv},
  eprint    = {2004.05150},
  timestamp = {Tue, 14 Apr 2020 16:40:34 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2004-05150.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yun2019,
  author    = {Chulhee Yun and
               Srinadh Bhojanapalli and
               Ankit Singh Rawat and
               Sashank J. Reddi and
               Sanjiv Kumar},
  title     = {Are Transformers universal approximators of sequence-to-sequence functions?},
  journal   = {CoRR},
  volume    = {abs/1912.10077},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.10077},
  eprinttype = {arXiv},
  eprint    = {1912.10077},
  timestamp = {Fri, 03 Jan 2020 16:10:45 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-10077.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{bollobas1998random,
  title={Random graphs},
  author={Bollob{\'a}s, B{\'e}la},
  booktitle={Modern graph theory},
  pages={215--252},
  year={1998},
  publisher={Springer}
}

@article{kaiming2015delving,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on
               ImageNet Classification},
  journal   = {CoRR},
  volume    = {abs/1502.01852},
  year      = {2015},
  url       = {http://arxiv.org/abs/1502.01852},
  eprinttype = {arXiv},
  eprint    = {1502.01852},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeZR015.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{walker1977,
author = {Walker, Alastair J.},
title = {An Efficient Method for Generating Discrete Random Variables with General Distributions},
year = {1977},
issue_date = {Sept. 1977},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {0098-3500},
url = {https://doi.org/10.1145/355744.355749},
doi = {10.1145/355744.355749},
journal = {ACM Trans. Math. Softw.},
month = {sep},
pages = {253–256},
numpages = {4}
}

@inproceedings{sst-2,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@inproceedings{qnli,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@inproceedings{qqp,
  title={Quora Question Pairs},
  author={Zihang Chen and Hongbo Zhang and Xiaoji Zhang and Leqi Zhao},
  year={2017}
}

@inproceedings{mnli,
    title = "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
    author = "Williams, Adina  and
      Nangia, Nikita  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1101",
    doi = "10.18653/v1/N18-1101",
    pages = "1112--1122",
    abstract = "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{bookcorpus,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@inproceedings{realnews,
 author = {Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Defending Against Neural Fake News},
 url = {https://proceedings.neurips.cc/paper/2019/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{sparsert,
  title={Sparsert: Accelerating unstructured sparsity on gpus for deep learning inference},
  author={Wang, Ziheng},
  journal={arXiv preprint arXiv:2008.11849},
  year={2020}
}

@INPROCEEDINGS{latency-aware,  
  author={Zhu, Maohua and Xie, Yuan},
  booktitle={2020 57th ACM/IEEE Design Automation Conference (DAC)},
  title={Taming Unstructured Sparsity on GPUs via Latency-Aware Optimization},
  year={2020},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/DAC18072.2020.9218644}}