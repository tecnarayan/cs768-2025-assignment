@article{Dai1999,
	title        = {Effects of Clouds,  Soil Moisture,  Precipitation,  and Water Vapor on Diurnal Temperature Range},
	author       = {Aiguo Dai and Kevin E. Trenberth and Thomas R. Karl},
	year         = 1999,
	month        = aug,
	journal      = {Journal of Climate},
	publisher    = {American Meteorological Society},
	volume       = 12,
	number       = 8,
	pages        = {2451--2473},
	doi          = {10.1175/1520-0442(1999)012<2451:eocsmp>2.0.co;2},
	url          = {https://doi.org/10.1175/1520-0442(1999)012<2451:eocsmp>2.0.co;2}
}
@article{Cahalan2010,
	title        = {Temperature responses to spectral solar variability on decadal time scales},
	author       = {Robert F. Cahalan and Guoyong Wen and Jerald W. Harder and Peter Pilewskie},
	year         = 2010,
	month        = apr,
	journal      = {Geophysical Research Letters},
	publisher    = {American Geophysical Union ({AGU})},
	volume       = 37,
	number       = 7,
	pages        = {n/a--n/a},
	doi          = {10.1029/2009gl041898},
	url          = {https://doi.org/10.1029/2009gl041898}
}
@misc{mila_2020,
	title        = {Le ministère de l'Environnement et de la lutte contre les changements climatiques octroie 840 000\$ à Mila, IVADO et deux universités québécoises pour soutenir un projet d'envergure au Maroc},
	year         = 2020,
	month        = {Nov},
	journal      = {Mila},
	url          = {https://mila.quebec/le-ministere-de-lenvironnement-et-de-la-lutte-contre-les-changements-climatiques-octroie-840-000-a-mila-ivado-et-deux-universites-quebecoises-pour-soutenir-un-projet-denvergure-au-maroc/}
}
@book{10.5555/551283,
	title        = {Introduction to Reinforcement Learning},
	author       = {Sutton, Richard S. and Barto, Andrew G.},
	year         = 1998,
	publisher    = {MIT Press},
	address      = {Cambridge, MA, USA},
	isbn         = {0262193981},
	edition      = {1st},
	abstract     = {From the Publisher:In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.}
}
@misc{2007.07978,
	title        = {CloudCast: A Satellite-Based Dataset and Baseline for Forecasting Clouds},
	author       = {A. H. Nielsen and A. Iosifidis and H. Karstoft},
	year         = 2020,
	eprint       = {arXiv:2007.07978}
}
@misc{2005.11423,
	title        = {Multi-view polarimetric scattering cloud tomography and retrieval of droplet size},
	author       = {Aviad Levis and Yoav Y. Schechner and Anthony B. Davis and Jesse Loveridge},
	year         = 2020,
	eprint       = {arXiv:2005.11423}
}
@article{Moin1998,
	title        = {{DIRECT} {NUMERICAL} {SIMULATION}: A Tool in Turbulence Research},
	author       = {Parviz Moin and Krishnan Mahesh},
	year         = 1998,
	month        = jan,
	journal      = {Annual Review of Fluid Mechanics},
	publisher    = {Annual Reviews},
	volume       = 30,
	number       = 1,
	pages        = {539--578},
	doi          = {10.1146/annurev.fluid.30.1.539},
	url          = {https://doi.org/10.1146/annurev.fluid.30.1.539}
}
@inproceedings{8100115,
	title        = {Image-to-Image Translation with Conditional Adversarial Networks},
	author       = {P. {Isola} and J. {Zhu} and T. {Zhou} and A. A. {Efros}},
	year         = 2017,
	booktitle    = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	volume       = {},
	number       = {},
	pages        = {5967--5976},
	doi          = {10.1109/CVPR.2017.632}
}
@inproceedings{8579015,
	title        = {High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs},
	author       = {T. {Wang} and M. {Liu} and J. {Zhu} and A. {Tao} and J. {Kautz} and B. {Catanzaro}},
	year         = 2018,
	booktitle    = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	volume       = {},
	number       = {},
	pages        = {8798--8807},
	doi          = {10.1109/CVPR.2018.00917}
}
@incollection{LEONARD1975237,
	title        = {Energy Cascade in Large-Eddy Simulations of Turbulent Fluid Flows},
	author       = {A. Leonard},
	year         = 1975,
	booktitle    = {Turbulent Diffusion in Environmental Pollution},
	publisher    = {Elsevier},
	series       = {Advances in Geophysics},
	volume       = 18,
	pages        = {237--248},
	doi          = {https://doi.org/10.1016/S0065-2687(08)60464-1},
	issn         = {0065-2687},
	url          = {https://www.sciencedirect.com/science/article/pii/S0065268708604641},
	editor       = {F.N. Frenkiel and R.E. Munn},
	abstract     = {Publisher Summary Numerical simulation of all the scales of a turbulent flow, even at modest Reynolds numbers, is generally not practical; however, most information of interest can be obtained by simulating the motion of the large-scale, energy containing eddies. This chapter describes the derivation of smoothed or filtered momentum, and the continuity equations for large-scale energy containing eddies. The large-scale fluctuations satisfy the filtered or averaged momentum and continuity equations. Averaging the nonlinear advection term yields two terms; one is the Reynolds stress contribution from the subgrid-scale turbulence, and the other is the filtered advection term for the large scales. In some models, the energy cascade is viewed solely as an energy loss of the large-scales because of an artificial viscosity arising from subgrid-scale motions. However, in most cases of interest, motions on the order of the dissipation length scale cannot be treated explicitly, and modifications of the Navier-Stokes equations must be introduced to simulate properly the energy cascade. Noting that the large-scale motions vary in a nonnegligible way over an averaging volume, the chapter investigates a more accurate, modified advective term in the momentum equations for these motions.}
}
@misc{2005.09023,
	title        = {Automating Turbulence Modeling by Multi-Agent Reinforcement Learning},
	author       = {Guido Novati and Hugues Lascombes de Laroussilhe and Petros Koumoutsakos},
	year         = 2020,
	eprint       = {arXiv:2005.09023}
}
@article{2010.08895,
	title        = {Fourier Neural Operator for Parametric Partial Differential Equations},
	author       = {Zong-Yi Li and Nikola B. Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
	year         = 2021,
	journal      = {ArXiv},
	volume       = {abs/2010.08895}
}
@article{Novati2021,
	title        = {Automating turbulence modelling by multi-agent reinforcement learning},
	author       = {Guido Novati and Hugues Lascombes de Laroussilhe and Petros Koumoutsakos},
	year         = 2021,
	month        = jan,
	journal      = {Nature Machine Intelligence},
	publisher    = {Springer Science and Business Media {LLC}},
	volume       = 3,
	number       = 1,
	pages        = {87--96},
	doi          = {10.1038/s42256-020-00272-0},
	url          = {https://doi.org/10.1038/s42256-020-00272-0}
}
@article{Maulik2018,
	title        = {Data-driven deconvolution for large eddy simulations of Kraichnan turbulence},
	author       = {R. Maulik and O. San and A. Rasheed and P. Vedula},
	year         = 2018,
	month        = dec,
	journal      = {Physics of Fluids},
	publisher    = {{AIP} Publishing},
	volume       = 30,
	number       = 12,
	pages        = 125109,
	doi          = {10.1063/1.5079582},
	url          = {https://doi.org/10.1063/1.5079582}
}
@article{1806.04731,
	title        = {Deep learning to represent sub-grid processes in climate models},
	author       = {Stephan Rasp and Michael S. Pritchard and Pierre Gentine},
	year         = 2018,
	eprint       = {arXiv:1806.04731},
	howpublished = {Proceedings of the National Academy of Sciences Sep 2018, 201810286; DOI: 10.1073/pnas.1810286115}
}
@article{Brenowitz2018,
	title        = {Prognostic Validation of a Neural Network Unified Physics Parameterization},
	author       = {N. D. Brenowitz and C. S. Bretherton},
	year         = 2018,
	month        = jun,
	journal      = {Geophysical Research Letters},
	publisher    = {American Geophysical Union ({AGU})},
	volume       = 45,
	number       = 12,
	pages        = {6289--6298},
	doi          = {10.1029/2018gl078510},
	url          = {https://doi.org/10.1029/2018gl078510}
}
@inbook{10.5555/3433701.3433712,
	title        = {MeshfreeFlowNet: A Physics-Constrained Deep Continuous Space-Time Super-Resolution Framework},
	author       = {Jiang, Chiyu "Max" and Esmaeilzadeh, Soheil and Azizzadenesheli, Kamyar and Kashinath, Karthik and Mustafa, Mustafa and Tchelepi, Hamdi A. and Marcus, Philip and Prabhat and Anandkumar, Anima},
	year         = 2020,
	booktitle    = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
	publisher    = {IEEE Press},
	isbn         = 9781728199986,
	abstract     = {We propose MeshfreeFlowNet, a novel deep learning-based super-resolution framework to generate continuous (grid-free) spatio-temporal solutions from the low-resolution inputs. While being computationally efficient, MeshfreeFlowNet accurately recovers the fine-scale quantities of interest. MeshfreeFlowNet allows for: (i) the output to be sampled at all spatio-temporal resolutions, (ii) a set of Partial Differential Equation (PDE) constraints to be imposed, and (iii) training on fixed-size inputs on arbitrarily sized spatio-temporal domains owing to its fully convolutional encoder.We empirically study the performance of MeshfreeFlowNet on the task of super-resolution of turbulent flows in the Rayleigh-B\'{e}nard convection problem. Across a diverse set of evaluation metrics, we show that MeshfreeFlowNet significantly outperforms existing baselines. Furthermore, we provide a large scale implementation of MeshfreeFlowNet and show that it efficiently scales across large clusters, achieving 96.80\% scaling efficiency on up to 128 GPUs and a training time of less than 4 minutes. We provide an open-source implementation of our method that supports arbitrary combinations of PDE constraints 1.},
	articleno    = 9,
	numpages     = 15
}
@misc{1711.10561,
	title        = {Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},
	author       = {Maziar Raissi and Paris Perdikaris and George Em Karniadakis},
	year         = 2017,
	eprint       = {arXiv:1711.10561}
}
@misc{1711.10566,
	title        = {Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations},
	author       = {Maziar Raissi and Paris Perdikaris and George Em Karniadakis},
	year         = 2017,
	eprint       = {arXiv:1711.10566}
}
@article{SMAGORINSKY1963,
	title        = {General Circulation Experiments with the Primitive Equations},
	author       = {J. Smagorinsky},
	year         = 1963,
	month        = mar,
	journal      = {Monthly Weather Review},
	publisher    = {American Meteorological Society},
	volume       = 91,
	number       = 3,
	pages        = {99--164},
	doi          = {10.1175/1520-0493(1963)091<0099:gcewtp>2.3.co;2},
	url          = {https://doi.org/10.1175/1520-0493(1963)091<0099:gcewtp>2.3.co;2}
}
@article{Park2021,
	title        = {Toward neural-network-based large eddy simulation: application to turbulent channel flow},
	author       = {Jonghwan Park and Haecheon Choi},
	year         = 2021,
	month        = mar,
	journal      = {Journal of Fluid Mechanics},
	publisher    = {Cambridge University Press ({CUP})},
	volume       = 914,
	doi          = {10.1017/jfm.2020.931},
	url          = {https://doi.org/10.1017/jfm.2020.931}
}
@inproceedings{Wang2020,
	title        = {Towards Physics-informed Deep Learning for Turbulent Flow Prediction},
	author       = {Rui Wang and Karthik Kashinath and Mustafa Mustafa and Adrian Albert and Rose Yu},
	year         = 2020,
	month        = jul,
	booktitle    = {Proceedings of the 26th {ACM} {SIGKDD} International Conference on Knowledge Discovery {\&} Data Mining},
	publisher    = {{ACM}},
	doi          = {10.1145/3394486.3403198},
	url          = {https://doi.org/10.1145/3394486.3403198}
}
@misc{1812.02822,
	title        = {Learning Implicit Fields for Generative Shape Modeling},
	author       = {Zhiqin Chen and Hao Zhang},
	year         = 2018,
	eprint       = {arXiv:1812.02822}
}
@misc{2006.09661,
	title        = {Implicit Neural Representations with Periodic Activation Functions},
	author       = {Vincent Sitzmann and Julien N. P. Martel and Alexander W. Bergman and David B. Lindell and Gordon Wetzstein},
	year         = 2020,
	eprint       = {arXiv:2006.09661}
}
@misc{2012.09161,
	title        = {Learning Continuous Image Representation with Local Implicit Image Function},
	author       = {Yinbo Chen and Sifei Liu and Xiaolong Wang},
	year         = 2020,
	eprint       = {arXiv:2012.09161}
}
@misc{2006.10739,
	title        = {Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},
	author       = {Matthew Tancik and Pratul P. Srinivasan and Ben Mildenhall and Sara Fridovich-Keil and Nithin Raghavan and Utkarsh Singhal and Ravi Ramamoorthi and Jonathan T. Barron and Ren Ng},
	year         = 2020,
	eprint       = {arXiv:2006.10739}
}
@article{iten2018iscovering,
	title        = {Discovering physical concepts with neural networks},
	author       = {Raban Iten and Tony Metger and Henrik Wilming and Lidia del Rio and Renato Renner},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1807.10300}
}
@misc{2006.05044,
	title        = {Neural Physicist: Learning Physical Dynamics from Image Sequences},
	author       = {Baocheng Zhu and Shijun Wang and James Zhang},
	year         = 2020,
	eprint       = {arXiv:2006.05044}
}
@misc{1312.6114,
	title        = {Auto-Encoding Variational Bayes},
	author       = {Diederik P Kingma and Max Welling},
	year         = 2013,
	eprint       = {arXiv:1312.6114}
}
@misc{2002.09405,
	title        = {Learning to Simulate Complex Physics with Graph Networks},
	author       = {Alvaro Sanchez-Gonzalez and Jonathan Godwin and Tobias Pfaff and Rex Ying and Jure Leskovec and Peter W. Battaglia},
	year         = 2020,
	eprint       = {arXiv:2002.09405}
}
@article{2010.03409,
	title        = {Learning Mesh-Based Simulation with Graph Networks},
	author       = {Tobias Pfaff and Meire Fortunato and Alvaro Sanchez-Gonzalez and Peter W. Battaglia},
	year         = 2020,
	eprint       = {arXiv:2010.03409},
	howpublished = {International Conference on Learning Representations (ICLR), 2021}
}
@article{PhysRevResearch.2.023068,
	title        = {Dedalus: A flexible framework for numerical simulations with spectral methods},
	author       = {Burns, Keaton J. and Vasil, Geoffrey M. and Oishi, Jeffrey S. and Lecoanet, Daniel and Brown, Benjamin P.},
	year         = 2020,
	month        = {Apr},
	journal      = {Phys. Rev. Research},
	publisher    = {American Physical Society},
	volume       = 2,
	pages        = {023068},
	doi          = {10.1103/PhysRevResearch.2.023068},
	url          = {https://link.aps.org/doi/10.1103/PhysRevResearch.2.023068},
	issue        = 2,
	numpages     = 39
}
@inproceedings{10.5555/2969239.2969329,
	title        = {Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting},
	author       = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
	year         = 2015,
	booktitle    = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1},
	location     = {Montreal, Canada},
	publisher    = {MIT Press},
	address      = {Cambridge, MA, USA},
	series       = {NIPS'15},
	pages        = {802–810},
	abstract     = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
	numpages     = 9
}
@misc{1707.02921,
	title        = {Enhanced Deep Residual Networks for Single Image Super-Resolution},
	author       = {Bee Lim and Sanghyun Son and Heewon Kim and Seungjun Nah and Kyoung Mu Lee},
	year         = 2017,
	eprint       = {arXiv:1707.02921}
}
@incollection{NEURIPS2019_9015,
	title        = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	author       = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems 32},
	publisher    = {Curran Associates, Inc.},
	pages        = {8024--8035},
	url          = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	editor       = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@misc{2006.04418,
	title        = {Learning Long-Term Dependencies in Irregularly-Sampled Time Series},
	author       = {Mathias Lechner and Ramin Hasani},
	year         = 2020,
	eprint       = {arXiv:2006.04418}
}
@article{rubanova2019atent,
	title        = {Latent ODEs for Irregularly-Sampled Time Series},
	author       = {Yulia Rubanova and Ricky T. Q. Chen and David Duvenaud},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1907.03907}
}
@misc{2003.08934,
	title        = {NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis},
	author       = {Ben Mildenhall and Pratul P. Srinivasan and Matthew Tancik and Jonathan T. Barron and Ravi Ramamoorthi and Ren Ng},
	year         = 2020,
	eprint       = {arXiv:2003.08934}
}
@misc{2003.04560,
	title        = {Frequency Bias in Neural Networks for Input of Non-Uniform Density},
	author       = {Ronen Basri and Meirav Galun and Amnon Geifman and David Jacobs and Yoni Kasten and Shira Kritchman},
	year         = 2020,
	eprint       = {arXiv:2003.04560}
}
@article{spec_bias,
	title        = {On the Spectral Bias of Neural Networks},
	author       = {Nasim Rahaman and Aristide Baratin and Devansh Arpit and Felix Draxler and Min Lin and Fred A. Hamprecht and Yoshua Bengio and Aaron Courville},
	year         = 2018,
	eprint       = {arXiv:1806.08734},
	howpublished = {ICML 2019}
}
@inproceedings{10.5555/2981562.2981710,
	title        = {Random Features for Large-Scale Kernel Machines},
	author       = {Rahimi, Ali and Recht, Benjamin},
	year         = 2007,
	booktitle    = {Proceedings of the 20th International Conference on Neural Information Processing Systems},
	location     = {Vancouver, British Columbia, Canada},
	publisher    = {Curran Associates Inc.},
	address      = {Red Hook, NY, USA},
	series       = {NIPS'07},
	pages        = {1177–1184},
	isbn         = 9781605603520,
	abstract     = {To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines.},
	numpages     = 8
}
@misc{1612.00222,
	title        = {Interaction Networks for Learning about Objects, Relations and Physics},
	author       = {Peter W. Battaglia and Razvan Pascanu and Matthew Lai and Danilo Rezende and Koray Kavukcuoglu},
	year         = 2016,
	eprint       = {arXiv:1612.00222}
}
@article{Brunton3932,
	title        = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	author       = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
	year         = 2016,
	journal      = {Proceedings of the National Academy of Sciences},
	publisher    = {National Academy of Sciences},
	volume       = 113,
	number       = 15,
	pages        = {3932--3937},
	doi          = {10.1073/pnas.1517384113},
	issn         = {0027-8424},
	url          = {https://www.pnas.org/content/113/15/3932},
	abstract     = {Understanding dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled technology, including aircraft, combustion engines, satellites, and electrical power. This work develops a novel framework to discover governing equations underlying a dynamical system simply from data measurements, leveraging advances in sparsity techniques and machine learning. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. There are many critical data-driven problems, such as understanding cognition from neural recordings, inferring climate patterns, determining stability of financial markets, predicting and suppressing the spread of disease, and controlling turbulence for greener transportation and energy. With abundant data and elusive laws, data-driven discovery of dynamics will continue to play an important role in these efforts.Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
	eprint       = {https://www.pnas.org/content/113/15/3932.full.pdf}
}
@article{udrescu2019i,
	title        = {AI Feynman: a Physics-Inspired Method for Symbolic Regression},
	author       = {Silviu-Marian Udrescu and Max Tegmark},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1905.11481}
}
@article{1895,
	title        = {{IV}. On the dynamical theory of incompressible viscous fluids and the determination of the criterion},
	year         = 1895,
	month        = dec,
	journal      = {Philosophical Transactions of the Royal Society of London. (A.)},
	publisher    = {The Royal Society},
	volume       = 186,
	pages        = {123--164},
	doi          = {10.1098/rsta.1895.0004},
	url          = {https://doi.org/10.1098/rsta.1895.0004}
}
@book{Evans2010,
	title        = {Partial Differential Equations},
	author       = {Lawrence Evans},
	year         = 2010,
	month        = mar,
	publisher    = {American Mathematical Society},
	doi          = {10.1090/gsm/019},
	url          = {https://doi.org/10.1090/gsm/019}
}
@inproceedings{OlsenKettle2011NumericalSO,
	title        = {Numerical solution of partial differential equations},
	author       = {L. Olsen-Kettle},
	year         = 2011
}
@misc{2109.05237,
	title        = {Physics-based Deep Learning},
	author       = {Nils Thuerey and Philipp Holl and Maximilian Mueller and Patrick Schnell and Felix Trost and Kiwon Um},
	year         = 2021,
	eprint       = {arXiv:2109.05237}
}
@article{DBLP:journals/corr/abs-2003-04919,
	title        = {Integrating Physics-Based Modeling with Machine Learning: A Survey},
	author       = {Jared Willard and Xiaowei Jia and Shaoming Xu and Michael S. Steinbach and Vipin Kumar},
	year         = 2020,
	journal      = {CoRR},
	volume       = {abs/2003.04919},
	url          = {https://arxiv.org/abs/2003.04919},
	publtype     = {informal},
	cdate        = 1577836800000
}
@article{Ronneberger2015,
	title        = {{U-Net}: Convolutional Networks for Biomedical Image Segmentation},
	author       = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	year         = 2015,
	month        = May,
	journal      = {Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015},
	added-at     = {2020-10-16T09:20:07.000+0200},
	biburl       = {https://www.bibsonomy.org/bibtex/23ba9ff7f6fa1f30fde6b2ebd9b005fee/annakrause},
	interhash    = {9158de16b2caff7458df054dc6fc2748},
	intrahash    = {3ba9ff7f6fa1f30fde6b2ebd9b005fee},
	keywords     = {SemanticSegmentation imported},
	timestamp    = {2020-10-16T09:20:35.000+0200}
}
@article{HochSchm97,
	title        = {Long Short-Term Memory},
	author       = {Sepp Hochreiter and Jürgen Schmidhuber},
	year         = 1997,
	journal      = {Neural Computation},
	volume       = 9,
	number       = 8,
	pages        = {1735--1780},
	optabstract  = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	optdoi       = {10.1162/neco.1997.9.8.1735},
	opteprint    = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
	opturl       = {http://dx.doi.org/10.1162/neco.1997.9.8.1735}
}
@article{Kovachki2021NeuralOL,
	title        = {Neural Operator: Learning Maps Between Function Spaces},
	author       = {Nikola B. Kovachki and Zongyi Li and Burigede Liu and K. Azizzadenesheli and K. Bhattacharya and Andrew Stuart and Animashree Anandkumar},
	year         = 2021,
	journal      = {ArXiv},
	volume       = {abs/2108.08481}
}
@inproceedings{DBLP:journals/corr/KingmaB14,
	title        = {Adam: {A} Method for Stochastic Optimization},
	author       = {Diederik P. Kingma and Jimmy Ba},
	year         = 2015,
	booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
	url          = {http://arxiv.org/abs/1412.6980},
	editor       = {Yoshua Bengio and Yann LeCun},
	timestamp    = {Thu, 25 Jul 2019 14:25:37 +0200},
	biburl       = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@misc{spectral,
	title        = {Spectral Methods},
	author       = {Philipp Schlatter},
	journal      = {Computational Fluid Dynamics SG2212}
}
@article{Ruelle1971,
	title        = {On the nature of turbulence},
	author       = {David Ruelle and Floris Takens},
	year         = 1971,
	month        = sep,
	journal      = {Communications in Mathematical Physics},
	publisher    = {Springer Science and Business Media {LLC}},
	volume       = 20,
	number       = 3,
	pages        = {167--192},
	doi          = {10.1007/bf01646553},
	url          = {https://doi.org/10.1007/bf01646553}
}
@article{ZHIYIN201511,
	title        = {Large-eddy simulation: Past, present and the future},
	author       = {Yang Zhiyin},
	year         = 2015,
	journal      = {Chinese Journal of Aeronautics},
	volume       = 28,
	number       = 1,
	pages        = {11--24},
	doi          = {https://doi.org/10.1016/j.cja.2014.12.007},
	issn         = {1000-9361},
	url          = {https://www.sciencedirect.com/science/article/pii/S1000936114002064},
	keywords     = {Gas turbine combustor, Inflow boundary condition generation methods, Large-eddy simulation (LES), Sub-grid scale (SGS) model, Turbulent flows},
	abstract     = {Large-eddy simulation (LES) was originally proposed for simulating atmospheric flows in the 1960s and has become one of the most promising and successful methodology for simulating turbulent flows with the improvement of computing power. It is now feasible to simulate complex engineering flows using LES. However, apart from the computing power, significant challenges still remain for LES to reach a level of maturity that brings this approach to the mainstream of engineering and industrial computations. This paper will describe briefly LES formalism first, present a quick glance at its history, review its current state focusing mainly on its applications in transitional flows and gas turbine combustor flows, discuss some major modelling and numerical challenges/issues that we are facing now and in the near future, and finish with the concluding remarks.}
}
@article{Hirt1969,
	title        = {Computer Studies of Time-Dependent Turbulent Flows},
	author       = {C. W. Hirt},
	year         = 1969,
	journal      = {Physics of Fluids},
	publisher    = {{AIP} Publishing},
	volume       = 12,
	number       = 12,
	pages        = {II--219},
	doi          = {10.1063/1.1692441},
	url          = {https://doi.org/10.1063/1.1692441}
}
@article{GENERALCIRCULATIONEXPERIMENTSWITHTHEPRIMITIVEEQUATIONS,
	title        = {GENERAL CIRCULATION EXPERIMENTS WITH THE PRIMITIVE EQUATIONS: I. THE BASIC EXPERIMENT},
	author       = {J.  SMAGORINSKY},
	year         = 1963,
	journal      = {Monthly Weather Review},
	publisher    = {American Meteorological Society},
	address      = {Boston MA, USA},
	volume       = 91,
	number       = 3,
	pages        = {99--164},
	doi          = {10.1175/1520-0493(1963)091<0099:GCEWTP>2.3.CO;2},
	url          = {https://journals.ametsoc.org/view/journals/mwre/91/3/1520-0493_1963_091_0099_gcewtp_2_3_co_2.xml}
}
@inproceedings{McDonough2007IntroductoryLO,
	title        = {Introductory Lectures on Turbulence: Physics, Mathematics and Modeling},
	author       = {James M. McDonough},
	year         = 2007
}
@article{forsythe1960finite,
	title        = {Finite-difference methods for partial differential equations},
	author       = {Forsythe, George Elmer and Wasow, Wolfgang Richard and others},
	year         = 1960,
	publisher    = {Wiley}
}
@article{726791,
	title        = {Gradient-based learning applied to document recognition},
	author       = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	year         = 1998,
	journal      = {Proceedings of the IEEE},
	volume       = 86,
	number       = 11,
	pages        = {2278--2324},
	doi          = {10.1109/5.726791}
}
@article{benard1900tourbillons,
	title        = {Les tourbillons cellulaires dans une nappe liquide},
	author       = {B{\'e}nard, Henri},
	year         = 1900,
	journal      = {Rev. Gen. Sci. Pures Appl.},
	volume       = 11,
	pages        = {1261--1271}
}
@article{doi:10.1080/14786441608635602,
	title        = {LIX. On convection currents in a horizontal layer of fluid, when the higher temperature is on the under side},
	author       = {Lord   Rayleigh   O.M.   F.R.S.},
	year         = 1916,
	journal      = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	publisher    = {Taylor & Francis},
	volume       = 32,
	number       = 192,
	pages        = {529--546},
	doi          = {10.1080/14786441608635602},
	url          = {https://doi.org/10.1080/14786441608635602},
	eprint       = {https://doi.org/10.1080/14786441608635602}
}
@article{doi:10.1080/14786447108640585,
	title        = {XLVI. Hydrokinetic solutions and observations},
	author       = {Sir   William   Thomson   F.R.S.},
	year         = 1871,
	journal      = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
	publisher    = {Taylor & Francis},
	volume       = 42,
	number       = 281,
	pages        = {362--377},
	doi          = {10.1080/14786447108640585},
	url          = {https://doi.org/10.1080/14786447108640585},
	eprint       = {https://doi.org/10.1080/14786447108640585}
}
@article{helm,
	title        = {Über discontinuierliche Flüssigkeits-Bewegungen [On the discontinuous movements of fluids]},
	author       = {Hermann Ludwig Ferdinand von Helmholtz},
	year         = 1868,
	journal      = {Monatsberichte der Königlichen Preussiche Akademie der Wissenschaften zu Berlin [Monthly Reports of the Royal Prussian Academy of Philosophy in Berlin]},
	volume       = 23,
	pages        = 215
}
@inproceedings{10.1007/978-3-642-66784-8_8,
	title        = {Instabilities in Fluid Dynamics},
	author       = {Koschmieder, E. L.},
	year         = 1977,
	booktitle    = {Synergetics},
	publisher    = {Springer Berlin Heidelberg},
	address      = {Berlin, Heidelberg},
	pages        = {70--78},
	isbn         = {978-3-642-66784-8},
	editor       = {Haken, Hermann},
	abstract     = {The investigation of instabilities in fluid dynamics is concerned with the following question: What happens to infinitesimal disturbances of a specific fluid flow, do the disturbances either grow or do they decay? In the case that the disturbances grow we call the fluid unstable. If the disturbances decay we call the fluid stable. For the occurrence of instability it is not necessary that all possible disturbances grow, just one growing disturbance means instability. Neither is it necessary that all disturbances are infinitesimal. However, the study of infinitesimal disturbances permits the use of the linear approximation of the Navier-Stokes equation and then leads in many cases to unambiguous analytical predictions about the conditions under which a fluid flow is unstable for a certain kind of disturbance. There are numerous types of instabilities in fluid dynamics, ranging from the very familiar instability of a water jet to the famous and cumbersome problem of turbulence. We will restrict the discussion here to two classical instabilities, B{\'e}nard convection and Taylor vortex flow. These two instabilities relate most closely to Synergetics.}
}
@misc{khi,
	title        = {Kelvin-Helmholtz Instabilities},
	author       = {ARABI, Sami and DOUBLET, Solène and HAFFNER, Florent and GHEERBRANT, Lorraine and ROUSSEL, Sophie and TETARD, Flavien},
	year         = 2015,
	url          = {https://bibli.ec-lyon.fr/sites/default/files/pe_29_e2014.pdf}
}
@misc{rbi,
	title        = {HELIUM CRYOSTAT FOR EXPERIMENTAL STUDY OF NATURAL TURBULENT CONVECTION},
	url          = {http://www.isibrno.cz/cryogenics/convection.html}
}
@book{ipcc,
	title        = {IPCC, 2019: Climate Change and Land: an IPCC special report on climate change, desertification, land degradation, sustainable land management, food security, and greenhouse gas fluxes in terrestrial ecosystems},
	author       = {P.R. Shukla and J. Skea and E. Calvo Buendia and V. Masson-Delmotte and H.-O. Pörtner and D. C. Roberts and P. Zhai, R. Slade and S. Connors and R. van Diemen and M. Ferrat and E. Haughey and S. Luz, S. Neogi and M. Pathak and J. Petzold and J. Portugal Pereira and P. Vyas and E. Huntley and K. Kissick and M. Belkacemi and J. Malley},
	year         = 2014
}
@article{Dissanayake1994,
	title        = {Neural-network-based approximations for solving partial differential equations},
	author       = {M. W. M. G. Dissanayake and N. Phan-Thien},
	year         = 1994,
	month        = mar,
	journal      = {Communications in Numerical Methods in Engineering},
	publisher    = {Wiley},
	volume       = 10,
	number       = 3,
	pages        = {195--201},
	doi          = {10.1002/cnm.1640100303},
	url          = {https://doi.org/10.1002/cnm.1640100303}
}
@article{Lagaris1998ArtificialNN,
	title        = {Artificial neural networks for solving ordinary and partial differential equations},
	author       = {Isaac E. Lagaris and Aristidis Likas and Dimitrios I. Fotiadis},
	year         = 1998,
	journal      = {IEEE transactions on neural networks},
	volume       = {9 5},
	pages        = {987--1000}
}
@misc{2003.01460,
	title        = {Disentangling Physical Dynamics from Unknown Factors for Unsupervised Video Prediction},
	author       = {Vincent Le Guen and Nicolas Thome},
	year         = 2020,
	eprint       = {arXiv:2003.01460}
}
@misc{2006.08762,
	title        = {Learning Incompressible Fluid Dynamics from Scratch -- Towards Fast, Differentiable Fluid Models that Generalize},
	author       = {Nils Wandel and Michael Weinmann and Reinhard Klein},
	year         = 2020,
	eprint       = {arXiv:2006.08762}
}
@misc{1711.07970,
	title        = {Deep Learning for Physical Processes: Incorporating Prior Scientific Knowledge},
	author       = {Emmanuel de Bezenac and Arthur Pajot and Patrick Gallinari},
	year         = 2017,
	eprint       = {arXiv:1711.07970}
}
@misc{1511.05440,
	title        = {Deep multi-scale video prediction beyond mean square error},
	author       = {Michael Mathieu and Camille Couprie and Yann LeCun},
	year         = 2015,
	eprint       = {arXiv:1511.05440}
}
@article{2005.00698,
	title        = {Deep ConvLSTM with self-attention for human activity decoding using wearables},
	author       = {Satya P. Singh and Aimé Lay-Ekuakille and Deepak Gangwar and Madan Kumar Sharma and Sukrit Gupta},
	year         = 2020,
	doi          = {10.1109/JSEN.2020.3045135},
	eprint       = {arXiv:2005.00698}
}
@article{Lin2020,
	title        = {Self-Attention {ConvLSTM} for Spatiotemporal Prediction},
	author       = {Zhihui Lin and Maomao Li and Zhuobin Zheng and Yangyang Cheng and Chun Yuan},
	year         = 2020,
	month        = apr,
	journal      = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	publisher    = {Association for the Advancement of Artificial Intelligence ({AAAI})},
	volume       = 34,
	number       = {07},
	pages        = {11531--11538},
	doi          = {10.1609/aaai.v34i07.6819},
	url          = {https://doi.org/10.1609/aaai.v34i07.6819}
}
@misc{2103.09504,
	title        = {PredRNN: A Recurrent Neural Network for Spatiotemporal Predictive Learning},
	author       = {Yunbo Wang and Haixu Wu and Jianjin Zhang and Zhifeng Gao and Jianmin Wang and Philip S. Yu and Mingsheng Long},
	year         = 2021,
	eprint       = {arXiv:2103.09504}
}
@inproceedings{10.5555/3294771.3294855,
	title        = {PredRNN: Recurrent Neural Networks for Predictive Learning Using Spatiotemporal LSTMs},
	author       = {Wang, Yunbo and Long, Mingsheng and Wang, Jianmin and Gao, Zhifeng and Yu, Philip S.},
	year         = 2017,
	booktitle    = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	location     = {Long Beach, California, USA},
	publisher    = {Curran Associates Inc.},
	address      = {Red Hook, NY, USA},
	series       = {NIPS'17},
	pages        = {879–888},
	isbn         = 9781510860964,
	abstract     = {The predictive learning of spatiotemporal sequences aims to generate future images by learning from the historical frames, where spatial appearances and temporal variations are two crucial structures. This paper models these structures by presenting a predictive recurrent neural network (PredRNN). This architecture is enlightened by the idea that spatiotemporal predictive learning should memorize both spatial appearances and temporal variations in a unified memory pool. Concretely, memory states are no longer constrained inside each LSTM unit. Instead, they are allowed to zigzag in two directions: across stacked RNN layers vertically and through all RNN states horizontally. The core of this network is a new Spatiotemporal LSTM (ST-LSTM) unit that extracts and memorizes spatial and temporal representations simultaneously. PredRNN achieves the state-of-the-art prediction performance on three video prediction datasets and is a more general framework, that can be easily extended to other predictive learning tasks by integrating with other architectures.},
	numpages     = 10
}
@article{2004.05214,
	title        = {A Review on Deep Learning Techniques for Video Prediction},
	author       = {Sergiu Oprea and Pablo Martinez-Gonzalez and Alberto Garcia-Garcia and John Alejandro Castro-Vargas and Sergio Orts-Escolano and Jose Garcia-Rodriguez and Antonis Argyros},
	year         = 2020,
	doi          = {10.1109/TPAMI.2020.3045007},
	eprint       = {arXiv:2004.05214}
}
@book{otexts,
title = "Forecasting: Principles and Practice",
author = "Hyndman, {Robin John} and George Athanasopoulos",
year = "2018",
language = "English",
publisher = "OTexts",
address = "Australia",
edition = "2nd",
}

@inproceedings{DBLP:journals/corr/BahdanauCB14,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{mp_pde,
Author = {Johannes Brandstetter and Daniel Worrall and Max Welling},
Title = {Message Passing Neural PDE Solvers},
Year = {2022},
Eprint = {arXiv:2202.03376},
}
@misc{unet_pde,
Author = {Kimberly Stachenfeld and Drummond B. Fielding and Dmitrii Kochkov and Miles Cranmer and Tobias Pfaff and Jonathan Godwin and Can Cui and Shirley Ho and Peter Battaglia and Alvaro Sanchez-Gonzalez},
Title = {Learned Coarse Models for Efficient Turbulence Simulation},
Year = {2021},
Eprint = {arXiv:2112.15275},
}

@inproceedings{mp_chem,
author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
title = {Neural Message Passing for Quantum Chemistry},
year = {2017},
publisher = {JMLR.org},
abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1263–1272},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{int_net,
 author = {Watters, Nicholas and Zoran, Daniel and Weber, Theophane and Battaglia, Peter and Pascanu, Razvan and Tacchetti, Andrea},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Visual Interaction Networks: Learning a Physics Simulator from Video},
 url = {https://proceedings.neurips.cc/paper/2017/file/8cbd005a556ccd4211ce43f309bc0eac-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{athena,
  doi = {10.3847/1538-4365/ab929b},
  url = {https://doi.org/10.3847/1538-4365/ab929b},
  year = {2020},
  month = jun,
  publisher = {American Astronomical Society},
  volume = {249},
  number = {1},
  pages = {4},
  author = {James M. Stone and Kengo Tomida and Christopher J. White and Kyle G. Felker},
  title = {The Athena++ Adaptive Mesh Refinement Framework: Design and Magnetohydrodynamic Solvers},
  journal = {The Astrophysical Journal Supplement Series}
}

@misc{1806.01242,
Author = {Alvaro Sanchez-Gonzalez and Nicolas Heess and Jost Tobias Springenberg and Josh Merel and Martin Riedmiller and Raia Hadsell and Peter Battaglia},
Title = {Graph networks as learnable physics engines for inference and control},
Year = {2018},
Eprint = {arXiv:1806.01242},
}

@inproceedings{multipole,
 author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Stuart, Andrew and Bhattacharya, Kaushik and Anandkumar, Anima},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {6755--6766},
 publisher = {Curran Associates, Inc.},
 title = {Multipole Graph Neural Operator for Parametric Partial Differential Equations},
 url = {https://proceedings.neurips.cc/paper/2020/file/4b21cf96d4cf612f239a6c322b10c8fe-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{neurop,
Author = {Nikola Kovachki and Zongyi Li and Burigede Liu and Kamyar Azizzadenesheli and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
Title = {Neural Operator: Learning Maps Between Function Spaces},
Year = {2021},
Eprint = {arXiv:2108.08481},
}

@misc{gkn,
Author = {Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
Title = {Neural Operator: Graph Kernel Network for Partial Differential Equations},
Year = {2020},
Eprint = {arXiv:2003.03485},
}

@misc{knn,
Author = {Charles R. Qi and Li Yi and Hao Su and Leonidas J. Guibas},
Title = {PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space},
Year = {2017},
Eprint = {arXiv:1706.02413},
}

@article{Kutta,
  added-at = {2010-10-02T18:22:22.000+0200},
  author = {Kutta, W.},
  biburl = {https://www.bibsonomy.org/bibtex/24ee695c671c31151cc9816e314dfa6c2/brouder},
  interhash = {989a28740cc3399e4195a03ee3e5131b},
  intrahash = {4ee695c671c31151cc9816e314dfa6c2},
  journal = {Zeit. Math. Phys.},
  keywords = {imported},
  pages = {435-53},
  timestamp = {2010-10-02T18:22:49.000+0200},
  title = {Beitrag zur n\"aherungsweisen {I}ntegration
 totaler {D}ifferentialgleichungen},
  volume = 46,
  year = 1901
}

@article{Runge1895,
  doi = {10.1007/bf01446807},
  url = {https://doi.org/10.1007/bf01446807},
  year = {1895},
  month = jun,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {46},
  number = {2},
  pages = {167--178},
  author = {C. Runge},
  title = {Ueber die numerische Auflösung von Differentialgleichungen},
  journal = {Mathematische Annalen}
}

@article{1808.04930,
Author = {Yohai Bar-Sinai and Stephan Hoyer and Jason Hickey and Michael P. Brenner},
Title = {Learning data driven discretizations for partial differential equations},
Year = {2018},
Eprint = {arXiv:1808.04930},
Howpublished = {PNAS July 30, 2019 116 (31) 15344-15349},
Doi = {10.1073/pnas.1814058116},
}

@article{Hrennikoff1941,
  doi = {10.1115/1.4009129},
  url = {https://doi.org/10.1115/1.4009129},
  year = {1941},
  month = dec,
  publisher = {{ASME} International},
  volume = {8},
  number = {4},
  pages = {A169--A175},
  author = {A. Hrennikoff},
  title = {Solution of Problems of Elasticity by the Framework Method},
  journal = {Journal of Applied Mechanics}
}

@article{Courant1943,
  doi = {10.1090/s0002-9904-1943-07818-4},
  url = {https://doi.org/10.1090/s0002-9904-1943-07818-4},
  year = {1943},
  publisher = {American Mathematical Society ({AMS})},
  volume = {49},
  number = {1},
  pages = {1--23},
  author = {R. Courant},
  title = {Variational methods for the solution of problems of equilibrium and vibrations},
  journal = {Bulletin of the American Mathematical Society}
}

@book{Richardson2007,
  doi = {10.1017/cbo9780511618291},
  url = {https://doi.org/10.1017/cbo9780511618291},
  year = {2007},
  publisher = {Cambridge University Press},
  author = {Lewis Fry Richardson and Peter Lynch},
  title = {Weather Prediction by Numerical Process}
}

@article{Phillips1956,
  doi = {10.1002/qj.49708235202},
  url = {https://doi.org/10.1002/qj.49708235202},
  year = {1956},
  month = apr,
  publisher = {Wiley},
  volume = {82},
  number = {352},
  pages = {123--164},
  author = {Norman A. Phillips},
  title = {The general circulation of the atmosphere: A numerical experiment},
  journal = {Quarterly Journal of the Royal Meteorological Society}
}

@article{Berger1989,
  doi = {10.1016/0021-9991(89)90035-1},
  url = {https://doi.org/10.1016/0021-9991(89)90035-1},
  year = {1989},
  month = may,
  publisher = {Elsevier {BV}},
  volume = {82},
  number = {1},
  pages = {64--84},
  author = {M.J. Berger and P. Colella},
  title = {Local adaptive mesh refinement for shock hydrodynamics},
  journal = {Journal of Computational Physics}
}

@article{Berger1984,
  doi = {10.1016/0021-9991(84)90073-1},
  url = {https://doi.org/10.1016/0021-9991(84)90073-1},
  year = {1984},
  month = mar,
  publisher = {Elsevier {BV}},
  volume = {53},
  number = {3},
  pages = {484--512},
  author = {Marsha J Berger and Joseph Oliger},
  title = {Adaptive mesh refinement for hyperbolic partial differential equations},
  journal = {Journal of Computational Physics}
}

@misc{adam,
Author = {Diederik P. Kingma and Jimmy Ba},
Title = {Adam: A Method for Stochastic Optimization},
Year = {2014},
Eprint = {arXiv:1412.6980},
}

@inbook{pytorch,
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and K\"{o}pf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs.In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {721},
numpages = {12}
}

@misc{lightning,
  doi = {10.5281/ZENODO.3828935},
  url = {https://zenodo.org/record/3828935},
  author = {Falcon,  William and Borovec,  Jirka and W\"{a}lchli,  Adrian and Eggert,  Nic and Schock,  Justus and Jordan,  Jeremy and Skafte,  Nicki and {Ir1dXD} and Bereznyuk,  Vadim and Harris,  Ethan and {Tullie Murrell} and Yu,  Peter and Præsius,  Sebastian and Addair,  Travis and Zhong,  Jacob and Lipin,  Dmitry and Uchida,  So and {Shreyas Bapat} and Schr\"{o}ter,  Hendrik and Dayma,  Boris and Karnachev,  Alexey and {Akshay Kulkarni} and {Shunta Komatsu} and {Martin.B} and {Jean-Baptiste SCHIRATTI} and Mary,  Hadrien and Byrne,  Donal and {Cristobal Eyzaguirre} and {Cinjon} and Bakhtin,  Anton},
  title = {PyTorchLightning/pytorch-lightning: 0.7.6 release},
  publisher = {Zenodo},
  year = {2020},
  copyright = {Open Access}
}

@inproceedings{optuna,
author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
title = {Optuna: A Next-Generation Hyperparameter Optimization Framework},
year = {2019},
isbn = {9781450362016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3292500.3330701},
doi = {10.1145/3292500.3330701},
abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining},
pages = {2623–2631},
numpages = {9},
keywords = {machine learning system, Bayesian optimization, black-box optimization, hyperparameter optimization},
location = {Anchorage, AK, USA},
series = {KDD '19}
}