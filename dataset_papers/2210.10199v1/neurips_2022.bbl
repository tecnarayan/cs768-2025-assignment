\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balandat et~al.(2020)Balandat, Karrer, Jiang, Daulton, Letham, Wilson,
  and Bakshy]{balandat2020botorch}
Maximilian Balandat, Brian Karrer, Daniel~R. Jiang, Samuel Daulton, Benjamin
  Letham, Andrew~Gordon Wilson, and Eytan Bakshy.
\newblock Botorch: {A} framework for efficient monte-carlo {Bayesian}
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Baptista and Poloczek(2018)]{baptista2018bayesian}
Ricardo Baptista and Matthias Poloczek.
\newblock Bayesian optimization of combinatorial structures.
\newblock In \emph{Proc. of ICML}, volume~80 of \emph{Proceedings of Machine
  Learning Research}, pages 471--480. {PMLR}, 2018.

\bibitem[Bengio et~al.(2013)Bengio, L{\'e}onard, and
  Courville]{bengio2013estimating}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{ArXiv preprint}, abs/1308.3432, 2013.

\bibitem[Bergstra et~al.(2011)Bergstra, Bardenet, Bengio, and K{\'{e}}gl]{tpe}
James Bergstra, R{\'{e}}mi Bardenet, Yoshua Bengio, and Bal{\'{a}}zs
  K{\'{e}}gl.
\newblock Algorithms for hyper-parameter optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 24: 25th
  Annual Conference on Neural Information Processing Systems 2011. Proceedings
  of a meeting held 12-14 December 2011, Granada, Spain}, pages 2546--2554,
  2011.

\bibitem[Berkenkamp et~al.(2019)Berkenkamp, Schoellig, and
  Krause]{JMLR:v20:18-213}
Felix Berkenkamp, Angela~P. Schoellig, and Andreas Krause.
\newblock No-regret {Bayesian} optimization with unknown hyperparameters.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (50):\penalty0 1--24, 2019.
\newblock URL \url{http://jmlr.org/papers/v20/18-213.html}.

\bibitem[Bliek et~al.(2021)Bliek, Guijt, Verwer, and de~Weerdt]{bliek2021}
Laurens Bliek, Arthur Guijt, Sicco Verwer, and Mathijs de~Weerdt.
\newblock Black-box mixed-variable optimisation using a surrogate model that
  satisfies integer constraints.
\newblock GECCO '21, page 1851–1859. Association for Computing Machinery,
  2021.

\bibitem[Boros and Hammer(2002)]{BOROS2002155}
Endre Boros and Peter~L. Hammer.
\newblock Pseudo-boolean optimization.
\newblock \emph{Discrete Applied Mathematics}, 123\penalty0 (1):\penalty0
  155--225, 2002.
\newblock ISSN 0166-218X.
\newblock \doi{https://doi.org/10.1016/S0166-218X(01)00341-9}.

\bibitem[Daulton et~al.(2020)Daulton, Balandat, and Bakshy]{daulton2020ehvi}
Samuel Daulton, Maximilian Balandat, and Eytan Bakshy.
\newblock Differentiable expected hypervolume improvement for parallel
  multi-objective bayesian optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Daulton et~al.(2021)Daulton, Balandat, and
  Bakshy]{NEURIPS2021_11704817}
Samuel Daulton, Maximilian Balandat, and Eytan Bakshy.
\newblock Parallel {Bayesian} optimization of multiple noisy objectives with
  expected hypervolume improvement.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 2187--2200. Curran Associates, Inc., 2021.

\bibitem[Daulton et~al.(2022{\natexlab{a}})Daulton, Cakmak, Balandat, Osborne,
  Zhou, and Bakshy]{daulton2022robust}
Samuel Daulton, Sait Cakmak, Maximilian Balandat, Michael~A Osborne, Enlu Zhou,
  and Eytan Bakshy.
\newblock Robust multi-objective {Bayesian} optimization under input noise.
\newblock \emph{arXiv preprint arXiv:2202.07549}, 2022{\natexlab{a}}.

\bibitem[Daulton et~al.(2022{\natexlab{b}})Daulton, Eriksson, Balandat, and
  Bakshy]{morbo}
Samuel Daulton, David Eriksson, Maximilian Balandat, and Eytan Bakshy.
\newblock Multi-objective {Bayesian} optimization over high-dimensional search
  spaces.
\newblock In \emph{Proceedings of the Thirty-Eighth Conference on Uncertainty
  in Artificial Intelligence, {UAI} 2022, Eindhoven, Netherlands, August 1-5,
  2022}, Proceedings of Machine Learning Research. {AUAI} Press,
  2022{\natexlab{b}}.

\bibitem[Davies and Morton(2016)]{davies2016recent}
Huw~ML Davies and Daniel Morton.
\newblock Recent advances in c--h functionalization.
\newblock \emph{The Journal of Organic Chemistry}, 81\penalty0 (2):\penalty0
  343--350, 2016.

\bibitem[Daxberger et~al.(2020)Daxberger, Makarova, Turchetta, and
  Krause]{mixed_var_bo}
Erik~A. Daxberger, Anastasia Makarova, Matteo Turchetta, and Andreas Krause.
\newblock Mixed-variable {Bayesian} optimization.
\newblock In \emph{Proceedings of the Twenty-Ninth International Joint
  Conference on Artificial Intelligence, {IJCAI} 2020}, pages 2633--2639.
  ijcai.org, 2020.
\newblock \doi{10.24963/ijcai.2020/365}.

\bibitem[Deshwal and Doppa(2021)]{deshwal2021}
Aryan Deshwal and Jana Doppa.
\newblock Combining latent space and structured kernels for {Bayesian}
  optimization over combinatorial spaces.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 8185--8200. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/44e76e99b5e194377e955b13fb12f630-Paper.pdf}.

\bibitem[Deshwal et~al.(2021{\natexlab{a}})Deshwal, Belakaria, and
  Doppa]{deshwal2021bayesian}
Aryan Deshwal, Syrine Belakaria, and Janardhan~Rao Doppa.
\newblock Bayesian optimization over hybrid spaces.
\newblock In \emph{Proc. of ICML}, volume 139 of \emph{Proceedings of Machine
  Learning Research}, pages 2632--2643. {PMLR}, 2021{\natexlab{a}}.

\bibitem[Deshwal et~al.(2021{\natexlab{b}})Deshwal, Belakaria, and
  Doppa]{deshwal2021mercer}
Aryan Deshwal, Syrine Belakaria, and Janardhan~Rao Doppa.
\newblock Mercer features for efficient combinatorial {Bayesian} optimization.
\newblock In \emph{Thirty-Fifth AAAI Conference on Artificial Intelligence
  (AAAI)}, pages 7210--7218, 2021{\natexlab{b}}.

\bibitem[Dreifuerst et~al.(2021)Dreifuerst, Daulton, Qian, Varkey, Balandat,
  Kasturia, Tomar, Yazdan, Ponnampalam, and Heath]{dreifuerst2021}
Ryan~M Dreifuerst, Samuel Daulton, Yuchen Qian, Paul Varkey, Maximilian
  Balandat, Sanjay Kasturia, Anoop Tomar, Ali Yazdan, Vish Ponnampalam, and
  Robert~W Heath.
\newblock Optimizing coverage and capacity in cellular networks using machine
  learning.
\newblock In \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 8138--8142. IEEE, 2021.

\bibitem[Dua and Graff(2017)]{uci}
Dheeru Dua and Casey Graff.
\newblock {UCI} machine learning repository, 2017.

\bibitem[Duchon et~al.(2004)Duchon, Flajolet, Louchard, and Schaeffer]{boltz}
Philippe Duchon, Philippe Flajolet, Guy Louchard, and Gilles Schaeffer.
\newblock Boltzmann samplers for the random generation of combinatorial
  structures.
\newblock \emph{Comb. Probab. Comput.}, 13\penalty0 (4–5):\penalty0
  577–625, jul 2004.
\newblock ISSN 0963-5483.
\newblock \doi{10.1017/S0963548304006315}.

\bibitem[Emmerich et~al.(2006)Emmerich, Giannakoglou, and
  Naujoks]{emmerich2006}
M.~T.~M. Emmerich, K.~C. Giannakoglou, and B.~Naujoks.
\newblock Single- and multiobjective evolutionary optimization assisted by
  gaussian random field metamodels.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 10\penalty0
  (4):\penalty0 421--439, 2006.

\bibitem[Eriksson and Poloczek(2021)]{scbo}
David Eriksson and Matthias Poloczek.
\newblock Scalable constrained {Bayesian} optimization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 730--738. PMLR, 2021.

\bibitem[Eriksson et~al.(2019)Eriksson, Pearce, Gardner, Turner, and
  Poloczek]{turbo}
David Eriksson, Michael Pearce, Jacob~R. Gardner, Ryan Turner, and Matthias
  Poloczek.
\newblock Scalable global optimization via local {Bayesian} optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pages 5497--5508, 2019.

\bibitem[Frazier(2018)]{frazier2018tutorial}
Peter~I Frazier.
\newblock A tutorial on {Bayesian} optimization.
\newblock \emph{ArXiv preprint}, abs/1807.02811, 2018.

\bibitem[Gardner et~al.(2014)Gardner, Kusner, Xu, Weinberger, and
  Cunningham]{gardner2014constrained}
Jacob~R. Gardner, Matt~J. Kusner, Zhixiang~Eddie Xu, Kilian~Q. Weinberger, and
  John~P. Cunningham.
\newblock Bayesian optimization with inequality constraints.
\newblock In \emph{Proc. of ICML}, volume~32 of \emph{{JMLR} Workshop and
  Conference Proceedings}, pages 937--945. JMLR.org, 2014.

\bibitem[Garnett(2022)]{garnett_bayesoptbook_2022}
Roman Garnett.
\newblock \emph{{Bayesian Optimization}}.
\newblock Cambridge University Press, 2022.
\newblock in preparation.

\bibitem[Garrido-Merch{\'a}n and
  Hern{\'a}ndez-Lobato(2020)]{GarridoMerchn2020DealingWC}
Eduardo~C. Garrido-Merch{\'a}n and D.~Hern{\'a}ndez-Lobato.
\newblock Dealing with categorical and integer-valued variables in {Bayesian}
  optimization with gaussian processes.
\newblock \emph{Neurocomputing}, 380:\penalty0 20--35, 2020.

\bibitem[Glynn(1990)]{glynn1990likelihood}
Peter~W Glynn.
\newblock Likelihood ratio gradient estimation for stochastic systems.
\newblock \emph{Communications of the ACM}, 33\penalty0 (10):\penalty0 75--84,
  1990.

\bibitem[Hansen et~al.(2019)Hansen, Brockhoff, Mersmann, Tusar, Tusar, ElHara,
  Sampaio, Atamna, Varelas, Batu, et~al.]{hansencomparing}
Nikolaus Hansen, Dimo Brockhoff, Olaf Mersmann, Tea Tusar, Dejan Tusar,
  Ouassim~Ait ElHara, Phillipe~R Sampaio, Asma Atamna, Konstantinos Varelas,
  Umut Batu, et~al.
\newblock Comparing continuous optimizers: numbbo/coco on github.
\newblock 2019.

\bibitem[H{\"a}se et~al.(2021)H{\"a}se, Aldeghi, Hickman, Roch, and
  Aspuru-Guzik]{hase2021gryffin}
Florian H{\"a}se, Matteo Aldeghi, Riley~J Hickman, Lo{\"\i}c~M Roch, and
  Al{\'a}n Aspuru-Guzik.
\newblock Gryffin: An algorithm for {Bayesian} optimization of categorical
  variables informed by expert knowledge.
\newblock \emph{Applied Physics Reviews}, 8\penalty0 (3):\penalty0 031406,
  2021.

\bibitem[Hutter et~al.(2011)Hutter, Hoos, and Leyton-Brown]{smac}
Frank Hutter, Holger~H. Hoos, and Kevin Leyton-Brown.
\newblock Sequential model-based optimization for general algorithm
  configuration.
\newblock In \emph{Proceedings of the 5th International Conference on Learning
  and Intelligent Optimization}, page 507–523. Springer-Verlag, 2011.
\newblock ISBN 9783642255656.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{jang2016categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In \emph{Proc. of ICLR}. OpenReview.net, 2017.

\bibitem[Jones et~al.(1998)Jones, Schonlau, and Welch]{jones98}
Donald~R. Jones, Matthias Schonlau, and William~J. Welch.
\newblock Efficient global optimization of expensive black-box functions.
\newblock \emph{Journal of Global Optimization}, 13:\penalty0 455--492, 1998.

\bibitem[Kandasamy et~al.(2015)Kandasamy, Schneider, and
  Poczos]{pmlr-v37-kandasamy15}
Kirthevasan Kandasamy, Jeff Schneider, and Barnabas Poczos.
\newblock High dimensional {Bayesian} optimisation and bandits via additive
  models.
\newblock In Francis Bach and David Blei, editors, \emph{Proceedings of the
  32nd International Conference on Machine Learning}, volume~37 of
  \emph{Proceedings of Machine Learning Research}, pages 295--304, Lille,
  France, 07--09 Jul 2015. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v37/kandasamy15.html}.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kleijnen and Rubinstein(1996)]{kleijnen1996optimization}
Jack~PC Kleijnen and Reuven~Y Rubinstein.
\newblock Optimization and sensitivity analysis of computer simulation models
  by the score function method.
\newblock \emph{European Journal of Operational Research}, 88\penalty0
  (3):\penalty0 413--427, 1996.

\bibitem[Liu et~al.(2022)Liu, Feng, Eriksson, Letham, and Bakshy]{sparse_bo}
Sulin Liu, Qing Feng, David Eriksson, Benjamin Letham, and Eytan Bakshy.
\newblock Sparse {Bayesian} optimization.
\newblock \emph{arXiv preprint arXiv:2203.01900}, 2022.

\bibitem[Maddison et~al.(2017)Maddison, Mnih, and Teh]{maddison2016concrete}
Chris~J. Maddison, Andriy Mnih, and Yee~Whye Teh.
\newblock The concrete distribution: {A} continuous relaxation of discrete
  random variables.
\newblock In \emph{Proc. of ICLR}. OpenReview.net, 2017.

\bibitem[Maddox et~al.(2021)Maddox, Balandat, Wilson, and Bakshy]{maddox2021}
Wesley~J Maddox, Maximilian Balandat, Andrew~G Wilson, and Eytan Bakshy.
\newblock Bayesian optimization with high-dimensional outputs.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, 2021.

\bibitem[Mohamed et~al.(2020)Mohamed, Rosca, Figurnov, and
  Mnih]{mohamed2020monte}
Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih.
\newblock Monte carlo gradient estimation in machine learning.
\newblock \emph{J. Mach. Learn. Res.}, 21\penalty0 (132):\penalty0 1--62, 2020.

\bibitem[Nguyen et~al.(2020)Nguyen, Gupta, Rana, Shilton, and
  Venkatesh]{nguyen2019}
Dang Nguyen, Sunil Gupta, Santu Rana, Alistair Shilton, and Svetha Venkatesh.
\newblock Bayesian optimization for categorical and category-specific
  continuous inputs.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of
  Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium
  on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York,
  NY, USA, February 7-12, 2020}, pages 5256--5263. {AAAI} Press, 2020.

\bibitem[Oh et~al.(2019)Oh, Tomczak, Gavves, and Welling]{oh2019combinatorial}
ChangYong Oh, Jakub~M. Tomczak, Efstratios Gavves, and Max Welling.
\newblock Combinatorial {Bayesian} optimization using the graph cartesian
  product.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pages 2910--2920, 2019.

\bibitem[Owen(2003)]{owen2003quasi}
Art~B Owen.
\newblock Quasi-monte carlo sampling.
\newblock \emph{Monte Carlo Ray Tracing: Siggraph}, 1:\penalty0 69--88, 2003.

\bibitem[Paria et~al.(2019)Paria, Kandasamy, and
  P{\'{o}}czos]{paria2018flexible}
Biswajit Paria, Kirthevasan Kandasamy, and Barnab{\'{a}}s P{\'{o}}czos.
\newblock A flexible framework for multi-objective {Bayesian} optimization
  using random scalarizations.
\newblock In \emph{Proceedings of the Thirty-Fifth Conference on Uncertainty in
  Artificial Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  volume 115 of \emph{Proceedings of Machine Learning Research}, pages
  766--776. {AUAI} Press, 2019.

\bibitem[Pelamatti et~al.(2021)Pelamatti, Brevault, Balesdent, Talbi, and
  Guerin]{pelamatti2021bayesian}
Julien Pelamatti, Lo{\"\i}c Brevault, Mathieu Balesdent, El-Ghazali Talbi, and
  Yannick Guerin.
\newblock Bayesian optimization of variable-size design space problems.
\newblock \emph{Optimization and Engineering}, 22\penalty0 (1):\penalty0
  387--447, 2021.

\bibitem[Rapin and Teytaud(2018)]{nevergrad}
J.~Rapin and O.~Teytaud.
\newblock {Nevergrad - A gradient-free optimization platform}.
\newblock \url{https://GitHub.com/FacebookResearch/Nevergrad}, 2018.

\bibitem[Rasmussen(2004)]{Rasmussen2004}
Carl~Edward Rasmussen.
\newblock Gaussian processes in machine learning.
\newblock In \emph{Advanced Lectures on Machine Learning: ML Summer Schools
  2003, Canberra, Australia, February 2 - 14, 2003, T{\"u}bingen, Germany,
  August 4 - 16, 2003, Revised Lectures}, 2004.

\bibitem[Robbins and Monro(1951)]{robbins_monro}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400--407, 1951.
\newblock ISSN 00034851.

\bibitem[Ru et~al.(2020)Ru, Alvi, Nguyen, Osborne, and Roberts]{ru2020bayesian}
Bin~Xin Ru, Ahsan~S. Alvi, Vu~Nguyen, Michael~A. Osborne, and Stephen~J.
  Roberts.
\newblock Bayesian optimisation over multiple continuous and categorical
  inputs.
\newblock In \emph{Proc. of ICML}, volume 119 of \emph{Proceedings of Machine
  Learning Research}, pages 8276--8285. {PMLR}, 2020.

\bibitem[Russo and Roy(2014)]{russo2014learning}
Daniel Russo and Benjamin~Van Roy.
\newblock Learning to optimize via posterior sampling.
\newblock \emph{arXiv preprint: arXiv 1301.2609}, 2014.

\bibitem[Samal et~al.(2021)Samal, Swain, Bandopadhaya, Dandanov, Poulkov,
  Routray, and Palai]{samal2021dynamic}
Soumya~Ranjan Samal, Kaliprasanna Swain, Shuvabrata Bandopadhaya, Nikolay
  Dandanov, Vladimir Poulkov, Sidheswar Routray, and Gopinath Palai.
\newblock Dynamic coverage optimization for 5g ultra-dense cellular networks
  based on their user densities.
\newblock 2021.

\bibitem[Shields et~al.(2021)Shields, Stevens, Li, Parasram, Damani, Alvarado,
  Janey, Adams, and Doyle]{shields2021bayesian}
Benjamin~J Shields, Jason Stevens, Jun Li, Marvin Parasram, Farhan Damani,
  Jesus I~Martinez Alvarado, Jacob~M Janey, Ryan~P Adams, and Abigail~G Doyle.
\newblock Bayesian reaction optimization as a tool for chemical synthesis.
\newblock \emph{Nature}, 590\penalty0 (7844):\penalty0 89--96, 2021.

\bibitem[Srinivas et~al.(2010)Srinivas, Krause, Kakade, and Seeger]{ucb}
Niranjan Srinivas, Andreas Krause, Sham~M. Kakade, and Matthias~W. Seeger.
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock In \emph{Proc. of ICML}, pages 1015--1022. Omnipress, 2010.

\bibitem[{The GPyOpt authors}(2016)]{gpyopt2016}
{The GPyOpt authors}.
\newblock {GPyOpt}: A {Bayesian} optimization framework in python.
\newblock \url{http://github.com/SheffieldML/GPyOpt}, 2016.

\bibitem[Tran et~al.(2019)Tran, Tran, and Wang]{tran2019constrained}
Anh Tran, Minh Tran, and Yan Wang.
\newblock Constrained mixed-integer gaussian mixture {Bayesian} optimization
  and its applications in designing fractal and auxetic metamaterials.
\newblock \emph{Structural and Multidisciplinary Optimization}, 59\penalty0
  (6):\penalty0 2131--2154, 2019.

\bibitem[Turner et~al.(2021)Turner, Eriksson, McCourt, Kiili, Laaksonen, Xu,
  and Guyon]{turner2021bayesian}
Ryan Turner, David Eriksson, Michael McCourt, Juha Kiili, Eero Laaksonen, Zhen
  Xu, and Isabelle Guyon.
\newblock {Bayesian} optimization is superior to random search for machine
  learning hyperparameter tuning: Analysis of the black-box optimization
  challenge 2020.
\newblock In \emph{NeurIPS 2020 Competition and Demonstration Track}, pages
  3--26, 2021.

\bibitem[Tu\v{s}ar et~al.(2019)Tu\v{s}ar, Brockhoff, and
  Hansen]{bbob_mixed_int}
Tea Tu\v{s}ar, Dimo Brockhoff, and Nikolaus Hansen.
\newblock Mixed-integer benchmark problems for single- and bi-objective
  optimization.
\newblock In \emph{Proceedings of the Genetic and Evolutionary Computation
  Conference}, GECCO '19, page 718–726. Association for Computing Machinery,
  2019.

\bibitem[Wan et~al.(2021)Wan, Nguyen, Ha, Ru, Lu, and Osborne]{wan2021think}
Xingchen Wan, Vu~Nguyen, Huong Ha, Bin~Xin Ru, Cong Lu, and Michael~A. Osborne.
\newblock Think global and act local: {Bayesian} optimisation over
  high-dimensional categorical and mixed search spaces.
\newblock In \emph{Proc. of ICML}, volume 139 of \emph{Proceedings of Machine
  Learning Research}, pages 10663--10674. {PMLR}, 2021.

\bibitem[Wan et~al.(2022)Wan, Lu, Parker-Holder, Ball, Nguyen, Ru, and
  Osborne]{wan2022bayesian}
Xingchen Wan, Cong Lu, Jack Parker-Holder, Philip~J Ball, Vu~Nguyen, Binxin Ru,
  and Michael Osborne.
\newblock Bayesian generational population-based training.
\newblock In \emph{ICLR Workshop on Agent Learning in Open-Endedness}, 2022.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Cai, Liu, Yang, and
  Ding]{oil_solbent}
Boqian Wang, Jiacheng Cai, Chuangui Liu, Jian Yang, and Xianting Ding.
\newblock Harnessing a novel machine-learning-assisted evolutionary algorithm
  to co-optimize three characteristics of an electrospun oil sorbent.
\newblock \emph{ACS Applied Materials \& Interfaces}, 12\penalty0
  (38):\penalty0 42842--42849, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Clark, Liu, and
  Frazier]{wang2016parallel}
Jialei Wang, Scott~C Clark, Eric Liu, and Peter~I Frazier.
\newblock Parallel {Bayesian} global optimization of expensive functions.
\newblock \emph{Operations Research}, 68\penalty0 (6):\penalty0 1850--1865,
  2020{\natexlab{b}}.

\bibitem[Wang et~al.(2017)Wang, Xiong, Ishibuchi, Wu, and Zhang]{WANG201725}
Rui Wang, Jian Xiong, Hisao Ishibuchi, Guohua Wu, and Tao Zhang.
\newblock On the effect of reference point in moea/d for multi-objective
  optimization.
\newblock \emph{Applied Soft Computing}, 58:\penalty0 25--34, 2017.
\newblock ISSN 1568-4946.
\newblock \doi{https://doi.org/10.1016/j.asoc.2017.04.002}.

\bibitem[Williams(1992)]{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 229--256, 1992.

\bibitem[Wilson et~al.(2018)Wilson, Hutter, and Deisenroth]{wilson2018maxbo}
James~T. Wilson, Frank Hutter, and Marc~Peter Deisenroth.
\newblock Maximizing acquisition functions for {Bayesian} optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
  December 3-8, 2018, Montr{\'{e}}al, Canada}, pages 9906--9917, 2018.

\bibitem[Yin et~al.(2019)Yin, Yue, and Zhou]{ARSM_ICML2019}
Mingzhang Yin, Yuguang Yue, and Mingyuan Zhou.
\newblock {ARSM:} augment-reinforce-swap-merge estimator for gradient
  backpropagation through categorical variables.
\newblock In \emph{Proc. of ICML}, volume~97 of \emph{Proceedings of Machine
  Learning Research}, pages 7095--7104. {PMLR}, 2019.

\bibitem[{Yin} et~al.(2020){Yin}, {Ho}, {Yan}, {Qian}, and
  {Zhou}]{yin2020probabilistic}
Mingzhang {Yin}, Nhat {Ho}, Bowei {Yan}, Xiaoning {Qian}, and Mingyuan {Zhou}.
\newblock {Probabilistic Best Subset Selection by Gradient-Based Optimization}.
\newblock \emph{arXiv e-prints}, 2020.

\bibitem[Zhang and Golovin(2020)]{golovin2020random}
Richard Zhang and Daniel Golovin.
\newblock Random hypervolume scalarizations for provable multi-objective black
  box optimization.
\newblock In \emph{Proc. of ICML}, volume 119 of \emph{Proceedings of Machine
  Learning Research}, pages 11096--11105. {PMLR}, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Tao, Chen, and Apley]{zhang2019latent}
Yichi Zhang, Siyu Tao, Wei Chen, and Daniel Apley.
\newblock A latent variable approach to gaussian process modeling with
  qualitative and quantitative factors.
\newblock \emph{Technometrics}, 62:\penalty0 1--19, 07 2019.
\newblock \doi{10.1080/00401706.2019.1638834}.

\end{thebibliography}
