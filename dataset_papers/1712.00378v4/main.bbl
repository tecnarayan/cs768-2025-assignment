\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arulkumaran et~al.(2017)Arulkumaran, Deisenroth, Brundage, and
  Bharath]{Arulkumaran:2017drlbrief}
Arulkumaran, K., Deisenroth, M.~P., Brundage, M., and Bharath, A.~A.
\newblock A brief survey of deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1708.05866}, 2017.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{Bellemare:2013arcade}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The {A}rcade {L}earning {E}nvironment: an evaluation platform for
  general agents.
\newblock \emph{Journal of Artificial Intelligence Research (JAIR)},
  47:\penalty0 253--279, 2013.

\bibitem[Bertsekas(1995)]{Bertsekas:1995dynamic}
Bertsekas, D.~P.
\newblock \emph{Dynamic programming and optimal control}.
\newblock Athena Scientific, 1995.

\bibitem[Bertsekas \& Tsitsiklis(1996)Bertsekas and
  Tsitsiklis]{Bertsekas:1996neuro}
Bertsekas, D.~P. and Tsitsiklis, J.
\newblock \emph{Neuro-Dynamic Programming}.
\newblock Athena Scientific, 1996.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{Brockman:2016gym}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Open{AI} {G}ym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Duan et~al.(2016)Duan, Chen, Houthooft, Schulman, and
  Abbeel]{Duan:2016benchmarking}
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1329--1338, 2016.

\bibitem[Florensa et~al.(2017)Florensa, Held, Wulfmeier, and
  Abbeel]{Florensa:2017reverse}
Florensa, C., Held, D., Wulfmeier, M., and Abbeel, P.
\newblock Reverse curriculum generation for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1707.05300}, 2017.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{Goodfellow:2016DLbook}
Goodfellow, I., Bengio, Y., and Courville, A.
\newblock \emph{Deep Learning}.
\newblock MIT Press, 2016.

\bibitem[Harada(1997)]{harada:1997time}
Harada, D.
\newblock Reinforcement learning with time.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, pp.\
  577--582. AAAI Press, 1997.

\bibitem[Heess et~al.(2017)Heess, Sriram, Lemmon, Merel, Wayne, Tassa, Erez,
  Wang, Eslami, Riedmiller, and Silver]{Heess:2017emergence}
Heess, N., Sriram, S., Lemmon, J., Merel, J., Wayne, G., Tassa, Y., Erez, T.,
  Wang, Z., Eslami, A., Riedmiller, M., and Silver, D.
\newblock Emergence of locomotion behaviours in rich environments.
\newblock \emph{arXiv preprint arXiv:1707.02286}, 2017.

\bibitem[Henderson et~al.(2017)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{Henderson:2017matters}
Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D.
\newblock Deep reinforcement learning that matters.
\newblock \emph{arXiv preprint arXiv:1709.06560}, 2017.

\bibitem[Hesse et~al.(2017)Hesse, Plappert, Radford, Schulman, Sidor, and
  Wu]{baselines}
Hesse, C., Plappert, M., Radford, A., Schulman, J., Sidor, S., and Wu, Y.
\newblock Open{AI} {B}aselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\bibitem[Kaelbling et~al.(1996)Kaelbling, Littman, and
  Moore]{Kaelbling:1996RLsurvey}
Kaelbling, L.~P., Littman, M.~L., and Moore, A.~W.
\newblock Reinforcement learning: a survey.
\newblock \emph{Journal of Artificial Intelligence Research (JAIR)},
  4:\penalty0 237--285, 1996.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{Lecun:2015deep}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Lin(1992)]{lin1992self}
Lin, L.-J.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine Learning}, 8\penalty0 (3-4):\penalty0 293--321, 1992.

\bibitem[Lovejoy(1991)]{Lovejoy:1991POMDPsurvey}
Lovejoy, W.~S.
\newblock A survey of algorithmic methods for partially observed {M}arkov
  decision processes.
\newblock \emph{Annals of Operations Research}, 28\penalty0 (1):\penalty0
  47--65, 1991.

\bibitem[Machado et~al.(2017)Machado, Bellemare, Talvitie, Veness, Hausknecht,
  and Bowling]{Machado:2017revisitALE}
Machado, M.~C., Bellemare, M.~G., Talvitie, E., Veness, J., Hausknecht, M., and
  Bowling, M.
\newblock Revisiting the {A}rcade {L}earning {E}nvironment: evaluation
  protocols and open problems for general agents.
\newblock \emph{arXiv preprint arXiv:1709.06009}, 2017.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{Mnih:2015natureDQN}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Ng, A.~Y., Harada, D., and Russell, S.
\newblock Policy invariance under reward transformations: theory and
  application to reward shaping.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  278--287, 1999.

\bibitem[Puterman(2014)]{Puterman:2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{Schaul:2016prior}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Schmidhuber(2015)]{Schmidhuber:2015NN}
Schmidhuber, J.
\newblock Deep learning in neural networks: an overview.
\newblock \emph{Neural Networks}, 61:\penalty0 85--117, 2015.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{Schulman:2016gae}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{Schulman:2017ppo}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sutton(1988)]{Sutton:1988TD}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine Learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{Sutton:1998}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: an Introduction}.
\newblock MIT Press, 1998.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{Sutton:2000pg}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  1057--1063, 2000.

\bibitem[Szepesvari(2010)]{Szepesvari:2010algorithms}
Szepesvari, C.
\newblock \emph{Algorithms for Reinforcement Learning}.
\newblock Morgan and Claypool, 2010.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{Todorov:2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock {MuJoCo}: a physics engine for model-based control.
\newblock In \emph{Intelligent Robots and Systems (IROS), IEEE/RSJ
  International Conference on}, pp.\  5026--5033, 2012.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{Watkins:1992Q}
Watkins, C.~J. and Dayan, P.
\newblock Q-learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 279--292, 1992.

\bibitem[White(2017)]{white2016unifying}
White, M.
\newblock Unifying task specification in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  3742--3750, 2017.

\bibitem[Whitehead \& Ballard(1991)Whitehead and
  Ballard]{Whitehead:1991aliasing}
Whitehead, S.~D. and Ballard, D.~H.
\newblock Learning to perceive and act by trial and error.
\newblock \emph{Machine Learning}, 7\penalty0 (1):\penalty0 45--83, 1991.

\bibitem[Wierstra et~al.(2009)Wierstra, F{\"o}rster, Peters, and
  Schmidhuber]{Wierstra:2009rpg}
Wierstra, D., F{\"o}rster, A., Peters, J., and Schmidhuber, J.
\newblock Recurrent policy gradients.
\newblock \emph{Logic Journal of IGPL}, 18\penalty0 (5):\penalty0 620--634,
  2009.

\bibitem[Williams(1992)]{Williams:1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine Learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Zhang \& Sutton(2017)Zhang and Sutton]{Zhang:2017deeper}
Zhang, S. and Sutton, R.~S.
\newblock A deeper look at experience replay.
\newblock \emph{arXiv preprint arXiv:1712.01275}, 2017.

\end{thebibliography}
