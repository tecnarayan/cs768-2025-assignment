\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alemohammad et~al.(2020)Alemohammad, Wang, Balestriero, and
  Baraniuk]{rnnntk}
Alemohammad, S., Wang, Z., Balestriero, R., and Baraniuk, R.
\newblock The recurrent neural tangent kernel.
\newblock \emph{ArXiv}, abs/2006.10246, 2020.

\bibitem[Allen-Zhu et~al.(2018)Allen-Zhu, Li, and
  Song]{allen-zhu_convergence_2018}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A {Convergence} {Theory} for {Deep} {Learning} via
  {Over}-{Parameterization}.
\newblock \emph{arXiv:1811.03962 [cs, math, stat]}, November 2018.
\newblock URL \url{http://arxiv.org/abs/1811.03962}.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and Wang]{arora}
Arora, S., Du, S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{attention1}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{CoRR}, abs/1409.0473, 2015.

\bibitem[Bruna et~al.(2014)Bruna, Zaremba, Szlam, and LeCun]{graph1}
Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y.
\newblock Spectral networks and locally connected networks on graphs.
\newblock \emph{CoRR}, abs/1312.6203, 2014.

\bibitem[Cho et~al.(2014)Cho, Merrienboer, Çaglar G{\"u}lçehre, Bahdanau,
  Bougares, Schwenk, and Bengio]{gru}
Cho, K., Merrienboer, B.~V., Çaglar G{\"u}lçehre, Bahdanau, D., Bougares, F.,
  Schwenk, H., and Bengio, Y.
\newblock Learning phrase representations using rnn encoder-decoder for
  statistical machine translation.
\newblock \emph{ArXiv}, abs/1406.1078, 2014.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{gp1}
Daniely, A., Frostig, R., and Singer, Y.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In \emph{NIPS}, 2016.

\bibitem[Defferrard et~al.(2016)Defferrard, Bresson, and Vandergheynst]{graph2}
Defferrard, M., Bresson, X., and Vandergheynst, P.
\newblock Convolutional neural networks on graphs with fast localized spectral
  filtering.
\newblock In \emph{NIPS}, 2016.

\bibitem[Du et~al.(2019)Du, Hou, P{\'o}czos, Salakhutdinov, Wang, and
  Xu]{graphntk}
Du, S., Hou, K., P{\'o}czos, B., Salakhutdinov, R., Wang, R., and Xu, K.
\newblock Graph neural tangent kernel: Fusing graph neural networks with graph
  kernels.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du_gradient_2018}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient {Descent} {Provably} {Optimizes} {Over}-parameterized
  {Neural} {Networks}.
\newblock \emph{arXiv:1810.02054 [cs, math, stat]}, October 2018.
\newblock URL \url{http://arxiv.org/abs/1810.02054}.

\bibitem[Duvenaud et~al.(2015)Duvenaud, Maclaurin, Aguilera-Iparraguirre,
  G{\'o}mez-Bombarelli, Hirzel, Aspuru-Guzik, and Adams]{graph3}
Duvenaud, D., Maclaurin, D., Aguilera-Iparraguirre, J., G{\'o}mez-Bombarelli,
  R., Hirzel, T., Aspuru-Guzik, A., and Adams, R.
\newblock Convolutional networks on graphs for learning molecular fingerprints.
\newblock In \emph{NIPS}, 2015.

\bibitem[Fukushima(1975)]{conv1}
Fukushima, K.
\newblock Cognitron: A self-organizing multilayered neural network.
\newblock \emph{Biological Cybernetics}, 20\penalty0 (3):\penalty0 121--136,
  1975.
\newblock \doi{10.1007/BF00342633}.
\newblock URL \url{https://doi.org/10.1007/BF00342633}.

\bibitem[Fukushima(1980)]{conv2}
Fukushima, K.
\newblock Neocognitron: A self-organizing neural network model for a mechanism
  of pattern recognition unaffected by shift in position.
\newblock \emph{Biological Cybernetics}, 36\penalty0 (4):\penalty0 193--202,
  1980.
\newblock \doi{10.1007/BF00344251}.
\newblock URL \url{https://doi.org/10.1007/BF00344251}.

\bibitem[Hanin \& Nica(2019)Hanin and Nica]{boris}
Hanin, B. and Nica, M.
\newblock Finite depth and width corrections to the neural tangent kernel.
\newblock \emph{ArXiv}, abs/1909.05989, 2019.

\bibitem[Hazan \& Jaakkola(2015)Hazan and Jaakkola]{gp2}
Hazan, T. and Jaakkola, T.
\newblock Steps toward deep kernel methods from infinite neural networks.
\newblock \emph{ArXiv}, abs/1508.05133, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{res1}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  770--778, 2016.

\bibitem[Henaff et~al.(2015)Henaff, Bruna, and LeCun]{graph4}
Henaff, M., Bruna, J., and LeCun, Y.
\newblock Deep convolutional networks on graph-structured data.
\newblock \emph{ArXiv}, abs/1506.05163, 2015.

\bibitem[Hinton \& Neal(1995)Hinton and Neal]{gp6}
Hinton, G.~E. and Neal, R.
\newblock Bayesian learning for neural networks.
\newblock 1995.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and Schmidhuber]{lstm}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 9:\penalty0 1735--1780, 1997.

\bibitem[Hron et~al.(2020)Hron, Bahri, Sohl-Dickstein, and
  Novak]{attention_ntk}
Hron, J., Bahri, Y., Sohl-Dickstein, J., and Novak, R.
\newblock Infinite attention: Nngp and ntk for deep attention networks.
\newblock In \emph{ICML}, 2020.

\bibitem[Huang et~al.(2017)Huang, Liu, and Weinberger]{res2}
Huang, G., Liu, Z., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  2261--2269, 2017.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{bn}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{ArXiv}, abs/1502.03167, 2015.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{NTK}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Kipf \& Welling(2017)Kipf and Welling]{graph5}
Kipf, T. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock \emph{ArXiv}, abs/1609.02907, 2017.

\bibitem[Lecun et~al.(1998)Lecun, Bottou, Bengio, and Haffner]{conv3}
Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86:\penalty0 2278--2324, 12 1998.
\newblock \doi{10.1109/5.726791}.

\bibitem[Lecun et~al.(2000)Lecun, Haffner, and Bengio]{conv4}
Lecun, Y., Haffner, P., and Bengio, Y.
\newblock Object recognition with gradient-based learning.
\newblock 08 2000.

\bibitem[Lee et~al.(2018)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-Dickstein]{gp4}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S., Pennington, J., and
  Sohl-Dickstein, J.
\newblock Deep neural networks as gaussian processes.
\newblock \emph{ArXiv}, abs/1711.00165, 2018.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Sohl-Dickstein, and
  Pennington]{wide}
Lee, J., Xiao, L., Schoenholz, S.~S., Bahri, Y., Sohl-Dickstein, J., and
  Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{ArXiv}, abs/1902.06720, 2019.

\bibitem[Li \& Nguyen(2019)Li and Nguyen]{tied-encoder}
Li, P. and Nguyen, P.-M.
\newblock On random deep weight-tied autoencoders: Exact asymptotic analysis,
  phase transitions, and implications to training.
\newblock In \emph{ICLR}, 2019.

\bibitem[Littwin et~al.(2020{\natexlab{a}})Littwin, Galanti, and Wolf]{resntk}
Littwin, E., Galanti, T., and Wolf, L.
\newblock On random kernels of residual architectures.
\newblock \emph{arXiv: Learning}, 2020{\natexlab{a}}.

\bibitem[Littwin et~al.(2020{\natexlab{b}})Littwin, Myara, Sabah, Susskind,
  Zhai, and Golan]{ce}
Littwin, E., Myara, B., Sabah, S., Susskind, J., Zhai, S., and Golan, O.
\newblock Collegial ensembles.
\newblock \emph{ArXiv}, abs/2006.07678, 2020{\natexlab{b}}.

\bibitem[Matthews et~al.(2018)Matthews, Rowland, Hron, Turner, and
  Ghahramani]{gp5}
Matthews, A., Rowland, M., Hron, J., Turner, R., and Ghahramani, Z.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock \emph{ArXiv}, abs/1804.11271, 2018.

\bibitem[Novak et~al.(2019)Novak, Xiao, Bahri, Lee, Yang, Hron, Abolafia,
  Pennington, and Sohl-Dickstein]{gp7}
Novak, R., Xiao, L., Bahri, Y., Lee, J., Yang, G., Hron, J., Abolafia, D.,
  Pennington, J., and Sohl-Dickstein, J.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock In \emph{ICLR}, 2019.

\bibitem[Roux \& Bengio(2007)Roux and Bengio]{gp3}
Roux, N.~L. and Bengio, Y.
\newblock Continuous neural networks.
\newblock In Meila, M. and Shen, X. (eds.), \emph{Proceedings of the Eleventh
  International Conference on Artificial Intelligence and Statistics}, volume~2
  of \emph{Proceedings of Machine Learning Research}, pp.\  404--411, San Juan,
  Puerto Rico, 21--24 Mar 2007. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v2/leroux07a.html}.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and Williams]{conv5}
Rumelhart, D., Hinton, G.~E., and Williams, R.~J.
\newblock Learning internal representations by error propagation.
\newblock 1986.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{attention2}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{ArXiv}, abs/1706.03762, 2017.

\bibitem[Yang(2019{\natexlab{a}})]{yang}
Yang, G.
\newblock Tensor programs i: Wide feedforward or recurrent neural networks of
  any architecture are gaussian processes.
\newblock \emph{ArXiv}, abs/1910.12478, 2019{\natexlab{a}}.

\bibitem[Yang(2019{\natexlab{b}})]{yangScalingLimit}
Yang, G.
\newblock Scaling {{Limits}} of {{Wide Neural Networks}} with {{Weight
  Sharing}}: {{Gaussian Process Behavior}}, {{Gradient Independence}}, and
  {{Neural Tangent Kernel Derivation}}.
\newblock \emph{arXiv:1902.04760 [cond-mat, physics:math-ph, stat]}, February
  2019{\natexlab{b}}.

\bibitem[Yang(2020{\natexlab{a}})]{yang2}
Yang, G.
\newblock Tensor programs ii: Neural tangent kernel for any architecture.
\newblock \emph{ArXiv}, abs/2006.14548, 2020{\natexlab{a}}.

\bibitem[Yang(2020{\natexlab{b}})]{yang3}
Yang, G.
\newblock Tensor programs iii: Neural matrix laws.
\newblock \emph{ArXiv}, abs/2009.10685, 2020{\natexlab{b}}.

\bibitem[Yang \& Hu(2020)Yang and Hu]{yang4}
Yang, G. and Hu, E.~J.
\newblock Feature learning in infinite-width neural networks.
\newblock \emph{ArXiv}, abs/2011.14522, 2020.

\bibitem[Yang \& Schoenholz(2017)Yang and Schoenholz]{yangMFResnet}
Yang, G. and Schoenholz, S.~S.
\newblock Mean {Field} {Residual} {Network}: {On} the {Edge} of {Chaos}.
\newblock In \emph{Advances in neural information processing systems}, 2017.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou_stochastic_2018}
Zou, D., Cao, Y., Zhou, D., and Gu, Q.
\newblock Stochastic {Gradient} {Descent} {Optimizes} {Over}-parameterized
  {Deep} {ReLU} {Networks}.
\newblock \emph{arXiv:1811.08888 [cs, math, stat]}, November 2018.
\newblock URL \url{http://arxiv.org/abs/1811.08888}.

\end{thebibliography}
