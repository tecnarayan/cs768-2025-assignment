\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ai et~al.(2021)Ai, Yu, Zhang, and Wang]{ai2021optimal}
M.~Ai, J.~Yu, H.~Zhang, and H.~Wang.
\newblock Optimal subsampling algorithms for big data regressions.
\newblock \emph{Statistica Sinica}, 31\penalty0 (2):\penalty0 749--772, 2021.

\bibitem[Aronszajn(1950)]{aronszajn1950theory}
N.~Aronszajn.
\newblock Theory of reproducing kernels.
\newblock \emph{Transactions of the American mathematical society}, 68\penalty0 (3):\penalty0 337--404, 1950.

\bibitem[Bach(2021)]{bach2021learning}
F.~Bach.
\newblock Learning theory from first principles.
\newblock \emph{Draft of a book, version of Sept}, 6:\penalty0 2021, 2021.

\bibitem[Bach et~al.(2012)Bach, Lacoste-Julien, and Obozinski]{bach2012equivalence}
F.~Bach, S.~Lacoste-Julien, and G.~Obozinski.
\newblock On the equivalence between herding and conditional gradient algorithms.
\newblock \emph{arXiv preprint arXiv:1203.4523}, 2012.

\bibitem[Bietti and Mairal(2019)]{bietti2019group}
A.~Bietti and J.~Mairal.
\newblock Group invariance, stability to deformations, and complexity of deep convolutional representations.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0 (1):\penalty0 876--924, 2019.

\bibitem[Chan et~al.(2021)Chan, Li, and Oymak]{chan2021marginal}
Y.-C. Chan, M.~Li, and S.~Oymak.
\newblock On the marginal benefit of active learning: Does self-supervision eat its cake?
\newblock In \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 3455--3459. IEEE, 2021.

\bibitem[Chen et~al.(2012)Chen, Welling, and Smola]{chen2012super}
Y.~Chen, M.~Welling, and A.~Smola.
\newblock Super-samples from kernel herding.
\newblock \emph{arXiv preprint arXiv:1203.3472}, 2012.

\bibitem[Cho et~al.(2022)Cho, Kim, Jung, and Kweon]{cho2022mcdal}
J.~W. Cho, D.-J. Kim, Y.~Jung, and I.~S. Kweon.
\newblock Mcdal: Maximum classifier discrepancy for active learning.
\newblock \emph{IEEE transactions on neural networks and learning systems}, 2022.

\bibitem[Coates et~al.(2011)Coates, Ng, and Lee]{coates2011analysis}
A.~Coates, A.~Ng, and H.~Lee.
\newblock An analysis of single-layer networks in unsupervised feature learning.
\newblock In \emph{Proceedings of the fourteenth international conference on artificial intelligence and statistics}, pages 215--223. JMLR Workshop and Conference Proceedings, 2011.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition}, pages 248--255. Ieee, 2009.

\bibitem[Freytag et~al.(2014)Freytag, Rodner, and Denzler]{freytag2014selecting}
A.~Freytag, E.~Rodner, and J.~Denzler.
\newblock Selecting influential examples: Active learning with expected model output changes.
\newblock In \emph{Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part IV 13}, pages 562--577. Springer, 2014.

\bibitem[Gao et~al.(2020)Gao, Zhang, Yu, Ar{\i}k, Davis, and Pfister]{gao2020consistency}
M.~Gao, Z.~Zhang, G.~Yu, S.~{\"O}. Ar{\i}k, L.~S. Davis, and T.~Pfister.
\newblock Consistency-based semi-supervised active learning: Towards minimizing labeling cost.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part X 16}, pages 510--526. Springer, 2020.

\bibitem[Graf and Luschgy(2007)]{graf2007foundations}
S.~Graf and H.~Luschgy.
\newblock \emph{Foundations of quantization for probability distributions}.
\newblock Springer, 2007.

\bibitem[Gretton et~al.(2006)Gretton, Borgwardt, Rasch, Sch{\"o}lkopf, and Smola]{gretton2006kernel}
A.~Gretton, K.~Borgwardt, M.~Rasch, B.~Sch{\"o}lkopf, and A.~Smola.
\newblock A kernel method for the two-sample-problem.
\newblock \emph{Advances in neural information processing systems}, 19, 2006.

\bibitem[Gretton et~al.(2012)Gretton, Borgwardt, Rasch, Sch{\"o}lkopf, and Smola]{gretton2012kernel}
A.~Gretton, K.~M. Borgwardt, M.~J. Rasch, B.~Sch{\"o}lkopf, and A.~Smola.
\newblock A kernel two-sample test.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0 (1):\penalty0 723--773, 2012.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem[Huang et~al.(2021)Huang, Wang, Xiong, Huan, and Dou]{huang2021semi}
S.~Huang, T.~Wang, H.~Xiong, J.~Huan, and D.~Dou.
\newblock Semi-supervised active learning with temporal output discrepancy.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 3447--3456, 2021.

\bibitem[Joshi et~al.(2009)Joshi, Porikli, and Papanikolopoulos]{joshi2009multi}
A.~J. Joshi, F.~Porikli, and N.~Papanikolopoulos.
\newblock Multi-class active learning for image classification.
\newblock In \emph{2009 ieee conference on computer vision and pattern recognition}, pages 2372--2379. IEEE, 2009.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Laine and Aila(2016)]{laine2016temporal}
S.~Laine and T.~Aila.
\newblock Temporal ensembling for semi-supervised learning.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Lee et~al.(2013)]{lee2013pseudo}
D.-H. Lee et~al.
\newblock Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks.
\newblock In \emph{Workshop on challenges in representation learning, ICML}, volume~3, page 896. Atlanta, 2013.

\bibitem[Lewis and Catlett(1994)]{lewis1994heterogeneous}
D.~D. Lewis and J.~Catlett.
\newblock Heterogeneous uncertainty sampling for supervised learning.
\newblock In \emph{Machine learning proceedings 1994}, pages 148--156. Elsevier, 1994.

\bibitem[Li et~al.(2023)Li, Wu, Liu, Yu, Yang, Han, and Liu]{li2023instant}
M.~Li, R.~Wu, H.~Liu, J.~Yu, X.~Yang, B.~Han, and T.~Liu.
\newblock Instant: Semi-supervised learning with instance-dependent thresholds.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Liu et~al.(2021)Liu, Ding, Zhong, Li, Dai, and He]{liu2021influence}
Z.~Liu, H.~Ding, H.~Zhong, W.~Li, J.~Dai, and C.~He.
\newblock Influence selection for active learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 9274--9283, 2021.

\bibitem[Lohr(2021)]{lohr2021sampling}
S.~L. Lohr.
\newblock \emph{Sampling: design and analysis}.
\newblock Chapman and Hall/CRC, 2021.

\bibitem[Luo et~al.(2013)Luo, Schwing, and Urtasun]{luo2013latent}
W.~Luo, A.~Schwing, and R.~Urtasun.
\newblock Latent structured active learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Mahmood et~al.(2021)Mahmood, Fidler, and Law]{mahmood2021low}
R.~Mahmood, S.~Fidler, and M.~T. Law.
\newblock Low budget active learning via wasserstein distance: An integer programming approach.
\newblock \emph{arXiv preprint arXiv:2106.02968}, 2021.

\bibitem[Mak and Joseph(2018)]{mak2018support}
S.~Mak and V.~R. Joseph.
\newblock Support points.
\newblock \emph{The Annals of Statistics}, 46\penalty0 (6A):\penalty0 2562--2592, 2018.

\bibitem[Muandet et~al.(2017)Muandet, Fukumizu, Sriperumbudur, Sch{\"o}lkopf, et~al.]{muandet2017kernel}
K.~Muandet, K.~Fukumizu, B.~Sriperumbudur, B.~Sch{\"o}lkopf, et~al.
\newblock Kernel mean embedding of distributions: A review and beyond.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 10\penalty0 (1-2):\penalty0 1--141, 2017.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and Ng]{netzer2011reading}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Paulsen and Raghupathi(2016)]{paulsen2016introduction}
V.~I. Paulsen and M.~Raghupathi.
\newblock \emph{An introduction to the theory of reproducing kernel Hilbert spaces}, volume 152.
\newblock Cambridge university press, 2016.

\bibitem[Pronzato(2021)]{pronzato2021performance}
L.~Pronzato.
\newblock Performance analysis of greedy algorithms for minimising a maximum mean discrepancy.
\newblock \emph{arXiv preprint arXiv:2101.07564}, 2021.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry, A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Sajjadi et~al.(2016)Sajjadi, Javanmardi, and Tasdizen]{sajjadi2016regularization}
M.~Sajjadi, M.~Javanmardi, and T.~Tasdizen.
\newblock Regularization with stochastic transformations and perturbations for deep semi-supervised learning.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Schmutz et~al.(2022)Schmutz, Humbert, and Mattei]{schmutz2022don}
H.~Schmutz, O.~Humbert, and P.-A. Mattei.
\newblock Donâ€™t fear the unlabelled: safe semi-supervised learning via debiasing.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Sener and Savarese(2018)]{sener2018active}
O.~Sener and S.~Savarese.
\newblock Active learning for convolutional neural networks: A core-set approach.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Shalit et~al.(2017)Shalit, Johansson, and Sontag]{shalit2017estimating}
U.~Shalit, F.~D. Johansson, and D.~Sontag.
\newblock Estimating individual treatment effect: generalization bounds and algorithms.
\newblock In \emph{International conference on machine learning}, pages 3076--3085. PMLR, 2017.

\bibitem[Shao et~al.(2024)Shao, Zhang, Du, Li, Wu, Chen, Wu, and Chen]{shao2024comprehensive}
Q.~Shao, K.~Zhang, B.~Du, Z.~Li, Y.~Wu, Q.~Chen, J.~Wu, and J.~Chen.
\newblock Comprehensive subset selection for ct volume compression to improve pulmonary disease screening efficiency.
\newblock In \emph{Artificial Intelligence and Data Science for Healthcare: Bridging Data-Centric AI and People-Centric Healthcare}, 2024.

\bibitem[Sinha et~al.(2019)Sinha, Ebrahimi, and Darrell]{sinha2019variational}
S.~Sinha, S.~Ebrahimi, and T.~Darrell.
\newblock Variational adversarial active learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 5972--5981, 2019.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Carlini, Zhang, Zhang, Raffel, Cubuk, Kurakin, and Li]{sohn2020fixmatch}
K.~Sohn, D.~Berthelot, N.~Carlini, Z.~Zhang, H.~Zhang, C.~A. Raffel, E.~D. Cubuk, A.~Kurakin, and C.-L. Li.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and confidence.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 596--608, 2020.

\bibitem[Song et~al.(2009)Song, Huang, Smola, and Fukumizu]{song2009hilbert}
L.~Song, J.~Huang, A.~Smola, and K.~Fukumizu.
\newblock Hilbert space embeddings of conditional distributions with applications to dynamical systems.
\newblock In \emph{Proceedings of the 26th Annual International Conference on Machine Learning}, pages 961--968, 2009.

\bibitem[Song et~al.(2019)Song, Berthelot, and Rostamizadeh]{song2019combining}
S.~Song, D.~Berthelot, and A.~Rostamizadeh.
\newblock Combining mixmatch and active learning for better accuracy with fewer labels.
\newblock \emph{arXiv preprint arXiv:1912.00594}, 2019.

\bibitem[Sriperumbudur et~al.(2012)Sriperumbudur, Fukumizu, Gretton, Sch{\"o}lkopf, and Lanckriet]{sriperumbudur2012empirical}
B.~K. Sriperumbudur, K.~Fukumizu, A.~Gretton, B.~Sch{\"o}lkopf, and G.~R. Lanckriet.
\newblock On the empirical estimation of integral probability metrics.
\newblock 2012.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and Hinton]{sutskever2013importance}
I.~Sutskever, J.~Martens, G.~Dahl, and G.~Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, pages 1139--1147. PMLR, 2013.

\bibitem[Tarvainen and Valpola(2017)]{tarvainen2017mean}
A.~Tarvainen and H.~Valpola.
\newblock Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Thompson(2012)]{thompson2012sampling}
S.~K. Thompson.
\newblock \emph{Sampling}, volume 755.
\newblock John Wiley \& Sons, 2012.

\bibitem[Tong and Koller(2001)]{tong2001support}
S.~Tong and D.~Koller.
\newblock Support vector machine active learning with applications to text classification.
\newblock \emph{Journal of machine learning research}, 2\penalty0 (Nov):\penalty0 45--66, 2001.

\bibitem[Wainwright(2019)]{wainwright2019high}
M.~J. Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint}, volume~48.
\newblock Cambridge university press, 2019.

\bibitem[Wang et~al.(2016)Wang, Zhang, Li, Zhang, and Lin]{wang2016cost}
K.~Wang, D.~Zhang, Y.~Li, R.~Zhang, and L.~Lin.
\newblock Cost-effective active learning for deep image classification.
\newblock \emph{IEEE Transactions on Circuits and Systems for Video Technology}, 27\penalty0 (12):\penalty0 2591--2600, 2016.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Lian, and Yu]{wang2022unsupervised}
X.~Wang, L.~Lian, and S.~X. Yu.
\newblock Unsupervised selective labeling for more effective semi-supervised learning.
\newblock In \emph{European Conference on Computer Vision}, pages 427--445. Springer, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Wu, Lian, and Yu]{wang2022debiased}
X.~Wang, Z.~Wu, L.~Lian, and S.~X. Yu.
\newblock Debiased learning from naturally imbalanced pseudo-labels.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 14647--14657, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{c}})Wang, Chen, Fan, Sun, Tao, Hou, Wang, Yang, Zhou, Guo, et~al.]{wang2022usb}
Y.~Wang, H.~Chen, Y.~Fan, W.~Sun, R.~Tao, W.~Hou, R.~Wang, L.~Yang, Z.~Zhou, L.-Z. Guo, et~al.
\newblock Usb: A unified semi-supervised learning benchmark for classification.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 3938--3961, 2022{\natexlab{c}}.

\bibitem[Wang et~al.(2022{\natexlab{d}})Wang, Chen, Heng, Hou, Fan, Wu, Wang, Savvides, Shinozaki, Raj, et~al.]{wang2022freematch}
Y.~Wang, H.~Chen, Q.~Heng, W.~Hou, Y.~Fan, Z.~Wu, J.~Wang, M.~Savvides, T.~Shinozaki, B.~Raj, et~al.
\newblock Freematch: Self-adaptive thresholding for semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:2205.07246}, 2022{\natexlab{d}}.

\bibitem[Wu et~al.(2023)Wu, Huo, Ren, and Zou]{wu2023optimal}
X.~Wu, Y.~Huo, H.~Ren, and C.~Zou.
\newblock Optimal subsampling via predictive inference.
\newblock \emph{Journal of the American Statistical Association}, \penalty0 (just-accepted):\penalty0 1--29, 2023.

\bibitem[Xie et~al.(2020{\natexlab{a}})Xie, Dai, Hovy, Luong, and Le]{xie2020unsupervised}
Q.~Xie, Z.~Dai, E.~Hovy, T.~Luong, and Q.~Le.
\newblock Unsupervised data augmentation for consistency training.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6256--6268, 2020{\natexlab{a}}.

\bibitem[Xie et~al.(2020{\natexlab{b}})Xie, Luong, Hovy, and Le]{xie2020self}
Q.~Xie, M.-T. Luong, E.~Hovy, and Q.~V. Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10687--10698, 2020{\natexlab{b}}.

\bibitem[Xie et~al.(2023)Xie, Lu, Yan, Yang, Tomizuka, and Zhan]{xie2023active}
Y.~Xie, H.~Lu, J.~Yan, X.~Yang, M.~Tomizuka, and W.~Zhan.
\newblock Active finetuning: Exploiting annotation budget in the pretraining-finetuning paradigm.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 23715--23724, 2023.

\bibitem[Xu et~al.(2024)Xu, Zhang, Zhang, Wu, Feng, and Chen]{xu2024predictive}
Y.~Xu, D.~Zhang, S.~Zhang, S.~Wu, Z.~Feng, and G.~Chen.
\newblock Predictive and near-optimal sampling for view materialization in video databases.
\newblock \emph{Proceedings of the ACM on Management of Data}, 2\penalty0 (1):\penalty0 1--27, 2024.

\bibitem[Yang et~al.(2023)Yang, Zhao, Qi, Qiao, Shi, and Zhao]{yang2023shrinking}
L.~Yang, Z.~Zhao, L.~Qi, Y.~Qiao, Y.~Shi, and H.~Zhao.
\newblock Shrinking class space for enhanced certainty in semi-supervised learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 16187--16196, 2023.

\bibitem[Yoo and Kweon(2019)]{yoo2019learning}
D.~Yoo and I.~S. Kweon.
\newblock Learning loss for active learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 93--102, 2019.

\bibitem[Yu et~al.(2022)Yu, Wang, Ai, and Zhang]{yu2022optimal}
J.~Yu, H.~Wang, M.~Ai, and H.~Zhang.
\newblock Optimal distributed subsampling for maximum quasi-likelihood estimators with massive data.
\newblock \emph{Journal of the American Statistical Association}, 117\penalty0 (537):\penalty0 265--276, 2022.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock In \emph{Procedings of the British Machine Vision Conference 2016}. British Machine Vision Association, 2016.

\bibitem[Zhang et~al.(2021)Zhang, Wang, Hou, Wu, Wang, Okumura, and Shinozaki]{zhang2021flexmatch}
B.~Zhang, Y.~Wang, W.~Hou, H.~Wu, J.~Wang, M.~Okumura, and T.~Shinozaki.
\newblock Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 18408--18419, 2021.

\bibitem[Zhang and Chen(2021)]{zhang2020concentration}
H.~Zhang and S.~X. Chen.
\newblock Concentration inequalities for statistical inference.
\newblock \emph{Communications in Mathematical Research}, 37\penalty0 (1):\penalty0 1--85, 2021.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Meng, Yu, Zhang, Zhong, and Ma]{zhang2023optimal}
J.~Zhang, C.~Meng, J.~Yu, M.~Zhang, W.~Zhong, and P.~Ma.
\newblock An optimal transport approach for selecting a representative subsample with application in efficient kernel density estimation.
\newblock \emph{Journal of Computational and Graphical Statistics}, 32\penalty0 (1):\penalty0 329--339, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Zhou, Zhou, and Zhang]{zhang2023model}
M.~Zhang, Y.~Zhou, Z.~Zhou, and A.~Zhang.
\newblock Model-free subsampling method based on uniform designs.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 2023{\natexlab{b}}.

\end{thebibliography}
