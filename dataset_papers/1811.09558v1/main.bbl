\begin{thebibliography}{10}

\bibitem{anderson1958introduction}
Theodore~Wilbur Anderson.
\newblock {\em An Introduction to Multivariate Statistical Analysis}.
\newblock Wiley New York, 1958.

\bibitem{auer2002b}
Peter Auer.
\newblock Using confidence bounds for exploitation-exploration tradeoffs.
\newblock {\em JMLR}, 3:397--422, 2002.

\bibitem{bardenet2013collaborative}
R{\'e}mi Bardenet, M{\'a}ty{\'a}s Brendel, Bal{\'a}zs K{\'e}gl, and Michele
  Sebag.
\newblock Collaborative hyperparameter tuning.
\newblock In {\em ICML}, 2013.

\bibitem{Baxter1996}
J~Baxter.
\newblock {A Bayesian/information theoretic model of bias learning}.
\newblock In {\em COLT}, New York, New York, USA, 1996.

\bibitem{bogunovic2016truncated}
Ilija Bogunovic, Jonathan Scarlett, Andreas Krause, and Volkan Cevher.
\newblock Truncated variance reduction: A unified approach to bayesian
  optimization and level-set estimation.
\newblock In {\em NIPS}, 2016.

\bibitem{brazdil1994characterizing}
Pavel Brazdil, Jo{\=a}o Gama, and Bob Henery.
\newblock Characterizing the applicability of classification algorithms using
  meta-level learning.
\newblock In {\em ECML}, 1994.

\bibitem{candes2009exact}
Emmanuel~J Cand{\`e}s and Benjamin Recht.
\newblock Exact matrix completion via convex optimization.
\newblock {\em Foundations of Computational mathematics}, 9(6):717, 2009.

\bibitem{chen2016learning}
Yutian Chen, Matthew~W Hoffman, Sergio~G{\'o}mez Colmenarejo, Misha Denil,
  Timothy~P Lillicrap, Matt Botvinick, and Nando de~Freitas.
\newblock Learning to learn without gradient descent by gradient descent.
\newblock In {\em ICML}, 2017.

\bibitem{CullyNature2015}
A.~Cully, J.~Clune, D.~Tarapore, and J.~Mouret.
\newblock Robots that adapt like animals.
\newblock {\em Nature}, 2015.

\bibitem{openrave}
R.~Diankov.
\newblock {\em Automated Construction of Robotic Manipulation Programs}.
\newblock PhD thesis, CMU Robotics Institute, August 2010.

\bibitem{duvenaud2011additive}
David~K Duvenaud, Hannes Nickisch, and Carl~E Rasmussen.
\newblock Additive {G}aussian processes.
\newblock In {\em NIPS}, 2011.

\bibitem{Eaton07}
M.~L. Eaton.
\newblock {\em Multivariate Statistics: A Vector Space Approach}.
\newblock Beachwood, Ohio, USA: Institute of Mathematical Statistics, 2007.

\bibitem{efron2017bayes}
Bradley Efron.
\newblock Bayes, oracle {B}ayes, and empirical {B}ayes.
\newblock 2017.

\bibitem{feurer2015efficient}
Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel
  Blum, and Frank Hutter.
\newblock Efficient and robust automated machine learning.
\newblock In {\em NIPS}, 2015.

\bibitem{feurer2018scalable}
Matthias Feurer, Benjamin Letham, and Eytan Bakshy.
\newblock Scalable meta-learning for {B}ayesian optimization.
\newblock {\em arXiv preprint arXiv:1802.02219}, 2018.

\bibitem{Feurer}
Matthias Feurer, Jost Springenberg, and Frank Hutter.
\newblock Initializing {B}ayesian hyperparameter optimization via
  meta-learning.
\newblock In {\em AAAI}, 2015.

\bibitem{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em ICML}, 2016.

\bibitem{Golovin2017}
Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John~Elliot
  Karro, and D.~Sculley.
\newblock Google vizier: A service for black-box optimization.
\newblock In {\em KDD}, 2017.

\bibitem{hennig2012}
Philipp Hennig and Christian~J Schuler.
\newblock Entropy search for information-efficient global optimization.
\newblock {\em JMLR}, 13:1809--1837, 2012.

\bibitem{hernandez2014predictive}
Jos{\'e}~Miguel Hern{\'a}ndez-Lobato, Matthew~W Hoffman, and Zoubin Ghahramani.
\newblock Predictive entropy search for efficient global optimization of
  black-box functions.
\newblock In {\em NIPS}, 2014.

\bibitem{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{igel2005no}
Christian Igel and Marc Toussaint.
\newblock A no-free-lunch theorem for non-uniform distributions of target
  functions.
\newblock {\em Journal of Mathematical Modelling and Algorithms},
  3(4):313--322, 2005.

\bibitem{kandasamy2018neural}
Kirthevasan Kandasamy, Willie Neiswanger, Jeff Schneider, Barnabas Poczos, and
  Eric Xing.
\newblock Neural architecture search with {B}ayesian optimisation and optimal
  transport.
\newblock {\em arXiv preprint arXiv:1802.07191}, 2018.

\bibitem{kandasamy2015high}
Kirthevasan Kandasamy, Jeff Schneider, and Barnabas Poczos.
\newblock High dimensional {B}ayesian optimisation and bandits via additive
  models.
\newblock In {\em ICML}, 2015.

\bibitem{kawaguchi2017deep}
Kenji Kawaguchi, Bo~Xie, Vikas Verma, and Le~Song.
\newblock Deep semi-random features for nonlinear function approximation.
\newblock In {\em AAAI}, 2017.

\bibitem{keener2011theoretical}
Robert~W Keener.
\newblock {\em Theoretical Statistics: Topics for a Core Course}.
\newblock Springer, 2011.

\bibitem{kim2017learning}
Beomjoon Kim, Leslie~Pack Kaelbling, and Tom{\'a}s Lozano-P{\'e}rez.
\newblock Learning to guide task and motion planning using score-space
  representation.
\newblock In {\em ICRA}, 2017.

\bibitem{krause2011contextual}
Andreas Krause and Cheng~S Ong.
\newblock Contextual {G}aussian process bandit optimization.
\newblock In {\em NIPS}, 2011.

\bibitem{kushner1964}
Harold~J Kushner.
\newblock A new method of locating the maximum point of an arbitrary multipeak
  curve in the presence of noise.
\newblock {\em Journal of Fluids Engineering}, 86(1):97--106, 1964.

\bibitem{lakshminarayanan2016simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In {\em NIPS}, 2017.

\bibitem{laurent2000adaptive}
Beatrice Laurent and Pascal Massart.
\newblock Adaptive estimation of a quadratic functional by model selection.
\newblock {\em Annals of Statistics}, pages 1302--1338, 2000.

\bibitem{lavalle2000rapidly}
Steven~M LaValle and James~J Kuffner~Jr.
\newblock Rapidly-exploring random trees: Progress and prospects.
\newblock In {\em Workshop on the Algorithmic Foundations of Robotics (WAFR)},
  2000.

\bibitem{li2016hyperband}
Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet
  Talwalkar.
\newblock Hyperband: A novel bandit-based approach to hyperparameter
  optimization.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2016.

\bibitem{lounici2014high}
Karim Lounici et~al.
\newblock High-dimensional covariance matrix estimation with missing
  observations.
\newblock {\em Bernoulli}, 20(3):1029--1058, 2014.

\bibitem{malkomestowards}
Gustavo Malkomes and Roman Garnett.
\newblock Towards automated {B}ayesian optimization.
\newblock In {\em ICML AutoML Workshop}, 2017.

\bibitem{malkomes2016bayesian}
Gustavo Malkomes, Charles Schaff, and Roman Garnett.
\newblock Bayesian optimization for automated model selection.
\newblock In {\em NIPS}, 2016.

\bibitem{minka1997}
T~P Minka and R~W Picard.
\newblock Learning how to learn is learning with point sets.
\newblock Technical report, MIT Media Lab, 1997.

\bibitem{mockus1974}
J.~Mo{\u{c}}kus.
\newblock On {B}ayesian methods for seeking the extremum.
\newblock In {\em Optimization Techniques IFIP Technical Conference}, 1974.

\bibitem{neal96}
R.M. Neal.
\newblock {\em {B}ayesian Learning for Neural Networks}.
\newblock Lecture Notes in Statistics 118. Springer, 1996.

\bibitem{petrone2014bayes}
Sonia Petrone, Judith Rousseau, and Catia Scricciolo.
\newblock Bayes and empirical {B}ayes: do they merge?
\newblock {\em Biometrika}, 101(2):285--302, 2014.

\bibitem{platt2002learning}
John~C Platt, Christopher~JC Burges, Steven Swenson, Christopher Weare, and
  Alice Zheng.
\newblock Learning a {G}aussian process prior for automatically generating
  music playlists.
\newblock In {\em NIPS}, 2002.

\bibitem{poloczek2017multi}
Matthias Poloczek, Jialei Wang, and Peter Frazier.
\newblock Multi-information source optimization.
\newblock In {\em NIPS}, 2017.

\bibitem{poloczek2016warm}
Matthias Poloczek, Jialei Wang, and Peter~I Frazier.
\newblock Warm starting {B}ayesian optimization.
\newblock In {\em Winter Simulation Conference (WSC)}. IEEE, 2016.

\bibitem{rasmussen2006gaussian}
Carl~Edward Rasmussen and Christopher~KI Williams.
\newblock {G}aussian processes for machine learning.
\newblock {\em The MIT Press}, 2006.

\bibitem{robbins1956empirical}
Herbert Robbins.
\newblock An empirical {B}ayes approach to statistics.
\newblock In {\em Third Berkeley Symp. Math. Statist. Probab.}, 1956.

\bibitem{UrgenSchmidhuber1995}
J~{Schmidhuber}.
\newblock On learning how to learn learning strategies.
\newblock Technical report, FKI-198-94 (revised), 1995.

\bibitem{shilton2017regret}
Alistair Shilton, Sunil Gupta, Santu Rana, and Svetha Venkatesh.
\newblock Regret bounds for transfer learning in {B}ayesian optimisation.
\newblock In {\em AISTATS}, 2017.

\bibitem{slotani1964tolerance}
Mlnoru Slotani.
\newblock Tolerance regions for a multivariate normal population.
\newblock {\em Annals of the Institute of Statistical Mathematics},
  16(1):135--153, 1964.

\bibitem{sniekers2015adaptive}
Suzanne Sniekers, Aad van~der Vaart, et~al.
\newblock Adaptive {B}ayesian credible sets in regression with a {G}aussian
  process prior.
\newblock {\em Electronic Journal of Statistics}, 9(2):2475--2527, 2015.

\bibitem{snoek2012practical}
Jasper Snoek, Hugo Larochelle, and Ryan~P Adams.
\newblock Practical {B}ayesian optimization of machine learning algorithms.
\newblock In {\em NIPS}, 2012.

\bibitem{srinivas2009gaussian}
Niranjan Srinivas, Andreas Krause, Sham~M Kakade, and Matthias Seeger.
\newblock {G}aussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock In {\em ICML}, 2010.

\bibitem{swersky2013multi}
Kevin Swersky, Jasper Snoek, and Ryan~P Adams.
\newblock Multi-task {B}ayesian optimization.
\newblock In {\em NIPS}, 2013.

\bibitem{wang2017maxvalue}
Zi~Wang and Stefanie Jegelka.
\newblock Max-value entropy search for efficient {B}ayesian optimization.
\newblock In {\em ICML}, 2017.

\bibitem{wang2014theoretical}
Ziyu Wang and Nando de~Freitas.
\newblock Theoretical analysis of {B}ayesian optimisation with unknown
  {G}aussian process hyper-parameters.
\newblock In {\em NIPS workshop on {B}ayesian Optimization}, 2014.

\bibitem{wolfram}
Eric~W. Weisstein.
\newblock Square root inequality.
\newblock MathWorld--A Wolfram Web Resource.
  \url{http://mathworld.wolfram.com/SquareRootInequality.html}, 1999-2018.

\bibitem{wolpert1997no}
David~H Wolpert and William~G Macready.
\newblock No free lunch theorems for optimization.
\newblock {\em IEEE transactions on evolutionary computation}, 1(1):67--82,
  1997.

\bibitem{yogatama2014efficient}
Dani Yogatama and Gideon Mann.
\newblock Efficient transfer learning method for automatic hyperparameter
  tuning.
\newblock In {\em AISTATS}, 2014.

\end{thebibliography}
