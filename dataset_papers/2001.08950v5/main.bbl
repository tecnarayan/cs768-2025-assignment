\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Correia \bgroup \em et al.\egroup
  }{2019}]{Correia_2019}
Gonçalo~M. Correia, Vlad Niculae, and André F.~T. Martins.
\newblock Adaptively sparse transformers.
\newblock {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, 2019.

\bibitem[\protect\citeauthoryear{Denil \bgroup \em et al.\egroup }{2013}]{svd}
Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, and Nando
  De~Freitas.
\newblock Predicting parameters in deep learning.
\newblock In {\em NIPS}, 2013.

\bibitem[\protect\citeauthoryear{Devlin \bgroup \em et al.\egroup
  }{2019}]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL}, 2019.

\bibitem[\protect\citeauthoryear{Gong \bgroup \em et al.\egroup
  }{2014}]{quantize}
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir Bourdev.
\newblock Compressing deep convolutional networks using vector quantization.
\newblock {\em arXiv preprint arXiv:1412.6115}, 2014.

\bibitem[\protect\citeauthoryear{Han \bgroup \em et al.\egroup }{2015}]{prune}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In {\em NIPS}, 2015.

\bibitem[\protect\citeauthoryear{He \bgroup \em et al.\egroup
  }{2017}]{he2017channel}
Yihui He, Xiangyu Zhang, and Jian Sun.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In {\em ICCV}, 2017.

\bibitem[\protect\citeauthoryear{Hinton \bgroup \em et al.\egroup
  }{2015}]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arxiv preprint: 1503.02531}, 2015.

\bibitem[\protect\citeauthoryear{Kim \bgroup \em et al.\egroup
  }{2015}]{kim2015compression}
Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu~Yang, and Dongjun
  Shin.
\newblock Compression of deep convolutional neural networks for fast and low
  power mobile applications.
\newblock {\em arxiv preprint: 1511.06530}, 2015.

\bibitem[\protect\citeauthoryear{Lai \bgroup \em et al.\egroup
  }{2017}]{lai2017large}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.
\newblock Race: Large-scale reading comprehension dataset from examinations.
\newblock {\em arXiv preprint arXiv:1704.04683}, 2017.

\bibitem[\protect\citeauthoryear{Lan \bgroup \em et al.\egroup }{2019}]{albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock {ALBERT}: A lite {BERT} for self-supervised learning of language
  representations.
\newblock {\em arXiv preprint arXiv:1909.11942}, 2019.

\bibitem[\protect\citeauthoryear{Lee \bgroup \em et al.\egroup
  }{2019}]{biomine}
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan~Ho So,
  and Jaewoo Kang.
\newblock {BioBERT}: {P}re-trained biomedical language representation model for
  biomedical text mining.
\newblock {\em Bioinformatics}, 2019.

\bibitem[\protect\citeauthoryear{Liu and Lapata}{2019}]{summarization}
Yang Liu and Mirella Lapata.
\newblock Text summarization with pretrained encoders.
\newblock {\em arXiv preprint arXiv:1908.08345}, 2019.

\bibitem[\protect\citeauthoryear{Liu \bgroup \em et al.\egroup
  }{2019}]{liu2019improving}
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
\newblock Improving multi-task deep neural networks via knowledge distillation
  for natural language understanding.
\newblock {\em arxiv preprint: 1904.09482}, 2019.

\bibitem[\protect\citeauthoryear{Maas \bgroup \em et al.\egroup
  }{2011}]{maas-EtAlACL-HLT2011}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In {\em ACL}, 2011.

\bibitem[\protect\citeauthoryear{Martins and
  Astudillo}{2016}]{martins2016softmax}
André F.~T. Martins and Ramón~Fernandez Astudillo.
\newblock From softmax to sparsemax: A sparse model of attention and
  multi-label classification.
\newblock 2016.

\bibitem[\protect\citeauthoryear{McCarley}{2019}]{mccarley2019pruning}
J.~S. McCarley.
\newblock Pruning a {BERT}-based question answering model.
\newblock {\em arxiv preprint: 1910.06360}, 2019.

\bibitem[\protect\citeauthoryear{Michel \bgroup \em et al.\egroup
  }{2019a}]{Head-Prune-Code}
Paul Michel, Omer Levy, and Graham Neubig, 2019.

\bibitem[\protect\citeauthoryear{Michel \bgroup \em et al.\egroup
  }{2019b}]{head-prune}
Paul Michel, Omer Levy, and Graham Neubig.
\newblock Are sixteen heads really better than one?
\newblock {\em arXiv preprint arXiv:1905.10650}, 2019.

\bibitem[\protect\citeauthoryear{Molchanov \bgroup \em et al.\egroup
  }{2016}]{molchanov2016pruning}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock {\em arxiv preprint: 1611.06440}, 2016.

\bibitem[\protect\citeauthoryear{Molchanov \bgroup \em et al.\egroup
  }{2017}]{molchanov2017variational}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock {\em arxiv preprint: 1701.05369}, 2017.

\bibitem[\protect\citeauthoryear{Peters \bgroup \em et al.\egroup
  }{2019}]{Peters_2019}
Ben Peters, Vlad Niculae, and André F.~T. Martins.
\newblock Sparse sequence-to-sequence models.
\newblock {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, 2019.

\bibitem[\protect\citeauthoryear{Rajpurkar \bgroup \em et al.\egroup
  }{2016}]{qnli}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQuAD}: 100,000+ questions for machine comprehension of text.
\newblock {\em arXiv preprint arXiv:1606.05250}, 2016.

\bibitem[\protect\citeauthoryear{Sanh \bgroup \em et al.\egroup
  }{2019a}]{DistilBERT-Code}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf, 2019.

\bibitem[\protect\citeauthoryear{Sanh \bgroup \em et al.\egroup
  }{2019b}]{distil-bert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock {DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper
  and lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[\protect\citeauthoryear{Sau and Balasubramanian}{2016}]{sau2016deep}
Bharat~Bhusan Sau and Vineeth~N. Balasubramanian.
\newblock Deep model compression: Distilling knowledge from noisy teachers.
\newblock {\em arxiv preprint: 1610.09650}, 2016.

\bibitem[\protect\citeauthoryear{Shen \bgroup \em et al.\egroup
  }{2019}]{shen2019qbert}
Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
  Michael~W. Mahoney, and Kurt Keutzer.
\newblock {Q-BERT}: Hessian based ultra low precision quantization of {BERT}.
\newblock {\em arxiv preprint:1909.05840}, 2019.

\bibitem[\protect\citeauthoryear{Sun \bgroup \em et al.\egroup
  }{2019a}]{BERT-PKD-Code}
Siqi Sun, Yu~Cheng, Zhe Gan, and Jingjing Liu, 2019.

\bibitem[\protect\citeauthoryear{Sun \bgroup \em et al.\egroup
  }{2019b}]{bert-pkd}
Siqi Sun, Yu~Cheng, Zhe Gan, and Jingjing Liu.
\newblock Patient knowledge distillation for {BERT} model compression.
\newblock {\em arXiv preprint arXiv:1908.09355}, 2019.

\bibitem[\protect\citeauthoryear{Wang \bgroup \em et al.\egroup }{2019a}]{glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In {\em ICLR}, 2019.

\bibitem[\protect\citeauthoryear{Wang \bgroup \em et al.\egroup
  }{2019b}]{wang2019structured}
Ziheng Wang, Jeremy Wohlwend, and Tao Lei.
\newblock Structured pruning of large language models.
\newblock {\em arxiv preprint: 1910.04732}, 2019.

\bibitem[\protect\citeauthoryear{Zafrir \bgroup \em et al.\egroup
  }{2019}]{zafrir2019q8bert}
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
\newblock {Q8BERT}: Quantized 8bit {BERT}.
\newblock {\em arxiv preprint:1910.06188}, 2019.

\bibitem[\protect\citeauthoryear{Zhao \bgroup \em et al.\egroup
  }{2019}]{sparse-transformer}
Guangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xuancheng Ren, Qi~Su, and Xu~Sun.
\newblock Explicit sparse transformer: Concentrated attention through explicit
  selection.
\newblock {\em arXiv,1912.11637}, 2019.

\end{thebibliography}
