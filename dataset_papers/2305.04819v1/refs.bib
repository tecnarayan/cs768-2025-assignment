@inproceedings{trpo,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

@article{ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{
kuba2022trust,
title={Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning},
author={Jakub Grudzien Kuba and Ruiqing Chen and Muning Wen and Ying Wen and Fanglei Sun and Jun Wang and Yaodong Yang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=EcGGFkNTxdJ}
}

@incollection{littman1994markov,
  title={Markov games as a framework for multi-agent reinforcement learning},
  author={Littman, Michael L},
  booktitle={Machine learning proceedings 1994},
  pages={157--163},
  year={1994},
  publisher={Elsevier}
}

@article{agarwal2021theory,
  title={On the theory of policy gradient methods: {O}ptimality, approximation, and distribution shift},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={98},
  pages={1--76},
  year={2021},
  publisher={Microtome Publishing}
}

@article{liu2019neural,
  title={Neural trust region/proximal policy optimization attains globally optimal policy},
  author={Liu, Boyi and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{cai2020provably,
  title={Provably efficient exploration in policy optimization},
  author={Cai, Qi and Yang, Zhuoran and Jin, Chi and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={1283--1294},
  year={2020},
  organization={PMLR}
}

@inproceedings{branavan2009reinforcement,
author = {Branavan, S. R. K. and Chen, Harr and Zettlemoyer, Luke S. and Barzilay, Regina},
title = {Reinforcement Learning for Mapping Instructions to Actions},
year = {2009},
publisher = {Association for Computational Linguistics},
address = {USA},
booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1 - Volume 1},
pages = {82â€“90},
numpages = {9},
location = {Suntec, Singapore},
series = {ACL '09}
}

@inproceedings{gimpel2010softmax,
  title={Softmax-margin {CRF}s: {T}raining log-linear models with cost functions},
  author={Gimpel, Kevin and Smith, Noah A},
  booktitle={Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={733--736},
  year={2010}
}

@inproceedings{heess2013actor,
  title={Actor-critic reinforcement learning with energy-based policies},
  author={Heess, Nicolas and Silver, David and Teh, Yee Whye},
  booktitle={European Workshop on Reinforcement Learning},
  pages={45--58},
  year={2013},
  organization={PMLR}
}

@article{xie2021bellman,
  title={Bellman-consistent pessimism for offline reinforcement learning},
  author={Xie, Tengyang and Cheng, Ching-An and Jiang, Nan and Mineiro, Paul and Agarwal, Alekh},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={6683--6694},
  year={2021}
}

@article{silver2016mastering,
  title={Mastering the game of {G}o with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{silver2017mastering,
  title={Mastering the game of {G}o without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{guo2016deep,
  title={Deep learning for reward design to improve {M}onte {C}arlo tree search in {A}tari games},
  author={Guo, Xiaoxiao and Singh, Satinder and Lewis, Richard and Lee, Honglak},
  journal={arXiv preprint arXiv:1604.07095},
  year={2016}
}

@inproceedings{tian2019elf,
  title={Elf opengo: An analysis and open reimplementation of {A}lpha{Z}ero},
  author={Tian, Yuandong and Ma, Jerry and Gong, Qucheng and Sengupta, Shubho and Chen, Zhuoyuan and Pinkerton, James and Zitnick, Larry},
  booktitle={International Conference on Machine Learning},
  pages={6244--6253},
  year={2019},
  organization={PMLR}
}

@article{shapley1953stochastic,
  title={Stochastic games},
  author={Shapley, Lloyd S},
  journal={Proceedings of the national academy of sciences},
  volume={39},
  number={10},
  pages={1095--1100},
  year={1953},
  publisher={National Acad Sciences}
}

@phdthesis{patek1997stochastic,
  title={Stochastic and shortest path games: theory and algorithms},
  author={Patek, Stephen David},
  year={1997},
  school={Massachusetts Institute of Technology}
}

@inproceedings{bai2020provable,
  title={Provable self-play algorithms for competitive reinforcement learning},
  author={Bai, Yu and Jin, Chi},
  booktitle={International Conference on Machine Learning},
  pages={551--560},
  year={2020},
  organization={PMLR}
}

@article{bai2020near,
  title={Near-Optimal Reinforcement Learning with Self-Play},
  author={Bai, Yu and Jin, Chi and Yu, Tiancheng},
  journal={arXiv preprint arXiv:2006.12007},
  year={2020}
}

@inproceedings{perolat2015approximate,
  title={Approximate dynamic programming for two-player zero-sum {M}arkov games},
  author={Perolat, Julien and Scherrer, Bruno and Piot, Bilal and Pietquin, Olivier},
  booktitle={International Conference on Machine Learning},
  pages={1321--1329},
  year={2015}
}

@article{hernandez2019survey,
  title={A survey and critique of multiagent deep reinforcement learning},
  author={Hernandez-Leal, Pablo and Kartal, Bilal and Taylor, Matthew E},
  journal={Autonomous Agents and Multi-Agent Systems},
  volume={33},
  number={6},
  pages={750--797},
  year={2019},
  publisher={Springer}
}

@article{brown2018superhuman,
  title={Superhuman {AI} for heads-up no-limit poker: Libratus beats top professionals},
  author={Brown, Noam and Sandholm, Tuomas},
  journal={Science},
  volume={359},
  number={6374},
  pages={418--424},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in {S}tar{C}raft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@article{lowe2017multi,
  title={Multi-agent actor-critic for mixed cooperative-competitive environments},
  author={Lowe, Ryan and Wu, Yi I and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{foerster2018counterfactual,
  title={Counterfactual multi-agent policy gradients},
  author={Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  year={2018}
}

@article{kraemer2016multi,
  title={Multi-agent reinforcement learning as a rehearsal for decentralized planning},
  author={Kraemer, Landon and Banerjee, Bikramjit},
  journal={Neurocomputing},
  volume={190},
  pages={82--94},
  year={2016},
  publisher={Elsevier}
}

@article{lockhart2019computing,
  title={Computing approximate equilibria in sequential adversarial games by exploitability descent},
  author={Lockhart, Edward and Lanctot, Marc and P{\'e}rolat, Julien and Lespiau, Jean-Baptiste and Morrill, Dustin and Timbers, Finbarr and Tuyls, Karl},
  journal={arXiv preprint arXiv:1903.05614},
  year={2019}
}

@inproceedings{zhang2020bi,
  title={Bi-level actor-critic for multi-agent coordination},
  author={Zhang, Haifeng and Chen, Weizhe and Huang, Zeren and Li, Minne and Yang, Yaodong and Zhang, Weinan and Wang, Jun},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={7325--7332},
  year={2020}
}

@inproceedings{yang2018mean,
  title={Mean field multi-agent reinforcement learning},
  author={Yang, Yaodong and Luo, Rui and Li, Minne and Zhou, Ming and Zhang, Weinan and Wang, Jun},
  booktitle={International conference on machine learning},
  pages={5571--5580},
  year={2018},
  organization={PMLR}
}

@article{wen2019probabilistic,
  title={Probabilistic recursive reasoning for multi-agent reinforcement learning},
  author={Wen, Ying and Yang, Yaodong and Luo, Rui and Wang, Jun and Pan, Wei},
  journal={arXiv preprint arXiv:1901.09207},
  year={2019}
}

@inproceedings{ding2022independent,
  title={Independent policy gradient for large-scale {M}arkov potential games: Sharper rates, function approximation, and game-agnostic convergence},
  author={Ding, Dongsheng and Wei, Chen-Yu and Zhang, Kaiqing and Jovanovic, Mihailo},
  booktitle={International Conference on Machine Learning},
  pages={5166--5220},
  year={2022},
  organization={PMLR}
}

@inproceedings{cen2021fast,
 author = {Cen, Shicong and Wei, Yuting and Chi, Yuejie},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {27952--27964},
 publisher = {Curran Associates, Inc.},
 title = {Fast Policy Extragradient Methods for Competitive Games with Entropy Regularization},
 year = {2021}
}

@article{cen2022faster,
  title={Faster last-iterate convergence of policy optimization in zero-sum {M}arkov games},
  author={Cen, Shicong and Chi, Yuejie and Du, Simon S and Xiao, Lin},
  journal={arXiv preprint arXiv:2210.01050},
  year={2022}
}

@inproceedings{zhao2022provably,
  title={Provably Efficient Policy Optimization for Two-Player Zero-Sum {M}arkov Games},
  author={Zhao, Yulai and Tian, Yuandong and Lee, Jason and Du, Simon},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2736--2761},
  year={2022},
  organization={PMLR}
}

@article{daskalakis2020independent,
  title={Independent policy gradient methods for competitive reinforcement learning},
  author={Daskalakis, Constantinos and Foster, Dylan J and Golowich, Noah},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={5527--5540},
  year={2020}
}

@inproceedings{xie2020learning,
  title={Learning zero-sum simultaneous-move {M}arkov games using function approximation and correlated equilibrium},
  author={Xie, Qiaomin and Chen, Yudong and Wang, Zhaoran and Yang, Zhuoran},
  booktitle={Conference on learning theory},
  pages={3674--3682},
  year={2020},
  organization={PMLR}
}

@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={International conference on machine learning},
  pages={387--395},
  year={2014},
  organization={PMLR}
}

@article{de2020independent,
  title={Is independent learning all you need in the {S}tar{C}raft multi-agent challenge?},
  author={de Witt, Christian Schroeder and Gupta, Tarun and Makoviichuk, Denys and Makoviychuk, Viktor and Torr, Philip HS and Sun, Mingfei and Whiteson, Shimon},
  journal={arXiv preprint arXiv:2011.09533},
  year={2020}
}

@article{yu2021surprising,
  title={The surprising effectiveness of {PPO} in cooperative, multi-agent games},
  author={Yu, Chao and Velu, Akash and Vinitsky, Eugene and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  journal={arXiv preprint arXiv:2103.01955},
  year={2021}
}

@inproceedings{agarwal2020optimality,
  title={Optimality and approximation with policy gradient methods in {M}arkov decision processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  booktitle={Conference on Learning Theory},
  pages={64--66},
  year={2020},
  organization={PMLR}
}

@article{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@inproceedings{shani2020adaptive,
  title={Adaptive trust region policy optimization: Global convergence and faster rates for regularized {MDP}s},
  author={Shani, Lior and Efroni, Yonathan and Mannor, Shie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={5668--5675},
  year={2020}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@inproceedings{duan2016benchmarking,
  title={Benchmarking deep reinforcement learning for continuous control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle={International conference on machine learning},
  pages={1329--1338},
  year={2016},
  organization={PMLR}
}

@article{liu2020provably,
  title={Provably good batch off-policy reinforcement learning without great exploration},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1264--1274},
  year={2020}
}

@inproceedings{jin2021pessimism,
  title={Is pessimism provably efficient for offline {RL}?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={5084--5096},
  year={2021},
  organization={PMLR}
}

@article{uehara2021pessimistic,
  title={Pessimistic model-based offline {RL}: {P}ac bounds and posterior sampling under partial coverage},
  author={Uehara, Masatoshi and Sun, Wen},
  journal={arXiv e-prints},
  pages={arXiv--2107},
  year={2021}
}

@article{rashidinejad2021bridging,
  title={Bridging offline reinforcement learning and imitation learning: {A} tale of pessimism},
  author={Rashidinejad, Paria and Zhu, Banghua and Ma, Cong and Jiao, Jiantao and Russell, Stuart},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={11702--11716},
  year={2021}
}

@article{chen2022offline,
  title={Offline reinforcement learning under value and density-ratio realizability: the power of gaps},
  author={Chen, Jinglin and Jiang, Nan},
  journal={arXiv preprint arXiv:2203.13935},
  year={2022}
}

@inproceedings{zhan2022offline,
  title={Offline reinforcement learning with realizability and single-policy concentrability},
  author={Zhan, Wenhao and Huang, Baihe and Huang, Audrey and Jiang, Nan and Lee, Jason},
  booktitle={Conference on Learning Theory},
  pages={2730--2775},
  year={2022},
  organization={PMLR}
}

@inproceedings{munos2003,
author = {Munos, R\'{e}mi},
title = {Error Bounds for Approximate Policy Iteration},
year = {2003},
booktitle={International Conference on Machine Learning},
pages = {560â€“567}
}

@article{munos2008finite,
  title={Finite-Time Bounds for Fitted Value Iteration.},
  author={Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  journal={Journal of Machine Learning Research},
  volume={9},
  number={5},
  year={2008}
}

@article{farahmand2010error,
  title={Error propagation for approximate policy and value iteration},
  author={Farahmand, Amir-massoud and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  year={2010}
}


@inproceedings{chen2019information,
  title={Information-theoretic considerations in batch reinforcement learning},
  author={Chen, Jinglin and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={1042--1051},
  year={2019},
  organization={PMLR}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{fan2020theoretical,
  title={A theoretical analysis of deep {Q}-learning},
  author={Fan, Jianqing and Wang, Zhaoran and Xie, Yuchen and Yang, Zhuoran},
  booktitle={Learning for Dynamics and Control},
  pages={486--489},
  year={2020},
  organization={PMLR}
}

@inproceedings{xie2020q,
  title={Q* approximation schemes for batch reinforcement learning: {A} theoretical comparison},
  author={Xie, Tengyang and Jiang, Nan},
  booktitle={Conference on Uncertainty in Artificial Intelligence},
  pages={550--559},
  year={2020},
  organization={PMLR}
}

@book{shalev2014understanding,
  title={Understanding machine learning: {F}rom theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={In Proc. 19th International Conference on Machine Learning},
  year={2002},
  organization={Citeseer}
}

@article{zhang2021multi,
  title={Multi-agent reinforcement learning: {A} selective overview of theories and algorithms},
  author={Zhang, Kaiqing and Yang, Zhuoran and Ba{\c{s}}ar, Tamer},
  journal={Handbook of Reinforcement Learning and Control},
  pages={321--384},
  year={2021},
  publisher={Springer}
}

@article{li2020breaking,
  title={Breaking the sample size barrier in model-based reinforcement learning with a generative model},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12861--12872},
  year={2020}
}

@article{kearns2002near,
  title={Near-optimal reinforcement learning in polynomial time},
  author={Kearns, Michael and Singh, Satinder},
  journal={Machine learning},
  volume={49},
  number={2},
  pages={209--232},
  year={2002},
  publisher={Springer}
}

@book{kakade2003sample,
  title={On the sample complexity of reinforcement learning},
  author={Kakade, Sham Machandranath},
  year={2003},
  publisher={University of London, University College London (United Kingdom)}
}

@article{sidford2018near,
  title={Near-optimal time and sample complexities for solving discounted Markov decision process with a generative model},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin F and Ye, Yinyu},
  journal={arXiv preprint arXiv:1806.01492},
  year={2018}
}

@article{du2019good,
  title={Is a good representation sufficient for sample efficient reinforcement learning?},
  author={Du, Simon S and Kakade, Sham M and Wang, Ruosong and Yang, Lin F},
  journal={arXiv preprint arXiv:1910.03016},
  year={2019}
}


@book{nesterov2003introductory,
  title={Introductory lectures on convex optimization: {A} basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@article{antos2008learning,
  title={Learning near-optimal policies with {B}ellman-residual minimization based fitted policy iteration and a single sample path},
  author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Machine Learning},
  volume={71},
  number={1},
  pages={89--129},
  year={2008},
  publisher={Springer}
}

@inproceedings{thrun1993issues,
  title={Issues in using function approximation for reinforcement learning},
  author={Thrun, Sebastian and Schwartz, Anton},
  booktitle={Proceedings of the Fourth Connectionist Models Summer School},
  volume={255},
  pages={263},
  year={1993},
  organization={Hillsdale, NJ}
}

@inproceedings{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

@article{laskin2020reinforcement,
  title={Reinforcement learning with augmented data},
  author={Laskin, Misha and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={19884--19895},
  year={2020}
}

@article{hou2020off,
  title={Off-policy maximum entropy reinforcement learning: {S}oft actor-critic with advantage weighted mixture policy (sac-awmp)},
  author={Hou, Zhimin and Zhang, Kuangen and Wan, Yi and Li, Dongyu and Fu, Chenglong and Yu, Haoyong},
  journal={arXiv preprint arXiv:2002.02829},
  year={2020}
}

@article{lee2020predictive,
  title={Predictive information accelerates learning in {RL}},
  author={Lee, Kuang-Huei and Fischer, Ian and Liu, Anthony and Guo, Yijie and Lee, Honglak and Canny, John and Guadarrama, Sergio},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11890--11901},
  year={2020}
}

@article{moskovitz2021tactical,
  title={Tactical optimism and pessimism for deep reinforcement learning},
  author={Moskovitz, Ted and Parker-Holder, Jack and Pacchiano, Aldo and Arbel, Michael and Jordan, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12849--12863},
  year={2021}
}

@inproceedings{jin2017escape,
  title={How to escape saddle points efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  booktitle={International conference on machine learning},
  pages={1724--1732},
  year={2017},
  organization={PMLR}
}

@inproceedings{papoudakis1benchmarking,
  title={Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks},
  author={Papoudakis, Georgios and Christianos, Filippos and Sch{\"a}fer, Lukas and Albrecht, Stefano V},
  year={2021},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)}
}

@inproceedings{
leonardos2022global,
title={Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games},
author={Stefanos Leonardos and Will Overman and Ioannis Panageas and Georgios Piliouras},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=gfwON7rAm4}
}

@inproceedings{fox2022independent,
  title={Independent natural policy gradient always converges in Markov potential games},
  author={Fox, Roy and Mcaleer, Stephen M and Overman, Will and Panageas, Ioannis},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4414--4425},
  year={2022},
  organization={PMLR}
}