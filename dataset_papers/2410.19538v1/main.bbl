\begin{thebibliography}{100}

\bibitem{allen1977short}
J.~Allen.
\newblock Short term spectral analysis, synthesis, and modification by discrete {Fourier} transform.
\newblock {\em IEEE transactions on acoustics, speech, and signal processing}, 25(3):235--238, 1977.

\bibitem{allen1977unified}
J.~B. Allen and L.~R. Rabiner.
\newblock A unified approach to short-time {Fourier} analysis and synthesis.
\newblock {\em Proceedings of the IEEE}, 65(11):1558--1564, 1977.

\bibitem{amari1972learning}
S.-I. Amari.
\newblock Learning patterns and pattern sequences by self-organizing nets of threshold elements.
\newblock {\em IEEE Transactions on computers}, 100(11):1197--1206, 1972.

\bibitem{anderson1982reverse}
B.~D. Anderson.
\newblock Reverse-time diffusion equation models.
\newblock {\em Stochastic Processes and their Applications}, 12(3):313--326, 1982.

\bibitem{arjovsky2016unitary}
M.~Arjovsky, A.~Shah, and Y.~Bengio.
\newblock Unitary evolution recurrent neural networks.
\newblock In {\em International conference on machine learning}. PMLR, 2016.

\bibitem{azencot2020forecasting}
O.~Azencot, N.~B. Erichson, V.~Lin, and M.~Mahoney.
\newblock Forecasting sequential data using consistent {Koopman} autoencoders.
\newblock In {\em International Conference on Machine Learning}, pages 475--485. PMLR, 2020.

\bibitem{azencot2019consistent}
O.~Azencot, W.~Yin, and A.~Bertozzi.
\newblock Consistent dynamic mode decomposition.
\newblock {\em SIAM Journal on Applied Dynamical Systems}, 18(3):1565--1585, 2019.

\bibitem{bar2024lumiere}
O.~Bar{-}Tal, H.~Chefer, O.~Tov, C.~Herrmann, R.~Paiss, S.~Zada, A.~Ephrat, J.~Hur, Y.~Li, T.~Michaeli, O.~Wang, D.~Sun, T.~Dekel, and I.~Mosseri.
\newblock Lumiere: A space-time diffusion model for video generation.
\newblock {\em arXiv preprint arXiv:2401.12945}, 2024.

\bibitem{bengio1994learning}
Y.~Bengio, P.~Simard, and P.~Frasconi.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock {\em IEEE transactions on neural networks}, 1994.

\bibitem{berman2024generative}
N.~Berman, E.~Kosman, D.~Di~Castro, and O.~Azencot.
\newblock Generative modeling of graphs via joint diffusion of node and edge attributes.
\newblock {\em arXiv preprint arXiv:2402.04046}, 2024.

\bibitem{berman2023multifactor}
N.~Berman, I.~Naiman, and O.~Azencot.
\newblock Multifactor sequential disentanglement via structured {Koopman} autoencoders.
\newblock In {\em The Eleventh International Conference on Learning Representations, {ICLR}}, 2023.

\bibitem{brophy2019quick}
E.~Brophy, Z.~Wang, and T.~E. Ward.
\newblock Quick and easy time series generation with established image-based {GAN}s.
\newblock {\em arXiv preprint arXiv:1902.05624}, 2019.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{candanedo2017data}
L.~M. Candanedo, V.~Feldheim, and D.~Deramaix.
\newblock Data driven prediction models of energy use of appliances in a low-energy house.
\newblock {\em Energy and buildings}, 140:81--97, 2017.

\bibitem{chen2021wavegrad}
N.~Chen, Y.~Zhang, H.~Zen, R.~J. Weiss, M.~Norouzi, and W.~Chan.
\newblock Wavegrad: Estimating gradients for waveform generation.
\newblock In {\em 9th International Conference on Learning Representations, {ICLR}}, 2021.

\bibitem{chen2018neural}
R.~T. Chen, Y.~Rubanova, J.~Bettencourt, and D.~K. Duvenaud.
\newblock Neural ordinary differential equations.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{chen2022resgrad}
Z.~Chen, Y.~Wu, Y.~Leng, J.~Chen, H.~Liu, X.~Tan, Y.~Cui, K.~Wang, L.~He, S.~Zhao, J.~Bian, and D.~P. Mandic.
\newblock {ResGrad}: Residual denoising diffusion probabilistic models for text to speech.
\newblock {\em arXiv preprint arXiv:2212.14518}, 2022.

\bibitem{chung2014empirical}
J.~Chung, C.~Gulcehre, K.~Cho, and Y.~Bengio.
\newblock Empirical evaluation of gated recurrent neural networks on sequence modeling.
\newblock {\em arXiv preprint arXiv:1412.3555}, 2014.

\bibitem{coletta2024constrained}
A.~Coletta, S.~Gopalakrishnan, D.~Borrajo, and S.~Vyetrenko.
\newblock On the constrained time-series generation problem.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{de2019gru}
E.~De~Brouwer, J.~Simm, A.~Arany, and Y.~Moreau.
\newblock {GRU-ODE-Bayes}: Continuous modeling of sporadically-observed time series.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{deng2020modeling}
R.~Deng, B.~Chang, M.~A. Brubaker, G.~Mori, and A.~Lehrmann.
\newblock Modeling continuous stochastic processes with dynamic normalizing flows.
\newblock {\em Advances in Neural Information Processing Systems}, 33:7805--7815, 2020.

\bibitem{desai2021timevae}
A.~Desai, C.~Freeman, Z.~Wang, and I.~Beaver.
\newblock {TimeVAE}: a variational auto-encoder for multivariate time series generation.
\newblock {\em arXiv preprint arXiv:2111.08095}, 2021.

\bibitem{donahue2018adversarial}
C.~Donahue, J.~J. McAuley, and M.~S. Puckette.
\newblock Adversarial audio synthesis.
\newblock In {\em 7th International Conference on Learning Representations, {ICLR}}, 2019.

\bibitem{erichson2021lipschitz}
N.~B. Erichson, O.~Azencot, A.~Queiruga, L.~Hodgkinson, and M.~W. Mahoney.
\newblock Lipschitz recurrent neural networks.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{esteban2017real}
C.~Esteban, S.~L. Hyland, and G.~R{\"a}tsch.
\newblock Real-valued (medical) time series generation with recurrent conditional {GANs}.
\newblock {\em arXiv preprint arXiv:1706.02633}, 2017.

\bibitem{flandrin2004empirical}
P.~Flandrin, G.~Rilling, and P.~Goncalves.
\newblock Empirical mode decomposition as a filter bank.
\newblock {\em IEEE signal processing letters}, 11(2):112--114, 2004.

\bibitem{fortuin2020gp}
V.~Fortuin, D.~Baranchuk, G.~R{\"a}tsch, and S.~Mandt.
\newblock {GP-VAE}: Deep probabilistic time series imputation.
\newblock In {\em International conference on artificial intelligence and statistics}, pages 1651--1661. PMLR, 2020.

\bibitem{geng2024one}
Z.~Geng, A.~Pokle, and J.~Z. Kolter.
\newblock One-step diffusion distillation via deep equilibrium models.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{godahewa2021monash}
R.~Godahewa, C.~Bergmeir, G.~I. Webb, R.~J. Hyndman, and P.~Montero-Manso.
\newblock Monash time series forecasting archive.
\newblock {\em arXiv preprint arXiv:2105.06643}, 2021.

\bibitem{goel2022s}
K.~Goel, A.~Gu, C.~Donahue, and C.~R{\'e}.
\newblock Itâ€™s raw! audio generation with state-space models.
\newblock In {\em International Conference on Machine Learning}, pages 7616--7633. PMLR, 2022.

\bibitem{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair, A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem{lamb2016professor}
A.~Goyal, A.~Lamb, Y.~Zhang, S.~Zhang, A.~C. Courville, and Y.~Bengio.
\newblock Professor forcing: A new algorithm for training recurrent networks.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{graves2013generating}
A.~Graves.
\newblock Generating sequences with recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1308.0850}, 2013.

\bibitem{greenberg2004automatic}
S.~Greenberg, W.~A. Ainsworth, A.~N. Popper, R.~R. Fay, N.~Mogran, H.~Bourlard, and H.~Hermansky.
\newblock Automatic speech recognition: An auditory perspective.
\newblock {\em Speech processing in the auditory system}, pages 309--338, 2004.

\bibitem{griffin1984signal}
D.~Griffin and J.~Lim.
\newblock Signal estimation from modified short-time {Fourier} transform.
\newblock {\em IEEE Transactions on acoustics, speech, and signal processing}, 32(2):236--243, 1984.

\bibitem{gu2021efficiently}
A.~Gu, K.~Goel, and C.~Re.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{guo2022systematic}
X.~Guo and L.~Zhao.
\newblock A systematic survey on deep generative models for graph generation.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 45(5):5370--5390, 2022.

\bibitem{hatami2018classification}
N.~Hatami, Y.~Gavet, and J.~Debayle.
\newblock Classification of time-series images using deep convolutional neural networks.
\newblock In {\em Tenth international conference on machine vision (ICMV 2017)}, volume 10696, pages 242--249. SPIE, 2018.

\bibitem{hellermann2021leveraging}
J.~Hellermann and S.~Lessmann.
\newblock Leveraging image-based generative adversarial networks for time series generation.
\newblock {\em arXiv preprint arXiv:2112.08060}, 2021.

\bibitem{ho2020denoising}
J.~Ho, A.~Jain, and P.~Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in neural information processing systems}, 33:6840--6851, 2020.

\bibitem{ho2022jmlr}
J.~Ho, C.~Saharia, W.~Chan, D.~J. Fleet, M.~Norouzi, and T.~Salimans.
\newblock Cascaded diffusion models for high fidelity image generation.
\newblock {\em Journal of Machine Learning Research}, 23(47):1--33, 2022.

\bibitem{hochreiter1997long}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{hopfield1982neural}
J.~J. Hopfield.
\newblock Neural networks and physical systems with emergent collective computational abilities.
\newblock {\em Proceedings of the national academy of sciences}, 1982.

\bibitem{jeon2022gt}
J.~Jeon, J.~Kim, H.~Song, S.~Cho, and N.~Park.
\newblock {GT-GAN}: General purpose time series synthesis with generative adversarial networks.
\newblock {\em Advances in Neural Information Processing Systems}, 35:36999--37010, 2022.

\bibitem{karras2022elucidating}
T.~Karras, M.~Aittala, T.~Aila, and S.~Laine.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:26565--26577, 2022.

\bibitem{karras2019style}
T.~Karras, S.~Laine, and T.~Aila.
\newblock A style-based generator architecture for generative adversarial networks.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 4401--4410, 2019.

\bibitem{kaufman2023data}
I.~Kaufman and O.~Azencot.
\newblock Data representations' study of latent image manifolds.
\newblock In {\em International Conference on Machine Learning, {ICML}}, volume 202 of {\em Proceedings of Machine Learning Research}, pages 15928--15945. {PMLR}, 2023.

\bibitem{kaufman2024analyzing}
I.~Kaufman and O.~Azencot.
\newblock Analyzing deep transformer models for time series forecasting via manifold learning.
\newblock {\em Transactions on Machine Learning Research, {TMLR}}, 2024.

\bibitem{kaufman2024first}
I.~Kaufman and O.~Azencot.
\newblock First-order manifold data augmentation for regression learning.
\newblock In {\em Forty-first International Conference on Machine Learning, {ICML}}, 2024.

\bibitem{kidger2021neural}
P.~Kidger, J.~Foster, X.~Li, and T.~J. Lyons.
\newblock Neural {SDEs} as infinite-dimensional {GAN}s.
\newblock In {\em International conference on machine learning}, pages 5453--5463. PMLR, 2021.

\bibitem{kidger2020neural}
P.~Kidger, J.~Morrill, J.~Foster, and T.~Lyons.
\newblock Neural controlled differential equations for irregular time series.
\newblock {\em Advances in Neural Information Processing Systems}, 33:6696--6707, 2020.

\bibitem{kong2021diffwave}
Z.~Kong, W.~Ping, J.~Huang, K.~Zhao, and B.~Catanzaro.
\newblock {DiffWave: A} versatile diffusion model for audio synthesis.
\newblock In {\em 9th International Conference on Learning Representations, {ICLR}}, 2021.

\bibitem{lai2018modeling}
G.~Lai, W.-C. Chang, Y.~Yang, and H.~Liu.
\newblock Modeling long-and short-term temporal patterns with deep neural networks.
\newblock In {\em The 41st international ACM SIGIR conference on research \& development in information retrieval}, pages 95--104, 2018.

\bibitem{li2023causal}
H.~Li, S.~Yu, and J.~C. Pr{\'{\i}}ncipe.
\newblock Causal recurrent variational autoencoder for medical time series generation.
\newblock In {\em Thirty-Seventh {AAAI} Conference on Artificial Intelligence, {AAAI}}, pages 8562--8570, 2023.

\bibitem{li2021sp}
R.~Li, X.~Li, K.-H. Hui, and C.-W. Fu.
\newblock {SP-GAN}: Sphere-guided 3d shape generation and manipulation.
\newblock {\em ACM Transactions on Graphics (TOG)}, 40(4):1--12, 2021.

\bibitem{li2024time}
Z.~Li, S.~Li, and X.~Yan.
\newblock Time series as images: Vision transformer for irregularly sampled time series.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{lim2023tsgm}
H.~Lim, M.~Kim, S.~Park, J.~Lee, and N.~Park.
\newblock {TSGM}: Regular and irregular time-series generation using score-based generative models.
\newblock {\em openreview.com}, 2023.

\bibitem{liu2023audioldm}
H.~Liu, Z.~Chen, Y.~Yuan, X.~Mei, X.~Liu, D.~Mandic, W.~Wang, and M.~D. Plumbley.
\newblock {A}udio{LDM}: Text-to-audio generation with latent diffusion models.
\newblock In {\em Proceedings of the 40th International Conference on Machine Learning}, volume 202 of {\em Proceedings of Machine Learning Research}, pages 21450--21474. PMLR, 23--29 Jul 2023.

\bibitem{lucic2018gans}
M.~Lucic, K.~Kurach, M.~Michalski, S.~Gelly, and O.~Bousquet.
\newblock Are {GANs} created equal? a large-scale study.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{lugmayr2022repaint}
A.~Lugmayr, M.~Danelljan, A.~Romero, F.~Yu, R.~Timofte, and L.~Van~Gool.
\newblock Repaint: Inpainting using denoising diffusion probabilistic models.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 11461--11471, 2022.

\bibitem{luo2019accuair}
Z.~Luo, J.~Huang, K.~Hu, X.~Li, and P.~Zhang.
\newblock Accuair: Winning solution to air quality prediction for {KDD} cup 2018.
\newblock In {\em Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, pages 1842--1850, 2019.

\bibitem{mcculloch1943logical}
W.~S. McCulloch and W.~Pitts.
\newblock A logical calculus of the ideas immanent in nervous activity.
\newblock {\em The bulletin of mathematical biophysics}, 1943.

\bibitem{menne2015long}
M.~Menne, C.~Williams~Jr, and R.~Vose.
\newblock Long-term daily climate records from stations across the contiguous united states, 2015.

\bibitem{mogren2016continuous}
O.~Mogren.
\newblock {C-RNN-GAN}: Continuous recurrent neural networks with adversarial training.
\newblock {\em arXiv preprint arXiv:1611.09904}, 2016.

\bibitem{naiman2023}
I.~Naiman and O.~Azencot.
\newblock An operator theoretic approach for analyzing sequence neural networks.
\newblock In {\em Thirty-Seventh {AAAI} Conference on Artificial Intelligence, {AAAI}}, pages 9268--9276. {AAAI} Press, 2023.

\bibitem{naiman2024generative}
I.~Naiman, N.~B. Erichson, P.~Ren, M.~W. Mahoney, and O.~Azencot.
\newblock Generative modeling of regular and irregular time series data via {Koopman VAE}s.
\newblock In {\em The Twelfth International Conference on Learning Representations, {ICLR}}, 2024.

\bibitem{narasimhan2024time}
S.~S. Narasimhan, S.~Agarwal, O.~Akcin, S.~Sanghavi, and S.~P. Chinchali.
\newblock Time weaver: {A} conditional time series generation model.
\newblock In {\em Forty-first International Conference on Machine Learning, {ICML}}, 2024.

\bibitem{nie2023time}
Y.~Nie, N.~H. Nguyen, P.~Sinthong, and J.~Kalagnanam.
\newblock A time series is worth 64 words: Long-term forecasting with transformers.
\newblock In {\em The Eleventh International Conference on Learning Representations, {ICLR}}, 2023.

\bibitem{niu2020permutation}
C.~Niu, Y.~Song, J.~Song, S.~Zhao, A.~Grover, and S.~Ermon.
\newblock Permutation invariant graph generation via score-based generative modeling.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 4474--4484. PMLR, 2020.

\bibitem{pascanu2013difficulty}
R.~Pascanu, T.~Mikolov, and Y.~Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In {\em International conference on machine learning}. PMLR, 2013.

\bibitem{popov2021grad}
V.~Popov, I.~Vovk, V.~Gogoryan, T.~Sadekova, and M.~Kudinov.
\newblock {Grad-TTS}: A diffusion probabilistic model for text-to-speech.
\newblock In {\em International Conference on Machine Learning}, pages 8599--8608. PMLR, 2021.

\bibitem{ramesh2022hierarchical}
A.~Ramesh, P.~Dhariwal, A.~Nichol, C.~Chu, and M.~Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock {\em arXiv preprint arXiv:2204.06125}, 1(2):3, 2022.

\bibitem{ren2024learning}
P.~Ren, R.~Nakata, M.~Lacour, I.~Naiman, N.~Nakata, J.~Song, Z.~Bi, O.~A. Malik, D.~Morozov, O.~Azencot, et~al.
\newblock Learning physics for unveiling hidden earthquake ground motions via conditional generative modeling.
\newblock {\em arXiv preprint arXiv:2407.15089}, 2024.

\bibitem{rombach2022high}
R.~Rombach, A.~Blattmann, D.~Lorenz, P.~Esser, and B.~Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem{rubanova2019latent}
Y.~Rubanova, R.~T. Chen, and D.~K. Duvenaud.
\newblock Latent ordinary differential equations for irregularly-sampled time series.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{rumelhart1985learning}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams.
\newblock Learning internal representations by error propagation.
\newblock Technical report, California Univ San Diego La Jolla Inst for Cognitive Science, 1985.

\bibitem{salimans2022progressive}
T.~Salimans and J.~Ho.
\newblock Progressive distillation for fast sampling of diffusion models.
\newblock In {\em The Tenth International Conference on Learning Representations, {ICLR}}, 2022.

\bibitem{schirmer2022modeling}
M.~Schirmer, M.~Eltayeb, S.~Lessmann, and M.~Rudolph.
\newblock Modeling irregular time series with continuous recurrent units.
\newblock In {\em International Conference on Machine Learning}, pages 19388--19405. PMLR, 2022.

\bibitem{shampine2007stiff}
L.~F. Shampine and S.~Thompson.
\newblock Stiff systems.
\newblock {\em Scholarpedia}, 2(3):2855, 2007.

\bibitem{silva2012predicting}
I.~Silva, G.~Moody, D.~J. Scott, L.~A. Celi, and R.~G. Mark.
\newblock Predicting in-hospital mortality of {ICU} patients: The physionet/computing in cardiology challenge 2012.
\newblock In {\em 2012 Computing in Cardiology}, pages 245--248. IEEE, 2012.

\bibitem{sohl2015deep}
J.~Sohl-Dickstein, E.~Weiss, N.~Maheswaranathan, and S.~Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In {\em International conference on machine learning}, pages 2256--2265. PMLR, 2015.

\bibitem{song2024improved}
Y.~Song and P.~Dhariwal.
\newblock Improved techniques for training consistency models.
\newblock In {\em 12th International Conference on Learning Representations, {ICLR}}, 2024.

\bibitem{song2023consistency}
Y.~Song, P.~Dhariwal, M.~Chen, and I.~Sutskever.
\newblock Consistency models.
\newblock In {\em International Conference on Machine Learning, {ICML}}, volume 202 of {\em Proceedings of Machine Learning Research}, pages 32211--32252. {PMLR}, 2023.

\bibitem{song2019generative}
Y.~Song and S.~Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{song2020improved}
Y.~Song and S.~Ermon.
\newblock Improved techniques for training score-based generative models.
\newblock {\em Advances in neural information processing systems}, 33:12438--12448, 2020.

\bibitem{song2021scorebased}
Y.~Song, J.~Sohl-Dickstein, D.~P. Kingma, A.~Kumar, S.~Ermon, and B.~Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{takens2006detecting}
F.~Takens.
\newblock Detecting strange attractors in turbulence.
\newblock In {\em Dynamical Systems and Turbulence, Warwick 1980: proceedings of a symposium held at the University of Warwick 1979/80}, pages 366--381. Springer, 2006.

\bibitem{tashiro2021csdi}
Y.~Tashiro, J.~Song, Y.~Song, and S.~Ermon.
\newblock {CSDI}: Conditional score-based diffusion models for probabilistic time series imputation.
\newblock {\em Advances in Neural Information Processing Systems}, 34:24804--24816, 2021.

\bibitem{todorov2012mujoco}
E.~Todorov, T.~Erez, and Y.~Tassa.
\newblock {MuJoCo}: A physics engine for model-based control.
\newblock In {\em 2012 IEEE/RSJ international conference on intelligent robots and systems}, pages 5026--5033. IEEE, 2012.

\bibitem{vahdat2020nvae}
A.~Vahdat and J.~Kautz.
\newblock Nvae: A deep hierarchical variational autoencoder.
\newblock {\em Advances in neural information processing systems}, 33:19667--19679, 2020.

\bibitem{van2016wavenet}
A.~van~den Oord, S.~Dieleman, H.~Zen, K.~Simonyan, O.~Vinyals, A.~Graves, N.~Kalchbrenner, A.~W. Senior, and K.~Kavukcuoglu.
\newblock {WaveNet: A} generative model for raw audio.
\newblock In {\em The 9th {ISCA} Speech Synthesis Workshop, {SSW}}, page 125. {ISCA}, 2016.

\bibitem{van2008visualizing}
L.~Van~der Maaten and G.~Hinton.
\newblock Visualizing data using {t-SNE}.
\newblock {\em Journal of machine learning research}, 9(11), 2008.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 2017.

\bibitem{vetterli1992wavelets}
M.~Vetterli and C.~Herley.
\newblock Wavelets and filter banks: Theory and design.
\newblock {\em IEEE transactions on signal processing}, 40(9):2207--2232, 1992.

\bibitem{wang2015imaging}
Z.~Wang and T.~Oates.
\newblock Imaging time-series to improve classification and imputation.
\newblock In {\em Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, {IJCAI}}, pages 3939--3945. {AAAI} Press, 2015.

\bibitem{wu2021autoformer}
H.~Wu, J.~Xu, J.~Wang, and M.~Long.
\newblock Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting.
\newblock {\em Advances in neural information processing systems}, 34:22419--22430, 2021.

\bibitem{yan2023swingnn}
Q.~Yan, Z.~Liang, Y.~Song, R.~Liao, and L.~Wang.
\newblock {SwinGNN}: Rethinking permutation invariance in diffusion models for graph generation.
\newblock {\em Trans. Mach. Learn. Res.}, 2024.

\bibitem{yildiz2019ode2vae}
C.~Yildiz, M.~Heinonen, and H.~Lahdesmaki.
\newblock {ODE2VAE}: Deep generative second order {ODEs} with bayesian neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{yoon2019time}
J.~Yoon, D.~Jarrett, and M.~Van~der Schaar.
\newblock Time-series generative adversarial networks.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{yuan2024diffusion}
X.~Yuan and Y.~Qiao.
\newblock {Diffusion-TS}: Interpretable diffusion for general time series generation.
\newblock In {\em The Twelfth International Conference on Learning Representations, {ICLR}}, 2024.

\bibitem{zeng2023transformers}
A.~Zeng, M.~Chen, L.~Zhang, and Q.~Xu.
\newblock Are transformers effective for time series forecasting?
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~37, pages 11121--11128, 2023.

\bibitem{zhou2021informer}
H.~Zhou, S.~Zhang, J.~Peng, S.~Zhang, J.~Li, H.~Xiong, and W.~Zhang.
\newblock Informer: Beyond efficient transformer for long sequence time-series forecasting.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~35, pages 11106--11115, 2021.

\bibitem{zhou2023deep}
L.~Zhou, M.~Poli, W.~Xu, S.~Massaroli, and S.~Ermon.
\newblock Deep latent state space models for time-series generation.
\newblock In {\em International Conference on Machine Learning}, pages 42625--42643. PMLR, 2023.

\bibitem{zhou2022fedformer}
T.~Zhou, Z.~Ma, Q.~Wen, X.~Wang, L.~Sun, and R.~Jin.
\newblock Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting.
\newblock In {\em International conference on machine learning}, pages 27268--27286. PMLR, 2022.

\end{thebibliography}
