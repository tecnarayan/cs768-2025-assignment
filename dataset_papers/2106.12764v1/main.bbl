\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel et~al.(2010)Abbeel, Coates, and Ng]{abbeel2010autonomous}
Abbeel, P., Coates, A., and Ng, A.~Y.
\newblock Autonomous helicopter aerobatics through apprenticeship learning.
\newblock \emph{The International Journal of Robotics Research}, 29\penalty0
  (13):\penalty0 1608--1639, 2010.

\bibitem[{Achiam} et~al.(2017){Achiam}, {Held}, {Tamar}, and
  {Abbeel}]{achiam2017cpo}
{Achiam}, J., {Held}, D., {Tamar}, A., and {Abbeel}, P.
\newblock Constrained policy optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  22--31, 2017.

\bibitem[Altman(1999)]{altman1999constrained}
Altman, E.
\newblock \emph{Constrained Markov decision processes}, volume~7.
\newblock CRC Press, 1999.

\bibitem[Altman \& Shwartz(2000)Altman and Shwartz]{altman2000constrained}
Altman, E. and Shwartz, A.
\newblock Constrained markov games: Nash equilibria.
\newblock In \emph{Advances in Dynamic Games and Applications}, pp.\  213--221.
  Springer, 2000.

\bibitem[Basu et~al.(2008)Basu, Bhattacharyya, and Borkar]{basu2008learning}
Basu, A., Bhattacharyya, T., and Borkar, V.~S.
\newblock A learning algorithm for risk-sensitive cost.
\newblock \emph{Mathematics of operations research}, 33\penalty0 (4):\penalty0
  880--898, 2008.

\bibitem[Blahoudek et~al.(2020)Blahoudek, Br{\'a}zdil, Novotn{\`y}, Ornik,
  Thangeda, and Topcu]{blahoudek2020qualitative}
Blahoudek, F., Br{\'a}zdil, T., Novotn{\`y}, P., Ornik, M., Thangeda, P., and
  Topcu, U.
\newblock Qualitative controller synthesis for consumption markov decision
  processes.
\newblock \emph{International Conference on Computer-aided Verification}, 2020.

\bibitem[Borkar(2002)]{borkar2002q}
Borkar, V.~S.
\newblock Q-learning for risk-sensitive control.
\newblock \emph{Mathematics of operations research}, 27\penalty0 (2):\penalty0
  294--311, 2002.

\bibitem[Bovopoulos \& Lazar(1992)Bovopoulos and Lazar]{bovopoulos1992effect}
Bovopoulos, A.~D. and Lazar, A.~A.
\newblock The effect of delayed feedback information on network performance.
\newblock \emph{Annals of Operations Research}, 36\penalty0 (1):\penalty0
  101--124, 1992.

\bibitem[Boyd et~al.(2003)Boyd, Xiao, and Mutapcic]{boyd2003subgradient}
Boyd, S., Xiao, L., and Mutapcic, A.
\newblock Subgradient methods.
\newblock \emph{lecture notes of EE392o, Stanford University, Autumn Quarter},
  2004:\penalty0 2004--2005, 2003.

\bibitem[{Chen} \& {Ames}(2019){Chen} and {Ames}]{chen2019duality}
{Chen}, Y. and {Ames}, A.~D.
\newblock Duality between density function and value function with applications
  in constrained optimal control and markov decision process.
\newblock \emph{arXiv preprint arXiv:1902.09583}, 2019.

\bibitem[Chen et~al.(2019)Chen, Singletary, and Ames]{chen2019density}
Chen, Y., Singletary, A., and Ames, A.
\newblock Density functions for guaranteed safety on robotic systems.
\newblock \emph{American Control Conference}, 10 2019.

\bibitem[Chen(2017)]{chen2017tutorial}
Chen, Y.-C.
\newblock A tutorial on kernel density estimation and recent advances.
\newblock \emph{Biostatistics \& Epidemiology}, 1\penalty0 (1):\penalty0
  161--187, 2017.

\bibitem[Chow et~al.(2018)Chow, Nachum, Duenez-Guzman, and
  Ghavamzadeh]{chow2018lyapunov}
Chow, Y., Nachum, O., Duenez-Guzman, E., and Ghavamzadeh, M.
\newblock A lyapunov-based approach to safe reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8092--8101, 2018.

\bibitem[Clouse \& Utgoff(1992)Clouse and Utgoff]{clouse1992teaching}
Clouse, J.~A. and Utgoff, P.~E.
\newblock A teaching method for reinforcement learning.
\newblock In \emph{Machine Learning Proceedings 1992}, pp.\  92--101. Elsevier,
  1992.

\bibitem[Dai et~al.(2017)Dai, Shaw, He, Li, and Song]{dai2017boosting}
Dai, B., Shaw, A., He, N., Li, L., and Song, L.
\newblock Boosting the actor with dual critic.
\newblock \emph{arXiv preprint arXiv:1712.10282}, 2017.

\bibitem[Dalal et~al.(2018)Dalal, Dvijotham, Vecerik, Hester, Paduraru, and
  Tassa]{dalal2018safe}
Dalal, G., Dvijotham, K., Vecerik, M., Hester, T., Paduraru, C., and Tassa, Y.
\newblock Safe exploration in continuous action spaces.
\newblock \emph{arXiv preprint arXiv:1801.08757}, 2018.

\bibitem[Ding et~al.(2020)Ding, Zhang, Basar, and Jovanovic]{ding2020natural}
Ding, D., Zhang, K., Basar, T., and Jovanovic, M.
\newblock Natural policy gradient primal-dual method for constrained markov
  decision processes.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Dotan Di~Castro \& Mannor(2012)Dotan Di~Castro and
  Mannor]{dotan2012policy}
Dotan Di~Castro, A.~T. and Mannor, S.
\newblock Policy gradients with variance related risk criteria.
\newblock In \emph{Proceedings of the 29th International Conference on Machine
  Learning, Edinburgh, Scotland, UK}, 2012.

\bibitem[Garc{\i}a \& Fern{\'a}ndez(2015)Garc{\i}a and
  Fern{\'a}ndez]{garcia2015comprehensive}
Garc{\i}a, J. and Fern{\'a}ndez, F.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 1437--1480, 2015.

\bibitem[Geibel \& Wysotzki(2005)Geibel and Wysotzki]{geibel2005risk}
Geibel, P. and Wysotzki, F.
\newblock Risk-sensitive reinforcement learning applied to control under
  constraints.
\newblock \emph{Journal of Artificial Intelligence Research}, 24:\penalty0
  81--108, 2005.

\bibitem[Geramifard et~al.(2013)Geramifard, Redding, and
  How]{geramifard2013intelligent}
Geramifard, A., Redding, J., and How, J.~P.
\newblock Intelligent cooperative control architecture: a framework for
  performance improvement using safe learning.
\newblock \emph{Journal of Intelligent \& Robotic Systems}, 72\penalty0
  (1):\penalty0 83--103, 2013.

\bibitem[Gu et~al.(2017)Gu, Holly, Lillicrap, and Levine]{gu2017deep}
Gu, S., Holly, E., Lillicrap, T., and Levine, S.
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock In \emph{2017 IEEE international conference on robotics and
  automation (ICRA)}, pp.\  3389--3396. IEEE, 2017.

\bibitem[Heger(1994)]{heger1994consideration}
Heger, M.
\newblock Consideration of risk in reinforcement learning.
\newblock In \emph{Machine Learning Proceedings 1994}, pp.\  105--111.
  Elsevier, 1994.

\bibitem[Hou \& Zhao(2017)Hou and Zhao]{hou2017optimization}
Hou, C. and Zhao, Q.
\newblock Optimization of web service-based control system for balance between
  network traffic and delay.
\newblock \emph{IEEE Transactions on Automation Science and Engineering},
  15\penalty0 (3):\penalty0 1152--1162, 2017.

\bibitem[Howard \& Matheson(1972)Howard and Matheson]{howard1972risk}
Howard, R.~A. and Matheson, J.~E.
\newblock Risk-sensitive markov decision processes.
\newblock \emph{Management science}, 18\penalty0 (7):\penalty0 356--369, 1972.

\bibitem[Kadota et~al.(2006)Kadota, Kurano, and Yasuda]{kadota2006discounted}
Kadota, Y., Kurano, M., and Yasuda, M.
\newblock Discounted markov decision processes with utility constraints.
\newblock \emph{Computers \& Mathematics with Applications}, 51\penalty0
  (2):\penalty0 279--284, 2006.

\bibitem[{Lillicrap} et~al.(2016){Lillicrap}, {Hunt}, {Pritzel}, {Heess},
  {Erez}, {Tassa}, {Silver}, and {Wierstra}]{lillicrap2016ddpg}
{Lillicrap}, T.~P., {Hunt}, J.~J., {Pritzel}, A., {Heess}, N., {Erez}, T.,
  {Tassa}, Y., {Silver}, D., and {Wierstra}, D.
\newblock Continuous control with deep reinforcement learning, 2016.

\bibitem[L{\"o}tjens et~al.(2019)L{\"o}tjens, Everett, and
  How]{lotjens2019safe}
L{\"o}tjens, B., Everett, M., and How, J.~P.
\newblock Safe reinforcement learning with model uncertainty estimates.
\newblock In \emph{2019 International Conference on Robotics and Automation
  (ICRA)}, pp.\  8662--8668. IEEE, 2019.

\bibitem[Luo \& Tseng(1993)Luo and Tseng]{luo1993convergence}
Luo, Z.-Q. and Tseng, P.
\newblock On the convergence rate of dual ascent methods for linearly
  constrained convex minimization.
\newblock \emph{Mathematics of Operations Research}, 18\penalty0 (4):\penalty0
  846--867, 1993.

\bibitem[Moldovan \& Abbeel(2012)Moldovan and Abbeel]{moldovan2012safe}
Moldovan, T.~M. and Abbeel, P.
\newblock Safe exploration in markov decision processes.
\newblock \emph{arXiv preprint arXiv:1205.4810}, 2012.

\bibitem[Nachum \& Dai(2020)Nachum and Dai]{nachum2020reinforcement}
Nachum, O. and Dai, B.
\newblock Reinforcement learning via fenchel-rockafellar duality.
\newblock \emph{arXiv preprint arXiv:2001.01866}, 2020.

\bibitem[Nachum et~al.(2019)Nachum, Chow, Dai, and Li]{nachum2019dualdice}
Nachum, O., Chow, Y., Dai, B., and Li, L.
\newblock Dualdice: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2318--2328, 2019.

\bibitem[Nilim \& El~Ghaoui(2005)Nilim and El~Ghaoui]{nilim2005robust}
Nilim, A. and El~Ghaoui, L.
\newblock Robust control of markov decision processes with uncertain transition
  matrices.
\newblock \emph{Operations Research}, 53\penalty0 (5):\penalty0 780--798, 2005.

\bibitem[Paternain et~al.(2019)Paternain, Chamon, Calvo-Fullana, and
  Ribeiro]{paternain2019constrained}
Paternain, S., Chamon, L., Calvo-Fullana, M., and Ribeiro, A.
\newblock Constrained reinforcement learning has zero duality gap.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  7553--7563, 2019.

\bibitem[Pham et~al.(2018)Pham, De~Magistris, and Tachibana]{pham2018optlayer}
Pham, T.-H., De~Magistris, G., and Tachibana, R.
\newblock Optlayer-practical constrained optimization for deep reinforcement
  learning in the real world.
\newblock In \emph{2018 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  6236--6243. IEEE, 2018.

\bibitem[{Rantzer}(2001)]{rantzer2001a}
{Rantzer}, A.
\newblock A dual to lyapunov's stability theorem.
\newblock \emph{Systems and Control Letters}, 42\penalty0 (3):\penalty0
  161--168, 2001.

\bibitem[Ray et~al.(2019)Ray, Achiam, and Amodei]{Ray2019}
Ray, A., Achiam, J., and Amodei, D.
\newblock {Benchmarking Safe Exploration in Deep Reinforcement Learning}.
\newblock 2019.

\bibitem[Rockafellar(1970)]{rockafellar1970convex}
Rockafellar, R.~T.
\newblock \emph{Convex analysis}.
\newblock Number~28. Princeton university press, 1970.

\bibitem[Sato et~al.(2001)Sato, Kimura, and Kobayashi]{sato2001td}
Sato, M., Kimura, H., and Kobayashi, S.
\newblock Td algorithm for the variance of return and mean-variance
  reinforcement learning.
\newblock \emph{Transactions of the Japanese Society for Artificial
  Intelligence}, 16\penalty0 (3):\penalty0 353--362, 2001.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Stooke et~al.(2020)Stooke, Achiam, and Abbeel]{stooke2020responsive}
Stooke, A., Achiam, J., and Abbeel, P.
\newblock Responsive safety in reinforcement learning by pid lagrangian
  methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9133--9143. PMLR, 2020.

\bibitem[Tang et~al.(2010)Tang, Singh, Goehausen, and
  Abbeel]{tang2010parameterized}
Tang, J., Singh, A., Goehausen, N., and Abbeel, P.
\newblock Parameterized maneuver learning for autonomous helicopter flight.
\newblock In \emph{2010 IEEE International Conference on Robotics and
  Automation}, pp.\  1142--1148. IEEE, 2010.

\bibitem[Tang et~al.(2019)Tang, Feng, Li, Zhou, and Liu]{tang2019doubly}
Tang, Z., Feng, Y., Li, L., Zhou, D., and Liu, Q.
\newblock Doubly robust bias reduction in infinite horizon off-policy
  estimation.
\newblock \emph{arXiv preprint arXiv:1910.07186}, 2019.

\bibitem[{Tessler} et~al.(2019){Tessler}, {Mankowitz}, and
  {Mannor}]{tessler2019reward}
{Tessler}, C., {Mankowitz}, D.~J., and {Mannor}, S.
\newblock Reward constrained policy optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Thomaz et~al.(2006)Thomaz, Breazeal, et~al.]{thomaz2006reinforcement}
Thomaz, A.~L., Breazeal, C., et~al.
\newblock Reinforcement learning with human teachers: Evidence of feedback and
  guidance with implications for learning performance.
\newblock In \emph{Aaai}, volume~6, pp.\  1000--1005. Boston, MA, 2006.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Traue et~al.(2019)Traue, Book, Kirchgässner, and
  Wallscheid]{traue2019reinforcement}
Traue, A., Book, G., Kirchgässner, W., and Wallscheid, O.
\newblock Towards a reinforcement learning environment toolbox for intelligent
  electric motor control, 2019.

\bibitem[Wasserman(2019)]{kerneldensitycmu}
Wasserman, L.
\newblock Lecture notes in statistical methods for machine learning, 2019.
\newblock URL \url{http://www.stat.cmu.edu/~larry/=sml/densityestimation.pdf}.
\newblock Online material, last visited on 2019/08/25.

\bibitem[Yang et~al.(2020)Yang, Rosca, Narasimhan, and
  Ramadge]{Yang2020Projection}
Yang, T.-Y., Rosca, J., Narasimhan, K., and Ramadge, P.~J.
\newblock Projection-based constrained policy optimization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zhang et~al.(2020)Zhang, Vuong, and Ross]{zhang2020first}
Zhang, Y., Vuong, Q., and Ross, K.
\newblock First order constrained optimization in policy space.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\end{thebibliography}
