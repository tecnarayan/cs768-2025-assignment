\begin{thebibliography}{}

\bibitem[Bartlett et~al., 2017]{DBLP:conf/nips/BartlettFT17}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M. (2017).
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In {\em Neural Inform. Process. Syst.}, pages 6240--6249.

\bibitem[Belkin et~al., 2019]{belkin2019reconciling}
Belkin, M., Hsu, D., Ma, S., and Mandal, S. (2019).
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(32):15849--15854.

\bibitem[Bibas and Feder, 2021]{bibas2021predictive}
Bibas, K. and Feder, M. (2021).
\newblock The predictive normalized maximum likelihood for over-parameterized
  linear regression with norm constraint: Regret and double descent.
\newblock {\em arXiv preprint arXiv:2102.07181}.

\bibitem[Bibas et~al., 2019a]{bibas2019deep}
Bibas, K., Fogel, Y., and Feder, M. (2019a).
\newblock Deep pnml: Predictive normalized maximum likelihood for deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1904.12286}.

\bibitem[Bibas et~al., 2019b]{bibas2019new}
Bibas, K., Fogel, Y., and Feder, M. (2019b).
\newblock A new look at an old problem: A universal learning approach to linear
  regression.
\newblock {\em Int. Symp. on Information Theory}.

\bibitem[Bibas et~al., 2021]{bibas2021learning}
Bibas, K., Weiss-Dicker, G., Cohen, D., Cahan, N., and Greenspan, H. (2021).
\newblock Learning rotation invariant features for cryogenic electron
  microscopy image reconstruction.
\newblock In {\em International Symposium on Biomedical Imaging (ISBI)}, pages
  563--566. IEEE.

\bibitem[Fisher, 1936]{fisher1936use}
Fisher, R.~A. (1936).
\newblock The use of multiple measurements in taxonomic problems.
\newblock {\em Annals of eugenics}, 7(2):179--188.

\bibitem[Fogel and Feder, 2018]{fogel2018universal}
Fogel, Y. and Feder, M. (2018).
\newblock Universal batch learning with log-loss.
\newblock In {\em Int. Symp. on Information Theory}, pages 21--25. IEEE.

\bibitem[Fu and Levine, 2021]{fu2021offline}
Fu, J. and Levine, S. (2021).
\newblock Offline model-based optimization via normalized maximum likelihood
  estimation.
\newblock In {\em Int. Conf. on Learning Representations}.

\bibitem[Gal and Ghahramani, 2016]{DBLP:conf/icml/GalG16}
Gal, Y. and Ghahramani, Z. (2016).
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em Int. Conf. Mach. Learning}.

\bibitem[He et~al., 2016]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J. (2016).
\newblock Deep residual learning for image recognition.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition}, pages
  770--778.

\bibitem[Hendrycks and Gimpel, 2017]{hendrycks17baseline}
Hendrycks, D. and Gimpel, K. (2017).
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock {\em Int. Conf. on Learning Representations}.

\bibitem[Hendrycks et~al., 2019a]{DBLP:conf/iclr/HendrycksMD19}
Hendrycks, D., Mazeika, M., and Dietterich, T.~G. (2019a).
\newblock Deep anomaly detection with outlier exposure.
\newblock In {\em Int. Conf. on Learning Representations}.

\bibitem[Hendrycks et~al., 2019b]{DBLP:conf/nips/HendrycksMKS19}
Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D. (2019b).
\newblock Using self-supervised learning can improve model robustness and
  uncertainty.
\newblock In {\em Neural Inform. Process. Syst.}

\bibitem[Huang et~al., 2017]{huang2017densely}
Huang, G., Liu, Z., van~der Maaten, L., and Weinberger, K.~Q. (2017).
\newblock Densely connected convolutional networks.
\newblock In {\em Proc. Conf. Comput. Vision Pattern Recognition}.

\bibitem[Jiang et~al., 2020]{DBLP:conf/iclr/JiangNMKB20}
Jiang, Y., Neyshabur, B., Mobahi, H., Krishnan, D., and Bengio, S. (2020).
\newblock Fantastic generalization measures and where to find them.
\newblock In {\em Int. Conf. on Learning Representations}.

\bibitem[Kaufman et~al., 2019]{DBLP:conf/bmvc/KaufmanBBCH19}
Kaufman, D., Bibas, K., Borenstein, E., Chertok, M., and Hassner, T. (2019).
\newblock Balancing specialization, generalization, and compression for
  detection and tracking.
\newblock In {\em Proc. British Mach. Vision Conf.}

\bibitem[Krizhevsky et~al., 2014]{krizhevsky2014cifar}
Krizhevsky, A., Nair, V., and Hinton, G. (2014).
\newblock The cifar-10 dataset.
\newblock {\em online: http://www. cs. toronto. edu/kriz/cifar. html}.

\bibitem[Lakshminarayanan et~al., 2017]{DBLP:conf/nips/Lakshminarayanan17}
Lakshminarayanan, B., Pritzel, A., and Blundell, C. (2017).
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In {\em Neural Inform. Process. Syst.}

\bibitem[Lee et~al., 2018]{lee2018simple}
Lee, K., Lee, K., Lee, H., and Shin, J. (2018).
\newblock A simple unified framework for detecting out-of-distribution samples
  and adversarial attacks.
\newblock In {\em Neural Inform. Process. Syst.}

\bibitem[Liang et~al., 2018]{liang2017enhancing}
Liang, S., Li, Y., and Srikant, R. (2018).
\newblock Enhancing the reliability of out-of-distribution image detection in
  neural networks.
\newblock In {\em Int. Conf. on Learning Representations}.

\bibitem[Liu et~al., 2020]{liu2020energy}
Liu, W., Wang, X., Owens, J., and Li, Y. (2020).
\newblock Energy-based out-of-distribution detection.
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[Malinin and Gales, 2018]{DBLP:conf/nips/MalininG18}
Malinin, A. and Gales, M. J.~F. (2018).
\newblock Predictive uncertainty estimation via prior networks.
\newblock In {\em Neural Inform. Process. Syst.}

\bibitem[Merhav and Feder, 1998]{merhav1998universal}
Merhav, N. and Feder, M. (1998).
\newblock Universal prediction.
\newblock {\em Trans. on Inform. Theory}, 44(6):2124--2147.

\bibitem[Mohseni et~al., 2020]{DBLP:conf/aaai/MohseniPYW20}
Mohseni, S., Pitale, M., Yadawa, J. B.~S., and Wang, Z. (2020).
\newblock Self-supervised learning for generalizable out-of-distribution
  detection.
\newblock In {\em AAAI Conf. on Artificial Intelligence}.

\bibitem[Nandy et~al., 2020]{DBLP:conf/nips/NandyHL20}
Nandy, J., Hsu, W., and Lee, M. (2020).
\newblock Towards maximizing the representation gap between in-domain {\&}
  out-of-distribution examples.
\newblock In {\em Neural Inform. Process. Syst.}

\bibitem[Netzer et~al., 2011]{netzer2011reading}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A.~Y. (2011).
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In {\em Neural Inform. Process. Syst. Workshops}, page~5.

\bibitem[Neyshabur et~al., 2018]{DBLP:conf/iclr/NeyshaburBS18}
Neyshabur, B., Bhojanapalli, S., and Srebro, N. (2018).
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In {\em Int. Conf. on Learning Representations}.

\bibitem[Papadopoulos et~al., 2021]{PAPADOPOULOS2021138}
Papadopoulos, A.-A., Rajati, M.~R., Shaikh, N., and Wang, J. (2021).
\newblock Outlier exposure with confidence control for out-of-distribution
  detection.
\newblock {\em Neurocomputing}.

\bibitem[Pesso et~al., 2021]{pesso2021utilizing}
Pesso, U., Bibas, K., and Feder, M. (2021).
\newblock Utilizing adversarial targeted attacks to boost adversarial
  robustness.

\bibitem[Rissanen and Roos, 2007]{rissanen2007conditional}
Rissanen, J. and Roos, T. (2007).
\newblock Conditional nml universal models.
\newblock In {\em 2007 Information Theory and Applications Workshop}, pages
  337--341. IEEE.

\bibitem[Roos and Rissanen, 2008]{roos2008sequentially}
Roos, T. and Rissanen, J. (2008).
\newblock On sequentially normalized maximum likelihood models.
\newblock {\em Compare}, 27(31):256.

\bibitem[Rosas et~al., 2020]{rosas2020learning}
Rosas, F.~E., Mediano, P.~A., and Gastpar, M. (2020).
\newblock Learning, compression, and leakage: Minimizing classification error
  via meta-universal compression principles.
\newblock {\em arXiv preprint arXiv:2010.07382}.

\bibitem[Sastry and Oore, 2020]{gram}
Sastry, C.~S. and Oore, S. (2020).
\newblock Detecting out-of-distribution examples with {G}ram matrices.
\newblock In {\em Int. Conf. Mach. Learning}.

\bibitem[Singh et~al., 2021]{singh2021uncertainty}
Singh, A., Sengupta, S., Rasheed, M.~A., Jayakumar, V., and Lakshminarayanan,
  V. (2021).
\newblock Uncertainty aware and explainable diagnosis of retinal disease.
\newblock In {\em Medical Imaging 2021: Imaging Informatics for Healthcare,
  Research, and Applications}.

\bibitem[van Amersfoort et~al., 2020]{DBLP:conf/icml/AmersfoortSTG20}
van Amersfoort, J., Smith, L., Teh, Y.~W., and Gal, Y. (2020).
\newblock Uncertainty estimation using a single deep deterministic neural
  network.
\newblock In {\em Int. Conf. Mach. Learning}.

\bibitem[Vapnik and Chervonenkis, 2015]{vapnik2015uniform}
Vapnik, V.~N. and Chervonenkis, A.~Y. (2015).
\newblock On the uniform convergence of relative frequencies of events to their
  probabilities.
\newblock In {\em Measures of complexity}, pages 11--30. Springer.

\bibitem[Vyas et~al., 2018]{vyas2018out}
Vyas, A., Jammalamadaka, N., Zhu, X., Das, D., Kaul, B., and Willke, T.~L.
  (2018).
\newblock Out-of-distribution detection using an ensemble of self supervised
  leave-out classifiers.
\newblock In {\em European Conf. Comput. Vision}, pages 560--574.

\bibitem[Willers et~al., 2020]{willers2020safety}
Willers, O., Sudholt, S., Raafatnia, S., and Abrecht, S. (2020).
\newblock Safety concerns and mitigation approaches regarding the use of deep
  learning in safety-critical perception tasks.
\newblock In {\em International Conference on Computer Safety, Reliability, and
  Security}, pages 336--350. Springer.

\bibitem[Xu et~al., 2015]{xu2015turkergaze}
Xu, P., Ehinger, K.~A., Zhang, Y., Finkelstein, A., Kulkarni, S.~R., and Xiao,
  J. (2015).
\newblock Turkergaze: Crowdsourcing saliency with webcam based eye tracking.
\newblock {\em arXiv preprint arXiv:1504.06755}.

\bibitem[Yu et~al., 2015]{yu15lsun}
Yu, F., Zhang, Y., Song, S., Seff, A., and Xiao, J. (2015).
\newblock Lsun: Construction of a large-scale image dataset using deep learning
  with humans in the loop.
\newblock {\em arXiv preprint arXiv:1506.03365}.

\bibitem[Zagoruyko and Komodakis, 2016]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N. (2016).
\newblock Wide residual networks.
\newblock In {\em Proc. British Mach. Vision Conf.}

\bibitem[Zhang, 2012]{zhang2012model}
Zhang, J. (2012).
\newblock Model selection with informative normalized maximum likelihood: Data
  prior and model prior.
\newblock In {\em Descriptive and normative approaches to human behavior},
  pages 303--319. World Scientific.

\bibitem[Zhong et~al., 2017]{zhong2017recovery}
Zhong, K., Song, Z., Jain, P., Bartlett, P.~L., and Dhillon, I.~S. (2017).
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In {\em Int. Conf. Mach. Learning}, pages 4140--4149.

\bibitem[Zhou and Levine, 2020]{zhou2020amortized}
Zhou, A. and Levine, S. (2020).
\newblock Amortized conditional normalized maximum likelihood.
\newblock {\em arXiv preprint arXiv:2011.02696}.

\bibitem[Zhuang et~al., 2020]{zhuang2020training}
Zhuang, H., Lin, Z., and Toh, K.-A. (2020).
\newblock Training a multilayer network with low-memory kernel-and-range
  projection.
\newblock {\em Journal of the Franklin Institute}, 357(1):522--550.

\end{thebibliography}
