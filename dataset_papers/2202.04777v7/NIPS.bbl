\begin{thebibliography}{}

\bibitem[Alemi et~al., 2018]{alemi2018fixing}
Alemi, A., Poole, B., Fischer, I., Dillon, J., Saurous, R.~A., and Murphy, K.
  (2018).
\newblock {Fixing a broken ELBO}.
\newblock In {\em International Conference on Machine Learning}, pages
  159--168. PMLR.

\bibitem[Baldi and Hornik, 1989]{baldi1989neural}
Baldi, P. and Hornik, K. (1989).
\newblock Neural networks and principal component analysis: Learning from
  examples without local minima.
\newblock {\em Neural networks}, 2(1):53--58.

\bibitem[Cavazza et~al., 2018]{cavazza2018dropout}
Cavazza, J., Morerio, P., Haeffele, B., Lane, C., Murino, V., and Vidal, R.
  (2018).
\newblock Dropout as a low-rank regularizer for matrix factorization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 435--444. PMLR.

\bibitem[Choromanska et~al., 2015a]{choromanska2015loss}
Choromanska, A., Henaff, M., Mathieu, M., Arous, G.~B., and LeCun, Y. (2015a).
\newblock The loss surfaces of multilayer networks.
\newblock In {\em Artificial Intelligence and Statistics}, pages 192--204.

\bibitem[Choromanska et~al., 2015b]{choromanska2015open}
Choromanska, A., LeCun, Y., and Arous, G.~B. (2015b).
\newblock Open problem: The landscape of the loss surfaces of multilayer
  networks.
\newblock In {\em Conference on Learning Theory}, pages 1756--1760. PMLR.

\bibitem[Fama, 1970]{fama1970efficient}
Fama, E.~F. (1970).
\newblock Efficient capital markets: A review of theory and empirical work.
\newblock {\em The journal of Finance}, 25(2):383--417.

\bibitem[Gal and Ghahramani, 2016]{gal2016dropout}
Gal, Y. and Ghahramani, Z. (2016).
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em international conference on machine learning}, pages
  1050--1059. PMLR.

\bibitem[Glorot and Bengio, 2010]{glorot2010understanding}
Glorot, X. and Bengio, Y. (2010).
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256. JMLR Workshop and
  Conference Proceedings.

\bibitem[Goodfellow et~al., 2014]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y. (2014).
\newblock Generative adversarial nets.
\newblock {\em Advances in neural information processing systems}, 27.

\bibitem[Gotmare et~al., 2018]{gotmare2018closer}
Gotmare, A., Keskar, N.~S., Xiong, C., and Socher, R. (2018).
\newblock A closer look at deep learning heuristics: Learning rate restarts,
  warmup and distillation.
\newblock {\em arXiv preprint arXiv:1810.13243}.

\bibitem[Hardt and Ma, 2016]{hardt2016identity}
Hardt, M. and Ma, T. (2016).
\newblock Identity matters in deep learning.
\newblock {\em arXiv preprint arXiv:1611.04231}.

\bibitem[Hastie et~al., 2019]{hastie2019surprises}
Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R.~J. (2019).
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em arXiv preprint arXiv:1903.08560}.

\bibitem[He et~al., 2020]{he2020piecewise}
He, F., Wang, B., and Tao, D. (2020).
\newblock Piecewise linear activations substantially shape the loss surfaces of
  neural networks.
\newblock {\em arXiv preprint arXiv:2003.12236}.

\bibitem[Kawaguchi, 2016]{kawaguchi2016deep}
Kawaguchi, K. (2016).
\newblock Deep learning without poor local minima.
\newblock {\em Advances in Neural Information Processing Systems}, 29:586--594.

\bibitem[Kingma and Welling, 2013]{kingma2013auto}
Kingma, D.~P. and Welling, M. (2013).
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}.

\bibitem[Kleinberg et~al., 2018]{kleinberg2018alternative}
Kleinberg, B., Li, Y., and Yuan, Y. (2018).
\newblock An alternative view: When does sgd escape local minima?
\newblock In {\em International Conference on Machine Learning}, pages
  2698--2707. PMLR.

\bibitem[Krogh and Hertz, 1992]{krogh1992simple}
Krogh, A. and Hertz, J.~A. (1992).
\newblock A simple weight decay can improve generalization.
\newblock In {\em Advances in neural information processing systems}, pages
  950--957.

\bibitem[Laurent and Brecht, 2018]{laurent2018deep}
Laurent, T. and Brecht, J. (2018).
\newblock Deep linear networks with arbitrary loss: All local minima are
  global.
\newblock In {\em International conference on machine learning}, pages
  2902--2907. PMLR.

\bibitem[Liu, 2021]{liu2021spurious}
Liu, B. (2021).
\newblock Spurious local minima are common for deep neural networks with
  piecewise linear activations.
\newblock {\em arXiv preprint arXiv:2102.13233}.

\bibitem[Liu et~al., 2021]{liu2021noise}
Liu, K., Ziyin, L., and Ueda, M. (2021).
\newblock Noise and fluctuation of finite learning rate stochastic gradient
  descent.
\newblock In {\em International Conference on Machine Learning}, pages
  7045--7056. PMLR.

\bibitem[Loshchilov and Hutter, 2017]{loshchilov2017decoupled}
Loshchilov, I. and Hutter, F. (2017).
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}.

\bibitem[Lu and Kawaguchi, 2017]{lu2017depth}
Lu, H. and Kawaguchi, K. (2017).
\newblock Depth creates no bad local minima.
\newblock {\em arXiv preprint arXiv:1702.08580}.

\bibitem[Lucas et~al., 2019]{lucas2019dont}
Lucas, J., Tucker, G., Grosse, R., and Norouzi, M. (2019).
\newblock {Don't Blame the ELBO! A Linear VAE Perspective on Posterior
  Collapse}.

\bibitem[Mackay, 1992]{mackay1992bayesian}
Mackay, D. J.~C. (1992).
\newblock {\em Bayesian methods for adaptive models}.
\newblock PhD thesis, California Institute of Technology.

\bibitem[Mehta et~al., 2021]{mehta2021loss}
Mehta, D., Chen, T., Tang, T., and Hauenstein, J. (2021).
\newblock The loss surface of deep linear networks viewed through the algebraic
  geometry lens.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}.

\bibitem[Mianjy and Arora, 2019]{mianjy2019dropout}
Mianjy, P. and Arora, R. (2019).
\newblock On dropout and nuclear norm regularization.
\newblock In {\em International Conference on Machine Learning}, pages
  4575--4584. PMLR.

\bibitem[Mori et~al., 2022]{mori2022power}
Mori, T., Ziyin, L., Liu, K., and Ueda, M. (2022).
\newblock Power-law escape rate of sgd.
\newblock In {\em International Conference on Machine Learning}, pages
  15959--15975. PMLR.

\bibitem[Ramachandran et~al., 2017]{ramachandran2017searching}
Ramachandran, P., Zoph, B., and Le, Q.~V. (2017).
\newblock Searching for activation functions.

\bibitem[Rangamani and Banburski-Fahey, 2022]{rangamani2022neural}
Rangamani, A. and Banburski-Fahey, A. (2022).
\newblock Neural collapse in deep homogeneous classifiers and the role of
  weight decay.
\newblock In {\em ICASSP 2022-2022 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 4243--4247. IEEE.

\bibitem[Safran and Shamir, 2018]{safran2018spurious}
Safran, I. and Shamir, O. (2018).
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock In {\em International conference on machine learning}, pages
  4433--4441. PMLR.

\bibitem[Saxe et~al., 2013]{saxe2013exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S. (2013).
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock {\em arXiv preprint arXiv:1312.6120}.

\bibitem[Srivastava et~al., 2014]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R. (2014).
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958.

\bibitem[Taghvaei et~al., 2017]{taghvaei2017regularization}
Taghvaei, A., Kim, J.~W., and Mehta, P. (2017).
\newblock How regularization affects the critical points in linear networks.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Tanaka et~al., 2020]{tanaka2020pruning}
Tanaka, H., Kunin, D., Yamins, D.~L., and Ganguli, S. (2020).
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock {\em Advances in Neural Information Processing Systems},
  33:6377--6389.

\bibitem[Tian, 2022]{tian2022deep}
Tian, Y. (2022).
\newblock Deep contrastive learning is provably (almost) principal component
  analysis.
\newblock {\em arXiv preprint arXiv:2201.12680}.

\bibitem[Venturi et~al., 2019]{venturi2019spurious}
Venturi, L., Bandeira, A.~S., and Bruna, J. (2019).
\newblock Spurious valleys in one-hidden-layer neural network optimization
  landscapes.
\newblock {\em Journal of Machine Learning Research}, 20:133.

\bibitem[Wang and Ziyin, 2022]{wang2022posterior}
Wang, Z. and Ziyin, L. (2022).
\newblock Posterior collapse of a linear latent variable model.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Yun et~al., 2018]{yun2018small}
Yun, C., Sra, S., and Jadbabaie, A. (2018).
\newblock Small nonlinearities in activation functions create bad local minima
  in neural networks.
\newblock {\em arXiv preprint arXiv:1802.03487}.

\bibitem[Ziyin et~al., 2021]{ziyin2021sgd}
Ziyin, L., Li, B., Simon, J.~B., and Ueda, M. (2021).
\newblock Sgd can converge to local maxima.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Ziyin et~al., 2023]{ziyin2023what}
Ziyin, L., Lubana, E.~S., Ueda, M., and Tanaka, H. (2023).
\newblock What shapes the loss landscape of self supervised learning?
\newblock In {\em The Eleventh International Conference on Learning
  Representations}.

\bibitem[Ziyin and Ueda, 2022]{ziyin2022exactb}
Ziyin, L. and Ueda, M. (2022).
\newblock Exact phase transitions in deep learning.
\newblock {\em arXiv preprint arXiv:2205.12510}.

\bibitem[Ziyin and Wang, 2023]{ziyin2022sparsity}
Ziyin, L. and Wang, Z. (2023).
\newblock {spred: Solving L1 Penalty with SGD}.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Ziyin et~al., 2022]{ziyin2022stochastic}
Ziyin, L., Zhang, H., Meng, X., Lu, Y., Xing, E., and Ueda, M. (2022).
\newblock Stochastic neural networks with infinite width are deterministic.

\end{thebibliography}
