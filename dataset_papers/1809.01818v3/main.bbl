\begin{thebibliography}{}

\bibitem[Abrol et~al., 2014]{abrol2014deterministic}
Abrol, F., Mandt, S., Ranganath, R., and Blei, D. (2014).
\newblock Deterministic annealing for stochastic variational inference.
\newblock {\em stat}, 1050:7.

\bibitem[Agakov and Barber, 2004]{agakov2004auxiliary}
Agakov, F.~V. and Barber, D. (2004).
\newblock An auxiliary variational method.
\newblock In {\em Neural Information Processing}.

\bibitem[Berg et~al., 2018]{berg2018sylvester}
Berg, R. v.~d., Hasenclever, L., Tomczak, J.~M., and Welling, M. (2018).
\newblock Sylvester normalizing flows for variational inference.
\newblock {\em arXiv preprint arXiv:1803.05649}.

\bibitem[Bowman et~al., 2016]{bowman2015generating}
Bowman, S.~R., Vilnis, L., Vinyals, O., Dai, A.~M., Jozefowicz, R., and Bengio,
  S. (2016).
\newblock Generating sentences from a continuous space.
\newblock In {\em SIGNLL Conference on Computational Natural Language Learning
  (CONLL)}.

\bibitem[Burda et~al., 2016]{burda2015importance}
Burda, Y., Grosse, R., and Salakhutdinov, R. (2016).
\newblock Importance weighted autoencoders.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Chung et~al., 2015]{chung2015recurrent}
Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A.~C., and Bengio, Y.
  (2015).
\newblock A recurrent latent variable model for sequential data.
\newblock In {\em Advances in neural information processing systems}.

\bibitem[Clevert et~al., 2016]{clevert2015fast}
Clevert, D.-A., Unterthiner, T., and Hochreiter, S. (2016).
\newblock Fast and accurate deep network learning by exponential linear units
  (elus).
\newblock {\em International Conference on Learning Representations}.

\bibitem[Cremer et~al., 2018]{cremer2018inference}
Cremer, C., Li, X., and Duvenaud, D. (2018).
\newblock Inference suboptimality in variational autoencoders.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Dieng et~al., 2017]{dieng2017variational}
Dieng, A.~B., Tran, D., Ranganath, R., Paisley, J., and Blei, D. (2017).
\newblock Variational inference via $\chi$ upper bound minimization.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Edwards and Storkey, 2017]{edwards2016towards}
Edwards, H. and Storkey, A. (2017).
\newblock Towards a neural statistician.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Fraccaro et~al., 2016]{fraccaro2016sequential}
Fraccaro, M., S{\o}nderby, S.~K., Paquet, U., and Winther, O. (2016).
\newblock Sequential neural models with stochastic layers.
\newblock In {\em Advances in neural information processing systems}, pages
  2199--2207.

\bibitem[Gulrajani et~al., 2017]{gulrajani2016pixelvae}
Gulrajani, I., Kumar, K., Ahmed, F., Taiga, A.~A., Visin, F., Vazquez, D., and
  Courville, A. (2017).
\newblock Pixelvae: A latent variable model for natural images.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Huang et~al., 2018]{huang2018neural}
Huang, C.-W., Krueger, D., Lacoste, A., and Courville, A. (2018).
\newblock Neural autoregressive flows.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Husz{\'a}r, 2017]{huszar2017variational}
Husz{\'a}r, F. (2017).
\newblock Variational inference using implicit distributions.
\newblock {\em arXiv preprint arXiv:1702.08235}.

\bibitem[Karl et~al., 2016]{karl2016deep}
Karl, M., Soelch, M., Bayer, J., and van~der Smagt, P. (2016).
\newblock Deep variational bayes filters: Unsupervised learning of state space
  models from raw data.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Katahira et~al., 2008]{katahira2008deterministic}
Katahira, K., Watanabe, K., and Okada, M. (2008).
\newblock Deterministic annealing variant of variational bayes method.
\newblock In {\em Journal of Physics: Conference Series}, volume~95, page
  012015. IOP Publishing.

\bibitem[Kim et~al., 2018]{kim2018semi}
Kim, Y., Wiseman, S., Miller, A.~C., Sontag, D., and Rush, A.~M. (2018).
\newblock Semi-amortized variational autoencoders.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Kingma et~al., 2016]{kingma2016improved}
Kingma, D.~P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and
  Welling, M. (2016).
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Kingma and Welling, 2014]{kingma2013auto}
Kingma, D.~P. and Welling, M. (2014).
\newblock Auto-encoding variational bayes.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Krishnan et~al., 2017]{krishnan2017challenges}
Krishnan, R.~G., Liang, D., and Hoffman, M. (2017).
\newblock On the challenges of learning with inference networks on sparse,
  high-dimensional data.
\newblock {\em arXiv preprint arXiv:1710.06085}.

\bibitem[Krueger et~al., 2017]{krueger2017bayesian}
Krueger, D., Huang, C.-W., Islam, R., Turner, R., Lacoste, A., and Courville,
  A. (2017).
\newblock Bayesian hypernetworks.
\newblock {\em arXiv preprint arXiv:1710.04759}.

\bibitem[Larochelle and Murray, 2011]{larochelle2011}
Larochelle, H. and Murray, I. (2011).
\newblock The neural autoregressive distribution estimator.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}.

\bibitem[Li and Turner, 2016]{li2016renyi}
Li, Y. and Turner, R.~E. (2016).
\newblock R{\'e}nyi divergence variational inference.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Maal{\o}e et~al., 2016]{maaloe2016auxiliary}
Maal{\o}e, L., S{\o}nderby, C.~K., S{\o}nderby, S.~K., and Winther, O. (2016).
\newblock Auxiliary deep generative models.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Mandt et~al., 2016]{mandt2016variational}
Mandt, S., McInerney, J., Abrol, F., Ranganath, R., and Blei, D. (2016).
\newblock Variational tempering.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}.

\bibitem[Marino et~al., 2018]{marinoiterative}
Marino, J., Yue, Y., and Mandt, S. (2018).
\newblock Iterative amortized inference.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Mescheder et~al., 2017]{mescheder2017adversarial}
Mescheder, L., Nowozin, S., and Geiger, A. (2017).
\newblock Adversarial variational bayes: Unifying variational autoencoders and
  generative adversarial networks.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Nair and Hinton, 2010]{nair2010rectified}
Nair, V. and Hinton, G.~E. (2010).
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Neal, 2001]{neal2001annealed}
Neal, R.~M. (2001).
\newblock Annealed importance sampling.
\newblock {\em Statistics and computing}, 11(2).

\bibitem[Nowozin, 2018]{nowozindebiasing}
Nowozin, S. (2018).
\newblock Debiasing evidence approximations: On importance-weighted
  autoencoders and jackknife variational inference.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Raiko et~al., 2007]{raiko2007building}
Raiko, T., Valpola, H., Harva, M., and Karhunen, J. (2007).
\newblock Building blocks for variational bayesian learning of latent variable
  models.
\newblock {\em Journal of Machine Learning Research}.

\bibitem[Rainforth et~al., 2018]{rainforth2018tighter}
Rainforth, T., Kosiorek, A.~R., Le, T.~A., Maddison, C.~J., Igl, M., Wood, F.,
  and Teh, Y.~W. (2018).
\newblock Tighter variational bounds are not necessarily better.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Ranganath et~al., 2016]{ranganath2016hierarchical}
Ranganath, R., Tran, D., and Blei, D. (2016).
\newblock Hierarchical variational models.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Rezende and Mohamed, 2015]{rezende2015variational}
Rezende, D.~J. and Mohamed, S. (2015).
\newblock Variational inference with normalizing flows.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Rezende et~al., 2014]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D. (2014).
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Salimans et~al., 2015]{salimans2015markov}
Salimans, T., Kingma, D., and Welling, M. (2015).
\newblock Markov chain monte carlo and variational inference: Bridging the gap.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Salimans and Kingma, 2016]{salimans2016weight}
Salimans, T. and Kingma, D.~P. (2016).
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Shabanian et~al., 2017]{shabanian2017variational}
Shabanian, S., Arpit, D., Trischler, A., and Bengio, Y. (2017).
\newblock Variational bi-lstms.
\newblock {\em arXiv preprint arXiv:1711.05717}.

\bibitem[Shi et~al., 2018]{shi2017implicit}
Shi, J., Sun, S., and Zhu, J. (2018).
\newblock Kernel implicit variational inference.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[S{\o}nderby et~al., 2016]{sonderby2016ladder}
S{\o}nderby, C.~K., Raiko, T., Maal{\o}e, L., S{\o}nderby, S.~K., and Winther,
  O. (2016).
\newblock Ladder variational autoencoders.
\newblock In {\em Advances in neural information processing systems}, pages
  3738--3746.

\bibitem[Tomczak and Welling, 2016]{tomczak2016improving}
Tomczak, J.~M. and Welling, M. (2016).
\newblock Improving variational auto-encoders using householder flow.
\newblock {\em arXiv preprint arXiv:1611.09630}.

\bibitem[Tomczak and Welling, 2017]{tomczak2017improving}
Tomczak, J.~M. and Welling, M. (2017).
\newblock Improving variational auto-encoders using convex combination linear
  inverse autoregressive flow.
\newblock In {\em Benelearn}.

\bibitem[Turner and Sahani, 2011]{turner2011two}
Turner, R.~E. and Sahani, M. (2011).
\newblock Two problems with variational expectation maximisation for
  time-series models.
\newblock {\em Bayesian Time series models}, 1(3.1):3--1.

\end{thebibliography}
