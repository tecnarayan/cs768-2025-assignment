\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel \& Ng(2004)Abbeel and Ng]{abbeel2004apprenticeship}
Abbeel, P. and Ng, A.~Y.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, pp.\ ~1, 2004.

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and
  Abbeel]{achiam2017constrained}
Achiam, J., Held, D., Tamar, A., and Abbeel, P.
\newblock Constrained policy optimization.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  22--31. JMLR. org, 2017.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock \emph{arXiv preprint arXiv:1707.01495}, 2017.

\bibitem[Cabi et~al.(2019)Cabi, Colmenarejo, Novikov, Konyushkova, Reed, Jeong,
  Zolna, Aytar, Budden, Vecerik, et~al.]{cabi2019scaling}
Cabi, S., Colmenarejo, S.~G., Novikov, A., Konyushkova, K., Reed, S., Jeong,
  R., Zolna, K., Aytar, Y., Budden, D., Vecerik, M., et~al.
\newblock Scaling data-driven robotics with reward sketching and batch
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1909.12200}, 2019.

\bibitem[Chebotar et~al.(2021)Chebotar, Hausman, Lu, Xiao, Kalashnikov, Varley,
  Irpan, Eysenbach, Julian, Finn, and Levine]{chebotar2021actionable}
Chebotar, Y., Hausman, K., Lu, Y., Xiao, T., Kalashnikov, D., Varley, J.,
  Irpan, A., Eysenbach, B., Julian, R., Finn, C., and Levine, S.
\newblock Actionable models: Unsupervised offline reinforcement learning of
  robotic skills.
\newblock \emph{arXiv preprint arXiv:2104.07749}, 2021.

\bibitem[Dasari et~al.(2020)Dasari, Ebert, Tian, Nair, Bucher, Schmeckpeper,
  Singh, Levine, and Finn]{dasari2020robonet}
Dasari, S., Ebert, F., Tian, S., Nair, S., Bucher, B., Schmeckpeper, K., Singh,
  S., Levine, S., and Finn, C.
\newblock Robonet: Large-scale multi-robot learning, 2020.

\bibitem[de~Lima \& Krohling(2021)de~Lima and Krohling]{de2021discovering}
de~Lima, L.~M. and Krohling, R.~A.
\newblock Discovering an aid policy to minimize student evasion using offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2104.10258}, 2021.

\bibitem[Dorfman et~al.(2021)Dorfman, Shenfeld, and Tamar]{dorfman2021offline}
Dorfman, R., Shenfeld, I., and Tamar, A.
\newblock Offline meta reinforcement learning--identifiability challenges and
  effective data collection strategies.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Duan et~al.(2020)Duan, Jia, and Wang]{duan2020minimax}
Duan, Y., Jia, Z., and Wang, M.
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2701--2709. PMLR, 2020.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{ernst2005tree}
Ernst, D., Geurts, P., and Wehenkel, L.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 6:\penalty0 503--556,
  2005.

\bibitem[Eysenbach et~al.(2020)Eysenbach, Geng, Levine, and
  Salakhutdinov]{eysenbach2020rewriting}
Eysenbach, B., Geng, X., Levine, S., and Salakhutdinov, R.
\newblock Rewriting history with inverse rl: Hindsight inference for policy
  improvement.
\newblock \emph{arXiv preprint arXiv:2002.11089}, 2020.

\bibitem[Eysenbach et~al.(2021)Eysenbach, Levine, and
  Salakhutdinov]{eysenbach2021replacing}
Eysenbach, B., Levine, S., and Salakhutdinov, R.
\newblock Replacing rewards with examples: Example-based policy search via
  recursive classification.
\newblock \emph{arXiv preprint arXiv:2103.12656}, 2021.

\bibitem[Finn et~al.(2016{\natexlab{a}})Finn, Levine, and
  Abbeel]{finn2016guided}
Finn, C., Levine, S., and Abbeel, P.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In \emph{International conference on machine learning}, pp.\  49--58.
  PMLR, 2016{\natexlab{a}}.

\bibitem[Finn et~al.(2016{\natexlab{b}})Finn, Tan, Duan, Darrell, Levine, and
  Abbeel]{finn2016deep}
Finn, C., Tan, X.~Y., Duan, Y., Darrell, T., Levine, S., and Abbeel, P.
\newblock Deep spatial autoencoders for visuomotor learning.
\newblock In \emph{2016 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  512--519. IEEE, 2016{\natexlab{b}}.

\bibitem[Fu et~al.(2018{\natexlab{a}})Fu, Luo, and Levine]{AIRLFu2018}
Fu, J., Luo, K., and Levine, S.
\newblock Learning robust rewards with adversarial inverse reinforcement
  learning.
\newblock \emph{International Conference on Learning Representations},
  2018{\natexlab{a}}.

\bibitem[Fu et~al.(2018{\natexlab{b}})Fu, Singh, Ghosh, Yang, and
  Levine]{VICEFu2018}
Fu, J., Singh, A., Ghosh, D., Yang, L., and Levine, S.
\newblock Variational inverse control with events: A general framework for
  data-driven reward definition.
\newblock \emph{Conference on Neural Information Processing Systems},
  2018{\natexlab{b}}.

\bibitem[Fu et~al.(2018{\natexlab{c}})Fu, Singh, Ghosh, Yang, and
  Levine]{fu2018variational}
Fu, J., Singh, A., Ghosh, D., Yang, L., and Levine, S.
\newblock Variational inverse control with events: A general framework for
  data-driven reward definition.
\newblock \emph{arXiv preprint arXiv:1805.11686}, 2018{\natexlab{c}}.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock D4rl: Datasets for deep data-driven reinforcement learning, 2020.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{fujimoto2021minimalist}
Fujimoto, S. and Gu, S.~S.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2106.06860}, 2021.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Meger, and Precup]{fujimoto2018off}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock \emph{arXiv preprint arXiv:1812.02900}, 2018.

\bibitem[Ghasemipour et~al.(2021)Ghasemipour, Schuurmans, and
  Gu]{ghasemipour2021emaq}
Ghasemipour, S. K.~S., Schuurmans, D., and Gu, S.~S.
\newblock Emaq: Expected-max q-learning operator for simple yet effective
  offline and online rl.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3682--3691. PMLR, 2021.

\bibitem[Ho \& Ermon(2016)Ho and Ermon]{GAIL2016Ho}
Ho, J. and Ermon, S.
\newblock Generative adversarial imitation learning.
\newblock \emph{Conference on Neural Information Processing Systems}, 2016.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2019way}
Jaques, N., Ghandeharioun, A., Shen, J.~H., Ferguson, C., Lapedriza, A., Jones,
  N., Gu, S., and Picard, R.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \emph{arXiv preprint arXiv:1907.00456}, 2019.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke,
  et~al.]{kalashnikov2018scalable}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E.,
  Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et~al.
\newblock Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock In \emph{Conference on Robot Learning}, pp.\  651--673. PMLR, 2018.

\bibitem[Kalashnikov et~al.(2021)Kalashnikov, Varley, Chebotar, Swanson,
  Jonschkowski, Finn, Levine, and Hausman]{kalashnikov2021mt}
Kalashnikov, D., Varley, J., Chebotar, Y., Swanson, B., Jonschkowski, R., Finn,
  C., Levine, S., and Hausman, K.
\newblock Mt-opt: Continuous multi-task robotic reinforcement learning at
  scale.
\newblock \emph{Conference on Robot Learning (CoRL)}, 2021.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T.
\newblock Morel: Model-based offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2005.05951}, 2020.

\bibitem[Konyushkova et~al.(2020)Konyushkova, Zolna, Aytar, Novikov, Reed,
  Cabi, and de~Freitas]{konyushkova2020semi}
Konyushkova, K., Zolna, K., Aytar, Y., Novikov, A., Reed, S., Cabi, S., and
  de~Freitas, N.
\newblock Semi-supervised reward learning for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2012.06899}, 2020.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Fergus, Tompson, and
  Nachum]{kostrikov2021offline}
Kostrikov, I., Fergus, R., Tompson, J., and Nachum, O.
\newblock Offline reinforcement learning with fisher divergence critic
  regularization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5774--5783. PMLR, 2021.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11761--11771, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.04779}, 2020.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{LangeGR12}
Lange, S., Gabel, T., and Riedmiller, M.~A.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement Learning}, volume~12. Springer, 2012.

\bibitem[Laroche et~al.(2019)Laroche, Trichelair, and
  Des~Combes]{laroche2019safe}
Laroche, R., Trichelair, P., and Des~Combes, R.~T.
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3652--3661. PMLR, 2019.

\bibitem[Lee et~al.(2021)Lee, Lee, and Kim]{lee2021representation}
Lee, B.-J., Lee, J., and Kim, K.-E.
\newblock Representation balancing offline model-based reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=QpNz8r_Ri2Y}.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Li et~al.(2020)Li, Pinto, and Abbeel]{li2020generalized}
Li, A.~C., Pinto, L., and Abbeel, P.
\newblock Generalized hindsight for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.11708}, 2020.

\bibitem[Li et~al.(2019)Li, Vuong, Liu, Liu, Ciosek, Ross, Christensen, and
  Su]{li2019multi}
Li, J., Vuong, Q., Liu, S., Liu, M., Ciosek, K., Ross, K., Christensen, H.~I.,
  and Su, H.
\newblock Multi-task batch reinforcement learning with metric learning.
\newblock \emph{arXiv preprint arXiv:1909.11373}, 2019.

\bibitem[Lin et~al.(2019)Lin, Baweja, and Held]{lin2019reinforcement}
Lin, X., Baweja, H.~S., and Held, D.
\newblock Reinforcement learning without ground-truth state.
\newblock \emph{arXiv preprint arXiv:1905.07866}, 2019.

\bibitem[Liu et~al.(2019)Liu, Trott, Socher, and Xiong]{liu2019competitive}
Liu, H., Trott, A., Socher, R., and Xiong, C.
\newblock Competitive experience replay.
\newblock \emph{arXiv preprint arXiv:1902.00528}, 2019.

\bibitem[Liu et~al.(2020)Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E.
\newblock Provably good batch reinforcement learning without great exploration.
\newblock \emph{arXiv preprint arXiv:2007.08202}, 2020.

\bibitem[Mitchell et~al.(2021)Mitchell, Rafailov, Peng, Levine, and
  Finn]{mitchell2021offline}
Mitchell, E., Rafailov, R., Peng, X.~B., Levine, S., and Finn, C.
\newblock Offline meta-reinforcement learning with advantage weighting.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7780--7791. PMLR, 2021.

\bibitem[Ng \& Russell(2000)Ng and Russell]{ng2000irl}
Ng, A.~Y. and Russell, S.~J.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Proceedings of the Seventeenth International Conference on
  Machine Learning}, ICML '00, 2000.

\bibitem[Pomerleau(1988)]{pomerleau1988alvinn}
Pomerleau, D.~A.
\newblock Alvinn: an autonomous land vehicle in a neural network.
\newblock In \emph{Proceedings of the 1st International Conference on Neural
  Information Processing Systems}, pp.\  305--313, 1988.

\bibitem[Rafailov et~al.(2021)Rafailov, Yu, Rajeswaran, and
  Finn]{Rafailov2020LOMPO}
Rafailov, R., Yu, T., Rajeswaran, A., and Finn, C.
\newblock Offline reinforcement learning from images with latent space models.
\newblock \emph{Learning for Decision Making and Control (L4DC)}, 2021.

\bibitem[Riedmiller(2005)]{riedmiller2005neural}
Riedmiller, M.
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In \emph{European Conference on Machine Learning}, pp.\  317--328.
  Springer, 2005.

\bibitem[Ross \& Bagnell(2012)Ross and Bagnell]{RossB12}
Ross, S. and Bagnell, D.
\newblock Agnostic system identification for model-based reinforcement
  learning.
\newblock In \emph{ICML}, 2012.

\bibitem[Shortreed et~al.(2011)Shortreed, Laber, Lizotte, Stroup, Pineau, and
  Murphy]{shortreed2011informing}
Shortreed, S.~M., Laber, E., Lizotte, D.~J., Stroup, T.~S., Pineau, J., and
  Murphy, S.~A.
\newblock Informing sequential clinical decision-making through reinforcement
  learning: an empirical study.
\newblock \emph{Machine learning}, 84\penalty0 (1-2):\penalty0 109--136, 2011.

\bibitem[Singh et~al.(2019)Singh, Yang, Hartikainen, Finn, and
  Levine]{singh2019end}
Singh, A., Yang, L., Hartikainen, K., Finn, C., and Levine, S.
\newblock End-to-end robotic reinforcement learning without reward engineering.
\newblock \emph{arXiv preprint arXiv:1904.07854}, 2019.

\bibitem[Singh et~al.(2020)Singh, Yu, Yang, Zhang, Kumar, and
  Levine]{singh2020cog}
Singh, A., Yu, A., Yang, J., Zhang, J., Kumar, A., and Levine, S.
\newblock Cog: Connecting new skills to past experience with offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.14500}, 2020.

\bibitem[Sun et~al.(2019)Sun, Li, Liu, Lin, and Zhou]{sun2019policy}
Sun, H., Li, Z., Liu, X., Lin, D., and Zhou, B.
\newblock Policy continuation with hindsight inverse dynamics.
\newblock \emph{arXiv preprint arXiv:1910.14055}, 2019.

\bibitem[Swazinna et~al.(2020)Swazinna, Udluft, and
  Runkler]{swazinna2020overcoming}
Swazinna, P., Udluft, S., and Runkler, T.
\newblock Overcoming model bias for robust offline deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2008.05533}, 2020.

\bibitem[Wang et~al.(2018)Wang, Zhang, He, and Zha]{Wang2018SupervisedRL}
Wang, L., Zhang, W., He, X., and Zha, H.
\newblock Supervised reinforcement learning with recurrent neural network for
  dynamic treatment recommendation.
\newblock \emph{Proceedings of the 24th ACM SIGKDD International Conference on
  Knowledge Discovery \& Data Mining}, 2018.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Xie et~al.(2018)Xie, Singh, Levine, and Finn]{xie2018few}
Xie, A., Singh, A., Levine, S., and Finn, C.
\newblock Few-shot goal inference for visuomotor learning and planning.
\newblock In \emph{Conference on Robot Learning}, pp.\  40--52. PMLR, 2018.

\bibitem[Xie et~al.(2019)Xie, Ebert, Levine, and Finn]{xie2019improvisation}
Xie, A., Ebert, F., Levine, S., and Finn, C.
\newblock Improvisation through physical understanding: Using novel objects as
  tools with visual foresight.
\newblock \emph{Robotics: Science and Systems (RSS)}, 2019.

\bibitem[Yang \& Nachum(2021)Yang and Nachum]{yang2021representation}
Yang, M. and Nachum, O.
\newblock Representation matters: Offline pretraining for sequential decision
  making.
\newblock \emph{arXiv preprint arXiv:2102.05815}, 2021.

\bibitem[Yang et~al.(2021)Yang, Levine, and Nachum]{yang2021trail}
Yang, M., Levine, S., and Nachum, O.
\newblock Trail: Near-optimal imitation learning with suboptimal data.
\newblock \emph{arXiv preprint arXiv:2110.14770}, 2021.

\bibitem[Yu et~al.(2020{\natexlab{a}})Yu, Quillen, He, Julian, Hausman, Finn,
  and Levine]{yu2020meta}
Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pp.\  1094--1100. PMLR,
  2020{\natexlab{a}}.

\bibitem[Yu et~al.(2020{\natexlab{b}})Yu, Quillen, He, Julian, Hausman, Finn,
  and Levine]{yu2020metaworld}
Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pp.\  1094--1100. PMLR,
  2020{\natexlab{b}}.

\bibitem[Yu et~al.(2020{\natexlab{c}})Yu, Thomas, Yu, Ermon, Zou, Levine, Finn,
  and Ma]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine, S., Finn, C., and Ma,
  T.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{arXiv preprint arXiv:2005.13239}, 2020{\natexlab{c}}.

\bibitem[Yu et~al.(2021{\natexlab{a}})Yu, Kumar, Chebotar, Hausman, Levine, and
  Finn]{yu2021conservative}
Yu, T., Kumar, A., Chebotar, Y., Hausman, K., Levine, S., and Finn, C.
\newblock Conservative data sharing for multi-task offline reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2109.08128}, 2021{\natexlab{a}}.

\bibitem[Yu et~al.(2021{\natexlab{b}})Yu, Kumar, Rafailov, Rajeswaran, Levine,
  and Finn]{yu2021combo}
Yu, T., Kumar, A., Rafailov, R., Rajeswaran, A., Levine, S., and Finn, C.
\newblock Combo: Conservative offline model-based policy optimization.
\newblock \emph{arXiv preprint arXiv:2102.08363}, 2021{\natexlab{b}}.

\bibitem[Zhan et~al.(2021)Zhan, Xu, Zhang, Huo, Zhu, Yin, and
  Zheng]{zhan2021deepthermal}
Zhan, X., Xu, H., Zhang, Y., Huo, Y., Zhu, X., Yin, H., and Zheng, Y.
\newblock Deepthermal: Combustion optimization for thermal power generating
  units using offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2102.11492}, 2021.

\bibitem[Zhou et~al.(2020)Zhou, Bajracharya, and Held]{zhou2020plas}
Zhou, W., Bajracharya, S., and Held, D.
\newblock Plas: Latent action space for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2011.07213}, 2020.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Aaai}, volume~8, pp.\  1433--1438. Chicago, IL, USA, 2008.

\end{thebibliography}
