\begin{thebibliography}{}

\bibitem[Agrawal et~al., 2016]{agrawal2016learning}
Agrawal, P., Nair, A.~V., Abbeel, P., Malik, J., and Levine, S. (2016).
\newblock Learning to poke by poking: Experiential learning of intuitive
  physics.
\newblock In {\em Advances in neural information processing systems}, pages
  5074--5082.

\bibitem[Andrychowicz et~al., 2017]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Abbeel, O.~P., and Zaremba, W. (2017).
\newblock Hindsight experience replay.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5048--5058.

\bibitem[Bajcsy et~al., 2017]{bajcsy2017learning}
Bajcsy, A., Losey, D.~P., Oâ€™Malley, M.~K., and Dragan, A.~D. (2017).
\newblock Learning robot objectives from physical human interaction.
\newblock {\em Proceedings of Machine Learning Research}, 78:217--226.

\bibitem[Barreto et~al., 2017]{barreto2017successor}
Barreto, A., Dabney, W., Munos, R., Hunt, J.~J., Schaul, T., van Hasselt,
  H.~P., and Silver, D. (2017).
\newblock Successor features for transfer in reinforcement learning.
\newblock In {\em Advances in neural information processing systems}, pages
  4055--4065.

\bibitem[Bengio et~al., 2017]{bengio2017independently}
Bengio, E., Thomas, V., Pineau, J., Precup, D., and Bengio, Y. (2017).
\newblock Independently controllable features.
\newblock {\em arXiv preprint arXiv:1703.07718}.

\bibitem[Berner et~al., 2019]{berner2019dota}
Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C.,
  Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et~al. (2019).
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1912.06680}.

\bibitem[Brown et~al., 2019]{brown2019extrapolating}
Brown, D.~S., Goo, W., Nagarajan, P., and Niekum, S. (2019).
\newblock Extrapolating beyond suboptimal demonstrations via inverse
  reinforcement learning from observations.
\newblock {\em arXiv preprint arXiv:1904.06387}.

\bibitem[Chen and Batmanghelich, 2019]{chen2019weakly}
Chen, J. and Batmanghelich, K. (2019).
\newblock Weakly supervised disentanglement by pairwise similarities.
\newblock {\em arXiv preprint arXiv:1906.01044}.

\bibitem[Chen et~al., 2018]{chen2018isolating}
Chen, T.~Q., Li, X., Grosse, R.~B., and Duvenaud, D.~K. (2018).
\newblock Isolating sources of disentanglement in variational autoencoders.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2610--2620.

\bibitem[Chen et~al., 2016]{chen2016infogan}
Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P.
  (2016).
\newblock Infogan: Interpretable representation learning by information
  maximizing generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, pages
  2172--2180.

\bibitem[Christiano et~al., 2017]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
  (2017).
\newblock Deep reinforcement learning from human preferences.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4299--4307.

\bibitem[Dasari et~al., 2019]{dasari2019robonet}
Dasari, S., Ebert, F., Tian, S., Nair, S., Bucher, B., Schmeckpeper, K., Singh,
  S., Levine, S., and Finn, C. (2019).
\newblock Robonet: Large-scale multi-robot learning.
\newblock {\em arXiv preprint arXiv:1910.11215}.

\bibitem[Dilokthanakul et~al., 2019]{dilokthanakul2019feature}
Dilokthanakul, N., Kaplanis, C., Pawlowski, N., and Shanahan, M. (2019).
\newblock Feature control as intrinsic motivation for hierarchical
  reinforcement learning.
\newblock {\em IEEE transactions on neural networks and learning systems},
  30(11):3409--3418.

\bibitem[Dosovitskiy and Koltun, 2016]{dosovitskiy2016learning}
Dosovitskiy, A. and Koltun, V. (2016).
\newblock Learning to act by predicting the future.
\newblock {\em arXiv preprint arXiv:1611.01779}.

\bibitem[Duan et~al., 2017]{duan2017one}
Duan, Y., Andrychowicz, M., Stadie, B., Ho, O.~J., Schneider, J., Sutskever,
  I., Abbeel, P., and Zaremba, W. (2017).
\newblock One-shot imitation learning.
\newblock In {\em Advances in neural information processing systems}, pages
  1087--1098.

\bibitem[Esmaeili et~al., 2018]{esmaeili2018structured}
Esmaeili, B., Wu, H., Jain, S., Bozkurt, A., Siddharth, N., Paige, B., Brooks,
  D.~H., Dy, J., and van~de Meent, J.-W. (2018).
\newblock Structured disentangled representations.
\newblock {\em arXiv preprint arXiv:1804.02086}.

\bibitem[Eysenbach et~al., 2019]{eysenbach2019search}
Eysenbach, B., Salakhutdinov, R.~R., and Levine, S. (2019).
\newblock Search on the replay buffer: Bridging planning and reinforcement
  learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  15220--15231.

\bibitem[Finn et~al., 2017]{finn2017model}
Finn, C., Abbeel, P., and Levine, S. (2017).
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1126--1135. JMLR. org.

\bibitem[Finn and Levine, 2017]{finn2017deep}
Finn, C. and Levine, S. (2017).
\newblock Deep visual foresight for planning robot motion.
\newblock In {\em 2017 IEEE International Conference on Robotics and Automation
  (ICRA)}, pages 2786--2793. IEEE.

\bibitem[Finn et~al., 2016a]{finn2016guided}
Finn, C., Levine, S., and Abbeel, P. (2016a).
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In {\em International Conference on Machine Learning}, pages 49--58.

\bibitem[Finn et~al., 2016b]{finn2016deep}
Finn, C., Tan, X.~Y., Duan, Y., Darrell, T., Levine, S., and Abbeel, P.
  (2016b).
\newblock Deep spatial autoencoders for visuomotor learning.
\newblock In {\em 2016 IEEE International Conference on Robotics and Automation
  (ICRA)}, pages 512--519. IEEE.

\bibitem[Florensa et~al., 2019]{florensa2019self}
Florensa, C., Degrave, J., Heess, N., Springenberg, J.~T., and Riedmiller, M.
  (2019).
\newblock Self-supervised learning of image embedding for continuous control.
\newblock {\em arXiv preprint arXiv:1901.00943}.

\bibitem[Florensa et~al., 2017]{florensa2017automatic}
Florensa, C., Held, D., Geng, X., and Abbeel, P. (2017).
\newblock Automatic goal generation for reinforcement learning agents.
\newblock {\em arXiv preprint arXiv:1705.06366}.

\bibitem[Fu et~al., 2017]{fu2017learning}
Fu, J., Luo, K., and Levine, S. (2017).
\newblock Learning robust rewards with adversarial inverse reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1710.11248}.

\bibitem[Fu et~al., 2018]{fu2018variational}
Fu, J., Singh, A., Ghosh, D., Yang, L., and Levine, S. (2018).
\newblock Variational inverse control with events: A general framework for
  data-driven reward definition.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8538--8547.

\bibitem[Gabbay and Hoshen, 2019]{gabbay2019latent}
Gabbay, A. and Hoshen, Y. (2019).
\newblock Latent optimization for non-adversarial representation
  disentanglement.
\newblock {\em arXiv preprint arXiv:1906.11796}.

\bibitem[Gelada et~al., 2019]{gelada2019deepmdp}
Gelada, C., Kumar, S., Buckman, J., Nachum, O., and Bellemare, M.~G. (2019).
\newblock Deepmdp: Learning continuous latent space models for representation
  learning.
\newblock {\em arXiv preprint arXiv:1906.02736}.

\bibitem[Ghasemipour et~al., 2019]{ghasemipour2019divergence}
Ghasemipour, S. K.~S., Zemel, R., and Gu, S. (2019).
\newblock A divergence minimization perspective on imitation learning methods.
\newblock {\em Conference on Robot Learning (CoRL)}.

\bibitem[Gilpin et~al., 2018]{gilpin2018explaining}
Gilpin, L.~H., Bau, D., Yuan, B.~Z., Bajwa, A., Specter, M., and Kagal, L.
  (2018).
\newblock Explaining explanations: An overview of interpretability of machine
  learning.
\newblock In {\em 2018 IEEE 5th International Conference on data science and
  advanced analytics (DSAA)}, pages 80--89. IEEE.

\bibitem[Gordon et~al., 2018]{gordon2018iqa}
Gordon, D., Kembhavi, A., Rastegari, M., Redmon, J., Fox, D., and Farhadi, A.
  (2018).
\newblock Iqa: Visual question answering in interactive environments.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4089--4098.

\bibitem[Goyal et~al., 2019]{goyal2019infobot}
Goyal, A., Islam, R., Strouse, D., Ahmed, Z., Botvinick, M., Larochelle, H.,
  Levine, S., and Bengio, Y. (2019).
\newblock Infobot: Transfer and exploration via the information bottleneck.
\newblock {\em arXiv preprint arXiv:1901.10902}.

\bibitem[Gu et~al., 2017]{gu2017deep}
Gu, S., Holly, E., Lillicrap, T., and Levine, S. (2017).
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock In {\em 2017 IEEE international conference on robotics and automation
  (ICRA)}, pages 3389--3396. IEEE.

\bibitem[Haarnoja et~al., 2018]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018).
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock {\em arXiv preprint arXiv:1801.01290}.

\bibitem[Hadfield-Menell et~al., 2017]{hadfield2017inverse}
Hadfield-Menell, D., Milli, S., Abbeel, P., Russell, S.~J., and Dragan, A.
  (2017).
\newblock Inverse reward design.
\newblock In {\em Advances in neural information processing systems}, pages
  6765--6774.

\bibitem[Hafner et~al., 2018]{hafner2018learning}
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and
  Davidson, J. (2018).
\newblock Learning latent dynamics for planning from pixels.
\newblock {\em arXiv preprint arXiv:1811.04551}.

\bibitem[Hartikainen et~al., 2019]{hartikainen2019dynamical}
Hartikainen, K., Geng, X., Haarnoja, T., and Levine, S. (2019).
\newblock Dynamical distance learning for unsupervised and semi-supervised
  skill discovery.
\newblock {\em arXiv preprint arXiv:1907.08225}.

\bibitem[Hausman et~al., 2018]{hausman2018learning}
Hausman, K., Springenberg, J.~T., Wang, Z., Heess, N., and Riedmiller, M.
  (2018).
\newblock Learning an embedding space for transferable robot skills.

\bibitem[Hazan et~al., 2018]{hazan2018provably}
Hazan, E., Kakade, S.~M., Singh, K., and Van~Soest, A. (2018).
\newblock Provably efficient maximum entropy exploration.
\newblock {\em arXiv preprint arXiv:1812.02690}.

\bibitem[Higgins et~al., 2018]{higgins2018towards}
Higgins, I., Amos, D., Pfau, D., Racaniere, S., Matthey, L., Rezende, D., and
  Lerchner, A. (2018).
\newblock Towards a definition of disentangled representations.
\newblock {\em arXiv preprint arXiv:1812.02230}.

\bibitem[Higgins et~al., 2017]{higgins2017beta}
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M.,
  Mohamed, S., and Lerchner, A. (2017).
\newblock beta-vae: Learning basic visual concepts with a constrained
  variational framework.
\newblock {\em Iclr}, 2(5):6.

\bibitem[Jaderberg et~al., 2016]{jaderberg2016reinforcement}
Jaderberg, M., Mnih, V., Czarnecki, W.~M., Schaul, T., Leibo, J.~Z., Silver,
  D., and Kavukcuoglu, K. (2016).
\newblock Reinforcement learning with unsupervised auxiliary tasks.
\newblock {\em arXiv preprint arXiv:1611.05397}.

\bibitem[Jain et~al., 2015]{jain2015learning}
Jain, A., Sharma, S., Joachims, T., and Saxena, A. (2015).
\newblock Learning preferences for manipulation tasks from online coactive
  feedback.
\newblock {\em The International Journal of Robotics Research},
  34(10):1296--1313.

\bibitem[Kaelbling, 1993]{kaelbling1993learning}
Kaelbling, L.~P. (1993).
\newblock Learning to achieve goals.
\newblock In {\em IJCAI}, pages 1094--1099. Citeseer.

\bibitem[Kaiser et~al., 2019]{kaiser2019model}
Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R.~H.,
  Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., et~al.
  (2019).
\newblock Model-based reinforcement learning for atari.
\newblock {\em arXiv preprint arXiv:1903.00374}.

\bibitem[Kim and Mnih, 2018]{kim2018disentangling}
Kim, H. and Mnih, A. (2018).
\newblock Disentangling by factorising.
\newblock {\em arXiv preprint arXiv:1802.05983}.

\bibitem[Lake et~al., 2017]{lake2017building}
Lake, B.~M., Ullman, T.~D., Tenenbaum, J.~B., and Gershman, S.~J. (2017).
\newblock Building machines that learn and think like people.
\newblock {\em Behavioral and brain sciences}, 40.

\bibitem[Laskey et~al., 2017]{laskey2017dart}
Laskey, M., Lee, J., Fox, R., Dragan, A., and Goldberg, K. (2017).
\newblock Dart: Noise injection for robust imitation learning.
\newblock {\em arXiv preprint arXiv:1703.09327}.

\bibitem[Lee et~al., 2019a]{lee2019stochastic}
Lee, A.~X., Nagabandi, A., Abbeel, P., and Levine, S. (2019a).
\newblock Stochastic latent actor-critic: Deep reinforcement learning with a
  latent variable model.
\newblock {\em arXiv preprint arXiv:1907.00953}.

\bibitem[Lee et~al., 2019b]{lee2019efficient}
Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., and Salakhutdinov,
  R. (2019b).
\newblock Efficient exploration via state marginal matching.
\newblock {\em arXiv preprint arXiv:1906.05274}.

\bibitem[Locatello et~al., 2018]{locatello2018challenging}
Locatello, F., Bauer, S., Lucic, M., R{\"a}tsch, G., Gelly, S., Sch{\"o}lkopf,
  B., and Bachem, O. (2018).
\newblock Challenging common assumptions in the unsupervised learning of
  disentangled representations.
\newblock {\em arXiv preprint arXiv:1811.12359}.

\bibitem[Machado et~al., 2017]{machado2017laplacian}
Machado, M.~C., Bellemare, M.~G., and Bowling, M. (2017).
\newblock A laplacian framework for option discovery in reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 2295--2304. JMLR. org.

\bibitem[Mahadevan, 2005]{mahadevan2005proto}
Mahadevan, S. (2005).
\newblock Proto-value functions: Developmental reinforcement learning.
\newblock In {\em Proceedings of the 22nd international conference on Machine
  learning}, pages 553--560. ACM.

\bibitem[Mandlekar et~al., 2018]{mandlekar2018roboturk}
Mandlekar, A., Zhu, Y., Garg, A., Booher, J., Spero, M., Tung, A., Gao, J.,
  Emmons, J., Gupta, A., Orbay, E., et~al. (2018).
\newblock Roboturk: A crowdsourcing platform for robotic skill learning through
  imitation.
\newblock {\em arXiv preprint arXiv:1811.02790}.

\bibitem[Matthey et~al., 2017]{dsprites17}
Matthey, L., Higgins, I., Hassabis, D., and Lerchner, A. (2017).
\newblock dsprites: Disentanglement testing sprites dataset.
\newblock https://github.com/deepmind/dsprites-dataset/.

\bibitem[Milli et~al., 2017]{milli2017should}
Milli, S., Hadfield-Menell, D., Dragan, A., and Russell, S. (2017).
\newblock Should robots be obedient?
\newblock {\em arXiv preprint arXiv:1705.09990}.

\bibitem[Nachum et~al., 2018]{nachum2018near}
Nachum, O., Gu, S., Lee, H., and Levine, S. (2018).
\newblock Near-optimal representation learning for hierarchical reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1810.01257}.

\bibitem[Nair et~al., 2018]{nair2018visual}
Nair, A.~V., Pong, V., Dalal, M., Bahl, S., Lin, S., and Levine, S. (2018).
\newblock Visual reinforcement learning with imagined goals.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9191--9200.

\bibitem[Pathak et~al., 2017]{pathak2017curiosity}
Pathak, D., Agrawal, P., Efros, A.~A., and Darrell, T. (2017).
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 16--17.

\bibitem[Pathak et~al., 2018]{pathak2018zero}
Pathak, D., Mahmoudieh, P., Luo, G., Agrawal, P., Chen, D., Shentu, Y.,
  Shelhamer, E., Malik, J., Efros, A.~A., and Darrell, T. (2018).
\newblock Zero-shot visual imitation.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 2050--2053.

\bibitem[Pong et~al., 2018]{pong2018temporal}
Pong, V., Gu, S., Dalal, M., and Levine, S. (2018).
\newblock Temporal difference models: Model-free deep rl for model-based
  control.
\newblock {\em arXiv preprint arXiv:1802.09081}.

\bibitem[Pong et~al., 2019]{pong2019skew}
Pong, V.~H., Dalal, M., Lin, S., Nair, A., Bahl, S., and Levine, S. (2019).
\newblock Skew-fit: State-covering self-supervised reinforcement learning.
\newblock {\em arXiv preprint arXiv:1903.03698}.

\bibitem[Pugh et~al., 2016]{pugh2016quality}
Pugh, J.~K., Soros, L.~B., and Stanley, K.~O. (2016).
\newblock Quality diversity: A new frontier for evolutionary computation.
\newblock {\em Frontiers in Robotics and AI}, 3:40.

\bibitem[Ratliff et~al., 2006]{ratliff2006maximum}
Ratliff, N.~D., Bagnell, J.~A., and Zinkevich, M.~A. (2006).
\newblock Maximum margin planning.
\newblock In {\em Proceedings of the 23rd international conference on Machine
  learning}, pages 729--736.

\bibitem[Ren et~al., 2019]{ren2019exploration}
Ren, Z., Dong, K., Zhou, Y., Liu, Q., and Peng, J. (2019).
\newblock Exploration via hindsight goal generation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  13464--13474.

\bibitem[Savinov et~al., 2018]{savinov2018semi}
Savinov, N., Dosovitskiy, A., and Koltun, V. (2018).
\newblock Semi-parametric topological memory for navigation.
\newblock {\em arXiv preprint arXiv:1803.00653}.

\bibitem[Schaul et~al., 2015]{schaul2015universal}
Schaul, T., Horgan, D., Gregor, K., and Silver, D. (2015).
\newblock Universal value function approximators.
\newblock In {\em International conference on machine learning}, pages
  1312--1320.

\bibitem[Shelhamer et~al., 2016]{shelhamer2016loss}
Shelhamer, E., Mahmoudieh, P., Argus, M., and Darrell, T. (2016).
\newblock Loss is its own reward: Self-supervision for reinforcement learning.
\newblock {\em arXiv preprint arXiv:1612.07307}.

\bibitem[Shu et~al., 2019]{shu2019weakly}
Shu, R., Chen, Y., Kumar, A., Ermon, S., and Poole, B. (2019).
\newblock Weakly supervised disentanglement with guarantees.
\newblock {\em arXiv preprint arXiv:1910.09772}.

\bibitem[Silver et~al., 2017]{silver2017mastering}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al. (2017).
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock {\em arXiv preprint arXiv:1712.01815}.

\bibitem[Singh et~al., 2019]{singh2019end}
Singh, A., Yang, L., Hartikainen, K., Finn, C., and Levine, S. (2019).
\newblock End-to-end robotic reinforcement learning without reward engineering.
\newblock {\em arXiv preprint arXiv:1904.07854}.

\bibitem[Stanley and Miikkulainen, 2002]{stanley2002evolving}
Stanley, K.~O. and Miikkulainen, R. (2002).
\newblock Evolving neural networks through augmenting topologies.
\newblock {\em Evolutionary computation}, 10(2):99--127.

\bibitem[Thomas et~al., 2017]{thomas2017independently}
Thomas, V., Pondard, J., Bengio, E., Sarfati, M., Beaudoin, P., Meurs, M.-J.,
  Pineau, J., Precup, D., and Bengio, Y. (2017).
\newblock Independently controllable features.
\newblock {\em arXiv preprint arXiv:1708.01289}.

\bibitem[van Steenkiste et~al., 2019]{van2019disentangled}
van Steenkiste, S., Locatello, F., Schmidhuber, J., and Bachem, O. (2019).
\newblock Are disentangled representations helpful for abstract visual
  reasoning?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  14222--14235.

\bibitem[Vecerik et~al., 2019]{vecerik2019practical}
Vecerik, M., Sushkov, O., Barker, D., Roth{\"o}rl, T., Hester, T., and Scholz,
  J. (2019).
\newblock A practical approach to insertion with variable socket position using
  deep reinforcement learning.
\newblock In {\em 2019 International Conference on Robotics and Automation
  (ICRA)}, pages 754--760. IEEE.

\bibitem[Venkattaramanujam et~al., 2019]{venkattaramanujam2019self}
Venkattaramanujam, S., Crawford, E., Doan, T., and Precup, D. (2019).
\newblock Self-supervised learning of distance functions for goal-conditioned
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1907.02998}.

\bibitem[Vinyals et~al., 2019]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al. (2019).
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock {\em Nature}, 575(7782):350--354.

\bibitem[Watter et~al., 2015]{watter2015embed}
Watter, M., Springenberg, J., Boedecker, J., and Riedmiller, M. (2015).
\newblock Embed to control: A locally linear latent dynamics model for control
  from raw images.
\newblock In {\em Advances in neural information processing systems}, pages
  2746--2754.

\bibitem[Xie et~al., 2018]{xie2018few}
Xie, A., Singh, A., Levine, S., and Finn, C. (2018).
\newblock Few-shot goal inference for visuomotor learning and planning.
\newblock {\em arXiv preprint arXiv:1810.00482}.

\bibitem[Yaman et~al., 2010]{yaman2010learning}
Yaman, F., Walsh, T.~J., Littman, M.~L., et~al. (2010).
\newblock Learning lexicographic preference models.
\newblock In {\em Preference learning}, pages 251--272. Springer.

\bibitem[Yarats et~al., 2019]{yarats2019improving}
Yarats, D., Zhang, A., Kostrikov, I., Amos, B., Pineau, J., and Fergus, R.
  (2019).
\newblock Improving sample efficiency in model-free reinforcement learning from
  images.
\newblock {\em arXiv preprint arXiv:1910.01741}.

\bibitem[Yu et~al., 2019]{yu2019unsupervised}
Yu, T., Shevchuk, G., Sadigh, D., and Finn, C. (2019).
\newblock Unsupervised visuomotor control through distributional planning
  networks.
\newblock {\em arXiv preprint arXiv:1902.05542}.

\bibitem[Zhang et~al., 2018]{zhang2018solar}
Zhang, M., Vikram, S., Smith, L., Abbeel, P., Johnson, M.~J., and Levine, S.
  (2018).
\newblock Solar: Deep structured latent representations for model-based
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1808.09105}.

\end{thebibliography}
