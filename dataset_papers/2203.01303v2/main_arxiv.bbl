\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Yasin Abbasi-Yadkori, D{\'a}vid P{\'a}l, and Csaba Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \emph{Advances in neural information processing systems},
  volume~24, 2011.

\bibitem[Ash et~al.(2021)Ash, Zhang, Goel, Krishnamurthy, and
  Kakade]{ash2021anti}
Jordan~T. Ash, Cyril Zhang, Surbhi Goel, Akshay Krishnamurthy, and Sham Kakade.
\newblock Anti-concentrated confidence bonuses for scalable exploration.
\newblock \emph{arXiv preprint arXiv:2110.11202}, 2021.

\bibitem[Casella and George(1992)]{casella1992explaining}
George Casella and Edward~I George.
\newblock Explaining the {G}ibbs sampler.
\newblock \emph{The American Statistician}, 46\penalty0 (3):\penalty0 167--174,
  1992.

\bibitem[Chapelle and Li(2011)]{chapelle2011empirical}
Olivier Chapelle and Lihong Li.
\newblock An empirical evaluation of {T}hompson sampling.
\newblock In \emph{Advances in neural information processing systems},
  volume~24, 2011.

\bibitem[Cover and Thomas(2006)]{cover2006elements}
Thomas~M. Cover and Joy~A. Thomas.
\newblock \emph{Elements of Information Theory}.
\newblock A Wiley-Interscience publication. Wiley, 2006.

\bibitem[Dimakopoulou and Van~Roy(2018)]{pmlr-v80-dimakopoulou18a}
Maria Dimakopoulou and Benjamin Van~Roy.
\newblock Coordinated exploration in concurrent reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, volume~80,
  pages 1271--1279, 2018.

\bibitem[Dwaracherla et~al.(2020)Dwaracherla, Lu, Ibrahimi, Osband, Wen, and
  Van~Roy]{Dwaracherla2020Hypermodels}
Vikranth Dwaracherla, Xiuyuan Lu, Morteza Ibrahimi, Ian Osband, Zheng Wen, and
  Benjamin Van~Roy.
\newblock Hypermodels for exploration.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Eckles and Kaptein(2019)]{eckles2019}
Dean Eckles and Maurits Kaptein.
\newblock Bootstrap {T}hompson sampling and sequential decision problems in the
  behavioral sciences.
\newblock \emph{SAGE Open}, 9\penalty0 (2), 2019.

\bibitem[Hao et~al.(2020)Hao, Zhou, Wen, and Sun]{hao2020low}
Botao Hao, Jie Zhou, Zheng Wen, and Will~Wei Sun.
\newblock Low-rank tensor bandits.
\newblock \emph{arXiv preprint arXiv:2007.15788}, 2020.

\bibitem[Honorio and Jaakkola(2014)]{pmlr-v33-honorio14}
Jean Honorio and Tommi Jaakkola.
\newblock {Tight bounds for the expected risk of linear classifiers and
  PAC-Bayes finite-sample guarantees}.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, volume~33, pages 384--392, 2014.

\bibitem[Huang et~al.(2022)Huang, Lam, Meisami, and
  Zhang]{huang2022generalized}
Ziyi Huang, Henry Lam, Amirhossein Meisami, and Haofeng Zhang.
\newblock Generalized {B}ayesian upper confidence bound with approximate
  inference for bandit problems.
\newblock \emph{arXiv preprint arXiv:2201.12955}, 2022.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems},
  volume~31, 2018.

\bibitem[Kveton et~al.(2019)Kveton, Szepesv{\'a}ri, Vaswani, Wen, Lattimore,
  and Ghavamzadeh]{kveton2019garbage}
Branislav Kveton, Csaba Szepesv{\'a}ri, Sharan Vaswani, Zheng Wen, Tor
  Lattimore, and Mohammad Ghavamzadeh.
\newblock Garbage in, reward out: Bootstrapping exploration in multi-armed
  bandits.
\newblock In \emph{International Conference on Machine Learning}, pages
  3601--3610, 2019.

\bibitem[Kveton et~al.(2020)Kveton, Szepesv{\'{a}}ri, Ghavamzadeh, and
  Boutilier]{pmlr-v115-kveton20a}
Branislav Kveton, Csaba Szepesv{\'{a}}ri, Mohammad Ghavamzadeh, and Craig
  Boutilier.
\newblock Perturbed-history exploration in stochastic linear bandits.
\newblock In \emph{Proceedings of The 35th Uncertainty in Artificial
  Intelligence Conference}, volume 115, pages 530--540, 2020.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Lee et~al.(2017)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-Dickstein]{lee2017deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S Schoenholz, Jeffrey
  Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock \emph{arXiv preprint arXiv:1711.00165}, 2017.

\bibitem[Lu and Van~Roy(2017)]{NIPS2017_49ad23d1}
Xiuyuan Lu and Benjamin Van~Roy.
\newblock Ensemble sampling.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem[Lu et~al.(2018)Lu, Wen, and Kveton]{lu2018efficient}
Xiuyuan Lu, Zheng Wen, and Branislav Kveton.
\newblock Efficient online recommendation via low-rank ensemble sampling.
\newblock In \emph{Proceedings of the 12th ACM Conference on Recommender
  Systems}, pages 460--464, 2018.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and
  Van~Roy]{NIPS2016_8d8818c8}
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van~Roy.
\newblock Deep exploration via bootstrapped {DQN}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~29, 2016.

\bibitem[Osband et~al.(2018)Osband, Aslanides, and
  Cassirer]{osband2018randomized}
Ian Osband, John Aslanides, and Albin Cassirer.
\newblock Randomized prior functions for deep reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem[Osband et~al.(2019)Osband, Roy, Russo, and Wen]{osband2019deep}
Ian Osband, Benjamin~Van Roy, Daniel~J. Russo, and Zheng Wen.
\newblock Deep exploration via randomized value functions.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (124):\penalty0 1--62, 2019.

\bibitem[Osband et~al.(2022)Osband, Wen, Asghari, Dwaracherla, Lu, Ibrahimi,
  Lawson, Hao, O'Donoghue, and Roy]{osband2022neural}
Ian Osband, Zheng Wen, Seyed~Mohammad Asghari, Vikranth Dwaracherla, Xiuyuan
  Lu, Morteza Ibrahimi, Dieterich Lawson, Botao Hao, Brendan O'Donoghue, and
  Benjamin~Van Roy.
\newblock The neural testbed: Evaluating joint predictions.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Phan et~al.(2019)Phan, Abbasi-Yadkori, and Domke]{phan2019thompson}
My~Phan, Yasin Abbasi-Yadkori, and Justin Domke.
\newblock Thompson sampling and approximate inference.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Roberts and Tweedie(1996)]{roberts1996exponential}
Gareth~O. Roberts and Richard~L. Tweedie.
\newblock Exponential convergence of {L}angevin distributions and their
  discrete approximations.
\newblock \emph{Bernoulli}, pages 341--363, 1996.

\bibitem[Russo and Van~Roy(2016)]{JMLR:v17:14-087}
Daniel Russo and Benjamin Van~Roy.
\newblock An information-theoretic analysis of {T}hompson sampling.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (68):\penalty0 1--30, 2016.

\bibitem[Russo et~al.(2018)Russo, Roy, Kazerouni, Osband, and Wen]{TSTutorial}
Daniel~J. Russo, Benjamin~Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen.
\newblock A tutorial on {T}hompson sampling.
\newblock \emph{Foundations and Trends® in Machine Learning}, 11\penalty0
  (1):\penalty0 1--96, 2018.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S. Sutton and Andrew~G. Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Thompson(1933)]{Thompson1933}
William~R. Thompson.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, 25\penalty0 (3/4):\penalty0 285--294, 1933.

\bibitem[Tsybakov(2009)]{Tsybakov2009}
Alexandre~B. Tsybakov.
\newblock \emph{Introduction to Nonparametric Estimation}.
\newblock Springer series in statistics. Springer, 2009.

\bibitem[Urteaga and Wiggins(2018)]{pmlr-v84-urteaga18a}
Iñigo Urteaga and Chris Wiggins.
\newblock Variational inference for the multi-armed contextual bandit.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, volume~84, pages 698--706, 2018.

\bibitem[Wen et~al.(2022)Wen, Osband, Qin, Lu, Ibrahimi, Dwaracherla, Asghari,
  and Roy]{wen2022predictions}
Zheng Wen, Ian Osband, Chao Qin, Xiuyuan Lu, Morteza Ibrahimi, Vikranth
  Dwaracherla, Mohammad Asghari, and Benjamin~Van Roy.
\newblock From predictions to decisions: The importance of joint predictive
  distributions.
\newblock \emph{arXiv preprint arXiv:2107.09224}, 2022.

\bibitem[Yang et~al.(2020)Yang, Eckles, Dhillon, and Aral]{yang2020targeting}
Jeremy Yang, Dean Eckles, Paramveer Dhillon, and Sinan Aral.
\newblock Targeting for long-term outcomes.
\newblock \emph{arXiv preprint arXiv:2110.15835}, 2020.

\bibitem[Yu et~al.(2020)Yu, Kveton, Wen, Zhang, and Mengshoel]{pmlr-v119-yu20b}
Tong Yu, Branislav Kveton, Zheng Wen, Ruiyi Zhang, and Ole~J. Mengshoel.
\newblock Graphical models meet bandits: A variational {T}hompson sampling
  approach.
\newblock In \emph{International Conference on Machine Learning}, volume 119,
  pages 10902--10912, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Wen, Chen, Fang, Yu, and
  Carin]{zhang2019scalable}
Ruiyi Zhang, Zheng Wen, Changyou Chen, Chen Fang, Tong Yu, and Lawrence Carin.
\newblock Scalable {T}hompson sampling via optimal transport.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, volume~89, pages 87--96, 2019.

\bibitem[Zhu and Van~Roy(2021)]{zhu2021deep}
Zheqing Zhu and Benjamin Van~Roy.
\newblock Deep exploration for recommendation systems.
\newblock \emph{arXiv preprint arXiv:2109.12509}, 2021.

\end{thebibliography}
