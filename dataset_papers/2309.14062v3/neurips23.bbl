\begin{thebibliography}{10}

\bibitem{akyureksubspace}
Afra~Feyza Aky{\"u}rek, Ekin Aky{\"u}rek, Derry Wijaya, and Jacob Andreas.
\newblock Subspace regularizers for few-shot class incremental learning.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2022.

\bibitem{asadi2023prototype}
Nader Asadi, MohammadReza Davari, Sudhir Mudur, Rahaf Aljundi, and Eugene Belilovsky.
\newblock Prototype-sample relation distillation: towards replay-free continual learning.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2023.

\bibitem{belouadah2018deesil}
Eden Belouadah and Adrian Popescu.
\newblock Deesil: Deep-shallow incremental learning.
\newblock In {\em European Conference on Computer Vision (ECCV) Workshops}, 2018.

\bibitem{bertinetto2018metalearning}
Luca Bertinetto, Joao~F. Henriques, Philip Torr, and Andrea Vedaldi.
\newblock Meta-learning with differentiable closed-form solvers.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2019.

\bibitem{bhat2022consistency}
Prashant~Shivaram Bhat, Bahram Zonooz, and Elahe Arani.
\newblock Consistency is the key to further mitigating catastrophic forgetting in continual learning.
\newblock In {\em Conference on Lifelong Learning Agents (CoLLAs)}, 2022.

\bibitem{bhat2023task}
Prashant~Shivaram Bhat, Bahram Zonooz, and Elahe Arani.
\newblock Task-aware information routing from common representation space in lifelong learning.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2023.

\bibitem{castro2018end}
Francisco~M Castro, Manuel~J Mar{\'\i}n-Jim{\'e}nez, Nicol{\'a}s Guil, Cordelia Schmid, and Karteek Alahari.
\newblock End-to-end incremental learning.
\newblock In {\em European Conference on Computer Vision (ECCV)}, 2018.

\bibitem{chi2022metafscil}
Zhixiang Chi, Li~Gu, Huan Liu, Yang Wang, Yuanhao Yu, and Jin Tang.
\newblock Metafscil: a meta-learning approach for few-shot class incremental learning.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022.

\bibitem{de2021continual}
Matthias De~Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu~Jia, Ale{\v{s}} Leonardis, Gregory Slabaugh, and Tinne Tuytelaars.
\newblock A continual learning survey: Defying forgetting in classification tasks.
\newblock {\em Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)}, 2021.

\bibitem{de2021continualP}
Matthias De~Lange and Tinne Tuytelaars.
\newblock Continual prototype evolution: Learning online from non-stationary data streams.
\newblock In {\em International Conference on Computer Vision (ICCV)}, 2021.

\bibitem{de2000Mahalanobis}
Roy De~Maesschalck, Delphine Jouan-Rimbaud, and D{\'e}sir{\'e}~L Massart.
\newblock The mahalanobis distance.
\newblock {\em Chemometrics and intelligent laboratory systems}, 2000.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2009.

\bibitem{dhamija2021self}
Akshay~Raj Dhamija, Touqeer Ahmad, Jonathan Schwan, Mohsen Jafarzadeh, Chunchun Li, and Terrance~E Boult.
\newblock Self-supervised features improve open-world learning.
\newblock {\em arXiv preprint arXiv:2102.07848}, 2021.

\bibitem{dhar2019learning}
Prithviraj Dhar, Rajat~Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa.
\newblock Learning without memorizing.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2019.

\bibitem{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2021.

\bibitem{douillard2020podnet}
Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle.
\newblock Podnet: Pooled outputs distillation for small-tasks incremental learning.
\newblock In {\em European Conference on Computer Vision (ECCV)}, 2020.

\bibitem{guerriero2018deepncm}
Samantha Guerriero, Barbara Caputo, and Thomas Mensink.
\newblock Deepncm: Deep nearest class mean classifiers.
\newblock {\em International Conference on Learning Representations Workshop (ICLR-W)}, 2018.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2016.

\bibitem{hendrycks2021many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of out-of-distribution generalization.
\newblock In {\em International Conference on Computer Vision (ICCV)}, 2021.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{hou2019learning}
Saihui Hou, Xinyu Pan, Chen~Change Loy, Zilei Wang, and Dahua Lin.
\newblock Learning a unified classifier incrementally via rebalancing.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2019.

\bibitem{janson2022simple}
Paul Janson, Wenxuan Zhang, Rahaf Aljundi, and Mohamed Elhoseiny.
\newblock A simple baseline that questions the use of pretrained-models in continual learning.
\newblock In {\em NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and Applications}, 2022.

\bibitem{jeeveswaran2023birt}
Kishaan Jeeveswaran, Prashant Bhat, Bahram Zonooz, and Elahe Arani.
\newblock Birt: Bio-inspired replay in vision transformers for continual learning.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2023.

\bibitem{kim2022multi}
Gyuhak Kim, Bing Liu, and Zixuan Ke.
\newblock A multi-head model for continual learning via out-of-distribution replay.
\newblock In {\em Conference on Lifelong Learning Agents (CoLLAs)}, 2022.

\bibitem{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the National Academy of Sciences (PNAS)}, 2017.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{kumar2022gdc}
Shakti Kumar and Hussain Zaidi.
\newblock Gdc-generalized distribution calibration for few-shot learning.
\newblock {\em arXiv preprint arXiv:2204.05230}, 2022.

\bibitem{le2015tiny}
Ya~Le and Xuan Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock {\em CS 231N}, 2015.

\bibitem{lee2018simple}
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.
\newblock A simple unified framework for detecting out-of-distribution samples and adversarial attacks.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem{li2017learning}
Zhizhong Li and Derek Hoiem.
\newblock Learning without forgetting.
\newblock {\em Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)}, 2017.

\bibitem{liu2022few}
Huan Liu, Li~Gu, Zhixiang Chi, Yang Wang, Yuanhao Yu, Jun Chen, and Jin Tang.
\newblock Few-shot class-incremental learning via entropy-regularized data-free replay.
\newblock In {\em European Conference on Computer Vision (ECCV)}, 2022.

\bibitem{liu2020more}
Yu~Liu, Sarah Parisot, Gregory Slabaugh, Xu~Jia, Ales Leonardis, and Tinne Tuytelaars.
\newblock More classifiers, less forgetting: A generic multi-classifier paradigm for incremental learning.
\newblock In {\em European Conference on Computer Vision (ECCV)}, 2020.

\bibitem{yuyang2023augmented}
Yuyang Liu, Yang Cong, Dipam Goswami, Xialei Liu, and Joost van~de Weijer.
\newblock Augmented box replay: Overcoming foreground shift for incremental object detection.
\newblock In {\em International Conference on Computer Vision (ICCV)}, 2023.

\bibitem{lomonaco2017core50}
Vincenzo Lomonaco and Davide Maltoni.
\newblock Core50: a new dataset and benchmark for continuous object recognition.
\newblock In {\em Conference on Robot Learning (CoRL)}, 2017.

\bibitem{ma2023progressive}
Chunwei Ma, Zhanghexuan Ji, Ziyun Huang, Yan Shen, Mingchen Gao, and Jinhui Xu.
\newblock Progressive voronoi diagram subdivision enables accurate data-free class-incremental learning.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2023.

\bibitem{masana2020class}
Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel Menta, Andrew~D Bagdanov, and Joost van~de Weijer.
\newblock Class-incremental learning: survey and performance evaluation.
\newblock {\em Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)}, 2022.

\bibitem{mccloskey1989catastrophic}
Michael McCloskey and Neal~J Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential learning problem.
\newblock In {\em Psychology of learning and motivation}. Elsevier, 1989.

\bibitem{mensink2013distance}
Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka.
\newblock Distance-based image classification: Generalizing to new classes at near-zero cost.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}, 2013.

\bibitem{munkhdalai2017meta}
Tsendsuren Munkhdalai and Hong Yu.
\newblock Meta networks.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2017.

\bibitem{nguyen2018variational}
Cuong~V Nguyen, Yingzhen Li, Thang~D Bui, and Richard~E Turner.
\newblock Variational continual learning.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2018.

\bibitem{panos2023session}
Aristeidis Panos, Yuriko Kobe, Daniel~Olmeda Reino, Rahaf Aljundi, and Richard~E. Turner.
\newblock First session adaptation: A strong replay-free baseline for class-incremental learning.
\newblock In {\em International Conference on Computer Vision (ICCV)}, 2023.

\bibitem{peng2022few}
Can Peng, Kun Zhao, Tianren Wang, Meng Li, and Brian~C Lovell.
\newblock Few-shot class-incremental learning from an open-set perspective.
\newblock In {\em European Conference on Computer Vision (ECCV)}, 2022.

\bibitem{petit2023fetril}
Gr{\'e}goire Petit, Adrian Popescu, Hugo Schindler, David Picard, and Bertrand Delezoide.
\newblock Fetril: Feature translation for exemplar-free class-incremental learning.
\newblock In {\em Winter Conference on Applications of Computer Vision (WACV)}, 2023.

\bibitem{rebuffi2017icarl}
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H Lampert.
\newblock icarl: Incremental classifier and representation learning.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2017.

\bibitem{ridnik1imagenet}
Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor.
\newblock Imagenet-21k pretraining for the masses.
\newblock In {\em Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2021.

\bibitem{robins1995catastrophic}
Anthony Robins.
\newblock Catastrophic forgetting, rehearsal and pseudorehearsal.
\newblock {\em Connection Science}, 1995.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International journal of computer vision}, 2015.

\bibitem{rusu2018meta}
Andrei~A Rusu, Neil~C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray Kavukcuoglu, and Raia Hadsell.
\newblock Meta-learning with latent embedding optimization.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2019.

\bibitem{sharif2014cnn}
Ali Sharif~Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson.
\newblock Cnn features off-the-shelf: an astounding baseline for recognition.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR) workshops}, 2014.

\bibitem{simon2022generalizing}
Christian Simon, Masoud Faraki, Yi-Hsuan Tsai, Xiang Yu, Samuel Schulter, Yumin Suh, Mehrtash Harandi, and Manmohan Chandraker.
\newblock On generalizing beyond domains in cross-domain continual learning.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022.

\bibitem{smith2021always}
James Smith, Yen-Chang Hsu, Jonathan Balloch, Yilin Shen, Hongxia Jin, and Zsolt Kira.
\newblock Always be dreaming: A new approach for data-free class-incremental learning.
\newblock In {\em International Conference on Computer Vision (ICCV)}, 2021.

\bibitem{steiner2022how}
Andreas~Peter Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer.
\newblock How to train your vit? data, augmentation, and regularization in vision transformers.
\newblock {\em Transactions on Machine Learning Research}, 2022.

\bibitem{tao2020few}
Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, and Yihong Gong.
\newblock Few-shot class-incremental learning.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2020.

\bibitem{triantafillou2019meta}
Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and Hugo Larochelle.
\newblock Meta-dataset: A dataset of datasets for learning to learn from few examples.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2020.

\bibitem{Tukey77}
John~W. Tukey.
\newblock {\em Exploratory data analysis}.
\newblock Addison-Wesley series in behavioral science : quantitative methods. Addison-Wesley, 1977.

\bibitem{van2019three}
Gido~M Van~de Ven and Andreas~S Tolias.
\newblock Three scenarios for continual learning.
\newblock {\em arXiv preprint arXiv:1904.07734}, 2019.

\bibitem{nessOn}
John Van~Ness.
\newblock {On the dominance of non-parametric Bayes rule discriminant algorithms in high dimensions}.
\newblock {\em Pattern Recognition}, 1980.

\bibitem{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)}, 2016.

\bibitem{wah2011caltech}
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock 2011.

\bibitem{wang2022foster}
Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan.
\newblock Foster: Feature boosting and compression for class-incremental learning.
\newblock In {\em European Conference on Computer Vision (ECCV)}, 2022.

\bibitem{wang2023comprehensive}
Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu.
\newblock A comprehensive survey of continual learning: Theory, method and application.
\newblock {\em arXiv preprint arXiv:2302.00487}, 2023.

\bibitem{wang2022dualprompt}
Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, et~al.
\newblock Dualprompt: Complementary prompting for rehearsal-free continual learning.
\newblock In {\em European Conference on Computer Vision (ECCV)}, 2022.

\bibitem{wang2022learning}
Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister.
\newblock Learning to prompt for continual learning.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022.

\bibitem{wu2019large}
Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu.
\newblock Large scale incremental learning.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2019.

\bibitem{xiang2008learning}
Shiming Xiang, Feiping Nie, and Changshui Zhang.
\newblock Learning a mahalanobis distance metric for data clustering and classification.
\newblock {\em Pattern recognition}, 2008.

\bibitem{yan2021dynamically}
Shipeng Yan, Jiangwei Xie, and Xuming He.
\newblock Der: Dynamically expandable representation for class incremental learning.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2021.

\bibitem{yangfree}
Shuo Yang, Lu~Liu, and Min Xu.
\newblock Free lunch for few-shot learning: Distribution calibration.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2021.

\bibitem{Yu_2020_CVPR}
Lu~Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, Shangling Jui, and Joost van~de Weijer.
\newblock Semantic drift compensation for class-incremental learning.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2020.

\bibitem{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In {\em British Machine Vision Conference (BMVC)}, 2016.

\bibitem{zhang2021few}
Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, and Yinghui Xu.
\newblock Few-shot incremental learning with continually evolved classifiers.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2021.

\bibitem{zhao2020maintaining}
Bowen Zhao, Xi~Xiao, Guojun Gan, Bin Zhang, and Shu-Tao Xia.
\newblock Maintaining discrimination and fairness in class incremental learning.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2020.

\bibitem{zhou2022forward}
Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shiliang Pu, and De-Chuan Zhan.
\newblock Forward compatible few-shot class-incremental learning.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022.

\bibitem{zhou2023pycil}
Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, and De-Chuan Zhan.
\newblock Pycil: a python toolbox for class-incremental learning.
\newblock {\em SCIENCE CHINA Information Sciences}, 2023.

\bibitem{zhou2023deep}
Da-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu.
\newblock Deep class-incremental learning: A survey.
\newblock {\em arXiv preprint arXiv:2302.03648}, 2023.

\bibitem{zhou2022model}
Da-Wei Zhou, Qi-Wei Wang, Han-Jia Ye, and De-Chuan Zhan.
\newblock A model or 603 exemplars: Towards memory-efficient class-incremental learning.
\newblock In {\em International Conference on Learning Representations (ICLR)}, 2022.

\bibitem{zhou2022few}
Da-Wei Zhou, Han-Jia Ye, Liang Ma, Di~Xie, Shiliang Pu, and De-Chuan Zhan.
\newblock Few-shot class-incremental learning by sampling multi-phase tasks.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}, 2022.

\bibitem{zhou2021co}
Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan.
\newblock Co-transport for class-incremental learning.
\newblock In {\em ACM International Conference on Multimedia}, 2021.

\bibitem{zhu2021class}
Fei Zhu, Zhen Cheng, Xu-Yao Zhang, and Cheng-lin Liu.
\newblock Class-incremental learning via dual augmentation.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem{zhu2021prototype}
Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-Lin Liu.
\newblock Prototype augmentation and self-supervision for incremental learning.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2021.

\bibitem{zhu2022self}
Kai Zhu, Wei Zhai, Yang Cao, Jiebo Luo, and Zheng-Jun Zha.
\newblock Self-sustaining representation expansion for non-exemplar class-incremental learning.
\newblock In {\em Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022.

\end{thebibliography}
