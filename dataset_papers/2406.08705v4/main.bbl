\begin{thebibliography}{10}

\bibitem{deng2023jailbreaker}
Gelei Deng, Yi~Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu.
\newblock Jailbreaker: Automated jailbreak across multiple large language model chatbots.
\newblock {\em arXiv preprint arXiv:2307.08715}, 2023.

\bibitem{bhardwaj2023red}
Rishabh Bhardwaj and Soujanya Poria.
\newblock Red-teaming large language models using chain of utterances for safety-alignment.
\newblock {\em arXiv preprint arXiv:2308.09662}, 2023.

\bibitem{guo2024cold}
Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, and Bin Hu.
\newblock Cold-attack: Jailbreaking llms with stealthiness and controllability.
\newblock {\em arXiv preprint arXiv:2402.08679}, 2024.

\bibitem{zeng2024johnny}
Yi~Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan Shi.
\newblock How johnny can persuade llms to jailbreak them: Rethinking persuasion to challenge ai safety by humanizing llms.
\newblock {\em arXiv preprint arXiv:2401.06373}, 2024.

\bibitem{zhao2024weak}
Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, and William~Yang Wang.
\newblock Weak-to-strong jailbreaking on large language models.
\newblock {\em arXiv preprint arXiv:2401.17256}, 2024.

\bibitem{huang2023catastrophic}
Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen.
\newblock Catastrophic jailbreak of open-source llms via exploiting generation.
\newblock {\em arXiv preprint arXiv:2310.06987}, 2023.

\bibitem{xu2023languagerl}
Zelai Xu, Chao Yu, Fei Fang, Yu~Wang, and Yi~Wu.
\newblock Language agents with reinforcement learning for strategic play in the werewolf game.
\newblock {\em arXiv preprint arXiv:2310.18940}, 2023.

\bibitem{wang2023adversarial}
Jiongxiao Wang, Zichen Liu, Keun~Hee Park, Muhao Chen, and Chaowei Xiao.
\newblock Adversarial demonstration attacks on large language models.
\newblock {\em arXiv preprint arXiv:2305.14950}, 2023.

\bibitem{lin2024pathseeker}
Zhihao Lin, Wei Ma, Mingyi Zhou, Yanjie Zhao, Haoyu Wang, Yang Liu, Jun Wang, and Li~Li.
\newblock Pathseeker: Exploring llm security vulnerabilities with a reinforcement learning-based jailbreak approach.
\newblock {\em arXiv preprint arXiv:2409.14177}, 2024.

\bibitem{liu2023jailbreaking}
Yi~Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu.
\newblock Jailbreaking chatgpt via prompt engineering: An empirical study.
\newblock {\em arXiv preprint arXiv:2305.13860}, 2023.

\bibitem{shen2023anything}
Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang.
\newblock " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.
\newblock {\em arXiv preprint arXiv:2308.03825}, 2023.

\bibitem{wei2023jailbroken}
Alexander Wei, Nika Haghtalab, and Jacob Steinhardt.
\newblock Jailbroken: How does {LLM} safety training fail?
\newblock In {\em NeurIPS}, 2023.

\bibitem{gcg}
Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language models.
\newblock {\em arXiv preprint arXiv:2307.15043}, 2023.

\bibitem{shen2024rapid}
Guangyu Shen, Siyuan Cheng, Kaiyuan Zhang, Guanhong Tao, Shengwei An, Lu~Yan, Zhuo Zhang, Shiqing Ma, and Xiangyu Zhang.
\newblock Rapid optimization for jailbreaking llms via subconscious exploitation and echopraxia.
\newblock {\em arXiv preprint arXiv:2402.05467}, 2024.

\bibitem{chao2023jailbreaking}
Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J Pappas, and Eric Wong.
\newblock Jailbreaking black box large language models in twenty queries.
\newblock {\em arXiv preprint arXiv:2310.08419}, 2023.

\bibitem{mehrotra2023tree}
Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi.
\newblock Tree of attacks: Jailbreaking black-box llms automatically.
\newblock {\em arXiv preprint arXiv:2312.02119}, 2023.

\bibitem{yuan2024gpt}
Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu.
\newblock {GPT}-4 is too smart to be safe: Stealthy chat with {LLM}s via cipher.
\newblock In {\em ICLR}, 2024.

\bibitem{li2023deepinception}
Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, and Bo~Han.
\newblock Deepinception: Hypnotize large language model to be jailbreaker.
\newblock {\em arXiv preprint arXiv:2311.03191}, 2023.

\bibitem{chang2024play}
Zhiyuan Chang, Mingyang Li, Yi~Liu, Junjie Wang, Qing Wang, and Yang Liu.
\newblock Play guessing game with llm: Indirect jailbreak attack with implicit clues.
\newblock {\em arXiv preprint arXiv:2402.09091}, 2024.

\bibitem{yu2023gptfuzzer}
Jiahao Yu, Xingwei Lin, and Xinyu Xing.
\newblock Gptfuzzer: Red teaming large language models with auto-generated jailbreak prompts.
\newblock {\em arXiv preprint arXiv:2309.10253}, 2023.

\bibitem{lapid2023open}
Raz Lapid, Ron Langberg, and Moshe Sipper.
\newblock Open sesame! universal black box jailbreaking of large language models.
\newblock {\em arXiv preprint arXiv:2309.01446}, 2023.

\bibitem{liu2023autodan}
Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao.
\newblock Autodan: Generating stealthy jailbreak prompts on aligned large language models.
\newblock {\em arXiv preprint arXiv:2310.04451}, 2023.

\bibitem{ppo}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\bibitem{jain2023baseline}
Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein.
\newblock Baseline defenses for adversarial attacks against aligned language models.
\newblock {\em arXiv preprint arXiv:2309.00614}, 2023.

\bibitem{li2024rain}
Yuhui Li, Fangyun Wei, Jinjing Zhao, Chao Zhang, and Hongyang Zhang.
\newblock {RAIN}: Your language models can align themselves without finetuning.
\newblock In {\em ICLR}, 2024.

\bibitem{shah2023scalable}
Rusheb Shah, Quentin~Feuillade Montixi, Soroush Pour, Arush Tagade, and Javier Rando.
\newblock Scalable and transferable black-box jailbreaks for language models via persona modulation.
\newblock In {\em NeurIPS workshop SoLaR}, 2023.

\bibitem{wang2023decodingtrust}
Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et~al.
\newblock Decodingtrust: A comprehensive assessment of trustworthiness in gpt models.
\newblock {\em arXiv preprint arXiv:2306.11698}, 2023.

\bibitem{gong2024effective}
Xueluan Gong, Mingzhe Li, Yilin Zhang, Fengyuan Ran, Chen Chen, Yanjiao Chen, Qian Wang, and Kwok-Yan Lam.
\newblock Effective and evasive fuzz testing-driven jailbreaking attacks against llms.
\newblock {\em arXiv preprint arXiv:2409.14866}, 2024.

\bibitem{wang2023investigating}
Yimu Wang, Peng Shi, and Hongyang Zhang.
\newblock Investigating the existence of" secret language''in language models.
\newblock {\em arXiv preprint arXiv:2307.12507}, 2023.

\bibitem{guo2021efficient}
Han Guo, Bowen Tan, Zhengzhong Liu, Eric~P Xing, and Zhiting Hu.
\newblock Efficient (soft) q-learning for text generation with limited good data.
\newblock {\em arXiv preprint arXiv:2106.07704}, 2021.

\bibitem{perez-red}
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.
\newblock Red teaming language models with language models.
\newblock In {\em EMNLP}, 2022.

\bibitem{yang2023sneakyprompt}
Yuchen Yang, Bo~Hui, Haolin Yuan, Neil Gong, and Yinzhi Cao.
\newblock Sneakyprompt: Jailbreaking text-to-image generative models.
\newblock In {\em Proceedings of the IEEE Symposium on Security and Privacy}, 2024.

\bibitem{hong2024curiositydriven}
Zhang-Wei Hong, Idan Shenfeld, Tsun-Hsuan Wang, Yung-Sung Chuang, Aldo Pareja, James~R. Glass, Akash Srivastava, and Pulkit Agrawal.
\newblock Curiosity-driven red-teaming for large language models.
\newblock In {\em ICLR}, 2024.

\bibitem{kumar2023certifying}
Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi, and Hima Lakkaraju.
\newblock Certifying llm safety against adversarial prompting.
\newblock {\em arXiv preprint arXiv:2309.02705}, 2023.

\bibitem{cao2023defending}
Bochuan Cao, Yuanpu Cao, Lu~Lin, and Jinghui Chen.
\newblock Defending against alignment-breaking attacks via robustly aligned llm.
\newblock {\em arXiv preprint arXiv:2309.14348}, 2023.

\bibitem{robey2023smoothllm}
Alexander Robey, Eric Wong, Hamed Hassani, and George Pappas.
\newblock Smooth{LLM}: Defending large language models against jailbreaking attacks.
\newblock In {\em NeurIPS workshop R0-FoMo}, 2023.

\bibitem{xie2023reminders}
Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, and Fangzhao Wu.
\newblock Defending chatgpt against jailbreak attack via self-reminders.
\newblock {\em Nature Machine Intelligence}, 2023.

\bibitem{wei2023jailbreak}
Zeming Wei, Yifei Wang, and Yisen Wang.
\newblock Jailbreak and guard aligned language models with only few in-context demonstrations.
\newblock {\em arXiv preprint arXiv:2310.06387}, 2023.

\bibitem{helbling2023llm}
Alec Helbling, Mansi Phute, Matthew Hull, and Duen~Horng Chau.
\newblock Llm self defense: By self examination, llms know they are being tricked.
\newblock {\em arXiv preprint arXiv:2308.07308}, 2023.

\bibitem{yuan2024rigorllm}
Zhuowen Yuan, Zidi Xiong, Yi~Zeng, Ning Yu, Ruoxi Jia, Dawn Song, and Bo~Li.
\newblock Rigorllm: Resilient guardrails for large language models against undesired content.
\newblock {\em arXiv preprint arXiv:2403.13031}, 2024.

\bibitem{zeng2024autodefense}
Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, and Qingyun Wu.
\newblock Autodefense: Multi-agent llm defense against jailbreak attacks.
\newblock {\em arXiv preprint arXiv:2403.04783}, 2024.

\bibitem{xu2024safedecoding}
Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill~Yuchen Lin, and Radha Poovendran.
\newblock Safedecoding: Defending against jailbreak attacks via safety-aware decoding.
\newblock {\em arXiv preprint arXiv:2402.08983}, 2024.

\bibitem{zhang2023defending}
Zhexin Zhang, Junxiao Yang, Pei Ke, and Minlie Huang.
\newblock Defending large language models against jailbreaking attacks through goal prioritization.
\newblock {\em arXiv preprint arXiv:2311.09096}, 2023.

\bibitem{moscato1989evolution}
Pablo Moscato et~al.
\newblock On evolution, search, optimization, genetic algorithms and martial arts: Towards memetic algorithms.

\bibitem{holland1992genetic}
John~H Holland.
\newblock Genetic algorithms.
\newblock {\em Scientific american}, 1992.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{hoos2018stochastic}
Holger~H Hoos and Thomas St$\ddot{\nu}$tzle.
\newblock Stochastic local search.
\newblock In {\em Handbook of Approximation Algorithms and Metaheuristics}. 2018.

\bibitem{ruder2016gd}
Sebastian Ruder.
\newblock An overview of gradient descent optimization algorithms.
\newblock {\em arXiv preprint arXiv:1609.04747}, 2016.

\bibitem{yan2024parafuzz}
Lu~Yan, Zhuo Zhang, Guanhong Tao, Kaiyuan Zhang, Xuan Chen, Guangyu Shen, and Xiangyu Zhang.
\newblock Parafuzz: An interpretability-driven technique for detecting poisoned samples in nlp.
\newblock {\em NeurIPS}, 2024.

\bibitem{rlprompt}
Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric Xing, and Zhiting Hu.
\newblock {RLP}rompt: Optimizing discrete text prompts with reinforcement learning.
\newblock In {\em EMNLP}, 2022.

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness, Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 2015.

\bibitem{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em ICML}, 2016.

\bibitem{bge_embedding}
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff.
\newblock C-pack: Packaged resources to advance general chinese embedding, 2023.

\bibitem{conneau2019unsupervised}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock {\em arXiv preprint arXiv:1911.02116}, 2019.

\bibitem{attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NeurIPS}, 2017.

\bibitem{Detoxify}
Laura Hanu and {Unitary team}.
\newblock Detoxify.
\newblock Github. https://github.com/unitaryai/detoxify, 2020.

\bibitem{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.

\bibitem{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts.
\newblock {\em arXiv preprint arXiv:2401.04088}, 2024.

\bibitem{openai-gpt35turbo1106}
{OpenAI}.
\newblock {gpt-3.5-turbo-1106}, 2023.

\bibitem{markov2023holistic}
Todor Markov, Chong Zhang, Sandhini Agarwal, Florentine~Eloundou Nekoul, Theodore Lee, Steven Adler, Angela Jiang, and Lilian Weng.
\newblock A holistic approach to undesired content detection in the real world.
\newblock In {\em AAAI}, 2023.

\bibitem{uncensored}
{TheBloke/WizardLM-7B-uncensored-GPTQ}.
\newblock \url{https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ}.

\bibitem{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{ding2023wolf}
Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, and Shujian Huang.
\newblock A wolf in sheep's clothing: Generalized nested jailbreak prompts can fool large language models easily.
\newblock {\em arXiv preprint arXiv:2311.08268}, 2023.

\bibitem{liu2023llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning, 2023.

\bibitem{zhu2023minigpt}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock {\em arXiv preprint arXiv:2304.10592}, 2023.

\bibitem{sora_website}
OpenAI.
\newblock Creating video from text.
\newblock \url{https://openai.com/sora}, 2024.

\bibitem{ataallah2024minigpt4}
Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny.
\newblock Minigpt4-video: Advancing multimodal llms for video understanding with interleaved visual-textual tokens.
\newblock {\em arXiv preprint arXiv:2404.03413}, 2024.

\bibitem{kirchenbauer2023watermark}
John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein.
\newblock A watermark for large language models.
\newblock In {\em ICML}, 2023.

\bibitem{zhou2024bileve}
Tong Zhou, Xuandong Zhao, Xiaolin Xu, and Shaolei Ren.
\newblock Bileve: Securing text provenance in large language models against spoofing with bi-level signature.
\newblock {\em arXiv preprint arXiv:2406.01946}, 2024.

\bibitem{wang2024survey}
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu~Chen, Yankai Lin, et~al.
\newblock A survey on large language model based autonomous agents.
\newblock {\em Frontiers of Computer Science}, 2024.

\bibitem{gpt4_0409}
OpenAI.
\newblock Gpt-4 turbo.
\newblock \url{https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4}, 2024.

\bibitem{ramamurthy2022reinforcement}
Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant{\'e} Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi.
\newblock Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization.
\newblock {\em arXiv preprint arXiv:2210.01241}, 2022.

\end{thebibliography}
