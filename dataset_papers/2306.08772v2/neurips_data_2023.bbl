\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Schuurmans, and
  Norouzi]{agarwal2020optimistic}
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  104--114. PMLR, 2020.

\bibitem[Agarwal et~al.(2021)Agarwal, Schwarzer, Castro, Courville, and
  Bellemare]{agarwal2021deep}
Rishabh Agarwal, Max Schwarzer, Pablo~Samuel Castro, Aaron~C Courville, and
  Marc Bellemare.
\newblock Deep reinforcement learning at the edge of the statistical precipice.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 29304--29320, 2021.

\bibitem[Akimov et~al.(2022)Akimov, Kurenkov, Nikulin, Tarasov, and
  Kolesnikov]{akimov2022let}
Dmitriy Akimov, Vladislav Kurenkov, Alexander Nikulin, Denis Tarasov, and
  Sergey Kolesnikov.
\newblock Let offline rl flow: Training conservative agents in the latent space
  of normalizing flows.
\newblock \emph{arXiv preprint arXiv:2211.11096}, 2022.

\bibitem[An et~al.(2021)An, Moon, Kim, and Song]{an2021uncertainty}
Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun~Oh Song.
\newblock Uncertainty-based offline reinforcement learning with diversified
  q-ensemble.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 7436--7447, 2021.

\bibitem[Arulkumaran et~al.(2017)Arulkumaran, Deisenroth, Brundage, and
  Bharath]{arulkumaran2017brief}
Kai Arulkumaran, Marc~Peter Deisenroth, Miles Brundage, and Anil~Anthony
  Bharath.
\newblock A brief survey of deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1708.05866}, 2017.

\bibitem[Baker et~al.(2022)Baker, Akkaya, Zhokov, Huizinga, Tang, Ecoffet,
  Houghton, Sampedro, and Clune]{baker2022video}
Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien
  Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune.
\newblock Video pretraining (vpt): Learning to act by watching unlabeled online
  videos.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 24639--24654, 2022.

\bibitem[Biewald(2020)]{wandb}
Lukas Biewald.
\newblock Experiment tracking with weights and biases, 2020.
\newblock URL \url{https://www.wandb.com/}.
\newblock Software available from wandb.com.

\bibitem[Chen et~al.(2020)Chen, Wei, Xu, Zheng, Yang, Xiong, Xu, and
  Li]{chen2020toward}
Chacha Chen, Hua Wei, Nan Xu, Guanjie Zheng, Ming Yang, Yuanhao Xiong, Kai Xu,
  and Zhenhui Li.
\newblock Toward a thousand lights: Decentralized deep reinforcement learning
  for large-scale traffic signal control.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  3414--3421, 2020.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 15084--15097, 2021.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Xu, Gatto, Jain, Kumar, and
  Chi]{Chen2022OffPolicyAF}
Minmin Chen, Can Xu, Vince Gatto, Devanshu Jain, Aviral Kumar, and Ed~H. Chi.
\newblock Off-policy actor-critic for recommender systems.
\newblock \emph{Proceedings of the 16th ACM Conference on Recommender Systems},
  2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Ghadirzadeh, Yu, Gao, Wang, Li,
  Liang, Finn, and Zhang]{chen2022latent}
Xi~Chen, Ali Ghadirzadeh, Tianhe Yu, Yuan Gao, Jianhao Wang, Wenzhe Li, Bin
  Liang, Chelsea Finn, and Chongjie Zhang.
\newblock Latent-variable advantage-weighted policy optimization for offline
  rl.
\newblock \emph{arXiv preprint arXiv:2203.08949}, 2022{\natexlab{b}}.

\bibitem[Engstrom et~al.(2020)Engstrom, Ilyas, Santurkar, Tsipras, Janoos,
  Rudolph, and Madry]{Engstrom2020ImplementationMI}
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus
  Janoos, L.~Rudolph, and Aleksander Madry.
\newblock Implementation matters in deep rl: A case study on ppo and trpo.
\newblock In \emph{ICLR}, 2020.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{fujimoto2021minimalist}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 20132--20145, 2021.

\bibitem[Ghosh et~al.(2022)Ghosh, Ajay, Agrawal, and Levine]{ghosh2022offline}
Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine.
\newblock Offline rl policies should be trained to be adaptive.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7513--7530. PMLR, 2022.

\bibitem[Guss et~al.(2019)Guss, Houghton, Topin, Wang, Codel, Veloso, and
  Salakhutdinov]{guss2019minerl}
William~H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel,
  Manuela Veloso, and Ruslan Salakhutdinov.
\newblock Minerl: a large-scale dataset of minecraft demonstrations.
\newblock In \emph{Proceedings of the 28th International Joint Conference on
  Artificial Intelligence}, pp.\  2442--2448, 2019.

\bibitem[Hafner et~al.(2023)Hafner, Pasukonis, Ba, and
  Lillicrap]{hafner2023mastering}
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap.
\newblock Mastering diverse domains through world models.
\newblock \emph{arXiv preprint arXiv:2301.04104}, 2023.

\bibitem[Hambro et~al.(2022{\natexlab{a}})Hambro, Mohanty, Babaev, Byeon,
  Chakraborty, Grefenstette, Jiang, Daejin, Kanervisto, Kim,
  et~al.]{hambro2022insights}
Eric Hambro, Sharada Mohanty, Dmitrii Babaev, Minwoo Byeon, Dipam Chakraborty,
  Edward Grefenstette, Minqi Jiang, Jo~Daejin, Anssi Kanervisto, Jongmin Kim,
  et~al.
\newblock Insights from the neurips 2021 nethack challenge.
\newblock In \emph{NeurIPS 2021 Competitions and Demonstrations Track}, pp.\
  41--52. PMLR, 2022{\natexlab{a}}.

\bibitem[Hambro et~al.(2022{\natexlab{b}})Hambro, Raileanu, Rothermel, Mella,
  Rockt{\"a}schel, Kuttler, and Murray]{hambrodungeons}
Eric Hambro, Roberta Raileanu, Danielle Rothermel, Vegard Mella, Tim
  Rockt{\"a}schel, Heinrich Kuttler, and Naila Murray.
\newblock Dungeons and data: A large-scale nethack dataset.
\newblock In \emph{Thirty-sixth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2022{\natexlab{b}}.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and Schmidhuber]{lstm}
Sepp Hochreiter and JÃ¼rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9:\penalty0 1735--80, 12 1997.
\newblock \doi{10.1162/neco.1997.9.8.1735}.

\bibitem[Huang et~al.(2021)Huang, Dossa, Ye, and Braga]{huang2021cleanrl}
Shengyi Huang, Rousslan Fernand~Julien Dossa, Chang Ye, and Jeff Braga.
\newblock Cleanrl: High-quality single-file implementations of deep
  reinforcement learning algorithms.
\newblock \emph{arXiv preprint arXiv:2111.08819}, 2021.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Nair, and
  Levine]{kostrikov2021offline}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock \emph{arXiv preprint arXiv:2110.06169}, 2021.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020.

\bibitem[Kumar et~al.(2021)Kumar, Singh, Tian, Finn, and Levine]{kumar2021a}
Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine.
\newblock A workflow for offline model-free robotic reinforcement learning.
\newblock In \emph{5th Annual Conference on Robot Learning}, 2021.
\newblock URL \url{https://openreview.net/forum?id=fy4ZBWxYbIo}.

\bibitem[Kumar et~al.(2022{\natexlab{a}})Kumar, Agarwal, Geng, Tucker, and
  Levine]{Kumar2022OfflineQO}
Aviral Kumar, Rishabh Agarwal, Xinyang Geng, G.~Tucker, and Sergey Levine.
\newblock Offline q-learning on diverse multi-task data both scales and
  generalizes.
\newblock \emph{ArXiv}, abs/2211.15144, 2022{\natexlab{a}}.

\bibitem[Kumar et~al.(2022{\natexlab{b}})Kumar, Agarwal, Geng, Tucker, and
  Levine]{kumar2022offline}
Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine.
\newblock Offline q-learning on diverse multi-task data both scales and
  generalizes.
\newblock \emph{arXiv preprint arXiv:2211.15144}, 2022{\natexlab{b}}.

\bibitem[K{\"u}ttler et~al.(2020)K{\"u}ttler, Nardelli, Miller, Raileanu,
  Selvatici, Grefenstette, and Rockt{\"a}schel]{kuttler2020nethack}
Heinrich K{\"u}ttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu,
  Marco Selvatici, Edward Grefenstette, and Tim Rockt{\"a}schel.
\newblock The nethack learning environment.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7671--7684, 2020.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and
  Hutter]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Lu et~al.(2022)Lu, Ball, Rudner, Parker-Holder, Osborne, and
  Teh]{lu2022challenges}
Cong Lu, Philip~J. Ball, Tim G.~J. Rudner, Jack Parker-Holder, Michael~A.
  Osborne, and Yee~Whye Teh.
\newblock Challenges and opportunities in offline reinforcement learning from
  visual observations, 2022.

\bibitem[Mazyavkina et~al.(2021)Mazyavkina, Sviridov, Ivanov, and
  Burnaev]{mazyavkina2021reinforcement}
Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev.
\newblock Reinforcement learning for combinatorial optimization: A survey.
\newblock \emph{Computers \& Operations Research}, 134:\penalty0 105400, 2021.

\bibitem[Mella et~al.(2022)Mella, Hambro, Rothermel, and
  K{\"{u}}ttler]{moolib2022}
Vegard Mella, Eric Hambro, Danielle Rothermel, and Heinrich K{\"{u}}ttler.
\newblock {moolib: A Platform for Distributed RL}.
\newblock 2022.
\newblock URL \url{https://github.com/facebookresearch/moolib}.

\bibitem[Michie et~al.(1990)Michie, Bain, and
  Hayes-Miches]{michie1990cognitive}
Donald Michie, Michael Bain, and J~Hayes-Miches.
\newblock Cognitive models from subcognitive skills.
\newblock \emph{IEE control engineering series}, 44:\penalty0 71--99, 1990.

\bibitem[Nair et~al.(2020)Nair, Dalal, Gupta, and Levine]{nair2020accelerating}
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock \emph{arXiv preprint arXiv:2006.09359}, 2020.

\bibitem[Nikulin et~al.(2023)Nikulin, Kurenkov, Tarasov, and
  Kolesnikov]{nikulin2023anti}
Alexander Nikulin, Vladislav Kurenkov, Denis Tarasov, and Sergey Kolesnikov.
\newblock Anti-exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:2301.13616}, 2023.

\bibitem[Orvieto et~al.(2023)Orvieto, Smith, Gu, Fernando, Gulcehre, Pascanu,
  and De]{orvieto2023resurrecting}
Antonio Orvieto, Samuel~L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre,
  Razvan Pascanu, and Soham De.
\newblock Resurrecting recurrent neural networks for long sequences, 2023.

\bibitem[Piterbarg et~al.(2023)Piterbarg, Pinto, and
  Fergus]{piterbarg2023nethack}
Ulyana Piterbarg, Lerrel Pinto, and Rob Fergus.
\newblock Nethack is hard to hack.
\newblock \emph{arXiv preprint arXiv:2305.19240}, 2023.

\bibitem[Qin et~al.(2022)Qin, Zhang, Gao, Chen, Li, Zhang, and
  Yu]{qin2022neorl}
Rong-Jun Qin, Xingyuan Zhang, Songyi Gao, Xiong-Hui Chen, Zewen Li, Weinan
  Zhang, and Yang Yu.
\newblock Neorl: A near real-world benchmark for offline reinforcement
  learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 24753--24765, 2022.

\bibitem[Smith et~al.(2022)Smith, Kostrikov, and
  Levine]{smithWalkParkLearning2022}
Laura Smith, Ilya Kostrikov, and Sergey Levine.
\newblock A {{Walk}} in the {{Park}}: {{Learning}} to {{Walk}} in 20 {{Minutes
  With Model-Free Reinforcement Learning}}, August 2022.

\bibitem[Tarasov et~al.(2022)Tarasov, Nikulin, Akimov, Kurenkov, and
  Kolesnikov]{tarasov2022corl}
Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey
  Kolesnikov.
\newblock {CORL}: Research-oriented deep offline reinforcement learning
  library.
\newblock In \emph{3rd Offline RL Workshop: Offline RL as a ''Launchpad''},
  2022.
\newblock URL \url{https://openreview.net/forum?id=SyAS49bBcv}.

\bibitem[Tarasov et~al.(2023)Tarasov, Kurenkov, Nikulin, and
  Kolesnikov]{tarasov2023revisiting}
Denis Tarasov, Vladislav Kurenkov, Alexander Nikulin, and Sergey Kolesnikov.
\newblock Revisiting the minimalist approach to offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2305.09836}, 2023.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Oriol Vinyals, Igor Babuschkin, Wojciech~M Czarnecki, Micha{\"e}l Mathieu,
  Andrew Dudzik, Junyoung Chung, David~H Choi, Richard Powell, Timo Ewalds,
  Petko Georgiev, et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Yu et~al.(2019)Yu, Xie, Xie, Zou, Zhang, Sun, Zhang, Zhang, and
  Jiang]{yu2019deep}
Liang Yu, Weiwei Xie, Di~Xie, Yulong Zou, Dengyin Zhang, Zhixin Sun, Linghua
  Zhang, Yue Zhang, and Tao Jiang.
\newblock Deep reinforcement learning for smart home energy management.
\newblock \emph{IEEE Internet of Things Journal}, 7\penalty0 (4):\penalty0
  2751--2762, 2019.

\bibitem[Zhou et~al.(2020)Zhou, Bajracharya, and Held]{PLAS_corl2020}
Wenxuan Zhou, Sujay Bajracharya, and David Held.
\newblock Plas: Latent action space for offline reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, 2020.

\end{thebibliography}
