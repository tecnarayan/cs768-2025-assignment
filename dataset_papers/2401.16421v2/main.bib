@string{acl_findings = "Findings of Association for Computational Linguistics (ACL)"}
@string{emnlp_findings = "Findings of Empirical Methods in Natural Language Processing (EMNLP)"}
@string{naacl_findings = "Findings of North American Chapter of the Association for Computational Linguistics (NAACL)"}

# conferences
@book{halliday2013halliday,
  title={Halliday's introduction to functional grammar},
  author={Halliday, Michael Alexander Kirkwood and Matthiessen, Christian MIM},
  year={2013},
  publisher={Routledge}
}

## nlp - cl conferences
@string{acl = "Association for Computational Linguistics (ACL)"}
@string{acl_system = "Association for Computational Linguistics (ACL): System Demonstrations"}
@string{emnlp = "Empirical Methods in Natural Language Processing (EMNLP)"}
@string{emnlp_demo = "Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations"}
@string{naacl = "North American Chapter of the Association for Computational Linguistics (NAACL)"}
@string{eacl = "European Chapter of the Association for Computational Linguistics (EACL)"}
@string{conll = "Computational Natural Language Learning (CoNLL)"}
@string{ijcnlp = "International Joint Conference on Natural Language Processing (IJCNLP)"}
@string{coling = "International Conference on Computational Linguistics (COLING)"}
@string{coling_acl = "International Conference on Computational Linguistics and Association for Computational Linguistics (COLING-ACL)"}
@string{emnlp_ijcnlp = "Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"}
@string{emnlp_conll = "Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)"}
@string{emnlp_nlc = "Empirical Methods in Natural Language Processing and Very Large Corpora (EMNLP-VLC)"}
@string{naacl_hlt = "North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)"}
@string{hlt_naacl = "Human Language Technologies: North American Chapter of the Association for Computational Linguistics (HLT-NAACL)"}
@string{acl_hlt = "Association for Computational Linguistics: Human Language Technologies (ACL-HLT)"}
@string{acl_ijcnlp = "Association for Computational Linguistics and International Joint Conference on Natural Language Processing (ACL-IJCNLP)"}
@string{lrec = "International Conference on Language Resources and Evaluation (LREC)"}

## machine learning conferences
@string{aistats = "Artificial Intelligence and Statistics (AISTATS)"}
@string{nips = "Advances in Neural Information Processing Systems (NIPS)"}
@string{neurips = "Advances in Neural Information Processing Systems (NeurIPS)"}
@string{icml = "International Conference on Machine Learning (ICML)"}
@string{aaai = "Conference on Artificial Intelligence (AAAI)"}
@string{ijcai = "International Joint Conference on Artificial Intelligence (IJCAI)"}
@string{colt = "Annual Conference on Computational Learning Theory (COLT)"}
@string{iclr = "International Conference on Learning Representations (ICLR)"}

## others
@string{interspeech = "Annual Conference of the International Speech Communication Association (INTERSPEECH)"}
@string{cvpr = "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"}
@string{iccv = "International Conference on Computer Vision (ICCV)"}

# journal
@string{jmlr = "The Journal of Machine Learning Research (JMLR)"}
@string{tacl = "Transactions of the Association of Computational Linguistics (TACL)"}
@string{nc = "Neural Computation"}
@string{cl = "Computational Linguistics"}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal=neurips,
  volume={30},
  year={2017},
}

@inproceedings{shaw-etal-2018-self,
    title = "Self-Attention with Relative Position Representations",
    author = "Shaw, Peter  and
      Uszkoreit, Jakob  and
      Vaswani, Ashish",
    booktitle = acl,
    month = jun,
    year = "2018",
}

@inproceedings{transformerxl,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "acl",
    month = jul,
    year = "2019",
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG},
}

@inproceedings{huang-etal-2020-improve,
    title = "Improve Transformer Models with Better Relative Position Embeddings",
    author = "Huang, Zhiheng  and
      Liang, Davis  and
      Xu, Peng  and
      Xiang, Bing",
    editor = "Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = emnlp,
    month = nov,
    year = "2020",
}

@inproceedings{
he2021deberta,
title={{\{}DEBERTA{\}}: {\{}DECODING{\}}-{\{}ENHANCED{\}} {\{}BERT{\}} {\{}WITH{\}} {\{}DISENTANGLED{\}} {\{}ATTENTION{\}}},
author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
booktitle=iclr,
year={2021},
}

@inproceedings{
ke2021rethinking,
title={Rethinking Positional Encoding in Language Pre-training},
author={Guolin Ke and Di He and Tie-Yan Liu},
booktitle=iclr,
year={2021},
}

@article{gpt1,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
year = "2018",
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
}


@misc{rope,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2021},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{alibi,
title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
author={Ofir Press and Noah Smith and Mike Lewis},
booktitle=iclr,
year={2022},
}


@misc{gptj,
  author = {Wang, Ben},
  title = {{Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{chi2022kerple,
  title={Kerple: Kernelized relative positional embedding for length extrapolation},
  author={Chi, Ta-Chung and Fan, Ting-Han and Ramadge, Peter J and Rudnicky, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8386--8399},
  year={2022},
}

@misc{chowdhery2022palm,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{chi2023dissecting,
  title={Dissecting transformer length extrapolation via the lens of receptive field analysis},
  author={Chi, Ta-Chung and Fan, Ting-Han and Rudnicky, Alexander and Ramadge, Peter},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={13522--13537},
  year={2023},
}

@inproceedings{randompos,
    title = "Randomized Positional Encodings Boost Length Generalization of Transformers",
    author = "Ruoss, Anian  and
      Del{\'e}tang, Gr{\'e}goire  and
      Genewein, Tim  and
      Grau-Moya, Jordi  and
      Csord{\'a}s, R{\'o}bert  and
      Bennani, Mehdi  and
      Legg, Shane  and
      Veness, Joel",
    booktitle = acl,
    month = jul,
    year = "2023",
}

@misc{touvron2023llama1,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@inproceedings{
Rae2020Compressive,
title={Compressive Transformers for Long-Range Sequence Modelling},
author={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},
booktitle=iclr,
year={2020},
}

@article{chen2023extending,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}

@misc{peng2023yarn,
      title={YaRN: Efficient Context Window Extension of Large Language Models}, 
      author={Bowen Peng and Jeffrey Quesnelle and Honglu Fan and Enrico Shippole},
      year={2023},
      eprint={2309.00071},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
}

@article{kazemnejad2023impact,
  title={The Impact of Positional Encoding on Length Generalization in Transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Ramamurthy, Karthikeyan Natesan and Das, Payel and Reddy, Siva},
  journal={arXiv preprint arXiv:2305.19466},
  year={2023}
}

@article{zhu2023pose,
  title={Pose: Efficient context window extension of llms via positional skip-wise training},
  author={Zhu, Dawei and Yang, Nan and Wang, Liang and Song, Yifan and Wu, Wenhao and Wei, Furu and Li, Sujian},
  journal={arXiv preprint arXiv:2309.10400},
  year={2023}
}

@article{li2023functional,
  title={Functional Interpolation for Relative Positions Improves Long Context Transformers},
  author={Li, Shanda and You, Chong and Guruganesh, Guru and Ainslie, Joshua and Ontanon, Santiago and Zaheer, Manzil and Sanghai, Sumit and Yang, Yiming and Kumar, Sanjiv and Bhojanapalli, Srinadh},
  journal={arXiv preprint arXiv:2310.04418},
  year={2023}
}

@article{liu2023scaling,
  title={Scaling laws of rope-based extrapolation},
  author={Liu, Xiaoran and Yan, Hang and Zhang, Shuo and An, Chenxin and Qiu, Xipeng and Lin, Dahua},
  journal={arXiv preprint arXiv:2310.05209},
  year={2023}
}

@article{chen2023clex,
  title={CLEX: Continuous Length Extrapolation for Large Language Models},
  author={Chen, Guanzheng and Li, Xin and Meng, Zaiqiao and Liang, Shangsong and Bing, Lidong},
  journal={arXiv preprint arXiv:2310.16450},
  year={2023}
}

@inproceedings{sun2022length,
    title = "A Length-Extrapolatable Transformer",
    author = "Sun, Yutao  and
      Dong, Li  and
      Patra, Barun  and
      Ma, Shuming  and
      Huang, Shaohan  and
      Benhaim, Alon  and
      Chaudhary, Vishrav  and
      Song, Xia  and
      Wei, Furu",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    publisher = "Association for Computational Linguistics",
}


@inproceedings{ratner2023parallel,
  title={Parallel context windows for large language models},
  author={Ratner, Nir and Levine, Yoav and Belinkov, Yonatan and Ram, Ori and Magar, Inbal and Abend, Omri and Karpas, Ehud and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6383--6402},
  year={2023}
}

@misc{han2023lminfinite,
    title={LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models},
    author={Chi Han and Qifan Wang and Wenhan Xiong and Yu Chen and Heng Ji and Sinong Wang},
    year={2023},
    eprint={2308.16137},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{xiao2023efficient,
    title={Efficient Streaming Language Models with Attention Sinks},
    author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
    year={2023},
    eprint={2309.17453},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{gao2020pile,
  title={The {P}ile: An 800{GB} dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@inproceedings{
wei2022chain,
title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
}

@inproceedings{
feng2023towards,
title={Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective},
author={Guhao Feng and Bohang Zhang and Yuntian Gu and Haotian Ye and Di He and Liwei Wang},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}

@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
}

@inproceedings{
pg19,
title={Compressive Transformers for Long-Range Sequence Modelling},
author={Jack W. Rae and Anna Potapenko and Siddhant M. Jayakumar and Chloe Hillier and Timothy P. Lillicrap},
booktitle={International Conference on Learning Representations},
year={2020},
}

@article{shaham2022scrolls,
  title={Scrolls: Standardized comparison over long language sequences},
  author={Shaham, Uri and Segal, Elad and Ivgi, Maor and Efrat, Avia and Yoran, Ori and Haviv, Adi and Gupta, Ankit and Xiong, Wenhan and Geva, Mor and Berant, Jonathan and others},
  journal={arXiv preprint arXiv:2201.03533},
  year={2022}
}

@article{fine1998hierarchical,
  title={The hierarchical hidden Markov model: Analysis and applications},
  author={Fine, Shai and Singer, Yoram and Tishby, Naftali},
  journal={Machine learning},
  volume={32},
  pages={41--62},
  year={1998},
  publisher={Springer}
}

@article{anil2022exploring,
  title={Exploring length generalization in large language models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38546--38556},
  year={2022}
}

@article{chowdhury2023monotonic,
  title={Monotonic Location Attention for Length Generalization},
  author={Chowdhury, Jishnu Ray and Caragea, Cornelia},
  journal={arXiv preprint arXiv:2305.20019},
  year={2023}
}

@article{shaw2018self,
  title={Self-attention with relative position representations},
  author={Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  journal={arXiv preprint arXiv:1803.02155},
  year={2018}
}

@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{ying2021transformers,
  title={Do transformers really perform badly for graph representation?},
  author={Ying, Chengxuan and Cai, Tianle and Luo, Shengjie and Zheng, Shuxin and Ke, Guolin and He, Di and Shen, Yanming and Liu, Tie-Yan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={28877--28888},
  year={2021}
}

@inproceedings{
dosovitskiy2021an,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    pages = "4171--4186",
}

@inproceedings{ruoss-etal-2023-randomized,
    title = "Randomized Positional Encodings Boost Length Generalization of Transformers",
    author = "Ruoss, Anian  and
      Del{\'e}tang, Gr{\'e}goire  and
      Genewein, Tim  and
      Grau-Moya, Jordi  and
      Csord{\'a}s, R{\'o}bert  and
      Bennani, Mehdi  and
      Legg, Shane  and
      Veness, Joel",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    pages = "1889--1903",
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@inproceedings{haviv-etal-2022-transformer,
    title = "Transformer Language Models without Positional Encodings Still Learn Positional Information",
    author = "Haviv, Adi  and
      Ram, Ori  and
      Press, Ofir  and
      Izsak, Peter  and
      Levy, Omer",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    pages = "1382--1390"
}


@inproceedings{huang2021govreport,
    title = "Efficient Attentions for Long Document Summarization",
    author = "Huang, Luyang  and
      Cao, Shuyang  and
      Parulian, Nikolaus  and
      Ji, Heng  and
      Wang, Lu",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    pages = "1419--1436"
}

@inproceedings{chen-etal-2022-summscreen,
    title = "{S}umm{S}creen: A Dataset for Abstractive Screenplay Summarization",
    author = "Chen, Mingda  and
      Chu, Zewei  and
      Wiseman, Sam  and
      Gimpel, Kevin",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    pages = "8602--8615"
}

@inproceedings{zhong2021qmsum,
    title = "{QMS}um: A New Benchmark for Query-based Multi-domain Meeting Summarization",
    author = "Zhong, Ming  and
      Yin, Da  and
      Yu, Tao  and
      Zaidi, Ahmad  and
      Mutuma, Mutethia  and
      Jha, Rahul  and
      Awadallah, Ahmed Hassan  and
      Celikyilmaz, Asli  and
      Liu, Yang  and
      Qiu, Xipeng  and
      Radev, Dragomir",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    pages = "5905--5921"
}

@inproceedings{dasigi2021qasper,
    title = "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers",
    author = "Dasigi, Pradeep  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Cohan, Arman  and
      Smith, Noah A.  and
      Gardner, Matt",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
}

@article{kocisky2018narrativeqa,
    title = "The {N}arrative{QA} Reading Comprehension Challenge",
    author = "Ko{\v{c}}isk{\'y}, Tom{\'a}{\v{s}}  and
      Schwarz, Jonathan  and
      Blunsom, Phil  and
      Dyer, Chris  and
      Hermann, Karl Moritz  and
      Melis, G{\'a}bor  and
      Grefenstette, Edward",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
}

@inproceedings{pang-etal-2022-quality,
    title = "{Q}u{ALITY}: Question Answering with Long Input Texts, Yes!",
    author = "Pang, Richard Yuanzhe  and
      Parrish, Alicia  and
      Joshi, Nitish  and
      Nangia, Nikita  and
      Phang, Jason  and
      Chen, Angelica  and
      Padmakumar, Vishakh  and
      Ma, Johnny  and
      Thompson, Jana  and
      He, He  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    pages = "5336--5358"
}

@inproceedings{koreeda-manning-2021-contractnli-dataset,
    title = "{C}ontract{NLI}: A Dataset for Document-level Natural Language Inference for Contracts",
    author = "Koreeda, Yuta  and
      Manning, Christopher",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    pages = "1907--1919"
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
}

@inproceedings{bisk2020piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}

@inproceedings{sakaguchi2020winogrande,
  title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
  author={Sakaguchi, Keisuke and Le Bras, Ronan and Bhagavatula, Chandra and Choi, Yejin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  pages={8732--8740},
  year={2020}
}
@inproceedings{lin-etal-2022-truthfulqa,
    title = "{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods",
    author = "Lin, Stephanie  and
      Hilton, Jacob  and
      Evans, Owain",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    pages = "3214--3252",
}
@inproceedings{lai-etal-2017-race,
    title = "{RACE}: Large-scale {R}e{A}ding Comprehension Dataset From Examinations",
    author = "Lai, Guokun  and
      Xie, Qizhe  and
      Liu, Hanxiao  and
      Yang, Yiming  and
      Hovy, Eduard",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    pages = "785--794",
}

@inproceedings{zellers2019hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}

@article{mmlu,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}

@inproceedings{
ainslie2023colt,
title={Co{LT}5: Faster Long-Range Transformers with Conditional Computation},
author={Joshua Ainslie and Tao Lei and Michiel de Jong and Santiago Ontanon and Siddhartha Brahma and Yury Zemlyanskiy and David Uthus and Mandy Guo and James Lee-Thorp and Yi Tay and Yun-Hsuan Sung and Sumit Sanghai},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
}

@article{blei2010probabilistic,
  title={Probabilistic topic models},
  author={Blei, David and Carin, Lawrence and Dunson, David},
  journal={IEEE signal processing magazine},
  volume={27},
  number={6},
  pages={55--65},
  year={2010},
  publisher={IEEE}
}

@article{ghahramani2001introduction,
  title={An introduction to hidden Markov models and Bayesian networks},
  author={Ghahramani, Zoubin},
  journal={International journal of pattern recognition and artificial intelligence},
  volume={15},
  number={01},
  pages={9--42},
  year={2001},
  publisher={World Scientific}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{rabiner1989tutorial,
  title={A tutorial on hidden Markov models and selected applications in speech recognition},
  author={Rabiner, Lawrence R},
  journal={Proceedings of the IEEE},
  volume={77},
  number={2},
  pages={257--286},
  year={1989},
  publisher={Ieee}
}

@book{hapke2019natural,
  title={Natural Language Processing in Action: Understanding, analyzing, and generating text with Python},
  author={Hapke, Hannes and Howard, Cole and Lane, Hobson},
  year={2019},
  publisher={Simon and Schuster}
}

@book{eilenberg1974automata,
  title={Automata, languages, and machines},
  author={Eilenberg, Samuel},
  year={1974},
  publisher={Academic press}
}

@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={993--1022},
  year={2003}
}

@article{rosen2012author,
  title={The author-topic model for authors and documents},
  author={Rosen-Zvi, Michal and Griffiths, Thomas and Steyvers, Mark and Smyth, Padhraic},
  journal={arXiv preprint arXiv:1207.4169},
  year={2012}
}

@article{chung2016hierarchical,
  title={Hierarchical multiscale recurrent neural networks},
  author={Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1609.01704},
  year={2016}
}

@article{jin2024llm,
  title={LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning},
  author={Jin, Hongye and Han, Xiaotian and Yang, Jingfeng and Jiang, Zhimeng and Liu, Zirui and Chang, Chia-Yuan and Chen, Huiyuan and Hu, Xia},
  journal={arXiv preprint arXiv:2401.01325},
  year={2024}
}

@article{griffiths2003hierarchical,
  title={Hierarchical topic models and the nested Chinese restaurant process},
  author={Griffiths, Thomas and Jordan, Michael and Tenenbaum, Joshua and Blei, David},
  journal={Advances in neural information processing systems},
  volume={16},
  year={2003}
}

@inproceedings{alur1999communicating,
  title={Communicating hierarchical state machines},
  author={Alur, Rajeev and Kannan, Sampath and Yannakakis, Mihalis},
  booktitle={Automata, Languages and Programming: 26th International Colloquium, ICALP’99 Prague, Czech Republic, July 11--15, 1999 Proceedings 26},
  pages={169--178},
  year={1999},
  organization={Springer}
}

@article{liu2022transformers,
  title={Transformers learn shortcuts to automata},
  author={Liu, Bingbin and Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  journal={arXiv preprint arXiv:2210.10749},
  year={2022}
}

@misc{eval-harness,
  author       = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = 12,
  year         = 2023,
  publisher    = {Zenodo},
  version      = {v0.4.0},
  doi          = {10.5281/zenodo.10256836},
  url          = {https://zenodo.org/records/10256836}
}


@article{lightman2023lets,
      title={Let's Verify Step by Step}, 
      author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
      journal={arXiv preprint arXiv:2305.20050},
      year={2023}
}

@article{liu2019fine,
  title={Fine-tune BERT for extractive summarization},
  author={Liu, Yang},
  journal={arXiv preprint arXiv:1903.10318},
  year={2019}
}

@inproceedings{bai2021segatron,
  title={Segatron: Segment-aware transformer for language modeling and understanding},
  author={Bai, He and Shi, Peng and Lin, Jimmy and Xie, Yuqing and Tan, Luchen and Xiong, Kun and Gao, Wen and Li, Ming},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={14},
  pages={12526--12534},
  year={2021}
}