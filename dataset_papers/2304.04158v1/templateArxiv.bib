@article{zhang2019all,
  title={Are all layers created equal?},
  author={Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
  journal={arXiv preprint arXiv:1902.01996},
  year={2019}
}
@inproceedings{
pham2022continual,
title={Continual Normalization: Rethinking Batch Normalization for Online Continual Learning},
author={Quang Pham and Chenghao Liu and Steven HOI},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=vwLLQ-HwqhZ}
}
@article{azizi2021big,
  title={Big self-supervised models advance medical image classification},
  author={Azizi, Shekoofeh and Mustafa, Basil and Ryan, Fiona and Beaver, Zachary and Freyberg, Jan and Deaton, Jonathan and Loh, Aaron and Karthikesalingam, Alan and Kornblith, Simon and Chen, Ting and others},
  journal={arXiv preprint arXiv:2101.05224},
  year={2021}
}
@inproceedings{Howard2018UniversalLM,
  title={Universal Language Model Fine-tuning for Text Classification},
  author={Jeremy Howard and Sebastian Ruder},
  booktitle={ACL},
  year={2018}
}
@article{van2019three,
  title={Three scenarios for continual learning},
  author={Van de Ven, Gido M and Tolias, Andreas S},
  journal={arXiv preprint arXiv:1904.07734},
  year={2019}
}
@article{lopez2017gradient,
  title={Gradient episodic memory for continual learning},
  author={Lopez-Paz, David and Ranzato, Marc'Aurelio},
  journal={Advances in neural information processing systems},
  volume={30},
  pages={6467--6476},
  year={2017}
}
@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}
@article{le2015tiny,
  title={Tiny imagenet visual recognition challenge},
  author={Le, Ya and Yang, Xuan},
  journal={CS 231N},
  volume={7},
  number={7},
  pages={3},
  year={2015}
}
@article{riemer2018learning,
  title={Learning to learn without forgetting by maximizing transfer and minimizing interference},
  author={Riemer, Matthew and Cases, Ignacio and Ajemian, Robert and Liu, Miao and Rish, Irina and Tu, Yuhai and Tesauro, Gerald},
  journal={arXiv preprint arXiv:1810.11910},
  year={2018}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@inproceedings{rebuffi2017icarl,
  title={icarl: Incremental classifier and representation learning},
  author={Rebuffi, Sylvestre-Alvise and Kolesnikov, Alexander and Sperl, Georg and Lampert, Christoph H},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={2001--2010},
  year={2017}
}
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}
@article{buzzega2020dark,
  title={Dark experience for general continual learning: a strong, simple baseline},
  author={Buzzega, Pietro and Boschini, Matteo and Porrello, Angelo and Abati, Davide and Calderara, Simone},
  journal={arXiv preprint arXiv:2004.07211},
  year={2020}
}
@article{chaudhry2018efficient,
  title={Efficient lifelong learning with a-gem},
  author={Chaudhry, Arslan and Ranzato, Marc'Aurelio and Rohrbach, Marcus and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:1812.00420},
  year={2018}
}
@article{benjamin2018measuring,
  title={Measuring and regularizing networks in function space},
  author={Benjamin, Ari S and Rolnick, David and Kording, Konrad},
  journal={arXiv preprint arXiv:1805.08289},
  year={2018}
}
@inproceedings{prabhu2020gdumb,
  title={Gdumb: A simple approach that questions our progress in continual learning},
  author={Prabhu, Ameya and Torr, Philip HS and Dokania, Puneet K},
  booktitle={European conference on computer vision},
  pages={524--540},
  year={2020},
  organization={Springer}
}
@inproceedings{schwarz2018progress,
  title={Progress \& compress: A scalable framework for continual learning},
  author={Schwarz, Jonathan and Czarnecki, Wojciech and Luketina, Jelena and Grabska-Barwinska, Agnieszka and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  booktitle={International Conference on Machine Learning},
  pages={4528--4537},
  year={2018},
  organization={PMLR}
}
@article{vitter1985random,
  title={Random sampling with a reservoir},
  author={Vitter, Jeffrey S},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={11},
  number={1},
  pages={37--57},
  year={1985},
  publisher={ACM New York, NY, USA}
}
@article{caruana1997multitask,
  title={Multitask learning},
  author={Caruana, Rich},
  journal={Machine learning},
  volume={28},
  number={1},
  pages={41--75},
  year={1997},
  publisher={Springer}
}
@article{mermillod2013stability,
  title={The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects},
  author={Mermillod, Martial and Bugaiska, Aur{\'e}lia and Bonin, Patrick},
  journal={Frontiers in psychology},
  volume={4},
  pages={504},
  year={2013},
  publisher={Frontiers}
}
@article{aljundi2019online,
  title={Online continual learning with maximally interfered retrieval},
  author={Aljundi, Rahaf and Caccia, Lucas and Belilovsky, Eugene and Caccia, Massimo and Lin, Min and Charlin, Laurent and Tuytelaars, Tinne},
  journal={arXiv preprint arXiv:1908.04742},
  year={2019}
}
@inproceedings{zenke2017continual,
  title={Continual learning through synaptic intelligence},
  author={Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
  booktitle={International Conference on Machine Learning},
  pages={3987--3995},
  year={2017},
  organization={PMLR}
}
@inproceedings{aljundi2018memory,
  title={Memory aware synapses: Learning what (not) to forget},
  author={Aljundi, Rahaf and Babiloni, Francesca and Elhoseiny, Mohamed and Rohrbach, Marcus and Tuytelaars, Tinne},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={139--154},
  year={2018}
}
@article{rusu2016progressive,
  title={Progressive neural networks},
  author={Rusu, Andrei A and Rabinowitz, Neil C and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  journal={arXiv preprint arXiv:1606.04671},
  year={2016}
}
@inproceedings{mallya2018packnet,
  title={Packnet: Adding multiple tasks to a single network by iterative pruning},
  author={Mallya, Arun and Lazebnik, Svetlana},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={7765--7773},
  year={2018}
}
@inproceedings{serra2018overcoming,
  title={Overcoming catastrophic forgetting with hard attention to the task},
  author={Serra, Joan and Suris, Didac and Miron, Marius and Karatzoglou, Alexandros},
  booktitle={International Conference on Machine Learning},
  pages={4548--4557},
  year={2018},
  organization={PMLR}
}
@inproceedings{li2017deeper,
  title={Deeper, broader and artier domain generalization},
  author={Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5542--5550},
  year={2017}
}
@inproceedings{volpi2021continual,
  title={Continual adaptation of visual representations via domain randomization and meta-learning},
  author={Volpi, Riccardo and Larlus, Diane and Rogez, Gr{\'e}gory},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={4443--4453},
  year={2021}
}
@inproceedings{li2020sequential,
  title={Sequential learning for domain generalization},
  author={Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy},
  booktitle={European Conference on Computer Vision},
  pages={603--619},
  year={2020},
  organization={Springer}
}
@article{medmnistv2,
    title={MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification},
    author={Yang, Jiancheng and Shi, Rui and Wei, Donglai and Liu, Zequan and Zhao, Lin and Ke, Bilian and Pfister, Hanspeter and Ni, Bingbing},
    journal={arXiv preprint arXiv:2110.14795},
    year={2021}
}
@article{derakhshani2022lifelonger,
  title={LifeLonger: A Benchmark for Continual Disease Classification},
  author={Derakhshani, Mohammad Mahdi and Najdenkoska, Ivona and van Sonsbeek, Tom and Zhen, Xiantong and Mahapatra, Dwarikanath and Worring, Marcel and Snoek, Cees GM},
  journal={arXiv preprint arXiv:2204.05737},
  year={2022}
}
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}
@inproceedings{hou2019learning,
  title={Learning a unified classifier incrementally via rebalancing},
  author={Hou, Saihui and Pan, Xinyu and Loy, Chen Change and Wang, Zilei and Lin, Dahua},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={831--839},
  year={2019}
}
@article{ramasesh2020anatomy,
  title={Anatomy of catastrophic forgetting: Hidden representations and task semantics},
  author={Ramasesh, Vinay V and Dyer, Ethan and Raghu, Maithra},
  journal={arXiv preprint arXiv:2007.07400},
  year={2020}
}

@article{ratcliff1990connectionist,
  title={Connectionist models of recognition memory: constraints imposed by learning and forgetting functions.},
  author={Ratcliff, Roger},
  journal={Psychological review},
  volume={97},
  number={2},
  pages={285},
  year={1990},
  publisher={American Psychological Association}
}
@article{robins1995catastrophic,
  title={Catastrophic forgetting, rehearsal and pseudorehearsal},
  author={Robins, Anthony},
  journal={Connection Science},
  volume={7},
  number={2},
  pages={123--146},
  year={1995},
  publisher={Taylor \& Francis}
}
@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Acad Sciences}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{
zou2022marginbased,
title={Margin-Based Few-Shot Class-Incremental Learning with Class-Level Overfitting Mitigation},
author={Yixiong Zou and Shanghang Zhang and Yuhua Li and Ruixuan Li},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=hyc27bDixNR}
}
@inproceedings{
ye2022taskfree,
title={Task-Free Continual Learning via Online Discrepancy Distance Learning},
author={Fei Ye and Adrian G. Bors},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=UFTcdcJrIl2}
}

@inproceedings{
ororbia2022lifelong,
title={Lifelong Neural Predictive Coding: Learning Cumulatively Online without Forgetting},
author={Alex Ororbia and Ankur Mali and C. Lee Giles and Daniel Kifer},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=ccYOWWNa5v2}
}

@inproceedings{
sun2022exploring,
title={Exploring Example Influence in Continual Learning},
author={Qing Sun and Fan Lyu and Fanhua Shang and Wei Feng and Liang Wan},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=u4dXcUEsN7B}
}

@InProceedings{10.1007/978-3-031-20083-0_31,
author="Jin, Hyundong
and Kim, Eunwoo",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="Helpful or Harmful: Inter-task Association in Continual Learning",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="519--535",
abstract="When optimizing sequentially incoming tasks, deep neural networks generally suffer from catastrophic forgetting due to their lack of ability to maintain knowledge from old tasks. This may lead to a significant performance drop of the previously learned tasks. To alleviate this problem, studies on continual learning have been conducted as a countermeasure. Nevertheless, it suffers from an increase in computational cost due to the expansion of the network size or a change in knowledge that is favorably linked to previous tasks. In this work, we propose a novel approach to differentiate helpful and harmful information for old tasks using a model search to learn a current task effectively. Given a new task, the proposed method discovers an underlying association knowledge from old tasks, which can provide additional support in acquiring the new task knowledge. In addition, by introducing a sensitivity measure to the loss of the current task from the associated tasks, we find cooperative relations between tasks while alleviating harmful interference. We apply the proposed approach to both task- and class-incremental scenarios in continual learning, using a wide range of datasets from small to large scales. Experimental results show that the proposed method outperforms a large variety of continual learning approaches for the experiments while effectively alleviating catastrophic forgetting.",
isbn="978-3-031-20083-0"
}

@InProceedings{10.1007/978-3-031-19812-0_7,
author="Ashok, Arjun
and Joseph, K. J.
and Balasubramanian, Vineeth N.",
editor="Avidan, Shai
and Brostow, Gabriel
and Ciss{\'e}, Moustapha
and Farinella, Giovanni Maria
and Hassner, Tal",
title="Class-Incremental Learning with Cross-Space Clustering and Controlled Transfer",
booktitle="Computer Vision -- ECCV 2022",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="105--122",
abstract="In class-incremental learning, the model is expected to learn new classes continually while maintaining knowledge on previous classes. The challenge here lies in preserving the model's ability to effectively represent prior classes in the feature space, while adapting it to represent incoming new classes. We propose two distillation-based objectives for class incremental learning that leverage the structure of the feature space to maintain accuracy on previous classes, as well as enable learning the new classes. In our first objective, termed cross-space clustering (CSC), we propose to use the feature space structure of the previous model to characterize directions of optimization that maximally preserve the class - directions that all instances of a specific class should collectively optimize towards, and those directions that they should collectively optimize away from. Apart from minimizing forgetting, such a class-level constraint indirectly encourages the model to reliably cluster all instances of a class in the current feature space, and further gives rise to a sense of ``herd-immunity'', allowing all samples of a class to jointly combat the model from forgetting the class. Our second objective termed controlled transfer (CT) tackles incremental learning from an important and understudied perspective of inter-class transfer. CT explicitly approximates and conditions the current model on the semantic similarities between incrementally arriving classes and prior classes. This allows the model to learn the incoming classes in such a way that it maximizes positive forward transfer from similar prior classes, thus increasing plasticity, and minimizes negative backward transfer on dissimilar prior classes, whereby strengthening stability. We perform extensive experiments on two benchmark datasets, adding our method (CSCCT) on top of three prominent class-incremental learning methods. We observe consistent performance improvement on a variety of experimental settings.",
isbn="978-3-031-19812-0"
}

@INPROCEEDINGS{9879483,
  author={Yan, Qingsen and Gong, Dong and Liu, Yuhang and van den Hengel, Anton and Shi, Javen Qinfeng},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Learning Bayesian Sparse Networks with Full Experience Replay for Continual Learning}, 
  year={2022},
  volume={},
  number={},
  pages={109-118},
  doi={10.1109/CVPR52688.2022.00021}}
  
  @INPROCEEDINGS{9878745,
  author={Toldo, Marco and Ozay, Mete},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Bring Evanescent Representations to Life in Lifelong Class Incremental Learning}, 
  year={2022},
  volume={},
  number={},
  pages={16711-16720},
  doi={10.1109/CVPR52688.2022.01623}}


@inproceedings{
twf,
  title={Transfer without Forgetting},
author={Boschini, Matteo and Bonicelli, Lorenzo and Porrello, Angelo and Bellitto, Giovanni and Pennisi, Matteo and Palazzo, Simone and Spampinato, Concetto and Calderara, Simone},
booktitle={European Conference on Computer Vision},
year={2022}
}

@inproceedings{
bonicelli2022on,
title={On the Effectiveness of Lipschitz-Driven Rehearsal in Continual Learning},
author={Lorenzo Bonicelli and Matteo Boschini and Angelo Porrello and Concetto Spampinato and Simone Calderara},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=TThSwRTt4IB}
}

@article{boschini2022class,
  title={Class-Incremental Continual Learning into the eXtended DER-verse},
  author={Boschini, Matteo and Bonicelli, Lorenzo and Buzzega, Pietro and Porrello, Angelo and Calderara, Simone},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  publisher={IEEE}
}

@INPROCEEDINGS {9880055,
author = {R. Tiwari and K. Killamsetty and R. Iyer and P. Shenoy},
booktitle = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {GCR: Gradient Coreset based Replay Buffer Selection for Continual Learning},
year = {2022},
volume = {},
issn = {},
pages = {99-108},
doi = {10.1109/CVPR52688.2022.00020},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR52688.2022.00020},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@inproceedings{
arani2022learning,
title={Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System},
author={Elahe Arani and Fahad Sarfraz and Bahram Zonooz},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=uxxFrDwrE7Y}
}

@inproceedings{
pourkeshavarzi2022looking,
title={Looking Back on Learned Experiences  For Class/task Incremental Learning},
author={Mozhgan PourKeshavarzi and Guoying Zhao and Mohammad Sabokrou},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=RxplU3vmBx}
}
@inproceedings{
wu2022pretrained,
title={Pretrained Language Model in Continual Learning: A Comparative Study},
author={Tongtong Wu and Massimo Caccia and Zhuang Li and Yuan-Fang Li and Guilin Qi and Gholamreza Haffari},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=figzpGMrdD}
}
@article{lesort2021continual,
  title={Continual learning in deep networks: an analysis of the last layer},
  author={Lesort, Timoth{\'e}e and George, Thomas and Rish, Irina},
  journal={arXiv preprint arXiv:2106.01834},
  year={2021}
}
@inproceedings{long2015learning,
  title={Learning transferable features with deep adaptation networks},
  author={Long, Mingsheng and Cao, Yue and Wang, Jianmin and Jordan, Michael},
  booktitle={International conference on machine learning},
  pages={97--105},
  year={2015},
  organization={PMLR}
}
@inproceedings{chang2019domain,
  title={Domain-specific batch normalization for unsupervised domain adaptation},
  author={Chang, Woong-Gi and You, Tackgeun and Seo, Seonguk and Kwak, Suha and Han, Bohyung},
  booktitle={Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition},
  pages={7354--7362},
  year={2019}
}