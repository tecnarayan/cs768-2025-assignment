\begin{thebibliography}{10}

\bibitem{yuan2021mest}
Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong, Ning Liu, Yifan
  Gong, Zheng Zhan, Chaoyang He, Qing Jin, et~al.
\newblock Mest: Accurate and fast memory-economic sparse training framework on
  the edge.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{lee2018snip}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{wang2019picking}
Chaoqi Wang, Guodong Zhang, and Roger Grosse.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2020.

\bibitem{mostafa2019parameter}
Hesham Mostafa and Xin Wang.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  4646--4655. PMLR, 2019.

\bibitem{evci2020rigging}
Utku Evci, Trevor Gale, Jacob Menick, Pablo~Samuel Castro, and Erich Elsen.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  2943--2952. PMLR, 2020.

\bibitem{liu2021we}
Shiwei Liu, Lu~Yin, Decebal~Constantin Mocanu, and Mykola Pechenizkiy.
\newblock Do we actually need dense over-parameterization? in-time
  over-parameterization in sparse training.
\newblock In {\em International Conference on Machine Learning}, pages
  6989--7000. PMLR, 2021.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{brock2017freezeout}
Andrew Brock, Theodore Lim, James~M Ritchie, and Nick Weston.
\newblock Freezeout: Accelerate training by progressively freezing layers.
\newblock {\em arXiv preprint arXiv:1706.04983}, 2017.

\bibitem{zhang2020accelerating}
Minjia Zhang and Yuxiong He.
\newblock Accelerating training of transformer-based language models with
  progressive layer dropping.
\newblock {\em Advances in Neural Information Processing Systems},
  33:14011--14023, 2020.

\bibitem{liu2021autofreeze}
Yuhan Liu, Saurabh Agarwal, and Shivaram Venkataraman.
\newblock Autofreeze: Automatically freezing model blocks to accelerate
  fine-tuning.
\newblock {\em arXiv preprint arXiv:2102.01386}, 2021.

\bibitem{kornblith2019similarity}
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.
\newblock Similarity of neural network representations revisited.
\newblock In {\em International Conference on Machine Learning}, pages
  3519--3529. PMLR, 2019.

\bibitem{gong2019efficient}
Linyuan Gong, Di~He, Zhuohan Li, Tao Qin, Liwei Wang, and Tieyan Liu.
\newblock Efficient training of bert by progressively stacking.
\newblock In {\em International Conference on Machine Learning}, pages
  2337--2346. PMLR, 2019.

\bibitem{lee2019would}
Jaejun Lee, Raphael Tang, and Jimmy Lin.
\newblock What would elsa do? freezing layers during transformer fine-tuning.
\newblock {\em arXiv preprint arXiv:1911.03090}, 2019.

\bibitem{gu2020transformer}
Xiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen Chen, and Jiawei Han.
\newblock On the transformer growth for progressive bert training.
\newblock {\em arXiv preprint arXiv:2010.12562}, 2020.

\bibitem{he2021pipetransformer}
Chaoyang He, Shen Li, Mahdi Soltanolkotabi, and Salman Avestimehr.
\newblock Pipetransformer: Automated elastic pipelining for distributed
  training of transformers.
\newblock {\em arXiv preprint arXiv:2102.03161}, 2021.

\bibitem{bengio2007scaling}
Yoshua Bengio, Yann LeCun, et~al.
\newblock Scaling learning algorithms towards ai.
\newblock {\em Large-scale kernel machines}, 34(5):1--41, 2007.

\bibitem{fan2017learning}
Yang Fan, Fei Tian, Tao Qin, Jiang Bian, and Tie-Yan Liu.
\newblock Learning what data to learn.
\newblock {\em arXiv preprint arXiv:1702.08635}, 2017.

\bibitem{toneva2018empirical}
Mariya Toneva, Alessandro Sordoni, Remi Tachet~des Combes, Adam Trischler,
  Yoshua Bengio, and Geoffrey~J Gordon.
\newblock An empirical study of example forgetting during deep neural network
  learning.
\newblock {\em arXiv preprint arXiv:1812.05159}, 2018.

\bibitem{mocanu2018scalable}
Decebal~Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong~H Nguyen,
  Madeleine Gibescu, and Antonio Liotta.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock {\em Nature Communications}, 9(1):1--12, 2018.

\bibitem{han2015deep}
Song Han, Huizi Mao, and William~J. Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2016.

\bibitem{guo2016dynamic}
Yiwen Guo, Anbang Yao, and Yurong Chen.
\newblock Dynamic network surgery for efficient dnns.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 1379--1387, 2016.

\bibitem{liu2021lottery}
Ning Liu, Geng Yuan, Zhengping Che, Xuan Shen, Xiaolong Ma, Qing Jin, Jian Ren,
  Jian Tang, Sijia Liu, and Yanzhi Wang.
\newblock Lottery ticket preserves weight correlation: Is it desirable or not?
\newblock In {\em International Conference on Machine Learning}, pages
  7011--7020. PMLR, 2021.

\bibitem{hu2016network}
Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang.
\newblock Network trimming: A data-driven neuron pruning approach towards
  efficient deep architectures.
\newblock {\em arXiv preprint arXiv:1607.03250}, 2016.

\bibitem{wen2016learning}
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 2074--2082, 2016.

\bibitem{molchanov2017variational}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  2498--2507. PMLR, 2017.

\bibitem{luo2017thinet}
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pages 5058--5066, 2017.

\bibitem{he2017channel}
Yihui He, Xiangyu Zhang, and Jian Sun.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pages 1389--1397, 2017.

\bibitem{neklyudov2017structured}
Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Structured bayesian pruning via log-normal multiplicative noise.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 6778--6787, 2017.

\bibitem{yu2018nisp}
Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad~I Morariu, Xintong Han,
  Mingfei Gao, Ching-Yung Lin, and Larry~S Davis.
\newblock Nisp: Pruning networks using neuron importance score propagation.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 9194--9203, 2018.

\bibitem{he2018soft}
Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi~Yang.
\newblock Soft filter pruning for accelerating deep convolutional neural
  networks.
\newblock In {\em International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2018.

\bibitem{dai2018compressing}
Bin Dai, Chen Zhu, Baining Guo, and David Wipf.
\newblock Compressing neural networks using the variational information
  bottleneck.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  1135--1144. PMLR, 2018.

\bibitem{dong2019network}
Xuanyi Dong and Yi~Yang.
\newblock Network pruning via transformable architecture search.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 759--770, 2019.

\bibitem{li2019compressing}
Tuanhui Li, Baoyuan Wu, Yujiu Yang, Yanbo Fan, Yong Zhang, and Wei Liu.
\newblock Compressing convolutional neural networks via factorized
  convolutional filters.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 3977--3986, 2019.

\bibitem{he2019filter}
Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi~Yang.
\newblock Filter pruning via geometric median for deep convolutional neural
  networks acceleration.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 4340--4349, 2019.

\bibitem{zhang2021structadmm}
Tianyun Zhang, Shaokai Ye, Xiaoyu Feng, Xiaolong Ma, Kaiqi Zhang, Zhengang Li,
  Jian Tang, Sijia Liu, Xue Lin, Yongpan Liu, et~al.
\newblock Structadmm: Achieving ultrahigh efficiency in structured pruning for
  dnns.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  2021.

\bibitem{ma2021non}
Xiaolong Ma, Sheng Lin, Shaokai Ye, Zhezhi He, Linfeng Zhang, Geng Yuan,
  Sia~Huat Tan, Zhengang Li, Deliang Fan, Xuehai Qian, et~al.
\newblock Non-structured dnn weight pruning--is it beneficial in any platform?
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  2021.

\bibitem{yang2018efficient}
Maurice Yang, Mahmoud Faraj, Assem Hussein, and Vincent Gaudet.
\newblock Efficient hardware realization of convolutional neural networks using
  intra-kernel regular pruning.
\newblock In {\em 2018 IEEE 48th International Symposium on Multiple-Valued
  Logic (ISMVL)}, pages 180--185. IEEE, 2018.

\bibitem{ma2020pconv}
Xiaolong Ma, Fu-Ming Guo, Wei Niu, Xue Lin, Jian Tang, Kaisheng Ma, Bin Ren,
  and Yanzhi Wang.
\newblock Pconv: The missing but desirable sparsity in dnn weight pruning for
  real-time execution on mobile devices.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 5117--5124, 2020.

\bibitem{niu2020patdnn}
Wei Niu, Xiaolong Ma, Sheng Lin, Shihao Wang, Xuehai Qian, Xue Lin, Yanzhi
  Wang, and Bin Ren.
\newblock Patdnn: Achieving real-time dnn execution on mobile devices with
  pattern-based weight pruning.
\newblock In {\em Proceedings of the Twenty-Fifth International Conference on
  Architectural Support for Programming Languages and Operating Systems}, pages
  907--922, 2020.

\bibitem{ma2020image}
Xiaolong Ma, Wei Niu, Tianyun Zhang, Sijia Liu, Sheng Lin, Hongjia Li, Wujie
  Wen, Xiang Chen, Jian Tang, Kaisheng Ma, et~al.
\newblock An image enhancing pattern-based sparsity for real-time inference on
  mobile devices.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 629--645. Springer, 2020.

\bibitem{dong2020rtmobile}
Peiyan Dong, Siyue Wang, Wei Niu, Chengming Zhang, Sheng Lin, Zhengang Li,
  Yifan Gong, Bin Ren, Xue Lin, and Dingwen Tao.
\newblock Rtmobile: Beyond real-time mobile acceleration of rnns for speech
  recognition.
\newblock In {\em 2020 57th ACM/IEEE Design Automation Conference}, pages 1--6.
  IEEE, 2020.

\bibitem{rumi2020accelerating}
Masuma~Akter Rumi, Xiaolong Ma, Yanzhi Wang, and Peng Jiang.
\newblock Accelerating sparse cnn inference on gpus with performance-aware
  weight pruning.
\newblock In {\em Proceedings of the ACM International Conference on Parallel
  Architectures and Compilation Techniques}, pages 267--278, 2020.

\bibitem{jin2021teachers}
Qing Jin, Jian Ren, Oliver~J Woodford, Jiazhuo Wang, Geng Yuan, Yanzhi Wang,
  and Sergey Tulyakov.
\newblock Teachers do more than teach: Compressing image-to-image models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 13600--13611, 2021.

\bibitem{zhang2021click}
Chengming Zhang, Geng Yuan, Wei Niu, Jiannan Tian, Sian Jin, Donglin Zhuang,
  Zhe Jiang, Yanzhi Wang, Bin Ren, Shuaiwen~Leon Song, and Dingwen Tao.
\newblock Clicktrain: Efficient and accurate end-to-end deep learning training
  via fine-grained architecture-preserving pruning.
\newblock In {\em Proceedings of the ACM International Conference on
  Supercomputing}, ICS '21, page 266â€“278, New York, NY, USA, 2021.
  Association for Computing Machinery.

\bibitem{niu2021grim}
Wei Niu, Zhengang Li, Xiaolong Ma, Peiyan Dong, Gang Zhou, Xuehai Qian, Xue
  Lin, Yanzhi Wang, and Bin Ren.
\newblock Grim: A general, real-time deep learning inference framework for
  mobile devices based on fine-grained structured weight sparsity.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2021.

\bibitem{guan2021cocopie}
Hui Guan, Shaoshan Liu, Xiaolong Ma, Wei Niu, Bin Ren, Xipeng Shen, Yanzhi
  Wang, and Pu~Zhao.
\newblock Cocopie: enabling real-time ai on off-the-shelf mobile devices via
  compression-compilation co-design.
\newblock {\em Communications of the ACM}, 64(6):62--68, 2021.

\bibitem{lym2019prunetrain}
Sangkug Lym, Esha Choukse, Siavash Zangeneh, Wei Wen, Sujay Sanghavi, and
  Mattan Erez.
\newblock Prunetrain: fast neural network training by dynamic sparse model
  reconfiguration.
\newblock In {\em Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis}, pages 1--13, 2019.

\bibitem{you2020drawing}
Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen,
  Yingyan Lin, Zhangyang Wang, and Richard~G. Baraniuk.
\newblock Drawing early-bird tickets: Toward more efficient training of deep
  networks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2020.

\bibitem{rajpal2020balancing}
Mohit Rajpal, Yehong Zhang, Bryan Kian, and Hsiang Low.
\newblock Balancing training time vs. performance with bayesian early pruning.
\newblock In {\em OpenView}, 2020.

\bibitem{tanaka2020pruning}
Hidenori Tanaka, Daniel Kunin, Daniel~L Yamins, and Surya Ganguli.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  33, 2020.

\bibitem{wimmer2020freezenet}
Paul Wimmer, Jens Mehnert, and Alexandru Condurache.
\newblock Freezenet: Full performance by reduced storage costs.
\newblock In {\em Proceedings of the Asian Conference on Computer Vision
  (ACCV)}, 2020.

\bibitem{van2020single}
Joost van Amersfoort, Milad Alizadeh, Sebastian Farquhar, Nicholas Lane, and
  Yarin Gal.
\newblock Single shot structured pruning before training.
\newblock {\em arXiv preprint arXiv:2007.00389}, 2020.

\bibitem{bellec2018deep}
Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein.
\newblock Deep rewiring: Training very sparse deep net-works.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{dettmers2019sparse}
Tim Dettmers and Luke Zettlemoyer.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock {\em arXiv preprint arXiv:1907.04840}, 2019.

\bibitem{huang2016deep}
Gao Huang, Yu~Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em European conference on computer vision}, pages 646--661.
  Springer, 2016.

\bibitem{shen2020reservoir}
Sheng Shen, Alexei Baevski, Ari~S Morcos, Kurt Keutzer, Michael Auli, and Douwe
  Kiela.
\newblock Reservoir transformers.
\newblock {\em arXiv preprint arXiv:2012.15045}, 2020.

\bibitem{ma2020layer}
Wenchi Ma, Miao Yu, Kaidong Li, and Guanghui Wang.
\newblock Why layer-wise learning is hard to scale-up and a possible solution
  via accelerated downsampling.
\newblock In {\em 2020 IEEE 32nd International Conference on Tools with
  Artificial Intelligence (ICTAI)}, pages 238--243. IEEE, 2020.

\bibitem{safayenikoo2021weight}
Pooneh Safayenikoo and Ismail Akturk.
\newblock Weight update skipping: Reducing training time for artificial neural
  networks.
\newblock {\em IEEE Journal on Emerging and Selected Topics in Circuits and
  Systems}, 11(4):563--574, 2021.

\bibitem{wang2022efficient}
Yiding Wang, Decang Sun, Kai Chen, Fan Lai, and Mosharaf Chowdhury.
\newblock Efficient dnn training with knowledge-guided layer freezing.
\newblock {\em arXiv preprint arXiv:2201.06227}, 2022.

\bibitem{aguilar2020knowledge}
Gustavo Aguilar, Yuan Ling, Yu~Zhang, Benjamin Yao, Xing Fan, and Chenlei Guo.
\newblock Knowledge distillation from internal representations.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 7350--7357, 2020.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2016.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 248--255, 2009.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\end{thebibliography}
