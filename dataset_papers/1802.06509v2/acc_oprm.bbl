\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{OSDI}, volume~16, pp.\  265--283, 2016.

\bibitem[Agarwal et~al.(2017)Agarwal, Allen-Zhu, Bullins, Hazan, and
  Ma]{agarwal2017finding}
Agarwal, N., Allen-Zhu, Z., Bullins, B., Hazan, E., and Ma, T.
\newblock Finding approximate local minima faster than gradient descent.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  1195--1199. ACM, 2017.

\bibitem[Arora et~al.(2018)Arora, Basu, Mianjy, and
  Mukherjee]{arora2018understanding}
Arora, R., Basu, A., Mianjy, P., and Mukherjee, A.
\newblock Understanding deep neural networks with rectified linear units.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Baldi \& Hornik(1989)Baldi and Hornik]{baldi1989neural}
Baldi, P. and Hornik, K.
\newblock Neural networks and principal component analysis: Learning from
  examples without local minima.
\newblock \emph{Neural networks}, 2\penalty0 (1):\penalty0 53--58, 1989.

\bibitem[Boyce et~al.(1969)Boyce, DiPrima, and Haines]{boyce1969elementary}
Boyce, W.~E., DiPrima, R.~C., and Haines, C.~W.
\newblock \emph{Elementary differential equations and boundary value problems},
  volume~9.
\newblock Wiley New York, 1969.

\bibitem[Buck(2003)]{buck2003advanced}
Buck, R.~C.
\newblock \emph{Advanced calculus}.
\newblock Waveland Press, 2003.

\bibitem[Carmon et~al.(2016)Carmon, Duchi, Hinder, and
  Sidford]{carmon2016accelerated}
Carmon, Y., Duchi, J.~C., Hinder, O., and Sidford, A.
\newblock Accelerated methods for non-convex optimization.
\newblock \emph{arXiv preprint arXiv:1611.00756}, 2016.

\bibitem[Choromanska et~al.(2015)Choromanska, Henaff, Mathieu, Arous, and
  LeCun]{choromanska2015loss}
Choromanska, A., Henaff, M., Mathieu, M., Arous, G.~B., and LeCun, Y.
\newblock The loss surfaces of multilayer networks.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  192--204,
  2015.

\bibitem[Cohen et~al.(2017)Cohen, Sharir, Levine, Tamari, Yakira, and
  Shashua]{cohen2017analysis}
Cohen, N., Sharir, O., Levine, Y., Tamari, R., Yakira, D., and Shashua, A.
\newblock Analysis and design of convolutional networks via hierarchical tensor
  decompositions.
\newblock \emph{arXiv preprint arXiv:1705.02302}, 2017.

\bibitem[Daniely(2017)]{daniely2017depth}
Daniely, A.
\newblock Depth separation for neural networks.
\newblock \emph{arXiv preprint arXiv:1702.08489}, 2017.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Eldan \& Shamir(2015)Eldan and Shamir]{eldan2015power}
Eldan, R. and Shamir, O.
\newblock The power of depth for feedforward neural networks.
\newblock \emph{arXiv preprint arXiv:1512.03965}, 2015.

\bibitem[Fukumizu(1998)]{fukumizu1998effect}
Fukumizu, K.
\newblock Effect of batch learning in multilayer neural networks.
\newblock \emph{Gen}, 1\penalty0 (04):\penalty0 1E--03, 1998.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{ge2015escaping}
Ge, R., Huang, F., Jin, C., and Yuan, Y.
\newblock Escaping from saddle points---online stochastic gradient for tensor
  decomposition.
\newblock In \emph{Conference on Learning Theory}, pp.\  797--842, 2015.

\bibitem[Goh(2017)]{goh2017why}
Goh, G.
\newblock Why momentum really works.
\newblock \emph{Distill}, 2017.
\newblock \doi{10.23915/distill.00006}.
\newblock URL \url{http://distill.pub/2017/momentum}.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT press Cambridge, 2016.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Vinyals, and
  Saxe]{goodfellow2014qualitatively}
Goodfellow, I.~J., Vinyals, O., and Saxe, A.~M.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock \emph{arXiv preprint arXiv:1412.6544}, 2014.

\bibitem[Haeffele \& Vidal(2015)Haeffele and Vidal]{Haeffele:2015vz}
Haeffele, B.~D. and Vidal, R.
\newblock {Global Optimality in Tensor Factorization, Deep Learning, and
  Beyond.}
\newblock \emph{CoRR abs/1202.2745}, cs.NA, 2015.

\bibitem[Hardt \& Ma(2016)Hardt and Ma]{hardt2016identity}
Hardt, M. and Ma, T.
\newblock Identity matters in deep learning.
\newblock \emph{arXiv preprint arXiv:1611.04231}, 2016.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{HAK07}
Hazan, E., Agarwal, A., and Kale, S.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Mach. Learn.}, 69\penalty0 (2-3):\penalty0 169--192, December
  2007.
\newblock ISSN 0885-6125.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{arXiv preprint arXiv:1512.03385}, 2015.

\bibitem[Helmke \& Moore(2012)Helmke and Moore]{helmke2012optimization}
Helmke, U. and Moore, J.~B.
\newblock \emph{Optimization and dynamical systems}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pp.\
  448--456, 2015.

\bibitem[Janzamin et~al.(2015)Janzamin, Sedghi, and
  Anandkumar]{Janzamin:2015uz}
Janzamin, M., Sedghi, H., and Anandkumar, A.
\newblock {Beating the Perils of Non-Convexity: Guaranteed Training of Neural
  Networks using Tensor Methods}.
\newblock \emph{CoRR abs/1506.08473}, 2015.

\bibitem[Jones et~al.(2001--)Jones, Oliphant, Peterson, et~al.]{scipy}
Jones, E., Oliphant, T., Peterson, P., et~al.
\newblock {SciPy}: Open source scientific tools for {Python}, 2001--.
\newblock URL \url{http://www.scipy.org/}.
\newblock [Online; accessed <today>].

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
Kawaguchi, K.
\newblock Deep learning without poor local minima.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  586--594, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lee et~al.(2017)Lee, Ge, Risteski, Ma, and Arora]{lee2017ability}
Lee, H., Ge, R., Risteski, A., Ma, T., and Arora, S.
\newblock On the ability of neural nets to express distributions.
\newblock \emph{arXiv preprint arXiv:1702.07028}, 2017.

\bibitem[Livni et~al.(2014)Livni, Shalev-Shwartz, and
  Shamir]{livni2013algorithm}
Livni, R., Shalev-Shwartz, S., and Shamir, O.
\newblock On the computational efficiency of training neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Nesterov(1983)]{nesterov1983method}
Nesterov, Y.
\newblock A method of solving a convex programming problem with convergence
  rate o (1/k2).
\newblock In \emph{Soviet Mathematics Doklady}, volume~27, pp.\  372--376,
  1983.

\bibitem[Nocedal(1980)]{jorge}
Nocedal, J.
\newblock Updating quasi-newton matrices with limited storage.
\newblock \emph{Mathematics of Computation}, 35\penalty0 (151):\penalty0
  773--782, 1980.

\bibitem[Raghu et~al.(2016)Raghu, Poole, Kleinberg, Ganguli, and
  Sohl-Dickstein]{raghu2016expressive}
Raghu, M., Poole, B., Kleinberg, J., Ganguli, S., and Sohl-Dickstein, J.
\newblock On the expressive power of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1606.05336}, 2016.

\bibitem[Rodriguez-Lujan et~al.(2014)Rodriguez-Lujan, Fonollosa, Vergara,
  Homer, and Huerta]{rodriguez2014calibration}
Rodriguez-Lujan, I., Fonollosa, J., Vergara, A., Homer, M., and Huerta, R.
\newblock On the calibration of sensor arrays for pattern recognition using the
  minimal number of experiments.
\newblock \emph{Chemometrics and Intelligent Laboratory Systems}, 130:\penalty0
  123--134, 2014.

\bibitem[Safran \& Shamir(2016)Safran and Shamir]{safran2016quality}
Safran, I. and Shamir, O.
\newblock On the quality of the initial basin in overspecified neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  774--782, 2016.

\bibitem[Safran \& Shamir(2017)Safran and Shamir]{safran2017spurious}
Safran, I. and Shamir, O.
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock \emph{arXiv preprint arXiv:1712.08968}, 2017.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{saxe2013exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6120}, 2013.

\bibitem[Soudry \& Carmon(2016)Soudry and Carmon]{soudry2016no}
Soudry, D. and Carmon, Y.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock \emph{arXiv preprint arXiv:1605.08361}, 2016.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G.~E., Krizhevsky, A., Sutskever, I., and
  Salakhutdinov, R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Su et~al.(2014)Su, Boyd, and Candes]{su2014differential}
Su, W., Boyd, S., and Candes, E.
\newblock A differential equation for modeling nesterovâ€™s accelerated
  gradient method: Theory and insights.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2510--2518, 2014.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman2012lecture}
Tieleman, T. and Hinton, G.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0
  (2):\penalty0 26--31, 2012.

\bibitem[Vergara et~al.(2012)Vergara, Vembu, Ayhan, Ryan, Homer, and
  Huerta]{vergara2012chemical}
Vergara, A., Vembu, S., Ayhan, T., Ryan, M.~A., Homer, M.~L., and Huerta, R.
\newblock Chemical gas sensor drift compensation using classifier ensembles.
\newblock \emph{Sensors and Actuators B: Chemical}, 166:\penalty0 320--329,
  2012.

\bibitem[Wibisono et~al.(2016)Wibisono, Wilson, and
  Jordan]{wibisono2016variational}
Wibisono, A., Wilson, A.~C., and Jordan, M.~I.
\newblock A variational perspective on accelerated methods in optimization.
\newblock \emph{Proceedings of the National Academy of Sciences}, 113\penalty0
  (47):\penalty0 E7351--E7358, 2016.

\bibitem[Zeiler(2012)]{zeiler2012adadelta}
Zeiler, M.~D.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\end{thebibliography}
