\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2012)Agarwal, Bartlett, Ravikumar, and
  Wainwright]{Agarwal2012InformationTheoreticLB}
Agarwal, A., Bartlett, P.~L., Ravikumar, P., and Wainwright, M.~J.
\newblock Information-theoretic lower bounds on the oracle complexity of
  stochastic convex optimization.
\newblock \emph{IEEE Transactions on Information Theory}, 58\penalty0
  (5):\penalty0 3235--3249, 2012.

\bibitem[Arjevani et~al.(2019)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{Arjevani2019LowerBF}
Arjevani, Y., Carmon, Y., Duchi, J.~C., Foster, D.~J., Srebro, N., and
  Woodworth, B.~E.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{ArXiv e-prints}, arXiv:1912.02365, 2019.

\bibitem[Balasubramanian et~al.(2021)Balasubramanian, Ghadimi, and
  Nguyen]{balasubramanian2020stochastic}
Balasubramanian, K., Ghadimi, S., and Nguyen, A.
\newblock Stochastic multi-level composition optimization algorithms with
  level-independent convergence rates.
\newblock \emph{ArXiv e-prints}, arXiv:2008.10526, 2021.

\bibitem[Bruno et~al.(2016)Bruno, Ahmed, Shapiro, and Street]{Bruno2016RiskNA}
Bruno, S., Ahmed, S., Shapiro, A., and Street, A.
\newblock Risk neutral and risk averse approaches to multistage renewable
  investment planning under uncertainty.
\newblock \emph{European Journal of Operational Research}, 250\penalty0
  (3):\penalty0 979--989, 2016.

\bibitem[Charles \& Papailiopoulos(2018)Charles and
  Papailiopoulos]{Charles2018StabilityAG}
Charles, Z. and Papailiopoulos, D.
\newblock Stability and generalization of learning algorithms that converge to
  global optima.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pp.\  745--754, 2018.

\bibitem[Chen et~al.(2021)Chen, Sun, and Yin]{chen2021solving}
Chen, T., Sun, Y., and Yin, W.
\newblock Solving stochastic compositional optimization is nearly as easy as
  solving stochastic optimization.
\newblock \emph{IEEE Transactions on Signal Processing}, 69:\penalty0
  4937--4948, 2021.

\bibitem[Chewi et~al.(2020)Chewi, Maunu, Rigollet, and
  Stromme]{Chewi2020GradientDA}
Chewi, S., Maunu, T., Rigollet, P., and Stromme, A.
\newblock Gradient descent algorithms for {B}ures-{W}asserstein barycenters.
\newblock In \emph{Proceedings of the 33rd Conference on Learning Theory}, pp.\
   1276--1304, 2020.

\bibitem[Cole et~al.(2017)Cole, Giné, and Vickery]{Cole2013HowDR}
Cole, S., Giné, X., and Vickery, J.
\newblock {How Does Risk Management Influence Production Decisions? Evidence
  from a Field Experiment}.
\newblock \emph{The Review of Financial Studies}, 30\penalty0 (6):\penalty0
  1935--1970, 2017.

\bibitem[Cutkosky \& Orabona(2019)Cutkosky and
  Orabona]{Cutkosky2019MomentumBasedVR}
Cutkosky, A. and Orabona, F.
\newblock Momentum-based variance reduction in non-convex {SGD}.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  15210--15219, 2019.

\bibitem[Dann et~al.(2014)Dann, Neumann, and Peters]{Dann2014PolicyEW}
Dann, C., Neumann, G., and Peters, J.
\newblock Policy evaluation with temporal differences: a survey and comparison.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 809--883,
  2014.

\bibitem[Dentcheva et~al.(2017)Dentcheva, Penev, and
  Ruszczynski]{Dentcheva2015StatisticalEO}
Dentcheva, D., Penev, S.~I., and Ruszczynski, A.
\newblock Statistical estimation of composite risk functionals and risk
  optimization problems.
\newblock \emph{Annals of the Institute of Statistical Mathematics},
  69\penalty0 (4):\penalty0 737--760, 2017.

\bibitem[Duchi et~al.(2010)Duchi, Hazan, and Singer]{COLT:Adaptive:Subgradient}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock In \emph{Proceedings of the 23rd Annual Conference on Learning
  Theory}, pp.\  257--269, 2010.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{Fang2018SPIDERNN}
Fang, C., Li, C.~J., Lin, Z., and Zhang, T.
\newblock Spider: Near-optimal non-convex optimization via stochastic path
  integrated differential estimator.
\newblock \emph{ArXiv e-prints}, arXiv:1807.01695, 2018.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{Finn2017ModelAgnosticMF}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pp.\  1126--1135, 2017.

\bibitem[Ghadimi et~al.(2020)Ghadimi, Ruszczynski, and Wang]{Ghadimi2020AST}
Ghadimi, S., Ruszczynski, A., and Wang, M.
\newblock A single timescale stochastic approximation method for nested
  stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (1):\penalty0
  960--979, 2020.

\bibitem[Guo et~al.(2021)Guo, Xu, Yin, Jin, and Yang]{guo2022stochastic}
Guo, Z., Xu, Y., Yin, W., Jin, R., and Yang, T.
\newblock On stochastic moving-average estimators for non-convex optimization.
\newblock \emph{ArXiv e-prints}, arXiv:2104.14840, 2021.

\bibitem[Huo et~al.(2018)Huo, Gu, Liu, and Huang]{Huo2018AcceleratedMF}
Huo, Z., Gu, B., Liu, J., and Huang, H.
\newblock Accelerated method for stochastic composition optimization with
  nonsmooth regularization.
\newblock In \emph{Proceedings of the 32nd AAAI Conference on Artificial
  Intelligence}, pp.\  3287--3294, 2018.

\bibitem[Ji et~al.(2020)Ji, Yang, and Liang]{Ji2020MultiStepMM}
Ji, K., Yang, J., and Liang, Y.
\newblock Multi-step model-agnostic meta-learning: Convergence and improved
  algorithms.
\newblock \emph{ArXiv e-prints}, arXiv:2002.07836, 2020.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{Karimi2016LinearCO}
Karimi, H., Nutini, J., and Schmidt, M.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the {P}olyak-{{\L}}ojasiewicz condition.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases}, pp.\
   795--811, 2016.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma:adam}
Kingma, D.~P. and Ba, J.~L.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Lake et~al.(2011)Lake, Salakhutdinov, Gross, and
  Tenenbaum]{Lake2011OneSL}
Lake, B.~M., Salakhutdinov, R., Gross, J., and Tenenbaum, J.~B.
\newblock One shot learning of simple visual concepts.
\newblock \emph{Proceedings of the Annual Meeting of the Cognitive Science
  Society}, 33, 2011.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Beirami, Sanjabi, and
  Smith]{li2020tilted}
Li, T., Beirami, A., Sanjabi, M., and Smith, V.
\newblock Tilted empirical risk minimization.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Beirami, Sanjabi, and
  Smith]{li2021tilted}
Li, T., Beirami, A., Sanjabi, M., and Smith, V.
\newblock On tilted losses in machine learning: Theory and applications.
\newblock \emph{ArXiv e-prints}, arXiv:2109.06141, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2021)Liu, Liu, and Tao]{Liu2021VarianceRM}
Liu, L., Liu, J., and Tao, D.
\newblock Variance reduced methods for non-convex composition optimization.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, pp.\  1--1, 2021.

\bibitem[Luo et~al.(2019)Luo, Xiong, and Liu]{luo2018adaptive}
Luo, L., Xiong, Y., and Liu, Y.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}c]{Nguyen2017SARAHAN}
Nguyen, L.~M., Liu, J., Scheinberg, K., and Tak{\'a}c, M.
\newblock Sarah: A novel method for machine learning problems using stochastic
  recursive gradient.
\newblock \emph{ArXiv e-prints}, arXiv:1703.00102, 2017.

\bibitem[Nouiehed et~al.(2019)Nouiehed, Sanjabi, Huang, Lee, and
  Razaviyayn]{Nouiehed2019SolvingAC}
Nouiehed, M., Sanjabi, M., Huang, T., Lee, J., and Razaviyayn, M.
\newblock Solving a class of non-convex min-max games using iterative first
  order methods.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, pp.\
  14905--14916, 2019.

\bibitem[Qi et~al.(2021)Qi, Guo, Xu, Jin, and Yang]{qi2021online}
Qi, Q., Guo, Z., Xu, Y., Jin, R., and Yang, T.
\newblock An online method for a class of distributionally robust optimization
  with non-convex objectives.
\newblock \emph{ArXiv e-prints}, arXiv:2006.10138, 2021.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{j.2018on}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of {A}dam and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Shapiro et~al.(2021)Shapiro, Dentcheva, and
  Ruszczynski]{Shapiro2009LecturesOS}
Shapiro, A., Dentcheva, D., and Ruszczynski, A.
\newblock \emph{Lectures on Stochastic Programming: Modeling and Theory, Third
  Edition}.
\newblock Society for Industrial and Applied Mathematics, 2021.

\bibitem[Wang et~al.(2017{\natexlab{a}})Wang, Fang, and
  Liu]{wang2017stochastic}
Wang, M., Fang, E.~X., and Liu, H.
\newblock Stochastic compositional gradient descent: algorithms for minimizing
  compositions of expected-value functions.
\newblock \emph{Mathematical Programming}, 161\penalty0 (1-2):\penalty0
  419--449, 2017{\natexlab{a}}.

\bibitem[Wang et~al.(2017{\natexlab{b}})Wang, Liu, and
  Fang]{DBLP:journals/jmlr/WangLF17}
Wang, M., Liu, J., and Fang, E.~X.
\newblock Accelerating stochastic composition optimization.
\newblock \emph{Journal of Machine Learning Research}, 18:\penalty0
  105:1--105:23, 2017{\natexlab{b}}.

\bibitem[Wang et~al.(2018)Wang, Ji, Zhou, Liang, and
  Tarokh]{Wang2018SpiderBoostAC}
Wang, Z., Ji, K., Zhou, Y., Liang, Y., and Tarokh, V.
\newblock Spiderboost: A class of faster variance-reduced algorithms for
  nonconvex optimization.
\newblock \emph{ArXiv e-prints}, arXiv:1810.10690, 2018.

\bibitem[Xie et~al.(2020)Xie, Wu, and Ward]{Xie2020LinearCO}
Xie, Y., Wu, X., and Ward, R.~A.
\newblock Linear convergence of adaptive stochastic gradient descent.
\newblock In \emph{Proceedings of the 23rd International Conference on
  Artificial Intelligence and Statistics}, pp.\  1475--1485, 2020.

\bibitem[Yang et~al.(2019)Yang, Wang, and Fang]{Yang2019MultilevelSG}
Yang, S., Wang, M., and Fang, E.~X.
\newblock Multilevel stochastic gradient methods for nested composition
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (1):\penalty0
  616--659, 2019.

\bibitem[Yuan \& Hu(2020)Yuan and Hu]{Yuan2020StochasticRM}
Yuan, H. and Hu, W.
\newblock Stochastic recursive momentum method for non-convex compositional
  optimization.
\newblock \emph{ArXiv e-prints}, arXiv:2006.01688, 2020.

\bibitem[Yuan et~al.(2019)Yuan, Lian, Li, Liu, and Hu]{Yuan2019EfficientSN}
Yuan, H., Lian, X., Li, C.~J., Liu, J., and Hu, W.
\newblock Efficient smooth non-convex stochastic compositional optimization via
  stochastic recursive gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, pp.\
  14905--14916, 2019.

\bibitem[Zhang \& Xiao(2019{\natexlab{a}})Zhang and Xiao]{Zhang2019ACR}
Zhang, J. and Xiao, L.
\newblock A composite randomized incremental gradient method.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pp.\  7454--7462, 2019{\natexlab{a}}.

\bibitem[Zhang \& Xiao(2019{\natexlab{b}})Zhang and Xiao]{Zhang2019ASC}
Zhang, J. and Xiao, L.
\newblock A stochastic composite gradient method with incremental variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems 33}, pp.\
  9075--9085, 2019{\natexlab{b}}.

\bibitem[Zhang \& Xiao(2021)Zhang and Xiao]{Zhang2021MultiLevelCS}
Zhang, J. and Xiao, L.
\newblock Multilevel composite stochastic optimization via nested variance
  reduction.
\newblock \emph{SIAM Journal on Optimization}, 31\penalty0 (2):\penalty0
  1131--1157, 2021.

\bibitem[Zhang \& Lan(2021)Zhang and Lan]{Zhang2020OptimalAF}
Zhang, Z. and Lan, G.
\newblock Optimal algorithms for convex nested stochastic composite
  optimization.
\newblock \emph{ArXiv e-prints}, arXiv:2011.10076, 2021.

\end{thebibliography}
