\def\cprime{$'$}
\begin{thebibliography}{10}

\bibitem{All:1974}
D.~M. Allen.
\newblock The relationship between variable selection and data augmentation and
  a method for prediction.
\newblock {\em Technometrics}, 16:125--127, 1974.

\bibitem{Arl_Mas:2009:pente}
S.~Arlot and P.~Massart.
\newblock Data-driven calibration of penalties for least-squares regression.
\newblock {\em J. Mach. Learn. Res.}, 10:245--279 (electronic), 2009.

\bibitem{Arl_Bac:2009:minikernel_nips}
Sylvain Arlot and Francis Bach.
\newblock Data-driven calibration of linear estimators with minimal penalties.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  December 2009.

\bibitem{Arl_Cel:2010:surveyCV}
Sylvain Arlot and Alain Celisse.
\newblock A survey of cross-validation procedures for model selection.
\newblock {\em Statist. Surv.}, 4:40--79, 2010.

\bibitem{grouplasso}
F.~Bach.
\newblock Consistency of the group {L}asso and multiple kernel learning.
\newblock {\em Journal of Machine Learning Research}, 9:1179--1225, 2008.

\bibitem{Bar_Gir_Hue:2007}
Yannick Baraud, Christophe Giraud, and Sylvie Huet.
\newblock {Gaussian model selection with an unknown variance}.
\newblock {\em Ann. Statist.}, 37(2):630--672, 2009.

\bibitem{Bar_Gir_Hue:2010}
Yannick Baraud, Christophe Giraud, and Sylvie Huet.
\newblock Estimator selection in the gaussian setting, 2010.
\newblock arXiv:1007.2096.

\bibitem{Bau_Per_Ros:2007}
Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco.
\newblock On regularization algorithms in learning theory.
\newblock {\em J. Complexity}, 23(1):52--72, 2007.

\bibitem{Bir_Mas:2006}
L.~Birg{\'e} and P.~Massart.
\newblock Minimal penalties for {G}aussian model selection.
\newblock {\em Probab. Theory Related Fields}, 138(1-2):33--73, 2007.

\bibitem{Bis_Hoh_Mun_Ruy:2007}
N.~Bissantz, T.~Hohage, A.~Munk, and F.~Ruymgaart.
\newblock Convergence rates of general regularization methods for statistical
  inverse problems and applications.
\newblock {\em SIAM J. Numer. Anal.}, 45(6):2610--2636 (electronic), 2007.

\bibitem{Cao_Gol:2006}
Y.~Cao and Y.~Golubev.
\newblock On oracle inequalities related to smoothing splines.
\newblock {\em Math. Methods Statist.}, 15(4):398--414 (2007), 2006.

\bibitem{Cap_DeV:2007}
A.~Caponnetto and E.~De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock {\em Found. Comput. Math.}, 7(3):331--368, 2007.

\bibitem{chapelle}
O.~Chapelle and V.~Vapnik.
\newblock Model selection for support vector machines.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  1999.

\bibitem{Cra_Wah:1979}
P.~Craven and G.~Wahba.
\newblock Smoothing noisy data with spline functions. {E}stimating the correct
  degree of smoothing by the method of generalized cross-validation.
\newblock {\em Numer. Math.}, 31(4):377--403, 1978/79.

\bibitem{Dal_Sal:2011}
Arnak~S. Dalalyan and Joseph Salmon.
\newblock Sharp oracle inequalities for aggregation of affine estimators, 2011.
\newblock arXiv:1104.3969.

\bibitem{Efr_Pin:1996}
Sam Efromovich and Mark Pinsker.
\newblock {Sharp-optimal and adaptive estimation for heteroscedastic
  nonparametric regression}.
\newblock {\em Statist. Sinica}, 6(4):925--942, 1996.

\bibitem{Efr:1986}
B.~Efron.
\newblock How biased is the apparent error rate of a prediction rule?
\newblock {\em J. Amer. Statist. Assoc.}, 81(394):461--470, 1986.

\bibitem{Gro_Wol:2009}
Oleg Grodzevich and Henry Wolkowicz.
\newblock Regularization using a parameterized trust region subproblem.
\newblock {\em Math. Program.}, 116(1-2):193--220, 2009.

\bibitem{Han_OLe:1993}
Per~Christian Hansen and Dianne~Prost O'Leary.
\newblock The use of the {$L$}-curve in the regularization of discrete
  ill-posed problems.
\newblock {\em SIAM J. Sci. Comput.}, 14(6):1487--1503, 1993.

\bibitem{Jud_Nem:2009}
Anatoli Juditsky and Arkadi Nemirovski.
\newblock Nonparametric denoising of signals with unknown local structure. {I}.
  {O}racle inequalities.
\newblock {\em Appl. Comput. Harmon. Anal.}, 27(2):157--179, 2009.

\bibitem{Lan_etal:2004}
G.~R.~G. Lanckriet, N.~Cristianini, P.~Bartlett, L.~El~Ghaoui, and M.~I.
  Jordan.
\newblock Learning the kernel matrix with semidefinite programming.
\newblock {\em J. Mach. Learn. Res.}, 5:27--72 (electronic), 2003/04.

\bibitem{Lau_Mas:2000}
B.~Laurent and P.~Massart.
\newblock Adaptive estimation of a quadratic functional by model selection.
\newblock {\em Ann. Statist.}, 28(5):1302--1338, 2000.

\bibitem{Leb:2005}
{\'E}.~Lebarbier.
\newblock Detecting multiple change-points in the mean of a gaussian process by
  model selection.
\newblock {\em Signal Proces.}, 85:717--736, 2005.

\bibitem{KCLi:1987}
K.-C. Li.
\newblock Asymptotic optimality for {$C\sb p$}, {$C\sb L$}, cross-validation
  and generalized cross-validation: discrete index set.
\newblock {\em Ann. Statist.}, 15(3):958--975, 1987.

\bibitem{KCLi:1986}
Ker-Chau Li.
\newblock Asymptotic optimality of {$C\sb L$} and generalized cross-validation
  in ridge regression with application to spline smoothing.
\newblock {\em Ann. Statist.}, 14(3):1101--1112, 1986.

\bibitem{LoG_Ros_Odo_DeV_Ver:2008}
L.~Lo~Gerfo, L.~Rosasco, F.~Odone, E.~De~Vito, and A.~Verri.
\newblock Spectral algorithms for supervised learning.
\newblock {\em Neural Comput.}, 20(7):1873--1897, 2008.

\bibitem{Mal:1973}
C.~L. Mallows.
\newblock Some comments on ${C}_p$.
\newblock {\em Technometrics}, 15:661--675, 1973.

\bibitem{Mas:2003:St-Flour}
P.~Massart.
\newblock {\em Concentration Inequalities and Model Selection}, volume 1896 of
  {\em Lecture Notes in Mathematics}.
\newblock Springer, Berlin, 2007.
\newblock Lectures from the 33rd Summer School on Probability Theory held in
  Saint-Flour, July 6--23, 2003, With a foreword by Jean Picard.

\bibitem{Mau_Mic:2008:slope}
C.~Maugis and B.~Michel.
\newblock Slope heuristics for variable selection and clustering via gaussian
  mixtures.
\newblock Technical Report 6550, INRIA, 2008.

\bibitem{Nad:1964}
E.~A. Nadaraya.
\newblock On estimating regression.
\newblock {\em Theory of Probability and its Applications}, 9(1):141--142,
  1964.

\bibitem{Pin:1980}
M.~S. Pinsker.
\newblock Optimal filtration of square-integrable signals in {G}aussian noise.
\newblock {\em Probl. Peredachi Inf.}, 16(2):52--68, 1980.

\bibitem{pontil}
M.~Pontil, A.~Argyriou, and T.~Evgeniou.
\newblock Multi-task feature learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2007.

\bibitem{simpleMKL}
A.~Rakotomamonjy, F.~Bach, S.~Canu, and Y.~Grandvalet.
\newblock Simple{MKL}.
\newblock {\em Journal of Machine Learning Research}, 9:2491--2521, 2008.

\bibitem{GP}
C.~E. Rasmussen and C.~Williams.
\newblock {\em Gaussian Processes for Machine Learning}.
\newblock MIT Press, 2006.

\bibitem{Rez_Hos:2009}
Mansoor Rezghi and S.~Mohammad Hosseini.
\newblock A new variant of {L}-curve for {T}ikhonov regularization.
\newblock {\em J. Comput. Appl. Math.}, 231(2):914--924, 2009.

\bibitem{scholkopf-smola-book}
B.~Sch{\"o}lkopf and A.~J. Smola.
\newblock {\em Learning with Kernels}.
\newblock MIT Press, 2001.

\bibitem{Shawe-Taylor04}
J.~Shawe-Taylor and N.~Cristianini.
\newblock {\em Kernel Methods for Pattern Analysis}.
\newblock Cambridge University Press, 2004.

\bibitem{Sol_Arl_Bac:2011}
Matthieu Solnon, Sylvain Arlot, and Francis Bach.
\newblock Multi-task regression using minimal penalties, 2011.
\newblock Work in progress.

\bibitem{Sto:1974}
M.~Stone.
\newblock Cross-validatory choice and assessment of statistical predictions.
\newblock {\em J. Roy. Statist. Soc. Ser. B}, 36:111--147, 1974.
\newblock With discussion by G. A. Barnard, A. C. Atkinson, L. K. Chan, A. P.
  Dawid, F. Downton, J. Dickey, A. G. Baker, O. Barndorff-Nielsen, D. R. Cox,
  S. Giesser, D. Hinkley, R. R. Hocking, and A. S. Young, and with a reply by
  the authors.

\bibitem{lasso}
R.~Tibshirani.
\newblock Regression shrinkage and selection via the {L}asso.
\newblock {\em Journal of The Royal Statistical Society Series B},
  58(1):267--288, 1996.

\bibitem{Tik_Mor:1981}
A.~N. Tikhonov and V.~A. Morozov.
\newblock Methods for the regularization of ill-posed problems.
\newblock {\em Vychisl. Metody i Programmirovanie}, (35):3--34, 1981.

\bibitem{Tra:2009}
Minh~Ngoc Tran.
\newblock Penalized maximum likelihood principle for choosing ridge parameter.
\newblock {\em Comm. Statist. Simulation and Computation}, 38(9):1610--1624,
  2009.

\bibitem{Tsy:2009}
Alexandre~B. Tsybakov.
\newblock {\em Introduction to nonparametric estimation}.
\newblock Springer Series in Statistics. Springer, New York, 2009.
\newblock Revised and extended from the 2004 French original, Translated by
  Vladimir Zaiats.

\bibitem{wahba}
G.~Wahba.
\newblock {\em Spline Models for Observational Data}.
\newblock SIAM, 1990.

\bibitem{Was:2006}
Larry Wasserman.
\newblock {\em {All of nonparametric statistics}}.
\newblock {Springer Texts in Statistics}. Springer, New York, 2006.

\bibitem{Wat:1964}
Geoffrey~S. Watson.
\newblock Smooth regression analysis.
\newblock {\em Sankhy\=a Ser. A}, 26:359--372, 1964.

\bibitem{Yan:1981}
Shie-Shien Yang.
\newblock Linear functions of concomitants of order statistics with application
  to nonparametric estimation of a regression function.
\newblock {\em J. Amer. Statist. Assoc.}, 76(375):658--662, 1981.

\bibitem{grouped}
M.~Yuan and Y.~Lin.
\newblock Model selection and estimation in regression with grouped variables.
\newblock {\em Journal of The Royal Statistical Society Series B},
  68(1):49--67, 2006.

\bibitem{Zha:2005}
T.~Zhang.
\newblock Learning bounds for kernel regression using effective data
  dimensionality.
\newblock {\em Neural Comput.}, 17(9):2077--2098, 2005.

\end{thebibliography}
