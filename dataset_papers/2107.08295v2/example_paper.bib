@ARTICLE{markov_feedback2,
  author={Tatikonda, Sekhar and Mitter, Sanjoy},
  journal={IEEE Transactions on Information Theory}, 
  title={The Capacity of Channels With Feedback}, 
  year={2009},
  volume={55},
  number={1},
  pages={323-349},
  doi={10.1109/TIT.2008.2008147}}
@ARTICLE{markov_feedback,
  author={Jun Chen and Berger, T.},
  journal={IEEE Transactions on Information Theory}, 
  title={The capacity of finite-State Markov Channels With feedback}, 
  year={2005},
  volume={51},
  number={3},
  pages={780-798},
  doi={10.1109/TIT.2004.842697}}
@inproceedings{mao2,
  title={Learning Agent Communication under Limited Bandwidth by Message Pruning},
  author={Hangyu Mao and Zhengchao Zhang and Zhen Xiao and Zhibo Gong and Yan Ni},
  booktitle={AAAI},
  year={2020}
}
@inproceedings{imac,
  author={Rundong Wang and Xu He and Runsheng Yu and Wei Qiu and Bo An and Zinovi Rabinovich},
  title={Learning Efficient Multi-agent Communication: An Information Bottleneck Approach},
  year={2020},
  cdate={1577836800000},
  pages={9908-9918},
  url={http://proceedings.mlr.press/v119/wang20i.html},
  booktitle={ICML},
}

@article{mao,
  author={Hangyu Mao and Zhengchao Zhang and Zhen Xiao and Zhibo Gong and Yan Ni},
  title={Learning multi-agent communication with double attentional deep reinforcement learning},
  year={2020},
  cdate={1577836800000},
  journal={Auton. Agents Multi Agent Syst.},
  volume={34},
  number={1},
  pages={32},
  url={https://doi.org/10.1007/s10458-020-09455-w}
}

@inproceedings{tarmac,
  author    = {Abhishek Das and
               Th{\'{e}}ophile Gervet and
               Joshua Romoff and
               Dhruv Batra and
               Devi Parikh and
               Mike Rabbat and
               Joelle Pineau},
  title     = {TarMAC: Targeted Multi-Agent Communication},
  booktitle = {{ICML}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {97},
  pages     = {1538--1546},
  publisher = {{PMLR}},
  year      = {2019}
}

@article{babbling,
Author = {Farrell, Joseph and Rabin, Matthew},
Title = {Cheap Talk},
Journal = {Journal of Economic Perspectives},
Volume = {10},
Number = {3},
Year = {1996},
Month = {September},
Pages = {103-118},
DOI = {10.1257/jep.10.3.103},
URL = {https://www.aeaweb.org/articles?id=10.1257/jep.10.3.103}}
@article{meteor,
  title={Meteor: Cryptographically Secure Steganography for Realistic Distributions},
  author={Gabriel Kaptchuk and Tushar M. Jois and Matthew Green and Aviel D. Rubin},
  journal={Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
  year={2021}
}
@article{Kaptchuk2021MeteorCS,
  title={Meteor: Cryptographically Secure Steganography for Realistic Distributions},
  author={Gabriel Kaptchuk and Tushar M. Jois and Matthew Green and Aviel D. Rubin},
  journal={Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
  year={2021}
}
@article{stegsurv,
title = {A Crypto-Steganography: A Survey},
journal = {International Journal of Advanced Computer Science and Applications},
doi = {10.14569/IJACSA.2014.050722},
url = {http://dx.doi.org/10.14569/IJACSA.2014.050722},
year = {2014},
publisher = {The Science and Information Organization},
volume = {5},
number = {7},
author = {Md. Khalid Imam Rahmani and Kamiya Arora and Naina Pal}
}
@inproceedings{ma-backprop,
 author = {Sukhbaatar, Sainbayar and szlam, arthur and Fergus, Rob},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Multiagent Communication with Backpropagation},
 url = {https://proceedings.neurips.cc/paper/2016/file/55b1927fdafef39c48e5b73b5d61ea60-Paper.pdf},
 volume = {29},
 year = {2016}
}
@article{ernst2012chromhmm,
  added-at = {2013-03-20T15:35:18.000+0100},
  author = {Ernst, Jason and Kellis, Manolis},
  biburl = {https://www.bibsonomy.org/bibtex/239b03bc5c57205c6b4b63816f373b06d/ytyoun},
  doi = {10.1038/nmeth.1906},
  interhash = {a26d7480682d85e96962b34601a67051},
  intrahash = {39b03bc5c57205c6b4b63816f373b06d},
  issn = {15487091},
  journal = {Nat Meth},
  keywords = {bioinformatics hidden.markov.model hmm},
  month = mar,
  number = 3,
  pages = {215--216},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2013-03-20T15:35:18.000+0100},
  title = {ChromHMM: automating chromatin-state discovery and characterization},
  volume = 9,
  year = 2012
}

@ARTICLE{hmms,
  author={Rabiner, L. and Juang, B.},
  journal={IEEE ASSP Magazine}, 
  title={An introduction to hidden Markov models}, 
  year={1986},
  volume={3},
  number={1},
  pages={4-16},
  doi={10.1109/MASSP.1986.1165342}}
@book{isi,
author = {Lathi, B. P.},
title = {Modern Digital and Analog Communication Systems 3e Osece},
year = {1998},
isbn = {0195110099},
publisher = {Oxford University Press, Inc.},
address = {USA},
edition = {3rd},
abstract = {From the Publisher: Lathi's trademark user-friendly and highly readable text presents a complete and modern treatment of communication systems. It begins by introducing students to the basics of communication systems without using probabilistic theory. Only after a solid knowledge base—an understanding of how communication systems work—has been built are concepts requiring probability theory covered. This third edition has been thoroughly updated and revised to include expanded coverage of digital communications. New topics discussed include spread-spectrum systems, cellular communication systems, global positioning systems (GPS), and an entire chapter on emerging digital technologies (such as SONET, ISDN, BISDN, ATM, and video compression). Ideal for the first communication systems course for electrical engineers, Modern Digital and Analog Communication Systems offers students a superb pedagogical style; it consistently does an excellent job of explaining difficult concepts clearly, using prose as well as mathematics. The author makes every effort to give intuitive insights—rather than just proofs—as well as heuristic explanations of theoretical results wherever possible. Featuring lucid explanations, well-chosen examples clarifying abstract mathematical results, and excellent illustrations, this unique text is highly informative and easily accessible to students. Superb pedagogical style — The book consistently does an excellent job of explaining difficult concepts clearly, using prose as well as mathematics. Every effort is made to give an intuitive insight - rather than just proofs - as well as heuristic explanations of theoretical results,wherever possible. The clear explanations, the well-chosen examples to clarify the abstract mathematical results, and the excellent illustrations make this book highly informative and easily accessible to an average student.}
}
@ARTICLE{fsmc,
  author={Hong Shen Wang and Moayeri, N.},
  journal={IEEE Transactions on Vehicular Technology}, 
  title={Finite-state Markov channel-a useful model for radio communication channels}, 
  year={1995},
  volume={44},
  number={1},
  pages={163-171},
  doi={10.1109/25.350282}}
@article{diayn,
  author    = {Benjamin Eysenbach and
               Abhishek Gupta and
               Julian Ibarz and
               Sergey Levine},
  title     = {Diversity is All You Need: Learning Skills without a Reward Function},
  journal   = {CoRR},
  volume    = {abs/1802.06070},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.06070},
  archivePrefix = {arXiv},
  eprint    = {1802.06070},
  timestamp = {Thu, 20 Dec 2018 16:30:14 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-06070.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{unequal-letter-costs,
author = {Golin, Mordecai J. and Kenyon, Claire and Young, Neal E.},
title = {Huffman Coding with Unequal Letter Costs},
year = {2002},
isbn = {1581134959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/509907.510020},
doi = {10.1145/509907.510020},
abstract = {(MATH) In the standard Huffman coding problem, one is given a set of words and for
each word a positive frequency. The goal is to encode each word w as a codeword c(w)
over a given alphabet. The encoding must be prefix free (no codeword is a prefix of
any other) and should minimize the weighted average codeword size Σw freq w, |c(w)|.
The problem has a well-known polynomial-time algorithm due to Huffman [15].Here we
consider the generalization in which the letters of the encoding alphabet may have
non-uniform lengths. The goal is to minimize the weighted average codeword length
Σw freq (w) cost(c(w)), where cost s is the sum of the (possibly non-uniform) lengths
of the letters in s. Despite much previous work, the problem is not known to be NP-hard,
nor was it previously known to have a polynomial-time approximation algorithm. Here
we describe a polynomial-time approximation scheme (PTAS) for the problem.},
booktitle = {Proceedings of the Thiry-Fourth Annual ACM Symposium on Theory of Computing},
pages = {785–791},
numpages = {7},
location = {Montreal, Quebec, Canada},
series = {STOC '02}
}
@inproceedings{length-limited,
author = {Larmore, Lawrence L. and Hirschberg, Daniel S.},
title = {Length-Limited Coding},
year = {1990},
isbn = {0898712513},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
booktitle = {Proceedings of the First Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {310–318},
numpages = {9},
location = {San Francisco, California, USA},
series = {SODA '90}
}
@inproceedings{
lazaridou2018emergence,
title={Emergence of Linguistic Communication from  Referential Games with Symbolic and Pixel Input},
author={Angeliki Lazaridou and Karl Moritz Hermann and Karl Tuyls and Stephen Clark},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HJGv1Z-AW},
}
@article{varun,
  title={Inference-Based Deterministic Messaging For Multi-Agent Communication},
  author={Varun Bhatt and M. Buro},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.02150}
}
@inproceedings{mackrl,
 author = {Schroeder de Witt, Christian and Foerster, Jakob and Farquhar, Gregory and Torr, Philip and Boehmer, Wendelin and Whiteson, Shimon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {9927--9939},
 publisher = {Curran Associates, Inc.},
 title = {Multi-Agent Common Knowledge Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2019/file/f968fdc88852a4a3a27a81fe3f57bfc5-Paper.pdf},
 volume = {32},
 year = {2019}
}

@misc{
jaques,
title={Intrinsic Social Motivation via Causal Influence in Multi-Agent {RL}},
author={Natasha Jaques and Angeliki Lazaridou and Edward Hughes and Caglar Gulcehre and Pedro A. Ortega and DJ Strouse and Joel Z. Leibo and Nando de Freitas},
year={2019},
url={https://openreview.net/forum?id=B1lG42C9Km},
}

@inproceedings{strouse,
author = {Strouse, D J and Kleiman-Weiner, Max and Tenenbaum, Josh and Botvinick, Matt and Schwab, David},
title = {Learning to Share and Hide Intentions Using Information Regularization},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Learning to cooperate with friends and compete with foes is a key component of multi-agent reinforcement learning. Typically to do so, one requires access to either a model of or interaction with the other agent(s). Here we show how to learn effective strategies for cooperation and competition in an asymmetric information game with no such model or interaction. Our approach is to encourage an agent to reveal or hide their intentions using an information-theoretic regularizer. We consider both the mutual information between goal and action given state, as well as the mutual information between goal and state. We show how to stochastically optimize these regularizers in a way that is easy to integrate with policy gradient reinforcement learning. Finally, we demonstrate that cooperative (competitive) policies learned with our approach lead to more (less) reward for a second agent in two simple asymmetric information games.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {10270–10281},
numpages = {12},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}

@INPROCEEDINGS{dragan,
  author={A. D. {Dragan} and K. C. T. {Lee} and S. S. {Srinivasa}},
  booktitle={2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, 
  title={Legibility and predictability of robot motion}, 
  year={2013},
  volume={},
  number={},
  pages={301-308},
  doi={10.1109/HRI.2013.6483603}}
  
@InProceedings{fergus-belief-modeling, title = {Modeling Others using Oneself in Multi-Agent Reinforcement Learning}, author = {Raileanu, Roberta and Denton, Emily and Szlam, Arthur and Fergus, Rob}, booktitle = {Proceedings of the 35th International Conference on Machine Learning}, pages = {4257--4266}, year = {2018}, editor = {Jennifer Dy and Andreas Krause}, volume = {80}, series = {Proceedings of Machine Learning Research}, address = {Stockholmsmässan, Stockholm Sweden}, month = {10--15 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v80/raileanu18a/raileanu18a.pdf}, url = {http://proceedings.mlr.press/v80/raileanu18a.html}, abstract = {We consider the multi-agent reinforcement learning setting with imperfect information. The reward function depends on the hidden goals of both agents, so the agents must infer the other players’ goals from their observed behavior in order to maximize their returns. We propose a new approach for learning in these domains: Self Other-Modeling (SOM), in which an agent uses its own policy to predict the other agent’s actions and update its belief of their hidden goal in an online manner. We evaluate this approach on three different tasks and show that the agents are able to learn better policies using their estimate of the other players’ goals, in both cooperative and competitive settings.} }
@article{stone-belief-modeling,
  title={Autonomous agents modelling other agents: A comprehensive survey and open problems},
  author={Stefano V. Albrecht and P. Stone},
  journal={Artif. Intell.},
  year={2018},
  volume={258},
  pages={66-95}
}
@article{,
  author    = {Jakob N. Foerster and
               Yannis M. Assael and
               Nando de Freitas and
               Shimon Whiteson},
  title     = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1605.06676},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.06676},
  archivePrefix = {arXiv},
  eprint    = {1605.06676},
  timestamp = {Mon, 13 Aug 2018 16:47:22 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/FoersterAFW16a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{mei2020global,
      title={On the Global Convergence Rates of Softmax Policy Gradient Methods}, 
      author={Jincheng Mei and Chenjun Xiao and Csaba Szepesvari and Dale Schuurmans},
      year={2020},
      eprint={2005.06392},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{eff-coop-irl, title = {An Efficient, Generalized {B}ellman Update For Cooperative Inverse Reinforcement Learning}, author = {Malik, Dhruv and Palaniappan, Malayandi and Fisac, Jaime and Hadfield-Menell, Dylan and Russell, Stuart and Dragan, Anca}, booktitle = {Proceedings of the 35th International Conference on Machine Learning}, pages = {3394--3402}, year = {2018}, editor = {Jennifer Dy and Andreas Krause}, volume = {80}, series = {Proceedings of Machine Learning Research}, address = {Stockholmsmässan, Stockholm Sweden}, month = {10--15 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v80/malik18a/malik18a.pdf}, url = {http://proceedings.mlr.press/v80/malik18a.html}, abstract = {Our goal is for AI systems to correctly identify and act according to their human user’s objectives. Cooperative Inverse Reinforcement Learning (CIRL) formalizes this value alignment problem as a two-player game between a human and robot, in which only the human knows the parameters of the reward function: the robot needs to learn them as the interaction unfolds. Previous work showed that CIRL can be solved as a POMDP, but with an action space size exponential in the size of the reward parameter space. In this work, we exploit a specific property of CIRL: the human is a full information agent. This enables us to derive an optimality-preserving modification to the standard Bellman update, which reduces the complexity of the problem by an exponential factor. Additionally, we show that our modified Bellman update allows us to relax CIRL’s assumption of human rationality. We apply this update to a variety of POMDP solvers, including exact methods, point-based methods, and Monte Carlo Tree Search methods. We find that it enables us to scale CIRL to non-trivial problems, with larger reward parameter spaces, and larger action spaces for both robot and human. In solutions to these larger problems, the human exhibits pedagogical (teaching) behavior, while the robot interprets it as such and attains higher value for the human.} }
@inproceedings{coop-irl,
 author = {Hadfield-Menell, Dylan and Russell, Stuart J and Abbeel, Pieter and Dragan, Anca},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {3909--3917},
 publisher = {Curran Associates, Inc.},
 title = {Cooperative Inverse Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2016/file/c3395dd46c34fa7fd8d729d8cf88b7a8-Paper.pdf},
 volume = {29},
 year = {2016}
}

@inproceedings{policy-belief-learning,
  author    = {Zheng Tian and
               Shihao Zou and
               Ian Davies and
               Tim Warr and
               Lisheng Wu and
               Haitham Bou{-}Ammar and
               Jun Wang},
  title     = {Learning to Communicate Implicitly by Actions},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2020, The Thirty-Second Innovative Applications of Artificial Intelligence
               Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
               Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
               February 7-12, 2020},
  pages     = {7261--7268},
  publisher = {{AAAI} Press},
  year      = {2020},
  url       = {https://aaai.org/ojs/index.php/AAAI/article/view/6217},
  timestamp = {Tue, 02 Feb 2021 08:00:03 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/TianZDWWBW20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{marl,
  author    = {Kaiqing Zhang and
               Zhuoran Yang and
               Tamer Basar},
  title     = {Multi-Agent Reinforcement Learning: {A} Selective Overview of Theories
               and Algorithms},
  journal   = {CoRR},
  volume    = {abs/1911.10635},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.10635},
  archivePrefix = {arXiv},
  eprint    = {1911.10635},
  timestamp = {Tue, 03 Dec 2019 14:15:54 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-10635.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{gt,
author = {Leyton-Brown, Kevin and Shoham, Yoav},
title = {Essentials of Game Theory: A Concise, Multidisciplinary Introduction},
year = {2008},
isbn = {1598295934},
publisher = {Morgan and Claypool Publishers},
edition = {1st}
}

@InProceedings{one-sided, title = {Optimally Solving Two-Agent Decentralized {POMDP}s Under One-Sided Information Sharing}, author = {Xie, Yuxuan and Dibangoye, Jilles and Buffet, Olivier}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {10473--10482}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/xie20a/xie20a.pdf}, url = { http://proceedings.mlr.press/v119/xie20a.html }, abstract = {Optimally solving decentralized partially observable Markov decision processes under either full or no information sharing received significant attention in recent years. However, little is known about how partial information sharing affects existing theory and algorithms. This paper addresses this question for a team of two agents, with one-sided information sharing—\ie both agents have imperfect information about the state of the world, but only one has access to what the other sees and does. From the perspective of a central planner, we show that the original problem can be reformulated into an equivalent information-state Markov decision process and solved as such. Besides, we prove that the optimal value function exhibits a specific form of uniform continuity. We also present a heuristic search algorithm utilizing this property and providing the first results for this family of problems.} }
@article{emergent-comm,
  title={Multi-Agent Cooperation and the Emergence of (Natural) Language},
  author={A. Lazaridou and Alexander Peysakhovich and M. Baroni},
  journal={ArXiv},
  year={2017},
  volume={abs/1612.07182}
}

@article{referential-cogsci,
author = {Spike, Matthew and Stadler, Kevin and Kirby, Simon and Smith, Kenny},
title = {Minimal Requirements for the Emergence of Learned Signaling},
journal = {Cognitive Science},
volume = {41},
number = {3},
pages = {623-658},
keywords = {Communication, Cultural evolution, Signaling games, Reinforcement learning, Feedback learning, Observational learning, Agent-based models, Exemplar theory},
doi = {https://doi.org/10.1111/cogs.12351},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12351},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12351},
year = {2017}
}

@article{referential-evoling,
author = {Smith, Kenny},
year = {2002},
month = {04},
pages = {},
title = {The Cultural Evolution of Communication in a Population of Neural Networks},
volume = {14},
journal = {Connection Science},
doi = {10.1080/09540090210164306}
}

@article{referential-alife,
author = {Steels, Luc},
year = {2013},
month = {03},
pages = {},
title = {Self-organization and Selection in Cultural Language Evolution},
doi = {10.1075/ais.3.02ste},
journal = {Experiments in Cultural Language Evolution}
}

@book{referential-gt,
	publisher = {Oxford University Press},
	author = {Brian Skyrms},
	year = {2010},
	title = {Signals: Evolution, Learning, and Information}
}

@book{ck-cs,
author = {Meyer, John-Jules Ch and Hoek, Wiebe Van Der},
title = {Epistemic Logic for AI and Computer Science},
year = {1995},
isbn = {052146014X},
publisher = {Cambridge University Press},
address = {USA},
abstract = {From the Publisher:Epistemic logic has grown from its philosophical beginnings to find diverse applications in computer science as a means of reasoning about the knowledge and belief of agents. This book, based on courses taught at universities and summer schools, provides a broad introduction to the subject; many exercises are included as well as their solutions. After presenting the necessary apparatus from mathematics and logic, the authors consider applications in the areas of common knowledge, distributed knowledge, explicit and implicit belief.}
}

@book{reasoning-about-knowledge,
author = {Fagin, Ronald and Halpern, Joseph Y. and Moses, Yoram and Vardi, Moshe Y.},
title = {Reasoning About Knowledge},
year = {2003},
isbn = {0262562006},
publisher = {MIT Press},
address = {Cambridge, MA, USA}
}

@InCollection{sep-common-knowledge,
	author       =	{Vanderschraaf, Peter and Sillari, Giacomo},
	title        =	{{Common Knowledge}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/spr2014/entries/common-knowledge/}},
	year         =	{2014},
	edition      =	{Spring 2014},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@article{channel-coding-with-costs,
author = {Iwata, Ken-ichi and Morii, Masakatu and Uyematsu, T.},
year = {1997},
month = {01},
pages = {2232-2237},
title = {An efficient universal coding algorithm for noiseless channel with symbols of unequal cost},
journal={IEICE Transactions on Fundamentals}
}
@article{mccr,
  author    = {Michal Sustr and
               Vojtech Kovar{\'{\i}}k and
               Viliam Lis{\'{y}}},
  title     = {Monte Carlo Continual Resolving for Online Strategy Computation in
               Imperfect Information Games},
  journal   = {CoRR},
  volume    = {abs/1812.07351},
  year      = {2018},
  url       = {http://arxiv.org/abs/1812.07351},
  archivePrefix = {arXiv},
  eprint    = {1812.07351},
  timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1812-07351.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{posg,
author = {Hansen, Eric A. and Bernstein, Daniel S. and Zilberstein, Shlomo},
title = {Dynamic Programming for Partially Observable Stochastic Games},
year = {2004},
isbn = {0262511835},
publisher = {AAAI Press},
abstract = {We develop an exact dynamic programming algorithm for partially observable stochastic games (POSGs). The algorithm is a synthesis of dynamic programming for partially observable Markov decision processes (POMDPs) and iterated elimination or dominated strategies in normal form games. We prove that when applied to finite-horizon POSGs, the algorithm iteratively eliminates very weakly dominated strategies without first forming a normal form representation of the game. For the special case in which agents share the same payoffs, the algorithm can be used to find an optimal solution. We present preliminary empirical results and discuss ways to further exploit POMDP theory in solving POSGs.},
booktitle = {Proceedings of the 19th National Conference on Artifical Intelligence},
pages = {709–715},
numpages = {7},
location = {San Jose, California},
series = {AAAI'04}
}

@book{information-theory,
author = {MacKay, David J. C.},
title = {Information Theory, Inference \& Learning Algorithms},
year = {2002},
isbn = {0521642981},
publisher = {Cambridge University Press},
address = {USA}
}

@inproceedings{epsilon-complexity,
  title={The complexity of multiagent systems: the price of silence},
  author={Zinovi Rabinovich and C. Goldman and J. S. Rosenschein},
  booktitle={AAMAS '03},
  year={2003}
}

@article{dec-control-survey,
  title={Decentralized stochastic control},
  author={Aditya Mahajan and Mehnaz Mannan},
  journal={Annals of Operations Research},
  year={2016},
  volume={241},
  pages={109-126}
}

@ARTICLE{swarm,
  author={L. E. {Barnes} and M. A. {Fields} and K. P. {Valavanis}},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)}, 
  title={Swarm Formation Control Utilizing Elliptical Surfaces and Limiting Functions}, 
  year={2009},
  volume={39},
  number={6},
  pages={1434-1445},
  doi={10.1109/TSMCB.2009.2018139}}
  
@inproceedings{crisis,
author = {Paquet, S\'{e}bastien and Tobin, Ludovic and Chaib-draa, Brahim},
title = {An Online POMDP Algorithm for Complex Multiagent Environments},
year = {2005},
isbn = {1595930930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1082473.1082620},
doi = {10.1145/1082473.1082620},
abstract = {In this paper, we present an online method for POMDPs, called RTBSS (Real-Time Belief Space Search), which is based on a look-ahead search to find the best action to execute at each cycle in an environment. We thus avoid the overwhelming complexity of computing a policy for each possible situation. By doing so, we show that this method is particularly efficient for large real-time environments where offline approaches are not applicable because of their complexity. We first describe the formalism of our online method, followed by some results on standard POMDPs. Then, we present an adaptation of our method for a complex multiagent environment and results showing its efficiency in such environments.},
booktitle = {Proceedings of the Fourth International Joint Conference on Autonomous Agents and Multiagent Systems},
pages = {970–977},
numpages = {8},
keywords = {online search, POMDP},
location = {The Netherlands},
series = {AAMAS '05}
}

@inproceedings{robotics,
author = {Seuken, Sven and Zilberstein, Shlomo},
title = {Memory-Bounded Dynamic Programming for DEC-POMDPs},
year = {2007},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Decentralized decision making under uncertainty has been shown to be intractable when each agent has different partial information about the domain. Thus, improving the applicability and scalability of planning algorithms is an important challenge. We present the first memory-bounded dynamic programming algorithm for finite-horizon decentralized POMDPs. A set of heuristics is used to identify relevant points of the infinitely large belief space. Using these belief points, the algorithm successively selects the best joint policies for each horizon. The algorithm is extremely efficient, having linear time and space complexity with respect to the horizon length. Experimental results show that it can handle horizons that are multiple orders of magnitude larger than what was previously possible, while achieving the same or better solution quality. These results significantly increase the applicability of decentralized decision-making techniques.},
booktitle = {Proceedings of the 20th International Joint Conference on Artifical Intelligence},
pages = {2009–2015},
numpages = {7},
location = {Hyderabad, India},
series = {IJCAI'07}
}

@MISC{comm-network,
    author = {Leonid Peshkin},
    title = {Reinforcement Learning by Policy Search},
    year = {2000}
}

@misc{hat-coding,
  author = {Jeff Wu},
  title = {WTFWThat},
  year = {2018},
  publisher = {GitHub},
  howpublished = {\url{https://github.com/WuTheFWasThat/hanabi.rs}}
}
@article{hanabi,
title	= {The Hanabi Challenge: A New Frontier for AI Research},
author	= {Nolan Bard and Jakob N. Foerster and Sarath Chandar and Neil Burch and Marc Lanctot and H. Francis Song and Emilio Parisotto and Vincent Dumoulin and Subhodeep Moitra and Edward Hughes and Iain Dunning and Shibl Mourad and Hugo Larochelle and Marc G. Bellemare and Michael Bowling},
year	= {2020},
URL	= {https://doi.org/10.1016/j.artint.2019.103216},
journal	= {Artificial Intelligence},
volume	= {280}
}

@inproceedings{sokota2021solving, 
title       = {Solving Common-Payoff Games with Approximate Policy Iteration}, 
journal     = {Proceedings of the AAAI Conference on Artificial Intelligence}, 
author      = {Sokota, Samuel and Lockhart, Edward and Timbers, Finbarr and Davoodi, Elnaz and D’Orazio, Ryan and Burch, Neil and Schmid, Martin and Bowling, Michael and Lanctot, Marc}, 
year        = {2021}
}

@inproceedings{
sad,
title={Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning},
author={Hengyuan Hu and Jakob N Foerster},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=B1xm3RVtwB}
}

@inproceedings{sparta,
  title={Improving Policies via Search in Cooperative Partially Observable Games},
  author={A. Lerer and H. Hu and Jakob N. Foerster and Noam Brown},
  booktitle={AAAI},
  year={2020}
}
@article{dibangoye2016optimally,
  title={Optimally solving Dec-POMDPs as continuous-state MDPs},
  author={Dibangoye, Jilles Steeve and Amato, Christopher and Buffet, Olivier and Charpillet, Fran{\c{c}}ois},
  journal={Journal of Artificial Intelligence Research},
  volume={55},
  pages={443--497},
  year={2016}
}


@article{load-balancing,
  title={An approximate dynamic programming approach to decentralized control of stochastic systems},
  author={R. Cogill and M. Rotkowitz and Benjamin Van Roy and S. Lall},
  journal={Lecture Notes in Control and Information Sciences},
  year={2006},
  pages={243-256}
}
@article{ale,
   title={The Arcade Learning Environment: An Evaluation Platform for General Agents},
   volume={47},
   ISSN={1076-9757},
   url={http://dx.doi.org/10.1613/jair.3912},
   DOI={10.1613/jair.3912},
   journal={Journal of Artificial Intelligence Research},
   publisher={AI Access Foundation},
   author={Bellemare, M. G. and Naddaf, Y. and Veness, J. and Bowling, M.},
   year={2013},
   month={Jun},
   pages={253–279}
}

@misc{mult-mec-appx,
      title={Efficient Approximate Minimum Entropy Coupling of Multiple Probability Distributions}, 
      author={Cheuk Ting Li},
      year={2020},
      eprint={2006.07955},
      archivePrefix={arXiv},
      primaryClass={cs.IT}
}
@ARTICLE{mec-appx,
  author={F. {Cicalese} and L. {Gargano} and U. {Vaccaro}},
  journal={IEEE Transactions on Information Theory}, 
  title={Minimum-Entropy Couplings and Their Applications}, 
  year={2019},
  volume={65},
  number={6},
  pages={3436-3451},
  doi={10.1109/TIT.2019.2894519}}

@article{mec-hardness,
title = {On the entropy of couplings},
journal = {Information and Computation},
volume = {242},
pages = {369-382},
year = {2015},
issn = {0890-5401},
doi = {https://doi.org/10.1016/j.ic.2015.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0890540115000450},
author = {Mladen Kovačević and Ivan Stanojević and Vojin Šenk},
keywords = {Coupling, Distribution with fixed marginals, Contingency table, Information measure, Entropy minimization, Maximization of information divergence, Subset sum, Partition, Entropy metric, Measure of dependence},
abstract = {In this paper, some general properties of Shannon information measures are investigated over sets of probability distributions with restricted marginals. Certain optimization problems associated with these functionals are shown to be NP-hard, and their special cases are found to be essentially information-theoretic restatements of well-known computational problems, such as the Subset sum and the 3-Partition. The notion of minimum entropy coupling is introduced and its relevance is demonstrated in information-theoretic, computational, and statistical contexts. Finally, a family of pseudometrics (on the space of discrete probability distributions) defined by these couplings is studied, in particular their relation to the total variation distance, and a new characterization of the conditional entropy is given.}
}
@inproceedings{horde,
author = {Sutton, Richard and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick and White, Adam and Precup, Doina},
year = {2011},
month = {01},
pages = {},
title = {Horde : A Scalable Real-time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction Categories and Subject Descriptors},
volume = {2},
journal = {Proceedings of the 10th International Conference on Autonomous Agents and Multiagent Systems}
}
@article{fosg,
  author    = {Vojtech Kovar{\'{\i}}k and
               Martin Schmid and
               Neil Burch and
               Michael Bowling and
               Viliam Lis{\'{y}}},
  title     = {Rethinking Formal Models of Partially Observable Multiagent Decision
               Making},
  journal   = {CoRR},
  volume    = {abs/1906.11110},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.11110},
  archivePrefix = {arXiv},
  eprint    = {1906.11110},
  timestamp = {Thu, 27 Jun 2019 18:54:51 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-11110.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{sac, title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor}, author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey}, booktitle = {Proceedings of the 35th International Conference on Machine Learning}, pages = {1861--1870}, year = {2018}, editor = {Jennifer Dy and Andreas Krause}, volume = {80}, series = {Proceedings of Machine Learning Research}, address = {Stockholmsmässan, Stockholm Sweden}, month = {10--15 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf}, url = {http://proceedings.mlr.press/v80/haarnoja18b.html}, abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.} }

@book{oliehoek_concise_2016,
  title={A concise introduction to decentralized POMDPs},
  author={Oliehoek, Frans A and Amato, Christopher and others},
  volume={1},
  year={2016},
  publisher={Springer}
}


@inproceedings{haarnoja_reinforcement_2017,
author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
title = {Reinforcement Learning with Deep Energy-Based Policies},
year = {2017},
publisher = {JMLR.org},
abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1352–1361},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@article{wei_multiagent_2018,
	title = {Multiagent {Soft} {Q}-{Learning}},
	url = {http://arxiv.org/abs/1804.09817},
	abstract = {Policy gradient methods are often applied to reinforcement learning in continuous multiagent games. These methods perform local search in the joint-action space, and as we show, they are susceptable to a game-theoretic pathology known as relative overgeneralization. To resolve this issue, we propose Multiagent Soft Q-learning, which can be seen as the analogue of applying Q-learning to continuous controls. We compare our method to MADDPG, a state-of-the-art approach, and show that our method achieves better coordination in multiagent cooperative tasks, converging to better local optima in the joint action space.},
	urldate = {2020-12-11},
	journal = {arXiv:1804.09817 [cs]},
	author = {Wei, Ermo and Wicke, Drew and Freelan, David and Luke, Sean},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.09817},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Accepted in AAAI 18 Spring Symposium},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/I5TVGJJS/Wei et al. - 2018 - Multiagent Soft Q-Learning.pdf:application/pdf;arXiv.org Snapshot:/home/cs/Zotero/storage/5884X4I9/1804.html:text/html}
}

@article{shi_soft_2019,
	title = {Soft {Policy} {Gradient} {Method} for {Maximum} {Entropy} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1909.03198},
	abstract = {Maximum entropy deep reinforcement learning (RL) methods have been demonstrated on a range of challenging continuous tasks. However, existing methods either suffer from severe instability when training on large off-policy data or cannot scale to tasks with very high state and action dimensionality such as 3D humanoid locomotion. Besides, the optimality of desired Boltzmann policy set for non-optimal soft value function is not persuasive enough. In this paper, we first derive soft policy gradient based on entropy regularized expected reward objective for RL with continuous actions. Then, we present an off-policy actor-critic, model-free maximum entropy deep RL algorithm called deep soft policy gradient (DSPG) by combining soft policy gradient with soft Bellman equation. To ensure stable learning while eliminating the need of two separate critics for soft value functions, we leverage double sampling approach to making the soft Bellman equation tractable. The experimental results demonstrate that our method outperforms in performance over off-policy prior methods.},
	urldate = {2020-12-11},
	journal = {arXiv:1909.03198 [cs, stat]},
	author = {Shi, Wenjie and Song, Shiji and Wu, Cheng},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.03198},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: to be published in Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/LT2A242L/Shi et al. - 2019 - Soft Policy Gradient Method for Maximum Entropy De.pdf:application/pdf;arXiv.org Snapshot:/home/cs/Zotero/storage/QPTNJTXY/1909.html:text/html}
}

@phdthesis{tatikonda_control_2000,
	title = {Control {Under} {Communication} {Constraints}},
	school = {Massachusetts Institute of Technology},
	author = {Tatikonda, Sekhar Chandra},
	year = {2000}
}


@inproceedings{nayyar,
	title = {The {Common}-{Information} {Approach} to {Decentralized} {Stochastic} {Control}},
	doi = {10.1007/978-3-319-02150-8_4},
	language = {English (US)},
	urldate = {2017-12-03},
	publisher = {Springer Verlag},
	author = {Nayyar, Ashutosh and Mahajan, Aditya and Teneketzis, Demosthenis},
	year = {2014},
}
@inproceedings{foerster_bayesian_2019,
	author        = "Jakob N. Foerster and H. Francis Song and Edward Hughes and Neil Burch and Iain Dunning and Shimon Whiteson and Matthew Botvinick and Michael Bowling",
	booktitle     = "{Proceedings of the 36th International Conference on Machine Learning}",
	ee            = "http://proceedings.mlr.press/v97/foerster19a.html",
	pages         = "1942--1951",
	publisher     = "{PMLR}",
	title         = "{Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning}",
	year          = 2019,
}

@article{berry_shannon_2011,
	title = {Shannon {Meets} {Nash} on the {Interference} {Channel}},
	volume = {57},
	issn = {1557-9654},
	doi = {10.1109/TIT.2011.2119730},
	abstract = {The interference channel is the simplest communication scenario where multiple autonomous users compete for shared resources. We combine game theory and information theory to define the notion of a Nash equilibrium region of the interference channel. The notion is game theoretic: it captures the selfish behavior of each user as they compete. The notion is also information theoretic: it allows each user to use arbitrary communication strategies as it optimizes its own performance. We give an exact characterization of the Nash equilibrium region of the two-user linear deterministic interference channel and an approximate characterization of the Nash equilibrium region of the two-user Gaussian interference channel to within 1 bit/s/Hz.},
	number = {5},
	journal = {IEEE Transactions on Information Theory},
	author = {Berry, R. A. and Tse, D. N. C.},
	month = may,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {arbitrary communication strategies, autonomous users, Encoding, Error probability, game theory, Game theory, Gaussian channels, Han–Kobayashi scheme, information theory, Integrated circuits, interference (signal), interference channel, Interference channels, Nash equilibrium, Nash equilibrium region, Receivers, selfish behavior, Shannon theory, shared resources, Transmitters, two-user Gaussian interference channel, two-user linear deterministic interference channel},
	pages = {2821--2836},
	file = {IEEE Xplore Abstract Record:/Users/cs/Zotero/storage/3SNYQFPR/5752421.html:text/html;Submitted Version:/Users/cs/Zotero/storage/Q7Q29NLG/Berry and Tse - 2011 - Shannon Meets Nash on the Interference Channel.pdf:application/pdf}
}

@article{levine_reinforcement_2018,
	title = {Reinforcement {Learning} and {Control} as {Probabilistic} {Inference}: {Tutorial} and {Review}},
	shorttitle = {Reinforcement {Learning} and {Control} as {Probabilistic} {Inference}},
	url = {http://arxiv.org/abs/1805.00909},
	abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
	urldate = {2020-12-13},
	journal = {arXiv:1805.00909 [cs, stat]},
	author = {Levine, Sergey},
	month = may,
	year = {2018},
	note = {arXiv: 1805.00909},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/XCJVGDAA/Levine - 2018 - Reinforcement Learning and Control as Probabilisti.pdf:application/pdf}
}
	
@inproceedings{haarnoja_soft_2018,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@book{sutton_reinforcement_2018,
	address = {Cambridge, MA, USA},
	title = {Reinforcement {Learning}: {An} {Introduction}},
	isbn = {978-0-262-03924-6},
	shorttitle = {Reinforcement {Learning}},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms. This second edition has been significantly expanded and updated, presenting new topics and updating coverage of other topics. Like the first edition, this second edition focuses on core online learning algorithms, with the more mathematical material set off in shaded boxes. Part I covers as much of reinforcement learning as possible without going beyond the tabular case for which exact solutions can be found. Many algorithms presented in this part are new to the second edition, including UCB, Expected Sarsa, and Double Learning. Part II extends these ideas to function approximation, with new sections on such topics as artificial neural networks and the Fourier basis, and offers expanded treatment of off-policy learning and policy-gradient methods. Part III has new chapters on reinforcement learning's relationships to psychology and neuroscience, as well as an updated case-studies chapter including AlphaGo and AlphaGo Zero, Atari game playing, and IBM Watson's wagering strategy. The final chapter discusses the future societal impacts of reinforcement learning.},
	publisher = {A Bradford Book},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {2018}
}

@INPROCEEDINGS{kostina_2011,
  author={Kostina, Victoria and Verdú, Sergio},
  booktitle={2011 IEEE Information Theory Workshop}, 
  title={Fixed-length lossy compression in the finite blocklength regime: Gaussian source}, 
  year={2011},
  volume={},
  number={},
  pages={457-461},
  doi={10.1109/ITW.2011.6089501}}
  
@inproceedings{ziebart_maximum_2008,
	address = {Chicago, Illinois},
	series = {{AAAI}'08},
	title = {Maximum entropy inverse reinforcement learning},
	isbn = {978-1-57735-368-3},
	abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling real-world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories.},
	urldate = {2020-12-13},
	booktitle = {Proceedings of the 23rd national conference on {Artificial} intelligence - {Volume} 3},
	publisher = {AAAI Press},
	author = {Ziebart, Brian D. and Maas, Andrew and Bagnell, J. Andrew and Dey, Anind K.},
	month = jul,
	year = {2008},
	pages = {1433--1438}
}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14236},
	doi = {10.1038/nature14236},
	abstract = {An artificial agent is developed that learns to play a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
	language = {en},
	number = {7540},
	urldate = {2020-12-13},
	journal = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
	month = feb,
	year = {2015},
	note = {Number: 7540
Publisher: Nature Publishing Group},
	pages = {529--533},
	file = {Full Text PDF:/home/cs/Zotero/storage/GK8P3BMA/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf:application/pdf}
}

@article{huffman_method_1952,
	title = {A {Method} for the {Construction} of {Minimum}-{Redundancy} {Codes}},
	volume = {40},
	issn = {2162-6634},
	doi = {10.1109/JRPROC.1952.273898},
	abstract = {An optimum method of coding an ensemble of messages consisting of a finite number of members is developed. A minimum-redundancy code is one constructed in such a way that the average number of coding digits per message is minimized.},
	number = {9},
	journal = {Proceedings of the IRE},
	author = {Huffman, D. A.},
	month = sep,
	year = {1952},
	note = {Conference Name: Proceedings of the IRE},
	keywords = {Transmitters},
	pages = {1098--1101},
	file = {IEEE Xplore Abstract Record:/home/cs/Zotero/storage/B5F38F8Y/4051119.html:text/html;IEEE Xplore Full Text PDF:/home/cs/Zotero/storage/Y65B9UF3/Huffman - 1952 - A Method for the Construction of Minimum-Redundanc.pdf:application/pdf}
}

@article{kingma_introduction_2019,
	title = {An {Introduction} to {Variational} {Autoencoders}},
	volume = {12},
	issn = {1935-8237, 1935-8245},
	url = {http://arxiv.org/abs/1906.02691},
	doi = {10.1561/2200000056},
	abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
	number = {4},
	urldate = {2020-12-13},
	journal = {Foundations and Trends® in Machine Learning},
	author = {Kingma, Diederik P. and Welling, Max},
	year = {2019},
	note = {arXiv: 1906.02691},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {307--392},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/QZRM57VC/Kingma und Welling - 2019 - An Introduction to Variational Autoencoders.pdf:application/pdf}
}


@misc{sokota_solving_2020,
	title = {Solving {Common}-{Payoff} {Games} with {Approximate} {Policy} {Iteration}},
	url = {https://era.library.ualberta.ca/items/17edd0aa-ec13-4cd4-8e30-a77b5d8c5ccc},
	abstract = {For artificially intelligent learning systems to be deployed widely in real-world settings, it is important that they be able to operate...},
	language = {en},
	urldate = {2021-01-02},
	journal = {ERA},
	author = {Sokota, Samuel},
	year = {2020},
	doi = {10.7939/r3-4gp9-jc31},
	file = {Full Text PDF:/home/cs/Zotero/storage/LJRUACMQ/Sokota - 2020 - Solving Common-Payoff Games with Approximate Polic.pdf:application/pdf;Snapshot:/home/cs/Zotero/storage/35KPHZDG/17edd0aa-ec13-4cd4-8e30-a77b5d8c5ccc.html:text/html}
}

@article{kovarik_rethinking_2020,
	title = {Rethinking {Formal} {Models} of {Partially} {Observable} {Multiagent} {Decision} {Making}},
	url = {http://arxiv.org/abs/1906.11110},
	abstract = {Multiagent decision-making in partially observable environments is usually modelled as either an extensive-form game (EFG) in game theory or a partially observable stochastic game (POSG) in multiagent reinforcement learning (MARL). One issue with the current situation is that while most practical problems can be modelled in both formalisms, the relationship of the two models is unclear, which hinders the transfer of ideas between the two communities. A second issue is that while EFGs have recently seen significant algorithmic progress, their classical formalization is unsuitable for efficient presentation of the underlying ideas, such as those around decomposition. To solve the first issue, we introduce factored-observation stochastic games (FOSGs), a minor modification of the POSG formalism which distinguishes between private and public observation and thereby greatly simplifies decomposition. To remedy the second issue, we show that FOSGs and POSGs are naturally connected to EFGs: by "unrolling" a FOSG into its tree form, we obtain an EFG. Conversely, any perfect-recall timeable EFG corresponds to some underlying FOSG in this manner. Moreover, this relationship justifies several minor modifications to the classical EFG formalization that recently appeared as an implicit response to the model's issues with decomposition. Finally, we illustrate the transfer of ideas between EFGs and MARL by presenting three key EFG techniques -- counterfactual regret minimization, sequence form, and decomposition -- in the FOSG framework.},
	urldate = {2021-01-02},
	journal = {arXiv:1906.11110 [cs]},
	author = {Kovařík, Vojtěch and Schmid, Martin and Burch, Neil and Bowling, Michael and Lisý, Viliam},
	month = oct,
	year = {2020},
	note = {arXiv: 1906.11110},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory},
	annote = {Comment: A 2020 update of the original 2019 version of the paper. (Rewrote the main text and clarified the relationship between FOSGs/POSGs and EFGs. Some of the technical results are now presented in the appendix.)},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/ECIEADM8/Kovařík et al. - 2020 - Rethinking Formal Models of Partially Observable M.pdf:application/pdf}
}


@article{shannon_mathematical_1948,
	title = {A {Mathematical} {Theory} of {Communication}},
	volume = {27},
	copyright = {© 1948 The Bell System Technical Journal},
	issn = {1538-7305},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1948.tb01338.x},
	doi = {https://doi.org/10.1002/j.1538-7305.1948.tb01338.x},
	language = {en},
	number = {3},
	urldate = {2021-01-03},
	journal = {Bell System Technical Journal},
	author = {Shannon, C. E.},
	year = {1948},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.1538-7305.1948.tb01338.x},
	pages = {379--423},
	file = {Snapshot:/home/cs/Zotero/storage/VGHAFHV6/j.1538-7305.1948.tb01338.html:text/html;Snapshot:/home/cs/Zotero/storage/W78AJZVL/j.1538-7305.1948.tb01338.html:text/html;Volltext:/home/cs/Zotero/storage/JGY6ZB7K/Shannon - 1948 - A Mathematical Theory of Communication.pdf:application/pdf}
}


@inproceedings{sutton_horde_2011,
	address = {Richland, SC},
	series = {{AAMAS} '11},
	title = {Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction},
	isbn = {978-0-9826571-6-4},
	shorttitle = {Horde},
	abstract = {Maintaining accurate world knowledge in a complex and changing environment is a perennial problem for robots and other artificial intelligence systems. Our architecture for addressing this problem, called Horde, consists of a large number of independent reinforcement learning sub-agents, or demons. Each demon is responsible for answering a single predictive or goal-oriented question about the world, thereby contributing in a factored, modular way to the system's overall knowledge. The questions are in the form of a value function, but each demon has its own policy, reward function, termination function, and terminal-reward function unrelated to those of the base problem. Learning proceeds in parallel by all demons simultaneously so as to extract the maximal training information from whatever actions are taken by the system as a whole. Gradient-based temporal-difference learning methods are used to learn efficiently and reliably with function approximation in this off-policy setting. Horde runs in constant time and memory per time step, and is thus suitable for learning online in real-time applications such as robotics. We present results using Horde on a multi-sensored mobile robot to successfully learn goal-oriented behaviors and long-term predictions from off-policy experience. Horde is a significant incremental step towards a real-time architecture for efficient learning of general knowledge from unsupervised sensorimotor interaction.},
	urldate = {2021-01-08},
	booktitle = {The 10th {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems} - {Volume} 2},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Sutton, Richard S. and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M. and White, Adam and Precup, Doina},
	month = may,
	year = {2011},
	keywords = {artificial intelligence, knowledge representation, off-policy learning, real-time, reinforcement learning, robotics, temporal-difference learning, value function approximation},
	pages = {761--768},
	file = {Full Text PDF:/Users/cs/Zotero/storage/SC2AKGTD/Sutton et al. - 2011 - Horde a scalable real-time architecture for learn.pdf:application/pdf}
}


@article{tatikonda_capacity_2008,
	title = {The capacity of channels with feedback},
	copyright = {Article is made available in accordance with the publisher's policy and may be subject to US copyright law. Please refer to the publisher's site for terms of use.},
	issn = {0018-9448},
	url = {https://dspace.mit.edu/handle/1721.1/53579},
	abstract = {In this paper, we introduce a general framework for treating channels with memory and feedback. First, we prove a general feedback channel coding theorem based on Massey's concept of directed information. Second, we present coding results for Markov channels. This requires determining appropriate sufficient statistics at the encoder and decoder. We give a recursive characterization of these sufficient statistics. Third, a dynamic programming framework for computing the capacity of Markov channels is presented. Fourth, it is shown that the average cost optimality equation (ACOE) can be viewed as an implicit single-letter characterization of the capacity. Fifth, scenarios with simple sufficient statistics are described. Sixth, error exponents for channels with feedback are presented.},
	language = {en\_US},
	urldate = {2021-01-09},
	journal = {IEEE},
	author = {Tatikonda, Sekhar and Mitter, Sanjoy},
	month = dec,
	year = {2008},
	note = {Accepted: 2010-04-08T14:15:24Z
Publisher: Institute of Electrical and Electronics Engineers},
	file = {Full Text PDF:/Users/cs/Zotero/storage/3QILUCL7/Tatikonda and Mitter - 2008 - The capacity of channels with feedback.pdf:application/pdf}
}


@book{wade_signal_1994,
	title = {Signal {Coding} and {Processing}},
	isbn = {978-0-521-42336-6},
	abstract = {Signal coding and signal processing are complex and critical elements of communications systems in broadcasting, satellite, and magnetic recording fields among others. Because both signal coding and processing are often found within a single electronic system (especially large communications systems), this book uniquely combines an introduction to both of these areas and provides an uncluttered theoretical treatment of optical coding and processing algorithms, together with design information for practical systems. The author examines Pulse Code Modulation fundamentals, surveys modern data compression techniques, introduces block and convolutional error control codes, and discusses modern transmission coding techniques. Complete with problems and solutions, and containing over 230 diagrams, this textbook will be invaluable to third- and fourth-year undergraduates in electronic, electrical, or communication engineering. It will also act as a useful reference for anyone working in this technologically important field.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Wade, Graham},
	month = sep,
	year = {1994},
	note = {Google-Books-ID: CJswCy7\_W8YC},
	keywords = {Technology \& Engineering / Electrical, Technology \& Engineering / Electronics / Optoelectronics, Technology \& Engineering / Telecommunications}
}


@article{jun_chen_capacity_2005,
	title = {The capacity of finite-{State} {Markov} {Channels} {With} feedback},
	volume = {51},
	issn = {1557-9654},
	doi = {10.1109/TIT.2004.842697},
	abstract = {We consider a class of finite-state Markov channels with feedback. We first introduce a simplified equivalent channel model, and then construct the optimal stationary and nonstationary input processes that maximize the long-term directed mutual information. Furthermore, we give a sufficient condition under which the channel's Shannon capacity can be achieved by a stationary input process. The corresponding converse coding theorem and direct coding theorem are proved.},
	number = {3},
	journal = {IEEE Transactions on Information Theory},
	author = {{Jun Chen} and Berger, T.},
	month = mar,
	year = {2005},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {channel capacity, Channel capacity, channel coding, Channel coding, channel Shannon capacity, Codes, coding theorem, direct coding theorem, Distributed computing, feedback, finite-state Markov channels, long-term directed mutual information, Markov channel, Markov processes, Mutual information, nonstationary input processes, Output feedback, Power capacitors, simplified equivalent channel model, State feedback, Sufficient conditions, Transmitters, typicality},
	pages = {780--798},
	file = {IEEE Xplore Full Text PDF:/Users/cs/Zotero/storage/FGHVCGW4/Jun Chen and Berger - 2005 - The capacity of finite-State Markov Channels With .pdf:application/pdf}
}


@article{barbier_robust_2013,
	title = {Robust error correction for real-valued signals via message-passing decoding and spatial coupling},
	url = {http://arxiv.org/abs/1304.6599},
	doi = {10.1109/ITW.2013.6691262},
	abstract = {We revisit the error correction scheme of real-valued signals when the codeword is corrupted by gross errors on a fraction of entries and a small noise on all the entries. Combining the recent developments of approximate message passing and the spatially-coupled measurement matrix in compressed sensing we show that the error correction and its robustness towards noise can be enhanced considerably. We discuss the performance in the large signal limit using previous results on state evolution, as well as for finite size signals through numerical simulations. Even for relatively small sizes, the approach proposed here outperforms convex-relaxation-based decoders.},
	urldate = {2021-01-09},
	journal = {2013 IEEE Information Theory Workshop (ITW)},
	author = {Barbier, Jean and Krzakala, Florent and Zdeborová, Lenka and Zhang, Pan},
	month = sep,
	year = {2013},
	note = {arXiv: 1304.6599},
	keywords = {Computer Science - Information Theory},
	pages = {1--5},
	annote = {Comment: 5 pages, 5 figures, IEEE Information Theory Workshop, Seville, September 9-13, 2013},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/A4GEG5AB/Barbier et al. - 2013 - Robust error correction for real-valued signals vi.pdf:application/pdf;arXiv.org Snapshot:/home/cs/Zotero/storage/NJSTY2SC/1304.html:text/html}
}

@article{donoho_compressed_2006,
	title = {Compressed sensing},
	volume = {52},
	doi = {10.1109/TIT.2006.871582},
	number = {4},
	journal = {IEEE Transactions on Information Theory},
	author = {Donoho, D.L.},
	month = apr,
	year = {2006},
	pages = {1289 -- 1306}
}

@book{lewis_convention_1969,
	title = {Convention: {A} {Philosophical} {Study}},
	shorttitle = {Convention},
	publisher = {Wiley-Blackwell},
	author = {Lewis, David K.},
	year = {1969}
}

@article{cao_overview_2013,
	title = {An overview of recent progress in the study of distributed multi-agent coordination},
	volume = {9},
	number = {1},
	journal = {IEEE Transactions on Industrial informatics},
	author = {Cao, Yongcan and Yu, Wenwu and Ren, Wei and Chen, Guanrong},
	year = {2013},
	pages = {427--438}
}

@article{huttenrauch_deep_2018,
	title = {Deep {Reinforcement} {Learning} for {Swarm} {Systems}},
	url = {http://arxiv.org/abs/1807.06613},
	abstract = {Recently, deep reinforcement learning (RL) methods have been applied successfully to multi-agent scenarios. Typically, these methods rely on a concatenation of agent states to represent the information content required for decentralized decision making. However, concatenation scales poorly to swarm systems with a large number of homogeneous agents as it does not exploit the fundamental properties inherent to these systems: (i) the agents in the swarm are interchangeable and (ii) the exact number of agents in the swarm is irrelevant. Therefore, we propose a new state representation for deep multi-agent RL based on mean embeddings of distributions. We treat the agents as samples of a distribution and use the empirical mean embedding as input for a decentralized policy. We define different feature spaces of the mean embedding using histograms, radial basis functions and a neural network learned end-to-end. We evaluate the representation on two well known problems from the swarm literature (rendezvous and pursuit evasion), in a globally and locally observable setup. For the local setup we furthermore introduce simple communication protocols. Of all approaches, the mean embedding representation using neural network features enables the richest information exchange between neighboring agents facilitating the development of more complex collective strategies.},
	urldate = {2019-03-21},
	journal = {arXiv:1807.06613 [cs, stat]},
	author = {Hüttenrauch, Maximilian and Šošić, Adrian and Neumann, Gerhard},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.06613},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Multiagent Systems, Computer Science - Systems and Control, Statistics - Machine Learning},
	annote = {Comment: 26 pages, 10 figures, version 2 (change to preprint style file)},
	file = {arXiv\:1807.06613 PDF:/home/cs/Zotero/storage/XJG3CX28/Hüttenrauch et al. - 2018 - Deep Reinforcement Learning for Swarm Systems.pdf:application/pdf;arXiv.org Snapshot:/home/cs/Zotero/storage/7NWKM4HV/1807.html:text/html}
}


@inproceedings{learning-to-comm,
 author = {Foerster, Jakob and Assael, Ioannis Alexandros and de Freitas, Nando and Whiteson, Shimon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {2137--2145},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2016/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf},
 volume = {29},
 year = {2016}
}

@article{dessi_focus_2019,
	title = {Focus on {What}'s {Informative} and {Ignore} {What}'s not: {Communication} {Strategies} in a {Referential} {Game}},
	shorttitle = {Focus on {What}'s {Informative} and {Ignore} {What}'s not},
	url = {http://arxiv.org/abs/1911.01892},
	abstract = {Research in multi-agent cooperation has shown that artificial agents are able to learn to play a simple referential game while developing a shared lexicon. This lexicon is not easy to analyze, as it does not show many properties of a natural language. In a simple referential game with two neural network-based agents, we analyze the object-symbol mapping trying to understand what kind of strategy was used to develop the emergent language. We see that, when the environment is uniformly distributed, the agents rely on a random subset of features to describe the objects. When we modify the objects making one feature non-uniformly distributed,the agents realize it is less informative and start to ignore it, and, surprisingly, they make a better use of the remaining features. This interesting result suggests that more natural, less uniformly distributed environments might aid in spurring the emergence of better-behaved languages.},
	urldate = {2021-01-10},
	journal = {arXiv:1911.01892 [cs]},
	author = {Dessì, Roberto and Bouchacourt, Diane and Crepaldi, Davide and Baroni, Marco},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.01892},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 3rd NeurIPS Workshop on Emergent Communication},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/ZUIFRDUR/Dessì et al. - 2019 - Focus on What's Informative and Ignore What's not.pdf:application/pdf;arXiv.org Snapshot:/home/cs/Zotero/storage/4RD7KJ58/1911.html:text/html}
}

@article{evtimova_emergent_2018,
	title = {Emergent {Communication} in a {Multi}-{Modal}, {Multi}-{Step} {Referential} {Game}},
	url = {http://arxiv.org/abs/1705.10369},
	abstract = {Inspired by previous work on emergent communication in referential games, we propose a novel multi-modal, multi-step referential game, where the sender and receiver have access to distinct modalities of an object, and their information exchange is bidirectional and of arbitrary duration. The multi-modal multi-step setting allows agents to develop an internal communication significantly closer to natural language, in that they share a single set of messages, and that the length of the conversation may vary according to the difficulty of the task. We examine these properties empirically using a dataset consisting of images and textual descriptions of mammals, where the agents are tasked with identifying the correct object. Our experiments indicate that a robust and efficient communication protocol emerges, where gradual information exchange informs better predictions and higher communication bandwidth improves generalization.},
	urldate = {2021-01-10},
	journal = {arXiv:1705.10369 [cs, math]},
	author = {Evtimova, Katrina and Drozdov, Andrew and Kiela, Douwe and Cho, Kyunghyun},
	month = apr,
	year = {2018},
	note = {arXiv: 1705.10369},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	annote = {Comment: Published as a conference paper at ICLR 2018. 12 pages},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/TU4WIG8P/Evtimova et al. - 2018 - Emergent Communication in a Multi-Modal, Multi-Ste.pdf:application/pdf;arXiv.org Snapshot:/home/cs/Zotero/storage/PUXDDZY7/1705.html:text/html}
}

@article{dagan_co-evolution_2020,
	title = {Co-evolution of language and agents in referential games},
	abstract = {Referential games offer a grounded learning environment for neural agents, that accounts for the functional aspects of language. However, they fail to account for another fundamental aspect of human language: Because languages are transmitted from generation to generation, they have to be learnable by new language users, which makes them subject to cultural evolution. Recent work has shown that incorporating cultural evolution in referential game results in considerable improvements in the properties of the languages that emerge in the game. In this work, we first substantiate this claim with a different data set and a wider array of evaluation metrics. Then, drawing inspiration from linguistic theories of human language evolution, we consider a scenario in which not only cultural but also genetic evolution is integrated. As our core contribution, we introduce the Language Transmission Engine, in which cultural evolution of the language is combined with genetic evolution of the agents' architecture. We show that this co-evolution scenario leads to across-the-board improvements on all considered metrics. These results stress that cultural evolution is important for language emergence studies, but also the suitability of the architecture itself should be considered.},
	journal = {ArXiv},
	author = {Dagan, Gautier and Hupkes, Dieuwke and Bruni, Elia},
	year = {2020},
	file = {Full Text PDF:/home/cs/Zotero/storage/26H4LAKZ/Dagan et al. - 2020 - Co-evolution of language and agents in referential.pdf:application/pdf}
}

@article{lazaridou_emergence_2018,
	title = {Emergence of {Linguistic} {Communication} from {Referential} {Games} with {Symbolic} and {Pixel} {Input}},
	url = {http://arxiv.org/abs/1804.03984},
	abstract = {The ability of algorithms to evolve or learn (compositional) communication protocols has traditionally been studied in the language evolution literature through the use of emergent communication tasks. Here we scale up this research by using contemporary deep learning methods and by training reinforcement-learning neural network agents on referential communication games. We extend previous work, in which agents were trained in symbolic environments, by developing agents which are able to learn from raw pixel data, a more challenging and realistic input representation. We find that the degree of structure found in the input data affects the nature of the emerged protocols, and thereby corroborate the hypothesis that structured compositional language is most likely to emerge when agents perceive the world as being structured.},
	urldate = {2021-01-10},
	journal = {arXiv:1804.03984 [cs]},
	author = {Lazaridou, Angeliki and Hermann, Karl Moritz and Tuyls, Karl and Clark, Stephen},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.03984},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	annote = {Comment: To appear at ICLR 2018},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/A3NSEYAV/Lazaridou et al. - 2018 - Emergence of Linguistic Communication from Referen.pdf:application/pdf}
}


@article{bard_hanabi_2020,
	title = {The {Hanabi} {Challenge}: {A} {New} {Frontier} for {AI} {Research}},
	volume = {280},
	issn = {00043702},
	shorttitle = {The {Hanabi} {Challenge}},
	url = {http://arxiv.org/abs/1902.00506},
	doi = {10.1016/j.artint.2019.103216},
	abstract = {From the early days of computing, games have been important testbeds for studying how well machines can do sophisticated decision making. In recent years, machine learning has made dramatic advances with artificial agents reaching superhuman performance in challenge domains like Go, Atari, and some variants of poker. As with their predecessors of chess, checkers, and backgammon, these game domains have driven research by providing sophisticated yet well-defined challenges for artificial intelligence practitioners. We continue this tradition by proposing the game of Hanabi as a new challenge domain with novel problems that arise from its combination of purely cooperative gameplay with two to five players and imperfect information. In particular, we argue that Hanabi elevates reasoning about the beliefs and intentions of other agents to the foreground. We believe developing novel techniques for such theory of mind reasoning will not only be crucial for success in Hanabi, but also in broader collaborative efforts, especially those with human partners. To facilitate future research, we introduce the open-source Hanabi Learning Environment, propose an experimental framework for the research community to evaluate algorithmic advances, and assess the performance of current state-of-the-art techniques.},
	urldate = {2021-01-10},
	journal = {Artificial Intelligence},
	author = {Bard, Nolan and Foerster, Jakob N. and Chandar, Sarath and Burch, Neil and Lanctot, Marc and Song, H. Francis and Parisotto, Emilio and Dumoulin, Vincent and Moitra, Subhodeep and Hughes, Edward and Dunning, Iain and Mourad, Shibl and Larochelle, Hugo and Bellemare, Marc G. and Bowling, Michael},
	month = mar,
	year = {2020},
	note = {arXiv: 1902.00506},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {103216},
	annote = {Comment: 32 pages, 5 figures, In Press (Artificial Intelligence)},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/3IYQX97T/Bard et al. - 2020 - The Hanabi Challenge A New Frontier for AI Resear.pdf:application/pdf;arXiv.org Snapshot:/home/cs/Zotero/storage/RNJ3MAVM/1902.html:text/html}
}


@article{kovacevic_entropy_2015,
	title = {On the entropy of couplings},
	volume = {242},
	issn = {0890-5401},
	url = {http://www.sciencedirect.com/science/article/pii/S0890540115000450},
	doi = {10.1016/j.ic.2015.04.003},
	abstract = {In this paper, some general properties of Shannon information measures are investigated over sets of probability distributions with restricted marginals. Certain optimization problems associated with these functionals are shown to be NP-hard, and their special cases are found to be essentially information-theoretic restatements of well-known computational problems, such as the Subset sum and the 3-Partition. The notion of minimum entropy coupling is introduced and its relevance is demonstrated in information-theoretic, computational, and statistical contexts. Finally, a family of pseudometrics (on the space of discrete probability distributions) defined by these couplings is studied, in particular their relation to the total variation distance, and a new characterization of the conditional entropy is given.},
	language = {en},
	urldate = {2021-01-10},
	journal = {Information and Computation},
	author = {Kovačević, Mladen and Stanojević, Ivan and Šenk, Vojin},
	month = jun,
	year = {2015},
	keywords = {Contingency table, Coupling, Distribution with fixed marginals, Entropy metric, Entropy minimization, Information measure, Maximization of information divergence, Measure of dependence, Partition, Subset sum},
	pages = {369--382},
	file = {ScienceDirect Full Text PDF:/home/cs/Zotero/storage/72MT7QS8/Kovačević et al. - 2015 - On the entropy of couplings.pdf:application/pdf}
}

@article{li_efficient_2020,
	title = {Efficient {Approximate} {Minimum} {Entropy} {Coupling} of {Multiple} {Probability} {Distributions}},
	url = {http://arxiv.org/abs/2006.07955},
	abstract = {Given a collection of probability distributions \$p\_\{1\},{\textbackslash}ldots,p\_\{m\}\$, the minimum entropy coupling is the coupling \$X\_\{1\},{\textbackslash}ldots,X\_\{m\}\$ (\$X\_\{i\}{\textbackslash}sim p\_\{i\}\$) with the smallest entropy \$H(X\_\{1\},{\textbackslash}ldots,X\_\{m\})\$. While this problem is known to be NP-hard, we present an efficient algorithm for computing a coupling with entropy within 2 bits from the optimal value. More precisely, we construct a coupling with entropy within 2 bits from the entropy of the greatest lower bound of \$p\_\{1\},{\textbackslash}ldots,p\_\{m\}\$ with respect to majorization. This construction is also valid when the collection of distributions is infinite, and when the supports of the distributions are infinite. Potential applications of our results include random number generation, entropic causal inference, and functional representation of random variables.},
	urldate = {2021-01-10},
	journal = {arXiv:2006.07955 [cs, math]},
	author = {Li, Cheuk Ting},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.07955},
	keywords = {Computer Science - Information Theory, Mathematics - Probability},
	annote = {Comment: 10 pages},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/6A9FUKVU/Li - 2020 - Efficient Approximate Minimum Entropy Coupling of .pdf:application/pdf;arXiv.org Snapshot:/home/cs/Zotero/storage/NGW83KR3/2006.html:text/html}
}

@article{cicalese_minimum--entropy_2019,
	title = {Minimum--{Entropy} {Couplings} and their {Applications}},
	url = {http://arxiv.org/abs/1901.07530},
	abstract = {Given two discrete random variables \$X\$ and \$Y,\$ with probability distributions \$\{{\textbackslash}bf p\}=(p\_1, {\textbackslash}ldots , p\_n)\$ and \$\{{\textbackslash}bf q\}=(q\_1, {\textbackslash}ldots , q\_m)\$, respectively, denote by \$\{{\textbackslash}cal C\}(\{{\textbackslash}bf p\}, \{{\textbackslash}bf q\})\$ the set of all couplings of \$\{{\textbackslash}bf p\}\$ and \$\{{\textbackslash}bf q\}\$, that is, the set of all bivariate probability distributions that have \$\{{\textbackslash}bf p\}\$ and \$\{{\textbackslash}bf q\}\$ as marginals. In this paper, we study the problem of finding a joint probability distribution in \$\{{\textbackslash}cal C\}(\{{\textbackslash}bf p\}, \{{\textbackslash}bf q\})\$ of {\textbackslash}emph\{minimum entropy\} (equivalently, a coupling that {\textbackslash}emph\{maximizes\} the mutual information between \$X\$ and \$Y\$), and we discuss several situations where the need for this kind of optimization naturally arises. Since the optimization problem is known to be NP-hard, we give an efficient algorithm to find a joint probability distribution in \$\{{\textbackslash}cal C\}(\{{\textbackslash}bf p\}, \{{\textbackslash}bf q\})\$ with entropy exceeding the minimum possible at most by \{1 bit\}, thus providing an approximation algorithm with an additive gap of at most 1 bit. Leveraging on this algorithm, we extend our result to the problem of finding a minimum--entropy joint distribution of arbitrary \$k{\textbackslash}geq 2\$ discrete random variables \$X\_1, {\textbackslash}ldots , X\_k\$, consistent with the known \$k\$ marginal distributions of the individual random variables \$X\_1, {\textbackslash}ldots , X\_k\$. In this case, our algorithm has an \{ additive gap of at most \${\textbackslash}log k\$ from optimum.\} We also discuss several related applications of our findings and \{extensions of our results to entropies different from the Shannon entropy.\}},
	urldate = {2021-01-10},
	journal = {arXiv:1901.07530 [cs, math]},
	author = {Cicalese, Ferdinando and Gargano, Luisa and Vaccaro, Ugo},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.07530},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Information Theory},
	annote = {Comment: This paper has been accepted for publication in IEEE Transactions on Information Theory. arXiv admin note: text overlap with arXiv:1701.05243},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/TD6EKJD3/Cicalese et al. - 2019 - Minimum--Entropy Couplings and their Applications.pdf:application/pdf;arXiv.org Snapshot:/home/cs/Zotero/storage/FVFKU339/1901.html:text/html}
}



@article{shi_soft_2019,
	title = {Soft {Policy} {Gradient} {Method} for {Maximum} {Entropy} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1909.03198},
	abstract = {Maximum entropy deep reinforcement learning (RL) methods have been demonstrated on a range of challenging continuous tasks. However, existing methods either suffer from severe instability when training on large off-policy data or cannot scale to tasks with very high state and action dimensionality such as 3D humanoid locomotion. Besides, the optimality of desired Boltzmann policy set for non-optimal soft value function is not persuasive enough. In this paper, we first derive soft policy gradient based on entropy regularized expected reward objective for RL with continuous actions. Then, we present an off-policy actor-critic, model-free maximum entropy deep RL algorithm called deep soft policy gradient (DSPG) by combining soft policy gradient with soft Bellman equation. To ensure stable learning while eliminating the need of two separate critics for soft value functions, we leverage double sampling approach to making the soft Bellman equation tractable. The experimental results demonstrate that our method outperforms in performance over off-policy prior methods.},
	urldate = {2020-12-11},
	journal = {arXiv:1909.03198 [cs, stat]},
	author = {Shi, Wenjie and Song, Shiji and Wu, Cheng},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.03198},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Machine Learning},
	annote = {Comment: to be published in Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/LT2A242L/Shi et al. - 2019 - Soft Policy Gradient Method for Maximum Entropy De.pdf:application/pdf;arXiv.org Snapshot:/home/cs/Zotero/storage/QPTNJTXY/1909.html:text/html}
}

@article{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	urldate = {2021-01-10},
	journal = {arXiv:1312.5602 [cs]},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013},
	note = {arXiv: 1312.5602},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: NIPS Deep Learning Workshop 2013},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/C9YF7D9C/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/home/cs/Zotero/storage/ET2T52I6/1312.html:text/html}
}

@article{feinstein_new_1954,
	title = {A new basic theorem of information theory},
	url = {https://dspace.mit.edu/handle/1721.1/4798},
	abstract = {"June 1, 1954." "This report is identical with a thesis submitted to the Department of Physics, M.I.T., ... for the degree of Doctor of Philosophy."},
	language = {eng},
	urldate = {2021-01-10},
	author = {Feinstein, Amiel},
	year = {1954},
	note = {Accepted: 2004-03-03T21:21:54Z
Publisher: Research Laboratory of Electronics, Massachusetts Institute of Technology},
	file = {Full Text PDF:/home/cs/Zotero/storage/FSRQH5BM/Feinstein - 1954 - A new basic theorem of information theory.pdf:application/pdf;Snapshot:/home/cs/Zotero/storage/LY9393JE/4798.html:text/html}
}

@article{machado_revisiting_2018,
	title = {Revisiting the arcade learning environment: evaluation protocols and open problems for general agents},
	volume = {61},
	issn = {1076-9757},
	shorttitle = {Revisiting the arcade learning environment},
	abstract = {The Arcade Learning Environment (ALE) is an evaluation platform that poses the challenge of building AI agents with general competency across dozens of Atari 2600 games. It supports a variety of different problem settings and it has been receiving increasing attention from the scientific community, leading to some high-pro\_le success stories such as the much publicized Deep Q-Networks (DQN). In this article we take a big picture look at how the ALE is being used by the research community. We show how diverse the evaluation methodologies in the ALE have become with time, and highlight some key concerns when evaluating agents in the ALE. We use this discussion to present some methodological best practices and provide new benchmark results using these best practices. To further the progress in the field, we introduce a new version of the ALE that supports multiple game modes and provides a form of stochasticity we call sticky actions. We conclude this big picture look by revisiting challenges posed when the ALE was introduced, summarizing the state-of-the-art in various problems and highlighting problems that remain open.},
	number = {1},
	journal = {Journal of Artificial Intelligence Research},
	author = {Machado, Marlos C. and Bellemare, Marc G. and Talvitie, Erik and Veness, Joel and Hausknecht, Matthew and Bowling, Michael},
	month = jan,
	year = {2018},
	pages = {523--562}
}

@article{brockman_openai_2016,
	title = {{OpenAI} {Gym}},
	url = {http://arxiv.org/abs/1606.01540},
	abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
	urldate = {2021-02-02},
	journal = {arXiv:1606.01540 [cs]},
	author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.01540},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/cs/Zotero/storage/4MRT85Z3/Brockman et al. - 2016 - OpenAI Gym.pdf:application/pdf;arXiv.org Snapshot:/home/cs/Zotero/storage/G53R72AB/1606.html:text/html}
}


@inproceedings{nair_rectified_2010,
	address = {Madison, WI, USA},
	series = {{ICML}'10},
	title = {Rectified linear units improve restricted boltzmann machines},
	isbn = {978-1-60558-907-7},
	abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
	urldate = {2021-02-02},
	booktitle = {Proceedings of the 27th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Nair, Vinod and Hinton, Geoffrey E.},
	month = jun,
	year = {2010},
	pages = {807--814}
}

@inproceedings{hasselt_deep_2016,
	address = {Phoenix, Arizona},
	series = {{AAAI}'16},
	title = {Deep reinforcement learning with double {Q}-{Learning}},
	abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
	urldate = {2021-02-02},
	booktitle = {Proceedings of the {Thirtieth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Hasselt, Hado van and Guez, Arthur and Silver, David},
	month = feb,
	year = {2016},
	pages = {2094--2100}
}


@misc{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
	language = {en},
	urldate = {2021-02-02},
	journal = {undefined},
	author = {Paszke, Adam and Gross, S. and Chintala, Soumith and Chanan, G. and Yang, E. and Devito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, L. and Lerer, A.},
	year = {2017}
}


@article{schulman_equivalence_2017,
	title = {Equivalence {Between} {Policy} {Gradients} and {Soft} {Q}-{Learning}},
	abstract = {Two of the leading approaches for model-free reinforcement learning are policy gradient methods and \$Q\$-learning methods. \$Q\$-learning methods can be effective and sample-efficient when they work, however, it is not well-understood why they work, since empirically, the \$Q\$-values they estimate are very inaccurate. A partial explanation may be that \$Q\$-learning methods are secretly implementing policy gradient updates: we show that there is a precise equivalence between \$Q\$-learning and policy gradient methods in the setting of entropy-regularized reinforcement learning, that "soft" (entropy-regularized) \$Q\$-learning is exactly equivalent to a policy gradient method. We also point out a connection between \$Q\$-learning methods and natural policy gradient methods. Experimentally, we explore the entropy-regularized versions of \$Q\$-learning and policy gradients, and we find them to perform as well as (or slightly better than) the standard variants on the Atari benchmark. We also show that the equivalence holds in practical settings by constructing a \$Q\$-learning method that closely matches the learning dynamics of A3C without using a target network or \${\textbackslash}epsilon\$-greedy exploration schedule.},
	author = {Schulman, John and Abbeel, Pieter and Chen, Xi},
	month = apr,
	year = {2017}
}


@incollection{tan_multi-agent_1997,
	address = {San Francisco, CA, USA},
	title = {Multi-agent reinforcement learning: independent vs. cooperative agents},
	isbn = {978-1-55860-495-7},
	shorttitle = {Multi-agent reinforcement learning},
	urldate = {2021-02-02},
	booktitle = {Readings in agents},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Tan, Ming},
	month = oct,
	year = {1997},
	pages = {487--494}
}

@inproceedings{sunehag_value-decomposition_2018,
	address = {Richland, SC},
	series = {{AAMAS} '18},
	title = {Value-{Decomposition} {Networks} {For} {Cooperative} {Multi}-{Agent} {Learning} {Based} {On} {Team} {Reward}},
	abstract = {We study the problem of cooperative multi-agent reinforcement learning with a single joint reward signal. This class of learning problems is difficult because of the often large combined action and observation spaces. In the fully centralized and decentralized approaches, we find the problem of spurious rewards and a phenomenon we call the "lazy agent'' problem, which arises due to partial observability. We address these problems by training individual agents with a novel value-decomposition network architecture, which learns to decompose the team value function into agent-wise value functions.},
	urldate = {2021-02-02},
	booktitle = {Proceedings of the 17th {International} {Conference} on {Autonomous} {Agents} and {MultiAgent} {Systems}},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	author = {Sunehag, Peter and Lever, Guy and Gruslys, Audrunas and Czarnecki, Wojciech Marian and Zambaldi, Vinicius and Jaderberg, Max and Lanctot, Marc and Sonnerat, Nicolas and Leibo, Joel Z. and Tuyls, Karl and Graepel, Thore},
	month = jul,
	year = {2018},
	keywords = {collaborative, dqn, multi-agent, neural networks, q-learning, reinforcement learning, value-decomposition},
	pages = {2085--2087},
	file = {Full Text PDF:/home/cs/Zotero/storage/MCQ978D6/Sunehag et al. - 2018 - Value-Decomposition Networks For Cooperative Multi.pdf:application/pdf}
}

@article{rashid_monotonic_2020,
	title = {Monotonic {Value} {Function} {Factorisation} for {Deep} {Multi}-{Agent} {Reinforcement} {Learning}},
	volume = {21},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v21/20-081.html},
	number = {178},
	urldate = {2021-02-03},
	journal = {Journal of Machine Learning Research},
	author = {Rashid, Tabish and Samvelyan, Mikayel and Witt, Christian Schroeder de and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
	year = {2020},
	pages = {1--51},
	file = {Full Text PDF:/home/cs/Zotero/storage/EDWQLRSA/Rashid et al. - 2020 - Monotonic Value Function Factorisation for Deep Mu.pdf:application/pdf;Snapshot:/home/cs/Zotero/storage/R7HGUAWK/20-081.html:text/html}
}


@inproceedings{kocaoglu_entropic_2017,
	title = {Entropic {Causality} and {Greedy} {Minimum} {Entropy} {Coupling}},
	doi = {10.1109/ISIT.2017.8006772},
	abstract = {We study the problem of identifying the causal relationship between two discrete random variables from observational data. We recently proposed a novel framework called entropie causality that works in a very general functional model but makes the assumption that the unobserved exogenous variable has small entropy in the true causal direction. This framework requires the solution of a minimum entropy coupling problem: Given marginal distributions of m discrete random variables, each on n states, find the joint distribution with minimum entropy, that respects the given marginals. This corresponds to minimizing a concave function of nm variables over a convex polytope defined by nm linear constraints, called a transportation polytope. Unfortunately, it was recently shown that this minimum entropy coupling problem is NP-hard, even for 2 variables with n states. Even representing points (joint distributions) over this space can require exponential complexity (in n, m) if done naively. In our recent work we introduced an efficient greedy algorithm to find an approximate solution for this problem. In this paper we analyze this algorithm and establish two results: that our algorithm always finds a local minimum and also is within an additive approximation error from the unknown global optimum.},
	booktitle = {2017 {IEEE} {International} {Symposium} on {Information} {Theory} ({ISIT})},
	author = {Kocaoglu, M. and Dimakis, A. G. and Vishwanath, S. and Hassibi, B.},
	month = jun,
	year = {2017},
	note = {ISSN: 2157-8117},
	keywords = {Additive noise, Approximation algorithms, causal relationship, cause-effect analysis, computational complexity, concave function, concave programming, convex polytope, convex programming, Couplings, discrete random variables, entropic causality, entropy, Entropy, exponential complexity, greedy algorithm, greedy algorithms, Greedy algorithms, greedy minimum entropy coupling, Inference algorithms, marginal distributions, NP-hard problem, observational data, Random variables, transportation polytope},
	pages = {1465--1469},
	file = {IEEE Xplore Full Text PDF:/home/cs/Zotero/storage/2H9D6B29/Kocaoglu et al. - 2017 - Entropic Causality and Greedy Minimum Entropy Coup.pdf:application/pdf}
}