\begin{thebibliography}{80}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abramson et~al.(2020)Abramson, Ahuja, Barr, Brussee, Carnevale,
  Cassin, Chhaparia, Clark, Damoc, Dudzik, et~al.]{abramson2020imitating}
Abramson, J., Ahuja, A., Barr, I., Brussee, A., Carnevale, F., Cassin, M.,
  Chhaparia, R., Clark, S., Damoc, B., Dudzik, A., et~al.
\newblock Imitating interactive intelligence.
\newblock \emph{arXiv preprint arXiv:2012.05672}, 2020.

\bibitem[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn,
  Gopalakrishnan, Hausman, Herzog, et~al.]{ahn2022can}
Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C.,
  Gopalakrishnan, K., Hausman, K., Herzog, A., et~al.
\newblock Do as {I} can, not as {I} say: Grounding language in robotic
  affordances.
\newblock \emph{arXiv preprint arXiv:2204.01691}, 2022.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, Ring, Rutherford, Cabi, Han, Gong, Samangooei,
  Monteiro, Menick, Borgeaud, Brock, Nematzadeh, Sharifzadeh, Binkowski,
  Barreira, Vinyals, Zisserman, and Simonyan]{alayrac2022flamingo}
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc,
  K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi,
  S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud,
  S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R.,
  Vinyals, O., Zisserman, A., and Simonyan, K.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Anderson et~al.(2018)Anderson, Wu, Teney, Bruce, Johnson,
  S{\"u}nderhauf, Reid, Gould, and van~den Hengel]{anderson2018vision}
Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., S{\"u}nderhauf, N.,
  Reid, I., Gould, S., and van~den Hengel, A.
\newblock Vision-and-language navigation: Interpreting visually-grounded
  navigation instructions in real environments.
\newblock In \emph{CVPR}, pp.\  3674--3683, 2018.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Pieter~Abbeel, and
  Zaremba]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P.,
  McGrew, B., Tobin, J., Pieter~Abbeel, O., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Artzi \& Zettlemoyer(2013)Artzi and Zettlemoyer]{artzi2013weakly}
Artzi, Y. and Zettlemoyer, L.
\newblock Weakly supervised learning of semantic parsers for mapping
  instructions to actions.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  1:\penalty0 49--62, 2013.

\bibitem[Bahdanau et~al.(2018)Bahdanau, Hill, Leike, Hughes, Hosseini, Kohli,
  and Grefenstette]{bahdanau2018learning}
Bahdanau, D., Hill, F., Leike, J., Hughes, E., Hosseini, A., Kohli, P., and
  Grefenstette, E.
\newblock Learning to understand goal specifications by modelling reward.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Barsalou(1983)]{barsalou1983ad}
Barsalou, L.~W.
\newblock Ad hoc categories.
\newblock \emph{Memory \& cognition}, 11:\penalty0 211--227, 1983.

\bibitem[Bigazzi et~al.(2021)Bigazzi, Landi, Cornia, Cascianelli, Baraldi, and
  Cucchiara]{bigazzi2021exploreexplain}
Bigazzi, R., Landi, F., Cornia, M., Cascianelli, S., Baraldi, L., and
  Cucchiara, R.
\newblock Explore and explain: Self-supervised navigation and recounting.
\newblock In \emph{25th International Conference on Pattern Recognition
  (ICPR)}, pp.\  1152--1159, 2021.

\bibitem[Blukis et~al.(2019)Blukis, Terme, Niklasson, Knepper, and
  Artzi]{Blukis2019LearningTM}
Blukis, V., Terme, Y., Niklasson, E., Knepper, R.~A., and Artzi, Y.
\newblock Learning to map natural language instructions to physical quadcopter
  control using simulated flight.
\newblock In \emph{CoRL}, 2019.

\bibitem[Blukis et~al.(2020)Blukis, Knepper, and Artzi]{Blukis2020FewshotOG}
Blukis, V., Knepper, R.~A., and Artzi, Y.
\newblock Few-shot object grounding and mapping for natural language robot
  instruction following.
\newblock \emph{arXiv preprint arXiv:2011.07384}, abs/2011.07384, 2020.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill,
  et~al.]{bommasani2021opportunities}
Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S.,
  Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Branavan et~al.(2012)Branavan, Silver, and
  Barzilay]{branavan2012learning}
Branavan, S., Silver, D., and Barzilay, R.
\newblock Learning to win by reading manuals in a {M}onte-{C}arlo framework.
\newblock \emph{Journal of Artificial Intelligence Research}, 43:\penalty0
  661--704, 2012.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{Brown2020LanguageMA}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T.~J., Child, R., Ramesh, A.,
  Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin,
  M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A.,
  Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock \emph{ArXiv}, abs/2005.14165, 2020.

\bibitem[Bucker et~al.(2022)Bucker, Figueredo, Haddadin, Kapoor, Ma, Vemprala,
  and Bonatti]{Bucker2022LaTTeLT}
Bucker, A. F.~C., Figueredo, L. F.~C., Haddadin, S., Kapoor, A., Ma, S.,
  Vemprala, S., and Bonatti, R.
\newblock La{T}{T}e: Language trajectory transform{E}r.
\newblock \emph{ArXiv}, abs/2208.02918, 2022.

\bibitem[Chai et~al.(2018)Chai, Gao, She, Yang, Saba-Sadiya, and
  Xu]{chai2018language}
Chai, J.~Y., Gao, Q., She, L., Yang, S., Saba-Sadiya, S., and Xu, G.
\newblock Language to action: Towards interactive task learning with physical
  agents.
\newblock In \emph{IJCAI}, pp.\  2--9, 2018.

\bibitem[Chan et~al.(2019)Chan, Wu, Kiros, Fidler, and Ba]{chan2019actrce}
Chan, H., Wu, Y., Kiros, J., Fidler, S., and Ba, J.
\newblock Actrce: Augmenting experience via teacher's advice for multi-goal
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1902.04546}, 2019.

\bibitem[Chaplot et~al.(2018)Chaplot, Sathyendra, Pasumarthi, Rajagopal, and
  Salakhutdinov]{chaplot2018gated}
Chaplot, D.~S., Sathyendra, K.~M., Pasumarthi, R.~K., Rajagopal, D., and
  Salakhutdinov, R.
\newblock Gated-attention architectures for task-oriented language grounding.
\newblock In \emph{AAAI}, 2018.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Xia, Ichter, Rao, Gopalakrishnan,
  Ryoo, Stone, and Kappler]{chen2022open}
Chen, B., Xia, F., Ichter, B., Rao, K., Gopalakrishnan, K., Ryoo, M.~S., Stone,
  A., and Kappler, D.
\newblock Open-vocabulary queryable scene representations for real world
  planning.
\newblock \emph{arXiv preprint arXiv:2209.09874}, 2022{\natexlab{a}}.

\bibitem[Chen \& Mooney(2011)Chen and Mooney]{chen2011learning}
Chen, D. and Mooney, R.
\newblock Learning to interpret natural language navigation instructions from
  observations.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~25, pp.\  859--865, 2011.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Guhur, Tapaswi, Schmid, and
  Laptev]{chen2022learning}
Chen, S., Guhur, P.-L., Tapaswi, M., Schmid, C., and Laptev, I.
\newblock Learning from unlabeled 3d environments for vision-and-language
  navigation.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXXIX}, pp.\  638--655.
  Springer, 2022{\natexlab{b}}.

\bibitem[Chen et~al.(2020)Chen, Gupta, and Marino]{Chen2020AskYH}
Chen, V., Gupta, A.~K., and Marino, K.
\newblock Ask your humans: Using human instructions to improve generalization
  in reinforcement learning.
\newblock \emph{ICLR}, 2020.

\bibitem[Cideron et~al.(2020)Cideron, Seurin, Strub, and
  Pietquin]{cideron2020higher}
Cideron, G., Seurin, M., Strub, F., and Pietquin, O.
\newblock Higher: Improving instruction following with hindsight generation for
  experience replay.
\newblock In \emph{2020 IEEE Symposium Series on Computational Intelligence
  (SSCI)}, pp.\  225--232. IEEE, 2020.

\bibitem[Co-Reyes et~al.(2018)Co-Reyes, Gupta, Sanjeev, Altieri, Andreas,
  DeNero, Abbeel, and Levine]{co2018guiding}
Co-Reyes, J.~D., Gupta, A., Sanjeev, S., Altieri, N., Andreas, J., DeNero, J.,
  Abbeel, P., and Levine, S.
\newblock Guiding policies with language via meta-learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Dasgupta et~al.(2022)Dasgupta, Kaeser-Chen, Marino, Ahuja, Babayan,
  Hill, and Fergus]{dasgupta2022collaborating}
Dasgupta, I., Kaeser-Chen, C., Marino, K., Ahuja, A., Babayan, S., Hill, F.,
  and Fergus, R.
\newblock Collaborating with language models for embodied reasoning.
\newblock In \emph{Second Workshop on Language and Reinforcement Learning},
  2022.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{bert18}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.~N.
\newblock B{E}{R}{T}: Pre-training of deep bidirectional transformers for
  language understanding.
\newblock 2018.
\newblock URL \url{https://arxiv.org/abs/1810.04805}.

\bibitem[Fan et~al.(2022)Fan, Wang, Jiang, Mandlekar, Yang, Zhu, Tang, Huang,
  Zhu, and Anandkumar]{Fan2022MineDojoBO}
Fan, L.~J., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A.,
  Huang, D.-A., Zhu, Y., and Anandkumar, A.
\newblock Mine{D}ojo: Building open-ended embodied agents with internet-scale
  knowledge.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Fried et~al.(2018)Fried, Hu, Cirik, Rohrbach, Andreas, Morency,
  Berg-Kirkpatrick, Saenko, Klein, and Darrell]{fried2018speaker}
Fried, D., Hu, R., Cirik, V., Rohrbach, A., Andreas, J., Morency, L.-P.,
  Berg-Kirkpatrick, T., Saenko, K., Klein, D., and Darrell, T.
\newblock Speaker-follower models for vision-and-language navigation.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Guhur et~al.(2021)Guhur, Tapaswi, Chen, Laptev, and
  Schmid]{guhur2021airbert}
Guhur, P.-L., Tapaswi, M., Chen, S., Laptev, I., and Schmid, C.
\newblock Airbert: In-domain pretraining for vision-and-language navigation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  1634--1643, 2021.

\bibitem[Hao et~al.(2020)Hao, Li, Li, Carin, and Gao]{hao2020towards}
Hao, W., Li, C., Li, X., Carin, L., and Gao, J.
\newblock Towards learning a generic agent for vision-and-language navigation
  via pre-training.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  13137--13146, 2020.

\bibitem[Harnad(1990)]{harnad1990symbol}
Harnad, S.
\newblock The symbol grounding problem.
\newblock \emph{Physica D: Nonlinear Phenomena}, 42\penalty0 (1-3):\penalty0
  335--346, 1990.

\bibitem[Hill et~al.(2021)Hill, Tieleman, von Glehn, Wong, Merzic, and
  Clark]{hill2021grounded}
Hill, F., Tieleman, O., von Glehn, T., Wong, N., Merzic, H., and Clark, S.
\newblock Grounded language learning fast and slow.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Hsu et~al.(2022)Hsu, Lum, Gao, Gu, Wu, and Finn]{hsu2022what}
Hsu, K., Lum, T. G.~W., Gao, R., Gu, S.~S., Wu, J., and Finn, C.
\newblock What makes certain pre-trained visual representations better for
  robotic learning?
\newblock In \emph{NeurIPS 2022 Foundation Models for Decision Making
  Workshop}, 2022.

\bibitem[Huang et~al.(2022{\natexlab{a}})Huang, Abbeel, Pathak, and
  Mordatch]{Huang2022LanguageMA}
Huang, W., Abbeel, P., Pathak, D., and Mordatch, I.
\newblock Language models as zero-shot planners: Extracting actionable
  knowledge for embodied agents.
\newblock \emph{ArXiv}, abs/2201.07207, 2022{\natexlab{a}}.

\bibitem[Huang et~al.(2022{\natexlab{b}})Huang, Xia, Xiao, Chan, Liang,
  Florence, Zeng, Tompson, Mordatch, Chebotar, Sermanet, Jackson, Brown, Luu,
  Levine, Hausman, and Ichter]{huang2022inner}
Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A.,
  Tompson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Jackson, T., Brown,
  N., Luu, L., Levine, S., Hausman, K., and Ichter, B.
\newblock Inner monologue: Embodied reasoning through planning with language
  models.
\newblock In \emph{6th Annual Conference on Robot Learning},
  2022{\natexlab{b}}.

\bibitem[Interactive Agents~Team(2021)]{interactiveagents2021creating}
Interactive Agents~Team, D.
\newblock Creating multimodal interactive agents with imitation and
  self-supervised learning.
\newblock \emph{arXiv preprint arXiv:2112.03763}, 2021.

\bibitem[Kirillov et~al.(2023)Kirillov, Mintun, Ravi, Mao, Rolland, Gustafson,
  Xiao, Whitehead, Berg, Lo, et~al.]{kirillov2023segment}
Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao,
  T., Whitehead, S., Berg, A.~C., Lo, W.-Y., et~al.
\newblock Segment anything.
\newblock \emph{arXiv preprint arXiv:2304.02643}, 2023.

\bibitem[Kollar et~al.(2010)Kollar, Tellex, Roy, and Roy]{kollar2010toward}
Kollar, T., Tellex, S., Roy, D., and Roy, N.
\newblock Toward understanding natural language directions.
\newblock In \emph{2010 5th ACM/IEEE International Conference on Human-Robot
  Interaction (HRI)}, pp.\  259--266. IEEE, 2010.

\bibitem[Kulick et~al.(2013)Kulick, Toussaint, Lang, and
  Lopes]{kulick2013active}
Kulick, J., Toussaint, M., Lang, T., and Lopes, M.
\newblock Active learning for teaching a robot grounded relational symbols.
\newblock In \emph{IJCAI}, pp.\  1451--1457. Citeseer, 2013.

\bibitem[Kuo et~al.(2022)Kuo, Cui, Gu, Piergiovanni, and Angelova]{kuo2022f}
Kuo, W., Cui, Y., Gu, X., Piergiovanni, A., and Angelova, A.
\newblock F-vlm: Open-vocabulary object detection upon frozen vision and
  language models.
\newblock \emph{arXiv preprint arXiv:2209.15639}, 2022.

\bibitem[Li et~al.(2021)Li, Xia, Mart{\'\i}n-Mart{\'\i}n, Lingelbach,
  Srivastava, Shen, Vainio, Gokmen, Dharan, Jain, et~al.]{li2021igibson}
Li, C., Xia, F., Mart{\'\i}n-Mart{\'\i}n, R., Lingelbach, M., Srivastava, S.,
  Shen, B., Vainio, K., Gokmen, C., Dharan, G., Jain, T., et~al.
\newblock igibson 2.0: Object-centric simulation for robot learning of everyday
  household tasks.
\newblock \emph{arXiv preprint arXiv:2108.03272}, 2021.

\bibitem[Li et~al.(2019)Li, Li, Xia, Bisk, Celikyilmaz, Gao, Smith, and
  Choi]{li2019robust}
Li, X., Li, C., Xia, Q., Bisk, Y., Celikyilmaz, A., Gao, J., Smith, N.~A., and
  Choi, Y.
\newblock Robust navigation with language pretraining and stochastic sampling.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pp.\  1494--1499, 2019.

\bibitem[Lin et~al.(2022)Lin, Fried, Klein, and Dragan]{lin2022inferring}
Lin, J., Fried, D., Klein, D., and Dragan, A.
\newblock Inferring rewards from language in context.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  8546--8560,
  Dublin, Ireland, May 2022. Association for Computational Linguistics.

\bibitem[Liu et~al.(2023)Liu, Fan, Johns, Yu, Xiao, and
  Anandkumar]{liu2023prismer}
Liu, S., Fan, L., Johns, E., Yu, Z., Xiao, C., and Anandkumar, A.
\newblock Prismer: A vision-language model with an ensemble of experts.
\newblock \emph{arXiv preprint arXiv:2303.02506}, 2023.

\bibitem[Lynch \& Sermanet(2020)Lynch and Sermanet]{lynch2020grounding}
Lynch, C. and Sermanet, P.
\newblock Grounding language in play.
\newblock \emph{arXiv preprint arXiv:2005.07648}, 2020.

\bibitem[Majumdar et~al.(2022)Majumdar, Aggarwal, Devnani, Hoffman, and
  Batra]{majumdarzson2022}
Majumdar, A., Aggarwal, G., Devnani, B.~S., Hoffman, J., and Batra, D.
\newblock Z{S}{O}{N}: Zero-shot object-goal navigation using multimodal goal
  embeddings.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Mei et~al.(2016)Mei, Bansal, and Walter]{mei2016listen}
Mei, H., Bansal, M., and Walter, M.~R.
\newblock Listen, attend, and walk: Neural mapping of navigational instructions
  to action sequences.
\newblock In \emph{AAAI}, 2016.

\bibitem[Mikolov et~al.(2013)Mikolov, Yih, and Zweig]{mikolov2013linguistic}
Mikolov, T., Yih, W.-t., and Zweig, G.
\newblock Linguistic regularities in continuous space word representations.
\newblock In \emph{Proceedings of the 2013 conference of the north american
  chapter of the association for computational linguistics: Human language
  technologies}, pp.\  746--751, 2013.

\bibitem[Minderer et~al.(2022)Minderer, Gritsenko, Stone, Neumann, Weissenborn,
  Dosovitskiy, Mahendran, Arnab, Dehghani, Shen, Wang, Zhai, Kipf, and
  Houlsby]{minderer2022owlvit}
Minderer, M., Gritsenko, A., Stone, A., Neumann, M., Weissenborn, D.,
  Dosovitskiy, A., Mahendran, A., Arnab, A., Dehghani, M., Shen, Z., Wang, X.,
  Zhai, X., Kipf, T., and Houlsby, N.
\newblock Simple open-vocabulary object detection.
\newblock In \emph{Computer Vision – ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23–27, 2022, Proceedings, Part X}, pp.\  728–755.
  Springer-Verlag, 2022.

\bibitem[Misra et~al.(2017)Misra, Langford, and Artzi]{misra2017mapping}
Misra, D., Langford, J., and Artzi, Y.
\newblock Mapping instructions and visual observations to actions with
  reinforcement learning.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1004--1015, 2017.

\bibitem[Mohan \& Laird(2014)Mohan and Laird]{mohan2014learning}
Mohan, S. and Laird, J.
\newblock Learning goal-oriented hierarchical tasks from situated interactive
  instruction.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~28, 2014.

\bibitem[Mooney(2008)]{mooney2008learning}
Mooney, R.~J.
\newblock Learning to connect language and perception.
\newblock In \emph{AAAI}, pp.\  1598--1601. Chicago, 2008.

\bibitem[Nair et~al.(2022)Nair, Rajeswaran, Kumar, Finn, and
  Gupta]{nair2022r3m}
Nair, S., Rajeswaran, A., Kumar, V., Finn, C., and Gupta, A.
\newblock R3{M}: A universal visual representation for robot manipulation.
\newblock \emph{arXiv preprint arXiv:2203.12601}, 2022.

\bibitem[Narasimhan et~al.(2018)Narasimhan, Barzilay, and
  Jaakkola]{narasimhan2018grounding}
Narasimhan, K., Barzilay, R., and Jaakkola, T.
\newblock Grounding language for transfer in deep reinforcement learning.
\newblock \emph{J. Artif. Int. Res.}, 63\penalty0 (1):\penalty0 849–874, sep
  2018.
\newblock ISSN 1076-9757.

\bibitem[Nguyen et~al.(2021)Nguyen, Misra, Schapire, Dud{\'\i}k, and
  Shafto]{nguyen2021interactive}
Nguyen, K.~X., Misra, D., Schapire, R., Dud{\'\i}k, M., and Shafto, P.
\newblock Interactive learning from activity description.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8096--8108. PMLR, 2021.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Pinto \& Gupta(2016)Pinto and Gupta]{pinto2016grasp}
Pinto, L. and Gupta, A.
\newblock Supersizing self-supervision: Learning to grasp from 50k tries and
  700 robot hours.
\newblock In \emph{2016 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  3406--3413, 2016.
\newblock \doi{10.1109/ICRA.2016.7487517}.

\bibitem[Quine(1960)]{Quine1960}
Quine, W. V.~O.
\newblock \emph{Word \& Object}.
\newblock MIT Press, 1960.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and
  Sutskever]{Radford2021LearningTV}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and
  Chen]{Ramesh2022HierarchicalTI}
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{ArXiv}, abs/2204.06125, 2022.

\bibitem[Savva et~al.(2019)Savva, Kadian, Maksymets, Zhao, Wijmans, Jain,
  Straub, Liu, Koltun, Malik, et~al.]{savva2019habitat}
Savva, M., Kadian, A., Maksymets, O., Zhao, Y., Wijmans, E., Jain, B., Straub,
  J., Liu, J., Koltun, V., Malik, J., et~al.
\newblock Habitat: A platform for embodied ai research.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pp.\  9339--9347, 2019.

\bibitem[Shah et~al.(2022)Shah, Osi{\'n}ski, Levine, et~al.]{shahlm2022}
Shah, D., Osi{\'n}ski, B., Levine, S., et~al.
\newblock L{M}-{N}av: Robotic navigation with large pre-trained models of
  language, vision, and action.
\newblock In \emph{6th Annual Conference on Robot Learning}, 2022.

\bibitem[Sharma et~al.(2022)Sharma, Torralba, and Andreas]{sharma2022skill}
Sharma, P., Torralba, A., and Andreas, J.
\newblock Skill induction and planning with latent language.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1713--1726,
  2022.

\bibitem[She et~al.(2014)She, Cheng, Chai, Jia, Yang, and Xi]{she2014teaching}
She, L., Cheng, Y., Chai, J.~Y., Jia, Y., Yang, S., and Xi, N.
\newblock Teaching robots new actions through natural language instructions.
\newblock In \emph{The 23rd IEEE International Symposium on Robot and Human
  Interactive Communication}, pp.\  868--873. IEEE, 2014.

\bibitem[Shridhar et~al.(2020)Shridhar, Thomason, Gordon, Bisk, Han, Mottaghi,
  Zettlemoyer, and Fox]{shridhar2020alfred}
Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R.,
  Zettlemoyer, L., and Fox, D.
\newblock A{L}{F}{R}{E}{D}: A benchmark for interpreting grounded instructions
  for everyday tasks.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  10740--10749, 2020.

\bibitem[Shridhar et~al.(2021)Shridhar, Manuelli, and
  Fox]{Shridhar2021CLIPortWA}
Shridhar, M., Manuelli, L., and Fox, D.
\newblock Cliport: What and where pathways for robotic manipulation.
\newblock \emph{Co{RL}}, 2021.

\bibitem[Singh et~al.(2022)Singh, Blukis, Mousavian, Goyal, Xu, Tremblay, Fox,
  Thomason, and Garg]{singh2022progprompt}
Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D.,
  Thomason, J., and Garg, A.
\newblock Progprompt: Generating situated robot task plans using large language
  models.
\newblock \emph{arXiv preprint arXiv:2209.11302}, 2022.

\bibitem[Sumers et~al.(2021)Sumers, Ho, Hawkins, Narasimhan, and
  Griffiths]{sumers2021learning}
Sumers, T.~R., Ho, M.~K., Hawkins, R.~D., Narasimhan, K., and Griffiths, T.~L.
\newblock Learning rewards from linguistic feedback.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  6002--6010, 2021.

\bibitem[Tam et~al.(2022)Tam, Rabinowitz, Lampinen, Roy, Chan, Strouse, Wang,
  Banino, and Hill]{tam2022semantic}
Tam, A., Rabinowitz, N.~C., Lampinen, A.~K., Roy, N.~A., Chan, S.~C., Strouse,
  D., Wang, J.~X., Banino, A., and Hill, F.
\newblock Semantic exploration from language abstractions and pretrained
  representations.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Tellex et~al.(2011)Tellex, Kollar, Dickerson, Walter, Banerjee,
  Teller, and Roy]{tellex2011understanding}
Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S.,
  and Roy, N.
\newblock Understanding natural language commands for robotic navigation and
  mobile manipulation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~25, pp.\  1507--1514, 2011.

\bibitem[Tellex et~al.(2020)Tellex, Gopalan, Kress-Gazit, and
  Matuszek]{tellex2020robots}
Tellex, S., Gopalan, N., Kress-Gazit, H., and Matuszek, C.
\newblock Robots that use language.
\newblock \emph{Annual Review of Control, Robotics, and Autonomous Systems},
  3\penalty0 (1), 2020.

\bibitem[Thomason et~al.(2017)Thomason, Padmakumar, Sinapov, Hart, Stone, and
  Mooney]{thomason2017opportunistic}
Thomason, J., Padmakumar, A., Sinapov, J., Hart, J., Stone, P., and Mooney, R.
\newblock Opportunistic active learning for grounding natural language
  descriptions.
\newblock In \emph{Proceedings of the 1st Annual Conference on Robot Learning
  (CoRL-17)}, 2017.

\bibitem[Winograd(1972)]{winograd1972understanding}
Winograd, T.
\newblock Understanding natural language.
\newblock \emph{Cognitive psychology}, 3\penalty0 (1):\penalty0 1--191, 1972.

\bibitem[Xiao et~al.(2022)Xiao, Chan, Sermanet, Wahid, Brohan, Hausman, Levine,
  and Tompson]{xiao2022robotic}
Xiao, T., Chan, H., Sermanet, P., Wahid, A., Brohan, A., Hausman, K., Levine,
  S., and Tompson, J.
\newblock Robotic skill acquisition via instruction augmentation with
  vision-language models.
\newblock \emph{arXiv preprint arXiv:2211.11736}, 2022.

\bibitem[Yan et~al.(2022)Yan, Carnevale, Georgiev, Santoro, Guy, Muldal, Hung,
  Abramson, Lillicrap, and Wayne]{yan2022intraagent}
Yan, C., Carnevale, F., Georgiev, P., Santoro, A., Guy, A., Muldal, A., Hung,
  C.-C., Abramson, J.~S., Lillicrap, T.~P., and Wayne, G.
\newblock Intra-agent speech permits zero-shot task acquisition.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Yu et~al.(2018)Yu, Zhang, and Xu]{yu2018interactive}
Yu, H., Zhang, H., and Xu, W.
\newblock Interactive grounded language acquisition and generalization in a
  2{D} world.
\newblock In \emph{ICLR}, 2018.

\bibitem[Zeng et~al.(2022)Zeng, Wong, Welker, Choromanski, Tombari, Purohit,
  Ryoo, Sindhwani, Lee, Vanhoucke, et~al.]{zeng2022socratic}
Zeng, A., Wong, A., Welker, S., Choromanski, K., Tombari, F., Purohit, A.,
  Ryoo, M., Sindhwani, V., Lee, J., Vanhoucke, V., et~al.
\newblock Socratic models: Composing zero-shot multimodal reasoning with
  language.
\newblock \emph{arXiv preprint arXiv:2204.00598}, 2022.

\bibitem[Zhong et~al.(2020)Zhong, Rocktäschel, and
  Grefenstette]{zhong2019rtfm}
Zhong, V., Rocktäschel, T., and Grefenstette, E.
\newblock {RTFM}: Generalising to new environment dynamics via reading.
\newblock In \emph{ICLR}, 2020.

\bibitem[Zhong et~al.(2021)Zhong, Hanjie, Wang, Narasimhan, and
  Zettlemoyer]{zhong2021silg}
Zhong, V., Hanjie, A.~W., Wang, S., Narasimhan, K., and Zettlemoyer, L.
\newblock S{I}{L}{G}: The multi-domain symbolic interactive language grounding
  benchmark.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 21505--21519, 2021.

\bibitem[Zhong et~al.(2022)Zhong, Mu, Zettlemoyer, Grefenstette, and
  Rockt{\"a}schel]{zhong2022improving}
Zhong, V., Mu, J., Zettlemoyer, L., Grefenstette, E., and Rockt{\"a}schel, T.
\newblock Improving policy learning via language dynamics distillation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\end{thebibliography}
