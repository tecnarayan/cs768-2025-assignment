\begin{thebibliography}{10}

\bibitem{zhang2022self}
Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik.
\newblock Self-supervised contrastive pre-training for time series via
  time-frequency consistency.
\newblock {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{malhotra2017timenet}
Pankaj Malhotra, Vishnu TV, Lovekesh Vig, Puneet Agarwal, and Gautam Shroff.
\newblock {TimeNet:} pre-trained deep recurrent neural network for time series
  classification.
\newblock {\em arXiv:1706.08838}, 2017.

\bibitem{lundberg2017unified}
Scott~M Lundberg and Su-In Lee.
\newblock A unified approach to interpreting model predictions.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{kusters2020conceptual}
Ferdinand Küsters, Peter Schichtel, Sheraz Ahmed, and Andreas Dengel.
\newblock Conceptual explanations of neural network prediction for time series.
\newblock In {\em 2020 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--6, 2020.

\bibitem{kindermans2019reliability}
Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof~T
  Sch{\"u}tt, Sven D{\"a}hne, Dumitru Erhan, and Been Kim.
\newblock The (un) reliability of saliency methods.
\newblock {\em Explainable AI: Interpreting, explaining and visualizing deep
  learning}, pages 267--280, 2019.

\bibitem{agarwal2023evaluating}
Chirag Agarwal, Owen Queen, Himabindu Lakkaraju, and Marinka Zitnik.
\newblock Evaluating explainability for graph neural networks.
\newblock {\em Scientific Data}, 10(1):144, 2023.

\bibitem{agarwal2022probing}
Chirag Agarwal, Marinka Zitnik, and Himabindu Lakkaraju.
\newblock Probing gnn explainers: A rigorous theoretical and empirical analysis
  of gnn explanation methods.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 8969--8996. PMLR, 2022.

\bibitem{vig2019multiscale}
Jesse Vig.
\newblock A multiscale visualization of attention in the transformer model.
\newblock {\em arXiv:1906.05714}, 2019.

\bibitem{abnar2020quantifying}
Samira Abnar and Willem Zuidema.
\newblock Quantifying attention flow in transformers.
\newblock {\em ACL}, 2020.

\bibitem{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
  Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et~al.
\newblock In-context learning and induction heads.
\newblock {\em arXiv:2209.11895}, 2022.

\bibitem{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in {GPT}.
\newblock {\em Advances in Neural Information Processing Systems},
  35:17359--17372, 2022.

\bibitem{vig2020investigating}
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo,
  Yaron Singer, and Stuart Shieber.
\newblock Investigating gender bias in language models using causal mediation
  analysis.
\newblock {\em Advances in Neural Information Processing Systems},
  33:12388--12401, 2020.

\bibitem{rogers2021primer}
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.
\newblock A primer in {BERTology}: What we know about how {BERT} works.
\newblock {\em Transactions of the Association for Computational Linguistics},
  8:842--866, 2021.

\bibitem{dai2021knowledge}
Damai Dai, Li~Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei.
\newblock Knowledge neurons in pretrained transformers.
\newblock {\em arXiv:2104.08696}, 2021.

\bibitem{belinkov2022probing}
Yonatan Belinkov.
\newblock Probing classifiers: Promises, shortcomings, and advances.
\newblock {\em Computational Linguistics}, 48(1):207--219, 2022.

\bibitem{burns2022discovering}
Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt.
\newblock Discovering latent knowledge in language models without supervision.
\newblock {\em arXiv:2212.03827}, 2022.

\bibitem{ismail2021improving}
Aya~Abdelsalam Ismail, Hector~Corrada Bravo, and Soheil Feizi.
\newblock Improving deep learning interpretability by saliency guided training.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{koh2020concept}
Pang~Wei Koh, Thao Nguyen, Yew~Siang Tang, Stephen Mussmann, Emma Pierson, Been
  Kim, and Percy Liang.
\newblock Concept bottleneck models.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 5338--5348. PMLR, 13--18 Jul
  2020.

\bibitem{agarwal2021neural}
Rishabh Agarwal, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich,
  Rich Caruana, and Geoffrey~E Hinton.
\newblock Neural additive models: Interpretable machine learning with neural
  nets.
\newblock {\em Advances in Neural Information Processing Systems},
  34:4699--4711, 2021.

\bibitem{lim2021temporal}
Bryan Lim, Sercan~{\"O} Ar{\i}k, Nicolas Loeff, and Tomas Pfister.
\newblock Temporal fusion transformers for interpretable multi-horizon time
  series forecasting.
\newblock {\em International Journal of Forecasting}, 37(4):1748--1764, 2021.

\bibitem{Oreshkin2019NBEATSNB}
Boris~N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio.
\newblock N-beats: Neural basis expansion analysis for interpretable time
  series forecasting.
\newblock {\em ArXiv}, abs/1905.10437, 2019.

\bibitem{henderson2021improving}
Ryan Henderson, Djork-Arn{\'e} Clevert, and Floriane Montanari.
\newblock Improving molecular graph neural network explainability with
  orthonormalization and induced sparsity.
\newblock In {\em International Conference on Machine Learning}, pages
  4203--4213. PMLR, 2021.

\bibitem{yuksekgonul2023post}
Mert Yuksekgonul, Maggie Wang, and James Zou.
\newblock Post-hoc concept bottleneck models.
\newblock In {\em International Conference on Learning Representations}, 2023.

\bibitem{doshi2017towards}
Finale Doshi-Velez and Been Kim.
\newblock Towards a rigorous science of interpretable machine learning.
\newblock {\em arXiv preprint arXiv:1702.08608}, 2017.

\bibitem{danilevsky2020survey}
Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and
  Prithviraj Sen.
\newblock A survey of the state of explainable ai for natural language
  processing.
\newblock {\em arXiv preprint arXiv:2010.00711}, 2020.

\bibitem{madsen2022post}
Andreas Madsen, Siva Reddy, and Sarath Chandar.
\newblock Post-hoc interpretability for neural nlp: A survey.
\newblock {\em ACM Computing Surveys}, 55(8):1--42, 2022.

\bibitem{bodria2021benchmarking}
Francesco Bodria, Fosca Giannotti, Riccardo Guidotti, Francesca Naretto, Dino
  Pedreschi, and Salvatore Rinzivillo.
\newblock Benchmarking and survey of explanation methods for black box models.
\newblock {\em arXiv preprint arXiv:2102.13076}, 2021.

\bibitem{linardatos2020explainable}
Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis.
\newblock Explainable ai: A review of machine learning interpretability
  methods.
\newblock {\em Entropy}, 23(1):18, 2020.

\bibitem{petsiuk2018rise}
Vitali Petsiuk, Abir Das, and Kate Saenko.
\newblock Rise: Randomized input sampling for explanation of black-box models.
\newblock In {\em British Machine Vision Conference}, 2018.

\bibitem{samek2021explaining}
Wojciech Samek, Gr{\'e}goire Montavon, Sebastian Lapuschkin, Christopher~J
  Anders, and Klaus-Robert M{\"u}ller.
\newblock Explaining deep neural networks and beyond: A review of methods and
  applications.
\newblock {\em Proceedings of the IEEE}, 109(3):247--278, 2021.

\bibitem{sundararajan2017axiomatic}
Mukund Sundararajan, Ankur Taly, and Qiqi Yan.
\newblock Axiomatic attribution for deep networks.
\newblock In {\em International conference on machine learning}, pages
  3319--3328. PMLR, 2017.

\bibitem{ghassemi2021false}
Marzyeh Ghassemi, Luke Oakden-Rayner, and Andrew~L Beam.
\newblock The false hope of current approaches to explainable artificial
  intelligence in health care.
\newblock {\em The Lancet Digital Health}, 3(11):e745--e750, 2021.

\bibitem{ying2019gnnexplainer}
Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec.
\newblock Gnnexplainer: Generating explanations for graph neural networks.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{schlegel2019towards}
Udo Schlegel, Hiba Arnout, Mennatallah El-Assady, Daniela Oelke, and Daniel~A
  Keim.
\newblock Towards a rigorous evaluation of xai methods on time series.
\newblock In {\em 2019 IEEE/CVF International Conference on Computer Vision
  Workshop (ICCVW)}, pages 4197--4201. IEEE, 2019.

\bibitem{ismail2020benchmarking}
Aya~Abdelsalam Ismail, Mohamed Gunady, Hector Corrada~Bravo, and Soheil Feizi.
\newblock Benchmarking deep learning interpretability in time series
  predictions.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 6441--6452. Curran Associates, Inc., 2020.

\bibitem{lakkaraju2019faithful}
Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Jure Leskovec.
\newblock Faithful and customizable explanations of black box models.
\newblock In {\em Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics,
  and Society}, pages 131--138, 2019.

\bibitem{ribeiro2018anchors}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock Anchors: High-precision model-agnostic explanations.
\newblock {\em Proceedings of the AAAI conference on artificial intelligence},
  32(1), 2018.

\bibitem{zeiler2014visualizing}
Matthew~D Zeiler and Rob Fergus.
\newblock Visualizing and understanding convolutional networks.
\newblock In {\em Computer Vision--ECCV 2014: 13th European Conference, Zurich,
  Switzerland, September 6-12, 2014, Proceedings, Part I 13}, pages 818--833.
  Springer, 2014.

\bibitem{selvaraju2017grad}
Ramprasaath~R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam,
  Devi Parikh, and Dhruv Batra.
\newblock Grad-cam: Visual explanations from deep networks via gradient-based
  localization.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 618--626, 2017.

\bibitem{adebayo2018sanity}
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt,
  and Been Kim.
\newblock Sanity checks for saliency maps.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{ribeiro2016lime}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock “why should i trust you?”: Explaining the predictions of any
  classifier.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, page 1135–1144, San Francisco
  California USA, Aug 2016. ACM.

\bibitem{lundberg2020local}
Scott~M Lundberg, Gabriel Erion, Hugh Chen, Alex DeGrave, Jordan~M Prutkin,
  Bala Nair, Ronit Katz, Jonathan Himmelfarb, Nisha Bansal, and Su-In Lee.
\newblock From local explanations to global understanding with explainable ai
  for trees.
\newblock {\em Nature machine intelligence}, 2(1):56--67, 2020.

\bibitem{zhang2021learning}
Haoran Zhang, Quaid Morris, Berk Ustun, and Marzyeh Ghassemi.
\newblock Learning optimal predictive checklists.
\newblock {\em Advances in Neural Information Processing Systems},
  34:1215--1229, 2021.

\bibitem{spinelli2022mate}
Indro Spinelli, Simone Scardapane, and Aurelio Uncini.
\newblock A meta-learning approach for training explainable graph neural
  networks.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems}, page
  1–9, 2022.

\bibitem{tsiligkaridis:cvpr:2022}
Theodoros Tsiligkaridis and Jay Roberts.
\newblock Understanding and increasing efficiency of frank-wolfe adversarial
  training.
\newblock In {\em CVPR}, 2022.

\bibitem{10.2312:mlvis.20211072}
Jay Roberts and Theodoros Tsiligkaridis.
\newblock {Controllably Sparse Perturbations of Robust Classifiers for
  Explaining Predictions and Probing Learned Concepts}.
\newblock In Daniel Archambault, Ian Nabney, and Jaakko Peltonen, editors, {\em
  Machine Learning Methods in Visualisation for Big Data}. The Eurographics
  Association, 2021.

\bibitem{roberts:covid:2020}
Jay Roberts and Theodoros Tsiligkaridis.
\newblock Ultrasound diagnosis of covid-19: Robustness and explainability.
\newblock In {\em Medical Imaging meets NeurIPS Workshop}, 2020.

\bibitem{robust_xai:spm:2022}
Ian~E. Nielsen, Dimah Dera, Ravi P.~Ramachandran Ghulam~Rasool, and
  Nidhal~Carla Bouay.
\newblock Robust explainability: A tutorial on gradient-based attribution
  methods for deep neural networks.
\newblock {\em IEEE Signal Processing Magazine}, 39(4), July 2022.

\bibitem{bastings2019interpretable}
Jasmijn Bastings, Wilker Aziz, and Ivan Titov.
\newblock Interpretable neural predictions with differentiable binary
  variables.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 2963--2977, Florence, Italy, July 2019.
  Association for Computational Linguistics.

\bibitem{miao2022interpretable}
Siqi Miao, Mia Liu, and Pan Li.
\newblock Interpretable and generalizable graph learning via stochastic
  attention mechanism.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, {\em Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of {\em Proceedings
  of Machine Learning Research}, pages 15524--15543. PMLR, 17--23 Jul 2022.

\bibitem{chen2019looks}
Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and
  Jonathan~K Su.
\newblock This looks like that: deep learning for interpretable image
  recognition.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{gautam2022protovae}
Srishti Gautam, Ahcene Boubekki, Stine Hansen, Suaiba Salahuddin, Robert
  Jenssen, Marina H{\"o}hne, and Michael Kampffmeyer.
\newblock Protovae: A trustworthy self-explainable prototypical variational
  model.
\newblock {\em Advances in Neural Information Processing Systems},
  35:17940--17952, 2022.

\bibitem{higgins2017scan}
Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher~P Burgess,
  Matko Bosnjak, Murray Shanahan, Matthew Botvinick, Demis Hassabis, and
  Alexander Lerchner.
\newblock Scan: Learning hierarchical compositional visual concepts.
\newblock {\em arXiv preprint arXiv:1707.03389}, 2017.

\bibitem{wu2022zeroc}
Tailin Wu, Megan Tjandrasuwita, Zhengxuan Wu, Xuelin Yang, Kevin Liu, Rok
  Sosic, and Jure Leskovec.
\newblock Zeroc: A neuro-symbolic model for zero-shot concept recognition and
  acquisition at inference time.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{mao2019neuro}
Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua~B Tenenbaum, and Jiajun Wu.
\newblock The neuro-symbolic concept learner: Interpreting scenes, words, and
  sentences from natural supervision.
\newblock {\em International Conference on Learning Representations}, 2019.

\bibitem{parvatharaju2021learning}
Prathyush~S Parvatharaju, Ramesh Doddaiah, Thomas Hartvigsen, and Elke~A
  Rundensteiner.
\newblock Learning saliency maps to explain deep time series classifiers.
\newblock In {\em Proceedings of the 30th ACM International Conference on
  Information \& Knowledge Management}, pages 1406--1415, 2021.

\bibitem{doddaiah2022class}
Ramesh Doddaiah, Prathyush Parvatharaju, Elke Rundensteiner, and Thomas
  Hartvigsen.
\newblock Class-specific explainability for deep time series classifiers.
\newblock In {\em Proceedings of the International Conference on Data Mining},
  2022.

\bibitem{crabbe2021explaining}
Jonathan Crabb{\'e} and Mihaela Van Der~Schaar.
\newblock Explaining time series predictions with dynamic masks.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 2166--2177. PMLR, 18--24 Jul 2021.

\bibitem{bento2021timeshap}
João Bento, Pedro Saleiro, André~F. Cruz, Mário A.~T. Figueiredo, and Pedro
  Bizarro.
\newblock Timeshap: Explaining recurrent models through sequence perturbations.
\newblock In {\em Proceedings of the 27th ACM SIGKDD Conference on Knowledge
  Discovery \& Data Mining}, page 2565–2573, Aug 2021.
\newblock arXiv:2012.00073 [cs].

\bibitem{Guidotti2020ExplainingAT}
Riccardo Guidotti and et~al.
\newblock Explaining any time series classifier.
\newblock {\em International Conference on Cognitive Machine Intelligence},
  2020.

\bibitem{mujkanovic2020timexplain}
F.and et~al Mujkanovic.
\newblock timexplain--a framework for explaining the predictions of time series
  classifiers.
\newblock {\em ArXiv}, 2020.

\bibitem{rojat2021explainable}
Thomas Rojat, Rapha{\"e}l Puget, David Filliat, Javier Del~Ser, Rodolphe Gelin,
  and Natalia D{\'\i}az-Rodr{\'\i}guez.
\newblock Explainable artificial intelligence (xai) on timeseries data: A
  survey.
\newblock {\em arXiv preprint arXiv:2104.00950}, 2021.

\bibitem{tonekaboni2020fit}
Sana Tonekaboni, Shalmali Joshi, Kieran Campbell, David~K Duvenaud, and Anna
  Goldenberg.
\newblock What went wrong and when? instance-wise feature importance for
  time-series black-box models.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 799--809. Curran Associates, Inc., 2020.

\bibitem{sivill2022limesegment}
Torty Sivill and Peter Flach.
\newblock Limesegment: Meaningful, realistic time series explanations.
\newblock In {\em Proceedings of The 25th International Conference on
  Artificial Intelligence and Statistics}, page 3418–3433. PMLR, May 2022.

\bibitem{rooke2021temporal}
Clayton Rooke, Jonathan Smith, Kin~Kwan Leung, Maksims Volkovs, and Saba
  Zuberi.
\newblock Temporal dependencies in feature importance for time series
  predictions.
\newblock {\em arXiv preprint arXiv:2107.14317}, 2021.

\bibitem{ye2009time}
Lexiang Ye and Eamonn Keogh.
\newblock Time series shapelets: a new primitive for data mining.
\newblock In {\em Proceedings of the 15th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 947--956, 2009.

\bibitem{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock {\em arXiv preprint arXiv:1312.6199}, 2013.

\bibitem{liao2022humancentered}
Q.~Vera Liao and Kush~R. Varshney.
\newblock Human-centered explainable ai (xai): From algorithms to user
  experiences, 2022.

\bibitem{azizmalayeri2022your}
Mohammad Azizmalayeri, Arshia~Soltani Moakar, Arman Zarei, Reihaneh Zohrabi,
  Mohammad~Taghi Manzuri, and Mohammad~Hossein Rohban.
\newblock Your out-of-distribution detection method is not robust!
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{hase2021ood}
Peter Hase, Harry Xie, and Mohit Bansal.
\newblock The out-of-distribution problem in explainability and search methods
  for feature importance explanations.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 3650--3666. Curran Associates, Inc., 2021.

\bibitem{hsieh2021evaluations}
Cheng-Yu Hsieh, Chih-Kuan Yeh, Xuanqing Liu, Pradeep~Kumar Ravikumar, Seungyeon
  Kim, Sanjiv Kumar, and Cho-Jui Hsieh.
\newblock Evaluations and methods for explanation through robustness analysis.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{qiu2021resisting}
Luyu Qiu, Yi~Yang, Caleb~Chen Cao, Jing Liu, Yueyuan Zheng, Hilary Hei~Ting
  Ngai, Janet Hsiao, and Lei Chen.
\newblock Resisting out-of-distribution data problem in perturbation of xai.
\newblock {\em arXiv preprint arXiv:2107.14000}, 2021.

\bibitem{jang2017categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{maddison2017concrete}
Chris~J. Maddison, Andriy Mnih, and Yee~Whye Teh.
\newblock The concrete distribution: A continuous relaxation of discrete random
  variables.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{miao2023interpretable}
Siqi Miao, Yunan Luo, Mia Liu, and Pan Li.
\newblock Interpretable geometric deep learning via learnable randomness
  injection.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{bengio2013estimating}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock {\em arXiv preprint arXiv:1308.3432}, 2013.

\bibitem{nguyen2020differentiable}
Thanh-Tung Nguyen, Xuan-Phi Nguyen, Shafiq Joty, and Xiaoli Li.
\newblock Differentiable window for dynamic local attention.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 6589--6599, Online, July 2020. Association
  for Computational Linguistics.

\bibitem{lei2016rationalizing}
Tao Lei, Regina Barzilay, and Tommi Jaakkola.
\newblock Rationalizing neural predictions.
\newblock In {\em Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pages 107--117, Austin, Texas, November 2016.
  Association for Computational Linguistics.

\bibitem{goel2022cyclip}
Shashank Goel, Hritik Bansal, Sumit Bhatia, Ryan~A. Rossi, Vishwa Vinay, and
  Aditya Grover.
\newblock Cy{CLIP}: Cyclic contrastive language-image pretraining.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{fong2019understanding}
Ruth Fong, Mandela Patrick, and Andrea Vedaldi.
\newblock Understanding deep networks via extremal perturbations and smooth
  masks.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 2950--2958, 2019.

\bibitem{Moody2001TheIO}
George~B. Moody and Roger~G. Mark.
\newblock The impact of the mit-bih arrhythmia database.
\newblock {\em IEEE Engineering in Medicine and Biology Magazine}, 20:45--50,
  2001.

\bibitem{reiss2012introducing}
Attila Reiss and Didier Stricker.
\newblock Introducing a new benchmarked dataset for activity monitoring.
\newblock In {\em 2012 16th international symposium on wearable computers},
  pages 108--109. IEEE, 2012.

\bibitem{andrzejak2001indications}
Ralph~G Andrzejak, Klaus Lehnertz, Florian Mormann, Christoph Rieke, Peter
  David, and Christian~E Elger.
\newblock Indications of nonlinear deterministic and finite-dimensional
  structures in time series of brain electrical activity: Dependence on
  recording region and brain state.
\newblock {\em Physical Review E}, 64(6):061907, 2001.

\bibitem{awav-bn36-19}
R.~Shohet, M.~Kandil, and J.J. McArthur.
\newblock Simulated boiler data for fault detection and classification, 2019.

\bibitem{Makowski2021neurokit}
Dominique Makowski, Tam Pham, Zen~J. Lau, Jan~C. Brammer, Fran{\c{c}}ois
  Lespinasse, Hung Pham, Christopher Schölzel, and S.~H.~Annabel Chen.
\newblock {NeuroKit}2: A python toolbox for neurophysiological signal
  processing.
\newblock {\em Behavior Research Methods}, 53(4):1689--1696, feb 2021.

\bibitem{leung2023temporal}
Kin~Kwan Leung, Clayton Rooke, Jonathan Smith, Saba Zuberi, and Maksims
  Volkovs.
\newblock Temporal dependencies in feature importance for time series
  prediction.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{Chuang2023CoRTXCF}
Yu-Neng Chuang, Guanchu Wang, Fan Yang, Quan Zhou, Pushkar Tripathi, Xuanting
  Cai, and Xia Hu.
\newblock Cortx: Contrastive framework for real-time explanation.
\newblock {\em ArXiv}, abs/2303.02794, 2023.

\bibitem{Vaswani2017AttentionIA}
Ashish Vaswani, Noam~M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NIPS}, 2017.

\end{thebibliography}
