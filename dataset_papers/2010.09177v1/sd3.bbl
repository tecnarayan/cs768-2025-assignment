\begin{thebibliography}{10}

\bibitem{achiam2019towards}
J.~Achiam, E.~Knight, and P.~Abbeel.
\newblock Towards characterizing divergence in deep q-learning.
\newblock {\em arXiv preprint arXiv:1903.08894}, 2019.

\bibitem{ahmed2019understanding}
Z.~Ahmed, N.~Le~Roux, M.~Norouzi, and D.~Schuurmans.
\newblock Understanding the impact of entropy on policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  151--160, 2019.

\bibitem{anschel2017averaged}
O.~Anschel, N.~Baram, and N.~Shimkin.
\newblock Averaged-dqn: Variance reduction and stabilization for deep
  reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 176--185. JMLR. org, 2017.

\bibitem{asadi2017alternative}
K.~Asadi and M.~L. Littman.
\newblock An alternative softmax operator for reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 243--252. JMLR. org, 2017.

\bibitem{barth2018distributed}
G.~Barth-Maron, M.~W. Hoffman, D.~Budden, W.~Dabney, D.~Horgan, D.~Tb,
  A.~Muldal, N.~Heess, and T.~Lillicrap.
\newblock Distributed distributional deterministic policy gradients.
\newblock {\em arXiv preprint arXiv:1804.08617}, 2018.

\bibitem{brockman2016openai}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}, 2016.

\bibitem{buckman2018sample}
J.~Buckman, D.~Hafner, G.~Tucker, E.~Brevdo, and H.~Lee.
\newblock Sample-efficient reinforcement learning with stochastic ensemble
  value expansion.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8224--8234, 2018.

\bibitem{catto2011box2d}
E.~Catto.
\newblock Box2d: A 2d physics engine for games.
\newblock 2011.

\bibitem{cesa2017boltzmann}
N.~Cesa-Bianchi, C.~Gentile, G.~Lugosi, and G.~Neu.
\newblock Boltzmann exploration done right.
\newblock In {\em Advances in neural information processing systems}, pages
  6284--6293, 2017.

\bibitem{ciosek2019better}
K.~Ciosek, Q.~Vuong, R.~Loftin, and K.~Hofmann.
\newblock Better exploration with optimistic actor critic.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1785--1796, 2019.

\bibitem{ciosek2018expected}
K.~Ciosek and S.~Whiteson.
\newblock Expected policy gradients.
\newblock In {\em Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem{dauphin2014identifying}
Y.~N. Dauphin, R.~Pascanu, C.~Gulcehre, K.~Cho, S.~Ganguli, and Y.~Bengio.
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock In {\em Advances in neural information processing systems}, pages
  2933--2941, 2014.

\bibitem{feinberg2018model}
V.~Feinberg, A.~Wan, I.~Stoica, M.~I. Jordan, J.~E. Gonzalez, and S.~Levine.
\newblock Model-based value estimation for efficient model-free reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1803.00101}, 2018.

\bibitem{td3_code}
S.~Fujimoto.
\newblock Open-source implementation for {\uppercase{td3}}.
\newblock \url{https://github.com/sfujim/TD3}.

\bibitem{fujimoto2018addressing}
S.~Fujimoto, H.~van Hoof, and D.~Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock 2018.

\bibitem{goodfellow2014qualitatively}
I.~J. Goodfellow, O.~Vinyals, and A.~M. Saxe.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock {\em arXiv preprint arXiv:1412.6544}, 2014.

\bibitem{gu2016continuous}
S.~Gu, T.~Lillicrap, I.~Sutskever, and S.~Levine.
\newblock Continuous deep q-learning with model-based acceleration.
\newblock In {\em International Conference on Machine Learning}, pages
  2829--2838, 2016.

\bibitem{haarnoja2017reinforcement}
T.~Haarnoja, H.~Tang, P.~Abbeel, and S.~Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In {\em International Conference on Machine Learning}, pages
  1352--1361, 2017.

\bibitem{haarnoja2018soft}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em International Conference on Machine Learning}, pages
  1861--1870, 2018.

\bibitem{hasselt2010double}
H.~V. Hasselt.
\newblock Double q-learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2613--2621, 2010.

\bibitem{horgan2018distributed}
D.~Horgan, J.~Quan, D.~Budden, G.~Barth-Maron, M.~Hessel, H.~Van~Hasselt, and
  D.~Silver.
\newblock Distributed prioritized experience replay.
\newblock {\em arXiv preprint arXiv:1803.00933}, 2018.

\bibitem{jaakkola1994convergence}
T.~Jaakkola, M.~I. Jordan, and S.~P. Singh.
\newblock Convergence of stochastic iterative dynamic programming algorithms.
\newblock In {\em Advances in neural information processing systems}, pages
  703--710, 1994.

\bibitem{khadka2018evolution}
S.~Khadka and K.~Tumer.
\newblock Evolution-guided policy gradient in reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1188--1200, 2018.

\bibitem{lan2020maxmin}
Q.~Lan, Y.~Pan, A.~Fyshe, and M.~White.
\newblock Maxmin q-learning: Controlling the estimation bias of q-learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{lillicrap2015continuous}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1509.02971}, 2015.

\bibitem{littman1996generalized}
M.~L. Littman and C.~Szepesv{\'a}ri.
\newblock A generalized reinforcement-learning model: Convergence and
  applications.
\newblock In {\em ICML}, volume~96, pages 310--318, 1996.

\bibitem{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529--533, 2015.

\bibitem{nachum2018smoothed}
O.~Nachum, M.~Norouzi, G.~Tucker, and D.~Schuurmans.
\newblock Smoothed action value functions for learning gaussian policies.
\newblock In {\em International Conference on Machine Learning}, pages
  3692--3700, 2018.

\bibitem{pan2020reinforcement}
L.~Pan, Q.~Cai, Q.~Meng, W.~Chen, and L.~Huang.
\newblock Reinforcement learning with dynamic boltzmann softmax updates.
\newblock In {\em Proceedings of the Twenty-Ninth International Joint
  Conference on Artificial Intelligence}, pages 1992--1998, 2020.

\bibitem{schulman2015trust}
J.~Schulman, S.~Levine, P.~Abbeel, M.~Jordan, and P.~Moritz.
\newblock Trust region policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  1889--1897, 2015.

\bibitem{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484, 2016.

\bibitem{silver2014deterministic}
D.~Silver, G.~Lever, N.~Heess, T.~Degris, D.~Wierstra, and M.~Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock 2014.

\bibitem{song2018revisiting}
Z.~Song, R.~E. Parr, and L.~Carin.
\newblock Revisiting the softmax bellman operator: New benefits and new
  perspective.
\newblock {\em arXiv preprint arXiv:1812.00456}, 2018.

\bibitem{sutton2011reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock Reinforcement learning: An introduction.
\newblock 2011.

\bibitem{thrun1993issues}
S.~Thrun and A.~Schwartz.
\newblock Issues in using function approximation for reinforcement learning.
\newblock In {\em Proceedings of the 1993 Connectionist Models Summer School
  Hillsdale, NJ. Lawrence Erlbaum}, 1993.

\bibitem{todorov2012mujoco}
E.~Todorov, T.~Erez, and Y.~Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em 2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033. IEEE, 2012.

\bibitem{van2016deep}
H.~Van~Hasselt, A.~Guez, and D.~Silver.
\newblock Deep reinforcement learning with double q-learning.
\newblock In {\em Thirtieth AAAI conference on artificial intelligence}, 2016.

\bibitem{watkins1989learning}
C.~J. C.~H. Watkins.
\newblock Learning from delayed rewards.
\newblock 1989.

\end{thebibliography}
