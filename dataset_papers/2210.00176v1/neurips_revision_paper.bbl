\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2019)Agrawal, Amos, Barratt, Boyd, Diamond, and
  Kolter]{agrawal2019differentiable}
Agrawal, A., Amos, B., Barratt, S., Boyd, S., Diamond, S., and Kolter, Z.
\newblock Differentiable convex optimization layers.
\newblock \emph{arXiv preprint arXiv:1910.12430}, 2019.

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  242--252. PMLR, 2019.

\bibitem[Arora et~al.(2016)Arora, Basu, Mianjy, and
  Mukherjee]{arora2016understanding}
Arora, R., Basu, A., Mianjy, P., and Mukherjee, A.
\newblock Understanding deep neural networks with rectified linear units.
\newblock \emph{arXiv preprint arXiv:1611.01491}, 2016.

\bibitem[Bach(2017)]{bach2017breaking}
Bach, F.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 629--681, 2017.

\bibitem[Basu(2014)]{basu2014algorithms}
Basu, S.
\newblock Algorithms in real algebraic geometry: a survey.
\newblock \emph{arXiv preprint arXiv:1409.1534}, 2014.

\bibitem[Bochnak et~al.(2013)Bochnak, Coste, and Roy]{bochnak2013real}
Bochnak, J., Coste, M., and Roy, M.-F.
\newblock \emph{Real algebraic geometry}, volume~36.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Daniely(2019)]{daniely2019neural}
Daniely, A.
\newblock Neural networks learning and memorization with (almost) no
  over-parameterization.
\newblock \emph{arXiv preprint arXiv:1911.09873}, 2019.

\bibitem[Dey et~al.(2020)Dey, Wang, and Xie]{dey2020approximation}
Dey, S.~S., Wang, G., and Xie, Y.
\newblock Approximation algorithms for training one-node relu neural networks.
\newblock \emph{IEEE Transactions on Signal Processing}, 68:\penalty0
  6696--6706, 2020.

\bibitem[Diamond \& Boyd(2016)Diamond and Boyd]{diamond2016cvxpy}
Diamond, S. and Boyd, S.
\newblock Cvxpy: A python-embedded modeling language for convex optimization.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2909--2913, 2016.

\bibitem[Domahidi et~al.(2013)Domahidi, Chu, and Boyd]{domahidi2013ecos}
Domahidi, A., Chu, E., and Boyd, S.
\newblock Ecos: An socp solver for embedded systems.
\newblock In \emph{2013 European Control Conference (ECC)}, pp.\  3071--3076.
  IEEE, 2013.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Froese et~al.(2021)Froese, Hertrich, and
  Niedermeier]{froese2021computational}
Froese, V., Hertrich, C., and Niedermeier, R.
\newblock The computational complexity of relu network training parameterized
  by data dimensionality.
\newblock \emph{arXiv preprint arXiv:2105.08675}, 2021.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pp.\  249--256. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem[Goel et~al.(2020)Goel, Klivans, Manurangsi, and
  Reichman]{goel2020tight}
Goel, S., Klivans, A., Manurangsi, P., and Reichman, D.
\newblock Tight hardness results for training depth-2 relu networks.
\newblock \emph{arXiv preprint arXiv:2011.13550}, 2020.

\bibitem[Gover(2014)]{gover2014congruence}
Gover, E.
\newblock Congruence and metrical invariants of zonotopes.
\newblock \emph{arXiv preprint arXiv:1401.4749}, 2014.

\bibitem[Hornik(1991)]{hornik1991approximation}
Hornik, K.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock \emph{Neural networks}, 4\penalty0 (2):\penalty0 251--257, 1991.

\bibitem[Ji \& Telgarsky(2019)Ji and Telgarsky]{ji2019polylogarithmic}
Ji, Z. and Telgarsky, M.
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock \emph{arXiv preprint arXiv:1909.12292}, 2019.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
LeCun, Y., Cortes, C., and Burges, C.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available:
  http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem[Maclagan \& Sturmfels(2015)Maclagan and
  Sturmfels]{maclagan2015introduction}
Maclagan, D. and Sturmfels, B.
\newblock \emph{Introduction to tropical geometry}, volume 161.
\newblock American Mathematical Soc., 2015.

\bibitem[McMullen(1971)]{mcmullen1971zonotopes}
McMullen, P.
\newblock On zonotopes.
\newblock \emph{Transactions of the American Mathematical Society},
  159:\penalty0 91--109, 1971.

\bibitem[Misiakos et~al.(2021)Misiakos, Smyrnis, Retsinas, and
  Maragos]{misiakos2021neural}
Misiakos, P., Smyrnis, G., Retsinas, G., and Maragos, P.
\newblock Neural network approximation based on hausdorff distance of tropical
  zonotopes.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Nakkiran et~al.(2021)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran2021deep}
Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2021\penalty0 (12):\penalty0 124003, 2021.

\bibitem[Oxley(2006)]{oxley2006matroid}
Oxley, J.~G.
\newblock \emph{Matroid theory}, volume~3.
\newblock Oxford University Press, USA, 2006.

\bibitem[Oymak \& Soltanolkotabi(2020)Oymak and
  Soltanolkotabi]{oymak2020toward}
Oymak, S. and Soltanolkotabi, M.
\newblock Toward moderate overparameterization: Global convergence guarantees
  for training shallow neural networks.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 84--105, 2020.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, et~al.]{pedregosa2011scikit}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
  O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et~al.
\newblock Scikit-learn: Machine learning in python.
\newblock \emph{the Journal of machine Learning research}, 12:\penalty0
  2825--2830, 2011.

\bibitem[Pilanci \& Ergen(2020)Pilanci and Ergen]{pilanci2020neural}
Pilanci, M. and Ergen, T.
\newblock Neural networks are convex regularizers: Exact polynomial-time convex
  optimization formulations for two-layer networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7695--7705. PMLR, 2020.

\bibitem[Richter-Gebert \& Ziegler(2017)Richter-Gebert and
  Ziegler]{richter20176}
Richter-Gebert, J. and Ziegler, G.~M.
\newblock \emph{6: Oriented matroids}.
\newblock Chapman and Hall/CRC, 2017.

\bibitem[Stanley et~al.(2004)]{stanley2004introduction}
Stanley, R.~P. et~al.
\newblock An introduction to hyperplane arrangements.
\newblock \emph{Geometric combinatorics}, 13\penalty0 (389-496):\penalty0 24,
  2004.

\bibitem[Wang et~al.(2021)Wang, Lacotte, and Pilanci]{wang2021hidden}
Wang, Y., Lacotte, J., and Pilanci, M.
\newblock The hidden convex optimization landscape of regularized two-layer
  relu networks: an exact characterization of optimal solutions.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Xu \& Yin(2013)Xu and Yin]{xu2013block}
Xu, Y. and Yin, W.
\newblock A block coordinate descent method for regularized multiconvex
  optimization with applications to nonnegative tensor factorization and
  completion.
\newblock \emph{SIAM Journal on imaging sciences}, 6\penalty0 (3):\penalty0
  1758--1789, 2013.

\bibitem[Zhang et~al.(2018)Zhang, Naitzat, and Lim]{zhang2018tropical}
Zhang, L., Naitzat, G., and Lim, L.-H.
\newblock Tropical geometry of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5824--5832. PMLR, 2018.

\bibitem[Ziegler(2012)]{ziegler2012lectures}
Ziegler, G.~M.
\newblock \emph{Lectures on polytopes}, volume 152.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Zou \& Gu(2019)Zou and Gu]{zou2019improved}
Zou, D. and Gu, Q.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:1906.04688}, 2019.

\end{thebibliography}
