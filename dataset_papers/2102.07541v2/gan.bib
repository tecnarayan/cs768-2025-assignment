
@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information Theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}

@article{rahimi2007random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  journal={NeurIPS},
  year={2007}
}


@article{chizat2018,
 author = {Chizat, L\'{e}na\"{\i}c and Bach, Francis},
 journal = {NeurIPS},
 title = {On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport},
 year = {2018}
}




@article{chizat2019sparse,
  title={Sparse optimization on measures with over-parameterized gradient descent},
  author={Chizat, Lenaic},
  journal={Mathematical Programming},
  year={2021}
}

@article{mei2019mean,
  title={Mean-field theory of two-layers neural networks: Dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={COLT},
  number={99},
  year={2019}
}

@article {MeiE7665,
	author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
	title = {A mean field view of the landscape of two-layer neural networks},
	volume = {115},
	number = {33},
	pages = {E7665--E7671},
	year = {2018},
	journal = {Proceedings of the National Academy of Sciences}
}





@article{Cybenko1989,
author = {Cybenko, G.},
year = {1989},
title = {Approximation by superpositions of a sigmoidal function},
journal = {Mathematics of Control, Signals and Systems},
volume = {2},
number = {4},
pages = {303--314},
}

@article{LESHNO1993,
title = "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function",
journal = "Neural Networks",
volume = "6",
number = "6",
pages = "861--867",
year = "1993",
author = "Moshe Leshno and Vladimir Ya. Lin and Allan Pinkus and Shimon Schocken",
}

@article{NTK2018,
 author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
 journal = {NeurIPS},
 title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
 year = {2018}
}


@article{kitchensink2008,
 author = {Rahimi, Ali and Recht, Benjamin},
 journal = {NeurIPS},
 title = {Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning},
 year = {2008}
}




@article{4797607,
   author = {Rahimi, Ali and Recht, Benjamin},
  journal = {Allerton Conference}, 
  title={Uniform approximation of functions with random bases}, 
  year={2008},
  }
  
  
@article{lee2018deep,
title={Deep Neural Networks as {G}aussian Processes},
author={Jaehoon Lee and Yasaman Bahri and Roman Novak and  Samuel S. Schoenholz and Jeffrey Pennington and Jascha Sohl-Dickstein},
journal ={ICLR},
year={2018},
}


@incollection{neal1996,
  title={Priors for infinite networks},
  author={Neal, Radford M},
  booktitle={Bayesian Learning for Neural Networks},
  pages={29--53},
  year={1996},
}


@article{pmlr-v119-lei20b,
title = {{SGD} Learns One-Layer Networks in {WGAN}s},
author = {Lei, Qi and Lee, Jason and Dimakis, Alex and Daskalakis, Constantinos},
journal = {ICML},
year = {2020}, 
}


@article{mean-field-gan2020,
title = {A mean-field analysis of two-player zero-sum games},
author = {Domingo-Enrich and Jelassi and Mensch and Rotskoff and Bruna},
journal = {NeurIPS},
year = {2020},
}


@article{pmlr-v97-hsieh19b,
  title = 	 {Finding Mixed {N}ash Equilibria of Generative Adversarial Networks},
  author =       {Hsieh, Ya-Ping and Liu, Chen and Cevher, Volkan},
journal = {ICML},
  year = 	 {2019},
}



@article{cai2019,
title = {Provable Convergence and Global Optimality of Generative Adversarial Network}, 
author = {Qi Cai and Zhuoran Yang and Jason D. Lee and Shaolei S. Du and Zhaoran Wang},
year = {2019},
journal = {OpenReview preprint}
}



@article{sanjabi2018,
 author = {Sanjabi, Maziar and Ba, Jimmy and Razaviyayn, Meisam and Lee, Jason D},
 journal = {NeurIPS},
 title = {On the Convergence and Robustness of Training {GANs} with Regularized Optimal Transport},
  year = {2018}
}



@article{jones1992,
author = "Jones, Lee K.",
journal = "Annals of Statistics",
ajournal = "Ann. Statist.",
number = "1",
pages = "608--613",
title = "A Simple Lemma on Greedy Approximation in {H}ilbert Space and Convergence Rates for Projection Pursuit Regression and Neural Network Training",
volume = "20",
year = "1992"
}


@article{SAF_1980-1981____A5_0,
     author = {Pisier, Gilles},
     title = {Remarques sur un r\'esultat non publi\'e de B. Maurey},
     journal = {S\'eminaire d'Analyse fonctionnelle (dit ``Maurey-Schwartz'')},
     year = {1980-1981},
}


@article{HORNIK1991251,
title = "Approximation capabilities of multilayer feedforward networks",
journal = "Neural Networks",
volume = "4",
number = "2",
pages = "251--257",
year = "1991",
author = "Kurt Hornik",
}

@article{rotskoff2018,
 author = {Rotskoff, Grant and Vanden-Eijnden, Eric},
 journal = {NeurIPS},
 title = {Parameters as interacting particles: Long time convergence and asymptotic error scaling of neural networks},
 year = {2018}
}





@article{Sirignano2020,
author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
title = {Mean Field Analysis of Neural Networks: A Law of Large Numbers},
journal = {SIAM Journal on Applied Mathematics},
volume = {80},
number = {2},
pages = {725--752},
year = {2020},
}



@article{Sirignano20202,
title = "Mean field analysis of neural networks: A central limit theorem",
journal = "Stochastic Processes and Their Applications",
volume = "130",
number = "3",
pages = "1820 - 1852",
year = "2020",
author = "Justin Sirignano and Konstantinos Spiliopoulos",
}




@article{gan_training_mescheder2018,
  title = 	 {Which Training Methods for {GAN}s do actually Converge?},
  author = 	 {Mescheder, L. and Geiger, A. and Nowozin, S.},
  journal = 	 {ICML},
  year = 	 {2018},
}


@article{goodfellow_gan_2014,
title = {Generative Adversarial Nets},
author = {Goodfellow, I. and Pouget-Abadie, J. and Mirza, M. and Xu, B. and Warde-Farley, D. and Ozair, S. and Courville, A. and Bengio, Y.},
journal = {NeurIPS},
year = {2014},
}

@article{goodfellow_tutorial,
author = {I. Goodfellow},
title = {{NIPS} 2016 Tutorial: Generative Adversarial Networks},
year = {2016},
journal = {arXiv:1701.00160}
}




@article{bottou2017_wass_gan,
  title = 	 {{W}asserstein Generative Adversarial Networks},
  author = 	 {M. Arjovsky and S. Chintala and L. Bottou},
  journal = 	 {ICML},
  year = 	 {2017},
}


@article{wass_gan_training2017,
title = {Improved Training of {W}asserstein {GANs}},
author = {Gulrajani, I. and Ahmed, F. and Arjovsky, M. and Dumoulin, V. and Courville, A. C.},
journal = {NeurIPS},
year = {2017},
}



@ARTICLE{Feizi2020,
  author={S. Feizi and F. Farnia and T. Ginart and D. Tse},
  journal={IEEE Journal on Selected Areas in Information Theory}, 
  title={Understanding {GANs} in the {LQG} Setting: Formulation, Generalization and Stability}, 
  year={2020},
  volume={1},
  number={1},
  pages={304--311},
}


@article{Rotskoff2019,
  title = 	 {Neuron birth-death dynamics accelerates gradient descent and converges asymptotically},
  author =       {Rotskoff, Grant and Jelassi, Samy and Bruna, Joan and Vanden-Eijnden, Eric},
  journal = 	 {ICML},
  year = 	 {2019},
}

@article{wu2018,
title = {No Spurious Local Minima in a Two Hidden Unit {ReLU} Network},
author = {Chenwei Wu and Jiajun Luo and Jason D. Lee},
year = {2018},
journal = {ICLR Workshop},
}


@article{ge2016,
 author = {Ge, Rong and Lee, Jason D and Ma, Tengyu},
 journal = {NeurIPS},
 title = {Matrix Completion has No Spurious Local Minimum},
 year = {2016}
}


@article{ge2017,
  title = 	 {No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis},
  author =       {Rong Ge and Chi Jin and Yi Zheng},
  journal = 	 {ICML},
  year = 	 {2017},
}


@article{Sanjabi2019,
title = {When Does Non-Orthogonal Tensor Decomposition Have No Spurious Local Minima?},
author = {Maziar Sanjabi and Sina Baharlouei and  Meisam Razaviyayn and  Jason D. Lee},
year = {2019},
journal = {arXiv:1911.09815},
}



@article{lee2016,
  title = 	 {Gradient Descent Only Converges to Minimizers},
  author = 	 {Jason D. Lee and Max Simchowitz and Michael I. Jordan and Benjamin Recht},
  journal = 	 {COLT},
  year = 	 {2016},}
}


@article{lee2019,
title = {First-order methods almost always avoid strict saddle points},
author = {Jason D. Lee and Ioannis Panageas and Georgios Piliouras and Max Simchowitz and Michael I. Jordan and Benjamin Recht},
journal = {Mathematical Programming},
volume = {176},
number = {1--2},
pages = {311--337},
year = {2019},
}


@article{jin17,
  title = 	 {How to Escape Saddle Points Efficiently},
  author =       {Chi Jin and Rong Ge and Praneeth Netrapalli and Sham M. Kakade and Michael I. Jordan},
  journal = 	 {ICML},
  year = 	 {2017},
}


@article{ge15,
  title={Escaping from saddle points-online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  journal={COLT},
  year={2015}
}


@article{Geiger_2020,
	year = 2020,
	volume = {2020},
	number = {11},
	pages = {113301},
	author = {Mario Geiger and Stefano Spigler and Arthur Jacot and Matthieu Wyart},
	title = {Disentangling feature and lazy training in deep neural networks},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
}


@article{
du2018gradient,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
journal={ICLR},
year={2019},
}


@article{NEURIPS2018_54fe976b,
 author = {Li, Yuanzhi and Liang, Yingyu},
 journal = {NeurIPS},
 title = {Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data},
 year = {2018}
}




@article{sun2020,
title = {Towards a Better Global Loss Landscape of {GANs}},
author = {Ruoyu Sun and Tiantian Fang and Alexander Schwing},
year = {2020},
journal = {NeurIPS},
}



@article{nguyen2018on,
title={On the loss landscape of a class of deep neural networks with no bad local valleys},
author={Quynh Nguyen and Mahesh Chandra Mukkamala and Matthias Hein},
journal={ICLR},
year={2019},
}

@article{pmlr-v80-liang18a,
  title = 	 {Understanding the Loss Surface of Neural Networks for Binary Classification},
  author =       {Liang, Shiyu and Sun, Ruoyu and Li, Yixuan and Srikant, Rayadurgam},
  journal = 	 {ICML},
  year = 	 {2018},
}


@article{NEURIPS2018_a0128693,
 author = {Liang, Shiyu and Sun, Ruoyu and Lee, Jason D and Srikant, R.},
 journal = {NeurIPS},
 title = {Adding One Neuron Can Eliminate All Bad Local Minima},
 year = {2018}
}


@article{Li2021,
author = {Dawei Li and Tian Ding and Ruoyu Sun},
title = {On the Benefit of Width for Neural Networks: Disappearance of Basins},
journal = {arXiv:1812.11039},
year = {2018},
}


@article{pmlr-v119-shevchenko20a,
  title = 	 {Landscape Connectivity and Dropout Stability of {SGD} Solutions for Over-parameterized Neural Networks},
  author =       {Shevchenko, Alexander and Mondelli, Marco},
  journal = 	 {ICML},
  year = 	 {2020},
}


@article{NEURIPS2018_be3087e7,
 author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
 journal = {NeurIPS},
 title = {Loss Surfaces, Mode Connectivity, and Fast Ensembling of {DNN}s},
 volume = {31},
 year = {2018}
}



@article{NEURIPS2019_46a4378f,
 author = {Kuditipudi, Rohith and Wang, Xiang and Lee, Holden and Zhang, Yi and Li, Zhiyuan and Hu, Wei and Ge, Rong and Arora, Sanjeev},
 journal = {NeurIPS},
 title = {Explaining Landscape Connectivity of Low-cost Solutions for Multilayer Nets},
 year = {2019}
}


@article{sun2020_review,
title = {Optimization for Deep Learning: An Overview},
author = {RuoYu Sun},
journal = {Journal of the Operations Research Society of China},
volume = {8},
number = {2},
pages = {249--294},
year = {2020}
}



@ARTICLE{9194023,
  author={R. Sun and D. Li and S. Liang and T. Ding and R. Srikant},
  journal={IEEE Signal Processing Magazine}, 
  title={The Global Landscape of Neural Networks: An Overview}, 
  year={2020},
  volume={37},
  number={5},
  pages={95--108},
  }
  
  
@article{SUSSMANN1992589,
title = "Uniqueness of the weights for minimal feedforward nets with a given input-output map",
journal = "Neural Networks",
volume = "5",
number = "4",
pages = "589--593",
year = "1992",
author = "H{\'e}ctor J. Sussmann",
}

@misc{Telgarsky2020,
  author        = {Matus Telgarsky},
  title         = {Deep learning theory lecture notes},
  month         = {Fall},
  year          = {2020},
  publisher={Department of Computer Science, University of Illinois at Urbana-Champaign}
}
@article{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={NeurIPS},
  year={2018}
}

@article{cho2019,
  author={J. Cho and C. Suh},
  journal = {Allerton Conference}, 
  title={Wasserstein {GAN} Can Perform {PCA}}, 
  year={2019},
  }

@article{johnson2015saddle,
  title={Saddle-point integration of {$C_\infty$ ``bump"} functions},
  author={Johnson, Steven G},
  journal={arXiv:1508.04376},
  year={2015}
}



@book{ASbook,
author = {Abramowitz, M. and Stegun, I. A.},
title = {Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables},
edition = {9th},
year = {1972},
}