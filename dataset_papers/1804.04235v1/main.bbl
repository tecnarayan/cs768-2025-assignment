\begin{thebibliography}{13}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{Duchi11Adaptive}
Duchi, John~C., Hazan, Elad, and Singer, Yoram.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121--2159,
  2011.
\newblock URL
  \url{http://dblp.uni-trier.de/db/journals/jmlr/jmlr12.html#DuchiHS11}.

\bibitem[Eckart \& Young(1936)Eckart and Young]{Eckart36Approximation}
Eckart, C. and Young, G.
\newblock The approximation of one matrix by another of lower rank.
\newblock \emph{Psychometrika}, 1\penalty0 (3):\penalty0 211--218, 1936.
\newblock \doi{10.1007/BF02288367}.

\bibitem[Finesso \& Spreij(2006)Finesso and Spreij]{Finesso06Nonnegative}
Finesso, Lorenzo and Spreij, Peter.
\newblock Nonnegative matrix factorization and {I}-divergence alternating
  minimization.
\newblock \emph{Linear Algebra and its Applications}, 416\penalty0
  (2):\penalty0 270 -- 287, 2006.
\newblock ISSN 0024-3795.
\newblock \doi{https://doi.org/10.1016/j.laa.2005.11.012}.
\newblock URL
  \url{http://www.sciencedirect.com/science/article/pii/S0024379505005665}.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'{a}}r, Girshick, Noordhuis,
  Wesolowski, Kyrola, Tulloch, Jia, and He]{Goyal17Accurate}
Goyal, Priya, Doll{\'{a}}r, Piotr, Girshick, Ross~B., Noordhuis, Pieter,
  Wesolowski, Lukasz, Kyrola, Aapo, Tulloch, Andrew, Jia, Yangqing, and He,
  Kaiming.
\newblock Accurate, large minibatch {SGD:} training imagenet in 1 hour.
\newblock \emph{CoRR}, abs/1706.02677, 2017.
\newblock URL \url{http://arxiv.org/abs/1706.02677}.

\bibitem[Gupta et~al.(2014)Gupta, Bengio, and Weston]{Gupta14}
Gupta, Maya~R., Bengio, Samy, and Weston, Jason.
\newblock Training highly multiclass classifiers.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 1461--1492,
  2014.
\newblock URL \url{http://jmlr.org/papers/v15/gupta14a.html}.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{Kingma14Adam}
Kingma, Diederik and Ba, Jimmy.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Lee \& Seung(1999)Lee and Seung]{Lee99Learning}
Lee, Daniel~D. and Seung, H.~Sebastian.
\newblock Learning the parts of objects by nonnegative matrix factorization.
\newblock \emph{Nature}, 401:\penalty0 788--791, 1999.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and Bengio]{Pascanu13Difficulty}
Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.
\newblock On the difficulty of training recurrent neural networks.
\newblock In Dasgupta, Sanjoy and McAllester, David (eds.), \emph{Proceedings
  of the 30th International Conference on Machine Learning}, volume~28 of
  \emph{Proceedings of Machine Learning Research}, pp.\  1310--1318, Atlanta,
  Georgia, USA, 17--19 Jun 2013. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v28/pascanu13.html}.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{Sashank18Convergence}
Reddi, Sashank~J., Kale, Satyen, and Kumar, Sanjiv.
\newblock On the convergence of adam and beyond.
\newblock \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=ryQu7f-RZ}.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{Shazeer17Outrageously}
Shazeer, Noam, Mirhoseini, Azalia, Maziarz, Krzysztof, Davis, Andy, Le, Quoc,
  Hinton, Geoffrey, and Dean, Jeff.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock In \emph{ICLR}, 2017.
\newblock URL \url{https://openreview.net/pdf?id=B1ckMDqlg}.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{Tieleman12RmsProp}
Tieleman, T. and Hinton, G.
\newblock {Lecture 6.5---RmsProp: Divide the gradient by a running average of
  its recent magnitude}.
\newblock COURSERA: Neural Networks for Machine Learning, 2012.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Vaswani17Attention}
Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion,
  Gomez, Aidan~N, Kaiser, {\L}ukasz, and Polosukhin, Illia.
\newblock Attention is all you need.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30}, pp.\  6000--6010. Curran Associates,
  Inc., 2017.
\newblock URL
  \url{http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}.

\bibitem[Zeiler(2012)]{Zeiler12Adadelta}
Zeiler, Matthew~D.
\newblock Adadelta: An adaptive learning rate method.
\newblock \emph{CoRR}, abs/1212.5701, 2012.
\newblock URL
  \url{http://dblp.uni-trier.de/db/journals/corr/corr1212.html#abs-1212-5701}.

\end{thebibliography}
