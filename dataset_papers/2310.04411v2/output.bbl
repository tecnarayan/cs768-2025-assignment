\begin{thebibliography}{10}

\bibitem{achiam2019charact}
Joshua Achiam, Ethan Knight, and Pieter Abbeel.
\newblock Towards characterizing divergence in deep q-learning.
\newblock {\em arXiv preprint arXiv:1903.08894}, 2019.

\bibitem{sac-n}
Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun~Oh Song.
\newblock Uncertainty-based offline reinforcement learning with diversified q-ensemble.
\newblock {\em NIPS}, 2021.

\bibitem{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{baird1995residual}
Leemon Baird.
\newblock Residual algorithms: Reinforcement learning with function approximation.
\newblock In {\em Machine Learning Proceedings}. 1995.

\bibitem{ball2023efficient}
Philip~J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine.
\newblock Efficient online reinforcement learning with offline data.
\newblock In {\em ICML}, 2023.

\bibitem{bhatt2019crossnorm}
Aditya Bhatt, Max Argus, Artemij Amiranashvili, and Thomas Brox.
\newblock Crossnorm: Normalization for off-policy td reinforcement learning.
\newblock {\em arXiv preprint arXiv:1902.05605}, 2019.

\bibitem{onestep}
David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna.
\newblock Offline rl without off-policy evaluation.
\newblock {\em NIPS}, 2021.

\bibitem{chen2022offline}
Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu.
\newblock Offline reinforcement learning via high-fidelity generative behavior modeling.
\newblock In {\em ICLR}, 2023.

\bibitem{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em NIPS}, 2021.

\bibitem{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning, 2020.

\bibitem{fu2019diagnosing}
Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine.
\newblock Diagnosing bottlenecks in deep q-learning algorithms.
\newblock In {\em ICML}, 2019.

\bibitem{td3+bc}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock {\em NIPS}, 2021.

\bibitem{td3}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em ICML}, 2018.

\bibitem{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em ICML}, 2019.

\bibitem{ghasemipour2022so}
Kamyar Ghasemipour, Shixiang~Shane Gu, and Ofir Nachum.
\newblock Why so pessimistic? estimating uncertainties for offline rl through ensembles, and why their independence matters.
\newblock {\em NIPS}, 2022.

\bibitem{gulcehre2020rlunplug}
Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio G{\'o}mez, Konrad Zolna, Rishabh Agarwal, Josh~S Merel, Daniel~J Mankowitz, Cosmin Paduraru, et~al.
\newblock Rl unplugged: A suite of benchmarks for offline reinforcement learning.
\newblock {\em NIPS}, 2020.

\bibitem{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In {\em ICML}, 2018.

\bibitem{han2022learning}
Yizeng Han, Yifan Pu, Zihang Lai, Chaofei Wang, Shiji Song, Junfeng Cao, Wenhui Huang, Chao Deng, and Gao Huang.
\newblock Learning to weight samples for dynamic early-exiting networks.
\newblock In {\em ECCV}, 2022.

\bibitem{Double-Q}
Hado Hasselt.
\newblock Double q-learning.
\newblock {\em NIPS}, 2010.

\bibitem{BN}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing internal covariate shift.
\newblock In {\em ICML}, 2015.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural networks.
\newblock {\em NIPS}, 2018.

\bibitem{traj_transformer}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock In {\em NIPS}, 2021.

\bibitem{jaques2019way}
Natasha Jaques, Asma Ghandeharioun, Judy~Hanwen Shen, Craig Ferguson, Ã€gata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind~W. Picard.
\newblock Way off-policy batch deep reinforcement learning of implicit human preferences in dialog.
\newblock {\em CoRR}, 2019.

\bibitem{kang2023efficient}
Bingyi Kang, Xiao Ma, Chao Du, Tianyu Pang, and Shuicheng Yan.
\newblock Efficient diffusion policies for offline reinforcement learning.
\newblock In {\em NIPS}, 2023.

\bibitem{kang2023improving}
Bingyi Kang, Xiao Ma, Yirui Wang, Yang Yue, and Shuicheng Yan.
\newblock Improving and benchmarking offline reinforcement learning algorithms, 2023.

\bibitem{IQL}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock In {\em NIPS Deep RL Workshop}, 2021.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em NIPS}, 2012.

\bibitem{scaled-ql}
Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine.
\newblock Offline q-learning on diverse multi-task data both scales and generalizes.
\newblock In {\em ICLR}, 2023.

\bibitem{dr3}
Aviral Kumar, Rishabh Agarwal, Tengyu Ma, Aaron Courville, George Tucker, and Sergey Levine.
\newblock Dr3: Value-based deep reinforcement learning requires explicit regularization.
\newblock In {\em ICLR}, 2022.

\bibitem{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock {\em NIPS}, 2019.

\bibitem{CQL}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock In {\em NIPS}, 2020.

\bibitem{lange2012batch}
Sascha Lange, Thomas Gabel, and Martin Riedmiller.
\newblock Batch reinforcement learning.
\newblock In {\em Reinforcement learning}. Springer, 2012.

\bibitem{levine2020tutorial}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on open problems.
\newblock {\em arXiv preprint arXiv:2005.01643}, 2020.

\bibitem{ddpg}
Timothy~P. Lillicrap, Jonathan~J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In {\em ICLR}, 2016.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em ICLR}, 2019.

\bibitem{lu2021power}
Rui Lu, Gao Huang, and Simon~S. Du.
\newblock On the power of multitask representation learning in linear mdp, 2021.

\bibitem{lu2022provable}
Rui Lu, Andrew Zhao, Simon~S. Du, and Gao Huang.
\newblock Provable general function class representation learning in multitask bandits and mdps.
\newblock In {\em NIPS}, 2022.

\bibitem{ma2022mutual}
Xiao Ma, Bingyi Kang, Zhongwen Xu, Min Lin, and Shuicheng Yan.
\newblock Mutual information regularized offline reinforcement learning.
\newblock In {\em NIPS}, 2023.

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness, Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 2015.

\bibitem{nair2020awac}
Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine.
\newblock Awac: Accelerating online reinforcement learning with offline datasets.
\newblock {\em arXiv preprint arXiv:2006.09359}, 2020.

\bibitem{lb-sac}
Alexander Nikulin, Vladislav Kurenkov, Denis Tarasov, Dmitry Akimov, and Sergey Kolesnikov.
\newblock Q-ensemble for offline rl: Don't scale the ensemble, scale the batch size.
\newblock {\em arXiv preprint arXiv:2211.11092}, 2022.

\bibitem{peng2019advantage}
Xue~Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
\newblock Advantage-weighted regression: Simple and scalable off-policy reinforcement learning.
\newblock {\em arXiv preprint arXiv:1910.00177}, 2019.

\bibitem{pu2023adaptive}
Yifan Pu, Yiru Wang, Zhuofan Xia, Yizeng Han, Yulin Wang, Weihao Gan, Zidong Wang, Shiji Song, and Gao Huang.
\newblock Adaptive rotated convolution for rotated object detection.
\newblock In {\em ICCV}, 2023.

\bibitem{WN}
Tim Salimans and Durk~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate training of deep neural networks.
\newblock {\em NIPS}, 2016.

\bibitem{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em JMLR}, 15(1):1929--1958, 2014.

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{tsitsiklis1996deadly_tirad}
JN~Tsitsiklis and B~Van~Roy.
\newblock An analysis of temporal-difference learning with function approximationtechnical.
\newblock {\em Rep. LIDS-P-2322). Lab. Inf. Decis. Syst. Massachusetts Inst. Technol. Tech. Rep}, 1996.

\bibitem{van2018deadly_triad}
Hado Van~Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil.
\newblock Deep reinforcement learning and the deadly triad.
\newblock {\em arXiv preprint arXiv:1812.02648}, 2018.

\bibitem{wang2021towards}
Chaofei Wang, Jiayu Xiao, Yizeng Han, Qisen Yang, Shiji Song, and Gao Huang.
\newblock Towards learning spatially discriminative feature representations.
\newblock In {\em ICCV}, 2021.

\bibitem{wang2022efficient}
Chaofei Wang, Qisen Yang, Rui Huang, Shiji Song, and Gao Huang.
\newblock Efficient knowledge distillation from model checkpoints.
\newblock {\em NIPS}, 2022.

\bibitem{wang2023diffusion}
Zhendong Wang, Jonathan~J Hunt, and Mingyuan Zhou.
\newblock Diffusion policies as an expressive policy class for offline reinforcement learning.
\newblock In {\em ICLR}, 2023.

\bibitem{wang2020critic}
Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh Merel, Jost~Tobias Springenberg, Scott~E. Reed, Bobak Shahriari, Noah~Y. Siegel, {\c{C}}aglar G{\"{u}}l{\c{c}}ehre, Nicolas Heess, and Nando de~Freitas.
\newblock Critic regularized regression.
\newblock In {\em NIPS}, 2020.

\bibitem{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:1911.11361}, 2019.

\bibitem{xu2021how}
Keyulu Xu, Mozhi Zhang, Jingling Li, Simon~Shaolei Du, Ken-Ichi Kawarabayashi, and Stefanie Jegelka.
\newblock How neural networks extrapolate: From feedforward to graph neural networks.
\newblock In {\em ICLR}, 2021.

\bibitem{yang2023boosting}
Qisen Yang, Shenzhi Wang, Matthieu~Gaetan Lin, Shiji Song, and Gao Huang.
\newblock Boosting offline reinforcement learning with action preference query.
\newblock In {\em ICML}, 2023.

\bibitem{yang2023hundreds}
Qisen Yang, Shenzhi Wang, Qihang Zhang, Gao Huang, and Shiji Song.
\newblock Hundreds guide millions: Adaptive offline reinforcement learning with expert guidance.
\newblock {\em arXiv preprint arXiv:2309.01448}, 2023.

\bibitem{yu2021combo}
Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.
\newblock Combo: Conservative offline model-based policy optimization.
\newblock {\em NIPS}, 2021.

\bibitem{yue2023offline}
Yang Yue, Bingyi Kang, Xiao Ma, Gao Huang, Shiji Song, and Shuicheng Yan.
\newblock Offline prioritized experience replay.
\newblock {\em arXiv preprint arXiv:2306.05412}, 2023.

\bibitem{yue2022red}
Yang Yue, Bingyi Kang, Xiao Ma, Zhongwen Xu, Gao Huang, and Shuicheng Yan.
\newblock Boosting offline reinforcement learning via data rebalancing.
\newblock In {\em Offline RL Workshop}, 2022.

\bibitem{yue2023vcr}
Yang Yue, Bingyi Kang, Zhongwen Xu, Gao Huang, and Shuicheng Yan.
\newblock Value-consistent representation learning for data-efficient reinforcement learning.
\newblock In {\em AAAI}, 2023.

\bibitem{zhang21y}
Shangtong Zhang, Hengshuai Yao, and Shimon Whiteson.
\newblock Breaking the deadly triad with a target network.
\newblock In {\em ICML}, 2021.

\end{thebibliography}
