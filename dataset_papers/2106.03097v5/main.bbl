\begin{thebibliography}{10}

\bibitem{FedDyn}
Durmus Alp~Emre Acar, Yue Zhao, Ramon~Matas Navarro, Matthew Mattina, Paul~N
  Whatmough, and Venkatesh Saligrama.
\newblock Federated learning based on dynamic regularization.
\newblock {\em arXiv preprint arXiv:2111.04263}, 2021.

\bibitem{fl_survey_technologies_applications}
Mohammed Aledhari, Rehma Razzak, Reza~M Parizi, and Fahad Saeed.
\newblock Federated learning: A survey on enabling technologies, protocols, and
  applications.
\newblock {\em IEEE Access}, 8:140699--140725, 2020.

\bibitem{MAS}
Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and
  Tinne Tuytelaars.
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 139--154, 2018.

\bibitem{CPR}
Sungmin Cha, Hsiang Hsu, Taebaek Hwang, Flavio~P Calmon, and Taesup Moon.
\newblock Cpr: Classifier-projection regularization for continual learning.
\newblock {\em arXiv preprint arXiv:2006.07326}, 2020.

\bibitem{RWalk}
Arslan Chaudhry, Puneet~K Dokania, Thalaiyasingam Ajanthan, and Philip~HS Torr.
\newblock Riemannian walk for incremental learning: Understanding forgetting
  and intransigence.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 532--547, 2018.

\bibitem{cl_hindsight}
Arslan Chaudhry, Albert Gordo, Puneet~Kumar Dokania, Philip Torr, and David
  Lopez-Paz.
\newblock Using hindsight to anchor past knowledge in continual learning.
\newblock {\em arXiv preprint arXiv:2002.08165}, 2(7), 2020.

\bibitem{cl_low_rank_orthogonal}
Arslan Chaudhry, Naeemullah Khan, Puneet~K Dokania, and Philip~HS Torr.
\newblock Continual learning in low-rank orthogonal subspaces.
\newblock {\em arXiv preprint arXiv:2010.11635}, 2020.

\bibitem{a-gem}
Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny.
\newblock Efficient lifelong learning with a-gem.
\newblock {\em arXiv preprint arXiv:1812.00420}, 2018.

\bibitem{FedBE}
Hong-You Chen and Wei-Lun Chao.
\newblock Fedbe: Making bayesian model ensemble applicable to federated
  learning.
\newblock {\em arXiv preprint arXiv:2009.01974}, 2020.

\bibitem{cinic10}
Luke~N Darlow, Elliot~J Crowley, Antreas Antoniou, and Amos~J Storkey.
\newblock Cinic-10 is not imagenet or cifar-10.
\newblock {\em arXiv preprint arXiv:1810.03505}, 2018.

\bibitem{mnist}
Li~Deng.
\newblock The mnist database of handwritten digit images for machine learning
  research.
\newblock {\em IEEE Signal Processing Magazine}, 29(6):141--142, 2012.

\bibitem{cutout}
Terrance DeVries and Graham~W Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock {\em arXiv preprint arXiv:1708.04552}, 2017.

\bibitem{fed_class_incremental}
Jiahua Dong, Lixu Wang, Zhen Fang, Gan Sun, Shichao Xu, Xiao Wang, and Qi~Zhu.
\newblock Federated class-incremental learning.
\newblock {\em arXiv preprint arXiv:2203.11473}, 2022.

\bibitem{OGD}
Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li.
\newblock Orthogonal gradient descent for continual learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3762--3773. PMLR, 2020.

\bibitem{catastrophic_forgetting}
Ian~J Goodfellow, Mehdi Mirza, Da~Xiao, Aaron Courville, and Yoshua Bengio.
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks.
\newblock {\em arXiv preprint arXiv:1312.6211}, 2013.

\bibitem{fedml}
Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi~Zhang, Hongyi Wang, Xiaoyang
  Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, et~al.
\newblock Fedml: A research library and benchmark for federated machine
  learning.
\newblock {\em arXiv preprint arXiv:2007.13518}, 2020.

\bibitem{knowledge_distillation}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{measuring_the_effects_of_noniid}
Tzu-Ming~Harry Hsu, Hang Qi, and Matthew Brown.
\newblock Measuring the effects of non-identical data distribution for
  federated visual classification.
\newblock {\em arXiv preprint arXiv:1909.06335}, 2019.

\bibitem{advances_open_problems_fl}
Peter Kairouz, H~Brendan McMahan, Brendan Avent, Aur{\'e}lien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
  Rachel Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock {\em arXiv preprint arXiv:1912.04977}, 2019.

\bibitem{SCAFFOLD}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
  Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In {\em International Conference on Machine Learning}, pages
  5132--5143. PMLR, 2020.

\bibitem{tighter_local_sgd}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock Tighter theory for local sgd on identical and heterogeneous data.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4519--4529. PMLR, 2020.

\bibitem{ewc}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the national academy of sciences},
  114(13):3521--3526, 2017.

\bibitem{federated_optimization}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Daniel Ramage, and Peter
  Richt{\'a}rik.
\newblock Federated optimization: Distributed machine learning for on-device
  intelligence.
\newblock {\em arXiv preprint arXiv:1610.02527}, 2016.

\bibitem{federated_learning}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Felix~X Yu, Peter Richt{\'a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock {\em arXiv preprint arXiv:1610.05492}, 2016.

\bibitem{cifar}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock Cifar-10 and cifar-100 datasets.
\newblock {\em URl: https://www. cs. toronto. edu/kriz/cifar. html}, 6, 2009.

\bibitem{FedMD}
Daliang Li and Junpu Wang.
\newblock Fedmd: Heterogenous federated learning via model distillation.
\newblock {\em arXiv preprint arXiv:1910.03581}, 2019.

\bibitem{FL_on_noniid_silos}
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He.
\newblock Federated learning on non-iid data silos: An experimental study.
\newblock {\em arXiv preprint arXiv:2102.02079}, 2021.

\bibitem{MOON}
Qinbin Li, Bingsheng He, and Dawn Song.
\newblock Model-contrastive federated learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10713--10722, 2021.

\bibitem{fl_challenges_methods}
Tian Li, Anit~Kumar Sahu, Ameet Talwalkar, and Virginia Smith.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock {\em IEEE Signal Processing Magazine}, 37(3):50--60, 2020.

\bibitem{FedProx}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock {\em Proceedings of Machine Learning and Systems}, 2:429--450, 2020.

\bibitem{On_the_convergence_fedavg_noniid}
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock {\em arXiv preprint arXiv:1907.02189}, 2019.

\bibitem{LwF}
Zhizhong Li and Derek Hoiem.
\newblock Learning without forgetting.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  40(12):2935--2947, 2017.

\bibitem{fed_ensemble_distill}
Tao Lin, Lingjing Kong, Sebastian~U Stich, and Martin Jaggi.
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock {\em arXiv preprint arXiv:2006.07242}, 2020.

\bibitem{CCVR}
Mi~Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and Jiashi Feng.
\newblock No fear of heterogeneity: Classifier calibration for federated
  learning with non-iid data.
\newblock {\em arXiv preprint arXiv:2106.05001}, 2021.

\bibitem{class_incremental_survey}
Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel Menta, Andrew~D Bagdanov,
  and Joost van~de Weijer.
\newblock Class-incremental learning: survey and performance evaluation on
  image classification.
\newblock {\em arXiv preprint arXiv:2010.15277}, 2020.

\bibitem{cl_forgetting2}
Michael McCloskey and Neal~J Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock In {\em Psychology of learning and motivation}, volume~24, pages
  109--165. Elsevier, 1989.

\bibitem{FedAvg}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem{FedAlign}
Matias Mendieta, Taojiannan Yang, Pu~Wang, Minwoo Lee, Zhengming Ding, and Chen
  Chen.
\newblock Local learning matters: Rethinking data heterogeneity in federated
  learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 8397--8406, 2022.

\bibitem{stability-plasticity}
Martial Mermillod, Aur{\'e}lia Bugaiska, and Patrick Bonin.
\newblock The stability-plasticity dilemma: Investigating the continuum from
  catastrophic forgetting to age-limited learning effects.
\newblock {\em Frontiers in psychology}, 4:504, 2013.

\bibitem{cl_forgetting1}
German~I Parisi, Ronald Kemker, Jose~L Part, Christopher Kanan, and Stefan
  Wermter.
\newblock Continual lifelong learning with neural networks: A review.
\newblock {\em Neural Networks}, 113:54--71, 2019.

\bibitem{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems 32}, pages 8024--8035. Curran Associates,
  Inc., 2019.

\bibitem{cl_original1}
Mark~B Ring.
\newblock Child: A first step towards continual learning.
\newblock In {\em Learning to learn}, pages 261--292. Springer, 1998.

\bibitem{FedCurv}
Neta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel, Daniel Benditkis, Liron
  Mor-Yosef, and Itai Zeitak.
\newblock Overcoming forgetting in federated learning on non-iid data.
\newblock {\em arXiv preprint arXiv:1910.07796}, 2019.

\bibitem{local_sgd_converges_fast}
Sebastian~U Stich.
\newblock Local sgd converges fast and communicates little.
\newblock {\em arXiv preprint arXiv:1805.09767}, 2018.

\bibitem{cl_original2}
Sebastian Thrun.
\newblock Lifelong learning algorithms.
\newblock In {\em Learning to learn}, pages 181--209. Springer, 1998.

\bibitem{FedMA}
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and
  Yasaman Khazaeni.
\newblock Federated learning with matched averaging.
\newblock {\em arXiv preprint arXiv:2002.06440}, 2020.

\bibitem{FedNova}
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H~Vincent Poor.
\newblock Tackling the objective inconsistency problem in heterogeneous
  federated optimization.
\newblock {\em arXiv preprint arXiv:2007.07481}, 2020.

\bibitem{local_sgd_better}
Blake Woodworth, Kumar~Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins,
  Brendan Mcmahan, Ohad Shamir, and Nathan Srebro.
\newblock Is local sgd better than minibatch sgd?
\newblock In {\em International Conference on Machine Learning}, pages
  10334--10343. PMLR, 2020.

\bibitem{FedReg}
Chencheng Xu, Zhiwei Hong, Minlie Huang, and Tao Jiang.
\newblock Acceleration of federated learning with alleviated forgetting in
  local training.
\newblock {\em arXiv preprint arXiv:2203.02645}, 2022.

\bibitem{federatedML_concepts_applications}
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.
\newblock Federated machine learning: Concept and applications.
\newblock {\em ACM Transactions on Intelligent Systems and Technology (TIST)},
  10(2):1--19, 2019.

\bibitem{FedWeIT}
Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung~Ju Hwang.
\newblock Federated continual learning with weighted inter-client transfer.
\newblock In {\em International Conference on Machine Learning}, pages
  12073--12086. PMLR, 2021.

\bibitem{FedMix}
Tehrim Yoon, Sumin Shin, Sung~Ju Hwang, and Eunho Yang.
\newblock Fedmix: Approximation of mixup under mean augmented federated
  learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{Fed2}
Fuxun Yu, Weishan Zhang, Zhuwei Qin, Zirui Xu, Di~Wang, Chenchen Liu, Zhi Tian,
  and Xiang Chen.
\newblock Fed2: Feature-aligned federated learning.
\newblock In {\em Proceedings of the 27th ACM SIGKDD Conference on Knowledge
  Discovery \& Data Mining}, pages 2066--2074, 2021.

\bibitem{PFNM}
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia
  Hoang, and Yasaman Khazaeni.
\newblock Bayesian nonparametric federated learning of neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  7252--7261. PMLR, 2019.

\bibitem{FedFTG}
Lin Zhang, Li~Shen, Liang Ding, Dacheng Tao, and Ling-Yu Duan.
\newblock Fine-tuning global model via data-free knowledge distillation for
  non-iid federated learning.
\newblock {\em arXiv preprint arXiv:2203.09249}, 2022.

\bibitem{fl_with_noniid}
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.
\newblock Federated learning with non-iid data.
\newblock {\em arXiv preprint arXiv:1806.00582}, 2018.

\bibitem{distilled_one_shot}
Yanlin Zhou, George Pu, Xiyao Ma, Xiaolin Li, and Dapeng Wu.
\newblock Distilled one-shot federated learning.
\newblock {\em arXiv preprint arXiv:2009.07999}, 2020.

\bibitem{FedGEN}
Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou.
\newblock Data-free knowledge distillation for heterogeneous federated
  learning.
\newblock {\em arXiv preprint arXiv:2105.10056}, 2021.

\end{thebibliography}
