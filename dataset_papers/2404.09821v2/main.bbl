\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Acu et~al.(2023)Acu, Rasa, and {\c{S}}teopoaie]{A2023strongly}
A.-M. Acu, I.~Rasa, and A.~E. {\c{S}}teopoaie.
\newblock Strongly convex squared norms.
\newblock \emph{Dolomites Research Notes on Approximation}, 16\penalty0 (2),
  2023.

\bibitem[Amos(2023)]{A2022amortizing}
B.~Amos.
\newblock On amortizing convex conjugates for optimal transport.
\newblock \emph{International Conference on Learning Representations}, 2023.

\bibitem[Amos et~al.(2017)Amos, Xu, and Kolter]{AX2017}
B.~Amos, L.~Xu, and J.~Z. Kolter.
\newblock Input convex neural networks.
\newblock In D.~Precup and Y.~W. Teh, editors, \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pages 146--155. PMLR, 06--11 Aug 2017.

\bibitem[Angwin et~al.(2016)Angwin, Larson, Mattu, and Kirchner]{A2016machine}
J.~Angwin, J.~Larson, S.~Mattu, and L.~Kirchner.
\newblock Machine bias: There’s software used across the country to predict
  future criminals. {A}nd it’s biased against blacks.
\newblock \emph{Propublica}, 2016.

\bibitem[Anil et~al.(2019)Anil, Lucas, and Grosse]{A2019sorting}
C.~Anil, J.~Lucas, and R.~Grosse.
\newblock Sorting out {L}ipschitz function approximation.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pages 291--301. PMLR, 09--15
  Jun 2019.

\bibitem[Araujo et~al.(2023)Araujo, Havens, Delattre, Allauzen, and
  Hu]{A2023unified}
A.~Araujo, A.~Havens, B.~Delattre, A.~Allauzen, and B.~Hu.
\newblock A unified algebraic perspective on {L}ipschitz neural networks.
\newblock \emph{International Conference on Learning Representations}, 2023.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and Bottou]{A2017wasserstein}
M.~Arjovsky, S.~Chintala, and L.~Bottou.
\newblock {W}asserstein generative adversarial networks.
\newblock In D.~Precup and Y.~W. Teh, editors, \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pages 214--223. PMLR, 06--11 Aug 2017.

\bibitem[Bai et~al.(2019)Bai, Kolter, and Koltun]{B2019deep}
S.~Bai, J.~Z. Kolter, and V.~Koltun.
\newblock Deep equilibrium models.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Bansal and Gupta(2017)]{B2017}
N.~Bansal and A.~Gupta.
\newblock Potential-function proofs for first-order methods.
\newblock \emph{arXiv preprint arXiv:1712.04581}, 2017.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and Telgarsky]{B2017spectrally}
P.~L. Bartlett, D.~J. Foster, and M.~J. Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Behrmann et~al.(2019)Behrmann, Grathwohl, Chen, Duvenaud, and
  Jacobsen]{B2019invertible}
J.~Behrmann, W.~Grathwohl, R.~T.~Q. Chen, D.~Duvenaud, and J.-H. Jacobsen.
\newblock Invertible residual networks.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pages 573--582. PMLR, 09--15
  Jun 2019.

\bibitem[Behrmann et~al.(2021)Behrmann, Vicol, Wang, Grosse, and
  Jacobsen]{B2021understanding}
J.~Behrmann, P.~Vicol, K.-C. Wang, R.~Grosse, and J.-H. Jacobsen.
\newblock Understanding and mitigating exploding inverses in invertible neural
  networks.
\newblock In A.~Banerjee and K.~Fukumizu, editors, \emph{Proceedings of the
  24th International Conference on Artificial Intelligence and Statistics},
  volume 130 of \emph{Proceedings of Machine Learning Research}, pages
  1792--1800. PMLR, 13--15 Apr 2021.

\bibitem[Bulatov(2011)]{B2011notmnist}
Y.~Bulatov.
\newblock {notMNIST} dataset.
\newblock 2011.
\newblock URL
  \url{https://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html}.

\bibitem[Chen et~al.(2019)Chen, Shi, and Zhang]{CS2018}
Y.~Chen, Y.~Shi, and B.~Zhang.
\newblock Optimal control via neural networks: A convex approach.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Cisse et~al.(2017)Cisse, Bojanowski, Grave, Dauphin, and
  Usunier]{C2017parseval}
M.~Cisse, P.~Bojanowski, E.~Grave, Y.~Dauphin, and N.~Usunier.
\newblock Parseval networks: Improving robustness to adversarial examples.
\newblock In D.~Precup and Y.~W. Teh, editors, \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pages 854--863. PMLR, 06--11 Aug 2017.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{D2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (61):\penalty0 2121--2159, 2011.

\bibitem[Farnia et~al.(2019)Farnia, Zhang, and Tse]{F2019generalizable}
F.~Farnia, J.~M. Zhang, and D.~Tse.
\newblock Generalizable adversarial training via spectral normalization.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Fazlyab et~al.(2019)Fazlyab, Robey, Hassani, Morari, and
  Pappas]{F2019efficient}
M.~Fazlyab, A.~Robey, H.~Hassani, M.~Morari, and G.~Pappas.
\newblock Efficient and accurate estimation of {L}ipschitz constants for deep
  neural networks.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Gal and Ghahramani(2016)]{G2016dropout}
Y.~Gal and Z.~Ghahramani.
\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In M.~F. Balcan and K.~Q. Weinberger, editors, \emph{Proceedings of
  The 33rd International Conference on Machine Learning}, volume~48 of
  \emph{Proceedings of Machine Learning Research}, pages 1050--1059, New York,
  New York, USA, 20--22 Jun 2016. PMLR.

\bibitem[Glorot and Bengio(2010)]{X2010understanding}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In Y.~W. Teh and M.~Titterington, editors, \emph{Proceedings of the
  13th International Conference on Artificial Intelligence and Statistics},
  volume~9 of \emph{Proceedings of Machine Learning Research}, pages 249--256,
  Chia Laguna Resort, Sardinia, Italy, 13--15 May 2010. PMLR.

\bibitem[Gouk et~al.(2021)Gouk, Frank, Pfahringer, and
  Cree]{G2021regularisation}
H.~Gouk, E.~Frank, B.~Pfahringer, and M.~J. Cree.
\newblock Regularisation of neural networks by enforcing {L}ipschitz
  continuity.
\newblock \emph{Machine Learning}, 110:\penalty0 393--416, 2021.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{G2017improved}
I.~Gulrajani, F.~Ahmed, M.~Arjovsky, V.~Dumoulin, and A.~C. Courville.
\newblock Improved training of {W}asserstein {GAN}s.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, and Swersky]{H2012neural}
G.~Hinton, N.~Srivastava, and K.~Swersky.
\newblock Neural networks for machine learning lecture 6a overview of
  mini-batch gradient descent.
\newblock \emph{Cited on}, 14\penalty0 (8):\penalty0 2, 2012.

\bibitem[Huang et~al.(2021)Huang, Chen, Tsirigotis, and Courville]{H2020convex}
C.-W. Huang, R.~T. Chen, C.~Tsirigotis, and A.~Courville.
\newblock Convex potential flows: Universal probability distributions with
  optimal transport and convex optimization.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Huang et~al.(2020)Huang, Liu, Zhu, Wan, Yuan, Li, and
  Shao]{H2020controllable}
L.~Huang, L.~Liu, F.~Zhu, D.~Wan, Z.~Yuan, B.~Li, and L.~Shao.
\newblock Controllable orthogonalization in training {DNN}s.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pages 6429--6438, 2020.

\bibitem[Huggins et~al.(2018)Huggins, Campbell, Kasprzak, and
  Broderick]{HC2018}
J.~H. Huggins, T.~Campbell, M.~Kasprzak, and T.~Broderick.
\newblock Practical bounds on the error of {B}ayesian posterior approximations:
  A nonasymptotic approach.
\newblock \emph{arXiv preprint arXiv:1809.09505}, 2018.

\bibitem[Jacobsen et~al.(2019)Jacobsen, Behrmann, Zemel, and
  Bethge]{J2018excessive}
J.-H. Jacobsen, J.~Behrmann, R.~Zemel, and M.~Bethge.
\newblock Excessive invariance causes adversarial vulnerability.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Janosi et~al.(1988)Janosi, Steinbrunn, Pfisterer, and
  Detrano]{J1998heart}
A.~Janosi, W.~Steinbrunn, M.~Pfisterer, and R.~Detrano.
\newblock {Heart Disease}.
\newblock UCI Machine Learning Repository, 1988.
\newblock {DOI}: https://doi.org/10.24432/C52P4X.

\bibitem[Jin and Lavaei(2020)]{J2020stability}
M.~Jin and J.~Lavaei.
\newblock Stability-certified reinforcement learning: A control-theoretic
  perspective.
\newblock \emph{IEEE Access}, 8:\penalty0 229086--229100, 2020.

\bibitem[Kim and Lee(2024)]{K2023scalable}
H.~Kim and J.-S. Lee.
\newblock Scalable monotonic neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Kingma and Ba(2015)]{K2015adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kinoshita et~al.(2023)Kinoshita, Oono, Fukumizu, Yoshida, and
  Maeda]{K2023}
Y.~Kinoshita, K.~Oono, K.~Fukumizu, Y.~Yoshida, and S.-i. Maeda.
\newblock Controlling posterior collapse by an inverse {L}ipschitz constraint
  on the decoder network.
\newblock In \emph{Proceedings of the 40th International Conference on Machine
  Learning}, Proceedings of Machine Learning Research. PMLR, 2023.

\bibitem[Kruse et~al.(2021)Kruse, Ardizzone, Rother, and
  K{\"o}the]{K2021benchmarking}
J.~Kruse, L.~Ardizzone, C.~Rother, and U.~K{\"o}the.
\newblock Benchmarking invertible architectures on inverse problems.
\newblock \emph{arXiv preprint arXiv:2101.10763}, 2021.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{L2017simple}
B.~Lakshminarayanan, A.~Pritzel, and C.~Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[LeCun and Cortes(2010)]{L2010mnist}
Y.~LeCun and C.~Cortes.
\newblock {MNIST} handwritten digit database.
\newblock 2010.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[Li et~al.(2019)Li, Haque, Anil, Lucas, Grosse, and
  Jacobsen]{L2019preventing}
Q.~Li, S.~Haque, C.~Anil, J.~Lucas, R.~B. Grosse, and J.-H. Jacobsen.
\newblock Preventing gradient attenuation in {L}ipschitz constrained
  convolutional networks.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Li et~al.(2020)Li, Zang, and Wu]{L2020markov}
S.~Z. Li, Z.~Zang, and L.~Wu.
\newblock Markov-{L}ipschitz deep learning.
\newblock \emph{arXiv preprint arXiv:2006.08256}, 2020.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Lin, Padhy, Tran, Bedrax~Weiss, and
  Lakshminarayanan]{L2020simple}
J.~Liu, Z.~Lin, S.~Padhy, D.~Tran, T.~Bedrax~Weiss, and B.~Lakshminarayanan.
\newblock Simple and principled uncertainty estimation with deterministic deep
  learning via distance awareness.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 7498--7512. Curran Associates, Inc., 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Han, Zhang, and
  Liu]{L2020certified}
X.~Liu, X.~Han, N.~Zhang, and Q.~Liu.
\newblock Certified monotonic neural networks.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 15427--15438. Curran Associates, Inc., 2020{\natexlab{b}}.

\bibitem[Megretski and Rantzer(1997)]{M1997system}
A.~Megretski and A.~Rantzer.
\newblock System analysis via integral quadratic constraints.
\newblock \emph{IEEE Transactions on Automatic Control}, 42\penalty0
  (6):\penalty0 819--830, 1997.
\newblock \doi{10.1109/9.587335}.

\bibitem[Meunier et~al.(2022)Meunier, Delattre, Araujo, and
  Allauzen]{M2022dynamical}
L.~Meunier, B.~J. Delattre, A.~Araujo, and A.~Allauzen.
\newblock A dynamical system perspective for {L}ipschitz neural networks.
\newblock In K.~Chaudhuri, S.~Jegelka, L.~Song, C.~Szepesvari, G.~Niu, and
  S.~Sabato, editors, \emph{Proceedings of the 39th International Conference on
  Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pages 15484--15500. PMLR, 17--23 Jul 2022.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida]{M2018spectral}
T.~Miyato, T.~Kataoka, M.~Koyama, and Y.~Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1802.05957}, 2018.

\bibitem[Neal(2012)]{N2012bayesian}
R.~M. Neal.
\newblock \emph{Bayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Nesterov(1983)]{N1983method}
Y.~E. Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate o$(1/k^2)$.
\newblock In \emph{Doklady Akademii Nauk}, volume 269, pages 543--547. Russian
  Academy of Sciences, 1983.

\bibitem[Nolte et~al.(2023)Nolte, Kitouni, and Williams]{N2023expressive}
N.~Nolte, O.~Kitouni, and M.~Williams.
\newblock Expressive monotonic neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Prach and Lampert(2022)]{P2022almost}
B.~Prach and C.~H. Lampert.
\newblock Almost-orthogonal layers for efficient general-purpose {L}ipschitz
  networks.
\newblock In \emph{Computer Vision – ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23–27, 2022, Proceedings, Part XXI}, page 350–365,
  Berlin, Heidelberg, 2022. Springer-Verlag.
\newblock ISBN 978-3-031-19802-1.
\newblock \doi{10.1007/978-3-031-19803-8_21}.

\bibitem[Quinlan(1993)]{Q1993auto}
R.~Quinlan.
\newblock {Auto MPG}.
\newblock UCI Machine Learning Repository, 1993.
\newblock {DOI}: https://doi.org/10.24432/C5859H.

\bibitem[Raissi et~al.(2019)Raissi, Perdikaris, and Karniadakis]{R2019physics}
M.~Raissi, P.~Perdikaris, and G.~Karniadakis.
\newblock Physics-informed neural networks: A deep learning framework for
  solving forward and inverse problems involving nonlinear partial differential
  equations.
\newblock \emph{Journal of Computational Physics}, 378:\penalty0 686--707,
  2019.
\newblock ISSN 0021-9991.
\newblock \doi{https://doi.org/10.1016/j.jcp.2018.10.045}.

\bibitem[Rakhlin et~al.(2012)Rakhlin, Shamir, and Sridharan]{R2011making}
A.~Rakhlin, O.~Shamir, and K.~Sridharan.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock 2012.

\bibitem[Rockafellar(1970)]{R1970maximal}
R.~Rockafellar.
\newblock On the maximal monotonicity of subdifferential mappings.
\newblock \emph{Pacific Journal of Mathematics}, 33\penalty0 (1):\penalty0
  209--216, 1970.

\bibitem[Rockafellar(1997)]{R1997convex}
R.~T. Rockafellar.
\newblock \emph{Convex analysis}, volume~11.
\newblock Princeton university press, 1997.

\bibitem[Runje and Shankaranarayana(2023)]{R2023constrained}
D.~Runje and S.~M. Shankaranarayana.
\newblock Constrained monotonic neural networks.
\newblock In A.~Krause, E.~Brunskill, K.~Cho, B.~Engelhardt, S.~Sabato, and
  J.~Scarlett, editors, \emph{Proceedings of the 40th International Conference
  on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning
  Research}, pages 29338--29353. PMLR, 23--29 Jul 2023.

\bibitem[Santambrogio(2015)]{S2015optimal}
F.~Santambrogio.
\newblock Optimal transport for applied mathematicians.
\newblock \emph{Birk{\"a}user, NY}, 55\penalty0 (58--63):\penalty0 94, 2015.

\bibitem[Scaman and Virmaux(2018)]{S2018lipschitz}
K.~Scaman and A.~Virmaux.
\newblock Lipschitz regularity of deep neural networks: Analysis and efficient
  estimation.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Shalev-Shwartz and Singer(2010)]{S2010equivalence}
S.~Shalev-Shwartz and Y.~Singer.
\newblock On the equivalence of weak learnability and linear separability: New
  relaxations and efficient boosting algorithms.
\newblock \emph{Machine learning}, 80:\penalty0 141--163, 2010.

\bibitem[Shamir and Zhang(2013)]{S2013stochastic}
O.~Shamir and T.~Zhang.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In S.~Dasgupta and D.~McAllester, editors, \emph{Proceedings of the
  30th International Conference on Machine Learning}, volume~28 of
  \emph{Proceedings of Machine Learning Research}, pages 71--79, Atlanta,
  Georgia, USA, 17--19 Jun 2013. PMLR.

\bibitem[Singla and Feizi(2021)]{S2021skew}
S.~Singla and S.~Feizi.
\newblock Skew orthogonal convolutions.
\newblock In M.~Meila and T.~Zhang, editors, \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 9756--9766. PMLR,
  18--24 Jul 2021.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{SZ2013}
C.~Szegedy, W.~Zaremba, I.~Sutskever, J.~Bruna, D.~Erhan, I.~Goodfellow, and
  R.~Fergus.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Trockman and Kolter(2021)]{T2021orthogonalizing}
A.~Trockman and J.~Z. Kolter.
\newblock Orthogonalizing convolutional layers with the cayley transform.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Van~Amersfoort et~al.(2020)Van~Amersfoort, Smith, Teh, and
  Gal]{V2020uncertainty}
J.~Van~Amersfoort, L.~Smith, Y.~W. Teh, and Y.~Gal.
\newblock Uncertainty estimation using a single deep deterministic neural
  network.
\newblock In H.~D. III and A.~Singh, editors, \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 9690--9700. PMLR,
  13--18 Jul 2020.

\bibitem[van~den Oord et~al.(2017)van~den Oord, Vinyals, and
  kavukcuoglu]{V2017neural}
A.~van~den Oord, O.~Vinyals, and k.~kavukcuoglu.
\newblock Neural discrete representation learning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Wang et~al.(2020)Wang, Chen, Chakraborty, and Yu]{W2020orthogonal}
J.~Wang, Y.~Chen, R.~Chakraborty, and S.~X. Yu.
\newblock Orthogonal convolutional neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, June 2020.

\bibitem[Wang and Manchester(2023)]{W2023direct}
R.~Wang and I.~Manchester.
\newblock Direct parameterization of {L}ipschitz-bounded deep networks.
\newblock In A.~Krause, E.~Brunskill, K.~Cho, B.~Engelhardt, S.~Sabato, and
  J.~Scarlett, editors, \emph{Proceedings of the 40th International Conference
  on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning
  Research}, pages 36093--36110. PMLR, 23--29 Jul 2023.

\bibitem[Wang et~al.(2024)Wang, Dvijotham, and Manchester]{W2024monotone}
R.~Wang, K.~D. Dvijotham, and I.~Manchester.
\newblock Monotone, bi-lipschitz, and polyak-{Ł}ojasiewicz networks.
\newblock In R.~Salakhutdinov, Z.~Kolter, K.~Heller, A.~Weller, N.~Oliver,
  J.~Scarlett, and F.~Berkenkamp, editors, \emph{Proceedings of the 41st
  International Conference on Machine Learning}, volume 235 of
  \emph{Proceedings of Machine Learning Research}, pages 50379--50399. PMLR,
  21--27 Jul 2024.

\bibitem[Wang et~al.(2021)Wang, Blei, and Cunningham]{WBC2021}
Y.~Wang, D.~Blei, and J.~P. Cunningham.
\newblock Posterior collapse and latent variable non-identifiability.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W.
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 5443--5455. Curran Associates, Inc., 2021.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{X2017fashion}
H.~Xiao, K.~Rasul, and R.~Vollgraf.
\newblock Fashion-{MNIST}: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Xu et~al.(2022)Xu, Li, and Li]{X2022lot}
X.~Xu, L.~Li, and B.~Li.
\newblock Lot: Layer-wise orthogonal training on improving $l_2$ certified
  robustness.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 18904--18915. Curran Associates, Inc., 2022.

\bibitem[Yu et~al.(2022)Yu, Li, Cai, and Li]{Y2022constructing}
T.~Yu, J.~Li, Y.~Cai, and P.~Li.
\newblock Constructing orthogonal convolutions in an explicit manner.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Jiang, He, and Wang]{Z2022rethinking}
B.~Zhang, D.~Jiang, D.~He, and L.~Wang.
\newblock Rethinking {L}ipschitz neural networks and certified robustness: A
  boolean function perspective.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~35,
  pages 19398--19413. Curran Associates, Inc., 2022.

\bibitem[Zhang et~al.(2020)Zhang, Gao, Unterman, and Arodz]{Z2020approximation}
H.~Zhang, X.~Gao, J.~Unterman, and T.~Arodz.
\newblock Approximation capabilities of neural {ODE}s and invertible residual
  networks.
\newblock In H.~D. III and A.~Singh, editors, \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 11086--11095. PMLR,
  13--18 Jul 2020.

\bibitem[Zhou(2018)]{Z2018fenchel}
X.~Zhou.
\newblock On the {F}enchel duality between strong convexity and {L}ipschitz
  continuous gradient.
\newblock \emph{arXiv preprint arXiv:1803.06573}, 2018.

\bibitem[Zlobec(2005)]{Z2005liu}
S.~Zlobec.
\newblock On the liu--floudas convexification of smooth programs.
\newblock \emph{Journal of Global Optimization}, 32\penalty0 (3):\penalty0
  401--407, 2005.

\end{thebibliography}
