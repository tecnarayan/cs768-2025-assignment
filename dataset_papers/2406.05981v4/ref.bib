@inproceedings{shi2022nasa,
  title={NASA: Neural architecture search and acceleration for hardware inspired hybrid networks},
  author={Shi, Huihong and You, Haoran and Zhao, Yang and Wang, Zhongfeng and Lin, Yingyan},
  booktitle={Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design},
  pages={1--9},
  year={2022}
}
@article{darvish2020pushing,
  title={Pushing the limits of narrow precision inferencing at cloud scale with microsoft floating point},
  author={Darvish Rouhani, Bita and Lo, Daniel and Zhao, Ritchie and Liu, Ming and Fowers, Jeremy and Ovtcharov, Kalin and Vinogradsky, Anna and Massengill, Sarah and Yang, Lita and Bittner, Ray and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={10271--10281},
  year={2020}
}
@article{yuan2024llm,
  title={Llm inference unveiled: Survey and roofline model insights},
  author={Yuan, Zhihang and Shang, Yuzhang and Zhou, Yang and Dong, Zhen and Xue, Chenhao and Wu, Bingzhe and Li, Zhikai and Gu, Qingyi and Lee, Yong Jae and Yan, Yan and others},
  journal={arXiv preprint arXiv:2402.16363},
  year={2024}
}
@article{lee2024lrq,
  title={LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices},
  author={Lee, Jung Hyun and Kim, Jeonghoon and Yang, June Yong and Kwon, Se Jung and Yang, Eunho and Yoo, Kang Min and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2407.11534},
  year={2024}
}
@article{shao2023omniquant,
  title={Omniquant: Omnidirectionally calibrated quantization for large language models},
  author={Shao, Wenqi and Chen, Mengzhao and Zhang, Zhaoyang and Xu, Peng and Zhao, Lirui and Li, Zhiqian and Zhang, Kaipeng and Gao, Peng and Qiao, Yu and Luo, Ping},
  journal={arXiv preprint arXiv:2308.13137},
  year={2023}
}
@inproceedings{lee2023flexround,
  title={Flexround: Learnable rounding based on element-wise division for post-training quantization},
  author={Lee, Jung Hyun and Kim, Jeonghoon and Kwon, Se Jung and Lee, Dongsoo},
  booktitle={International Conference on Machine Learning},
  pages={18913--18939},
  year={2023},
  organization={PMLR}
}
@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}
@article{zhu2024scalable,
  title        = {Scalable MatMul-free Language Modeling},
  author       = {Zhu, Rui-Jie and Zhang, Yu and Sifferman, Ethan and Sheaves, Tyler and Wang, Yiqiao and Richmond, Dustin and Zhou, Peng and Eshraghian, Jason K},
  year         = 2024,
  journal      = {arXiv preprint arXiv:2406.02528}
}
@article{frantar2022optimal,
	title        = {{Optimal Brain Compression: A Framework for
Accurate Post-Training Quantization and Pruning}},
	author       = {Frantar, Elias and Alistarh, Dan},
	year         = 2022,
	journal      = {NeurIPS}
}
@inproceedings{hassibi1993optimal,
	title        = {{Optimal Brain Surgeon and General Network Pruning}},
	author       = {Hassibi, Babak and Stork, David G and Wolff, Gregory J},
	year         = 1993,
	booktitle    = {IEEE international conference on neural networks}
}
@article{brito2014quaternary,
	title        = {{Quaternary logic lookup table in standard CMOS}},
	author       = {Brito, Diogo and Rabuske, Taimur G and Fernandes, Jorge R and others},
	year         = 2014,
	journal      = {TVLSI},
}
@misc{nvidia2020a100,
	title        = {{NVIDIA A100 Tensor Core GPU}},
	author       = {{NVIDIA Corporation}},
	year         = 2020,
	note         = {Datasheet},
	howpublished = {\url{https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf}}
}
@article{pilehvar2018wic,
	title        = {{WiC: the word-in-context dataset for evaluating context-sensitive meaning representations}},
	author       = {Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1808.09121}
}
@inproceedings{mostafazadeh2017lsdsem,
	title        = {{LSDSem 2017 Shared Task: The Story Cloze Test}},
	author       = {Mostafazadeh, Nasrin and Roth, Michael and Louis, Annie and others},
	year         = 2017,
	booktitle    = {Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics}
}
@inproceedings{levesque2012winograd,
	title        = {{The winograd schema challenge}},
	author       = {Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
	year         = 2012,
	booktitle    = {Thirteenth international conference on the principles of knowledge representation and reasoning}
}
@book{dagan2022recognizing,
	title        = {{Recognizing Textual Entailment: Models and Applications}},
	author       = {Dagan, Ido and Roth, Dan and Zanzotto, Fabio and others},
	year         = 2022,
        publisher={Springer Nature}
}
@inproceedings{tata2003piqa,
	title        = {{PiQA: An Algebra for Querying Protein Data Sets}},
	author       = {Tata, Sandeep and Patel, Jignesh M},
	year         = 2003,
	booktitle    = {SSDBM}
}
@inproceedings{afshar2018copa,
	title        = {{COPA: Constrained PARAFAC2 for sparse \& large datasets}},
	author       = {Afshar, Ardavan and Perros, Ioakeim and Papalexakis, Evangelos E and others},
	year         = 2018,
	booktitle    = {CIKM}
}
@inproceedings{de2019commitmentbank,
	title        = {{The commitmentbank: Investigating projection in naturally occurring discourse}},
	author       = {De Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},
	year         = 2019,
	booktitle    = {proceedings of Sinn und Bedeutung}
}
@article{clark2019boolq,
	title        = {{BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions}},
	author       = {Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and others},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1905.10044}
}
@article{boratko2018systematic,
	title        = {{A Systematic Classification of Knowledge, Reasoning, and Context within the ARC Dataset}},
	author       = {Boratko, Michael and Padigela, Harshit and Mikkilineni, Divyendra and others},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1806.00358}
}
@article{le2023bloom,
	title        = {{BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}},
	author       = {Scao, Teven Le and Fan, Angela and Akiki, Christopher and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2211.05100}
}
@article{jiang2023mistral,
	title        = {{Mistral 7B}},
	author       = {Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2310.06825}
}
@article{team2024gemma,
	title        = {{Gemma: Open Models Based on Gemini Research and Technology}},
	author       = {Mesnard, Thomas and Hardin, Cassidy and others},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2403.08295}
}
@inproceedings{li2023denseshift,
	title        = {{DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization}},
	author       = {Li, Xinlin and Liu, Bang and Yang, Rui Heng and others},
	year         = 2023,
	booktitle    = {ICCV}
}
@article{li2019additive,
	title        = {{Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks}},
	author       = {Li, Yuhang and Dong, Xin and Wang, Wei},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1909.13144}
}
@inproceedings{jeon2020biqgemm,
	title        = {{BiQGEMM: Matrix Multiplication with Lookup Table For Binary-Coding-based Quantized DNNs}},
	author       = {Jeon, Yongkweon and Park, Baeseong and Kwon, Se Jung and others},
	year         = 2020,
	booktitle    = {SC}
}
@inproceedings{sentieys2021approximate,
	title        = {{Approximate Computing for DNN}},
	author       = {Sentieys, Olivier},
	year         = 2021,
	booktitle    = {CSW 2021-HiPEAC Computing Systems Week}
}
@article{han2016eie,
	title        = {{EIE: Efficient Inference Engine on Compressed Deep Neural Network}},
	author       = {Han, Song and Liu, Xingyu and Mao, Huizi and others},
	year         = 2016,
	journal      = {ACM SIGARCH Computer Architecture News},
}
@inproceedings{horowitz20141,
	title        = {{1.1 Computing's Energy Problem (and what we can do about it)}},
	author       = {Horowitz, Mark},
	year         = 2014,
	booktitle    = {ISSCC}
}
@inproceedings{guo2017network,
	title        = {{Network Sketching: Exploiting Binary Structure in Deep CNNs}},
	author       = {Guo, Yiwen and Yao, Anbang and Zhao, Hao and others},
	year         = 2017,
	booktitle    = {CVPR}
}
@misc{kwon2020post,
	title        = {{Post-Training Weighted Quantization of Neural Networks for Language Models}},
	author       = {Se Jung Kwon and Dongsoo Lee and Yongkweon Jeon and others},
	year         = 2021,
	howpublished = {\url{https://openreview.net/forum?id=2Id6XxTjz7c}}
}
@inproceedings{biq,
	title        = {{Mr.BiQ: Post-Training Non-Uniform Quantization
based on Minimizing the Reconstruction Error}},
	author       = {Jeon, Yongkweon and Lee, Chungman and Cho, Eulrang and others},
	year         = 2022,
	booktitle    = {CVPR}
}
@article{xu2018alternating,
	title        = {{Alternating Multi-bit Quantization for Recurrent Neural Networks}},
	author       = {Xu, Chen and Yao, Jianqiang and Lin, Zhouchen and others},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1802.00150}
}
@article{chee2024quip,
	title        = {{QuIP: 2-Bit Quantization of Large Language Models With Guarantees}},
	author       = {Chee, Jerry and Cai, Yaohui and Kuleshov, Volodymyr and others},
	year         = 2024,
	journal      = {NeurIPS}
}
@article{huang2024billm,
	title        = {{BiLLM: Pushing the Limit of Post-Training Quantization for LLMs}},
	author       = {Huang, Wei and Liu, Yangdong and Qin, Haotong and others},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2402.04291}
}
@inproceedings{dettmers2023case,
	title        = {{The Case for 4-bit Precision: k-bit Inference Scaling Laws}},
	author       = {Dettmers, Tim and Zettlemoyer, Luke},
	year         = 2023,
	booktitle    = {ICML}
}
@article{yao2022zeroquant,
	title        = {{ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers}},
	author       = {Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and others},
	year         = 2022,
	journal      = {NeurIPS}
}
@article{dettmers2022gpt3,
	title        = {{GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale}},
	author       = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and others},
	year         = 2022,
	journal      = {NeurIPS}
}
@article{shen2024edgeqat,
	title        = {{EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge}},
	author       = {Shen, Xuan and Kong, Zhenglun and Yang, Changdi and others},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2402.10787}
}
@article{liu2023llm,
	title        = {{LLM-QAT: Data-free Quantization Aware Training for Large Language Models}},
	author       = {Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2305.17888}
}
@article{you2024shiftaddvit,
	title        = {{ShiftAddViT: Mixture of multiplication primitives towards efficient vision transformer}},
	author       = {You, Haoran and Shi, Huihong and Guo, Yipin and others},
	year         = 2024,
	journal      = {NeurIPS}
}
@article{yang2023gated,
	title        = {{Gated Linear Attention Transformers with Hardware-Efficient Training}},
	author       = {Yang, Songlin and Wang, Bailin and Shen, Yikang and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2312.06635}
}
@inproceedings{you2024linear,
	title        = {{When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models}},
	author       = {Haoran You and Yichao Fu and Zheng Wang and others},
	year         = 2024,
	booktitle    = {ICML}
}
@article{gromov2024unreasonable,
	title        = {{The Unreasonable Ineffectiveness of the Deeper Layers}},
	author       = {Gromov, Andrey and Tirumala, Kushal and Shapourian, Hassan and others},
	year         = 2024,
	journal      = {arXiv preprint arXiv:2403.17887}
}
@article{sun2023simple,
	title        = {{A Simple and Effective Pruning Approach for Large Language Models}},
	author       = {Sun, Mingjie and Liu, Zhuang and Bair, Anna and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2306.11695}
}
@article{ma2023llm,
	title        = {{LLM-Pruner: On the Structural Pruning of Large Language Models}},
	author       = {Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
	year         = 2023,
	journal      = {NeurIPS}
}
@article{park2022lut,
	title        = {{LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models}},
	author       = {Park, Gunho and Park, Baeseong and Kim, Minsub and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2206.09557}
}
@article{flops_memory_summary,
	title        = {{AI and Memory Wall}},
	author       = {Gholami, Amir and Yao, Zhewei and Kim, Sehoon and others},
	year         = 2024,
	journal      = {IEEE Micro Journal}
}
@article{lin2023awq,
	title        = {{AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration}},
	author       = {Lin, Ji and Tang, Jiaming and Tang, Haotian and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2306.00978}
}
@inproceedings{transformer,
	title        = {{Attention is All you Need}},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and others},
	year         = 2017,
	booktitle    = {NeurIPS}
}
@inproceedings{vit,
	title        = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
	author       = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and others},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations}
}
@article{zheng2023judging,
	title        = {{Judging LLM-as-a-judge with MT-Bench and Chatbot Arena}},
	author       = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2306.05685}
}
@article{rae2019compressive,
	title        = {{Compressive transformers for long-range sequence modelling}},
	author       = {Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and others},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1911.05507}
}
@inproceedings{see2017get,
	title        = {{Get To The Point: Summarization with Pointer-Generator Networks}},
	author       = {See, Abigail and Liu, Peter J and Manning, Christopher D},
	year         = 2017,
	booktitle    = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}
}
@inproceedings{lin-2004-rouge,
	title        = {{ROUGE: A Package for Automatic Evaluation of Summaries}},
	author       = {Lin, Chin-Yew},
	year         = 2004,
	booktitle    = {Text Summarization Branches Out},
	address      = {Barcelona, Spain},
	url          = {https://www.aclweb.org/anthology/W04-1013}
}
@software{together2023redpajama,
	title        = {{RedPajama: an Open Dataset for Training Large Language Models}},
	author       = {Together Computer},
	year         = 2023,
	url          = {https://github.com/togethercomputer/RedPajama-Data}
}
@misc{chatgpt,
	title        = {{ChatGPT: Language Model for Dialogue Generation}},
	author       = {{OpenAI}},
	year         = 2023,
	note         = {Website},
	howpublished = {\url{https://www.openai.com/chatgpt/}}
}
@article{gpt4,
	title        = {{GPT-4 Technical Report}},
	author       = {OpenAI},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2303.08774}
}
@article{dao2022flashattention,
	title        = {{FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}},
	author       = {Dao, Tri and Fu, Dan and Ermon, Stefano and others},
	year         = 2022,
	journal      = {NeurIPS}
}
@article{touvron2023llama,
	title        = {{LLaMA: Open and Efficient Foundation Language Models}},
	author       = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2302.13971}
}
@article{touvron2023llama2,
	title        = {{Llama 2: Open Foundation and Fine-Tuned Chat Models}},
	author       = {Touvron, Hugo and Martin, Louis and Stone, Kevin and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2307.09288}
}
@article{wang2018glue,
	title        = {{GLUE: A multi-task benchmark and analysis platform for natural language understanding}},
	author       = {Wang, Alex and Singh, Amanpreet and Michael, Julian and others},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1804.07461}
}
@article{gpt3,
	title        = {{Language models are few-shot learners}},
	author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and others},
	year         = 2020,
	journal      = {NeurIPS}
}
@inproceedings{xiao2023smoothquant,
	title        = {{SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models}},
	author       = {Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and others},
	year         = 2023,
	booktitle    = {ICML}
}
@inproceedings{dolan-brockett-2005-automatically,
	title        = {{Automatically Constructing a Corpus of Sentential Paraphrases}},
	author       = {Dolan, William B.  and Brockett, Chris},
	year         = 2005,
	booktitle    = {Proceedings of the Third International Workshop on Paraphrasing ({IWP}2005)},
	url          = {https://aclanthology.org/I05-5002}
}
@inproceedings{Chen2017QuoraQP,
	title        = {{Quora Question Pairs}},
	author       = {Zihang Chen and Hongbo Zhang and Xiaoji Zhang and others},
	year         = 2017
}
@inproceedings{rajpurkar-etal-2016-squad,
	title        = {{SQuAD: 100,000+ Questions for Machine Comprehension of Text}},
	author       = {Rajpurkar, Pranav  and Zhang, Jian  and Lopyrev, Konstantin  and others},
	year         = 2016,
	booktitle    = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
	address      = {Austin, Texas},
	url          = {https://aclanthology.org/D16-1264},
	editor       = {Su, Jian  and Duh, Kevin  and Carreras, Xavier}
}
@inproceedings{williams-etal-2018-broad,
	title        = {{A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference}},
	author       = {Williams, Adina  and Nangia, Nikita  and Bowman, Samuel},
	year         = 2018,
	booktitle    = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	address      = {New Orleans, Louisiana},
	url          = {https://aclanthology.org/N18-1101},
	editor       = {Walker, Marilyn  and Ji, Heng  and Stent, Amanda},
	abstract     = {This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.}
}
@inproceedings{wnli,
	title        = {{The winograd schema challenge}},
	author       = {Levesque, {Hector J.} and Ernest Davis and Leora Morgenstern},
	year         = 2012,
	booktitle    = {13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012},
	series       = {Proceedings of the International Conference on Knowledge Representation and Reasoning},
	isbn         = 9781577355601,
	note         = {13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012 ; Conference date: 10-06-2012 Through 14-06-2012},
	abstract     = {In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. A Wino-grad schema is a pair of sentences that differ only in one or two words and that contain a referential ambiguity that is resolved in opposite directions in the two sentences. We have compiled a collection of Winograd schemas, designed so that the correct answer is obvious to the human reader, but cannot easily be found using selectional restrictions or statistical techniques over text corpora. A contestant in the Winograd Schema Challenge is presented with a collection of one sentence from each pair, and required to achieve human-level accuracy in choosing the correct disambiguation.},
	language     = {English (US)}
}
@inproceedings{RTE,
	title        = {{The PASCAL Recognising Textual Entailment Challenge}},
	author       = {Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
	year         = 2006,
	booktitle    = {Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment},
	address      = {Berlin, Heidelberg},
	isbn         = {978-3-540-33428-6},
	editor       = {Qui{\~{n}}onero-Candela, Joaquin and Dagan, Ido and Magnini, Bernardo and d'Alch{\'e}-Buc, Florence},
	abstract     = {This paper describes the PASCAL Network of Excellence first Recognising Textual Entailment (RTE-1) Challenge benchmark. The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task.}
}
@inproceedings{socher-etal-2013-recursive,
	title        = {{Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank}},
	author       = {Socher, Richard  and Perelygin, Alex  and Wu, Jean  and others},
	year         = 2013,
	booktitle    = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	address      = {Seattle, Washington, USA},
	url          = {https://aclanthology.org/D13-1170},
	editor       = {Yarowsky, David  and Baldwin, Timothy  and Korhonen, Anna  and Livescu, Karen  and Bethard, Steven}
}
@article{bard,
	title        = {{Google’s AI chatbot “Bard”: A Side-by-Side Comparison with ChatGPT and its Utilization in Ophthalmology}},
	author       = {Waisberg, Ethan and Ong, Joshua and Masalkhi, Mouayad and others},
	year         = 2023,
	journal      = {Eye}
}
@article{zhang2022opt,
	title        = {{OPT: Open Pre-trained Transformer Language Models}},
	author       = {Zhang, Susan and Roller, Stephen and Goyal, Naman and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2205.01068}
}
@misc{llama3,
	title        = {{LLaMA 3}},
	author       = {Meta AI},
	year         = 2024,
	howpublished = {\url{https://github.com/meta-llama/llama3}}
}
@article{team2023gemini,
	title        = {{Gemini: A Family of Highly Capable Multimodal Models}},
	author       = {Anil, Rohan and Borgeaud, Sebastian and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2312.11805}
}
@inproceedings{DALL-E,
	title        = {{Zero-shot text-to-image generation}},
	author       = {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and others},
	year         = 2021,
	booktitle    = {ICML}
}
@article{roberts2022t5x,
	title        = {{Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$}},
	author       = {Roberts, Adam and Chung, Hyung Won and Levskaya, Anselm and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2203.17189},
	url          = {https://arxiv.org/abs/2203.17189}
}
@article{gpt1,
	title        = {{Improving language understanding by generative pre-training}},
	author       = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and others},
	year         = 2018
}
@article{gpt2,
	title        = {{Language models are unsupervised multitask learners}},
	author       = {Radford, Alec and Wu, Jeffrey and Child, Rewon and others},
	year         = 2019,
	journal      = {OpenAI blog},
	number       = 8
}
@article{zhu2023minigpt,
	title        = {{Minigpt-4: Enhancing vision-language understanding with advanced large language models}},
	author       = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2304.10592}
}
@article{chen2023minigptv2,
	title        = {{MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning}},
	author       = {Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2310.09478}
}
@article{devlin2018bert,
	title        = {{Bert: Pre-training of deep bidirectional transformers for language understanding}},
	author       = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and others},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1810.04805}
}
@inproceedings{frantar2022optq,
	title        = {{OPTQ: Accurate Quantization for Generative Pre-trained Transformers}},
	author       = {Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and others},
	year         = 2022,
	booktitle    = {ICLR}
}
@article{h2o,
	title        = {{H $ \_2 $ O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models}},
	author       = {Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2306.14048}
}
@article{zhu2021long,
	title        = {{Long-short transformer: Efficient transformers for language and vision}},
	author       = {Zhu, Chen and Ping, Wei and Xiao, Chaowei and others},
	year         = 2021,
	journal      = {NeurIPS}
}
@inproceedings{wolf-etal-2020-transformers,
	title        = {{Transformers: State-of-the-Art Natural Language Processing}},
	author       = {Thomas Wolf and Lysandre Debut and Victor Sanh and others},
	year         = 2020,
	booktitle    = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
	address      = {Online},
	url          = {https://www.aclweb.org/anthology/2020.emnlp-demos.6}
}
@inproceedings{katharopoulos2020transformers,
	title        = {{Transformers are rnns: Fast autoregressive transformers with linear attention}},
	author       = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and others},
	year         = 2020,
	booktitle    = {ICML}
}
@inproceedings{liu2021swin,
	title        = {{Swin transformer: Hierarchical vision transformer using shifted windows}},
	author       = {Liu, Ze and Lin, Yutong and Cao, Yue and others},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision}
}
@article{qin2023scaling,
	title        = {{Scaling transnormer to 175 billion parameters}},
	author       = {Qin, Zhen and Li, Dong and Sun, Weigao and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2307.14995}
}
@article{transnormer,
	title        = {{The devil in linear transformer}},
	author       = {Qin, Zhen and Han, Xiaodong and Sun, Weixuan and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2210.10340}
}
@inproceedings{xiong2021nystromformer,
	title        = {{Nystr\"omformer: A nystr\"om-based algorithm for approximating self-attention}},
	author       = {Xiong, Yunyang and Zeng, Zhanpeng and Chakraborty, Rudrasis and others},
	year         = 2021,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	number       = 16
}
@inproceedings{you2023castling,
	title        = {{Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference}},
	author       = {You, Haoran and Xiong, Yunyang and Dai, Xiaoliang and others},
	year         = 2023,
	booktitle    = {CVPR}
}
@inproceedings{arar2022learned,
	title        = {{Learned Queries for Efficient Local Attention}},
	author       = {Arar, Moab and Shamir, Ariel and Bermano, Amit H},
	year         = 2022,
	booktitle    = {CVPR}
}
@misc{wang2020linformer,
	title        = {{Linformer: Self-Attention with Linear Complexity}},
	author       = {Sinong Wang and Belinda Z. Li and Madian Khabsa and others},
	year         = 2020,
	eprint       = {2006.04768}
}
@inproceedings{tu2022maxvit,
	title        = {{Maxvit: Multi-axis vision transformer}},
	author       = {Tu, Zhengzhong and Talebi, Hossein and Zhang, Han and others},
	year         = 2022,
	booktitle    = {Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXIV}
}
@inproceedings{choromanski2021rethinking,
	title        = {{Rethinking Attention with Performers}},
	author       = {Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and others},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=Ua6zuk0WRH}
}
@article{bolya2022hydra,
	title        = {{Hydra attention: Efficient attention with many heads}},
	author       = {Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2209.07484}
}
@article{lu2021soft,
	title        = {{SOFT: softmax-free transformer with linear complexity}},
	author       = {Lu, Jiachen and Yao, Jinghan and Zhang, Junge and others},
	year         = 2021,
	journal      = {NeurIPS}
}
@inproceedings{cai2023efficientvit,
	title        = {{EfficientViT: Lightweight Multi-Scale Attention for High-Resolution Dense Prediction}},
	author       = {Cai, Han and Li, Junyan and Hu, Muyan and others},
	year         = 2023,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision}
}
@inproceedings{flash,
	title        = {{Transformer quality in linear time}},
	author       = {Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and others},
	year         = 2022,
	booktitle    = {ICML}
}
@inproceedings{merity2017pointer,
	title        = {{Pointer Sentinel Mixture Models}},
	author       = {Stephen Merity and Caiming Xiong and James Bradbury and others},
	year         = 2017,
	booktitle    = {ICLR},
}
@inproceedings{guo-etal-2020-wiki,
	title        = {{Wiki-40B: Multilingual Language Model Dataset}},
	author       = {Guo, Mandy  and Dai, Zihang  and Vrande{\v{c}}i{\'c}, Denny  and others},
	year         = 2020,
	booktitle    = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
	address      = {Marseille, France},
	isbn         = {979-10-95546-34-4},
	url          = {https://aclanthology.org/2020.lrec-1.297},
	editor       = {Calzolari, Nicoletta  and B{\'e}chet, Fr{\'e}d{\'e}ric  and Blache, Philippe  and Choukri, Khalid  and Cieri, Christopher  and Declerck, Thierry  and Goggi, Sara  and Isahara, Hitoshi  and Maegaard, Bente  and Mariani, Joseph  and Mazo, H{\'e}l{\`e}ne  and Moreno, Asuncion  and Odijk, Jan  and Piperidis, Stelios},
	abstract     = {We propose a new multilingual language model benchmark that is composed of 40+ languages spanning several scripts and linguistic families. With around 40 billion characters, we hope this new resource will accelerate the research of multilingual modeling. We train monolingual causal language models using a state-of-the-art model (Transformer-XL) establishing baselines for many languages. We also introduce the task of multilingual causal language modeling where we train our model on the combined text of 40+ languages from Wikipedia with different vocabulary sizes and evaluate on the languages individually. We released the cleaned-up text of 40+ Wikipedia language editions, the corresponding trained monolingual language models, and several multilingual language models with different fixed vocabulary sizes.},
	language     = {English}
}
@article{hutter2012human,
	title        = {{The human knowledge compression contest}},
	author       = {Hutter, Marcus},
	year         = 2012,
	journal      = {URL http://prize. hutter1. net}
}
@inproceedings{vasudevan2017parallel,
	title        = {{Parallel multi channel convolution using general matrix multiplication}},
	author       = {Vasudevan, Aravind and Anderson, Andrew and Gregg, David},
	year         = 2017,
	booktitle    = {2017 IEEE 28th international conference on application-specific systems, architectures and processors (ASAP)}
}
@article{strubell2019energy,
	title        = {{Energy and policy considerations for deep learning in NLP}},
	author       = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1906.02243}
}
@article{chen2023longlora,
	title        = {{LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models}},
	author       = {Chen, Yukang and Qian, Shengju and Tang, Haotian and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2309.12307}
}
@article{2020t5,
	title        = {{Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}},
	author       = {Colin Raffel and Noam Shazeer and Adam Roberts and others},
	year         = 2020,
	journal      = {Journal of Machine Learning Research},
	number       = 140,
	url          = {http://jmlr.org/papers/v21/20-074.html}
}
@article{performer,
	title        = {{Rethinking attention with performers}},
	author       = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and others},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2009.14794}
}
@article{chen2023accelerating,
	title        = {{Accelerating large language model decoding with speculative sampling}},
	author       = {Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2302.01318}
}
@article{schuster2022confident,
	title        = {{Confident adaptive language modeling}},
	author       = {Schuster, Tal and Fisch, Adam and Gupta, Jai and others},
	year         = 2022,
	journal      = {NeurIPS}
}
@article{miao2023specinfer,
	title        = {{SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification}},
	author       = {Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2305.09781}
}
@article{kim2023big,
	title        = {{Big little transformer decoder}},
	author       = {Kim, Sehoon and Mangalam, Karttikeya and Malik, Jitendra and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2302.07863}
}
@inproceedings{leviathan2023fast,
	title        = {{Fast inference from transformers via speculative decoding}},
	author       = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
	year         = 2023,
	booktitle    = {ICML}
}
@article{marcus1993building,
	title        = {{Building a large annotated corpus of English: The Penn Treebank}},
	author       = {Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
	year         = 1993
}
@inproceedings{dauphin2017language,
	title        = {{Language modeling with gated convolutional networks}},
	author       = {Dauphin, Yann N and Fan, Angela and Auli, Michael and others},
	year         = 2017,
	booktitle    = {ICML}
}
@misc{cai2023medusa,
	title        = {{Medusa: Simple framework for accelerating llm generation with multiple decoding heads}},
	author       = {Cai, Tianle and Li, Yuhong and Geng, Zhengyang and others},
	year         = 2023
}
@article{bae2023fast,
	title        = {{Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding}},
	author       = {Bae, Sangmin and Ko, Jongwoo and Song, Hwanjun and others},
	year         = 2023,
	journal      = {arXiv preprint arXiv:2310.05424}
}
@inproceedings{yoso,
	title        = {{You only sample (almost) once: Linear cost self-attention via bernoulli sampling}},
	author       = {Zeng, Zhanpeng and Xiong, Yunyang and Ravi, Sathya and others},
	year         = 2021,
	booktitle    = {ICML}
}
@inproceedings{li2022mvitv2,
	title        = {{MViTv2: Improved Multiscale Vision Transformers for Classification and Detection}},
	author       = {Li, Yanghao and Wu, Chao-Yuan and Fan, Haoqi and others},
	year         = 2022,
	booktitle    = {CVPR}
}
@inproceedings{DeiT,
	title        = {{Training data-efficient image transformers \& distillation through attention}},
	author       = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and others},
	year         = 2021,
	booktitle    = {ICML},
}
@article{graham2021levit,
	title        = {{LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference}},
	author       = {Graham, Ben and El-Nouby, Alaaeldin and Touvron, Hugo and others},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2104.01136}
}
@article{mehta2021mobilevit,
	title        = {{Mobilevit: light-weight, general-purpose, and mobile-friendly vision transformer}},
	author       = {Mehta, Sachin and Rastegari, Mohammad},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2110.02178}
}
@inproceedings{shen2021efficient,
	title        = {{Efficient attention: Attention with linear complexities}},
	author       = {Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and others},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF winter conference on applications of computer vision}
}
@article{cai2022efficientvit,
	title        = {{EfficientViT: Enhanced Linear Attention for High-Resolution Low-Computation Visual Recognition}},
	author       = {Cai, Han and Gan, Chuang and Han, Song},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2205.14756}
}
@inproceedings{liu2022ecoformer,
	title        = {{EcoFormer: Energy-Saving Attention with Linear Complexity}},
	author       = {Liu, Jing and Pan, Zizheng and He, Haoyu and others},
	year         = 2022,
	booktitle    = {NeurIPS}
}
@article{shu2021adder,
	title        = {{Adder Attention for Vision Transformer}},
	author       = {Shu, Han and Wang, Jiahao and Chen, Hanting and others},
	year         = 2021,
	journal      = {NeurIPS}
}
@inproceedings{chen2021autoformer,
	title        = {{Autoformer: Searching transformers for visual recognition}},
	author       = {Chen, Minghao and Peng, Houwen and Fu, Jianlong and others},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision}
}
@article{wang2022pvt,
	title        = {{Pvt v2: Improved baselines with pyramid vision transformer}},
	author       = {Wang, Wenhai and Xie, Enze and Li, Xiang and others},
	year         = 2022,
	journal      = {Computational Visual Media},
	number       = 3
}
@article{slope,
	title        = {{SLoPe: Double-Pruned Sparse Plus Lazy Low-rank Adapter Pretraining of LLMs}},
        author       = {Mozaffari, Mohammad and Yazdanbakhsh, Amir and Zhang, Zhao and Mehri Dahnavi, Maryam},
        year         = 2024,
	journal      = {arXiv preprint arXiv:2405.16325}
}

@article{qs-theory,
      title={{Effective Interplay between Sparsity and Quantization: From Theory to Practice}}, 
      author={Simla Burcu Harma and Ayan Chakraborty and Elizaveta Kostenok and Danila Mishin and Dongho Ha and Babak Falsafi and Martin Jaggi and Ming Liu and Yunho Oh and Suvinay Subramanian and Amir Yazdanbakhsh},
      year={2024},
      journal={arXiv preprint arXiv:2405.20935},
}
@article{chen2021scatterbrain,
	title        = {{Scatterbrain: Unifying sparse and low-rank attention}},
	author       = {Chen, Beidi and Dao, Tri and Winsor, Eric and others},
	year         = 2021,
	journal      = {NeurIPS}
}
@article{xue1986adaptive,
	title        = {{Adaptive Equalizer Based on a Power-Of-Two-Quantized-LMF Algorithm}},
	author       = {Xue, Ping and Liu, Bede},
	year         = 1986,
	journal      = {IEEE transactions on acoustics, speech, and signal processing},
}
@article{gwee2008low,
	title        = {{A Low-Voltage Micropower Asynchronous Multiplier With Shift–Add Multiplication Approach}},
	author       = {Gwee, Bah-Hwee and Chang, Joseph S and Shi, Yiqiong and others},
	year         = 2008,
	journal      = {IEEE Transactions on Circuits and Systems I: Regular Papers},
}
@inproceedings{you2022vitcod,
	title        = {{ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design}},
	author       = {You, Haoran and Sun, Zhanyi and Shi, Huihong and others},
	year         = 2023,
	booktitle    = {The 29th IEEE International Symposium on High-Performance Computer Architecture (HPCA 2023)}
}
@inproceedings{fan2022adaptable,
	title        = {{Adaptable Butterfly Accelerator for Attention-based NNs via Hardware and Algorithm Co-design}},
	author       = {Fan, Hongxiang and Chau, Thomas and Venieris, Stylianos I and others},
	year         = 2022,
	booktitle    = {2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)}
}
@article{you2020shiftaddnet,
	title        = {{ShiftAddNet: A Hardware-Inspired Deep Network}},
	author       = {You, Haoran and Chen, Xiaohan and Zhang, Yongan and others},
	year         = 2020,
	journal      = {NeurIPS}
}
@article{paszke2019pytorch,
	title        = {{Pytorch: An imperative style, high-performance deep learning library}},
	author       = {Paszke, Adam and Gross, Sam and Massa, Francisco and others},
	year         = 2019,
	journal      = {NeurIPS}
}
@inproceedings{chen2020addernet,
	title        = {{AdderNet: Do We Really Need Multiplications in Deep Learning?}},
	author       = {Chen, Hanting and Wang, Yunhe and Xu, Chunjing and others},
	year         = 2020,
	booktitle    = {CVPR}
}
@article{chen2021empirical,
	title        = {{An empirical study of adder neural networks for object detection}},
	author       = {Chen, Xinghao and Xu, Chang and Dong, Minjing and others},
	year         = 2021,
	journal      = {NeurIPS}
}
@article{bolya2022token,
	title        = {{Token Merging: Your ViT But Faster}},
	author       = {Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2210.09461}
}
@article{rao2021dynamicvit,
	title        = {{Dynamicvit: Efficient vision transformers with dynamic token sparsification}},
	author       = {Rao, Yongming and Zhao, Wenliang and Liu, Benlin and others},
	year         = 2021,
	journal      = {NeurIPS}
}
@inproceedings{yin2022avit,
	title        = {{A-ViT: Adaptive Tokens for Efficient Vision Transformer}},
	author       = {Yin, Hongxu and Vahdat, Arash and Alvarez, Jose M and others},
	year         = 2022,
	booktitle    = {CVPR}
}
@inproceedings{yu2022mia,
	title        = {{Mia-former: efficient and robust vision transformers via multi-grained input-adaptation}},
	author       = {Yu, Zhongzhi and Fu, Yonggan and Li, Sicheng and others},
	year         = 2022,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	number       = 8
}
@inproceedings{chen2017revisiting,
	title        = {{Revisiting Unreasonable Effectiveness of Data in Deep Learning Era}},
	author       = {Chen Sun and Abhinav Shrivastava and Saurabh Singh and others},
	year         = 2017,
	booktitle    = {ICCV},
	url          = {https://arxiv.org/abs/1707.02968}
}
@inproceedings{yuan2021tokens,
	title        = {{Tokens-to-token vit: Training vision transformers from scratch on imagenet}},
	author       = {Yuan, Li and Chen, Yunpeng and Wang, Tao and others},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision}
}
@inproceedings{PVT_ICCV,
	title        = {{Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction Without Convolutions}},
	author       = {Wang, Wenhai and Xie, Enze and Li, Xiang and others},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}
}
@article{chen2021crossvit,
	title        = {{Crossvit: Cross-attention multi-scale vision transformer for image classification}},
	author       = {Chen, Chun-Fu and Fan, Quanfu and Panda, Rameswar},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2103.14899}
}
@inproceedings{fan2021multiscale,
	title        = {{Multiscale vision transformers}},
	author       = {Fan, Haoqi and Xiong, Bo and Mangalam, Karttikeya and others},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision}
}
@inproceedings{heo2021pit,
	title        = {{Rethinking Spatial Dimensions of Vision Transformers}},
	author       = {Byeongho Heo and Sangdoo Yun and Dongyoon Han and others},
	year         = 2021,
	booktitle    = {International Conference on Computer Vision (ICCV)}
}
@inproceedings{sandler2018mobilenetv2,
	title        = {{Mobilenetv2: Inverted residuals and linear bottlenecks}},
	author       = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and others},
	year         = 2018,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition}
}
@article{ali2021xcit,
	title        = {{Xcit: Cross-covariance image transformers}},
	author       = {Ali, Alaaeldin and Touvron, Hugo and Caron, Mathilde and others},
	year         = 2021,
	journal      = {NeurIPS}
}
@inproceedings{wang2022shift,
	title        = {{When Shift Operation Meets Vision Transformer: An Extremely Simple Alternative to Attention Mechanism}},
	author       = {Wang, Guangting and Zhao, Yucheng and Tang, Chuanxin and others},
	year         = 2022,
	booktitle    = {AAAI}
}
@article{courbariaux2016binarized,
	title        = {{Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1}},
	author       = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and others},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1602.02830}
}
@inproceedings{juefei2017local,
	title        = {{Local Binary Convolutional Neural Networks}},
	author       = {Juefei-Xu, Felix and Naresh Boddeti, Vishnu and Savvides, Marios},
	year         = 2017,
	booktitle    = {CVPR}
}
@article{liu2022bit,
	title        = {{Bit: Robustly binarized multi-distilled transformer}},
	author       = {Liu, Zechun and Oguz, Barlas and Pappu, Aasish and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2205.13016}
}
@inproceedings{adder_distillation,
	title        = {{Kernel Based Progressive Distillation for Adder Neural Networks}},
	author       = {Xu, Yixing and Xu, Chang and Chen, Xinghao and others},
	year         = 2020,
	booktitle    = {NeurIPS}
}
@article{adder_hardware,
	title        = {{AdderNet and its Minimalist Hardware Design for Energy-Efficient Artificial Intelligence}},
	author       = {Wang, Yunhe and Huang, Mingqiang and Han, Kai and others},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2101.10015}
}
@inproceedings{wu2018shift,
	title        = {{Shift: A Zero FLOP, Zero Parameter Alternative to Spatial Convolutions}},
	author       = {Wu, Bichen and Wan, Alvin and Yue, Xiangyu and others},
	year         = 2018,
	booktitle    = {CVPR}
}
@inproceedings{elhoushi2021deepshift,
	title        = {{DeepShift: Towards Multiplication-Less Neural Networks}},
	author       = {Elhoushi, Mostafa and Chen, Zihao and Shafiq, Farhan and others},
	year         = 2021,
	booktitle    = {CVPR}
}
@article{he2022bivit,
	title        = {{BiViT: Extremely Compressed Binary Vision Transformer}},
	author       = {He, Yefei and Lou, Zhenyu and Zhang, Luoming and others},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2211.07091}
}
@inproceedings{you2022shiftaddnas,
	title        = {{ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks}},
	author       = {You, Haoran and Li, Baopu and Huihong, Shi and others},
	year         = 2022,
	booktitle    = {ICLR}
}
@inproceedings{li2022searching,
	title        = {{Searching for Energy-Efficient Hybrid Adder-Convolution Neural Networks}},
	author       = {Li, Wenshuo and Chen, Xinghao and Bai, Jinyu and others},
	year         = 2022,
	booktitle    = {CVPR}
}
@article{chen2018tvm,
	title        = {{TVM: An automated end-to-end optimizing compiler for deep learning}},
	author       = {Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and others},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1802.04799}
}
@article{lian2019high,
	title        = {{High-performance FPGA-based CNN accelerator with block-floating-point arithmetic}},
	author       = {Lian, Xiaocong and Liu, Zhenyu and Song, Zhourui and others},
	year         = 2019,
	journal      = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
	number       = 8
}
@article{zhou2022energon,
	title        = {{Energon: Toward Efficient Acceleration of Transformers Using Dynamic Sparse Attention}},
	author       = {Zhou, Zhe and Liu, Junlin and Gu, Zhenyu and others},
	year         = 2022,
	journal      = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	number       = 1
}
@inproceedings{qu2022dota,
	title        = {{DOTA: detect and omit weak attentions for scalable transformer acceleration}},
	author       = {Qu, Zheng and Liu, Liu and Tu, Fengbin and others},
	year         = 2022,
	booktitle    = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems}
}
@article{fedus2022switch,
	title        = {{Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity}},
	author       = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	year         = 2022,
	journal      = {The Journal of Machine Learning Research},
	number       = 1
}
@article{he2021fastmoe,
	title        = {{Fastmoe: A fast mixture-of-expert training system}},
	author       = {He, Jiaao and Qiu, Jiezhong and Zeng, Aohan and others},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2103.13262}
}
@article{shen2021nimble,
	title        = {{Nimble: Efficiently compiling dynamic neural networks for model inference}},
	author       = {Shen, Haichen and Roesch, Jared and Chen, Zhi and others},
	year         = 2021,
	journal      = {Proceedings of Machine Learning and Systems}
}
@article{mildenhall2021nerf,
	title        = {{Nerf: Representing scenes as neural radiance fields for view synthesis}},
	author       = {Mildenhall, Ben and Srinivasan, Pratul P and Tancik, Matthew and others},
	year         = 2021,
	journal      = {Communications of the ACM},
	number       = 1
}
@inproceedings{wizadwongsa2021nex,
	title        = {{Nex: Real-time view synthesis with neural basis expansion}},
	author       = {Wizadwongsa, Suttisak and Phongthawee, Pakkapon and Yenphraphai, Jiraphon and others},
	year         = 2021,
	booktitle    = {CVPR}
}
@inproceedings{GNT,
	title        = {{Is Attention All That NeRF Needs?}},
	author       = {Mukund Varma T and Peihao Wang and Xuxi Chen and others},
	year         = 2023,
	booktitle    = {The Eleventh International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=xE-LtsE-xx}
}
@article{mildenhall2019local,
	title        = {{Local light field fusion: Practical view synthesis with prescriptive sampling guidelines}},
	author       = {Mildenhall, Ben and Srinivasan, Pratul P and Ortiz-Cayon, Rodrigo and others},
	year         = 2019,
	journal      = {ACM Transactions on Graphics (TOG)},
}
@inproceedings{deng2009imagenet,
	title        = {{ImageNet: A large-scale hierarchical image database}},
	author       = {Deng, Jia and Dong, Wei and Socher, Richard and others},
	year         = 2009,
	booktitle    = {CVPR}
}
@article{chen2016eyeriss,
	title        = {{Eyeriss: An Energy-efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks}},
	author       = {Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel S and others},
	year         = 2016,
	journal      = {JSSCC},
}
@article{wang2004image,
	title        = {{Image quality assessment: from error visibility to structural similarity}},
	author       = {Wang, Zhou and Bovik, Alan C and Sheikh, Hamid R and others},
	year         = 2004,
	journal      = {IEEE transactions on image processing},
}
@inproceedings{zhang2018unreasonable,
	title        = {{The unreasonable effectiveness of deep features as a perceptual metric}},
	author       = {Zhang, Richard and Isola, Phillip and Efros, Alexei A and others},
	year         = 2018,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition}
}
@inproceedings{zhao2020dnn,
	title        = {{DNN-Chip Predictor: An Analytical Performance Predictor for DNN Accelerators with Various Dataflows and Hardware Architectures}},
	author       = {Zhao, Yang and Li, Chaojian and Wang, Yue and others},
	year         = 2020,
	booktitle    = {ICASSP}
}
@article{riquelme2021scaling,
	title        = {{Scaling Vision with Sparse Mixture of Experts}},
	author       = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and others},
	year         = 2021,
	journal      = {NeurIPS}
}
@article{shazeer2017outrageously,
	title        = {{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}},
	author       = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and others},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1701.06538}
}
@article{hubara2017quantized,
	title        = {{Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations}},
	author       = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and others},
	year         = 2017,
	journal      = {The Journal of Machine Learning Research},
	number       = 1
}
@inproceedings{Loshchilov2017DecoupledWD,
	title        = {{Decoupled Weight Decay Regularization}},
	author       = {Ilya Loshchilov and Frank Hutter},
	year         = 2017,
	booktitle    = {International Conference on Learning Representations}
}
