\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akrout et~al.(2019)Akrout, Wilson, Humphreys, Lillicrap, and
  Tweed]{akrout2019deep}
Akrout, M., Wilson, C., Humphreys, P., Lillicrap, T., and Tweed, D.~B.
\newblock Deep learning without weight transport.
\newblock \emph{NeurIPS}, 32, 2019.

\bibitem[Bartunov et~al.(2018)Bartunov, Santoro, Richards, Marris, Hinton, and
  Lillicrap]{bartunov2018assessing}
Bartunov, S., Santoro, A., Richards, B., Marris, L., Hinton, G.~E., and
  Lillicrap, T.
\newblock Assessing the scalability of biologically-motivated deep learning
  algorithms and architectures.
\newblock \emph{NeurIPS}, 31, 2018.

\bibitem[Baydin et~al.(2022)Baydin, Pearlmutter, Syme, Wood, and
  Torr]{baydin2022gradients}
Baydin, A.~G., Pearlmutter, B.~A., Syme, D., Wood, F., and Torr, P.
\newblock Gradients without backpropagation.
\newblock \emph{arXiv preprint arXiv:2202.08587}, 2022.

\bibitem[Beck \& Tetruashvili(2013)Beck and Tetruashvili]{beck2013convergence}
Beck, A. and Tetruashvili, L.
\newblock On the convergence of block coordinate descent type methods.
\newblock \emph{SIAM journal on Optimization}, 23\penalty0 (4):\penalty0
  2037--2060, 2013.

\bibitem[Belilovsky et~al.(2019)Belilovsky, Eickenberg, and
  Oyallon]{belilovsky2019greedy}
Belilovsky, E., Eickenberg, M., and Oyallon, E.
\newblock Greedy layerwise learning can scale to imagenet.
\newblock In \emph{ICML}, pp.\  583--593. PMLR, 2019.

\bibitem[Belilovsky et~al.(2020)Belilovsky, Eickenberg, and
  Oyallon]{belilovsky2020decoupled}
Belilovsky, E., Eickenberg, M., and Oyallon, E.
\newblock Decoupled greedy learning of cnns.
\newblock In \emph{ICML}, pp.\  736--745. PMLR, 2020.

\bibitem[Bengio et~al.(2006)Bengio, Lamblin, Popovici, and
  Larochelle]{bengio2006greedy}
Bengio, Y., Lamblin, P., Popovici, D., and Larochelle, H.
\newblock Greedy layer-wise training of deep networks.
\newblock \emph{NeurIPS}, 19, 2006.

\bibitem[Bengio et~al.(2015)Bengio, Lee, Bornschein, Mesnard, and
  Lin]{bengio2015towards}
Bengio, Y., Lee, D.-H., Bornschein, J., Mesnard, T., and Lin, Z.
\newblock Towards biologically plausible deep learning.
\newblock \emph{arXiv preprint arXiv:1502.04156}, 2015.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{chen2016training}
Chen, T., Xu, B., Zhang, C., and Guestrin, C.
\newblock Training deep nets with sublinear memory cost.
\newblock \emph{arXiv preprint arXiv:1604.06174}, 2016.

\bibitem[Clark et~al.(2021)Clark, Abbott, and Chung]{clark2021credit}
Clark, D., Abbott, L., and Chung, S.
\newblock Credit assignment through broadcasting a global error vector.
\newblock \emph{NeurIPS}, 34:\penalty0 10053--10066, 2021.

\bibitem[Crick(1989)]{crick1989recent}
Crick, F.
\newblock The recent excitement about neural networks.
\newblock \emph{Nature}, 337\penalty0 (6203):\penalty0 129--132, 1989.

\bibitem[Dellaferrera \& Kreiman(2022)Dellaferrera and
  Kreiman]{dellaferrera2022error}
Dellaferrera, G. and Kreiman, G.
\newblock Error-driven input modulation: solving the credit assignment problem
  without a backward pass.
\newblock In \emph{ICML}, pp.\  4937--4955. PMLR, 2022.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{arXiv preprint arXiv:2010.11929}, 2021.

\bibitem[Du et~al.(2023)Du, Yang, Tao, and Hsieh]{du2023problem}
Du, Y., Yang, Y., Tao, D., and Hsieh, M.-H.
\newblock Problem-dependent power of quantum neural networks on multiclass
  classification.
\newblock \emph{Physical Review Letters}, 131\penalty0 (14):\penalty0 140601,
  2023.

\bibitem[Fournier et~al.(2023)Fournier, Patel, Eickenberg, Oyallon, and
  Belilovsky]{fournier2023preventing}
Fournier, L., Patel, A., Eickenberg, M., Oyallon, E., and Belilovsky, E.
\newblock Preventing dimensional collapse in contrastive local learning with
  subsampling.
\newblock In \emph{ICML 2023 Workshop on Localized Learning (LLW)}, 2023.

\bibitem[Frenkel et~al.(2021)Frenkel, Lefebvre, and Bol]{frenkel2021learning}
Frenkel, C., Lefebvre, M., and Bol, D.
\newblock Learning without feedback: Fixed random learning signals allow for
  feedforward training of deep neural networks.
\newblock \emph{Frontiers in neuroscience}, 15:\penalty0 629892, 2021.

\bibitem[Gomez et~al.(2017)Gomez, Ren, Urtasun, and
  Grosse]{gomez2017reversible}
Gomez, A.~N., Ren, M., Urtasun, R., and Grosse, R.~B.
\newblock The reversible residual network: Backpropagation without storing
  activations.
\newblock \emph{NeurIPS}, 30, 2017.

\bibitem[Grossberg(1987)]{grossberg1987competitive}
Grossberg, S.
\newblock Competitive learning: From interactive activation to adaptive
  resonance.
\newblock \emph{Cognitive science}, 11\penalty0 (1):\penalty0 23--63, 1987.

\bibitem[Halvagal \& Zenke(2023)Halvagal and Zenke]{halvagal2023combination}
Halvagal, M.~S. and Zenke, F.
\newblock The combination of hebbian and predictive plasticity learns invariant
  object representations in deep sensory networks.
\newblock \emph{Nature Neuroscience}, pp.\  1--10, 2023.

\bibitem[He \& Su(2023)He and Su]{he2023law}
He, H. and Su, W.~J.
\newblock A law of data separation in deep learning.
\newblock \emph{Proceedings of the National Academy of Sciences}, 120\penalty0
  (36):\penalty0 e2221704120, 2023.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, pp.\  770--778, 2016.

\bibitem[Hinton(2022)]{hinton2022forward}
Hinton, G.
\newblock The forward-forward algorithm: Some preliminary investigations.
\newblock \emph{arXiv preprint arXiv:2212.13345}, 2022.

\bibitem[Hinton et~al.(2006)Hinton, Osindero, and Teh]{hinton2006fast}
Hinton, G.~E., Osindero, S., and Teh, Y.-W.
\newblock A fast learning algorithm for deep belief nets.
\newblock \emph{Neural computation}, 18\penalty0 (7):\penalty0 1527--1554,
  2006.

\bibitem[Illing et~al.(2021)Illing, Ventura, Bellec, and
  Gerstner]{illing2021local}
Illing, B., Ventura, J., Bellec, G., and Gerstner, W.
\newblock Local plasticity rules can learn deep representations using
  self-supervised contrastive predictions.
\newblock \emph{NeurIPS}, 34:\penalty0 30365--30379, 2021.

\bibitem[Jaderberg et~al.(2017)Jaderberg, Czarnecki, Osindero, Vinyals, Graves,
  Silver, and Kavukcuoglu]{jaderberg2017decoupled}
Jaderberg, M., Czarnecki, W.~M., Osindero, S., Vinyals, O., Graves, A., Silver,
  D., and Kavukcuoglu, K.
\newblock Decoupled neural interfaces using synthetic gradients.
\newblock In \emph{ICML}, pp.\  1627--1635. PMLR, 2017.

\bibitem[Journ{\'e} et~al.(2023)Journ{\'e}, Rodriguez, Guo, and
  Moraitis]{journ2023hebbian}
Journ{\'e}, A., Rodriguez, H.~G., Guo, Q., and Moraitis, T.
\newblock Hebbian deep learning without feedback.
\newblock In \emph{ICLR}, 2023.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Karimi, H., Nutini, J., and Schmidt, M.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases:
  European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23,
  2016, Proceedings, Part I 16}, pp.\  795--811. Springer, 2016.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Li et~al.(2020)Li, Xiao, Fang, Dai, Xu, and Lin]{li2020training}
Li, J., Xiao, M., Fang, C., Dai, Y., Xu, C., and Lin, Z.
\newblock Training neural networks by lifted proximal operator machines.
\newblock \emph{TPAMI}, 44\penalty0 (6):\penalty0 3334--3348, 2020.

\bibitem[Liao et~al.(2016)Liao, Leibo, and Poggio]{liao2016important}
Liao, Q., Leibo, J., and Poggio, T.
\newblock How important is weight symmetry in backpropagation?
\newblock In \emph{AAAI}, volume~30, 2016.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Cownden, Tweed, and
  Akerman]{lillicrap2016random}
Lillicrap, T.~P., Cownden, D., Tweed, D.~B., and Akerman, C.~J.
\newblock Random synaptic feedback weights support error backpropagation for
  deep learning.
\newblock \emph{Nature communications}, 7\penalty0 (1):\penalty0 13276, 2016.

\bibitem[Lillicrap et~al.(2020)Lillicrap, Santoro, Marris, Akerman, and
  Hinton]{lillicrap2020backpropagation}
Lillicrap, T.~P., Santoro, A., Marris, L., Akerman, C.~J., and Hinton, G.
\newblock Backpropagation and the brain.
\newblock \emph{Nature Reviews Neuroscience}, 21\penalty0 (6):\penalty0
  335--346, 2020.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu2021swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{ICCV}, pp.\  10012--10022, 2021.

\bibitem[L{\"o}we et~al.(2019)L{\"o}we, O'Connor, and Veeling]{lowe2019putting}
L{\"o}we, S., O'Connor, P., and Veeling, B.
\newblock Putting an end to end-to-end: Gradient-isolated learning of
  representations.
\newblock \emph{NeurIPS}, 32, 2019.

\bibitem[Mostafa et~al.(2018)Mostafa, Ramesh, and
  Cauwenberghs]{mostafa2018deep}
Mostafa, H., Ramesh, V., and Cauwenberghs, G.
\newblock Deep supervised learning using local errors.
\newblock \emph{Frontiers in neuroscience}, 12:\penalty0 608, 2018.

\bibitem[N{\o}kland(2016)]{nokland2016direct}
N{\o}kland, A.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock \emph{NeurIPS}, 29, 2016.

\bibitem[N{\o}kland \& Eidnes(2019)N{\o}kland and Eidnes]{nokland2019training}
N{\o}kland, A. and Eidnes, L.~H.
\newblock Training neural networks with local error signals.
\newblock In \emph{ICML}, pp.\  4839--4850. PMLR, 2019.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan2020prevalence}
Papyan, V., Han, X., and Donoho, D.~L.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, 2020.

\bibitem[Ren et~al.(2023)Ren, Kornblith, Liao, and Hinton]{ren2023scaling}
Ren, M., Kornblith, S., Liao, R., and Hinton, G.
\newblock Scaling forward gradient with local losses.
\newblock In \emph{ICLR}, 2023.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{rumelhart1986learning}
Rumelhart, D.~E., Hinton, G.~E., and Williams, R.~J.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Nature}, 323\penalty0 (6088):\penalty0 533--536, 1986.

\bibitem[Shin(2022)]{shin2022effects}
Shin, Y.
\newblock Effects of depth, width, and initialization: A convergence analysis
  of layer-wise training for deep linear neural networks.
\newblock \emph{Analysis and Applications}, 20\penalty0 (01):\penalty0 73--119,
  2022.

\bibitem[Siddiqui et~al.(2023)Siddiqui, Krueger, LeCun, and
  Deny]{siddiqui2023blockwise}
Siddiqui, S.~A., Krueger, D., LeCun, Y., and Deny, S.
\newblock Blockwise self-supervised learning at scale.
\newblock \emph{arXiv preprint arXiv:2302.01647}, 2023.

\bibitem[Silver et~al.(2022)Silver, Goyal, Danihelka, Hessel, and van
  Hasselt]{silver2021learning}
Silver, D., Goyal, A., Danihelka, I., Hessel, M., and van Hasselt, H.
\newblock Learning by directional gradient descent.
\newblock In \emph{ICLR}, 2022.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{ICLR}, 2015.

\bibitem[Song et~al.(2024)Song, Millidge, Salvatori, Lukasiewicz, Xu, and
  Bogacz]{song2024inferring}
Song, Y., Millidge, B., Salvatori, T., Lukasiewicz, T., Xu, Z., and Bogacz, R.
\newblock Inferring neural activity before plasticity as a foundation for
  learning beyond backpropagation.
\newblock \emph{Nature Neuroscience}, pp.\  1--11, 2024.

\bibitem[Taylor et~al.(2016)Taylor, Burmeister, Xu, Singh, Patel, and
  Goldstein]{taylor2016training}
Taylor, G., Burmeister, R., Xu, Z., Singh, B., Patel, A., and Goldstein, T.
\newblock Training neural networks without gradients: A scalable admm approach.
\newblock In \emph{ICML}, pp.\  2722--2731. PMLR, 2016.

\bibitem[Wang et~al.(2021)Wang, Ni, Song, Yang, and Huang]{wang2021revisiting}
Wang, Y., Ni, Z., Song, S., Yang, L., and Huang, G.
\newblock Revisiting locally supervised learning: an alternative to end-to-end
  training.
\newblock In \emph{ICLR}, 2021.

\bibitem[Xiao et~al.(2019)Xiao, Chen, Liao, and
  Poggio]{xiao2018biologicallyplausible}
Xiao, W., Chen, H., Liao, Q., and Poggio, T.
\newblock Biologically-plausible learning algorithms can scale to large
  datasets.
\newblock In \emph{ICLR}, 2019.

\bibitem[Xiong et~al.(2020)Xiong, Ren, and Urtasun]{xiong2020loco}
Xiong, Y., Ren, M., and Urtasun, R.
\newblock Loco: Local contrastive representation learning.
\newblock \emph{NeurIPS}, 33:\penalty0 11142--11153, 2020.

\bibitem[Yang et~al.(2022{\natexlab{a}})Yang, Chen, Li, Xie, Lin, and
  Tao]{yang2022inducing}
Yang, Y., Chen, S., Li, X., Xie, L., Lin, Z., and Tao, D.
\newblock Inducing neural collapse in imbalanced learning: Do we really need a
  learnable classifier at the end of deep neural network?
\newblock \emph{NeurIPS}, 35:\penalty0 37991--38002, 2022{\natexlab{a}}.

\bibitem[Yang et~al.(2022{\natexlab{b}})Yang, Wang, Yuan, and
  Lin]{yang2022towards}
Yang, Y., Wang, H., Yuan, H., and Lin, Z.
\newblock Towards theoretically inspired neural initialization optimization.
\newblock In \emph{NeurIPS}, volume~35, pp.\  18983--18995, 2022{\natexlab{b}}.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Yuan, Li, Lin, Torr, and
  Tao]{yang2023neural}
Yang, Y., Yuan, H., Li, X., Lin, Z., Torr, P., and Tao, D.
\newblock Neural collapse inspired feature-classifier alignment for few-shot
  class-incremental learning.
\newblock In \emph{ICLR}, 2023{\natexlab{a}}.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Yuan, Li, Wu, Zhang, Lin, Torr,
  Tao, and Ghanem]{yang2023neural-extend}
Yang, Y., Yuan, H., Li, X., Wu, J., Zhang, L., Lin, Z., Torr, P., Tao, D., and
  Ghanem, B.
\newblock Neural collapse terminus: A unified solution for class incremental
  learning and its variants.
\newblock \emph{arXiv preprint arXiv:2308.01746}, 2023{\natexlab{b}}.

\bibitem[You et~al.(2020)You, Chen, Wang, and Shen]{you2020l2}
You, Y., Chen, T., Wang, Z., and Shen, Y.
\newblock L2-gcn: Layer-wise and learned efficient training of graph
  convolutional networks.
\newblock In \emph{CVPR}, pp.\  2127--2135, 2020.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar2021barlow}
Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In \emph{ICML}, pp.\  12310--12320. PMLR, 2021.

\bibitem[Zhong et~al.(2023)Zhong, Cui, Yang, Wu, Qi, Zhang, and
  Jia]{zhong2023understanding}
Zhong, Z., Cui, J., Yang, Y., Wu, X., Qi, X., Zhang, X., and Jia, J.
\newblock Understanding imbalanced semantic segmentation through neural
  collapse.
\newblock In \emph{CVPR}, pp.\  19550--19560, 2023.

\bibitem[Zhu et~al.(2021)Zhu, Ding, Zhou, Li, You, Sulam, and
  Qu]{zhu2021geometric}
Zhu, Z., Ding, T., Zhou, J., Li, X., You, C., Sulam, J., and Qu, Q.
\newblock A geometric analysis of neural collapse with unconstrained features.
\newblock \emph{NeurIPS}, 34:\penalty0 29820--29834, 2021.

\end{thebibliography}
