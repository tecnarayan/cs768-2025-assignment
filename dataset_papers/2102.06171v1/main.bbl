\begin{thebibliography}{92}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arpit et~al.(2016)Arpit, Zhou, Kota, and
  Govindaraju]{arpit2016normalization}
Arpit, D., Zhou, Y., Kota, B., and Govindaraju, V.
\newblock Normalization propagation: A parametric technique for removing
  internal covariate shift in deep networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1168--1176, 2016.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Babuschkin et~al.(2020)Babuschkin, Baumli, Bell, Bhupatiraju, Bruce,
  Buchlovsky, Budden, Cai, Clark, Danihelka, Fantacci, Godwin, Jones, Hennigan,
  Hessel, Kapturowski, Keck, Kemaev, King, Martens, Mikulik, Norman, Quan,
  Papamakarios, Ring, Ruiz, Sanchez, Schneider, Sezener, Spencer, Srinivasan,
  Stokowiec, and Viola]{deepmind2020jax}
Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J., Buchlovsky,
  P., Budden, D., Cai, T., Clark, A., Danihelka, I., Fantacci, C., Godwin, J.,
  Jones, C., Hennigan, T., Hessel, M., Kapturowski, S., Keck, T., Kemaev, I.,
  King, M., Martens, L., Mikulik, V., Norman, T., Quan, J., Papamakarios, G.,
  Ring, R., Ruiz, F., Sanchez, A., Schneider, R., Sezener, E., Spencer, S.,
  Srinivasan, S., Stokowiec, W., and Viola, F.
\newblock The {D}eep{M}ind {JAX} {E}cosystem, 2020.
\newblock URL \url{http://github.com/deepmind}.

\bibitem[Bachlechner et~al.(2020)Bachlechner, Majumder, Mao, Cottrell, and
  McAuley]{bachlechner2020rezero}
Bachlechner, T., Majumder, B.~P., Mao, H.~H., Cottrell, G.~W., and McAuley, J.
\newblock Rezero is all you need: Fast convergence at large depth.
\newblock \emph{arXiv preprint arXiv:2003.04887}, 2020.

\bibitem[Balduzzi et~al.(2017)Balduzzi, Frean, Leary, Lewis, Ma, and
  McWilliams]{balduzzi2017shattered}
Balduzzi, D., Frean, M., Leary, L., Lewis, J., Ma, K. W.-D., and McWilliams, B.
\newblock The shattered gradients problem: If resnets are the answer, then what
  is the question?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  342--350, 2017.

\bibitem[Bello(2021)]{bello2021lambdanetworks}
Bello, I.
\newblock Lambdanetworks: Modeling long-range interactions without attention.
\newblock In \emph{International Conference on Learning Representations
  {ICLR}}, 2021.
\newblock URL \url{https://openreview.net/forum?id=xTJEN-ggl1b}.

\bibitem[Bernstein et~al.(2020)Bernstein, Vahdat, Yue, and
  Liu]{bernstein2020distance}
Bernstein, J., Vahdat, A., Yue, Y., and Liu, M.-Y.
\newblock On the distance between two neural networks and the stability of
  learning.
\newblock \emph{arXiv preprint arXiv:2002.03432}, 2020.

\bibitem[Bjorck et~al.(2018)Bjorck, Gomes, Selman, and
  Weinberger]{bjorck2018understanding}
Bjorck, N., Gomes, C.~P., Selman, B., and Weinberger, K.~Q.
\newblock Understanding batch normalization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7694--7705, 2018.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, and Wanderman-Milne]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., and Wanderman-Milne, S.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brock et~al.(2021)Brock, De, and Smith]{brock2021characterizing}
Brock, A., De, S., and Smith, S.~L.
\newblock Characterizing signal propagation to close the performance gap in
  unnormalized resnets.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR}}, 2021.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pp.\
  1597--1607. PMLR, 2020.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{cubuk2020randaugment}
Cubuk, E.~D., Zoph, B., Shlens, J., and Le, Q.~V.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops}, pp.\  702--703, 2020.

\bibitem[De \& Smith(2020)De and Smith]{de2020batch}
De, S. and Smith, S.
\newblock Batch normalization biases residual blocks towards the identity
  function in deep networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR}}, 2021.
\newblock URL \url{https://openreview.net/forum?id=YicbFdNTTy}.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2021sharpnessaware}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR}}, 2021.
\newblock URL \url{https://openreview.net/forum?id=6Tm1mposlrM}.

\bibitem[Gitman \& Ginsburg(2017)Gitman and Ginsburg]{gitman2017comparison}
Gitman, I. and Ginsburg, B.
\newblock Comparison of batch normalization and weight normalization algorithms
  for the large-scale image classification.
\newblock \emph{arXiv preprint arXiv:1709.08145}, 2017.

\bibitem[Gong et~al.(2020)Gong, Ren, Ye, and Liu]{gong2020maxup}
Gong, C., Ren, T., Ye, M., and Liu, Q.
\newblock Maxup: A simple way to improve generalization of neural network
  training.
\newblock \emph{arXiv preprint arXiv:2002.09024}, 2020.

\bibitem[Google(2021)]{tpu_performance}
Google.
\newblock {Cloud TPU Performance Guide}.
\newblock \url{https://cloud.google.com/tpu/docs/performance-guide}, 2021.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Goyal, P., Doll{\'a}r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Gueguen et~al.(2018)Gueguen, Sergeev, Kadlec, Liu, and
  Yosinski]{gueguen2018faster}
Gueguen, L., Sergeev, A., Kadlec, B., Liu, R., and Yosinski, J.
\newblock Faster neural networks straight from jpeg.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 3933--3944, 2018.

\bibitem[Hanin \& Rolnick(2018)Hanin and Rolnick]{hanin2018start}
Hanin, B. and Rolnick, D.
\newblock How to start training: The effect of initialization and architecture.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  571--581, 2018.

\bibitem[Harris et~al.(2020)Harris, Millman, van~der Walt, Gommers, Virtanen,
  Cournapeau, Wieser, Taylor, Berg, Smith, Kern, Picus, Hoyer, van Kerkwijk,
  Brett, Haldane, del R{\'i}o, Wiebe, Peterson, G{\'e}rard-Marchant, Sheppard,
  Reddy, Weckesser, Abbasi, Gohlke, and Oliphant]{harris2020numpy}
Harris, C.~R., Millman, K.~J., van~der Walt, S.~J., Gommers, R., Virtanen, P.,
  Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N.~J., Kern, R.,
  Picus, M., Hoyer, S., van Kerkwijk, M.~H., Brett, M., Haldane, A., del
  R{\'i}o, J.~F., Wiebe, M., Peterson, P., G{\'e}rard-Marchant, P., Sheppard,
  K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T.~E.
\newblock Array programming with numpy.
\newblock \emph{Nature}, 585\penalty0 (7825):\penalty0 357--362, Sep 2020.
\newblock ISSN 1476-4687.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{he2016identity}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European conference on computer vision}, pp.\  630--645.
  Springer, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{he2016resnets}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016{\natexlab{b}}.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  9729--9738, 2020.

\bibitem[He et~al.(2019)He, Zhang, Zhang, Zhang, Xie, and Li]{he2019bag}
He, T., Zhang, Z., Zhang, H., Zhang, Z., Xie, J., and Li, M.
\newblock Bag of tricks for image classification with convolutional neural
  networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  558--567, 2019.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gaussian}
Hendrycks, D. and Gimpel, K.
\newblock Gaussian error linear units ({GELUs}).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Hennigan et~al.(2020)Hennigan, Cai, Norman, and
  Babuschkin]{haiku2020github}
Hennigan, T., Cai, T., Norman, T., and Babuschkin, I.
\newblock {H}aiku: {S}onnet for {JAX}, 2020.
\newblock URL \url{http://github.com/deepmind/dm-haiku}.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer2017train}
Hoffer, E., Hubara, I., and Soudry, D.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1731--1741, 2017.

\bibitem[Hooker(2020)]{hooker2020hardware}
Hooker, S.
\newblock The hardware lottery.
\newblock \emph{arXiv preprint arXiv:2009.06489}, 2020.

\bibitem[Hu et~al.(2018)Hu, Shen, and Sun]{hu2018squeeze}
Hu, J., Shen, L., and Sun, G.
\newblock Squeeze-and-excitation networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  7132--7141, 2018.

\bibitem[Huang et~al.(2016)Huang, Sun, Liu, Sedra, and
  Weinberger]{huang2016deep}
Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger, K.~Q.
\newblock Deep networks with stochastic depth.
\newblock In \emph{European conference on computer vision}, pp.\  646--661.
  Springer, 2016.

\bibitem[Huang et~al.(2017)Huang, Liu, Liu, Lang, and Tao]{huang2017centered}
Huang, L., Liu, X., Liu, Y., Lang, B., and Tao, D.
\newblock Centered weight normalization in accelerating training of deep neural
  networks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  2803--2811, 2017.

\bibitem[Huang et~al.(2020)Huang, Qin, Zhou, Zhu, Liu, and
  Shao]{huang2020normalization}
Huang, L., Qin, J., Zhou, Y., Zhu, F., Liu, L., and Shao, L.
\newblock Normalization techniques in training dnns: Methodology, analysis and
  application.
\newblock \emph{arXiv preprint arXiv:2009.12836}, 2020.

\bibitem[Ioffe(2017)]{ioffe2017batch}
Ioffe, S.
\newblock Batch renormalization: Towards reducing minibatch dependence in
  batch-normalized models.
\newblock \emph{arXiv preprint arXiv:1702.03275}, 2017.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batchnorm}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{ICML}, 2015.

\bibitem[Jacot et~al.(2019)Jacot, Gabriel, and Hongler]{jacot2019freeze}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Freeze and chaos for dnns: an ntk view of batch normalization,
  checkerboard and boundary effects.
\newblock \emph{arXiv preprint arXiv:1907.05715}, 2019.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kolesnikov et~al.(2019)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{kolesnikov2019large}
Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and
  Houlsby, N.
\newblock Large scale learning of general visual representations for transfer.
\newblock \emph{arXiv preprint arXiv:1912.11370}, 2019.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems},
  25:\penalty0 1097--1105, 2012.

\bibitem[LeCun et~al.(2012)LeCun, Bottou, Orr, and
  M{\"u}ller]{lecun2012efficient}
LeCun, Y.~A., Bottou, L., Orr, G.~B., and M{\"u}ller, K.-R.
\newblock Efficient backprop.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  9--48.
  Springer, 2012.

\bibitem[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{loshchilov2016sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and
  Hutter]{loshchilov2017decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Luo et~al.(2018)Luo, Wang, Shao, and Peng]{luo2018towards}
Luo, P., Wang, X., Shao, W., and Peng, Z.
\newblock Towards understanding regularization in batch normalization.
\newblock \emph{arXiv preprint arXiv:1809.00846}, 2018.

\bibitem[Mahajan et~al.(2018)Mahajan, Girshick, Ramanathan, He, Paluri, Li,
  Bharambe, and Van Der~Maaten]{mahajan2018exploring}
Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y.,
  Bharambe, A., and Van Der~Maaten, L.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  {ECCV}}, pp.\  181--196, 2018.

\bibitem[Merity et~al.(2018)Merity, Keskar, and Socher]{merity2018regularizing}
Merity, S., Keskar, N.~S., and Socher, R.
\newblock Regularizing and optimizing {LSTM} language models.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Nesterov(1983)]{nesterov1983}
Nesterov, Y.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence $ o(1/k^2)$.
\newblock \emph{Doklady AN USSR}, pp.\  (269), 543--547, 1983.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Pascanu, R., Mikolov, T., and Bengio, Y.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1310--1318, 2013.

\bibitem[Pham et~al.(2020)Pham, Xie, Dai, and Le]{pham2020meta}
Pham, H., Xie, Q., Dai, Z., and Le, Q.~V.
\newblock Meta pseudo labels.
\newblock \emph{arXiv preprint arXiv:2003.10580}, 2020.

\bibitem[Pham et~al.(2019)Pham, Lutellier, Qi, and Tan]{pham2019cradle}
Pham, H.~V., Lutellier, T., Qi, W., and Tan, L.
\newblock Cradle: cross-backend validation to detect and localize bugs in deep
  learning libraries.
\newblock In \emph{2019 IEEE/ACM 41st International Conference on Software
  Engineering (ICSE)}, pp.\  1027--1038. IEEE, 2019.

\bibitem[Polyak(1964)]{polyak1964some}
Polyak, B.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics}, pp.\
  4(5):1--17, 1964.

\bibitem[Qiao et~al.(2019)Qiao, Wang, Liu, Shen, and Yuille]{qiao2019weight}
Qiao, S., Wang, H., Liu, C., Shen, W., and Yuille, A.
\newblock Weight standardization.
\newblock \emph{arXiv preprint arXiv:1903.10520}, 2019.

\bibitem[Qin et~al.(2020)Qin, Fang, Zhang, Liu, Wang, and
  Wang]{qin2020resizemix}
Qin, J., Fang, J., Zhang, Q., Liu, W., Wang, X., and Wang, X.
\newblock Resizemix: Mixing data with preserved object information and true
  labels.
\newblock \emph{arXiv preprint arXiv:2012.11101}, 2020.

\bibitem[Radford et~al.(2016)Radford, Metz, and Chintala]{radford2016dcgan}
Radford, A., Metz, L., and Chintala, S.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock In \emph{4th International Conference on Learning Representations,
  {ICLR}}, 2016.

\bibitem[Radosavovic et~al.(2020)Radosavovic, Kosaraju, Girshick, He, and
  Doll{\'a}r]{radosavovic2020designing}
Radosavovic, I., Kosaraju, R.~P., Girshick, R., He, K., and Doll{\'a}r, P.
\newblock Designing network design spaces.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  10428--10436, 2020.

\bibitem[Raghu et~al.(2017{\natexlab{a}})Raghu, Gilmer, Yosinski, and
  Sohl-Dickstein]{raghu2017svcca}
Raghu, M., Gilmer, J., Yosinski, J., and Sohl-Dickstein, J.
\newblock Svcca: Singular vector canonical correlation analysis for deep
  learning dynamics and interpretability.
\newblock \emph{Advances in neural information processing systems},
  30:\penalty0 6076--6085, 2017{\natexlab{a}}.

\bibitem[Raghu et~al.(2017{\natexlab{b}})Raghu, Poole, Kleinberg, Ganguli, and
  Sohl-Dickstein]{raghu2017expressive}
Raghu, M., Poole, B., Kleinberg, J., Ganguli, S., and Sohl-Dickstein, J.
\newblock On the expressive power of deep neural networks.
\newblock In \emph{international conference on machine learning}, pp.\
  2847--2854. PMLR, 2017{\natexlab{b}}.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951A}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, pp.\  22(3):400--407,
  1951.

\bibitem[Rota~Bul{\`o} et~al.(2018)Rota~Bul{\`o}, Porzi, and
  Kontschieder]{rota2018place}
Rota~Bul{\`o}, S., Porzi, L., and Kontschieder, P.
\newblock In-place activated batchnorm for memory-optimized training of dnns.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  5639--5647, 2018.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC2015}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock Image{N}et large scale visual recognition challenge.
\newblock \emph{IJCV}, 115:\penalty0 211--252, 2015.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  4510--4520, 2018.

\bibitem[Sandler et~al.(2019)Sandler, Baccash, Zhmoginov, and
  Howard]{sandler2019non}
Sandler, M., Baccash, J., Zhmoginov, A., and Howard, A.
\newblock Non-discriminative data or weak model? on the relative importance of
  data and model resolution.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision Workshops}, pp.\  0--0, 2019.

\bibitem[Santurkar et~al.(2018)Santurkar, Tsipras, Ilyas, and
  Madry]{santurkar2018does}
Santurkar, S., Tsipras, D., Ilyas, A., and Madry, A.
\newblock How does batch normalization help optimization?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2483--2493, 2018.

\bibitem[Shao et~al.(2020)Shao, Hu, Wang, Xue, and Raj]{shao2020normalization}
Shao, J., Hu, K., Wang, C., Xue, X., and Raj, B.
\newblock Is normalization indispensable for training deep neural network?
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Shen et~al.(2020)Shen, Yao, Gholami, Mahoney, and
  Keutzer]{shen2020powernorm}
Shen, S., Yao, Z., Gholami, A., Mahoney, M., and Keutzer, K.
\newblock Powernorm: Rethinking batch normalization in transformers.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8741--8751. PMLR, 2020.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{simonyan2015vgg}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{3rd International Conference on Learning Representations,
  {ICLR}}, 2015.

\bibitem[Singh \& Shrivastava(2019)Singh and Shrivastava]{singh2019evalnorm}
Singh, S. and Shrivastava, A.
\newblock Evalnorm: Estimating batch normalization statistics for evaluation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  3633--3641, 2019.

\bibitem[Smith et~al.(2020)Smith, Elsen, and De]{smith2020generalization}
Smith, S., Elsen, E., and De, S.
\newblock On the generalization benefit of noise in stochastic gradient
  descent.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9058--9067. PMLR, 2020.

\bibitem[Srinivas et~al.(2021)Srinivas, Lin, Parmar, Shlens, Abbeel, and
  Vaswani]{srinivas2021bottleneck}
Srinivas, A., Lin, T.-Y., Parmar, N., Shlens, J., Abbeel, P., and Vaswani, A.
\newblock Bottleneck transformers for visual recognition.
\newblock \emph{arXiv preprint arXiv:2101.11605}, 2021.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Srivastava et~al.(2015)Srivastava, Greff, and
  Schmidhuber]{srivastava2015highway}
Srivastava, R.~K., Greff, K., and Schmidhuber, J.
\newblock Highway networks.
\newblock \emph{arXiv preprint arXiv:1505.00387}, 2015.

\bibitem[Summers \& Dinneen(2019)Summers and Dinneen]{summers2019four}
Summers, C. and Dinneen, M.~J.
\newblock Four things everyone should know to improve batch normalization.
\newblock \emph{arXiv preprint arXiv:1906.03548}, 2019.

\bibitem[Sun et~al.(2017)Sun, Shrivastava, Singh, and Gupta]{sun17revisiting}
Sun, C., Shrivastava, A., Singh, S., and Gupta, A.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In \emph{ICCV}, 2017.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1139--1147, 2013.

\bibitem[Szegedy et~al.(2016{\natexlab{a}})Szegedy, Ioffe, Vanhoucke, and
  Alemi]{szegedy2016inception}
Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A.
\newblock Inception-v4, inception-resnet and the impact of residual connections
  on learning.
\newblock \emph{arXiv preprint arXiv:1602.07261}, 2016{\natexlab{a}}.

\bibitem[Szegedy et~al.(2016{\natexlab{b}})Szegedy, Vanhoucke, Ioffe, Shlens,
  and Wojna]{szegedy2016rethinking}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  2818--2826, 2016{\natexlab{b}}.

\bibitem[Tan \& Le(2019)Tan and Le]{tan2019efficientnet}
Tan, M. and Le, Q.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6105--6114, 2019.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman2012rmsprop}
Tieleman, T. and Hinton, G.
\newblock Rmsprop: Divide the gradient by a running average of its recent
  magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, pp.\
  4(2):26--31, 2012.

\bibitem[Touvron et~al.(2019)Touvron, Vedaldi, Douze, and
  J{\'e}gou]{touvron2019fixing}
Touvron, H., Vedaldi, A., Douze, M., and J{\'e}gou, H.
\newblock Fixing the train-test resolution discrepancy.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8252--8262, 2019.

\bibitem[Touvron et~al.(2020)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{touvron2020training}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou,
  H.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock \emph{arXiv preprint arXiv:2012.12877}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Wu \& He(2018)Wu and He]{wu2018group}
Wu, Y. and He, K.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pp.\  3--19, 2018.

\bibitem[Xie et~al.(2020)Xie, Luong, Hovy, and Le]{xie2020self}
Xie, Q., Luong, M.-T., Hovy, E., and Le, Q.~V.
\newblock Self-training with noisy student improves imagenet classification.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  10687--10698, 2020.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2017aggregated}
Xie, S., Girshick, R., Doll{\'a}r, P., Tu, Z., and He, K.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  1492--1500, 2017.

\bibitem[Yang et~al.(2019)Yang, Pennington, Rao, Sohl-Dickstein, and
  Schoenholz]{yang2019mean}
Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., and Schoenholz, S.~S.
\newblock A mean field theory of batch normalization.
\newblock \emph{arXiv preprint arXiv:1902.08129}, 2019.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017large}
You, Y., Gitman, I., and Ginsburg, B.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[You et~al.(2019)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{you2019large}
You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X.,
  Demmel, J., Keutzer, K., and Hsieh, C.-J.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR}}, 2019.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{yun2019cutmix}
Yun, S., Han, D., Oh, S.~J., Chun, S., Choe, J., and Yoo, Y.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  6023--6032, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Dauphin, and
  Ma]{zhang2019fixup}
Zhang, H., Dauphin, Y.~N., and Ma, T.
\newblock Fixup initialization: Residual learning without normalization.
\newblock \emph{arXiv preprint arXiv:1901.09321}, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Goodfellow, Metaxas, and
  Odena]{zhang2019self}
Zhang, H., Goodfellow, I., Metaxas, D., and Odena, A.
\newblock Self-attention generative adversarial networks.
\newblock In \emph{International conference on machine learning}, pp.\
  7354--7363. PMLR, 2019{\natexlab{b}}.

\bibitem[Zhang et~al.(2020)Zhang, He, Sra, and Jadbabaie]{zhang2019gradient}
Zhang, J., He, T., Sra, S., and Jadbabaie, A.
\newblock Why gradient clipping accelerates training: A theoretical
  justification for adaptivity.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR}}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJgnXpVYwS}.

\end{thebibliography}
