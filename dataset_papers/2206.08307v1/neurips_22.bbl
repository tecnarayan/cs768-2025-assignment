\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal and Duchi(2011)]{Agarwal2011:delayed}
Alekh Agarwal and John~C Duchi.
\newblock
  \href{http://papers.nips.cc/paper/4247-distributed-delayed-stochastic-optimization.pdf}{Distributed
  delayed stochastic optimization}.
\newblock In \emph{Advances in Neural Information Processing Systems 24}, pages
  873--881. Curran Associates, Inc., 2011.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{Alistarh2017:qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock
  \href{http://papers.nips.cc/paper/6768-qsgd-communication-efficient-sgd-via-gradient-quantization-and-encoding.pdf}{{QSGD}:
  Communication-efficient {SGD} via gradient quantization and encoding}.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pages
  1709--1720. Curran Associates, Inc., 2017.

\bibitem[Alistarh et~al.(2018)Alistarh, Hoefler, Johansson, Konstantinov,
  Khirirat, and Renggli]{Alistarh2018:topk}
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Nikola Konstantinov, Sarit
  Khirirat, and Cedric Renggli.
\newblock
  \href{http://papers.nips.cc/paper/7837-the-convergence-of-sparsified-gradient-methods.pdf}{The
  convergence of sparsified gradient methods}.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{NeurIPS - Advances in Neural Information
  Processing Systems 31}, pages 5977--5987. Curran Associates, Inc., 2018.

\bibitem[Arjevani et~al.(2019)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{Arjevani2019:LowerBoundSGD}
Yossi Arjevani, Yair Carmon, John~C. Duchi, Dylan~J. Foster, Nathan Srebro, and
  Blake~E. Woodworth.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{ArXiv}, abs/1912.02365, 2019.

\bibitem[Arjevani et~al.(2020)Arjevani, Shamir, and
  Srebro]{arjevani20:delayedSGD}
Yossi Arjevani, Ohad Shamir, and Nathan Srebro.
\newblock \href{https://proceedings.mlr.press/v117/arjevani20a.html}{A tight
  convergence analysis for stochastic gradient descent with delayed updates}.
\newblock In Aryeh Kontorovich and Gergely Neu, editors, \emph{Proceedings of
  the 31st International Conference on Algorithmic Learning Theory}, volume 117
  of \emph{Proceedings of Machine Learning Research}, pages 111--132. PMLR, 08
  Feb--11 Feb 2020.

\bibitem[Assran et~al.(2019)Assran, Loizou, Ballas, and
  Rabbat]{Assran:2018sdggradpush}
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Michael Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}. PMLR, 2019.

\bibitem[Avdiukhin and Kasiviswanathan(2021)]{Avdiukhin21:FL_delayed}
Dmitrii Avdiukhin and Shiva Kasiviswanathan.
\newblock \href{https://proceedings.mlr.press/v139/avdiukhin21a.html}{Federated
  learning under arbitrary communication patterns}.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 425--435. PMLR, 18--24
  Jul 2021.

\bibitem[Aviv et~al.(2021)Aviv, Hakimi, Schuster, and
  Levy]{Aviv21:delayed_average}
Rotem~Zamir Aviv, Ido Hakimi, Assaf Schuster, and Kfir~Yehuda Levy.
\newblock Learning under delayed feedback: Implicitly adapting to gradient
  delays.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}. PMLR, 2021.

\bibitem[Aytekin et~al.(2016)Aytekin, Feyzmahdavian, and
  Johansson]{Arda16:distributed_async}
Arda Aytekin, Hamid~Reza Feyzmahdavian, and Mikael Johansson.
\newblock \href{https://arxiv.org/abs/1610.05507}{Analysis and implementation
  of an asynchronous optimization algorithm for the parameter server}, 2016.

\bibitem[Bertsekas and Tsitsiklis(1989)]{Bertsekas1989:parallel}
D.P. Bertsekas and J.N. Tsitsiklis.
\newblock \emph{Parallel and Distributed Computation: Numerical Methods}.
\newblock Prentice-Hall, 1989.

\bibitem[Bonawitz et~al.(2019)Bonawitz, Eichner, Grieskamp, Huba, Ingerman,
  Ivanov, Kiddon, Kone\v{c}n\'{y}, Mazzocchi, McMahan, Van~Overveldt, Petrou,
  Ramage, and Roselander]{Bonawitz19:over-selection}
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
  Ingerman, Vladimir Ivanov, Chlo\'{e} Kiddon, Jakub Kone\v{c}n\'{y}, Stefano
  Mazzocchi, Brendan McMahan, Timon Van~Overveldt, David Petrou, Daniel Ramage,
  and Jason Roselander.
\newblock
  \href{https://proceedings.mlsys.org/paper/2019/file/bd686fd640be98efaae0091fa301e613-Paper.pdf}{Towards
  federated learning at scale: System design}.
\newblock In A.~Talwalkar, V.~Smith, and M.~Zaharia, editors, \emph{Proceedings
  of Machine Learning and Systems}, volume~1, pages 374--388, 2019.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{Bottou2018:book}
L.~Bottou, F.~Curtis, and J.~Nocedal.
\newblock \href{https://doi.org/10.1137/16M1080173}{Optimization methods for
  large-scale machine learning}.
\newblock \emph{SIAM Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Chaturapruek et~al.(2015)Chaturapruek, Duchi, and
  R\'{e}]{Chaturapruek2015:noise}
Sorathan Chaturapruek, John~C Duchi, and Christopher R\'{e}.
\newblock
  \href{http://papers.nips.cc/paper/6031-asynchronous-stochastic-convex-optimization-the-noise-is-in-the-noise-and-sgd-dont-care.pdf}{Asynchronous
  stochastic convex optimization: the noise is in the noise and {SGD} don't
  care}.
\newblock In \emph{Advances in Neural Information Processing Systems 28}, pages
  1531--1539. Curran Associates, Inc., 2015.

\bibitem[Cohen et~al.(2021)Cohen, Daniely, Drori, Koren, and
  Schain]{Cohen2021_pickySGD}
Alon Cohen, Amit Daniely, Yoel Drori, Tomer Koren, and Mariano Schain.
\newblock
  \href{https://proceedings.neurips.cc/paper/2021/file/4b85256c4881edb6c0776df5d81f6236-Paper.pdf}{Asynchronous
  stochastic optimization robust to arbitrary delays}.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 9024--9035. Curran Associates, Inc., 2021.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Ranzato,
  Senior, Tucker, Yang, Le, and Ng]{dean2012large}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Marc'aurelio Ranzato, Andrew Senior, Paul Tucker, Ke~Yang, Quoc~V. Le, and
  Andrew~Y. Ng.
\newblock Large scale distributed deep networks.
\newblock In \emph{NIPS - Advances in Neural Information Processing Systems},
  pages 1223--1231, 2012.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{Dekel2012:minibatch}
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.
\newblock \href{http://dl.acm.org/citation.cfm?id=2503308.2188391}{Optimal
  distributed online prediction using mini-batches}.
\newblock \emph{Journal of Machine Learning Resesearch (JMLR)}, 13\penalty0
  (1):\penalty0 165--202, 2012.

\bibitem[Dutta et~al.(2018)Dutta, Joshi, Ghosh, Dube, and
  Nagpurkar]{Dutta18:stale_gradients}
Sanghamitra Dutta, Gauri Joshi, Soumyadip Ghosh, Parijat Dube, and Priya
  Nagpurkar.
\newblock \href{http://proceedings.mlr.press/v84/dutta18a/dutta18a.pdf}{Slow
  and stale gradients can win the race: Error-runtime trade-offs in distributed
  sgd}.
\newblock In \emph{Proceedings of the Twenty-First International Conference on
  Artificial Intelligence and Statistics}, pages 803--812. PMLR, 2018.

\bibitem[Even et~al.(2021)Even, Hendrikx, and
  Massoulie]{Mathieu21:continuous_time_decentralized_delays}
Mathieu Even, Hadrien Hendrikx, and Laurent Massoulie.
\newblock \href{https://arxiv.org/abs/2106.03585}{Decentralized optimization
  with heterogeneous delays: a continuous-time approach}, 2021.

\bibitem[Feyzmahdavian et~al.(2016)Feyzmahdavian, Aytekin, and
  Johansson]{Feyzmahdavian2016:async}
H.~R. Feyzmahdavian, A.~Aytekin, and M.~Johansson.
\newblock An asynchronous mini-batch algorithm for regularized stochastic
  optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 61\penalty0
  (12):\penalty0 3740--3754, Dec 2016.
\newblock ISSN 0018-9286.

\bibitem[Ghadimi and Lan(2013)]{Ghadimi2013:SGDrate}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM J. Optim.}, 23:\penalty0 2341--2368, 2013.

\bibitem[Glasgow and
  Wootters(2020)]{Glasgow20:async_variance_reduced_distributed}
Margalit Glasgow and Mary Wootters.
\newblock \href{https://arxiv.org/abs/2009.10717}{Asynchronous distributed
  optimization with stochastic delays}, 2020.

\bibitem[Gu et~al.(2021)Gu, Huang, Zhang, and Huang]{Xinran21:MIFA}
Xinran Gu, Kaixuan Huang, Jingzhao Zhang, and Longbo Huang.
\newblock
  \href{https://proceedings.neurips.cc/paper/2021/file/64be20f6dd1dd46adf110cf871e3ed35-Paper.pdf}{Fast
  federated learning in the presence of arbitrary device unavailability}.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 12052--12064. Curran Associates, Inc., 2021.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, D'Oliveira, Eichner, Rouayheb, Evans,
  Gardner, Garrett, Gascón, Ghazi, Gibbons, Gruteser, Harchaoui, He, He, Huo,
  Hutchinson, Hsu, Jaggi, Javidi, Joshi, Khodak, Konečný, Korolova,
  Koushanfar, Koyejo, Lepoint, Liu, Mittal, Mohri, Nock, Özgür, Pagh,
  Raykova, Qi, Ramage, Raskar, Song, Song, Stich, Sun, Suresh, Tramèr,
  Vepakomma, Wang, Xiong, Xu, Yang, Yu, Yu, and Zhao]{Kairouz2019:federated}
Peter Kairouz, H.~Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
  Rachel Cummings, Rafael G.~L. D'Oliveira, Hubert Eichner, Salim~El Rouayheb,
  David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi,
  Phillip~B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He,
  Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri
  Joshi, Mikhail Khodak, Jakub Konečný, Aleksandra Korolova, Farinaz
  Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal,
  Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova,
  Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song, Sebastian~U.
  Stich, Ziteng Sun, Ananda~Theertha Suresh, Florian Tramèr, Praneeth
  Vepakomma, Jianyu Wang, Li~Xiong, Zheng Xu, Qiang Yang, Felix~X. Yu, Han Yu,
  and Sen Zhao.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends® in Machine Learning}, 14\penalty0
  (1--2):\penalty0 1--210, 2021.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{Karimireddy19:scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank~J. Reddi,
  Sebastian~U. Stich, and Ananda~Theertha Suresh.
\newblock \href{http://arxiv.org/abs/1910.06378}{{SCAFFOLD:} stochastic
  controlled averaging for on-device federated learning}.
\newblock \emph{CoRR}, abs/1910.06378, 2019.

\bibitem[Koloskova et~al.(2020)Koloskova, Loizou, Boreiri, Jaggi, and
  Stich]{koloskova2020unified}
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and
  Sebastian~U. Stich.
\newblock A unified theory of decentralized sgd with changing topology and
  local updates.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML)}. PMLR, 2020.

\bibitem[Leblond et~al.(2018)Leblond, Pedregosa, and
  Lacoste-Julien]{Leblond18:async_svrg_saga_etc}
Remi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien.
\newblock \href{http://jmlr.org/papers/v19/17-650.html}{Improved asynchronous
  parallel optimization analysis for stochastic incremental methods}.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0
  (81):\penalty0 1--68, 2018.

\bibitem[Lian et~al.(2015)Lian, Huang, Li, and Liu]{lian2015asynchronous}
Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji~Liu.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2737--2745, 2015.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{Lian2017:decentralizedSGD}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock
  \href{http://papers.nips.cc/paper/7117-can-decentralized-algorithms-outperform-centralized-algorithms-a-case-study-for-decentralized-parallel-stochastic-gradient-descent.pdf}{Can
  decentralized algorithms outperform centralized algorithms? a case study for
  decentralized parallel stochastic gradient descent}.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pages
  5330--5340. Curran Associates, Inc., 2017.

\bibitem[Mangasarian and Solodov(1994)]{Mangasarian1994}
Olvi~L. Mangasarian and Mikhail~V. Solodov.
\newblock Backpropagation convergence via deterministic nonmonotone perturbed
  minimization.
\newblock In J.~Cowan, G.~Tesauro, and J.~Alspector, editors, \emph{Advances in
  Neural Information Processing Systems}, volume~6. Morgan-Kaufmann, 1994.

\bibitem[Mania et~al.(2017)Mania, Pan, Papailiopoulos, Recht, Ramchandran, and
  Jordan]{mania2017:perturbed_iterate}
Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan
  Ramchandran, and Michael~I. Jordan.
\newblock \href{https://doi.org/10.1137/16M1057000}{Perturbed iterate analysis
  for asynchronous stochastic optimization}.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2202--2229, 2017.
\newblock \doi{10.1137/16M1057000}.

\bibitem[McDonald et~al.(2010)McDonald, Hall, and
  Mann]{mcdonald2010distributed}
Ryan McDonald, Keith Hall, and Gideon Mann.
\newblock Distributed training strategies for the structured perceptron.
\newblock In \emph{Human Language Technologies: The 2010 Annual Conference of
  the North American Chapter of the Association for Computational Linguistics},
  pages 456--464. Association for Computational Linguistics, 2010.

\bibitem[McMahan and Streeter(2014)]{McMahan14:delay_adaptive_online}
Brendan McMahan and Matthew Streeter.
\newblock
  \href{https://proceedings.neurips.cc/paper/2014/file/5cce8dede893813f879b873962fb669f-Paper.pdf}{Delay-tolerant
  algorithms for asynchronous distributed online learning}.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~Lawrence, and K.Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~27. Curran Associates, Inc., 2014.

\bibitem[McMahan et~al.(2016)McMahan, Moore, Ramage, and
  y~Arcas]{McMahan16:FedLearning}
H.~Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise~Ag{\"{u}}era
  y~Arcas.
\newblock \href{http://arxiv.org/abs/1602.05629}{Federated learning of deep
  networks using model averaging}.
\newblock \emph{arXiv preprint arXiv:1602.05629}, 2016.

\bibitem[Mishchenko et~al.(2022)Mishchenko, Bach, Even, and
  Woodworth]{Mishchenko22:async}
Konstantin Mishchenko, Francis Bach, Mathieu Even, and Blake Woodworth.
\newblock \href{https://arxiv.org/abs/2206.07638}{Asynchronous {SGD} beats
  minibatch {SGD} under arbitrary delays}, 2022.

\bibitem[Nedić(2020)]{Nedic2020:survey}
Angelia Nedić.
\newblock Distributed gradient methods for convex machine learning problems in
  networks: Distributed optimization.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (3):\penalty0
  92--101, 2020.

\bibitem[Nguyen et~al.(2022)Nguyen, Malik, Zhan, Yousefpour, Rabbat, Malek, and
  Huba]{Nguyen22:FedBuff}
John Nguyen, Kshitiz Malik, Hongyua Zhan, Ashka Yousefpour, Mike Rabbat, Mani
  Malek, and Dzmitry Huba.
\newblock Federated learning with buffered asynchronous aggregation.
\newblock In \emph{Proceedings of The 25th International Conference on
  Artificial Intelligence and Statistics}. PMLR, 2022.

\bibitem[Nguyen et~al.(2018)Nguyen, Nguyen, van Dijk, Richtarik, Scheinberg,
  and Takac]{nguyen18:SGD_and_hogwild_without_bounded_gradient}
Lam Nguyen, Phuong~Ha Nguyen, Marten van Dijk, Peter Richtarik, Katya
  Scheinberg, and Martin Takac.
\newblock \href{https://proceedings.mlr.press/v80/nguyen18c.html}{{SGD} and
  {Hogwild!} {C}onvergence without the bounded gradients assumption}.
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 3750--3758. PMLR,
  10--15 Jul 2018.

\bibitem[Niu et~al.(2011)Niu, Recht, Re, and Wright]{Niu2011:hogwild}
Feng Niu, Benjamin Recht, Christopher Re, and Stephen~J. Wright.
\newblock \href{http://dl.acm.org/citation.cfm?id=2986459.2986537}{{HOGWILD!}:
  A lock-free approach to parallelizing stochastic gradient descent}.
\newblock In \emph{Proceedings of the 24th International Conference on Neural
  Information Processing Systems}, pages 693--701. Curran Associates Inc.,
  2011.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{International Conference on Machine Learning}, pages
  8821--8831. PMLR, 2021.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and
  Chen]{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 2022.

\bibitem[Recht et~al.(2011)Recht, Re, Wright, and Niu]{Recht2011:hogwild}
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.
\newblock
  \href{http://papers.nips.cc/paper/4390-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.pdf}{Hogwild:
  A lock-free approach to parallelizing stochastic gradient descent}.
\newblock In J.~Shawe-Taylor, R.~S. Zemel, P.~L. Bartlett, F.~Pereira, and
  K.~Q. Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems 24}, pages 693--701. Curran Associates, Inc., 2011.

\bibitem[Robbins and Monro(1951)]{Robbins:1951sgd}
Herbert Robbins and Sutton Monro.
\newblock {A Stochastic Approximation Method}.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400--407, September 1951.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Sra et~al.(2016)Sra, Yu, Li, and Smola]{sra16:adadelay}
Suvrit Sra, Adams~Wei Yu, Mu~Li, and Alex Smola.
\newblock \href{http://proceedings.mlr.press/v51/sra16.html}{Adadelay: Delay
  adaptive distributed stochastic optimization}.
\newblock In \emph{Proceedings of the 19th International Conference on
  Artificial Intelligence and Statistics}, volume~51 of \emph{Proceedings of
  Machine Learning Research}, pages 957--965. PMLR, 2016.

\bibitem[Stich et~al.(2021)Stich, Mohtashami, and
  Jaggi]{Stich20:critical_params}
Sebastian Stich, Amirkeivan Mohtashami, and Martin Jaggi.
\newblock
  \href{http://proceedings.mlr.press/v130/stich21a/stich21a.pdf}{Critical
  parameters for scalable distributed learning with large batches and
  asynchronous updates}.
\newblock In \emph{Proceedings of The 24th International Conference on
  Artificial Intelligence and Statistics}, pages 4042--4050. PMLR, 2021.

\bibitem[Stich(2019)]{Stich2018:LocalSGD}
Sebastian~U. Stich.
\newblock \href{https://arxiv.org/abs/1805.09767}{Local {SGD} converges fast
  and communicates little}.
\newblock \emph{ICLR - International Conference on Learning Representations},
  art. arXiv:1805.09767, 2019.

\bibitem[Stich and Karimireddy(2020)]{Stich20:error-feedback}
Sebastian~U. Stich and Sai~Praneeth Karimireddy.
\newblock \href{http://jmlr.org/papers/v21/19-748.html}{The error-feedback
  framework: {SGD} with delayed gradients}.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (237):\penalty0 1--36, 2020.

\bibitem[Vogels et~al.(2019)Vogels, Karimireddy, and Jaggi]{Vogels19:power}
Thijs Vogels, Sai~Praneeth Karimireddy, and Martin Jaggi.
\newblock
  \href{http://papers.nips.cc/paper/7434-accelerated-stochastic-matrix-inversion-general-theory-and-speeding-up-bfgs-rules-for-faster-second-order-optimization.pdf}{{PowerSGD}:
  Practical low-rank gradient compression for distributed optimization}.
\newblock In \emph{Advances in Neural Information Processing Systems 32
  (NeurIPS)}, pages 1626--1636. Curran Associates, Inc., 2019.

\bibitem[Wang et~al.(2020)Wang, Fu, He, Hao, and Wu]{wang2020survey}
Meng Wang, Weijie Fu, Xiangnan He, Shijie Hao, and Xindong Wu.
\newblock A survey on large-scale machine learning.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 2020.

\bibitem[Wu et~al.(2022)Wu, Magnusson, Feyzmahdavian, and
  Johansson]{Xuyang22:delay-adaptive-stepsizes}
Xuyang Wu, Sindri Magnusson, Hamid~Reza Feyzmahdavian, and Mikael Johansson.
\newblock \href{https://arxiv.org/abs/2202.08550}{Delay-adaptive step-sizes for
  asynchronous learning}, 2022.

\bibitem[Yan et~al.(2020)Yan, Niu, Ding, Zheng, Wu, Chen, Tang, and
  Wu]{Yikai20:unavailable_devices_cd}
Yikai Yan, Chaoyue Niu, Yucheng Ding, Zhenzhe Zheng, Fan Wu, Guihai Chen,
  Shaojie Tang, and Zhihua Wu.
\newblock \href{https://arxiv.org/abs/2002.07399}{Distributed non-convex
  optimization with sublinear speedup under intermittent client availability},
  2020.

\bibitem[Yang et~al.(2021)Yang, Zhang, Khanduri, and Liu]{Haibo21:anarchicFL}
Haibo Yang, Xin Zhang, Prashant Khanduri, and Jia Liu.
\newblock \href{https://arxiv.org/abs/2108.09875}{Anarchic federated learning},
  2021.

\bibitem[Zhang et~al.(2016)Zhang, Gupta, Lian, and
  Liu]{Zhang16:async_weight_down_lr}
Wei Zhang, Suyog Gupta, Xiangru Lian, and Ji~Liu.
\newblock
  \href{https://www.ijcai.org/Proceedings/16/Papers/335.pdf}{Staleness-aware
  async-sgd for distributed deep learning}.
\newblock In \emph{Proceedings of the Twenty-Fifth International Joint
  Conference on Artificial Intelligence (IJCAI-16)}, 2016.

\bibitem[Zheng et~al.(2017)Zheng, Meng, Wang, Chen, Yu, Ma, and
  Liu]{Zheng17:delay_compensation}
Shuxin Zheng, Qi~Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma, and
  Tie-Yan Liu.
\newblock Asynchronous stochastic gradient descent with delay compensation.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}. PMLR, 2017.

\bibitem[Zinkevich et~al.(2010)Zinkevich, Weimer, Li, and
  Smola]{zinkevich2010parallelized}
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex~J Smola.
\newblock Parallelized stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2595--2603, 2010.

\end{thebibliography}
