\begin{thebibliography}{77}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ainsworth et~al.(2022)Ainsworth, Hayase, and
  Srinivasa]{ainsworth2022git}
Ainsworth, S.~K., Hayase, J., and Srinivasa, S.
\newblock Git re-basin: Merging models modulo permutation symmetries.
\newblock \emph{arXiv preprint arXiv:2209.04836}, 2022.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock \emph{NeurIPS}, 30:\penalty0 1709--1720, 2017.

\bibitem[Baxter(2000)]{baxter2000model}
Baxter, J.
\newblock A model of inductive bias learning.
\newblock \emph{JAIR}, 12:\penalty0 149--198, 2000.

\bibitem[Bibikar et~al.(2022)Bibikar, Vikalo, Wang, and Chen]{feddst}
Bibikar, S., Vikalo, H., Wang, Z., and Chen, X.
\newblock Federated dynamic sparse training: Computing less, communicating
  less, yet learning better.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pp.\  6080--6088, 2022.

\bibitem[Briggs et~al.(2020)Briggs, Fan, and Andras]{briggs2020federated}
Briggs, C., Fan, Z., and Andras, P.
\newblock Federated learning with hierarchical clustering of local updates to
  improve training on non-iid data.
\newblock In \emph{IJCNN}, pp.\  1--9. IEEE, 2020.

\bibitem[Caldas et~al.(2018)Caldas, Duddu, Wu, Li, Kone{\v{c}}n{\'y}, McMahan,
  Smith, and Talwalkar]{caldas2018leaf}
Caldas, S., Duddu, S. M.~K., Wu, P., Li, T., Kone{\v{c}}n{\'y}, J., McMahan,
  H.~B., Smith, V., and Talwalkar, A.
\newblock Leaf: A benchmark for federated settings.
\newblock \emph{arXiv preprint arXiv:1812.01097}, 2018.

\bibitem[Chai et~al.(2019)Chai, Fayyaz, Fayyaz, Anwar, Zhou, Baracaldo, Ludwig,
  and Cheng]{chai2019towards}
Chai, Z., Fayyaz, H., Fayyaz, Z., Anwar, A., Zhou, Y., Baracaldo, N., Ludwig,
  H., and Cheng, Y.
\newblock Towards taming the resource and data heterogeneity in federated
  learning.
\newblock In \emph{2019 USENIX Conference on Operational Machine Learning (OpML
  19)}, pp.\  19--21, 2019.

\bibitem[Chai et~al.(2020)Chai, Ali, Zawad, Truex, Anwar, Baracaldo, Zhou,
  Ludwig, Yan, and Cheng]{chai2020tifl}
Chai, Z., Ali, A., Zawad, S., Truex, S., Anwar, A., Baracaldo, N., Zhou, Y.,
  Ludwig, H., Yan, F., and Cheng, Y.
\newblock Tifl: A tier-based federated learning system.
\newblock In \emph{Proceedings of the 29th International Symposium on
  High-Performance Parallel and Distributed Computing}, pp.\  125--136, 2020.

\bibitem[Chen et~al.(2022)Chen, Gao, Kuang, Li, and Ding]{chen2022pflbench}
Chen, D., Gao, D., Kuang, W., Li, Y., and Ding, B.
\newblock p{FL}-bench: A comprehensive benchmark for personalized federated
  learning.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Chen et~al.(2023)Chen, Gao, Xie, Pan, Li, Li, Ding, and
  Zhou]{chen2023FSReal}
Chen, D., Gao, D., Xie, Y., Pan, X., Li, Z., Li, Y., Ding, B., and Zhou, J.
\newblock {FS}-{REAL}: Towards real-world cross-device federated learning.
\newblock In \emph{KDD}, 2023.

\bibitem[Cohen et~al.(2017)Cohen, Afshar, Tapson, and
  Van~Schaik]{cohen2017emnist}
Cohen, G., Afshar, S., Tapson, J., and Van~Schaik, A.
\newblock Emnist: Extending mnist to handwritten letters.
\newblock In \emph{IJCNN}, pp.\  2921--2926. IEEE, 2017.

\bibitem[Collins et~al.(2021)Collins, Hassani, Mokhtari, and
  Shakkottai]{Liam2021Shared}
Collins, L., Hassani, H., Mokhtari, A., and Shakkottai, S.
\newblock Exploiting shared representations for personalized federated
  learning.
\newblock In \emph{ICML}, volume 139, pp.\  2089--2099, 18--24 Jul 2021.

\bibitem[Corinzia \& Buhmann(2019)Corinzia and
  Buhmann]{corinzia2019variational}
Corinzia, L. and Buhmann, J.~M.
\newblock Variational federated multi-task learning, 2019.

\bibitem[Deng et~al.(2020)Deng, Li, Han, Shi, and Xie]{deng2020model}
Deng, L., Li, G., Han, S., Shi, L., and Xie, Y.
\newblock Model compression and hardware acceleration for neural networks: A
  comprehensive survey.
\newblock \emph{Proceedings of the IEEE}, 108\penalty0 (4):\penalty0 485--532,
  2020.

\bibitem[Diao et~al.(2021)Diao, Ding, and
  Tarokh]{diaoHeteroFLComputationCommunication2020}
Diao, E., Ding, J., and Tarokh, V.
\newblock Heterofl: Computation and communication efficient federated learning
  for heterogeneous clients.
\newblock In \emph{ICLR}, 2021.

\bibitem[Dinh et~al.(2020{\natexlab{a}})Dinh, Tran, and
  Nguyen]{Dinh2020PersonalizedFL}
Dinh, C.~T., Tran, N.~H., and Nguyen, T.~D.
\newblock Personalized federated learning with moreau envelopes.
\newblock \emph{ArXiv}, abs/2006.08848, 2020{\natexlab{a}}.

\bibitem[Dinh et~al.(2020{\natexlab{b}})Dinh, Tran, and
  Nguyen]{dinh2020personalized}
Dinh, C.~T., Tran, N.~H., and Nguyen, T.~D.
\newblock Personalized federated learning with moreau envelopes.
\newblock In \emph{NeurIPS}, 2020{\natexlab{b}}.

\bibitem[Fallah et~al.(2020)Fallah, Mokhtari, and
  Ozdaglar]{fallahPersonalizedFederatedLearning2020}
Fallah, A., Mokhtari, A., and Ozdaglar, A.
\newblock Personalized federated learning with theoretical guarantees: A
  model-agnostic meta-learning approach.
\newblock In \emph{NeurIPS}, volume~33, pp.\  3557--3568, 2020.

\bibitem[Ghosh et~al.(2020)Ghosh, Chung, Yin, and
  Ramchandran]{ghoshEfficientFrameworkClustered2020a}
Ghosh, A., Chung, J., Yin, D., and Ramchandran, K.
\newblock An efficient framework for clustered federated learning.
\newblock In \emph{NeurIPS}, volume~33, 2020.

\bibitem[Haddadpour et~al.(2021)Haddadpour, Kamani, Mokhtari, and
  Mahdavi]{haddadpour2021federated}
Haddadpour, F., Kamani, M.~M., Mokhtari, A., and Mahdavi, M.
\newblock Federated learning with compression: Unified analysis and sharp
  guarantees.
\newblock In \emph{AISTATS}, pp.\  2350--2358. PMLR, 2021.

\bibitem[Hamer et~al.(2020)Hamer, Mohri, and Suresh]{hamer2020fedboost}
Hamer, J., Mohri, M., and Suresh, A.~T.
\newblock Fedboost: A communication-efficient algorithm for federated learning.
\newblock In \emph{ICML}, pp.\  3973--3983. PMLR, 2020.

\bibitem[Hardy et~al.(1952)Hardy, Littlewood, P{\'o}lya, P{\'o}lya,
  et~al.]{hardy1952inequalities}
Hardy, G.~H., Littlewood, J.~E., P{\'o}lya, G., P{\'o}lya, G., et~al.
\newblock \emph{Inequalities}.
\newblock Cambridge university press, 1952.

\bibitem[He et~al.(2020)He, Annavaram, and
  Avestimehr]{annavaramGroupKnowledgeTransfer}
He, C., Annavaram, M., and Avestimehr, S.
\newblock Group knowledge transfer: Federated learning of large cnns at the
  edge.
\newblock \emph{NeurIPS}, 33, 2020.

\bibitem[Hong et~al.(2021)Hong, Zhu, Yu, Wang, Dodge, and
  Zhou]{hong2021federated}
Hong, J., Zhu, Z., Yu, S., Wang, Z., Dodge, H., and Zhou, J.
\newblock Federated adversarial debiasing for fair and transferable
  representations.
\newblock In \emph{KDD}, 2021.

\bibitem[Hong et~al.(2022)Hong, Wang, Wang, and Zhou]{hong2022efficient}
Hong, J., Wang, H., Wang, Z., and Zhou, J.
\newblock Efficient split-mix federated learning for on-demand and in-situ
  customization.
\newblock In \emph{ICLR}, 2022.

\bibitem[Huang et~al.(2022)Huang, Liu, Li, He, Lin, and Tao]{huang2022FedSpa}
Huang, T., Liu, S., Li, S., He, F., Lin, W., and Tao, D.
\newblock Achieving personalized federated learning with sparse local models.
\newblock \emph{ArXiv}, abs/2201.11380, 2022.

\bibitem[Huang et~al.(2021)Huang, Chu, Zhou, Wang, Liu, Pei, and
  Zhang]{huangPersonalizedFederatedLearning2020}
Huang, Y., Chu, L., Zhou, Z., Wang, L., Liu, J., Pei, J., and Zhang, Y.
\newblock Personalized cross-silo federated learning on non-iid data.
\newblock In \emph{AAAI}, volume~35, pp.\  7865--7873, 2021.

\bibitem[Hubara et~al.(2016)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{hubara2016binarized}
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized neural networks.
\newblock \emph{NeurIPS}, 29, 2016.

\bibitem[Jiang et~al.(2019)Jiang, Kone{\v c}n{\'y}, Rush, and
  Kannan]{jiangImprovingFederatedLearning2019}
Jiang, Y., Kone{\v c}n{\'y}, J., Rush, K., and Kannan, S.
\newblock Improving {{Federated Learning Personalization}} via {{Model Agnostic
  Meta Learning}}.
\newblock \emph{arXiv:1909.12488}, 2019.

\bibitem[Jiang et~al.(2022)Jiang, Wang, Valls, Ko, Lee, Leung, and
  Tassiulas]{jiang2022model}
Jiang, Y., Wang, S., Valls, V., Ko, B.~J., Lee, W.-H., Leung, K.~K., and
  Tassiulas, L.
\newblock Model pruning enables efficient federated learning on edge devices.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2022.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, D’Oliveira, Eichner, Rouayheb, Evans,
  Gardner, Garrett, Gascón, Ghazi, Gibbons, Gruteser, Harchaoui, He, He, Huo,
  Hutchinson, Hsu, Jaggi, Javidi, Joshi, Khodak, Konecný, Korolova,
  Koushanfar, Koyejo, Lepoint, Liu, Mittal, Mohri, Nock, Özgür, Pagh, Qi,
  Ramage, Raskar, Raykova, Song, Song, Stich, Sun, Suresh, Tramèr, Vepakomma,
  Wang, Xiong, Xu, Yang, Yu, Yu, and Zhao]{kairouz2019advances}
Kairouz, P., McMahan, H.~B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.~N.,
  Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., D’Oliveira, R. G.~L.,
  Eichner, H., Rouayheb, S.~E., Evans, D., Gardner, J., Garrett, Z., Gascón,
  A., Ghazi, B., Gibbons, P.~B., Gruteser, M., Harchaoui, Z., He, C., He, L.,
  Huo, Z., Hutchinson, B., Hsu, J., Jaggi, M., Javidi, T., Joshi, G., Khodak,
  M., Konecný, J., Korolova, A., Koushanfar, F., Koyejo, S., Lepoint, T., Liu,
  Y., Mittal, P., Mohri, M., Nock, R., Özgür, A., Pagh, R., Qi, H., Ramage,
  D., Raskar, R., Raykova, M., Song, D., Song, W., Stich, S.~U., Sun, Z.,
  Suresh, A.~T., Tramèr, F., Vepakomma, P., Wang, J., Xiong, L., Xu, Z., Yang,
  Q., Yu, F.~X., Yu, H., and Zhao, S.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends® in Machine Learning}, 14\penalty0
  (1–2):\penalty0 1--210, 2021.
\newblock ISSN 1935-8237.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020scaffold}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh,
  A.~T.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{ICML}, pp.\  5132--5143. PMLR, 2020.

\bibitem[Khodak et~al.(2019)Khodak, Balcan, and Talwalkar]{khodak2019adaptive}
Khodak, M., Balcan, M.-F.~F., and Talwalkar, A.~S.
\newblock Adaptive gradient-based meta-learning methods.
\newblock In \emph{NeurIPS}, pp.\  5917--5928, 2019.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Msc thesis, 2009.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Sun, Zeng, Zhang, Li, and
  Chen]{li2021fedmask}
Li, A., Sun, J., Zeng, X., Zhang, M., Li, H., and Chen, Y.
\newblock Fedmask: Joint computation and communication-efficient personalized
  federated learning via heterogeneous masking.
\newblock In \emph{SenSys}, pp.\  42--55, 2021{\natexlab{a}}.

\bibitem[Li \& Wang(2019)Li and Wang]{fedmd}
Li, D. and Wang, J.
\newblock Fedmd: Heterogenous federated learning via model distillation.
\newblock 2019.

\bibitem[Li et~al.(2020)Li, Sahu, Talwalkar, and Smith]{li2020federated}
Li, T., Sahu, A.~K., Talwalkar, A., and Smith, V.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (3):\penalty0
  50--60, 2020.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Hu, Beirami, and Smith]{li2021ditto}
Li, T., Hu, S., Beirami, A., and Smith, V.
\newblock Ditto: Fair and robust federated learning through personalization.
\newblock In \emph{ICML}, pp.\  6357--6368. PMLR, 2021{\natexlab{b}}.

\bibitem[Li et~al.(2021{\natexlab{c}})Li, Hu, Beirami, and
  Smith]{liDittoFairRobust2021}
Li, T., Hu, S., Beirami, A., and Smith, V.
\newblock Ditto: Fair and robust federated learning through personalization.
\newblock In \emph{ICML}, pp.\  6357--6368, 2021{\natexlab{c}}.

\bibitem[Li et~al.(2019)Li, Huang, Yang, Wang, and Zhang]{li2019convergence}
Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z.
\newblock On the convergence of fedavg on non-iid data.
\newblock In \emph{ICLR}, 2019.

\bibitem[Li et~al.(2021{\natexlab{d}})Li, Jiang, Zhang, Kamp, and
  Dou]{li2021fedbn}
Li, X., Jiang, M., Zhang, X., Kamp, M., and Dou, Q.
\newblock Fed{\{}bn{\}}: Federated learning on non-{\{}iid{\}} features via
  local batch normalization.
\newblock In \emph{ICLR}, 2021{\natexlab{d}}.

\bibitem[Liang et~al.(2020)Liang, Liu, Ziyin, Salakhutdinov, and
  Morency]{liang2020think}
Liang, P.~P., Liu, T., Ziyin, L., Salakhutdinov, R., and Morency, L.-P.
\newblock Think locally, act globally: Federated learning with local and global
  representations.
\newblock \emph{arXiv preprint arXiv:2001.01523}, 2020.

\bibitem[Lin et~al.(2020)Lin, Kong, Stich, and
  Jaggi]{linEnsembleDistillationRobust2021}
Lin, T., Kong, L., Stich, S.~U., and Jaggi, M.
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock \emph{NeurIPS}, 33, 2020.

\bibitem[Luo et~al.(2019)Luo, Ren, Peng, Zhang, and Li]{luo2019SwitchableNnorm}
Luo, P., Ren, J., Peng, Z., Zhang, R., and Li, J.
\newblock Differentiable learning-to-normalize via switchable normalization.
\newblock \emph{ICLR}, 2019.

\bibitem[Ma et~al.(2022)Ma, Long, Zhou, Jiang, and Zhang]{ma2022convergence}
Ma, J., Long, G., Zhou, T., Jiang, J., and Zhang, C.
\newblock On the convergence of clustered federated learning.
\newblock \emph{arXiv preprint arXiv:2202.06187}, 2022.

\bibitem[Marfoq et~al.(2021)Marfoq, Neglia, Bellet, Kameni, and
  Vidal]{marfoq2021fedEM}
Marfoq, O., Neglia, G., Bellet, A., Kameni, L., and Vidal, R.
\newblock Federated multi-task learning under a mixture of distributions.
\newblock \emph{NeurIPS}, 34, 2021.

\bibitem[Maurer \& Pontil(2012)Maurer and Pontil]{struct-sparsity}
Maurer, A. and Pontil, M.
\newblock Structured sparsity and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 671–690, mar 2012.
\newblock ISSN 1532-4435.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{AISTATS}, pp.\  1273--1282. PMLR, 2017.

\bibitem[Meng et~al.(2021)Meng, Rambhatla, and Liu]{meng2021cross}
Meng, C., Rambhatla, S., and Liu, Y.
\newblock Cross-node federated graph neural network for spatio-temporal data
  modeling.
\newblock In \emph{KDD}, pp.\  1202--1211, 2021.

\bibitem[Minh \& Carl(2021)Minh and Carl]{fedpnas}
Minh, H. and Carl, K.
\newblock Neurips 2021, workshop on new frontiers in federated learning.
\newblock In \emph{Personalized neural architecture search for federated
  learning}, 2021.

\bibitem[Muhammad et~al.(2020)Muhammad, Wang, O'Reilly-Morgan, Tragos, Smyth,
  Hurley, Geraci, and Lawlor]{muhammad2020fedfast}
Muhammad, K., Wang, Q., O'Reilly-Morgan, D., Tragos, E., Smyth, B., Hurley, N.,
  Geraci, J., and Lawlor, A.
\newblock Fedfast: Going beyond average for faster training of federated
  recommender systems.
\newblock In \emph{KDD}, pp.\  1234--1242, 2020.

\bibitem[Nguyen et~al.(2022)Nguyen, Malik, Zhan, Yousefpour, Rabbat, Malek, and
  Huba]{nguyen2022federated}
Nguyen, J., Malik, K., Zhan, H., Yousefpour, A., Rabbat, M., Malek, M., and
  Huba, D.
\newblock Federated learning with buffered asynchronous aggregation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3581--3607. PMLR, 2022.

\bibitem[Ozkara et~al.(2021)Ozkara, Singh, Data, and Diggavi]{ozkara2021quped}
Ozkara, K., Singh, N., Data, D., and Diggavi, S.
\newblock Quped: Quantized personalization via distillation with applications
  to federated learning.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Qin et~al.(2023)Qin, Yao, Chen, Li, Ding, and
  Cheng]{qin2023revistingpfl}
Qin, Z., Yao, L., Chen, D., Li, Y., Ding, B., and Cheng, M.
\newblock Revisiting personalized federated learning: Robustness against
  backdoor attacks.
\newblock In \emph{KDD}, 2023.

\bibitem[Qiu et~al.(2022)Qiu, Fernandez-Marques, Gusmao, Gao, Parcollet, and
  Lane]{zerofl}
Qiu, X., Fernandez-Marques, J., Gusmao, P.~P., Gao, Y., Parcollet, T., and
  Lane, N.~D.
\newblock Zero{FL}: Efficient on-device training for federated learning with
  local sparsity.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Reddi et~al.(2021)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\'y}, Kumar, and McMahan]{reddi2021adaptive}
Reddi, S.~J., Charles, Z., Zaheer, M., Garrett, Z., Rush, K.,
  Kone{\v{c}}n{\'y}, J., Kumar, S., and McMahan, H.~B.
\newblock Adaptive federated optimization.
\newblock In \emph{ICLR}, 2021.

\bibitem[Reisizadeh et~al.(2020)Reisizadeh, Mokhtari, Hassani, Jadbabaie, and
  Pedarsani]{reisizadeh2020fedpaq}
Reisizadeh, A., Mokhtari, A., Hassani, H., Jadbabaie, A., and Pedarsani, R.
\newblock Fedpaq: A communication-efficient federated learning method with
  periodic averaging and quantization.
\newblock In \emph{AISTATS}, pp.\  2021--2031. PMLR, 2020.

\bibitem[Rothchild et~al.(2020)Rothchild, Panda, Ullah, Ivkin, Stoica,
  Braverman, Gonzalez, and Arora]{rothchild2020fetchsgd}
Rothchild, D., Panda, A., Ullah, E., Ivkin, N., Stoica, I., Braverman, V.,
  Gonzalez, J., and Arora, R.
\newblock Fetchsgd: Communication-efficient federated learning with sketching.
\newblock In \emph{ICML}, pp.\  8253--8265. PMLR, 2020.

\bibitem[Sattler et~al.(2020)Sattler, M{\"u}ller, and
  Samek]{sattler2020clustered}
Sattler, F., M{\"u}ller, K.-R., and Samek, W.
\newblock Clustered federated learning: Model-agnostic distributed multitask
  optimization under privacy constraints.
\newblock \emph{TNNLS}, 2020.

\bibitem[Shamsian et~al.(2021)Shamsian, Navon, Fetaya, and
  Chechik]{shamsian2021personalized}
Shamsian, A., Navon, A., Fetaya, E., and Chechik, G.
\newblock Personalized federated learning using hypernetworks.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{ICML}, volume 139, pp.\
  9489--9502, 7 2021.

\bibitem[Singhal et~al.(2021)Singhal, Sidahmed, Garrett, Wu, Rush, and
  Prakash]{singhal2021federated}
Singhal, K., Sidahmed, H., Garrett, Z., Wu, S., Rush, J., and Prakash, S.
\newblock Federated reconstruction: Partially local federated learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Smith et~al.(2017)Smith, Chiang, Sanjabi, and
  Talwalkar]{smithFederatedMultiTaskLearning2018}
Smith, V., Chiang, C.-K., Sanjabi, M., and Talwalkar, A.
\newblock Federated {{Multi}}-{{Task Learning}}.
\newblock In \emph{NeurIPS}, volume~30, pp.\  4427--4437, 2017.

\bibitem[Tan et~al.(2021)Tan, Yu, Cui, and Yang]{tan2021towards}
Tan, A.~Z., Yu, H., Cui, L., and Yang, Q.
\newblock Towards personalized federated learning.
\newblock \emph{arXiv preprint arXiv:2103.00710}, 2021.

\bibitem[Tianchun et~al.(2022)Tianchun, Wei, Dongsheng, Wenchao, Jingchao,
  Liang, Haifeng, and Zhang]{fedmn}
Tianchun, W., Wei, C., Dongsheng, L., Wenchao, Y., Jingchao, N., Liang, T.,
  Haifeng, C., and Zhang, X.
\newblock Icdm.
\newblock In \emph{Personalized Federated Learning via Heterogeneous Modular
  Networks}, 2022.

\bibitem[Wang et~al.(2022)Wang, Kuang, Xie, Yao, Li, Ding, and Zhou]{fsgnn}
Wang, Z., Kuang, W., Xie, Y., Yao, L., Li, Y., Ding, B., and Zhou, J.
\newblock Federatedscope-gnn: Towards a unified, comprehensive and efficient
  package for federated graph learning.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, pp.\  4110–4120, 2022.

\bibitem[Ward~Jr(1963)]{ward1963hierarchical}
Ward~Jr, J.~H.
\newblock Hierarchical grouping to optimize an objective function.
\newblock \emph{JASA}, 58\penalty0 (301):\penalty0 236--244, 1963.

\bibitem[Xie et~al.(2023)Xie, Wang, Gao, Chen, Yao, Kuang, Li, Ding, and
  Zhou]{fs}
Xie, Y., Wang, Z., Gao, D., Chen, D., Yao, L., Kuang, W., Li, Y., Ding, B., and
  Zhou, J.
\newblock Federatedscope: A flexible federated learning platform for
  heterogeneity.
\newblock \emph{Proceedings of the VLDB Endowment}, 16\penalty0 (5):\penalty0
  1059--1072, 2023.

\bibitem[Yang et~al.(2020)Yang, He, Zhang, and
  Cao]{yangFedStegFederatedTransfer2020}
Yang, H., He, H., Zhang, W., and Cao, X.
\newblock {{FedSteg}}: {{A Federated Transfer Learning Framework}} for {{Secure
  Image Steganalysis}}.
\newblock \emph{IEEE TNSE}, 2020.

\bibitem[Yang et~al.(2019)Yang, Liu, Chen, and Tong]{Yang2019FederatedML}
Yang, Q., Liu, Y., Chen, T., and Tong, Y.
\newblock Federated machine learning: Concept and applications.
\newblock \emph{arXiv: Artificial Intelligence}, 2019.

\bibitem[Yuan et~al.(2022)Yuan, Morningstar, Ning, and Singhal]{yuan2022what}
Yuan, H., Morningstar, W.~R., Ning, L., and Singhal, K.
\newblock What do we mean by generalization in federated learning?
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Yurochkin et~al.(2019{\natexlab{a}})Yurochkin, Agarwal, Ghosh,
  Greenewald, and Hoang]{NEURIPS2019_ecb287ff}
Yurochkin, M., Agarwal, M., Ghosh, S., Greenewald, K., and Hoang, N.
\newblock Statistical model aggregation via parameter matching.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32, 2019{\natexlab{a}}.

\bibitem[Yurochkin et~al.(2019{\natexlab{b}})Yurochkin, Agarwal, Ghosh,
  Greenewald, Hoang, and Khazaeni]{yurochkin2019bayesian}
Yurochkin, M., Agarwal, M., Ghosh, S., Greenewald, K., Hoang, N., and Khazaeni,
  Y.
\newblock Bayesian nonparametric federated learning of neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7252--7261. PMLR, 2019{\natexlab{b}}.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Guo, Ma, Wang, Xu, and
  Wu]{zhang2021parameterized}
Zhang, J., Guo, S., Ma, X., Wang, H., Xu, W., and Wu, F.
\newblock Parameterized knowledge transfer for personalized federated learning.
\newblock In \emph{NeurIPS}, volume~34, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2020)Zhang, Sapra, Fidler, Yeung, and
  Alvarez]{zhang2020personalized}
Zhang, M., Sapra, K., Fidler, S., Yeung, S., and Alvarez, J.~M.
\newblock Personalized federated learning with first order model optimization.
\newblock In \emph{ICLR}, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Gu, Deng, Gu, Bo, Pei, and
  Huang]{zhang2021asysqn}
Zhang, Q., Gu, B., Deng, C., Gu, S., Bo, L., Pei, J., and Huang, H.
\newblock Asysqn: Faster vertical federated learning algorithms with better
  computation resource utilization.
\newblock In \emph{KDD}, pp.\  3917--3927, 2021{\natexlab{b}}.

\bibitem[Zhu \& Han(2019)Zhu and Han]{zhu2020deepLeakage}
Zhu, L. and Han, S.
\newblock Deep leakage from gradients.
\newblock In \emph{NeurIPS}, volume~32. Springer, 2019.

\bibitem[Zhu et~al.(2021)Zhu, Hong, and
  Zhou]{zhuDataFreeKnowledgeDistillation2021}
Zhu, Z., Hong, J., and Zhou, J.
\newblock Data-free knowledge distillation for heterogeneous federated
  learning.
\newblock In \emph{ICML}, 2021.

\end{thebibliography}
