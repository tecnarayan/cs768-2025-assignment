% Generated by IEEEtran.bst, version: 1.13 (2008/09/30)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{jia2018transfer}
Y.~Jia, Y.~Zhang, R.~Weiss, Q.~Wang, J.~Shen, F.~Ren, P.~Nguyen, R.~Pang, I.~L.
  Moreno, Y.~Wu \emph{et~al.}, ``Transfer learning from speaker verification to
  multispeaker text-to-speech synthesis,'' in \emph{Advances in neural
  information processing systems}, 2018, pp. 4480--4490.

\bibitem{cooper2020zero}
E.~Cooper, C.-I. Lai, Y.~Yasuda, F.~Fang, X.~Wang, N.~Chen, and J.~Yamagishi,
  ``Zero-shot multi-speaker text-to-speech with state-of-the-art neural speaker
  embeddings,'' in \emph{ICASSP 2020-2020 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2020, pp. 6184--6188.

\bibitem{choi2020attentron}
S.~Choi, S.~Han, D.~Kim, and S.~Ha, ``Attentron: Few-shot text-to-speech
  utilizing attention-based variable-length embedding,'' \emph{arXiv preprint
  arXiv:2005.08484}, 2020.

\bibitem{casanova2021sc}
E.~Casanova, C.~Shulby, E.~Gölge, N.~M. Müller, F.~S. de~Oliveira,
  A.~{Candido Jr.}, A.~da~Silva~Soares, S.~M. Aluisio, and M.~A. Ponti,
  ``{SC-GlowTTS: An Efficient Zero-Shot Multi-Speaker Text-To-Speech Model},''
  in \emph{Proc. Interspeech 2021}, 2021, pp. 3645--3649.

\bibitem{arik2018neural}
S.~Arik, J.~Chen, K.~Peng, W.~Ping, and Y.~Zhou, ``Neural voice cloning with a
  few samples,'' in \emph{Advances in Neural Information Processing Systems},
  2018, pp. 10\,019--10\,029.

\bibitem{deepvoice3}
W.~Ping, K.~Peng, A.~Gibiansky, S.~O. Arik, A.~Kannan, S.~Narang, J.~Raiman,
  and J.~Miller, ``Deep voice 3: 2000-speaker neural text-to-speech,''
  \emph{arXiv preprint arXiv:1710.07654}, 2017.

\bibitem{tacotron2}
J.~Shen, R.~Pang, R.~J. Weiss, M.~Schuster, N.~Jaitly, Z.~Yang, Z.~Chen,
  Y.~Zhang, Y.~Wang, R.~Skerrv-Ryan \emph{et~al.}, ``Natural tts synthesis by
  conditioning wavenet on mel spectrogram predictions,'' in \emph{2018 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 4779--4783.

\bibitem{ge2e}
L.~Wan, Q.~Wang, A.~Papir, and I.~L. Moreno, ``Generalized end-to-end loss for
  speaker verification,'' in \emph{2018 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em plus 0.5em minus
  0.4em\relax IEEE, 2018, pp. 4879--4883.

\bibitem{cai2018exploring}
W.~Cai, J.~Chen, and M.~Li, ``Exploring the encoding layer and loss function in
  end-to-end speaker and language recognition system,'' \emph{arXiv preprint
  arXiv:1804.05160}, 2018.

\bibitem{snyder2018x}
D.~Snyder, D.~Garcia-Romero, G.~Sell, D.~Povey, and S.~Khudanpur, ``X-vectors:
  Robust dnn embeddings for speaker recognition,'' in \emph{2018 IEEE
  International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2018, pp. 5329--5333.

\bibitem{kumar21c_interspeech}
N.~Kumar, S.~Goel, A.~Narang, and B.~Lall, ``{Normalization Driven Zero-Shot
  Multi-Speaker Speech Synthesis},'' in \emph{Proc. Interspeech 2021}, 2021,
  pp. 1354--1358.

\bibitem{baevski2020wav2vec}
A.~Baevski, Y.~Zhou, A.~Mohamed, and M.~Auli, ``wav2vec 2.0: A framework for
  self-supervised learning of speech representations,'' \emph{Advances in
  Neural Information Processing Systems}, vol.~33, 2020.

\bibitem{tan2021survey}
X.~Tan, T.~Qin, F.~Soong, and T.-Y. Liu, ``A survey on neural speech
  synthesis,'' \emph{arXiv preprint arXiv:2106.15561}, 2021.

\bibitem{veaux2016superseded}
C.~Veaux, J.~Yamagishi, K.~MacDonald \emph{et~al.}, ``Superseded-cstr vctk
  corpus: English multi-speaker corpus for cstr voice cloning toolkit,''
  \emph{University of Edinburgh. The Centre for Speech Technology Research
  (CSTR)}, 2016.

\bibitem{cao2019end}
Y.~Cao, X.~Wu, S.~Liu, J.~Yu, X.~Li, Z.~Wu, X.~Liu, and H.~Meng, ``End-to-end
  code-switched tts with mix of monolingual recordings,'' in \emph{ICASSP
  2019-2019 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2019, pp.
  6935--6939.

\bibitem{zhang2019learning}
Y.~Zhang, R.~J. Weiss, H.~Zen, Y.~Wu, Z.~Chen, R.~Skerry-Ryan, Y.~Jia,
  A.~Rosenberg, and B.~Ramabhadran, ``Learning to speak fluently in a foreign
  language: Multilingual speech synthesis and cross-language voice cloning,''
  \emph{Proc. Interspeech 2019}, pp. 2080--2084, 2019.

\bibitem{nekvinda2020one}
T.~Nekvinda and O.~Du{\v{s}}ek, ``One model, many languages: Meta-learning for
  multilingual text-to-speech,'' \emph{Proc. Interspeech 2020}, pp. 2972--2976,
  2020.

\bibitem{li2021light}
S.~Li, B.~Ouyang, L.~Li, and Q.~Hong, ``Light-tts: Lightweight multi-speaker
  multi-lingual text-to-speech,'' in \emph{ICASSP 2021-2021 IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2021, pp. 8383--8387.

\bibitem{kim2021conditional}
J.~Kim, J.~Kong, and J.~Son, ``Conditional variational autoencoder with
  adversarial learning for end-to-end text-to-speech,'' \emph{arXiv preprint
  arXiv:2106.06103}, 2021.

\bibitem{kim2020glow}
J.~Kim, S.~Kim, J.~Kong, and S.~Yoon, ``Glow-tts: A generative flow for
  text-to-speech via monotonic alignment search,'' \emph{arXiv preprint
  arXiv:2005.11129}, 2020.

\bibitem{dinh2016density}
\BIBentryALTinterwordspacing
L.~Dinh, J.~Sohl{-}Dickstein, and S.~Bengio, ``Density estimation using real
  {NVP},'' in \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}.\hskip 1em plus 0.5em minus 0.4em\relax OpenReview.net, 2017.
  [Online]. Available: \url{https://openreview.net/forum?id=HkpbnH9lx}
\BIBentrySTDinterwordspacing

\bibitem{oord2016wavenet}
A.~v.~d. Oord, S.~Dieleman, H.~Zen, K.~Simonyan, O.~Vinyals, A.~Graves,
  N.~Kalchbrenner, A.~Senior, and K.~Kavukcuoglu, ``Wavenet: A generative model
  for raw audio,'' \emph{arXiv preprint arXiv:1609.03499}, 2016.

\bibitem{kong2020hifi}
J.~Kong, J.~Kim, and J.~Bae, ``Hifi-gan: Generative adversarial networks for
  efficient and high fidelity speech synthesis,'' \emph{arXiv preprint
  arXiv:2010.05646}, 2020.

\bibitem{kingma2013auto}
D.~P. Kingma and M.~Welling, ``Auto-encoding variational bayes,'' \emph{arXiv
  preprint arXiv:1312.6114}, 2013.

\bibitem{prenger2019waveglow}
R.~Prenger, R.~Valle, and B.~Catanzaro, ``Waveglow: A flow-based generative
  network for speech synthesis,'' in \emph{ICASSP 2019-2019 IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP)}.\hskip 1em
  plus 0.5em minus 0.4em\relax IEEE, 2019, pp. 3617--3621.

\bibitem{xin21_interspeech}
D.~Xin, Y.~Saito, S.~Takamichi, T.~Koriyama, and H.~Saruwatari,
  ``{Cross-Lingual Speaker Adaptation Using Domain Adaptation and Speaker
  Consistency Loss for Text-To-Speech Synthesis},'' in \emph{Proc. Interspeech
  2021}, 2021, pp. 1614--1618.

\bibitem{binkowski2019high}
M.~Bi{\'n}kowski, J.~Donahue, S.~Dieleman, A.~Clark, E.~Elsen, N.~Casagrande,
  L.~C. Cobo, and K.~Simonyan, ``High fidelity speech synthesis with
  adversarial networks,'' in \emph{International Conference on Learning
  Representations}, 2019.

\bibitem{ren2020fastspeech}
Y.~Ren, C.~Hu, X.~Tan, T.~Qin, S.~Zhao, Z.~Zhao, and T.-Y. Liu, ``Fastspeech 2:
  Fast and high-quality end-to-end text to speech,'' in \emph{International
  Conference on Learning Representations}, 2021.

\bibitem{heo2020clova}
H.~S. Heo, B.-J. Lee, J.~Huh, and J.~S. Chung, ``Clova baseline system for the
  voxceleb speaker recognition challenge 2020,'' \emph{arXiv preprint
  arXiv:2009.14153}, 2020.

\bibitem{chung2020in}
J.~S. Chung, J.~Huh, S.~Mun, M.~Lee, H.~S. Heo, S.~Choe, C.~Ham, S.~Jung, B.-J.
  Lee, and I.~Han, ``In defence of metric learning for speaker recognition,''
  in \emph{Interspeech}, 2020.

\bibitem{chung2018voxceleb2}
\BIBentryALTinterwordspacing
J.~S. Chung, A.~Nagrani, and A.~Zisserman, ``Voxceleb2: Deep speaker
  recognition,'' in \emph{Proc. Interspeech 2018}, 2018, pp. 1086--1090.
  [Online]. Available: \url{http://dx.doi.org/10.21437/Interspeech.2018-1929}
\BIBentrySTDinterwordspacing

\bibitem{nagrani2017voxceleb}
A.~Nagrani, J.~S. Chung, and A.~Zisserman, ``Voxceleb: a large-scale speaker
  identification dataset,'' \emph{arXiv preprint arXiv:1706.08612}, 2017.

\bibitem{PratapXSSC20}
\BIBentryALTinterwordspacing
V.~Pratap, Q.~Xu, A.~Sriram, G.~Synnaeve, and R.~Collobert, ``{MLS:} {A}
  large-scale multilingual dataset for speech research,'' in \emph{Interspeech
  2020, 21st Annual Conference of the International Speech Communication
  Association, Virtual Event, Shanghai, China, 25-29 October 2020}, H.~Meng,
  B.~Xu, and T.~F. Zheng, Eds.\hskip 1em plus 0.5em minus 0.4em\relax {ISCA},
  2020, pp. 2757--2761. [Online]. Available:
  \url{https://doi.org/10.21437/Interspeech.2020-2826}
\BIBentrySTDinterwordspacing

\bibitem{zen2019libritts}
H.~Zen, V.~Dang, R.~Clark, Y.~Zhang, R.~J. Weiss, Y.~Jia, Z.~Chen, and Y.~Wu,
  ``Libritts: A corpus derived from librispeech for text-to-speech,''
  \emph{arXiv preprint arXiv:1904.02882}, 2019.

\bibitem{casanova2020ttsportuguese}
E.~Casanova, A.~C. Junior, C.~Shulby, F.~S. de~Oliveira, J.~P. Teixeira, M.~A.
  Ponti, and S.~M. Aluisio, ``Tts-portuguese corpus: a corpus for speech
  synthesis in brazilian portuguese,'' 2020.

\bibitem{Hao_2021}
\BIBentryALTinterwordspacing
X.~Hao, X.~Su, R.~Horaud, and X.~Li, ``Fullsubnet: A full-band and sub-band
  fusion model for real-time single-channel speech enhancement,'' \emph{ICASSP
  2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}, Jun 2021. [Online]. Available:
  \url{http://dx.doi.org/10.1109/ICASSP39728.2021.9414177}
\BIBentrySTDinterwordspacing

\bibitem{mailabs}
\BIBentryALTinterwordspacing
{Munich Artificial Intelligence Laboratories GmbH}, ``The m-ailabs speech
  dataset – caito,'' 2017. [Online]. Available:
  \url{https://www.caito.de/2019/01/the-m-ailabs-speech-dataset/}
\BIBentrySTDinterwordspacing

\bibitem{ardila2020common}
R.~Ardila, M.~Branson, K.~Davis, M.~Kohler, J.~Meyer, M.~Henretty, R.~Morais,
  L.~Saunders, F.~Tyers, and G.~Weber, ``Common voice: A massively-multilingual
  speech corpus,'' in \emph{Proceedings of the 12th Language Resources and
  Evaluation Conference}, 2020, pp. 4218--4222.

\bibitem{ito2017lj}
K.~Ito \emph{et~al.}, ``The lj speech dataset,'' 2017.

\bibitem{loshchilov2017decoupled}
I.~Loshchilov and F.~Hutter, ``Decoupled weight decay regularization,'' 2017.

\bibitem{paszke2017automatic}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga \emph{et~al.}, ``Pytorch: An imperative
  style, high-performance deep learning library,'' \emph{Advances in neural
  information processing systems}, vol.~32, pp. 8026--8037, 2019.

\bibitem{mos}
F.~Ribeiro, D.~Flor{\^e}ncio, C.~Zhang, and M.~Seltzer, ``Crowdmos: An approach
  for crowdsourcing mean opinion score studies,'' in \emph{Acoustics, Speech
  and Signal Processing (ICASSP), 2011 IEEE International Conference on}.\hskip
  1em plus 0.5em minus 0.4em\relax IEEE, 2011, pp. 2416--2419.

\bibitem{Jemine2019Master}
C.~Jemine, ``Master thesis: Real-time voice cloning,'' 2019.

\bibitem{wang2021noisevc}
S.~Wang and D.~Borth, ``Noisevc: Towards high quality zero-shot voice
  conversion,'' \emph{arXiv preprint arXiv:2104.06074}, 2021.

\bibitem{qian2019autovc}
K.~Qian, Y.~Zhang, S.~Chang, X.~Yang, and M.~Hasegawa-Johnson, ``Autovc:
  Zero-shot voice style transfer with only autoencoder loss,'' in
  \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2019, pp. 5210--5219.

\bibitem{casanova2020end}
E.~Casanova, A.~C. Junior, F.~S. de~Oliveira, C.~Shulby, J.~P. Teixeira, M.~A.
  Ponti, and S.~M. Aluisio, ``End-to-end speech synthesis applied to brazilian
  portuguese,'' \emph{arXiv preprint arXiv:2005.05144}, 2020.

\end{thebibliography}
