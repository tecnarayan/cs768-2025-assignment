\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{WWDK19}

\bibitem[ACJ{\etalchar{+}}21]{agarwal2021online}
Naman Agarwal, Syomantak Chaudhuri, Prateek Jain, Dheeraj Nagaraj, and Praneeth
  Netrapalli.
\newblock Online target q-learning with reverse experience replay: Efficiently
  finding the optimal policy for linear mdps.
\newblock {\em arXiv preprint arXiv:2110.08440}, 2021.

\bibitem[AHKS20]{agarwal2020pc}
Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun.
\newblock Pc-pg: Policy cover directed exploration for provable policy gradient
  learning.
\newblock {\em arXiv preprint arXiv:2007.08459}, 2020.

\bibitem[AJS{\etalchar{+}}20]{ayoub2020model}
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin~F Yang.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock {\em arXiv preprint arXiv:2006.01107}, 2020.

\bibitem[AKKS20]{agarwal2020flambe}
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock {\em arXiv preprint arXiv:2006.10814}, 2020.

\bibitem[AMP21]{al2021adaptive}
Aymen Al~Marjani and Alexandre Proutiere.
\newblock Adaptive sampling for best policy identification in markov decision
  processes.
\newblock In {\em International Conference on Machine Learning}, pages
  7459--7468. PMLR, 2021.

\bibitem[AYPS11]{Abbasi11}
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2011.

\bibitem[Bai95]{baird1995residual}
Leemon Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In {\em International Conference on Machine Learning (ICML)}. 1995.

\bibitem[BLL{\etalchar{+}}11]{beygelzimer2011contextual}
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire.
\newblock Contextual bandit algorithms with supervised learning guarantees.
\newblock In {\em Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pages 19--26. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[BRS18]{bhandari2018finite}
Jalaj Bhandari, Daniel Russo, and Raghav Singal.
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock In {\em Conference on learning theory}, pages 1691--1692. PMLR, 2018.

\bibitem[BT96]{bertsekas1996neuro}
Dimitri~P Bertsekas and John~N Tsitsiklis.
\newblock {\em Neuro-dynamic programming}.
\newblock Athena Scientific, 1996.

\bibitem[CMS20]{carvalho2020new}
Diogo Carvalho, Francisco~S Melo, and Pedro Santos.
\newblock A new convergent variant of q-learning with linear function
  approximation.
\newblock {\em Advances in Neural Information Processing Systems},
  33:19412--19421, 2020.

\bibitem[CYLW19]{cai2019neural}
Qi~Cai, Zhuoran Yang, Jason Lee, and Zhaoran Wang.
\newblock Neural temporal-difference learning converges to global optima.
\newblock 2019.

\bibitem[DKL{\etalchar{+}}21]{du2021bilinear}
Simon~S Du, Sham~M Kakade, Jason~D Lee, Shachar Lovett, Gaurav Mahajan, Wen
  Sun, and Ruosong Wang.
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock {\em arXiv preprint arXiv:2103.10897}, 2021.

\bibitem[DWW21]{DuaWaiWan21}
Y.~Duan, M.~J. Wainwright, and M.~Wang.
\newblock Optimal value estimation using kernel-based temporal difference
  methods.
\newblock Technical report, Princeton University, September 2021.

\bibitem[EDMB03]{even2003learning}
Eyal Even-Dar, Yishay Mansour, and Peter Bartlett.
\newblock Learning rates for q-learning.
\newblock {\em Journal of machine learning Research}, 5(1), 2003.

\bibitem[FGKS15]{frostig2015competing}
Roy Frostig, Rong Ge, Sham~M Kakade, and Aaron Sidford.
\newblock Competing with the empirical risk minimizer in a single pass.
\newblock In {\em Conference on learning theory}, pages 728--763. PMLR, 2015.

\bibitem[FKSLX21]{foster2021offline}
Dylan~J. Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu.
\newblock Offline reinforcement learning: Fundamental barriers for value
  function approximation, 2021.

\bibitem[FWXY20]{fan2020theoretical}
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang.
\newblock A theoretical analysis of deep q-learning.
\newblock In {\em Learning for Dynamics and Control}, pages 486--489. PMLR,
  2020.

\bibitem[Gor95]{gordon1995stable}
Geoffrey~J Gordon.
\newblock Stable function approximation in dynamic programming.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  261--268. 1995.

\bibitem[GVL12]{golub2012matrix}
Gene~H Golub and Charles~F Van~Loan.
\newblock {\em Matrix Computations}.
\newblock JHU Press, 2012.

\bibitem[HZG21]{he2021logarithmic}
Jiafan He, Dongruo Zhou, and Quanquan Gu.
\newblock Logarithmic regret for reinforcement learning with linear function
  approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  4171--4180. PMLR, 2021.

\bibitem[JAZBJ18]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem[JJS94]{jaakkola1994convergence}
Tommi Jaakkola, Michael~I Jordan, and Satinder~P Singh.
\newblock On the convergence of stochastic iterative dynamic programming
  algorithms.
\newblock {\em Neural computation}, 6(6):1185--1201, 1994.

\bibitem[JKA{\etalchar{+}}17]{jiang17contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E.
  Schapire.
\newblock Contextual decision processes with low {B}ellman rank are
  {PAC}-learnable.
\newblock In Doina Precup and Yee~Whye Teh, editors, {\em International
  Conference on Machine Learning (ICML)}, volume~70 of {\em Proceedings of
  Machine Learning Research}, pages 1704--1713, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[JLM21]{jin2021bellman}
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi.
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock {\em arXiv preprint arXiv:2102.00815}, 2021.

\bibitem[JYWJ20]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, 2020.

\bibitem[KAL16]{krishnamurthy2016pac}
Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Pac reinforcement learning with rich observations.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  pages 1840--1848, 2016.

\bibitem[KS99]{kearns1999finite}
Michael Kearns and Satinder Singh.
\newblock Finite-sample convergence rates for q-learning and indirect
  algorithms.
\newblock {\em Advances in neural information processing systems}, pages
  996--1002, 1999.

\bibitem[KSWY21]{kong2021online}
Dingwen Kong, Ruslan Salakhutdinov, Ruosong Wang, and Lin~F. Yang.
\newblock Online sub-sampling for reinforcement learning with general function
  approximation, 2021.

\bibitem[KXWJ21]{KhaXiaWaiJor21}
K.~Khamaru, E.~Xia, M.~J. Wainwright, and M.~I. Jordan.
\newblock Instance-optimality in optimal value estimation: Adaptivity via
  variance-reduced {Q}-learning.
\newblock Technical report, UC Berkeley, June 2021.
\newblock Arxiv technical report 2106.14352.

\bibitem[LCC{\etalchar{+}}21]{li2021q}
Gen Li, Changxiao Cai, Yuxin Chen, Yuantao Gu, Yuting Wei, and Yuejie Chi.
\newblock Is q-learning minimax optimal? a tight sample complexity analysis.
\newblock {\em arXiv preprint arXiv:2102.06548}, 2021.

\bibitem[Lin92]{lin1992self}
Long-Ji Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock {\em Machine learning}, 8(3-4):293--321, 1992.

\bibitem[LLG{\etalchar{+}}20]{liu2020finite}
Bo~Liu, Ji~Liu, Mohammad Ghavamzadeh, Sridhar Mahadevan, and Marek Petrik.
\newblock Finite-sample analysis of proximal gradient td algorithms.
\newblock {\em arXiv preprint arXiv:2006.14364}, 2020.

\bibitem[LMWJ20]{li2020root}
Chris~Junchi Li, Wenlong Mou, Martin~J Wainwright, and Michael~I Jordan.
\newblock Root-sgd: Sharp nonasymptotics and asymptotic efficiency in a single
  algorithm.
\newblock {\em arXiv preprint arXiv:2008.12690}, 2020.

\bibitem[LS18]{lakshminarayanan2018linear}
Chandrashekar Lakshminarayanan and Csaba Szepesvari.
\newblock Linear stochastic approximation: How far does constant step-size and
  iterate averaging go?
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1347--1355. PMLR, 2018.

\bibitem[LS22]{liu2022provably}
Shuang Liu and Hao Su.
\newblock Provably efficient kernelized q-learning.
\newblock {\em arXiv preprint arXiv:2204.10349}, 2022.

\bibitem[LXY22]{li2022note}
Ziniu Li, Tian Xu, and Yang Yu.
\newblock A note on target q-learning for solving finite mdps with a generative
  oracle.
\newblock {\em arXiv preprint arXiv:2203.11489}, 2022.

\bibitem[MCK{\etalchar{+}}21]{modi2021model}
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal.
\newblock Model-free representation learning and exploration in low-rank mdps.
\newblock {\em arXiv preprint arXiv:2102.07035}, 2021.

\bibitem[Meh17]{mehta2017fast}
Nishant Mehta.
\newblock Fast rates with high probability in exp-concave statistical learning.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1085--1093.
  PMLR, 2017.

\bibitem[MJTS20]{modi2020sample}
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh.
\newblock Sample complexity of reinforcement learning using linearly combined
  model ensembles.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2010--2020. PMLR, 2020.

\bibitem[MKS{\etalchar{+}}13]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[MKS{\etalchar{+}}15]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em nature}, 518(7540):529--533, 2015.

\bibitem[MKW{\etalchar{+}}22]{MouKhaWaiBarJor22}
W.~Mou, K.~Khamaru, M.~J. Wainwright, P.~L. Bartlett, and M.~I. Jordan.
\newblock Optimal variance-reduced stochastic approximation in {B}anach spaces.
\newblock Technical report, UC Berkeley, January 2022.

\bibitem[MM09]{mehta2009q}
Prashant Mehta and Sean Meyn.
\newblock Q-learning and pontryagin's minimum principle.
\newblock In {\em Proceedings of the 48h IEEE Conference on Decision and
  Control (CDC) held jointly with 2009 28th Chinese Control Conference}, pages
  3598--3605. IEEE, 2009.

\bibitem[MMR08]{melo2008analysis}
Francisco~S Melo, Sean~P Meyn, and M~Isabel Ribeiro.
\newblock An analysis of reinforcement learning with function approximation.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2008.

\bibitem[MP09]{MP09}
Andreas Maurer and Massimiliano Pontil.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock In {\em Conference on Learning Theory (COLT)}, 2009.

\bibitem[MS08]{munos2008finite}
R{\'e}mi Munos and Csaba Szepesv{\'a}ri.
\newblock Finite-time bounds for fitted value iteration.
\newblock {\em Journal of Machine Learning Research}, 9(May):815--857, 2008.

\bibitem[PP02]{perkins2002existence}
Theodore~J Perkins and Mark~D Pendrith.
\newblock On the existence of fixed points for q-learning and sarsa in
  partially observable domains.
\newblock In {\em ICML}, pages 490--497, 2002.

\bibitem[Put94]{puterman1994markov}
Martin~L. Puterman.
\newblock {\em Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc., New York, NY, USA, 1994.

\bibitem[Rie05]{riedmiller2005neural}
Martin Riedmiller.
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In {\em European conference on machine learning}, pages 317--328.
  Springer, 2005.

\bibitem[S{\etalchar{+}}98]{szepesvari1998asymptotic}
Csaba Szepesv{\'a}ri et~al.
\newblock The asymptotic convergence-rate of q-learning.
\newblock {\em Advances in neural information processing systems}, pages
  1064--1070, 1998.

\bibitem[SB78]{shreve1978alternative}
Steven~E Shreve and Dimitri~P Bertsekas.
\newblock Alternative theoretical frameworks for finite horizon discrete-time
  stochastic optimal control.
\newblock {\em SIAM Journal on control and optimization}, 16(6):953--978, 1978.

\bibitem[SJ19]{simchowitz2019non}
Max Simchowitz and Kevin Jamieson.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock {\em arXiv preprint arXiv:1905.03814}, 2019.

\bibitem[SJK{\etalchar{+}}18]{sun2018model}
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Model-based reinforcement learning in contextual decision processes.
\newblock {\em arXiv preprint arXiv:1811.08540}, 2018.

\bibitem[SKKS09]{srinivas2009gaussian}
Niranjan Srinivas, Andreas Krause, Sham~M Kakade, and Matthias Seeger.
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock {\em arXiv preprint arXiv:0912.3995}, 2009.

\bibitem[SLW{\etalchar{+}}22]{shi2022pessimistic}
Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi.
\newblock Pessimistic q-learning for offline reinforcement learning: Towards
  optimal sample complexity.
\newblock {\em arXiv preprint arXiv:2202.13890}, 2022.

\bibitem[SMSC21]{santos2021understanding}
Pedro~P Santos, Francisco~S Melo, Alberto Sardinha, and Diogo~S Carvalho.
\newblock Understanding the impact of data distribution on q-learning with
  function approximation.
\newblock {\em arXiv preprint arXiv:2111.11758}, 2021.

\bibitem[TPL21]{tirinzoni2021fully}
Andrea Tirinzoni, Matteo Pirotta, and Alessandro Lazaric.
\newblock A fully problem-dependent regret lower bound for finite-horizon mdps.
\newblock {\em arXiv preprint arXiv:2106.13013}, 2021.

\bibitem[Tro15]{tropp2015}
Joel~A. Tropp.
\newblock An introduction to matrix concentration inequalities.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  8(1-2):1--230, 2015.

\bibitem[Tsi94]{tsitsiklis1994asynchronous}
John~N Tsitsiklis.
\newblock Asynchronous stochastic approximation and q-learning.
\newblock {\em Machine learning}, 16(3):185--202, 1994.

\bibitem[TVR97]{tsitsiklis1997analysis}
John~N Tsitsiklis and Benjamin Van~Roy.
\newblock Analysis of temporal-diffference learning with function
  approximation.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  1997.

\bibitem[Wai19a]{wainwright2019stochastic}
Martin~J Wainwright.
\newblock Stochastic approximation with cone-contractive operators: Sharp
  $\ell_{\infty}$-bounds for $q$-learning.
\newblock {\em arXiv preprint arXiv:1905.06265}, 2019.

\bibitem[Wai19b]{wainwright2019variance}
Martin~J Wainwright.
\newblock Variance-reduced q-learning is minimax optimal.
\newblock {\em arXiv preprint arXiv:1906.04697}, 2019.

\bibitem[WAS20]{weisz2020exponential}
Gellert Weisz, Philip Amortila, and Csaba Szepesv{\'a}ri.
\newblock Exponential lower bounds for planning in mdps with
  linearly-realizable optimal action-value functions.
\newblock {\em arXiv preprint arXiv:2010.01374}, 2020.

\bibitem[Wat89]{watkins1989learning}
Christopher~{John Cornish Hellaby} Watkins.
\newblock Learning from delayed rewards.
\newblock 1989.

\bibitem[WCS{\etalchar{+}}21]{wagenmaker2021firstorder}
Andrew Wagenmaker, Yifang Chen, Max Simchowitz, Simon~S. Du, and Kevin
  Jamieson.
\newblock First-order regret in reinforcement learning with linear function
  approximation: A robust estimation approach, 2021.

\bibitem[WD92]{watkins1992q}
Christopher~JCH Watkins and Peter Dayan.
\newblock Q-learning.
\newblock {\em Machine learning}, 8(3-4):279--292, 1992.

\bibitem[WFK20]{wang2020statistical}
Ruosong Wang, Dean~P Foster, and Sham~M Kakade.
\newblock What are the statistical limits of offline rl with linear function
  approximation?
\newblock {\em arXiv preprint arXiv:2010.11895}, 2020.

\bibitem[WSG21]{weisz2021tensorplan}
Gell{\'e}rt Weisz, Csaba Szepesv{\'a}ri, and Andr{\'a}s Gy{\"o}rgy.
\newblock Tensorplan and the few actions lower bound for planning in mdps under
  linear realizability of optimal value functions.
\newblock {\em arXiv preprint arXiv:2110.02195}, 2021.

\bibitem[WSJ21]{wagenmaker2021beyond}
Andrew Wagenmaker, Max Simchowitz, and Kevin Jamieson.
\newblock Beyond no regret: Instance-dependent pac reinforcement learning.
\newblock {\em arXiv preprint arXiv:2108.02717}, 2021.

\bibitem[WSY20]{wang2020provably}
Ruosong Wang, Ruslan Salakhutdinov, and Lin~F. Yang.
\newblock Provably efficient reinforcement learning with general value function
  approximation, 2020.

\bibitem[WWDK19]{wang2019optimism}
Yining Wang, Ruosong Wang, Simon~S Du, and Akshay Krishnamurthy.
\newblock Optimism in reinforcement learning with generalized linear function
  approximation.
\newblock {\em arXiv preprint arXiv:1912.04136}, 2019.

\bibitem[WWK21]{wang2021exponential}
Yuanhao Wang, Ruosong Wang, and Sham~M Kakade.
\newblock An exponential lower bound for linearly-realizable mdps with constant
  suboptimality gap.
\newblock {\em arXiv preprint arXiv:2103.12690}, 2021.

\bibitem[XCJ{\etalchar{+}}21]{xie2021bellman}
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal.
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2106.06926}, 2021.

\bibitem[XKWJ22]{XiaKhaWaiJor22}
E.~Xia, K.~Khamaru, M.~J. Wainwright, and M.~I. Jordan.
\newblock Instance-dependent confidence and early stopping in reinforcement
  learning.
\newblock Technical report, UC Berkeley, January 2022.

\bibitem[XMD21]{xu2021fine}
Haike Xu, Tengyu Ma, and Simon~S Du.
\newblock Fine-grained gap-dependent bounds for tabular mdps via adaptive
  multi-step bootstrap.
\newblock {\em arXiv preprint arXiv:2102.04692}, 2021.

\bibitem[XZZ21]{xu2021constraints}
Haoran Xu, Xianyuan Zhan, and Xiangyu Zhu.
\newblock Constraints penalized q-learning for safe offline reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2107.09003}, 2021.

\bibitem[YLCF22]{yan2022efficacy}
Yuling Yan, Gen Li, Yuxin Chen, and Jianqing Fan.
\newblock The efficacy of pessimism in asynchronous q-learning.
\newblock {\em arXiv preprint arXiv:2203.07368}, 2022.

\bibitem[YW20]{yang2020reinforcement}
Lin~F Yang and Mengdi Wang.
\newblock Reinforcement leaning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2020.

\bibitem[YW21]{yin2021towards}
Ming Yin and Yu-Xiang Wang.
\newblock Towards instance-optimal offline reinforcement learning with
  pessimism.
\newblock {\em arXiv preprint arXiv:2110.08695}, 2021.

\bibitem[YYD21]{yang2021q}
Kunhe Yang, Lin Yang, and Simon Du.
\newblock Q-learning with logarithmic regret.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1576--1584. PMLR, 2021.

\bibitem[Zan20]{zanette2020exponential}
Andrea Zanette.
\newblock Exponential lower bounds for batch reinforcement learning: Batch rl
  can be exponentially harder than online rl.
\newblock {\em arXiv preprint arXiv:2012.08005}, 2020.

\bibitem[ZB19]{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2019.

\bibitem[ZBJ19]{zanette2019b}
Andrea Zanette, Emma Brunskill, and Mykel {J. Kochenderfer}.
\newblock Almost horizon-free structure-aware best policy identification with a
  generative model.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem[ZBPL20]{zanette2020frequentist}
Andrea Zanette, David Brandfonbrener, Matteo Pirotta, and Alessandro Lazaric.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In {\em AISTATS}, 2020.

\bibitem[ZCA21]{zanette2021cautiously}
Andrea Zanette, Ching-An Cheng, and Alekh Agarwal.
\newblock Cautiously optimistic policy optimization and exploration with linear
  function approximation.
\newblock {\em arXiv preprint arXiv:2103.12923}, 2021.

\bibitem[ZLKB20]{zanette2020learning}
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2020.

\bibitem[ZWB21]{zanette2021provable}
Andrea Zanette, Martin~J Wainwright, and Emma Brunskill.
\newblock Provable benefits of actor-critic methods for offline reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2108.08812}, 2021.

\bibitem[ZZJ20]{zhang2020almost}
Zihan Zhang, Yuan Zhou, and Xiangyang Ji.
\newblock Almost optimal model-free reinforcement learningvia
  reference-advantage decomposition.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\end{thebibliography}
