\begin{thebibliography}{17}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Y.~Abbasi-Yadkori, D.~P{\'a}l, and C.~Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2312--2320, 2011.

\bibitem[Ailon et~al.(2014)Ailon, Karnin, and Joachims]{ailon2014reducing}
N.~Ailon, Z.~Karnin, and T.~Joachims.
\newblock Reducing dueling bandits to cardinal bandits.
\newblock In \emph{International Conference on Machine Learning}, pages
  856--864, 2014.

\bibitem[Cesa-Bianchi and Lugosi(2012)]{cesa2012combinatorial}
N.~Cesa-Bianchi and G.~Lugosi.
\newblock Combinatorial bandits.
\newblock \emph{Journal of Computer and System Sciences}, 78\penalty0
  (5):\penalty0 1404--1422, 2012.

\bibitem[Chen et~al.(2016)Chen, Wang, Yuan, and Wang]{chen2016combinatorial}
W.~Chen, Y.~Wang, Y.~Yuan, and Q.~Wang.
\newblock Combinatorial multi-armed bandit and its extension to
  probabilistically triggered arms.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1746--1778, 2016.

\bibitem[Combes et~al.(2017)Combes, Magureanu, and
  Proutiere]{combes2017minimal}
R.~Combes, S.~Magureanu, and A.~Proutiere.
\newblock Minimal exploration in structured stochastic bandits.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1761--1769, 2017.

\bibitem[Filippi et~al.(2010)Filippi, Cappe, Garivier, and
  Szepesv{\'a}ri]{filippi2010}
S.~Filippi, O.~Cappe, A.~Garivier, and C.~Szepesv{\'a}ri.
\newblock Parametric bandits: The generalized linear case.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  586--594, 2010.

\bibitem[Katariya et~al.(2017{\natexlab{a}})Katariya, Kveton, {Sz}epesv{\'a}ri,
  Vernade, and Wen]{katariya2016stochastic}
S.~Katariya, B.~Kveton, C.~{Sz}epesv{\'a}ri, C.~Vernade, and Z.~Wen.
\newblock Stochastic rank-1 bandits (long version).
\newblock In \emph{AISTATS}, volume~54 of \emph{PMLR}, pages 392--401, April
  2017{\natexlab{a}}.

\bibitem[Katariya et~al.(2017{\natexlab{b}})Katariya, Kveton, Szepesv{\'a}ri,
  Vernade, and Wen]{katariyabernoulli}
S.~Katariya, B.~Kveton, C.~Szepesv{\'a}ri, C.~Vernade, and Z.~Wen.
\newblock Bernoulli rank-1 bandits for click feedback.
\newblock \emph{International Joint Conference on Artificial Intelligence},
  2017{\natexlab{b}}.

\bibitem[Komiyama et~al.(2015)Komiyama, Honda, Kashima, and Nakagawa]{KHKN15}
J.~Komiyama, J.~Honda, H.~Kashima, and H.~Nakagawa.
\newblock Regret lower bound and optimal algorithm in dueling bandit problem.
\newblock In \emph{Conference on Learning Theory}, pages 1141--1154, 2015.

\bibitem[Lai and Robbins(1985)]{lai1985asymptotically}
T.~L. Lai and H.~Robbins.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock \emph{Advances in applied mathematics}, 6\penalty0 (1):\penalty0
  4--22, 1985.

\bibitem[Lattimore and {Sz}epesv{\'a}ri(2017)]{lattimore2016end}
T.~Lattimore and C.~{Sz}epesv{\'a}ri.
\newblock The end of optimism? {A}n asymptotic analysis of finite-armed linear
  bandits (long version).
\newblock In \emph{AISTATS}, volume~54 of \emph{PMLR}, pages 728--737, April
  2017.

\bibitem[Urvoy et~al.(2013)Urvoy, Clerot, F{\'e}raud, and
  Naamane]{urvoy2013generic}
T.~Urvoy, F.~Clerot, R.~F{\'e}raud, and S.~Naamane.
\newblock Generic exploration and k-armed voting bandits.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning (ICML-13)}, pages 91--99, 2013.

\bibitem[Wu and Liu(2016)]{WL16}
H.~Wu and X.~Liu.
\newblock Double thompson sampling for dueling bandits.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  649--657, 2016.

\bibitem[Yue and Joachims(2009)]{yue2009interactively}
Y.~Yue and T.~Joachims.
\newblock Interactively optimizing information retrieval systems as a dueling
  bandits problem.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 1201--1208. ACM, 2009.

\bibitem[Yue and Joachims(2011)]{yue2011beat}
Y.~Yue and T.~Joachims.
\newblock Beat the mean bandit.
\newblock In \emph{Proceedings of the 28th International Conference on Machine
  Learning (ICML-11)}, pages 241--248, 2011.

\bibitem[Yue et~al.(2012)Yue, Broder, Kleinberg, and Joachims]{yue2012k}
Y.~Yue, J.~Broder, R.~Kleinberg, and T.~Joachims.
\newblock The k-armed dueling bandits problem.
\newblock \emph{Journal of Computer and System Sciences}, 78\penalty0
  (5):\penalty0 1538--1556, 2012.

\bibitem[Zoghi et~al.(2014)Zoghi, Whiteson, Munos, and
  Rijke]{zoghi2014relative}
M.~Zoghi, S.~Whiteson, R.~Munos, and M.~Rijke.
\newblock Relative upper confidence bound for the k-armed dueling bandit
  problem.
\newblock In \emph{International Conference on Machine Learning}, pages 10--18,
  2014.

\end{thebibliography}
