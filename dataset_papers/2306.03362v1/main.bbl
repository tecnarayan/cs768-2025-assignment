\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarap(2018)]{relu}
Agarap, A.~F.
\newblock Deep learning using rectified linear units (relu).
\newblock \emph{CoRR}, abs/1803.08375, 2018.

\bibitem[Agarwal et~al.(2021)Agarwal, Schwarzer, Castro, Courville, and
  Bellemare]{rliable}
Agarwal, R., Schwarzer, M., Castro, P.~S., Courville, A.~C., and Bellemare, M.
\newblock Deep reinforcement learning at the edge of the statistical precipice.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 29304--29320, 2021.

\bibitem[Anonymous(2023)]{anonymous2023actorcritic}
Anonymous.
\newblock Actor-critic alignment for offline-to-online reinforcement learning.
\newblock In \emph{Submitted to International Conference on Learning
  Representations}, 2023.

\bibitem[Author(2021)]{anonymous}
Author, N.~N.
\newblock Suppressed for anonymity, 2021.

\bibitem[Booher(2019)]{PPO+BC}
Booher, J.
\newblock Bc + rl : Imitation learning from non-optimal demonstrations.
\newblock 2019.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{openai_gym}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{CoRR}, abs/1606.01540, 2016.

\bibitem[Burges et~al.(2005)Burges, Shaked, Renshaw, Lazier, Deeds, Hamilton,
  and Hullender]{ranknet}
Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., and
  Hullender, G.
\newblock Learning to rank using gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pp.\  89--96,
  2005.

\bibitem[Burges et~al.(2006)Burges, Ragno, and Le]{lambdarank}
Burges, C., Ragno, R., and Le, Q.
\newblock Learning to rank with nonsmooth cost functions.
\newblock \emph{Advances in Neural Information Processing Systems}, 19, 2006.

\bibitem[Burges(2010)]{lambdamart}
Burges, C.~J.
\newblock From ranknet to lambdarank to lambdamart: An overview.
\newblock \emph{Learning}, 11\penalty0 (23-581):\penalty0 81, 2010.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{DT}
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P.,
  Srinivas, A., and Mordatch, I.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 15084--15097, 2021.

\bibitem[Chi et~al.(2021)Chi, Liu, Yang, Lan, Liu, Han, Cheung, and
  Kwok]{chi2021tohan}
Chi, H., Liu, F., Yang, W., Lan, L., Liu, T., Han, B., Cheung, W., and Kwok, J.
\newblock Tohan: A one-step approach towards few-shot hypothesis adaptation.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 20970--20982, 2021.

\bibitem[Duda et~al.(2000)Duda, Hart, and Stork]{DudaHart2nd}
Duda, R.~O., Hart, P.~E., and Stork, D.~G.
\newblock \emph{Pattern Classification}.
\newblock John Wiley and Sons, 2nd edition, 2000.

\bibitem[Fu et~al.(2019)Fu, Kumar, Soh, and Levine]{fu2019diagnosing}
Fu, J., Kumar, A., Soh, M., and Levine, S.
\newblock Diagnosing bottlenecks in deep q-learning algorithms.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2021--2030. PMLR, 2019.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{Fu2020D4RLDF}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock {D4RL:} datasets for deep data-driven reinforcement learning.
\newblock \emph{CoRR}, abs/2004.07219, 2020.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{td3+bc}
Fujimoto, S. and Gu, S.~S.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 20132--20145, 2021.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and
  Precup]{Off_Policy_Deep_Reinforcement_Learning_without_Exploration}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2052--2062. PMLR, 2019.

\bibitem[F{\"{u}}rnkranz \& H{\"{u}}llermeier(2003)F{\"{u}}rnkranz and
  H{\"{u}}llermeier]{Pairwise_Preference_Learning_and_Ranking}
F{\"{u}}rnkranz, J. and H{\"{u}}llermeier, E.
\newblock Pairwise preference learning and ranking.
\newblock In \emph{{ECML}}, volume 2837 of \emph{Lecture Notes in Computer
  Science}, pp.\  145--156. Springer, 2003.

\bibitem[F{\"{u}}rnkranz \& H{\"{u}}llermeier(2010)F{\"{u}}rnkranz and
  H{\"{u}}llermeier]{Preference_Learning}
F{\"{u}}rnkranz, J. and H{\"{u}}llermeier, E. (eds.).
\newblock \emph{Preference Learning}.
\newblock Springer, 2010.

\bibitem[Ghasemipour et~al.(2021)Ghasemipour, Schuurmans, and Gu]{EMaQ}
Ghasemipour, S. K.~S., Schuurmans, D., and Gu, S.~S.
\newblock Emaq: Expected-max q-learning operator for simple yet effective
  offline and online rl.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3682--3691. PMLR, 2021.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{sac}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}. Citeseer,
  2002.

\bibitem[Kearns(1989)]{kearns89}
Kearns, M.~J.
\newblock \emph{Computational Complexity of Machine Learning}.
\newblock PhD thesis, Department of Computer Science, Harvard University, 1989.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{Adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kiran et~al.(2021)Kiran, Sobh, Talpaert, Mannion, Al~Sallab, Yogamani,
  and P{\'e}rez]{autonomous_driving_survey}
Kiran, B.~R., Sobh, I., Talpaert, V., Mannion, P., Al~Sallab, A.~A., Yogamani,
  S., and P{\'e}rez, P.
\newblock Deep reinforcement learning for autonomous driving: A survey.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems},
  23\penalty0 (6):\penalty0 4909--4926, 2021.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Fergus, Tompson, and
  Nachum]{Fisher-BRC}
Kostrikov, I., Fergus, R., Tompson, J., and Nachum, O.
\newblock Offline reinforcement learning with fisher divergence critic
  regularization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5774--5783. PMLR, 2021.

\bibitem[Kostrikov et~al.(2022)Kostrikov, Nair, and Levine]{IQL}
Kostrikov, I., Nair, A., and Levine, S.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock In \emph{{International Conference on Learning Representations}},
  2022.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and Levine]{BEAR}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Kumar et~al.(2020{\natexlab{a}})Kumar, Gupta, and
  Levine]{kumar2020discor}
Kumar, A., Gupta, A., and Levine, S.
\newblock Discor: Corrective feedback in reinforcement learning via
  distribution correction.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18560--18572, 2020{\natexlab{a}}.

\bibitem[Kumar et~al.(2020{\natexlab{b}})Kumar, Zhou, Tucker, and Levine]{CQL}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020{\natexlab{b}}.

\bibitem[Langley(2000)]{langley00}
Langley, P.
\newblock Crafting papers on machine learning.
\newblock In Langley, P. (ed.), \emph{Proceedings of the 17th International
  Conference on Machine Learning (ICML 2000)}, pp.\  1207--1216, Stanford, CA,
  2000. Morgan Kaufmann.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and
  Fu]{offline_survey_sergey}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{CoRR}, abs/2005.01643, 2020.

\bibitem[Li et~al.(2007)Li, Wu, and Burges]{mcrank}
Li, P., Wu, Q., and Burges, C.
\newblock Mcrank: Learning to rank using multiple classification and gradient
  boosting.
\newblock \emph{Advances in Neural Information Processing Systems}, 20, 2007.

\bibitem[Liu et~al.(2020)Liu, See, Ngiam, Celi, Sun, Feng,
  et~al.]{healthcare_survey}
Liu, S., See, K.~C., Ngiam, K.~Y., Celi, L.~A., Sun, X., Feng, M., et~al.
\newblock Reinforcement learning for clinical decision support in critical
  care: comprehensive review.
\newblock \emph{Journal of Medical Internet Research}, 22\penalty0
  (7):\penalty0 e18477, 2020.

\bibitem[Michalski et~al.(1983)Michalski, Carbonell, and
  Mitchell]{MachineLearningI}
Michalski, R.~S., Carbonell, J.~G., and Mitchell, T.~M. (eds.).
\newblock \emph{Machine Learning: An Artificial Intelligence Approach, Vol. I}.
\newblock Tioga, Palo Alto, CA, 1983.

\bibitem[Mitchell(1980)]{mitchell80}
Mitchell, T.~M.
\newblock The need for biases in learning generalizations.
\newblock Technical report, Computer Science Department, Rutgers University,
  New Brunswick, MA, 1980.

\bibitem[Nair et~al.(2018)Nair, McGrew, Andrychowicz, Zaremba, and
  Abbeel]{DDPG_BC}
Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P.
\newblock Overcoming exploration in reinforcement learning with demonstrations.
\newblock In \emph{International Conference on Robotics and Automation}, 2018.

\bibitem[Nair et~al.(2020)Nair, Dalal, Gupta, and Levine]{AWAC}
Nair, A., Dalal, M., Gupta, A., and Levine, S.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock \emph{CoRR}, abs/2006.09359, 2020.

\bibitem[Newell \& Rosenbloom(1981)Newell and Rosenbloom]{Newell81}
Newell, A. and Rosenbloom, P.~S.
\newblock Mechanisms of skill acquisition and the law of practice.
\newblock In Anderson, J.~R. (ed.), \emph{Cognitive Skills and Their
  Acquisition}, chapter~1, pp.\  1--51. Lawrence Erlbaum Associates, Inc.,
  Hillsdale, NJ, 1981.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and
  Levine]{Advantage_Weighted_Regression}
Peng, X.~B., Kumar, A., Zhang, G., and Levine, S.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock \emph{CoRR}, abs/1910.00177, 2019.

\bibitem[Peters \& Schaal(2007)Peters and Schaal]{reward_weighted_regression}
Peters, J. and Schaal, S.
\newblock Reinforcement learning by reward-weighted regression for operational
  space control.
\newblock In \emph{{ICML}}, volume 227 of \emph{{ACM} International Conference
  Proceeding Series}, pp.\  745--750. {ACM}, 2007.

\bibitem[Prudencio et~al.(2022)Prudencio, M{\'{a}}ximo, and
  Colombini]{offline_survey_rafael}
Prudencio, R.~F., M{\'{a}}ximo, M. R. O.~A., and Colombini, E.~L.
\newblock A survey on offline reinforcement learning: Taxonomy, review, and
  open problems.
\newblock \emph{CoRR}, abs/2203.01387, 2022.

\bibitem[Rajeswaran et~al.(2018)Rajeswaran, Kumar, Gupta, Vezzani, Schulman,
  Todorov, and Levine]{adroit}
Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E.,
  and Levine, S.
\newblock Learning complex dexterous manipulation with deep reinforcement
  learning and demonstrations.
\newblock In \emph{Proceedings of Robotics: Science and Systems}, Pittsburgh,
  Pennsylvania, June 2018.
\newblock \doi{10.15607/RSS.2018.XIV.049}.

\bibitem[Samuel(1959)]{Samuel59}
Samuel, A.~L.
\newblock Some studies in machine learning using the game of checkers.
\newblock \emph{IBM Journal of Research and Development}, 3\penalty0
  (3):\penalty0 211--229, 1959.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{TRPO}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1889--1897. PMLR, 2015.

\bibitem[Singh et~al.(2022)Singh, Kumar, and Singh]{robotics_survey}
Singh, B., Kumar, R., and Singh, V.~P.
\newblock Reinforcement learning in robotic applications: a comprehensive
  survey.
\newblock \emph{Artif. Intell. Rev.}, 55\penalty0 (2):\penalty0 945--990, 2022.

\bibitem[Singla et~al.(2021)Singla, Rafferty, Radanovic, and
  Heffernan]{education_survey}
Singla, A., Rafferty, A.~N., Radanovic, G., and Heffernan, N.~T.
\newblock Reinforcement learning for education: Opportunities and challenges.
\newblock \emph{CoRR}, abs/2107.08828, 2021.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998introduction}
Sutton, R.~S. and Barto, A.~G.
\newblock Introduction to reinforcement learning, 1998.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: {A} physics engine for model-based control.
\newblock In \emph{{IROS}}, pp.\  5026--5033. {IEEE}, 2012.

\bibitem[Wang et~al.(2020)Wang, Novikov, Zolna, Merel, Springenberg, Reed,
  Shahriari, Siegel, Gulcehre, Heess, et~al.]{Critic_Regularized_Regression}
Wang, Z., Novikov, A., Zolna, K., Merel, J.~S., Springenberg, J.~T., Reed,
  S.~E., Shahriari, B., Siegel, N., Gulcehre, C., Heess, N., et~al.
\newblock Critic regularized regression.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7768--7778, 2020.

\bibitem[Wilmer et~al.(2009)Wilmer, Levin, and Peres]{wilmer2009markov}
Wilmer, E., Levin, D.~A., and Peres, Y.
\newblock Markov chains and mixing times.
\newblock \emph{American Mathematical Soc., Providence}, 2009.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, F{\"u}rnkranz,
  et~al.]{preference_survey}
Wirth, C., Akrour, R., Neumann, G., F{\"u}rnkranz, J., et~al.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (136):\penalty0 1--46, 2017.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{BRAC}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{CoRR}, abs/1911.11361, 2019.

\bibitem[Yu et~al.(2020)Yu, Min, Zhao, Mei, Wang, Li, Ng, and
  Li]{yu2020dynamic}
Yu, Y., Min, X., Zhao, S., Mei, J., Wang, F., Li, D., Ng, K., and Li, S.
\newblock Dynamic knowledge distillation for black-box hypothesis transfer
  learning.
\newblock \emph{CoRR}, abs/2007.12355, 2020.

\bibitem[Yue et~al.(2022)Yue, Kang, Ma, Xu, Huang, and YAN]{yue2022boosting}
Yue, Y., Kang, B., Ma, X., Xu, Z., Huang, G., and YAN, S.
\newblock Boosting offline reinforcement learning via data rebalancing.
\newblock \emph{Advances in Neural Information Processing Systems, Workshop},
  2022.

\bibitem[Yue et~al.(2023)Yue, Kang, Xu, Huang, and Yan]{yue2022value}
Yue, Y., Kang, B., Xu, Z., Huang, G., and Yan, S.
\newblock Value-consistent representation learning for data-efficient
  reinforcement learning.
\newblock \emph{Association for the Advancement of Artificial Intelligence},
  2023.

\end{thebibliography}
