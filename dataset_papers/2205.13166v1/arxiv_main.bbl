\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[fri({\natexlab{a}})]{fried1}
Makefriedman1 dataset.
\newblock
  \url{https://scikit-learn.org/stable/modules/generated/sklearn.datasets.makefriedman1.html},
  {\natexlab{a}}.
\newblock Accessed: 2022-01-25.

\bibitem[fri({\natexlab{b}})]{fried2}
Makefriedman2 dataset.
\newblock
  \url{https://scikit-learn.org/stable/modules/generated/sklearn.datasets.makefriedman2.html},
  {\natexlab{b}}.
\newblock Accessed: 2022-01-25.

\bibitem[fri({\natexlab{c}})]{fried3}
Makefriedman3 dataset.
\newblock
  \url{https://scikit-learn.org/stable/modules/generated/sklearn.datasets.makefriedman3.html},
  {\natexlab{c}}.
\newblock Accessed: 2022-01-25.

\bibitem[Acharya et~al.(2017)Acharya, Diakonikolas, Li, and
  Schmidt]{acharya2017sample}
J.~Acharya, I.~Diakonikolas, J.~Li, and L.~Schmidt.
\newblock Sample-optimal density estimation in nearly-linear time.
\newblock In \emph{Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium
  on Discrete Algorithms}, pages 1278--1289. SIAM, 2017.

\bibitem[Achlioptas and McSherry(2005)]{achlioptas2005spectral}
D.~Achlioptas and F.~McSherry.
\newblock On spectral learning of mixtures of distributions.
\newblock In \emph{Conference on Learning Theory}, 2005.

\bibitem[Arora and Kannan(2001)]{arora2001learning}
S.~Arora and R.~Kannan.
\newblock Learning mixtures of arbitrary gaussians.
\newblock In \emph{Symposium on Theory of Computing}, 2001.

\bibitem[Balakrishnan et~al.(2017)Balakrishnan, Wainwright, and
  Yu]{balakrishnan2017statistical}
S.~Balakrishnan, M.~J. Wainwright, and B.~Yu.
\newblock Statistical guarantees for the em algorithm: From population to
  sample-based analysis.
\newblock \emph{The Annals of Statistics}, 45\penalty0 (1):\penalty0 77--120,
  2017.

\bibitem[Bal{\'a}zs(2016)]{balazs2016convex}
G.~Bal{\'a}zs.
\newblock Convex regression: theory, practice, and applications.
\newblock 2016.

\bibitem[Belkin and Sinha(2010)]{belkin2010polynomial}
M.~Belkin and K.~Sinha.
\newblock Polynomial learning of distribution families.
\newblock In \emph{Foundations of Computer Science}, 2010.

\bibitem[Chaganty and Liang(2013)]{chaganty2013spectral}
A.~T. Chaganty and P.~Liang.
\newblock Spectral experts for estimating mixtures of linear regressions.
\newblock In \emph{International Conference on Machine Learning}, pages
  1040--1048. PMLR, 2013.

\bibitem[Chan et~al.(2014)Chan, Diakonikolas, Servedio, and
  Sun]{chan2014efficient}
S.-O. Chan, I.~Diakonikolas, R.~A. Servedio, and X.~Sun.
\newblock Efficient density estimation via piecewise polynomial approximation.
\newblock In \emph{Proceedings of the forty-sixth annual ACM symposium on
  Theory of computing}, pages 604--613. ACM, 2014.

\bibitem[Dasgupta(1999)]{dasgupta1999learning}
S.~Dasgupta.
\newblock Learning mixtures of gaussians.
\newblock In \emph{Foundations of Computer Science}, pages 634--644, 1999.

\bibitem[De~Veaux(1989)]{de1989mixtures}
R.~D. De~Veaux.
\newblock Mixtures of linear regressions.
\newblock \emph{Computational Statistics \& Data Analysis}, 8\penalty0
  (3):\penalty0 227--245, 1989.

\bibitem[Diakonikolas et~al.(2018)Diakonikolas, Kane, and
  Stewart]{diakonikolas2018list}
I.~Diakonikolas, D.~M. Kane, and A.~Stewart.
\newblock List-decodable robust mean estimation and learning mixtures of
  spherical gaussians.
\newblock In \emph{Proceedings of the 50th Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 1047--1060. ACM, 2018.

\bibitem[Faria and Soromenho(2010)]{faria2010fitting}
S.~Faria and G.~Soromenho.
\newblock Fitting mixtures of linear regressions.
\newblock \emph{Journal of Statistical Computation and Simulation}, 80\penalty0
  (2):\penalty0 201--225, 2010.

\bibitem[Feldman et~al.(2008)Feldman, O'Donnell, and
  Servedio]{feldman2008learning}
J.~Feldman, R.~O'Donnell, and R.~A. Servedio.
\newblock Learning mixtures of product distributions over discrete domains.
\newblock \emph{SIAM Journal on Computing}, 2008.

\bibitem[Ghosh and Kannan(2020)]{ghosh2020alternating}
A.~Ghosh and R.~Kannan.
\newblock Alternating minimization converges super-linearly for mixed linear
  regression.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1093--1103. PMLR, 2020.

\bibitem[Ghosh et~al.(2019)Ghosh, Pananjady, Guntuboyina, and
  Ramchandran]{ghosh2019max}
A.~Ghosh, A.~Pananjady, A.~Guntuboyina, and K.~Ramchandran.
\newblock Max-affine regression: Provable, tractable, and near-optimal
  statistical estimation.
\newblock \emph{arXiv preprint arXiv:1906.09255}, 2019.

\bibitem[Ghosh et~al.(2020)Ghosh, Chung, Yin, and
  Ramchandran]{ghosh2020efficient}
A.~Ghosh, J.~Chung, D.~Yin, and K.~Ramchandran.
\newblock An efficient framework for clustered federated learning.
\newblock \emph{arXiv preprint arXiv:2006.04088}, 2020.

\bibitem[Hardt and Price(2015)]{hardt2015tight}
M.~Hardt and E.~Price.
\newblock Tight bounds for learning a mixture of two gaussians.
\newblock In \emph{Symposium on Theory of Computing}, 2015.

\bibitem[Hopkins and Li(2018)]{hopkins2018mixture}
S.~B. Hopkins and J.~Li.
\newblock Mixture models, robustness, and sum of squares proofs.
\newblock In \emph{Symposium on Theory of Computing}, 2018.

\bibitem[Hsu et~al.(2011)Hsu, Kakade, and Zhang]{hsu2011analysis}
D.~Hsu, S.~M. Kakade, and T.~Zhang.
\newblock An analysis of random design linear regression.
\newblock \emph{arXiv preprint arXiv:1106.2363}, 2011.

\bibitem[Jain et~al.(2013)Jain, Netrapalli, and Sanghavi]{jain2013low}
P.~Jain, P.~Netrapalli, and S.~Sanghavi.
\newblock Low-rank matrix completion using alternating minimization.
\newblock In \emph{Proceedings of the forty-fifth annual ACM symposium on
  Theory of computing}, pages 665--674, 2013.

\bibitem[Kalai et~al.(2010)Kalai, Moitra, and Valiant]{kalai2010efficiently}
A.~T. Kalai, A.~Moitra, and G.~Valiant.
\newblock Efficiently learning mixtures of two gaussians.
\newblock In \emph{Proceedings of the forty-second ACM symposium on Theory of
  computing}, pages 553--562. ACM, 2010.

\bibitem[Kannan et~al.(2016)Kannan, Kurach, Ravi, Kaufmann, Tomkins, Miklos,
  Corrado, Lukacs, Ganea, Young, et~al.]{kannan2016smart}
A.~Kannan, K.~Kurach, S.~Ravi, T.~Kaufmann, A.~Tomkins, B.~Miklos, G.~Corrado,
  L.~Lukacs, M.~Ganea, P.~Young, et~al.
\newblock Smart reply: Automated response suggestion for email.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 955--964, 2016.

\bibitem[Karmalkar et~al.(2019)Karmalkar, Klivans, and
  Kothari]{karmalkar2019list}
S.~Karmalkar, A.~Klivans, and P.~Kothari.
\newblock List-decodable linear regression.
\newblock \emph{Advances in neural information processing systems}, 2019.

\bibitem[Klusowski et~al.(2019)Klusowski, Yang, and
  Brinda]{klusowski2019estimating}
J.~M. Klusowski, D.~Yang, and W.~Brinda.
\newblock Estimating the coefficients of a mixture of two linear regressions by
  expectation maximization.
\newblock \emph{IEEE Transactions on Information Theory}, 65\penalty0
  (6):\penalty0 3515--3524, 2019.

\bibitem[Kothari et~al.(2018)Kothari, Steinhardt, and
  Steurer]{kothari2018robust}
P.~K. Kothari, J.~Steinhardt, and D.~Steurer.
\newblock Robust moment estimation and improved clustering via sum of squares.
\newblock In \emph{Proceedings of the 50th Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 1035--1046. ACM, 2018.

\bibitem[Krishnamurthy et~al.(2019)Krishnamurthy, Mazumdar, McGregor, and
  Pal]{kris2019sampling}
A.~Krishnamurthy, A.~Mazumdar, A.~McGregor, and S.~Pal.
\newblock Sample complexity of learning mixture of sparse linear regressions.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Kwon and Caramanis(2018)]{kwon2018global}
J.~Kwon and C.~Caramanis.
\newblock Global convergence of em algorithm for mixtures of two component
  linear regression.
\newblock \emph{arXiv preprint arXiv:1810.05752}, 2018.

\bibitem[Li and Liang(2018)]{li2018learning}
Y.~Li and Y.~Liang.
\newblock Learning mixtures of linear regressions with nearly optimal
  complexity.
\newblock In \emph{Conference On Learning Theory}, pages 1125--1144. PMLR,
  2018.

\bibitem[Lu et~al.(2013)Lu, Hoi, and Wang]{lu2013second}
J.~Lu, S.~Hoi, and J.~Wang.
\newblock Second order online collaborative filtering.
\newblock In \emph{Asian Conference on Machine Learning}, pages 325--340. PMLR,
  2013.

\bibitem[Mazumdar and Pal(2020)]{mazumdar2020recovery}
A.~Mazumdar and S.~Pal.
\newblock Recovery of sparse signals from a mixture of linear samples.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and
  Talwalkar]{mohri2018foundations}
M.~Mohri, A.~Rostamizadeh, and A.~Talwalkar.
\newblock \emph{Foundations of machine learning}.
\newblock MIT press, 2018.

\bibitem[Moitra and Valiant(2010)]{moitra2010settling}
A.~Moitra and G.~Valiant.
\newblock Settling the polynomial learnability of mixtures of gaussians.
\newblock In \emph{Foundations of Computer Science}, 2010.

\bibitem[Netrapalli et~al.(2015)Netrapalli, Jain, and
  Sanghavi]{netrapalli2015phase}
P.~Netrapalli, P.~Jain, and S.~Sanghavi.
\newblock Phase retrieval using alternating minimization.
\newblock \emph{IEEE Transactions on Signal Processing}, 63\penalty0
  (18):\penalty0 4814--4826, 2015.

\bibitem[Resnick and Varian(1997)]{resnick1997recommender}
P.~Resnick and H.~R. Varian.
\newblock Recommender systems.
\newblock \emph{Communications of the ACM}, 40\penalty0 (3):\penalty0 56--58,
  1997.

\bibitem[Shen and Sanghavi(2019)]{shen2019iterative}
Y.~Shen and S.~Sanghavi.
\newblock Iterative least trimmed squares for mixed linear regression.
\newblock \emph{arXiv preprint arXiv:1902.03653}, 2019.

\bibitem[St{\"a}dler et~al.(2010)St{\"a}dler, B{\"u}hlmann, and Van
  De~Geer]{stadler2010l}
N.~St{\"a}dler, P.~B{\"u}hlmann, and S.~Van De~Geer.
\newblock l1-penalization for mixture regression models.
\newblock \emph{Test}, 19\penalty0 (2):\penalty0 209--256, 2010.

\bibitem[Vassilvitskii and Arthur(2006)]{vassilvitskii2006k}
S.~Vassilvitskii and D.~Arthur.
\newblock k-means++: The advantages of careful seeding.
\newblock In \emph{Proceedings of the eighteenth annual ACM-SIAM symposium on
  Discrete algorithms}, pages 1027--1035, 2006.

\bibitem[Vershynin(2018)]{vershynin2018high}
R.~Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Viele and Tong(2002)]{viele2002modeling}
K.~Viele and B.~Tong.
\newblock Modeling with mixtures of linear regressions.
\newblock \emph{Statistics and Computing}, 12\penalty0 (4):\penalty0 315--330,
  2002.

\bibitem[Yi et~al.(2014)Yi, Caramanis, and Sanghavi]{yi2014alternating}
X.~Yi, C.~Caramanis, and S.~Sanghavi.
\newblock Alternating minimization for mixed linear regression.
\newblock In \emph{International Conference on Machine Learning}, pages
  613--621. PMLR, 2014.

\bibitem[Yi et~al.(2016)Yi, Caramanis, and Sanghavi]{yi2016solving}
X.~Yi, C.~Caramanis, and S.~Sanghavi.
\newblock Solving a mixture of many random linear equations by tensor
  decomposition and alternating minimization.
\newblock \emph{arXiv preprint arXiv:1608.05749}, 2016.

\bibitem[Yin et~al.(2019)Yin, Pedarsani, Chen, and
  Ramchandran]{yin2018learning}
D.~Yin, R.~Pedarsani, Y.~Chen, and K.~Ramchandran.
\newblock Learning mixtures of sparse linear regressions using sparse graph
  codes.
\newblock \emph{IEEE Transactions on Information Theory}, 65\penalty0
  (3):\penalty0 1430--1451, 2019.

\end{thebibliography}
