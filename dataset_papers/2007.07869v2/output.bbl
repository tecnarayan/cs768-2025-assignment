\begin{thebibliography}{10}

\bibitem{He2015ResNet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in {\em Proceedings of the IEEE conference on computer vision
  and pattern recognition}, 2016.

\bibitem{Bock2018BigGan}
A.~Brock, J.~Donahue, and K.~Simonyan, ``Large scale {GAN} training for high
  fidelity natural image synthesis,'' in {\em International Conference on
  Learning Representations}, 2019.

\bibitem{Devlin2018BERT}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova, ``Bert: Pre-training of deep
  bidirectional transformers for language understanding,'' {\em arXiv preprint
  arXiv:1810.04805}, 2018.

\bibitem{VanDenOord2018WaveNet}
A.~v.~d. Oord, S.~Dieleman, H.~Zen, K.~Simonyan, O.~Vinyals, A.~Graves,
  N.~Kalchbrenner, A.~Senior, and K.~Kavukcuoglu, ``Wavenet: A generative model
  for raw audio,'' {\em arXiv preprint arXiv:1609.03499}, 2016.

\bibitem{Kingma2015Adam}
D.~P. Kingma and J.~Ba, ``Adam: {A} method for stochastic optimization,'' in
  {\em International Conference on Learning Representations} (Y.~Bengio and
  Y.~LeCun, eds.), 2015.

\bibitem{Loshchilov2017WarmRestarts}
I.~Loshchilov and F.~Hutter, ``Sgdr: Stochastic gradient descent with warm
  restarts,'' in {\em International Conference on Learning Representations},
  2017.

\bibitem{Li2020BudgetedSchedules}
M.~Li, E.~Yumer, and D.~Ramanan, ``Budgeted training: Rethinking deep neural
  network training under resource constraints,'' in {\em International
  Conference on Learning Representations}, 2020.

\bibitem{Goyal2017LargeMinibatchSGD}
P.~Goyal, P.~Doll{\'a}r, R.~Girshick, P.~Noordhuis, L.~Wesolowski, A.~Kyrola,
  A.~Tulloch, Y.~Jia, and K.~He, ``Accurate, large minibatch sgd: Training
  imagenet in 1 hour,'' {\em arXiv preprint arXiv:1706.02677}, 2017.

\bibitem{Dong2020AutoHAS}
X.~Dong, M.~Tan, A.~W. Yu, D.~Peng, B.~Gabrys, and Q.~V. Le, ``Autohas:
  Differentiable hyper-parameter and architecture search,'' 2020.

\bibitem{loshchilov2018AdamW}
I.~Loshchilov and F.~Hutter, ``Decoupled weight decay regularization,'' in {\em
  International Conference on Learning Representations}, 2019.

\bibitem{Ioffe2015BatchNorm}
S.~Ioffe and C.~Szegedy, ``Batch normalization: Accelerating deep network
  training by reducing internal covariate shift,'' in {\em International
  conference on Machine Learning} (F.~Bach and D.~Blei, eds.), Proceedings of
  Machine Learning Research, (Lille, France), PMLR, 07--09 Jul 2015.

\bibitem{Li2017Hyperband}
L.~Li, K.~G. Jamieson, G.~DeSalvo, A.~Rostamizadeh, and A.~Talwalkar,
  ``Hyperband: {A} novel bandit-based approach to hyperparameter
  optimization,'' {\em J. Mach. Learn. Res.}, vol.~18, pp.~185:1--185:52, 2017.

\bibitem{Falkner2018BOHB}
S.~Falkner, A.~Klein, and F.~Hutter, ``{BOHB}: Robust and efficient
  hyperparameter optimization at scale,'' in {\em Proceedings of the 35th
  International Conference on Machine Learning}, pp.~1436--1445, 2018.

\bibitem{Baydin2018HypergradientDescent}
A.~G. Baydin, R.~Cornish, D.~M. Rubio, M.~Schmidt, and F.~Wood, ``Online
  learning rate adaptation with hypergradient descent,'' in {\em International
  Conference on Learning Representations}, 2018.

\bibitem{Wu2018ShortHorizonBias}
Y.~Wu, M.~Ren, R.~Liao, and R.~Grosse., ``Understanding short-horizon bias in
  stochastic meta-optimization,'' in {\em International Conference on Learning
  Representations}, 2018.

\bibitem{Franceschi2017ForwardAndReverseGradientBasedHO}
L.~Franceschi, M.~Donini, P.~Frasconi, and M.~Pontil, ``Forward and reverse
  gradient-based hyperparameter optimization,'' in {\em International
  conference on Machine Learning} (D.~Precup and Y.~W. Teh, eds.), Proceedings
  of Machine Learning Research, (International Convention Centre, Sydney,
  Australia), PMLR, 06--11 Aug 2017.

\bibitem{Donini2019SchedulingLearningRateNewInsights}
M.~{Donini}, L.~{Franceschi}, M.~{Pontil}, O.~{Majumder}, and P.~{Frasconi},
  ``{Scheduling the Learning Rate via Hypergradients: New Insights and a New
  Algorithm},'' {\em arXiv e-prints}, Oct. 2019.

\bibitem{Feurer2019HyperparamOptBookChapter}
M.~Feurer and F.~Hutter, {\em Chapter 1: Hyperparameter Optimization}.
\newblock Cham: Springer International Publishing, 2019.

\bibitem{Snoek2015ScalableBayesianOptimizationDeepNets}
J.~Snoek, O.~Rippel, K.~Swersky, R.~Kiros, N.~Satish, N.~Sundaram, M.~Patwary,
  M.~Prabhat, and R.~Adams, ``Scalable bayesian optimization using deep neural
  networks,'' in {\em International conference on Machine Learning}, 2015.

\bibitem{Zoph2017NASwithRL}
B.~Zoph and Q.~V. Le, ``Neural architecture search with reinforcement
  learning,'' in {\em International Conference on Learning Representations},
  2017.

\bibitem{Jaderberg2017PopBasedTrainingNets}
M.~Jaderberg, V.~Dalibard, S.~Osindero, W.~M. Czarnecki, J.~Donahue, A.~Razavi,
  O.~Vinyals, T.~Green, I.~Dunning, K.~Simonyan, {\em et~al.}, ``Population
  based training of neural networks,'' {\em arXiv preprint arXiv:1711.09846},
  2017.

\bibitem{Bengio2000GradientBasedHyperparamOpt}
Y.~Bengio, ``Gradient-based optimization of hyperparameters,'' {\em Neural
  Comput.}, Aug. 2000.

\bibitem{Hospedales2020MetaLearningSurvey}
T.~Hospedales, A.~Antoniou, P.~Micaelli, and A.~Storkey, ``Meta-learning in
  neural networks: A survey,'' {\em arXiv preprint arXiv:2004.05439}, 2020.

\bibitem{Finn2017MAML}
C.~Finn, P.~Abbeel, and S.~Levine, ``Model-agnostic meta-learning for fast
  adaptation of deep networks,'' in {\em Proceedings of the 34th International
  Conference on Machine Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11
  August 2017} (D.~Precup and Y.~W. Teh, eds.), vol.~70 of {\em Proceedings of
  Machine Learning Research}, pp.~1126--1135, {PMLR}, 2017.

\bibitem{Franceschi2018BilevelPF}
L.~Franceschi, P.~Frasconi, S.~Salzo, R.~Grazzi, and M.~Pontil, ``Bilevel
  programming for hyperparameter optimization and meta-learning,'' in {\em
  International conference on Machine Learning}, 2018.

\bibitem{Werbos1990BackpropagationThroughTime}
P.~J. {Werbos}, ``Backpropagation through time: what it does and how to do
  it,'' {\em Proceedings of the IEEE}, 1990.

\bibitem{Williams1989AlgorithmRecurrentNeuralNetwork}
R.~J. {Williams} and D.~{Zipser}, ``A learning algorithm for continually
  running fully recurrent neural networks,'' {\em Neural Computation}, 1989.

\bibitem{Domke2012OptimizationBasedModeling}
J.~Domke, ``Generic methods for optimization-based modeling,'' in {\em
  International Conference on Artificial Intelligence and Statistics} (N.~D.
  Lawrence and M.~Girolami, eds.), Proceedings of Machine Learning Research,
  (La Palma, Canary Islands), PMLR, 21--23 Apr 2012.

\bibitem{Maclaurin2015GradientHO}
D.~{Maclaurin}, D.~{Duvenaud}, and R.~P. {Adams}, ``{Gradient-based
  Hyperparameter Optimization through Reversible Learning},'' {\em arXiv
  e-prints}, Feb. 2015.

\bibitem{Pedregosa2016GradientHO}
F.~Pedregosa, ``Hyperparameter optimization with approximate gradient,'' in
  {\em International conference on Machine Learning}, 2016.

\bibitem{Fu2016DrMAD}
J.~Fu, H.~Luo, J.~Feng, K.~H. Low, and T.-S. Chua, ``Drmad: Distilling
  reverse-mode automatic differentiation for optimizing hyperparameters of deep
  neural networks,'' in {\em IJCAI}, 2016.

\bibitem{Shaban2018TruncatedBackPropBilevelOpt}
A.~Shaban, C.-A. Cheng, N.~Hatch, and B.~Boots, ``Truncated back-propagation
  for bilevel optimization,'' in {\em AISTATS}, 2019.

\bibitem{Larsen1996OptimalUseOfValidation}
J.~{Larsen}, L.~K. {Hansen}, C.~{Svarer}, and M.~{Ohlsson}, ``Design and
  regularization of neural networks: the optimal use of a validation set,'' in
  {\em Neural Networks for Signal Processing VI. Proceedings of the 1996 IEEE
  Signal Processing Society Workshop}, pp.~62--71, 1996.

\bibitem{Rajeswaran2019ImplicitMAML}
A.~Rajeswaran, C.~Finn, S.~M. Kakade, and S.~Levine, ``Meta-learning with
  implicit gradients,'' in {\em Advances in Neural Information Processing
  Systems}, Curran Associates, Inc., 2019.

\bibitem{Lorraine2019MillionsHyperparamOptimImplicit}
J.~{Lorraine}, P.~{Vicol}, and D.~{Duvenaud}, ``{Optimizing Millions of
  Hyperparameters by Implicit Differentiation},'' {\em arXiv e-prints}, Nov.
  2019.

\bibitem{Metz2019PathologiesInTrainingOptimizers}
L.~Metz, N.~Maheswaranathan, J.~Nixon, D.~Freeman, and J.~Sohl-Dickstein,
  ``Understanding and correcting pathologies in the training of learned
  optimizers,'' vol.~97, pp.~4556--4565, 09--15 Jun 2019.

\bibitem{Luketina2016GradBasedRegLearning}
J.~Luketina, M.~Berglund, K.~Greff, and T.~Raiko, ``Scalable gradient-based
  tuning of continuous regularization hyperparameters,'' in {\em International
  conference on Machine Learning}, 2016.

\bibitem{Liu2019DARTS}
H.~Liu, K.~Simonyan, and Y.~Yang, ``{DARTS}: Differentiable architecture
  search,'' in {\em International Conference on Learning Representations},
  2019.

\bibitem{Wang2018DatasetDistillation}
T.~Wang, J.~Zhu, A.~Torralba, and A.~A. Efros, ``Dataset distillation,'' {\em
  CoRR}, vol.~abs/1811.10959, 2018.

\bibitem{Ren2018LearningToReweight}
M.~Ren, W.~Zeng, B.~Yang, and R.~Urtasun, ``Learning to reweight examples for
  robust deep learning,'' in {\em International conference on Machine
  Learning}, 2018.

\bibitem{Bengio1993RNNGradientIssues}
Y.~{Bengio}, P.~{Frasconi}, and P.~{Simard}, ``The problem of learning
  long-term dependencies in recurrent networks,'' in {\em IEEE International
  Conference on Neural Networks}, 1993.

\bibitem{Bengio1994LongTermGradInefficient}
Y.~{Bengio}, P.~{Simard}, and P.~{Frasconi}, ``Learning long-term dependencies
  with gradient descent is difficult,'' {\em IEEE Transactions on Neural
  Networks}, 1994.

\bibitem{Parmas2018PIPPSCurseOfChaos}
P.~Parmas, C.~E. Rasmussen, J.~Peters, and K.~Doya, ``{PIPPS}: Flexible
  model-based policy search robust to the curse of chaos,'' vol.~80 of {\em
  Proceedings of Machine Learning Research}, (Stockholmsm√§ssan, Stockholm
  Sweden), pp.~4065--4074, PMLR, 10--15 Jul 2018.

\bibitem{Hochreiter1997LSTM}
S.~Hochreiter and J.~Schmidhuber, ``Long short-term memory,'' {\em Neural
  computation}, 1997.

\bibitem{Pascanu2013DifficultyOfRNNs}
R.~Pascanu, T.~Mikolov, and Y.~Bengio, ``On the difficulty of training
  recurrent neural networks,'' in {\em International conference on Machine
  Learning}, 2013.

\bibitem{Flennerhag2020WarpedGradientDescent}
S.~Flennerhag, A.~A. Rusu, R.~Pascanu, F.~Visin, H.~Yin, and R.~Hadsell,
  ``Meta-learning with warped gradient descent,'' in {\em International
  Conference on Learning Representations}, 2020.

\bibitem{Zagoruyko2016WRN}
S.~Zagoruyko and N.~Komodakis, ``Wide residual networks,'' in {\em BMVC}, 2016.

\bibitem{Stackelberg1952BilevelOptimization}
H.~Stackelberg, {\em {The Theory Of Market Economy}}.
\newblock Oxford University Press, 1952.

\bibitem{Paszke2019Pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito,
  M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and
  S.~Chintala, ``Pytorch: An imperative style, high-performance deep learning
  library,'' in {\em Advances in Neural Information Processing Systems}, Curran
  Associates, Inc., 2019.

\bibitem{Walters1982ErgodicTheory}
P.~Walters, {\em An Introduction to Ergodic Theory}.
\newblock Graduate texts in mathematics, Springer-Verlag, 1982.

\bibitem{Boltzmann1896Ergodicity}
L.~Boltzmann, {\em Vorlesungen uber Gastheorie}.
\newblock J.A. Barth, 1896.

\bibitem{Peters2019ErgodicityEconomics}
O.~Peters, ``The ergodicity problem in economics,'' {\em Nature Physics},
  vol.~15, pp.~1216--1221, Dec 2019.

\bibitem{Leslie2017SuperConvergence}
L.~N. Smith and N.~Topin, ``Super-convergence: Very fast training of residual
  networks using large learning rates,'' {\em CoRR}, vol.~abs/1708.07120, 2017.

\bibitem{Li2019RSforNAS}
L.~Li and A.~Talwalkar, ``Random search and reproducibility for neural
  architecture search,'' in {\em UAI}, 2019.

\bibitem{Cubuk2020RandaugmentPA}
E.~D. Cubuk, B.~Zoph, J.~Shlens, and Q.~V. Le, ``Randaugment: Practical
  automated data augmentation with a reduced search space,'' {\em 2020 IEEE/CVF
  Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  pp.~3008--3017, 2020.

\bibitem{HpBandster}
A.~Klein. \url{https://automl.github.io/HpBandSter/build/html/quickstart.html},
  2019.

\bibitem{Zela2020RobustifyingDARTS}
A.~Zela, T.~Elsken, T.~Saikia, Y.~Marrakchi, T.~Brox, and F.~Hutter,
  ``Understanding and robustifying differentiable architecture search,'' in
  {\em International Conference on Learning Representations}, 2020.

\bibitem{Jacobs1988DecayLearningRateFromSign}
``Increased rates of convergence through learning rate adaptation,'' {\em
  Neural Networks}, vol.~1, no.~4, pp.~295--307, 1988.

\bibitem{Riedmiller1993RPROP}
M.~{Riedmiller} and H.~{Braun}, ``A direct adaptive method for faster
  backpropagation learning: the rprop algorithm,'' in {\em IEEE International
  Conference on Neural Networks}, pp.~586--591 vol.1, 1993.

\bibitem{Bernstein2018SignSGD}
J.~Bernstein, Y.~Wang, K.~Azizzadenesheli, and A.~Anandkumar, ``{SIGNSGD:}
  compressed optimisation for non-convex problems,'' in {\em Proceedings of the
  35th International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018} (J.~G. Dy and
  A.~Krause, eds.), vol.~80 of {\em Proceedings of Machine Learning Research},
  pp.~559--568, {PMLR}, 2018.

\bibitem{Safaryan2019StochasticSignDescentMethods}
M.~{Safaryan} and P.~{Richt{\'a}rik}, ``{On Stochastic Sign Descent Methods},''
  {\em arXiv e-prints}, p.~arXiv:1905.12938, May 2019.

\bibitem{Bradbury2018JAX}
J.~Bradbury, R.~Frostig, P.~Hawkins, M.~J. Johnson, C.~Leary, D.~Maclaurin,
  G.~Necula, A.~Paszke, J.~Vander{P}las, S.~Wanderman-{M}ilne, and Q.~Zhang,
  ``{JAX}: composable transformations of {P}ython+{N}um{P}y programs,'' 2018.

\end{thebibliography}
