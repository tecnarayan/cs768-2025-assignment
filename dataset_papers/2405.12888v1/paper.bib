@book{Bonnard,
  TITLE = {{Geometric and Numerical Optimal Control - Application to Swimming at Low Reynolds Number and Magnetic Resonance Imaging}},
  AUTHOR = {Bonnard, Bernard and Chyba, Monique and Rouot, J{\'e}r{\'e}my},
  PUBLISHER = {{Springer Int. Publishing}},
  SERIES = {SpringerBriefs in Mathematics},
  PAGES = {XIV-108 p.},
  YEAR = {2018},
  KEYWORDS = {Optimal Control ; Calculus of Variations ; Swimming at Low Reynolds Number ; Magnetic Resonance Imaging ; Numerical Calculations},
  PDF = {https://hal.inria.fr/hal-01226734v8/file/book2018.pdf},
  HAL_ID = {hal-01226734},
  HAL_VERSION = {v8},
}

@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}

@article{bregman1967relaxation,
  title={The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming},
  author={Bregman, Lev M},
  journal={USSR computational mathematics and mathematical physics},
  volume={7},
  number={3},
  pages={200--217},
  year={1967},
  publisher={Elsevier}
}

@article{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  volume={2},
  year={2010}
}

@inproceedings{makkuva2020optimal,
  title={Optimal transport mapping via input convex neural networks},
  author={Makkuva, Ashok and Taghvaei, Amirhossein and Oh, Sewoong and Lee, Jason},
  booktitle={International Conference on Machine Learning},
  pages={6672--6681},
  year={2020},
  organization={PMLR}
}

@article{Saxe,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@article{Bah,
  title={Learning deep linear neural networks: Riemannian gradient flows and convergence to global minimizers},
  author={Bah, Bubacarr and Rauhut, Holger and Terstiege, Ulrich and Westdickenberg, Michael},
  journal={Information and Inference: A Journal of the IMA},
  volume={11},
  number={1},
  pages={307--353},
  year={2022},
  publisher={Oxford University Press}
}

@article{Du,
  title={Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},
  author={Du, Simon S and Hu, Wei and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{Arora18a,
title={A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks},
author={Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
booktitle={International Conference on Learning Representations},
year={2019}
}

@inproceedings{Arora18b,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle={International Conference on Machine Learning},
  pages={244--253},
  year={2018},
  organization={PMLR}
}

@inproceedings{Tarmoun,
  title={Understanding the dynamics of gradient flow in overparameterized linear models},
  author={Tarmoun, Salma and Franca, Guilherme and Haeffele, Benjamin D and Vidal, Rene},
  booktitle={International Conference on Machine Learning},
  pages={10153--10161},
  year={2021},
  organization={PMLR}
}

@inproceedings{Min,
  title={On the explicit role of initialization on the convergence and implicit bias of overparametrized linear networks},
  author={Min, Hancheng and Tarmoun, Salma and Vidal, Ren{\'e} and Mallada, Enrique},
  booktitle={International Conference on Machine Learning},
  pages={7760--7768},
  year={2021},
  organization={PMLR}
}

@inproceedings{Kunin,
title={Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics},
author={Daniel Kunin and Javier Sagastuy-Brena and Surya Ganguli and Daniel LK Yamins and Hidenori Tanaka},
booktitle={International Conference on Learning Representations},
year={2021},
}

@article{Tanaka,
  title={Noether’s learning dynamics: Role of symmetry breaking in neural networks},
  author={Tanaka, Hidenori and Kunin, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{zhao,
title={Symmetries, Flat Minima, and the Conserved Quantities of Gradient Flow},
author={Bo Zhao and Iordan Ganev and Robin Walters and Rose Yu and Nima Dehmamy},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}

@inproceedings{
zhang,
title={Understanding deep learning requires rethinking generalization},
author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
booktitle={International Conference on Learning Representations},
year={2017},
}

@article{belkin,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proc. of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}

@inproceedings{li22,
 author = {Li, Zhiyuan and Wang, Tianhao and Lee, Jason D and Arora, Sanjeev},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {34626--34640},
 publisher = {Curran Associates, Inc.},
 title = {Implicit Bias of Gradient Descent on Reparametrized Models: On Equivalence to Mirror Descent},
 volume = {35},
 year = {2022}
}

@article{gunasekar2017implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}


@inproceedings{
ji,
title={Gradient descent aligns the layers of deep linear networks},
author={Ziwei Ji and Matus Telgarsky},
booktitle={International Conference on Learning Representations},
year={2019},
}

@article{Noether1918,
author = {Noether, E.},
journal = {Nachrichten von der Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse},
language = {ger},
pages = {235-257},
title = {Invariante Variationsprobleme},
volume = {1918},
year = {1918},
}


@book{lee,
	address = {New York, NY},
	series = {Graduate {Texts} in {Mathematics}},
	title = {Introduction to {Smooth} {Manifolds}},
	volume = {218},
	isbn = {978-1-4419-9981-8 978-1-4419-9982-5},
	language = {en},
	urldate = {2023-01-25},
	publisher = {Springer},
	author = {Lee, John M.},
	year = {2012},
	doi = {10.1007/978-1-4419-9982-5},
	keywords = {de Rham cohomology, differential forms, first-order partial differential equations, foliations, Frobenius theorem, immersed and embedded submanifolds, Lie group, Sard’s theorem, smooth manifolds, Smooth structures, Stokes's theorem, Tangent vectors and covectors, tensors, vector bundles, vector fields and flows, Whitney approximation theorem, Whitney embedding theorem},
}


@article{stock_embedding_2022,
	title = {An {Embedding} of {ReLU} {Networks} and an {Analysis} of their {Identifiability}},
	doi = {10.1007/s00365-022-09578-1},
	abstract = {Neural networks with the Rectified Linear Unit (ReLU) nonlinearity are described by a vector of parameters \$þeta\$, and realized as a piecewise linear continuous function \$R\_þeta: x ın {\textbackslash}mathbb R{\textasciicircum}d {\textbackslash}mapsto R\_þeta(x) ın {\textbackslash}mathbb R{\textasciicircum}k\$.Natural scalings and permutations operations on the parameters \$þeta\$ leave the realization unchanged, leading to equivalence classes of parameters that yield the same realization. These considerations in turn lead to the notion of identifiability – the ability to recover (the equivalence class of) \$þeta\$ from the sole knowledge of its realization $_{\textrm{þeta}}$. The overall objective of this paper is to introduce an embedding for ReLU neural networks of any depth, \${\textbackslash}Phi(þeta)\$, that is invariant to scalings and that provides a locally linear parameterization of the realization of the network. Leveraging these two key properties, we derive some conditions under which a deep ReLU network is indeed locally identifiable from the knowledge of the realization on a finite set of samples \$x\_i ın {\textbackslash}mathbb R{\textasciicircum}d$^{\textrm{d\$}}$.},
	journal = {Constructive Approximation},
	author = {Stock, Pierre and Gribonval, Rémi},
	year = {2022},
	note = {Publisher: Springer Verlag},
	file = {Stock et Gribonval - 2022 - An Embedding of ReLU Networks and an Analysis of t.pdf:/Users/remi/Zotero/storage/MBTIKZ7E/Stock et Gribonval - 2022 - An Embedding of ReLU Networks and an Analysis of t.pdf:application/pdf},
}

@article{neyshabur,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}

@article{soudry,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
   year={2018},
  publisher={JMLR. org}
}

@inproceedings{gunasekar,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1832--1841},
  year={2018},
  organization={PMLR}
}

@inproceedings{chizat,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conf. on Learning Theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}

@article{isidori,
  title={Nonlinear System Control},
  author={Isidori, A},
  journal={New York: Springer Verlag},
  volume={61},
  pages={225--236},
  year={1995}
}

@manual{sagemath,
  Key          = {SageMath},
  Author       = {{The Sage Developers}},
  Title        = {{S}ageMath, the {S}age {M}athematics {S}oftware {S}ystem ({V}ersion 9.7)},
  note         = {{\tt https://www.sagemath.org}},
  Year         = {2022},
}

@book{MLtheory,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@inproceedings{jigradient,
  title={Gradient descent follows the regularization path for general losses},
  author={Ji, Ziwei and Dud{\'\i}k, Miroslav and Schapire, Robert E and Telgarsky, Matus},
  booktitle={Conf. on Learning Theory},
  pages={2109--2136},
  year={2020},
  organization={PMLR}
}

@article{neyshaburthesis,
  title={Implicit regularization in deep learning},
  author={Neyshabur, Behnam},
  journal={arXiv preprint arXiv:1709.01953},
  year={2017}
}
@article{functionaldependence-gradient,
 ISSN = {00029890, 19300972},
 author = {W. F. Newns},
 journal = {The American Mathematical Monthly},
 number = {8},
 pages = {911--920},
 publisher = {Mathematical Association of America},
 title = {Functional Dependence},
 urldate = {2023-06-08},
 volume = {74},
 year = {1967}
}

@article{functionaldependence,
 ISSN = {00029947},
 author = {Arthur B. Brown},
 journal = {Transactions of the American Mathematical Society},
 number = {2},
 pages = {379--394},
 publisher = {American Mathematical Society},
 title = {Functional Dependence},
 urldate = {2023-06-28},
 volume = {38},
 year = {1935}
}

@article{głuch2021noether,
  title={Noether: The More Things Change, the More Stay the Same},
  author={Grzegorz Głuch and Rüdiger Urbanke},
  journal={arXiv preprint arXiv:2104.05508},
  year={2021}
}

@softwareversion{marcotte:hal-04261339v1,
  TITLE = {{Code for reproducible research. Abide by the Law and Follow the Flow: Conservation Laws for Gradient Flows}},
  AUTHOR = {Marcotte, Sibylle and Gribonval, R{\'e}mi and Peyr{\'e}, Gabriel},
  NOTE = {},
  YEAR = {2023},
  MONTH = Oct,
  REPOSITORY = {https://github.com/sibyllema/Conservation_laws},
  LICENSE = {BSD 3-Clause License},
  FILE = {https://hal.science/hal-04261339/file/code_hal.zip},
  HAL_ID = {hal-04261339},
  HAL_VERSION = {v1},
}


@InProceedings{azulay21,
  title = 	 {On the Implicit Bias of Initialization Shape: Beyond Infinitesimal Mirror Descent},
  author =       {Azulay, Shahar and Moroshko, Edward and Nacson, Mor Shpigel and Woodworth, Blake E and Srebro, Nathan and Globerson, Amir and Soudry, Daniel},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {468--477},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR}
}

@article{marcotte2023abide,
  title={Abide by the law and follow the flow: Conservation laws for gradient flows},
  author={Marcotte, Sibylle and Gribonval, R{\'e}mi and Peyr{\'e}, Gabriel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={Ussr computational mathematics and mathematical physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}



@inproceedings{amos2017input,
  title={Input convex neural networks},
  author={Amos, Brandon and Xu, Lei and Kolter, J Zico},
  booktitle={International Conference on Machine Learning},
  pages={146--155},
  year={2017},
  organization={PMLR}
}


@article{lee1999learning,
  title={Learning the parts of objects by non-negative matrix factorization},
  author={Lee, Daniel D and Seung, H Sebastian},
  journal={Nature},
  volume={401},
  number={6755},
  pages={788--791},
  year={1999},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{nesterov1983method,
  title={A method of solving a convex programming problem with convergence rate O$\bigl(k^2 \bigr)$},
  author={Nesterov, Yurii Evgen'evich},
  booktitle={Doklady Akademii Nauk},
  volume={269},
  number={3},
  pages={543--547},
  year={1983},
  organization={Russian Academy of Sciences}
}

@article{raskutti2015information,
  title={The information geometry of mirror descent},
  author={Raskutti, Garvesh and Mukherjee, Sayan},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={3},
  pages={1451--1457},
  year={2015},
  publisher={IEEE}
}
@book{nemirovskij1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovskij, Arkadij Semenovi{\v{c}} and Yudin, David Borisovich},
  year={1983},
  publisher={Wiley-Interscience}
}

@article{su2016differential,
  title={A differential equation for modeling Nesterov's accelerated gradient method: Theory and insights},
  author={Su, Weijie and Boyd, Stephen and Candes, Emmanuel J},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={153},
  pages={1--43},
  year={2016}
}

@article{jordan,
  title={A variational perspective on accelerated methods in optimization},
  author={Wibisono, Andre and Wilson, Ashia C and Jordan, Michael I},
  journal={proceedings of the National Academy of Sciences},
  volume={113},
  number={47},
  pages={E7351--E7358},
  year={2016},
  publisher={National Acad Sciences}
}

@inproceedings{wang,
 author = {Wang, Bohan and Meng, Qi and Zhang, Huishuai and Sun, Ruoyu and Chen, Wei and Ma, Zhi-Ming and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {26764--26776},
 publisher = {Curran Associates, Inc.},
 title = {Does Momentum Change the Implicit Regularization on Separable Data?},
 volume = {35},
 year = {2022}
}

@inproceedings{martens2010deep,
  title={Deep learning via Hessian-free optimization},
  author={Martens, James},
  booktitle={Proceedings of the 27th International Conference on International Conference on Machine Learning},
  pages={735--742},
  year={2010}
}

@book{martens2012training,
  title={Training Deep and Recurrent Networks with Hessian-Free Optimization},
  author={Martens, James and Sutskever, Ilya},
  journal={Neural Networks: Tricks of the Trade},
  pages={479--535},
  year={2012},
  publisher={Springer}
}

@article{amari1998natural,
  title={Natural gradient works efficiently in learning},
  author={Amari, Shun-Ichi},
  journal={Neural computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998},
  publisher={MIT Press}
}
@inproceedings{martens2015optimizing,
  title={Optimizing neural networks with kronecker-factored approximate curvature},
  author={Martens, James and Grosse, Roger},
  booktitle={International conference on machine learning},
  pages={2408--2417},
  year={2015},
  organization={PMLR}
}

@inproceedings{leeNMF,
 author = {Lee, Daniel and Seung, H. Sebastian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {Algorithms for Non-negative Matrix Factorization},
 volume = {13},
 year = {2000}
}