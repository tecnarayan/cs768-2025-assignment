\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2022)Ahn, Zhang, and Sra]{ahn2022understanding}
Kwangjun Ahn, Jingzhao Zhang, and Suvrit Sra.
\newblock Understanding the unstable convergence of gradient descent.
\newblock \emph{arXiv preprint arXiv:2204.01050}, 2022.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li, and
  Liang]{allen2019learning}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock \emph{Advances in neural information processing systems},
  2019{\natexlab{a}}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li, and
  Song]{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pages
  242--252. PMLR, 2019{\natexlab{b}}.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, and
  Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  322--332. PMLR, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock \emph{Advances in Neural Information Processing Systems}, 32,
  2019{\natexlab{b}}.

\bibitem[Bubeck et~al.(2015)]{bubeck2015convex}
S{\'e}bastien Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Chatterji et~al.(2021{\natexlab{a}})Chatterji, Long, and
  Bartlett]{chatterji2021doesB}
Niladri~S Chatterji, Philip~M Long, and Peter Bartlett.
\newblock When does gradient descent with logistic loss interpolate using deep
  networks with smoothed relu activations?
\newblock In \emph{Conference on Learning Theory}, pages 927--1027. PMLR,
  2021{\natexlab{a}}.

\bibitem[Chatterji et~al.(2021{\natexlab{b}})Chatterji, Long, and
  Bartlett]{chatterji2021doesA}
Niladri~S Chatterji, Philip~M Long, and Peter~L Bartlett.
\newblock When does gradient descent with logistic loss find interpolating
  two-layer networks?
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (159):\penalty0 1--48, 2021{\natexlab{b}}.

\bibitem[Cheridito et~al.(2022)Cheridito, Jentzen, Riekert, and
  Rossmannek]{cheridito2022proof}
Patrick Cheridito, Arnulf Jentzen, Adrian Riekert, and Florian Rossmannek.
\newblock A proof of convergence for gradient descent in the training of
  artificial neural networks for constant target functions.
\newblock \emph{Journal of Complexity}, page 101646, 2022.

\bibitem[Chizat and Bach(2018)]{chizat2018global}
Lenaic Chizat and Francis Bach.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Cho and Saul(2009)]{cho2009kernel}
Youngmin Cho and Lawrence Saul.
\newblock Kernel methods for deep learning.
\newblock \emph{Advances in neural information processing systems}, 22, 2009.

\bibitem[Cohen et~al.(2021)Cohen, Kaur, Li, Kolter, and
  Talwalkar]{cohen2021gradient}
Jeremy~M Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock \emph{arXiv preprint arXiv:2103.00065}, 2021.

\bibitem[Daniely(2017)]{daniely2017sgd}
Amit Daniely.
\newblock Sgd learns the conjugate kernel class of the network.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Ding et~al.(2019)Ding, Li, and Sun]{ding2019sub}
Tian Ding, Dawei Li, and Ruoyu Sun.
\newblock Sub-optimal local minima exist for neural networks with almost all
  non-linear activations.
\newblock \emph{arXiv preprint arXiv:1911.01413}, 2019.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International conference on machine learning}, pages
  1675--1685. PMLR, 2019.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[E et~al.(2019)E, Ma, Wang, and Wu]{weinan2019analysis}
Weinan E, Chao Ma, Qingcan Wang, and Lei Wu.
\newblock Analysis of the gradient descent algorithm for a deep neural network
  model with skip-connections.
\newblock \emph{arXiv preprint arXiv:1904.05263}, 2019.

\bibitem[E et~al.(2020)E, Ma, and Wu]{weinan2020comparative}
Weinan E, Chao Ma, and Lei Wu.
\newblock A comparative analysis of optimization and generalization properties
  of two-layer neural network and random feature models under gradient descent
  dynamics.
\newblock \emph{Science China Mathematics}, 63\penalty0 (7):\penalty0 1235,
  2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{arXiv preprint arXiv:1806.07572}, 2018.

\bibitem[Jentzen and Riekert(2021)]{jentzen2021proof}
Arnulf Jentzen and Adrian Riekert.
\newblock A proof of convergence for the gradient descent optimization method
  with random initializations in the training of neural networks with relu
  activation for piecewise linear target functions.
\newblock \emph{arXiv preprint arXiv:2108.04620}, 2021.

\bibitem[Ji and Telgarsky(2018)]{ji2018gradient}
Ziwei Ji and Matus Telgarsky.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock \emph{arXiv preprint arXiv:1810.02032}, 2018.

\bibitem[Ji and Telgarsky(2020)]{ji2020directional}
Ziwei Ji and Matus Telgarsky.
\newblock Directional convergence and alignment in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17176--17186, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lakshminarayanan and Singh(2020)]{lakshminarayanan2020deep}
Chandrashekar Lakshminarayanan and Amit~Vikram Singh.
\newblock Deep gated networks: A framework to understand training and
  generalisation in deep learning.
\newblock \emph{arXiv preprint arXiv:2002.03996}, 2020.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Li and Liang(2018)]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Li and Yuan(2017)]{li2017convergence}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Li et~al.(2020)Li, Ma, and Zhang]{li2020learning}
Yuanzhi Li, Tengyu Ma, and Hongyang~R Zhang.
\newblock Learning over-parametrized two-layer neural networks beyond ntk.
\newblock In \emph{Conference on learning theory}, pages 2613--2682. PMLR,
  2020.

\bibitem[Livni et~al.(2014)Livni, Shalev-Shwartz, and
  Shamir]{livni2014computational}
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir.
\newblock On the computational efficiency of training neural networks.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Lyu and Li(2019)]{lyu2019gradient}
Kaifeng Lyu and Jian Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock \emph{arXiv preprint arXiv:1906.05890}, 2019.

\bibitem[Lyu et~al.(2021)Lyu, Li, Wang, and Arora]{lyu2021gradient}
Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora.
\newblock Gradient descent on two-layer nets: Margin maximization and
  simplicity bias.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Ma et~al.(2020)Ma, Wu, and E]{ma2020quenching}
Chao Ma, Lei Wu, and Weinan E.
\newblock The quenching-activation behavior of the gradient descent dynamics
  for two-layer neural network models.
\newblock \emph{arXiv preprint arXiv:2006.14450}, 2020.

\bibitem[Ma et~al.(2022)Ma, Wu, and Ying]{ma2022multiscale}
Chao Ma, Lei Wu, and Lexing Ying.
\newblock The multiscale structure of neural network loss functions: The effect
  on optimization and origin.
\newblock \emph{arXiv preprint arXiv:2204.11326}, 2022.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{mei2018mean}
Song Mei, Andrea Montanari, and Phan-Minh Nguyen.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (33):\penalty0 E7665--E7671, 2018.

\bibitem[Mei et~al.(2019)Mei, Misiakiewicz, and Montanari]{mei2019mean}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Mean-field theory of two-layers neural networks: dimension-free
  bounds and kernel limit.
\newblock In \emph{Conference on Learning Theory}, pages 2388--2464. PMLR,
  2019.

\bibitem[Phuong and Lampert(2020)]{phuong2020inductive}
Mary Phuong and Christoph~H Lampert.
\newblock The inductive bias of relu networks on orthogonally separable data.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Safran and Shamir(2018)]{safran2018spurious}
Itay Safran and Ohad Shamir.
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock In \emph{International conference on machine learning}, pages
  4433--4441. PMLR, 2018.

\bibitem[Safran et~al.(2021)Safran, Yehudai, and Shamir]{safran2021effects}
Itay~M Safran, Gilad Yehudai, and Ohad Shamir.
\newblock The effects of mild over-parameterization on the optimization
  landscape of shallow relu neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 3889--3934. PMLR,
  2021.

\bibitem[Sun et~al.(2020)Sun, Li, Liang, Ding, and Srikant]{sun2020global}
Ruoyu Sun, Dawei Li, Shiyu Liang, Tian Ding, and Rayadurgam Srikant.
\newblock The global landscape of neural networks: An overview.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (5):\penalty0
  95--108, 2020.

\bibitem[Tian(2017)]{tian2017analytical}
Yuandong Tian.
\newblock An analytical formula of population gradient for two-layered relu
  network and its applications in convergence and critical point analysis.
\newblock In \emph{International conference on machine learning}, pages
  3404--3413. PMLR, 2017.

\bibitem[Wu(2022)]{wu2022learning}
Lei Wu.
\newblock Learning a single neuron for non-monotonic activation functions.
\newblock \emph{arXiv preprint arXiv:2202.08064}, 2022.

\bibitem[Wu et~al.(2018)Wu, Ma, and E]{wu2018sgd}
Lei Wu, Chao Ma, and Weinan E.
\newblock How sgd selects the global minima in over-parameterized learning: A
  dynamical stability perspective.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 8279--8288, 2018.

\bibitem[Yehudai and Ohad(2020)]{yehudai2020learning}
Gilad Yehudai and Shamir Ohad.
\newblock Learning a single neuron with gradient methods.
\newblock In \emph{Conference on Learning Theory}, pages 3756--3786. PMLR,
  2020.

\bibitem[Yehudai and Shamir(2019)]{yehudai2019power}
Gilad Yehudai and Ohad Shamir.
\newblock On the power and limitations of random features for understanding
  neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization
  (2016).
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2017.

\bibitem[Zhong et~al.(2017)Zhong, Song, Jain, Bartlett, and
  Dhillon]{zhong2017recovery}
Kai Zhong, Zhao Song, Prateek Jain, Peter~L Bartlett, and Inderjit~S Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In \emph{International conference on machine learning}, pages
  4140--4149. PMLR, 2017.

\bibitem[Zhou et~al.(2021)Zhou, Ge, and Jin]{zhou2021local}
Mo~Zhou, Rong Ge, and Chi Jin.
\newblock A local convergence theory for mildly over-parameterized two-layer
  neural network.
\newblock In \emph{Conference on Learning Theory}, pages 4577--4632. PMLR,
  2021.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock \emph{arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
