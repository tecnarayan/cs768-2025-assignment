\begin{thebibliography}{}

\bibitem[Abernethy et~al., 2009]{abernethy2009new}
Abernethy, J., Bach, F., Evgeniou, T., and Vert, J.-P. (2009).
\newblock A new approach to collaborative filtering: Operator estimation with
  spectral regularization.
\newblock {\em Journal of Machine Learning Research}, 10(Mar):803--826.

\bibitem[Argyriou et~al., 2008]{argyriou2008convex}
Argyriou, A., Evgeniou, T., and Pontil, M. (2008).
\newblock Convex multi-task feature learning.
\newblock {\em Machine Learning}, 73(3):243--272.

\bibitem[Balcan et~al., 2019]{balcan2019provable}
Balcan, M.-F., Khodak, M., and Talwalkar, A. (2019).
\newblock Provable guarantees for gradient-based meta-learning.
\newblock In {\em International Conference on Machine Learning}, pages
  424--433.

\bibitem[Baxter, 2000]{baxter2000model}
Baxter, J. (2000).
\newblock A model of inductive bias learning.
\newblock {\em J. Artif. Intell. Res.}, 12(149--198):3.

\bibitem[Bertinetto et~al., 2018]{bertinetto2018meta}
Bertinetto, L., Henriques, J.~F., Torr, P.~H., and Vedaldi, A. (2018).
\newblock Meta-learning with differentiable closed-form solvers.
\newblock {\em arXiv preprint arXiv:1805.08136}.

\bibitem[Bousquet and Elisseeff, 2002]{bousquet2002stability}
Bousquet, O. and Elisseeff, A. (2002).
\newblock Stability and generalization.
\newblock {\em Journal of machine learning research}, 2(Mar):499--526.

\bibitem[Bullins et~al., 2019]{hazan19}
Bullins, B., Hazan, E., Kalai, A., and Livni, R. (2019).
\newblock Generalize across tasks: Efficient algorithms for linear
  representation learning.
\newblock In {\em Algorithmic Learning Theory}, pages 235--246.

\bibitem[Cai et~al., 2020]{cai2020weighted}
Cai, T.~T., Liang, T., and Rakhlin, A. (2020).
\newblock Weighted message passing and minimum energy flow for heterogeneous
  stochastic block models with side information.
\newblock {\em Journal of Machine Learning Research}, 21(11):1--34.

\bibitem[Caruana, 1997]{caruana1997multitask}
Caruana, R. (1997).
\newblock Multitask learning.
\newblock {\em Machine learning}, 28(1):41--75.

\bibitem[Denevi et~al., 2019a]{denevi2019learning}
Denevi, G., Ciliberto, C., Grazzi, R., and Pontil, M. (2019a).
\newblock Learning-to-learn stochastic gradient descent with biased
  regularization.
\newblock In {\em International Conference on Machine Learning}, pages
  1566--1575.

\bibitem[Denevi et~al., 2018]{denevi2018}
Denevi, G., Ciliberto, C., Stamos, D., and Pontil, M. (2018).
\newblock Incremental learning-to-learn with statistical guarantees.
\newblock In {\em Proc. 34th Conference on Uncertainty in Artificial
  Intelligence (UAI)}.

\bibitem[Denevi et~al., 2020]{denevi2020advantage}
Denevi, G., Pontil, M., and Ciliberto, C. (2020).
\newblock The advantage of conditional meta-learning for biased regularization
  and fine tuning.
\newblock {\em Advances in Neural Information Processing Systems}, 33.

\bibitem[Denevi et~al., 2019b]{denevi2019online}
Denevi, G., Stamos, D., Ciliberto, C., and Pontil, M. (2019b).
\newblock Online-within-online meta-learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  13089--13099.

\bibitem[Finn et~al., 2017]{pmlr-v70-finn17a}
Finn, C., Abbeel, P., and Levine, S. (2017).
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, volume~70 of {\em Proceedings of Machine Learning Research}, pages
  1126--1135. PMLR.

\bibitem[Finn and Levine, 2018]{finn2018meta}
Finn, C. and Levine, S. (2018).
\newblock Meta-learning and universality: Deep representations and gradient
  descent can approximate any learning algorithm.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Finn et~al., 2019]{finn2019online}
Finn, C., Rajeswaran, A., Kakade, S., and Levine, S. (2019).
\newblock Online meta-learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1920--1930.

\bibitem[Harper and Konstan, 2015]{harper2015movielens}
Harper, F.~M. and Konstan, J.~A. (2015).
\newblock The movielens datasets: History and context.
\newblock {\em Acm transactions on interactive intelligent systems (tiis)},
  5(4):1--19.

\bibitem[Hogben, 2006]{hogben2006handbook}
Hogben, L. (2006).
\newblock {\em Handbook of linear algebra}.
\newblock CRC press.

\bibitem[Hogben, 2013]{hogben2013handbook}
Hogben, L. (2013).
\newblock {\em Handbook of linear algebra}.
\newblock CRC Press.

\bibitem[Jacob et~al., 2009]{jacob2009clustered}
Jacob, L., Vert, J.-p., and Bach, F.~R. (2009).
\newblock Clustered multi-task learning: A convex formulation.
\newblock In {\em Advances in neural information processing systems}, pages
  745--752.

\bibitem[Jerfel et~al., 2019]{jerfel2019reconciling}
Jerfel, G., Grant, E., Griffiths, T., and Heller, K.~A. (2019).
\newblock Reconciling meta-learning and continual learning with online mixtures
  of tasks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9119--9130.

\bibitem[Khodak et~al., 2019]{khodak2019adaptive}
Khodak, M., Balcan, M.-F.~F., and Talwalkar, A.~S. (2019).
\newblock Adaptive gradient-based meta-learning methods.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5915--5926.

\bibitem[Lenk et~al., 1996]{lenk1996hierarchical}
Lenk, P.~J., DeSarbo, W.~S., Green, P.~E., and Young, M.~R. (1996).
\newblock Hierarchical bayes conjoint analysis: Recovery of partworth
  heterogeneity from reduced experimental designs.
\newblock {\em Marketing Science}, 15(2):173--191.

\bibitem[Maurer, 2009]{maurer2009transfer}
Maurer, A. (2009).
\newblock Transfer bounds for linear feature learning.
\newblock {\em Machine Learning}, 75(3):327--350.

\bibitem[Maurer et~al., 2013]{maurer2013sparse}
Maurer, A., Pontil, M., and Romera-Paredes, B. (2013).
\newblock Sparse coding for multitask and transfer learning.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Maurer et~al., 2016]{maurer2016benefit}
Maurer, A., Pontil, M., and Romera-Paredes, B. (2016).
\newblock The benefit of multitask representation learning.
\newblock {\em The Journal of Machine Learning Research}, 17(1):2853--2884.

\bibitem[McDonald et~al., 2016]{Andrew}
McDonald, A.~M., Pontil, M., and Stamos, D. (2016).
\newblock New perspectives on k-support and cluster norms.
\newblock {\em Journal of Machine Learning Research}, 17(155):1--38.

\bibitem[Micchelli et~al., 2013]{micchelli2013regularizers}
Micchelli, C.~A., Morales, J.~M., and Pontil, M. (2013).
\newblock Regularizers for structured sparsity.
\newblock {\em Advances in Computational Mathematics}, 38(3):455--489.

\bibitem[Pentina and Lampert, 2014]{pentina2014pac}
Pentina, A. and Lampert, C. (2014).
\newblock A {PAC}-{B}ayesian bound for lifelong learning.
\newblock In {\em International Conference on Machine Learning}, pages
  991--999.

\bibitem[Rusu et~al., 2018]{rusu2018meta}
Rusu, A.~A., Rao, D., Sygnowski, J., Vinyals, O., Pascanu, R., Osindero, S.,
  and Hadsell, R. (2018).
\newblock Meta-learning with latent embedding optimization.
\newblock {\em arXiv preprint arXiv:1807.05960}.

\bibitem[Shalev-Shwartz and Ben-David, 2014]{shalev2014understanding}
Shalev-Shwartz, S. and Ben-David, S. (2014).
\newblock {\em Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press.

\bibitem[Steinwart and Christmann, 2008]{steinwart2008support}
Steinwart, I. and Christmann, A. (2008).
\newblock {\em Support vector machines}.
\newblock Springer Science \& Business Media.

\bibitem[Tripuraneni et~al., 2020]{tripuraneni2020provable}
Tripuraneni, N., Jin, C., and Jordan, M.~I. (2020).
\newblock Provable meta-learning of linear representations.
\newblock {\em arXiv preprint arXiv:2002.11684}.

\bibitem[Vuorio et~al., 2019]{vuorio2019multimodal}
Vuorio, R., Sun, S.-H., Hu, H., and Lim, J.~J. (2019).
\newblock Multimodal model-agnostic meta-learning via task-aware modulation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1--12.

\bibitem[Wang et~al., 2020]{wang2020structured}
Wang, R., Demiris, Y., and Ciliberto, C. (2020).
\newblock A structured prediction approach for conditional meta-learning.
\newblock {\em Advances in Neural Information Processing Systems}.

\bibitem[Yao et~al., 2019]{yao2019hierarchically}
Yao, H., Wei, Y., Huang, J., and Li, Z. (2019).
\newblock Hierarchically structured meta-learning.
\newblock {\em arXiv preprint arXiv:1905.05301}.

\end{thebibliography}
