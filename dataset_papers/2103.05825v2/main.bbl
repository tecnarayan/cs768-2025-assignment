\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson et~al.(2018)Anderson, Wu, Teney, Bruce, Johnson,
  S{\"u}nderhauf, Reid, Gould, and van~den Hengel]{anderson2018vision}
P.~Anderson, Q.~Wu, D.~Teney, J.~Bruce, M.~Johnson, N.~S{\"u}nderhauf, I.~Reid,
  S.~Gould, and A.~van~den Hengel.
\newblock Vision-and-language navigation: Interpreting visually-grounded
  navigation instructions in real environments.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR)}, 2018.

\bibitem[Andreas(2020)]{andreas2020geca}
J.~Andreas.
\newblock Good-enough compositional data augmentation.
\newblock In \emph{Association for Computational Linguistics (ACL)}, 2020.

\bibitem[Andreas et~al.(2017)Andreas, Klein, and Levine]{andreas2017sketches}
J.~Andreas, D.~Klein, and S.~Levine.
\newblock Modular multitask reinforcement learning with policy sketches.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{andrychowicz2017hindsight}
M.~Andrychowicz, F.~Wolski, A.~Ray, J.~Schneider, R.~Fong, P.~Welinder,
  B.~McGrew, J.~Tobin, P.~Abbeel, and W.~Zaremba.
\newblock Hindsight experience replay.
\newblock \emph{arXiv preprint arXiv:1707.01495}, 2017.

\bibitem[Arumugam et~al.(2017)Arumugam, Karamcheti, Gopalan, Wong, and
  Tellex]{arumugam2017accurately}
D.~Arumugam, S.~Karamcheti, N.~Gopalan, L.~L.~S. Wong, and S.~Tellex.
\newblock Accurately and efficiently interpreting human-robot instructions of
  varying granularities.
\newblock In \emph{Robotics: Science and Systems (RSS)}, 2017.

\bibitem[Bahdanau et~al.(2019)Bahdanau, Hill, Leike, Hughes, Hosseini, Kohli,
  and Grefenstette]{bahdanau2019reward}
D.~Bahdanau, F.~Hill, J.~Leike, E.~Hughes, S.~A. Hosseini, P.~Kohli, and
  E.~Grefenstette.
\newblock Learning to understand goal specifications by modelling reward.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Behboudian et~al.(2020)Behboudian, Satsangi, Taylor, Harutyunyan, and
  Bowling]{behboudian2020useful}
P.~Behboudian, Y.~Satsangi, M.~E. Taylor, A.~Harutyunyan, and M.~Bowling.
\newblock Useful policy invariant shaping from arbitrary advice.
\newblock \emph{arXiv preprint arXiv:2011.01297}, 2020.

\bibitem[Bisk et~al.(2020)Bisk, Holtzman, Thomason, Andreas, Bengio, Chai,
  Lapata, Lazaridou, May, Nisnevich, Pinto, and Turian]{bisk2020experience}
Y.~Bisk, A.~Holtzman, J.~Thomason, J.~Andreas, Y.~Bengio, J.~Chai, M.~Lapata,
  A.~Lazaridou, J.~May, A.~Nisnevich, N.~Pinto, and J.~Turian.
\newblock Experience grounds language.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  2020.

\bibitem[Burda et~al.(2019)Burda, Edwards, Pathak, Storkey, Darrell, and
  Efros]{burda2019curiosity}
Y.~Burda, H.~Edwards, D.~Pathak, A.~Storkey, T.~Darrell, and A.~A. Efros.
\newblock Large-scale study of curiosity-driven learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Campero et~al.(2020)Campero, Raileanu, K{\"{u}}ttler, Tenenbaum,
  Rockt{\"{a}}schel, and Grefenstette]{campero2020learning}
A.~Campero, R.~Raileanu, H.~K{\"{u}}ttler, J.~B. Tenenbaum,
  T.~Rockt{\"{a}}schel, and E.~Grefenstette.
\newblock Learning with {AMIG}o: Adversarially motivated intrinsic goals.
\newblock \emph{arXiv preprint arXiv:2006.12122}, 2020.

\bibitem[Chevalier-Boisvert et~al.(2019)Chevalier-Boisvert, Bahdanau, Lahlou,
  Willems, Saharia, Nguyen, and Bengio]{chevalierboisvert2019babyai}
M.~Chevalier-Boisvert, D.~Bahdanau, S.~Lahlou, L.~Willems, C.~Saharia, T.~H.
  Nguyen, and Y.~Bengio.
\newblock {BabyAI}: A platform to study the sample efficiency of grounded
  language learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Cideron et~al.(2019)Cideron, Seurin, Strub, and
  Pietquin]{cideron2019selfeducated}
G.~Cideron, M.~Seurin, F.~Strub, and O.~Pietquin.
\newblock Self-educated language agent with hindsight experience replay for
  instruction following.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Cobbe et~al.(2020)Cobbe, Hesse, Hilton, and
  Schulman]{cobbe2020leveraging}
K.~Cobbe, C.~Hesse, J.~Hilton, and J.~Schulman.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Colas et~al.(2020)Colas, Karch, Lair, Dussoux, Moulin-Frier, Dominey,
  and Oudeyer]{colas2020language}
C.~Colas, T.~Karch, N.~Lair, J.-M. Dussoux, C.~Moulin-Frier, P.~F. Dominey, and
  P.-Y. Oudeyer.
\newblock Language as a cognitive tool to imagine goals in curiosity driven
  exploration.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Das et~al.(2018)Das, Gkioxari, Lee, Parikh, and Batra]{das2018modular}
A.~Das, G.~Gkioxari, S.~Lee, D.~Parikh, and D.~Batra.
\newblock Neural modular control for embodied question answering.
\newblock In \emph{Conference on Robot Learning (CORL)}, 2018.

\bibitem[Fu et~al.(2019)Fu, Korattikara, Levine, and
  Guadarrama]{fu2019lang2goals}
J.~Fu, A.~Korattikara, S.~Levine, and S.~Guadarrama.
\newblock From language to goals: Inverse reinforcement learning for
  vision-based instruction following.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Goyal et~al.(2019{\natexlab{a}})Goyal, Niekum, and
  Mooney]{goyal2019shaping}
P.~Goyal, S.~Niekum, and R.~J. Mooney.
\newblock Using natural language for reward shaping in reinforcement learning.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2019{\natexlab{a}}.

\bibitem[Goyal et~al.(2019{\natexlab{b}})Goyal, Shalit, and
  Kim]{goyal2019explaining}
Y.~Goyal, U.~Shalit, and B.~Kim.
\newblock Explaining classifiers with causal concept effect ({CaCE}).
\newblock \emph{arXiv preprint arXiv:1907.07165}, 2019{\natexlab{b}}.

\bibitem[Grzes(2017)]{grzes2017reward}
M.~Grzes.
\newblock Reward shaping in episodic reinforcement learning.
\newblock In \emph{International Conference on Autonomous Agents and Multiagent
  Systems (AAMAS)}, 2017.

\bibitem[Hausknecht and Stone(2015)]{hausknecht2015deeprq}
M.~Hausknecht and P.~Stone.
\newblock Deep recurrent {Q}-learning for partially observable {MDP}s.
\newblock \emph{arXiv preprint arXiv:1507.06527}, 2015.

\bibitem[Hermann et~al.(2017)Hermann, Hill, Green, Wang, Faulkner, Soyer,
  Szepesvari, Czarnecki, Jaderberg, Teplyashin, Wainwright, Apps, Hassabis, and
  Blunsom]{hermann2017grounded}
K.~M. Hermann, F.~Hill, S.~Green, F.~Wang, R.~Faulkner, H.~Soyer,
  D.~Szepesvari, W.~Czarnecki, M.~Jaderberg, D.~Teplyashin, M.~Wainwright,
  C.~Apps, D.~Hassabis, and P.~Blunsom.
\newblock Grounded language learning in a simulated {3D} world.
\newblock \emph{arXiv preprint arXiv:1706.06551}, 2017.

\bibitem[Hill et~al.(2020)Hill, Mokra, Wong, and Harley]{hill2020human}
F.~Hill, S.~Mokra, N.~Wong, and T.~Harley.
\newblock Human instruction-following with deep reinforcement learning via
  transfer-learning from text.
\newblock \emph{arXiv preprint arXiv:2005.09382}, 2020.

\bibitem[Jia and Liang(2016)]{jia2016recombination}
R.~Jia and P.~Liang.
\newblock Data recombination for neural semantic parsing.
\newblock In \emph{Association for Computational Linguistics (ACL)}, 2016.

\bibitem[Jiang et~al.(2019)Jiang, Gu, Murphy, and Finn]{jiang2019abstraction}
Y.~Jiang, S.~S. Gu, K.~P. Murphy, and C.~Finn.
\newblock Language as an abstraction for hierarchical deep reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Karamcheti et~al.(2017)Karamcheti, Williams, Arumugam, Rhee, Gopalan,
  Wong, and Tellex]{karamcheti2017draggns}
S.~Karamcheti, E.~C. Williams, D.~Arumugam, M.~Rhee, N.~Gopalan, L.~L.~S. Wong,
  and S.~Tellex.
\newblock A tale of two draggns: A hybrid approach for interpreting
  action-oriented and goal-oriented instructions.
\newblock In \emph{First Workshop on Language Grounding for Robotics @ ACL},
  2017.

\bibitem[Karamcheti et~al.(2020)Karamcheti, Sadigh, and
  Liang]{karamcheti2020decomposition}
S.~Karamcheti, D.~Sadigh, and P.~Liang.
\newblock Learning adaptive language interfaces through decomposition.
\newblock In \emph{EMNLP Workshop for Interactive and Executable Semantic
  Parsing (IntEx-SemPar)}, 2020.

\bibitem[Kollar et~al.(2010)Kollar, Tellex, Roy, and Roy]{kollar10directions}
T.~Kollar, S.~Tellex, D.~Roy, and N.~Roy.
\newblock Toward understanding natural language directions.
\newblock In \emph{Human-Robot Interaction}, pages 259--266, 2010.

\bibitem[Luketina et~al.(2019)Luketina, Nardelli, Farquhar, Foerster, Andreas,
  Grefenstette, Whiteson, and Rockt{\"{a}}schel]{luketina2019survey}
J.~Luketina, N.~Nardelli, G.~Farquhar, J.~Foerster, J.~Andreas,
  E.~Grefenstette, S.~Whiteson, and T.~Rockt{\"{a}}schel.
\newblock A survey of reinforcement learning informed by natural language.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2019.

\bibitem[Lynch and Sermanet(2020)]{lynch2020grounding}
C.~Lynch and P.~Sermanet.
\newblock Grounding language in play.
\newblock \emph{arXiv preprint arXiv:2005.07648}, 2020.

\bibitem[McGovern and Barto(2001)]{mcgovern2001subgoals}
A.~McGovern and A.~G. Barto.
\newblock Automatic discovery of subgoals in reinforcement learning using
  diverse density.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2001.

\bibitem[Misra et~al.(2017)Misra, Langford, and Artzi]{misra2017mapping}
D.~K. Misra, J.~Langford, and Y.~Artzi.
\newblock Mapping instructions and visual observations to actions with
  reinforcement learning.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  2017.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
A.~Y. Ng, D.~Harada, and S.~Russell.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  volume~99, pages 278--287, 1999.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and
  Darrell]{pathak2017curiosity}
D.~Pathak, P.~Agrawal, A.~A. Efros, and T.~Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR)}, pages
  16--17, 2017.

\bibitem[Perez et~al.(2018)Perez, Strub, Vries, Dumoulin, and
  Courville]{perez2018film}
E.~Perez, F.~Strub, H.~D. Vries, V.~Dumoulin, and A.~C. Courville.
\newblock {FiLM}: Visual reasoning with a general conditioning layer.
\newblock In \emph{Association for the Advancement of Artificial Intelligence
  (AAAI)}, 2018.

\bibitem[Raileanu and Rockt{\"{a}}schel(2020)]{raileanu2020ride}
R.~Raileanu and T.~Rockt{\"{a}}schel.
\newblock {RIDE}: Rewarding impact-driven exploration for
  procedurally-generated environments.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Randl{\o{}}v and Alstr{\o{}}m(1998)]{randlov1998learning}
J.~Randl{\o{}}v and P.~Alstr{\o{}}m.
\newblock Learning to drive a bicycle using reinforcement learning and shaping.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 1998.

\bibitem[Schmidhuber(1991)]{schmidhuber1991adaptive}
J.~Schmidhuber.
\newblock Adaptive confidence and adaptive curiosity.
\newblock Technical report, Institut fur Informatik, Technische Universitat
  Munchen, Arcisstr. 21, 800 Munchen 2, 1991.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017ppo}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shridhar et~al.(2020)Shridhar, Thomason, Gordon, Bisk, Han, Mottaghi,
  Zettlemoyer, and Fox]{shridhar2020alfred}
M.~Shridhar, J.~Thomason, D.~Gordon, Y.~Bisk, W.~Han, R.~Mottaghi,
  L.~Zettlemoyer, and D.~Fox.
\newblock Alfred: A benchmark for interpreting grounded instructions for
  everyday tasks.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR)}, 2020.

\bibitem[Stolle and Precup(2002)]{stolle2002options}
M.~Stolle and D.~Precup.
\newblock Learning options in reinforcement learning.
\newblock In \emph{Proceedings of the 5th International Symposium on
  Abstraction, Reformulation and Approximation}, 2002.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton1999between}
R.~S. Sutton, D.~Precup, and S.~Singh.
\newblock Between {MDP}s and semi-{MDP}s: A framework for temporal abstraction
  in reinforcement learning.
\newblock \emph{Articial intelligence}, 112:\penalty0 181--211, 1999.

\bibitem[Tellex et~al.(2011)Tellex, Kollar, Dickerson, Walter, Banerjee,
  Teller, and Roy]{tellex2011understanding}
S.~Tellex, T.~Kollar, S.~Dickerson, M.~R. Walter, A.~G. Banerjee, S.~J. Teller,
  and N.~Roy.
\newblock Understanding natural language commands for robotic navigation and
  mobile manipulation.
\newblock In \emph{Association for the Advancement of Artificial Intelligence
  (AAAI)}, 2011.

\bibitem[Thomason et~al.(2015)Thomason, Zhang, Mooney, and
  Stone]{thomason2015learning}
J.~Thomason, S.~Zhang, R.~J. Mooney, and P.~Stone.
\newblock Learning to interpret natural language commands through human-robot
  dialog.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2015.

\bibitem[Wang et~al.(2017)Wang, Ginn, Liang, and Manning]{wang2017naturalizing}
S.~I. Wang, S.~Ginn, P.~Liang, and C.~D. Manning.
\newblock Naturalizing a programming language via interactive learning.
\newblock In \emph{Association for Computational Linguistics (ACL)}, 2017.

\bibitem[Wang et~al.(2019)Wang, Huang, Celikyilmaz, Gao, Shen, Wang, Wang, and
  Zhang]{wang2019rcm}
X.~E. Wang, Q.~Huang, A.~Celikyilmaz, J.~Gao, D.~Shen, Y.-F. Wang, W.~Y. Wang,
  and L.~Zhang.
\newblock Reinforced cross-modal matching and self-supervised imitation
  learning for vision-language navigation.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR)}, 2019.

\bibitem[Waytowich et~al.(2019)Waytowich, Barton, Lawhern, and
  Warnell]{waytowich2019narration}
N.~Waytowich, S.~L. Barton, V.~Lawhern, and G.~Warnell.
\newblock A narration-based reward shaping approach using grounded natural
  language commands.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Yu et~al.(2017)Yu, Zhang, and Xu]{yu2017compositional}
H.~Yu, H.~Zhang, and W.~Xu.
\newblock A deep compositional framework for human-like language acquisition in
  virtual environment.
\newblock \emph{arXiv preprint arXiv:1703.09831}, 2017.

\end{thebibliography}
