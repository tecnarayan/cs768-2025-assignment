
%%%%%new citations from icml reviews
@article{levy2017generalizing,
  title={Generalizing hamiltonian monte carlo with neural networks},
  author={Levy, Daniel and Hoffman, Matthew D and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.09268},
  year={2017}
}

@inproceedings{caterini2018hamiltonian,
  title={Hamiltonian variational auto-encoder},
  author={Caterini, Anthony L and Doucet, Arnaud and Sejdinovic, Dino},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8167--8177},
  year={2018}
}

@article{ruiz2019contrastive,
  title={A contrastive divergence for combining variational inference and mcmc},
  author={Ruiz, Francisco JR and Titsias, Michalis K},
  journal={arXiv preprint arXiv:1905.04062},
  year={2019}
}
%%%%%

@article{murray2012gpu,
  title={GPU acceleration of the particle filter: the Metropolis resampler},
  author={Murray, Lawrence},
  journal={arXiv preprint arXiv:1202.6163},
  year={2012}
}

@misc{murray2013parallel,
    title={Parallel resampling in the particle filter},
    author={Lawrence M. Murray and Anthony Lee and Pierre E. Jacob},
    year={2013},
    eprint={1301.4019},
    archivePrefix={arXiv},
    primaryClass={stat.CO}
}

@book{brooks2011handbook,
  title={Handbook of markov chain monte carlo},
  author={Brooks, Steve and Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
  year={2011},
  publisher={CRC press}
}


@inproceedings{le2019revisiting,
  title = {Revisiting Reweighted Wake-Sleep for Models with Stochastic Control Flow},
  author = {Le, Tuan Anh and Kosiorek, Adam R. and Siddharth, N. and Teh, Yee Whye and Wood, Frank},
  booktitle = {Uncertainty in Artificial Intelligence},
  year = {2019},
  note = {Le and Kosiorek contributed equally.}
}


@inproceedings{srivastava2015unsupervised,
  title={Unsupervised learning of video representations using lstms},
  author={Srivastava, Nitish and Mansimov, Elman and Salakhudinov, Ruslan},
  booktitle={International conference on machine learning},
  pages={843--852},
  year={2015}
}


@inproceedings{mnih2016variational,
  title = {Variational {{Inference}} for {{Monte Carlo Objectives}}},
  abstract = {Recent progress in deep latent variable models has largely been driven by the development of flexible and scalable variational inference methods. Variational training of this type involves maximizi...},
  language = {en},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Mnih, Andriy and Rezende, Danilo},
  month = jun,
  year = {2016},
  pages = {2188-2196},
  file = {/Users/janwillem/Zotero/storage/7XIFG2DU/Mnih and Rezende - 2016 - Variational Inference for Monte Carlo Objectives.pdf}
}

@inproceedings{tucker2017rebar,
  title = {{{REBAR}}: {{Low}}-Variance, Unbiased Gradient Estimates for Discrete Latent Variable Models},
  shorttitle = {{{REBAR}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tucker, George and Mnih, Andriy and Maddison, Chris J. and Lawson, John and {Sohl-Dickstein}, Jascha},
  year = {2017},
  pages = {2624--2633},
  file = {/Users/janwillem/Zotero/storage/B7QJ66R6/Tucker - 2017 - REBAR.pdf;/Users/janwillem/Zotero/storage/V6ZYDCJD/6856-rebar-low-variance-unbiased-gradient-estimates-for-discrete-latent-variable-models.html}
}


@article{grathwohl2018backpropagation,
  title = {Backpropagation through the {{Void}}: {{Optimizing}} Control Variates for Black-Box Gradient Estimation},
  shorttitle = {Backpropagation through the {{Void}}},
  journal = {International Conference on Learning Representations},
  author = {Grathwohl, Will and Choi, Dami and Wu, Yuhuai and Roeder, Geoff and Duvenaud, David},
  year = {2018},
  file = {/Users/janwillem/Zotero/storage/SYLN75CQ/Grathwohl - 2017 - Backpropagation through the Void.pdf;/Users/janwillem/Zotero/storage/6G235HQZ/1711.html}
}

@article{weber2019credit,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1901.01761},
  primaryClass = {cs, stat},
  title = {Credit {{Assignment Techniques}} in {{Stochastic Computation Graphs}}},
  abstract = {Stochastic computation graphs (SCGs) provide a formalism to represent structured optimization problems arising in artificial intelligence, including supervised, unsupervised, and reinforcement learning. Previous work has shown that an unbiased estimator of the gradient of the expected loss of SCGs can be derived from a single principle. However, this estimator often has high variance and requires a full model evaluation per data point, making this algorithm costly in large graphs. In this work, we address these problems by generalizing concepts from the reinforcement learning literature. We introduce the concepts of value functions, baselines and critics for arbitrary SCGs, and show how to use them to derive lower-variance gradient estimates from partial model evaluations, paving the way towards general and efficient credit assignment for gradient-based optimization. In doing so, we demonstrate how our results unify recent advances in the probabilistic inference and reinforcement learning literature.},
  language = {en},
  journal = {arXiv:1901.01761 [cs, stat]},
  author = {Weber, Th{\'e}ophane and Heess, Nicolas and Buesing, Lars and Silver, David},
  month = jan,
  year = {2019},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  file = {/Users/janwillem/Zotero/storage/VIUG7PBY/Weber et al. - 2019 - Credit Assignment Techniques in Stochastic Computa.pdf}
}

@article{maddison2016concrete,
  title={The concrete distribution: A continuous relaxation of discrete random variables},
  author={Maddison, Chris J and Mnih, Andriy and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1611.00712},
  year={2016}
}

@inproceedings{jang2017categorical,
  title={Categorical Reparametrization with Gumble-Softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  booktitle={International Conference on Learning Representations 2017},
  year={2017},
  organization={OpenReviews. net}
}

@article{williams1992simple,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  volume = {8},
  issn = {0885-6125},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  number = {3-4},
  journal = {Machine Learning},
  doi = {10.1007/BF00992696},
  author = {Williams, Ronald J.},
  year = {1992},
  keywords = {Reinforcement learning,connectionist networks,gradient descent,mathematical analysis},
  pages = {229-256},
  file = {/Users/janwillem/Zotero/storage/FP24PEJJ/Williams - 1992 - Simple statistical gradient-following algorithms for connectionist.pdf}
}

@inproceedings{naesseth2018variational,
  title = {Variational {{Sequential Monte Carlo}}},
  abstract = {Many recent advances in large scale probabilistic inference rely on variational methods. The success of variational approaches depends on (i) formulating a flexible parametric family of distributio...},
  language = {en},
  booktitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Naesseth, Christian and Linderman, Scott and Ranganath, Rajesh and Blei, David},
  month = mar,
  year = {2018},
  pages = {968-977},
  file = {/Users/janwillem/Zotero/storage/T2QS6IJY/Naesseth et al. - 2018 - Variational Sequential Monte Carlo.pdf}
}




@inproceedings{le2018auto-encoding,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.10306},
  title = {Auto-{{Encoding Sequential Monte Carlo}}},
  abstract = {We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Le, Tuan Anh and Igl, Maximilian and Rainforth, Tom and Jin, Tom and Wood, Frank},
  year = {2018},
  keywords = {Statistics - Machine Learning},
  file = {/Users/janwillem/Zotero/storage/FDZEBMIF/Le et al. - 2017 - Auto-Encoding Sequential Monte Carlo.pdf}
}




@incollection{maddison2017filtering,
  title = {Filtering {{Variational Objectives}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  publisher = {{Curran Associates, Inc.}},
  author = {Maddison, Chris J and Lawson, John and Tucker, George and Heess, Nicolas and Norouzi, Mohammad and Mnih, Andriy and Doucet, Arnaud and Teh, Yee},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {6573--6583},
  file = {/Users/janwillem/Zotero/storage/NTI2KB8Q/Maddison - 2017 - Filtering Variational Objectives.pdf}
}




@inproceedings{burda2016importance,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.00519},
  title = {Importance {{Weighted Autoencoders}}},
  abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  booktitle = {International {{Conference}} on {{Representations}}},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  year = {2016},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  file = {/Users/janwillem/Zotero/storage/6X4GWDCZ/Burda et al. - 2015 - Importance Weighted Autoencoders.pdf}
}



@book{bishop2006pattern,
  address = {{New York}},
  title = {Pattern Recognition and Machine Learning},
  isbn = {978-0-387-31073-2},
  publisher = {{Springer}},
  author = {Bishop, C M},
  year = {2006},
  file = {/Users/janwillem/Zotero/storage/8VS6RH8A/Bishop - 2006 - Pattern recognition and machine learning.pdf}
}




@phdthesis{beal2003variational,
  title = {Variational {{Algorithms}} for {{Approximate Bayesian Inference}}},
  abstract = {The Bayesian framework for machine learning allows for the incorporation of prior knowledge in a coherent way, avoids overfitting problems, and provides a principled basis for selecting between alternative models. Unfortunately the computations required are usually intractable. This thesis presents a unified variational Bayesian (VB) framework which approximates these computations in models with latent variables using a lower bound on the marginal likelihood. Chapter 1 presents background material on Bayesian inference, graphical models, and propaga- tion algorithms. Chapter 2 forms the theoretical core of the thesis, generalising the expectation- maximisation (EM) algorithm for learning maximum likelihood parameters to the VB EM al- gorithm which integrates over model parameters. The algorithm is then specialised to the large family of conjugate-exponential (CE) graphical models, and several theorems are presented to pave the road for automated VB derivation procedures in both directed and undirected graphs (Bayesian and Markov networks, respectively). Chapters 3-5 derive and apply the VB EM algorithm to three commonly-used and important models: mixtures of factor analysers, linear dynamical systems, and hidden Markov models. It is shown how model selection tasks such as determining the dimensionality, cardinality, or number of variables are possible using VB approximations. Also explored are methods for combining sampling procedures with variational approximations, to estimate the tightness of VB bounds and to obtain more effective sampling algorithms. Chapter 6 applies VB learning to a long-standing problem of scoring discrete-variable directed acyclic graphs, and compares the performance to annealed importance sampling amongst other methods. Throughout, the VB approximation is compared to other methods including sampling, Cheeseman-Stutz, and asymptotic approximations such as BIC. The thesis concludes with a discussion of evolving directions for model selection including infinite models and alternative approximations to the marginal likelihood. 2},
  author = {Beal, Matthew J},
  year = {2003},
  file = {/Users/janwillem/Zotero/storage/UN9GEVXJ/Beal - 2003 - Variational Algorithms for Approximate Bayesian Inference.pdf}
}



@inproceedings{johnson2016composing,
  title = {Composing Graphical Models with Neural Networks for Structured Representations and Fast Inference},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Johnson, Matthew and Duvenaud, David K. and Wiltschko, Alex and Adams, Ryan P. and Datta, Sandeep R.},
  year = {2016},
  pages = {2946--2954},
  file = {/Users/janwillem/Zotero/storage/YIY5XRGW/Johnson - 2016 - Composing graphical models with neural networks for structured representations.pdf;/Users/janwillem/Zotero/storage/Z6E9ZYIT/6378-composing-graphical-models-with-neural-networks-for-structured-representations-and-fast-in.html}
}



@article{kingma2013auto-encoding,
  title = {Auto-Encoding Variational Bayes},
  journal = {International Conference on Learning Representations},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2013},
  file = {/Users/janwillem/Zotero/storage/STCIKV6S/Kingma - 2013 - Auto-encoding variational bayes.pdf;/Users/janwillem/Zotero/storage/2Q7PK8GU/1312.html}
}


@inproceedings{rezende2014stochastic,
  address = {{Bejing, China}},
  series = {Proceedings of {{Machine Learning Research}}},
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  volume = {32},
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation \textendash{} rules for gradient backpropagation through stochastic variables \textendash{} and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  publisher = {{PMLR}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  editor = {Xing, Eric P. and Jebara, Tony},
  month = jun,
  year = {2014},
  pages = {1278-1286},
  file = {/Users/janwillem/Zotero/storage/FDD9NQIT/Rezende - 2014 - Stochastic Backpropagation and Approximate Inference in Deep Generative Models.pdf},
  number = {2}
}

@incollection{kosiorek2018sequential,
  title = {Sequential {{Attend}}, {{Infer}}, {{Repeat}}: {{Generative Modelling}} of {{Moving Objects}}},
  shorttitle = {Sequential {{Attend}}, {{Infer}}, {{Repeat}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  publisher = {{Curran Associates, Inc.}},
  author = {Kosiorek, Adam and Kim, Hyunjik and Teh, Yee Whye and Posner, Ingmar},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {8606--8616}
}




@inproceedings{esmaeili2019structured,
  title = {Structured {{Neural Topic Models}} for {{Reviews}}},
  abstract = {We present Variational Aspect-based Latent Topic Allocation (VALTA), a family of autoencoding topic models that learn aspect-based representations of reviews. VALTA defines a user-item encoder that...},
  language = {en},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Esmaeili, Babak and Huang, Hongyi and Wallace, Byron and van de Meent, Jan-Willem},
  month = apr,
  year = {2019},
  pages = {3429-3439},
  file = {/Users/janwillem/Zotero/storage/GSMNVYWF/Esmaeili et al. - 2019 - Structured Neural Topic Models for Reviews.pdf},
  note = {00000}
}



@incollection{eslami2016attend,
  title = {Attend, {{Infer}}, {{Repeat}}: {{Fast Scene Understanding}} with {{Generative Models}}},
  shorttitle = {Attend, {{Infer}}, {{Repeat}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  publisher = {{Curran Associates, Inc.}},
  author = {Eslami, S. M. Ali and Heess, Nicolas and Weber, Theophane and Tassa, Yuval and Szepesvari, David and {kavukcuoglu}, koray and Hinton, Geoffrey E},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {3225--3233}
}




@article{mohamed2019monte,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1906.10652},
  primaryClass = {cs, math, stat},
  title = {Monte {{Carlo Gradient Estimation}} in {{Machine Learning}}},
  abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators--exploring their historical developments, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
  journal = {arXiv:1906.10652 [cs, math, stat]},
  author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  month = jun,
  year = {2019},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/janwillem/Zotero/storage/GKNH6SEU/Mohamed et al. - 2019 - Monte Carlo Gradient Estimation in Machine Learnin.pdf},
  note = {00000}
}



@article{wainwright2008graphical,
  title = {Graphical {{Models}}, {{Exponential Families}}, and {{Variational Inference}}},
  volume = {1},
  number = {1\textendash{}2},
  journal = {Foundations and Trends in Machine Learning},
  doi = {10.1561/2200000001},
  author = {Wainwright, Martin J and Jordan, Michael I},
  year = {2008},
  pages = {1-305},
}

@article{delmoral2006sequential,
  title = {Sequential {{Monte Carlo}} Samplers},
  volume = {68},
  number = {3},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  doi = {10.1111/j.1467-9868.2006.00553.x},
  author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
  month = jun,
  year = {2006},
  keywords = {markov chain monte carlo,constants,importance sampling,methods,ratio of normalizing,resampling,sequential monte carlo methods,simulated annealing},
  pages = {411-436},
  file = {/Users/janwillem/Zotero/storage/5EEY5NTT/Del Moral - 2006 - Sequential Monte Carlo samplers.pdf}
}

@article{tucker2018doubly,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.04152},
  primaryClass = {cs, stat},
  title = {Doubly {{Reparameterized Gradient Estimators}} for {{Monte Carlo Objectives}}},
  abstract = {Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma \& Welling, 2013; Rezende et al., 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al., 2018; Le et al., 2018). Roeder et al. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein \& Bengio, 2014), and the jackknife variational inference (JVI) gradient (Nowozin, 2018). Finally, we show that this computationally efficient, unbiased drop-in gradient estimator translates to improved performance for all three objectives on several modeling tasks.},
  journal = {arXiv:1810.04152 [cs, stat]},
  author = {Tucker, George and Lawson, Dieterich and Gu, Shixiang and Maddison, Chris J.},
  month = oct,
  year = {2018},
  keywords = {Statistics - Machine Learning,Computer Science - Machine Learning},
  file = {/Users/janwillem/Zotero/storage/ACJZ37NY/Tucker et al. - 2018 - Doubly Reparameterized Gradient Estimators for Mon.pdf}
}

@article{maddison2017concrete,
  title = {The Concrete Distribution: {{A}} Continuous Relaxation of Discrete Random Variables},
  shorttitle = {The Concrete Distribution},
  journal = {International Conference on Learning Representations},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  year = {2017},
  file = {/Users/janwillem/Zotero/storage/TMFZZVTS/Maddison - 2016 - The concrete distribution.pdf;/Users/janwillem/Zotero/storage/SI8DKSUC/1611.html}
}


@article{gilks2001following,
  title = {Following a Moving Target\textemdash{{Monte Carlo}} Inference for Dynamic {{Bayesian}} Models},
  volume = {63},
  copyright = {2001 Royal Statistical Society},
  issn = {1467-9868},
  abstract = {Markov chain Monte Carlo (MCMC) sampling is a numerically intensive simulation technique which has greatly improved the practicality of Bayesian inference and prediction. However, MCMC sampling is too slow to be of practical use in problems involving a large number of posterior (target) distributions, as in dynamic modelling and predictive model selection. Alternative simulation techniques for tracking moving target distributions, known as particle filters, which combine importance sampling, importance resampling and MCMC sampling, tend to suffer from a progressive degeneration as the target sequence evolves. We propose a new technique, based on these same simulation methodologies, which does not suffer from this progressive degeneration.},
  language = {en},
  number = {1},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  doi = {10.1111/1467-9868.00280},
  author = {Gilks, Walter R. and Berzuini, Carlo},
  year = {2001},
  keywords = {Bayesian inference,Dynamic model,Hidden Markov model,Importance resampling,Importance sampling,Markov chain Monte Carlo methods,Particle filter,Predictive model selection,Sequential imputation,Simulation,Tracking},
  pages = {127-146},
  file = {/Users/janwillem/Zotero/storage/GIPPY67P/Gilks and Berzuini - 2001 - Following a moving target—Monte Carlo inference fo.pdf}
}




@inproceedings{wood2014new,
  title = {A New Approach to Probabilistic Programming Inference},
  abstract = {We introduce and demonstrate a new approach to inference in expressive probabilistic programming languages based on particle Markov chain Monte Carlo. Our approach is simple to implement and easy to parallelize. It applies to Turing-complete probabilistic programming languages and supports accurate inference in models that make use of complex control flow, including stochastic recursion. It also includes primitives from Bayesian nonparametric statistics. Our experiments show that this approach can be more efficient than previously introduced single-site Metropolis-Hastings methods.},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Wood, Frank and {van de Meent}, Jan-Willem and Mansinghka, Vikash},
  year = {2014},
  pages = {1024-1032},
  file = {/Users/janwillem/Zotero/storage/S8MXWWHV/Wood - 2014 - A new approach to probabilistic programming inference.pdf}
}

@article{hinton1995wake,
  title={The" wake-sleep" algorithm for unsupervised neural networks},
  author={Hinton, Geoffrey E and Dayan, Peter and Frey, Brendan J and Neal, Radford M},
  journal={Science},
  volume={268},
  number={5214},
  pages={1158--1161},
  year={1995},
  publisher={American Association for the Advancement of Science}
}d

@article{neal2001annealed,
  title={Annealed importance sampling},
  author={Neal, Radford M},
  journal={Statistics and computing},
  volume={11},
  number={2},
  pages={125--139},
  year={2001},
  publisher={Springer}
}

@article{bornschein2014reweighted,
  title={Reweighted wake-sleep},
  author={Bornschein, J{\"o}rg and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.2751},
  year={2014}
}

@inproceedings{naesseth2015nested,
  title={Nested sequential monte carlo methods},
  author={Naesseth, Christian and Lindsten, Fredrik and Schon, Thomas},
  booktitle={International Conference on Machine Learning},
  pages={1292--1301},
  year={2015}
}

@inproceedings{siddharth2017learning,
  title = {Learning Disentangled Representations with Semi-Supervised Deep Generative Models},
  author = {Siddharth, N. and Paige, Brooks and van de Meent, Jan-Willem and Desmaison, Alban and Goodman, Noah D. and Kohli, Pushmeet and Wood, Frank and Torr, Philip},
  booktitle = {Advances in Neural Information Processing Systems 30},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  pages = {5927--5937},
  year = {2017}
}

@article{paszke2017automatic,
  title = {Automatic Differentiation in {{PyTorch}}},
  author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year = {2017}
}

@article{bingham2018pyro,
  title = {Pyro: {{Deep Universal Probabilistic Programming}}},
  shorttitle = {Pyro},
  language = {en},
  author = {Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D.},
  month = oct,
  year = {2018}
}

@article{tran2016edward,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.09787},
  primaryClass = {cs, stat},
  title = {Edward: {{A}} Library for Probabilistic Modeling, Inference, and Criticism},
  shorttitle = {Edward},
  abstract = {Probabilistic modeling is a powerful approach for analyzing empirical information. We describe Edward, a library for probabilistic modeling. Edward's design reflects an iterative process pioneered by George Box: build a model of a phenomenon, make inferences about the model given data, and criticize the model's fit to the data. Edward supports a broad class of probabilistic models, efficient algorithms for inference, and many techniques for model criticism. The library builds on top of TensorFlow to support distributed training and hardware such as GPUs. Edward enables the development of complex probabilistic models and their algorithms at a massive scale.},
  journal = {arXiv:1610.09787 [cs, stat]},
  author = {Tran, Dustin and Kucukelbir, Alp and Dieng, Adji B. and Rudolph, Maja and Liang, Dawen and Blei, David M.},
  month = oct,
  year = {2016},
  keywords = {Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Programming Languages,Statistics - Computation,Statistics - Applications},
  file = {/Users/janwillem/Zotero/storage/A6GQ8EIZ/Tran - 2016 - Edward.pdf}
}

@article{kim2018semi,
  title={Semi-Amortized Variational Autoencoders},
  author={Kim, Yoon and Wiseman, Sam and Miller, Andrew C and Sontag, David and Rush, Alexander M},
  journal={arXiv preprint arXiv:1802.02550},
  year={2018}
}


@article{molchanov2018doubly,
  title={Doubly Semi-Implicit Variational Inference},
  author={Molchanov, Dmitry and Kharitonov, Valery and Sobolev, Artem and Vetrov, Dmitry},
  journal={arXiv preprint arXiv:1810.02789},
  year={2018}
}

@article{jaynes1982rationale,
  title={On the rationale of maximum-entropy methods},
  author={Jaynes, Edwin T},
  journal={Proceedings of the IEEE},
  volume={70},
  number={9},
  pages={939--952},
  year={1982},
  publisher={IEEE}
}

@inproceedings{huang2018improving,
  title={Improving explorability in variational inference with annealed variational objectives},
  author={Huang, Chin-Wei and Tan, Shawn and Lacoste, Alexandre and Courville, Aaron C},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9701--9711},
  year={2018}
}

@article{pakman2018amortized,
  title={Amortized Bayesian inference for clustering models},
  author={Pakman, Ari and Paninski, Liam},
  journal={arXiv preprint arXiv:1811.09747},
  year={2018}
}

@article{li2017approximate,
  title={Approximate inference with amortised mcmc},
  author={Li, Yingzhen and Turner, Richard E and Liu, Qiang},
  journal={arXiv preprint arXiv:1702.08343},
  year={2017}
}

@article{marino2018iterative,
  title={Iterative amortized inference},
  author={Marino, Joseph and Yue, Yisong and Mandt, Stephan},
  journal={International Conference on Machine Learning},
  year={2018}
}

@article{huang2019hierarchical,
  title={Hierarchical Importance Weighted Autoencoders},
  author={Huang, Chin-Wei and Sankaran, Kris and Dhekane, Eeshan and Lacoste, Alexandre and Courville, Aaron},
  journal={arXiv preprint arXiv:1905.04866},
  year={2019}
}

@inproceedings{cremer2018inference,
  title={Inference Suboptimality in Variational Autoencoders},
  author={Cremer, Chris and Li, Xuechen and Duvenaud, David},
  booktitle={International Conference on Machine Learning},
  pages={1086--1094},
  year={2018}
}

@inproceedings{wang2018meta,
  title={Meta-learning MCMC proposals},
  author={Wang, Tongzhou and Wu, Yi and Moore, Dave and Russell, Stuart J},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4146--4156},
  year={2018}
}

@inproceedings{salimans2015markov,
  title={Markov chain monte carlo and variational inference: Bridging the gap},
  author={Salimans, Tim and Kingma, Diederik and Welling, Max},
  booktitle={International Conference on Machine Learning},
  pages={1218--1226},
  year={2015}
}


@book{doucet2001sequential,
  address = {{New York, NY}},
  title = {Sequential {{Monte Carlo Methods}} in {{Practice}}},
  isbn = {978-1-4419-2887-0 978-1-4757-3437-9},
  publisher = {{Springer New York}},
  editor = {Doucet, Arnaud and Freitas, Nando and Gordon, Neil},
  year = {2001},
  doi = {10.1007/978-1-4757-3437-9}
}



@inproceedings{maaloe2016auxiliary,
  title={Auxiliary Deep Generative Models},
  author={Maal{\o}e, Lars and S{\o}nderby, Casper Kaae and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  booktitle={International Conference on Machine Learning},
  pages={1445--1453},
  year={2016}
}

@inproceedings{ranganath2016hierarchical,
  title={Hierarchical variational models},
  author={Ranganath, Rajesh and Tran, Dustin and Blei, David},
  booktitle={International Conference on Machine Learning},
  pages={324--333},
  year={2016}
}

@inproceedings{hoffman2017learning,
  title={Learning deep latent Gaussian models with Markov chain Monte Carlo},
  author={Hoffman, Matthew D},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1510--1519},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{jaderberg2015spatial,
  title={Spatial transformer networks},
  author={Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2017--2025},
  year={2015}
}