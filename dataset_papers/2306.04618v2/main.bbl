\begin{thebibliography}{100}

\bibitem{agarwal2021temporal}
Oshin Agarwal and Ani Nenkova.
\newblock Temporal effects on pre-trained models for language processing tasks.
\newblock {\em arXiv preprint arXiv:2111.12790}, 2021.

\bibitem{Arora2021TypesOO}
Udit Arora, William Huang, and He~He.
\newblock Types of out-of-distribution texts and how to detect them.
\newblock In {\em Proceedings of EMNLP}, 2021.

\bibitem{au-etal-2022-ener}
Ting Wai~Terence Au, Vasileios Lampos, and Ingemar Cox.
\newblock {E}-{NER} {---} an annotated named entity recognition corpus of legal
  text.
\newblock In {\em Proceedings of the Natural Legal Language Processing
  Workshop}, 2022.

\bibitem{bartolo-etal-2020-advqa}
Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus
  Stenetorp.
\newblock Beat the {AI}: Investigating adversarial human annotation for reading
  comprehension.
\newblock {\em Transactions of ACL}, 2020.

\bibitem{bastan2022bionli}
Mohaddeseh Bastan, Mihai Surdeanu, and Niranjan Balasubramanian.
\newblock Bionli: Generating a biomedical nli dataset using lexico-semantic
  constraints for adversarial examples.
\newblock In {\em Findings of EMNLP}, 2022.

\bibitem{borkan2019civil_comments}
Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman.
\newblock Nuanced metrics for measuring unintended bias with real data for text
  classification.
\newblock {\em CoRR}, 2019.

\bibitem{bowman-etal-2015-snli}
Samuel~R. Bowman, Gabor Angeli, Christopher Potts, and Christopher~D. Manning.
\newblock A large annotated corpus for learning natural language inference.
\newblock In {\em Proceedings of EMNLP}, 2015.

\bibitem{bowman-dahl-2021-will}
Samuel~R. Bowman and George Dahl.
\newblock What will it take to fix benchmarking in natural language
  understanding?
\newblock In {\em Proceedings of NAACL}, 2021.

\bibitem{brown2020gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In {\em Preceedings of NeurIPS}, 2020.

\bibitem{chandra-etal-2020-abuseanalyzer}
Mohit Chandra, Ashwin Pathak, Eesha Dutta, Paryul Jain, Manish Gupta, Manish
  Shrivastava, and Ponnurangam Kumaraguru.
\newblock {A}buse{A}nalyzer: Abuse detection, severity and target prediction
  for gab posts.
\newblock In {\em Proceedings of COLING}, 2020.

\bibitem{chen-etal-2021-hiddencut}
Jiaao Chen, Dinghan Shen, Weizhu Chen, and Diyi Yang.
\newblock {H}idden{C}ut: Simple data augmentation for natural language
  understanding with better generalizability.
\newblock In {\em Proceedings of ACL-IJCNLP}, 2021.

\bibitem{chen2022should}
Yangyi Chen, Hongcheng Gao, Ganqu Cui, Fanchao Qi, Longtao Huang, Zhiyuan Liu,
  and Maosong Sun.
\newblock Why should adversarial perturbations be imperceptible? rethink the
  research paradigm in adversarial nlp.
\newblock In {\em Proceedings of EMNLP}, 2022.

\bibitem{chen2023adversarial}
Yangyi Chen, Hongcheng Gao, Ganqu Cui, Lifan Yuan, Dehan Kong, Hanlu Wu, Ning
  Shi, Bo~Yuan, Longtao Huang, Hui Xue, et~al.
\newblock From adversarial arms race to model-centric evaluation: Motivating a
  unified automatic robustness evaluation framework.
\newblock In {\em Findings of ACL}, 2023.

\bibitem{chen2022close}
Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji.
\newblock A close look into the calibration of pre-trained language models.
\newblock In {\em Proceedings of ACL}, 2023.

\bibitem{cheng-etal-2021-posterior}
Hao Cheng, Xiaodong Liu, Lis Pereira, Yaoliang Yu, and Jianfeng Gao.
\newblock Posterior differential regularization with f-divergence for improving
  model robustness.
\newblock In {\em Proceedings of NAACL:HLT}, 2021.

\bibitem{clark-etal-2019-dont}
Christopher Clark, Mark Yatskar, and Luke Zettlemoyer.
\newblock Don{'}t take the easy way out: Ensemble based methods for avoiding
  known dataset biases.
\newblock In {\em Proceedings of EMNLP-IJCNLP}, 2019.

\bibitem{clark2020learning}
Christopher Clark, Mark Yatskar, and Luke Zettlemoyer.
\newblock Learning to model and ignore dataset bias with mixed capacity
  ensembles.
\newblock {\em arXiv preprint arXiv:2011.03856}, 2020.

\bibitem{DBLP:conf/nips/CuiYHCLS22}
Ganqu Cui, Lifan Yuan, Bingxiang He, Yangyi Chen, Zhiyuan Liu, and Maosong Sun.
\newblock A unified evaluation of textual backdoor learning: Frameworks and
  benchmarks.
\newblock In {\em Proceedings of NeurIPS}, 2022.

\bibitem{das-etal-2022-container}
Sarkar Snigdha~Sarathi Das, Arzoo Katiyar, Rebecca Passonneau, and Rui Zhang.
\newblock {CONT}ai{NER}: Few-shot named entity recognition via contrastive
  learning.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, 2022.

\bibitem{davidson2017hsol}
Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber.
\newblock Automated hate speech detection and the problem of offensive
  language.
\newblock In {\em Proceedings of AAAI on Web and Social Media}, 2017.

\bibitem{de-gibert-etal-2018-hatespeech}
Ona de~Gibert, Naiara Perez, Aitor Garc{\'\i}a-Pablos, and Montse Cuadros.
\newblock Hate speech dataset from a white supremacy forum.
\newblock In {\em Proceedings of Workshop on Abusive Language Online ({ALW}2)},
  2018.

\bibitem{de2019commitmentbank}
Marie-Catherine De~Marneffe, Mandy Simons, and Judith Tonhauser.
\newblock The commitmentbank: Investigating projection in naturally occurring
  discourse.
\newblock In {\em Proceedings of Sinn und Bedeutung}, 2019.

\bibitem{derczynski-etal-2017-wnut}
Leon Derczynski, Eric Nichols, Marieke van Erp, and Nut Limsopatham.
\newblock Results of the {WNUT}2017 shared task on novel and emerging entity
  recognition.
\newblock In {\em Proceedings of Workshop on Noisy User-generated Text}, 2017.

\bibitem{DBLP:conf/naacl/DevlinCLT19}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of NAACL}, 2019.

\bibitem{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proc. of NAACL}, 2019.

\bibitem{ding-etal-2021-nerd}
Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu~Han, Pengjun Xie, Haitao
  Zheng, and Zhiyuan Liu.
\newblock Few-{NERD}: A few-shot named entity recognition dataset.
\newblock In {\em Proceedings of ACL-IJCNLP}, 2021.

\bibitem{du-etal-2021-towards}
Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt,
  Jiuxiang Gu, Tong Sun, and Xia Hu.
\newblock Towards interpreting and mitigating shortcut learning behavior of
  {NLU} models.
\newblock In {\em Proceedings of NAACL:HLT}, 2021.

\bibitem{dunn2017searchqa}
Matthew Dunn, Levent Sagun, Mike Higgins, V.~Ugur G{\"{u}}ney, Volkan Cirik,
  and Kyunghyun Cho.
\newblock Searchqa: {A} new q{\&}a dataset augmented with context from a search
  engine.
\newblock {\em CoRR}, 2017.

\bibitem{elsherief-etal-2021-implicit_hate}
Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn
  Seybolt, Munmun De~Choudhury, and Diyi Yang.
\newblock Latent hatred: A benchmark for understanding implicit hate speech.
\newblock In {\em Proceedings of EMNLP}, 2021.

\bibitem{fisch2019mrqa}
Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen.
\newblock {MRQA} 2019 shared task: Evaluating generalization in reading
  comprehension.
\newblock In {\em Proceedings of 2nd Machine Reading for Reading Comprehension
  (MRQA) Workshop at EMNLP}, 2019.

\bibitem{gao-etal-2021-simcse}
Tianyu Gao, Xingcheng Yao, and Danqi Chen.
\newblock {S}im{CSE}: Simple contrastive learning of sentence embeddings.
\newblock In {\em Proceedings of EMNLP}, 2021.

\bibitem{gardner2021competency}
Matt Gardner, William Merrill, Jesse Dodge, Matthew~E Peters, Alexis Ross,
  Sameer Singh, and Noah Smith.
\newblock Competency problems: On finding and removing artifacts in language
  data.
\newblock {\em arXiv preprint arXiv:2104.08646}, 2021.

\bibitem{goel2021robustness}
Karan Goel, Nazneen Rajani, Jesse Vig, Samson Tan, Jason Wu, Stephan Zheng,
  Caiming Xiong, Mohit Bansal, and Christopher R{\'e}.
\newblock Robustness gym: Unifying the nlp evaluation landscape.
\newblock {\em arXiv preprint arXiv:2101.04840}, 2021.

\bibitem{DBLP:conf/naacl/GoelRVTBR21}
Karan Goel, Nazneen~Fatema Rajani, Jesse Vig, Zachary Taschdjian, Mohit Bansal,
  and Christopher R{\'{e}}.
\newblock Robustness gym: Unifying the {NLP} evaluation landscape.
\newblock In Avi Sil and Xi~Victoria Lin, editors, {\em Proceedings of
  NAACL-HLT}, 2021.

\bibitem{Goyal2022NewsSA}
Tanya Goyal, Junyi~Jessy Li, and Greg Durrett.
\newblock News summarization and evaluation in the era of gpt-3.
\newblock {\em CoRR}, 2022.

\bibitem{hartvigsen-etal-2022-toxigen}
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray,
  and Ece Kamar.
\newblock {T}oxi{G}en: A large-scale machine-generated dataset for adversarial
  and implicit hate speech detection.
\newblock In {\em Proceedings of ACL}, 2022.

\bibitem{he2019unlearn}
He~He, Sheng Zha, and Haohan Wang.
\newblock Unlearn dataset bias in natural language inference by fitting the
  residual.
\newblock {\em arXiv preprint arXiv:1908.10763}, 2019.

\bibitem{he2021deberta}
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.
\newblock Deberta: Decoding-enhanced bert with disentangled attention.
\newblock In {\em Proceedings of ICLR}, 2021.

\bibitem{hendrycks-etal-2020-pretrained}
Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and
  Dawn Song.
\newblock Pretrained transformers improve out-of-distribution robustness.
\newblock In {\em Proceedings of ACL}, 2020.

\bibitem{houlsby2019adapter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  de~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock 2019.

\bibitem{huang2018examining}
Xiaolei Huang and Michael~J Paul.
\newblock Examining temporality in document classification.
\newblock In {\em Proceedings of ACL}, 2018.

\bibitem{hupkes2022genbench}
Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar,
  Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra,
  Arabella Sinclair, Dennis Ulmer, Florian Schottmann, Khuyagbaatar Batsuren,
  Kaiser Sun, Koustuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske,
  Ryan Cotterell, and Zhijing Jin.
\newblock State-of-the-art generalisation research in {NLP}: a taxonomy and
  review.
\newblock {\em CoRR}, 2022.

\bibitem{ilyas2019adversarial}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock In {\em Proceedings of NeurIPS}, 2019.

\bibitem{joshi-etal-2017-triviaqa}
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.
\newblock {T}rivia{QA}: A large scale distantly supervised challenge dataset
  for reading comprehension.
\newblock In {\em Proceedings of ACL}, 2017.

\bibitem{kaushik2019learning}
Divyansh Kaushik, Eduard Hovy, and Zachary~C. Lipton.
\newblock Learning the difference that makes a difference with
  counterfactually-augmented data.
\newblock In {\em Proceedings of ICLR}, 2020.

\bibitem{kavumba-etal-2022-prompt}
Pride Kavumba, Ryo Takahashi, and Yusuke Oda.
\newblock Are prompt-based models clueless?
\newblock In {\em Proceedings of ACL}, 2022.

\bibitem{ke2020dsc}
Zixuan Ke, Bing Liu, Hao Wang, and Lei Shu.
\newblock Continual learning with knowledge transfer for sentiment
  classification.
\newblock In {\em Proceedings of ECML-PKDD}, 2020.

\bibitem{koreeda-manning-2021-contractnli-dataset}
Yuta Koreeda and Christopher Manning.
\newblock {C}ontract{NLI}: A dataset for document-level natural language
  inference for contracts.
\newblock In {\em Findings of EMNLP}, 2021.

\bibitem{kwiatkowski-etal-2019-natural_question}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
  Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin,
  Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang,
  Andrew~M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.
\newblock Natural questions: A benchmark for question answering research.
\newblock {\em Transactions of ACL}, 2019.

\bibitem{le2022perturbations}
Thai Le, Jooyoung Lee, Kevin Yen, Yifan Hu, and Dongwon Lee.
\newblock Perturbations in the wild: Leveraging human-written text
  perturbations for realistic adversarial attack and defense.
\newblock {\em arXiv preprint arXiv:2203.10346}, 2022.

\bibitem{lee-etal-2021-learning-perturb}
Seanie Lee, Minki Kang, Juho Lee, and Sung~Ju Hwang.
\newblock Learning to perturb word embeddings for out-of-distribution {QA}.
\newblock In {\em Proceedings of ACL-IJCNLP}, 2021.

\bibitem{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock {\em arXiv preprint arXiv:2104.08691}, 2021.

\bibitem{lester2021softprompt}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In {\em Proceedings of EMNLP}, 2021.

\bibitem{DBLP:journals/corr/abs-2304-14475}
Jiazhao Li, Yijin Yang, Zhuofeng Wu, V.~G.~Vinod Vydiswaran, and Chaowei Xiao.
\newblock Chatgpt as an attack tool: Stealthy textual backdoor attack via
  blackbox generative model trigger.
\newblock {\em CoRR}, 2023.

\bibitem{li2018textbugger}
Jinfeng Li, Shouling Ji, Tianyu Du, Bo~Li, and Ting Wang.
\newblock Textbugger: Generating adversarial text against real-world
  applications.
\newblock {\em arXiv preprint arXiv:1812.05271}, 2018.

\bibitem{liang-etal-2021-super}
Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu, Pengcheng
  He, Tuo Zhao, and Weizhu Chen.
\newblock Super tickets in pre-trained language models: From model compression
  to improving generalization.
\newblock In {\em Proceedings of ACL-IJCNLP}, 2021.

\bibitem{lin2017focalloss}
Tsung-Yi Lin, Priya Goyal, Ross~B. Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Focal loss for dense object detection.
\newblock 2017.

\bibitem{liu-etal-2022-wanli}
Alisa Liu, Swabha Swayamdipta, Noah~A. Smith, and Yejin Choi.
\newblock Wanli: Worker and ai collaboration for natural language inference
  dataset creation.
\newblock In {\em Findings of EMNLP}, 2022.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em CoRR}, 2019.

\bibitem{liu2021crossner}
Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei Ji, Samuel Cahyawijaya,
  Andrea Madotto, and Pascale Fung.
\newblock Crossner: Evaluating cross-domain named entity recognition.
\newblock In {\em Proceedings of AAAI}, 2021.

\bibitem{ludwig-etal-2022-improving}
Florian Ludwig, Klara Dolos, Torsten Zesch, and Eleanor Hobley.
\newblock Improving generalization of hate speech detection systems to novel
  target groups via domain adaptation.
\newblock In {\em Proceedings of the Sixth Workshop on Online Abuse and Harms
  (WOAH)}, 2022.

\bibitem{ma2019domain}
Xiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nallapati, and Bing Xiang.
\newblock Domain adaptation with bert-based domain classification and data
  selection.
\newblock In {\em Proceedings of the 2nd Workshop on Deep Learning Approaches
  for Low-Resource NLP (DeepLo 2019)}, 2019.

\bibitem{maas2011imdb}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In {\em Proceedings of NAACL}, 2011.

\bibitem{mahabadi2020end}
Rabeeh~Karimi Mahabadi, Yonatan Belinkov, and James Henderson.
\newblock End-to-end bias mitigation by modelling biases in corpora.
\newblock In {\em Proceedings of ACL}, 2020.

\bibitem{mcAuley2013amazon}
Julian McAuley and Jure Leskovec.
\newblock Hidden factors and hidden topics: Understanding rating dimensions
  with review text.
\newblock In {\em Proceedings of ACM Conference on Recommender Systems}, 2013.

\bibitem{mccoy2019right}
R~Thomas McCoy, Ellie Pavlick, and Tal Linzen.
\newblock Right for the wrong reasons: Diagnosing syntactic heuristics in
  natural language inference.
\newblock {\em arXiv preprint arXiv:1902.01007}, 2019.

\bibitem{miller2020squadshifts}
John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt.
\newblock The effect of natural distribution shift on question answering
  models.
\newblock In {\em Proceedings of ICML}, 2020.

\bibitem{miller2021accuracy}
John~P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang~Wei Koh,
  Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt.
\newblock Accuracy on the line: on the strong correlation between
  out-of-distribution and in-distribution generalization.
\newblock In {\em Proceedings of ICML}, 2021.

\bibitem{Min2022RethinkingTR}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
  Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning
  work?
\newblock In {\em Proceedings of EMNLP}, 2022.

\bibitem{nakov-etal-2016-semeval}
Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio Sebastiani, and Veselin
  Stoyanov.
\newblock {S}em{E}val-2016 task 4: Sentiment analysis in {T}witter.
\newblock In {\em Proceedings of International Workshop on Semantic Evaluation
  ({S}em{E}val)}, 2016.

\bibitem{nejadgholi-kiritchenko-2020-cross}
Isar Nejadgholi and Svetlana Kiritchenko.
\newblock On cross-dataset generalization in automatic detection of online
  abuse.
\newblock In {\em Proceedings of the Fourth Workshop on Online Abuse and
  Harms}, 2020.

\bibitem{ng-etal-2020-ssmba}
Nathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi.
\newblock {SSMBA}: Self-supervised manifold based data augmentation for
  improving out-of-domain robustness.
\newblock In {\em Proceedings of EMNLP}, 2020.

\bibitem{nie2019anli}
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe
  Kiela.
\newblock Adversarial nli: A new benchmark for natural language understanding.
\newblock In {\em Proceedings of ACL}, 2020.

\bibitem{paranjape2022agro}
Bhargavi Paranjape, Pradeep Dasigi, Vivek Srikumar, Luke Zettlemoyer, and
  Hannaneh Hajishirzi.
\newblock Agro: Adversarial discovery of error-prone groups for robust
  optimization.
\newblock 2022.

\bibitem{potts-etal-2021-dynasent}
Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela.
\newblock {D}yna{S}ent: A dynamic benchmark for sentiment analysis.
\newblock In {\em Proceedings of ACL-IJCNLP}, 2021.

\bibitem{Radford2021clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em Proceedings of ICML}, 2021.

\bibitem{raffel2020t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em Journal of Machine Learning Research}, 2020.

\bibitem{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock In {\em Proceedings of EMNLP}, 2016.

\bibitem{ramponi2020neural}
Alan Ramponi and Barbara Plank.
\newblock Neural unsupervised domain adaptation in nlp---a survey.
\newblock {\em arXiv preprint arXiv:2006.00632}, 2020.

\bibitem{reimers2019sentencebert}
Nils Reimers and Iryna Gurevych.
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock In {\em Proceedings of EMNLP}, 2019.

\bibitem{sanh2021multitask}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H. Bach, Lintang Sutawika,
  Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja,
  Manan Dey, M~Saiful Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma,
  Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta,
  Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,
  Zheng~Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,
  Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason~Alan
  Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and
  Alexander~M. Rush.
\newblock Multitask prompted training enables zero-shot task generalization,
  2022.

\bibitem{shi2022stepGame}
Zhengxiang Shi, Qiang Zhang, and Aldo Lipani.
\newblock Stepgame: A new benchmark for robust multi-hop spatial reasoning in
  texts.
\newblock In {\em Proceedings of AAAI}, 2022.

\bibitem{socher-etal-2013-recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning,
  Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In {\em Proceedings of EMNLP}, 2013.

\bibitem{szegedy2016labelsmoothing}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em Proceedings of CVPR}, 2016.

\bibitem{talman-chatzikyriakidis-2019-testing}
Aarne Talman and Stergios Chatzikyriakidis.
\newblock Testing the generalization power of neural network models across
  {NLI} benchmarks.
\newblock In {\em Proceedings of ACL Workshop BlackboxNLP: Analyzing and
  Interpreting Neural Networks for NLP}, 2019.

\bibitem{talmor-berant-2019-multiqa}
Alon Talmor and Jonathan Berant.
\newblock {M}ulti{QA}: An empirical investigation of generalization and
  transfer in reading comprehension.
\newblock In {\em Proceedings of ACL}, 2019.

\bibitem{taori2020measuring}
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht,
  and Ludwig Schmidt.
\newblock Measuring robustness to natural distribution shifts in image
  classification.
\newblock In {\em Proceedings of NeurIPS}, 2020.

\bibitem{sang2003conll}
Erik~F. Tjong Kim~Sang and Fien De~Meulder.
\newblock Introduction to the conll-2003 shared task: Language-independent
  named entity recognition.
\newblock In {\em Proceedings of NAACL}, 2003.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em ArXiv}, 2023.

\bibitem{trischler-etal-2017-newsqa}
Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni,
  Philip Bachman, and Kaheer Suleman.
\newblock {N}ews{QA}: A machine comprehension dataset.
\newblock In {\em Proceedings of Workshop on Representation Learning for
  {NLP}}, 2017.

\bibitem{tu2020empirical}
Lifu Tu, Garima Lalwani, Spandana Gella, and He~He.
\newblock An empirical study on robustness to spurious correlations using
  pre-trained language models.
\newblock {\em Transactions of the Association for Computational Linguistics},
  2020.

\bibitem{DBLP:conf/nips/WangPNSMHLB19}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
  Felix Hill, Omer Levy, and Samuel~R. Bowman.
\newblock Superglue: {A} stickier benchmark for general-purpose language
  understanding systems.
\newblock In {\em Proceedings of NeurIPS}, 2019.

\bibitem{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock {\em arXiv preprint arXiv:1804.07461}, 2018.

\bibitem{DBLP:conf/iclr/WangSMHLB19}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman.
\newblock {GLUE:} {A} multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In {\em Proceedings of ICLR}, 2019.

\bibitem{wang2020infobert}
Boxin Wang, Shuohang Wang, Yu~Cheng, Zhe Gan, Ruoxi Jia, Bo~Li, and Jingjing
  Liu.
\newblock Infobert: Improving robustness of language models from an information
  theoretic perspective.
\newblock 2021.

\bibitem{wang2021adversarial}
Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu~Cheng, Jianfeng Gao,
  Ahmed~Hassan Awadallah, and Bo~Li.
\newblock Adversarial glue: A multi-task benchmark for robustness evaluation of
  language models.
\newblock {\em arXiv preprint arXiv:2111.02840}, 2021.

\bibitem{wang2022indentifying}
Tianlu Wang, Rohit Sridhar, Diyi Yang, and Xuezhi Wang.
\newblock Identifying and mitigating spurious correlations for improving
  robustness in nlp models.
\newblock In {\em Findings of NAACL:HLT}, 2022.

\bibitem{wang-etal-2021-textflint}
Xiao Wang, Qin Liu, Tao Gui, Qi~Zhang, Yicheng Zou, Xin Zhou, Jiacheng Ye,
  Yongxin Zhang, Rui Zheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li, Chong Zhang,
  Ruotian Ma, Zichu Fei, Ruijian Cai, Jun Zhao, Xingwu Hu, Zhiheng Yan, Yiding
  Tan, Yuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, Bolin Zhu, Xiaoyu Xing,
  Jinlan Fu, Yue Zhang, Minlong Peng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei,
  Xipeng Qiu, and Xuanjing Huang.
\newblock {T}ext{F}lint: Unified multilingual robustness evaluation toolkit for
  natural language processing.
\newblock In {\em Proc. of ACL}, 2021.

\bibitem{wang-etal-2022-measure}
Xuezhi Wang, Haohan Wang, and Diyi Yang.
\newblock Measure and improve robustness in {NLP} models: A survey.
\newblock In {\em Proceedings of NAACL}, 2022.

\bibitem{Wei2022InverseSC}
Jason Wei, Yi~Tay, and Quoc~V. Le.
\newblock Inverse scaling can become u-shaped.
\newblock {\em ArXiv}, 2022.

\bibitem{wei-zou-2019-eda}
Jason Wei and Kai Zou.
\newblock {EDA}: Easy data augmentation techniques for boosting performance on
  text classification tasks.
\newblock In {\em Proceedings of EMNLP-IJCNLP}, 2019.

\bibitem{williams-etal-2018-broad}
Adina Williams, Nikita Nangia, and Samuel Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In {\em Proceedings of NAACL}, 2018.

\bibitem{wu-etal-2021-polyjuice}
Tongshuang Wu, Marco~Tulio Ribeiro, Jeffrey Heer, and Daniel Weld.
\newblock Polyjuice: Generating counterfactuals for explaining, evaluating, and
  improving models.
\newblock In {\em Proceedings of ACL-IJCNLP}, 2021.

\bibitem{wu2021esimcse}
Xing Wu, Chaochen Gao, Liangjun Zang, Jizhong Han, Zhongyuan Wang, and Songlin
  Hu.
\newblock Esimcse: Enhanced sample building method for contrastive learning of
  unsupervised sentence embedding.
\newblock In {\em Proceedings of COLING}, 2022.

\bibitem{Xue2022CLIPViPAP}
Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Rui Song, Houqiang Li, and
  Jiebo Luo.
\newblock Clip-vip: Adapting pre-trained image-text model to video-language
  representation alignment.
\newblock In {\em Proceedings of ICLR}, 2022.

\bibitem{yang2022factmix}
Linyi Yang, Lifan Yuan, Leyang Cui, Wenyang Gao, and Yue Zhang.
\newblock Factmix: Using a few labeled in-domain examples to generalize to
  cross-domain named entity recognition.
\newblock In {\em Proceedings of COLING}, 2022.

\bibitem{yang2022glue}
Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong
  Wang, Xing Xie, and Yue Zhang.
\newblock Glue-x: Evaluating natural language understanding models from an
  out-of-distribution generalization perspective.
\newblock In {\em Findings of ACL}, 2023.

\bibitem{yang-etal-2018-hotpotqa}
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan
  Salakhutdinov, and Christopher~D. Manning.
\newblock {H}otpot{QA}: A dataset for diverse, explainable multi-hop question
  answering.
\newblock In {\em Proceedings of EMNLP}, 2018.

\bibitem{ye-etal-2022-robust}
Hai Ye, Yuyang Ding, Juntao Li, and Hwee~Tou Ng.
\newblock Robust question answering against distribution shifts with test-time
  adaption: An empirical study.
\newblock In {\em Findings of EMNLP}, 2022.

\bibitem{yin2021docnli}
Wenpeng Yin, Dragomir Radev, and Caiming Xiong.
\newblock Docnli: A large-scale dataset for document-level natural language
  inference.
\newblock In {\em Findings of ACL}, 2021.

\bibitem{yogatama2019learning}
Dani Yogatama, Cyprien de~Masson d'Autume, Jerome Connor, Tomas Kocisky, Mike
  Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris
  Dyer, and Phil Blunsom.
\newblock Learning and evaluating general linguistic intelligence, 2019.

\bibitem{DBLP:journals/corr/abs-2110-15317}
Lifan Yuan, Yichi Zhang, Yangyi Chen, and Wei Wei.
\newblock Bridge the gap between {CV} and nlp! {A} gradient-based textual
  adversarial attack framework.
\newblock In {\em Findings of ACL}, 2023.

\bibitem{zampieri-etal-2019-olid}
Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra,
  and Ritesh Kumar.
\newblock Predicting the type and target of offensive posts in social media.
\newblock In {\em Proceedings of NAACL}, 2019.

\bibitem{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In {\em Proceedings of ACL}, 2019.

\bibitem{Zhang2021DiveID}
Aston Zhang, Zachary~Chase Lipton, Mu~Li, and Alex Smola.
\newblock Dive into deep learning.
\newblock {\em CoRR}, 2021.

\bibitem{zhang2015character}
Xiang Zhang, Junbo Zhao, and Yann LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock In {\em Proceedings of NeurIPS}, 2015.

\bibitem{zhang2019paws}
Yuan Zhang, Jason Baldridge, and Luheng He.
\newblock Paws: Paraphrase adversaries from word scrambling.
\newblock {\em arXiv preprint arXiv:1904.01130}, 2019.

\bibitem{zhu2019freelb}
Chen Zhu, Yu~Cheng, Zhe Gan, S.~Sun, Tom Goldstein, and Jingjing Liu.
\newblock Freelb: Enhanced adversarial training for natural language
  understanding.
\newblock In {\em Proceedings of ICLR}, 2019.

\end{thebibliography}
