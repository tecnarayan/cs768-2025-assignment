@misc{vinyals2017pointer,
      title={Pointer Networks},
      author={Oriol Vinyals and Meire Fortunato and Navdeep Jaitly},
      year={2017},
      eprint={1506.03134},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{GTSRB,
title = "Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition",
journal = "Neural Networks",
year = "2012",
author = "J. Stallkamp and M. Schlipsing and J. Salmen and C. Igel",
}

@inproceedings{Lee2020,
title={Gradients as a Measure of Uncertainty in Neural Networks},author={Lee, Jinsol and AlRegib, Ghassan},booktitle={IEEE International Conference on Image Processing (ICIP)},year={2020}}

@inproceedings{Lee2018,
author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
title = {A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks},
year = {2018},
booktitle = {International Conference on Neural Information Processing Systems (NeurIPS)},
}

@InProceedings{Oberdiek2018,
author="Oberdiek, Philipp
and Rottmann, Matthias
and Gottschalk, Hanno",
editor="Pancioni, Luca
and Schwenker, Friedhelm
and Trentin, Edmondo",
title="Classification Uncertainty of Deep Neural Networks Based on Gradient Information",
booktitle="Artificial Neural Networks in Pattern Recognition",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="113--125",
isbn="978-3-319-99978-4"
}

@misc{gpt1,
      title={Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions}, 
      author={Gokul Yenduri and Ramalingam M and Chemmalar Selvi G and Supriya Y and Gautam Srivastava and Praveen Kumar Reddy Maddikunta and Deepti Raj G and Rutvij H Jhaveri and Prabadevi B and Weizheng Wang and Athanasios V. Vasilakos and Thippa Reddy Gadekallu},
      year={2023},
      eprint={2305.10435},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:160025533}
}

@article{dcfnet,
	title={{DCFNet}: Deep Neural Network with Decomposed Convolutional Filters},
	author={Qiu, Qiang and Cheng, Xiuyuan and Calderbank, Robert and Sapiro, Guillermo},
	journal={International Conference on Machine Learning (ICML)},
	year={2018}
}

@article{RotDCF,
  title={RotDCF: Decomposition of Convolutional Filters for Rotation-Equivariant Deep Networks},
  author={Xiuyuan Cheng and Qiang Qiu and Robert Calderbank and Guillermo Sapiro},
  journal={ArXiv},
  year={2018},
}

@misc{gpt3,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Li2018,
  title={Visualizing the Loss Landscape of Neural Nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={Neural Information Processing Systems (NeurIPS)},
  year={2018}
}

@article{Arvanitidis2017,
author = {Arvanitidis, Georgios and Hansen, Lars and Hauberg, Søren},
year = {2017},
journal = {International Conference on Learning Representations (ICLR)},
title = {Latent Space Oddity: on the Curvature of Deep Generative Models}
}

@InProceedings{Ramalho2020,
author="Ramalho, Tiago
and Miranda, Miguel",
editor="Shehory, Onn
and Farchi, Eitan
and Barash, Guy",
title="Density Estimation in Representation Space to Predict Model Uncertainty",
booktitle="Engineering Dependable and Secure Machine Learning Systems",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="84--96",
isbn="978-3-030-62144-5"
}

@inproceedings{Lakshminarayanan2017,
 author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
 booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
 title = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
 year = {2017}
}

@article{Konkle2011,
author = {Konkle, Talia and Oliva, Aude},
year = {2011},
title = {Canonical Visual Size for Real-World Objects},
journal = {Journal of experimental psychology, human perception and performance},
}

@misc{Harang2018,
    author = {Richard Harang and Ethan M. Rudd},
    title = {Towards Principled Uncertainty Estimation for Deep Neural Networks},
    year = 2018,
    url = {https://arxiv.org/abs/1810.12278}
}

@inproceedings{Geifman2018,
  title={Bias-Reduced Uncertainty Estimation for Deep Neural Classifiers},
  author={Yonatan Geifman and Guy Uziel and Ran El-Yaniv},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018},
}

@article{Lloyd1982,
  title={Least squares quantization in PCM},
  author={Stuart P. Lloyd},
  journal={IEEE Trans. Inf. Theory},
  year={1982},
  volume={28},
  pages={129-136}
}

@article{GELU,
  title={Gaussian Error Linear Units (GELUs)},
  author={Dan Hendrycks and Kevin Gimpel},
  journal={ArXiv},
  year={2016},
}

@article{Gawlikowski2023,
author = {Gawlikowski, Jakob and Tassi, Cedrique and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and Shahzad, Muhammad and Yang, Wen and Bamler, Richard and Zhu, Xiao},
year = {2023},
month = {07},
pages = {1-77},
title = {A survey of uncertainty in deep neural networks},
journal = {Artificial Intelligence Review},
doi = {10.1007/s10462-023-10562-9}
}

@article{Shorten2019,
  title={A survey on Image Data Augmentation for Deep Learning},
  author={Connor Shorten and Taghi M. Khoshgoftaar},
  journal={Journal of Big Data},
  year={2019},
}

@article{Harris2001,
    author = {Harris, Irina M. and Harris, Justin A. and Caine, Diana},
    title = "{Object Orientation Agnosia: A Failure to Find the Axis?}",
    journal = {Journal of Cognitive Neuroscience},
    year = {2001},
}

@ARTICLE{SplineTheory,

  author={Balestriero, Randall and Baraniuk, Richard G.},

  journal={Proceedings of the IEEE}, 

  title={Mad Max: Affine Spline Insights Into Deep Learning}, 

  year={2021},

  volume={109},

  number={5},

  pages={704-727},

  doi={10.1109/JPROC.2020.3042100}}


@article{Leibo2011,
author = {Leibo, Joel and Mutch, Jim and Poggio, Tomaso},
year = {2011},
title = {Learning to discount transformations as the computational goal of visual cortex},
journal = {Nature Precedings},
}

@article{Gomez2008,
author = {Gómez, Pablo and Shutter, Jennifer and Rouder, Jeffrey},
year = {2008},
title = {Memory for objects in canonical and noncanonical viewpoints},
journal = {Psychonomic bulletin and review},
}

@article{Dosovitskiy2020,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@article{Kingma2015AdamAM,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2015},
  volume={abs/1412.6980}
}

@INPROCEEDINGS{Dai2017,

  author={Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},

  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 

  title={Deformable Convolutional Networks}, 

  year={2017},

  volume={},

  number={},

  pages={764-773},

  doi={10.1109/ICCV.2017.89}}


@inproceedings{Shu2018,
  title={A Refined Spatial Transformer Network},
  author={Chang Shu and Xi Chen and Chong Yu and Hua Han},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2018}
}

@inproceedings{Duvenaud2020,
title	= {Your classifier is secretly an energy based model and you should treat it like one},
author	= {David Duvenaud and Jackson Wang and Jorn Jacobsen and Kevin Swersky and Mohammad Norouzi and Will Grathwohl},
booktitle = {International Conference on Learning Representations (ICLR)},
year	= {2020}
}


@article{LeCun2006,
title = "A tutorial on energy-based learning",
author = "Yann Lecun and Sumit Chopra and Raia Hadsell and Ranzato, {Marc Aurelio} and Huang, {Fu Jie}",
year = "2006",
journal = "Predicting structured data",
}


@INPROCEEDINGS{Lin2017,
  author={Lin, Chen-Hsuan and Lucey, Simon},
  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Inverse Compositional Spatial Transformer Networks}, 
  year={2017},
}


@InProceedings{Kaba2023,
  title = 	 {Equivariance with Learned Canonicalization Functions},
  author =       {Kaba, S\'{e}kou-Oumar and Mondal, Arnab Kumar and Zhang, Yan and Bengio, Yoshua and Ravanbakhsh, Siamak},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  year = 	 {2023},
}


@article{Bengio2012,
  title={Representation Learning: A Review and New Perspectives},
  author={Yoshua Bengio and Aaron C. Courville and Pascal Vincent},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2012},
}

@inproceedings{Song2021,
author = {Song, Hanyu and Li, Peizhao and Liu, Hongfu},
title = {Deep Clustering Based Fair Outlier Detection},
year = {2021},
isbn = {9781450383325},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447548.3467225},
doi = {10.1145/3447548.3467225},
abstract = {In this paper, we focus on the fairness issues regarding unsupervised outlier detection. Traditional algorithms, without a specific design for algorithmic fairness, could implicitly encode and propagate statistical bias in data and raise societal concerns. To correct such unfairness and deliver a fair set of potential outlier candidates, we propose Deep Clustering based Fair Outlier Detection (DCFOD) that learns a good representation for utility maximization while enforcing the learnable representation to be subgroup-invariant on the sensitive attribute. Considering the coupled and reciprocal nature between clustering and outlier detection, we leverage deep clustering to discover the intrinsic cluster structure and out-of-structure instances. Meanwhile, an adversarial training erases the sensitive pattern for instances for fairness adaptation. Technically, we propose an instance-level weighted representation learning strategy to enhance the joint deep clustering and outlier detection, where the dynamic weight module re-emphasizes contributions of likely-inliers while mitigating the negative impact from outliers. Demonstrated by experiments on eight datasets comparing to 17 outlier detection algorithms, our DCFOD method consistently achieves superior performance on both the outlier detection validity and two types of fairness notions in outlier detection.},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
pages = {1481–1489},
numpages = {9},
keywords = {outlier detection, fair representation learning, deep clustering on outlier detection},
location = {Virtual Event, Singapore},
series = {KDD '21}
}


@inproceedings{Li2018Oversmooth,
title = "Deeper insights into graph convolutional networks for semi-supervised learning",
abstract = "Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semi-supervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires considerable amount of labeled data for validation and model selection. In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.",
author = "Qimai Li and Zhichao Han and Wu, {Xiao Ming}",
year = "2018",
series = "32nd AAAI Conference on Artificial Intelligence",
pages = "3538--3545",
}

@inproceedings{Gal2016,
author = {Gal, Yarin and Ghahramani, Zoubin},
title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
year = {2016},
booktitle = {International Conference on Machine Learning (ICML)},
}

@article{Hendrycks2019,
  title={Deep Anomaly Detection with Outlier Exposure},
  author={Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas},
  journal={Proceedings of the International Conference on Learning Representations},
  year={2019}
}

@misc{Fang2022,
    author = {Zhen Fang and Yixuan Li and Jie Lu and Jiahua Dong and Bo Han and Feng Liu},
    title = {Is Out-of-Distribution Detection Learnable?},
    year = 2022,
    url = {https://openreview.net/forum?id=sde_7ZzGXOE}
}

@article{Osband2016,
  title={Risk versus Uncertainty in Deep Learning: Bayes, Bootstrap and the Dangers of Dropout},
  author={Ian Osband},
  journal={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2016},
}

@InProceedings{Nalisnick2019,
  title = 	 {Dropout as a Structured Shrinkage Prior},
  author =       {Nalisnick, Eric and Hernandez-Lobato, Jose Miguel and Smyth, Padhraic},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4712--4722},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/nalisnick19a/nalisnick19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/nalisnick19a.html},
}


@article{Rubin1981,
author = {Donald B. Rubin},
title = {{The Bayesian Bootstrap}},
volume = {9},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {130 -- 134},
keywords = {Dirichlet, jackknife, Model-free inference},
year = {1981},
doi = {10.1214/aos/1176345338},
URL = {https://doi.org/10.1214/aos/1176345338}
}


@article{Osawa2019,
author = {Osawa, Kazuki and Swaroop, Siddharth and Jain, Anirudh and Eschenhagen, Runa and Turner, Richard E. and Yokota, Rio and Khan, Mohammad Emtiyaz},
title = {Practical Deep Learning with Bayesian Principles},
year = {2019},
journal = {International Conference on Neural Information Processing Systems (NeurIPS)},
}

@inproceedings{Keller2021TopographicVL,
 author = {Keller, T. Anderson and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {28585--28597},
 publisher = {Curran Associates, Inc.},
 title = {Topographic VAEs learn Equivariant Capsules},
 volume = {34},
 year = {2021}
}

@misc{Mandelbaum2017,
    author = {Amit Mandelbaum and Daphna Weinshall},
    title = {Distance-based Confidence Score for Neural Network Classifiers},
    year = 2017,
    url = {https://arxiv.org/abs/1709.09844}
}

@inproceedings{Ciosek2020,
title={Conservative Uncertainty Estimation By Fitting Prior Networks},
author={Kamil Ciosek and Vincent Fortuin and Ryota Tomioka and Katja Hofmann and Richard Turner},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJlahxHYDS}
}

@INPROCEEDINGS{IsotropicFilter2018,  author={Chen, Yifang and Lyu, Zi Xian and Kang, Xiangui and Wang, Z. Jane},  booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},   title={A Rotation-Invariant Convolutional Neural Network for Image Enhancement Forensics},   year={2018},  volume={},  number={},  pages={2111-2115},  doi={10.1109/ICASSP.2018.8462057}}


@misc{kalra2021rotation,
      title={Towards Rotation Invariance in Object Detection},
      author={Agastya Kalra and Guy Stoppi and Bradley Brown and Rishav Agarwal and Achuta Kadambi},
      year={2021},
      eprint={2109.13488},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@ARTICLE{Cheng2016Rotation,

  author={Cheng, Gong and Zhou, Peicheng and Han, Junwei},

  journal={IEEE Transactions on Geoscience and Remote Sensing},

  title={Learning Rotation-Invariant Convolutional Neural Networks for Object Detection in VHR Optical Remote Sensing Images},

  year={2016},

  volume={54},

  number={12},

  pages={7405-7415},

  doi={10.1109/TGRS.2016.2601622}
}

@INPROCEEDINGS{HAR21,  author={Bexten, Simone and Schmidt, Johann and Walter, Christoph and Elkmann, Norbert},  booktitle={2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA )},   title={Human Action Recognition as part of a Natural Machine Operation Framework},   year={2021},  volume={},  number={},  pages={1-8},  doi={10.1109/ETFA45728.2021.9613331}}


@misc{vaswani2017attention,
      title={Attention Is All You Need},
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{tai2019equivariant,
  title={{Equivariant Transformer Networks}},
  author={Tai, Kai Sheng and Bailis, Peter and Valiant, Gregory},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2019}
}

@article{olah2020,
  author = {Olah, Chris and Cammarata, Nick and Voss, Chelsea and Schubert, Ludwig and Goh, Gabriel},
  title = {Naturally Occurring Equivariance in Neural Networks},
  journal = {Distill},
  year = {2020},
}

@inproceedings{delchevalerie2021,
 author = {Delchevalerie, Valentin and Bibal, Adrien and Fr\'{e}nay, Beno\^{\i}t and Mayer, Alexandre},
 booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
 title = {Achieving Rotational Invariance with Bessel-Convolutional Neural Networks},
 year = {2021}
}


@article{bao2020,
      title={Equivariant neural networks and equivarification}, 
      author={Erkao Bao and Linqi Song},
      year={2020},
      journal={ArXiv},
}

@inproceedings{Shakerinava2022,
 author = {Shakerinava, Mehran and Mondal, Arnab Kumar and Ravanbakhsh, Siamak},
 booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
 title = {Structuring Representations Using Group Invariants},
 year = {2022}
}

@conference{PolarTransformer,
title = "Polar transformer networks",
booktitle = "International Conference on Learning Representations (ICLR)",
author = "Carlos Esteves and Christine Allen-Blanchette and Xiaowei Zhou and Kostas Daniilidis",
year = "2018",
}

@misc{dwivedi2021generalization,
      title={A Generalization of Transformer Networks to Graphs},
      author={Vijay Prakash Dwivedi and Xavier Bresson},
      year={2021},
      eprint={2012.09699},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ma2019combinatorial,
      title={Combinatorial Optimization by Graph Pointer Networks and Hierarchical Reinforcement Learning},
      author={Qiang Ma and Suwen Ge and Danyang He and Darshan Thaker and Iddo Drori},
      year={2019},
      eprint={1911.04936},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Detlefsen2018,
  title = {Deep Diffeomorphic Transformer Networks},
  author = {Nicki Skafte Detlefsen and Oren Freifeld and S{\o}ren Hauberg},
  journal = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018},
  publisher={IEEE}
}

@article{LANG2021114666,
title = {NeuroEvolution of augmenting topologies for solving a two-stage hybrid flow shop scheduling problem: A comparison of different solution strategies},
journal = {Expert Systems with Applications},
volume = {172},
pages = {114666},
year = {2021},
issn = {0957-4174},
author = {Sebastian Lang and Tobias Reggelin and Johann Schmidt and Marcel Müller and Abdulrahman Nahhas},
keywords = {Artificial intelligence, Machine learning, Neural network, Neuro evolution, Scheduling, Flow shop},
abstract = {The article investigates the application of NeuroEvolution of Augmenting Topologies (NEAT) to generate and parameterize artificial neural networks (ANN) on determining allocation and sequencing decisions in a two-stage hybrid flow shop scheduling environment with family setup times. NEAT is a machine-learning and neural architecture search algorithm, which generates both, the structure and the hyper-parameters of an ANN. Our experiments show that NEAT can compete with state-of-the-art approaches in terms of solution quality and outperforms them regarding computational efficiency. The main contributions of this article are: (i) A comparison of five different strategies, evaluated with 14 different experiments, on how ANNs can be applied for solving allocation and sequencing problems in a hybrid flow shop environment, (ii) a comparison of the best identified NEAT strategy with traditional heuristic and metaheuristic approaches concerning solution quality and computational efficiency.}
}

@article{vgg,
  title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author={Karen Simonyan and Andrew Zisserman},
  journal={CoRR},
  year={2015},
  volume={abs/1409.1556}
}

@inproceedings{Wu2024,
  title={Test-Time Domain Adaptation by Learning Domain-Aware Batch Normalization},
  author={Wu, Yanan and Chi, Zhixiang and Wang, Yang and Plataniotis, Konstantinos N and Feng, Songhe},
  booktitle={Conference on Artificial Intelligence (AAAI)},
  year={2024}
}

@inproceedings{Wang2021,
title={Tent: Fully Test-Time Adaptation by Entropy Minimization},
author={Dequan Wang and Evan Shelhamer and Shaoteng Liu and Bruno Olshausen and Trevor Darrell},
booktitle={International Conference on Learning Representations (ICLR)},
year={2021},
}

@article{Wang2022,
  title={Continual Test-Time Domain Adaptation},
  author={Wang, Qin and Fink, Olga and Van Gool, Luc and Dai, Dengxin},
  journal={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022}
}

@inproceedings{Mondal2023,
title={Equivariant Adaptation of Large Pretrained Models},
author={Arnab Kumar Mondal and Siba Smarak Panigrahi and S{\'e}kou-Oumar Kaba and Sai Rajeswar and Siamak Ravanbakhsh},
booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
year={2023},
}

@article{Tuggener2023,
      title={Efficient Rotation Invariance in Deep Neural Networks through Artificial Mental Rotation}, 
      author={Lukas Tuggener and Thilo Stadelmann and Jürgen Schmidhuber},
      year={2023},
      journal={ArXiv},
}

@inproceedings{jaderberg2016spatial,
author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
title = {Spatial Transformer Networks},
year = {2015},
booktitle = {International Conference on Neural Information Processing Systems (NeurIPS)},
}

@inproceedings{Liu2020,
 author = {Liu, Weitang and Wang, Xiaoyun and Owens, John and Li, Yixuan},
 booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
 title = {Energy-based Out-of-distribution Detection},
 year = {2020}
}

@inproceedings{Jake2017,
 author = {Snell, Jake and Swersky, Kevin and Zemel, Richard},
 booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
 title = {Prototypical Networks for Few-shot Learning},
 year = {2017}
}


@misc{InstanceNorm,
  doi = {10.48550/ARXIV.1607.08022},
  
  url = {https://arxiv.org/abs/1607.08022},
  
  author = {Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Instance Normalization: The Missing Ingredient for Fast Stylization},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{LayerNorm,
  doi = {10.48550/ARXIV.1607.06450},
  
  url = {https://arxiv.org/abs/1607.06450},
  
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Layer Normalization},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Srivastava2014,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
year = {2014},
issue_date = {January 2014},
publisher = {JMLR.org},
volume = {15},
number = {1},
issn = {1532-4435},
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
journal = {J. Mach. Learn. Res.},
month = {jan},
pages = {1929–1958},
numpages = {30},
keywords = {deep learning, regularization, neural networks, model combination}
}

@article{SCHMIDT2021805,
title = {Approaching Scheduling Problems via a Deep Hybrid Greedy Model and Supervised Learning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {1},
pages = {805-810},
year = {2021},
note = {17th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.08.095},
author = {Johann Schmidt and Sebastian Stober},
keywords = {Scheduling, Optimization, Learning, Neural Networks, Heuristic, Supervision},
abstract = {Scheduling still constitutes a challenging problem, especially for complex problem settings involving due dates and sequence-dependent setups. The majority of existing approaches use heuristics or meta-heuristics, like Genetic Algorithms or Reinforcement Learning. We show that a supervised learning framework can learn and generalize from generated optimal target schedules, which amplifies convergence compared to unsupervised methods. We present a deep hybrid greedy framework, which can predict near-optimal schedules by utilizing the following key mechanisms: (i) Through the interplay between heuristics and a deep neural network our hybrid model can combine the benefits. Specifically, complex patterns from optimal schedules can be learned by a neural network. We reduce the computational costs by outsourcing trivial decisions to heuristics and therefore, allowing consistent decisions during training. (ii) The problem complexity can be reduced, by employing a greedy prediction scheme, where one job at a time is predicted. (iii) We propose a re-scheduling mechanism for idle jobs, which enables long-term cost reduction and renders the framework reactive and dynamic. Through the heuristics and the neural network, our model is real-time capable during inference. We compare our model against prevailing scheduling heuristics and our model outperformed one of them in terms of makespan and lateness minimization. The key purpose of this work is to give a proof of concept, that supervised learning is applicable for complex scheduling problems.}
}

@INPROCEEDINGS{He2016,

  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},

  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Deep Residual Learning for Image Recognition}, 

  year={2016},

  volume={},

  number={},

  pages={770-778},

  doi={10.1109/CVPR.2016.90}}


@article{Imagenet2015,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
}


@article{AbuMostafa1990,
title = {Learning from hints in neural networks},
journal = {Journal of Complexity},
volume = {6},
number = {2},
pages = {192-198},
year = {1990},
issn = {0885-064X},
doi = {https://doi.org/10.1016/0885-064X(90)90006-Y},
author = {Yaser S Abu-Mostafa},
abstract = {Learning from examples is the process of taking input-output examples of an unknown function ƒ and infering an implementation of ƒ. Learning from hints allows for general information about ƒ to be used instead of just input-output examples. We introduce a method for incorporating any invariance hint about ƒ in any descent method for learning from examples. We also show that learning in a neural network remains NP-complete with a certain, biologically plausible, hint about the network. We discuss the information value and the complexity value of hibts.}
}

@INPROCEEDINGS{Djolonga2021,
  author={Djolonga, Josip and Yung, Jessica and Tschannen, Michael and Romijnders, Rob and Beyer, Lucas and Kolesnikov, Alexander and Puigcerver, Joan and Minderer, Matthias and D’Amour, Alexander and Moldovan, Dan and Gelly, Sylvain and Houlsby, Neil and Zhai, Xiaohua and Lucic, Mario},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={On Robustness and Transferability of Convolutional Neural Networks}, 
  year={2021},
}


@article{Zhou2020,
  title={Meta-Learning Symmetries by Reparameterization},
  author={Allan Zhou and Tom Knowles and Chelsea Finn},
  journal={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@article{Schmidt2022,
  author = {Schmidt, Johann and Stober, Sebastian},
  title = {Learning Continuous Rotation Canonicalization with Radial Beam Sampling},
  journal = {ArXiv},
  year = {2022},
}

@article{Adam,
  title={Adam: A Method for Stochastic Optimization},
  author={Diederik P. Kingma and Jimmy Ba},
  journal={CoRR},
  year={2014},
  volume={abs/1412.6980}
}

@article{Hernandez2018,
  author = {Hernández-García, Alex and König, Peter},
  title = {Data augmentation instead of explicit regularization}, 
  journal = {ArXiv},
  year = {2018},
}

@article{Graf2006,
author = {Graf, Markus},
year = {2006},
title = {Coordinate Transformations in Object Recognition},
journal = {Psychological bulletin},
}

@inproceedings{Chefer2022,
title={Optimizing Relevance Maps of Vision Transformers Improves Robustness},
author={Hila Chefer and Idan Schwartz and Lior Wolf},
booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
year={2022},
}

@inproceedings{AdamW,
  title={Decoupled Weight Decay Regularization},
  author={Ilya Loshchilov and Frank Hutter},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017},
}

@article{FashionMNIST,
  title={Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  author={Han Xiao and Kashif Rasul and Roland Vollgraf},
  journal={ArXiv},
  year={2017},
}

@ARTICLE{LeNet,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
}

@article{Sanbonmatsu1998,
  title={Selective hypothesis testing},
  author={Sanbonmatsu, David M and Posavac, Steven S and Kardes, Frank R and Mantel, Susan P},
  journal={Psychonomic Bulletin \& Review},
  year={1998},
}

@article{Sterzer2007,
author = {Philipp Sterzer  and Andreas Kleinschmidt },
title = {A neural basis for inference in perceptual ambiguity},
journal = {Proceedings of the National Academy of Sciences},
volume = {104},
number = {1},
pages = {323-328},
year = {2007},
doi = {10.1073/pnas.0609006104},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.0609006104},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0609006104},}


@InProceedings{Moon2020,
  title = 	 {Confidence-Aware Learning for Deep Neural Networks},
  author =       {Moon, Jooyoung and Kim, Jihyo and Shin, Younghak and Hwang, Sangheum},
  booktitle = 	 {International Conference on Machine Learning (ICML)},
  year = 	 {2020},
}

@inproceedings{kotelevskii2022,
 author = {Kotelevskii, Nikita and Artemenkov, Aleksandr and Fedyanin, Kirill and Noskov, Fedor and Fishkov, Alexander and Shelmanov, Artem and Vazhentsev, Artem and Petiushko, Aleksandr and Panov, Maxim},
 booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
 title = {Nonparametric Uncertainty Quantification for Single Deterministic Neural Network},
 year = {2022}
}


@inproceedings{Balestriero2022ADI,
 author = {Balestriero, Randall and Misra, Ishan and LeCun, Yann},
 booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
 title = {A Data-Augmentation Is Worth A Thousand Samples: Analytical Moments And Sampling-Free Training},
 year = {2022}
}

@article{LeJeune2019,
  author = {LeJeune, Daniel and Balestriero, Randall and Javadi, Hamid and Baraniuk, Richard G.},
  title = {Implicit Rugosity Regularization via Data Augmentation},
  journal = {ArXiv},
  year = {2019},
}

@article{Huang2021,
  title={DAIR: Data Augmented Invariant Regularization},
  author={Tianjian Huang and Chinnadhurai Sankar and Pooyan Amini and Satwik Kottur and Alborz Geramifard and Meisam Razaviyayn and Ahmad Beirami},
  journal={ArXiv},
  year={2021},
}

@article{Balestriero2022,
  title={The Effects of Regularization and Data Augmentation are Class Dependent},
  author={Randall Balestriero and L{\'e}on Bottou and Yann LeCun},
  journal={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2022}
}


@article{AbuMostafa1993,
author = {Abu-Mostafa, Yaser S.},
title = {Hints and the VC Dimension},
year = {1993},
issue_date = {March 1993},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {5},
number = {2},
issn = {0899-7667},
doi = {10.1162/neco.1993.5.2.278},
abstract = {Learning from hints is a generalization of learning from
examples that allows for a variety of information about the unknown
function to be used in the learning process. In this paper, we use
the VC dimension, an established tool for analyzing learning from
examples, to analyze learning from hints. In particular, we show
how the VC dimension is affected by the introduction of a hint. We
also derive a new quantity that defines a VC dimension for the hint
itself. This quantity is used to estimate the number of examples
needed to "absorb" the hint. We carry out the analysis for two
types of hints, invariances and catalysts. We also describe how the
same method can be applied to other types of hints.},
journal = {Neural Comput.},
month = {mar},
pages = {278–288},
numpages = {11}
}

@book{Seckel2004,
  title={Masters of deception: Escher, Dali \& the artists of optical illusion},
  author={Al Seckel},
  year={2004},
  publisher={New York : Sterling Pub. Co.},
  isbn={1402705778}
}

@book{Borodin2017,
  title={Stochastic Processes},
  author={Andrei N. Borodin},
  year={2017},
  publisher={Birkhäuser Cham},
}

@inproceedings{Goodfellow2015,
title	= {Explaining and Harnessing Adversarial Examples},
author	= {Ian Goodfellow and Jonathon Shlens and Christian Szegedy},
year	= {2015},
URL	= {http://arxiv.org/abs/1412.6572},
booktitle	= {International Conference on Learning Representations}
}

@inproceedings{Elsayed2018,
 author = {Elsayed, Gamaleldin and Shankar, Shreya and Cheung, Brian and Papernot, Nicolas and Kurakin, Alexey and Goodfellow, Ian and Sohl-Dickstein, Jascha},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Adversarial Examples that Fool both Computer Vision and Time-Limited Humans},
 url = {https://proceedings.neurips.cc/paper/2018/file/8562ae5e286544710b2e7ebe9858833b-Paper.pdf},
 volume = {31},
 year = {2018}
}


@book{hall2015lie,
  title={Lie Groups, Lie Algebras, and Representations: An Elementary Introduction},
  author={Hall, B.},
  series={Graduate Texts in Mathematics},
  year={2015},
  publisher={Springer International Publishing}
}

@article{palmer1981,
  title={Canonical perspective and the perception of objects},
  author={Palmer, S. E. and Rosch, E. and Chase, P.},
  year={1981},
  journal = {Vision Research},
}


@article{hu2019exploring,
title = {Exploring weight symmetry in deep neural networks},
journal = {Computer Vision and Image Understanding},
year = {2019},
author = {Shell Xu Hu and Sergey Zagoruyko and Nikos Komodakis},
}

@inproceedings{Marcos_2017,
  year = 2017,
  author = {Diego Marcos and Michele Volpi and Nikos Komodakis and Devis Tuia},
  title = {Rotation Equivariant Vector Field Networks},
  booktitle = {IEEE International Conference on Computer Vision (ICCV)}
}


@article{kondor2018covariant,
  doi = {10.48550/ARXIV.1801.02144},

  author = {Kondor, Risi and Son, Hy Truong and Pan, Horace and Anderson, Brandon and Trivedi, Shubhendu},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Covariant Compositional Networks For Learning Graphs},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{badrinarayanan2015understanding,
      title={Understanding symmetries in deep networks},
      author={Vijay Badrinarayanan and Bamdev Mishra and Roberto Cipolla},
      year={2015},
      eprint={1511.01029},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{Gens2014,
author = {Gens, Robert and Domingos, Pedro},
title = {Deep Symmetry Networks},
year = {2014},
booktitle = {International Conference on Neural Information Processing Systems (NeurIPS)},
}

@inproceedings{Goodfellow2009,
author = {Goodfellow, Ian J. and Le, Quoc V. and Saxe, Andrew M. and Lee, Honglak and Ng, Andrew Y.},
title = {Measuring Invariances in Deep Networks},
year = {2009},
booktitle = {International Conference on Neural Information Processing Systems (NeurIPS)},
}


@misc{gurobi,
  author = {{Gurobi Optimization, LLC}},
  title = {{Gurobi Optimizer Reference Manual}},
  year = 2021,
  url = "https://www.gurobi.com"
}

@misc{vaswani2017attention,
      title={Attention Is All You Need},
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hottung2021efficient,
      title={Efficient Active Search for Combinatorial Optimization Problems},
      author={André Hottung and Yeong-Dae Kwon and Kevin Tierney},
      year={2021},
      eprint={2106.05126},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@INPROCEEDINGS{HeInit,  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)},   title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},   year={2015},  volume={},  number={},  pages={1026-1034},  doi={10.1109/ICCV.2015.123}}


@TechReport{LFW,
  author =       {Gary B. Huang and Manu Ramesh and Tamara Berg and
                  Erik Learned-Miller},
  title =        {Labeled Faces in the Wild: A Database for Studying
                  Face Recognition in Unconstrained Environments},
  institution =  {University of Massachusetts, Amherst},
  year =         2007,
  number =       {07-49},
  month =        {October}}

@inproceedings{CIFAR10,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Alex Krizhevsky},
  year={2009}
}

@inproceedings{Ghamizi2023,
      title={On Evaluating Adversarial Robustness of Chest X-ray Classification: Pitfalls and Best Practices}, 
      author={Salah Ghamizi and Maxime Cordy and Michail Papadakis and Yves Le Traon},
      year={2022},
      booktitle={ArXiv},
}


@Article{Mehrjardi2020,
AUTHOR = {Taghizadeh-Mehrjardi, Ruhollah and Nabiollahi, Kamal and Rasoli, Leila and Kerry, Ruth and Scholten, Thomas},
TITLE = {Land Suitability Assessment and Agricultural Production Sustainability Using Machine Learning Models},
JOURNAL = {Agronomy},
YEAR = {2020},
}

@article{Moen2019,
author = {Moen, Erick and Bannon, Dylan and Kudo, Takamasa and Graf, William and Covert, Markus and Valen, David},
year = {2019},
title = {Deep learning for cellular image analysis},
journal = {Nature Methods},
}

@article{Gomez2020,
    author = {Gómez, Catalina and Neira, Mauricio and Hernández Hoyos, Marcela and Arbeláez, Pablo and Forero-Romero, Jaime E},
    title = "{Classifying image sequences of astronomical transients with deep neural networks}",
    journal = {Monthly Notices of the Royal Astronomical Society},
    year = {2020},
}

@Article{Andrade2020,
AUTHOR = {Andrade, R. B. and Costa, G. A. O. P. and Mota, G. L. A. and Ortega, M. X. and Feitosa, R. Q. and Soto, P. J. and Heipke, C.},
TITLE = {EVALUATION OF SEMANTIC SEGMENTATION METHODS FOR DEFORESTATION DETECTION IN THE AMAZON},
JOURNAL = {The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
YEAR = {2020}
}

@article{Inoue2020,
author = {Yoshio Inoue},
title = {Satellite- and drone-based remote sensing of crops and soils for smart farming – a review},
journal = {Soil Science and Plant Nutrition},
year = {2020},
}

@inproceedings{dai2023,
title={Fundamental limits on the robustness of image classifiers},
author={Zheng Dai and David Gifford},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=gpmL0D4VjN4}
}

@article{Bresenham1965AlgorithmFC,
  title={Algorithm for Computer Control of a Digital Plotter},
  author={Jack Bresenham},
  journal={IBM Syst. J.},
  year={1965},
  volume={4},
  pages={25-30}
}

@inproceedings{COIL100,
  title={Columbia Object Image Library (COIL100)},
  author={Sheila J. Nayar},
  year={1996}
}

@inproceedings{NEURIPS2020_f4573fc7,
 author = {Chen, Shuxiao and Dobriban, Edgar and Lee, Jane},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {21321--21333},
 publisher = {Curran Associates, Inc.},
 title = {A Group-Theoretic Framework for Data Augmentation},
 volume = {33},
 year = {2020}
}

@inproceedings{BatchNorm,
author = {Ioffe, Sergey and Szegedy, Christian},
title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
year = {2015},
booktitle = {International Conference on Machine Learning (ICML)},
}

@INPROCEEDINGS{DenseNet,

  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},

  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Densely Connected Convolutional Networks}, 

  year={2017},

  volume={},

  number={},

  pages={2261-2269},

  doi={10.1109/CVPR.2017.243}}


@article{Simonyan2014,
  title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author={Karen Simonyan and Andrew Zisserman},
  journal={CoRR},
  year={2014},
  volume={abs/1409.1556}
}

@INPROCEEDINGS{MobileNet,

  author={Howard, Andrew and Sandler, Mark and Chen, Bo and Wang, Weijun and Chen, Liang-Chieh and Tan, Mingxing and Chu, Grace and Vasudevan, Vijay and Zhu, Yukun and Pang, Ruoming and Adam, Hartwig and Le, Quoc},

  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 

  title={Searching for MobileNetV3}, 

  year={2019},

  volume={},

  number={},

  pages={1314-1324},

  doi={10.1109/ICCV.2019.00140}}


@INPROCEEDINGS{ResNeXt,

  author={Xie, Saining and Girshick, Ross and Dollár, Piotr and Tu, Zhuowen and He, Kaiming},

  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Aggregated Residual Transformations for Deep Neural Networks}, 

  year={2017},

  volume={},

  number={},

  pages={5987-5995},

  doi={10.1109/CVPR.2017.634}}


@book{nash2016,
  title={A Friendly Introduction to Group Theory},
  author={Nash, D.},
  year={2016},
  publisher={CreateSpace Independent Publishing Platform}
}

@INPROCEEDINGS{InceptionNet,

  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},

  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 

  title={Rethinking the Inception Architecture for Computer Vision}, 

  year={2016},

  volume={},

  number={},

  pages={2818-2826},

  doi={10.1109/CVPR.2016.308}}


@inproceedings{Ustyuzhaninov2020Rotation,
title={Rotation-invariant clustering of neuronal responses in primary visual cortex},
author={Ivan Ustyuzhaninov and Santiago A. Cadena and Emmanouil Froudarakis and Paul G. Fahey and Edgar Y. Walker and Erick Cobos and Jacob Reimer and Fabian H. Sinz and Andreas S. Tolias and Matthias Bethge and Alexander S. Ecker},
booktitle={International Conference on Learning Representations (ICLR)},
year={2020},
}


@InProceedings{kernelDA2019,
  title =    {A Kernel Theory of Modern Data Augmentation},
  author =       {Dao, Tri and Gu, Albert and Ratner, Alexander and Smith, Virginia and De Sa, Chris and Re, Christopher},
  booktitle =    {International Conference on Machine Learning (ICML)},
  year =   {2019},
}

@article{ecker2018rotationequivariant,
      title={A rotation-equivariant convolutional neural network model of primary visual cortex},
      author={Alexander S. Ecker and Fabian H. Sinz and Emmanouil Froudarakis and Paul G. Fahey and Santiago A. Cadena and Edgar Y. Walker and Erick Cobos and Jacob Reimer and Andreas S. Tolias and Matthias Bethge},
      year={2019},
      journal={International Conference on Learning Representations (ICLR)},
}

@article{Nabarro2021DataAI,
  title={Data augmentation in Bayesian neural networks and the cold posterior effect},
  author={Seth Nabarro and Stoil Ganev and Adria Garriga-Alonso and Vincent Fortuin and Mark van der Wilk and Laurence Aitchison},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.05586}
}

@INPROCEEDINGS{Worrall2017,

  author={Worrall, Daniel E. and Garbin, Stephan J. and Turmukhambetov, Daniyar and Brostow, Gabriel J.},

  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},

  title={Harmonic Networks: Deep Translation and Rotation Equivariance},

  year={2017},

  volume={},

  number={},

  pages={7168-7177},

  doi={10.1109/CVPR.2017.758}
}


@misc{kool2019attention,
      title={Attention, Learn to Solve Routing Problems!},
      author={Wouter Kool and Herke van Hoof and Max Welling},
      year={2019},
      eprint={1803.08475},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{farina2021symmetrydriven,
      title={Symmetry-driven graph neural networks},
      author={Francesco Farina and Emma Slade},
      year={2021},
      volume={2105.14058},
      journal={arXiv},
      primaryClass={cs.LG}
}

@article{Weiler2017,
  title={Learning Steerable Filters for Rotation Equivariant CNNs},
  author={Maurice Weiler and Fred A. Hamprecht and Martin Storath},
  journal={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
}

@inproceedings{Weiler2019,
 author = {Weiler, Maurice and Cesa, Gabriele},
 booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
 title = {General E(2)-Equivariant Steerable CNNs},
 year = {2019}
}

@article{xu2019powerful,
  doi = {10.48550/ARXIV.1810.00826},
  
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {How Powerful are Graph Neural Networks?},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{Weisfeiler1968,
  added-at = {2019-05-21T10:10:49.000+0200},
  author = {Weisfeiler, Boris and Lehman, A. A.},
  interhash = {19503885fa41b29ec57a26cd3f535a14},
  intrahash = {d69974889cbfc06859c224edb0c9791d},
  journal = {Nauchno-Technicheskaya Informatsia},
  keywords = {imported},
  number = {N9},
  pages = {12--16},
  timestamp = {2019-05-21T10:10:49.000+0200},
  title = {{A Reduction of a Graph to a Canonical Form and an Algebra Arising During This Reduction}},
  volume = {Ser. 2},
  year = 1968
}



@misc{dehaan2020natural,
      title={Natural Graph Networks},
      author={Pim de Haan and Taco Cohen and Max Welling},
      year={2020},
      eprint={2007.08349},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
kipf2017semisupervised,
title={Semi-Supervised Classification with Graph Convolutional Networks},
author={Thomas N. Kipf and Max Welling},
booktitle={International Conference on Learning Representations},
year={2017},
}

@article{tsne,
  title={Visualizing Data using t-SNE},
  author={Laurens van der Maaten and Geoffrey E. Hinton},
  journal={Journal of Machine Learning Research},
  year={2008},
  volume={9},
  pages={2579-2605}
}

@misc{ma2019combinatorial,
      title={Combinatorial Optimization by Graph Pointer Networks and Hierarchical Reinforcement Learning},
      author={Qiang Ma and Suwen Ge and Danyang He and Darshan Thaker and Iddo Drori},
      year={2019},
      eprint={1911.04936},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{bronstein2021geometric,
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
  title = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
  journal = {ArXiv},
  year = {2021},
}

@article{Chen2019InvarianceRV,
  title={Invariance reduces Variance: Understanding Data Augmentation in Deep Learning and Beyond},
  author={Shuxiao Chen and Edgar Dobriban and Jane Lee},
  journal={ArXiv},
  year={2019},
}



@InProceedings{cohen2016group,
  title =    {Group Equivariant Convolutional Networks},
  author =   {Cohen, Taco and Welling, Max},
  booktitle =    {International Conference on Machine Learning (ICML)},
  year =   {2016},
}


@misc{vinyals2017pointer,
      title={Pointer Networks},
      author={Oriol Vinyals and Meire Fortunato and Navdeep Jaitly},
      year={2017},
      eprint={1506.03134},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{DeepSets,
 author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Sets},
 volume = {30},
 year = {2017}
}


@article{ARTHANARI2000173,
title = {An alternate formulation of the symmetric traveling salesman problem and its properties},
journal = {Discrete Applied Mathematics},
volume = {98},
number = {3},
pages = {173-190},
year = {2000},
issn = {0166-218X},
doi = {https://doi.org/10.1016/S0166-218X(99)00154-7},
author = {T.S Arthanari and M Usha},
keywords = {Traveling salesman problem, Subtour elimination polytope, Problem formulations},
abstract = {In this paper we give an alternate formulation of the symmetric traveling salesman problem and give its properties. We compare the polytope defined by this formulation, U(n), with the standard subtour elimination polytope SEP(n). We show U(n)⊆SEP(n).}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
} 

@Article{convnext,
  author  = {Zhuang Liu and Hanzi Mao and Chao-Yuan Wu and Christoph Feichtenhofer and Trevor Darrell and Saining Xie},
  title   = {A ConvNet for the 2020s},
  journal = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year    = {2022},
}

@InProceedings{Engstrom2019,
title = {Exploring the Landscape of Spatial Robustness},
author = {Engstrom, Logan and Tran, Brandon and Tsipras, Dimitris and Schmidt, Ludwig and Madry, Aleksander},
booktitle = {International Conference on Machine Learning (ICML)},
year = {2019},
}

@inproceedings{Schmidt2021,
author={Bexten, Simone and Schmidt, Johann and Walter, Christoph and Elkmann, Norbert},
booktitle ={IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)},
title={Human Action Recognition as part of a Natural Machine Operation Framework},
year={2021},
}

@book{Landau2005,
author = {Landau, David and Binder, Kurt},
title = {A Guide to Monte Carlo Simulations in Statistical Physics},
year = {2005},
publisher = {Cambridge University Press},
}


@inproceedings{Schmidt2024,
  author = {Schmidt, Johann and Köhler, Benjamin and Borstell, Hagen},
  title = {Reviving Simulated Annealing: Lifting its Degeneracies for Real-Time Job Scheduling},
  booktitle = {Hawaii International Conference on System Sciences (HICSS)},
  year = {2024},
}

@InProceedings{efficientnet,
  title =    {{E}fficient{N}et: Rethinking Model Scaling for Convolutional Neural Networks},
  author =       {Tan, Mingxing and Le, Quoc},
  booktitle =    {International Conference on Machine Learning (ICML)},
  year =   {2019},
}



@inproceedings{recht2019imagenet,
  title={Do ImageNet Classifiers Generalize to ImageNet?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle={International Conference on Machine Learning},
  pages={5389--5400},
  year={2019}
}

@article{simonyan2014deep,
  title={Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  author={Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
  journal={CoRR},
  year={2014},
  volume={abs/1312.6034}
}

@misc{Gudimella2017,
  doi = {10.48550/ARXIV.1709.06977},
  
  author = {Gudimella, Aditya and Story, Ross and Shaker, Matineh and Kong, Ruofan and Brown, Matthew and Shnayder, Victor and Campos, Marcos},
  
  keywords = {Artificial Intelligence (cs.AI), Robotics (cs.RO), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Deep Reinforcement Learning for Dexterous Manipulation with Concept Networks},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{Saeed2021,
author = {Saeed, Faisal and Muhammad Jamal, Ahmed and junaid, Malik and Hong, Kim and Paul, Anand and Kavitha, Muthu Subash},
year = {2021},
month = {12},
pages = {},
title = {A robust approach for industrial small-object detection using an improved faster regional convolutional neural network},
volume = {11},
journal = {Scientific Reports},
doi = {10.1038/s41598-021-02805-y}
}
