\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balduzzi et~al.(2015)Balduzzi, Vanchinathan, and
  Buhmann]{balduzzi2015kickback}
Balduzzi, D., Vanchinathan, H., and Buhmann, J.~M.
\newblock Kickback cuts backprop's red-tape: Biologically plausible credit
  assignment in neural networks.
\newblock In \emph{AAAI}, pp.\  485--491, 2015.

\bibitem[Bengio et~al.(2009)]{bengio2009learning}
Bengio, Y. et~al.
\newblock Learning deep architectures for ai.
\newblock \emph{Foundations and trends{\textregistered} in Machine Learning},
  2\penalty0 (1):\penalty0 1--127, 2009.

\bibitem[Bottou et~al.(2016)Bottou, Curtis, and
  Nocedal]{bottou2016optimization}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{arXiv preprint arXiv:1606.04838}, 2016.

\bibitem[Carreira-Perpinan \& Wang(2014)Carreira-Perpinan and
  Wang]{carreira2014distributed}
Carreira-Perpinan, M. and Wang, W.
\newblock Distributed optimization of deeply nested systems.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  10--19, 2014.

\bibitem[Czarnecki et~al.(2017)Czarnecki, {\'S}wirszcz, Jaderberg, Osindero,
  Vinyals, and Kavukcuoglu]{czarnecki2017understanding}
Czarnecki, W.~M., {\'S}wirszcz, G., Jaderberg, M., Osindero, S., Vinyals, O.,
  and Kavukcuoglu, K.
\newblock Understanding synthetic gradients and decoupled neural interfaces.
\newblock \emph{arXiv preprint arXiv:1703.00522}, 2017.

\bibitem[Eldan \& Shamir(2016)Eldan and Shamir]{eldan2016power}
Eldan, R. and Shamir, O.
\newblock The power of depth for feedforward neural networks.
\newblock In \emph{Conference on Learning Theory}, pp.\  907--940, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, and Swersky]{hinton2012neural}
Hinton, G., Srivastava, N., and Swersky, K.
\newblock Neural networks for machine learning-lecture 6a-overview of
  mini-batch gradient descent, 2012.

\bibitem[Huang et~al.(2016)Huang, Liu, Weinberger, and van~der
  Maaten]{huang2016densely}
Huang, G., Liu, Z., Weinberger, K.~Q., and van~der Maaten, L.
\newblock Densely connected convolutional networks.
\newblock \emph{arXiv preprint arXiv:1608.06993}, 2016.

\bibitem[Jaderberg et~al.(2016)Jaderberg, Czarnecki, Osindero, Vinyals, Graves,
  and Kavukcuoglu]{jaderberg2016decoupled}
Jaderberg, M., Czarnecki, W.~M., Osindero, S., Vinyals, O., Graves, A., and
  Kavukcuoglu, K.
\newblock Decoupled neural interfaces using synthetic gradients.
\newblock \emph{arXiv preprint arXiv:1608.05343}, 2016.

\bibitem[Johnson(2017)]{benchmark}
Johnson, J.
\newblock Benchmarks for popular cnn models.
\newblock \url{https://github.com/jcjohnson/cnn-benchmarks}, 2017.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[N{\o}kland(2016)]{nokland2016direct}
N{\o}kland, A.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1037--1045, 2016.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Qian(1999)]{qian1999momentum}
Qian, N.
\newblock On the momentum term in gradient descent learning algorithms.
\newblock \emph{Neural networks}, 12\penalty0 (1):\penalty0 145--151, 1999.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951stochastic}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pp.\  400--407, 1951.

\bibitem[Rumelhart et~al.(1988)Rumelhart, Hinton, Williams,
  et~al.]{rumelhart1988learning}
Rumelhart, D.~E., Hinton, G.~E., Williams, R.~J., et~al.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Cognitive modeling}, 5\penalty0 (3):\penalty0 1, 1988.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
  Vanhoucke, V., and Rabinovich, A.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  1--9, 2015.

\bibitem[Taylor et~al.(2016)Taylor, Burmeister, Xu, Singh, Patel, and
  Goldstein]{taylor2016training}
Taylor, G., Burmeister, R., Xu, Z., Singh, B., Patel, A., and Goldstein, T.
\newblock Training neural networks without gradients: A scalable admm approach.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2722--2731, 2016.

\bibitem[Telgarsky(2016)]{telgarsky2016benefits}
Telgarsky, M.
\newblock Benefits of depth in neural networks.
\newblock \emph{arXiv preprint arXiv:1602.04485}, 2016.

\bibitem[Yadan et~al.(2013)Yadan, Adams, Taigman, and Ranzato]{yadan2013multi}
Yadan, O., Adams, K., Taigman, Y., and Ranzato, M.
\newblock Multi-gpu training of convnets.
\newblock \emph{arXiv preprint arXiv:1312.5853}, 2013.

\end{thebibliography}
