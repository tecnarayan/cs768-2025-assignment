\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu(2017)]{allen2017katyusha}
Allen-Zhu, Z.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  1200--1205. ACM, 2017.

\bibitem[Bergou et~al.(2019)Bergou, Gorbunov, and
  Richt{\'a}rik]{bergou2019stochastic}
Bergou, E.~H., Gorbunov, E., and Richt{\'a}rik, P.
\newblock Stochastic three points method for unconstrained smooth minimization.
\newblock \emph{arXiv preprint arXiv:1902.03591}, 2019.

\bibitem[Bubeck et~al.(2012)Bubeck, Cesa-Bianchi, et~al.]{bubeck2012regret}
Bubeck, S., Cesa-Bianchi, N., et~al.
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  5\penalty0 (1):\penalty0 1--122, 2012.

\bibitem[Chen et~al.(2017)Chen, Zhang, Sharma, Yi, and Hsieh]{chen2017zoo}
Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J.
\newblock Zoo: Zeroth order optimization based black-box attacks to deep neural
  networks without training substitute models.
\newblock In \emph{Proceedings of the 10th ACM Workshop on Artificial
  Intelligence and Security}, pp.\  15--26. ACM, 2017.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1646--1654, 2014.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Fang, C., Li, C.~J., Lin, Z., and Zhang, T.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  689--699, 2018.

\bibitem[Gadat et~al.(2018)Gadat, Panloup, Saadane,
  et~al.]{gadat2018stochastic}
Gadat, S., Panloup, F., Saadane, S., et~al.
\newblock Stochastic heavy ball.
\newblock \emph{Electronic Journal of Statistics}, 12\penalty0 (1):\penalty0
  461--529, 2018.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Gorbunov et~al.(2018)Gorbunov, Dvurechensky, and
  Gasnikov]{gorbunov2018accelerated}
Gorbunov, E., Dvurechensky, P., and Gasnikov, A.
\newblock An accelerated method for derivative-free smooth stochastic convex
  optimization.
\newblock \emph{arXiv preprint arXiv:1802.09022}, 2018.

\bibitem[Gorbunov et~al.(2019)Gorbunov, Bibi, Sener, Bergou, and
  Richt{\'a}rik]{gorbunov2019stochastic}
Gorbunov, E., Bibi, A., Sener, O., Bergou, E.~H., and Richt{\'a}rik, P.
\newblock A stochastic derivative free optimization method with momentum.
\newblock \emph{arXiv preprint arXiv:1905.13278}, 2019.

\bibitem[Ji et~al.(2019)Ji, Wang, Zhou, and Liang]{ji2019improved}
Ji, K., Wang, Z., Zhou, Y., and Liang, Y.
\newblock Improved zeroth-order variance reduced algorithms and analysis for
  nonconvex optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3100--3109, 2019.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson2013accelerating}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  315--323, 2013.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lan(2012)]{lan2012optimal}
Lan, G.
\newblock An optimal method for stochastic composite optimization.
\newblock \emph{Mathematical Programming}, 133\penalty0 (1-2):\penalty0
  365--397, 2012.

\bibitem[Lan \& Zhou(2018)Lan and Zhou]{lan2018optimal}
Lan, G. and Zhou, Y.
\newblock An optimal randomized incremental gradient method.
\newblock \emph{Mathematical programming}, 171\penalty0 (1-2):\penalty0
  167--215, 2018.

\bibitem[Lan et~al.(2019)Lan, Li, and Zhou]{lan2019unified}
Lan, G., Li, Z., and Zhou, Y.
\newblock A unified variance-reduced accelerated gradient method for convex
  optimization.
\newblock \emph{arXiv preprint arXiv:1905.12412}, 2019.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{lin2015universal}
Lin, H., Mairal, J., and Harchaoui, Z.
\newblock A universal catalyst for first-order optimization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3384--3392, 2015.

\bibitem[Liu et~al.(2018{\natexlab{a}})Liu, Cheng, Hsieh, and
  Tao]{liu2018stochasticb}
Liu, L., Cheng, M., Hsieh, C.-J., and Tao, D.
\newblock Stochastic zeroth-order optimization via variance reduction method.
\newblock \emph{arXiv preprint arXiv:1805.11811}, 2018{\natexlab{a}}.

\bibitem[Liu et~al.(2018{\natexlab{b}})Liu, Kailkhura, Chen, Ting, Chang, and
  Amini]{liu2018stochastic}
Liu, S., Kailkhura, B., Chen, P.-Y., Ting, P., Chang, S., and Amini, L.
\newblock Zeroth-order stochastic variance reduction for nonconvex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3727--3737, 2018{\natexlab{b}}.

\bibitem[Matyas(1965)]{matyas1965random}
Matyas, J.
\newblock Random optimization.
\newblock \emph{Automation and Remote control}, 26\penalty0 (2):\penalty0
  246--253, 1965.

\bibitem[Nelder \& Mead(1965)Nelder and Mead]{nelder1965simplex}
Nelder, J.~A. and Mead, R.
\newblock A simplex method for function minimization.
\newblock \emph{The computer journal}, 7\penalty0 (4):\penalty0 308--313, 1965.

\bibitem[Nesterov(2014)]{nesterov2004introbook}
Nesterov, Y.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer Publishing Company, Incorporated, 1 edition, 2014.
\newblock ISBN 1461346916.

\bibitem[Nesterov \& Spokoiny(2011)Nesterov and Spokoiny]{nesterov2017random}
Nesterov, Y. and Spokoiny, V.
\newblock Random gradient-free minimization of convex functions.
\newblock \emph{Foundations of Computational Mathematics}, 17\penalty0
  (2):\penalty0 527--566, 2011.

\bibitem[Nesterov(1983)]{nesterov1983method}
Nesterov, Y.~E.
\newblock A method for solving the convex programming problem with convergence
  rate o (1/k\^{} 2).
\newblock In \emph{Dokl. akad. nauk Sssr}, volume 269, pp.\  543--547, 1983.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Takáč]{nguyen2017sarah}
Nguyen, L.~M., Liu, J., Scheinberg, K., and Takáč, M.
\newblock Sarah: A novel method for machine learning problems using stochastic
  recursive gradient, 2017.

\bibitem[Orvieto et~al.(2019)Orvieto, Kohler, and Lucchi]{orvieto2019role}
Orvieto, A., Kohler, J., and Lucchi, A.
\newblock The role of memory in stochastic optimization.
\newblock In \emph{UAI}, 2019.

\bibitem[Polyak(1964)]{polyak1964some}
Polyak, B.~T.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Salimans et~al.(2017)Salimans, Ho, Chen, Sidor, and
  Sutskever]{salimans2017evolution}
Salimans, T., Ho, J., Chen, X., Sidor, S., and Sutskever, I.
\newblock Evolution strategies as a scalable alternative to reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1703.03864}, 2017.

\bibitem[{Shang} et~al.(2017){Shang}, {Liu}, {Cheng}, and
  {Zhuo}]{fanhua2017simKatyusha}
{Shang}, F., {Liu}, Y., {Cheng}, J., and {Zhuo}, J.
\newblock {Fast Stochastic Variance Reduced Gradient Method with Momentum
  Acceleration for Machine Learning}.
\newblock \emph{arXiv e-prints}, Mar 2017.

\bibitem[Zhou et~al.(2020)Zhou, Xu, and Gu]{zhou2018nested}
Zhou, D., Xu, P., and Gu, Q.
\newblock Stochastic nested variance reduction for nonconvex optimization.
\newblock \emph{Journal of Machine Learning Research}, 2020.

\end{thebibliography}
