\begin{thebibliography}{}

\bibitem[Chawla(2009)]{chawla2009data}
Chawla, N.~V. (2009).
\newblock Data mining for imbalanced datasets: An overview.
\newblock In \emph{Data Mining and Knowledge Discovery Handbook},  875--886.
  Springer.

\bibitem[Chawla \emph{et~al.}(2002)Chawla, Bowyer, Hall, and
  Kegelmeyer]{chawla2002smote}
Chawla, N.~V., Bowyer, K.~W., Hall, L.~O., and Kegelmeyer, W.~P. (2002).
\newblock Smote: synthetic minority over-sampling technique.
\newblock \emph{Journal of Artificial Intelligence Research} \textbf{16},
  321--357.

\bibitem[Chawla \emph{et~al.}(2004)Chawla, Japkowicz, and
  Kotcz]{chawla2004editorial}
Chawla, N.~V., Japkowicz, N., and Kotcz, A. (2004).
\newblock Editorial: special issue on learning from imbalanced data sets.
\newblock \emph{ACM SIGKDD Explorations Newsletter} \textbf{6}, 1, 1--6.

\bibitem[Chen \emph{et~al.}(2016)Chen, Sun, Li, Lu, and Hua]{chen2016deep}
Chen, J., Sun, B., Li, H., Lu, H., and Hua, X.-S. (2016).
\newblock Deep ctr prediction in display advertising.
\newblock In \emph{Proceedings of the 24th ACM International Conference on
  Multimedia (MM)},  811--820.

\bibitem[Cheng \emph{et~al.}(2010)Cheng, Huang,
  \emph{et~al.}]{cheng2010bootstrap}
Cheng, G., Huang, J.~Z., \emph{et~al.} (2010).
\newblock Bootstrap consistency for general semiparametric m-estimation.
\newblock \emph{The Annals of Statistics} \textbf{38}, 5, 2884--2915.

\bibitem[Douzas and Bacao(2017)]{douzas2017self}
Douzas, G. and Bacao, F. (2017).
\newblock Self-organizing map oversampling (somo) for imbalanced data set
  learning.
\newblock \emph{Expert Systems with Applications} \textbf{82}, 40--52.

\bibitem[Drummond \emph{et~al.}(2003)Drummond, Holte,
  \emph{et~al.}]{drummond2003c4}
Drummond, C., Holte, R.~C., \emph{et~al.} (2003).
\newblock C4. 5, class imbalance, and cost sensitivity: why under-sampling
  beats over-sampling.
\newblock In \emph{Workshop on Learning from Imbalanced Datasets II}, vol.~11,
  1--8. Citeseer.

\bibitem[Estabrooks \emph{et~al.}(2004)Estabrooks, Jo, and
  Japkowicz]{estabrooks2004multiple}
Estabrooks, A., Jo, T., and Japkowicz, N. (2004).
\newblock A multiple resampling method for learning from imbalanced data sets.
\newblock \emph{Computational intelligence} \textbf{20}, 1, 18--36.

\bibitem[Fithian and Hastie(2014)]{fithian2014local}
Fithian, W. and Hastie, T. (2014).
\newblock Local case-control sampling: Efficient subsampling in imbalanced data
  sets.
\newblock \emph{Annals of statistics} \textbf{42}, 5, 1693.

\bibitem[Guo \emph{et~al.}(2017)Guo, Tang, Ye, Li, and He]{guo2017deepfm}
Guo, H., Tang, R., Ye, Y., Li, Z., and He, X. (2017).
\newblock Deepfm: a factorization-machine based neural network for ctr
  prediction.
\newblock \emph{arXiv preprint arXiv:1703.04247} .

\bibitem[Han \emph{et~al.}(2005)Han, Wang, and Mao]{Han2005Borderline}
Han, H., Wang, W.-Y., and Mao, B.-H. (2005).
\newblock Borderline-smote: A new over-sampling method in imbalanced data sets
  learning.
\newblock In D.-S. Huang, X.-P. Zhang, and G.-B. Huang, eds., \emph{Advances in
  Intelligent Computing},  878--887, Berlin, Heidelberg. Springer Berlin
  Heidelberg.

\bibitem[Han \emph{et~al.}(2020)Han, Tan, Yang, Zhang,
  \emph{et~al.}]{han2020local}
Han, L., Tan, K.~M., Yang, T., Zhang, T., \emph{et~al.} (2020).
\newblock Local uncertainty sampling for large-scale multiclass logistic
  regression.
\newblock \emph{Annals of Statistics} \textbf{48}, 3, 1770--1788.

\bibitem[He \emph{et~al.}(2014)He, Pan, Jin, Xu, Liu, Xu, Shi, Atallah,
  Herbrich, Bowers, \emph{et~al.}]{he2014practical}
He, X., Pan, J., Jin, O., Xu, T., Liu, B., Xu, T., Shi, Y., Atallah, A.,
  Herbrich, R., Bowers, S., \emph{et~al.} (2014).
\newblock Practical lessons from predicting clicks on ads at facebook.
\newblock In \emph{Proceedings of the Eighth International Workshop on Data
  Mining for Online Advertising},  1--9.

\bibitem[Hesterberg(1995)]{hesterberg1995weighted}
Hesterberg, T. (1995).
\newblock Weighted average importance sampling and defensive mixture
  distributions.
\newblock \emph{Technometrics} \textbf{37}, 2, 185--194.

\bibitem[Hjort and Pollard(2011)]{hjort2011asymptotics}
Hjort, N.~L. and Pollard, D. (2011).
\newblock Asymptotics for minimisers of convex processes.
\newblock \emph{arXiv preprint arXiv:1107.3806} .

\bibitem[Huang \emph{et~al.}(2020)Huang, Sharma, Sun, Xia, Zhang, Pronin,
  Padmanabhan, Ottaviano, and Yang]{huang2020embedding}
Huang, J.-T., Sharma, A., Sun, S., Xia, L., Zhang, D., Pronin, P., Padmanabhan,
  J., Ottaviano, G., and Yang, L. (2020).
\newblock Embedding-based retrieval in facebook search.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining},  2553--2561.

\bibitem[Japkowicz(2000)]{Japkowicz2000}
Japkowicz, N. (2000).
\newblock Learning from imbalanced data sets: Papers from the {AAAI} workshop,
  {AAAI, 2000. Technical Report WS-00-05}.

\bibitem[Kilbertus \emph{et~al.}(2018)Kilbertus, Parascandolo, and
  Sch{\"o}lkopf]{kilbertus2018generalization}
Kilbertus, N., Parascandolo, G., and Sch{\"o}lkopf, B. (2018).
\newblock Generalization in anti-causal learning.
\newblock \emph{arXiv preprint arXiv:1812.00524} .

\bibitem[King and Zeng(2001)]{king2001logistic}
King, G. and Zeng, L. (2001).
\newblock Logistic regression in rare events data.
\newblock \emph{Political analysis} \textbf{9}, 2, 137--163.

\bibitem[Lema{\^\i}tre \emph{et~al.}(2017)Lema{\^\i}tre, Nogueira, and
  Aridas]{lemaitre2017imbalanced}
Lema{\^\i}tre, G., Nogueira, F., and Aridas, C.~K. (2017).
\newblock Imbalanced-learn: A python toolbox to tackle the curse of imbalanced
  datasets in machine learning.
\newblock \emph{The Journal of Machine Learning Research} \textbf{18}, 1,
  559--563.

\bibitem[{Liu} \emph{et~al.}(2009){Liu}, {Wu}, and {Zhou}]{Zhou2009Exploratory}
{Liu}, X., {Wu}, J., and {Zhou}, Z. (2009).
\newblock Exploratory undersampling for class-imbalance learning.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part B
  (Cybernetics)} \textbf{39}, 2, 539--550.

\bibitem[Mathew \emph{et~al.}(2017)Mathew, Pang, Luo, and
  Leong]{mathew2017classification}
Mathew, J., Pang, C.~K., Luo, M., and Leong, W.~H. (2017).
\newblock Classification of imbalanced data by oversampling in kernel space of
  support vector machines.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}
  \textbf{29}, 9, 4065--4076.

\bibitem[McMahan \emph{et~al.}(2013)McMahan, Holt, Sculley, Young, Ebner,
  Grady, Nie, Phillips, Davydov, Golovin, \emph{et~al.}]{mcmahan2013ad}
McMahan, H.~B., Holt, G., Sculley, D., Young, M., Ebner, D., Grady, J., Nie,
  L., Phillips, T., Davydov, E., Golovin, D., \emph{et~al.} (2013).
\newblock Ad click prediction: a view from the trenches.
\newblock In \emph{Proceedings of the 19th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining},  1222--1230.

\bibitem[Owen and Zhou(2000)]{owen2000safe}
Owen, A. and Zhou, Y. (2000).
\newblock Safe and effective importance sampling.
\newblock \emph{Journal of the American Statistical Association} \textbf{95},
  449, 135--143.

\bibitem[Owen(2007)]{owen2007infinitely}
Owen, A.~B. (2007).
\newblock Infinitely imbalanced logistic regression.
\newblock \emph{The Journal of Machine Learning Research} \textbf{8}, 761--773.

\bibitem[Pukelsheim(2006)]{pukelsheim2006optimal}
Pukelsheim, F. (2006).
\newblock \emph{Optimal design of experiments}.
\newblock SIAM.

\bibitem[Rahman and Davis(2013)]{rahman2013addressing}
Rahman, M.~M. and Davis, D. (2013).
\newblock Addressing the class imbalance problem in medical datasets.
\newblock \emph{International Journal of Machine Learning and Computing}
  \textbf{3}, 2, 224.

\bibitem[Sch{\"o}lkopf \emph{et~al.}(2012)Sch{\"o}lkopf, Janzing, Peters,
  Sgouritsa, Zhang, and Mooij]{scholkopf2012causal}
Sch{\"o}lkopf, B., Janzing, D., Peters, J., Sgouritsa, E., Zhang, K., and
  Mooij, J. (2012).
\newblock On causal and anticausal learning.
\newblock In \emph{ICML 2012: Proceedings of the 29th International Conference
  on Machine Learning, Edinburgh, Scotland, June 26-July 1, 2012},  1255--1262.

\bibitem[Sun \emph{et~al.}(2007)Sun, Kamel, Wong, and Wang]{sun2007cost}
Sun, Y., Kamel, M.~S., Wong, A.~K., and Wang, Y. (2007).
\newblock Cost-sensitive boosting for classification of imbalanced data.
\newblock \emph{Pattern Recognition} \textbf{40}, 12, 3358--3378.

\bibitem[Ting and Brochu(2018)]{ting2018optimal}
Ting, D. and Brochu, E. (2018).
\newblock Optimal subsampling with influence functions.
\newblock In \emph{Advances in neural information processing systems},
  3650--3659.

\bibitem[van~der Vaart(1998)]{Vaart:98}
van~der Vaart, A. (1998).
\newblock \emph{Asymptotic Statistics}.
\newblock Cambridge University Press, London.

\bibitem[Wang(2019)]{wang2019more}
Wang, H. (2019).
\newblock More efficient estimation for logistic regression with optimal
  subsamples.
\newblock \emph{Journal of Machine Learning Research} \textbf{20}, 132, 1--59.

\bibitem[Wang(2020)]{Wang2020RareICML}
Wang, H. (2020).
\newblock Logistic regression for massive data with rare events.
\newblock In H.~D. III and A.~Singh, eds., \emph{Proceedings of the 37th
  International Conference on Machine Learning}, vol. 119 of \emph{Proceedings
  of Machine Learning Research},  9829--9836. PMLR.

\bibitem[Wang and Ma(2021)]{wang2021optimal}
Wang, H. and Ma, Y. (2021).
\newblock Optimal subsampling for quantile regression in big data.
\newblock \emph{Biometrika} \textbf{108}, 1, 99--112.

\bibitem[Wang \emph{et~al.}(2018)Wang, Zhu, and Ma]{WangZhuMa2018}
Wang, H., Zhu, R., and Ma, P. (2018).
\newblock Optimal subsampling for large sample logistic regression.
\newblock \emph{Journal of the American Statistical Association} \textbf{113},
  522, 829--844.

\bibitem[Xiong and Li(2008)]{xiong2008some}
Xiong, S. and Li, G. (2008).
\newblock Some results on the convergence of conditional distributions.
\newblock \emph{Statistics \& Probability Letters} \textbf{78}, 18, 3249--3253.

\bibitem[Yu \emph{et~al.}(2020)Yu, Wang, Ai, and Zhang]{yu2020quasi}
Yu, J., Wang, H., Ai, M., and Zhang, H. (2020).
\newblock Optimal distributed subsampling for maximum quasi-likelihood
  estimators with massive data.
\newblock \emph{Journal of the American Statistical Association} \textbf{0}, 0,
  1--12.

\end{thebibliography}
