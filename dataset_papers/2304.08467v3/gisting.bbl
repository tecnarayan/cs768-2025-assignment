\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Askell et~al.(2021)Askell, Bai, Chen, Drain, Ganguli, Henighan, Jones,
  Joseph, Mann, DasSarma, et~al.]{askell2021general}
A.~Askell, Y.~Bai, A.~Chen, D.~Drain, D.~Ganguli, T.~Henighan, A.~Jones,
  N.~Joseph, B.~Mann, N.~DasSarma, et~al.
\newblock A general language assistant as a laboratory for alignment.
\newblock \emph{arXiv preprint arXiv:2112.00861}, 2021.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{Beltagy2020Longformer}
I.~Beltagy, M.~E. Peters, and A.~Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv:2004.05150}, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chen(2022)]{chen2022transformer}
C.~Chen.
\newblock Transformer inference arithmetic.
\newblock \url{https://kipp.ly/blog/transformer-inference-arithmetic/}, 2022.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, Stoica, and Xing]{vicuna}
W.-L. Chiang, Z.~Li, Z.~Lin, Y.~Sheng, Z.~Wu, H.~Zhang, L.~Zheng, S.~Zhuang,
  Y.~Zhuang, J.~E. Gonzalez, I.~Stoica, and E.~P. Xing.
\newblock Vicuna: An open-source chatbot impressing {GPT}-4 with 90\% {ChatGPT}
  quality.
\newblock \url{https://vicuna.lmsys.org/}, 2023.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
R.~Child, S.~Gray, A.~Radford, and I.~Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choi et~al.(2022)Choi, Jo, Jang, and Seo]{choi2022prompt}
E.~Choi, Y.~Jo, J.~Jang, and M.~Seo.
\newblock Prompt injection: Parameterization of fixed inputs.
\newblock \emph{arXiv preprint arXiv:2206.11349}, 2022.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, et~al.]{chung2022scaling}
H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, E.~Li, X.~Wang,
  M.~Dehghani, S.~Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Z.~Dai, Z.~Yang, Y.~Yang, J.~G. Carbonell, Q.~Le, and R.~Salakhutdinov.
\newblock Transformer-{XL}: Attentive language models beyond a fixed-length
  context.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 2978--2988, 2019.

\bibitem[Dixon and Massey~Jr(1951)]{dixon1951introduction}
W.~J. Dixon and F.~J. Massey~Jr.
\newblock \emph{Introduction to statistical analysis.}
\newblock McGraw-Hill, 1951.

\bibitem[Geng et~al.(2023)Geng, Gudibande, Liu, Wallace, Abbeel, Levine, and
  Song]{koala_blogpost_2023}
X.~Geng, A.~Gudibande, H.~Liu, E.~Wallace, P.~Abbeel, S.~Levine, and D.~Song.
\newblock Koala: A dialogue model for academic research.
\newblock Blog post, April 2023.
\newblock URL \url{https://bair.berkeley.edu/blog/2023/04/03/koala/}.

\bibitem[Gilardi et~al.(2023)Gilardi, Alizadeh, and Kubli]{gilardi2023chatgpt}
F.~Gilardi, M.~Alizadeh, and M.~Kubli.
\newblock {ChatGPT} outperforms crowd-workers for text-annotation tasks.
\newblock \emph{arXiv preprint arXiv:2303.15056}, 2023.

\bibitem[Ha et~al.(2017)Ha, Dai, and Le]{ha2016hypernetworks}
D.~Ha, A.~Dai, and Q.~V. Le.
\newblock Hyper{N}etworks.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
J.~Hoffmann, S.~Borgeaud, A.~Mensch, E.~Buchatskaya, T.~Cai, E.~Rutherford,
  D.~d.~L. Casas, L.~A. Hendricks, J.~Welbl, A.~Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
N.~Houlsby, A.~Giurgiu, S.~Jastrzebski, B.~Morrone, Q.~De~Laroussilhe,
  A.~Gesmundo, M.~Attariyan, and S.~Gelly.
\newblock Parameter-efficient transfer learning for {NLP}.
\newblock In \emph{International Conference on Machine Learning}, pages
  2790--2799. PMLR, 2019.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{hu2022lora}
E.~J. Hu, Y.~Shen, P.~Wallis, Z.~Allen-Zhu, Y.~Li, S.~Wang, L.~Wang, and
  W.~Chen.
\newblock {LoRA}: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Huang et~al.(2023)Huang, Kwak, and An]{huang2023chatgpt}
F.~Huang, H.~Kwak, and J.~An.
\newblock Is {ChatGPT} better than human annotators? {P}otential and
  limitations of {ChatGPT} in explaining implicit hate speech.
\newblock \emph{arXiv preprint arXiv:2302.07736}, 2023.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
B.~Lester, R.~Al-Rfou, and N.~Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 3045--3059, 2021.

\bibitem[Li and Liang(2021)]{li2021prefix}
X.~L. Li and P.~Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 4582--4597, 2021.

\bibitem[Lin(2004)]{lin2004rouge}
C.-Y. Lin.
\newblock {ROUGE}: A package for automatic evaluation of summaries.
\newblock In \emph{Text summarization branches out}, pages 74--81, 2004.

\bibitem[Liu et~al.(2018)Liu, Saleh, Pot, Goodrich, Sepassi, Kaiser, and
  Shazeer]{liu2018generating}
P.~J. Liu, M.~Saleh, E.~Pot, B.~Goodrich, R.~Sepassi, L.~Kaiser, and
  N.~Shazeer.
\newblock Generating {W}ikipedia by summarizing long sequences.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[OpenAI(2022)]{chatgpt}
OpenAI.
\newblock Introducing {ChatGPT}.
\newblock \url{https://openai.com/blog/chatgpt}, 2022.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
L.~Ouyang, J.~Wu, X.~Jiang, D.~Almeida, C.~Wainwright, P.~Mishkin, C.~Zhang,
  S.~Agarwal, K.~Slama, A.~Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  27730--27744, 2022.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, et~al.
\newblock {P}y{T}orch: An imperative style, high-performance deep learning
  library.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Phang et~al.(2022)Phang, Mao, He, and Chen]{phang2022hypertuning}
J.~Phang, Y.~Mao, P.~He, and W.~Chen.
\newblock Hyper{T}uning: Toward adapting large language models without
  back-propagation.
\newblock \emph{arXiv preprint arXiv:2211.12485}, 2022.

\bibitem[Pope et~al.(2022)Pope, Douglas, Chowdhery, Devlin, Bradbury, Levskaya,
  Heek, Xiao, Agrawal, and Dean]{pope2022efficiently}
R.~Pope, S.~Douglas, A.~Chowdhery, J.~Devlin, J.~Bradbury, A.~Levskaya,
  J.~Heek, K.~Xiao, S.~Agrawal, and J.~Dean.
\newblock Efficiently scaling transformer inference.
\newblock \emph{arXiv preprint arXiv:2211.05102}, 2022.

\bibitem[Rae et~al.(2020)Rae, Potapenko, Jayakumar, Hillier, and
  Lillicrap]{rae2020compressive}
J.~W. Rae, A.~Potapenko, S.~M. Jayakumar, C.~Hillier, and T.~P. Lillicrap.
\newblock Compressive transformers for long-range sequence modelling.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
C.~Raffel, N.~Shazeer, A.~Roberts, K.~Lee, S.~Narang, M.~Matena, Y.~Zhou,
  W.~Li, and P.~J. Liu.
\newblock Exploring the limits of transfer learning with a unified
  {T}ext-to-{T}ext {T}ransformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 5485--5551, 2020.

\bibitem[Rasley et~al.(2020)Rasley, Rajbhandari, Ruwase, and
  He]{rasley2020deepspeed}
J.~Rasley, S.~Rajbhandari, O.~Ruwase, and Y.~He.
\newblock Deep{S}peed: System optimizations enable training deep learning
  models with over 100 billion parameters.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 3505--3506, 2020.

\bibitem[Snell et~al.(2022)Snell, Klein, and Zhong]{snell2022learning}
C.~Snell, D.~Klein, and R.~Zhong.
\newblock Learning by distilling context.
\newblock \emph{arXiv preprint arXiv:2209.15189}, 2022.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto]{alpaca}
R.~Taori, I.~Gulrajani, T.~Zhang, Y.~Dubois, X.~Li, C.~Guestrin, P.~Liang, and
  T.~B. Hashimoto.
\newblock Stanford {A}lpaca: An instruction-following {LLaMA} model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Bahri, and Metzler]{tay2022efficient}
Y.~Tay, M.~Dehghani, D.~Bahri, and D.~Metzler.
\newblock Efficient transformers: A survey.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (6):\penalty0 1--28, 2022.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix,
  B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, et~al.
\newblock {LLaMA}: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Wang et~al.(2023)Wang, Liang, Meng, Shi, Li, Xu, Qu, and
  Zhou]{wang2023chatgpt}
J.~Wang, Y.~Liang, F.~Meng, H.~Shi, Z.~Li, J.~Xu, J.~Qu, and J.~Zhou.
\newblock Is {ChatGPT} a good {NLG} evaluator? {A} preliminary study.
\newblock \emph{arXiv preprint arXiv:2303.04048}, 2023.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Kordi, Mishra, Liu, Smith,
  Khashabi, and Hajishirzi]{wang2022self}
Y.~Wang, Y.~Kordi, S.~Mishra, A.~Liu, N.~A. Smith, D.~Khashabi, and
  H.~Hajishirzi.
\newblock Self-{I}nstruct: Aligning language model with self generated
  instructions.
\newblock \emph{arXiv preprint arXiv:2212.10560}, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Mishra, Alipoormolabashi, Kordi,
  Mirzaei, Naik, Ashok, Dhanasekaran, Arunkumar, Stap, et~al.]{wang2022super}
Y.~Wang, S.~Mishra, P.~Alipoormolabashi, Y.~Kordi, A.~Mirzaei, A.~Naik,
  A.~Ashok, A.~S. Dhanasekaran, A.~Arunkumar, D.~Stap, et~al.
\newblock {S}uper-{N}atural{I}nstructions: Generalization via declarative
  instructions on 1600+ {NLP} tasks.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 5085--5109, 2022{\natexlab{b}}.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du,
  Dai, and Le]{wei2022finetuned}
J.~Wei, M.~Bosma, V.~Zhao, K.~Guu, A.~W. Yu, B.~Lester, N.~Du, A.~M. Dai, and
  Q.~V. Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, Xia, Chi,
  Le, Zhou, et~al.]{wei2022chain}
J.~Wei, X.~Wang, D.~Schuurmans, M.~Bosma, F.~Xia, E.~H. Chi, Q.~V. Le, D.~Zhou,
  et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.

\bibitem[Wingate et~al.(2022)Wingate, Shoeybi, and Sorensen]{wingate2022prompt}
D.~Wingate, M.~Shoeybi, and T.~Sorensen.
\newblock Prompt compression and contrastive conditioning for controllability
  and toxicity reduction in language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pages 5621--5634, Abu Dhabi, United Arab Emirates, Dec. 2022.
  Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.findings-emnlp.412}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Le~Scao, Gugger, Drame, Lhoest, and Rush]{wolf2020transformers}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz, J.~Davison, S.~Shleifer, P.~von Platen,
  C.~Ma, Y.~Jernite, J.~Plu, C.~Xu, T.~Le~Scao, S.~Gugger, M.~Drame, Q.~Lhoest,
  and A.~Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online,
  Oct. 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-demos.6}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-demos.6}.

\bibitem[Wu et~al.(2022)Wu, Rabe, Hutchins, and Szegedy]{wu2022memorizing}
Y.~Wu, M.~N. Rabe, D.~Hutchins, and C.~Szegedy.
\newblock Memorizing transformers.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Zhang et~al.(2021)Zhang, Gong, Shen, Li, Lv, Duan, and
  Chen]{zhang2021poolingformer}
H.~Zhang, Y.~Gong, Y.~Shen, W.~Li, J.~Lv, N.~Duan, and W.~Chen.
\newblock Poolingformer: Long document modeling with pooling attention.
\newblock In \emph{International Conference on Machine Learning}, pages
  12437--12446. PMLR, 2021.

\end{thebibliography}
