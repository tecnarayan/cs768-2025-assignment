\begin{thebibliography}{76}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Argenson \& Dulac-Arnold(2021)Argenson and
  Dulac-Arnold]{argenson2020model}
Argenson, A. and Dulac-Arnold, G.
\newblock Model-based offline planning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021.

\bibitem[Banach(1922)]{banach1922operations}
Banach, S.
\newblock Sur les op{\'e}rations dans les ensembles abstraits et leur
  application aux {\'e}quations int{\'e}grales.
\newblock \emph{Fund. math}, 1922.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{Bellemare2016UnifyingCE}
Bellemare, M.~G., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2016.

\bibitem[Bellman(1957)]{bellman}
Bellman, R.
\newblock A markovian decision process.
\newblock \emph{Indiana University Mathematics Journal}, 1957.

\bibitem[Bertsekas \& Tsitsiklis(1996)Bertsekas and
  Tsitsiklis]{Bertsekas1996NeuroDynamicP}
Bertsekas, D. and Tsitsiklis, J.
\newblock Neuro-dynamic programming.
\newblock 1996.

\bibitem[Bertsekas \& Tsitsiklis(1991)Bertsekas and
  Tsitsiklis]{bertsekas1991some}
Bertsekas, D.~P. and Tsitsiklis, J.~N.
\newblock Some aspects of parallel and distributed iterative algorithms-a
  survey.
\newblock \emph{Automatica}, 1991.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and Zhang]{jax}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Bromley et~al.(1994)Bromley, Guyon, LeCun, S{\"a}ckinger, and
  Shah]{bromley1994signature}
Bromley, J., Guyon, I., LeCun, Y., S{\"a}ckinger, E., and Shah, R.
\newblock Signature verification using a" siamese" time delay neural network.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 1994.

\bibitem[Buckman et~al.(2021)Buckman, Gelada, and
  Bellemare]{buckman2020importance}
Buckman, J., Gelada, C., and Bellemare, M.~G.
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021.

\bibitem[Castro(2020)]{castro2020scalable}
Castro, P.~S.
\newblock Scalable methods for computing state similarity in deterministic
  markov decision processes.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2020.

\bibitem[Comanici et~al.(2012)Comanici, Panangaden, and
  Precup]{comanici2012fly}
Comanici, G., Panangaden, P., and Precup, D.
\newblock On-the-fly algorithms for bisimulation metrics.
\newblock In \emph{International Conference on Quantitative Evaluation of
  Systems}, 2012.

\bibitem[Dadashi et~al.(2021)Dadashi, Hussenot, Geist, and Pietquin]{pwil}
Dadashi, R., Hussenot, L., Geist, M., and Pietquin, O.
\newblock Primal wasserstein imitation learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021.

\bibitem[Dean \& Givan(1997)Dean and Givan]{dean1997model}
Dean, T. and Givan, R.
\newblock Model minimization in markov decision processes.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 1997.

\bibitem[Dulac{-}Arnold et~al.(2019)Dulac{-}Arnold, Mankowitz, and
  Hester]{rlchallenges}
Dulac{-}Arnold, G., Mankowitz, D.~J., and Hester, T.
\newblock Challenges of real-world reinforcement learning.
\newblock \emph{CoRR}, abs/1904.12901, 2019.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{ernst2005tree}
Ernst, D., Geurts, P., and Wehenkel, L.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 2005.

\bibitem[Ferns \& Precup(2014)Ferns and Precup]{ferns2014bisimulation}
Ferns, N. and Precup, D.
\newblock Bisimulation metrics are optimal value functions.
\newblock In \emph{Uncertainty in Artificial Intelligence (UAI)}, 2014.

\bibitem[Ferns et~al.(2004)Ferns, Panangaden, and Precup]{ferns2004metrics}
Ferns, N., Panangaden, P., and Precup, D.
\newblock Metrics for finite markov decision processes.
\newblock In \emph{Uncertainty in Artificial Intelligence (UAI)}, 2004.

\bibitem[Ferns et~al.(2006)Ferns, Castro, Precup, and
  Panangaden]{ferns2012methods}
Ferns, N., Castro, P.~S., Precup, D., and Panangaden, P.
\newblock Methods for computing state similarity in markov decision processes.
\newblock \emph{Uncertainty in Artificial Intelligence (UAI)}, 2006.

\bibitem[Ferns et~al.(2011)Ferns, Panangaden, and
  Precup]{ferns2011bisimulation}
Ferns, N., Panangaden, P., and Precup, D.
\newblock Bisimulation metrics for continuous markov decision processes.
\newblock \emph{SIAM Journal on Computing}, 2011.

\bibitem[Friedman et~al.(1977)Friedman, Bentley, and Finkel]{kd_tree}
Friedman, J., Bentley, J., and Finkel, R.
\newblock An algorithm for finding best matches in logarithmic expected time.
\newblock \emph{ACM Trans. Math. Softw.}, 1977.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and
  Meger]{pmlr-v80-fujimoto18a}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Gelada et~al.(2019)Gelada, Kumar, Buckman, Nachum, and
  Bellemare]{gelada2019deepmdp}
Gelada, C., Kumar, S., Buckman, J., Nachum, O., and Bellemare, M.~G.
\newblock Deepmdp: Learning continuous latent space models for representation
  learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Givan et~al.(2003)Givan, Dean, and Greig]{givan2003equivalence}
Givan, R., Dean, T., and Greig, M.
\newblock Equivalence notions and model minimization in markov decision
  processes.
\newblock \emph{Artificial Intelligence}, 2003.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{pmlr-v80-haarnoja18b}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Henderson et~al.(2018)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{henderson}
Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D.
\newblock Deep reinforcement learning that matters.
\newblock \emph{AAAI Conference on Artificial Intelligence}, 2018.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2019way}
Jaques, N., Ghandeharioun, A., Shen, J.~H., Ferguson, C., Lapedriza, A., Jones,
  N., Gu, S., and Picard, R.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \emph{arXiv preprint arXiv:1907.00456}, 2019.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T.
\newblock Morel: Model-based offline reinforcement learning.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Kim \& Park(2018)Kim and Park]{gmmil}
Kim, K.-E. and Park, H.
\newblock Imitation learning via kernel mean embedding.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2018.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{DBLP:journals/corr/KingmaB14}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{Kingma2014AutoEncodingVB}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{CoRR}, abs/1312.6114, 2014.

\bibitem[Konda \& Tsitsiklis(2000)Konda and Tsitsiklis]{konda}
Konda, V. and Tsitsiklis, J.
\newblock Actor-critic algorithms.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2000.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Tucker, and Levine]{kumar2019stabilizing}
Kumar, A., Fu, J., Tucker, G., and Levine, S.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Lagoudakis \& Parr(2003)Lagoudakis and Parr]{lagoudakis2003least}
Lagoudakis, M.~G. and Parr, R.
\newblock Least-squares policy iteration.
\newblock \emph{Journal of Machine Learning Research}, 2003.

\bibitem[Lange et~al.()Lange, Gabel, and Riedmiller]{lange2012batch}
Lange, S., Gabel, T., and Riedmiller, M.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning}.

\bibitem[Laroche et~al.(2019)Laroche, Trichelair, and
  Des~Combes]{laroche2019safe}
Laroche, R., Trichelair, P., and Des~Combes, R.~T.
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[{Le Lan} et~al.(2021){Le Lan}, Bellemare, and
  Castro]{Lan2021MetricsAC}
{Le Lan}, C., Bellemare, M.~G., and Castro, P.~S.
\newblock Metrics and continuity in reinforcement learning.
\newblock \emph{AAAI Conference on Artificial Intelligence}, 2021.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Li et~al.(2006)Li, Walsh, and Littman]{li2006towards}
Li, L., Walsh, T.~J., and Littman, M.~L.
\newblock Towards a unified theory of state abstraction for mdps.
\newblock \emph{International Symposium on Artificial Intelligence and
  Mathematics (ISAIM)}, 2006.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2019continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Lin(1992)]{experiencereplay}
Lin, L.~J.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine Learning}, 1992.

\bibitem[Mahadevan \& Maggioni(2007)Mahadevan and
  Maggioni]{protovaluefunctions}
Mahadevan, S. and Maggioni, M.
\newblock Proto-value functions: A laplacian framework for learning
  representation and control in markov decision processes.
\newblock \emph{Journal of Machine Learning Research}, 2007.

\bibitem[Melo \& Lopes(2010)Melo and Lopes]{melo}
Melo, F.~S. and Lopes, M.
\newblock Learning from demonstration using mdp induced metrics.
\newblock In \emph{European Conference on Machine Learning (ECML)}, 2010.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and
  Hassabis]{Mnih2015HumanlevelCT}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M.~A., Fidjeland, A.~K., Ostrovski, G.,
  Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D.,
  Wierstra, D., Legg, S., and Hassabis, D.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 2015.

\bibitem[Munos \& Szepesvari(2008)Munos and Szepesvari]{Munos2008FiniteTimeBF}
Munos, R. and Szepesvari, C.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 2008.

\bibitem[Nadjahi et~al.(2019)Nadjahi, Laroche, and des Combes]{nadjahi2019safe}
Nadjahi, K., Laroche, R., and des Combes, R.~T.
\newblock Safe policy improvement with soft baseline bootstrapping.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, 2019.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Louppe, Prettenhofer, Weiss, Weiss, VanderPlas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{sklearn}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
  O., Blondel, M., Louppe, G., Prettenhofer, P., Weiss, R., Weiss, R.~J.,
  VanderPlas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and
  Duchesnay, E.
\newblock Scikit-learn: Machine learning in python.
\newblock \emph{Journal of Machine Learning Research}, 2011.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{peng2019advantage}
Peng, X.~B., Kumar, A., Zhang, G., and Levine, S.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.00177}, 2019.

\bibitem[Pietquin et~al.(2011)Pietquin, Geist, Chandramohan, and
  Frezza-Buet]{pietquin2011sample}
Pietquin, O., Geist, M., Chandramohan, S., and Frezza-Buet, H.
\newblock Sample-efficient batch reinforcement learning for dialogue management
  optimization.
\newblock \emph{ACM Transactions on Speech and Language Processing (TSLP)},
  2011.

\bibitem[Pomerleau(1991)]{bc}
Pomerleau, D.~A.
\newblock Efficient training of artificial neural networks for autonomous
  navigation.
\newblock \emph{Neural computation}, 1991.

\bibitem[Rajeswaran et~al.(2018)Rajeswaran, Kumar, Gupta, Vezzani, Schulman,
  Todorov, and Levine]{Rajeswaran-RSS-18}
Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E.,
  and Levine, S.
\newblock {Learning Complex Dexterous Manipulation with Deep Reinforcement
  Learning and Demonstrations}.
\newblock In \emph{Robotics: Science and Systems (RSS)}, 2018.

\bibitem[Ravindran \& Barto(2003)Ravindran and Barto]{ravindran2003relativized}
Ravindran, B. and Barto, A.~G.
\newblock Relativized options: Choosing the right transformation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2003.

\bibitem[Ravindran \& Barto(2004)Ravindran and Barto]{ravindran2004approximate}
Ravindran, B. and Barto, A.~G.
\newblock Approximate homomorphisms: A framework for non-exact minimization in
  markov decision processes.
\newblock 2004.

\bibitem[Riedmiller(2005)]{riedmiller2005neural}
Riedmiller, M.
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In \emph{European Conference on Machine Learning}, 2005.

\bibitem[Schaul et~al.(2015)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2015.

\bibitem[Schmidhuber(1991)]{jurgen}
Schmidhuber, J.
\newblock A possibility for implementing curiosity and boredom in
  model-building neural controllers.
\newblock 1991.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{Schulman2015TrustRP}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2015.

\bibitem[Siegel et~al.(2020)Siegel, Springenberg, Berkenkamp, Abdolmaleki,
  Neunert, Lampe, Hafner, and Riedmiller]{siegel2020keep}
Siegel, N.~Y., Springenberg, J.~T., Berkenkamp, F., Abdolmaleki, A., Neunert,
  M., Lampe, T., Hafner, R., and Riedmiller, M.
\newblock Keep doing what worked: Behavioral modelling priors for offline
  reinforcement learning.
\newblock \emph{Internation Conference on Learning Representations}, 2020.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silver2014deterministic}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2014.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, van~den
  Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman,
  Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel,
  and Hassabis]{go}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., van~den Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I.,
  Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 2016.

\bibitem[Sim{\~a}o et~al.(2020)Sim{\~a}o, Laroche, and Combes]{simao2019safe}
Sim{\~a}o, T.~D., Laroche, R., and Combes, R. T.~d.
\newblock Safe policy improvement with an estimated baseline policy.
\newblock \emph{International Conference on Autonomous Agents and Multi-Agent
  Systems (AAMAS)}, 2020.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{suttonbarto}
Sutton, R. and Barto, A.
\newblock Introduction to reinforcement learning.
\newblock 1998.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{Sutton1999PolicyGM}
Sutton, R., McAllester, D.~A., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 1999.

\bibitem[Taylor et~al.(2008)Taylor, Precup, and Panagaden]{taylor2008bounding}
Taylor, J., Precup, D., and Panagaden, P.
\newblock Bounding performance loss in approximate mdp homomorphisms.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2008.

\bibitem[Thrun(1992)]{thrun}
Thrun, S.
\newblock Efficient exploration in reinforcement learning.
\newblock 1992.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock \emph{International Conference on Intelligent Robots and Systems},
  2012.

\bibitem[van~der Pol et~al.(2020)van~der Pol, Kipf, Oliehoek, and
  Welling]{van2020plannable}
van~der Pol, E., Kipf, T., Oliehoek, F.~A., and Welling, M.
\newblock Plannable approximations to mdp homomorphisms: Equivariance under
  actions.
\newblock \emph{International Conference on Autonomous Agents and Multi-Agent
  Systems (AAMAS)}, 2020.

\bibitem[Villani(2008)]{villani}
Villani, C.
\newblock Optimal transport: Old and new.
\newblock 2008.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, Oh, Horgan, Kroiss, Danihelka, Huang,
  Sifre, Cai, Agapiou, Jaderberg, Vezhnevets, Leblond, Pohlen, Dalibard,
  Budden, Sulsky, Molloy, Paine, G{\"{u}}l{\c{c}}ehre, Wang, Pfaff, Wu, Ring,
  Yogatama, W{\"{u}}nsch, McKinney, Smith, Schaul, Lillicrap, Kavukcuoglu,
  Hassabis, Apps, and Silver]{starcraft}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., Oh, J., Horgan, D.,
  Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou, J.~P.,
  Jaderberg, M., Vezhnevets, A.~S., Leblond, R., Pohlen, T., Dalibard, V.,
  Budden, D., Sulsky, Y., Molloy, J., Paine, T.~L., G{\"{u}}l{\c{c}}ehre,
  {\c{C}}., Wang, Z., Pfaff, T., Wu, Y., Ring, R., Yogatama, D., W{\"{u}}nsch,
  D., McKinney, K., Smith, O., Schaul, T., Lillicrap, T.~P., Kavukcuoglu, K.,
  Hassabis, D., Apps, C., and Silver, D.
\newblock Grandmaster level in starcraft {II} using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 2019.

\bibitem[Wolfe \& Barto(2006)Wolfe and Barto]{wolfe2006decision}
Wolfe, A.~P. and Barto, A.~G.
\newblock Decision tree methods for finding reusable mdp homomorphisms.
\newblock In \emph{The National Conference on Artificial Intelligence}, 2006.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine, S., Finn, C., and Ma,
  T.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Zhang et~al.(2021)Zhang, McAllister, Calandra, Gal, and
  Levine]{zhang2020learning}
Zhang, A., McAllister, R., Calandra, R., Gal, Y., and Levine, S.
\newblock Learning invariant representations for reinforcement learning without
  reconstruction.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021.

\end{thebibliography}
