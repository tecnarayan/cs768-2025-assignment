\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2018)Achiam, Edwards, Amodei, and
  Abbeel]{Achiam2018VariationalOD}
Achiam, J., Edwards, H., Amodei, D., and Abbeel, P.
\newblock Variational option discovery algorithms.
\newblock \emph{ArXiv}, abs/1807.10299, 2018.

\bibitem[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn,
  Gopalakrishnan, Hausman, Herzog, Ho, Hsu, Ibarz, Ichter, Irpan, Jang, Ruano,
  Jeffrey, Jesmonth, Joshi, Julian, Kalashnikov, Kuang, Lee, Levine, Lu, Luu,
  Parada, Pastor, Quiambao, Rao, Rettinghouse, Reyes, Sermanet, Sievers, Tan,
  Toshev, Vanhoucke, Xia, Xiao, Xu, Xu, and Yan]{saycan2022arxiv}
Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C.,
  Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu, J., Ibarz, J.,
  Ichter, B., Irpan, A., Jang, E., Ruano, R.~J., Jeffrey, K., Jesmonth, S.,
  Joshi, N., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S.,
  Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse,
  J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V.,
  Xia, F., Xiao, T., Xu, P., Xu, S., and Yan, M.
\newblock Do as i can and not as i say: Grounding language in robotic
  affordances.
\newblock In \emph{arXiv preprint arXiv:2204.01691}, 2022.

\bibitem[Andreas et~al.(2017)Andreas, Klein, and Levine]{andreas2017modular}
Andreas, J., Klein, D., and Levine, S.
\newblock Modular multitask reinforcement learning with policy sketches.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  166--175. PMLR, 2017.

\bibitem[Basu et~al.(2018)Basu, Singhal, and Dragan]{basu2018learning}
Basu, C., Singhal, M., and Dragan, A.~D.
\newblock Learning from richer human guidance: Augmenting comparison-based
  learning with feature queries.
\newblock In \emph{Proceedings of the 2018 ACM/IEEE International Conference on
  Human-Robot Interaction}, pp.\  132--140, 2018.

\bibitem[Chakraborti et~al.(2019)Chakraborti, Sreedharan, Grover, and
  Kambhampati]{chakraborti2019plan}
Chakraborti, T., Sreedharan, S., Grover, S., and Kambhampati, S.
\newblock Plan explanations as model reconciliation--an empirical study.
\newblock In \emph{2019 14th ACM/IEEE International Conference on Human-Robot
  Interaction (HRI)}, pp.\  258--266. IEEE, 2019.

\bibitem[Chevalier-Boisvert et~al.(2018)Chevalier-Boisvert, Willems, and
  Pal]{gym_minigrid}
Chevalier-Boisvert, M., Willems, L., and Pal, S.
\newblock Minimalistic gridworld environment for openai gym.
\newblock \url{https://github.com/maximecb/gym-minigrid}, 2018.

\bibitem[De~Giacomo et~al.(2019)De~Giacomo, Iocchi, Favorito, and
  Patrizi]{de2019foundations}
De~Giacomo, G., Iocchi, L., Favorito, M., and Patrizi, F.
\newblock Foundations for restraining bolts: Reinforcement learning with
  ltlf/ldlf restraining specifications.
\newblock In \emph{Proceedings of the international conference on automated
  planning and scheduling}, volume~29, pp.\  128--136, 2019.

\bibitem[Eysenbach et~al.(2019)Eysenbach, Gupta, Ibarz, and
  Levine]{Eysenbach2019DiversityIA}
Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock \emph{ArXiv}, abs/1802.06070, 2019.

\bibitem[Feng et~al.(2018)Feng, Zhuo, and
  Kambhampati]{DBLP:conf/ijcai/FengZK18}
Feng, W., Zhuo, H.~H., and Kambhampati, S.
\newblock Extracting action sequences from texts based on deep reinforcement
  learning.
\newblock In Lang, J. (ed.), \emph{Proceedings of the Twenty-Seventh
  International Joint Conference on Artificial Intelligence, {IJCAI} 2018, July
  13-19, 2018, Stockholm, Sweden}, pp.\  4064--4070. ijcai.org, 2018.
\newblock \doi{10.24963/ijcai.2018/565}.
\newblock URL \url{https://doi.org/10.24963/ijcai.2018/565}.

\bibitem[Fikes \& Nilsson(1971)Fikes and Nilsson]{Fikes71STRIPS}
Fikes, R. and Nilsson, N.~J.
\newblock {STRIPS:} {A} new approach to the application of theorem proving to
  problem solving.
\newblock \emph{Artif. Intell.}, 2\penalty0 (3/4):\penalty0 189--208, 1971.

\bibitem[Florensa et~al.(2017)Florensa, Duan, and
  Abbeel]{florensa2017stochastic}
Florensa, C., Duan, Y., and Abbeel, P.
\newblock Stochastic neural networks for hierarchical reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1704.03012}, 2017.

\bibitem[Geffner \& Bonet(2013)Geffner and Bonet]{geffner2013concise}
Geffner, H. and Bonet, B.
\newblock A concise introduction to models and methods for automated planning.
\newblock \emph{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}, 8\penalty0 (1):\penalty0 1--141, 2013.

\bibitem[Goyal et~al.(2019)Goyal, Niekum, and Mooney]{goyal2019using}
Goyal, P., Niekum, S., and Mooney, R.~J.
\newblock Using natural language for reward shaping in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1903.02020}, 2019.

\bibitem[Grzes \& Kudenko(2008)Grzes and Kudenko]{grzes2008plan}
Grzes, M. and Kudenko, D.
\newblock Plan-based reward shaping for reinforcement learning.
\newblock In \emph{2008 4th International IEEE Conference Intelligent Systems},
  volume~2, pp.\  10--22. IEEE, 2008.

\bibitem[Guan et~al.(2021)Guan, Verma, Guo, Zhang, and
  Kambhampati]{guan2021widening}
Guan, L., Verma, M., Guo, S., Zhang, R., and Kambhampati, S.
\newblock Widening the pipeline in human-guided reinforcement learning with
  explanation and context-aware data augmentation.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1352--1361. PMLR, 2017.

\bibitem[Hadfield{-}Menell et~al.(2017)Hadfield{-}Menell, Milli, Abbeel,
  Russell, and Dragan]{Menell17Invers}
Hadfield{-}Menell, D., Milli, S., Abbeel, P., Russell, S.~J., and Dragan, A.~D.
\newblock Inverse reward design.
\newblock In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H.~M., Fergus,
  R., Vishwanathan, S. V.~N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30: Annual Conference on Neural Information
  Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pp.\
  6765--6774, 2017.

\bibitem[Hernandez et~al.(2021)Hernandez, Sreedharan, and
  Kambhampati]{gpt3-to-plan}
Hernandez, A.~O., Sreedharan, S., and Kambhampati, S.
\newblock Gpt3-to-plan: Extracting plans from text using {GPT-3}.
\newblock \emph{CoRR}, abs/2106.07131, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.07131}.

\bibitem[Icarte et~al.(2018)Icarte, Klassen, Valenzano, and
  McIlraith]{icarte18aUsing}
Icarte, R.~T., Klassen, T., Valenzano, R., and McIlraith, S.
\newblock Using reward machines for high-level task specification and
  decomposition in reinforcement learning.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  2107--2116. PMLR, 10--15 Jul 2018.

\bibitem[Illanes et~al.(2020)Illanes, Yan, Icarte, and
  McIlraith]{IllanesYIM20Symbolic}
Illanes, L., Yan, X., Icarte, R.~T., and McIlraith, S.~A.
\newblock Symbolic plans as high-level instructions for reinforcement learning.
\newblock In \emph{Proceedings of the Thirtieth International Conference on
  Automated Planning and Scheduling, Nancy, France, October 26-30, 2020}, pp.\
  540--550. {AAAI} Press, 2020.

\bibitem[Jothimurugan et~al.(2021)Jothimurugan, Bansal, Bastani, and
  Alur]{jothimurugan2021compositional}
Jothimurugan, K., Bansal, S., Bastani, O., and Alur, R.
\newblock Compositional reinforcement learning from logical specifications.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 10026--10039, 2021.

\bibitem[Kambhampati(2007)]{rao-model-lite}
Kambhampati, S.
\newblock Model-lite planning for the web age masses: The challenges of
  planning with incomplete and evolving domain models.
\newblock In \emph{Proceedings of the Twenty-Second {AAAI} Conference on
  Artificial Intelligence, July 22-26, 2007, Vancouver, British Columbia,
  Canada}, pp.\  1601--1605. {AAAI} Press, 2007.

\bibitem[Kambhampati et~al.(1996)Kambhampati, Ihrig, and
  Srivastava]{kambhampati1996candidate}
Kambhampati, S., Ihrig, L.~H., and Srivastava, B.
\newblock A candidate set based analysis of subgoal interactions in conjunctive
  goal planning.
\newblock In \emph{AIPS}, pp.\  125--133, 1996.

\bibitem[Kambhampati et~al.(2022)Kambhampati, Sreedharan, Verma, Zha, and
  Guan]{kambhampati2022symbols}
Kambhampati, S., Sreedharan, S., Verma, M., Zha, Y., and Guan, L.
\newblock Symbols as a lingua franca for bridging human-ai chasm for
  explainable and advisable ai systems.
\newblock In \emph{AAAI}, 2022.

\bibitem[Keyder et~al.(2010)Keyder, Richter, and Helmert]{KeyderRH10Sound}
Keyder, E., Richter, S., and Helmert, M.
\newblock Sound and complete landmarks for and/or graphs.
\newblock In Coelho, H., Studer, R., and Wooldridge, M.~J. (eds.), \emph{{ECAI}
  2010 - 19th European Conference on Artificial Intelligence, Lisbon, Portugal,
  August 16-20, 2010, Proceedings}, volume 215 of \emph{Frontiers in Artificial
  Intelligence and Applications}, pp.\  335--340. {IOS} Press, 2010.

\bibitem[Kokel et~al.(2021)Kokel, Manoharan, Natarajan, Ravindran, and
  Tadepalli]{kokel2021reprel}
Kokel, H., Manoharan, A., Natarajan, S., Ravindran, B., and Tadepalli, P.
\newblock Reprel: Integrating relational planning and reinforcement learning
  for effective abstraction.
\newblock In \emph{Proceedings of the International Conference on Automated
  Planning and Scheduling}, volume~31, pp.\  533--541, 2021.

\bibitem[Korf(1987)]{Korf-serializability}
Korf, R.~E.
\newblock Planning as search: A quantitative approach.
\newblock \emph{Artif. Intell.}, 33:\penalty0 65--88, 1987.

\bibitem[Kulkarni et~al.(2016)Kulkarni, Narasimhan, Saeedi, and
  Tenenbaum]{kulkarni2016hierarchical}
Kulkarni, T.~D., Narasimhan, K., Saeedi, A., and Tenenbaum, J.
\newblock Hierarchical deep reinforcement learning: Integrating temporal
  abstraction and intrinsic motivation.
\newblock \emph{Advances in neural information processing systems},
  29:\penalty0 3675--3683, 2016.

\bibitem[Kumar et~al.(2020)Kumar, Kumar, Levine, and Finn]{Kumar2020OneSI}
Kumar, S., Kumar, A., Levine, S., and Finn, C.
\newblock One solution is not all you need: Few-shot extrapolation via
  structured maxent rl.
\newblock \emph{ArXiv}, abs/2010.14484, 2020.

\bibitem[Lee et~al.(2019)Lee, Eysenbach, Parisotto, Xing, Levine, and
  Salakhutdinov]{lee2019efficient}
Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., and Salakhutdinov,
  R.
\newblock Efficient exploration via state marginal matching.
\newblock \emph{arXiv preprint arXiv:1906.05274}, 2019.

\bibitem[Li et~al.(2006)Li, Walsh, and Littman]{LiWL06Towards}
Li, L., Walsh, T.~J., and Littman, M.~L.
\newblock Towards a unified theory of state abstraction for mdps.
\newblock In \emph{International Symposium on Artificial Intelligence and
  Mathematics, {ISAIM} 2006, Fort Lauderdale, Florida, USA, January 4-6, 2006},
  2006.

\bibitem[Lyu et~al.(2019)Lyu, Yang, Liu, and Gustafson]{lyu2019sdrl}
Lyu, D., Yang, F., Liu, B., and Gustafson, S.
\newblock Sdrl: interpretable and data-efficient deep reinforcement learning
  leveraging symbolic planning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  2970--2977, 2019.

\bibitem[Marthi et~al.(2007)Marthi, Russell, and Wolfe]{MarthiRW07Angelic}
Marthi, B., Russell, S.~J., and Wolfe, J.~A.
\newblock Angelic semantics for high-level actions.
\newblock In Boddy, M.~S., Fox, M., and Thi{\'{e}}baux, S. (eds.),
  \emph{Proceedings of the Seventeenth International Conference on Automated
  Planning and Scheduling, {ICAPS} 2007, Providence, Rhode Island, USA,
  September 22-26, 2007}, pp.\  232--239. {AAAI}, 2007.

\bibitem[Miller(2019)]{miller2019explanation}
Miller, T.
\newblock Explanation in artificial intelligence: Insights from the social
  sciences.
\newblock \emph{Artificial intelligence}, 267:\penalty0 1--38, 2019.

\bibitem[Myers(1996)]{myers-advisable}
Myers, K.~L.
\newblock Advisable planning systems.
\newblock \emph{Advanced Planning Technology}, 535:\penalty0 206--209, 1996.

\bibitem[Nguyen et~al.(2017)Nguyen, Sreedharan, and
  Kambhampati]{nguyen2017robust}
Nguyen, T., Sreedharan, S., and Kambhampati, S.
\newblock Robust planning with incomplete domain models.
\newblock \emph{Artificial Intelligence}, 245:\penalty0 134--161, 2017.

\bibitem[Nguyen et~al.(2012)Nguyen, Do, Gerevini, Serina, Srivastava, and
  Kambhampati]{tuan-diverse}
Nguyen, T.~A., Do, M.~B., Gerevini, A., Serina, I., Srivastava, B., and
  Kambhampati, S.
\newblock Generating diverse plans to handle unknown and partially known user
  preferences.
\newblock \emph{Artif. Intell.}, 190:\penalty0 1--31, 2012.

\bibitem[Oh et~al.(2018)Oh, Guo, Singh, and Lee]{oh2018self}
Oh, J., Guo, Y., Singh, S., and Lee, H.
\newblock Self-imitation learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3878--3887. PMLR, 2018.

\bibitem[Sreedharan et~al.(2020)Sreedharan, Soni, Verma, Srivastava, and
  Kambhampati]{Sreedharan20Bridge}
Sreedharan, S., Soni, U., Verma, M., Srivastava, S., and Kambhampati, S.
\newblock Bridging the gap: Providing post-hoc symbolic explanations for
  sequential decision-making problems with black box simulators.
\newblock \emph{CoRR}, abs/2002.01080, 2020.

\bibitem[Srivastava et~al.(2016)Srivastava, Russell, and
  Pinto]{SrivastavaRP16Meta}
Srivastava, S., Russell, S.~J., and Pinto, A.
\newblock Metaphysics of planning domain descriptions.
\newblock In Schuurmans, D. and Wellman, M.~P. (eds.), \emph{Proceedings of the
  Thirtieth {AAAI} Conference on Artificial Intelligence, February 12-17, 2016,
  Phoenix, Arizona, {USA}}, pp.\  1074--1080. {AAAI} Press, 2016.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton1999between}
Sutton, R.~S., Precup, D., and Singh, S.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artificial intelligence}, 112\penalty0 (1-2):\penalty0
  181--211, 1999.

\bibitem[Tversky \& Kahneman(1993)Tversky and
  Kahneman]{tversky1993probabilistic}
Tversky, A. and Kahneman, D.
\newblock Probabilistic reasoning.
\newblock \emph{Readings in philosophy and cognitive science}, pp.\  43--68,
  1993.

\bibitem[Vemula et~al.(2020)Vemula, Oza, Bagnell, and
  Likhachev]{DBLP:conf/rss/VemulaOBL20}
Vemula, A., Oza, Y., Bagnell, J.~A., and Likhachev, M.
\newblock Planning and execution using inaccurate models with provable
  guarantees.
\newblock In Toussaint, M., Bicchi, A., and Hermans, T. (eds.), \emph{Robotics:
  Science and Systems XVI, Virtual Event / Corvalis, Oregon, USA, July 12-16,
  2020}, 2020.
\newblock \doi{10.15607/RSS.2020.XVI.001}.
\newblock URL \url{https://doi.org/10.15607/RSS.2020.XVI.001}.

\bibitem[Vemula et~al.(2021)Vemula, Bagnell, and Likhachev]{vemula2021cmax++}
Vemula, A., Bagnell, J.~A., and Likhachev, M.
\newblock Cmax++: Leveraging experience in planning and execution using
  inaccurate models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  6147--6155, 2021.

\bibitem[Yang et~al.(2018)Yang, Lyu, Liu, and Gustafson]{yang2018peorl}
Yang, F., Lyu, D., Liu, B., and Gustafson, S.
\newblock Peorl: Integrating symbolic planning and hierarchical reinforcement
  learning for robust decision-making.
\newblock \emph{arXiv preprint arXiv:1804.07779}, 2018.

\bibitem[Zhang et~al.(2018)Zhang, Sukhbaatar, Lerer, Szlam, and
  Fergus]{zhang2018composable}
Zhang, A., Sukhbaatar, S., Lerer, A., Szlam, A., and Fergus, R.
\newblock Composable planning with attributes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5842--5851. PMLR, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Torabi, Guan, Ballard, and
  Stone]{zhang2019leveraging}
Zhang, R., Torabi, F., Guan, L., Ballard, D.~H., and Stone, P.
\newblock Leveraging human guidance for deep reinforcement learning tasks.
\newblock \emph{arXiv preprint arXiv:1909.09906}, 2019.

\end{thebibliography}
