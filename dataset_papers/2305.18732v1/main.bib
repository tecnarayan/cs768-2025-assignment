@inproceedings{
    Ren2020balms,
    title={Balanced Meta-Softmax for Long-Tailed Visual Recognition},
    author={Jiawei Ren and Cunjun Yu and Shunan Sheng and Xiao Ma and Haiyu Zhao and Shuai Yi and Hongsheng Li},
    booktitle={Proceedings of Neural Information Processing Systems(NeurIPS)},
    month = {Dec},
    year={2020}
}

@inproceedings{kang2019decoupling,
  title={Decoupling representation and classifier for long-tailed recognition},
  author={Kang, Bingyi and Xie, Saining and Rohrbach, Marcus and Yan, Zhicheng
          and Gordo, Albert and Feng, Jiashi and Kalantidis, Yannis},
  booktitle={Eighth International Conference on Learning Representations (ICLR)},
  year={2020}
}

@inproceedings{kobayashi2021cvpr,
  title={t-vMF Similarity for Regularizing In-Class Feature Distribution},
  author={Takumi Kobayashi},
  booktitle={Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021}
}

@inproceedings{liu2016large,
  title={Large-Margin Softmax Loss for Convolutional Neural Networks},
  author={Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Yang, Meng},
  booktitle={Proceedings of The 33rd International Conference on Machine Learning},
  pages={507--516},
  year={2016}
}

@InProceedings{Liu_2017_CVPR,
  title = {SphereFace: Deep Hypersphere Embedding for Face Recognition},
  author = {Liu, Weiyang and Wen, Yandong and Yu, Zhiding and Li, Ming and Raj, Bhiksha and Song, Le},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2017}
}

@INPROCEEDINGS{8953658,  author={Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   title={ArcFace: Additive Angular Margin Loss for Deep Face Recognition},   year={2019},  volume={},  number={},  pages={4685-4694},  doi={10.1109/CVPR.2019.00482}}

@Article{RePEc:spr:alstar:v:105:y:2021:i:2:d:10.1007_s10182-020-00380-7,
  author={Joseph D. Bailey and Edward A. Codling},
  title={{Emergence of the wrapped Cauchy distribution in mixed directional data}},
  journal={AStA Advances in Statistical Analysis},
  year=2021,
  volume={105},
  number={2},
  pages={229-246},
  month={June},
  keywords={Circular distributions; Wrapped normal; Wrapped Cauchy; Animal movement; Directional data},
  doi={10.1007/s10182-020-00380-},
  abstract={ Inferring the most appropriate distribution (or distributions) to describe observed directional data is important in many applications of circular statistics. In particular, animal movement paths are typically analysed and modelled by considering the distribution of step lengths and turning (or absolute) angles. Here we demonstrate that a single-wrapped Cauchy distribution can appear to fit directional data mixed from two different underlying wrapped normal distributions. We derive mathematical expressions to calculate the parameter space for which this occurs and illustrate the result by analysing an example data set of the movements of African bull elephants (Loxodonta Africana). We conclude that the presence of a wrapped Cauchy distribution in observed directional data can, in certain cases, be explained by data coming from two distinct underlying distributions. We discuss how this may relate to the presence of multiple movement modes within an observed path when analysing animal movement data.},
  url={https://ideas.repec.org/a/spr/alstar/v105y2021i2d10.1007_s10182-020-00380-7.html}
}

@INPROCEEDINGS{8953658,  author={Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   title={ArcFace: Additive Angular Margin Loss for Deep Face Recognition},   year={2019},  volume={},  number={},  pages={4685-4694},  doi={10.1109/CVPR.2019.00482}}

@InProceedings{DBLP:journals/corr/abs-1901-07711,
author = {Hayat, Munawar and Khan, Salman and Zamir, Syed Waqas and Shen, Jianbing and Shao, Ling},
title = {Gaussian Affinity for Max-Margin Class Imbalanced Learning},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@inproceedings{
yang2021free,
title={Free Lunch for Few-shot Learning:  Distribution Calibration},
author={Yang, Shuo and Liu, Lu and Xu, Min},
booktitle={International Conference on Learning Representations (ICLR)},
year={2021},
}

@InProceedings{DBLP:journals/corr/abs-1708-02002,
author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
title = {Focal Loss for Dense Object Detection},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@inproceedings{cao2019learning,
  title={Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss},
  author={Cao, Kaidi and Wei, Colin and Gaidon, Adrien and Arechiga, Nikos and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@InProceedings{tan2020eql,
  title={Equalization Loss for Long-Tailed Object Recognition},
  author={Jingru Tan, Changbao Wang, Buyu Li, Quanquan Li, 
  Wanli Ouyang, Changqing Yin, Junjie Yan},
  booktitle={ArXiv:2003.05176},
  year={2020}
}


@article{DBLP:journals/corr/abs-2001-01385,
  title={Identifying and Compensating for Feature Deviation in Imbalanced Deep Learning},
  author={Han-Jia Ye and Hong-You Chen and De-Chuan Zhan and Wei-Lun Chao},
  journal={ArXiv},
  year={2020},
  volume={abs/2001.01385}
}

@InProceedings{10.1007/11538059_91,
author="Han, Hui
and Wang, Wen-Yuan
and Mao, Bing-Huan",
editor="Huang, De-Shuang
and Zhang, Xiao-Ping
and Huang, Guang-Bin",
title="Borderline-SMOTE: A New Over-Sampling Method in Imbalanced Data Sets Learning",
booktitle="Advances in Intelligent Computing",
year="2005",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="878--887",
abstract="In recent years, mining with imbalanced data sets receives more and more attentions in both theoretical and practical aspects. This paper introduces the importance of imbalanced data sets and their broad application domains in data mining, and then summarizes the evaluation metrics and the existing methods to evaluate and solve the imbalance problem. Synthetic minority over-sampling technique (SMOTE) is one of the over-sampling methods addressing this problem. Based on SMOTE method, this paper presents two new minority over-sampling methods, borderline-SMOTE1 and borderline-SMOTE2, in which only the minority examples near the borderline are over-sampled. For the minority class, experiments show that our approaches achieve better TP rate and F-value than SMOTE and random over-sampling methods.",
isbn="978-3-540-31902-3"
}

@inproceedings{Kubt1997AddressingTC,
  title={Addressing the Curse of Imbalanced Training Sets: One-Sided Selection},
  author={Miroslav Kub{\'a}t and Stan Matwin},
  booktitle={ICML},
  year={1997}
}

@INPROCEEDINGS{7780949,  author={Huang, Chen and Li, Yining and Loy, Chen Change and Tang, Xiaoou},  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Learning Deep Representation for Imbalanced Classification},   year={2016},  volume={},  number={},  pages={5375-5384},  doi={10.1109/CVPR.2016.580}}


@ARTICLE{DBLP:journals/corr/abs-1806-00194,
  author={Huang, Chen and Li, Yining and Loy, Chen Change and Tang, Xiaoou},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Deep Imbalanced Learning for Face Recognition and Attribute Prediction}, 
  year={2020},
  volume={42},
  number={11},
  pages={2781-2794},
  doi={10.1109/TPAMI.2019.2914680}}


@inproceedings{cui2019classbalancedloss,
  title={Class-Balanced Loss Based on Effective Number of Samples},
  author={Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{openlongtailrecognition,
  title={Large-Scale Long-Tailed Recognition in an Open World},
  author={Liu, Ziwei and Miao, Zhongqi and Zhan, Xiaohang and Wang, Jiayun and Gong, Boqing and Yu, Stella X.},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}

@article{krizhevsky2009learning,
  added-at = {2021-01-21T03:01:11.000+0100},
  author = {Krizhevsky, Alex},
  biburl = {https://www.bibsonomy.org/bibtex/2fe5248afe57647d9c85c50a98a12145c/s364315},
  interhash = {cc2d42f2b7ef6a4e76e47d1a50c8cd86},
  intrahash = {fe5248afe57647d9c85c50a98a12145c},
  keywords = {},
  pages = {},
  timestamp = {2021-01-21T03:01:11.000+0100},
  title = {Learning Multiple Layers of Features from Tiny Images},
  url = {https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf},
  year = 2009
}

@INPROCEEDINGS{5206848,  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},   title={ImageNet: A large-scale hierarchical image database},   year={2009},  volume={},  number={},  pages={248-255},  doi={10.1109/CVPR.2009.5206848}}

@INPROCEEDINGS{DBLP:journals/corr/HornASSAPB17,
  author={Van Horn, Grant and Mac Aodha, Oisin and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={The iNaturalist Species Classification and Detection Dataset}, 
  year={2018},
  volume={},
  number={},
  pages={8769-8778},
  doi={10.1109/CVPR.2018.00914}}


@article{articleBailey,
author = {Bailey, Joseph and Codling, Edward},
year = {2020},
month = {10},
pages = {},
title = {Emergence of the wrapped Cauchy distribution in mixed directional data},
volume = {105},
journal = {AStA Advances in Statistical Analysis},
doi = {10.1007/s10182-020-00380-7}
}

@INPROCEEDINGS{7410480,  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)},   title={Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},   year={2015},  volume={},  number={},  pages={1026-1034},  doi={10.1109/ICCV.2015.123}}


@InProceedings{pmlr-v9-glorot10a,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {},
  volume = 	 {},
  series = 	 {},
  address = 	 {},
  month = 	 {},
  publisher =    {},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}

@article{Kingma2014AutoEncodingVB,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@InProceedings{pmlr-v48-gal16,
  title = 	 {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = 	 {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  editor = 	 {},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {},
  month = 	 {},
  publisher =    {},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/gal16.html},
  abstract = 	 {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}

@INPROCEEDINGS {9157756,
author = {J. Chang and Z. Lan and C. Cheng and Y. Wei},
booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Data Uncertainty Learning in Face Recognition},
year = {2020},
volume = {},
issn = {},
pages = {5709-5718},
keywords = {uncertainty;face;face recognition;noise measurement;data models;training;gaussian distribution},
doi = {10.1109/CVPR42600.2020.00575},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.00575},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@inproceedings{10.5555/2969830.2969856,
author = {Bridle, John S.},
title = {Training Stochastic Model Recognition Algorithms as Networks Can Lead to Maximum Mutual Information Estimation of Parameters},
year = {1989},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {One of the attractions of neural network approaches to pattern recognition is the use of a discrimination-based training method. We show that once we have modified the output layer of a multilayer perceptron to provide mathematically correct probability distributions, and replaced the usual squared error criterion with a probability-based score, the result is equivalent to Maximum Mutual Information training, which has been used successfully to improve the performance of hidden Markov models for speech recognition. If the network is specially constructed to perform the recognition computations of a given kind of stochastic model based classifier then we obtain a method for discrimination-based training of the parameters of the models. Examples include an HMM-based word discriminator, which we call an 'Alphanet'.},
booktitle = {Proceedings of the 2nd International Conference on Neural Information Processing Systems},
pages = {211–217},
numpages = {7},
series = {NIPS'89}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={},
    year={2016}
}

@article{10.1006/jmva.1999.1863,
author = {Holmstr\"{o}m, Lasse},
title = {The Accuracy and the Computational Complexity of a Multivariate Binned Kernel Density Estimator},
year = {2000},
issue_date = {Feb. 2000},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {72},
number = {2},
issn = {0047-259X},
url = {https://doi.org/10.1006/jmva.1999.1863},
doi = {10.1006/jmva.1999.1863},
abstract = {The computational cost of multivariate kernel density estimation can be reduced by prebinning the data. The data are discretized to a grid and a weighted kernel estimator is computed. We report results on the accuracy of such a binned kernel estimator and discuss the computational complexity of the estimator as measured by its average number of nonzero terms.},
journal = {J. Multivar. Anal.},
month = {feb},
pages = {264–309},
numpages = {46},
keywords = {estimation error, binning, computational complexity, Kernel density estimation}
}

@article{10.1214/aoms/1177728190,
author = {Murray Rosenblatt},
title = {{Remarks on Some Nonparametric Estimates of a Density Function}},
volume = {27},
journal = {The Annals of Mathematical Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {832 -- 837},
year = {1956},
doi = {10.1214/aoms/1177728190},
URL = {https://doi.org/10.1214/aoms/1177728190}
}

@article{10.2307/2237880,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2237880},
 author = {Emanuel Parzen},
 journal = {The Annals of Mathematical Statistics},
 number = {3},
 pages = {1065--1076},
 publisher = {Institute of Mathematical Statistics},
 title = {On Estimation of a Probability Density Function and Mode},
 urldate = {2022-05-01},
 volume = {33},
 year = {1962}
}

@INPROCEEDINGS{cui2021parametric,
  author={Cui, Jiequan and Zhong, Zhisheng and Liu, Shu and Yu, Bei and Jia, Jiaya},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Parametric Contrastive Learning}, 
  year={2021},
  volume={},
  number={},
  pages={695-704},
  doi={10.1109/ICCV48922.2021.00075}}


@book{Jammalamadaka2001Topics,
  abstract = {This research monograph on circular data analysis covers some recent advances in the field, besides providing a brief introduction to, and a review of, existing methods and models. The primary focus is on recent research into topics such as change-point problems, predictive distributions, circular correlation and regression, etc. An important feature of this work is the S-plus subroutines provided for analyzing actual data sets. Coupled with the discussion of new theoretical research, the book should benefit both the researcher and the practitioner.},
  added-at = {2018-06-18T21:23:34.000+0200},
  author = {Jammalamadaka, S. Rao and SenGupta, Ashis},
  biburl = {https://www.bibsonomy.org/bibtex/2210b2da2df92f766834aaa2455796723/pbett},
  citeulike-article-id = {13421692},
  citeulike-linkout-0 = {http://dx.doi.org/10.1142/9789812779267},
  comment = {(private-note)The CRAN packages 'circular' and 'circstats' use this as the main reference.},
  doi = {10.1142/9789812779267},
  interhash = {28a6da4360f898ed59ec8d63253cf420},
  intrahash = {210b2da2df92f766834aaa2455796723},
  isbn = {9789812779267},
  keywords = {textbook statistics},
  posted-at = {2014-11-07 07:52:45},
  priority = {2},
  publisher = {World Scientific Publishing Co. Pte. Ltd.},
  timestamp = {2018-06-22T18:34:29.000+0200},
  title = {Topics in Circular Statistics},
  url = {http://dx.doi.org/10.1142/9789812779267},
  volume = 5,
  year = 2001
}

@misc{https://doi.org/10.48550/arxiv.1901.05555,
  doi = {10.48550/ARXIV.1901.05555},
  
  url = {https://arxiv.org/abs/1901.05555},
  
  author = {Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Class-Balanced Loss Based on Effective Number of Samples},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{10.1007/978-3-319-46478-7_31,
author="Wen, Yandong
and Zhang, Kaipeng
and Li, Zhifeng
and Qiao, Yu",
editor="Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max",
title="A Discriminative Feature Learning Approach for Deep Face Recognition",
booktitle="Computer Vision -- ECCV 2016",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="499--515",
abstract="Convolutional neural networks (CNNs) have been widely used in computer vision community, significantly improving the state-of-the-art. In most of the available CNNs, the softmax loss function is used as the supervision signal to train the deep model. In order to enhance the discriminative power of the deeply learned features, this paper proposes a new supervision signal, called center loss, for face recognition task. Specifically, the center loss simultaneously learns a center for deep features of each class and penalizes the distances between the deep features and their corresponding class centers. More importantly, we prove that the proposed center loss function is trainable and easy to optimize in the CNNs. With the joint supervision of softmax loss and center loss, we can train a robust CNNs to obtain the deep features with the two key learning objectives, inter-class dispension and intra-class compactness as much as possible, which are very essential to face recognition. It is encouraging to see that our CNNs (with such joint supervision) achieve the state-of-the-art accuracy on several important face recognition benchmarks, Labeled Faces in the Wild (LFW), YouTube Faces (YTF), and MegaFace Challenge. Especially, our new approach achieves the best results on MegaFace (the largest public domain face benchmark) under the protocol of small training set (contains under 500000 images and under 20000 persons), significantly improving the previous results and setting new state-of-the-art for both face recognition and face verification tasks.",
isbn="978-3-319-46478-7"
}

@book{rasmussen2005gaussian,
  added-at = {2010-06-29T18:30:57.000+0200},
  asin = {026218253X},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  biburl = {https://www.bibsonomy.org/bibtex/2628768a62cfbefc942f9ed0b5ba837c3/ahmedjawwad4u},
  description = {Amazon.com: Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning) (9780262182539): Carl…},
  dewey = {519.23},
  ean = {9780262182539},
  interhash = {119334669b0af3633804667a892f6728},
  intrahash = {628768a62cfbefc942f9ed0b5ba837c3},
  isbn = {026218253X},
  keywords = {Gaussi},
  publisher = {The MIT Press},
  timestamp = {2010-06-29T18:30:57.000+0200},
  title = {Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)},
  year = 2005
}


@article{DBLP:journals/corr/abs-2011-06225,
title = {A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
journal = {Information Fusion},
volume = {76},
pages = {243-297},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001081},
author = {Moloud Abdar and Farhad Pourpanah and Sadiq Hussain and Dana Rezazadegan and Li Liu and Mohammad Ghavamzadeh and Paul Fieguth and Xiaochun Cao and Abbas Khosravi and U. Rajendra Acharya and Vladimir Makarenkov and Saeid Nahavandi},
keywords = {Artificial intelligence, Uncertainty quantification, Deep learning, Machine learning, Bayesian statistics, Ensemble learning},
abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.}
}

@INPROCEEDINGS{https://doi.org/10.48550/arxiv.1512.03385,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}}


@inproceedings{wang2020long,
  title={Long-tailed Recognition by Routing Diverse Distribution-Aware Experts},
  author={Wang, Xudong and Lian, Long and Miao, Zhongqi and Liu, Ziwei and Yu, Stella},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{zhang2021test,
  title={Test-Agnostic Long-Tailed Recognition by Test-Time Aggregating Diverse Experts with Self-Supervision},
  author={Zhang, Yifan and Hooi, Bryan and Hong, Lanqing and Feng, Jiashi},
  journal={arXiv preprint arXiv:2107.09249},
  year={2021}
}

@inproceedings{https://doi.org/10.48550/arxiv.2103.14267,
title = "Contrastive Learning based Hybrid Networks for Long-Tailed Image Classification",
author = "Peng Wang and Kai Han and Xiu-Shen Wei and Lei Zhang and Lei Wang",
year = "2021",
language = "Undefined/Unknown",
booktitle = "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
}

@inproceedings{DBLP:conf/iclr/KangLXYF21,
  author    = {Bingyi Kang and
               Yu Li and
               Sa Xie and
               Zehuan Yuan and
               Jiashi Feng},
  title     = {Exploring Balanced Feature Spaces for Representation Learning},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=OqtLIabPTit},
  timestamp = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/KangLXYF21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{9577506,  author={Zhang, Songyang and Li, Zeming and Yan, Shipeng and He, Xuming and Sun, Jian},  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Distribution Alignment: A Unified Framework for Long-tail Visual Recognition},   year={2021},  volume={},  number={},  pages={2361-2370},  doi={10.1109/CVPR46437.2021.00239}}

@inproceedings{han2018coteaching,
  title={Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting},
  author={Shu, Jun and Xie, Qi and Yi, Lixuan and Zhao, Qian and Zhou, Sanping and Xu, Zongben and Meng, Deyu},
  booktitle={NeurIPS},
  year={2019}
}

@article{Feng2021ExploringCE,
  title={Exploring Classification Equilibrium in Long-Tailed Object Detection},
  author={Chengjian Feng and Yujie Zhong and Weilin Huang},
  journal={2021 IEEE/CVF International Conference on Computer Vision (ICCV)},
  year={2021},
  pages={3397-3406}
}

@InProceedings{Zhang_2021_ICCV,
    author    = {Zhang, Xing and Wu, Zuxuan and Weng, Zejia and Fu, Huazhu and Chen, Jingjing and Jiang, Yu-Gang and Davis, Larry S.},
    title     = {VideoLT: Large-Scale Long-Tailed Video Recognition},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {7960-7969}
}

@InProceedings{Yun_2021_CVPR,
    author    = {Yun, Sangdoo and Oh, Seong Joon and Heo, Byeongho and Han, Dongyoon and Choe, Junsuk and Chun, Sanghyuk},
    title     = {Re-Labeling ImageNet: From Single to Multi-Labels, From Global to Localized Labels},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {2340-2350}
}


@InProceedings{pmlr-v119-shankar20c,
  title = 	 {Evaluating Machine Accuracy on {I}mage{N}et},
  author =       {Shankar, Vaishaal and Roelofs, Rebecca and Mania, Horia and Fang, Alex and Recht, Benjamin and Schmidt, Ludwig},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8634--8644},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/shankar20c/shankar20c.pdf},
  url = 	 {https://proceedings.mlr.press/v119/shankar20c.html},
  abstract = 	 {We evaluate a wide range of ImageNet models with five trained human labelers. In our year-long experiment, trained humans first annotated 40,000 images from the ImageNet and ImageNetV2 test sets with multi-class labels to enable a semantically coherent evaluation. Then we measured the classification accuracy of the five trained humans on the full task with 1,000 classes. Only the latest models from 2020 are on par with our best human labeler, and human accuracy on the 590 object classes is still 4% and 10% higher than the best model on ImageNet and ImageNetV2, respectively. Moreover, humans achieve the same accuracy on ImageNet and ImageNetV2, while all models see a consistent accuracy drop. Overall, our results show that there is still substantial room for improvement on ImageNet and direct accuracy comparisons between humans and machines may overstate machine performance.}
}

@article{DBLP:journals/corr/abs-2006-07159,
  author    = {Lucas Beyer and
               Olivier J. H{\'{e}}naff and
               Alexander Kolesnikov and
               Xiaohua Zhai and
               A{\"{a}}ron van den Oord},
  title     = {Are we done with ImageNet?},
  journal   = {CoRR},
  volume    = {abs/2006.07159},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.07159},
  eprinttype = {arXiv},
  eprint    = {2006.07159},
  timestamp = {Fri, 20 Nov 2020 14:04:05 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-07159.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wu2021advlt,
 author =  {Tong Wu, Ziwei Liu, Qingqiu Huang, Yu Wang, and Dahua Lin},
 title = {Adversarial Robustness under Long-Tailed Distribution},
 booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 year = {2021}
 }

 @inproceedings{cao2020heteroskedastic,
  title={Heteroskedastic and imbalanced deep learning with adaptive regularization},
  author={Cao, Kaidi and Chen, Yining and Lu, Junwei and Arechiga, Nikos and Gaidon, Adrien and Ma, Tengyu},
  booktitle={International Conference on Learning Representations}, 
  year={2021} 
}

@misc{zhang2023deep,
      title={Deep Long-Tailed Learning: A Survey}, 
      author={Yifan Zhang and Bingyi Kang and Bryan Hooi and Shuicheng Yan and Jiashi Feng},
      year={2023},
      eprint={2110.04596},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@book{doi:10.1142/4031,
author = {Jammalamadaka, S Rao and SenGupta, Ashis},
title = {Topics in Circular Statistics},
publisher = {WORLD SCIENTIFIC},
year = {2001},
doi = {10.1142/4031},
address = {},
edition   = {},
URL = {https://www.worldscientific.com/doi/abs/10.1142/4031},
eprint = {https://www.worldscientific.com/doi/pdf/10.1142/4031}
}

@inproceedings{hong2023long,
  title={Long-Tailed Partial Label Learning via Dynamic Rebalancing},
  author={Hong, Feng and Yao, Jiangchao and Zhou, Zhihan and Zhang, Ya and Wang, Yanfeng},
  booktitle={{ICLR}},
  year={2023}
}