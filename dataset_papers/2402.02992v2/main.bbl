\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and Man{\'e}]{Amodei2016concrete}
Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Man{\'e}, D.
\newblock Concrete problems in {AI} safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen, et~al.]{Anil2023palm}
Anil, R., Dai, A.~M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et~al.
\newblock {PaLM 2} technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem[Azar et~al.(2023)Azar, Rowland, Piot, Guo, Calandriello, Valko, and Munos]{Azar2023general}
Azar, M.~G., Rowland, M., Piot, B., Guo, D., Calandriello, D., Valko, M., and Munos, R.
\newblock A general theoretical paradigm to understand learning from human preferences.
\newblock \emph{arXiv preprint arXiv:2310.12036}, 2023.

\bibitem[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain, Fort, Ganguli, Henighan, et~al.]{Bai2022training}
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022.

\bibitem[Beeching et~al.(2023)Beeching, Fourrier, Habib, Han, Lambert, Rajani, Sanseviero, Tunstall, and Wolf]{Beeching2023open}
Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert, N., Rajani, N., Sanseviero, O., Tunstall, L., and Wolf, T.
\newblock {Open LLM} leaderboard.
\newblock \url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}, 2023.

\bibitem[Casper et~al.(2023)Casper, Davies, Shi, Gilbert, Scheurer, Rando, Freedman, Korbak, Lindner, Freire, Wang, Marks, Segerie, Carroll, Peng, Christoffersen, Damani, Slocum, Anwar, Siththaranjan, Nadeau, Michaud, Pfau, Krasheninnikov, Chen, Langosco, Hase, Biyik, Dragan, Krueger, Sadigh, and Hadfield-Menell]{Casper2023open}
Casper, S., Davies, X., Shi, C., Gilbert, T.~K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., Wang, T.~T., Marks, S., Segerie, C.-R., Carroll, M., Peng, A., Christoffersen, P., Damani, M., Slocum, S., Anwar, U., Siththaranjan, A., Nadeau, M., Michaud, E.~J., Pfau, J., Krasheninnikov, D., Chen, X., Langosco, L., Hase, P., Biyik, E., Dragan, A., Krueger, D., Sadigh, D., and Hadfield-Menell, D.
\newblock Open problems and fundamental limitations of reinforcement learning from human feedback.
\newblock \emph{Transactions on Machine Learning Research (TMLR)}, 2023.

\bibitem[Chen et~al.(2023)Chen, Borgeaud, Irving, Lespiau, Sifre, and Jumper]{Chen2023accelerating}
Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J.
\newblock Accelerating large language model decoding with speculative sampling.
\newblock \emph{arXiv preprint arXiv:2302.01318}, 2023.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{Christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{Proceedings of the Conference on Neural Information Processing Systems}, 2017.

\bibitem[Cui et~al.(2023)Cui, Yuan, Ding, Yao, Zhu, Ni, Xie, Liu, and Sun]{Cui2023ultrafeedback}
Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z., and Sun, M.
\newblock {UltraFeedback}: Boosting language models with high-quality feedback.
\newblock \emph{arXiv preprint arXiv:2310.01377}, 2023.

\bibitem[Deng \& Raffel(2023)Deng and Raffel]{Deng2023reward}
Deng, H. and Raffel, C.
\newblock Reward-augmented decoding: Efficient controlled text generation with a unidirectional reward model.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  11781--11791. Association for Computational Linguistics, 2023.

\bibitem[Gulcehre et~al.(2015)Gulcehre, Firat, Xu, Cho, Barrault, Lin, Bougares, Schwenk, and Bengio]{Gulcehre2015using}
Gulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L., Lin, H.-C., Bougares, F., Schwenk, H., and Bengio, Y.
\newblock On using monolingual corpora in neural machine translation.
\newblock \emph{arXiv preprint arXiv:1503.03535}, 2015.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{Hu2022lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lo{RA}: low-rank adaptation of large language models.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2022.

\bibitem[Huang et~al.(2024)Huang, Sengupta, Bonadiman, Lai, Gupta, Pappas, Mansour, Kirchoff, and Roth]{Huang2024deal}
Huang, J.~Y., Sengupta, S., Bonadiman, D., Lai, Y.-a., Gupta, A., Pappas, N., Mansour, S., Kirchoff, K., and Roth, D.
\newblock Deal: Decoding-time alignment for large language models.
\newblock \emph{arXiv preprint arXiv:2402.06147}, 2024.

\bibitem[Jang et~al.(2023)Jang, Kim, Lin, Wang, Hessel, Zettlemoyer, Hajishirzi, Choi, and Ammanabrolu]{Jang2023personalized}
Jang, J., Kim, S., Lin, B.~Y., Wang, Y., Hessel, J., Zettlemoyer, L., Hajishirzi, H., Choi, Y., and Ammanabrolu, P.
\newblock Personalized soups: Personalized large language model alignment via post-hoc parameter merging.
\newblock \emph{arXiv preprint arXiv:2310.11564}, 2023.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{Jiang2023mistral}
Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al.
\newblock Mistral {7B}.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Khanov et~al.(2024)Khanov, Burapacheep, and Li]{Khanov2024alignment}
Khanov, M., Burapacheep, J., and Li, Y.
\newblock Alignment as reward-guided search.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2024.

\bibitem[Korbak et~al.(2022)Korbak, Perez, and Buckley]{Korbak2022rl}
Korbak, T., Perez, E., and Buckley, C.
\newblock {RL} with {KL} penalties is better viewed as {B}ayesian inference.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2022.

\bibitem[Leviathan et~al.(2023)Leviathan, Kalman, and Matias]{Leviathan2023fast}
Leviathan, Y., Kalman, M., and Matias, Y.
\newblock Fast inference from transformers via speculative decoding.
\newblock In \emph{Proceedings of the International Conference on Machine Learning (ICML)}, Proceedings of Machine Learning Research. PMLR, 2023.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K\"{u}ttler, Lewis, Yih, Rockt\"{a}schel, Riedel, and Kiela]{Lewis2020retrieval}
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K\"{u}ttler, H., Lewis, M., Yih, W.-t., Rockt\"{a}schel, T., Riedel, S., and Kiela, D.
\newblock Retrieval-augmented generation for knowledge-intensive {NLP} tasks.
\newblock In \emph{Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Li et~al.(2023)Li, Holtzman, Fried, Liang, Eisner, Hashimoto, Zettlemoyer, and Lewis]{Li2023contrastive}
Li, X.~L., Holtzman, A., Fried, D., Liang, P., Eisner, J., Hashimoto, T., Zettlemoyer, L., and Lewis, M.
\newblock Contrastive decoding: Open-ended text generation as optimization.
\newblock In \emph{Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)}, 2023.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Han, Wang, Tsvetkov, Choi, and Smith]{Liu2024tuning}
Liu, A., Han, X., Wang, Y., Tsvetkov, Y., Choi, Y., and Smith, N.~A.
\newblock Tuning language models by proxy.
\newblock \emph{arXiv preprint arXiv:2401.08565}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Zhao, Joshi, Khalman, Saleh, Liu, and Liu]{Liu2024statistical}
Liu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M., Liu, P.~J., and Liu, J.
\newblock Statistical rejection sampling improves preference optimization.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2024{\natexlab{b}}.

\bibitem[Lu et~al.(2023)Lu, Brahman, West, Jung, Chandu, Ravichander, Ammanabrolu, Jiang, Ramnath, Dziri, et~al.]{Lu2023inference}
Lu, X., Brahman, F., West, P., Jung, J., Chandu, K., Ravichander, A., Ammanabrolu, P., Jiang, L., Ramnath, S., Dziri, N., et~al.
\newblock Inference-time policy adapters {(IPA)}: Tailoring extreme-scale lms without fine-tuning.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pp.\  6863--6883, 2023.

\bibitem[Mitchell et~al.(2024)Mitchell, Rafailov, Sharma, Finn, and Manning]{Mitchell2024emulator}
Mitchell, E., Rafailov, R., Sharma, A., Finn, C., and Manning, C.~D.
\newblock An emulator for fine-tuning large language models using small language models.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2024.

\bibitem[Munos et~al.(2023)Munos, Valko, Calandriello, Azar, Rowland, Guo, Tang, Geist, Mesnard, Michi, Selvi, Girgin, Momchev, Bachem, Mankowitz, Precup, and Piot]{Munos2023nash}
Munos, R., Valko, M., Calandriello, D., Azar, M.~G., Rowland, M., Guo, D., Tang, Y., Geist, M., Mesnard, T., Michi, A., Selvi, M., Girgin, S., Momchev, N., Bachem, O., Mankowitz, D.~J., Precup, D., and Piot, B.
\newblock Nash learning from human feedback.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Narayan et~al.(2018)Narayan, Cohen, and Lapata]{Narayan2018dont}
Narayan, S., Cohen, S.~B., and Lapata, M.
\newblock Don{'}t give me the details, just the summary! {T}opic-aware convolutional neural networks for extreme summarization.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)}, 2018.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Gray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe]{Ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Gray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Pan et~al.(2022)Pan, Bhatia, and Steinhardt]{Pan2022effects}
Pan, A., Bhatia, K., and Steinhardt, J.
\newblock The effects of reward misspecification: Mapping and mitigating misaligned models.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2022.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and Finn]{Rafailov2023direct}
Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C.~D., and Finn, C.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock In \emph{Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{Raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research (JMLR)}, 21\penalty0 (1):\penalty0 5485--5551, 2020.

\bibitem[Ramamurthy et~al.(2023)Ramamurthy, Ammanabrolu, Brantley, Hessel, Sifa, Bauckhage, Hajishirzi, and Choi]{Ramamurthy2023reinforcement}
Ramamurthy, R., Ammanabrolu, P., Brantley, K., Hessel, J., Sifa, R., Bauckhage, C., Hajishirzi, H., and Choi, Y.
\newblock Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2023.

\bibitem[Rame et~al.(2023)Rame, Couairon, Shukor, Dancette, Gaya, Soulier, and Cord]{Rame2023rewarded}
Rame, A., Couairon, G., Shukor, M., Dancette, C., Gaya, J.-B., Soulier, L., and Cord, M.
\newblock Rewarded soups: Towards {Pareto}-optimal alignment by interpolating weights fine-tuned on diverse rewards.
\newblock In \emph{Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Roberts et~al.(2022)Roberts, Chung, Levskaya, Mishra, Bradbury, Andor, Narang, Lester, Gaffney, Mohiuddin, Hawthorne, Lewkowycz, Salcianu, van Zee, Austin, Goodman, Soares, Hu, Tsvyashchenko, Chowdhery, Bastings, Bulian, Garcia, Ni, Chen, Kenealy, Clark, Lee, Garrette, Lee-Thorp, Raffel, Shazeer, Ritter, Bosma, Passos, Maitin-Shepard, Fiedel, Omernick, Saeta, Sepassi, Spiridonov, Newlan, and Gesmundo]{Roberts2022t5x}
Roberts, A., Chung, H.~W., Levskaya, A., Mishra, G., Bradbury, J., Andor, D., Narang, S., Lester, B., Gaffney, C., Mohiuddin, A., Hawthorne, C., Lewkowycz, A., Salcianu, A., van Zee, M., Austin, J., Goodman, S., Soares, L.~B., Hu, H., Tsvyashchenko, S., Chowdhery, A., Bastings, J., Bulian, J., Garcia, X., Ni, J., Chen, A., Kenealy, K., Clark, J.~H., Lee, S., Garrette, D., Lee-Thorp, J., Raffel, C., Shazeer, N., Ritter, M., Bosma, M., Passos, A., Maitin-Shepard, J., Fiedel, N., Omernick, M., Saeta, B., Sepassi, R., Spiridonov, A., Newlan, J., and Gesmundo, A.
\newblock Scaling up models and data with $\texttt{t5x}$ and $\texttt{seqio}$.
\newblock \emph{arXiv preprint arXiv:2203.17189}, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.17189}.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and Abbeel]{Schulman2015high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage estimation.
\newblock In \emph{Proceedings of the International Conference on Learning Representations (ICLR)}, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{Schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Stahlberg et~al.(2018)Stahlberg, Cross, and Stoyanov]{Stahlberg2018simple}
Stahlberg, F., Cross, J., and Stoyanov, V.
\newblock Simple fusion: Return of the language model.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation (WMT)}, pp.\  204--211. Association for Computational Linguistics, October 2018.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, and Christiano]{Stiennon2020learning}
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P.~F.
\newblock Learning to summarize with human feedback.
\newblock In \emph{Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Sun et~al.(2023)Sun, Gupta, and Iyyer]{Sun2023exploring}
Sun, S., Gupta, D., and Iyyer, M.
\newblock Exploring the impact of low-rank adaptation on the performance, efficiency, and regularization of {RLHF}.
\newblock \emph{arXiv preprint arXiv:2309.09055}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{Touvron2023llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Tunstall et~al.(2023{\natexlab{a}})Tunstall, Beeching, Lambert, Rajani, Rasul, Belkada, Huang, von Werra, Fourrier, Habib, et~al.]{Tunstall2023zephyr}
Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier, C., Habib, N., et~al.
\newblock Zephyr: Direct distillation of {LM} alignment.
\newblock \emph{arXiv preprint arXiv:2310.16944}, 2023{\natexlab{a}}.

\bibitem[Tunstall et~al.(2023{\natexlab{b}})Tunstall, Beeching, Lambert, Rajani, Rush, and Wolf]{Tunstall2023alignment}
Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rush, A.~M., and Wolf, T.
\newblock The alignment handbook.
\newblock \url{https://github.com/huggingface/alignment-handbook}, 2023{\natexlab{b}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{Vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Proceedings of the Conference on Neural Information Processing Systems}, volume~30, 2017.

\bibitem[Zhao et~al.(2023)Zhao, Joshi, Liu, Khalman, Saleh, and Liu]{Zhao2023slic}
Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M., and Liu, P.~J.
\newblock {SLiC-HF}: Sequence likelihood calibration with human feedback.
\newblock \emph{arXiv preprint arXiv:2305.10425}, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{Zheng2023judging}
Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et~al.
\newblock Judging {LLM}-as-a-judge with {MT-Bench} and chatbot arena.
\newblock In \emph{Proceedings of the Conference on Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, and Irving]{Ziegler2019finetuning}
Ziegler, D.~M., Stiennon, N., Wu, J., Brown, T.~B., Radford, A., Amodei, D., Christiano, P., and Irving, G.
\newblock Fine-tuning language models from human preferences.
\newblock \emph{arXiv preprint arXiv:1909.08593}, 2019.

\end{thebibliography}
