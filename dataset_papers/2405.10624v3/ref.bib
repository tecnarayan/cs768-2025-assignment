@article{vaswani2022near,
  title={Near-optimal sample complexity bounds for constrained MDPs},
  author={Vaswani, Sharan and Yang, Lin and Szepesv{\'a}ri, Csaba},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3110--3122},
  year={2022}
}

@inproceedings{chen2022learning,
  title={Learning infinite-horizon average-reward Markov decision process with constraints},
  author={Chen, Liyu and Jain, Rahul and Luo, Haipeng},
  booktitle={International Conference on Machine Learning},
  pages={3246--3270},
  year={2022},
  
}

@inproceedings{agarwal2022regret,
  title={Regret guarantees for model-based reinforcement learning with long-term average constraints},
  author={Agarwal, Mridul and Bai, Qinbo and Aggarwal, Vaneet},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={22--31},
  year={2022},
  
}

@inproceedings{wei2022provably,
  title={A provably-efficient model-free algorithm for infinite-horizon average-reward constrained Markov decision processes},
  author={Wei, Honghao and Liu, Xin and Ying, Lei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={4},
  pages={3868--3876},
  year={2022}
}

@inproceedings{ghosh2022achieving,
  title={Achieving sub-linear regret in infinite horizon average reward constrained mdp with linear function approximation},
  author={Ghosh, Arnob and Zhou, Xingyu and Shroff, Ness},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{bai2024learning,
  title={Learning General Parameterized Policies for Infinite Horizon Average Reward Constrained MDPs via Primal-Dual Policy Gradient Algorithm},
  author={Bai, Qinbo and Mondal, Washim Uddin and Aggarwal, Vaneet},
  journal={arXiv preprint arXiv:2402.02042},
  year={2024}
}

@article{he2021nearly,
  title={Nearly minimax optimal reinforcement learning for discounted MDPs},
  author={He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22288--22300},
  year={2021}
}


@inproceedings{wei2021provably,
  title={Triple-q: A model-free algorithm for constrained reinforcement learning with sublinear regret and zero constraint violation},
  author={Wei, Honghao and Liu, Xin and Ying, Lei},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3274--3307},
  year={2022}
}

@inproceedings{bai2022achieving,
  title={Achieving zero constraint violation for constrained reinforcement learning via primal-dual approach},
  author={Bai, Qinbo and Bedi, Amrit Singh and Agarwal, Mridul and Koppel, Alec and Aggarwal, Vaneet},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={4},
  pages={3682--3689},
  year={2022}
}

@article{liu2021learning,
  title={Learning policies with zero or bounded constraint violation for constrained mdps},
  author={Liu, Tao and Zhou, Ruida and Kalathil, Dileep and Kumar, Panganamala and Tian, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17183--17193},
  year={2021}
}

@inproceedings{ding2021provably,
  title={Provably efficient safe exploration via primal-dual policy optimization},
  author={Ding, Dongsheng and Wei, Xiaohan and Yang, Zhuoran and Wang, Zhaoran and Jovanovic, Mihailo},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3304--3312},
  year={2021},
  
}

@article{efroni2020exploration,
  title={Exploration-exploitation in constrained mdps},
  author={Efroni, Yonathan and Mannor, Shie and Pirotta, Matteo},
  journal={arXiv preprint arXiv:2003.02189},
  year={2020}
}

@inproceedings{wei2020model,
  title={Model-free reinforcement learning in infinite-horizon average-reward markov decision processes},
  author={Wei, Chen-Yu and Jahromi, Mehdi Jafarnia and Luo, Haipeng and Sharma, Hiteshi and Jain, Rahul},
  booktitle={International conference on machine learning},
  pages={10170--10180},
  year={2020},
  
}

@inproceedings{wei2021learning,
  title={Learning infinite-horizon average-reward mdps with linear function approximation},
  author={Wei, Chen-Yu and Jahromi, Mehdi Jafarnia and Luo, Haipeng and Jain, Rahul},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3007--3015},
  year={2021},
  
}

@inproceedings{bai2024regret,
  title={Regret analysis of policy gradient algorithm for infinite horizon average reward markov decision processes},
  author={Bai, Qinbo and Mondal, Washim Uddin and Aggarwal, Vaneet},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={10},
  pages={10980--10988},
  year={2024}
}

@article{agrawal2017optimistic,
  title={Optimistic posterior sampling for reinforcement learning: worst-case regret bounds},
  author={Agrawal, Shipra and Jia, Randy},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{auer2008near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@inproceedings{mondal2024improved,
  title={Improved sample complexity analysis of natural policy gradient algorithm with general parameterization for infinite horizon discounted reward {M}arkov decision processes},
  author={Mondal, Washim U and Aggarwal, Vaneet},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3097--3105},
  year={2024},
  
}

@article{mondal2017traffic,
  title={Traffic-aware green cognitive radio},
  author={Mondal, Washim Uddin and Biswas, Sudipta and Das, Goutam and Ray, Priyadip},
  journal={Physical Communication},
  volume={23},
  pages={20--28},
  year={2017},
  publisher={Elsevier}
}

@article{garaffa2021reinforcement,
  title={Reinforcement learning for mobile robotics exploration: A survey},
  author={Garaffa, Lu{\'\i}za Caetano and Basso, Maik and Konzen, Andr{\'e}a Aparecida and de Freitas, Edison Pignaton},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={34},
  number={8},
  pages={3796--3810},
  year={2021},
  publisher={IEEE}
}

@article{liu2021policy,
  title={Policy optimization for constrained mdps with provable fast global convergence},
  author={Liu, Tao and Zhou, Ruida and Kalathil, Dileep and Kumar, PR and Tian, Chao},
  journal={arXiv preprint arXiv:2111.00552},
  year={2021}
}

@inproceedings{zeng2022finite,
  title={Finite-time complexity of online primal-dual natural actor-critic algorithm for constrained Markov decision processes},
  author={Zeng, Sihan and Doan, Thinh T and Romberg, Justin},
  booktitle={2022 IEEE 61st Conference on Decision and Control (CDC)},
  pages={4028--4033},
  year={2022},
  organization={IEEE}
}

@article{ding2020natural,
  title={Natural policy gradient primal-dual method for constrained markov decision processes},
  author={Ding, Dongsheng and Zhang, Kaiqing and Basar, Tamer and Jovanovic, Mihailo},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={8378--8390},
  year={2020}
}

@inproceedings{xu2021crpo,
  title={Crpo: A new approach for safe reinforcement learning with convergence guarantee},
  author={Xu, Tengyu and Liang, Yingbin and Lan, Guanghui},
  booktitle={International Conference on Machine Learning},
  pages={11480--11491},
  year={2021},
  
}

@InProceedings{Chi2019, 
	title = {Provably efficient reinforcement learning with linear function approximation}, author = {Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I}, 
	booktitle = {Proceedings of Thirty Third Conference on Learning Theory}, 
	pages = {2137--2143}, 
	year = {2020}, 
	editor = {Jacob Abernethy and Shivani Agarwal}, 
	volume = {125}, 
	series = {Proceedings of Machine Learning Research}, 
	address = {}, 
	month = {09--12 Jul}, 
	publisher = {PMLR}, 
}

@article{ding2023convergence,
  title={Convergence and sample complexity of natural policy gradient primal-dual methods for constrained MDPs},
  author={Ding, Dongsheng and Zhang, Kaiqing and Duan, Jiali and Ba{\c{s}}ar, Tamer and Jovanovi{\'c}, Mihailo R},
  journal={arXiv preprint arXiv:2206.02346},
  year={2022}
}

@article{yuan2022linear,
  title={Linear convergence of natural policy gradient methods with log-linear policies},
  author={Yuan, Rui and Du, Simon S and Gower, Robert M and Lazaric, Alessandro and Xiao, Lin},
  journal={arXiv preprint arXiv:2210.01400},
  year={2022}
}

@article{cen2022fast,
  title={Fast global convergence of natural policy gradient methods with entropy regularization},
  author={Cen, Shicong and Cheng, Chen and Chen, Yuxin and Wei, Yuting and Chi, Yuejie},
  journal={Operations Research},
  volume={70},
  number={4},
  pages={2563--2578},
  year={2022},
  publisher={INFORMS}
}

@article{lan2023policy,
  title={Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes},
  author={Lan, Guanghui},
  journal={Mathematical programming},
  volume={198},
  number={1},
  pages={1059--1106},
  year={2023},
  publisher={Springer}
}

@article{zhan2023policy,
  title={Policy mirror descent for regularized reinforcement learning: A generalized framework with linear convergence},
  author={Zhan, Wenhao and Cen, Shicong and Huang, Baihe and Chen, Yuxin and Lee, Jason D and Chi, Yuejie},
  journal={SIAM Journal on Optimization},
  volume={33},
  number={2},
  pages={1061--1091},
  year={2023},
  publisher={SIAM}
}

@inproceedings{bhandari2021linear,
  title={On the linear convergence of policy gradient methods for finite mdps},
  author={Bhandari, Jalaj and Russo, Daniel},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2386--2394},
  year={2021},
  
}

@article{xiao2022convergence,
  title={On the convergence rates of policy gradient methods},
  author={Xiao, Lin},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={12887--12922},
  year={2022},
  publisher={JMLRORG}
}

@inproceedings{mei2020global,
  title={On the global convergence rates of softmax policy gradient methods},
  author={Mei, Jincheng and Xiao, Chenjun and Szepesvari, Csaba and Schuurmans, Dale},
  booktitle={International Conference on Machine Learning},
  pages={6820--6829},
  year={2020},
  
}

@inproceedings{shani2020adaptive,
  title={Adaptive trust region policy optimization: Global convergence and faster rates for regularized mdps},
  author={Shani, Lior and Efroni, Yonathan and Mannor, Shie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={5668--5675},
  year={2020}
}

@inproceedings{papini2018stochastic,
  title={Stochastic variance-reduced policy gradient},
  author={Papini, Matteo and Binaghi, Damiano and Canonaco, Giuseppe and Pirotta, Matteo and Restelli, Marcello},
  booktitle={International conference on machine learning},
  pages={4026--4035},
  year={2018},
  
}

@inproceedings{xu2020improved,
  title={An improved convergence analysis of stochastic variance-reduced policy gradient},
  author={Xu, Pan and Gao, Felicia and Gu, Quanquan},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={541--551},
  year={2020},
  
}

@inproceedings{xu2019sample,
  title={Sample Efficient Policy Gradient Methods with Recursive Variance Reduction},
  author={Xu, Pan and Gao, Felicia and Gu, Quanquan},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{pham2020hybrid,
  title={A hybrid stochastic policy gradient algorithm for reinforcement learning},
  author={Pham, Nhan and Nguyen, Lam and Phan, Dzung and Nguyen, Phuong Ha and Dijk, Marten and Tran-Dinh, Quoc},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={374--385},
  year={2020},
  
}

@inproceedings{gargiani2022page,
  title={PAGE-PG: A simple and loopless variance-reduced policy gradient method with probabilistic gradient estimation},
  author={Gargiani, Matilde and Zanelli, Andrea and Martinelli, Andrea and Summers, Tyler and Lygeros, John},
  booktitle={International Conference on Machine Learning},
  pages={7223--7240},
  year={2022},
  
}

@article{yuan2020stochastic,
  title={Stochastic recursive momentum for policy gradient methods},
  author={Yuan, Huizhuo and Lian, Xiangru and Liu, Ji and Zhou, Yuren},
  journal={arXiv preprint arXiv:2003.04302},
  year={2020}
}

@inproceedings{huang2020momentum,
  title={Momentum-based policy gradient methods},
  author={Huang, Feihu and Gao, Shangqian and Pei, Jian and Huang, Heng},
  booktitle={International conference on machine learning},
  pages={4422--4433},
  year={2020},
  
}

@inproceedings{shen2019hessian,
  title={Hessian aided policy gradient},
  author={Shen, Zebang and Ribeiro, Alejandro and Hassani, Hamed and Qian, Hui and Mi, Chao},
  booktitle={International conference on machine learning},
  pages={5729--5738},
  year={2019},
  
}

@article{salehkaleybar2022adaptive,
title={Momentum-Based Policy Gradient with Second-Order Information},
author={Saber Salehkaleybar and Mohammadsadegh Khorasani and Negar Kiyavash and Niao He and Patrick Thiran},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
}


@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@article{liu2020improved,
  title={An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods},
  author={Liu, Yanli and Zhang, Kaiqing and Basar, Tamer and Yin, Wotao},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7624--7636},
  year={2020}
}

@article{agarwal2021theory,
  title={On the theory of policy gradient methods: Optimality, approximation, and distribution shift},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={4431--4506},
  year={2021},
  publisher={JMLRORG}
}

@article{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@inproceedings{jain2018accelerating,
  title={Accelerating stochastic gradient descent for least squares regression},
  author={Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
  booktitle={Conference On Learning Theory},
  pages={545--604},
  year={2018},
  
}

@article{nesterov2012efficiency,
  title={Efficiency of coordinate descent methods on huge-scale optimization problems},
  author={Nesterov, Yu},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={2},
  pages={341--362},
  year={2012},
  publisher={SIAM}
}


@article{Mengdi2021,
  title={On the convergence and sample efficiency of variance-reduced policy gradient method},
  author={Zhang, Junyu and Ni, Chengzhuo and Szepesvari, Csaba and Wang, Mengdi },
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2228--2240},
  year={2021}
}


@InProceedings{Alekh2020, 
 	title = {Optimality and Approximation with Policy Gradient Methods in Markov Decision Processes}, 
 	author = {Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav}, 
 	booktitle = {Proceedings of Thirty Third Conference on Learning Theory}, 
 	pages = {64--66}, 
 	year = {2020}, 
 	editor = {Jacob Abernethy and Shivani Agarwal}, 
 	volume = {125}, 
 	series = {Proceedings of Machine Learning Research}, 
 	address = {}, 
 	month = {09--12 Jul}, 
 	publisher = {PMLR}, 
 	pdf = {http://proceedings.mlr.press/v125/agarwal20a/agarwal20a.pdf}, 
} 


@inproceedings{wang2019neural,
  title={Neural Policy Gradient Methods: Global Optimality and Rates of Convergence},
  author={Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{zhang2020global,
  title={Global convergence of policy gradient methods to (almost) locally optimal policies},
  author={Zhang, Kaiqing and Koppel, Alec and Zhu, Hao and Basar, Tamer},
  journal={SIAM Journal on Control and Optimization},
  volume={58},
  number={6},
  pages={3586--3612},
  year={2020},
  publisher={SIAM}
}

@inproceedings{bai2023achieving,
  title={Achieving zero constraint violation for constrained reinforcement learning via conservative natural policy gradient primal-dual algorithm},
  author={Bai, Qinbo and Bedi, Amrit Singh and Aggarwal, Vaneet},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={6},
  pages={6737--6744},
  year={2023}
}


@article{
mondal2023mean,
title={Mean-Field Control based Approximation of Multi-Agent Reinforcement Learning in Presence of a Non-decomposable Shared Global State},
author={Washim Uddin Mondal and Vaneet Aggarwal and Satish Ukkusuri},
journal={Transactions on Machine Learning Research},
year={2023},
note={}
}

@article{bach2013non,
  title={Non-strongly-convex smooth stochastic approximation with convergence rate O (1/n)},
  author={Bach, Francis and Moulines, Eric},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{jain2016parallelizing,
  author  = {Prateek Jain and Sham M. Kakade and Rahul Kidambi and Praneeth Netrapalli and Aaron Sidford},
  title   = {Parallelizing Stochastic Gradient Descent for Least Squares Regression: Mini-batching, Averaging, and Model Misspecification},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {223},
  pages   = {1--42}
}


@inproceedings{fatkhullin2023stochastic,
  title={Stochastic policy gradient methods: Improved sample complexity for fisher-non-degenerate policies},
  author={Fatkhullin, Ilyas and Barakat, Anas and Kireeva, Anastasia and He, Niao},
  booktitle={International Conference on Machine Learning},
  pages={9827--9869},
  year={2023}
}
@inproceedings{yuan2022general,
  title={A general sample complexity analysis of vanilla policy gradient},
  author={Yuan, Rui and Gower, Robert M and Lazaric, Alessandro},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3332--3380},
  year={2022},
  
}

@inproceedings{ding2022global,
  title={On the global optimum convergence of momentum-based policy gradient},
  author={Ding, Yuhao and Zhang, Junzi and Lavaei, Javad},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1910--1934},
  year={2022},
  
}

@article{masiha2022stochastic,
  title={Stochastic second-order methods improve best-known sample complexity of {SGD} for gradient-dominated functions},
  author={Masiha, Saeed and Salehkaleybar, Saber and He, Niao and Kiyavash, Negar and Thiran, Patrick},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={10862--10875},
  year={2022}
}

@article{ling2023cooperating,
  title={Cooperating Graph Neural Networks with Deep Reinforcement Learning for Vaccine Prioritization},
  author={Lu Ling and Washim Uddin Mondal and Satish V and Ukkusuri},
 journal={arXiv preprint arXiv:2305.05163},
  year={2023}
}

@article{singh2023maximize,
  title={How to maximize clicks for display advertisement in digital marketing? A reinforcement learning approach},
  author={Singh, Vinay and Nanavati, Brijesh and Kar, Arpan Kumar and Gupta, Agam},
  journal={Information Systems Frontiers},
  volume={25},
  number={4},
  pages={1621--1638},
  year={2023},
  publisher={Springer}
}
@article{gonzalez2023asap,
  title={Asap: A semi-autonomous precise system for telesurgery during communication delays},
  author={Gonzalez, Glebys and Balakuntala, Mythra and Agarwal, Mridul and Low, Tomas and Knoth, Bruce and Kirkpatrick, Andrew W and McKee, Jessica and Hager, Gregory and Aggarwal, Vaneet and Xue, Yexiang and others},
  journal={IEEE Transactions on Medical Robotics and Bionics},
  volume={5},
  number={1},
  pages={66--78},
  year={2023},
  publisher={IEEE}
}
@inproceedings{geng2020multi,
  title={A multi-agent reinforcement learning perspective on distributed traffic engineering},
  author={Geng, Nan and Lan, Tian and others},
  booktitle={2020 IEEE 28th International Conference on Network Protocols (ICNP)},
  pages={1--11},
  year={2020},
  organization={IEEE}
}
@article{al2019deeppool,
  title={Deeppool: Distributed model-free algorithm for ride-sharing using deep reinforcement learning},
  author={Al-Abbasi, Abubakr O and Ghosh, Arnob and Aggarwal, Vaneet},
  journal={IEEE Transactions on Intelligent Transportation Systems},
  volume={20},
  number={12},
  pages={4714--4727},
  year={2019},
  publisher={IEEE}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{chen2022finite,
  title={Finite-sample analysis of off-policy natural actor--critic with linear function approximation},
  author={Chen, Zaiwei and Khodadadian, Sajad and Maguluri, Siva Theja},
  journal={IEEE Control Systems Letters},
  volume={6},
  pages={2611--2616},
  year={2022},
  publisher={IEEE}
}

@inproceedings{chen2022sample,
  title={Sample complexity of policy-based methods under off-policy sampling and linear function approximation},
  author={Chen, Zaiwei and Maguluri, Siva Theja},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={11195--11214},
  year={2022},
  
}

@article{wu2020finite,
  title={A finite-time analysis of two time-scale actor-critic methods},
  author={Wu, Yue Frank and Zhang, Weitong and Xu, Pan and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17617--17628},
  year={2020}
}

@article{khodadadian2022finite,
  title={Finite sample analysis of two-time-scale natural actor-critic algorithm},
  author={Khodadadian, Sajad and Doan, Thinh T and Romberg, Justin and Maguluri, Siva Theja},
  journal={IEEE Transactions on Automatic Control},
  year={2022},
  publisher={IEEE}
}

@article{qiu2021finite,
  title={On finite-time convergence of actor-critic algorithm},
  author={Qiu, Shuang and Yang, Zhuoran and Ye, Jieping and Wang, Zhaoran},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={2},
  number={2},
  pages={652--664},
  year={2021},
  publisher={IEEE}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  
}

@article{bai2023regret,
  title={Regret Analysis of Policy Gradient Algorithm for Infinite Horizon Average Reward Markov Decision Processes},
  author={Bai, Qinbo and Mondal, Washim Uddin and Aggarwal, Vaneet},
  journal={arXiv preprint arXiv:2309.01922},
  year={2023}
}