\begin{thebibliography}{10}

\bibitem{al2019deeppool}
Al-Abbasi, A.~O., A.~Ghosh, V.~Aggarwal.
\newblock Deeppool: Distributed model-free algorithm for ride-sharing using deep reinforcement learning.
\newblock \emph{IEEE Transactions on Intelligent Transportation Systems}, 20(12):4714--4727, 2019.

\bibitem{geng2020multi}
Geng, N., T.~Lan, et~al.
\newblock A multi-agent reinforcement learning perspective on distributed traffic engineering.
\newblock In \emph{2020 IEEE 28th International Conference on Network Protocols (ICNP)}, pages 1--11. IEEE, 2020.

\bibitem{gonzalez2023asap}
Gonzalez, G., M.~Balakuntala, M.~Agarwal, et~al.
\newblock Asap: A semi-autonomous precise system for telesurgery during communication delays.
\newblock \emph{IEEE Transactions on Medical Robotics and Bionics}, 5(1):66--78, 2023.

\bibitem{bai2023achieving}
Bai, Q., A.~S. Bedi, V.~Aggarwal.
\newblock Achieving zero constraint violation for constrained reinforcement learning via conservative natural policy gradient primal-dual algorithm.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, vol.~37, pages 6737--6744. 2023.

\bibitem{ding2020natural}
Ding, D., K.~Zhang, T.~Basar, M.~Jovanovic.
\newblock Natural policy gradient primal-dual method for constrained markov decision processes.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:8378--8390, 2020.

\bibitem{xu2021crpo}
Xu, T., Y.~Liang, G.~Lan.
\newblock Crpo: A new approach for safe reinforcement learning with convergence guarantee.
\newblock In \emph{International Conference on Machine Learning}, pages 11480--11491. 2021.

\bibitem{liu2020improved}
Liu, Y., K.~Zhang, T.~Basar, W.~Yin.
\newblock An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:7624--7636, 2020.

\bibitem{jain2018accelerating}
Jain, P., S.~M. Kakade, R.~Kidambi, P.~Netrapalli, A.~Sidford.
\newblock Accelerating stochastic gradient descent for least squares regression.
\newblock In \emph{Conference On Learning Theory}, pages 545--604. 2018.

\bibitem{liu2021policy}
Liu, T., R.~Zhou, D.~Kalathil, P.~Kumar, C.~Tian.
\newblock Policy optimization for constrained mdps with provable fast global convergence.
\newblock \emph{arXiv preprint arXiv:2111.00552}, 2021.

\bibitem{zeng2022finite}
Zeng, S., T.~T. Doan, J.~Romberg.
\newblock Finite-time complexity of online primal-dual natural actor-critic algorithm for constrained markov decision processes.
\newblock In \emph{2022 IEEE 61st Conference on Decision and Control (CDC)}, pages 4028--4033. IEEE, 2022.

\bibitem{vaswani2022near}
Vaswani, S., L.~Yang, C.~Szepesv{\'a}ri.
\newblock Near-optimal sample complexity bounds for constrained mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:3110--3122, 2022.

\bibitem{agarwal2021theory}
Agarwal, A., S.~M. Kakade, J.~D. Lee, G.~Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation, and distribution shift.
\newblock \emph{The Journal of Machine Learning Research}, 22(1):4431--4506, 2021.

\bibitem{bhandari2021linear}
Bhandari, J., D.~Russo.
\newblock On the linear convergence of policy gradient methods for finite mdps.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 2386--2394. 2021.

\bibitem{cen2022fast}
Cen, S., C.~Cheng, Y.~Chen, Y.~Wei, Y.~Chi.
\newblock Fast global convergence of natural policy gradient methods with entropy regularization.
\newblock \emph{Operations Research}, 70(4):2563--2578, 2022.

\bibitem{lan2023policy}
Lan, G.
\newblock Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes.
\newblock \emph{Mathematical programming}, 198(1):1059--1106, 2023.

\bibitem{zhan2023policy}
Zhan, W., S.~Cen, B.~Huang, Y.~Chen, J.~D. Lee, Y.~Chi.
\newblock Policy mirror descent for regularized reinforcement learning: A generalized framework with linear convergence.
\newblock \emph{SIAM Journal on Optimization}, 33(2):1061--1091, 2023.

\bibitem{xu2020improved}
Xu, P., F.~Gao, Q.~Gu.
\newblock An improved convergence analysis of stochastic variance-reduced policy gradient.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 541--551. 2020.

\bibitem{gargiani2022page}
Gargiani, M., A.~Zanelli, A.~Martinelli, T.~Summers, J.~Lygeros.
\newblock Page-pg: A simple and loopless variance-reduced policy gradient method with probabilistic gradient estimation.
\newblock In \emph{International Conference on Machine Learning}, pages 7223--7240. 2022.

\bibitem{huang2020momentum}
Huang, F., S.~Gao, J.~Pei, H.~Huang.
\newblock Momentum-based policy gradient methods.
\newblock In \emph{International conference on machine learning}, pages 4422--4433. 2020.

\bibitem{salehkaleybar2022adaptive}
Salehkaleybar, S., M.~Khorasani, N.~Kiyavash, N.~He, P.~Thiran.
\newblock Momentum-based policy gradient with second-order information.
\newblock \emph{Transactions on Machine Learning Research}, 2024.

\bibitem{shen2019hessian}
Shen, Z., A.~Ribeiro, H.~Hassani, H.~Qian, C.~Mi.
\newblock Hessian aided policy gradient.
\newblock In \emph{International conference on machine learning}, pages 5729--5738. 2019.

\bibitem{chen2022finite}
Chen, Z., S.~Khodadadian, S.~T. Maguluri.
\newblock Finite-sample analysis of off-policy natural actor--critic with linear function approximation.
\newblock \emph{IEEE Control Systems Letters}, 6:2611--2616, 2022.

\bibitem{chen2022sample}
Chen, Z., S.~T. Maguluri.
\newblock Sample complexity of policy-based methods under off-policy sampling and linear function approximation.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 11195--11214. 2022.

\bibitem{khodadadian2022finite}
Khodadadian, S., T.~T. Doan, J.~Romberg, S.~T. Maguluri.
\newblock Finite sample analysis of two-time-scale natural actor-critic algorithm.
\newblock \emph{IEEE Transactions on Automatic Control}, 2022.

\bibitem{fatkhullin2023stochastic}
Fatkhullin, I., A.~Barakat, A.~Kireeva, N.~He.
\newblock Stochastic policy gradient methods: Improved sample complexity for fisher-non-degenerate policies.
\newblock In \emph{International Conference on Machine Learning}, pages 9827--9869. 2023.

\bibitem{masiha2022stochastic}
Masiha, S., S.~Salehkaleybar, N.~He, N.~Kiyavash, P.~Thiran.
\newblock Stochastic second-order methods improve best-known sample complexity of {SGD} for gradient-dominated functions.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:10862--10875, 2022.

\bibitem{mondal2024improved}
Mondal, W.~U., V.~Aggarwal.
\newblock Improved sample complexity analysis of natural policy gradient algorithm with general parameterization for infinite horizon discounted reward {M}arkov decision processes.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 3097--3105. 2024.

\bibitem{efroni2020exploration}
Efroni, Y., S.~Mannor, M.~Pirotta.
\newblock Exploration-exploitation in constrained mdps.
\newblock \emph{arXiv preprint arXiv:2003.02189}, 2020.

\bibitem{liu2021learning}
Liu, T., R.~Zhou, D.~Kalathil, P.~Kumar, C.~Tian.
\newblock Learning policies with zero or bounded constraint violation for constrained mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:17183--17193, 2021.

\bibitem{ding2021provably}
Ding, D., X.~Wei, Z.~Yang, Z.~Wang, M.~Jovanovic.
\newblock Provably efficient safe exploration via primal-dual policy optimization.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 3304--3312. 2021.

\bibitem{he2021nearly}
He, J., D.~Zhou, Q.~Gu.
\newblock Nearly minimax optimal reinforcement learning for discounted mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:22288--22300, 2021.

\bibitem{wei2021provably}
Wei, H., X.~Liu, L.~Ying.
\newblock Triple-q: A model-free algorithm for constrained reinforcement learning with sublinear regret and zero constraint violation.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 3274--3307. 2022.

\bibitem{bai2022achieving}
Bai, Q., A.~S. Bedi, M.~Agarwal, A.~Koppel, V.~Aggarwal.
\newblock Achieving zero constraint violation for constrained reinforcement learning via primal-dual approach.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, vol.~36, pages 3682--3689. 2022.

\bibitem{sutton1999policy}
Sutton, R.~S., D.~McAllester, S.~Singh, Y.~Mansour.
\newblock Policy gradient methods for reinforcement learning with function approximation.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem{Mengdi2021}
Zhang, J., C.~Ni, C.~Szepesvari, M.~Wang.
\newblock On the convergence and sample efficiency of variance-reduced policy gradient method.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:2228--2240, 2021.

\bibitem{Chi2019}
Jin, C., Z.~Yang, Z.~Wang, M.~I. Jordan.
\newblock Provably efficient reinforcement learning with linear function approximation.
\newblock In J.~Abernethy, S.~Agarwal, eds., \emph{Proceedings of Thirty Third Conference on Learning Theory}, vol. 125 of \emph{Proceedings of Machine Learning Research}, pages 2137--2143. PMLR, 2020.

\bibitem{wang2019neural}
Wang, L., Q.~Cai, Z.~Yang, Z.~Wang.
\newblock Neural policy gradient methods: Global optimality and rates of convergence.
\newblock In \emph{International Conference on Learning Representations}. 2019.

\bibitem{zhang2020global}
Zhang, K., A.~Koppel, H.~Zhu, T.~Basar.
\newblock Global convergence of policy gradient methods to (almost) locally optimal policies.
\newblock \emph{SIAM Journal on Control and Optimization}, 58(6):3586--3612, 2020.

\bibitem{mondal2023mean}
Mondal, W.~U., V.~Aggarwal, S.~Ukkusuri.
\newblock Mean-field control based approximation of multi-agent reinforcement learning in presence of a non-decomposable shared global state.
\newblock \emph{Transactions on Machine Learning Research}, 2023.

\bibitem{ding2023convergence}
Ding, D., K.~Zhang, J.~Duan, T.~Ba{\c{s}}ar, M.~R. Jovanovi{\'c}.
\newblock Convergence and sample complexity of natural policy gradient primal-dual methods for constrained mdps.
\newblock \emph{arXiv preprint arXiv:2206.02346}, 2022.

\end{thebibliography}
