
  @article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Gemini, Team},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}
@article{openai2023gpt,
  title={Gpt-4 technical report. arxiv 2303.08774},
  author={OpenAI, R},
  journal={View in Article},
  volume={2},
  number={5},
  year={2023}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@article{team2024gemma2,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

@article{velivckovic2024softmax,
  title={softmax is not enough (for sharp out-of-distribution)},
  author={Veli{\v{c}}kovi{\'c}, Petar and Perivolaropoulos, Christos and Barbero, Federico and Pascanu, Razvan},
  journal={arXiv preprint arXiv:2410.01104},
  year={2024}
}

@article{barbero2024round,
  title={Round and Round We Go! What makes Rotary Positional Encodings useful?},
  author={Barbero, Federico and Vitvitskyi, Alex and Perivolaropoulos, Christos and Pascanu, Razvan and Veli{\v{c}}kovi{\'c}, Petar},
  journal={arXiv preprint arXiv:2410.06205},
  year={2024}
}

@article{kazemnejad2024impact,
  title={The impact of positional encoding on length generalization in transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}
@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{perez2021attention,
  title={Attention is turing-complete},
  author={P{\'e}rez, Jorge and Barcel{\'o}, Pablo and Marinkovic, Javier},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={75},
  pages={1--35},
  year={2021}
}

@article{team2024gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

@article{su2024roformer,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{pillai2005perron,
  title={The Perron-Frobenius theorem: some of its applications},
  author={Pillai, S Unnikrishna and Suel, Torsten and Cha, Seunghun},
  journal={IEEE Signal Processing Magazine},
  volume={22},
  number={2},
  pages={62--75},
  year={2005},
  publisher={IEEE}
}

@inproceedings{zhai2023stabilizing,
  title={Stabilizing transformer training by preventing attention entropy collapse},
  author={Zhai, Shuangfei and Likhomanenko, Tatiana and Littwin, Etai and Busbridge, Dan and Ramapuram, Jason and Zhang, Yizhe and Gu, Jiatao and Susskind, Joshua M},
  booktitle={International Conference on Machine Learning},
  pages={40770--40803},
  year={2023},
  organization={PMLR}
}

@article{hahn2020theoretical,
  title={Theoretical limitations of self-attention in neural sequence models},
  author={Hahn, Michael},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={156--171},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{xiong2020layer,
  title={On layer normalization in the transformer architecture},
  author={Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng, Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={10524--10533},
  year={2020},
  organization={PMLR}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{topping2021understanding,
  title={Understanding over-squashing and bottlenecks on graphs via curvature},
  author={Topping, Jake and Di Giovanni, Francesco and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M},
  journal={International Conference on Learning Representations},
  year={2022}
}

@inproceedings{di2023over,
  title={On over-squashing in message passing neural networks: The impact of width, depth, and topology},
  author={Di Giovanni, Francesco and Giusti, Lorenzo and Barbero, Federico and Luise, Giulia and Lio, Pietro and Bronstein, Michael M},
  booktitle={International Conference on Machine Learning},
  pages={7865--7885},
  year={2023},
  organization={PMLR}
}

@inproceedings{
barbero2024localityaware,
title={Locality-Aware Graph Rewiring in {GNN}s},
author={Federico Barbero and Ameya Velingker and Amin Saberi and Michael M. Bronstein and Francesco Di Giovanni},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=4Ua4hKiAJX}
}

@article{
giovanni2024how,
title={How does over-squashing affect the power of {GNN}s?},
author={Francesco Di Giovanni and T. Konstantin Rusch and Michael Bronstein and Andreea Deac and Marc Lackenby and Siddhartha Mishra and Petar Veli{\v{c}}kovi{\'c}},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=KJRoQvRWNs},
note={}
}

@inproceedings{
alon2021on,
title={On the Bottleneck of Graph Neural Networks and its Practical Implications},
author={Uri Alon and Eran Yahav},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=i80OPhOCVH2}
}

@article{merrill2023expresssive,
  title={The expresssive power of transformers with chain of thought},
  author={Merrill, William and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:2310.07923},
  year={2023}
}

@article{chiang2022overcoming,
  title={Overcoming a theoretical limitation of self-attention},
  author={Chiang, David and Cholak, Peter},
  journal={arXiv preprint arXiv:2202.12172},
  year={2022}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@inproceedings{dudzik2024asynchronous,
  title={Asynchronous algorithmic alignment with cocycles},
  author={Dudzik, Andrew Joseph and von Glehn, Tamara and Pascanu, Razvan and Veli{\v{c}}kovi{\'c}, Petar},
  booktitle={Learning on Graphs Conference},
  pages={3--1},
  year={2024},
  organization={PMLR}
}

@article{schick2024toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@misc{peng2024limitations,
      title={On Limitations of the Transformer Architecture}, 
      author={Binghui Peng and Srini Narayanan and Christos Papadimitriou},
      year={2024},
      eprint={2402.08164},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{dehghani2019universal,
      title={Universal Transformers}, 
      author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Łukasz Kaiser},
      year={2019},
      eprint={1807.03819},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{deletang2023neural,
      title={Neural Networks and the Chomsky Hierarchy}, 
      author={Grégoire Delétang and Anian Ruoss and Jordi Grau-Moya and Tim Genewein and Li Kevin Wenliang and Elliot Catt and Chris Cundy and Marcus Hutter and Shane Legg and Joel Veness and Pedro A. Ortega},
      year={2023},
      eprint={2207.02098},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{weiss2021thinking,
      title={Thinking Like Transformers}, 
      author={Gail Weiss and Yoav Goldberg and Eran Yahav},
      year={2021},
      eprint={2106.06981},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Article{HochSchm97,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9}
}

@ARTICLE{bengio94,
  author={Bengio, Y. and Simard, P. and Frasconi, P.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Learning long-term dependencies with gradient descent is difficult}, 
  year={1994},
  volume={5},
  number={2},
  pages={157-166}
}

@misc{herranzcelotti2024stabilizing,
      title={Stabilizing RNN Gradients through Pre-training}, 
      author={Luca Herranz-Celotti and Jean Rouat},
      year={2024},
      eprint={2308.12075},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@InProceedings{pascanu13,
  title = 	 {On the difficulty of training recurrent neural networks},
  author = 	 {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1310--1318},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},


}
