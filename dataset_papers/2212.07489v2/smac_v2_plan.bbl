\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[tor(2018)]{torchcraftAIGithub}
Torchcraftai: A bot platform for machine learning research on starcraft: Brood
  war.
\newblock \url{https://github.com/TorchCraft/TorchCraftAI}, 2018.
\newblock GitHub repository.

\bibitem[Bard et~al.(2020)Bard, Foerster, Chandar, Burch, Lanctot, Song,
  Parisotto, Dumoulin, Moitra, Hughes, et~al.]{bard2020hanabi}
Nolan Bard, Jakob~N Foerster, Sarath Chandar, Neil Burch, Marc Lanctot,
  H~Francis Song, Emilio Parisotto, Vincent Dumoulin, Subhodeep Moitra, Edward
  Hughes, et~al.
\newblock The hanabi challenge: A new frontier for ai research.
\newblock \emph{Artificial Intelligence}, 280:\penalty0 103216, 2020.

\bibitem[Bernstein et~al.(2002)Bernstein, Givan, Immerman, and
  Zilberstein]{bernstein2002complexity}
Daniel~S Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein.
\newblock The complexity of decentralized control of markov decision processes.
\newblock \emph{Mathematics of operations research}, 27\penalty0 (4):\penalty0
  819--840, 2002.

\bibitem[Carroll et~al.(2019)Carroll, Shah, Ho, Griffiths, Seshia, Abbeel, and
  Dragan]{overcooked}
Micah Carroll, Rohin Shah, Mark~K Ho, Tom Griffiths, Sanjit Seshia, Pieter
  Abbeel, and Anca Dragan.
\newblock On the utility of learning about humans for human-ai coordination.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/f5b1b89d98b7286673128a5fb112cb9a-Paper.pdf}.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems}, 34, 2021.

\bibitem[Chevalier-Boisvert et~al.(2018)Chevalier-Boisvert, Willems, and
  Pal]{gym_minigrid}
Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal.
\newblock Minimalistic gridworld environment for {O}pen{AI} {G}ym.
\newblock \url{https://github.com/maximecb/gym-minigrid}, 2018.

\bibitem[Cobbe et~al.(2020)Cobbe, Hesse, Hilton, and Schulman]{procgen}
Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pages 2048--2056, 2020.

\bibitem[de~Witt et~al.(2020)de~Witt, Gupta, Makoviichuk, Makoviychuk, Torr,
  Sun, and Whiteson]{de2020independent}
Christian~Schroeder de~Witt, Tarun Gupta, Denys Makoviichuk, Viktor
  Makoviychuk, Philip~HS Torr, Mingfei Sun, and Shimon Whiteson.
\newblock Is independent learning all you need in the starcraft multi-agent
  challenge?
\newblock \emph{arXiv preprint arXiv:2011.09533}, 2020.

\bibitem[Du et~al.(2019)Du, Han, Fang, Liu, Dai, and Tao]{du2019liir}
Yali Du, Lei Han, Meng Fang, Ji~Liu, Tianhong Dai, and Dacheng Tao.
\newblock Liir: Learning individual intrinsic reward in multi-agent
  reinforcement learning.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d'Alch'e Buc,
  E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural Information
  Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/07a9d3fed4c5ea6b17e80258dee231fa-Paper.pdf}.

\bibitem[Foerster et~al.(2016)Foerster, Assael, de~Freitas, and
  Whiteson]{foerster2016learning}
Jakob Foerster, Ioannis~Alexandros Assael, Nando de~Freitas, and Shimon
  Whiteson.
\newblock Learning to communicate with deep multi-agent reinforcement learning.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2016/file/c7635bfd99248a2cdef8249ef7bfbef4-Paper.pdf}.

\bibitem[Gorsane et~al.(2022)Gorsane, Mahjoub, de~Kock, Dubb, Singh, and
  Pretorius]{gorsane2022towards}
Rihab Gorsane, Omayma Mahjoub, Ruan de~Kock, Roland Dubb, Siddarth Singh, and
  Arnu Pretorius.
\newblock Towards a standardised performance evaluation protocol for
  cooperative marl.
\newblock \emph{arXiv preprint arXiv:2209.10485}, 2022.

\bibitem[Gupta et~al.(2021)Gupta, Mahajan, Peng, B{\"o}hmer, and
  Whiteson]{gupta2021uneven}
Tarun Gupta, Anuj Mahajan, Bei Peng, Wendelin B{\"o}hmer, and Shimon Whiteson.
\newblock Uneven: Universal value exploration for multi-agent reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3930--3941. PMLR, 2021.

\bibitem[Hu et~al.(2021)Hu, Jiang, Harding, Wu, and Liao]{hu2021rethinking}
Jian Hu, Siyang Jiang, Seth~Austin Harding, Haibin Wu, and SW~Liao.
\newblock Rethinking the implementation tricks and monotonicity constraint in
  cooperative multi-agent reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2102.03479}, 2021.

\bibitem[Hu et~al.(2020)Hu, Zhu, Chang, and Liang]{hu2020updet}
Siyi Hu, Fengda Zhu, Xiaojun Chang, and Xiaodan Liang.
\newblock Updet: Universal multi-agent rl via policy decoupling with
  transformers.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Iqbal et~al.(2021)Iqbal, De~Witt, Peng, Boehmer, Whiteson, and
  Sha]{iqbal2021randomized}
Shariq Iqbal, Christian A~Schroeder De~Witt, Bei Peng, Wendelin Boehmer, Shimon
  Whiteson, and Fei Sha.
\newblock Randomized entity-wise factorization for multi-agent reinforcement
  learning.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 4596--4606. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/iqbal21a.html}.

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{janner2021offline}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock \emph{Advances in neural information processing systems}, 34, 2021.

\bibitem[Juliani et~al.(2019)Juliani, Khalifa, Berges, Harper, Teng, Henry,
  Crespi, Togelius, and Lange]{obstacletower}
Arthur Juliani, Ahmed Khalifa, Vincent{-}Pierre Berges, Jonathan Harper, Ervin
  Teng, Hunter Henry, Adam Crespi, Julian Togelius, and Danny Lange.
\newblock {Obstacle Tower: {A} Generalization Challenge in Vision, Control, and
  Planning}.
\newblock In \emph{IJCAI}, 2019.

\bibitem[Justesen et~al.(2018)Justesen, Torrado, Bontrager, Khalifa, Togelius,
  and Risi]{pcg_illuminating}
Niels Justesen, Ruben~Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa,
  Julian Togelius, and Sebastian Risi.
\newblock Procedural level generation improves generality of deep reinforcement
  learning.
\newblock \emph{CoRR}, abs/1806.10729, 2018.

\bibitem[Kirk et~al.(2021)Kirk, Zhang, Grefenstette, and
  Rockt{\"a}schel]{kirk2022survey}
Robert Kirk, Amy Zhang, Edward Grefenstette, and Tim Rockt{\"a}schel.
\newblock A survey of generalisation in deep reinforcement learning.
\newblock \emph{arXiv e-prints}, pages arXiv--2111, 2021.

\bibitem[Kurach et~al.(2020)Kurach, Raichuk, Sta{\'n}czyk, Zaj{\k{a}}c, Bachem,
  Espeholt, Riquelme, Vincent, Michalski, Bousquet, et~al.]{kurach2020google}
Karol Kurach, Anton Raichuk, Piotr Sta{\'n}czyk, Micha{\l} Zaj{\k{a}}c, Olivier
  Bachem, Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski,
  Olivier Bousquet, et~al.
\newblock Google research football: A novel reinforcement learning environment.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 4501--4510, 2020.

\bibitem[K{\"{u}}ttler et~al.(2020)K{\"{u}}ttler, Nardelli, Miller, Raileanu,
  Selvatici, Grefenstette, and Rockt{\"{a}}schel]{kuettler2020nethack}
Heinrich K{\"{u}}ttler, Nantas Nardelli, Alexander~H. Miller, Roberta Raileanu,
  Marco Selvatici, Edward Grefenstette, and Tim Rockt{\"{a}}schel.
\newblock {The NetHack Learning Environment}.
\newblock In \emph{Proceedings of the Conference on Neural Information
  Processing Systems (NeurIPS)}, 2020.

\bibitem[Lanctot et~al.(2019)Lanctot, Lockhart, Lespiau, Zambaldi, Upadhyay,
  P\'{e}rolat, Srinivasan, Timbers, Tuyls, Omidshafiei, Hennes, Morrill,
  Muller, Ewalds, Faulkner, Kram\'{a}r, Vylder, Saeta, Bradbury, Ding,
  Borgeaud, Lai, Schrittwieser, Anthony, Hughes, Danihelka, and
  Ryan-Davis]{LanctotEtAl2019OpenSpiel}
Marc Lanctot, Edward Lockhart, Jean-Baptiste Lespiau, Vinicius Zambaldi,
  Satyaki Upadhyay, Julien P\'{e}rolat, Sriram Srinivasan, Finbarr Timbers,
  Karl Tuyls, Shayegan Omidshafiei, Daniel Hennes, Dustin Morrill, Paul Muller,
  Timo Ewalds, Ryan Faulkner, J\'{a}nos Kram\'{a}r, Bart~De Vylder, Brennan
  Saeta, James Bradbury, David Ding, Sebastian Borgeaud, Matthew Lai, Julian
  Schrittwieser, Thomas Anthony, Edward Hughes, Ivo Danihelka, and Jonah
  Ryan-Davis.
\newblock {OpenSpiel}: A framework for reinforcement learning in games.
\newblock \emph{CoRR}, abs/1908.09453, 2019.
\newblock URL \url{http://arxiv.org/abs/1908.09453}.

\bibitem[Leibo et~al.(2017)Leibo, Zambaldi, Lanctot, Marecki, and
  Graepel]{leibo17ssd}
Joel~Z. Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore
  Graepel.
\newblock Multi-agent reinforcement learning in sequential social dilemmas,
  2017.
\newblock URL \url{https://arxiv.org/abs/1702.03037}.

\bibitem[Leibo et~al.(2021)Leibo, nez Guzm\'an, Vezhnevets, Agapiou, Sunehag,
  Koster, Matyas, Beattie, Mordatch, and Graepel]{leibo2021meltingpot}
Joel~Z. Leibo, Edgar~Du\'e\ nez Guzm\'an, Alexander~Sasha Vezhnevets, John~P.
  Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charles Beattie, Igor
  Mordatch, and Thore Graepel.
\newblock Scalable evaluation of multi-agent reinforcement learning with
  melting pot.
\newblock PMLR, 2021.

\bibitem[Lowe et~al.(2017)Lowe, Tamar, Harb, Pieter~Abbeel, and
  Mordatch]{lowe2017multiagent}
Ryan Lowe, Aviv Tamar, Jean Harb, OpenAI Pieter~Abbeel, and Igor Mordatch.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Mahajan et~al.(2019)Mahajan, Rashid, Samvelyan, and
  Whiteson]{mahajan2019maven}
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson.
\newblock Maven: Multi-agent variational exploration.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Mahajan et~al.(2021)Mahajan, Samvelyan, Mao, Makoviychuk, Garg,
  Kossaifi, Whiteson, Zhu, and Anandkumar]{mahajan2021tesseract}
Anuj Mahajan, Mikayel Samvelyan, Lei Mao, Viktor Makoviychuk, Animesh Garg,
  Jean Kossaifi, Shimon Whiteson, Yuke Zhu, and Animashree Anandkumar.
\newblock Tesseract: Tensorised actors for multi-agent reinforcement learning.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 7301--7312. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/mahajan21a.html}.

\bibitem[Mahajan et~al.(2022)Mahajan, Samvelyan, Gupta, Ellis, Sun,
  Rockt{\"a}schel, and Whiteson]{mahajan2022generalization}
Anuj Mahajan, Mikayel Samvelyan, Tarun Gupta, Benjamin Ellis, Mingfei Sun, Tim
  Rockt{\"a}schel, and Shimon Whiteson.
\newblock Generalization in cooperative multi-agent systems.
\newblock \emph{arXiv preprint arXiv:2202.00104}, 2022.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Oliehoek and Amato(2016)]{olihoek2016decpomdps}
Frans~A. Oliehoek and Christopher Amato.
\newblock \emph{A Concise Introduction to Decentralized POMDPs}.
\newblock Springer Publishing Company, Incorporated, 1st edition, 2016.
\newblock ISBN 3319289276.

\bibitem[Peng et~al.(2021)Peng, Rashid, de~Witt, Kamienny, Torr, Boehmer, and
  Whiteson]{peng2021facmac}
Bei Peng, Tabish Rashid, Christian~Schroeder de~Witt, Pierre-Alexandre
  Kamienny, Philip Torr, Wendelin Boehmer, and Shimon Whiteson.
\newblock {FACMAC}: Factored multi-agent centralised policy gradients.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan,
  editors, \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=WxH774N0mEu}.

\bibitem[Rashid et~al.(2018)Rashid, Samvelyan, Schroeder, Farquhar, Foerster,
  and Whiteson]{rashid2018qmix}
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob
  Foerster, and Shimon Whiteson.
\newblock Qmix: Monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4295--4304. PMLR, 2018.

\bibitem[Rashid et~al.(2020)Rashid, Farquhar, Peng, and
  Whiteson]{rashid2020weighted}
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson.
\newblock Weighted qmix: Expanding monotonic value function factorisation for
  deep multi-agent reinforcement learning.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 10199--10210. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/73a427badebe0e32caa2e1fc7530b7f3-Paper.pdf}.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov,
  Barth-Maron, Gimenez, Sulsky, Kay, Springenberg, et~al.]{reed2022generalist}
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio~Gomez Colmenarejo, Alexander
  Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay,
  Jost~Tobias Springenberg, et~al.
\newblock A generalist agent.
\newblock \emph{arXiv preprint arXiv:2205.06175}, 2022.

\bibitem[Resnick et~al.(2018)Resnick, Eldridge, Ha, Britz, Foerster, Togelius,
  Cho, and Bruna]{resnick2018pommerman}
Cinjon Resnick, Wes Eldridge, David Ha, Denny Britz, Jakob Foerster, Julian
  Togelius, Kyunghyun Cho, and Joan Bruna.
\newblock Pommerman: A multi-agent playground.
\newblock \emph{arXiv preprint arXiv:1809.07124}, 2018.

\bibitem[Risi and Togelius(2020)]{pcg}
Sebastian Risi and Julian Togelius.
\newblock Increasing generality in machine learning through procedural content
  generation.
\newblock \emph{Nature Machine Intelligence}, 2, 08 2020.
\newblock \doi{10.1038/s42256-020-0208-z}.

\bibitem[Samvelyan et~al.(2019)Samvelyan, Rashid, Schroeder~de Witt, Farquhar,
  Nardelli, Rudner, Hung, Torr, Foerster, and Whiteson]{samvelyan2019starcraft}
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder~de Witt, Gregory
  Farquhar, Nantas Nardelli, Tim~GJ Rudner, Chia-Man Hung, Philip~HS Torr,
  Jakob Foerster, and Shimon Whiteson.
\newblock The starcraft multi-agent challenge.
\newblock In \emph{Proceedings of the 18th International Conference on
  Autonomous Agents and MultiAgent Systems}, pages 2186--2188, 2019.

\bibitem[Samvelyan et~al.(2021)Samvelyan, Kirk, Kurin, Parker-Holder, Jiang,
  Hambro, Petroni, Kuttler, Grefenstette, and
  Rockt{\"a}schel]{samvelyan2021minihack}
Mikayel Samvelyan, Robert Kirk, Vitaly Kurin, Jack Parker-Holder, Minqi Jiang,
  Eric Hambro, Fabio Petroni, Heinrich Kuttler, Edward Grefenstette, and Tim
  Rockt{\"a}schel.
\newblock Minihack the planet: A sandbox for open-ended reinforcement learning
  research.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2021.

\bibitem[Schmidhuber(2019)]{schmidhuber2019reinforcement}
Juergen Schmidhuber.
\newblock Reinforcement learning upside down: Don't predict rewards--just map
  them to actions.
\newblock \emph{arXiv preprint arXiv:1912.02875}, 2019.

\bibitem[Son et~al.(2019)Son, Kim, Kang, Hostallero, and Yi]{son2019qtran}
Kyunghwan Son, Daewoo Kim, Wan~Ju Kang, David~Earl Hostallero, and Yung Yi.
\newblock Qtran: Learning to factorize with transformation for cooperative
  multi-agent reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5887--5896. PMLR, 2019.

\bibitem[Suarez et~al.(2019)Suarez, Du, Isola, and Mordatch]{neural_mmo}
Joseph Suarez, Yilun Du, Phillip Isola, and Igor Mordatch.
\newblock Neural mmo: A massively multiagent game environment for training and
  evaluating intelligent agents.
\newblock \emph{arXiv preprint arXiv:1903.00784}, 2019.

\bibitem[Sun et~al.(2022)Sun, Kurin, Liu, Devlin, Qin, Hofmann, and
  Whiteson]{sun2022you}
Mingfei Sun, Vitaly Kurin, Guoqing Liu, Sam Devlin, Tao Qin, Katja Hofmann, and
  Shimon Whiteson.
\newblock You may not need ratio clipping in ppo.
\newblock \emph{arXiv preprint arXiv:2202.00079}, 2022.

\bibitem[Sun et~al.(2023)Sun, Devlin, Beck, Hofmann, and
  Whiteson]{sun2022monotonic}
Mingfei Sun, Sam Devlin, Jacob Beck, Katja Hofmann, and Shimon Whiteson.
\newblock Trust region bounds for decentralized ppo under non-stationarity.
\newblock In \emph{Proceedings of the 2023 International Conference on
  Autonomous Agents and Multiagent Systems}, AAMAS ’23, page 5–13,
  Richland, SC, 2023. International Foundation for Autonomous Agents and
  Multiagent Systems.
\newblock ISBN 9781450394321.

\bibitem[Sunehag et~al.(2018)Sunehag, Lever, Gruslys, Czarnecki, Zambaldi,
  Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, et~al.]{sunehag2017vdn}
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech~Marian Czarnecki, Vinicius
  Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel~Z Leibo, Karl
  Tuyls, et~al.
\newblock Value-decomposition networks for cooperative multi-agent learning
  based on team reward.
\newblock In \emph{Proceedings of the 17th International Conference on
  Autonomous Agents and MultiAgent Systems}, pages 2085--2087, 2018.

\bibitem[Synnaeve et~al.(2016)Synnaeve, Nardelli, Auvolat, Chintala, Lacroix,
  Lin, Richoux, and Usunier]{torchcraft}
Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala, Timothée
  Lacroix, Zeming Lin, Florian Richoux, and Nicolas Usunier.
\newblock Torchcraft: a library for machine learning research on real-time
  strategy games, 2016.
\newblock URL \url{https://arxiv.org/abs/1611.00625}.

\bibitem[Terry et~al.(2021)Terry, Black, Grammel, Jayakumar, Hari, Sullivan,
  Santos, Dieffendahl, Horsch, Perez-Vicente, et~al.]{terry2021pettingzoo}
Justin~K Terry, Benjamin Black, Nathaniel Grammel, Mario Jayakumar, Ananth
  Hari, Ryan Sullivan, Luis Santos, Clemens Dieffendahl, Caroline Horsch,
  Rodrigo Perez-Vicente, et~al.
\newblock Pettingzoo: Gym for multi-agent reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Vinitsky et~al.(2019)Vinitsky, Jaques, Leibo, Castenada, and
  Hughes]{SSDOpenSource}
Eugene Vinitsky, Natasha Jaques, Joel Leibo, Antonio Castenada, and Edward
  Hughes.
\newblock An open source implementation of sequential social dilemma games.
\newblock
  \url{https://github.com/eugenevinitsky/sequential_social_dilemma_games/issues/182},
  2019.
\newblock GitHub repository.

\bibitem[Vinyals et~al.(2017)Vinyals, Ewalds, Bartunov, Georgiev, Vezhnevets,
  Yeo, Makhzani, Küttler, Agapiou, Schrittwieser, Quan, Gaffney, Petersen,
  Simonyan, Schaul, van Hasselt, Silver, Lillicrap, Calderone, Keet, Brunasso,
  Lawrence, Ekermo, Repp, and Tsing]{sc2le}
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander~Sasha
  Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou,
  Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen
  Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy Lillicrap,
  Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo,
  Jacob Repp, and Rodney Tsing.
\newblock Starcraft ii: A new challenge for reinforcement learning, 2017.
\newblock URL \url{https://arxiv.org/abs/1708.04782}.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, Oh, Horgan, Kroiss, Danihelka, Huang,
  Sifre, Cai, Agapiou, Jaderberg, Vezhnevets, Leblond, Pohlen, Dalibard,
  Budden, Sulsky, Molloy, Paine, G{\"{u}}l{\c{c}}ehre, Wang, Pfaff, Wu, Ring,
  Yogatama, W{\"{u}}nsch, McKinney, Smith, Schaul, Lillicrap, Kavukcuoglu,
  Hassabis, Apps, and Silver]{alphastar}
Oriol Vinyals, Igor Babuschkin, Wojciech~M. Czarnecki, Micha{\"{e}}l Mathieu,
  Andrew Dudzik, Junyoung Chung, David~H. Choi, Richard Powell, Timo Ewalds,
  Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel Kroiss, Ivo Danihelka, Aja
  Huang, Laurent Sifre, Trevor Cai, John~P. Agapiou, Max Jaderberg,
  Alexander~Sasha Vezhnevets, R{\'{e}}mi Leblond, Tobias Pohlen, Valentin
  Dalibard, David Budden, Yury Sulsky, James Molloy, Tom~L. Paine, {\c{C}}aglar
  G{\"{u}}l{\c{c}}ehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Roman Ring, Dani
  Yogatama, Dario W{\"{u}}nsch, Katrina McKinney, Oliver Smith, Tom Schaul,
  Timothy~P. Lillicrap, Koray Kavukcuoglu, Demis Hassabis, Chris Apps, and
  David Silver.
\newblock Grandmaster level in starcraft {II} using multi-agent reinforcement
  learning.
\newblock \emph{Nat.}, 575\penalty0 (7782):\penalty0 350--354, 2019.
\newblock \doi{10.1038/s41586-019-1724-z}.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Ren, Liu, Yu, and
  Zhang]{wang2020qplex}
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang.
\newblock Qplex: Duplex dueling multi-agent q-learning.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Dong, Lesser, and
  Zhang]{wang2020roma}
Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang.
\newblock Roma: Multi-agent reinforcement learning with emergent roles.
\newblock \emph{arXiv preprint arXiv:2003.08039}, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2020{\natexlab{c}})Wang, Gupta, Mahajan, Peng, Whiteson,
  and Zhang]{wang2020rode}
Tonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon Whiteson, and
  Chongjie Zhang.
\newblock Rode: Learning roles to decompose multi-agent tasks.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{c}}.

\bibitem[Yang et~al.(2018)Yang, Luo, Li, Zhou, Zhang, and Wang]{yang2018mean}
Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang.
\newblock Mean field multi-agent reinforcement learning.
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 5571--5580. PMLR,
  10--15 Jul 2018.
\newblock URL \url{https://proceedings.mlr.press/v80/yang18d.html}.

\bibitem[Yu et~al.(2021)Yu, Velu, Vinitsky, Wang, Bayen, and
  Wu]{yu2021surprising}
Chao Yu, Akash Velu, Eugene Vinitsky, Yu~Wang, Alexandre Bayen, and Yi~Wu.
\newblock The surprising effectiveness of ppo in cooperative, multi-agent
  games.
\newblock \emph{arXiv preprint arXiv:2103.01955}, 2021.

\end{thebibliography}
