\begin{thebibliography}{74}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdulhai et~al.(2023)Abdulhai, White, Snell, Sun, Hong, Zhai, Xu, and
  Levine]{abdulhai2023lmrl}
Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang
  Zhai, Kelvin Xu, and Sergey Levine.
\newblock Lmrl gym: Benchmarks for multi-turn reinforcement learning with
  language models.
\newblock \emph{arXiv preprint arXiv:2311.18232}, 2023.

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,
  Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
  Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
  Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri,
  Taropa, Bailey, Chen, et~al.]{anil2023palm}
Rohan Anil, Andrew~M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
  Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
  et~al.
\newblock Palm-2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem[Azerbayev et~al.(2023)Azerbayev, Schoelkopf, Paster, Dos~Santos,
  McAleer, Jiang, Deng, Biderman, and Welleck]{azerbayev2023llemma}
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos~Santos,
  Stephen~Marcus McAleer, Albert~Q Jiang, Jia Deng, Stella Biderman, and Sean
  Welleck.
\newblock Llemma: An open language model for mathematics.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2023.

\bibitem[Baheti et~al.(2024)Baheti, Lu, Brahman, Le~Bras, Sap, and
  Riedl]{baheti2023improving}
Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le~Bras, Maarten Sap, and Mark
  Riedl.
\newblock Leftover-lunch: Advantage-based offline reinforcement learning for
  language models.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Bird(2006)]{bird2006nltk}
Steven Bird.
\newblock Nltk: the natural language toolkit.
\newblock In \emph{Proceedings of the COLING/ACL 2006 Interactive Presentation
  Sessions}, pages 69--72, 2006.

\bibitem[Bisk et~al.(2020)Bisk, Zellers, Bras, Gao, and Choi]{Bisk2020}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Thirty-Fourth AAAI Conference on Artificial Intelligence},
  2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Burns et~al.(2023)Burns, Izmailov, Kirchner, Baker, Gao,
  Aschenbrenner, Chen, Ecoffet, Joglekar, Leike, et~al.]{burns2023weak}
Collin Burns, Pavel Izmailov, Jan~Hendrik Kirchner, Bowen Baker, Leo Gao,
  Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan
  Leike, et~al.
\newblock Weak-to-strong generalization: Eliciting strong capabilities with
  weak supervision.
\newblock \emph{arXiv preprint arXiv:2312.09390}, 2023.

\bibitem[Chen et~al.(2024)Chen, Deng, Yuan, Ji, and Gu]{chen2024self}
Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu.
\newblock Self-play fine-tuning converts weak language models to strong
  language models.
\newblock \emph{arXiv preprint arXiv:2401.01335}, 2024.

\bibitem[Cheng et~al.(2023{\natexlab{a}})Cheng, Xie, Bai, Dai, and
  Du]{cheng2023everyone}
Pengyu Cheng, Jiawen Xie, Ke~Bai, Yong Dai, and Nan Du.
\newblock Everyone deserves a reward: Learning customized human preferences.
\newblock \emph{arXiv preprint arXiv:2309.03126}, 2023{\natexlab{a}}.

\bibitem[Cheng et~al.(2023{\natexlab{b}})Cheng, Yang, Li, Dai, and
  Du]{cheng2023adversarial}
Pengyu Cheng, Yifan Yang, Jian Li, Yong Dai, and Nan Du.
\newblock Adversarial preference optimization.
\newblock \emph{arXiv preprint arXiv:2311.08045}, 2023{\natexlab{b}}.

\bibitem[Chu et~al.(2023)Chu, Chen, Chen, Yu, He, Wang, Peng, Liu, Qin, and
  Liu]{chu2023survey}
Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang,
  Weihua Peng, Ming Liu, Bing Qin, and Ting Liu.
\newblock A survey of chain of thought reasoning: Advances, frontiers and
  future.
\newblock \emph{arXiv preprint arXiv:2309.15402}, 2023.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord]{Clark2018ThinkYH}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
  Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock \emph{ArXiv}, abs/1803.05457, 2018.

\bibitem[Coulom(2006)]{coulom2006efficient}
R{\'e}mi Coulom.
\newblock Efficient selectivity and backup operators in monte-carlo tree
  search.
\newblock In \emph{International conference on computers and games}, pages
  72--83. Springer, 2006.

\bibitem[Cui et~al.(2020)Cui, Wu, Liu, Zhang, and Zhou]{mutual}
Leyang Cui, Yu~Wu, Shujie Liu, Yue Zhang, and Ming Zhou.
\newblock Mutual: A dataset for multi-turn dialogue reasoning.
\newblock In \emph{Proceedings of the 58th Conference of the Association for
  Computational Linguistics}. Association for Computational Linguistics, 2020.

\bibitem[Davies(2020)]{davies_coca_2019}
Mark Davies.
\newblock {COCA}: Corpus of contemporary american english, 2020.
\newblock URL \url{https://www.english-corpora.org/coca/}.

\bibitem[Ding et~al.(2023)Ding, Zhang, Wang, Xu, Ma, Zhang, Qin, Rajmohan, Lin,
  and Zhang]{ding2023everything}
Ruomeng Ding, Chaoyun Zhang, Lu~Wang, Yong Xu, Minghua Ma, Wei Zhang, Si~Qin,
  Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang.
\newblock Everything of thoughts: Defying the law of penrose triangle for
  thought generation.
\newblock \emph{arXiv preprint arXiv:2311.04254}, 2023.

\bibitem[Dong et~al.(2023{\natexlab{a}})Dong, Yuan, Lu, Li, Xue, Liu, Wang,
  Yuan, Zhou, and Zhou]{dong2023abilities}
Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng
  Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou.
\newblock How abilities in large language models are affected by supervised
  fine-tuning data composition.
\newblock \emph{arXiv preprint arXiv:2310.05492}, 2023{\natexlab{a}}.

\bibitem[Dong et~al.(2023{\natexlab{b}})Dong, Xiong, Goyal, Pan, Diao, Zhang,
  Shum, and Zhang]{dong2023raft}
Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang,
  Kashun Shum, and Tong Zhang.
\newblock Raft: Reward ranked finetuning for generative foundation model
  alignment.
\newblock \emph{arXiv preprint arXiv:2304.06767}, 2023{\natexlab{b}}.

\bibitem[Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster,
  Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds,
  Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and
  Zou]{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony
  DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h,
  Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang,
  Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric
  Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 12 2023.
\newblock URL \url{https://zenodo.org/records/10256836}.

\bibitem[Gulcehre et~al.(2023)Gulcehre, Paine, Srinivasan, Konyushkova, Weerts,
  Sharma, Siddhant, Ahern, Wang, Gu, et~al.]{gulcehre2023reinforced}
Caglar Gulcehre, Tom~Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte
  Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie
  Gu, et~al.
\newblock Reinforced self-training (rest) for language modeling.
\newblock \emph{arXiv preprint arXiv:2308.08998}, 2023.

\bibitem[Hausknecht et~al.(2020)Hausknecht, Ammanabrolu, C{\^o}t{\'e}, and
  Yuan]{hausknecht2020interactive}
Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre C{\^o}t{\'e}, and
  Xingdi Yuan.
\newblock Interactive fiction games: A colossal adventure.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 7903--7910, 2020.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{DBLP:journals/corr/abs-2009-03300}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{CoRR}, abs/2009.03300, 2020.
\newblock URL \url{https://arxiv.org/abs/2009.03300}.

\bibitem[Huang et~al.(2023)Huang, Gu, Hou, Wu, Wang, Yu, and
  Han]{huang2023large}
Jiaxin Huang, Shixiang~Shane Gu, Le~Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu,
  and Jiawei Han.
\newblock Large language models can self-improve.
\newblock In \emph{The 2023 Conference on Empirical Methods in Natural Language
  Processing}, 2023.

\bibitem[Jiao et~al.(2023)Jiao, Wang, Huang, Wang, and Tu]{jiao2023chatgpt}
Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu.
\newblock Is chatgpt a good translator? a preliminary study.
\newblock \emph{arXiv preprint arXiv:2301.08745}, 2023.

\bibitem[Koco{\'n} et~al.(2023)Koco{\'n}, Cichecki, Kaszyca, Kochanek,
  Szyd{\l}o, Baran, Bielaniewicz, Gruza, Janz, Kanclerz,
  et~al.]{kocon2023chatgpt}
Jan Koco{\'n}, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika
  Szyd{\l}o, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz,
  Kamil Kanclerz, et~al.
\newblock Chatgpt: Jack of all trades, master of none.
\newblock \emph{Information Fusion}, 99:\penalty0 101861, 2023.

\bibitem[Konda and Tsitsiklis(1999)]{konda1999actor}
Vijay Konda and John Tsitsiklis.
\newblock Actor-critic algorithms.
\newblock In \emph{Advances in neural information processing systems},
  volume~12, 1999.

\bibitem[Kullback(1997)]{kullback1997information}
Solomon Kullback.
\newblock \emph{Information theory and statistics}.
\newblock Courier Corporation, 1997.

\bibitem[Lewis et~al.(2017)Lewis, Yarats, Dauphin, Parikh, and
  Batra]{lewis2017deal}
Mike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh, and Dhruv Batra.
\newblock Deal or no deal? end-to-end learning of negotiation dialogues.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, pages 2443--2453, 2017.

\bibitem[Littman(1994)]{littman1994markov}
Michael~L Littman.
\newblock Markov games as a framework for multi-agent reinforcement learning.
\newblock In \emph{Machine learning proceedings 1994}, pages 157--163.
  Elsevier, 1994.

\bibitem[Liu et~al.(2023)Liu, Liu, Cui, Teng, Duan, Zhou, and Zhang]{10174688}
Hanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, and Yue
  Zhang.
\newblock Logiqa 2.0 — an improved dataset for logical reasoning in natural
  language understanding.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, pages 1--16, 2023.

\bibitem[Liu et~al.(2021)Liu, Cui, Liu, Huang, Wang, and Zhang]{liu2021logiqa}
Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang.
\newblock Logiqa: a challenge dataset for machine reading comprehension with
  logical reasoning.
\newblock In \emph{Proceedings of the Twenty-Ninth International Conference on
  International Joint Conferences on Artificial Intelligence}, pages
  3622--3628, 2021.

\bibitem[Loria et~al.(2018)]{loria2018textblob}
Steven Loria et~al.
\newblock textblob documentation.
\newblock \emph{Release 0.15}, 2\penalty0 (8):\penalty0 269, 2018.

\bibitem[Ma et~al.(2023)Ma, Yang, Gao, Ci, Gao, Pan, and Yang]{ma2023red}
Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan, and Yaodong
  Yang.
\newblock Red teaming game: A game-theoretic framework for red teaming language
  models.
\newblock \emph{arXiv e-prints}, pages arXiv--2310, 2023.

\bibitem[Neal(2001)]{neal2001annealed}
Radford~M Neal.
\newblock Annealed importance sampling.
\newblock \emph{Statistics and computing}, 11:\penalty0 125--139, 2001.

\bibitem[Oh et~al.(2018)Oh, Guo, Singh, and Lee]{oh2018self}
Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee.
\newblock Self-imitation learning.
\newblock In \emph{International conference on machine learning}, pages
  3878--3887. PMLR, 2018.

\bibitem[OpenAI(2023{\natexlab{a}})]{chatgpt}
OpenAI.
\newblock Chat{GPT}, {M}ar 14 version.
\newblock \url{https://chat.openai.com/chat}, 2023{\natexlab{a}}.

\bibitem[OpenAI(2023{\natexlab{b}})]{openai2022gpt4}
OpenAI.
\newblock G{P}{T}-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023{\natexlab{b}}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, pages 27730--27744, 2022.

\bibitem[Pan et~al.(2023)Pan, Albalak, Wang, and Wang]{pan2023logic}
Liangming Pan, Alon Albalak, Xinyi Wang, and William~Yang Wang.
\newblock Logic-lm: Empowering large language models with symbolic solvers for
  faithful logical reasoning.
\newblock In \emph{The 2023 Conference on Empirical Methods in Natural Language
  Processing}, 2023.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and
  Finn]{rafailov2023direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher~D
  Manning, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a
  reward model.
\newblock \emph{arXiv preprint arXiv:2305.18290}, 2023.

\bibitem[Raina et~al.(2024)Raina, Liusie, and Gales]{raina2024llm}
Vyas Raina, Adian Liusie, and Mark Gales.
\newblock Is llm-as-a-judge robust? investigating universal adversarial attacks
  on zero-shot llm assessment.
\newblock \emph{arXiv preprint arXiv:2402.14016}, 2024.

\bibitem[Ramamurthy et~al.(2023)Ramamurthy, Ammanabrolu, Brantley, Hessel,
  Sifa, Bauckhage, Hajishirzi, and Choi]{ramamurthy2022reinforcement}
Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant{\'e} Brantley, Jack Hessel,
  Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi.
\newblock Is reinforcement learning (not) for natural language processing:
  Benchmarks, baselines, and building blocks for natural language policy
  optimization.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Sakaguchi et~al.(2019)Sakaguchi, Bras, Bhagavatula, and
  Choi]{sakaguchi2019winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{arXiv preprint arXiv:1907.10641}, 2019.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pages
  1889--1897. PMLR, 2015.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{schulman2015high}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter
  Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Singh et~al.(2023)Singh, Co-Reyes, Agarwal, Anand, Patil, Liu,
  Harrison, Lee, Xu, Parisi, et~al.]{singh2023beyond}
Avi Singh, John~D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil,
  Peter~J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et~al.
\newblock Beyond human data: Scaling self-training for problem-solving with
  language models.
\newblock \emph{arXiv preprint arXiv:2312.06585}, 2023.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch,
  Brown, Santoro, Gupta, Garriga-Alonso, and et~al]{srivastava2022imitation}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar
  Abid, Adam Fisch, Adam~R. Brown, Adam Santoro, Aditya Gupta, Adrià
  Garriga-Alonso, and et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models, 2022.

\bibitem[Surameery and Shakor(2023)]{surameery2023use}
Nigar M~Shafiq Surameery and Mohammed~Y Shakor.
\newblock Use chat gpt to solve programming bugs.
\newblock \emph{International Journal of Information Technology \& Computer
  Engineering (IJITC) ISSN: 2455-5290}, 3\penalty0 (01):\penalty0 17--22, 2023.

\bibitem[Sutton(1988)]{sutton1988learning}
Richard~S Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3:\penalty0 9--44, 1988.

\bibitem[Suzgun et~al.(2022)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung,
  Chowdhery, Le, Chi, Zhou, , and Wei]{suzgun2022challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay,
  Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, , and
  Jason Wei.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve
  them.
\newblock \emph{arXiv preprint arXiv:2210.09261}, 2022.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,
  Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,
  Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Tian et~al.(2023)Tian, Lu, Li, Tang, Cheung, Klein, and
  Bissyand{\'e}]{tian2023chatgpt}
Haoye Tian, Weiqi Lu, Tsz~On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques Klein,
  and Tegawend{\'e}~F Bissyand{\'e}.
\newblock Is chatgpt the ultimate programming assistant--how far is it?
\newblock \emph{arXiv preprint arXiv:2304.11938}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,
  Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Turpin et~al.(2024)Turpin, Michael, Perez, and
  Bowman]{turpin2024language}
Miles Turpin, Julian Michael, Ethan Perez, and Samuel Bowman.
\newblock Language models don't always say what they think: unfaithful
  explanations in chain-of-thought prompting.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Oriol Vinyals, Igor Babuschkin, Wojciech~M Czarnecki, Micha{\"e}l Mathieu,
  Andrew Dudzik, Junyoung Chung, David~H Choi, Richard Powell, Timo Ewalds,
  Petko Georgiev, et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou,
  et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V
  Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock In \emph{Advances in neural information processing systems},
  volume~35, pages 24824--24837, 2022.

\bibitem[Wu et~al.(2024)Wu, Zhu, Yang, Xu, Fu, Wei, and Fu]{wu2024enhance}
Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, and Haobo Fu.
\newblock Enhance reasoning for large language models in the game werewolf.
\newblock \emph{arXiv preprint arXiv:2402.02330}, 2024.

\bibitem[Xie et~al.(2024)Xie, Cheng, Liang, Dai, and Du]{xie-etal-2024-chunk}
Jiawen Xie, Pengyu Cheng, Xiao Liang, Yong Dai, and Nan Du.
\newblock Chunk, align, select: A simple long-sequence processing method for
  transformers.
\newblock In \emph{Proceedings of the 62nd Annual Meeting of the Association
  for Computational Linguistics}, volume~1, pages 13500--13519. Association for
  Computational Linguistics, August 2024.

\bibitem[Xu et~al.(2023)Xu, Wang, Li, Luo, Wang, Liu, and Liu]{xu2023exploring}
Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and
  Yang Liu.
\newblock Exploring large language models for communication games: An empirical
  study on werewolf.
\newblock \emph{arXiv preprint arXiv:2309.04658}, 2023.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Xiao, Wang, Zhang, Bian, Yin, Lv,
  Pan, Wang, Yan, et~al.]{yang2023baichuan}
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce~Bian, Chao Yin, Chenxu
  Lv, Da~Pan, Dian Wang, Dong Yan, et~al.
\newblock Baichuan 2: Open large-scale language models.
\newblock \emph{arXiv preprint arXiv:2309.10305}, 2023{\natexlab{a}}.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Jin, Tang, Han, Feng, Jiang,
  Zhong, Yin, and Hu]{yang2023harnessing}
Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming
  Jiang, Shaochen Zhong, Bing Yin, and Xia Hu.
\newblock Harnessing the power of llms in practice: A survey on chatgpt and
  beyond.
\newblock \emph{ACM Transactions on Knowledge Discovery from Data},
  2023{\natexlab{b}}.

\bibitem[Yao et~al.(2024)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and
  Narasimhan]{yao2024tree}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and
  Karthik Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language
  models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Yao et~al.(2021)Yao, Zhong, Zhang, Han, Wang, Zhang, Xiao, Zeng, Liu,
  and Sun]{yao2021adversarial}
Yuan Yao, Haoxi Zhong, Zhengyan Zhang, Xu~Han, Xiaozhi Wang, Kai Zhang, Chaojun
  Xiao, Guoyang Zeng, Zhiyuan Liu, and Maosong Sun.
\newblock Adversarial language games for advanced natural language
  intelligence.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 14248--14256, 2021.

\bibitem[Yuan et~al.(2024)Yuan, Pang, Cho, Sukhbaatar, Xu, and
  Weston]{yuan2024self}
Weizhe Yuan, Richard~Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing
  Xu, and Jason Weston.
\newblock Self-rewarding language models.
\newblock \emph{arXiv preprint arXiv:2401.10020}, 2024.

\bibitem[Yuan et~al.(2023)Yuan, Yuan, Tan, Wang, Huang, and
  Huang]{yuan2023rrhf}
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang.
\newblock Rrhf: Rank responses to align language models with human feedback
  without tears.
\newblock \emph{arXiv preprint arXiv:2304.05302}, 2023.

\bibitem[Zeng et~al.(2023)Zeng, Dai, Cheng, Hu, Chen, Du, and
  Xu]{zeng2023diverse}
Dun Zeng, Yong Dai, Pengyu Cheng, Tianhao Hu, Wanshun Chen, Nan Du, and Zenglin
  Xu.
\newblock On diverse preferences for large language model alignment.
\newblock \emph{arXiv preprint arXiv:2312.07401}, 2023.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Press, Merrill, Liu, and
  Smith]{zhang2023language}
Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah~A Smith.
\newblock How language model hallucinations can snowball.
\newblock \emph{arXiv preprint arXiv:2305.13534}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Lu, and
  Jaitly]{zhang2023entity}
Yizhe Zhang, Jiarui Lu, and Navdeep Jaitly.
\newblock The entity-deduction arena: A playground for probing the
  conversational reasoning and planning capabilities of llms.
\newblock \emph{arXiv preprint arXiv:2310.01468}, 2023{\natexlab{b}}.

\end{thebibliography}
