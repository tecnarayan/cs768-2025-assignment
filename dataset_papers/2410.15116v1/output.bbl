\begin{thebibliography}{109}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adolphs et~al.(2021)Adolphs, Shuster, Urbanek, Szlam, and
  Weston]{reason1}
Adolphs, L., Shuster, K., Urbanek, J., Szlam, A., and Weston, J.
\newblock Reason first, then respond: Modular generation for knowledge-infused
  dialogue.
\newblock \emph{arXiv preprint arXiv:2111.05204}, 2021.

\bibitem[Anagnostidis et~al.(2023)Anagnostidis, Pavllo, Biggio, Noci, Lucchi,
  and Hoffmann]{more4}
Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., and Hoffmann,
  T.
\newblock Dynamic context pruning for efficient and interpretable
  autoregressive transformers.
\newblock \emph{arXiv preprint arXiv:2305.15805}, 2023.

\bibitem[Berant et~al.(2013)Berant, Chou, Frostig, and Liang]{webq}
Berant, J., Chou, A., Frostig, R., and Liang, P.
\newblock Semantic parsing on freebase from question-answer pairs.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pp.\  1533--1544, 2013.

\bibitem[Bi et~al.(2024)Bi, Chen, Chen, Chen, Dai, Deng, Ding, Dong, Du, Fu,
  et~al.]{deepseek}
Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K.,
  Du, Q., Fu, Z., et~al.
\newblock Deepseek llm: Scaling open-source language models with longtermism.
\newblock \emph{arXiv preprint arXiv:2401.02954}, 2024.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{few-shot}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Bunescu \& Uduehi(2022)Bunescu and Uduehi]{nlp_selfinfo}
Bunescu, R. and Uduehi, O.~O.
\newblock Distribution-based measures of surprise for creative language:
  Experiments with humor and metaphor.
\newblock In \emph{Proceedings of the 3rd Workshop on Figurative Language
  Processing (FLP)}, pp.\  68--78, 2022.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Pasupat, Singh, Lee, and
  Guu]{purr}
Chen, A., Pasupat, P., Singh, S., Lee, H., and Guu, K.
\newblock Purr: Efficiently editing language model hallucinations by denoising
  language model corruptions.
\newblock \emph{arXiv preprint arXiv:2305.14908}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Borgeaud, Irving, Lespiau, Sifre,
  and Jumper]{tuili1}
Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J.
\newblock Accelerating large language model decoding with speculative sampling.
\newblock \emph{arXiv preprint arXiv:2302.01318}, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2017)Chen, Fisch, Weston, and Bordes]{refs_rc1}
Chen, D., Fisch, A., Weston, J., and Bordes, A.
\newblock Reading wikipedia to answer open-domain questions.
\newblock \emph{arXiv preprint arXiv:1704.00051}, 2017.

\bibitem[Chen et~al.(2023{\natexlab{c}})Chen, Zhao, Zhang, Chern, Gao, Liu, He,
  et~al.]{felm}
Chen, S., Zhao, Y., Zhang, J., Chern, I., Gao, S., Liu, P., He, J., et~al.
\newblock Felm: Benchmarking factuality evaluation of large language models.
\newblock \emph{arXiv preprint arXiv:2310.00741}, 2023{\natexlab{c}}.

\bibitem[Cheng et~al.(2021)Cheng, Shen, Liu, He, Chen, and Gao]{uni_ralm}
Cheng, H., Shen, Y., Liu, X., He, P., Chen, W., and Gao, J.
\newblock Unitedqa: A hybrid approach for open domain question answering.
\newblock \emph{arXiv preprint arXiv:2101.00178}, 2021.

\bibitem[Chern et~al.(2023{\natexlab{a}})Chern, Chern, Chen, Yuan, Feng, Zhou,
  He, Neubig, Liu, et~al.]{factchecking}
Chern, I., Chern, S., Chen, S., Yuan, W., Feng, K., Zhou, C., He, J., Neubig,
  G., Liu, P., et~al.
\newblock Factool: Factuality detection in generative ai--a tool augmented
  framework for multi-task and multi-domain scenarios.
\newblock \emph{arXiv preprint arXiv:2307.13528}, 2023{\natexlab{a}}.

\bibitem[Chern et~al.(2023{\natexlab{b}})Chern, Wang, Das, Sharma, Liu, Neubig,
  et~al.]{Contrastive1}
Chern, I.-C., Wang, Z., Das, S., Sharma, B., Liu, P., Neubig, G., et~al.
\newblock Improving factuality of abstractive summarization via contrastive
  reward learning.
\newblock \emph{arXiv preprint arXiv:2307.04507}, 2023{\natexlab{b}}.

\bibitem[Chevalier et~al.(2023)Chevalier, Wettig, Ajith, and Chen]{more1}
Chevalier, A., Wettig, A., Ajith, A., and Chen, D.
\newblock Adapting language models to compress contexts.
\newblock \emph{arXiv preprint arXiv:2305.14788}, 2023.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  Garcia, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov,
  Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira,
  Child, Polozov, Lee, Zhou, Wang, Saeta, Diaz, Firat, Catasta, Wei,
  Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
  Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K.,
  Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N.,
  Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J.,
  Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A.,
  Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K.,
  Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov,
  A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.~M., Pillai,
  T.~S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee,
  K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J.,
  Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N.
\newblock Palm: Scaling language modeling with pathways, 2022.

\bibitem[Creswell et~al.()Creswell, Shanahan, and Higgins]{SI}
Creswell, A., Shanahan, M., and Higgins, I.
\newblock Selection-inference: Exploiting large language models for
  interpretable logical reasoning, 2022.
\newblock \emph{URL https://arxiv. org/abs/2205.09712}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dhuliawala et~al.(2023)Dhuliawala, Komeili, Xu, Raileanu, Li,
  Celikyilmaz, and Weston]{cov}
Dhuliawala, S., Komeili, M., Xu, J., Raileanu, R., Li, X., Celikyilmaz, A., and
  Weston, J.
\newblock Chain-of-verification reduces hallucination in large language models.
\newblock \emph{arXiv preprint arXiv:2309.11495}, 2023.

\bibitem[El-Kassas et~al.(2021)El-Kassas, Salama, Rafea, and Mohamed]{auto-sum}
El-Kassas, W.~S., Salama, C.~R., Rafea, A.~A., and Mohamed, H.~K.
\newblock Automatic text summarization: A comprehensive survey.
\newblock \emph{Expert systems with applications}, 165:\penalty0 113679, 2021.

\bibitem[Fei et~al.(2023)Fei, Niu, Zhou, Hou, Bai, Deng, and Han]{more3}
Fei, W., Niu, X., Zhou, P., Hou, L., Bai, B., Deng, L., and Han, W.
\newblock Extending context window of large language models via semantic
  compression.
\newblock \emph{arXiv preprint arXiv:2312.09571}, 2023.

\bibitem[Frantar \& Alistarh(2023)Frantar and Alistarh]{qmoe}
Frantar, E. and Alistarh, D.
\newblock Qmoe: Practical sub-1-bit compression of trillion-parameter models.
\newblock \emph{arXiv preprint arXiv:2310.16795}, 2023.

\bibitem[Galitsky(2023)]{truthcheck}
Galitsky, B.~A.
\newblock Truth-o-meter: Collaborating with llm in fighting its hallucinations.
\newblock \emph{arXiv preprint}, 2023.

\bibitem[Gao et~al.(2023{\natexlab{a}})Gao, Dai, Pasupat, Chen, Chaganty, Fan,
  Zhao, Lao, Lee, Juan, et~al.]{rarr}
Gao, L., Dai, Z., Pasupat, P., Chen, A., Chaganty, A.~T., Fan, Y., Zhao, V.,
  Lao, N., Lee, H., Juan, D.-C., et~al.
\newblock Rarr: Researching and revising what language models say, using
  language models.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  16477--16508,
  2023{\natexlab{a}}.

\bibitem[Gao et~al.(2023{\natexlab{b}})Gao, Xiong, Gao, Jia, Pan, Bi, Dai, Sun,
  and Wang]{retri_survey}
Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y., Dai, Y., Sun, J., and
  Wang, H.
\newblock Retrieval-augmented generation for large language models: A survey.
\newblock \emph{arXiv preprint arXiv:2312.10997}, 2023{\natexlab{b}}.

\bibitem[Ge et~al.(2023{\natexlab{a}})Ge, Hu, Wang, Chen, and Wei]{more2}
Ge, T., Hu, J., Wang, X., Chen, S.-Q., and Wei, F.
\newblock In-context autoencoder for context compression in a large language
  model.
\newblock \emph{arXiv preprint arXiv:2307.06945}, 2023{\natexlab{a}}.

\bibitem[Ge et~al.(2023{\natexlab{b}})Ge, Hua, Ji, Tan, Xu, and Zhang]{ogi}
Ge, Y., Hua, W., Ji, J., Tan, J., Xu, S., and Zhang, Y.
\newblock Openagi: When llm meets domain experts.
\newblock \emph{arXiv preprint arXiv:2304.04370}, 2023{\natexlab{b}}.

\bibitem[Gurnee et~al.(2023)Gurnee, Nanda, Pauly, Harvey, Troitskii, and
  Bertsimas]{finding}
Gurnee, W., Nanda, N., Pauly, M., Harvey, K., Troitskii, D., and Bertsimas, D.
\newblock Finding neurons in a haystack: Case studies with sparse probing,
  2023.

\bibitem[Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang]{1ralms}
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.
\newblock Retrieval augmented language model pre-training.
\newblock In \emph{International conference on machine learning}, pp.\
  3929--3938. PMLR, 2020.

\bibitem[Hanna et~al.(2023)Hanna, Liu, and Variengien]{greater}
Hanna, M., Liu, O., and Variengien, A.
\newblock How does gpt-2 compute greater-than?: Interpreting mathematical
  abilities in a pre-trained language model, 2023.

\bibitem[Hsieh et~al.(2023)Hsieh, Li, Yeh, Nakhost, Fujii, Ratner, Krishna,
  Lee, and Pfister]{dist1}
Hsieh, C.-Y., Li, C.-L., Yeh, C.-K., Nakhost, H., Fujii, Y., Ratner, A.,
  Krishna, R., Lee, C.-Y., and Pfister, T.
\newblock Distilling step-by-step! outperforming larger language models with
  less training data and smaller model sizes.
\newblock \emph{arXiv preprint arXiv:2305.02301}, 2023.

\bibitem[Huang et~al.(2023)Huang, Kwak, and An]{coe}
Huang, F., Kwak, H., and An, J.
\newblock Chain of explanation: New prompting method to generate quality
  natural language explanation for implicit hate speech.
\newblock In \emph{Companion Proceedings of the ACM Web Conference 2023}, pp.\
  90--93, 2023.

\bibitem[Izacard \& Grave(2020{\natexlab{a}})Izacard and Grave]{lever_reader}
Izacard, G. and Grave, E.
\newblock Leveraging passage retrieval with generative models for open domain
  question answering.
\newblock \emph{arXiv preprint arXiv:2007.01282}, 2020{\natexlab{a}}.

\bibitem[Izacard \& Grave(2020{\natexlab{b}})Izacard and Grave]{reader1}
Izacard, G. and Grave, E.
\newblock Leveraging passage retrieval with generative models for open domain
  question answering.
\newblock \emph{arXiv preprint arXiv:2007.01282}, 2020{\natexlab{b}}.

\bibitem[Izacard et~al.(2022)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick,
  Dwivedi-Yu, Joulin, Riedel, and Grave]{few_ralms}
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T.,
  Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E.
\newblock Few-shot learning with retrieval augmented language models.
\newblock \emph{arXiv preprint arXiv:2208.03299}, 2022.

\bibitem[Jiang et~al.(2023{\natexlab{a}})Jiang, Wu, Lin, Yang, and
  Qiu]{llmlingua}
Jiang, H., Wu, Q., Lin, C.-Y., Yang, Y., and Qiu, L.
\newblock Llmlingua: Compressing prompts for accelerated inference of large
  language models.
\newblock \emph{arXiv preprint arXiv:2310.05736}, 2023{\natexlab{a}}.

\bibitem[Jiang et~al.(2023{\natexlab{b}})Jiang, Wu, Luo, Li, Lin, Yang, and
  Qiu]{Longllmlingua}
Jiang, H., Wu, Q., Luo, X., Li, D., Lin, C.-Y., Yang, Y., and Qiu, L.
\newblock Longllmlingua: Accelerating and enhancing llms in long context
  scenarios via prompt compression.
\newblock \emph{arXiv preprint arXiv:2310.06839}, 2023{\natexlab{b}}.

\bibitem[Jiang et~al.(2023{\natexlab{c}})Jiang, Xu, Gao, Sun, Liu, Dwivedi-Yu,
  Yang, Callan, and Neubig]{arag}
Jiang, Z., Xu, F.~F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y.,
  Callan, J., and Neubig, G.
\newblock Active retrieval augmented generation.
\newblock \emph{arXiv preprint arXiv:2305.06983}, 2023{\natexlab{c}}.

\bibitem[Jiang et~al.(2023{\natexlab{d}})Jiang, Xu, Gao, Sun, Liu, Dwivedi-Yu,
  Yang, Callan, and Neubig]{ground}
Jiang, Z., Xu, F.~F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y.,
  Callan, J., and Neubig, G.
\newblock Active retrieval augmented generation.
\newblock \emph{arXiv preprint arXiv:2305.06983}, 2023{\natexlab{d}}.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer]{triviaqa}
Joshi, M., Choi, E., Weld, D.~S., and Zettlemoyer, L.
\newblock Triviaqa: A large scale distantly supervised challenge dataset for
  reading comprehension.
\newblock \emph{arXiv preprint arXiv:1705.03551}, 2017.

\bibitem[Karpukhin et~al.(2020{\natexlab{a}})Karpukhin, O{\u{g}}uz, Min, Lewis,
  Wu, Edunov, Chen, and Yih]{dense}
Karpukhin, V., O{\u{g}}uz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen,
  D., and Yih, W.-t.
\newblock Dense passage retrieval for open-domain question answering.
\newblock \emph{arXiv preprint arXiv:2004.04906}, 2020{\natexlab{a}}.

\bibitem[Karpukhin et~al.(2020{\natexlab{b}})Karpukhin, O{\u{g}}uz, Min, Lewis,
  Wu, Edunov, Chen, and Yih]{refs_rc2}
Karpukhin, V., O{\u{g}}uz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen,
  D., and Yih, W.-t.
\newblock Dense passage retrieval for open-domain question answering.
\newblock \emph{arXiv preprint arXiv:2004.04906}, 2020{\natexlab{b}}.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{rnntransformers}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{International conference on machine learning}, pp.\
  5156--5165. PMLR, 2020.

\bibitem[Kim et~al.(2023)Kim, Nam, Mo, Park, Lee, Seo, Ha, and Shin]{sure}
Kim, J., Nam, J., Mo, S., Park, J., Lee, S.-W., Seo, M., Ha, J.-W., and Shin,
  J.
\newblock Sure: Improving open-domain question answering of llms via summarized
  retrieval.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2023.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{zeroshot}
Kojima, T., Gu, S.~S., Reid, M., Matsuo, Y., and Iwasawa, Y.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{Advances in neural information processing systems},
  35:\penalty0 22199--22213, 2022.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins,
  Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee, et~al.]{nq}
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti,
  C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et~al.
\newblock Natural questions: a benchmark for question answering research.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  7:\penalty0 453--466, 2019.

\bibitem[Lai et~al.(2017)Lai, Xie, Liu, Yang, and Hovy]{race}
Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E.
\newblock Race: Large-scale reading comprehension dataset from examinations.
\newblock \emph{arXiv preprint arXiv:1704.04683}, 2017.

\bibitem[Leviathan et~al.(2023)Leviathan, Kalman, and Matias]{tuili2}
Leviathan, Y., Kalman, M., and Matias, Y.
\newblock Fast inference from transformers via speculative decoding.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  19274--19286. PMLR, 2023.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal,
  K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel, et~al.]{ralms_nlp}
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N.,
  K{\"u}ttler, H., Lewis, M., Yih, W.-t., Rockt{\"a}schel, T., et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9459--9474, 2020.

\bibitem[Li et~al.(2023)Li, Dong, Lin, and Guerin]{information_compressing}
Li, Y., Dong, B., Lin, C., and Guerin, F.
\newblock Compressing context to enhance inference efficiency of large language
  models.
\newblock \emph{arXiv preprint arXiv:2310.06201}, 2023.

\bibitem[Ling et~al.(2023)Ling, Fang, Li, Huang, Lee, Memisevic, and
  Su]{DeductiveVerification}
Ling, Z., Fang, Y., Li, X., Huang, Z., Lee, M., Memisevic, R., and Su, H.
\newblock Deductive verification of chain-of-thought reasoning.
\newblock \emph{arXiv preprint arXiv:2306.03872}, 2023.

\bibitem[Liu et~al.(2023)Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and
  Liang]{lostinmid}
Liu, N.~F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F.,
  and Liang, P.
\newblock Lost in the middle: How language models use long contexts.
\newblock \emph{arXiv preprint arXiv:2307.03172}, 2023.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach, 2019.

\bibitem[Ma et~al.(2023)Ma, Cheng, Zhang, Liu, Nyberg, and Gao]{retrieve2}
Ma, K., Cheng, H., Zhang, Y., Liu, X., Nyberg, E., and Gao, J.
\newblock Chain-of-skills: A configurable model for open-domain question
  answering.
\newblock \emph{arXiv preprint arXiv:2305.03130}, 2023.

\bibitem[Madaan et~al.(2023)Madaan, Tandon, Gupta, Hallinan, Gao, Wiegreffe,
  Alon, Dziri, Prabhumoye, Yang, et~al.]{selfrefine}
Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon,
  U., Dziri, N., Prabhumoye, S., Yang, Y., et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock \emph{arXiv preprint arXiv:2303.17651}, 2023.

\bibitem[Maynez et~al.(2020)Maynez, Narayan, Bohnet, and McDonald]{summary}
Maynez, J., Narayan, S., Bohnet, B., and McDonald, R.
\newblock On faithfulness and factuality in abstractive summarization.
\newblock \emph{arXiv preprint arXiv:2005.00661}, 2020.

\bibitem[Miao et~al.(2023{\natexlab{a}})Miao, Teh, and Rainforth]{selfcheck}
Miao, N., Teh, Y.~W., and Rainforth, T.
\newblock Selfcheck: Using llms to zero-shot check their own step-by-step
  reasoning.
\newblock \emph{arXiv preprint arXiv:2308.00436}, 2023{\natexlab{a}}.

\bibitem[Miao et~al.(2023{\natexlab{b}})Miao, Oliaro, Zhang, Cheng, Jin, Chen,
  and Jia]{hallu_toward_survey}
Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Jin, H., Chen, T., and Jia, Z.
\newblock Towards efficient generative large language model serving: A survey
  from algorithms to systems.
\newblock \emph{arXiv preprint arXiv:2312.15234}, 2023{\natexlab{b}}.

\bibitem[Mielke et~al.(2022)Mielke, Szlam, Dinan, and Boureau]{generation1}
Mielke, S.~J., Szlam, A., Dinan, E., and Boureau, Y.-L.
\newblock Reducing conversational agents’ overconfidence through linguistic
  calibration.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  10:\penalty0 857--872, 2022.

\bibitem[Nguyen et~al.(2023)Nguyen, Ting, Ciuc{\u{a}}, O'Neill, Sun,
  Jab{\l}o{\'n}ska, Kruk, Perkowski, Miller, Li, et~al.]{tianwenllm}
Nguyen, T.~D., Ting, Y.-S., Ciuc{\u{a}}, I., O'Neill, C., Sun, Z.-C.,
  Jab{\l}o{\'n}ska, M., Kruk, S., Perkowski, E., Miller, J., Li, J., et~al.
\newblock Astrollama: Towards specialized foundation models in astronomy.
\newblock \emph{arXiv preprint arXiv:2309.06126}, 2023.

\bibitem[OpenAI(2020)]{chatgpt}
OpenAI.
\newblock Chatgpt: A large-scale generative model for conversation, 2020.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Peng et~al.(2023)Peng, Galley, He, Cheng, Xie, Hu, Huang, Liden, Yu,
  Chen, et~al.]{checkagain}
Peng, B., Galley, M., He, P., Cheng, H., Xie, Y., Hu, Y., Huang, Q., Liden, L.,
  Yu, Z., Chen, W., et~al.
\newblock Check your facts and try again: Improving large language models with
  external knowledge and automated feedback.
\newblock \emph{arXiv preprint arXiv:2302.12813}, 2023.

\bibitem[Qu et~al.(2020{\natexlab{a}})Qu, Ding, Liu, Liu, Ren, Zhao, Dong, Wu,
  and Wang]{retrieve1}
Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, W.~X., Dong, D., Wu, H., and
  Wang, H.
\newblock Rocketqa: An optimized training approach to dense passage retrieval
  for open-domain question answering.
\newblock \emph{arXiv preprint arXiv:2010.08191}, 2020{\natexlab{a}}.

\bibitem[Qu et~al.(2020{\natexlab{b}})Qu, Ding, Liu, Liu, Ren, Zhao, Dong, Wu,
  and Wang]{rocket}
Qu, Y., Ding, Y., Liu, J., Liu, K., Ren, R., Zhao, W.~X., Dong, D., Wu, H., and
  Wang, H.
\newblock Rocketqa: An optimized training approach to dense passage retrieval
  for open-domain question answering.
\newblock \emph{arXiv preprint arXiv:2010.08191}, 2020{\natexlab{b}}.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever,
  et~al.]{gpt1}
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et~al.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{arXiv preprint}, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{gpt2}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, et~al.]{scaling}
Rae, J.~W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F.,
  Aslanides, J., Henderson, S., Ring, R., Young, S., et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Robertson et~al.(2009)Robertson, Zaragoza, et~al.]{bm25}
Robertson, S., Zaragoza, H., et~al.
\newblock The probabilistic relevance framework: Bm25 and beyond.
\newblock \emph{Foundations and Trends{\textregistered} in Information
  Retrieval}, 3\penalty0 (4):\penalty0 333--389, 2009.

\bibitem[Roit et~al.(2023)Roit, Ferret, Shani, Aharoni, Cideron, Dadashi,
  Geist, Girgin, Hussenot, Keller, et~al.]{rl_llm}
Roit, P., Ferret, J., Shani, L., Aharoni, R., Cideron, G., Dadashi, R., Geist,
  M., Girgin, S., Hussenot, L., Keller, O., et~al.
\newblock Factually consistent summarization via reinforcement learning with
  textual entailment feedback.
\newblock \emph{arXiv preprint arXiv:2306.00186}, 2023.

\bibitem[Roller et~al.(2020)Roller, Dinan, Goyal, Ju, Williamson, Liu, Xu, Ott,
  Shuster, Smith, et~al.]{opendomain}
Roller, S., Dinan, E., Goyal, N., Ju, D., Williamson, M., Liu, Y., Xu, J., Ott,
  M., Shuster, K., Smith, E.~M., et~al.
\newblock Recipes for building an open-domain chatbot.
\newblock \emph{arXiv preprint arXiv:2004.13637}, 2020.

\bibitem[Sachan et~al.(2023)Sachan, Lewis, Yogatama, Zettlemoyer, Pineau, and
  Zaheer]{que_ralm}
Sachan, D.~S., Lewis, M., Yogatama, D., Zettlemoyer, L., Pineau, J., and
  Zaheer, M.
\newblock Questions are all you need to train a dense passage retriever.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  11:\penalty0 600--616, 2023.

\bibitem[Shannon(1948)]{communication}
Shannon, C.~E.
\newblock A mathematical theory of communication.
\newblock \emph{The Bell system technical journal}, 27\penalty0 (3):\penalty0
  379--423, 1948.

\bibitem[Shi et~al.(2022)Shi, Suzgun, Freitag, Wang, Srivats, Vosoughi, Chung,
  Tay, Ruder, Zhou, et~al.]{multilingual}
Shi, F., Suzgun, M., Freitag, M., Wang, X., Srivats, S., Vosoughi, S., Chung,
  H.~W., Tay, Y., Ruder, S., Zhou, D., et~al.
\newblock Language models are multilingual chain-of-thought reasoners.
\newblock \emph{arXiv preprint arXiv:2210.03057}, 2022.

\bibitem[Shi et~al.(2023)Shi, Chen, Misra, Scales, Dohan, Chi, Sch{\"a}rli, and
  Zhou]{irrelevant}
Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E.~H., Sch{\"a}rli,
  N., and Zhou, D.
\newblock Large language models can be easily distracted by irrelevant context.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  31210--31227. PMLR, 2023.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{megantron}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro,
  B.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock \emph{Cornell University - arXiv,Cornell University - arXiv}, Sep
  2019.

\bibitem[Shuster et~al.(2021)Shuster, Poff, Chen, Kiela, and Weston]{rarh}
Shuster, K., Poff, S., Chen, M., Kiela, D., and Weston, J.
\newblock Retrieval augmentation reduces hallucination in conversation.
\newblock \emph{arXiv preprint arXiv:2104.07567}, 2021.

\bibitem[Singh et~al.(2021)Singh, Reddy, Hamilton, Dyer, and Yogatama]{end2end}
Singh, D., Reddy, S., Hamilton, W., Dyer, C., and Yogatama, D.
\newblock End-to-end training of multi-document reader and retriever for
  open-domain question answering.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 25968--25981, 2021.

\bibitem[Sparck~Jones(1972)]{tf-idf}
Sparck~Jones, K.
\newblock A statistical interpretation of term specificity and its application
  in retrieval.
\newblock \emph{Journal of documentation}, 28\penalty0 (1):\penalty0 11--21,
  1972.

\bibitem[Sun et~al.(2023)Sun, Shi, Gao, Ren, de~Rijke, and Ren]{Contrastive2}
Sun, W., Shi, Z., Gao, S., Ren, P., de~Rijke, M., and Ren, Z.
\newblock Contrastive learning reduces hallucination in conversations.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~37, pp.\  13618--13626, 2023.

\bibitem[Team(2023)]{gemini}
Team, G.
\newblock Gemini: A family of highly capable multimodal models, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux,
  Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and
  Lample]{llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
  T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin,
  A., Grave, E., and Lample, G.
\newblock Llama: Open and efficient foundation language models, 2023.

\bibitem[Varshney et~al.(2023)Varshney, Yao, Zhang, Chen, and Yu]{confidence}
Varshney, N., Yao, W., Zhang, H., Chen, J., and Yu, D.
\newblock A stitch in time saves nine: Detecting and mitigating hallucinations
  of llms by validating low-confidence generation.
\newblock \emph{arXiv preprint arXiv:2307.03987}, 2023.

\bibitem[Vu et~al.(2023)Vu, Iyyer, Wang, Constant, Wei, Wei, Tar, Sung, Zhou,
  Le, et~al.]{search1}
Vu, T., Iyyer, M., Wang, X., Constant, N., Wei, J., Wei, J., Tar, C., Sung,
  Y.-H., Zhou, D., Le, Q., et~al.
\newblock Freshllms: Refreshing large language models with search engine
  augmentation.
\newblock \emph{arXiv preprint arXiv:2310.03214}, 2023.

\bibitem[Wang \& Sennrich(2020)Wang and Sennrich]{exposurebias}
Wang, C. and Sennrich, R.
\newblock On exposure bias, hallucination and domain shift in neural machine
  translation.
\newblock \emph{arXiv preprint arXiv:2005.03642}, 2020.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Ma, Dong, Huang, Wang, Ma, Yang,
  Wang, Wu, and Wei]{bitnet}
Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L., Yang, F., Wang, R.,
  Wu, Y., and Wei, F.
\newblock Bitnet: Scaling 1-bit transformers for large language models.
\newblock \emph{arXiv preprint arXiv:2310.11453}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Sun, Chen, Li, and Gao]{cok}
Wang, J., Sun, Q., Chen, N., Li, X., and Gao, M.
\newblock Boosting language models reasoning with chain-of-knowledge prompting.
\newblock \emph{arXiv preprint arXiv:2306.06427}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Variengien, Conmy, Shlegeris, and
  Steinhardt]{104interpretability}
Wang, K., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J.
\newblock Interpretability in the wild: a circuit for indirect object
  identification in gpt-2 small, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Duan, Wang, Li, Xian, Yin, Rong,
  and Xiong]{mul_know}
Wang, K., Duan, F., Wang, S., Li, P., Xian, Y., Yin, C., Rong, W., and Xiong,
  Z.
\newblock Knowledge-driven cot: Exploring faithful reasoning in llms for
  knowledge-intensive question answering.
\newblock \emph{arXiv preprint arXiv:2308.13259}, 2023{\natexlab{c}}.

\bibitem[Wang et~al.(2023{\natexlab{d}})Wang, Li, Dai, Chen, Zhou, Meng, Zhou,
  and Sun]{wang2023label}
Wang, L., Li, L., Dai, D., Chen, D., Zhou, H., Meng, F., Zhou, J., and Sun, X.
\newblock Label words are anchors: An information flow perspective for
  understanding in-context learning.
\newblock \emph{arXiv preprint arXiv:2305.14160}, 2023{\natexlab{d}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Wei, Schuurmans, Le, Chi, Narang,
  Chowdhery, and Zhou]{selfconsist}
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A.,
  and Zhou, D.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2022{\natexlab{b}}.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou,
  et~al.]{cot1}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V.,
  Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 24824--24837, 2022.

\bibitem[Wu et~al.(2023{\natexlab{a}})Wu, Irsoy, Lu, Dabravolski, Dredze,
  Gehrmann, Kambadur, Rosenberg, and Mann]{finace}
Wu, S., Irsoy, O., Lu, S., Dabravolski, V., Dredze, M., Gehrmann, S., Kambadur,
  P., Rosenberg, D., and Mann, G.
\newblock Bloomberggpt: A large language model for finance.
\newblock \emph{arXiv preprint arXiv:2303.17564}, 2023{\natexlab{a}}.

\bibitem[Wu et~al.(2023{\natexlab{b}})Wu, Hu, Shi, Dziri, Suhr, Ammanabrolu,
  Smith, Ostendorf, and Hajishirzi]{generation2}
Wu, Z., Hu, Y., Shi, W., Dziri, N., Suhr, A., Ammanabrolu, P., Smith, N.~A.,
  Ostendorf, M., and Hajishirzi, H.
\newblock Fine-grained human feedback gives better rewards for language model
  training.
\newblock \emph{arXiv preprint arXiv:2306.01693}, 2023{\natexlab{b}}.

\bibitem[Wu et~al.(2023{\natexlab{c}})Wu, Hu, Shi, Dziri, Suhr, Ammanabrolu,
  Smith, Ostendorf, and Hajishirzi]{rl_llm2}
Wu, Z., Hu, Y., Shi, W., Dziri, N., Suhr, A., Ammanabrolu, P., Smith, N.~A.,
  Ostendorf, M., and Hajishirzi, H.
\newblock Fine-grained human feedback gives better rewards for language model
  training.
\newblock \emph{arXiv preprint arXiv:2306.01693}, 2023{\natexlab{c}}.

\bibitem[Xu et~al.(2023)Xu, Shi, and Choi]{recomp}
Xu, F., Shi, W., and Choi, E.
\newblock Recomp: Improving retrieval-augmented lms with context compression
  and selective augmentation.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2023.

\bibitem[Yoran et~al.(2023)Yoran, Wolfson, Ram, and Berant]{making_rb}
Yoran, O., Wolfson, T., Ram, O., and Berant, J.
\newblock Making retrieval-augmented language models robust to irrelevant
  context.
\newblock \emph{arXiv preprint arXiv:2310.01558}, 2023.

\bibitem[Yu et~al.(2021{\natexlab{a}})Yu, Zhu, Fang, Yu, Wang, Xu, Ren, Yang,
  and Zeng]{kg_ralms}
Yu, D., Zhu, C., Fang, Y., Yu, W., Wang, S., Xu, Y., Ren, X., Yang, Y., and
  Zeng, M.
\newblock Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain
  question answering.
\newblock \emph{arXiv preprint arXiv:2110.04330}, 2021{\natexlab{a}}.

\bibitem[Yu et~al.(2021{\natexlab{b}})Yu, Zhu, Fang, Yu, Wang, Xu, Ren, Yang,
  and Zeng]{reader2}
Yu, D., Zhu, C., Fang, Y., Yu, W., Wang, S., Xu, Y., Ren, X., Yang, Y., and
  Zeng, M.
\newblock Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain
  question answering.
\newblock \emph{arXiv preprint arXiv:2110.04330}, 2021{\natexlab{b}}.

\bibitem[Yu et~al.(2022)Yu, Zhu, Li, Hu, Wang, Ji, and Jiang]{beni_ralms}
Yu, W., Zhu, C., Li, Z., Hu, Z., Wang, Q., Ji, H., and Jiang, M.
\newblock A survey of knowledge-enhanced text generation.
\newblock \emph{ACM Computing Surveys}, 54\penalty0 (11s):\penalty0 1--38,
  2022.

\bibitem[Yu et~al.(2023{\natexlab{a}})Yu, Zhang, Pan, Ma, Wang, and
  Yu]{search2}
Yu, W., Zhang, H., Pan, X., Ma, K., Wang, H., and Yu, D.
\newblock Chain-of-note: Enhancing robustness in retrieval-augmented language
  models.
\newblock \emph{arXiv preprint arXiv:2311.09210}, 2023{\natexlab{a}}.

\bibitem[Yu et~al.(2023{\natexlab{b}})Yu, Zhang, Liang, Jiang, and
  Sabharwal]{end_ralms}
Yu, W., Zhang, Z., Liang, Z., Jiang, M., and Sabharwal, A.
\newblock Improving language models via plug-and-play retrieval feedback.
\newblock \emph{arXiv preprint arXiv:2305.14002}, 2023{\natexlab{b}}.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, et~al.]{bigbird}
Zaheer, M., Guruganesh, G., Dubey, K.~A., Ainslie, J., Alberti, C., Ontanon,
  S., Pham, P., Ravula, A., Wang, Q., Yang, L., et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 17283--17297, 2020.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Press, Merrill, Liu, and
  Smith]{snowball}
Zhang, M., Press, O., Merrill, W., Liu, A., and Smith, N.~A.
\newblock How language model hallucinations can snowball.
\newblock \emph{arXiv preprint arXiv:2305.13534}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Li, Cui, Cai, Liu, Fu, Huang,
  Zhao, Zhang, Chen, et~al.]{hallu1}
Zhang, Y., Li, Y., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E.,
  Zhang, Y., Chen, Y., et~al.
\newblock Siren's song in the ai ocean: A survey on hallucination in large
  language models.
\newblock \emph{arXiv preprint arXiv:2309.01219}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2023{\natexlab{c}})Zhang, Zhang, Li, Zhao, Karypis, and
  Smola]{mm}
Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and Smola, A.
\newblock Multimodal chain-of-thought reasoning in language models.
\newblock \emph{arXiv preprint arXiv:2302.00923}, 2023{\natexlab{c}}.

\bibitem[Zhao et~al.(2023)Zhao, Lin, Zhu, Ye, Chen, Zheng, Ceze, Krishnamurthy,
  Chen, and Kasikci]{atom}
Zhao, Y., Lin, C.-Y., Zhu, K., Ye, Z., Chen, L., Zheng, S., Ceze, L.,
  Krishnamurthy, A., Chen, T., and Kasikci, B.
\newblock Atom: Low-bit quantization for efficient and accurate llm serving.
\newblock \emph{arXiv preprint arXiv:2310.19102}, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li,
  Li, Xing, et~al.]{vicuna}
Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z.,
  Li, Z., Li, D., Xing, E., et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{arXiv preprint arXiv:2306.05685}, 2023.

\bibitem[Zhu et~al.(2021)Zhu, Lei, Wang, Zheng, Poria, and Chua]{refs_rc3}
Zhu, F., Lei, W., Wang, C., Zheng, J., Poria, S., and Chua, T.-S.
\newblock Retrieving and reading: A comprehensive survey on open-domain
  question answering.
\newblock \emph{arXiv preprint arXiv:2101.00774}, 2021.

\bibitem[Zhu et~al.(2023)Zhu, Li, Liu, Ma, and Wang]{qinglianghua}
Zhu, X., Li, J., Liu, Y., Ma, C., and Wang, W.
\newblock A survey on model compression for large language models.
\newblock \emph{arXiv preprint arXiv:2308.07633}, 2023.

\end{thebibliography}
