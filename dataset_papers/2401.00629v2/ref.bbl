\begin{thebibliography}{}

\bibitem[Achiam et~al., 2017]{AchHelDav_17}
Achiam, J., Held, D., Tamar, A., and Abbeel, P. (2017).
\newblock Constrained policy optimization.
\newblock In {\em Int. Conf. Machine Learning (ICML)}, volume~70, pages 22--31. JMLR.

\bibitem[Agarwal et~al., 2021]{AgaKakLee_21}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G. (2021).
\newblock On the theory of policy gradient methods: Optimality, approximation, and distribution shift.
\newblock {\em Journal of Machine Learning Research}, 22(98):1--76.

\bibitem[Altman, 1999]{Alt_99}
Altman, E. (1999).
\newblock {\em Constrained Markov decision processes}, volume~7.
\newblock CRC Press.

\bibitem[Bhardwaj et~al., 2024]{BhaXieBoo_24}
Bhardwaj, M., Xie, T., Boots, B., Jiang, N., and Cheng, C.-A. (2024).
\newblock Adversarial model for offline reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Chen et~al., 2022a]{ChenZhaWen_22}
Chen, F., Zhang, J., and Wen, Z. (2022a).
\newblock A near-optimal primal-dual method for off-policy learning in cmdp.
\newblock In {\em Advances Neural Information Processing Systems (NeurIPS)}, volume~35, pages 10521--10532.

\bibitem[Chen and Jiang, 2019]{ChenJia_19}
Chen, J. and Jiang, N. (2019).
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In {\em Int. Conf. Machine Learning (ICML)}, pages 1042--1051. PMLR.

\bibitem[Chen and Jiang, 2022]{CjeJia_22}
Chen, J. and Jiang, N. (2022).
\newblock Offline reinforcement learning under value and density-ratio realizability: the power of gaps.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 378--388. PMLR.

\bibitem[Chen et~al., 2022b]{CheJaiLuo_22}
Chen, L., Jain, R., and Luo, H. (2022b).
\newblock Learning infinite-horizon average-reward markov decision process with constraints.
\newblock In {\em Int. Conf. Machine Learning (ICML)}, pages 3246--3270. PMLR.

\bibitem[Chen et~al., 2021]{CheLuRaj_21}
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. (2021).
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em Advances in neural information processing systems}, 34:15084--15097.

\bibitem[Cheng et~al., 2020]{CheKolAga_20}
Cheng, C.-A., Kolobov, A., and Agarwal, A. (2020).
\newblock Policy improvement via imitation of multiple oracles.
\newblock In {\em Advances Neural Information Processing Systems (NeurIPS)}, volume~33, pages 5587--5598.

\bibitem[Cheng et~al., 2022]{CheXieJia_22}
Cheng, C.-A., Xie, T., Jiang, N., and Agarwal, A. (2022).
\newblock Adversarially trained actor critic for offline reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 3852--3878. PMLR.

\bibitem[Chow et~al., 2017]{ChoGhaJan_17}
Chow, Y., Ghavamzadeh, M., Janson, L., and Pavone, M. (2017).
\newblock Risk-constrained reinforcement learning with percentile risk criteria.
\newblock {\em The Journal of Machine Learning Research}, 18(1):6070--6120.

\bibitem[Cui and Du, 2022]{cui2022offline}
Cui, Q. and Du, S.~S. (2022).
\newblock When are offline two-player zero-sum markov games solvable?
\newblock {\em Advances in Neural Information Processing Systems}, 35:25779--25791.

\bibitem[Efroni et~al., 2020]{EfrManPir_20}
Efroni, Y., Mannor, S., and Pirotta, M. (2020).
\newblock Exploration-exploitation in constrained {MDP}s.
\newblock {\em arXiv preprint arXiv:2003.02189}.

\bibitem[Even-Dar et~al., 2009]{EveEyaKak_09}
Even-Dar, E., Kakade, S.~M., and Mansour, Y. (2009).
\newblock Online markov decision processes.
\newblock {\em Mathematics of Operations Research}, 34(3):726--736.

\bibitem[Fujimoto et~al., 2019]{FujMegPre_19}
Fujimoto, S., Meger, D., and Precup, D. (2019).
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em International conference on machine learning}, pages 2052--2062. PMLR.

\bibitem[Fujimoto et~al., 2018]{FujHooMeg_18}
Fujimoto, S., van Hoof, H., and Meger, D. (2018).
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em Int. Conf. Machine Learning (ICML)}, pages 1582--1591.

\bibitem[Geist et~al., 2019]{GeiSchPie_19}
Geist, M., Scherrer, B., and Pietquin, O. (2019).
\newblock A theory of regularized markov decision processes.
\newblock In {\em International Conference on Machine Learning}, pages 2160--2169. PMLR.

\bibitem[Haarnoja et~al., 2018]{HaaZhoAur_18}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018).
\newblock {Soft Actor-Critic: O}ff-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In {\em Int. Conf. Machine Learning (ICML)}, pages 1861--1870.

\bibitem[Hong et~al., 2024]{HonLiTew_24}
Hong, K., Li, Y., and Tewari, A. (2024).
\newblock A primal-dual-critic algorithm for offline constrained reinforcement learning.
\newblock In {\em Int. Conf. Artificial Intelligence and Statistics (AISTATS)}, pages 280--288. PMLR.

\bibitem[Isele et~al., 2018]{IseNakFuj_18}
Isele, D., Nakhaei, A., and Fujimura, K. (2018).
\newblock Safe reinforcement learning on autonomous vehicles.
\newblock In {\em 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, pages 1--6. IEEE.

\bibitem[Kakade and Langford, 2002]{KakLan_02}
Kakade, S. and Langford, J. (2002).
\newblock Approximately optimal approximate reinforcement learning.
\newblock In {\em Int. Conf. Machine Learning (ICML)}, pages 267--274.

\bibitem[Kakade, 2001]{Kak_01}
Kakade, S.~M. (2001).
\newblock A natural policy gradient.
\newblock In {\em Advances Neural Information Processing Systems (NeurIPS)}.

\bibitem[Kingma and Ba, 2015]{Kin_Ba_15}
Kingma, D.~P. and Ba, J. (2015).
\newblock Adam: {A} method for stochastic optimization.
\newblock In Bengio, Y. and LeCun, Y., editors, {\em Int. Conf. on Learning Representations (ICLR)}.

\bibitem[Kumar et~al., 2019]{KumFuSoh_19}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S. (2019).
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Kumar et~al., 2022]{KumHonSin_22}
Kumar, A., Hong, J., Singh, A., and Levine, S. (2022).
\newblock Should i run offline reinforcement learning or behavioral cloning?
\newblock In {\em Int. Conf. on Learning Representations (ICLR)}.

\bibitem[Laroche et~al., 2019]{LarTriDes_19}
Laroche, R., Trichelair, P., and Des~Combes, R.~T. (2019).
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In {\em International conference on machine learning}, pages 3652--3661. PMLR.

\bibitem[Le et~al., 2019]{LeVolYue_19}
Le, H., Voloshin, C., and Yue, Y. (2019).
\newblock Batch policy learning under constraints.
\newblock In {\em International Conference on Machine Learning}, pages 3703--3712. PMLR.

\bibitem[Lee et~al., 2021]{LeeJeoLee_21}
Lee, J., Jeon, W., Lee, B., Pineau, J., and Kim, K.-E. (2021).
\newblock Optidice: Offline policy optimization via stationary distribution correction estimation.
\newblock In Meila, M. and Zhang, T., editors, {\em Int. Conf. Machine Learning (ICML)}, volume 139 of {\em Proceedings of Machine Learning Research}, pages 6120--6130. PMLR.

\bibitem[Lee et~al., 2022]{LeePadMan_22}
Lee, J., Paduraru, C., Mankowitz, D.~J., Heess, N., Precup, D., Kim, K.-E., and Guez, A. (2022).
\newblock Coptidice: Offline constrained reinforcement learning via stationary distribution correction estimation.
\newblock {\em arXiv preprint arXiv:2204.08957}.

\bibitem[Liao et~al., 2022]{LiaoQiWan_22}
Liao, P., Qi, Z., Wan, R., Klasnja, P., and Murphy, S.~A. (2022).
\newblock Batch policy learning in average reward markov decision processes.
\newblock {\em Annals of statistics}, 50(6):3364.

\bibitem[Liu et~al., 2019]{LiuCaiYan_19}
Liu, B., Cai, Q., Yang, Z., and Wang, Z. (2019).
\newblock Neural trust region/proximal policy optimization attains globally optimal policy.
\newblock {\em Advances in neural information processing systems}, 32.

\bibitem[Liu et~al., 2023a]{LiuGuoLin_23}
Liu, Z., Guo, Z., Lin, H., Yao, Y., Zhu, J., Cen, Z., Hu, H., Yu, W., Zhang, T., Tan, J., et~al. (2023a).
\newblock Datasets and benchmarks for offline safe reinforcement learning.
\newblock {\em arXiv preprint arXiv:2306.09303}.

\bibitem[Liu et~al., 2023b]{LiuGuoYao_23}
Liu, Z., Guo, Z., Yao, Y., Cen, Z., Yu, W., Zhang, T., and Zhao, D. (2023b).
\newblock Constrained decision transformer for offline safe reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages 21611--21630. PMLR.

\bibitem[Ozdaglar et~al., 2023]{OzdPatZha_23}
Ozdaglar, A.~E., Pattathil, S., Zhang, J., and Zhang, K. (2023).
\newblock Revisiting the linear-programming framework for offline rl with general function approximation.
\newblock In {\em International Conference on Machine Learning}, pages 26769--26791. PMLR.

\bibitem[Pirotta et~al., 2013]{PirResPec_13}
Pirotta, M., Restelli, M., Pecorino, A., and Calandriello, D. (2013).
\newblock Safe policy iteration.
\newblock In {\em Int. Conf. Machine Learning (ICML)}, pages 307--315. PMLR.

\bibitem[Rajaraman et~al., 2020]{RajYanJia_20}
Rajaraman, N., Yang, L., Jiao, J., and Ramchandran, K. (2020).
\newblock Toward the fundamental limits of imitation learning.
\newblock {\em Advances Neural Information Processing Systems (NeurIPS)}, 33:2914--2924.

\bibitem[Rashidinejad et~al., 2021]{RasZhuMa_21}
Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. (2021).
\newblock Bridging offline reinforcement learning and imitation learning: A tale of pessimism.
\newblock In {\em Advances Neural Information Processing Systems (NeurIPS)}, volume~34, pages 11702--11716.

\bibitem[Rashidinejad et~al., 2022]{RasZhuYan_22}
Rashidinejad, P., Zhu, H., Yang, K., Russell, S., and Jiao, J. (2022).
\newblock Optimal conservative offline rl with general function approximation via augmented lagrangian.
\newblock {\em arXiv preprint arXiv:2211.00716}.

\bibitem[Siegel et~al., 2020]{SieSprJos_20}
Siegel, N.~Y., Springenberg, J.~T., Berkenkamp, F., Abdolmaleki, A., Neunert, M., Lampe, T., Hafner, R., Heess, N., and Riedmiller, M. (2020).
\newblock Keep doing what worked: Behavioral modelling priors for offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2002.08396}.

\bibitem[Stooke et~al., 2020]{StoAchAbb_20}
Stooke, A., Achiam, J., and Abbeel, P. (2020).
\newblock Responsive safety in reinforcement learning by pid lagrangian methods.
\newblock In {\em Int. Conf. Machine Learning (ICML)}, pages 9133--9143. PMLR.

\bibitem[Uehara et~al., 2020]{UehHuaJia_20}
Uehara, M., Huang, J., and Jiang, N. (2020).
\newblock Minimax weight and q-function learning for off-policy evaluation.
\newblock In {\em International Conference on Machine Learning}, pages 9659--9668. PMLR.

\bibitem[Uehara et~al., 2024]{UehKalLee_24}
Uehara, M., Kallus, N., Lee, J.~D., and Sun, W. (2024).
\newblock Offline minimax soft-q-learning under realizability and partial coverage.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Uehara and Sun, 2021]{UehSun_21}
Uehara, M. and Sun, W. (2021).
\newblock Pessimistic model-based offline reinforcement learning under partial coverage.
\newblock {\em arXiv preprint arXiv:2107.06226}.

\bibitem[Von~Stackelberg, 2010]{VonHei_10}
Von~Stackelberg, H. (2010).
\newblock {\em Market structure and equilibrium}.
\newblock Springer Science \& Business Media.

\bibitem[Wang et~al., 2019]{WanCaiYan_19}
Wang, L., Cai, Q., Yang, Z., and Wang, Z. (2019).
\newblock Neural policy gradient methods: Global optimality and rates of convergence.
\newblock {\em arXiv preprint arXiv:1909.01150}.

\bibitem[Wu et~al., 2021]{WuZhaYan_21}
Wu, R., Zhang, Y., Yang, Z., and Wang, Z. (2021).
\newblock Offline constrained multi-objective reinforcement learning via pessimistic dual value iteration.
\newblock In {\em Advances Neural Information Processing Systems (NeurIPS)}, volume~34, pages 25439--25451.

\bibitem[Xie et~al., 2021]{XieCheJia_21}
Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2021).
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 34:6683--6694.

\bibitem[Xie and Jiang, 2020]{XieJia_20}
Xie, T. and Jiang, N. (2020).
\newblock Q* approximation schemes for batch reinforcement learning: A theoretical comparison.
\newblock In {\em Conference on Uncertainty in Artificial Intelligence}, pages 550--559. PMLR.

\bibitem[Xie and Jiang, 2021]{XieJia_21}
Xie, T. and Jiang, N. (2021).
\newblock Batch value-function approximation with only realizability.
\newblock In {\em Int. Conf. Machine Learning (ICML)}, pages 11404--11413. PMLR.

\bibitem[Xu et~al., 2022]{XuZhaZhu_22}
Xu, H., Zhan, X., and Zhu, X. (2022).
\newblock Constraints penalized q-learning for safe offline reinforcement learning.
\newblock In {\em AAAI Conf. Artificial Intelligence}, volume~36, pages 8753--8760.

\bibitem[Yin and Wang, 2021]{YinWan_21}
Yin, M. and Wang, Y.-X. (2021).
\newblock Towards instance-optimal offline reinforcement learning with pessimism.
\newblock {\em Advances in neural information processing systems}, 34:4065--4078.

\bibitem[Zhan et~al., 2022]{ZhaHuaHua_22}
Zhan, W., Huang, B., Huang, A., Jiang, N., and Lee, J. (2022).
\newblock Offline reinforcement learning with realizability and single-policy concentrability.
\newblock In {\em Proc. Conf. Learning Theory (COLT)}, pages 2730--2775. PMLR.

\bibitem[Zhang et~al., 2020]{ZhaKopBed_20}
Zhang, J., Koppel, A., Bedi, A.~S., Szepesvari, C., and Wang, M. (2020).
\newblock Variational policy gradient method for reinforcement learning with general utilities.
\newblock {\em Advances in Neural Information Processing Systems}, 33:4572--4583.

\bibitem[Zheng et~al., 2024]{ZheLiYu_24}
Zheng, Y., Li, J., Yu, D., Yang, Y., Li, S.~E., Zhan, X., and Liu, J. (2024).
\newblock Safe offline reinforcement learning with feasibility-guided diffusion model.
\newblock {\em arXiv preprint arXiv:2401.10700}.

\bibitem[Zhu et~al., 2023]{ZhuRasJia_23}
Zhu, H., Rashidinejad, P., and Jiao, J. (2023).
\newblock Importance weighted actor-critic for optimal conservative offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2301.12714}.

\end{thebibliography}
