\begin{thebibliography}{103}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjevani et~al.(2019)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{arjevani19lower_stoch_NC_arxiv}
Arjevani, Y., Carmon, Y., Duchi, J.~C., Foster, D.~J., Srebro, N., and
  Woodworth, B.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1912.02365}, 2019.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky17WGANs_icml}
Arjovsky, M., Chintala, S., and Bottou, L.
\newblock Wasserstein generative adversarial networks.
\newblock In \emph{International conference on machine learning}, pp.\
  214--223. PMLR, 2017.

\bibitem[Beznosikov et~al.(2020)Beznosikov, Samokhin, and
  Gasnikov]{beznosikov20dist_SP_arxiv}
Beznosikov, A., Samokhin, V., and Gasnikov, A.
\newblock Distributed saddle-point problems: Lower bounds, optimal algorithms
  and federated gans.
\newblock \emph{arXiv preprint arXiv:2010.13112}, 2020.

\bibitem[Beznosikov et~al.(2021{\natexlab{a}})Beznosikov, Dvurechensky,
  Koloskova, Samokhin, Stich, and Gasnikov]{gasnikov21dec_stoch_EG_VI_arxiv}
Beznosikov, A., Dvurechensky, P., Koloskova, A., Samokhin, V., Stich, S.~U.,
  and Gasnikov, A.
\newblock Decentralized local stochastic extra-gradient for variational
  inequalities.
\newblock \emph{arXiv preprint arXiv:2106.08315}, 2021{\natexlab{a}}.

\bibitem[Beznosikov et~al.(2021{\natexlab{b}})Beznosikov, Richt{\'a}rik,
  Diskin, Ryabinin, and Gasnikov]{richtarik21dist_VI_comm_arxiv}
Beznosikov, A., Richt{\'a}rik, P., Diskin, M., Ryabinin, M., and Gasnikov, A.
\newblock Distributed methods with compressed communication for solving
  variational inequalities, with theoretical guarantees.
\newblock \emph{arXiv preprint arXiv:2110.03313}, 2021{\natexlab{b}}.

\bibitem[Beznosikov et~al.(2021{\natexlab{c}})Beznosikov, Rogozin, Kovalev, and
  Gasnikov]{gasnikov21decen_deter_cc_icoa}
Beznosikov, A., Rogozin, A., Kovalev, D., and Gasnikov, A.
\newblock Near-optimal decentralized algorithms for saddle point problems over
  time-varying networks.
\newblock In \emph{International Conference on Optimization and Applications},
  pp.\  246--257. Springer, 2021{\natexlab{c}}.

\bibitem[Beznosikov et~al.(2021{\natexlab{d}})Beznosikov, Scutari, Rogozin, and
  Gasnikov]{beznosikov21dist_sp_neurips}
Beznosikov, A., Scutari, G., Rogozin, A., and Gasnikov, A.
\newblock Distributed saddle-point problems under similarity.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, 2021{\natexlab{d}}.

\bibitem[Beznosikov et~al.(2021{\natexlab{e}})Beznosikov, Sushko, Sadiev, and
  Gasnikov]{gasnikov21dec_person_FL_arxiv}
Beznosikov, A., Sushko, V., Sadiev, A., and Gasnikov, A.
\newblock Decentralized personalized federated min-max problems.
\newblock \emph{arXiv preprint arXiv:2106.07289}, 2021{\natexlab{e}}.

\bibitem[Bonawitz et~al.(2019)Bonawitz, Eichner, Grieskamp, Huba, Ingerman,
  Ivanov, Kiddon, Kone{\v{c}}n{\`y}, Mazzocchi, McMahan,
  et~al.]{bonawitz19towardsFL_arxiv}
Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V.,
  Kiddon, C., Kone{\v{c}}n{\`y}, J., Mazzocchi, S., McMahan, H.~B., et~al.
\newblock Towards federated learning at scale: System design.
\newblock \emph{arXiv preprint arXiv:1902.01046}, 2019.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{bottou18optML_siam}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{Siam Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Charles \& Papailiopoulos(2018)Charles and
  Papailiopoulos]{charles18generalization_icml}
Charles, Z. and Papailiopoulos, D.
\newblock Stability and generalization of learning algorithms that converge to
  global optima.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  745--754. PMLR, 2018.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Yang, Shen, and
  Pang]{chen20dist_GAN_quantize_arxiv}
Chen, X., Yang, S., Shen, L., and Pang, X.
\newblock A distributed training algorithm of generative adversarial networks
  with quantized gradients.
\newblock \emph{arXiv preprint arXiv:2010.13359}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Zhou, Xu, and
  Liang]{liang20proxGDA_KL_iclr}
Chen, Z., Zhou, Y., Xu, T., and Liang, Y.
\newblock Proximal gradient descent-ascent: Variable convergence under k{\l}
  geometry.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.

\bibitem[Dai et~al.(2017)Dai, He, Pan, Boots, and Song]{dai17learning_aistats}
Dai, B., He, N., Pan, Y., Boots, B., and Song, L.
\newblock Learning from conditional distributions via dual embeddings.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1458--1467.
  PMLR, 2017.

\bibitem[Dai et~al.(2018)Dai, Shaw, Li, Xiao, He, Liu, Chen, and
  Song]{dai18sbeed_RL_nonlin_FA_icml}
Dai, B., Shaw, A., Li, L., Xiao, L., He, N., Liu, Z., Chen, J., and Song, L.
\newblock Sbeed: Convergent reinforcement learning with nonlinear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1125--1134. PMLR, 2018.

\bibitem[Daskalakis et~al.(2018)Daskalakis, Ilyas, Syrgkanis, and
  Zeng]{daskalakis18GANs_iclr}
Daskalakis, C., Ilyas, A., Syrgkanis, V., and Zeng, H.
\newblock Training gans with optimism.
\newblock In \emph{International Conference on Learning Representations (ICLR
  2018)}, 2018.

\bibitem[Daskalakis et~al.(2021)Daskalakis, Skoulakis, and
  Zampetakis]{daskalakis21constr_minmax_sigact}
Daskalakis, C., Skoulakis, S., and Zampetakis, M.
\newblock The complexity of constrained min-max optimization.
\newblock In \emph{Proceedings of the 53rd Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  1466--1478, 2021.

\bibitem[Davis \& Drusvyatskiy(2019)Davis and Drusvyatskiy]{davis19wc_siam}
Davis, D. and Drusvyatskiy, D.
\newblock Stochastic model-based minimization of weakly convex functions.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (1):\penalty0
  207--239, 2019.

\bibitem[Deng \& Mahdavi(2021)Deng and Mahdavi]{mahdavi21localSGDA_aistats}
Deng, Y. and Mahdavi, M.
\newblock Local stochastic gradient descent ascent: Convergence analysis and
  communication efficiency.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1387--1395. PMLR, 2021.

\bibitem[Deng et~al.(2020)Deng, Kamani, and
  Mahdavi]{mahdavi20dist_robustfl_neurips}
Deng, Y., Kamani, M.~M., and Mahdavi, M.
\newblock Distributionally robust federated averaging.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  15111--15122, 2020.

\bibitem[Diakonikolas et~al.(2021)Diakonikolas, Daskalakis, and
  Jordan]{diakonikolas21NC_NC_aistats}
Diakonikolas, J., Daskalakis, C., and Jordan, M.
\newblock Efficient methods for structured nonconvex-nonconcave min-max
  optimization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2746--2754. PMLR, 2021.

\bibitem[Drusvyatskiy \& Paquette(2019)Drusvyatskiy and
  Paquette]{drusvyatskiy19wc_mathprog}
Drusvyatskiy, D. and Paquette, C.
\newblock Efficiency of minimizing compositions of convex functions and smooth
  maps.
\newblock \emph{Mathematical Programming}, 178\penalty0 (1):\penalty0 503--558,
  2019.

\bibitem[Fiez et~al.(2021)Fiez, Ratliff, Mazumdar, Faulkner, and
  Narang]{fiez21NC_PL_SC_neurips}
Fiez, T., Ratliff, L., Mazumdar, E., Faulkner, E., and Narang, A.
\newblock Global convergence to local minmax equilibrium in classes of
  nonconvex zero-sum games.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow14GANs_neurips}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems},
  volume~27, 2014.

\bibitem[Gu et~al.(2021)Gu, Huang, Zhang, and Huang]{gu21mifa_neurips}
Gu, X., Huang, K., Zhang, J., and Huang, L.
\newblock Fast federated learning in the presence of arbitrary device
  unavailability.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, 2021.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani17improved_WGANs_neurips}
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A.~C.
\newblock Improved training of wasserstein gans.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem[Guminov \& Gasnikov(2017)Guminov and
  Gasnikov]{gasnikov17acc_quasar_convex_arxiv}
Guminov, S. and Gasnikov, A.
\newblock Accelerated methods for $\alpha$-weakly-quasi-convex problems.
\newblock \emph{arXiv preprint arXiv:1710.00797}, 2017.

\bibitem[Guo et~al.(2020)Guo, Liu, Yuan, Shen, Liu, and
  Yang]{guo20DeepAUC_icml}
Guo, Z., Liu, M., Yuan, Z., Shen, L., Liu, W., and Yang, T.
\newblock Communication-efficient distributed stochastic {AUC} maximization
  with deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3864--3874. PMLR, 2020.

\bibitem[Haddadpour \& Mahdavi(2019)Haddadpour and
  Mahdavi]{haddadpour19conv_FL_arxiv}
Haddadpour, F. and Mahdavi, M.
\newblock On the convergence of local descent methods in federated learning.
\newblock \emph{arXiv preprint arXiv:1910.14425}, 2019.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel17gans_neurips}
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Hinder et~al.(2020)Hinder, Sidford, and
  Sohoni]{hinder20near_opt_star_convex_colt}
Hinder, O., Sidford, A., and Sohoni, N.
\newblock Near-optimal methods for minimizing star-convex functions and beyond.
\newblock In \emph{Conference on Learning Theory}, pp.\  1894--1938. PMLR,
  2020.

\bibitem[Hou et~al.(2021)Hou, Thekumparampil, Fanti, and Oh]{hou21FedSP_arxiv}
Hou, C., Thekumparampil, K.~K., Fanti, G., and Oh, S.
\newblock Efficient algorithms for federated saddle point optimization.
\newblock \emph{arXiv preprint arXiv:2102.06333}, 2021.

\bibitem[Hsieh et~al.(2021)Hsieh, Mertikopoulos, and
  Cevher]{hsieh21limits_minmax_icml}
Hsieh, Y.-P., Mertikopoulos, P., and Cevher, V.
\newblock The limits of min-max optimization algorithms: Convergence to
  spurious non-critical sets.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4337--4348. PMLR, 2021.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot18NTK_neurips}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, pp.\  8580--8589, 2018.

\bibitem[Jiang \& Agrawal(2018)Jiang and Agrawal]{jiang18linear_neurips}
Jiang, P. and Agrawal, G.
\newblock A linear speedup analysis of distributed deep learning with sparse
  and quantized communication.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2530--2541, 2018.

\bibitem[Jin et~al.(2020)Jin, Netrapalli, and Jordan]{jin20local_opt_NCNC_icml}
Jin, C., Netrapalli, P., and Jordan, M.
\newblock What is local optimality in nonconvex-nonconcave minimax
  optimization?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4880--4889. PMLR, 2020.

\bibitem[Jin(2020)]{jin20quasar_convex_arxiv}
Jin, J.
\newblock On the convergence of first order methods for quasar-convex
  optimization.
\newblock \emph{arXiv preprint arXiv:2010.04937}, 2020.

\bibitem[Kairouz et~al.(2019)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, et~al.]{kairouz19advancesFL_arxiv}
Kairouz, P., McMahan, H.~B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.~N.,
  Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{arXiv preprint arXiv:1912.04977}, 2019.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and
  Schmidt]{schmidt16lin_conv_PL_kdd}
Karimi, H., Nutini, J., and Schmidt, M.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  795--811. Springer, 2016.

\bibitem[Khaled et~al.(2020)Khaled, Mishchenko, and
  Richt{\'a}rik]{khaled20localSGD_aistats}
Khaled, A., Mishchenko, K., and Richt{\'a}rik, P.
\newblock Tighter theory for local sgd on identical and heterogeneous data.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  4519--4529. PMLR, 2020.

\bibitem[Kleinberg et~al.(2018)Kleinberg, Li, and Yuan]{kleinberg18icml}
Kleinberg, B., Li, Y., and Yuan, Y.
\newblock An alternative view: When does sgd escape local minima?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2698--2707. PMLR, 2018.

\bibitem[Koloskova et~al.(2020)Koloskova, Loizou, Boreiri, Jaggi, and
  Stich]{koloskova20unified_localSGD_icml}
Koloskova, A., Loizou, N., Boreiri, S., Jaggi, M., and Stich, S.
\newblock A unified theory of decentralized sgd with changing topology and
  local updates.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5381--5393. PMLR, 2020.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Ramage, and
  Richt{\'a}rik]{konevcny16federated}
Kone{\v{c}}n{\`y}, J., McMahan, H.~B., Ramage, D., and Richt{\'a}rik, P.
\newblock Federated optimization: Distributed machine learning for on-device
  intelligence.
\newblock \emph{arXiv preprint arXiv:1610.02527}, 2016.

\bibitem[L{\'e}aut{\'e} \& Faltings(2013)L{\'e}aut{\'e} and
  Faltings]{leaute13protecting}
L{\'e}aut{\'e}, T. and Faltings, B.
\newblock Protecting privacy through distributed computation in multi-agent
  decision making.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  649--695, 2013.

\bibitem[Lee \& Kim(2021)Lee and Kim]{lee21NCNC_structured_neurips}
Lee, S. and Kim, D.
\newblock Fast extra gradient methods for smooth structured
  nonconvex-nonconcave minimax problems.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, 2021.

\bibitem[Lei et~al.(2021)Lei, Yang, Yang, and
  Ying]{lei21stability_Minimax_icml}
Lei, Y., Yang, Z., Yang, T., and Ying, Y.
\newblock Stability and generalization of stochastic gradient methods for
  minimax problems.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6175--6186. PMLR, 2021.

\bibitem[Li et~al.(2021)Li, Tian, Zhang, and
  Jadbabaie]{li21lower_bd_NCSC_neurips}
Li, H., Tian, Y., Zhang, J., and Jadbabaie, A.
\newblock Complexity lower bounds for nonconvex-strongly-concave min-max
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, 2021.

\bibitem[Li \& Yuan(2017)Li and Yuan]{li17relu_neurips}
Li, Y. and Yuan, Y.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem[Liao et~al.(2021)Liao, Shen, Duan, Kolar, and
  Tao]{liao21local_AdaGrad_CC_arxiv}
Liao, L., Shen, L., Duan, J., Kolar, M., and Tao, D.
\newblock Local adagrad-type algorithm for stochastic convex-concave minimax
  problems.
\newblock \emph{arXiv preprint arXiv:2106.10022}, 2021.

\bibitem[Lin et~al.(2020{\natexlab{a}})Lin, Jin, and Jordan]{lin_GDA_icml20}
Lin, T., Jin, C., and Jordan, M.
\newblock On gradient descent ascent for nonconvex-concave minimax problems.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6083--6093. PMLR, 2020{\natexlab{a}}.

\bibitem[Lin et~al.(2020{\natexlab{b}})Lin, Jin, and
  Jordan]{lin20near_opt_det_colt}
Lin, T., Jin, C., and Jordan, M.~I.
\newblock Near-optimal algorithms for minimax optimization.
\newblock In \emph{Conference on Learning Theory}, pp.\  2738--2779. PMLR,
  2020{\natexlab{b}}.

\bibitem[Liu et~al.(2022)Liu, Zhu, and Belkin]{liu22overparameter_NN_elsevier}
Liu, C., Zhu, L., and Belkin, M.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock \emph{Applied and Computational Harmonic Analysis}, 2022.

\bibitem[Liu et~al.(2020)Liu, Mroueh, Zhang, Cui, Ross, and
  Das]{liu20dec_GANs_neurips}
Liu, M.~L., Mroueh, Y., Zhang, W., Cui, X., Ross, J., and Das, P.
\newblock Decentralized parallel algorithm for training generative adversarial
  nets.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  11056--11070, 2020.

\bibitem[Liu et~al.(2019)Liu, Mokhtari, Ozdaglar, Pattathil, Shen, and
  Zheng]{ozdaglar19dec_prox_sp_arxiv}
Liu, W., Mokhtari, A., Ozdaglar, A., Pattathil, S., Shen, Z., and Zheng, N.
\newblock A decentralized proximal point-type method for saddle point problems.
\newblock \emph{arXiv preprint arXiv:1910.14380}, 2019.

\bibitem[Lu et~al.(2020)Lu, Tsaknakis, Hong, and Chen]{lu20HiBSA_NC_C_ieee}
Lu, S., Tsaknakis, I., Hong, M., and Chen, Y.
\newblock Hybrid block successive approximation for one-sided non-convex
  min-max problems: algorithms and applications.
\newblock \emph{IEEE Transactions on Signal Processing}, 68:\penalty0
  3676--3691, 2020.

\bibitem[Luo et~al.(2020)Luo, Ye, Huang, and Zhang]{luo20SREDA_ncsc_neurips}
Luo, L., Ye, H., Huang, Z., and Zhang, T.
\newblock Stochastic recursive gradient descent ascent for stochastic
  nonconvex-strongly-concave minimax problems.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  20566--20577, 2020.

\bibitem[Luo et~al.(2021)Luo, Xie, Zhang, and Zhang]{luo21near_opt_FS_cc_arxiv}
Luo, L., Xie, G., Zhang, T., and Zhang, Z.
\newblock Near optimal stochastic algorithms for finite-sum unbalanced
  convex-concave minimax optimization.
\newblock \emph{arXiv preprint arXiv:2106.01761}, 2021.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry18adversarial_iclr}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{fedavg17aistats}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1273--1282.
  PMLR, 2017.

\bibitem[Mertikopoulos et~al.(2018)Mertikopoulos, Lecouat, Zenati, Foo,
  Chandrasekhar, and Piliouras]{mertikopoulos18optMD_SP_iclr}
Mertikopoulos, P., Lecouat, B., Zenati, H., Foo, C.-S., Chandrasekhar, V., and
  Piliouras, G.
\newblock Optimistic mirror descent in saddle-point problems: Going the extra
  (gradient) mile.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Mohri et~al.(2019)Mohri, Sivek, and Suresh]{mohri19agnosticFL_icml}
Mohri, M., Sivek, G., and Suresh, A.~T.
\newblock Agnostic federated learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4615--4625. PMLR, 2019.

\bibitem[Namkoong \& Duchi(2016)Namkoong and Duchi]{namkoong16SG_DRO_neurips}
Namkoong, H. and Duchi, J.~C.
\newblock Stochastic gradient methods for distributionally robust optimization
  with f-divergences.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~29, 2016.

\bibitem[Namkoong \& Duchi(2017)Namkoong and
  Duchi]{namkoong17var_regular_neurips}
Namkoong, H. and Duchi, J.~C.
\newblock Variance-based regularization with convex objectives.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem[Nedi{\'c} \& Ozdaglar(2009)Nedi{\'c} and
  Ozdaglar]{nedic09subgradient_jota}
Nedi{\'c}, A. and Ozdaglar, A.
\newblock Subgradient methods for saddle-point problems.
\newblock \emph{Journal of optimization theory and applications}, 142\penalty0
  (1):\penalty0 205--228, 2009.

\bibitem[Nemirovski(2004)]{nemirovski04prox_siam}
Nemirovski, A.
\newblock Prox-method with rate of convergence o (1/t) for variational
  inequalities with lipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock \emph{SIAM Journal on Optimization}, 15\penalty0 (1):\penalty0
  229--251, 2004.

\bibitem[Nesterov(2018)]{nesterov18book}
Nesterov, Y.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Nouiehed et~al.(2019)Nouiehed, Sanjabi, Huang, Lee, and
  Razaviyayn]{nouiehed19minimax_neurips19}
Nouiehed, M., Sanjabi, M., Huang, T., Lee, J.~D., and Razaviyayn, M.
\newblock Solving a class of non-convex min-max games using iterative first
  order methods.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, pp.\  14934--14942, 2019.

\bibitem[Ouyang \& Xu(2021)Ouyang and Xu]{ouyang21lower_cc_bilinear_mathprog}
Ouyang, Y. and Xu, Y.
\newblock Lower complexity bounds of first-order methods for convex-concave
  bilinear saddle-point problems.
\newblock \emph{Mathematical Programming}, 185\penalty0 (1):\penalty0 1--35,
  2021.

\bibitem[Polyak(1963)]{polyak63PL}
Polyak, B.~T.
\newblock Gradient methods for minimizing functionals.
\newblock \emph{Zhurnal vychislitel'noi matematiki i matematicheskoi fiziki},
  3\penalty0 (4):\penalty0 643--653, 1963.

\bibitem[Qiu et~al.(2020)Qiu, Yang, Wei, Ye, and
  Wang]{qiu20single_timescale_ncsc}
Qiu, S., Yang, Z., Wei, X., Ye, J., and Wang, Z.
\newblock Single-timescale stochastic nonconvex-concave optimization for smooth
  nonlinear {TD} learning.
\newblock \emph{arXiv preprint arXiv:2008.10103}, 2020.

\bibitem[Rafique et~al.(2021)Rafique, Liu, Lin, and Yang]{rafique18WCC_oms}
Rafique, H., Liu, M., Lin, Q., and Yang, T.
\newblock Weakly-convex--concave min--max optimization: provable algorithms and
  applications in machine learning.
\newblock \emph{Optimization Methods and Software}, pp.\  1--35, 2021.

\bibitem[Reisizadeh et~al.(2020)Reisizadeh, Farnia, Pedarsani, and
  Jadbabaie]{reisizadeh20robustfl_neurips}
Reisizadeh, A., Farnia, F., Pedarsani, R., and Jadbabaie, A.
\newblock Robust federated learning: The case of affine distribution shifts.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  21554--21565, 2020.

\bibitem[Rogozin et~al.(2021)Rogozin, Beznosikov, Dvinskikh, Kovalev,
  Dvurechensky, and Gasnikov]{rogozin21dec_local_global_var_cc_arxiv}
Rogozin, A., Beznosikov, A., Dvinskikh, D., Kovalev, D., Dvurechensky, P., and
  Gasnikov, A.
\newblock Decentralized distributed optimization for saddle point problems.
\newblock \emph{arXiv preprint arXiv:2102.07758}, 2021.

\bibitem[Ruan et~al.(2021)Ruan, Zhang, Liang, and
  Joe-Wong]{ruan21device_part_FL_aistats}
Ruan, Y., Zhang, X., Liang, S.-C., and Joe-Wong, C.
\newblock Towards flexible device participation in federated learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3403--3411. PMLR, 2021.

\bibitem[Shen et~al.(2021)Shen, Du, Zhao, Zhang, Ji, and
  Gao]{shen21fedmm_arxiv}
Shen, Y., Du, J., Zhao, H., Zhang, B., Ji, Z., and Gao, M.
\newblock Fed{MM}: Saddle point optimization for federated adversarial domain
  adaptation.
\newblock \emph{arXiv preprint arXiv:2110.08477}, 2021.

\bibitem[Sinha et~al.(2017)Sinha, Namkoong, and
  Duchi]{sinha17certifiable_robust_iclr}
Sinha, A., Namkoong, H., and Duchi, J.
\newblock Certifiable distributional robustness with principled adversarial
  training.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Spiridonoff et~al.(2021)Spiridonoff, Olshevsky, and
  Paschalidis]{spiridonoff21comm_eff_SGD_neurips}
Spiridonoff, A., Olshevsky, A., and Paschalidis, I.~C.
\newblock Communication-efficient sgd: From local sgd to one-shot averaging.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, 2021.

\bibitem[Stich(2018)]{stich18localSGD_iclr}
Stich, S.~U.
\newblock Local sgd converges fast and communicates little.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Stich \& Karimireddy(2020)Stich and Karimireddy]{stich20error_fb_jmlr}
Stich, S.~U. and Karimireddy, S.~P.
\newblock The error-feedback framework: Better rates for sgd with delayed
  gradients and compressed updates.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 1--36,
  2020.

\bibitem[Thekumparampil et~al.(2019)Thekumparampil, Jain, Netrapalli, and
  Oh]{thekumparampil19NC_C_neurips}
Thekumparampil, K.~K., Jain, P., Netrapalli, P., and Oh, S.
\newblock Efficient algorithms for smooth minimax optimization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Tran-Dinh et~al.(2020)Tran-Dinh, Liu, and
  Nguyen]{tran20hybrid_NCLin_neurips}
Tran-Dinh, Q., Liu, D., and Nguyen, L.~M.
\newblock Hybrid variance-reduced sgd algorithms for minimax problems with
  nonconvex-linear function.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  11096--11107, 2020.

\bibitem[Wang et~al.(2019)Wang, Yurochkin, Sun, Papailiopoulos, and
  Khazaeni]{wang19FL_iclr}
Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D., and Khazaeni, Y.
\newblock Federated learning with matched averaging.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Wang \& Joshi(2021)Wang and Joshi]{wang21coopSGD_jmlr}
Wang, J. and Joshi, G.
\newblock Cooperative {SGD}: A unified framework for the design and analysis of
  local-update sgd algorithms.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (213):\penalty0 1--50, 2021.

\bibitem[Wang et~al.(2020)Wang, Liu, Liang, Joshi, and
  Poor]{joshi20fednova_neurips}
Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H.~V.
\newblock Tackling the objective inconsistency problem in heterogeneous
  federated optimization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  7611--7623, 2020.

\bibitem[Wang et~al.(2021)Wang, Zhang, Liu, Chen, Xu, Fardad, and
  Li]{wang21adversarial_minmax_neurips}
Wang, J., Zhang, T., Liu, S., Chen, P.-Y., Xu, J., Fardad, M., and Li, B.
\newblock Adversarial attack generation empowered by min-max optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Wang \& Li(2020)Wang and Li]{wang20improved_cc_neurips}
Wang, Y. and Li, J.
\newblock Improved algorithms for convex-concave minimax optimization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  4800--4810, 2020.

\bibitem[Xian et~al.(2021)Xian, Huang, Zhang, and
  Huang]{xian21dec_ncsc_storm_neurips}
Xian, W., Huang, F., Zhang, Y., and Huang, H.
\newblock A faster decentralized algorithm for nonconvex minimax problems.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, 2021.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao17fashionMNIST}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Xie et~al.(2020)Xie, Luo, Lian, and Zhang]{xie20lower_FS_cc_icml}
Xie, G., Luo, L., Lian, Y., and Zhang, Z.
\newblock Lower complexity bounds for finite-sum convex-concave minimax
  optimization problems.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10504--10513. PMLR, 2020.

\bibitem[Xie et~al.(2021)Xie, Zhang, Zhang, Shen, and
  Qian]{xie21NC_PL_FL_arxiv}
Xie, J., Zhang, C., Zhang, Y., Shen, Z., and Qian, H.
\newblock A federated learning framework for nonconvex-pl minimax problems.
\newblock \emph{arXiv preprint arXiv:2105.14216}, 2021.

\bibitem[Xing et~al.(2016)Xing, Ho, Xie, and Wei]{xing2016strategies}
Xing, E.~P., Ho, Q., Xie, P., and Wei, D.
\newblock Strategies and principles of distributed machine learning on big
  data.
\newblock \emph{Engineering}, 2\penalty0 (2):\penalty0 179--195, 2016.

\bibitem[Xu et~al.(2020)Xu, Zhang, Xu, and Lan]{lan_unified_ncc_arxiv20}
Xu, Z., Zhang, H., Xu, Y., and Lan, G.
\newblock A unified single-loop alternating gradient projection algorithm for
  nonconvex-concave and convex-nonconcave minimax problems.
\newblock \emph{arXiv preprint arXiv:2006.02032}, 2020.

\bibitem[Yang et~al.(2021{\natexlab{a}})Yang, Fang, and
  Liu]{yang21partial_client_iclr}
Yang, H., Fang, M., and Liu, J.
\newblock Achieving linear speedup with partial worker participation in non-iid
  federated learning.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.

\bibitem[Yang et~al.(2020{\natexlab{a}})Yang, Kiyavash, and
  He]{yang20NCNC_VR_neurips}
Yang, J., Kiyavash, N., and He, N.
\newblock Global convergence and variance reduction for a class of
  nonconvex-nonconcave minimax problems.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  1153--1165, 2020{\natexlab{a}}.

\bibitem[Yang et~al.(2020{\natexlab{b}})Yang, Zhang, Kiyavash, and
  He]{kiyavash20catalyst_neurips}
Yang, J., Zhang, S., Kiyavash, N., and He, N.
\newblock A catalyst framework for minimax optimization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  5667--5678, 2020{\natexlab{b}}.

\bibitem[Yang et~al.(2021{\natexlab{b}})Yang, Orvieto, Lucchi, and
  He]{yang21NCPL_arxiv}
Yang, J., Orvieto, A., Lucchi, A., and He, N.
\newblock Faster single-loop algorithms for minimax optimization without strong
  concavity.
\newblock \emph{arXiv preprint arXiv:2112.05604}, 2021{\natexlab{b}}.

\bibitem[Yang et~al.(2018)Yang, Andrew, Eichner, Sun, Li, Kong, Ramage, and
  Beaufays]{yang18FL_google_arxiv}
Yang, T., Andrew, G., Eichner, H., Sun, H., Li, W., Kong, N., Ramage, D., and
  Beaufays, F.
\newblock Applied federated learning: Improving google keyboard query
  suggestions.
\newblock \emph{arXiv preprint arXiv:1812.02903}, 2018.

\bibitem[Yoon \& Ryu(2021)Yoon and Ryu]{yoon21acc_ncc_icml}
Yoon, T. and Ryu, E.~K.
\newblock Accelerated algorithms for smooth convex-concave minimax problems
  with o (1/k\^{}2) rate on squared gradient norm.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12098--12109. PMLR, 2021.

\bibitem[Yu et~al.(2019)Yu, Jin, and Yang]{yu19icml_momentum}
Yu, H., Jin, R., and Yang, S.
\newblock On the linear speedup analysis of communication efficient momentum
  {SGD} for distributed non-convex optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7184--7193. PMLR, 2019.

\bibitem[Yuan et~al.(2021)Yuan, Guo, Xu, Ying, and Yang]{yuan21FedDeepAUC_icml}
Yuan, Z., Guo, Z., Xu, Y., Ying, Y., and Yang, T.
\newblock Federated deep {AUC} maximization for heterogeneous data with a
  constant communication complexity.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12219--12229. PMLR, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Xiao, Sun, and
  Luo]{tomluo_1_loop_ncc_neurips20}
Zhang, J., Xiao, P., Sun, R., and Luo, Z.
\newblock A single-loop smoothed gradient descent-ascent algorithm for
  nonconvex-concave min-max problems.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  7377--7389, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Yang, Guzm{\'a}n, Kiyavash, and
  He]{zhang21NCSC_uai}
Zhang, S., Yang, J., Guzm{\'a}n, C., Kiyavash, N., and He, N.
\newblock The complexity of nonconvex-strongly-concave minimax optimization.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pp.\
  482--492. PMLR, 2021.

\bibitem[Zhou \& Cong(2018)Zhou and Cong]{zhou18localSGD_ijcai}
Zhou, F. and Cong, G.
\newblock On the convergence properties of a k-step averaging stochastic
  gradient descent algorithm for nonconvex optimization.
\newblock In \emph{Proceedings of the 27th International Joint Conference on
  Artificial Intelligence}, pp.\  3219--3227, 2018.

\end{thebibliography}
