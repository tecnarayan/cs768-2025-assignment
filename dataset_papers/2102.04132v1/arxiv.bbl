\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Yasin Abbasi-Yadkori, D{\'a}vid P{\'a}l, and Csaba Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2312--2320, 2011.

\bibitem[Abbasi-Yadkori et~al.(2012)Abbasi-Yadkori, Pal, and
  Szepesvari]{abbasi2012online}
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari.
\newblock Online-to-confidence-set conversions and application to sparse
  stochastic bandits.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1--9. PMLR,
  2012.

\bibitem[Ando and Zhang(2005)]{ando2005framework}
Rie~Kubota Ando and Tong Zhang.
\newblock A framework for learning predictive structures from multiple tasks
  and unlabeled data.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Nov):\penalty0 1817--1853, 2005.

\bibitem[Arora et~al.(2020)Arora, Du, Kakade, Luo, and
  Saunshi]{arora2020provable}
Sanjeev Arora, Simon~S Du, Sham Kakade, Yuping Luo, and Nikunj Saunshi.
\newblock Provable representation learning for imitation learning via bi-level
  optimization.
\newblock \emph{arXiv preprint arXiv:2002.10544}, 2020.

\bibitem[Auer(2002)]{auer2002using}
Peter Auer.
\newblock Using confidence bounds for exploitation-exploration trade-offs.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 397--422, 2002.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin~F Yang.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock \emph{arXiv preprint arXiv:2006.01107}, 2020.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  263--272. PMLR, 2017.

\bibitem[Baxter(2000)]{baxter2000model}
Jonathan Baxter.
\newblock A model of inductive bias learning.
\newblock \emph{Journal of artificial intelligence research}, 12:\penalty0
  149--198, 2000.

\bibitem[Bengio et~al.(2013)Bengio, Courville, and
  Vincent]{bengio2013representation}
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
\newblock Representation learning: A review and new perspectives.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 35\penalty0 (8):\penalty0 1798--1828, 2013.

\bibitem[Brunskill and Li(2013)]{brunskill13mtrl}
Emma Brunskill and Lihong Li.
\newblock Sample complexity of multi-task reinforcement learning.
\newblock In \emph{Proceedings of the Twenty-Ninth Conference on Uncertainty in
  Artificial Intelligence (UAI-13)}, pages 122--131, 2013.

\bibitem[Carpentier and Munos(2012)]{carpentier2012bandit}
Alexandra Carpentier and R{\'e}mi Munos.
\newblock Bandit theory meets compressed sensing for high dimensional
  stochastic linear bandit.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 190--198.
  PMLR, 2012.

\bibitem[Caruana(1997)]{caruana1997multitask}
Rich Caruana.
\newblock Multitask learning.
\newblock \emph{Machine learning}, 28\penalty0 (1):\penalty0 41--75, 1997.

\bibitem[Chu et~al.(2011)Chu, Li, Reyzin, and Schapire]{chu2011contextual}
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire.
\newblock Contextual bandits with linear payoff functions.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pages 208--214, 2011.

\bibitem[Dani et~al.(2008)Dani, Hayes, and Kakade]{dani2008stochastic}
Varsha Dani, Thomas~P Hayes, and Sham~M Kakade.
\newblock Stochastic linear optimization under bandit feedback.
\newblock 2008.

\bibitem[D'Eramo et~al.(2019)D'Eramo, Tateo, Bonarini, Restelli, and
  Peters]{d2019sharing}
Carlo D'Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan
  Peters.
\newblock Sharing knowledge in multi-task deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Du et~al.(2020)Du, Hu, Kakade, Lee, and Lei]{du2020few}
Simon~S Du, Wei Hu, Sham~M Kakade, Jason~D Lee, and Qi~Lei.
\newblock Few-shot learning via learning the representation, provably.
\newblock \emph{arXiv preprint arXiv:2002.09434}, 2020.

\bibitem[Hao et~al.(2020)Hao, Lattimore, and Wang]{hao2020high}
Botao Hao, Tor Lattimore, and Mengdi Wang.
\newblock High-dimensional sparse linear bandits.
\newblock \emph{arXiv preprint arXiv:2011.04020}, 2020.

\bibitem[Hessel et~al.(2019)Hessel, Soyer, Espeholt, Czarnecki, Schmitt, and
  van Hasselt]{hessel2019multi}
Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt,
  and Hado van Hasselt.
\newblock Multi-task deep reinforcement learning with popart.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3796--3803, 2019.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E
  Schapire.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, pages
  1704--1713. PMLR, 2017.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock \emph{arXiv preprint arXiv:1807.03765}, 2018.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pages 2137--2143. PMLR,
  2020.

\bibitem[Jun et~al.(2019)Jun, Willett, Wright, and Nowak]{jun2019bilinear}
Kwang-Sung Jun, Rebecca Willett, Stephen Wright, and Robert Nowak.
\newblock Bilinear bandits with low-rank structure.
\newblock In \emph{International Conference on Machine Learning}, pages
  3163--3172. PMLR, 2019.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Lattimore et~al.(2015)Lattimore, Crammer, and
  Szepesv{\'a}ri]{lattimore2015linear}
Tor Lattimore, Koby Crammer, and Csaba Szepesv{\'a}ri.
\newblock Linear multi-resource allocation with semi-bandit feedback.
\newblock In \emph{NIPS}, pages 964--972, 2015.

\bibitem[Lattimore et~al.(2020)Lattimore, Szepesvari, and
  Weisz]{lattimore2020learning}
Tor Lattimore, Csaba Szepesvari, and Gellert Weisz.
\newblock Learning with good feature representations in bandits and in rl with
  a generative model.
\newblock In \emph{International Conference on Machine Learning}, pages
  5662--5670. PMLR, 2020.

\bibitem[Li et~al.(2014)Li, Zhang, Zhang, Huang, and Zhang]{li2014joint}
Jiayi Li, Hongyan Zhang, Liangpei Zhang, Xin Huang, and Lefei Zhang.
\newblock Joint collaborative representation with multitask learning for
  hyperspectral image classification.
\newblock \emph{IEEE Transactions on Geoscience and Remote Sensing},
  52\penalty0 (9):\penalty0 5923--5936, 2014.

\bibitem[Li et~al.(2019{\natexlab{a}})Li, Wang, and Zhou]{li2019nearly}
Yingkai Li, Yining Wang, and Yuan Zhou.
\newblock Nearly minimax-optimal regret for linearly parameterized bandits.
\newblock \emph{arXiv preprint arXiv:1904.00242}, 2019{\natexlab{a}}.

\bibitem[Li et~al.(2019{\natexlab{b}})Li, Wang, and Zhou]{li2019tight}
Yingkai Li, Yining Wang, and Yuan Zhou.
\newblock Tight regret bounds for infinite-armed linear contextual bandits.
\newblock \emph{arXiv preprint arXiv:1905.01435}, 2019{\natexlab{b}}.

\bibitem[Liu et~al.(2016)Liu, Dogan, and Hofmann]{liu2016decoding}
Lydia~T Liu, Urun Dogan, and Katja Hofmann.
\newblock Decoding multitask dqn in the world of minecraft.
\newblock In \emph{The 13th European Workshop on Reinforcement Learning (EWRL)
  2016}, 2016.

\bibitem[Liu et~al.(2019)Liu, He, Chen, and Gao]{liu2019multi}
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
\newblock Multi-task deep neural networks for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1901.11504}, 2019.

\bibitem[Lu et~al.(2020)Lu, Meisami, and Tewari]{lu2020low}
Yangyi Lu, Amirhossein Meisami, and Ambuj Tewari.
\newblock Low-rank generalized linear bandit problems.
\newblock \emph{arXiv preprint arXiv:2006.02948}, 2020.

\bibitem[Maurer et~al.(2016)Maurer, Pontil, and
  Romera-Paredes]{maurer2016benefit}
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes.
\newblock The benefit of multitask representation learning.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2853--2884, 2016.

\bibitem[Parisotto et~al.(2015)Parisotto, Ba, and
  Salakhutdinov]{parisotto2015actor}
Emilio Parisotto, Jimmy~Lei Ba, and Ruslan Salakhutdinov.
\newblock Actor-mimic: Deep multitask and transfer reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1511.06342}, 2015.

\bibitem[Ramsundar et~al.(2015)Ramsundar, Kearnes, Riley, Webster, Konerding,
  and Pande]{ramsundar2015massively}
Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David
  Konerding, and Vijay Pande.
\newblock Massively multitask networks for drug discovery.
\newblock \emph{arXiv preprint arXiv:1502.02072}, 2015.

\bibitem[Rusmevichientong and Tsitsiklis(2010)]{rusmevichientong2010linearly}
Paat Rusmevichientong and John~N Tsitsiklis.
\newblock Linearly parameterized bandits.
\newblock \emph{Mathematics of Operations Research}, 35\penalty0 (2):\penalty0
  395--411, 2010.

\bibitem[Taylor and Stone(2009)]{taylor2009transfer}
Matthew~E Taylor and Peter Stone.
\newblock Transfer learning for reinforcement learning domains: A survey.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0 (7), 2009.

\bibitem[Teh et~al.(2017)Teh, Bapst, Czarnecki, Quan, Kirkpatrick, Hadsell,
  Heess, and Pascanu]{teh2017distral}
Yee Teh, Victor Bapst, Wojciech~M Czarnecki, John Quan, James Kirkpatrick, Raia
  Hadsell, Nicolas Heess, and Razvan Pascanu.
\newblock Distral: Robust multitask reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4496--4506, 2017.

\bibitem[Thrun and Pratt(1998)]{thrun1998learning}
Sebastian Thrun and Lorien Pratt.
\newblock Learning to learn: Introduction and overview.
\newblock In \emph{Learning to learn}, pages 3--17. Springer, 1998.

\bibitem[Tripuraneni et~al.(2020)Tripuraneni, Jin, and
  Jordan]{tripuraneni2020provable}
Nilesh Tripuraneni, Chi Jin, and Michael~I Jordan.
\newblock Provable meta-learning of linear representations.
\newblock \emph{arXiv preprint arXiv:2002.11684}, 2020.

\bibitem[Wang et~al.(2020)Wang, Du, Yang, and Kakade]{wang2020long}
Ruosong Wang, Simon~S Du, Lin~F Yang, and Sham~M Kakade.
\newblock Is long horizon reinforcement learning more difficult than short
  horizon reinforcement learning?
\newblock \emph{arXiv preprint arXiv:2005.00527}, 2020.

\bibitem[Wilson et~al.(2007)Wilson, Fern, Ray, and Tadepalli]{wilson2007multi}
Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli.
\newblock Multi-task reinforcement learning: a hierarchical bayesian approach.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pages 1015--1022, 2007.

\bibitem[Yang et~al.(2020)Yang, Hu, Lee, and Du]{yang2020provable}
Jiaqi Yang, Wei Hu, Jason~D. Lee, and Simon~S. Du.
\newblock Provable benefits of representation learning in linear bandits, 2020.

\bibitem[Yang and Wang(2020)]{yang2020reinforcement}
Lin Yang and Mengdi Wang.
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In \emph{International Conference on Machine Learning}, pages
  10746--10756. PMLR, 2020.

\bibitem[Yang and Wang(2019)]{yang2019sample}
Lin~F Yang and Mengdi Wang.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock \emph{arXiv preprint arXiv:1902.04779}, 2019.

\bibitem[Zanette et~al.(2020{\natexlab{a}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock \emph{arXiv preprint arXiv:2003.00153}, 2020{\natexlab{a}}.

\bibitem[Zanette et~al.(2020{\natexlab{b}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020provably}
Andrea Zanette, Alessandro Lazaric, Mykel~J Kochenderfer, and Emma Brunskill.
\newblock Provably efficient reward-agnostic navigation with linear value
  iteration.
\newblock \emph{arXiv preprint arXiv:2008.07737}, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2020)Zhang, Ji, and Du]{zhang2020reinforcement}
Zihan Zhang, Xiangyang Ji, and Simon~S Du.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock \emph{arXiv preprint arXiv:2009.13503}, 2020.

\bibitem[Zhou et~al.(2020{\natexlab{a}})Zhou, Gu, and
  Szepesvari]{zhou2020nearly}
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari.
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock \emph{arXiv preprint arXiv:2012.08507}, 2020{\natexlab{a}}.

\bibitem[Zhou et~al.(2020{\natexlab{b}})Zhou, He, and Gu]{zhou2020provably}
Dongruo Zhou, Jiafan He, and Quanquan Gu.
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock \emph{arXiv preprint arXiv:2006.13165}, 2020{\natexlab{b}}.

\end{thebibliography}
