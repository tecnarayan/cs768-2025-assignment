\begin{thebibliography}{10}

\bibitem{aribandi2022ext}
Vamsi Aribandi, Yi~Tay, Tal Schuster, Jinfeng Rao, Huaixiu~Steven Zheng,
  Sanket~Vaibhav Mehta, Honglei Zhuang, Vinh~Q. Tran, Dara Bahri, Jianmo Ni,
  Jai~Prakash Gupta, Kai Hui, Sebastian Ruder, and Donald Metzler.
\newblock Ext5: Towards extreme multi-task scaling for transfer learning.
\newblock In {\em The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net, 2022.

\bibitem{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski,
  David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock {\em ArXiv preprint}, abs/2108.07732, 2021.

\bibitem{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
  Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback.
\newblock {\em ArXiv preprint}, abs/2204.05862, 2022.

\bibitem{10.1145/3442188.3445922}
Emily~M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
  Shmitchell.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In {\em Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency}, FAccT '21, page 610–623, New York, NY,
  USA, 2021. Association for Computing Machinery.

\bibitem{bender-koller-2020-climbing}
Emily~M. Bender and Alexander Koller.
\newblock Climbing towards {NLU}: {On} meaning, form, and understanding in the
  age of data.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 5185--5198, Online, 2020. Association for
  Computational Linguistics.

\bibitem{Bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Ronan LeBras, Jianfeng Gao, and Yejin Choi.
\newblock {PIQA:} reasoning about physical commonsense in natural language.
\newblock In {\em The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of
  Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium
  on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York,
  NY, USA, February 7-12, 2020}, pages 7432--7439. {AAAI} Press, 2020.

\bibitem{10.5555/3495724.3495883}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, {\em Advances in
  Neural Information Processing Systems 33: Annual Conference on Neural
  Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
  virtual}, 2020.

\bibitem{bojar-etal-2014-findings}
Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder.
\newblock Findings of the 2009 {W}orkshop on {S}tatistical {M}achine
  {T}ranslation.
\newblock In {\em Proceedings of the Fourth Workshop on Statistical Machine
  Translation}, pages 1--28, Athens, Greece, 2009. Association for
  Computational Linguistics.

\bibitem{carlini2021extracting}
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel
  Herbert-Voss, Katherine Lee, Adam Roberts, Tom~B Brown, Dawn Song, Ulfar
  Erlingsson, et~al.
\newblock Extracting training data from large language models.
\newblock In {\em USENIX Security Symposium}, volume~6, 2021.

\bibitem{codealpaca}
Sahil Chaudhary.
\newblock Code alpaca: An instruction-following llama model for code
  generation.
\newblock https://github.com/sahil280114/codealpaca, 2023.

\bibitem{chen2024alpagasus}
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav,
  Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin.
\newblock Alpagasus: Training a better alpaca model with fewer data.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{chen2021codex}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde
  de~Oliveira~Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
  Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy
  Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
  Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens
  Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings, Matthias
  Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss,
  William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor
  Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher
  Hesse, Andrew~N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,
  Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter
  Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
  Wojciech Zaremba.
\newblock Evaluating large language models trained on code.
\newblock {\em ArXiv preprint}, abs/2107.03374, 2021.

\bibitem{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality, 2023.

\bibitem{JMLR:v25:23-0870}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus,
  Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson,
  Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha
  Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter,
  Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew
  Dai, Hongkun Yu, Slav Petrov, Ed~H. Chi, Jeff Dean, Jacob Devlin, Adam
  Roberts, Denny Zhou, Quoc~V. Le, and Jason Wei.
\newblock Scaling instruction-finetuned language models.
\newblock {\em Journal of Machine Learning Research}, 25(70):1--53, 2024.

\bibitem{clark-etal-2018-semi}
Kevin Clark, Minh-Thang Luong, Christopher~D. Manning, and Quoc Le.
\newblock Semi-supervised sequence modeling with cross-view training.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 1914--1925, Brussels, Belgium, 2018.
  Association for Computational Linguistics.

\bibitem{Clark2018ThinkYH}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
  Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock {\em ArXiv preprint}, abs/1803.05457, 2018.

\bibitem{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano,
  Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems, 2021.

\bibitem{DollyV2}
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali
  Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin.
\newblock Free dolly: Introducing the world's first truly open
  instruction-tuned llm, 2023.

\bibitem{dao2023flashattention2}
Tri Dao.
\newblock Flashattention-2: Faster attention with better parallelism and work
  partitioning, 2023.

\bibitem{gunasekar2023textbooks}
Suriya Gunasekar, Yi~Zhang, Jyoti Aneja, Caio C{\'e}sar~Teodoro Mendes, Allie
  Del~Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
  de~Rosa, Olli Saarikivi, et~al.
\newblock Textbooks are all you need.
\newblock {\em ArXiv preprint}, abs/2306.11644, 2023.

\bibitem{hartvigsen-etal-2022-toxigen}
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray,
  and Ece Kamar.
\newblock {T}oxi{G}en: A large-scale machine-generated dataset for adversarial
  and implicit hate speech detection.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 3309--3326, Dublin,
  Ireland, 2022. Association for Computational Linguistics.

\bibitem{he2023preserving}
Guande He, Jianfei Chen, and Jun Zhu.
\newblock Preserving pre-trained features helps calibrate fine-tuned language
  models.
\newblock {\em ArXiv preprint}, abs/2305.19249, 2023.

\bibitem{hendrycks2021aligning}
Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song,
  and Jacob Steinhardt.
\newblock Aligning {AI} with shared human values.
\newblock In {\em 9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.

\bibitem{hendrycks2021measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In {\em 9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.

\bibitem{honovich-etal-2023-unnatural}
Or~Honovich, Thomas Scialom, Omer Levy, and Timo Schick.
\newblock Unnatural instructions: Tuning language models with (almost) no human
  labor.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, {\em
  Proceedings of the 61st Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)}, pages 14409--14428, Toronto, Canada,
  2023. Association for Computational Linguistics.

\bibitem{hosking2024human}
Tom Hosking, Phil Blunsom, and Max Bartolo.
\newblock Human feedback is not gold standard.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{overfitting2023}
Jeremy Howard and Jonathan Whitaker.
\newblock Can llms learn from a single example?, 2023.

\bibitem{hu2022lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen{-}Zhu, Yuanzhi Li,
  Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In {\em The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net, 2022.

\bibitem{Mathew2024Instruction}
Mathew Huerta-Enochian.
\newblock Instruction fine-tuning: Does prompt loss matter?, 2024.

\bibitem{ivison2023camels}
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters,
  Pradeep Dasigi, Joel Jang, David Wadden, Noah~A Smith, Iz~Beltagy, et~al.
\newblock Camels in a changing climate: Enhancing lm adaptation with tulu 2,
  2023.

\bibitem{jain2024neftune}
Neel Jain, Ping yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu,
  Gowthami Somepalli, Brian~R. Bartoldson, Bhavya Kailkhura, Avi Schwarzschild,
  Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein.
\newblock {NEFT}une: Noisy embeddings improve instruction finetuning.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{jha2023limit}
Aditi Jha, Sam Havens, Jeremey Dohmann, Alex Trott, and Jacob Portes.
\newblock Limit: Less is more for instruction tuning across evaluation
  paradigms.
\newblock {\em ArXiv preprint}, abs/2311.13133, 2023.

\bibitem{khashabi-etal-2020-unifiedqa}
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord,
  Peter Clark, and Hannaneh Hajishirzi.
\newblock {UNIFIEDQA}: Crossing format boundaries with a single {QA} system.
\newblock In {\em Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pages 1896--1907, Online, 2020. Association for Computational
  Linguistics.

\bibitem{leike2018scalable}
Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane
  Legg.
\newblock Scalable agent alignment via reward modeling: a research direction.
\newblock {\em ArXiv preprint}, abs/1811.07871, 2018.

\bibitem{ea01b9c0db064caca6986b925d75f2bb}
{Hector J.} Levesque, Ernest Davis, and Leora Morgenstern.
\newblock The winograd schema challenge.
\newblock In {\em 13th International Conference on the Principles of Knowledge
  Representation and Reasoning, KR 2012}, Proceedings of the International
  Conference on Knowledge Representation and Reasoning, pages 552--561.
  Institute of Electrical and Electronics Engineers Inc., 2012.
\newblock 13th International Conference on the Principles of Knowledge
  Representation and Reasoning, KR 2012 ; Conference date: 10-06-2012 Through
  14-06-2012.

\bibitem{li2024selfalignment}
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer,
  Jason~E Weston, and Mike Lewis.
\newblock Self-alignment with instruction backtranslation.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{alpaca_eval}
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Alpacaeval: An automatic evaluator of instruction-following models,
  2023.

\bibitem{lin-etal-2022-truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock {T}ruthful{QA}: Measuring how models mimic human falsehoods.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 3214--3252, Dublin,
  Ireland, 2022. Association for Computational Linguistics.

\bibitem{liu-etal-2023-compositional}
Fangyu Liu, Qianchu Liu, Shruthi Bannur, Fernando P{\'e}rez-Garc{\'\i}a, Naoto
  Usuyama, Sheng Zhang, Tristan Naumann, Aditya Nori, Hoifung Poon, Javier
  Alvarez-Valle, Ozan Oktay, and Stephanie~L. Hyland.
\newblock Compositional zero-shot domain transfer with text-to-text models.
\newblock {\em Transactions of the Association for Computational Linguistics},
  11:1097--1113, 2023.

\bibitem{liu2023makes}
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.
\newblock What makes good data for alignment? a comprehensive study of
  automatic data selection in instruction tuning.
\newblock {\em ArXiv preprint}, abs/2312.15685, 2023.

\bibitem{mihaylov-etal-2018-suit}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 2381--2391, Brussels, Belgium, 2018.
  Association for Computational Linguistics.

\bibitem{min-etal-2022-metaicl}
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi.
\newblock {M}eta{ICL}: Learning to learn in context.
\newblock In {\em Proceedings of the 2022 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 2791--2809, Seattle, United States, 2022. Association
  for Computational Linguistics.

\bibitem{mishra-etal-2022-cross}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.
\newblock Cross-task generalization via natural language crowdsourcing
  instructions.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 3470--3487, Dublin,
  Ireland, 2022. Association for Computational Linguistics.

\bibitem{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.
\newblock Training language models to follow instructions with human feedback.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{paperno-etal-2016-lambada}
Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Ngoc~Quan Pham,
  Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
  Fern{\'a}ndez.
\newblock The {LAMBADA} dataset: Word prediction requiring a broad discourse
  context.
\newblock In {\em Proceedings of the 54th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 1525--1534, Berlin,
  Germany, 2016. Association for Computational Linguistics.

\bibitem{papineni-etal-2002-bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock {B}leu: a method for automatic evaluation of machine translation.
\newblock In {\em Proceedings of the 40th Annual Meeting of the Association for
  Computational Linguistics}, pages 311--318, Philadelphia, Pennsylvania, USA,
  2002. Association for Computational Linguistics.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8), 2019.

\bibitem{t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em J. Mach. Learn. Res.}, 21:140:1--140:67, 2020.

\bibitem{reddy-etal-2019-coqa}
Siva Reddy, Danqi Chen, and Christopher~D. Manning.
\newblock {C}o{QA}: A conversational question answering challenge.
\newblock {\em Transactions of the Association for Computational Linguistics},
  7:249--266, 2019.

\bibitem{sakaguchi2019winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock In {\em The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of
  Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium
  on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York,
  NY, USA, February 7-12, 2020}, pages 8732--8740. {AAAI} Press, 2020.

\bibitem{sanhmultitask}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H. Bach, Lintang Sutawika,
  Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
  M~Saiful Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma, Eliza
  Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal~V. Nayak, Debajyoti Datta,
  Jonathan Chang, Mike~Tian{-}Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,
  Zheng~Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,
  Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F{\'{e}}vry, Jason~Alan
  Fries, Ryan Teehan, Teven~Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and
  Alexander~M. Rush.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In {\em The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net, 2022.

\bibitem{sennrich-etal-2016-edinburgh}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock {E}dinburgh neural machine translation systems for {WMT} 16.
\newblock In {\em Proceedings of the First Conference on Machine Translation:
  Volume 2, Shared Task Papers}, pages 371--376, Berlin, Germany, 2016.
  Association for Computational Linguistics.

\bibitem{shi-etal-2022-learning}
Zhengxiang Shi, Yue Feng, and Aldo Lipani.
\newblock Learning to execute actions or ask clarification questions.
\newblock In {\em Findings of the Association for Computational Linguistics:
  NAACL 2022}, pages 2060--2070, Seattle, United States, 2022. Association for
  Computational Linguistics.

\bibitem{shi2023dont}
Zhengxiang Shi and Aldo Lipani.
\newblock Don{\textquoteright}t stop pretraining? make prompt-based fine-tuning
  powerful learner.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.

\bibitem{shi2023dept}
Zhengxiang Shi and Aldo Lipani.
\newblock De{PT}: Decomposed prompt tuning for parameter-efficient fine-tuning.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{shi2023rethinking}
Zhengxiang Shi, Francesco Tonolini, Nikolaos Aletras, Emine Yilmaz, Gabriella
  Kazai, and Yunlong Jiao.
\newblock Rethinking semi-supervised learning with language models.
\newblock In {\em Findings of ACL 2023}, Toronto, Canada, 2023. Association for
  Computational Linguistics.

\bibitem{Shi_Zhang_Lipani_2022}
Zhengxiang Shi, Qiang Zhang, and Aldo Lipani.
\newblock Stepgame: A new benchmark for robust multi-hop spatial reasoning in
  texts.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pages 11321--11329, Jun. 2022.

\bibitem{Understanding2024shi}
Zhengyan Shi, Sander Land, Acyr Locatelli, Matthieu Geist, and Max Bartolo.
\newblock Understanding likelihood over-optimisation in direct alignment
  algorithms, 2024.

\bibitem{singh2024aya}
Shivalika Singh, Freddie Vargus, Daniel Dsouza, B{\"o}rje~F Karlsson, Abinaya
  Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas,
  Laura OMahony, et~al.
\newblock Aya dataset: An open-access collection for multilingual instruction
  tuning.
\newblock {\em ArXiv preprint}, abs/2402.06619, 2024.

\bibitem{suzgun-etal-2023-challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay,
  Hyung~Won Chung, Aakanksha Chowdhery, Quoc Le, Ed~Chi, Denny Zhou, and Jason
  Wei.
\newblock Challenging {BIG}-bench tasks and whether chain-of-thought can solve
  them.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, {\em
  Findings of the Association for Computational Linguistics: ACL 2023}, pages
  13003--13051, Toronto, Canada, 2023. Association for Computational
  Linguistics.

\bibitem{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model, 2023.

\bibitem{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em ArXiv preprint}, abs/2307.09288, 2023.

\bibitem{wang-etal-2023-self-instruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language models with self-generated
  instructions.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, {\em
  Proceedings of the 61st Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers)}, pages 13484--13508, Toronto, Canada,
  2023. Association for Computational Linguistics.

\bibitem{wang-etal-2022-super}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Atharva Naik, Arjun Ashok, Arut~Selvan Dhanasekaran, Anjana
  Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai,
  Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi,
  Kuntal~Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali
  Purohit, Neeraj Varshney, Phani~Rohitha Kaza, Pulkit Verma, Ravsehaj~Singh
  Puri, Rushang Karia, Savan Doshi, Shailaja~Keyur Sampat, Siddhartha Mishra,
  Sujan Reddy~A, Sumanta Patro, Tanay Dixit, and Xudong Shen.
\newblock Super-{N}atural{I}nstructions: Generalization via declarative
  instructions on 1600+ {NLP} tasks.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 5085--5109, Abu Dhabi, United Arab
  Emirates, 2022. Association for Computational Linguistics.

\bibitem{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In {\em The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net, 2022.

\bibitem{pmlr-v202-wu23d}
Bin Wu, Jinyuan Fang, Xiangxiang Zeng, Shangsong Liang, and Qiang Zhang.
\newblock Adaptive compositional continual meta-learning.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
  Sivan Sabato, and Jonathan Scarlett, editors, {\em Proceedings of the 40th
  International Conference on Machine Learning}, volume 202 of {\em Proceedings
  of Machine Learning Research}, pages 37358--37378. PMLR, 23--29 Jul 2023.

\bibitem{10.1145/3485447.3512036}
Bin Wu, Zaiqiao Meng, Qiang Zhang, and Shangsong Liang.
\newblock Meta-learning helps personalized product search.
\newblock In {\em Proceedings of the ACM Web Conference 2022}, WWW '22, page
  2277–2287, New York, NY, USA, 2022. Association for Computing Machinery.

\bibitem{xia2024less}
Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi
  Chen.
\newblock Less: Selecting influential data for instruction tuning, 2024.

\bibitem{xu2024wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang
  Tao, Qingwei Lin, and Daxin Jiang.
\newblock Wizard{LM}: Empowering large pre-trained language models to follow
  complex instructions.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{xu2023rethinking}
Yang Xu, Yongqiang Yao, Yufan Huang, Mengnan Qi, Maoquan Wang, Bin Gu, and Neel
  Sundaresan.
\newblock Rethinking the instruction quality: Lift is what you need, 2023.

\bibitem{xue2023to}
Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You.
\newblock To repeat or not to repeat: Insights from scaling {LLM} under
  token-crisis.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.

\bibitem{yang2024bayesian1}
Adam~X. Yang, Maxime Robeyns, Thomas Coste, Jun Wang, Haitham Bou-Ammar, and
  Laurence Aitchison.
\newblock Bayesian reward models for llm alignment, 2024.

\bibitem{yang2024bayesian}
Adam~X Yang, Maxime Robeyns, Xi~Wang, and Laurence Aitchison.
\newblock Bayesian low-rank adaptation for large language models.
\newblock In {\em ICLR}, 2024.

\bibitem{zellers-etal-2019-hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock {H}ella{S}wag: Can a machine really finish your sentence?
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 4791--4800, Florence, Italy, 2019.
  Association for Computational Linguistics.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em ArXiv preprint}, abs/2205.01068, 2022.

\bibitem{zhao2024long}
Hao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion.
\newblock Long is more for alignment: A simple but tough-to-beat baseline for
  instruction fine-tuning, 2024.

\bibitem{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph~E.
  Gonzalez, and Ion Stoica.
\newblock Judging {LLM}-as-a-judge with {MT}-bench and chatbot arena.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2023.

\bibitem{zhou2023lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe
  Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke
  Zettlemoyer, and Omer Levy.
\newblock {LIMA}: Less is more for alignment.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing
  Systems}, 2023.

\bibitem{zhou2024batch}
Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine~A
  Heller, and Subhrajit Roy.
\newblock Batch calibration: Rethinking calibration for in-context learning and
  prompt engineering.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{10.1162/tacl_a_00662}
Han Zhou, Xingchen Wan, Ivan Vulić, and Anna Korhonen.
\newblock {AutoPEFT: Automatic Configuration Search for Parameter-Efficient
  Fine-Tuning}.
\newblock {\em Transactions of the Association for Computational Linguistics},
  12:525--542, 2024.

\end{thebibliography}
