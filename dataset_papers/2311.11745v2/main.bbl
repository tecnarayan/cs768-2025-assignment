\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arthur \& Vassilvitskii(2007)Arthur and Vassilvitskii]{arthur2007k}
Arthur, D. and Vassilvitskii, S.
\newblock K-means++ the advantages of careful seeding.
\newblock In \emph{Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms}, pp.\  1027--1035, 2007.

\bibitem[Bernard(2021)]{phonemizer20}
Bernard, M.
\newblock Phonemizer.
\newblock \url{https://github.com/bootphon/phonemizer}, 2021.

\bibitem[Casanova et~al.(2022)Casanova, Weber, Shulby, Junior, G{\"o}lge, and Ponti]{casanova2022yourtts}
Casanova, E., Weber, J., Shulby, C.~D., Junior, A.~C., G{\"o}lge, E., and Ponti, M.~A.
\newblock Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2709--2720. PMLR, 2022.

\bibitem[Chen et~al.(2020)Chen, Tan, Ren, Xu, Sun, Zhao, and Qin]{chen20r_interspeech}
Chen, M., Tan, X., Ren, Y., Xu, J., Sun, H., Zhao, S., and Qin, T.
\newblock {MultiSpeech: Multi-Speaker Text to Speech with Transformer}.
\newblock In \emph{Proc. Interspeech 2020}, pp.\  4024--4028, 2020.
\newblock \doi{10.21437/Interspeech.2020-3139}.

\bibitem[Chen et~al.(2021)Chen, Tan, Li, Liu, Qin, sheng zhao, and Liu]{chen2021adaspeech}
Chen, M., Tan, X., Li, B., Liu, Y., Qin, T., sheng zhao, and Liu, T.-Y.
\newblock Adaspeech: Adaptive text to speech for custom voice.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Drynvt7gg4L}.

\bibitem[Chen et~al.(2022)Chen, Wang, Chen, Wu, Liu, Chen, Li, Kanda, Yoshioka, Xiao, et~al.]{chen2022wavlm}
Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., et~al.
\newblock Wavlm: Large-scale self-supervised pre-training for full stack speech processing.
\newblock \emph{IEEE Journal of Selected Topics in Signal Processing}, 16\penalty0 (6):\penalty0 1505--1518, 2022.

\bibitem[Chen et~al.(2019)Chen, Assael, Shillingford, Budden, Reed, Zen, Wang, Cobo, Trask, Laurie, Gulcehre, van~den Oord, Vinyals, and de~Freitas]{chen2018sample}
Chen, Y., Assael, Y., Shillingford, B., Budden, D., Reed, S., Zen, H., Wang, Q., Cobo, L.~C., Trask, A., Laurie, B., Gulcehre, C., van~den Oord, A., Vinyals, O., and de~Freitas, N.
\newblock Sample efficient adaptive text-to-speech.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rkzjUoAcFX}.

\bibitem[Choi et~al.(2023)Choi, Yang, Lee, and Kim]{choi2023nansy}
Choi, H.-S., Yang, J., Lee, J., and Kim, H.
\newblock {NANSY}++: Unified voice synthesis with neural analysis and synthesis.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=elDEe8LYW7-}.

\bibitem[Cooper et~al.(2020)Cooper, Lai, Yasuda, Fang, Wang, Chen, and Yamagishi]{cooper2020zero}
Cooper, E., Lai, C.-I., Yasuda, Y., Fang, F., Wang, X., Chen, N., and Yamagishi, J.
\newblock Zero-shot multi-speaker text-to-speech with state-of-the-art neural speaker embeddings.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  6184--6188. IEEE, 2020.

\bibitem[Gibiansky et~al.(2017)Gibiansky, Arik, Diamos, Miller, Peng, Ping, Raiman, and Zhou]{gibiansky2017deep}
Gibiansky, A., Arik, S., Diamos, G., Miller, J., Peng, K., Ping, W., Raiman, J., and Zhou, Y.
\newblock Deep voice 2: Multi-speaker neural text-to-speech.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.
\newblock Generative {A}dversarial {N}ets.
\newblock \emph{Advances in Neural Information Processing Systems}, 27:\penalty0 2672--2680, 2014.

\bibitem[Hayashi et~al.(2019)Hayashi, Watanabe, Toda, Takeda, Toshniwal, and Livescu]{hayashi2019pre}
Hayashi, T., Watanabe, S., Toda, T., Takeda, K., Toshniwal, S., and Livescu, K.
\newblock Pre-trained text embeddings for enhanced text-to-speech synthesis.
\newblock 2019.

\bibitem[Hsu et~al.(2019)Hsu, Zhang, Weiss, Zen, Wu, Cao, and Wang]{hsu2018hierarchical}
Hsu, W.-N., Zhang, Y., Weiss, R., Zen, H., Wu, Y., Cao, Y., and Wang, Y.
\newblock Hierarchical {G}enerative {M}odeling for {C}ontrollable {S}peech {S}ynthesis.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rygkk305YQ}.

\bibitem[Hsu et~al.(2021)Hsu, Bolte, Tsai, Lakhotia, Salakhutdinov, and Mohamed]{hsu2021hubert}
Hsu, W.-N., Bolte, B., Tsai, Y.-H.~H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A.
\newblock Hubert: Self-supervised speech representation learning by masked prediction of hidden units.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 29:\penalty0 3451--3460, 2021.

\bibitem[Huang et~al.(2022)Huang, Lin, Liu, Chen, and Lee]{huang2022meta}
Huang, S.-F., Lin, C.-J., Liu, D.-R., Chen, Y.-C., and Lee, H.-y.
\newblock Meta-tts: Meta-learning for few-shot speaker adaptive text-to-speech.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 30:\penalty0 1558--1571, 2022.

\bibitem[Jia et~al.(2018)Jia, Zhang, Weiss, Wang, Shen, Ren, Chen, Nguyen, Pang, Lopez-Moreno, et~al.]{jia2018transfer}
Jia, Y., Zhang, Y., Weiss, R.~J., Wang, Q., Shen, J., Ren, F., Chen, Z., Nguyen, P., Pang, R., Lopez-Moreno, I., et~al.
\newblock Transfer {L}earning from {S}peaker {V}erification to {M}ultispeaker {T}ext-{T}o-{S}peech {S}ynthesis.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Jia et~al.(2021)Jia, Zen, Shen, Zhang, and Wu]{jia2021png}
Jia, Y., Zen, H., Shen, J., Zhang, Y., and Wu, Y.
\newblock Png bert: Augmented bert on phonemes and graphemes for neural tts.
\newblock \emph{arXiv preprint arXiv:2103.15060}, 2021.

\bibitem[Kahn et~al.(2020)Kahn, Riviere, Zheng, Kharitonov, Xu, Mazar{\'e}, Karadayi, Liptchinsky, Collobert, Fuegen, et~al.]{kahn2020libri}
Kahn, J., Riviere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazar{\'e}, P.-E., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., et~al.
\newblock Libri-light: A benchmark for asr with limited or no supervision.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  7669--7673. IEEE, 2020.

\bibitem[Kenter et~al.(2020)Kenter, Sharma, and Clark]{kenter2020improving}
Kenter, T., Sharma, M., and Clark, R.
\newblock Improving the prosody of rnn-based english text-to-speech synthesis by incorporating a bert model.
\newblock 2020.

\bibitem[Kim et~al.(2020)Kim, Kim, Kong, and Yoon]{kim2020glow}
Kim, J., Kim, S., Kong, J., and Yoon, S.
\newblock Glow-{TTS}: A {G}enerative {F}low for {T}ext-to-{S}peech via {M}onotonic {A}lignment {S}earch.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Kim et~al.(2021)Kim, Kong, and Son]{kim2021conditional}
Kim, J., Kong, J., and Son, J.
\newblock Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5530--5540. PMLR, 2021.

\bibitem[Kong et~al.(2020)Kong, Kim, and Bae]{kong2020hifi}
Kong, J., Kim, J., and Bae, J.
\newblock Hi{F}i-{GAN}: {G}enerative {A}dversarial networks for {E}fficient and {H}igh {F}idelity {S}peech {S}ynthesis.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Larsen et~al.(2016)Larsen, S{\o}nderby, Larochelle, and Winther]{larsen2016autoencoding}
Larsen, A. B.~L., S{\o}nderby, S.~K., Larochelle, H., and Winther, O.
\newblock Autoencoding beyond pixels using a learned similarity metric.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1558--1566. PMLR, 2016.

\bibitem[Le et~al.(2023)Le, Vyas, Shi, Karrer, Sari, Moritz, Williamson, Manohar, Adi, Mahadeokar, et~al.]{le2023voicebox}
Le, M., Vyas, A., Shi, B., Karrer, B., Sari, L., Moritz, R., Williamson, M., Manohar, V., Adi, Y., Mahadeokar, J., et~al.
\newblock Voicebox: Text-guided multilingual universal speech generation at scale.
\newblock \emph{arXiv preprint arXiv:2306.15687}, 2023.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{loshchilov2018decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled {W}eight {D}ecay {R}egularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Mao et~al.(2017)Mao, Li, Xie, Lau, Wang, and Paul~Smolley]{mao2017least}
Mao, X., Li, Q., Xie, H., Lau, R.~Y., Wang, Z., and Paul~Smolley, S.
\newblock Least squares generative adversarial networks.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pp.\  2794--2802, 2017.

\bibitem[Panayotov et~al.(2015)Panayotov, Chen, Povey, and Khudanpur]{panayotov2015librispeech}
Panayotov, V., Chen, G., Povey, D., and Khudanpur, S.
\newblock Librispeech: an asr corpus based on public domain audio books.
\newblock In \emph{2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)}, pp.\  5206--5210. IEEE, 2015.

\bibitem[Park(2018)]{park2018}
Park, K.
\newblock Kss dataset: Korean single speaker speech dataset.
\newblock \url{https://kaggle.com/bryanpark/korean-single-speaker-speech-dataset}, 2018.

\bibitem[Ping et~al.(2018)Ping, Peng, Gibiansky, Arik, Kannan, Narang, Raiman, and Miller]{ping2018deep}
Ping, W., Peng, K., Gibiansky, A., Arik, S.~O., Kannan, A., Narang, S., Raiman, J., and Miller, J.
\newblock Deep voice 3: 2000-speaker neural text-to-speech.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=HJtEm4p6Z}.

\bibitem[Shen et~al.(2023)Shen, Ju, Tan, Liu, Leng, He, Qin, Zhao, and Bian]{shen2023naturalspeech}
Shen, K., Ju, Z., Tan, X., Liu, Y., Leng, Y., He, L., Qin, T., Zhao, S., and Bian, J.
\newblock Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers.
\newblock \emph{arXiv preprint arXiv:2304.09116}, 2023.

\bibitem[Tan et~al.(2022)Tan, Chen, Liu, Cong, Zhang, Liu, Wang, Leng, Yi, He, et~al.]{tan2022naturalspeech}
Tan, X., Chen, J., Liu, H., Cong, J., Zhang, C., Liu, Y., Wang, X., Leng, Y., Yi, Y., He, L., et~al.
\newblock Naturalspeech: End-to-end text to speech synthesis with human-level quality.
\newblock \emph{arXiv preprint arXiv:2205.04421}, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is {A}ll you {N}eed.
\newblock \emph{Advances in Neural Information Processing Systems}, 30:\penalty0 5998--6008, 2017.

\bibitem[Veaux et~al.(2017)Veaux, Yamagishi, MacDonald, et~al.]{veaux2017cstr}
Veaux, C., Yamagishi, J., MacDonald, K., et~al.
\newblock C{STR VCTK} corpus: {E}nglish multi-speaker corpus for {CSTR} voice cloning toolkit.
\newblock \emph{University of Edinburgh. The Centre for Speech Technology Research (CSTR)}, 2017.

\bibitem[Wang et~al.(2023)Wang, Chen, Wu, Zhang, Zhou, Liu, Chen, Liu, Wang, Li, et~al.]{wang2023neural}
Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., et~al.
\newblock Neural codec language models are zero-shot text to speech synthesizers.
\newblock \emph{arXiv preprint arXiv:2301.02111}, 2023.

\bibitem[Wang et~al.(2018)Wang, Stanton, Zhang, Ryan, Battenberg, Shor, Xiao, Jia, Ren, and Saurous]{wang2018style}
Wang, Y., Stanton, D., Zhang, Y., Ryan, R.-S., Battenberg, E., Shor, J., Xiao, Y., Jia, Y., Ren, F., and Saurous, R.~A.
\newblock Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis.
\newblock In \emph{International conference on machine learning}, pp.\  5180--5189. PMLR, 2018.

\bibitem[Xu et~al.(2021)Xu, Song, Zhang, Zhang, He, and Zhou]{xu2021improving}
Xu, G., Song, W., Zhang, Z., Zhang, C., He, X., and Zhou, B.
\newblock Improving prosody modelling with cross-utterance bert embeddings for end-to-end speech synthesis.
\newblock In \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pp.\  6079--6083. IEEE, 2021.

\bibitem[Yin et~al.(2022)Yin, Tang, Liu, Wang, Zhao, Zhao, Xiong, Zhao, and Luo]{yin22_interspeech}
Yin, D., Tang, C., Liu, Y., Wang, X., Zhao, Z., Zhao, Y., Xiong, Z., Zhao, S., and Luo, C.
\newblock {RetrieverTTS: Modeling Decomposed Factors for Text-Based Speech Insertion}.
\newblock In \emph{Proc. Interspeech 2022}, pp.\  1571--1575, 2022.
\newblock \doi{10.21437/Interspeech.2022-245}.

\bibitem[Zhou et~al.(2022)Zhou, Song, Li, Zhang, Wu, Bian, Su, and Meng]{zhou22d_interspeech}
Zhou, Y., Song, C., Li, X., Zhang, L., Wu, Z., Bian, Y., Su, D., and Meng, H.
\newblock {Content-Dependent Fine-Grained Speaker Embedding for Zero-Shot Speaker Adaptation in Text-to-Speech Synthesis}.
\newblock In \emph{Proc. Interspeech 2022}, pp.\  2573--2577, 2022.
\newblock \doi{10.21437/Interspeech.2022-10054}.

\end{thebibliography}
