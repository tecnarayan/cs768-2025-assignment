\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,
  Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.~L.,
  Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Bai et~al.(2022)Bai, Jones, Ndousse, Askell, Chen, DasSarma, Drain,
  Fort, Ganguli, Henighan, et~al.]{bai2022training}
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D.,
  Fort, S., Ganguli, D., Henighan, T., et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback.
\newblock \emph{arXiv preprint arXiv:2204.05862}, 2022.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley,
  O’Brien, Hallahan, Khan, Purohit, Prashanth, Raff,
  et~al.]{biderman2023pythia}
Biderman, S., Schoelkopf, H., Anthony, Q.~G., Bradley, H., O’Brien, K.,
  Hallahan, E., Khan, M.~A., Purohit, S., Prashanth, U.~S., Raff, E., et~al.
\newblock Pythia: A suite for analyzing large language models across training
  and scaling.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2397--2430. PMLR, 2023.

\bibitem[Bradley \& Terry(1952)Bradley and Terry]{bradley1952rank}
Bradley, R.~A. and Terry, M.~E.
\newblock Rank analysis of incomplete block designs: I. the method of paired
  comparisons.
\newblock \emph{Biometrika}, 39\penalty0 (3/4):\penalty0 324--345, 1952.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,
  Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E.,
  Lee, P., Lee, Y.~T., Li, Y., Lundberg, S., et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan,
  Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry,
  Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet,
  Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol,
  Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike,
  Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder,
  McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{chen2021evaluating}
Chen, M., Tworek, J., Jun, H., Yuan, Q., de~Oliveira~Pinto, H.~P., Kaplan, J.,
  Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger,
  G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S.,
  Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C.,
  Tillet, P., Such, F.~P., Cummings, D., Plappert, M., Chantzis, F., Barnes,
  E., Herbert-Voss, A., Guss, W.~H., Nichol, A., Paino, A., Tezak, N., Tang,
  J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr,
  A.~N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight,
  M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei,
  D., McCandlish, S., Sutskever, I., and Zaremba, W.
\newblock Evaluating large language models trained on code, 2021.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang,
  Zhuang, Gonzalez, et~al.]{chiang2023vicuna}
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L.,
  Zhuang, S., Zhuang, Y., Gonzalez, J.~E., et~al.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt
  quality.
\newblock \emph{See https://vicuna. lmsys. org (accessed 14 April 2023)}, 2023.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, et~al.]{chung2022scaling}
Chung, H.~W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang,
  X., Dehghani, M., Brahma, S., et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Dong et~al.(2023)Dong, Xiong, Goyal, Pan, Diao, Zhang, Shum, and
  Zhang]{dong2023raft}
Dong, H., Xiong, W., Goyal, D., Pan, R., Diao, S., Zhang, J., Shum, K., and
  Zhang, T.
\newblock Raft: Reward ranked finetuning for generative foundation model
  alignment.
\newblock \emph{arXiv preprint arXiv:2304.06767}, 2023.

\bibitem[Ganguli et~al.(2022)Ganguli, Lovitt, Kernion, Askell, Bai, Kadavath,
  Mann, Perez, Schiefer, Ndousse, et~al.]{ganguli2022red}
Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann,
  B., Perez, E., Schiefer, N., Ndousse, K., et~al.
\newblock Red teaming language models to reduce harms: Methods, scaling
  behaviors, and lessons learned.
\newblock \emph{arXiv preprint arXiv:2209.07858}, 2022.

\bibitem[Gao et~al.(2023)Gao, Madaan, Zhou, Alon, Liu, Yang, Callan, and
  Neubig]{gao2023pal}
Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., and
  Neubig, G.
\newblock Pal: Program-aided language models, 2023.

\bibitem[Glaese et~al.(2022)Glaese, McAleese, Trębacz, Aslanides, Firoiu,
  Ewalds, Rauh, Weidinger, Chadwick, Thacker, et~al.]{glaese2022improving}
Glaese, A., McAleese, N., Trębacz, M., Aslanides, J., Firoiu, V., Ewalds, T.,
  Rauh, M., Weidinger, L., Chadwick, M., Thacker, P., et~al.
\newblock Improving alignment of dialogue agents via targeted human judgements.
\newblock \emph{arXiv preprint arXiv:2209.14375}, 2022.

\bibitem[Havrilla et~al.(2023)Havrilla, Zhuravinskyi, Phung, Tiwari, Tow,
  Biderman, Anthony, and Castricato]{havrilla2023trlx}
Havrilla, A., Zhuravinskyi, M., Phung, D., Tiwari, A., Tow, J., Biderman, S.,
  Anthony, Q., and Castricato, L.
\newblock trlx: A framework for large scale reinforcement learning from human
  feedback.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  8578--8595, 2023.

\bibitem[Huang et~al.(2023)Huang, Yu, Ma, Zhong, Feng, Wang, Chen, Peng, Feng,
  Qin, et~al.]{huang2023survey}
Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W.,
  Feng, X., Qin, B., et~al.
\newblock A survey on hallucination in large language models: Principles,
  taxonomy, challenges, and open questions.
\newblock \emph{arXiv preprint arXiv:2311.05232}, 2023.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot,
  de~las Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock,
  Scao, Lavril, Wang, Lacroix, and Sayed]{jiang2023mistral}
Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., de~las
  Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud,
  L.~R., Lachaux, M.-A., Stock, P., Scao, T.~L., Lavril, T., Wang, T., Lacroix,
  T., and Sayed, W.~E.
\newblock Mistral 7b, 2023.

\bibitem[Khalifa et~al.(2020)Khalifa, Elsahar, and
  Dymetman]{khalifa2020distributional}
Khalifa, M., Elsahar, H., and Dymetman, M.
\newblock A distributional approach to controlled text generation.
\newblock \emph{arXiv preprint arXiv:2012.11635}, 2020.

\bibitem[Knox et~al.(2022)Knox, Hatgis-Kessell, Booth, Niekum, Stone, and
  Allievi]{knox2022models}
Knox, W.~B., Hatgis-Kessell, S., Booth, S., Niekum, S., Stone, P., and Allievi,
  A.
\newblock Models of human preference for learning reward functions.
\newblock \emph{arXiv preprint arXiv:2206.02231}, 2022.

\bibitem[Knox et~al.(2023)Knox, Hatgis-Kessell, Adalgeirsson, Booth, Dragan,
  Stone, and Niekum]{knox2023learning}
Knox, W.~B., Hatgis-Kessell, S., Adalgeirsson, S.~O., Booth, S., Dragan, A.,
  Stone, P., and Niekum, S.
\newblock Learning optimal advantage from preferences and mistaking it for
  reward.
\newblock \emph{arXiv preprint arXiv:2310.02456}, 2023.

\bibitem[Koh et~al.(2022)Koh, Ju, Liu, and Pan]{Koh_2022}
Koh, H.~Y., Ju, J., Liu, M., and Pan, S.
\newblock An empirical survey on long document summarization: Datasets, models,
  and metrics.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (8):\penalty0 1–35,
  December 2022.
\newblock ISSN 1557-7341.
\newblock \doi{10.1145/3545176}.
\newblock URL \url{http://dx.doi.org/10.1145/3545176}.

\bibitem[Langley(2000)]{langley00}
Langley, P.
\newblock Crafting papers on machine learning.
\newblock In Langley, P. (ed.), \emph{Proceedings of the 17th International
  Conference on Machine Learning (ICML 2000)}, pp.\  1207--1216, Stanford, CA,
  2000. Morgan Kaufmann.

\bibitem[Liu et~al.(2023)Liu, Zhao, Joshi, Khalman, Saleh, Liu, and
  Liu]{liu2023statistical}
Liu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M., Liu, P.~J., and Liu, J.
\newblock Statistical rejection sampling improves preference optimization.
\newblock \emph{arXiv preprint arXiv:2309.06657}, 2023.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and
  Potts]{maas2011learning}
Maas, A., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A.~Y., and Potts, C.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th annual meeting of the association
  for computational linguistics: Human language technologies}, pp.\  142--150,
  2011.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,
  C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27730--27744, 2022.

\bibitem[Perez et~al.(2022)Perez, Huang, Song, Cai, Ring, Aslanides, Glaese,
  McAleese, and Irving]{perez2202red}
Perez, E., Huang, S., Song, F., Cai, T., Ring, R., Aslanides, J., Glaese, A.,
  McAleese, N., and Irving, G.
\newblock Red teaming language models with language models, 2022.
\newblock \emph{URL https://arxiv. org/abs/2202.03286}, 2022.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rafailov et~al.(2023)Rafailov, Sharma, Mitchell, Ermon, Manning, and
  Finn]{rafailov2023direct}
Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C.~D., and Finn, C.
\newblock Direct preference optimization: Your language model is secretly a
  reward model.
\newblock \emph{arXiv preprint arXiv:2305.18290}, 2023.

\bibitem[Rafailov et~al.(2024)Rafailov, Hejna, Park, and Finn]{rafailov2024r}
Rafailov, R., Hejna, J., Park, R., and Finn, C.
\newblock From $ r $ to {$ Q^* $}: {Y}our {L}anguage {M}odel is {S}ecretly a
  {Q-F}unction.
\newblock \emph{arXiv preprint arXiv:2404.12358}, 2024.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 5485--5551, 2020.

\bibitem[Rauh et~al.(2022)Rauh, Mellor, Uesato, Huang, Welbl, Weidinger,
  Dathathri, Glaese, Irving, Gabriel, Isaac, and
  Hendricks]{rauh2022characteristics}
Rauh, M., Mellor, J., Uesato, J., Huang, P.-S., Welbl, J., Weidinger, L.,
  Dathathri, S., Glaese, A., Irving, G., Gabriel, I., Isaac, W., and Hendricks,
  L.~A.
\newblock Characteristics of harmful text: Towards rigorous benchmarking of
  language models, 2022.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897. PMLR, 2015.

\bibitem[Sheng et~al.(2021)Sheng, Chang, Natarajan, and
  Peng]{sheng2021societal}
Sheng, E., Chang, K.-W., Natarajan, P., and Peng, N.
\newblock Societal biases in language generation: Progress and challenges.
\newblock \emph{arXiv preprint arXiv:2105.04054}, 2021.

\bibitem[Song et~al.(2023)Song, Yu, Li, Yu, Huang, Li, and
  Wang]{song2023preference}
Song, F., Yu, B., Li, M., Yu, H., Huang, F., Li, Y., and Wang, H.
\newblock Preference ranking optimization for human alignment.
\newblock \emph{arXiv preprint arXiv:2306.17492}, 2023.

\bibitem[Stiennon et~al.(2022)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei, and Christiano]{stiennon2022learning}
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D.~M., Lowe, R., Voss, C., Radford,
  A., Amodei, D., and Christiano, P.
\newblock Learning to summarize from human feedback, 2022.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto]{taori2023alpaca}
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang,
  P., and Hashimoto, T.~B.
\newblock Alpaca: A strong, replicable instruction-following model.
\newblock \emph{Stanford Center for Research on Foundation Models.
  https://crfm. stanford. edu/2023/03/13/alpaca. html}, 3\penalty0
  (6):\penalty0 7, 2023.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,
  Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R.,
  Schalkwyk, J., Dai, A.~M., Hauth, A., et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,
  Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
  Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Tunstall et~al.(2023)Tunstall, Beeching, Lambert, Rajani, Rasul,
  Belkada, Huang, von Werra, Fourrier, Habib, et~al.]{tunstall2023zephyr}
Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y.,
  Huang, S., von Werra, L., Fourrier, C., Habib, N., et~al.
\newblock Zephyr: Direct distillation of lm alignment.
\newblock \emph{arXiv preprint arXiv:2310.16944}, 2023.

\bibitem[Vu et~al.(2023)Vu, He, Haffari, and Shareghi]{vu2023koala}
Vu, T.-T., He, X., Haffari, G., and Shareghi, E.
\newblock Koala: An index for quantifying overlaps with pre-training corpora,
  2023.

\bibitem[Wang et~al.(2023)Wang, Jiang, Yang, Liu, and Chen]{wang2023beyond}
Wang, C., Jiang, Y., Yang, C., Liu, H., and Chen, Y.
\newblock Beyond reverse kl: Generalizing direct preference optimization with
  diverse divergence constraints.
\newblock \emph{arXiv preprint arXiv:2309.16240}, 2023.

\bibitem[Weidinger et~al.(2021)Weidinger, Mellor, Rauh, Griffin, Uesato, Huang,
  Cheng, Glaese, Balle, Kasirzadeh, et~al.]{weidinger2021ethical}
Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S.,
  Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., et~al.
\newblock Ethical and social risks of harm from language models.
\newblock \emph{arXiv preprint arXiv:2112.04359}, 2021.

\bibitem[Wiher et~al.(2022)Wiher, Meister, and Cotterell]{wiher2022decoding}
Wiher, G., Meister, C., and Cotterell, R.
\newblock On decoding strategies for neural text generators.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  10:\penalty0 997--1012, 2022.

\bibitem[Workshop et~al.(2022)Workshop, Scao, Fan, Akiki, Pavlick, Ili{\'c},
  Hesslow, Castagn{\'e}, Luccioni, Yvon, et~al.]{workshop2022bloom}
Workshop, B., Scao, T.~L., Fan, A., Akiki, C., Pavlick, E., Ili{\'c}, S.,
  Hesslow, D., Castagn{\'e}, R., Luccioni, A.~S., Yvon, F., et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem[Wu et~al.(2023)Wu, Hu, Shi, Dziri, Suhr, Ammanabrolu, Smith,
  Ostendorf, and Hajishirzi]{wu2023fine}
Wu, Z., Hu, Y., Shi, W., Dziri, N., Suhr, A., Ammanabrolu, P., Smith, N.~A.,
  Ostendorf, M., and Hajishirzi, H.
\newblock Fine-grained human feedback gives better rewards for language model
  training.
\newblock \emph{arXiv preprint arXiv:2306.01693}, 2023.

\bibitem[Yuan et~al.(2023)Yuan, Yuan, Tan, Wang, Huang, and
  Huang]{yuan2023rrhf}
Yuan, Z., Yuan, H., Tan, C., Wang, W., Huang, S., and Huang, F.
\newblock Rrhf: Rank responses to align language models with human feedback
  without tears.
\newblock \emph{arXiv preprint arXiv:2304.05302}, 2023.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li,
  Li, Xing, et~al.]{zheng2023judging}
Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z.,
  Li, Z., Li, D., Xing, E., et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{arXiv preprint arXiv:2306.05685}, 2023.

\bibitem[Zhong et~al.(2024)Zhong, Feng, Xiong, Zhao, He, Bian, and
  Wang]{zhong2024dpo}
Zhong, H., Feng, G., Xiong, W., Zhao, L., He, D., Bian, J., and Wang, L.
\newblock Dpo meets ppo: Reinforced token optimization for rlhf.
\newblock \emph{arXiv preprint arXiv:2404.18922}, 2024.

\end{thebibliography}
