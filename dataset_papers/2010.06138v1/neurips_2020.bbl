\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Bapna et~al.(2019)Bapna, Arivazhagan, and Firat]{bapna2019simple}
Ankur Bapna, Naveen Arivazhagan, and Orhan Firat.
\newblock Simple, scalable adaptation for neural machine translation.
\newblock \emph{arXiv preprint arXiv:1909.08478}, 2019.

\bibitem[Chen et~al.(2019)Chen, Gan, Cheng, Liu, and Liu]{chen2019distilling}
Yen-Chun Chen, Zhe Gan, Yu~Cheng, Jingzhou Liu, and Jingjing Liu.
\newblock Distilling the knowledge of bert for text generation.
\newblock \emph{arXiv preprint arXiv:1911.03829}, 2019.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dong et~al.(2019)Dong, Yang, Wang, Wei, Liu, Wang, Gao, Zhou, and
  Hon]{dong2019unified}
Li~Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu~Wang, Jianfeng Gao,
  Ming Zhou, and Hsiao-Wuen Hon.
\newblock Unified language model pre-training for natural language
  understanding and generation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  13042--13054, 2019.

\bibitem[Edunov et~al.(2018)Edunov, Ott, Auli, and
  Grangier]{edunov2018understanding}
Sergey Edunov, Myle Ott, Michael Auli, and David Grangier.
\newblock Understanding back-translation at scale.
\newblock \emph{arXiv preprint arXiv:1808.09381}, 2018.

\bibitem[Ghazvininejad et~al.(2019)Ghazvininejad, Levy, Liu, and
  Zettlemoyer]{ghazvininejad2019mask}
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer.
\newblock Mask-predict: Parallel decoding of conditional masked language
  models.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 6114--6123, 2019.

\bibitem[Gu et~al.(2017)Gu, Bradbury, Xiong, Li, and Socher]{gu2017non}
Jiatao Gu, James Bradbury, Caiming Xiong, Victor~OK Li, and Richard Socher.
\newblock Non-autoregressive neural machine translation.
\newblock \emph{arXiv preprint arXiv:1711.02281}, 2017.

\bibitem[Gu et~al.(2019)Gu, Wang, and Zhao]{gu2019levenshtein}
Jiatao Gu, Changhan Wang, and Junbo Zhao.
\newblock Levenshtein transformer.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11179--11189, 2019.

\bibitem[Guo et~al.(2019)Guo, Tan, He, Qin, Xu, and Liu]{guo2019non}
Junliang Guo, Xu~Tan, Di~He, Tao Qin, Linli Xu, and Tie-Yan Liu.
\newblock Non-autoregressive neural machine translation with enhanced decoder
  input.
\newblock In \emph{Thirty-Third AAAI Conference on Artificial Intelligence},
  pages 3723--3730, 2019.

\bibitem[Guo et~al.(2020{\natexlab{a}})Guo, Tan, Xu, Qin, Chen, and
  Liu]{guo2019fine}
Junliang Guo, Xu~Tan, Linli Xu, Tao Qin, Enhong Chen, and Tie-Yan Liu.
\newblock Fine-tuning by curriculum learning for non-autoregressive neural
  machine translation.
\newblock In \emph{Thirty-Fourth AAAI Conference on Artificial Intelligence},
  pages 7839--7846, 2020{\natexlab{a}}.

\bibitem[Guo et~al.(2020{\natexlab{b}})Guo, Xu, and Chen]{guo2020jointly}
Junliang Guo, Linli Xu, and Enhong Chen.
\newblock Jointly masked sequence-to-sequence model for non-autoregressive
  neural machine translation.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 376--385, 2020{\natexlab{b}}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock \emph{arXiv preprint arXiv:1902.00751}, 2019.

\bibitem[Ji et~al.(2020)Ji, Zhang, Duan, Zhang, Chen, and Luo]{ji2020cross}
Baijun Ji, Zhirui Zhang, Xiangyu Duan, Min Zhang, Boxing Chen, and Weihua Luo.
\newblock Cross-lingual pre-training based transfer for zero-shot neural
  machine translation.
\newblock In \emph{Thirty-Fourth AAAI Conference on Artificial Intelligence},
  pages 115--122, 2020.

\bibitem[Kim and Rush(2016)]{kim2016sequence}
Yoon Kim and Alexander~M Rush.
\newblock Sequence-level knowledge distillation.
\newblock \emph{arXiv preprint arXiv:1606.07947}, 2016.

\bibitem[Lample and Conneau(2019)]{lample2019cross}
Guillaume Lample and Alexis Conneau.
\newblock Cross-lingual language model pretraining.
\newblock \emph{arXiv preprint arXiv:1901.07291}, 2019.

\bibitem[Lee et~al.(2019)Lee, Cho, and Kang]{lee2019mixout}
Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.
\newblock Mixout: Effective regularization to finetune large-scale pretrained
  language models.
\newblock \emph{arXiv preprint arXiv:1909.11299}, 2019.

\bibitem[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{lewis2019bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}, 2019.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[McCloskey and Cohen(1989)]{mccloskey1989catastrophic}
Michael McCloskey and Neal~J Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock In \emph{Psychology of learning and motivation}, volume~24, pages
  109--165. Elsevier, 1989.

\bibitem[Nallapati et~al.(2016)Nallapati, Zhou, Gulcehre, Xiang,
  et~al.]{nallapati2016abstractive}
Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et~al.
\newblock Abstractive text summarization using sequence-to-sequence rnns and
  beyond.
\newblock \emph{arXiv preprint arXiv:1602.06023}, 2016.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu]{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th Annual Meeting of the Association
  for Computational Linguistics}, pages 311--318, 2002.

\bibitem[Peters et~al.(2018)Peters, Neumann, Iyyer, Gardner, Clark, Lee, and
  Zettlemoyer]{peters2018deep}
Matthew~E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
  Kenton Lee, and Luke Zettlemoyer.
\newblock Deep contextualized word representations.
\newblock \emph{arXiv preprint arXiv:1802.05365}, 2018.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{URL https://s3-us-west-2. amazonaws.
  com/openai-assets/researchcovers/languageunsupervised/language understanding
  paper. pdf}, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Bilen, and Vedaldi]{rebuffi2017learning}
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
\newblock Learning multiple visual domains with residual adapters.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  506--516, 2017.

\bibitem[Song et~al.(2019)Song, Tan, Qin, Lu, and Liu]{song2019mass}
Kaitao Song, Xu~Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
\newblock Mass: Masked sequence to sequence pre-training for language
  generation.
\newblock \emph{arXiv preprint arXiv:1905.02450}, 2019.

\bibitem[Stern et~al.(2019)Stern, Chan, Kiros, and
  Uszkoreit]{stern2019insertion}
Mitchell Stern, William Chan, Jamie Kiros, and Jakob Uszkoreit.
\newblock Insertion transformer: Flexible sequence generation via insertion
  operations.
\newblock \emph{arXiv preprint arXiv:1902.03249}, 2019.

\bibitem[Sun et~al.(2019)Sun, Li, Wang, He, Lin, and Deng]{sun2019fast}
Zhiqing Sun, Zhuohan Li, Haoqing Wang, Di~He, Zi~Lin, and Zhihong Deng.
\newblock Fast structured decoding for sequence models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3011--3020, 2019.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  3104--3112, 2014.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}, 2018.

\bibitem[Wang et~al.(2020)Wang, Tang, Duan, Wei, Huang, Cao, Jiang, Zhou,
  et~al.]{wang2020k}
Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Cuihong Cao,
  Daxin Jiang, Ming Zhou, et~al.
\newblock K-adapter: Infusing knowledge into pre-trained models with adapters.
\newblock \emph{arXiv preprint arXiv:2002.01808}, 2020.

\bibitem[Weng et~al.(2019)Weng, Yu, Huang, Cheng, and Luo]{weng2019acquiring}
Rongxiang Weng, Heng Yu, Shujian Huang, Shanbo Cheng, and Weihua Luo.
\newblock Acquiring knowledge from pre-trained model to neural machine
  translation.
\newblock \emph{arXiv preprint arXiv:1912.01774}, 2019.

\bibitem[Yang et~al.(2019{\natexlab{a}})Yang, Wang, Zhou, Zhao, Yu, Zhang, and
  Li]{yang2019towards}
Jiacheng Yang, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Yong Yu, Weinan Zhang,
  and Lei Li.
\newblock Towards making the most of bert in neural machine translation.
\newblock \emph{arXiv preprint arXiv:1908.05672}, 2019{\natexlab{a}}.

\bibitem[Yang et~al.(2019{\natexlab{b}})Yang, Dai, Yang, Carbonell,
  Salakhutdinov, and Le]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov,
  and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock In \emph{Advances in neural information processing systems}, pages
  5754--5764, 2019{\natexlab{b}}.

\bibitem[Zhang et~al.(2018)Zhang, Liu, Li, Zhou, and Chen]{zhang2018joint}
Zhirui Zhang, Shujie Liu, Mu~Li, Ming Zhou, and Enhong Chen.
\newblock Joint training for neural machine translation models with monolingual
  data.
\newblock \emph{arXiv preprint arXiv:1803.00353}, 2018.

\bibitem[Zhu et~al.(2020)Zhu, Xia, Wu, He, Qin, Zhou, Li, and
  Liu]{zhu2020incorporating}
Jinhua Zhu, Yingce Xia, Lijun Wu, Di~He, Tao Qin, Wengang Zhou, Houqiang Li,
  and Tie-Yan Liu.
\newblock Incorporating bert into neural machine translation.
\newblock \emph{arXiv preprint arXiv:2002.06823}, 2020.

\end{thebibliography}
