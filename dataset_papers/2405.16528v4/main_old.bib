

@misc{zhao2024galore,
      title={GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection}, 
      author={Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian},
      year={2024},
      eprint={2403.03507},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.03507}, 
}


# LoRA PAPERS

@misc{lialin2023relora,
      title={ReLoRA: High-Rank Training Through Low-Rank Updates}, 
      author={Vladislav Lialin and Namrata Shivagunde and Sherin Muckatira and Anna Rumshisky},
      year={2023},
      eprint={2307.05695},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.05695}, 
}

@misc{dettmers2023qlora,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
note={arXiv:2305.14314},
}

@misc{meng2024pissa,
      title={PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models}, 
      author={Fanxu Meng and Zhaohui Wang and Muhan Zhang},
      year={2024},
      eprint={2404.02948},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      note={arXiv:2404.02948}
}

@misc{hayou2024loraplus,
      title={LoRA+: Efficient Low Rank Adaptation of Large Models}, 
      author={Soufiane Hayou and Nikhil Ghosh and Bin Yu},
      year={2024},
      eprint={2402.12354},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12354}, 
}


@misc{li2023loftq,
      title={LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models}, 
      author={Yixiao Li and Yifan Yu and Chen Liang and Pengcheng He and Nikos Karampatziakis and Weizhu Chen and Tuo Zhao},
      year={2023},
      eprint={2310.08659},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.08659}, 
}

@misc{perez2023training,
      title={Training and inference of large language models using 8-bit floating point}, 
      author={Sergio P. Perez and Yan Zhang and James Briggs and Charlie Blake and Josh Levy-Kramer and Paul Balanca and Carlo Luschi and Stephen Barlow and Andrew William Fitzgibbon},
      year={2023},
      eprint={2309.17224},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.17224}, 
}


@misc{kim2023memoryefficientPEQA,
      title={Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization}, 
      author={Jeonghoon Kim and Jung Hyun Lee and Sungdong Kim and Joonsuk Park and Kang Min Yoo and Se Jung Kwon and Dongsoo Lee},
      year={2023},
      eprint={2305.14152},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      note={arXiv:2305.14152}
}


@misc{wang2018training,
      title={Training Deep Neural Networks with 8-bit Floating Point Numbers}, 
      author={Naigang Wang and Jungwook Choi and Daniel Brand and Chia-Yu Chen and Kailash Gopalakrishnan},
      year={2018},
      eprint={1812.08011},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1812.08011}, 
}


@misc{banner2018scalable,
      title={Scalable Methods for 8-bit Training of Neural Networks}, 
      author={Ron Banner and Itay Hubara and Elad Hoffer and Daniel Soudry},
      year={2018},
      eprint={1805.11046},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1805.11046}, 
}


@misc{chmiel2022logarithmic,
      title={Accurate Neural Training with 4-bit Matrix Multiplications at Standard Formats}, 
      author={Brian Chmiel and Ron Banner and Elad Hoffer and Hilla Ben Yaacov and Daniel Soudry},
      year={2024},
      eprint={2112.10769},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2112.10769}, 
}

@misc{xi2024jetfire,
      title={Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data Flow and Per-Block Quantization}, 
      author={Haocheng Xi and Yuxiang Chen and Kang Zhao and Kai Jun Teh and Jianfei Chen and Jun Zhu},
      year={2024},
      eprint={2403.12422},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.12422}, 
}


@misc{zhang2023lorafa,
      title={LoRA-FA: Memory-efficient Low-rank Adaptation for Large Language Models Fine-tuning}, 
      author={Longteng Zhang and Lin Zhang and Shaohuai Shi and Xiaowen Chu and Bo Li},
      year={2023},
      eprint={2308.03303},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.03303}, 
}

@misc{liu2024dora,
      title={DoRA: Weight-Decomposed Low-Rank Adaptation}, 
      author={Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
      year={2024},
      eprint={2402.09353},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.09353}, 
}


@misc{ma2024era1bitllmslarge,
      title={The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits}, 
      author={Shuming Ma and Hongyu Wang and Lingxiao Ma and Lei Wang and Wenhui Wang and Shaohan Huang and Li Dong and Ruiping Wang and Jilong Xue and Furu Wei},
      year={2024},
      eprint={2402.17764},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17764}, 
}
# Qunatiztaion

@misc{heo2024rethinking,
      title={Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models}, 
      author={Jung Hwan Heo and Jeonghoon Kim and Beomseok Kwon and Byeongwook Kim and Se Jung Kwon and Dongsoo Lee},
      year={2024},
      eprint={2309.15531},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      note={arXiv:2309.15531}
}

@misc{pmlr-v202-lee23h,
      title={FlexRound: Learnable Rounding based on Element-wise Division for Post-Training Quantization}, 
      author={Jung Hyun Lee and Jeonghoon Kim and Se Jung Kwon and Dongsoo Lee},
      year={2024},
      eprint={2306.00317},
    note={arXiv:2306.00317},
      archivePrefix={arXiv},
      primaryClass={cs.LG}, 
}


@misc{yao2022zeroquant,
      title={ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers}, 
      author={Zhewei Yao and Reza Yazdani Aminabadi and Minjia Zhang and Xiaoxia Wu and Conglong Li and Yuxiong He},
      year={2022},
      eprint={2206.01861},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.01861}, 
}


@misc{jung2018learning,
      title={Learning to Quantize Deep Networks by Optimizing Quantization Intervals with Task Loss}, 
      author={Sangil Jung and Changyong Son and Seohyung Lee and Jinwoo Son and Youngjun Kwak and Jae-Joon Han and Sung Ju Hwang and Changkyu Choi},
      year={2018},
      eprint={1808.05779},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1808.05779}, 
}


@misc{wang2023bitnet,
      title={BitNet: Scaling 1-bit Transformers for Large Language Models}, 
      author={Hongyu Wang and Shuming Ma and Li Dong and Shaohan Huang and Huaijie Wang and Lingxiao Ma and Fan Yang and Ruiping Wang and Yi Wu and Furu Wei},
      year={2023},
      eprint={2310.11453},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
note={arXiv:2310.11453}
}

@misc{shao2024omniquant,
      title={OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models}, 
      author={Wenqi Shao and Mengzhao Chen and Zhaoyang Zhang and Peng Xu and Lirui Zhao and Zhiqian Li and Kaipeng Zhang and Peng Gao and Yu Qiao and Ping Luo},
      year={2024},
      eprint={2308.13137},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2308.13137}, 
}

@misc{surveyquant,
      title={A Survey of Quantization Methods for Efficient Neural Network Inference}, 
      author={Amir Gholami and Sehoon Kim and Zhen Dong and Zhewei Yao and Michael W. Mahoney and Kurt Keutzer},
      year={2021},
      eprint={2103.13630},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.13630}, 
}

@misc{frantar2023gptq,
      title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers}, 
      author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
      year={2023},
      eprint={2210.17323},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2210.17323}, 
}


@misc{hu2021lora,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}


@misc{bai2021efficient,
      title={Towards Efficient Post-training Quantization of Pre-trained Language Models}, 
      author={Haoli Bai and Lu Hou and Lifeng Shang and Xin Jiang and Irwin King and Michael R. Lyu},
      year={2021},
      eprint={2109.15082},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.15082}, 
}

@misc{shen2019qbert,
      title={Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT}, 
      author={Sheng Shen and Zhen Dong and Jiayu Ye and Linjian Ma and Zhewei Yao and Amir Gholami and Michael W. Mahoney and Kurt Keutzer},
      year={2019},
      eprint={1909.05840},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.05840}, 
}

@misc{xi2023training,
      title={Training Transformers with 4-bit Integers}, 
      author={Haocheng Xi and Changhao Li and Jianfei Chen and Jun Zhu},
      year={2023},
      eprint={2306.11987},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.11987}, 
}

@misc{bai2021binarybert,
      title={BinaryBERT: Pushing the Limit of BERT Quantization}, 
      author={Haoli Bai and Wei Zhang and Lu Hou and Lifeng Shang and Jing Jin and Xin Jiang and Qun Liu and Michael Lyu and Irwin King},
      year={2021},
      eprint={2012.15701},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2012.15701}, 
}

@inproceedings{Zafrir_2019,
   title={Q8BERT: Quantized 8Bit BERT},
   url={http://dx.doi.org/10.1109/EMC2-NIPS53020.2019.00016},
   DOI={10.1109/emc2-nips53020.2019.00016},
   booktitle={2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS)},
   publisher={IEEE},
   author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
   year={2019},
   month=dec }


@misc{xiao2024smoothquant,
      title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models}, 
      author={Guangxuan Xiao and Ji Lin and Mickael Seznec and Hao Wu and Julien Demouth and Song Han},
      year={2024},
      eprint={2211.10438},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.10438}, 
}

@misc{dettmers2208llm,
      title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale}, 
      author={Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer},
      year={2022},
      eprint={2208.07339},
      archivePrefix={arXiv},
      primaryClass={cs.LG}, 
      note={arXiv:2208.07339}
}


@misc{dettmers20228bit,
      title={8-bit Optimizers via Block-wise Quantization}, 
      author={Tim Dettmers and Mike Lewis and Sam Shleifer and Luke Zettlemoyer},
      year={2022},
      eprint={2110.02861},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.02861}, 
}


@misc{park2024lutgemm,
      title={LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models}, 
      author={Gunho Park and Baeseong Park and Minsub Kim and Sungjae Lee and Jeonghoon Kim and Beomseok Kwon and Se Jung Kwon and Byeongwook Kim and Youngjoo Lee and Dongsoo Lee},
      year={2024},
      eprint={2206.09557},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2206.09557}, 
}

@misc{dettmers2023case,
      title={The case for 4-bit precision: k-bit Inference Scaling Laws}, 
      author={Tim Dettmers and Luke Zettlemoyer},
      year={2023},
      eprint={2212.09720},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2212.09720}, 
      note={arXiv:2212.09720}
}

@misc{he2023debertav3,
      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, 
      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
      year={2023},
      eprint={2111.09543},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2111.09543}, 
}

@misc{2019t5,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.10683}, 
}

@misc{liao2024apiq,
      title={ApiQ: Finetuning of 2-Bit Quantized Large Language Model}, 
      author={Baohao Liao and Christian Herold and Shahram Khadivi and Christof Monz},
      year={2024},
      eprint={2402.05147},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.05147}, 
}

@misc{zheng2024llamafactory,
      title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models}, 
      author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
      year={2024},
      eprint={2403.13372},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.13372}, 
}


@misc{zeng2023glm130b,
      title={GLM-130B: An Open Bilingual Pre-trained Model}, 
      author={Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and Weng Lam Tam and Zixuan Ma and Yufei Xue and Jidong Zhai and Wenguang Chen and Peng Zhang and Yuxiao Dong and Jie Tang},
      year={2023},
      eprint={2210.02414},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.02414}, 
}

@misc{pope2022efficiently,
      title={Efficiently Scaling Transformer Inference}, 
      author={Reiner Pope and Sholto Douglas and Aakanksha Chowdhery and Jacob Devlin and James Bradbury and Anselm Levskaya and Jonathan Heek and Kefan Xiao and Shivani Agrawal and Jeff Dean},
      year={2022},
      eprint={2211.05102},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.05102}, 
}


@misc{peng2023fp8lm,
      title={FP8-LM: Training FP8 Large Language Models}, 
      author={Houwen Peng and Kan Wu and Yixuan Wei and Guoshuai Zhao and Yuxiang Yang and Ze Liu and Yifan Xiong and Ziyue Yang and Bolin Ni and Jingcheng Hu and Ruihang Li and Miaosen Zhang and Chen Li and Jia Ning and Ruizhe Wang and Zheng Zhang and Shuguang Liu and Joe Chau and Han Hu and Peng Cheng},
      year={2023},
      eprint={2310.18313},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      note={arXiv:2310.18313} 
}

@misc{wortsman2023stable,
      title={Stable and low-precision training for large-scale vision-language models}, 
      author={Mitchell Wortsman and Tim Dettmers and Luke Zettlemoyer and Ari Morcos and Ali Farhadi and Ludwig Schmidt},
      year={2023},
      eprint={2304.13013},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.13013}, 
}

@misc{tseng2024quip,
      title={QuIP: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks}, 
      author={Albert Tseng and Jerry Chee and Qingyao Sun and Volodymyr Kuleshov and Christopher De Sa},
      year={2024},
      eprint={2402.04396},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.04396}, 
}

@misc{egiazarian2024extreme,
      title={Extreme Compression of Large Language Models via Additive Quantization}, 
      author={Vage Egiazarian and Andrei Panferov and Denis Kuznedelev and Elias Frantar and Artem Babenko and Dan Alistarh},
      year={2024},
      eprint={2401.06118},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.06118}, 
}


@misc{dettmers2023spqr,
      title={SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression}, 
      author={Tim Dettmers and Ruslan Svirschevski and Vage Egiazarian and Denis Kuznedelev and Elias Frantar and Saleh Ashkboos and Alexander Borzunov and Torsten Hoefler and Dan Alistarh},
      year={2023},
      eprint={2306.03078},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.03078}, 
}

@misc{liu2023llm,
      title={LLM-QAT: Data-Free Quantization Aware Training for Large Language Models}, 
      author={Zechun Liu and Barlas Oguz and Changsheng Zhao and Ernie Chang and Pierre Stock and Yashar Mehdad and Yangyang Shi and Raghuraman Krishnamoorthi and Vikas Chandra},
      year={2023},
      eprint={2305.17888},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
note={arXiv:2205.17888} 
}

@misc{diao2023lmflow,
      title={LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models}, 
      author={Shizhe Diao and Rui Pan and Hanze Dong and Ka Shun Shum and Jipeng Zhang and Wei Xiong and Tong Zhang},
      year={2024},
      eprint={2306.12420},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.12420}, 
}

@misc{shazeer2018adafactor,
      title={Adafactor: Adaptive Learning Rates with Sublinear Memory Cost}, 
      author={Noam Shazeer and Mitchell Stern},
      year={2018},
      eprint={1804.04235},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1804.04235}, 
}

@misc{larsen2022degrees,
      title={How many degrees of freedom do we need to train deep networks: a loss landscape perspective}, 
      author={Brett W. Larsen and Stanislav Fort and Nic Becker and Surya Ganguli},
      year={2022},
      eprint={2107.05802},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2107.05802}, 
}

@misc{gurari2018gradient,
      title={Gradient Descent Happens in a Tiny Subspace}, 
      author={Guy Gur-Ari and Daniel A. Roberts and Ethan Dyer},
      year={2018},
      eprint={1812.04754},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1812.04754}, 
}

@misc{anil2019memoryefficient,
      title={Memory-Efficient Adaptive Optimization}, 
      author={Rohan Anil and Vineet Gupta and Tomer Koren and Yoram Singer},
      year={2019},
      eprint={1901.11150},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1901.11150}, 
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}, 
      note={arXiv:1412.6980}
}

@INPROCEEDINGS{Idelbayev_2020_CVPR,
  author={Idelbayev, Yerlan and Carreira-Perpiñán, Miguel Á.},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Low-Rank Compression of Neural Nets: Learning the Rank of Each Layer}, 
  year={2020},
  volume={},
  number={},
  pages={8046-8056},
  keywords={Neural networks;Training;Cost function;Tensile stress;Image coding;Matrix decomposition},
  doi={10.1109/CVPR42600.2020.00807}}


@misc{jaderberg2014speeding,
      title={Speeding up Convolutional Neural Networks with Low Rank Expansions}, 
      author={Max Jaderberg and Andrea Vedaldi and Andrew Zisserman},
      year={2014},
      eprint={1405.3866},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1405.3866}, 
}

@misc{schotthöfer2022lowrank,
      title={Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations}, 
      author={Steffen Schotthöfer and Emanuele Zangrando and Jonas Kusch and Gianluca Ceruti and Francesco Tudisco},
      year={2022},
      eprint={2205.13571},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2205.13571}, 
}

@misc{winata2020lightweight,
      title={Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank Transformer}, 
      author={Genta Indra Winata and Samuel Cahyawijaya and Zhaojiang Lin and Zihan Liu and Pascale Fung},
      year={2020},
      eprint={1910.13923},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1910.13923}, 
}

@misc{zhao2024inrank,
      title={InRank: Incremental Low-Rank Learning}, 
      author={Jiawei Zhao and Yifei Zhang and Beidi Chen and Florian Schäfer and Anima Anandkumar},
      year={2024},
      eprint={2306.11250},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.11250}, 
}

@misc{lin2020hotcake,
      title={HOTCAKE: Higher Order Tucker Articulated Kernels for Deeper CNN Compression}, 
      author={Rui Lin and Ching-Yun Ko and Zhuolun He and Cong Chen and Yuan Cheng and Hao Yu and Graziano Chesi and Ngai Wong},
      year={2020},
      eprint={2002.12663},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.12663}, 
}

@misc{kossaifi2017tensor,
      title={Tensor Contraction Layers for Parsimonious Deep Nets}, 
      author={Jean Kossaifi and Aran Khanna and Zachary C. Lipton and Tommaso Furlanello and Anima Anandkumar},
      year={2017},
      eprint={1706.00439},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.00439}, 
}

@misc{bhojanapalli2020lowrank,
      title={Low-Rank Bottleneck in Multi-head Attention Models}, 
      author={Srinadh Bhojanapalli and Chulhee Yun and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},
      year={2020},
      eprint={2002.07028},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.07028}, 
}

@misc{frankle2020stabilizing,
      title={Stabilizing the Lottery Ticket Hypothesis}, 
      author={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},
      year={2020},
      eprint={1903.01611},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1903.01611}, 
}

@misc{wang2020linformer,
      title={Linformer: Self-Attention with Linear Complexity}, 
      author={Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
      year={2020},
      eprint={2006.04768},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2006.04768}, 
}

@misc{aghajanyan2020intrinsic,
      title={Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning}, 
      author={Armen Aghajanyan and Luke Zettlemoyer and Sonal Gupta},
      year={2020},
      eprint={2012.13255},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2012.13255}, 
}

@misc{zhang2017understanding,
      title={Understanding deep learning requires rethinking generalization}, 
      author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
      year={2017},
      eprint={1611.03530},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.03530}, 
}

@misc{singh2021analytic,
      title={Analytic Insights into Structure and Rank of Neural Network Hessian Maps}, 
      author={Sidak Pal Singh and Gregor Bachmann and Thomas Hofmann},
      year={2021},
      eprint={2106.16225},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.16225}, 
}

@misc{frankle2019lottery,
      title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks}, 
      author={Jonathan Frankle and Michael Carbin},
      year={2019},
      eprint={1803.03635},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1803.03635}, 
}

@misc{wang-etal-2018-glue,
      title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
      author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2019},
      eprint={1804.07461},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1804.07461}, 
}

@misc{touvron2023llama,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      note={arXiv:2307.09288}
}


@misc{lv2023parameter,
      title={Full Parameter Fine-tuning for Large Language Models with Limited Resources}, 
      author={Kai Lv and Yuqing Yang and Tengxiao Liu and Qinghui Gao and Qipeng Guo and Xipeng Qiu},
      year={2024},
      eprint={2306.09782},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.09782}, 
}

@misc{zhang2019root,
      title={Root Mean Square Layer Normalization}, 
      author={Biao Zhang and Rico Sennrich},
      year={2019},
      eprint={1910.07467},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.07467}, 
}

@misc{shazeer2020glu,
      title={GLU Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.05202}, 
}

@misc{wang2019glue,
      title={GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding}, 
      author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
      year={2019},
      eprint={1804.07461},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1804.07461}, 
}


@misc{hoffmann2022training,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL}, 
      note={arXiv:2203.15556}
}



%%% NEW

@inproceedings{barkarson-etal-2022-evolving,
    title = "Evolving Large Text Corpora: Four Versions of the {I}celandic {G}igaword Corpus",
    author = "Barkarson, Starka{\dh}ur  and
      Steingr{\'\i}msson, Stein{\th}{\'o}r  and
      Hafsteinsd{\'o}ttir, Hildur",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.254",
    pages = "2371--2381",
    abstract = "The Icelandic Gigaword Corpus was first published in 2018. Since then new versions have been published annually, containing new texts from additional sources as well as from previous sources. This paper describes the evolution of the corpus in its first four years. All versions are made available under permissive licenses and with each new version the texts are annotated with the latest and most accurate tools. We show how the corpus has grown almost 50{\%} in size from the first version to the fourth and how it was restructured in order to better accommodate different meta-data for different subcorpora. Furthermore, other services have been set up to facilitate usage of the corpus for different use cases. These include a keyword-in-context concordance tool, an n-gram viewer, a word frequency database and pre-trained word embeddings.",
}

@misc{cobbe2021trainingverifierssolvemath,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.14168}, 
}

@misc{li2023loftqlorafinetuningawarequantizationlarge,
      title={LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models}, 
      author={Yixiao Li and Yifan Yu and Chen Liang and Pengcheng He and Nikos Karampatziakis and Weizhu Chen and Tuo Zhao},
      year={2023},
      eprint={2310.08659},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.08659}, 
}

@misc{guo2024lqloralowrankplusquantized,
      title={LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning}, 
      author={Han Guo and Philip Greengard and Eric P. Xing and Yoon Kim},
      year={2024},
      eprint={2311.12023},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.12023}, 
}