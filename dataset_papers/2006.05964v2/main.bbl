\begin{thebibliography}{10}

\bibitem{sezener2020online}
Eren Sezener, Marcus Hutter, David Budden, Jianan Wang, and Joel Veness.
\newblock Online learning in contextual bandits using gated linear networks.
\newblock {\em arXiv preprint arXiv:2002.11611}, 2020.

\bibitem{veness2017online}
Joel Veness, Tor Lattimore, Avishkar Bhoopchand, Agnieszka Grabska-Barwinska,
  Christopher Mattern, and Peter Toth.
\newblock Online learning with gated linear networks.
\newblock {\em arXiv preprint arXiv:1712.01897}, 2017.

\bibitem{veness2019gated}
Joel Veness, Tor Lattimore, Avishkar Bhoopchand, David Budden, Christopher
  Mattern, Agnieszka Grabska-Barwinska, Peter Toth, Simon Schmitt, and Marcus
  Hutter.
\newblock Gated linear networks.
\newblock {\em arXiv preprint arXiv:1910.01526}, 2019.

\bibitem{welling2005exponential}
Max Welling, Michal Rosen-Zvi, and Geoffrey~E Hinton.
\newblock Exponential family harmoniums with an application to information
  retrieval.
\newblock In {\em Advances in neural information processing systems}, pages
  1481--1488, 2005.

\bibitem{williams2002products}
Christopher Williams, Felix~V Agakov, and Stephen~N Felderhof.
\newblock Products of gaussians.
\newblock In {\em Advances in neural information processing systems}, pages
  1017--1024, 2002.

\bibitem{peng2019mcp}
Xue~Bin Peng, Michael Chang, Grace Zhang, Pieter Abbeel, and Sergey Levine.
\newblock Mcp: Learning composable hierarchical control with multiplicative
  compositional policies.
\newblock {\em arXiv preprint arXiv:1905.09808}, 2019.

\bibitem{cao2014generalized}
Yanshuai Cao and David~J Fleet.
\newblock Generalized product of experts for automatic and principled fusion of
  gaussian process predictions.
\newblock {\em arXiv preprint arXiv:1410.7827}, 2014.

\bibitem{marblestone2020product}
Adam Marblestone, Yan Wu, and Greg Wayne.
\newblock Product kanerva machines: Factorized bayesian memory.
\newblock {\em arXiv preprint arXiv:2002.02385}, 2020.

\bibitem{deisenroth2015distributed}
Marc~Peter Deisenroth and Jun~Wei Ng.
\newblock Distributed gaussian processes.
\newblock {\em arXiv preprint arXiv:1502.02843}, 2015.

\bibitem{lee2015deeply}
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu.
\newblock Deeply-supervised nets.
\newblock In {\em Artificial intelligence and statistics}, pages 562--570,
  2015.

\bibitem{rasmus2015semi}
Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko.
\newblock Semi-supervised learning with ladder networks.
\newblock In {\em Advances in neural information processing systems}, pages
  3546--3554, 2015.

\bibitem{lowe2019putting}
Sindy L{\"o}we, Peter O'Connor, and Bastiaan Veeling.
\newblock Putting an end to end-to-end: Gradient-isolated learning of
  representations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3033--3045, 2019.

\bibitem{lee2018gradient}
Yoonho Lee and Seungjin Choi.
\newblock Gradient-based meta-learning with learned layerwise metric and
  subspace.
\newblock {\em arXiv preprint arXiv:1801.05558}, 2018.

\bibitem{alain2016understanding}
Guillaume Alain and Yoshua Bengio.
\newblock Understanding intermediate layers using linear classifier probes.
\newblock {\em arXiv preprint arXiv:1610.01644}, 2016.

\bibitem{sussillo2009generating}
David Sussillo and Larry~F Abbott.
\newblock Generating coherent patterns of activity from chaotic neural
  networks.
\newblock {\em Neuron}, 63(4):544--557, 2009.

\bibitem{nokland2019training}
Arild N{\o}kland and Lars~Hiller Eidnes.
\newblock Training neural networks with local error signals.
\newblock {\em arXiv preprint arXiv:1901.06656}, 2019.

\bibitem{mostafa2018deep}
Hesham Mostafa, Vishwajith Ramesh, and Gert Cauwenberghs.
\newblock Deep supervised learning using local errors.
\newblock {\em Frontiers in neuroscience}, 12:608, 2018.

\bibitem{schmidhuber1992learning}
J{\"u}rgen Schmidhuber.
\newblock Learning to control fast-weight memories: An alternative to dynamic
  recurrent networks.
\newblock {\em Neural Computation}, 4(1):131--139, 1992.

\bibitem{ha2016hypernetworks}
David Ha, Andrew Dai, and Quoc~V Le.
\newblock Hypernetworks.
\newblock {\em arXiv preprint arXiv:1609.09106}, 2016.

\bibitem{cheung2019superposition}
Brian Cheung, Alexander Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno
  Olshausen.
\newblock Superposition of many models into one.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10867--10876, 2019.

\bibitem{von2019continual}
Johannes von Oswald, Christian Henning, Jo{\~a}o Sacramento, and Benjamin~F
  Grewe.
\newblock Continual learning with hypernetworks.
\newblock {\em arXiv preprint arXiv:1906.00695}, 2019.

\bibitem{perez2018film}
Ethan Perez, Florian Strub, Harm De~Vries, Vincent Dumoulin, and Aaron
  Courville.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock In {\em Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem{Hazan16}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock {\em Found. Trends Optim.}, 2(3-4):157--325, 2016.

\bibitem{bromiley2018}
P.A. Bromiley.
\newblock Products and convolutions of gaussian probability density functions.
\newblock 2018.

\bibitem{wainwright2008graphical}
Martin~J Wainwright and Michael~I Jordan.
\newblock Graphical models, exponential families, and variational inference.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  1(1-2):1--305, 2008.

\bibitem{Zinkevich03}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In Tom Fawcett and Nina Mishra, editors, {\em ICML}, pages 928--936.
  AAAI Press, 2003.

\bibitem{silverman85}
B.~W. Silverman.
\newblock Some aspects of the spline smoothing approach to non-parametric
  regression curve fitting.
\newblock {\em Journal of the Royal Statistical Society. Series B
  (Methodological)}, 47(1):1--52, 1985.

\bibitem{kersting2007most}
Kristian Kersting, Christian Plagemann, Patrick Pfaff, and Wolfram Burgard.
\newblock Most likely heteroscedastic gaussian process regression.
\newblock In {\em Proceedings of the 24th international conference on Machine
  learning}, pages 393--400, 2007.

\bibitem{Charikar2002}
Moses~S. Charikar.
\newblock Similarity estimation techniques from rounding algorithms.
\newblock In {\em STOC '02: Proceedings of the thiry-fourth annual ACM
  symposium on Theory of computing}, pages 380--388, New York, NY, USA, 2002.
  ACM.

\bibitem{graves2011practical}
Alex Graves.
\newblock Practical variational inference for neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  2348--2356, 2011.

\bibitem{hernandez2015probabilistic}
Jos{\'e}~Miguel Hern{\'a}ndez-Lobato and Ryan Adams.
\newblock Probabilistic backpropagation for scalable learning of bayesian
  neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1861--1869, 2015.

\bibitem{gal2015dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock {\em arXiv preprint arXiv:1506.02142}, 2015.

\bibitem{VenessNHB12}
Joel Veness, Kee~Siong Ng, Marcus Hutter, and Michael~H. Bowling.
\newblock Context tree switching.
\newblock In James~A. Storer and Michael~W. Marcellin, editors, {\em 2012 Data
  Compression Conference, Snowbird, UT, USA, April 10-12, 2012}, pages
  327--336. {IEEE} Computer Society, 2012.

\bibitem{Boyd04}
Stephen Boyd and Lieven Vandenberghe.
\newblock {\em Convex Optimization}.
\newblock Cambridge University Press, USA, 2004.

\bibitem{vijayakumar2000locally}
Sethu Vijayakumar and Stefan Schaal.
\newblock Locally weighted projection regression: An o (n) algorithm for
  incremental real time learning in high dimensional space.
\newblock In {\em Proceedings of the Seventeenth International Conference on
  Machine Learning (ICML 2000)}, volume~1, pages 288--293, 2000.

\bibitem{arik2019tabnet}
Sercan~O Arik and Tomas Pfister.
\newblock Tabnet: Attentive interpretable tabular learning.
\newblock {\em arXiv preprint arXiv:1908.07442}, 2019.

\bibitem{riquelme2018deep}
Carlos Riquelme, George Tucker, and Jasper Snoek.
\newblock Deep bayesian bandits showdown: An empirical comparison of bayesian
  deep networks for thompson sampling.
\newblock {\em arXiv preprint arXiv:1802.09127}, 2018.

\bibitem{auer2002ucb}
Peter Auer, Nicol\`{o} Cesa-Bianchi, and Paul Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock {\em Mach. Learn.}, 47(2-3):235--256, May 2002.

\bibitem{vincent2011connection}
Pascal Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock {\em Neural computation}, 23(7):1661--1674, 2011.

\bibitem{bigdeli2020learning}
Siavash~A Bigdeli, Geng Lin, Tiziano Portenier, L~Andrea Dunbar, and Matthias
  Zwicker.
\newblock Learning generative models using denoising density estimators.
\newblock {\em arXiv preprint arXiv:2001.02728}, 2020.

\bibitem{sohl2015deep}
Jascha Sohl-Dickstein, Eric~A Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock {\em arXiv preprint arXiv:1503.03585}, 2015.

\bibitem{saremi2019neural}
Saeed Saremi and Aapo Hyvarinen.
\newblock Neural empirical bayes.
\newblock {\em Journal of Machine Learning Research}, 20:1--23, 2019.

\bibitem{hyvarinen2005estimation}
Aapo Hyv{\"a}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock {\em Journal of Machine Learning Research}, 6(Apr):695--709, 2005.

\bibitem{alain2014regularized}
Guillaume Alain and Yoshua Bengio.
\newblock What regularized auto-encoders learn from the data-generating
  distribution.
\newblock {\em The Journal of Machine Learning Research}, 15(1):3563--3593,
  2014.

\bibitem{neal2011mcmc}
Radford~M Neal et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock {\em Handbook of markov chain monte carlo}, 2(11):2, 2011.

\bibitem{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, and Skye Wanderman-Milne.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.

\bibitem{chex2020github}
David Budden, Matteo Hessel, Iurii Kemaev, Stephen Spencer, and Fabio Viola.
\newblock Chex: Testing made fun, in {JAX!}, 2020.

\bibitem{haiku2020github}
Tom Hennigan, Trevor Cai, Tamara Norman, and Igor Babuschkin.
\newblock {H}aiku: {S}onnet for {JAX}, 2020.

\bibitem{optax2020github}
Matteo Hessel, David Budden, Fabio Viola, Mihaela Rosca, Eren Sezener, and Tom
  Hennigan.
\newblock Optax: Composable gradient transformation and optimisation, in
  {JAX!}, 2020.

\bibitem{rlax2020github}
David Budden, Matteo Hessel, John Quan, Steven Kapturowski, Kate Baumli, Surya
  Bhupatiraju, Aurelia Guy, and Michael King.
\newblock {RL}ax: {R}einforcement {L}earning in {JAX}, 2020.

\bibitem{bishop2006prml}
Christopher~M. Bishop.
\newblock {\em Pattern Recognition and Machine Learning (Information Science
  and Statistics)}.
\newblock Springer-Verlag, Berlin, Heidelberg, 2006.

\bibitem{veness2012context}
Joel Veness, Kee~Siong Ng, Marcus Hutter, and Michael Bowling.
\newblock Context tree switching.
\newblock In {\em 2012 Data Compression Conference}, pages 327--336. IEEE,
  2012.

\bibitem{abadi2016tensorflow}
Mart{\'\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et~al.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
\newblock {\em arXiv preprint arXiv:1603.04467}, 2016.

\bibitem{bellemare2016unifying}
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton,
  and Remi Munos.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock In {\em Advances in neural information processing systems}, pages
  1471--1479, 2016.

\end{thebibliography}
