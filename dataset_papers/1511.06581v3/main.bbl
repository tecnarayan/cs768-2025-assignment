\begin{thebibliography}{26}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2015)Ba, Mnih, and Kavukcuoglu]{Ba:2015}
Ba, J., Mnih, V., and Kavukcuoglu, K.
\newblock Multiple object recognition with visual attention.
\newblock In \emph{ICLR}, 2015.

\bibitem[{Baird}(1993)]{Baird:1993}
{Baird}, L.C.
\newblock Advantage updating.
\newblock Technical Report WL-TR-93-1146, Wright-Patterson Air Force Base,
  1993.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{Bellemare:2013}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bellemare et~al.(2016)Bellemare, Ostrovski, Guez, Thomas, and
  Munos]{Bellemare2015Persistent}
Bellemare, M.~G., Ostrovski, G., Guez, A., Thomas, P.~S., and Munos, R.
\newblock Increasing the action gap: New operators for reinforcement learning.
\newblock In \emph{AAAI}, 2016.
\newblock To appear.

\bibitem[Bengio et~al.(2013)Bengio, Boulanger-Lewandowski, and
  Pascanu]{Bengio:2013}
Bengio, Y., Boulanger-Lewandowski, N., and Pascanu, R.
\newblock Advances in optimizing recurrent networks.
\newblock In \emph{ICASSP}, pp.\  8624--8628, 2013.

\bibitem[Fukushima(1980)]{Fukushima:1980}
Fukushima, K.
\newblock {N}eocognitron: {A} self-organizing neural network model for a
  mechanism of pattern recognition unaffected by shift in position.
\newblock \emph{Biological Cybernetics}, 36:\penalty0 193--202, 1980.

\bibitem[Guo et~al.(2014)Guo, Singh, Lee, Lewis, and Wang]{Guo:2014}
Guo, X., Singh, S., Lee, H., Lewis, R.~L., and Wang, X.
\newblock Deep learning for real-time {Atari} game play using offline
  {Monte-Carlo} tree search planning.
\newblock In \emph{NIPS}, pp.\  3338--3346. 2014.

\bibitem[Harmon \& {Baird}(1996)Harmon and {Baird}]{Harmon:1996}
Harmon, M.E. and {Baird}, L.C.
\newblock Multi-player residual advantage learning with general function
  approximation.
\newblock Technical Report WL-TR-1065, Wright-Patterson Air Force Base, 1996.

\bibitem[Harmon et~al.(1995)Harmon, Baird, and Klopf]{Harmon:1995}
Harmon, M.E., Baird, L.C., and Klopf, A.H.
\newblock Advantage updating applied to a differential game.
\newblock In G.~Tesauro, D.S.~Touretzky and Leen, T.K. (eds.), \emph{NIPS},
  1995.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{LeCun:2015}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Levine et~al.(2015)Levine, Finn, Darrell, and Abbeel]{levine2015end}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{arXiv preprint arXiv:1504.00702}, 2015.

\bibitem[Lin(1993)]{Lin:1993}
Lin, L.J.
\newblock \emph{Reinforcement learning for robots using neural networks}.
\newblock PhD thesis, School of Computer Science, Carnegie Mellon University,
  1993.

\bibitem[Maddison et~al.(2015)Maddison, Huang, Sutskever, and
  Silver]{Maddison:2015}
Maddison, C.~J., Huang, A., Sutskever, I., and Silver, D.
\newblock {Move Evaluation in Go Using Deep Convolutional Neural Networks}.
\newblock In \emph{ICLR}, 2015.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{Mnih:2015}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Nair et~al.(2015)Nair, Srinivasan, Blackwell, Alcicek, Fearon, Maria,
  Panneershelvam, Suleyman, Beattie, Petersen, Legg, Mnih, Kavukcuoglu, and
  Silver]{Nair:2015}
Nair, A., Srinivasan, P., Blackwell, S., Alcicek, C., Fearon, R., Maria, A.~De,
  Panneershelvam, V., Suleyman, M., Beattie, C., Petersen, S., Legg, S., Mnih,
  V., Kavukcuoglu, K., and Silver, D.
\newblock Massively parallel methods for deep reinforcement learning.
\newblock In \emph{Deep Learning Workshop, ICML}, 2015.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and Silver]{Schaul:2015}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock In \emph{ICLR}, 2016.

\bibitem[Schulman et~al.(2015)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{schulman2015advantage}
Schulman, J., Moritz, P., Levine, S., Jordan, M.~I., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock arXiv preprint arXiv:1506.02438, 2015.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, van~den
  Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman,
  Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel,
  and Hassabis]{alphago}
Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., van~den Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I.,
  Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T., and Hassabis, D.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 01 2016.

\bibitem[Simonyan et~al.(2013)Simonyan, Vedaldi, and Zisserman]{Simonyan:2013}
Simonyan, K., Vedaldi, A., and Zisserman, A.
\newblock Deep inside convolutional networks: Visualising image classification
  models and saliency maps.
\newblock \emph{arXiv preprint arXiv:1312.6034}, 2013.

\bibitem[Stadie et~al.(2015)Stadie, Levine, and Abbeel]{Stadie:2015}
Stadie, B.~C., Levine, S., and Abbeel, P.
\newblock Incentivizing exploration in reinforcement learning with deep
  predictive models.
\newblock \emph{arXiv preprint arXiv:1507.00814}, 2015.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{SuttonBarto:1998}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Introduction to reinforcement learning}.
\newblock MIT Press, 1998.

\bibitem[Sutton et~al.(2000)Sutton, Mcallester, Singh, and
  Mansour]{Sutton:2000}
Sutton, R.~S., Mcallester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{NIPS}, pp.\  1057--1063, 2000.

\bibitem[{van Hasselt}(2010)]{vanHasselt:2010}
{van Hasselt}, H.
\newblock Double {Q}-learning.
\newblock \emph{NIPS}, 23:\penalty0 2613--2621, 2010.

\bibitem[{van Hasselt} et~al.(2015){van Hasselt}, Guez, and
  Silver]{vanHasselt:2015}
{van Hasselt}, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double {Q}-learning.
\newblock \emph{arXiv preprint arXiv:1509.06461}, 2015.

\bibitem[{van Seijen} et~al.(2009){van Seijen}, {van Hasselt}, Whiteson, and
  Wiering]{vanSeijen:2009}
{van Seijen}, H., {van Hasselt}, H., Whiteson, S., and Wiering, M.
\newblock A theoretical and empirical analysis of {Expected Sarsa}.
\newblock In \emph{IEEE Symposium on Adaptive Dynamic Programming and
  Reinforcement Learning}, pp.\  177--184. 2009.

\bibitem[Watter et~al.(2015)Watter, Springenberg, Boedecker, and
  Riedmiller]{Watter:2015}
Watter, M., Springenberg, J.~T., Boedecker, J., and Riedmiller, M.~A.
\newblock Embed to control: {A} locally linear latent dynamics model for
  control from raw images.
\newblock In \emph{NIPS}, 2015.

\end{thebibliography}
