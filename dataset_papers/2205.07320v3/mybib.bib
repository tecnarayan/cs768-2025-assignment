@article{hendrycks2019robustness,
      title={Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
      author={Hendrycks, Dan and Dietterich, Thomas},
      journal={Proceedings of the International Conference on Learning Representations},
      year={2019}
    }

@article{ansuini2020similarity,
  title={On the Similarity between Hidden Layers of Pruned and Unpruned Convolutional Neural Networks.},
  author={Ansuini, Alessio and Medvet, Eric and Pellegrino, Felice Andrea and Zullich, Marco and De Marsico, M and di Baja, G Sanniti and Fred, A},
  booktitle={ICPRAM},
  pages={52--59},
  year={2020}
}

@article{liang2021pruning,
  title={Pruning and quantization for deep neural network acceleration: A survey},
  author={Liang, Tailin and Glossner, John and Wang, Lei and Shi, Shaobo and Zhang, Xiaotong},
  journal={Neurocomputing},
  volume={461},
  pages={370--403},
  year={2021},
  publisher={Elsevier}
}

@article{
he2022can,
title={Can network pruning benefit deep learning under label noise?},
author={Zheng He and Quanzhi Zhu and Zengchang Qin},
year={2022},
}

@inproceedings{nakkiran2019deep,
  title={Deep Double Descent: Where Bigger Models and More Data Hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{bartoldson2020generalization,
  title={The generalization-stability tradeoff in neural network pruning},
  author={Bartoldson, Brian and Morcos, Ari and Barbu, Adrian and Erlebacher, Gordon},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20852--20864},
  year={2020}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={ICLR},
  year={2017}
}

@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence},
  year={2017}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@article{lewkowycz2020large,
  title={The large learning rate phase of deep learning: the catapult mechanism},
  author={Lewkowycz, Aitor and Bahri, Yasaman and Dyer, Ethan and Sohl-Dickstein, Jascha and Gur-Ari, Guy},
  year={2020}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@article{hayou2021probabilistic,
  title={Probabilistic fine-tuning of pruning masks and PAC-Bayes self-bounded learning},
  author={Hayou, Soufiane and He, Bobby and Dziugaite, Gintare Karolina},
  journal={arXiv preprint arXiv:2110.11804},
  year={2021}
}

@article{germain2016pac,
  title={PAC-Bayesian theory meets Bayesian inference},
  author={Germain, Pascal and Bach, Francis and Lacoste, Alexandre and Lacoste-Julien, Simon},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@article{alquier2016properties,
  title={On the properties of variational approximations of Gibbs posteriors},
  author={Alquier, Pierre and Ridgway, James and Chopin, Nicolas},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={8374--8414},
  year={2016},
  publisher={JMLR. org}
}

@inproceedings{liu2018rethinking,
  title={Rethinking the Value of Network Pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{you2019drawing,
  title={Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks},
  author={You, Haoran and Li, Chaojian and Xu, Pengfei and Fu, Yonggan and Wang, Yue and Chen, Xiaohan and Baraniuk, Richard G and Wang, Zhangyang and Lin, Yingyan},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{bain2021visualizing,
  title={Visualizing the Loss Landscape of Winning Lottery Tickets},
  author={Bain, Robert},
  journal={arXiv preprint arXiv:2112.08538},
  year={2021}
}

@inproceedings{foret2020sharpness,
  title={Sharpness-aware Minimization for Efficiently Improving Generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{li2018visualizing,
  title={Visualizing the Loss Landscape of Neural Nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  booktitle={NIPS'18: Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={6391--6401},
  year={2018},
  organization={Curran Associates Inc.}
}

@inproceedings{yao2020pyhessian,
  title={Pyhessian: Neural networks through the lens of the hessian},
  author={Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W},
  booktitle={2020 IEEE International Conference on Big Data (Big Data)},
  pages={581--590},
  year={2020},
  organization={IEEE}
}

@inproceedings{tsuzuku2020normalized,
  title={Normalized flat minima: Exploring scale invariant definition of flat minima for neural networks using pac-bayesian analysis},
  author={Tsuzuku, Yusuke and Sato, Issei and Sugiyama, Masashi},
  booktitle={International Conference on Machine Learning},
  pages={9636--9647},
  year={2020},
  organization={PMLR}
}

@article{frankle2019stabilizing,
  title={Stabilizing the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
  journal={arXiv preprint arXiv:1903.01611},
  year={2019}
}

@article{zhou2019deconstructing,
  title={Deconstructing lottery tickets: Zeros, signs, and the supermask},
  author={Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{arpit2017closer,
  title={A Closer Look at Memorization in Deep Networks},
  author={Arpit, Devansh and Jastrzebski, Stanislaw K and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron C and Bengio, Yoshua and others},
  booktitle={ICML},
  year={2017}
}

@inproceedings{frankle2018lottery,
  title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  author={Frankle, Jonathan and Carbin, Michael},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{sanyal2020benign,
  title={How Benign is Benign Overfitting?},
  author={Sanyal, Amartya and Dokania, Puneet K and Kanade, Varun and Torr, Philip},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{stephenson2020geometry,
  title={On the geometry of generalization and memorization in deep neural networks},
  author={Stephenson, Cory and Ganesh, Abhinav and Hui, Yue and Tang, Hanlin and Chung, SueYeon and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{feldman2020neural,
  title={What neural networks memorize and why: Discovering the long tail via influence estimation},
  author={Feldman, Vitaly and Zhang, Chiyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2881--2891},
  year={2020}
}

@article{hooker2019compressed,
  title={What do compressed deep neural networks forget?},
  author={Hooker, Sara and Courville, Aaron and Clark, Gregory and Dauphin, Yann and Frome, Andrea},
  journal={arXiv preprint arXiv:1911.05248},
  year={2019}
}

@article{li2019towards,
  title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{tonolini2020variational,
  title={Variational Sparse Coding},
  author={Tonolini, Francesco and Jensen, Bj{\o}rn Sand and Murray-Smith, Roderick},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={690--700},
  year={2020},
  organization={PMLR}
}

@book{mackay2003information,
  title={Information theory, inference and learning algorithms},
  author={MacKay, David JC and Mac Kay, David JC and others},
  year={2003},
  publisher={Cambridge university press}
}

@article{xie2021artificial,
  title={Artificial neural variability for deep learning: on overfitting, noise memorization, and catastrophic forgetting},
  author={Xie, Zeke and He, Fengxiang and Fu, Shaopeng and Sato, Issei and Tao, Dacheng and Sugiyama, Masashi},
  journal={Neural computation},
  volume={33},
  number={8},
  pages={2163--2192},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@InProceedings{Ramanujan_2020_CVPR,
author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
title = {What's Hidden in a Randomly Weighted Neural Network?},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@inproceedings{dziugaite2020role,
  title={On the role of data in PAC-Bayes},
  author={Dziugaite, Gintare Karolina and Hsu, Kyle and Gharbieh, Waseem and Arpino, Gabriel and Roy, Daniel},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={604--612},
  year={2021},
  organization={PMLR}
}

@article{savarese2020winning,
  title={Winning the lottery with continuous sparsification},
  author={Savarese, Pedro and Silva, Hugo and Maire, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11380--11390},
  year={2020}
}

@inproceedings{han2015learning,
  title={Learning both Weights and Connections for Efficient Neural Network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William J},
  booktitle={NIPS},
  year={2015}
}

@article{han2016dsd,
  title={DSD: Dense-Sparse-Dense Training for Deep Neural Networks},
  author={Han, Song and Pool, Jeff and Narang, Sharan and Mao, Huizi and Gong, Enhao and Tang, Shijian and Elsen, Erich and Vajda, Peter and Paluri, Manohar and Tran, John and others},
  journal={In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net},
  year={2017}
}

@inproceedings{liu2021lottery,
  title={Lottery Ticket Preserves Weight Correlation: Is It Desirable or Not?},
  author={Liu, Ning and Yuan, Geng and Che, Zhengping and Shen, Xuan and Ma, Xiaolong and Jin, Qing and Ren, Jian and Tang, Jian and Liu, Sijia and Wang, Yanzhi},
  booktitle={International Conference on Machine Learning},
  pages={7011--7020},
  year={2021},
  organization={PMLR}
}

@article{zhang2021lottery,
  title={Why Lottery Ticket Wins? A Theoretical Perspective of Sample Complexity on Sparse Neural Networks},
  author={Zhang, Shuai and Wang, Meng and Liu, Sijia and Chen, Pin-Yu and Xiong, Jinjun},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{malach2020proving,
  title={Proving the lottery ticket hypothesis: Pruning is all you need},
  author={Malach, Eran and Yehudai, Gilad and Shalev-Schwartz, Shai and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={6682--6691},
  year={2020},
  organization={PMLR}
}

@article{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@inproceedings{hassibi1993optimal,
  title={Optimal brain surgeon and general network pruning},
  author={Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  booktitle={IEEE international conference on neural networks},
  pages={293--299},
  year={1993},
  organization={IEEE}
}

@article{simonyan2015very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={ICLR},
  year={2015}
}

@article{zhang2017understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={ICLR},
  year={2017}
}

@inproceedings{xia2021robust,
  title={Robust early-learning: Hindering the memorization of noisy labels},
  author={Xia, Xiaobo and Liu, Tongliang and Han, Bo and Gong, Chen and Wang, Nannan and Ge, Zongyuan and Chang, Yi},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{
neyshabur2018a,
title={A {PAC}-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks},
author={Behnam Neyshabur and Srinadh Bhojanapalli and Nathan Srebro},
booktitle={International Conference on Learning Representations},
year={2018},
}

@inproceedings{
Jiang*2020Fantastic,
title={Fantastic Generalization Measures and Where to Find Them},
author={Yiding Jiang* and Behnam Neyshabur* and Hossein Mobahi and Dilip Krishnan and Samy Bengio},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{nagarajan2019generalization,
  title={Generalization in Deep Networks: The Role of Distance from Initialization},
  author={Nagarajan, Vaishnavh and Kolter, J Zico},
  booktitle={NIPS workshop on Deep Learning: Bridging Theory and Practice},
  year={2019}
}

@inproceedings{
Jastrzebski2020The,
title={The Break-Even Point on Optimization Trajectories of Deep Neural Networks},
author={Stanislaw Jastrzebski and Maciej Szymczak and Stanislav Fort and Devansh Arpit and Jacek Tabor and Kyunghyun Cho* and Krzysztof Geras*},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{
xie2021a,
title={A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima},
author={Zeke Xie and Issei Sato and Masashi Sugiyama},
booktitle={International Conference on Learning Representations},
year={2021},
}

@techreport{lawrence1998size,
  title={What size neural network gives optimal generalization? Convergence properties of backpropagation},
  author={Lawrence, Steve and Giles, C Lee and Tsoi, Ah Chung},
  year={1998}
}

@inproceedings{DBLP:journals/corr/NeyshaburTS14,
  author={Behnam Neyshabur and Ryota Tomioka and Nathan Srebro},
  title={In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning.},
  year={2015},
  cdate={1420070400000},
  booktitle={ICLR (Workshop)},
}

@misc{open_lth,
  author = {Jonathan Frankle},
  title = {OpenLTH: A Framework for Lottery Tickets and Beyond},
  year = {2020},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/facebookresearch/open_lth}},
}

@inproceedings{dziugaite2021role,
    title={On the role of data in PAC-Bayes bounds},
    author={Dziugaite, Gintare Karolina and Hsu, Kyle and Gharbieh, Waseem and Arpino, Gabriel and Roy, Daniel M},
    booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
    year={2021}
}

@inproceedings{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={1019--1028},
  year={2017},
  organization={PMLR}
}

@article{yao2018hessian,
  title={Hessian-based analysis of large batch training and robustness to adversaries},
  author={Yao, Zhewei and Gholami, Amir and Lei, Qi and Keutzer, Kurt and Mahoney, Michael W},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}