\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{WCYW19}

\bibitem[AKKS20]{agarwal2020flambe}
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock {\em arXiv preprint arXiv:2006.10814}, 2020.

\bibitem[AKLM20]{agarwal2020optimality}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock In {\em Conference on Learning Theory}, pages 64--66, 2020.

\bibitem[AMS07]{antos2007fitted}
Andr{\'a}s Antos, R{\'e}mi Munos, and Csaba Szepesv{\'a}ri.
\newblock Fitted q-iteration in continuous action-space mdps.
\newblock 2007.

\bibitem[ASM08]{antos2008learning}
Andr{\'a}s Antos, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock {\em Machine Learning}, 71(1):89--129, 2008.

\bibitem[ASN20]{agarwal2020optimistic}
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  104--114. PMLR, 2020.

\bibitem[AYPS11]{Abbasi11}
Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2011.

\bibitem[Ber95]{Bertsekas_dyn1}
D.~P. Bertsekas.
\newblock {\em Dynamic programming and stochastic control}, volume~1.
\newblock Athena Scientific, Belmont, MA, 1995.

\bibitem[BGB20]{buckman2020importance}
Jacob Buckman, Carles Gelada, and Marc~G Bellemare.
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock {\em arXiv preprint arXiv:2009.06799}, 2020.

\bibitem[BR20]{bhandari2020note}
Jalaj Bhandari and Daniel Russo.
\newblock A note on the linear convergence of policy gradient methods.
\newblock {\em arXiv preprint arXiv:2007.11120}, 2020.

\bibitem[BT96]{bertsekas1996neuro}
Dimitri~P Bertsekas and John~N Tsitsiklis.
\newblock {\em Neuro-dynamic programming}.
\newblock Athena Scientific, 1996.

\bibitem[Bub14]{bubeck2014convex}
S{\'e}bastien Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock {\em arXiv preprint arXiv:1405.4980}, 2014.

\bibitem[CJ19]{chen2019information}
Jinglin Chen and Nan Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1042--1051, 2019.

\bibitem[DJL21]{duan2021risk}
Yaqi Duan, Chi Jin, and Zhiyuan Li.
\newblock Risk bounds and rademacher complexity in batch reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2103.13883}, 2021.

\bibitem[DKL{\etalchar{+}}21]{du2021bilinear}
Simon~S Du, Sham~M Kakade, Jason~D Lee, Shachar Lovett, Gaurav Mahajan, Wen
  Sun, and Ruosong Wang.
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock {\em arXiv preprint arXiv:2103.10897}, 2021.

\bibitem[dlPLS09]{PenLaiSha09}
V.~H. de~la {P}ena, T.~L. Lai, and Q.~M. Shao.
\newblock {\em Self-normalized processes}.
\newblock Springer, 2009.

\bibitem[DW20]{duan2020minimax}
Yaqi Duan and Mengdi Wang.
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock {\em arXiv preprint arXiv:2002.09516}, 2020.

\bibitem[FCG18]{farajtabar2018more}
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.
\newblock More robust doubly robust off-policy evaluation.
\newblock In {\em International Conference on Machine Learning}, pages
  1447--1456. PMLR, 2018.

\bibitem[FGSM16]{farahmand2016regularized}
Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv{\'a}ri, and Shie
  Mannor.
\newblock Regularized policy iteration with nonparametric function spaces.
\newblock {\em The Journal of Machine Learning Research}, 17(1):4809--4874,
  2016.

\bibitem[FSM10]{farahmand2010error}
Amir-massoud Farahmand, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Error propagation for approximate policy and value iteration.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2010.

\bibitem[FWXY20]{fan2020theoretical}
Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang.
\newblock A theoretical analysis of deep q-learning.
\newblock In {\em Learning for Dynamics and Control}, pages 486--489. PMLR,
  2020.

\bibitem[FYW20]{fu2020single}
Zuyue Fu, Zhuoran Yang, and Zhaoran Wang.
\newblock Single-timescale actor-critic provably finds globally optimal policy.
\newblock {\em arXiv preprint arXiv:2008.00483}, 2020.

\bibitem[GSP19]{geist2019theory}
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin.
\newblock A theory of regularized markov decision processes.
\newblock In {\em International Conference on Machine Learning}, pages
  2160--2169. PMLR, 2019.

\bibitem[HJD{\etalchar{+}}21]{hao2021bootstrapping}
Botao Hao, Xiang Ji, Yaqi Duan, Hao Lu, Csaba Szepesv{\'a}ri, and Mengdi Wang.
\newblock Bootstrapping statistical inference for off-policy evaluation.
\newblock {\em arXiv preprint arXiv:2102.03607}, 2021.

\bibitem[HTAL17]{haarnoja2017reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In {\em International Conference on Machine Learning}, pages
  1352--1361. PMLR, 2017.

\bibitem[HWS{\etalchar{+}}15]{heess2015learning}
Nicolas Heess, Greg Wayne, David Silver, Timothy Lillicrap, Yuval Tassa, and
  Tom Erez.
\newblock Learning continuous control policies by stochastic value gradients,
  2015.

\bibitem[HZAL18]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor, 2018.

\bibitem[JGS{\etalchar{+}}19]{jaques2019way}
Natasha Jaques, Asma Ghandeharioun, Judy~Hanwen Shen, Craig Ferguson, Agata
  Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock {\em arXiv preprint arXiv:1907.00456}, 2019.

\bibitem[JH20]{jiang2020minimax}
Nan Jiang and Jiawei Huang.
\newblock Minimax value interval for off-policy evaluation and policy
  optimization.
\newblock {\em arXiv preprint arXiv:2002.02081}, 2020.

\bibitem[JKA{\etalchar{+}}17]{jiang17contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E.
  Schapire.
\newblock Contextual decision processes with low {B}ellman rank are
  {PAC}-learnable.
\newblock In Doina Precup and Yee~Whye Teh, editors, {\em International
  Conference on Machine Learning (ICML)}, volume~70 of {\em Proceedings of
  Machine Learning Research}, pages 1704--1713, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[JL16]{jiang2016doubly}
Nan Jiang and Lihong Li.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  652--661. PMLR, 2016.

\bibitem[JLM21]{jin2021bellman}
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi.
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock {\em arXiv preprint arXiv:2102.00815}, 2021.

\bibitem[JYW20]{jin2020pessimism}
Ying Jin, Zhuoran Yang, and Zhaoran Wang.
\newblock Is pessimism provably efficient for offline rl?
\newblock {\em arXiv preprint arXiv:2012.15085}, 2020.

\bibitem[JYWJ20]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, 2020.

\bibitem[K{\etalchar{+}}03]{kakade2003sample}
Sham~Machandranath Kakade et~al.
\newblock {\em On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London London, England, 2003.

\bibitem[Kak01]{kakade2001natural}
Sham~M Kakade.
\newblock A natural policy gradient.
\newblock {\em Advances in neural information processing systems}, 14, 2001.

\bibitem[KFTL19]{kumar2019stabilizing}
Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock {\em arXiv preprint arXiv:1906.00949}, 2019.

\bibitem[KJVM21]{khodadadian2021linear}
Sajad Khodadadian, Prakirt~Raj Jhunjhunwala, Sushil~Mahavir Varma, and
  Siva~Theja Maguluri.
\newblock On the linear convergence of natural policy gradient algorithm.
\newblock {\em arXiv preprint arXiv:2105.01424}, 2021.

\bibitem[KRNJ20]{kidambi2020morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2005.05951}, 2020.

\bibitem[KT00]{konda2000actor}
Vijay~R Konda and John~N Tsitsiklis.
\newblock Actor-critic algorithms.
\newblock In {\em Advances in neural information processing systems}, pages
  1008--1014, 2000.

\bibitem[KT03]{KonTsi03}
V.~R. Konda and J.~N. Tsitsiklis.
\newblock On actor-critic algorithms.
\newblock {\em {S}{I}{A}{M} Jour. Opt. Control}, 42(4):1143--1166, 2003.

\bibitem[KU19]{kallus2019efficiently}
Nathan Kallus and Masatoshi Uehara.
\newblock Efficiently breaking the curse of horizon in off-policy evaluation
  with double reinforcement learning.
\newblock {\em arXiv preprint arXiv:1909.05850}, 2019.

\bibitem[KZTL20]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2006.04779}, 2020.

\bibitem[Lan21]{lan2021policy}
Guanghui Lan.
\newblock Policy mirror descent for reinforcement learning: Linear convergence,
  new sampling complexity, and generalized problem classes.
\newblock {\em arXiv preprint arXiv:2102.00135}, 2021.

\bibitem[LGR{\etalchar{+}}18]{liu2018representation}
Yao Liu, Omer Gottesman, Aniruddh Raghu, Matthieu Komorowski, Aldo~A Faisal,
  Finale Doshi-Velez, and Emma Brunskill.
\newblock Representation balancing mdps for off-policy policy evaluation.
\newblock {\em Advances in Neural Information Processing Systems},
  31:2644--2653, 2018.

\bibitem[LKTF20]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock {\em arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[LLTZ18]{liu2018breaking}
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5356--5366, 2018.

\bibitem[LQM20]{liao2020batch}
Peng Liao, Zhengling Qi, and Susan Murphy.
\newblock Batch policy learning in average reward markov decision processes.
\newblock {\em arXiv preprint arXiv:2007.11771}, 2020.

\bibitem[LSAB20]{liu2020provably}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Provably good batch reinforcement learning without great exploration.
\newblock {\em arXiv preprint arXiv:2007.08202}, 2020.

\bibitem[LTDC19]{laroche2019safe}
Romain Laroche, Paul Trichelair, and Remi~Tachet Des~Combes.
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In {\em International Conference on Machine Learning}, pages
  3652--3661. PMLR, 2019.

\bibitem[MCK{\etalchar{+}}21]{modi2021model}
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal.
\newblock Model-free representation learning and exploration in low-rank mdps.
\newblock {\em arXiv preprint arXiv:2102.07035}, 2021.

\bibitem[MLL{\etalchar{+}}14]{mandel2014offline}
Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic.
\newblock Offline policy evaluation across representations with applications to
  educational games.
\newblock In {\em AAMAS}, volume 1077, 2014.

\bibitem[Mun03]{munos2003error}
R{\'e}mi Munos.
\newblock Error bounds for approximate policy iteration.
\newblock In {\em ICML}, volume~3, pages 560--567, 2003.

\bibitem[Mun05]{munos2005error}
R{\'e}mi Munos.
\newblock Error bounds for approximate value iteration.
\newblock In {\em AAAI Conference on Artificial Intelligence (AAAI)}, 2005.

\bibitem[NB03]{nedic2003least}
A~Nedi{\'c} and Dimitri~P Bertsekas.
\newblock Least squares policy evaluation algorithms with linear function
  approximation.
\newblock {\em Discrete Event Dynamic Systems}, 13(1):79--110, 2003.

\bibitem[NCDL19]{nachum2019dualdice}
Ofir Nachum, Yinlam Chow, Bo~Dai, and Lihong Li.
\newblock Dualdice: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock {\em arXiv preprint arXiv:1906.04733}, 2019.

\bibitem[ND20]{nachum2020reinforcement}
Ofir Nachum and Bo~Dai.
\newblock Reinforcement learning via fenchel-rockafellar duality.
\newblock {\em arXiv preprint arXiv:2001.01866}, 2020.

\bibitem[NDGL20]{nair2020accelerating}
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock {\em arXiv preprint arXiv:2006.09359}, 2020.

\bibitem[NDK{\etalchar{+}}19]{nachum2019algaedice}
Ofir Nachum, Bo~Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale
  Schuurmans.
\newblock Algaedice: Policy gradient from arbitrary experience.
\newblock {\em arXiv preprint arXiv:1912.02074}, 2019.

\bibitem[Put94]{puterman1994markov}
Martin~L. Puterman.
\newblock {\em Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc., New York, NY, USA, 1994.

\bibitem[RM15]{raskutti2015information}
Garvesh Raskutti and Sayan Mukherjee.
\newblock The information geometry of mirror descent.
\newblock {\em IEEE Transactions on Information Theory}, 61(3):1451--1457,
  2015.

\bibitem[RZM{\etalchar{+}}21]{rashidinejad2021bridging}
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell.
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock {\em arXiv preprint arXiv:2103.12021}, 2021.

\bibitem[SB78]{shreve1978alternative}
Steven~E Shreve and Dimitri~P Bertsekas.
\newblock Alternative theoretical frameworks for finite horizon discrete-time
  stochastic optimal control.
\newblock {\em SIAM Journal on control and optimization}, 16(6):953--978, 1978.

\bibitem[SB18]{SutBar18}
R.~S. Sutton and A.~G. Barto.
\newblock {\em {R}einforcement {L}earning: {A}n {I}ntroduction}.
\newblock {M}{I}{T} {P}ress, Cambridge, {M}{A}, 2nd edition, 2018.

\bibitem[SEM20]{shani2020adaptive}
Lior Shani, Yonathan Efroni, and Shie Mannor.
\newblock Adaptive trust region policy optimization: Global convergence and
  faster rates for regularized mdps.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 5668--5675, 2020.

\bibitem[SMS{\etalchar{+}}99]{sutton1999policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, Yishay Mansour, et~al.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em NIPs}, volume~99, pages 1057--1063. Citeseer, 1999.

\bibitem[SSB{\etalchar{+}}20]{siegel2020keep}
Noah~Y Siegel, Jost~Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki,
  Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin
  Riedmiller.
\newblock Keep doing what worked: Behavioral modelling priors for offline
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2002.08396}, 2020.

\bibitem[TB16]{thomas2016data}
Philip Thomas and Emma Brunskill.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2139--2148, 2016.

\bibitem[TdSB{\etalchar{+}}19]{thomas2019preventing}
Philip~S Thomas, Bruno~Castro da~Silva, Andrew~G Barto, Stephen Giguere, Yuriy
  Brun, and Emma Brunskill.
\newblock Preventing undesirable behavior of intelligent machines.
\newblock {\em Science}, 366(6468):999--1004, 2019.

\bibitem[TFL{\etalchar{+}}19]{tang2019doubly}
Ziyang Tang, Yihao Feng, Lihong Li, Dengyong Zhou, and Qiang Liu.
\newblock Doubly robust bias reduction in infinite horizon off-policy
  estimation.
\newblock {\em arXiv preprint arXiv:1910.07186}, 2019.

\bibitem[Tsy09]{Tsybakov09}
A.~B. Tsybakov.
\newblock {\em Introduction to Nonparametric Estimation}.
\newblock Springer, New York, 2009.

\bibitem[TTG15]{thomas2015high}
Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh.
\newblock High confidence policy improvement.
\newblock In {\em International Conference on Machine Learning}, pages
  2380--2388, 2015.

\bibitem[UHJ20]{uehara2020minimax}
Masatoshi Uehara, Jiawei Huang, and Nan Jiang.
\newblock Minimax weight and q-function learning for off-policy evaluation.
\newblock In {\em International Conference on Machine Learning}, pages
  9659--9668. PMLR, 2020.

\bibitem[US21]{uehara2021pessimistic}
Masatoshi Uehara and Wen Sun.
\newblock Pessimistic model-based offline rl: Pac bounds and posterior sampling
  under partial coverage, 2021.

\bibitem[VJY21]{voloshin2021minimax}
Cameron Voloshin, Nan Jiang, and Yisong Yue.
\newblock Minimax model learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1612--1620. PMLR, 2021.

\bibitem[Wai19]{wainwright2019high}
Martin~J Wainwright.
\newblock {\em High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[WAS20]{weisz2020exponential}
Gellert Weisz, Philip Amortila, and Csaba Szepesv{\'a}ri.
\newblock Exponential lower bounds for planning in mdps with
  linearly-realizable optimal action-value functions.
\newblock {\em arXiv preprint arXiv:2010.01374}, 2020.

\bibitem[WCYW19]{wang2019neural}
Lingxiao Wang, Qi~Cai, Zhuoran Yang, and Zhaoran Wang.
\newblock Neural policy gradient methods: Global optimality and rates of
  convergence.
\newblock {\em arXiv preprint arXiv:1909.01150}, 2019.

\bibitem[WN{\.Z}{\etalchar{+}}20]{wang2020critic}
Ziyu Wang, Alexander Novikov, Konrad {\.Z}o{\l}na, Jost~Tobias Springenberg,
  Scott Reed, Bobak Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre,
  Nicolas Heess, et~al.
\newblock Critic regularized regression.
\newblock {\em arXiv preprint arXiv:2006.15134}, 2020.

\bibitem[WTN19]{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[WZS{\etalchar{+}}21]{wu2021uncertainty}
Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan
  Salakhutdinov, and Hanlin Goh.
\newblock Uncertainty weighted actor-critic for offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2105.08140}, 2021.

\bibitem[XCJ{\etalchar{+}}21]{xie2021bellmanconsistent}
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal.
\newblock Bellman-consistent pessimism for offline reinforcement learning,
  2021.

\bibitem[XJ20a]{xie2020batch}
Tengyang Xie and Nan Jiang.
\newblock Batch value-function approximation with only realizability.
\newblock {\em arXiv preprint arXiv:2008.04990}, 2020.

\bibitem[XJ20b]{xie2020Q}
Tengyang Xie and Nan Jiang.
\newblock Q* approximation schemes for batch reinforcement learning: A
  theoretical comparison.
\newblock volume 124 of {\em Proceedings of Machine Learning Research}, pages
  550--559, Virtual, 03--06 Aug 2020. PMLR.

\bibitem[XMW19]{xie2019towards}
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang.
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9668--9678, 2019.

\bibitem[YBW20]{yin2020near}
Ming Yin, Yu~Bai, and Yu-Xiang Wang.
\newblock Near optimal provable uniform convergence in off-policy evaluation
  for reinforcement learning.
\newblock {\em arXiv preprint arXiv:2007.03760}, 2020.

\bibitem[YND{\etalchar{+}}20]{yang2020off}
Mengjiao Yang, Ofir Nachum, Bo~Dai, Lihong Li, and Dale Schuurmans.
\newblock Off-policy evaluation via the regularized lagrangian.
\newblock {\em arXiv preprint arXiv:2007.03438}, 2020.

\bibitem[YTY{\etalchar{+}}20]{yu2020mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine,
  Chelsea Finn, and Tengyu Ma.
\newblock Mopo: Model-based offline policy optimization.
\newblock {\em arXiv preprint arXiv:2005.13239}, 2020.

\bibitem[YW20a]{yang2020reinforcement}
Lin~F Yang and Mengdi Wang.
\newblock Reinforcement leaning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2020.

\bibitem[YW20b]{yin2020asymptotically}
Ming Yin and Yu-Xiang Wang.
\newblock Asymptotically efficient off-policy evaluation for tabular
  reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3948--3958. PMLR, 2020.

\bibitem[Zan20]{zanette2020exponential}
Andrea Zanette.
\newblock Exponential lower bounds for batch reinforcement learning: Batch rl
  can be exponentially harder than online rl.
\newblock {\em arXiv preprint arXiv:2012.08005}, 2020.

\bibitem[ZBPL20]{zanette2020frequentist}
Andrea Zanette, David Brandfonbrener, Matteo Pirotta, and Alessandro Lazaric.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In {\em AISTATS}, 2020.

\bibitem[ZDLS20]{zhang2020gendice}
Ruiyi Zhang, Bo~Dai, Lihong Li, and Dale Schuurmans.
\newblock Gendice: Generalized offline estimation of stationary values.
\newblock {\em arXiv preprint arXiv:2002.09072}, 2020.

\bibitem[ZKB{\etalchar{+}}20]{zhang2020variational}
Junyu Zhang, Alec Koppel, Amrit~Singh Bedi, Csaba Szepesvari, and Mengdi Wang.
\newblock Variational policy gradient method for reinforcement learning with
  general utilities.
\newblock {\em arXiv preprint arXiv:2007.02151}, 2020.

\bibitem[ZLKB20a]{zanette2020learning}
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2020.

\bibitem[ZLKB20b]{zanette2020provably}
Andrea Zanette, Alessandro Lazaric, Mykel~J Kochenderfer, and Emma Brunskill.
\newblock Provably efficient reward-agnostic navigation with linear value
  iteration.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\end{thebibliography}
