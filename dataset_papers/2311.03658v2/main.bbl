\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alain \& Bengio(2017)Alain and Bengio]{alain2017understanding}
Alain, G. and Bengio, Y.
\newblock Understanding intermediate layers using linear classifier probes.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=ryF7rTqgl}.

\bibitem[Arora et~al.(2016)Arora, Li, Liang, Ma, and Risteski]{arora2016latent}
Arora, S., Li, Y., Liang, Y., Ma, T., and Risteski, A.
\newblock A latent variable model approach to {PMI}-based word embeddings.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  4:\penalty0 385--399, 2016.

\bibitem[Belinkov(2022)]{belinkov2022probing}
Belinkov, Y.
\newblock Probing classifiers: Promises, shortcomings, and advances.
\newblock \emph{Computational Linguistics}, 48\penalty0 (1):\penalty0 207--219,
  2022.

\bibitem[Bowman et~al.(2016)Bowman, Vilnis, Vinyals, Dai, Jozefowicz, and
  Bengio]{bowman2016generating}
Bowman, S.~R., Vilnis, L., Vinyals, O., Dai, A., Jozefowicz, R., and Bengio, S.
\newblock Generating sentences from a continuous space.
\newblock In \emph{Proceedings of the 20th {SIGNLL} Conference on Computational
  Natural Language Learning}, pp.\  10--21, Berlin, Germany, August 2016.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/K16-1002}.
\newblock URL \url{https://aclanthology.org/K16-1002}.

\bibitem[Chang et~al.(2022)Chang, Tu, and Bergen]{chang2022geometry}
Chang, T., Tu, Z., and Bergen, B.
\newblock The geometry of multilingual language model representations.
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  119--136, 2022.

\bibitem[Chen et~al.(2021)Chen, Fu, Xu, Xie, Tan, Chen, and
  Jing]{chen2021probing}
Chen, B., Fu, Y., Xu, G., Xie, P., Tan, C., Chen, M., and Jing, L.
\newblock Probing {BERT} in hyperbolic spaces.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Chiang et~al.(2020)Chiang, Camacho-Collados, and
  Pardos]{chiang2020understanding}
Chiang, H.-Y., Camacho-Collados, J., and Pardos, Z.
\newblock Understanding the source of semantic regularities in word embeddings.
\newblock In \emph{Proceedings of the 24th Conference on Computational Natural
  Language Learning}, pp.\  119--131, 2020.

\bibitem[Choe et~al.(2020)Choe, Park, and Kim]{choe2020word2word}
Choe, Y.~J., Park, K., and Kim, D.
\newblock word2word: A collection of bilingual lexicons for 3,564 language
  pairs.
\newblock In \emph{Proceedings of the Twelfth Language Resources and Evaluation
  Conference}, pp.\  3036--3045, 2020.

\bibitem[Drozd et~al.(2016)Drozd, Gladkova, and Matsuoka]{drozd2016word}
Drozd, A., Gladkova, A., and Matsuoka, S.
\newblock Word embeddings, analogies, and machine learning: Beyond {king - man
  + woman = queen}.
\newblock In \emph{Proceedings of COLING 2016, the 26th International
  Conference on Computational Linguistics: Technical papers}, pp.\  3519--3530,
  2016.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann,
  Askell, Bai, Chen, Conerly, et~al.]{elhage2021mathematical}
Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell,
  A., Bai, Y., Chen, A., Conerly, T., et~al.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 1, 2021.

\bibitem[Elhage et~al.(2022)Elhage, Hume, Olsson, Schiefer, Henighan, Kravec,
  Hatfield-Dodds, Lasenby, Drain, Chen, et~al.]{elhage2022toy}
Elhage, N., Hume, T., Olsson, C., Schiefer, N., Henighan, T., Kravec, S.,
  Hatfield-Dodds, Z., Lasenby, R., Drain, D., Chen, C., et~al.
\newblock Toy models of superposition.
\newblock \emph{arXiv preprint arXiv:2209.10652}, 2022.

\bibitem[Ethayarajh(2019)]{ethayarajh2019contextual}
Ethayarajh, K.
\newblock How contextual are contextualized word representations? {Comparing}
  the geometry of {BERT}, {ELMo}, and {GPT-2} embeddings.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pp.\  55--65, 2019.

\bibitem[Fournier et~al.(2020)Fournier, Dupoux, and
  Dunbar]{fournier2020analogies}
Fournier, L., Dupoux, E., and Dunbar, E.
\newblock Analogies minus analogy test: measuring regularities in word
  embeddings.
\newblock In \emph{Proceedings of the 24th Conference on Computational Natural
  Language Learning}, pp.\  365--375, Online, 2020. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2020.conll-1.29}.
\newblock URL \url{https://aclanthology.org/2020.conll-1.29}.

\bibitem[Geva et~al.(2022)Geva, Caciularu, Wang, and
  Goldberg]{geva2022transformer}
Geva, M., Caciularu, A., Wang, K., and Goldberg, Y.
\newblock Transformer feed-forward layers build predictions by promoting
  concepts in the vocabulary space.
\newblock In \emph{Proceedings of the Conference on Empirical Methods in
  Natural Language Processing}, pp.\  30--45, 2022.

\bibitem[Gladkova et~al.(2016)Gladkova, Drozd, and
  Matsuoka]{gladkova2016analogy}
Gladkova, A., Drozd, A., and Matsuoka, S.
\newblock Analogy-based detection of morphological and semantic relations with
  word embeddings: what works and what doesnâ€™t.
\newblock In \emph{Proceedings of the NAACL Student Research Workshop}, pp.\
  8--15, 2016.

\bibitem[Goldberg \& Levy(2014)Goldberg and Levy]{goldberg2014word2vec}
Goldberg, Y. and Levy, O.
\newblock word2vec explained: deriving {Mikolov} et al.'s negative-sampling
  word-embedding method.
\newblock \emph{arXiv preprint arXiv:1402.3722}, 2014.

\bibitem[Gurnee \& Tegmark(2023)Gurnee and Tegmark]{LMSpaceTime:2023}
Gurnee, W. and Tegmark, M.
\newblock Language models represent space and time.
\newblock \emph{arXiv preprint arXiv:2310.02207}, art. arXiv:2310.02207,
  October 2023.
\newblock \doi{10.48550/arXiv.2310.02207}.

\bibitem[Hendel et~al.(2023)Hendel, Geva, and Globerson]{hendel2023incontext}
Hendel, R., Geva, M., and Globerson, A.
\newblock In-context learning creates task vectors.
\newblock \emph{arXiv preprint arXiv:2310.15916}, 2023.

\bibitem[Hernandez et~al.(2023)Hernandez, Sharma, Haklay, Meng, Wattenberg,
  Andreas, Belinkov, and Bau]{hernandez2023linearity}
Hernandez, E., Sharma, A.~S., Haklay, T., Meng, K., Wattenberg, M., Andreas,
  J., Belinkov, Y., and Bau, D.
\newblock Linearity of relation decoding in transformer language models.
\newblock \emph{arXiv preprint arXiv:2308.09124}, 2023.

\bibitem[Hewitt \& Manning(2019)Hewitt and Manning]{hewitt2019structural}
Hewitt, J. and Manning, C.~D.
\newblock A structural probe for finding syntax in word representations.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4129--4138, 2019.

\bibitem[Higgins et~al.(2016)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{higgins2016beta}
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M.,
  Mohamed, S., and Lerchner, A.
\newblock beta-{VAE}: Learning basic visual concepts with a constrained
  variational framework.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Higgins et~al.(2018)Higgins, Amos, Pfau, Racaniere, Matthey, Rezende,
  and Lerchner]{higgins2018towards}
Higgins, I., Amos, D., Pfau, D., Racaniere, S., Matthey, L., Rezende, D., and
  Lerchner, A.
\newblock Towards a definition of disentangled representations.
\newblock \emph{arXiv preprint arXiv:1812.02230}, 2018.

\bibitem[Hyvarinen \& Morioka(2016)Hyvarinen and
  Morioka]{hyvarinen2016unsupervised}
Hyvarinen, A. and Morioka, H.
\newblock Unsupervised feature extraction by time-contrastive learning and
  nonlinear {ICA}.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Jiang et~al.(2023)Jiang, Aragam, and Veitch]{jiang2023uncovering}
Jiang, Y., Aragam, B., and Veitch, V.
\newblock Uncovering meanings of embeddings via partial orthogonality.
\newblock \emph{arXiv preprint arXiv:2310.17611}, 2023.

\bibitem[Khemakhem et~al.(2020)Khemakhem, Kingma, Monti, and
  Hyvarinen]{khemakhem2020variational}
Khemakhem, I., Kingma, D., Monti, R., and Hyvarinen, A.
\newblock Variational autoencoders and nonlinear {ICA}: A unifying framework.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2207--2217. PMLR, 2020.

\bibitem[Kim et~al.(2018)Kim, Wattenberg, Gilmer, Cai, Wexler, Viegas,
  et~al.]{kim2018interpretability}
Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., et~al.
\newblock Interpretability beyond feature attribution: Quantitative testing
  with concept activation vectors ({TCAV}).
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2668--2677. PMLR, 2018.

\bibitem[Kudo \& Richardson(2018)Kudo and Richardson]{kudo2018sentencepiece}
Kudo, T. and Richardson, J.
\newblock {SentencePiece}: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pp.\  66--71, 2018.

\bibitem[Lample et~al.(2018)Lample, Conneau, Ranzato, Denoyer, and
  J{\'e}gou]{lample2018word}
Lample, G., Conneau, A., Ranzato, M., Denoyer, L., and J{\'e}gou, H.
\newblock Word translation without parallel data.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Levy \& Goldberg(2014)Levy and Goldberg]{levy2014linguistic}
Levy, O. and Goldberg, Y.
\newblock Linguistic regularities in sparse and explicit word representations.
\newblock In \emph{Proceedings of the Eighteenth Conference on Computational
  Natural Language Learning}, pp.\  171--180, 2014.

\bibitem[Li et~al.(2020)Li, Zhou, He, Wang, Yang, and Li]{li2020sentence}
Li, B., Zhou, H., He, J., Wang, M., Yang, Y., and Li, L.
\newblock On the sentence embeddings from pre-trained language models.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  9119--9130, 2020.

\bibitem[Li et~al.(2022)Li, Hopkins, Bau, Vi{\'e}gas, Pfister, and
  Wattenberg]{li2022emergent}
Li, K., Hopkins, A.~K., Bau, D., Vi{\'e}gas, F., Pfister, H., and Wattenberg,
  M.
\newblock Emergent world representations: Exploring a sequence model trained on
  a synthetic task.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Meng et~al.(2022)Meng, Bau, Andonian, and Belinkov]{meng2022locating}
Meng, K., Bau, D., Andonian, A., and Belinkov, Y.
\newblock Locating and editing factual associations in {GPT}.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 17359--17372, 2022.

\bibitem[Merullo et~al.(2023)Merullo, Eickhoff, and
  Pavlick]{merullo2023language}
Merullo, J., Eickhoff, C., and Pavlick, E.
\newblock Language models implement simple word2vec-style vector arithmetic.
\newblock \emph{arXiv preprint arXiv:2305.16130}, 2023.

\bibitem[Mesnard et~al.(2024)Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak,
  Sifre, Rivi{\`e}re, Kale, Love, et~al.]{team2024gemma}
Mesnard, T., Hardin, C., Dadashi, R., Bhupatiraju, S., Pathak, S., Sifre, L.,
  Rivi{\`e}re, M., Kale, M.~S., Love, J., et~al.
\newblock Gemma: Open models based on gemini research and technology.
\newblock \emph{arXiv preprint arXiv:2403.08295}, 2024.

\bibitem[Mikolov et~al.(2013{\natexlab{a}})Mikolov, Le, and
  Sutskever]{mikolov2013exploiting}
Mikolov, T., Le, Q.~V., and Sutskever, I.
\newblock Exploiting similarities among languages for machine translation.
\newblock \emph{arXiv preprint arXiv:1309.4168}, 2013{\natexlab{a}}.

\bibitem[Mikolov et~al.(2013{\natexlab{b}})Mikolov, Sutskever, Chen, Corrado,
  and Dean]{mikolov2013distributed}
Mikolov, T., Sutskever, I., Chen, K., Corrado, G.~S., and Dean, J.
\newblock Distributed representations of words and phrases and their
  compositionality.
\newblock \emph{Advances in Neural Information Processing Systems}, 26,
  2013{\natexlab{b}}.

\bibitem[Mikolov et~al.(2013{\natexlab{c}})Mikolov, Yih, and
  Zweig]{mikolov2013linguistic}
Mikolov, T., Yih, W.-T., and Zweig, G.
\newblock Linguistic regularities in continuous space word representations.
\newblock In \emph{Proceedings of the 2013 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  746--751, 2013{\natexlab{c}}.

\bibitem[Mimno \& Thompson(2017)Mimno and Thompson]{mimno2017strange}
Mimno, D. and Thompson, L.
\newblock The strange geometry of skip-gram with negative sampling.
\newblock In Palmer, M., Hwa, R., and Riedel, S. (eds.), \emph{Proceedings of
  the 2017 Conference on Empirical Methods in Natural Language Processing},
  pp.\  2873--2878, Copenhagen, Denmark, 2017. Association for Computational
  Linguistics.
\newblock \doi{10.18653/v1/D17-1308}.
\newblock URL \url{https://aclanthology.org/D17-1308}.

\bibitem[{Moran} et~al.(2021){Moran}, {Sridhar}, {Wang}, and
  {Blei}]{MoranDeepGenIdent:2021}
{Moran}, G.~E., {Sridhar}, D., {Wang}, Y., and {Blei}, D.~M.
\newblock Identifiable deep generative models via sparse decoding.
\newblock \emph{arXiv preprint arXiv:2110.10804}, art. arXiv:2110.10804,
  October 2021.
\newblock \doi{10.48550/arXiv.2110.10804}.

\bibitem[Nanda et~al.(2023)Nanda, Lee, and Wattenberg]{nanda2023emergent}
Nanda, N., Lee, A., and Wattenberg, M.
\newblock Emergent linear representations in world models of self-supervised
  sequence models.
\newblock \emph{arXiv preprint arXiv:2309.00941}, 2023.

\bibitem[nostalgebraist(2020)]{nostalgebraist2020logitlens}
nostalgebraist.
\newblock {Interpreting {GPT}: the logit lens}, 2020.
\newblock URL
  \url{https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Peng et~al.(2022)Peng, Stevenson, Lin, and Li]{peng2022understanding}
Peng, X., Stevenson, M., Lin, C., and Li, C.
\newblock Understanding linearity of cross-lingual word embedding mappings.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=8HuyXvbvqX}.

\bibitem[Pennington et~al.(2014)Pennington, Socher, and
  Manning]{pennington2014glove}
Pennington, J., Socher, R., and Manning, C.~D.
\newblock {GloVe: Global vectors for word representation}.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  1532--1543, 2014.

\bibitem[Perera et~al.(2023)Perera, Trager, Zancato, Achille, and
  Soatto]{perera2023prompt}
Perera, P., Trager, M., Zancato, L., Achille, A., and Soatto, S.
\newblock Prompt algebra for task composition.
\newblock \emph{arXiv preprint arXiv:2306.00310}, 2023.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Reif et~al.(2019)Reif, Yuan, Wattenberg, Viegas, Coenen, Pearce, and
  Kim]{reif2019visualizing}
Reif, E., Yuan, A., Wattenberg, M., Viegas, F.~B., Coenen, A., Pearce, A., and
  Kim, B.
\newblock Visualizing and measuring the geometry of {BERT}.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Rogers et~al.(2021)Rogers, Kovaleva, and Rumshisky]{rogers2021primer}
Rogers, A., Kovaleva, O., and Rumshisky, A.
\newblock {A primer in BERTology: What we know about how BERT works}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  8:\penalty0 842--866, 2021.

\bibitem[Ruder et~al.(2019)Ruder, Vuli{\'c}, and S{\o}gaard]{ruder2019survey}
Ruder, S., Vuli{\'c}, I., and S{\o}gaard, A.
\newblock A survey of cross-lingual word embedding models.
\newblock \emph{Journal of Artificial Intelligence Research}, 65:\penalty0
  569--631, 2019.

\bibitem[Sch{\"o}lkopf et~al.(2021)Sch{\"o}lkopf, Locatello, Bauer, Ke,
  Kalchbrenner, Goyal, and Bengio]{scholkopf2021toward}
Sch{\"o}lkopf, B., Locatello, F., Bauer, S., Ke, N.~R., Kalchbrenner, N.,
  Goyal, A., and Bengio, Y.
\newblock Toward causal representation learning.
\newblock \emph{Proceedings of the IEEE}, 109\penalty0 (5):\penalty0 612--634,
  2021.

\bibitem[Todd et~al.(2023)Todd, Li, Sharma, Mueller, Wallace, and
  Bau]{todd2023function}
Todd, E., Li, M.~L., Sharma, A.~S., Mueller, A., Wallace, B.~C., and Bau, D.
\newblock Function vectors in large language models.
\newblock \emph{arXiv preprint arXiv:2310.15213}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei,
  Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Ferrer, Chen, Cucurull,
  Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini,
  Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril,
  Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton,
  Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang,
  Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang,
  Rodriguez, Stojnic, Edunov, and Scialom]{touvron2023llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
  Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L.,
  Ferrer, C.~C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu,
  W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S.,
  Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
  A., Koura, P.~S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
  Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y.,
  Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva,
  R., Smith, E.~M., Subramanian, R., Tan, X.~E., Tang, B., Taylor, R.,
  Williams, A., Kuan, J.~X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A.,
  Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and
  Scialom, T.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Trager et~al.(2023)Trager, Perera, Zancato, Achille, Bhatia, and
  Soatto]{trager2023linear}
Trager, M., Perera, P., Zancato, L., Achille, A., Bhatia, P., and Soatto, S.
\newblock Linear spaces of meanings: Compositional structures in
  vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  15395--15404, 2023.

\bibitem[Turner et~al.(2023)Turner, Thiergart, Udell, Leech, Mini, and
  MacDiarmid]{ActivationAddition:2023}
Turner, A.~M., Thiergart, L., Udell, D., Leech, G., Mini, U., and MacDiarmid,
  M.
\newblock Activation addition: Steering language models without optimization.
\newblock \emph{arXiv preprint arXiv:2308.10248}, art. arXiv:2308.10248, August
  2023.
\newblock \doi{10.48550/arXiv.2308.10248}.

\bibitem[Ushio et~al.(2021)Ushio, Anke, Schockaert, and
  Camacho-Collados]{ushio2021bert}
Ushio, A., Anke, L.~E., Schockaert, S., and Camacho-Collados, J.
\newblock {BERT is to NLP what AlexNet is to CV}: {Can} pre-trained language
  models identify analogies?
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pp.\  3609--3624, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Vylomova et~al.(2016)Vylomova, Rimell, Cohn, and
  Baldwin]{vylomova2016take}
Vylomova, E., Rimell, L., Cohn, T., and Baldwin, T.
\newblock Take and took, gaggle and goose, book and read: Evaluating the
  utility of vector differences for lexical relation learning.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1671--1682,
  2016.

\bibitem[Wang et~al.(2023)Wang, Gui, Negrea, and Veitch]{wang2023concept}
Wang, Z., Gui, L., Negrea, J., and Veitch, V.
\newblock Concept algebra for score-based conditional models.
\newblock \emph{arXiv preprint arXiv:2302.03693}, 2023.

\bibitem[Zhu \& de~Melo(2020)Zhu and de~Melo]{zhu2020sentence}
Zhu, X. and de~Melo, G.
\newblock Sentence analogies: Linguistic regularities in sentence embeddings.
\newblock In \emph{Proceedings of the 28th International Conference on
  Computational Linguistics}, pp.\  3389--3400, 2020.

\bibitem[Zimmermann et~al.(2021)Zimmermann, Sharma, Schneider, Bethge, and
  Brendel]{zimmermann2021contrastive}
Zimmermann, R.~S., Sharma, Y., Schneider, S., Bethge, M., and Brendel, W.
\newblock Contrastive learning inverts the data generating process.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12979--12990. PMLR, 2021.

\bibitem[Zou et~al.(2023)Zou, Phan, Chen, Campbell, Guo, Ren, Pan, Yin,
  Mazeika, Dombrowski, Goel, Li, Byun, Wang, Mallen, Basart, Koyejo, Song,
  Fredrikson, Kolter, and Hendrycks]{zou2023representation}
Zou, A., Phan, L., Chen, S., Campbell, J., Guo, P., Ren, R., Pan, A., Yin, X.,
  Mazeika, M., Dombrowski, A.-K., Goel, S., Li, N., Byun, M.~J., Wang, Z.,
  Mallen, A., Basart, S., Koyejo, S., Song, D., Fredrikson, M., Kolter, Z., and
  Hendrycks, D.
\newblock Representation engineering: A top-down approach to {AI} transparency.
\newblock \emph{arXiv preprint arXiv:2310.01405}, 2023.

\end{thebibliography}
