\begin{thebibliography}{100}

\bibitem{bahdanau2015neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em The International Conference on Learning Representations}, 2015.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and
  et~al.
\newblock Language models are few-shot learners.
\newblock In {\em Advances in neural information processing systems},
  volume~33, pages 1877--1901, 2020.

\bibitem{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock {\em arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, ≈Åukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems},
  volume~30, 2017.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock {\em arXiv preprint arXiv:2108.07258}, 2021.

\bibitem{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In {\em International Conference on Machine Learning}, pages
  8821--8831. PMLR, 2021.

\bibitem{dosovitskiy2021vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International conference on machine learning}, pages
  8748--8763. PMLR, 2021.

\bibitem{driess2023palm}
Danny Driess, Fei Xia, Mehdi~SM Sajjadi, Corey Lynch, Aakanksha Chowdhery,
  Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et~al.
\newblock Palm-e: An embodied multimodal language model.
\newblock {\em arXiv preprint arXiv:2303.03378}, 2023.

\bibitem{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~34, pages 15084--15097, 2021.

\bibitem{reed2022generalist}
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio~Gomez Colmenarejo, Alexander
  Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay,
  Jost~Tobias Springenberg, et~al.
\newblock A generalist agent.
\newblock {\em arXiv preprint arXiv:2205.06175}, 2022.

\bibitem{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 3045--3059, 2021.

\bibitem{oymak2023role}
Samet Oymak, Ankit~Singh Rawat, Mahdi Soltanolkotabi, and Christos
  Thrampoulidis.
\newblock On the role of attention in prompt-tuning.
\newblock In {\em International Conference on Machine Learning}, 2023.

\bibitem{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock {\em arXiv preprint arXiv:2101.00190}, 2021.

\bibitem{rosset2003margin}
Saharon Rosset, Ji~Zhu, and Trevor Hastie.
\newblock Margin maximizing loss functions.
\newblock {\em Advances in neural information processing systems}, 16, 2003.

\bibitem{suggala2018connecting}
Arun Suggala, Adarsh Prasad, and Pradeep~K Ravikumar.
\newblock Connecting optimization and regularization paths.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{ji2020gradient}
Ziwei Ji, Miroslav Dud{\'\i}k, Robert~E Schapire, and Matus Telgarsky.
\newblock Gradient descent follows the regularization path for general losses.
\newblock In {\em Conference on Learning Theory}, pages 2109--2136. PMLR, 2020.

\bibitem{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock {\em The Journal of Machine Learning Research}, 19(1):2822--2878,
  2018.

\bibitem{dong2021attention}
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
\newblock Attention is not all you need: Pure attention loses rank doubly
  exponentially with depth.
\newblock In {\em International Conference on Machine Learning}, pages
  2793--2803. PMLR, 2021.

\bibitem{krizhevsky2014cifar}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock The cifar-10 dataset.
\newblock {\em online: http://www. cs. toronto. edu/kriz/cifar. html}, 55(5),
  2014.

\bibitem{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In {\em International Conference on Machine Learning}, pages
  1832--1841. PMLR, 2018.

\bibitem{nacson2019convergence}
Mor~Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique~Pamplona
  Savarese, Nathan Srebro, and Daniel Soudry.
\newblock Convergence of gradient descent on separable data.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 3420--3428. PMLR, 2019.

\bibitem{ji2021characterizing}
Ziwei Ji and Matus Telgarsky.
\newblock Characterizing the implicit bias via a primal-dual analysis.
\newblock In {\em Algorithmic Learning Theory}, pages 772--804. PMLR, 2021.

\bibitem{moroshko2020implicit}
Edward Moroshko, Blake~E Woodworth, Suriya Gunasekar, Jason~D Lee, Nati Srebro,
  and Daniel Soudry.
\newblock Implicit bias in deep linear classification: Initialization scale vs
  training accuracy.
\newblock {\em Advances in neural information processing systems},
  33:22182--22193, 2020.

\bibitem{ji2020directional}
Ziwei Ji and Matus Telgarsky.
\newblock Directional convergence and alignment in deep learning.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 17176--17186. Curran Associates, Inc., 2020.

\bibitem{ji2018risk}
Ziwei Ji and Matus Telgarsky.
\newblock Risk and parameter convergence of logistic regression.
\newblock {\em arXiv preprint arXiv:1803.07300}, 2018.

\bibitem{ji2019implicit}
Ziwei Ji and Matus Telgarsky.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In {\em Conference on Learning Theory}, pages 1772--1798. PMLR, 2019.

\bibitem{woodworth2020kernel}
Blake Woodworth, Suriya Gunasekar, Jason~D Lee, Edward Moroshko, Pedro
  Savarese, Itay Golan, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In {\em Conference on Learning Theory}, pages 3635--3673. PMLR, 2020.

\bibitem{yun2020unifying}
Chulhee Yun, Shankar Krishnan, and Hossein Mobahi.
\newblock A unifying view on implicit bias in training linear neural networks.
\newblock {\em arXiv preprint arXiv:2010.02501}, 2020.

\bibitem{vaskevicius2019implicit}
Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini.
\newblock Implicit regularization for optimal sparse recovery.
\newblock {\em Advances in Neural Information Processing Systems},
  32:2972--2983, 2019.

\bibitem{amid2020winnowing}
Ehsan Amid and Manfred~K Warmuth.
\newblock Winnowing with gradient descent.
\newblock In {\em Conference on Learning Theory}, pages 163--182. PMLR, 2020.

\bibitem{amid2020reparameterizing}
Ehsan Amid and Manfred~KK Warmuth.
\newblock Reparameterizing mirror descent as gradient descent.
\newblock {\em Advances in Neural Information Processing Systems},
  33:8430--8439, 2020.

\bibitem{li2019towards}
Yuanzhi Li, Colin Wei, and Tengyu Ma.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock {\em arXiv preprint arXiv:1907.04595}, 2019.

\bibitem{blanc2020implicit}
Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant.
\newblock Implicit regularization for deep neural networks driven by an
  ornstein-uhlenbeck like process.
\newblock In {\em Conference on learning theory}, pages 483--513. PMLR, 2020.

\bibitem{haochen2020shape}
Jeff~Z HaoChen, Colin Wei, Jason~D Lee, and Tengyu Ma.
\newblock Shape matters: Understanding the implicit bias of the noise
  covariance.
\newblock {\em arXiv preprint arXiv:2006.08680}, 2020.

\bibitem{li2022what}
Zhiyuan Li, Tianhao Wang, and Sanjeev Arora.
\newblock What happens after {SGD} reaches zero loss? --a mathematical
  framework.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{damian2021label}
Alex Damian, Tengyu Ma, and Jason Lee.
\newblock Label noise sgd provably prefers flat global minimizers.
\newblock {\em arXiv preprint arXiv:2106.06530}, 2021.

\bibitem{zou2021benefits}
Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu, Dean~P Foster, and
  Sham Kakade.
\newblock The benefits of implicit regularization from sgd in least squares
  problems.
\newblock {\em Advances in Neural Information Processing Systems},
  34:5456--5468, 2021.

\bibitem{qian2019implicit}
Qian Qian and Xiaoyuan Qian.
\newblock The implicit bias of adagrad on separable data.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{wang2021momentum}
Bohan Wang, Qi~Meng, Huishuai Zhang, Ruoyu Sun, Wei Chen, and Zhi-Ming Ma.
\newblock Momentum doesn't change the implicit bias.
\newblock {\em arXiv preprint arXiv:2110.03891}, 2021.

\bibitem{wang2021implicit}
Bohan Wang, Qi~Meng, Wei Chen, and Tie-Yan Liu.
\newblock The implicit bias for adaptive optimization algorithms on homogeneous
  neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  10849--10858. PMLR, 2021.

\bibitem{ji2021fast}
Ziwei Ji, Nathan Srebro, and Matus Telgarsky.
\newblock Fast margin maximization via dual acceleration.
\newblock In {\em International Conference on Machine Learning}, pages
  4860--4869. PMLR, 2021.

\bibitem{cheng2016long}
Jianpeng Cheng, Li~Dong, and Mirella Lapata.
\newblock Long short-term memory-networks for machine reading.
\newblock In {\em Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pages 551--561, Austin, Texas, November 2016.
  Association for Computational Linguistics.

\bibitem{parikh2016decomposable}
Ankur Parikh, Oscar T{\"a}ckstr{\"o}m, Dipanjan Das, and Jakob Uszkoreit.
\newblock A decomposable attention model for natural language inference.
\newblock In {\em Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pages 2249--2255, Austin, Texas, November 2016.
  Association for Computational Linguistics.

\bibitem{paulus2017deep}
Romain Paulus, Caiming Xiong, and Richard Socher.
\newblock A deep reinforced model for abstractive summarization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{lin2017structured}
Zhouhan Lin, Minwei Feng, Cicero Nogueira~dos Santos, Mo~Yu, Bing Xiang, Bowen
  Zhou, and Yoshua Bengio.
\newblock A structured self-attentive sentence embedding.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{edelman2022inductive}
Benjamin~L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In {\em International Conference on Machine Learning}, pages
  5793--5831. PMLR, 2022.

\bibitem{sahiner2022unraveling}
Arda Sahiner, Tolga Ergen, Batu Ozturkler, John Pauly, Morteza Mardani, and
  Mert Pilanci.
\newblock Unraveling attention via convex duality: Analysis and interpretations
  of vision transformers.
\newblock In {\em International Conference on Machine Learning}, pages
  19050--19088. PMLR, 2022.

\bibitem{ergen2022convexifying}
Tolga Ergen, Behnam Neyshabur, and Harsh Mehta.
\newblock Convexifying transformers: Improving optimization and understanding
  of transformer networks.
\newblock {\em arXiv:2211.11052}, 2022.

\bibitem{baldi2022quarks}
Pierre Baldi and Roman Vershynin.
\newblock The quarks of attention.
\newblock {\em arXiv:2202.08371}, 2022.

\bibitem{jelassi2022vision}
Samy Jelassi, Michael~Eli Sander, and Yuanzhi Li.
\newblock Vision transformers provably learn spatial structure.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{li2023theoretical}
Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen.
\newblock A theoretical understanding of shallow vision transformers: Learning,
  generalization, and sample complexity.
\newblock {\em arXiv preprint arXiv:2302.06015}, 2023.

\bibitem{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1675--1685. PMLR, 2019.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock {\em arXiv preprint arXiv:1806.07572}, 2018.

\bibitem{oymak2020toward}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Toward moderate overparameterization: Global convergence guarantees
  for training shallow neural networks.
\newblock {\em IEEE Journal on Selected Areas in Information Theory},
  1(1):84--105, 2020.

\bibitem{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em International Conference on Machine Learning}, pages
  242--252. PMLR, 2019.

\bibitem{vapnik2006estimation}
Vladimir Vapnik.
\newblock {\em Estimation of dependences based on empirical data}.
\newblock Springer Science \& Business Media, 2006.

\bibitem{bartlett1996valid}
Peter Bartlett.
\newblock For valid generalization the size of the weights is more important
  than the size of the network.
\newblock {\em Advances in neural information processing systems}, 9, 1996.

\bibitem{novikoff1963convergence}
Albert~B Novikoff.
\newblock On convergence proofs for perceptrons.
\newblock Technical report, STANFORD RESEARCH INST MENLO PARK CA, 1963.

\bibitem{bartlett1998boosting}
Peter Bartlett, Yoav Freund, Wee~Sun Lee, and Robert~E Schapire.
\newblock Boosting the margin: A new explanation for the effectiveness of
  voting methods.
\newblock {\em The annals of statistics}, 26(5):1651--1686, 1998.

\bibitem{zhang2005boosting}
Tong Zhang and Bin Yu.
\newblock Boosting with early stopping: Convergence and consistency.
\newblock {\em Annals of Statistics}, page 1538, 2005.

\bibitem{telgarsky2013margins}
Matus Telgarsky.
\newblock Margins, shrinkage, and boosting.
\newblock In {\em International Conference on Machine Learning}, pages
  307--315. PMLR, 2013.

\bibitem{kini2021label}
Ganesh~Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos
  Thrampoulidis.
\newblock Label-imbalanced and group-sensitive classification under
  overparameterization.
\newblock {\em Advances in Neural Information Processing Systems},
  34:18970--18983, 2021.

\bibitem{soltanolkotabi2023implicit}
Mahdi Soltanolkotabi, Dominik St{\"o}ger, and Changzhi Xie.
\newblock Implicit balancing and regularization: Generalization and convergence
  guarantees for overparameterized asymmetric matrix sensing.
\newblock {\em arXiv:2303.14244}, 2023.

\bibitem{taheri2023generalization}
Hossein Taheri and Christos Thrampoulidis.
\newblock On generalization of decentralized learning with separable data.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4917--4945. PMLR, 2023.

\bibitem{oymak2019overparameterized}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Overparameterized nonlinear learning: Gradient descent takes the
  shortest path?
\newblock In {\em International Conference on Machine Learning}, pages
  4951--4960. PMLR, 2019.

\bibitem{ji2018gradient}
Ziwei Ji and Matus Telgarsky.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock {\em arXiv preprint arXiv:1810.02032}, 2018.

\bibitem{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{lyu2019gradient}
Kaifeng Lyu and Jian Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock {\em arXiv preprint arXiv:1906.05890}, 2019.

\bibitem{chizat2020implicit}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In {\em Conference on Learning Theory}, pages 1305--1338. PMLR, 2020.

\bibitem{frei2023benign}
Spencer Frei, Gal Vardi, Peter~L Bartlett, and Nathan Srebro.
\newblock Benign overfitting in linear classifiers and leaky relu networks from
  kkt conditions for margin maximization.
\newblock {\em arXiv e-prints}, pages arXiv--2303, 2023.

\bibitem{vardi2022margin}
Gal Vardi, Ohad Shamir, and Nati Srebro.
\newblock On margin maximization in linear and relu networks.
\newblock {\em Advances in Neural Information Processing Systems},
  35:37024--37036, 2022.

\bibitem{azizan2021stochastic}
Navid Azizan, Sahin Lale, and Babak Hassibi.
\newblock Stochastic mirror descent on overparameterized nonlinear models.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  33(12):7717--7727, 2021.

\bibitem{azizanstochastic}
Navid Azizan and Babak Hassibi.
\newblock Stochastic gradient/mirror descent: Minimax optimality and implicit
  regularization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem{zhou2018deep}
Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
  Yan, Junqi Jin, Han Li, and Kun Gai.
\newblock Deep interest network for click-through rate prediction.
\newblock In {\em Proceedings of the 24th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 1059--1068, 2018.

\bibitem{chen2019behavior}
Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou.
\newblock Behavior sequence transformer for e-commerce recommendation in
  alibaba.
\newblock In {\em Proceedings of the 1st International Workshop on Deep
  Learning Practice for High-Dimensional Sparse Data}, pages 1--4, 2019.

\bibitem{sun2019bert4rec}
Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.
\newblock Bert4rec: Sequential recommendation with bidirectional encoder
  representations from transformer.
\newblock In {\em Proceedings of the 28th ACM International Conference on
  Information and Knowledge Management}, pages 1441--1450, 2019.

\bibitem{chen2018best}
Mia~Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey,
  George Foster, Llion Jones, Mike Schuster, Noam Shazeer, Niki Parmar, Ashish
  Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Zhifeng Chen, Yonghui Wu, and
  Macduff Hughes.
\newblock The best of both worlds: Combining recent advances in neural machine
  translation.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 76--86, Melbourne,
  Australia, July 2018. Association for Computational Linguistics.

\bibitem{janner2021reinforcement}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Reinforcement learning as one big sequence modeling problem.
\newblock In {\em ICML 2021 Workshop on Unsupervised Reinforcement Learning},
  2021.

\bibitem{zheng2022online}
Qinqing Zheng, Amy Zhang, and Aditya Grover.
\newblock Online decision transformer.
\newblock In {\em Proceedings of the 39th International Conference on Machine
  Learning}, 2022.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In {\em International Conference on Machine Learning}, pages
  10347--10357. PMLR, 2021.

\bibitem{jiang2021all}
Zi-Hang Jiang, Qibin Hou, Li~Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran
  Wang, and Jiashi Feng.
\newblock All tokens matter: Token labeling for training better vision
  transformers.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~34, pages 18590--18602, 2021.

\bibitem{kim2021lipschitz}
Hyunjik Kim, George Papamakarios, and Andriy Mnih.
\newblock The lipschitz constant of self-attention.
\newblock In {\em International Conference on Machine Learning}, pages
  5562--5571. PMLR, 2021.

\bibitem{hron2020infinite}
Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak.
\newblock Infinite attention: Nngp and ntk for deep attention networks.
\newblock In {\em International Conference on Machine Learning}, pages
  4376--4386. PMLR, 2020.

\bibitem{yang2020tensor}
Greg Yang.
\newblock Tensor programs ii: Neural tangent kernel for any architecture.
\newblock {\em arXiv preprint arXiv:2006.14548}, 2020.

\bibitem{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz
  Kaiser.
\newblock Universal transformers.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{yun2019transformers}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv
  Kumar.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{giannou2023looped}
Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason~D Lee, and
  Dimitris Papailiopoulos.
\newblock Looped transformers as programmable computers.
\newblock {\em arXiv:2301.13196}, 2023.

\bibitem{levine2020limits}
Yoav Levine, Noam Wies, Or~Sharir, Hofit Bata, and Amnon Shashua.
\newblock Limits to depth efficiencies of self-attention.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 22640--22651, 2020.

\bibitem{snell2021approximating}
Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt.
\newblock Approximating how single head attention learns.
\newblock {\em arXiv preprint arXiv:2103.07601}, 2021.

\bibitem{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock {\em arXiv preprint arXiv:2109.01652}, 2021.

\bibitem{akyurek2022learning}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with
  linear models.
\newblock {\em arXiv:2211.15661}, 2022.

\bibitem{garg2022can}
Shivam Garg, Dimitris Tsipras, Percy~S Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? a case study of simple
  function classes.
\newblock {\em Advances in Neural Information Processing Systems},
  35:30583--30598, 2022.

\bibitem{li2023transformers}
Yingcong Li, M~Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak.
\newblock Transformers as algorithms: Generalization and stability in
  in-context learning.
\newblock In {\em International Conference on Machine Learning}, 2023.

\bibitem{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and
  Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock {\em arXiv preprint arXiv:2201.11903}, 2022.

\bibitem{feng2023towards}
Guhao Feng, Yuntian Gu, Bohang Zhang, Haotian Ye, Di~He, and Liwei Wang.
\newblock Towards revealing the mystery behind chain of thought: a theoretical
  perspective.
\newblock {\em arXiv preprint arXiv:2305.15408}, 2023.

\bibitem{li2023dissecting}
Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, and
  Samet Oymak.
\newblock Dissecting chain-of-thought: A study on compositional in-context
  learning of mlps.
\newblock {\em arXiv preprint arXiv:2305.18869}, 2023.

\bibitem{tian2023scan}
Yuandong Tian, Yiping Wang, Beidi Chen, and Simon Du.
\newblock Scan and snap: Understanding training dynamics and token composition
  in 1-layer transformer.
\newblock {\em arXiv:2305.16380}, 2023.

\bibitem{nguyen2023primal}
Tan~Minh Nguyen, Tam~Minh Nguyen, Nhat Ho, Andrea~L Bertozzi, Richard Baraniuk,
  and Stanley Osher.
\newblock A primal-dual framework for transformers and neural networks.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{tarzanagh2023transformers}
Davoud~Ataee Tarzanagh, Yingcong Li, Christos Thrampoulidis, and Samet Oymak.
\newblock Transformers as support vector machines.
\newblock {\em arXiv preprint arXiv:2308.16898}, 2023.

\end{thebibliography}
