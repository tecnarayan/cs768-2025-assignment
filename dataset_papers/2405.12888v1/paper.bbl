\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amari(1998)]{amari1998natural}
Amari, S.-I.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural computation}, 10\penalty0 (2):\penalty0 251--276, 1998.

\bibitem[Amos et~al.(2017)Amos, Xu, and Kolter]{amos2017input}
Amos, B., Xu, L., and Kolter, J.~Z.
\newblock Input convex neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  146--155. PMLR, 2017.

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{Arora18b}
Arora, S., Cohen, N., and Hazan, E.
\newblock On the optimization of deep networks: Implicit acceleration by overparameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  244--253. PMLR, 2018.

\bibitem[Arora et~al.(2019)Arora, Cohen, Golowich, and Hu]{Arora18a}
Arora, S., Cohen, N., Golowich, N., and Hu, W.
\newblock A convergence analysis of gradient descent for deep linear neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Bah et~al.(2022)Bah, Rauhut, Terstiege, and Westdickenberg]{Bah}
Bah, B., Rauhut, H., Terstiege, U., and Westdickenberg, M.
\newblock Learning deep linear neural networks: Riemannian gradient flows and convergence to global minimizers.
\newblock \emph{Information and Inference: A Journal of the IMA}, 11\penalty0 (1):\penalty0 307--353, 2022.

\bibitem[Bonnard et~al.(2018)Bonnard, Chyba, and Rouot]{Bonnard}
Bonnard, B., Chyba, M., and Rouot, J.
\newblock \emph{{Geometric and Numerical Optimal Control - Application to Swimming at Low Reynolds Number and Magnetic Resonance Imaging}}.
\newblock SpringerBriefs in Mathematics. {Springer Int. Publishing}, 2018.

\bibitem[Bregman(1967)]{bregman1967relaxation}
Bregman, L.~M.
\newblock The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming.
\newblock \emph{USSR computational mathematics and mathematical physics}, 7\penalty0 (3):\penalty0 200--217, 1967.

\bibitem[Bubeck et~al.(2015)]{bubeck2015convex}
Bubeck, S. et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Chizat \& Bach(2020)Chizat and Bach]{chizat}
Chizat, L. and Bach, F.
\newblock Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss.
\newblock In \emph{Conf. on Learning Theory}, pp.\  1305--1338. PMLR, 2020.

\bibitem[Du et~al.(2018)Du, Hu, and Lee]{Du}
Du, S.~S., Hu, W., and Lee, J.~D.
\newblock Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Głuch \& Urbanke(2021)Głuch and Urbanke]{głuch2021noether}
Głuch, G. and Urbanke, R.
\newblock Noether: The more things change, the more stay the same.
\newblock \emph{arXiv preprint arXiv:2104.05508}, 2021.

\bibitem[Ji \& Telgarsky(2019)Ji and Telgarsky]{Ji}
Ji, Z. and Telgarsky, M.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kunin et~al.(2021)Kunin, Sagastuy-Brena, Ganguli, Yamins, and Tanaka]{Kunin}
Kunin, D., Sagastuy-Brena, J., Ganguli, S., Yamins, D.~L., and Tanaka, H.
\newblock Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
LeCun, Y., Cortes, C., and Burges, C.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem[Lee \& Seung(1999)Lee and Seung]{lee1999learning}
Lee, D.~D. and Seung, H.~S.
\newblock Learning the parts of objects by non-negative matrix factorization.
\newblock \emph{Nature}, 401\penalty0 (6755):\penalty0 788--791, 1999.

\bibitem[Makkuva et~al.(2020)Makkuva, Taghvaei, Oh, and Lee]{makkuva2020optimal}
Makkuva, A., Taghvaei, A., Oh, S., and Lee, J.
\newblock Optimal transport mapping via input convex neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  6672--6681. PMLR, 2020.

\bibitem[Marcotte et~al.(2024)Marcotte, Gribonval, and Peyr{\'e}]{marcotte2023abide}
Marcotte, S., Gribonval, R., and Peyr{\'e}, G.
\newblock Abide by the law and follow the flow: Conservation laws for gradient flows.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Martens(2010)]{martens2010deep}
Martens, J.
\newblock Deep learning via hessian-free optimization.
\newblock In \emph{Proceedings of the 27th International Conference on International Conference on Machine Learning}, pp.\  735--742, 2010.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
Martens, J. and Grosse, R.
\newblock Optimizing neural networks with kronecker-factored approximate curvature.
\newblock In \emph{International conference on machine learning}, pp.\  2408--2417. PMLR, 2015.

\bibitem[Martens \& Sutskever(2012)Martens and Sutskever]{martens2012training}
Martens, J. and Sutskever, I.
\newblock \emph{Training Deep and Recurrent Networks with Hessian-Free Optimization}.
\newblock Springer, 2012.

\bibitem[Min et~al.(2021)Min, Tarmoun, Vidal, and Mallada]{Min}
Min, H., Tarmoun, S., Vidal, R., and Mallada, E.
\newblock On the explicit role of initialization on the convergence and implicit bias of overparametrized linear networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\  7760--7768. PMLR, 2021.

\bibitem[Nemirovskij \& Yudin(1983)Nemirovskij and Yudin]{nemirovskij1983problem}
Nemirovskij, A.~S. and Yudin, D.~B.
\newblock \emph{Problem complexity and method efficiency in optimization}.
\newblock Wiley-Interscience, 1983.

\bibitem[Nesterov(1983)]{nesterov1983method}
Nesterov, Y.~E.
\newblock A method of solving a convex programming problem with convergence rate o$\bigl(k^2 \bigr)$.
\newblock In \emph{Doklady Akademii Nauk}, volume 269, pp.\  543--547. Russian Academy of Sciences, 1983.

\bibitem[Noether(1918)]{Noether1918}
Noether, E.
\newblock Invariante variationsprobleme.
\newblock \emph{Nachrichten von der Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse}, 1918:\penalty0 235--257, 1918.

\bibitem[Polyak(1964)]{polyak1964some}
Polyak, B.~T.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{Ussr computational mathematics and mathematical physics}, 4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Raskutti \& Mukherjee(2015)Raskutti and Mukherjee]{raskutti2015information}
Raskutti, G. and Mukherjee, S.
\newblock The information geometry of mirror descent.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0 (3):\penalty0 1451--1457, 2015.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{Saxe}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6120}, 2013.

\bibitem[Su et~al.(2016)Su, Boyd, and Candes]{su2016differential}
Su, W., Boyd, S., and Candes, E.~J.
\newblock A differential equation for modeling nesterov's accelerated gradient method: Theory and insights.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0 (153):\penalty0 1--43, 2016.

\bibitem[Tanaka \& Kunin(2021)Tanaka and Kunin]{Tanaka}
Tanaka, H. and Kunin, D.
\newblock Noether’s learning dynamics: Role of symmetry breaking in neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Tarmoun et~al.(2021)Tarmoun, Franca, Haeffele, and Vidal]{Tarmoun}
Tarmoun, S., Franca, G., Haeffele, B.~D., and Vidal, R.
\newblock Understanding the dynamics of gradient flow in overparameterized linear models.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10153--10161. PMLR, 2021.

\bibitem[{The Sage Developers}(2022)]{sagemath}
{The Sage Developers}.
\newblock \emph{{S}ageMath, the {S}age {M}athematics {S}oftware {S}ystem ({V}ersion 9.7)}, 2022.
\newblock {\tt https://www.sagemath.org}.

\bibitem[Wibisono et~al.(2016)Wibisono, Wilson, and Jordan]{jordan}
Wibisono, A., Wilson, A.~C., and Jordan, M.~I.
\newblock A variational perspective on accelerated methods in optimization.
\newblock \emph{proceedings of the National Academy of Sciences}, 113\penalty0 (47):\penalty0 E7351--E7358, 2016.

\bibitem[Zhao et~al.(2023)Zhao, Ganev, Walters, Yu, and Dehmamy]{zhao}
Zhao, B., Ganev, I., Walters, R., Yu, R., and Dehmamy, N.
\newblock Symmetries, flat minima, and the conserved quantities of gradient flow.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\end{thebibliography}
