\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2018)Arora, Basu, Mianjy, and
  Mukherjee]{arora2018understanding}
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee.
\newblock Understanding deep neural networks with rectified linear units.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Boyd and Vandenberghe(2004)]{boyd2004convex}
Stephen Boyd and Lieven Vandenberghe.
\newblock \emph{Convex Optimization}.
\newblock Cambridge University Press, 2004.

\bibitem[Chen et~al.(2021)Chen, Lee, Garudadri, and Rao]{chen2021resnests}
Kuan-Lin Chen, Ching-Hua Lee, Harinath Garudadri, and Bhaskar~D. Rao.
\newblock {ResNEsts} and {DenseNEsts}: Block-based {DNN} models with improved
  representation guarantees.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3413--3424, 2021.

\bibitem[Chua and Deng(1988)]{chua1988canonical}
Leon~O. Chua and An-Chang Deng.
\newblock Canonical piecewise-linear representation.
\newblock \emph{IEEE Transactions on Circuits and Systems}, 35\penalty0
  (1):\penalty0 101--111, 1988.

\bibitem[Cybenko(1989)]{cybenko1989approximation}
George Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of Control, Signals and Systems}, 2\penalty0
  (4):\penalty0 303--314, 1989.

\bibitem[Eldan and Shamir(2016)]{eldan2016power}
Ronen Eldan and Ohad Shamir.
\newblock The power of depth for feedforward neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 907--940. PMLR, 2016.

\bibitem[Fukushima(1980)]{fukushima1980neocognitron}
Kunihiko Fukushima.
\newblock Neocognitron: A self-organizing neural network model for a mechanism
  of pattern recognition unaffected by shift in position.
\newblock \emph{Biological Cybernetics}, 36\penalty0 (4):\penalty0 193--202,
  1980.

\bibitem[Goodfellow et~al.(2013)Goodfellow, Warde-Farley, Mirza, Courville, and
  Bengio]{goodfellow2013maxout}
Ian Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua
  Bengio.
\newblock Maxout networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1319--1327. PMLR, 2013.

\bibitem[Hanin and Rolnick(2019{\natexlab{a}})]{hanin2019complexity}
Boris Hanin and David Rolnick.
\newblock Complexity of linear regions in deep networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  2596--2604. PMLR, 2019{\natexlab{a}}.

\bibitem[Hanin and Rolnick(2019{\natexlab{b}})]{hanin2019deep}
Boris Hanin and David Rolnick.
\newblock Deep {ReLU} networks have surprisingly few activation patterns.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2019{\natexlab{b}}.

\bibitem[He et~al.(2020)He, Li, Xu, and Zheng]{he2020relu}
Juncai He, Lin Li, Jinchao Xu, and Chunyue Zheng.
\newblock {ReLU} deep neural networks and linear finite elements.
\newblock \emph{Journal of Computational Mathematics}, 38\penalty0
  (3):\penalty0 502--527, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  pages 770--778. IEEE, 2016.

\bibitem[Hertrich et~al.(2021)Hertrich, Basu, Di~Summa, and
  Skutella]{hertrich2021towards}
Christoph Hertrich, Amitabh Basu, Marco Di~Summa, and Martin Skutella.
\newblock Towards lower bounds on the depth of {ReLU} neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3336--3348, 2021.

\bibitem[Hinz(2021)]{hinz2021analysis}
Peter Hinz.
\newblock \emph{An analysis of the piece-wise affine structure of ReLU
  feed-forward neural networks}.
\newblock PhD thesis, ETH Zurich, 2021.

\bibitem[Hinz and van~de Geer(2019)]{hinz2019framework}
Peter Hinz and Sara van~de Geer.
\newblock A framework for the construction of upper bounds on the number of
  affine linear regions of relu feed-forward neural networks.
\newblock \emph{IEEE Transactions on Information Theory}, 65\penalty0
  (11):\penalty0 7304--7324, 2019.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and
  White]{hornik1989multilayer}
Kurt Hornik, Maxwell Stinchcombe, and Halbert White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural Networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Huang et~al.(2017)Huang, Liu, van~der Maaten, and
  Weinberger]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens van~der Maaten, and Kilian~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  pages 4700--4708. IEEE, 2017.

\bibitem[Hwang and Heinecke(2019)]{hwang2019rectifying}
Wen-Liang Hwang and Andreas Heinecke.
\newblock Un-rectifying non-linear networks for signal representation.
\newblock \emph{IEEE Transactions on Signal Processing}, 68:\penalty0 196--210,
  2019.

\bibitem[Karmarkar(1984)]{karmarkar1984new}
Narendra Karmarkar.
\newblock A new polynomial-time algorithm for linear programming.
\newblock In \emph{Proceedings of the 16th Annual ACM Symposium on Theory of
  Computing}, pages 302--311, 1984.
\newblock Revised version: \textit{Combinatorica} 4:373--395, 1984.

\bibitem[Khachiyan(1979)]{khachiyan1979polynomial}
Leonid~Genrikhovich Khachiyan.
\newblock A polynomial algorithm in linear programming.
\newblock \emph{Doklady Akademii Nauk}, 244\penalty0 (5):\penalty0 1093--1096,
  1979.
\newblock Translated in \textit{Soviet Mathematics Doklady} 20(1):191--194,
  1979.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.
\newblock Image{N}et classification with deep convolutional neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1097--1105, 2012.

\bibitem[Lu et~al.(2017)Lu, Pu, Wang, Hu, and Wang]{lu2017expressive}
Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang.
\newblock The expressive power of neural networks: A view from the width.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Maas et~al.(2013)Maas, Hannun, and Ng]{maas2013rectifier}
Andrew~L. Maas, Awni~Y. Hannun, and Andrew~Y. Ng.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In \emph{International Conference on Machine Learning}, 2013.

\bibitem[Magnani and Boyd(2009)]{magnani2009convex}
Alessandro Magnani and Stephen~P. Boyd.
\newblock Convex piecewise-linear fitting.
\newblock \emph{Optimization and Engineering}, 10\penalty0 (1):\penalty0 1--17,
  2009.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A. Rusu, Joel Veness,
  Marc~G. Bellemare, Alex Graves, Martin Riedmiller, Andreas~K. Fidjeland,
  Georg Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mont{\'u}far(2017)]{montufar2017notes}
Guido Mont{\'u}far.
\newblock Notes on the number of linear regions of deep neural networks.
\newblock In \emph{International Conference on Sampling Theory and
  Applications}, 2017.

\bibitem[Mont{\'u}far et~al.(2014)Mont{\'u}far, Pascanu, Cho, and
  Bengio]{montufar2014number}
Guido Mont{\'u}far, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio.
\newblock On the number of linear regions of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2924--2932, 2014.

\bibitem[Nair and Hinton(2010)]{nair2010rectified}
Vinod Nair and Geoffrey~E. Hinton.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In \emph{International Conference on Machine Learning}, pages
  807--814, 2010.

\bibitem[Ovchinnikov(2002)]{ovchinnikov2002max}
Sergei Ovchinnikov.
\newblock Max-min representation of piecewise linear functions.
\newblock \emph{Contributions to Algebra and Geometry}, 43\penalty0
  (1):\penalty0 297--302, 2002.

\bibitem[Pascanu et~al.(2014)Pascanu, Mont{\'u}far, and
  Bengio]{pascanu2013number}
Razvan Pascanu, Guido Mont{\'u}far, and Yoshua Bengio.
\newblock On the number of response regions of deep feed forward networks with
  piece-wise linear activations.
\newblock \emph{International Conference on Learning Representations}, 2014.

\bibitem[Raghu et~al.(2017)Raghu, Poole, Kleinberg, Ganguli, and
  Sohl-Dickstein]{raghu2017expressive}
Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha
  Sohl-Dickstein.
\newblock On the expressive power of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  2847--2854. PMLR, 2017.

\bibitem[Renegar(1988)]{renegar1988polynomial}
James Renegar.
\newblock A polynomial-time algorithm, based on {Newton}'s method, for linear
  programming.
\newblock \emph{Mathematical Programming}, 40\penalty0 (1):\penalty0 59--93,
  1988.

\bibitem[Rolnick and Kording(2020)]{rolnick2020reverse}
David Rolnick and Konrad Kording.
\newblock Reverse-engineering deep {ReLU} networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  8178--8187. PMLR, 2020.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock {U-Net}: Convolutional networks for biomedical image segmentation.
\newblock In \emph{International Conference on Medical Image Computing and
  Computer Assisted Intervention}, pages 234--241. Springer, 2015.

\bibitem[Serra et~al.(2018)Serra, Tjandraatmadja, and
  Ramalingam]{serra2018bounding}
Thiago Serra, Christian Tjandraatmadja, and Srikumar Ramalingam.
\newblock Bounding and counting linear regions of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  4558--4566. PMLR, 2018.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, van~den
  Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman,
  Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel,
  and Hassabis]{silver2016mastering}
David Silver, Aja Huang, Chris~J. Maddison, Arthur Guez, Laurent Sifre, George
  van~den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal
  Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray
  Kavukcuoglu, Thore Graepel, and Demis Hassabis.
\newblock Mastering the game of {G}o with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Simonyan and Zisserman(2015)]{simonyan2015very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Smale(1998)]{smale1998mathematical}
Steve Smale.
\newblock Mathematical problems for the next century.
\newblock \emph{The Mathematical Intelligencer}, 20\penalty0 (2):\penalty0
  7--15, 1998.

\bibitem[Tarela and Martínez(1999)]{tarela1999region}
J.~M. Tarela and M.~V. Martínez.
\newblock Region configurations for realizability of lattice piecewise-linear
  models.
\newblock \emph{Mathematical and Computer Modelling}, 30\penalty0
  (11-12):\penalty0 17--27, 1999.

\bibitem[Tarela et~al.(1990)Tarela, Alonso, and
  Martínez]{tarela1990representation}
J.~M. Tarela, E.~Alonso, and M.~V. Martínez.
\newblock A representation method for {PWL} functions oriented to parallel
  processing.
\newblock \emph{Mathematical and Computer Modelling}, 13\penalty0
  (10):\penalty0 75--83, 1990.

\bibitem[Telgarsky(2016)]{telgarsky2016benefits}
Matus Telgarsky.
\newblock Benefits of depth in neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 1517--1539. PMLR,
  2016.

\bibitem[Vardi et~al.(2021)Vardi, Reichman, Pitassi, and Shamir]{vardi2021size}
Gal Vardi, Daniel Reichman, Toniann Pitassi, and Ohad Shamir.
\newblock Size and depth separation in approximating benign functions with
  neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 4195--4223. PMLR,
  2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Vavasis and Ye(1996)]{vavasis1996primal}
Stephen~A. Vavasis and Yinyu Ye.
\newblock A primal-dual interior point method whose running time depends only
  on the constraint matrix.
\newblock \emph{Mathematical Programming}, 74\penalty0 (1):\penalty0 79--120,
  1996.

\bibitem[Wang(2004)]{wang2004general}
Shuning Wang.
\newblock General constructive representations for continuous piecewise-linear
  functions.
\newblock \emph{IEEE Transactions on Circuits and Systems I: Regular Papers},
  51\penalty0 (9):\penalty0 1889--1896, 2004.

\bibitem[Wang and Sun(2005)]{wang2005generalization}
Shuning Wang and Xusheng Sun.
\newblock Generalization of hinging hyperplanes.
\newblock \emph{IEEE Transactions on Information Theory}, 51\penalty0
  (12):\penalty0 4425--4431, 2005.

\bibitem[Zaslavsky(1975)]{zaslavsky1975facing}
Thomas Zaslavsky.
\newblock \emph{Facing up to arrangements: Face-count formulas for partitions
  of space by hyperplanes}, volume 154.
\newblock American Mathematical Society, 1975.

\bibitem[Zeiler et~al.(2013)Zeiler, Ranzato, Monga, Mao, Yang, Le, Nguyen,
  Senior, Vanhoucke, Dean, and Hinton]{zeiler2013rectified}
Matthew~D. Zeiler, Marc'Aurelio Ranzato, Rajat Monga, Min Mao, Kun Yang,
  Quoc~V. Le, Patrick Nguyen, Alan Senior, Vincent Vanhoucke, Jeffrey Dean, and
  Geoffrey~E. Hinton.
\newblock On rectified linear units for speech processing.
\newblock In \emph{International Conference on Acoustics, Speech and Signal
  Processing}, pages 3517--3521. IEEE, 2013.

\end{thebibliography}
