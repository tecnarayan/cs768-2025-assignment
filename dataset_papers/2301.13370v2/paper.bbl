\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi \& Plotkin(2020)Abadi and Plotkin]{AbadiP20}
Abadi, M. and Plotkin, G.~D.
\newblock A simple differentiable programming language.
\newblock \emph{Proceedings of the {ACM} on Programming Languages}, 4\penalty0
  ({POPL}):\penalty0 38:1--38:28, 2020.

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, Kudlur, Levenberg, Monga, Moore, Murray, Steiner,
  Tucker, Vasudevan, Warden, Wicke, Yu, and Zheng]{Tensorflow16}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R.,
  Moore, S., Murray, D.~G., Steiner, B., Tucker, P.~A., Vasudevan, V., Warden,
  P., Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow:} {A} system for large-scale machine learning.
\newblock In \emph{Symposium on Operating Systems Design and Implementation
  ({OSDI})}, pp.\  265--283, 2016.

\bibitem[Barthe et~al.(2020)Barthe, Crubill{\'{e}}, Lago, and
  Gavazzo]{BartheCLG20}
Barthe, G., Crubill{\'{e}}, R., Lago, U.~D., and Gavazzo, F.
\newblock On the versatility of open logical relations - continuity, automatic
  differentiation, and a containment theorem.
\newblock In \emph{European Symposium on Programming ({ESOP})}, pp.\  56--83,
  2020.

\bibitem[Barton et~al.(2018)Barton, Khan, Stechlinski, and Watson]{BartonKSW18}
Barton, P.~I., Khan, K.~A., Stechlinski, P., and Watson, H. A.~J.
\newblock Computationally relevant generalized derivatives: {Theory,}
  evaluation and applications.
\newblock \emph{Optimization Methods and Software}, 33\penalty0 (4-6):\penalty0
  1030--1072, 2018.

\bibitem[Baydin et~al.(2016)Baydin, Pearlmutter, and Siskind]{Diffsharp16}
Baydin, A.~G., Pearlmutter, B.~A., and Siskind, J.~M.
\newblock {Diffsharp:} {An} {AD} library for {.NET} languages.
\newblock In \emph{International Conference on Algorithmic Differentiation
  (AD)}, 2016.
\newblock Also arXiv:1611.03423.

\bibitem[Baydin et~al.(2017)Baydin, Pearlmutter, Radul, and
  Siskind]{BaydinPRS17}
Baydin, A.~G., Pearlmutter, B.~A., Radul, A.~A., and Siskind, J.~M.
\newblock Automatic differentiation in machine learning: {A} survey.
\newblock \emph{Journal of Machine Learning Research}, 18:\penalty0
  153:1--153:43, 2017.

\bibitem[Bergstra et~al.(2010)Bergstra, Breuleux, Bastien, Lamblin, Pascanu,
  Desjardins, Turian, Warde-Farley, and Bengio]{Theano10}
Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins,
  G., Turian, J., Warde-Farley, D., and Bengio, Y.
\newblock {Theano:} {A} {CPU} and {GPU} math compiler in {Python}.
\newblock In \emph{Python in Science Conference (SciPy)}, pp.\  18--24, 2010.

\bibitem[Bertoin et~al.(2021)Bertoin, Bolte, Gerchinovitz, and
  Pauwels]{BertoinBGP21}
Bertoin, D., Bolte, J., Gerchinovitz, S., and Pauwels, E.
\newblock Numerical influence of {ReLU}'(0) on backpropagation.
\newblock In \emph{Annual Conference on Neural Information Processing Systems
  (NeurIPS)}, pp.\  468--479, 2021.

\bibitem[Bolte \& Pauwels(2020{\natexlab{a}})Bolte and Pauwels]{BolteP20a}
Bolte, J. and Pauwels, E.
\newblock Conservative set valued fields, automatic differentiation, stochastic
  gradient method and deep learning.
\newblock \emph{Mathematical Programming}, 188:\penalty0 19â€“51,
  2020{\natexlab{a}}.

\bibitem[Bolte \& Pauwels(2020{\natexlab{b}})Bolte and Pauwels]{BolteP20b}
Bolte, J. and Pauwels, E.
\newblock {A mathematical model for automatic differentiation in machine
  learning}.
\newblock In \emph{Annual Conference on Neural Information Processing Systems
  (NeurIPS)}, pp.\  10809--10819, 2020{\natexlab{b}}.

\bibitem[Bolte et~al.(2023)Bolte, Boustany, Pauwels, and
  Pesquet{-}Popescu]{BolteBPP23}
Bolte, J., Boustany, R., Pauwels, E., and Pesquet{-}Popescu, B.
\newblock On the complexity of nonsmooth automatic differentiation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023.

\bibitem[Brunel et~al.(2020)Brunel, Mazza, and Pagani]{BrunelMP20}
Brunel, A., Mazza, D., and Pagani, M.
\newblock Backpropagation in the simply typed lambda-calculus with linear
  negation.
\newblock \emph{Proceedings of the {ACM} on Programming Languages}, 4\penalty0
  ({POPL}):\penalty0 64:1--64:27, 2020.

\bibitem[Burden et~al.(2015)Burden, Faires, and Burden]{BurdenFB15}
Burden, R.~L., Faires, J.~D., and Burden, A.~M.
\newblock \emph{Numerical analysis}.
\newblock Cengage learning, 10th edition, 2015.

\bibitem[Clarke(1975)]{Clarke75}
Clarke, F.~H.
\newblock Generalized gradients and applications.
\newblock \emph{Transactions of the American Mathematical Society},
  205:\penalty0 247--262, 1975.

\bibitem[Clarke(1990)]{Clarke90}
Clarke, F.~H.
\newblock \emph{Optimization and nonsmooth analysis}.
\newblock Classics in Applied Mathematics: Volume~5. {SIAM}, 1990.

\bibitem[Collobert et~al.(2011)Collobert, Kavukcuoglu, and Farabet]{Torch11}
Collobert, R., Kavukcuoglu, K., and Farabet, C.
\newblock {Torch7:} {A} {Matlab}-like environment for machine learning.
\newblock In \emph{NIPS BigLearn Workshop}, 2011.

\bibitem[Cui \& Pang(2021)Cui and Pang]{CuiP21}
Cui, Y. and Pang, J.-S.
\newblock \emph{Modern nonconvex nondifferentiable optimization}.
\newblock MOS-SIAM Series on Optimization. {SIAM}, 2021.

\bibitem[Davis et~al.(2020)Davis, Drusvyatskiy, Kakade, and Lee]{DavisDKL20}
Davis, D., Drusvyatskiy, D., Kakade, S.~M., and Lee, J.~D.
\newblock Stochastic subgradient method converges on tame functions.
\newblock \emph{Foundations of Computational Mathematics}, 20\penalty0
  (1):\penalty0 119--154, 2020.

\bibitem[Elliott(2018)]{Elliott18}
Elliott, C.
\newblock The simple essence of automatic differentiation.
\newblock \emph{Proceedings of the {ACM} on Programming Languages}, 2\penalty0
  ({ICFP}):\penalty0 70:1--70:29, 2018.

\bibitem[Frostig et~al.(2018)Frostig, Johnson, and Leary]{Jax18b}
Frostig, R., Johnson, M., and Leary, C.
\newblock Compiling machine learning programs via high-level tracing.
\newblock In \emph{SysML Conference}, 2018.

\bibitem[Griewank \& Walther(2008)Griewank and Walther]{GriewankW08}
Griewank, A. and Walther, A.
\newblock \emph{Evaluating derivatives: {Principles} and techniques of
  algorithmic differentiation}.
\newblock {SIAM}, 2nd edition, 2008.

\bibitem[Hasco{\"{e}}t \& Pascual(2013)Hasco{\"{e}}t and Pascual]{Tapenade13}
Hasco{\"{e}}t, L. and Pascual, V.
\newblock The {Tapenade} automatic differentiation tool: {Principles}, model,
  and specification.
\newblock \emph{{ACM} Transactions on Mathematical Software}, 39\penalty0
  (3):\penalty0 20:1--20:43, 2013.

\bibitem[Huot et~al.(2020)Huot, Staton, and V{\'{a}}k{\'{a}}r]{HuotSV20}
Huot, M., Staton, S., and V{\'{a}}k{\'{a}}r, M.
\newblock Correctness of automatic differentiation via diffeologies and
  categorical gluing.
\newblock In \emph{International Conference on Foundations of Software Science
  and Computation Structures ({FoSSaCS})}, pp.\  319--338, 2020.

\bibitem[Huot et~al.(2023)Huot, Lew, Mansinghka, and Staton]{HuotLMS22}
Huot, M., Lew, A.~K., Mansinghka, V.~K., and Staton, S.
\newblock {{\(\omega\)}PAP} spaces: {Reasoning} denotationally about
  higher-order, recursive probabilistic and differentiable programs.
\newblock \emph{arXiv:2302.10636}, 2023.

\bibitem[Jacot et~al.(2018)Jacot, Hongler, and Gabriel]{JacotHG18}
Jacot, A., Hongler, C., and Gabriel, F.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Annual Conference on Neural Information Processing Systems
  (NeurIPS)}, pp.\  8580--8589, 2018.

\bibitem[Jia et~al.(2014)Jia, Shelhamer, Donahue, Karayev, Long, Girshick,
  Guadarrama, and Darrell]{Caffe14}
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R.~B.,
  Guadarrama, S., and Darrell, T.
\newblock {Caffe:} {Convolutional} architecture for fast feature embedding.
\newblock In \emph{International Conference on Multimedia ({MM})}, pp.\
  675--678, 2014.

\bibitem[Kakade \& Lee(2018)Kakade and Lee]{KakadeL18}
Kakade, S.~M. and Lee, J.~D.
\newblock Provably correct automatic sub-differentiation for qualified
  programs.
\newblock In \emph{Annual Conference on Neural Information Processing Systems
  (NeurIPS)}, pp.\  7125--7135, 2018.

\bibitem[Khan \& Barton(2015)Khan and Barton]{KhanB15}
Khan, K.~A. and Barton, P.~I.
\newblock A vector forward mode of automatic differentiation for generalized
  derivative evaluation.
\newblock \emph{Optimization Methods and Software}, 30\penalty0 (6):\penalty0
  1185--1212, 2015.

\bibitem[Kidger \& Lyons(2020)Kidger and Lyons]{kidger20}
Kidger, P. and Lyons, T.
\newblock Universal approximation with deep narrow networks.
\newblock In \emph{Conference on Learning Theory (COLT)}, pp.\  2306--2327,
  2020.

\bibitem[Krawiec et~al.(2022)Krawiec, Jones, Krishnaswami, Ellis, Eisenberg,
  and Fitzgibbon]{KrawiecJKEEF22}
Krawiec, F., Jones, S.~P., Krishnaswami, N., Ellis, T., Eisenberg, R.~A., and
  Fitzgibbon, A.~W.
\newblock Provably correct, asymptotically efficient, higher-order reverse-mode
  automatic differentiation.
\newblock \emph{Proceedings of the {ACM} on Programming Languages}, 6\penalty0
  ({POPL}):\penalty0 48:1--48:30, 2022.

\bibitem[Laurent \& von Brecht(2018)Laurent and von Brecht]{LaurentB18a}
Laurent, T. and von Brecht, J.
\newblock The multilinear structure of {ReLU} networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2914--2922, 2018.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{LecunBH15}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Lee et~al.(2020)Lee, Yu, Rival, and Yang]{LeeYRY20}
Lee, W., Yu, H., Rival, X., and Yang, H.
\newblock On correctness of automatic differentiation for non-differentiable
  functions.
\newblock In \emph{Annual Conference on Neural Information Processing Systems
  (NeurIPS)}, pp.\  6719--6730, 2020.

\bibitem[Lu et~al.(2017)Lu, Pu, Wang, Hu, and Wang]{lu17}
Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L.
\newblock {The expressive power of neural networks: A view from the width}.
\newblock In \emph{Annual Conference on Neural Information Processing Systems
  (NeurIPS)}, pp.\  6232--6240, 2017.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and Adams]{Autograd15}
Maclaurin, D., Duvenaud, D., and Adams, R.~P.
\newblock {Autograd:} {Effortless} gradients in {Numpy}.
\newblock In \emph{ICML AutoML Workshop}, 2015.

\bibitem[Mazza \& Pagani(2021)Mazza and Pagani]{MazzaP21}
Mazza, D. and Pagani, M.
\newblock Automatic differentiation in {PCF}.
\newblock \emph{Proceedings of the {ACM} on Programming Languages}, 5\penalty0
  ({POPL}):\penalty0 28:1--28:27, 2021.

\bibitem[Park et~al.(2021)Park, Yun, Lee, and Shin]{park21}
Park, S., Yun, C., Lee, J., and Shin, J.
\newblock {Minimum width for universal approximation}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{Pytorch17}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in {PyTorch}.
\newblock In \emph{NIPS Autodiff Workshop}, 2017.

\bibitem[Pearlmutter \& Siskind(2008)Pearlmutter and Siskind]{PearlmutterS08}
Pearlmutter, B.~A. and Siskind, J.~M.
\newblock Reverse-mode {AD} in a functional framework: {Lambda} the ultimate
  backpropagator.
\newblock \emph{{ACM} Transactions on Programming Languages and Systems},
  30\penalty0 (2):\penalty0 7:1--7:36, 2008.

\bibitem[Radul et~al.(2023)Radul, Paszke, Frostig, Johnson, and
  Maclaurin]{RadulPFJM23}
Radul, A., Paszke, A., Frostig, R., Johnson, M.~J., and Maclaurin, D.
\newblock You only linearize once: Tangents transpose to gradients.
\newblock \emph{Proceedings of the {ACM} on Programming Languages}, 7\penalty0
  ({POPL}):\penalty0 43:1--43:29, 2023.

\bibitem[Revels et~al.(2016)Revels, Lubin, and Papamarkou]{Juliadiff16}
Revels, J., Lubin, M., and Papamarkou, T.
\newblock Forward-mode automatic differentiation in {Julia}.
\newblock \emph{arXiv:1607.07892}, 2016.

\bibitem[Rockafellar \& Wets(1998)Rockafellar and Wets]{RockafellarW98}
Rockafellar, R.~T. and Wets, R. J.-B.
\newblock \emph{Variational analysis}.
\newblock A Series of Comprehensive Studies in Mathematics: Volume~317.
  Springer Science \& Business Media, 1998.

\bibitem[Schmidhuber(2015)]{Schmidhuber15-DL}
Schmidhuber, J.
\newblock Deep learning in neural networks: {An} overview.
\newblock \emph{Neural Networks}, 61:\penalty0 85--117, 2015.

\bibitem[Scholtes(2012)]{Scholtes12}
Scholtes, S.
\newblock \emph{Introduction to piecewise differentiable equations}.
\newblock SpringerBriefs in Optimization. Springer Science \& Business Media,
  2012.

\bibitem[Seide \& Agarwal(2016)Seide and Agarwal]{CNTK16}
Seide, F. and Agarwal, A.
\newblock {CNTK:} {Microsoft}'s open-source deep-learning toolkit.
\newblock In \emph{International Conference on Knowledge Discovery and Data
  Mining ({KDD})}, pp.\  2135, 2016.

\bibitem[Slusanschi \& Dumitrel(2016)Slusanschi and Dumitrel]{Adijac16}
Slusanschi, E. and Dumitrel, V.
\newblock {ADiJaC} -- automatic differentiation of {Java} classfiles.
\newblock \emph{{ACM} Transactions on Mathematical Software}, 43\penalty0
  (2):\penalty0 9:1--9:33, 2016.

\bibitem[Smeding \& V{\'{a}}k{\'{a}}r(2023)Smeding and
  V{\'{a}}k{\'{a}}r]{SmedingV23}
Smeding, T. and V{\'{a}}k{\'{a}}r, M.
\newblock Efficient dual-numbers reverse {AD} via well-known program
  transformations.
\newblock \emph{Proceedings of the {ACM} on Programming Languages}, 7\penalty0
  ({POPL}):\penalty0 54:1--54:28, 2023.

\bibitem[Tokui et~al.(2019)Tokui, Okuta, Akiba, Niitani, Ogawa, Saito, Suzuki,
  Uenishi, Vogel, and Vincent]{Chainer19}
Tokui, S., Okuta, R., Akiba, T., Niitani, Y., Ogawa, T., Saito, S., Suzuki, S.,
  Uenishi, K., Vogel, B., and Vincent, H.~Y.
\newblock {Chainer:} {A} deep learning framework for accelerating the research
  cycle.
\newblock In \emph{International Conference on Knowledge Discovery {\&} Data
  Mining ({KDD})}, pp.\  2002--2011, 2019.

\bibitem[V{\'{a}}k{\'{a}}r(2021)]{Vakar21}
V{\'{a}}k{\'{a}}r, M.
\newblock Reverse {AD} at higher types: Pure, principled and denotationally
  correct.
\newblock In \emph{European Symposium on Programming ({ESOP})}, pp.\  607--634,
  2021.

\bibitem[van Merrienboer et~al.(2018)van Merrienboer, Moldovan, and
  Wiltschko]{Tangent18}
van Merrienboer, B., Moldovan, D., and Wiltschko, A.~B.
\newblock {Tangent:} {Automatic} differentiation using source-code
  transformation for dynamically typed array programming.
\newblock In \emph{Annual Conference on Neural Information Processing Systems
  (NeurIPS)}, pp.\  6259--6268, 2018.

\bibitem[Walther \& Griewank(2012)Walther and Griewank]{Adolc12}
Walther, A. and Griewank, A.
\newblock Getting started with {ADOL-C}.
\newblock In \emph{Combinatorial Scientific Computing}, chapter~7, pp.\
  181--202. Chapman \& Hall/CRC Computational Science, 2012.

\end{thebibliography}
