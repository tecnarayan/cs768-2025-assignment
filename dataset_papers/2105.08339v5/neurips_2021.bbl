\begin{thebibliography}{10}

\bibitem{mcmahan2017communication}
H.~Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
  Blaise~Ag{\"{u}}era y~Arcas.
\newblock {Communication-Efficient Learning of Deep Networks from Decentralized
  Data}.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1273--1282,
  2017.

\bibitem{pmlr-v70-suresh17a}
Ananda~Theertha Suresh, X~Yu Felix, Sanjiv Kumar, and H~Brendan McMahan.
\newblock {Distributed Mean Estimation With Limited Communication}.
\newblock In {\em International Conference on Machine Learning}, pages
  3329--3337. PMLR, 2017.

\bibitem{icde2021}
Yuval Alfassi, Moshe Gabel, Gal Yehuda, and Daniel Keren.
\newblock A {D}istance-{B}ased {S}cheme for {R}educing {B}andwidth in
  {D}istributed {G}eometric {M}onitoring.
\newblock In {\em 2021 IEEE 37th International Conference on Data Engineering
  ({ICDE})}, 2021.

\bibitem{NIPS2012_6aca9700}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Marc\textquotesingle~aurelio Ranzato, Andrew Senior, Paul Tucker, Ke~Yang,
  Quoc Le, and Andrew Ng.
\newblock {Large Scale Distributed Deep Networks}.
\newblock In F.~Pereira, C.~J.~C. Burges, L.~Bottou, and K.~Q. Weinberger,
  editors, {\em Advances in Neural Information Processing Systems}, volume~25.
  Curran Associates, Inc., 2012.

\bibitem{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock {Megatron-lm: Training multi-billion parameter language models using
  model parallelism}.
\newblock {\em arXiv preprint arXiv:1909.08053}, 2019.

\bibitem{NEURIPS2019_093f65e0}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,
  HyoukJoong Lee, Jiquan Ngiam, Quoc~V Le, Yonghui Wu, and zhifeng Chen.
\newblock {GPipe: Efficient Training of Giant Neural Networks using Pipeline
  Parallelism}.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{ben2020send}
Ran Ben-Basat, Michael Mitzenmacher, and Shay Vargaftik.
\newblock {How to Send a Real Number Using a Single Bit (and Some Shared
  Randomness)}.
\newblock {\em arXiv preprint arXiv:2010.02331}, 2020.

\bibitem{wen2017terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock {TernGrad: Ternary Gradients to Reduce Communication in Distributed
  Deep Learning}.
\newblock In {\em Advances in neural information processing systems}, pages
  1509--1519, 2017.

\bibitem{NIPS2017_6c340f25}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock {QSGD: Communication-Efficient Sgd via Gradient Quantization and
  Encoding}.
\newblock {\em Advances in Neural Information Processing Systems},
  30:1709--1720, 2017.

\bibitem{konevcny2018randomized}
Jakub Kone{\v{c}}n{\`y} and Peter Richt{\'a}rik.
\newblock {Randomized Distributed Mean Estimation: Accuracy vs. Communication}.
\newblock {\em Frontiers in Applied Mathematics and Statistics}, 4:62, 2018.

\bibitem{caldas2018expanding}
Sebastian Caldas, Jakub Konečný, H~Brendan McMahan, and Ameet Talwalkar.
\newblock {Expanding the Reach of Federated Learning by Reducing Client
  Resource Requirements}.
\newblock {\em arXiv preprint arXiv:1812.07210}, 2018.

\bibitem{bai2021gradient}
Youhui Bai, Cheng Li, Quan Zhou, Jun Yi, Ping Gong, Feng Yan, Ruichuan Chen,
  and Yinlong Xu.
\newblock Gradient compression supercharged high-performance data parallel dnn
  training.
\newblock In {\em The 28th ACM Symposium on Operating Systems Principles (SOSP
  2021)}, 2021.

\bibitem{lyubarskii2010uncertainty}
Yurii Lyubarskii and Roman Vershynin.
\newblock {Uncertainty Principles and Vector Quantization}.
\newblock {\em IEEE Transactions on Information Theory}, 56(7):3491--3501,
  2010.

\bibitem{iaab006}
Mher Safaryan, Egor Shulgin, and Peter Richt{\'a}rik.
\newblock {Uncertainty Principle for Communication Compression in Distributed
  and Federated Learning and the Search for an Optimal Compressor}.
\newblock {\em arXiv preprint arXiv:2002.08958}, 2020.

\bibitem{davies2021new}
Peter Davies, Vijaykrishna Gurunanthan, Niusha Moshrefi, Saleh Ashkboos, and
  Dan Alistarh.
\newblock {New Bounds For Distributed Mean Estimation and Variance Reduction}.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{newman1991private}
Ilan Newman.
\newblock {Private vs. Common Random Bits in Communication Complexity}.
\newblock {\em Information processing letters}, 39(2):67--71, 1991.

\bibitem{mezzadri2006generate}
Francesco Mezzadri.
\newblock {How to Generate Random Matrices From the Classical Compact Groups}.
\newblock {\em arXiv preprint math-ph/0609050}, 2006.

\bibitem{wedderburn1975generating}
RWM Wedderburn.
\newblock Generating {R}andom {R}otations.
\newblock {\em Research Report}, 1975.

\bibitem{heiberger1978generation}
Richard~M Heiberger.
\newblock {Generation of Random Orthogonal Matrices}.
\newblock {\em Journal of the Royal Statistical Society: Series C (Applied
  Statistics)}, 27(2):199--206, 1978.

\bibitem{stewart1980efficient}
Gilbert~W Stewart.
\newblock {The Efficient Generation of Random Orthogonal Matrices With an
  Application to Condition Estimators}.
\newblock {\em SIAM Journal on Numerical Analysis}, 17(3):403--409, 1980.

\bibitem{tanner1982remark}
Martin~A Tanner and Ronald~A Thisted.
\newblock {Remark as r42: A Remark on as 127. Generation of Random Orthogonal
  Matrices}.
\newblock {\em Journal of the Royal Statistical Society. Series C (Applied
  Statistics)}, 31(2):190--192, 1982.

\bibitem{beznosikov2020biased}
Aleksandr Beznosikov, Samuel Horv{\'a}th, Peter Richt{\'a}rik, and Mher
  Safaryan.
\newblock On biased compression for distributed learning.
\newblock {\em arXiv preprint arXiv:2002.12410}, 2020.

\bibitem{gronlund2017fast}
Allan Gr{\o}nlund, Kasper~Green Larsen, Alexander Mathiasen, Jesper~Sindahl
  Nielsen, Stefan Schneider, and Mingzhou Song.
\newblock {Fast Exact K-Means, K-Medians and Bregman Divergence Clustering in
  1D}.
\newblock {\em arXiv preprint arXiv:1701.07204}, 2017.

\bibitem{pytorchqrfact}
Mario~Lezcano Casado.
\newblock {GeoTorch’s Documentation. A Library for Constrained Optimization
  and Manifold Optimization for Deep Learning in Pytorch.}
\newblock
  \url{https://geotorch.readthedocs.io/en/latest/_modules/geotorch/so.html},
  2021.
\newblock accessed 17-May-21.

\bibitem{ailon2009fast}
Nir Ailon and Bernard Chazelle.
\newblock {The Fast Johnson--Lindenstrauss Transform and Approximate Nearest
  Neighbors}.
\newblock {\em SIAM Journal on computing}, 39(1):302--322, 2009.

\bibitem{fino1976unified}
Bernard~J. Fino and V.~Ralph Algazi.
\newblock Unified matrix treatment of the fast walsh-hadamard transform.
\newblock {\em IEEE Transactions on Computers}, 25(11):1142--1146, 1976.

\bibitem{uberHadamard}
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski.
\newblock Measuring the intrinsic dimension of objective landscapes.
\newblock In {\em International Conference on Learning Representations}, 2018.
\newblock Code available at:
  \url{https://github.com/uber-research/intrinsic-dimension}.

\bibitem{horadam2012Hadamard}
Kathy~J Horadam.
\newblock {\em {Hadamard Matrices and Their Applications}}.
\newblock Princeton university press, 2012.

\bibitem{khintchine1923dyadische}
Aleksandr Khintchine.
\newblock {\"U}ber {D}yadische {B}r{\"u}che.
\newblock {\em Mathematische Zeitschrift}, 18(1):109--116, 1923.

\bibitem{szarek1976best}
S~Szarek.
\newblock {On the Best Constants in the Khinchin Inequality}.
\newblock {\em Studia Mathematica}, 2(58):197--208, 1976.

\bibitem{filmus2012khintchine}
Yuval Filmus.
\newblock {Khintchine-Kahane using Fourier Analysis}.
\newblock {\em Posted at \url{http://www.cs.toronto.edu/yuvalf/KK.pdf}}, 2012.

\bibitem{latala1994best}
Rafa{\l} Lata{\l}a and Krzysztof Oleszkiewicz.
\newblock {On the Best Constant In the Khinchin-Kahane Inequality}.
\newblock {\em Studia Mathematica}, 109(1):101--104, 1994.

\bibitem{chmiel2020neural}
Brian Chmiel, Liad Ben-Uri, Moran Shkolnik, Elad Hoffer, Ron Banner, and Daniel
  Soudry.
\newblock {Neural Gradients Are Near-Lognormal: Improved Quantized and Sparse
  Training}.
\newblock {\em arXiv preprint arXiv:2006.08173}, 2020.

\bibitem{banner2018post}
Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry.
\newblock {Post-Training 4-Bit Quantization of Convolution Networks for
  Rapid-Deployment}.
\newblock {\em arXiv preprint arXiv:1810.05723}, 2018.

\bibitem{ye2020accelerating}
Xucheng Ye, Pengcheng Dai, Junyu Luo, Xin Guo, Yingjie Qi, Jianlei Yang, and
  Yiran Chen.
\newblock {Accelerating CNN Training by Pruning Activation Gradients}.
\newblock In {\em European Conference on Computer Vision}, pages 322--338.
  Springer, 2020.

\bibitem{spruill2007asymptotic}
Marcus Spruill et~al.
\newblock {Asymptotic Distribution of Coordinates on High Dimensional Spheres}.
\newblock {\em Electronic communications in probability}, 12:234--247, 2007.

\bibitem{diaconis1987dozen}
Persi Diaconis and David Freedman.
\newblock {A Dozen de Finetti-Style Results in Search of a Theory}.
\newblock In {\em Annales de l'IHP Probabilit{\'e}s et statistiques},
  volume~23, pages 397--423, 1987.

\bibitem{rachev1991approximate}
Svetlozar~T Rachev, L~Ruschendorf, et~al.
\newblock {Approximate Independence of Distributions on Spheres and Their
  Stability Properties}.
\newblock {\em The Annals of Probability}, 19(3):1311--1337, 1991.

\bibitem{stam1982limit}
Adriaan~J Stam.
\newblock {Limit Theorems for Uniform Distributions on Spheres in
  High-Dimensional Euclidean Spaces}.
\newblock {\em Journal of Applied probability}, 19(1):221--228, 1982.

\bibitem{rader1969new}
Charles~M Rader.
\newblock {A New Method of Generating Gaussian Random Variables by Computer}.
\newblock Technical report, Massachusetts Institute of Technology, Lincoln Lab,
  1969.

\bibitem{thomas2013parallel}
David~B Thomas.
\newblock {Parallel Generation of Gaussian Random Numbers Using the
  Table-Hadamard Transform}.
\newblock In {\em 2013 IEEE 21st Annual International Symposium on
  Field-Programmable Custom Computing Machines}, pages 161--168. IEEE, 2013.

\bibitem{herendi1997fast}
Tam{\'a}s Herendi, Thomas Siegl, and Robert~F Tichy.
\newblock {Fast Gaussian Random Number Generation Using Linear
  Transformations}.
\newblock {\em Computing}, 59(2):163--181, 1997.

\bibitem{NIPS2019_9015}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock {PyTorch: An Imperative Style, High-Performance Deep Learning
  Library}.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems 32}, pages 8026--8037. Curran Associates,
  Inc., 2019.

\bibitem{tensorflowfed}
Google.
\newblock {TensorFlow Federated: Machine Learning on Decentralized Data}.
\newblock \url{https://www.tensorflow.org/federated}, 2020.
\newblock accessed 25-Mar-20.

\bibitem{konecy2017federated}
Jakub Konečný, H.~Brendan McMahan, Felix~X. Yu, Peter Richtárik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock {Federated Learning: Strategies for Improving Communication
  Efficiency}, 2017.

\bibitem{WangSLCPW18}
Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary~B. Charles, Dimitris~S.
  Papailiopoulos, and Stephen Wright.
\newblock {ATOMO: Communication-efficient Learning via Atomic Sparsification}.
\newblock In {\em NeurIPS}, pages 9872--9883, 2018.

\bibitem{NEURIPS2019_d9fbed9d}
Thijs Vogels, Sai~Praneeth Karimireddy, and Martin Jaggi.
\newblock {PowerSGD: Practical Low-Rank Gradient Compression for Distributed
  Optimization}.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32, 2019.

\bibitem{NEURIPS2018_b440509a}
Sebastian~U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi.
\newblock {Sparsified SGD with Memory}.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock {Gradient-Based Learning Applied to Document Recognition}.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lecun2010mnist}
Yann LeCun, Corinna Cortes, and CJ~Burges.
\newblock Mnist handwritten digit database.
\newblock {\em ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  2, 2010.

\bibitem{cohen2017emnist}
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van~Schaik.
\newblock {EMNIST: Extending MNIST to Handwritten Letters}.
\newblock In {\em 2017 International Joint Conference on Neural Networks
  (IJCNN)}, pages 2921--2926. IEEE, 2017.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock {Learning Multiple Layers of Features From Tiny Images}.
\newblock 2009.

\bibitem{shakespeare}
William Shakespeare.
\newblock {The Complete Works of William Shakespeare}.
\newblock \url{https://www.gutenberg.org/ebooks/100}.

\bibitem{stackoverflowdb}
{Stack Overflow Data}.
\newblock \url{https://www.kaggle.com/stackoverflow/stackoverflow}.
\newblock accessed 01-Mar-21.

\bibitem{ivkin2019communication}
Nikita Ivkin, Daniel Rothchild, Enayat Ullah, Vladimir Braverman, Ion Stoica,
  and Raman Arora.
\newblock {Communication-Efficient Distributed Sgd With Sketching}.
\newblock {\em arXiv preprint arXiv:1903.04488}, 2019.

\bibitem{rothchild2020fetchsgd}
Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica,
  Vladimir Braverman, Joseph Gonzalez, and Raman Arora.
\newblock {FetchSGD: Communication-Efficient Federated Learning With
  Sketching}.
\newblock In {\em International Conference on Machine Learning}, pages
  8253--8265. PMLR, 2020.

\bibitem{charikar2002finding}
Moses Charikar, Kevin Chen, and Martin Farach-Colton.
\newblock Finding frequent items in data streams.
\newblock In {\em International Colloquium on Automata, Languages, and
  Programming}, pages 693--703. Springer, 2002.

\bibitem{tensorflowfedkashincode}
The~TensorFlow Authors.
\newblock {TensorFlow Federated: Compression via Kashin's representation from
  Hadamard transform}.
\newblock
  \url{https://github.com/tensorflow/model-optimization/blob/9193d70f6e7c9f78f7c63336bd68620c4bc6c2ca/tensorflow_model_optimization/python/core/internal/tensor_encoding/stages/research/kashin.py#L92}.
\newblock accessed 16-May-21.

\bibitem{caldas2019leaf}
Sebastian Caldas, Sai Meher~Karthik Duddu, Peter Wu, Tian Li, Jakub Konečný,
  H.~Brendan McMahan, Virginia Smith, and Ameet Talwalkar.
\newblock {LEAF: A Benchmark for Federated Settings}, 2019.

\bibitem{reddi2020adaptive}
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Konečný, Sanjiv Kumar, and H.~Brendan McMahan.
\newblock Adaptive {F}ederated {O}ptimization, 2020.

\bibitem{Hochreiter1997LongSM}
S.~Hochreiter and J.~Schmidhuber.
\newblock {Long Short-Term Memory}.
\newblock {\em Neural Computation}, 9:1735--1780, 1997.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock {Deep Residual Learning for Image Recognition}.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{seide20141}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock {1-Bit Stochastic Gradient Descent and Its Application to
  Data-Parallel Distributed Training of Speech DNNs}.
\newblock In {\em Fifteenth Annual Conference of the International Speech
  Communication Association}, 2014.

\bibitem{karimireddy2019error}
Sai~Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi.
\newblock {Error Feedback Fixes SignSGD and other Gradient Compression
  Schemes}.
\newblock In {\em International Conference on Machine Learning}, pages
  3252--3261, 2019.

\bibitem{vargaftik2021communication}
Shay Vargaftik, Ran~Ben Basat, Amit Portnoy, Gal Mendelson, Yaniv Ben-Itzhak,
  and Michael Mitzenmacher.
\newblock Communication-efficient federated learning via robust distributed
  mean estimation.
\newblock {\em arXiv preprint arXiv:2108.08842}, 2021.

\bibitem{openSource}
Shay Vargaftik, Ran~Ben Basat, Amit Portnoy, Gal Mendelson, Yaniv Ben-Itzhak,
  and Michael Mitzenmacher.
\newblock {DRIVE open source code}.
\newblock
  \url{https://github.com/amitport/DRIVE-One-bit-Distributed-Mean-Estimation},
  2021.

\bibitem{muller1959note}
Mervin~E Muller.
\newblock {A Note on a Method for Generating Points Uniformly on N-Dimensional
  Spheres}.
\newblock {\em Communications of the ACM}, 2(4):19--20, 1959.

\bibitem{bennett1962probability}
George Bennett.
\newblock {Probability Inequalities for the Sum of Independent Random
  Variables}.
\newblock {\em Journal of the American Statistical Association},
  57(297):33--45, 1962.

\bibitem{maurer2003bound}
Andreas Maurer et~al.
\newblock {A Bound on the Deviation Probability for Sums of Non-Negative Random
  Variables}.
\newblock {\em J. Inequalities in Pure and Applied Mathematics}, 4(1):15, 2003.

\bibitem{szablowski1998uniform}
Pawe{\l}~J Szab{\l}owski.
\newblock {Uniform Distributions on Spheres in Finite Dimensional $L_\alpha$
  and Their Generalizations}.
\newblock {\em Journal of multivariate analysis}, 64(2):103--117, 1998.

\bibitem{esseen1956moment}
Carl-Gustav Esseen.
\newblock {A Moment Inequality With an Application to the Central Limit
  Theorem}.
\newblock {\em Scandinavian Actuarial Journal}, 1956(2):160--170, 1956.

\bibitem{tensorflowmo}
Google.
\newblock {TensorFlow Model Optimization Toolkit}.
\newblock \url{https://www.tensorflow.org/model_optimization}, 2021.
\newblock accessed 19-Mar-21.

\bibitem{apache2}
Apache license, version 2.0.
\newblock \url{https://www.apache.org/licenses/LICENSE-2.0}.
\newblock accessed 19-Mar-21.

\bibitem{pytorchlicense}
Pytorch license.
\newblock
  \url{https://github.com/pytorch/pytorch/blob/aaccdc39965ade4b61b7852329739e777e244c25/LICENSE}.
\newblock accessed 19-Mar-21.

\bibitem{CCASA3}
{Creative Commons Attribution-Share Alike 3.0 license.}
\newblock \url{https://creativecommons.org/licenses/by-sa/3.0/}.
\newblock accessed 01-Mar-21.

\bibitem{tensorflowfedencsum}
The~TensorFlow Authors.
\newblock {TensorFlow Federated: EncodedSumFactory class}.
\newblock
  \url{https://github.com/tensorflow/federated/blob/v0.19.0/tensorflow_federated/python/aggregators/encoded.py#L40-L142}.
\newblock accessed 19-May-21.

\end{thebibliography}
