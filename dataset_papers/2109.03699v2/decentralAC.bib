@inproceedings{ma2021,
	title={Greedy-{GQ} with Variance Reduction: Finite-time Analysis and Improved Complexity},
	author={Ma Shaocong and Chen Ziyi and Zhou Yi and Zou Shaofeng},
	booktitle={Proc. International Conference on Learning Representations (ICLR)},
	year={2021},
}

@InProceedings{ma20a,
	title = 	 {Variance-Reduced Off-Policy TDC Learning: Non-Asymptotic Convergence Analysis},
	author =       {Ma Shaocong and Zhou Yi and Zou Shaofeng},
	booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	year = 	 {2020}
}

@InProceedings{Doan19a,
	title = 	 {Finite-Time Analysis of Distributed {TD}(0) with Linear Function Approximation on Multi-Agent Reinforcement Learning},
	author={Doan, Thinh and Maguluri, Siva and Romberg, Justin},
	booktitle={Proc. International Conference on Machine Learning (ICML)},
	pages = 	 {1626--1635},
	year = 	 {2019},
	volume = 	 {97},
	month = 	 {09--15 Jun}
}

@inproceedings{Wai2018,
	title = {Multi-Agent Reinforcement Learning via Double Averaging Primal-Dual Optimization},
    author={Wai, Hoi-To and Yang, Zhuoran and Wang, Zhaoran and Hong, Mingyi},
	year = {2018},
	booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	pages = {9672--9683},
	numpages = {12},
}


@inproceedings{zou2019finite,
	title={Finite-sample analysis for sarsa with linear function approximation},
	author={Zou, Shaofeng and Xu, Tengyu and Liang, Yingbin},
	booktitle={Proc. Advances in Neural Information Processing Systems},
	pages={8665--8675},
	year={2019},
}

@inproceedings{lin2019communication,
	title={A communication-efficient multi-agent actor-critic algorithm for distributed reinforcement learning},
	author={Lin, Yixuan and Zhang, Kaiqing and Yang, Zhuoran and Wang, Zhaoran and Ba{\c{s}}ar, Tamer and Sandhu, Romeil and Liu, Ji},
	booktitle={2019 IEEE 58th Conference on Decision and Control (CDC)},
	pages={5562--5567},
	year={2019},
}

@inproceedings{sutton2008convergent,
	title={A convergent O(n) algorithm for off-policy temporal-difference learning with linear function approximation},
	author={Sutton, Richard S and Szepesv{\'a}ri, Csaba and Maei, Hamid Reza},
	booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	volume={21},
	pages={1609--1616},
	year={2008},
}

@article{hong2020two,
	title={A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic},
	author={Hong, Mingyi and Wai, Hoi-To and Wang, Zhaoran and Yang, Zhuoran},
	journal={ArXiv:2007.05170},
	year={2020},
}

@inproceedings{amato2009achieving,
	title={Achieving goals in decentralized POMDPs},
	author={Amato, Christopher and Zilberstein, Shlomo},
	booktitle={Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems-Volume 1},
	pages={593--600},
	year={2009},
}

@inproceedings{konda2000actor,
	title={Actor-critic algorithms},
	author={Konda, Vijay R and Tsitsiklis, John N},
	booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	pages={1008--1014},
	year={2000},
}

@article{konda2002actor,
  title={Actor-critic algorithms (Ph.D. thesis)},
  author={Konda, V},
  journal={Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology},
  year={2002},
}

@inproceedings{perolat2018actor,
	title={Actor-critic fictitious play in simultaneous move multistage games},
	author={Perolat, Julien and Piot, Bilal and Pietquin, Olivier},
	booktitle={Proc. International Conference on Artificial Intelligence and Statistics},
	pages={919--928},
	year={2018},
}

@article{bhatnagar2010actor,
  title={An actor--critic algorithm with function approximation for discounted cost constrained Markov decision processes},
  author={Bhatnagar, Shalabh},
  journal={Systems \& Control Letters},
  volume={59},
  number={12},
  pages={760--766},
  year={2010},
}

@inproceedings{lin2019asynchronous,
	title={An Asynchronous Multi-Agent Actor-Critic Algorithm for Distributed Reinforcement Learning},
	author={Lin, Yixuan and Luo, Yuehan and Zhang, Kaiqing and Yang, Zhuoran and Wang, Zhaoran and Basar, Tamer and Sandhu, Romeil and Liu, Ji},
	booktitle={NeurIPS Optimization Foundations for Reinforcement Learning Workshop},
	year={2019}
}

@inproceedings{Liu2020An,
	author = {Liu, Yanli and Zhang, Kaiqing and Basar, Tamer and Yin, Wotao},
	booktitle = {Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	pages = {7624--7636},
	title = {An Improved Analysis of  (Variance-Reduced) Policy Gradient and Natural Policy Gradient Methods},
	volume = {33},
	year = {2020}
}

@article{yang2020overview,
	title={An Overview of Multi-Agent Reinforcement Learning from Game Theoretical Perspective},
	author={Yang, Yaodong and Wang, Jun},
	journal={ArXiv:2011.00583},
	year={2020}
}

@article{shen2020asynchronous,
	title={Asynchronous Advantage Actor Critic: Non-asymptotic Analysis and Linear Speedup},
	author={Shen, Han and Zhang, Kaiqing and Hong, Mingyi and Chen, Tianyi},
	journal={ArXiv:2012.15511},
	year={2020}
}

@inproceedings{mnih2016asynchronous,
	title={Asynchronous methods for deep reinforcement learning},
	author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
	booktitle={Proc. International Conference on Machine Learning (ICML)},
	pages={1928--1937},
	year={2016}
}

@phdthesis{maei2011gradient,
	title={Gradient temporal-difference learning algorithms},
	school= {University of Alberta},
    author={Maei, Hamid Reza},
	year= {2011}
}

@article{Yan2013,
	title ={A Survey and Analysis of Multi-Robot Coordination},
    author={Yan, Zhi and Jouandeau, Nicolas and Cherif, Arab Ali},
	journal = {International Journal of Advanced Robotic Systems},
	volume = {10},
	number = {12},
	pages = {399},
	year = {2013},
}

@InProceedings{Yanmaz2017,
	title={Communication and Coordination for Drone Networks},
	author={Yanmaz, Ev{\c{s}}en
	and Quaritsch, Markus
	and Yahyanejad, Saeed
	and Rinner, Bernhard
	and Hellwagner, Hermann
	and Bettstetter, Christian},
	booktitle={Proc. International Conference on Ad Hoc Networks},
	pages={79--91},
	year={2017}
}

@article{pennesi2010distributed,
	title={A distributed actor-critic algorithm and applications to mobile sensor network coordination problems},
	author={Pennesi, Paris and Paschalidis, Ioannis Ch},
	journal={IEEE Transactions on Automatic Control},
	volume={55},
	number={2},
	pages={492--497},
	year={2010}
}

@inproceedings{yang2018finite,
	title={A finite sample analysis of the actor-critic algorithm},
	author={Yang, Zhuoran and Zhang, Kaiqing and Hong, Mingyi and Ba{\c{s}}ar, Tamer},
	booktitle={2018 IEEE Conference on Decision and Control (CDC)},
	pages={2759--2764},
	year={2018}
}

@InProceedings{bhandari2018finite,
	title={A finite time analysis of temporal difference learning with linear function approximation},
	author={Bhandari, Jalaj and Russo, Daniel and Singal, Raghav},
	booktitle={Proc. Conference on Learning Theory (COLT)},
	volume={75},
	pages={1691--1692},
	year={2018}
}

@InProceedings{wu2020finite,
	title = {A Finite-Time Analysis of Two Time-Scale Actor-Critic Methods},
    author = {Wu, Yue Frank and ZHANG, Weitong and Xu, Pan and Gu, Quanquan},
	booktitle = {Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	pages = {17617--17628},
	volume = {33},
	year = {2020}
}

@article{silver2018general,
	title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
	author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
	journal={Science},
	volume={362},
	number={6419},
	pages={1140--1144},
	year={2018}
}

@article{chalaki2020hysteretic,
	title={A Hysteretic Q-learning Coordination Framework for Emerging Mobility Systems in Smart Cities},
	author={Chalaki, Behdad and Malikopoulos, Andreas A},
	journal={ArXiv:2011.03137},
	year={2020}
}

@article{suttle2019multi,
	title={A multi-agent off-policy actor-critic algorithm for distributed reinforcement learning},
	author={Suttle, Wesley and Yang, Zhuoran and Zhang, Kaiqing and Wang, Zhaoran and Basar, Tamer and Liu, Ji},
	journal={ArXiv:1903.06372},
	year={2019}
}

@article{wang2019multistep,
	title={A multistep Lyapunov approach for finite-time analysis of biased stochastic approximation},
	author={Wang, Gang and Li, Bingcong and Giannakis, Georgios B},
	journal={ArXiv:1909.04299},
	year={2019}
}

@inproceedings{kakade2001natural,
  title={A natural policy gradient},
  author={Kakade, Sham M},
  booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
  volume={14},
  year={2001}
}

@article{xu2020primal,
	title={A Primal Approach to Constrained Policy Optimization: Global Optimality and Finite-Time Analysis},
	author={Xu, Tengyu and Liang, Yingbin and Lan, Guanghui},
	journal={ArXiv:2011.05869},
	year={2020}
}

@inproceedings{Dalal_2020, 
	title={A Tale of Two-Timescale Reinforcement Learning with the Tightest Finite-Time Bound}, 
	author={Dalal, Gal and Szorenyi, Balazs and Thoppe, Gugan}, 
	booktitle={Proc. Association for the Advancement of Artificial Intelligence (AAAI)},
	year={2020}, 
	volume={34}, 
	pages={3701-3708} 
}

@inproceedings{fan2020theoretical,
	title={A theoretical analysis of deep Q-learning},
	author={Fan, Jianqing and Wang, Zhaoran and Xie, Yuchen and Yang, Zhuoran},
	booktitle={Proc. Learning for Dynamics and Control (L4DC)},
	pages={486--489},
	year={2020}
}

@inproceedings{wang2019achieving,
	title={Achieving cooperation through deep multiagent reinforcement learning in sequential prisoner's dilemmas},
	author={Wang, Weixun and Hao, Jianye and Wang, Yixi and Taylor, Matthew},
	booktitle={Proc. of International Conference on Distributed Artificial Intelligence (DAI)},
	pages={1--7},
	year={2019}
}

@article{srinivasan2018actor,
	title={Actor-critic policy optimization in partially observable multiagent environments},
	author={Srinivasan, Sriram and Lanctot, Marc and Zambaldi, Vinicius and P{\'e}rolat, Julien and Tuyls, Karl and Munos, R{\'e}mi and Bowling, Michael},
	journal={ArXiv:1810.09026},
	year={2018}
}

@article{tsitsiklis1997analysis,
	title={An analysis of temporal-difference learning with function approximation},
	author={Tsitsiklis, John N and Van Roy, Benjamin},
	journal={IEEE transactions on automatic control},
	volume={42},
	number={5},
	pages={674--690},
	year={1997}
}

@inproceedings{abbeel2007application,
	title={An application of reinforcement learning to aerobatic helicopter flight},
	author={Abbeel, Pieter and Coates, Adam and Quigley, Morgan and Ng, Andrew Y},
	booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	pages={1--8},
	year={2007}
}

@article{xin2019introduction,
	title={An introduction to decentralized stochastic optimization with gradient tracking},
	author={Xin, Ran and Kar, Soummya and Khan, Usman A},
	journal={ArXiv:1907.09648},
	year={2019}
}

@article{wang2005application,
	title={Application of reinforcement learning for agent-based production scheduling},
	author={Wang, Yi-Chi and Usher, John M},
	journal={Engineering Applications of Artificial Intelligence},
	volume={18},
	number={1},
	pages={73--82},
	year={2005}
}

@inproceedings{wang2020breaking,
	title={Breaking the Curse of Many Agents: Provable Mean Embedding Q-Iteration for Mean-Field Reinforcement Learning},
	author={Wang, Lingxiao and Yang, Zhuoran and Wang, Zhaoran},
	booktitle={Proc. International Conference on Machine Learning (ICML)},
	pages={10092--10103},
	year={2020}
}

@inproceedings{hu2019characterizing,
	title={Characterizing the exact behaviors of temporal difference learning algorithms using Markov jump linear system theory},
    author={Hu, Bin and Syed, Usman Ahmed},
	booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	pages={8479--8490},
	year={2019}
}

@article{lyu2021contrasting,
	title={Contrasting Centralized and Decentralized Critics in Multi-Agent Reinforcement Learning},
	author={Lyu, Xueguang and Xiao, Yuchen and Daley, Brett and Amato, Christopher},
	journal={ArXiv:2102.04402},
	year={2021}
}

@article{maei2018convergent,
	title={Convergent actor-critic algorithms under off-policy training and function approximation},
	author={Maei, Hamid Reza},
	journal={ArXiv:1802.07842},
	year={2018}
}

@inproceedings{Jaakkola1993,
	title = {Convergence of Stochastic Iterative Dynamic Programming Algorithms},
	author = {Jaakkola, Tommi and Jordan, Michael and Singh, Satinder},
	booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	pages = {703--710},
	volume = {6},
	year = {1993}
}

@article{ogren2004cooperative,
	title={Cooperative control of mobile sensor networks: Adaptive gradient climbing in a distributed environment},
	author={Ogren, Petter and Fiorelli, Edward and Leonard, Naomi Ehrich},
	journal={IEEE Transactions on Automatic control},
	volume={49},
	number={8},
	pages={1292--1302},
	year={2004}
}

@inproceedings{bono2018cooperative,
	title={Cooperative multi-agent policy gradient},
	author={Bono, Guillaume and Dibangoye, Jilles Steeve and Matignon, La{\"e}titia and Pereyron, Florian and Simonin, Olivier},
	booktitle={Proc. Joint European Conference on Machine Learning and Knowledge Discovery in Databases (ECML PKDD)},
	pages={459--476},
	year={2018}
}

@article{zhang2020cooperative,
	title={Cooperative Multi-Agent Reinforcement Learning with Partial Observations},
	author={Zhang, Yan and Zavlanos, Michael M},
	journal={ArXiv:2006.10822},
	year={2020}
}

@inproceedings{foerster2018counterfactual,
	title={Counterfactual multi-agent policy gradients},
	author={Foerster, Jakob and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
	booktitle={Proc. Association for the Advancement of Artificial Intelligence (AAAI)},
	volume={32},
	year={2018}
}

@article{cortes2004coverage,
	title={Coverage control for mobile sensing networks},
	author={Cortes, Jorge and Martinez, Sonia and Karatas, Timur and Bullo, Francesco},
	journal={IEEE Transactions on robotics and Automation},
	volume={20},
	number={2},
	pages={243--255},
	year={2004}
}

@article{zhang2016data,
	title={Data-driven optimal consensus control for discrete-time multi-agent systems with unknown dynamics using reinforcement learning method},
	author={Zhang, Huaguang and Jiang, He and Luo, Yanhong and Xiao, Geyang},
	journal={IEEE Transactions on Industrial Electronics},
	volume={64},
	number={5},
	pages={4091--4100},
	year={2016},
	publisher={IEEE}
}

@article{Krishnamurthy2008,
	title={Decentralized Adaptive Filtering Algorithms for Sensor Activation in an Unattended Ground Sensor Network}, 
	author={Krishnamurthy, Vikram and Maskery, Michael and Yin, George},
	journal={IEEE Transactions on Signal Processing}, 
	year={2008},
	volume={56},
	number={12},
	pages={6086-6101}
}

@article{saha2020decentralized,
	title={Decentralized optimization over noisy, rate-constrained networks: How to agree by talking about how we disagree},
	author={Saha, Rajarshi and Rini, Stefano and Rao, Milind and Goldsmith, Andrea},
	journal={ArXiv:2010.11292},
	year={2020}
}

@inproceedings{lu2021decentralized,
	title={Decentralized Policy Gradient Descent Ascent for Safe Multi-Agent Reinforcement Learning},
	author={Lu, Songtao and Zhang, Kaiqing and Chen, Tianyi and Basar, Tamer and Horesh, Lior},
	booktitle={Proc. Association for the Advancement of Artificial Intelligence (AAAI)},
	year={2021}
}

@inproceedings{wang2020decentralized,
	title={Decentralized TD Tracking with Linear Function Approximation and its Finite-Time Analysis},
	author={Wang, Gang and Lu, Songtao and Giannakis, Georgios and Tesauro, Gerald and Sun, Jian},
	booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	volume={33},
	year={2020}
}

@article{chen2020delay,
	title={Delay-aware multi-agent reinforcement learning},
	author={Chen, Baiming and Xu, Mengdi and Liu, Zuxin and Li, Liang and Zhao, Ding},
	journal={ArXiv:2005.05441},
	year={2020}
}

@article{macua2017diff,
	title={Diff-dac: Distributed actor-critic for average multitask deep reinforcement learning},
	author={Macua, Sergio Valcarcel and Tukiainen, Aleksi and Hern{\'a}ndez, Daniel Garc{\'\i}a-Oca{\~n}a and Baldazo, David and de Cote, Enrique Munoz and Zazo, Santiago},
	journal={ArXiv:1710.10363},
	year={2017}
}

@article{alfano2021dimension,
  title={Dimension-Free Rates for Natural Policy Gradient in Multi-Agent Reinforcement Learning},
  author={Alfano, Carlo and Rebeschini, Patrick},
  journal={ArXiv:2109.11692},
  year={2021}
}

@article{chang2020distributed,
	title={Distributed Learning in the Nonconvex World: From batch data to streaming and beyond},
	author={Chang, Tsung-Hui and Hong, Mingyi and Wai, Hoi-To and Zhang, Xinwei and Lu, Songtao},
	journal={IEEE Signal Processing Magazine},
	volume={37},
	number={3},
	pages={26--38},
	year={2020}
}

@inproceedings{zhang2019distributed,
	title={Distributed off-policy actor-critic reinforcement learning with policy consensus},
	author={Zhang, Yan and Zavlanos, Michael M},
	booktitle={Proc, Conference on Decision and Control (CDC)},
	pages={4674--4679},
	year={2019}
}

@article{macua2014distributed,
	title={Distributed policy evaluation under multiple behavior strategies},
	author={Macua, Sergio Valcarcel and Chen, Jianshu and Zazo, Santiago and Sayed, Ali H},
	journal={IEEE Transactions on Automatic Control},
	volume={60},
	number={5},
	pages={1260--1274},
	year={2014}
}

@article{heredia2019distributed,
	title={Distributed multi-agent reinforcement learning by actor-critic method},
	author={Heredia, Paulo C and Mou, Shaoshuai},
	journal={IFAC-PapersOnLine},
	volume={52},
	number={20},
	pages={363--368},
	year={2019}
}

@article{venturini2021distributed,
	title={Distributed Reinforcement Learning for Flexible and Efficient UAV Swarm Control},
	author={Venturini, Federico and Mason, Federico and Pase, Francesco and Chiariotti, Federico and Testolin, Alberto and Zanella, Andrea and Zorzi, Michele},
	journal={ArXiv:2103.04666},
	year={2021}
}

@article{mathkar2016distributed,
	title={Distributed reinforcement learning via gossip},
	author={Mathkar, Adwaitvedant and Borkar, Vivek S},
	journal={IEEE Transactions on Automatic Control},
	volume={62},
	number={3},
	pages={1465--1470},
	year={2016}
}

@article{pu2020distributed,
	title={Distributed stochastic gradient tracking methods},
	author={Pu, Shi and Nedi{\'c}, Angelia},
	journal={Mathematical Programming},
	pages={1--49},
	year={2020}
}

@article{liu2021distributed,
  title={Distributed TD (0) with Almost No Communication},
  author={Liu, Rui and Olshevsky, Alex},
  journal={ArXiv:2104.07855},
  year={2021}
}

@article{stankovic2020distributed,
	title={Distributed Value Function Approximation for Collaborative Multi-Agent Reinforcement Learning},
	author={Stankovic, Milos S and Beko, Marko and Stankovic, Srdjan S},
	journal={ArXiv:2006.10443},
	year={2020}
}

@article{xu2021doubly,
  title={Doubly Robust Off-Policy Actor-Critic: Convergence and Optimality},
  author={Xu, Tengyu and Yang, Zhuoran and Wang, Zhaoran and Liang, Yingbin},
  journal={ArXiv:2102.11866},
  year={2021}
}

@article{ornia2021event,
  title={Event-Based Communication in Multi-Agent Distributed Q-Learning},
  author={Ornia, Daniel Jarne and Mazo Jr, Manuel},
  journal={ArXiv:2109.01417},
  year={2021}
}

@inproceedings{sutton2009fast,
	title={Fast gradient-descent methods for temporal-difference learning with linear function approximation},
    author={Sutton, Richard S and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv{\'a}ri, Csaba and Wiewiora, Eric},
	booktitle={Proc. International Conference on Machine Learning (ICML)},
    year={2009},
	pages={993--1000}
}

@inproceedings{Sutton-PG,
    author = {Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
	booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
    title = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
    volume = {12},
    year = {2000},
}

@inproceedings{sun2020finite,
	title={Finite-Sample Analysis of Decentralized Temporal-Difference Learning with Linear Function Approximation},
    author={Sun, Jun and Wang, Gang and Giannakis, Georgios B and Yang, Qinmin and Yang, Zaiyue},
	booktitle={Proc. International Conference on Artificial Intelligence and Statistics (AISTATS)},
	pages={4485--4495},
	year={2020}
}

@article{zhang2018finite,
	title={Finite-Sample Analysis For Decentralized Batch Multi-Agent Reinforcement Learning With Networked Agents},
	author={Zhang, Kaiqing and Yang, Zhuoran and Liu, Han and Zhang, Tong and Basar, Tamer},
	journal={IEEE Transactions on Automatic Control (TAC)},
	year={2021}
}

@inproceedings{dalal2018finite,
	title={Finite sample analyses for TD (0) with function approximation},
	author={Dalal, Gal and Sz{\"o}r{\'e}nyi, Bal{\'a}zs and Thoppe, Gugan and Mannor, Shie},
	booktitle={Proc. Association for the Advancement of Artificial Intelligence (AAAI)},
	volume={32},
	year={2018}
}

@inproceedings{heredia2020finite,
	title={Finite-Sample Analysis of Distributed Q-learning for Multi-Agent Networks},
	author={Heredia, Paulo and Ghadialy, Hasan and Mou, Shaoshuai},
	booktitle={2020 American Control Conference (ACC)},
	pages={3511--3516},
	year={2020}
}

@article{khodadadian2021finite,
	title={Finite Sample Analysis of Two-Time-Scale Natural Actor-Critic Algorithm},
	author={Khodadadian, Sajad and Doan, Thinh T and Maguluri, Siva Theja and Romberg, Justin},
	journal={ArXiv:2101.10506},
	year={2021}
}

@inproceedings{2018Finite,
	title={Finite Sample Analysis of Two-Timescale Stochastic Approximation with Applications to Reinforcement Learning},
	author={Dalal, Gal  and  Szorenyi, Balazs  and  Thoppe, Gugan  and  Mannor, Shie },
	year={2018},
	booktitle={Proc. Conference on Learning Theory (COLT)}
}

@inproceedings{liu2015finite,
	title={Finite-Sample Analysis of Proximal Gradient TD Algorithms},
	author={Liu, Bo and Liu, Ji and Ghavamzadeh, Mohammad and Mahadevan, Sridhar and Petrik, Marek},
	booktitle={Proc. Conference on Uncertainty in Artificial Intelligence (UAI)},
	pages={504--513},
	year={2015}
}

@inproceedings{kaledin2020finite,
	title={Finite time analysis of linear two-timescale stochastic approximation with Markovian noise},
	author={Kaledin, Maxim and Moulines, Eric and Naumov, Alexey and Tadic, Vladislav and Wai, Hoi-To},	
	booktitle={Proc. Conference on Learning Theory (COLT)},
	pages={2144--2203},
	year={2020}
}

@inproceedings{srikant2019finite,
	title={Finite-time error bounds for linear stochastic approximation andtd learning},
	author={Srikant, Rayadurgam and Ying, Lei},
	booktitle={Proc. Conference on Learning Theory (COLT)},
	pages={2803--2830},
	year={2019},
}

@inproceedings{Gupta2019finite,
	title = {Finite-Time Performance Bounds and Adaptive Learning Rate Selection for Two Time-Scale Reinforcement Learning},
	author = {Gupta, Harsh and Srikant, R. and Ying, Lei},
	booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	pages = {4704--4713},
	volume = {32},
	year = {2019}
}

@inproceedings{zhang2018fully,
	title={Fully decentralized multi-agent reinforcement learning with networked agents},
	author={Zhang, Kaiqing and Yang, Zhuoran and Liu, Han and Zhang, Tong and Basar, Tamer},
	booktitle={Proc. International Conference on Machine Learning (ICML)},
	pages={5872--5881},
	year={2018}
}

@inproceedings{thomas2015high,
	title={High-confidence off-policy evaluation},
	author={Thomas, Philip and Theocharous, Georgios and Ghavamzadeh, Mohammad},    
	booktitle={Proc. Association for the Advancement of Artificial Intelligence (AAAI)},
	volume={29},
	number={1},
	year={2015}
}

@inproceedings{xu2020improving,
	title={Improving Sample Complexity Bounds for (Natural) Actor-Critic Algorithms},
	author={Xu, Tengyu and Wang, Zhe and Liang, Yingbin},
	booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	volume={33},
	year={2020}
}

@inproceedings{bhatnagar2007incremental,
  title={Incremental natural actor-critic algorithms},
  author={Bhatnagar, Shalabh and Ghavamzadeh, Mohammad and Lee, Mark and Sutton, Richard S},
  booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
  volume={20},
  pages={105--112},
  year={2007}
}

@article{daskalakis2021independent,
	title={Independent policy gradient methods for competitive reinforcement learning},
	author={Daskalakis, Constantinos and Foster, Dylan J and Golowich, Noah},
	journal={ArXiv:2101.04233},
	year={2021}
}

@article{chen2021infinite,
  title={Infinite-Horizon Offline Reinforcement Learning with Linear Function Approximation: Curse of Dimensionality and Algorithm},
  author={Chen, Lin and Scherrer, Bruno and Bartlett, Peter L},
  journal={ArXiv:2103.09847},
  year={2021}
}

@article{zhang2021intelligent,
	title={Intelligent Electric Vehicle Charging Recommendation Based on Multi-Agent Reinforcement Learning},
	author={Zhang, Weijia and Liu, Hao and Wang, Fan and Xu, Tong and Xin, Haoran and Dou, Dejing and Xiong, Hui},
	journal={ArXiv:2102.07359},
	year={2021}
}

@article{bai2021joint,
  title={Joint Optimization of Multi-Objective Reinforcement Learning with Policy Gradient Based Algorithm},
  author={Bai, Qinbo and Agarwal, Mridul and Aggarwal, Vaneet},
  journal={ArXiv:2105.14125},
  year={2021}
}


@inproceedings{dibangoye2018learning,
	title={Learning to act in decentralized partially observable MDPs},
	author={Dibangoye, Jilles and Buffet, Olivier},
	booktitle={Proc. International Conference on Machine Learning (ICML)},
	pages={1233--1242},
	year={2018},
	organization={PMLR}
}

@article{sutton1988learning,
	title={Learning to predict by the methods of temporal differences},
	author={Sutton, Richard S},
	journal={Machine learning},
	volume={3},
	number={1},
	pages={9--44},
	year={1988}
}

@inproceedings{lakshminarayanan2018linear,
	title={Linear stochastic approximation: How far does constant step-size and iterate averaging go?},
	author={Lakshminarayanan, Chandrashekar and Szepesvari, Csaba},
	booktitle={Proc. International Conference on Artificial Intelligence and Statistics (AISTATS)},
	pages={1347--1355},
	year={2018}
}

@article{ghosh2020model,
  title={Model Free Reinforcement Learning Algorithm for Stationary Mean field Equilibrium for Multiple Types of Agents},
  author={Ghosh, Arnob and Aggarwal, Vaneet},
  journal={ArXiv:2012.15377},
  year={2020}
}

@article{ma2021modeling,
	title={Modeling the Interaction between Agents in Cooperative Multi-Agent Reinforcement Learning},
	author={Ma, Xiaoteng and Yang, Yiqin and Li, Chenghao and Lu, Yiwen and Zhao, Qianchuan and Jun, Yang},
	journal={ArXiv:2102.06042},
	year={2021}
}

@article{lowe2017multi,
	title={Multi-agent actor-critic for mixed cooperative-competitive environments},
	author={Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
	journal={ArXiv:1706.02275},
	year={2017}
}

@article{cassano2020multi,
	title={Multi-Agent Fully Decentralized Value Function Learning with Linear Convergence Rates},
	author={Cassano, Lucas and Yuan, Kun and Sayed, Ali H},
	journal={IEEE Transactions on Automatic Control},
	year={2020}
}

@article{DTDC,
  title={Multi-Agent Off-Policy TD Learning: Finite-Time Analysis with Near-Optimal Sample Complexity and Communication Complexity},
  author={Chen, Ziyi and Zhou, Yi and Chen, Rongrong},
  journal={ArXiv:2103.13147},
  year={2021}
}

@article{zhang2019multi,
	title={Multi-agent reinforcement learning: A selective overview of theories and algorithms},
	author={Zhang, Kaiqing and Yang, Zhuoran and Ba{\c{s}}ar, Tamer},
	journal={ArXiv:1911.10635},
	year={2019}
}

@article{wei2018multiagent,
	title={Multiagent soft q-learning},
	author={Wei, Ermo and Wicke, Drew and Freelan, David and Luke, Sean},
	journal={ArXiv:1804.09817},
	year={2018}
}

@article{hammond2021multi,
	title={Multi-Agent Reinforcement Learning with Temporal Logic Specifications},
	author={Hammond, Lewis and Abate, Alessandro and Gutierrez, Julian and Wooldridge, Michael},
	journal={ArXiv:2102.00582},
	year={2021}
}

@inproceedings{stankovic2016multi,
	title={Multi-agent temporal-difference learning with linear function approximation: Weak convergence under time-varying network topologies},
	author={Stankovi{\'c}, Milo{\v{s}} S and Stankovi{\'c}, Srdjan S},
	booktitle={Proc. American Control Conference (ACC)},
	pages={167--172},
	year={2016}
}

@article{peters2008natural,
  title={Natural actor-critic},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1180--1190},
  year={2008}
}

@article{bhatnagar2009natural,
	title={Natural actor--critic algorithms},
	author={Bhatnagar, Shalabh and Sutton, Richard S and Ghavamzadeh, Mohammad and Lee, Mark},
	journal={Automatica},
	volume={45},
	number={11},
	pages={2471--2482},
	year={2009}

}
@article{luo2019natural,
	title={Natural actor-critic converges globally for hierarchical linear quadratic regulator},
	author={Luo, Yuwei and Yang, Zhuoran and Wang, Zhaoran and Kolar, Mladen},
	journal={ArXiv:1912.06875},
	year={2019}
}

@inproceedings{ding2020natural,
  title={Natural Policy Gradient Primal-Dual Method for Constrained Markov Decision Processes},
  author={Ding, Dongsheng and Zhang, Kaiqing and Basar, Tamer and Jovanovic, Mihailo},
  booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
  volume={33},
  year={2020}
}

@inproceedings{zhang2018networked,
	title={Networked multi-agent reinforcement learning in continuous spaces},
	author={Zhang, Kaiqing and Yang, Zhuoran and Basar, Tamer},
	booktitle={Proc. 2018 IEEE Conference on Decision and Control (CDC)},
	pages={2771--2776},
	year={2018},
	organization={IEEE}
}

@article{wang2019neural,
  title={Neural policy gradient methods: Global optimality and rates of convergence},
  author={Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  journal={ArXiv:1909.01150},
  year={2019}
}

@article{hennes2019neural,
	title={Neural replicator dynamics},
	author={Hennes, Daniel and Morrill, Dustin and Omidshafiei, Shayegan and Munos, Remi and Perolat, Julien and Lanctot, Marc and Gruslys, Audrunas and Lespiau, Jean-Baptiste and Parmas, Paavo and Duenez-Guzman, Edgar and others},
	journal={ArXiv:1906.00190},
	year={2019}
}

@inproceedings{hennes2020neural,
	title={Neural Replicator Dynamics: Multiagent Learning via Hedging Policy Gradients},
	author={Hennes, Daniel and Morrill, Dustin and Omidshafiei, Shayegan and Munos, R{\'e}mi and Perolat, Julien and Lanctot, Marc and Gruslys, Audrunas and Lespiau, Jean-Baptiste and Parmas, Paavo and Du{\'e}{\~n}ez-Guzm{\'a}n, Edgar and others},
	booktitle={Proc. International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)},
	pages={492--501},
	year={2020}
}

@article{xu2020non,
	title={Non-asymptotic convergence analysis of two time-scale (natural) actor-critic algorithms},
	author={Xu, Tengyu and Wang, Zhe and Liang, Yingbin},
	journal={ArXiv:2005.03557},
	year={2020}
}

@article{wang2020off,
	title={Off-Policy Multi-Agent Decomposed Policy Gradients},
	author={Wang, Yihan and Han, Beining and Wang, Tonghan and Dong, Heng and Zhang, Chongjie},
	journal={ArXiv:2007.12322},
	year={2020}
}

@inproceedings{korda2015td,
	title={On TD (0) with function approximation: Concentration bounds and a centered variant with exponential convergence},
	author={Korda, Nathaniel and La, Prashanth},
	booktitle={Proc. International Conference on Machine Learning (ICML)},
	pages={626--634},
	year={2015}
}

@article{tadic2001convergence,
	title={On the convergence of temporal-difference learning with linear function approximation},
	author={Tadi{\'c}, Vladislav},
	journal={Machine learning},
	volume={42},
	number={3},
	pages={241--267},
	year={2001}
}

@inproceedings{qiu2019finite,
	title={On the finite-time convergence of actor-critic algorithm},
	author={Qiu, Shuang and Yang, Zhuoran and Ye, Jieping and Wang, Zhaoran},
	booktitle={NeurIPS Optimization Foundations for Reinforcement Learning Workshop},
	year={2019}
}

@article{kumar2019sample,
  title={On the sample complexity of actor-critic method for reinforcement learning with function approximation},
  author={Kumar, Harshat and Koppel, Alec and Ribeiro, Alejandro},
  journal={ArXiv:1910.08412},
  year={2019}
}

@article{agarwal2019theory,
	title={On the theory of policy gradient methods: Optimality, approximation, and distribution shift},
	author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
	journal={ArXiv:1908.00261},
	year={2019}
}

@inproceedings{scaman2017optimal,
	title={Optimal algorithms for smooth and strongly convex distributed optimization in networks},
	author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin Tat and Massouli{\'e}, Laurent},
	booktitle={Proc. International Conference on Machine Learning (ICML)},
	year = 	 {2017},
	pages = 	 {3027--3036},
	volume = 	 {70}	
}

@inproceedings{agarwal2020optimality,
  title={Optimality and approximation with policy gradient methods in markov decision processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  booktitle={Conference on Learning Theory (COLT)},
  pages={64--66},
  year={2020}
}

@article{hauskrecht2000planning,
	title={Planning treatment of ischemic heart disease with partially observable Markov decision processes},
	author={Hauskrecht, Milos and Fraser, Hamish},
	journal={Artificial Intelligence in Medicine},
	volume={18},
	number={3},
	pages={221--244},
	year={2000}
}

@inproceedings{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation.},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay and others},
  booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
  volume={99},
  pages={1057--1063},
  year={1999}
}

@article{lan2021policy,
	title={Policy Mirror Descent for Reinforcement Learning: Linear Convergence, New Sampling Complexity, and Generalized Problem Classes},
	author={Lan, Guanghui},
	journal={ArXiv:2102.00135},
	year={2021}
}

@article{zhao2021provably,
	title={Provably efficient policy gradient methods for two-player zero-sum Markov games},
	author={Zhao, Yulai and Tian, Yuandong and Lee, Jason D and Du, Simon S},
	journal={ArXiv:2102.08903},
	year={2021}
}

@inproceedings{yang2019provably,
  title={Provably global convergence of actor-critic: A case for linear quadratic regulator with ergodic cost},
  author={Yang, Zhuoran and Chen, Yongxin and Hong, Mingyi and Wang, Zhaoran},
  booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@article{lee2018primal,
	title={Primal-Dual Distributed Temporal Difference Learning},
	author={Lee, Donghwan and Hu, Jianghai},
	journal={ArXiv:1805.07918},
	year={2018}
}

@inproceedings{zhang2020provably,
	title={Provably convergent two-timescale off-policy actor-critic with function approximation},
	author={Zhang, Shangtong and Liu, Bo and Yao, Hengshuai and Whiteson, Shimon},
	booktitle={Proc. International Conference on Machine Learning (ICML)},
	pages={11204--11213},
	year={2020}
}

@article{chen2021proximal,
  title={Proximal Gradient Descent-Ascent: Variable Convergence under K\L Geometry},
  author={Chen, Ziyi and Zhou, Yi and Xu, Tengyu and Liang, Yingbin},
  journal={ArXiv:2102.04653},
  year={2021}
}

@article{kar2013cal,
	title={QD-Learning: A collaborative distributed strategy for multi-agent reinforcement learning through consensus + innovations},
	author={Kar, Soummya and Moura, Jos{\'e} M. F. and Poor, H. Vincent},
	journal={IEEE Transactions on Signal Processing},
	volume={61},
	number={7},
	pages={1848--1862},
	year={2013}
}

@article{kar2013qd,
	title={${{\cal Q} {\cal D}}$-Learning: A Collaborative Distributed Strategy for Multi-Agent Reinforcement Learning Through  ${\rm Consensus} + {\rm Innovations}$}, 
    author={Kar, Soummya  and  Moura, Jos{\'e} M. F.  and  Poor, H. Vincent },
	journal={IEEE Transactions on Signal Processing}, 
	year={2013},
	volume={61},
	number={7},
	pages={1848-1862},
	doi={10.1109/TSP.2013.2241057}
}

@article{kar2012qd,
	title={QD-Learning: A Collaborative Distributed Strategy for Multi-Agent Reinforcement Learning Through Consensus},
	author={Kar, Soummya and Moura, Jos{\'e} MF and Poor, H. Vincent},
	journal={ArXiv:1205.0047},
	year={2012}
}

@inproceedings{xu2020reanalysis,
	title={Reanalysis of variance reduced temporal difference learning},
	author={Xu, Tengyu and Wang, Zhe and Zhou, Yi and Liang, Yingbin},
	booktitle={Proc. International Conference on Learning Representations (ICLR)},
	year={2020}
}

@book{sutton2018reinforcement,
	title={Reinforcement learning: An introduction (Second Edition)},
	author={Sutton, Richard S and Barto, Andrew G},
	year={2018}
}

@inproceedings{baird1995residual,
	title={Residual algorithms: Reinforcement learning with function approximation},
	author={Baird, Leemon},
	booktitle={Proc. International Conference on Machine Learning (ICML)},
	pages={30--37},
	year={1995}
}

@inproceedings{qiu2021rmix,
  title={RMIX: Learning Risk-Sensitive Policies forCooperative Reinforcement Learning Agents},
  author={Qiu, Wei and Wang, Xinrun and Yu, Runsheng and Wang, Rundong and He, Xu and An, Bo and Obraztsova, Svetlana and Rabinovich, Zinovi},
  booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
  year={2021}
}

@inproceedings{banerjee2012sample,
	title={Sample bounded distributed reinforcement learning for decentralized POMDPs},
	author={Banerjee, Bikramjit and Lyle, Jeremy and Kraemer, Landon and Yellamraju, Rajesh},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={26},
	number={1},
	year={2012}
}

@article{xu2020sample,
	title={Sample Complexity Bounds for Two Timescale Value-based Reinforcement Learning Algorithms},
	author={Xu, Tengyu and Liang, Yingbin},
	journal={ArXiv:2011.05053},
	year={2020}
}

@article{yang2020sample,
	title={Sample Complexity of Policy Gradient Finding Second-Order Stationary Points},
	author={Yang, Long and Zheng, Qian and Pan, Gang},
	journal={ArXiv:2012.01491},
	year={2020}
}

@article{qu2020scalable,
	title={Scalable Multi-Agent Reinforcement Learning for Networked Systems with Average Reward},
	author={Qu, Guannan and Lin, Yiheng and Wierman, Adam and Li, Na},
	journal={ArXiv:2006.06626},
	year={2020}
}

@article{bennis2013self,
	title={Self-organization in small cell networks: A reinforcement learning approach},
	author={Bennis, Mehdi and Perlaza, Samir M and Blasco, Pol and Han, Zhu and Poor, H. Vincent},
	journal={IEEE Transactions on Wireless Communications},
	volume={12},
	number={7},
	pages={3202--3212},
	year={2013}
}

@article{xiao2021shaping,
	title={Shaping Advice in Deep Multi-Agent Reinforcement Learning},
	author={Xiao, Baicen and Ramasubramanian, Bhaskar and Poovendran, Radha},
	journal={ArXiv:2103.15941},
	year={2021}
}

@article{singh2020squarm,
	title={SQuARM-SGD: Communication-Efficient Momentum SGD for Decentralized Optimization},
	author={Singh, Navjot and Data, Deepesh and George, Jemin and Diggavi, Suhas},
	journal={ArXiv:2005.07041},
	year={2020}
}

@incollection{gordon1995stable,
	title={Stable function approximation in dynamic programming},
	author={Gordon, Geoffrey J},
	booktitle={Machine Learning Proceedings 1995},
	pages={261--268},
	year={1995}
}

@book{borkar2009stochastic,
	title={Stochastic approximation: a dynamical systems viewpoint},
	author={Borkar, Vivek S},
	volume={48},
	year={2009}
}

@article{tesauro1995temporal,
	title={Temporal difference learning and TD-Gammon},
	author={Tesauro, Gerald},
	journal={Communications of the ACM},
	volume={38},
	number={3},
	pages={58--68},
	year={1995}
}

@article{dayan1992convergence,
	title={The convergence of TD ($\lambda$) for general $\lambda$},
	author={Dayan, Peter},
	journal={Machine learning},
	volume={8},
	number={3-4},
	pages={341--362},
	year={1992}
}

@inproceedings{claus1998dynamics,
	title={The dynamics of reinforcement learning in cooperative multiagent systems},
	author={Claus, Caroline and Boutilier, Craig},
	booktitle={Proc. Association for the Advancement of Artificial Intelligence (AAAI)},
	volume={1998},
	number={746-752},
	pages={2},
	year={1998}
}

@inproceedings{lin2020toward,
	title={Toward Resilient Multi-Agent Actor-Critic Algorithms for Distributed Reinforcement Learning},
	author={Lin, Yixuan and Gade, Shripad and Sandhu, Romeil and Liu, Ji},
	booktitle={2020 American Control Conference (ACC)},
	pages={3953--3958},
	year={2020}
}

@inproceedings{xu2019two,
	title={Two time-scale off-policy TD learning: Non-asymptotic analysis over Markovian samples},
	author={Xu, Tengyu and Zou, Shaofeng and Liang, Yingbin},
	booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS)},
	pages={10634--10644},
	year={2019}
}

@article{yuan2020towards,
	title={Towards User Scheduling for 6G: A Fairness-Oriented Scheduler Using Multi-Agent Reinforcement Learning},
	author={Yuan, Mingqi and Cao, Qi and Pun, Man-on and Chen, Yi},
	journal={ArXiv:2012.15081},
	year={2020}
}

@inproceedings{qu2019value,
	title={Value propagation for decentralized networked deep multi-agent reinforcement learning},
	author={Qu, Chao and Mannor, Shie and Xu, Huan and Qi, Yuan and Song, Le and Xiong, Junwu},
	booktitle={Proc. Advances in Neural Information Processing Systems (NeurIPS))},
	pages={1184--1193},
	year={2019}
}


