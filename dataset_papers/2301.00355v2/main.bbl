\begin{thebibliography}{77}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Askell et~al.(2021)Askell, Bai, Chen, Drain, Ganguli, Henighan, Jones,
  Joseph, Mann, DasSarma, et~al.]{askell2021general}
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan,
  Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et~al.
\newblock A general language assistant as a laboratory for alignment.
\newblock \emph{ArXiv preprint}, abs/2112.00861, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.00861}.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen,
  Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022.

\bibitem[Banchs(2012)]{banchs2012movie}
Rafael~E. Banchs.
\newblock Movie-{D}i{C}: a movie dialogue corpus for research and development.
\newblock In \emph{Proceedings of the 50th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pp.\  203--207, Jeju
  Island, Korea, 2012. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/P12-2040}.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill,
  et~al.]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{ArXiv preprint}, abs/2108.07258, 2021.
\newblock URL \url{https://arxiv.org/abs/2108.07258}.

\bibitem[Brown et~al.(2019)Brown, Goo, Nagarajan, and
  Niekum]{brown2019extrapolating}
Daniel~S. Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum.
\newblock Extrapolating beyond suboptimal demonstrations via inverse
  reinforcement learning from observations.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
  \emph{Proceedings of the 36th International Conference on Machine Learning,
  {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  783--792. {PMLR},
  2019.
\newblock URL \url{http://proceedings.mlr.press/v97/brown19a.html}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin (eds.), \emph{Advances in Neural
  Information Processing Systems 33: Annual Conference on Neural Information
  Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.

\bibitem[BusinessInsider(2018)]{get_mad_siri}
BusinessInsider.
\newblock {People found a really easy way to make Siri curse}.
\newblock
  https://www.businessinsider.co.za/apple-siri-swears-when-asked-for-second-definition-of-mother-2018-4,
  2018.
\newblock [Online; accessed May 18th, 2022].

\bibitem[Cao et~al.(2021)Cao, Izacard, Riedel, and
  Petroni]{de2020autoregressive}
Nicola~De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni.
\newblock Autoregressive entity retrieval.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=5k8F6UU39V}.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Paul~F. Christiano, Jan Leike, Tom~B. Brown, Miljan Martic, Shane Legg, and
  Dario Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna~M. Wallach,
  Rob Fergus, S.~V.~N. Vishwanathan, and Roman Garnett (eds.), \emph{Advances
  in Neural Information Processing Systems 30: Annual Conference on Neural
  Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
  {USA}}, pp.\  4299--4307, 2017.

\bibitem[Codevilla et~al.(2019)Codevilla, Santana, L{\'{o}}pez, and
  Gaidon]{codevilla2019exploring}
Felipe Codevilla, Eder Santana, Antonio~M. L{\'{o}}pez, and Adrien Gaidon.
\newblock Exploring the limitations of behavior cloning for autonomous driving.
\newblock In \emph{2019 {IEEE/CVF} International Conference on Computer Vision,
  {ICCV} 2019, Seoul, Korea (South), October 27 - November 2, 2019}, pp.\
  9328--9337. {IEEE}, 2019.
\newblock \doi{10.1109/ICCV.2019.00942}.
\newblock URL \url{https://doi.org/10.1109/ICCV.2019.00942}.

\bibitem[Danescu-Niculescu-Mizil \& Lee(2011)Danescu-Niculescu-Mizil and
  Lee]{danescu2011chameleons}
Cristian Danescu-Niculescu-Mizil and Lillian Lee.
\newblock Chameleons in imagined conversations: A new approach to understanding
  coordination of linguistic style in dialogs.
\newblock In \emph{Proceedings of the 2nd Workshop on Cognitive Modeling and
  Computational Linguistics}, pp.\  76--87, Portland, Oregon, USA, 2011.
  Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/W11-0609}.

\bibitem[Dathathri et~al.(2020)Dathathri, Madotto, Lan, Hung, Frank, Molino,
  Yosinski, and Liu]{dathathri2019plug}
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero
  Molino, Jason Yosinski, and Rosanne Liu.
\newblock Plug and play language models: {A} simple approach to controlled text
  generation.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.
\newblock URL \url{https://openreview.net/forum?id=H1edEyBKDS}.

\bibitem[Du et~al.(2022{\natexlab{a}})Du, Raheja, Kumar, Kim, Lopez, and
  Kang]{du-etal-2022-understanding-iterative}
Wanyu Du, Vipul Raheja, Dhruv Kumar, Zae~Myung Kim, Melissa Lopez, and Dongyeop
  Kang.
\newblock Understanding iterative revision from human-written text.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  3573--3590,
  Dublin, Ireland, 2022{\natexlab{a}}. Association for Computational
  Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.250}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.250}.

\bibitem[Du et~al.(2022{\natexlab{b}})Du, Raheja, Kumar, Kim, Lopez, and
  Kang]{du2022understanding}
Wanyu Du, Vipul Raheja, Dhruv Kumar, Zae~Myung Kim, Melissa Lopez, and Dongyeop
  Kang.
\newblock Understanding iterative revision from human-written text.
\newblock \emph{ArXiv preprint}, abs/2203.03802, 2022{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2203.03802}.

\bibitem[Emelin et~al.(2021)Emelin, Le~Bras, Hwang, Forbes, and
  Choi]{emelin-etal-2021-moral}
Denis Emelin, Ronan Le~Bras, Jena~D. Hwang, Maxwell Forbes, and Yejin Choi.
\newblock Moral stories: Situated reasoning about norms, intents, actions, and
  their consequences.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  698--718, Online and Punta Cana,
  Dominican Republic, 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.54}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.54}.

\bibitem[Faltings et~al.(2021)Faltings, Galley, Hintz, Brockett, Quirk, Gao,
  and Dolan]{faltings-etal-2021-text}
Felix Faltings, Michel Galley, Gerold Hintz, Chris Brockett, Chris Quirk,
  Jianfeng Gao, and Bill Dolan.
\newblock Text editing by command.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pp.\  5259--5274, Online, 2021. Association for Computational
  Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.414}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.414}.

\bibitem[Forbes et~al.(2020)Forbes, Hwang, Shwartz, Sap, and
  Choi]{forbes2020social}
Maxwell Forbes, Jena~D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi.
\newblock Social chemistry 101: Learning to reason about social and moral
  norms.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  653--670, Online, 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.48}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.48}.

\bibitem[Gehman et~al.(2020)Gehman, Gururangan, Sap, Choi, and
  Smith]{gehman2020realtoxicityprompts}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A. Smith.
\newblock {R}eal{T}oxicity{P}rompts: Evaluating neural toxic degeneration in
  language models.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2020}, pp.\  3356--3369, Online, 2020. Association for Computational
  Linguistics.
\newblock \doi{10.18653/v1/2020.findings-emnlp.301}.
\newblock URL \url{https://aclanthology.org/2020.findings-emnlp.301}.

\bibitem[Gu et~al.(2019)Gu, Wang, and Zhao]{gu2019levenshtein}
Jiatao Gu, Changhan Wang, and Junbo Zhao.
\newblock Levenshtein transformer.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett (eds.), \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pp.\  11179--11189, 2019.

\bibitem[Gururangan et~al.(2020)Gururangan, Marasovi{\'c}, Swayamdipta, Lo,
  Beltagy, Downey, and Smith]{gururangan2020don}
Suchin Gururangan, Ana Marasovi{\'c}, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy,
  Doug Downey, and Noah~A. Smith.
\newblock Don{'}t stop pretraining: Adapt language models to domains and tasks.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  8342--8360, Online, 2020. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.740}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.740}.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In Jennifer~G. Dy and Andreas Krause (eds.), \emph{Proceedings of the
  35th International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  1856--1865. {PMLR},
  2018.
\newblock URL \url{http://proceedings.mlr.press/v80/haarnoja18b.html}.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Burns, Basart, Critch,
  Li, Song, and Steinhardt]{hendrycks2020aligning}
Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song,
  and Jacob Steinhardt.
\newblock Aligning {AI} with shared human values.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net,
  2021{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=dNy\_RKzJacY}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Carlini, Schulman, and
  Steinhardt]{hendrycks2021unsolved}
Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt.
\newblock Unsolved problems in ml safety.
\newblock \emph{ArXiv preprint}, abs/2109.13916, 2021{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2109.13916}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{ArXiv preprint}, abs/2203.15556, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.15556}.

\bibitem[Holtzman et~al.(2021)Holtzman, West, Shwartz, Choi, and
  Zettlemoyer]{holtzman2021surface}
Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer.
\newblock Surface form competition: Why the highest probability answer isn{'}t
  always right.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  7038--7051, Online and Punta Cana,
  Dominican Republic, 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.564}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.564}.

\bibitem[Insider(2016)]{get_mad}
Insider.
\newblock {Microsoft's virtual assistant 'will get mad' if you 'say things that
  are particularly a--holeish'}.
\newblock
  https://www.businessinsider.com/microsoft-cortana-will-get-mad-at-bad-behaviour-2016-2/,
  2016.
\newblock [Online; accessed May 18th, 2022].

\bibitem[Jiang et~al.(2021)Jiang, Hwang, Bhagavatula, Bras, Forbes, Borchardt,
  Liang, Etzioni, Sap, and Choi]{jiang2021delphi}
Liwei Jiang, Jena~D Hwang, Chandra Bhagavatula, Ronan~Le Bras, Maxwell Forbes,
  Jon Borchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi.
\newblock Delphi: Towards machine ethics and norms.
\newblock \emph{ArXiv preprint}, abs/2110.07574, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.07574}.

\bibitem[Jurafsky(2000)]{jurafsky2000speech}
Dan Jurafsky.
\newblock \emph{Speech \& language processing}.
\newblock Pearson Education India, 2000.

\bibitem[Keskar et~al.(2019)Keskar, McCann, Varshney, Xiong, and
  Socher]{keskar2019ctrl}
Nitish~Shirish Keskar, Bryan McCann, Lav~R Varshney, Caiming Xiong, and Richard
  Socher.
\newblock Ctrl: A conditional transformer language model for controllable
  generation.
\newblock \emph{arXiv preprint arXiv:1909.05858}, 2019.

\bibitem[Lampinen et~al.(2022)Lampinen, Dasgupta, Chan, Matthewson, Tessler,
  Creswell, McClelland, Wang, and Hill]{lampinen2022can}
Andrew~K Lampinen, Ishita Dasgupta, Stephanie~CY Chan, Kory Matthewson,
  Michael~Henry Tessler, Antonia Creswell, James~L McClelland, Jane~X Wang, and
  Felix Hill.
\newblock Can language models learn from explanations in context?
\newblock \emph{ArXiv preprint}, abs/2204.02329, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.02329}.

\bibitem[Le et~al.(2018)Le, Jiang, Agarwal, Dud{\'{\i}}k, Yue, and
  III]{le2018hierarchical}
Hoang~Minh Le, Nan Jiang, Alekh Agarwal, Miroslav Dud{\'{\i}}k, Yisong Yue, and
  Hal~Daum{\'{e}} III.
\newblock Hierarchical imitation and reinforcement learning.
\newblock In Jennifer~G. Dy and Andreas Krause (eds.), \emph{Proceedings of the
  35th International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  2923--2932. {PMLR},
  2018.
\newblock URL \url{http://proceedings.mlr.press/v80/le18a.html}.

\bibitem[Lee et~al.(2022)Lee, Liang, and Yang]{lee2022coauthor}
Mina Lee, Percy Liang, and Qian Yang.
\newblock Coauthor: Designing a human-ai collaborative writing dataset for
  exploring language model capabilities.
\newblock \emph{ArXiv preprint}, abs/2201.06796, 2022.
\newblock URL \url{https://arxiv.org/abs/2201.06796}.

\bibitem[Li \& Liang(2021)Li and Liang]{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pp.\  4582--4597,
  Online, 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.353}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.353}.

\bibitem[Lin(2004)]{lin-2004-rouge}
Chin-Yew Lin.
\newblock {ROUGE}: A package for automatic evaluation of summaries.
\newblock In \emph{Text Summarization Branches Out}, pp.\  74--81, Barcelona,
  Spain, 2004. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/W04-1013}.

\bibitem[Lin et~al.(2021)Lin, Hilton, and Evans]{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock \emph{ArXiv preprint}, abs/2109.07958, 2021.
\newblock URL \url{https://arxiv.org/abs/2109.07958}.

\bibitem[Litman et~al.(2017)Litman, Robinson, and
  Abberbock]{litman2017turkprime}
Leib Litman, Jonathan Robinson, and Tzvi Abberbock.
\newblock Turkprime. com: A versatile crowdsourcing data acquisition platform
  for the behavioral sciences.
\newblock \emph{Behavior research methods}, 49\penalty0 (2):\penalty0 433--442,
  2017.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Sap, Lu, Swayamdipta, Bhagavatula,
  Smith, and Choi]{liu2021dexperts}
Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula,
  Noah~A. Smith, and Yejin Choi.
\newblock {DE}xperts: Decoding-time controlled text generation with experts and
  anti-experts.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pp.\  6691--6706,
  Online, 2021{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.522}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.522}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Yuan, Fu, Jiang, Hayashi, and
  Neubig]{liu2021pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock \emph{ArXiv preprint}, abs/2107.13586, 2021{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2107.13586}.

\bibitem[Liu et~al.(2021{\natexlab{c}})Liu, Gao, Jia, Xu, and
  Vosoughi]{liu2021non}
Ruibo Liu, Chongyang Gao, Chenyan Jia, Guangxuan Xu, and Soroush Vosoughi.
\newblock Non-parallel text style transfer with self-parallel supervision.
\newblock In \emph{The Tenth International Conference on Learning
  Representations (ICLR 2022)}, 2021{\natexlab{c}}.
\newblock URL \url{https://openreview.net/pdf?id=-TSe5o7STVR}.

\bibitem[Liu et~al.(2021{\natexlab{d}})Liu, Wang, Jia, and
  Vosoughi]{Liu_Wang_Jia_Vosoughi_2021}
Ruibo Liu, Lili Wang, Chenyan Jia, and Soroush Vosoughi.
\newblock Political depolarization of news articles using attribute-aware word
  embeddings.
\newblock \emph{Proceedings of the International AAAI Conference on Web and
  Social Media}, 15\penalty0 (1):\penalty0 385--396, 2021{\natexlab{d}}.
\newblock URL \url{https://ojs.aaai.org/index.php/ICWSM/article/view/18069}.

\bibitem[Liu et~al.(2021{\natexlab{e}})Liu, Zheng, Gupta, Gaonkar, Gao,
  Vosoughi, Shokouhi, and Awadallah]{liu2021knowledge}
Ruibo Liu, Guoqing Zheng, Shashank Gupta, Radhika Gaonkar, Chongyang Gao,
  Soroush Vosoughi, Milad Shokouhi, and Ahmed~Hassan Awadallah.
\newblock Knowledge infused decoding.
\newblock In \emph{The Tenth International Conference on Learning
  Representations (ICLR 2022)}, 2021{\natexlab{e}}.
\newblock URL \url{https://openreview.net/pdf?id=upnDJ7itech}.

\bibitem[Liu et~al.(2022)Liu, Jia, Wei, Xu, and Vosoughi]{liu2022quantifying}
Ruibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu, and Soroush Vosoughi.
\newblock Quantifying and alleviating political bias in language models.
\newblock \emph{Artificial Intelligence}, 304:\penalty0 103654, 2022.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{ArXiv preprint}, abs/1907.11692, 2019.
\newblock URL \url{https://arxiv.org/abs/1907.11692}.

\bibitem[Lourie et~al.(2020)Lourie, Le~Bras, and Choi]{lourie2020scruples}
Nicholas Lourie, Ronan Le~Bras, and Yejin Choi.
\newblock Scruples: A corpus of community ethical judgments on 32, 000
  real-life anecdotes.
\newblock \emph{ArXiv preprint}, abs/2008.09094, 2020.
\newblock URL \url{https://arxiv.org/abs/2008.09094}.

\bibitem[Ma et~al.(2020)Ma, Liu, Wang, and Vosoughi]{ma2020emoji}
Weicheng Ma, Ruibo Liu, Lili Wang, and Soroush Vosoughi.
\newblock Emoji prediction: Extensions and benchmarking.
\newblock \emph{ArXiv preprint}, abs/2007.07389, 2020.
\newblock URL \url{https://arxiv.org/abs/2007.07389}.

\bibitem[Malmi et~al.(2019)Malmi, Krause, Rothe, Mirylenka, and
  Severyn]{malmi2019encode}
Eric Malmi, Sebastian Krause, Sascha Rothe, Daniil Mirylenka, and Aliaksei
  Severyn.
\newblock Encode, tag, realize: High-precision text editing.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pp.\  5054--5065, Hong Kong,
  China, 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1510}.
\newblock URL \url{https://aclanthology.org/D19-1510}.

\bibitem[Malmi et~al.(2020)Malmi, Severyn, and Rothe]{malmi2020unsupervised}
Eric Malmi, Aliaksei Severyn, and Sascha Rothe.
\newblock Unsupervised text style transfer with padded masked language models.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  8671--8680, Online, 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.699}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.699}.

\bibitem[Marasovi{\'c} et~al.(2021)Marasovi{\'c}, Beltagy, Downey, and
  Peters]{marasovic2021few}
Ana Marasovi{\'c}, Iz~Beltagy, Doug Downey, and Matthew~E Peters.
\newblock Few-shot self-rationalization with natural language prompts.
\newblock \emph{ArXiv preprint}, abs/2111.08284, 2021.
\newblock URL \url{https://arxiv.org/abs/2111.08284}.

\bibitem[Mishra et~al.(2021)Mishra, Khashabi, Baral, and
  Hajishirzi]{mishra2021cross}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.
\newblock Cross-task generalization via natural language crowdsourcing
  instructions.
\newblock \emph{ArXiv preprint}, abs/2104.08773, 2021.
\newblock URL \url{https://arxiv.org/abs/2104.08773}.

\bibitem[Monroe et~al.(2008)Monroe, Colaresi, and Quinn]{monroe2008fightin}
Burt~L Monroe, Michael~P Colaresi, and Kevin~M Quinn.
\newblock Fightin' words: Lexical feature selection and evaluation for
  identifying the content of political conflict.
\newblock \emph{Political Analysis}, 16\penalty0 (4):\penalty0 372--403, 2008.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
R{\'{e}}mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc~G. Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In Daniel~D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle
  Guyon, and Roman Garnett (eds.), \emph{Advances in Neural Information
  Processing Systems 29: Annual Conference on Neural Information Processing
  Systems 2016, December 5-10, 2016, Barcelona, Spain}, pp.\  1046--1054, 2016.

\bibitem[Narang et~al.(2020)Narang, Raffel, Lee, Roberts, Fiedel, and
  Malkan]{narang2020wt5}
Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and
  Karishma Malkan.
\newblock Wt5?! training text-to-text models to explain their predictions.
\newblock \emph{ArXiv preprint}, abs/2004.14546, 2020.
\newblock URL \url{https://arxiv.org/abs/2004.14546}.

\bibitem[Nye et~al.(2021)Nye, Andreassen, Gur-Ari, Michalewski, Austin, Bieber,
  Dohan, Lewkowycz, Bosma, Luan, et~al.]{nye2021show}
Maxwell Nye, Anders~Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob
  Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David
  Luan, et~al.
\newblock Show your work: Scratchpads for intermediate computation with
  language models.
\newblock \emph{ArXiv preprint}, abs/2112.00114, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.00114}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{ArXiv preprint}, abs/2203.02155, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.02155}.

\bibitem[Paul(2014)]{paul2014transformative}
Laurie~Ann Paul.
\newblock \emph{Transformative experience}.
\newblock OUP Oxford, 2014.

\bibitem[Perez et~al.(2022)Perez, Huang, Song, Cai, Ring, Aslanides, Glaese,
  McAleese, and Irving]{perez2022red}
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John
  Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving.
\newblock Red teaming language models with language models.
\newblock \emph{ArXiv preprint}, abs/2202.03286, 2022.
\newblock URL \url{https://arxiv.org/abs/2202.03286}.

\bibitem[Pettigrew(2019)]{pettigrew2019choosing}
Richard Pettigrew.
\newblock \emph{Choosing for changing selves}.
\newblock Oxford University Press, 2019.

\bibitem[Quinton(1973)]{alma991000229559705706}
Anthony. Quinton.
\newblock \emph{Utilitarian ethics.}
\newblock New studies in ethics. St. Martin's Press, New York, 1973.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{ArXiv preprint}, abs/1910.10683, 2019.
\newblock URL \url{https://arxiv.org/abs/1910.10683}.

\bibitem[Reid \& Neubig(2022)Reid and Neubig]{reid2022learning}
Machel Reid and Graham Neubig.
\newblock Learning to model editing processes.
\newblock \emph{ArXiv preprint}, abs/2205.12374, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.12374}.

\bibitem[Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai,
  Chaffin, Stiegler, Scao, Raja, et~al.]{sanh2021multitask}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H Bach, Lintang Sutawika,
  Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja,
  et~al.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock \emph{ArXiv preprint}, abs/2110.08207, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.08207}.

\bibitem[Sap et~al.(2020)Sap, Gabriel, Qin, Jurafsky, Smith, and
  Choi]{sap2019social}
Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah~A. Smith, and
  Yejin Choi.
\newblock Social bias frames: Reasoning about social and power implications of
  language.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  5477--5490, Online, 2020. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.486}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.486}.

\bibitem[Schick et~al.(2021)Schick, Udupa, and Sch{\"u}tze]{schick2021self}
Timo Schick, Sahana Udupa, and Hinrich Sch{\"u}tze.
\newblock Self-diagnosis and self-debiasing: A proposal for reducing
  corpus-based bias in {NLP}.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:\penalty0 1408--1424, 2021.
\newblock \doi{10.1162/tacl_a_00434}.
\newblock URL \url{https://aclanthology.org/2021.tacl-1.84}.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael~I. Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In Francis~R. Bach and David~M. Blei (eds.), \emph{Proceedings of the
  32nd International Conference on Machine Learning, {ICML} 2015, Lille,
  France, 6-11 July 2015}, volume~37 of \emph{{JMLR} Workshop and Conference
  Proceedings}, pp.\  1889--1897. JMLR.org, 2015.
\newblock URL \url{http://proceedings.mlr.press/v37/schulman15.html}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{ArXiv preprint}, abs/1707.06347, 2017.
\newblock URL \url{https://arxiv.org/abs/1707.06347}.

\bibitem[Stiennon et~al.(2020)Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss,
  Radford, Amodei, and Christiano]{stiennon2020learning}
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel~M. Ziegler, Ryan Lowe, Chelsea
  Voss, Alec Radford, Dario Amodei, and Paul~F. Christiano.
\newblock Learning to summarize with human feedback.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin (eds.), \emph{Advances in Neural
  Information Processing Systems 33: Annual Conference on Neural Information
  Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual}, 2020.

\bibitem[Talmor et~al.(2022)Talmor, Yoran, Bras, Bhagavatula, Goldberg, Choi,
  and Berant]{talmor2022commonsenseqa}
Alon Talmor, Ori Yoran, Ronan~Le Bras, Chandra Bhagavatula, Yoav Goldberg,
  Yejin Choi, and Jonathan Berant.
\newblock Commonsenseqa 2.0: Exposing the limits of ai through gamification.
\newblock \emph{ArXiv preprint}, abs/2201.05320, 2022.
\newblock URL \url{https://arxiv.org/abs/2201.05320}.

\bibitem[Van~Staveren(2007)]{van2007beyond}
Irene Van~Staveren.
\newblock Beyond utilitarianism and deontology: Ethics in economics.
\newblock \emph{Review of Political Economy}, 19\penalty0 (1):\penalty0 21--35,
  2007.

\bibitem[Wang et~al.(2021)Wang, Gao, Huang, Liu, Ma, and
  Vosoughi]{wang2021embedding}
Lili Wang, Chongyang Gao, Chenghan Huang, Ruibo Liu, Weicheng Ma, and Soroush
  Vosoughi.
\newblock Embedding heterogeneous networks into hyperbolic space without
  meta-path.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~35, pp.\  10147--10155, 2021.

\bibitem[Wang et~al.(2022)Wang, Mishra, Alipoormolabashi, Kordi, Mirzaei,
  Arunkumar, Ashok, Dhanasekaran, Naik, Stap, et~al.]{wang2022benchmarking}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut~Selvan Dhanasekaran, Atharva
  Naik, David Stap, et~al.
\newblock Benchmarking generalization via in-context instructions on 1,600+
  language tasks.
\newblock \emph{ArXiv preprint}, abs/2204.07705, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.07705}.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Chi, Le, and
  Zhou]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed~Chi, Quoc Le, and
  Denny Zhou.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock \emph{ArXiv preprint}, abs/2201.11903, 2022.
\newblock URL \url{https://arxiv.org/abs/2201.11903}.

\bibitem[Weidinger et~al.(2021)Weidinger, Mellor, Rauh, Griffin, Uesato, Huang,
  Cheng, Glaese, Balle, Kasirzadeh, et~al.]{weidinger2021ethical}
Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato,
  Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et~al.
\newblock Ethical and social risks of harm from language models.
\newblock \emph{ArXiv preprint}, abs/2112.04359, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.04359}.

\bibitem[Welleck et~al.(2020)Welleck, Kulikov, Roller, Dinan, Cho, and
  Weston]{welleck2019neural}
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and
  Jason Weston.
\newblock Neural text generation with unlikelihood training.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.
\newblock URL \url{https://openreview.net/forum?id=SJeYe0NtvH}.

\bibitem[Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and
  Singh]{zhao2021calibrate}
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.
\newblock Calibrate before use: Improving few-shot performance of language
  models.
\newblock In Marina Meila and Tong Zhang (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning, {ICML} 2021, 18-24 July 2021,
  Virtual Event}, volume 139 of \emph{Proceedings of Machine Learning
  Research}, pp.\  12697--12706. {PMLR}, 2021.
\newblock URL \url{http://proceedings.mlr.press/v139/zhao21c.html}.

\bibitem[Ziegler et~al.(2019)Ziegler, Stiennon, Wu, Brown, Radford, Amodei,
  Christiano, and Irving]{ziegler2019fine}
Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario
  Amodei, Paul Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences.
\newblock \emph{ArXiv preprint}, abs/1909.08593, 2019.
\newblock URL \url{https://arxiv.org/abs/1909.08593}.

\bibitem[Ziems et~al.(2022)Ziems, Yu, Wang, Halevy, and Yang]{ziems2022moral}
Caleb Ziems, Jane~A Yu, Yi-Chia Wang, Alon Halevy, and Diyi Yang.
\newblock The moral integrity corpus: A benchmark for ethical dialogue systems.
\newblock \emph{ArXiv preprint}, abs/2204.03021, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.03021}.

\end{thebibliography}
