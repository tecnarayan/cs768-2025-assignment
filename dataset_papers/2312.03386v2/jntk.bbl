\begin{thebibliography}{}

\bibitem[Abid and Izeboudjen, 2020]{Abid2020}
Abid, F. and Izeboudjen, N. (2020).
\newblock Predicting forest fire in algeria using data mining techniques: Case
  study of the decision tree algorithm.
\newblock In Ezziyyani, M., editor, {\em Advanced Intelligent Systems for
  Sustainable Development (AI2SD'2019)}, pages 363--370, Cham. Springer
  International Publishing.

\bibitem[Arora et~al., 2019]{Arora2019OnEC}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R.~R., and Wang, R.
  (2019).
\newblock On exact computation with an infinitely wide neural net.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc.

\bibitem[Chen et~al., 2021]{Yilan2021}
Chen, Y., Huang, W., Nguyen, L., and Weng, T.-W. (2021).
\newblock On the equivalence between neural network and support vector machine.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
  J.~W., editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 23478--23490. Curran Associates, Inc.

\bibitem[Du et~al., 2019a]{du19c}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. (2019a).
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In Chaudhuri, K. and Salakhutdinov, R., editors, {\em Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of {\em
  Proceedings of Machine Learning Research}, pages 1675--1685. PMLR.

\bibitem[Du et~al., 2019b]{du2019}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X. (2019b).
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In Chaudhuri, K. and Salakhutdinov, R., editors, {\em Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of {\em
  Proceedings of Machine Learning Research}, pages 1675--1685. PMLR.

\bibitem[Goodfellow et~al., 2015]{Goodfellow2014}
Goodfellow, I.~J., Shlens, J., and Szegedy, C. (2015).
\newblock Explaining and harnessing adversarial examples.
\newblock In Bengio, Y. and LeCun, Y., editors, {\em 3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}.

\bibitem[Hoffman et~al., 2019]{hoffman2019}
Hoffman, J., Roberts, D.~A., and Yaida, S. (2019).
\newblock Robust learning with jacobian regularization.

\bibitem[Hu et~al., 2020]{Hu2020Simple}
Hu, W., Li, Z., and Yu, D. (2020).
\newblock Simple and effective regularization methods for training on noisily
  labeled data with generalization guarantee.
\newblock In {\em 8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net.

\bibitem[Jacot et~al., 2018]{Jacot2018}
Jacot, A., Gabriel, F., and Hongler, C. (2018).
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems}, volume~31. Curran Associates, Inc.

\bibitem[Jakubovitz and Giryes, 2018]{jakubovitz2018}
Jakubovitz, D. and Giryes, R. (2018).
\newblock Improving dnn robustness to adversarial attacks using jacobian
  regularization.
\newblock In Ferrari, V., Hebert, M., Sminchisescu, C., and Weiss, Y., editors,
  {\em Computer Vision -- ECCV 2018}, pages 525--541, Cham. Springer
  International Publishing.

\bibitem[Karakida et~al., 2023]{karakida23a}
Karakida, R., Takase, T., Hayase, T., and Osawa, K. (2023).
\newblock Understanding gradient regularization in deep learning: Efficient
  finite-difference computation and implicit bias.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
  and Scarlett, J., editors, {\em Proceedings of the 40th International
  Conference on Machine Learning}, volume 202 of {\em Proceedings of Machine
  Learning Research}, pages 15809--15827. PMLR.

\bibitem[Lee et~al., 2022]{LeeYYL22}
Lee, H., Yun, E., Yang, H., and Lee, J. (2022).
\newblock Scale mixtures of neural network gaussian processes.
\newblock In {\em The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}.
  OpenReview.net.

\bibitem[Lee et~al., 2018]{lee2018deep}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S.~S., Pennington, J., and
  Sohl{-}Dickstein, J. (2018).
\newblock Deep neural networks as gaussian processes.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net.

\bibitem[Lee et~al., 2019]{Lee19NTKLinear}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J. (2019).
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc.

\bibitem[Liu et~al., 2024]{liu2024}
Liu, D., Wu, L.~Y., Li, B., Boussaid, F., Bennamoun, M., Xie, X., and Liang, C.
  (2024).
\newblock Jacobian norm with selective input gradient regularization for
  interpretable adversarial defense.
\newblock {\em Pattern Recogn.}, 145(C).

\bibitem[Liu and Zenke, 2020]{Liu20pruningNTK}
Liu, T. and Zenke, F. (2020).
\newblock Finding trainable sparse networks through neural tangent transfer.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning (ICML'20)}, pages 6336--6347.

\bibitem[Lohweg, 2013]{banknote}
Lohweg, V. (2013).
\newblock {{B}anknote authentication}.
\newblock UCI Machine Learning Repository.
\newblock {DOI}: https://doi.org/10.24432/C55P57.

\bibitem[Ma and Ying, 2021]{chao2021}
Ma, C. and Ying, L. (2021).
\newblock On linear stability of sgd and input-smoothness of neural networks.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
  J.~W., editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 16805--16817. Curran Associates, Inc.

\bibitem[Matthews et~al., 2018]{Matthews2018}
Matthews, A.~G., Hron, J., Rowland, M., Turner, R.~E., and Ghahramani, Z.
  (2018).
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}. OpenReview.net.

\bibitem[Neal, 1996]{Neal1996}
Neal, R.~M. (1996).
\newblock Priors for infinite networks.
\newblock In {\em Bayesian Learning for Neural Networks}, pages 29--53.
  Springer New York.

\bibitem[Nguyen et~al., 2021]{nguyen2021}
Nguyen, Q., Mondelli, M., and Montufar, G.~F. (2021).
\newblock Tight bounds on the smallest eigenvalue of the neural tangent kernel
  for deep relu networks.
\newblock In Meila, M. and Zhang, T., editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 8119--8129. PMLR.

\bibitem[Peck et~al., 2017]{Peck2017}
Peck, J., Roels, J., Goossens, B., and Saeys, Y. (2017).
\newblock Lower bounds on the robustness to adversarial perturbations.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc.

\bibitem[Peluchetti et~al., 2020]{Favaro2020}
Peluchetti, S., Favaro, S., and Fortini, S. (2020).
\newblock Stable behaviour of infinitely wide deep neural networks.
\newblock In Chiappa, S. and Calandra, R., editors, {\em Proceedings of the
  Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of {\em Proceedings of Machine Learning Research},
  pages 1137--1146. PMLR.

\bibitem[Poole et~al., 2016]{Poole16}
Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., and Ganguli, S. (2016).
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R.,
  editors, {\em Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc.

\bibitem[Rasmussen and Williams, 2005]{GPbook}
Rasmussen, C.~E. and Williams, C. K.~I. (2005).
\newblock {\em Gaussian Processes for Machine Learning (Adaptive Computation
  and Machine Learning)}.
\newblock The MIT Press.

\bibitem[Sejnowski and Gorman, 2017]{connectionistbench}
Sejnowski, T. and Gorman, R. (2017).
\newblock {Connectionist Bench (Sonar, Mines vs. Rocks)}.
\newblock UCI Machine Learning Repository.
\newblock {DOI}: https://doi.org/10.24432/C5T01Q.

\bibitem[Tsilivis and Kempe, 2022]{tsilivis2022what}
Tsilivis, N. and Kempe, J. (2022).
\newblock What can the neural tangent kernel tell us about adversarial
  robustness?
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A., editors, {\em Advances in Neural Information Processing Systems},
  volume~35, pages 18116--18130. Curran Associates, Inc.

\bibitem[Vershynin, 2018]{BookHDP}
Vershynin, R. (2018).
\newblock {\em High-Dimensional Probability: An Introduction with Applications
  in Data Science}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press.

\bibitem[Wang et~al., 2021]{Wang2021}
Wang, J., Chen, J., Sun, Y., Ma, X., Wang, D., Sun, J., and Cheng, P. (2021).
\newblock {\em RobOT: Robustness-Oriented Testing for Deep Learning Systems},
  page 300â€“311.
\newblock IEEE Press.

\bibitem[Yang, 2019]{Yang2019TensorPI}
Yang, G. (2019).
\newblock Wide feedforward or recurrent neural networks of any architecture are
  gaussian processes.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc.

\bibitem[Yang, 2020]{Yang2020a}
Yang, G. (2020).
\newblock Tensor programs ii: Neural tangent kernel for any architecture.

\bibitem[Yang et~al., 2021]{Yang22Transfer}
Yang, G., Hu, E., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N.,
  Pachocki, J., Chen, W., and Gao, J. (2021).
\newblock Tuning large neural networks via zero-shot hyperparameter transfer.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
  J.~W., editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 17084--17097. Curran Associates, Inc.

\bibitem[Yang and Hu, 2021]{Yang21FeatureLearning}
Yang, G. and Hu, E.~J. (2021).
\newblock Tensor programs iv: Feature learning in infinite-width neural
  networks.
\newblock In Meila, M. and Zhang, T., editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 11727--11737. PMLR.

\bibitem[Yang and Littwin, 2021]{Yang21NTK}
Yang, G. and Littwin, E. (2021).
\newblock Tensor programs iib: Architectural universality of neural tangent
  kernel training dynamics.
\newblock In Meila, M. and Zhang, T., editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 11762--11772. PMLR.

\bibitem[Zhang and Wei, 2022]{subweibull}
Zhang, H. and Wei, H. (2022).
\newblock Sharper sub-weibull concentrations.
\newblock {\em Mathematics}, 10(13).

\bibitem[Zhang et~al., 2019]{Zhang2019}
Zhang, H., Zhang, P., and Hsieh, C. (2019).
\newblock Recurjac: An efficient recursive algorithm for bounding jacobian
  matrix of neural networks and its applications.
\newblock In {\em The Thirty-Third {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2019, Honolulu, Hawaii, USA, January 27 - February 1,
  2019}, pages 5757--5764. {AAAI} Press.

\end{thebibliography}
