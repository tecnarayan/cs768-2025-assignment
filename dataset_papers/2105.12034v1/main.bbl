\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2020)Andrychowicz, Raichuk, Sta{\'n}czyk, Orsini,
  Girgin, Marinier, Hussenot, Geist, Pietquin, Michalski,
  et~al.]{andrychowicz2020matters}
Andrychowicz, M., Raichuk, A., Sta{\'n}czyk, P., Orsini, M., Girgin, S.,
  Marinier, R., Hussenot, L., Geist, M., Pietquin, O., Michalski, M., et~al.
\newblock What matters in on-policy reinforcement learning? a large-scale
  empirical study.
\newblock \emph{arXiv preprint arXiv:2006.05990}, 2020.

\bibitem[Argall et~al.(2009)Argall, Chernova, Veloso, and
  Browning]{argall2009survey}
Argall, B.~D., Chernova, S., Veloso, M., and Browning, B.
\newblock A survey of robot learning from demonstration.
\newblock \emph{Robotics and autonomous systems}, 57\penalty0 (5):\penalty0
  469--483, 2009.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bergstra \& Bengio(2012)Bergstra and Bengio]{bergstra2012random}
Bergstra, J. and Bengio, Y.
\newblock Random search for hyper-parameter optimization.
\newblock \emph{Journal of machine learning research}, 13\penalty0 (2), 2012.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, Debiak, Dennison,
  Farhi, Fischer, Hashme, Hesse, et~al.]{dota}
Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C.,
  Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and Zhang]{jax}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Burda et~al.(2018)Burda, Edwards, Storkey, and
  Klimov]{burda2018exploration}
Burda, Y., Edwards, H., Storkey, A., and Klimov, O.
\newblock Exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:1810.12894}, 2018.

\bibitem[Cobbe et~al.(2020)Cobbe, Hesse, Hilton, and
  Schulman]{cobbe2020leveraging}
Cobbe, K., Hesse, C., Hilton, J., and Schulman, J.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  2048--2056. PMLR, 2020.

\bibitem[Dadashi et~al.(2020)Dadashi, Hussenot, Geist, and
  Pietquin]{dadashi2020primal}
Dadashi, R., Hussenot, L., Geist, M., and Pietquin, O.
\newblock Primal wasserstein imitation learning.
\newblock \emph{arXiv preprint arXiv:2006.04678}, 2020.

\bibitem[Ecoffet et~al.(2021)Ecoffet, Huizinga, Lehman, Stanley, and
  Clune]{ecoffet2021first}
Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K.~O., and Clune, J.
\newblock First return, then explore.
\newblock \emph{Nature}, 590\penalty0 (7847):\penalty0 580--586, 2021.

\bibitem[Engstrom et~al.(2020)Engstrom, Ilyas, Santurkar, Tsipras, Janoos,
  Rudolph, and Madry]{engstrom2020implementation}
Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L.,
  and Madry, A.
\newblock Implementation matters in deep policy gradients: A case study on ppo
  and trpo.
\newblock \emph{arXiv preprint arXiv:2005.12729}, 2020.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{ernst2005tree}
Ernst, D., Geurts, P., and Wehenkel, L.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 6:\penalty0 503--556,
  2005.

\bibitem[Feldt(1998)]{feldt1998generating}
Feldt, R.
\newblock Generating diverse software versions with genetic programming: an
  experimental study.
\newblock \emph{IEE Proceedings-Software}, 145\penalty0 (6):\penalty0 228--236,
  1998.

\bibitem[Finn et~al.(2016)Finn, Levine, and Abbeel]{finn2016guided}
Finn, C., Levine, S., and Abbeel, P.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In \emph{International conference on machine learning}, pp.\  49--58.
  PMLR, 2016.

\bibitem[Flamary \& Courty(2017)Flamary and Courty]{flamary2017pot}
Flamary, R. and Courty, N.
\newblock Pot python optimal transport library, 2017.
\newblock URL \url{https://pythonot.github.io/}.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Ghasemipour et~al.(2020)Ghasemipour, Zemel, and
  Gu]{ghasemipour2020divergence}
Ghasemipour, S. K.~S., Zemel, R., and Gu, S.
\newblock A divergence minimization perspective on imitation learning methods.
\newblock In \emph{Conference on Robot Learning}, pp.\  1259--1277. PMLR, 2020.

\bibitem[Gulcehre et~al.(2020)Gulcehre, Wang, Novikov, Paine, Colmenarejo,
  Zolna, Agarwal, Merel, Mankowitz, Paduraru, et~al.]{gulcehre2020rl}
Gulcehre, C., Wang, Z., Novikov, A., Paine, T.~L., Colmenarejo, S.~G., Zolna,
  K., Agarwal, R., Merel, J., Mankowitz, D., Paduraru, C., et~al.
\newblock Rl unplugged: Benchmarks for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.13888}, 2020.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1870. PMLR, 2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Hartikainen,
  Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, et~al.]{haarnoja2018soft2}
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar,
  V., Zhu, H., Gupta, A., Abbeel, P., et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018{\natexlab{b}}.

\bibitem[Heek et~al.(2020)Heek, Levskaya, Oliver, Ritter, Rondepierre, Steiner,
  and van {Z}ee]{flax}
Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre, B., Steiner, A.,
  and van {Z}ee, M.
\newblock {F}lax: A neural network library and ecosystem for {JAX}, 2020.
\newblock URL \url{http://github.com/google/flax}.

\bibitem[Henderson et~al.(2018)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{henderson2018deep}
Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D.
\newblock Deep reinforcement learning that matters.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Hester et~al.(2018)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Horgan, Quan, Sendonaris, Osband, et~al.]{hester2018deep}
Hester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B.,
  Horgan, D., Quan, J., Sendonaris, A., Osband, I., et~al.
\newblock Deep q-learning from demonstrations.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Ho \& Ermon(2016)Ho and Ermon]{ho2016generative}
Ho, J. and Ermon, S.
\newblock Generative adversarial imitation learning.
\newblock \emph{arXiv preprint arXiv:1606.03476}, 2016.

\bibitem[Hoffman et~al.(2020)Hoffman, Shahriari, Aslanides, Barth-Maron,
  Behbahani, Norman, Abdolmaleki, Cassirer, Yang, Baumli, et~al.]{acme}
Hoffman, M., Shahriari, B., Aslanides, J., Barth-Maron, G., Behbahani, F.,
  Norman, T., Abdolmaleki, A., Cassirer, A., Yang, F., Baumli, K., et~al.
\newblock Acme: A research framework for distributed reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.00979}, 2020.

\bibitem[Jaderberg et~al.(2017)Jaderberg, Dalibard, Osindero, Czarnecki,
  Donahue, Razavi, Vinyals, Green, Dunning, Simonyan,
  et~al.]{jaderberg2017population}
Jaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W.~M., Donahue, J.,
  Razavi, A., Vinyals, O., Green, T., Dunning, I., Simonyan, K., et~al.
\newblock Population based training of neural networks.
\newblock \emph{arXiv preprint arXiv:1711.09846}, 2017.

\bibitem[Ke et~al.(2019)Ke, Barnes, Sun, Lee, Choudhury, and
  Srinivasa]{ke2019imitation}
Ke, L., Barnes, M., Sun, W., Lee, G., Choudhury, S., and Srinivasa, S.
\newblock Imitation learning as $ f $-divergence minimization.
\newblock \emph{arXiv preprint arXiv:1905.12888}, 2019.

\bibitem[Kim et~al.(2013)Kim, Farahmand, Pineau, and Precup]{kim2013learning}
Kim, B., Farahmand, A.-m., Pineau, J., and Precup, D.
\newblock Learning from limited demonstrations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2859--2867, 2013.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kostrikov et~al.(2018)Kostrikov, Agrawal, Dwibedi, Levine, and
  Tompson]{kostrikov2018discriminator}
Kostrikov, I., Agrawal, K.~K., Dwibedi, D., Levine, S., and Tompson, J.
\newblock Discriminator-actor-critic: Addressing sample inefficiency and reward
  bias in adversarial imitation learning.
\newblock \emph{arXiv preprint arXiv:1809.02925}, 2018.

\bibitem[Kumar(2016)]{Kumar2016thesis}
Kumar, V.
\newblock \emph{Manipulators and Manipulation in high dimensional spaces}.
\newblock PhD thesis, University of Washington, Seattle, 2016.
\newblock URL
  \url{https://digital.lib.washington.edu/researchworks/handle/1773/38104}.

\bibitem[Lagoudakis \& Parr(2003)Lagoudakis and Parr]{lagoudakis2003least}
Lagoudakis, M.~G. and Parr, R.
\newblock Least-squares policy iteration.
\newblock \emph{The Journal of Machine Learning Research}, 4:\penalty0
  1107--1149, 2003.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012batch}
Lange, S., Gabel, T., and Riedmiller, M.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning}, pp.\  45--73. Springer, 2012.

\bibitem[Larochelle et~al.(2007)Larochelle, Erhan, Courville, Bergstra, and
  Bengio]{larochelle2007empirical}
Larochelle, H., Erhan, D., Courville, A., Bergstra, J., and Bengio, Y.
\newblock An empirical evaluation of deep architectures on problems with many
  factors of variation.
\newblock In \emph{Proceedings of the 24th international conference on Machine
  learning}, pp.\  473--480, 2007.

\bibitem[LeCun et~al.(2012)LeCun, Bottou, Orr, and
  M{\"u}ller]{lecun2012efficient}
LeCun, Y.~A., Bottou, L., Orr, G.~B., and M{\"u}ller, K.-R.
\newblock Efficient backprop.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  9--48.
  Springer, 2012.

\bibitem[Lee et~al.(2020)Lee, Fischer, Liu, Guo, Lee, Canny, and
  Guadarrama]{pi-sac}
Lee, K.-H., Fischer, I., Liu, A., Guo, Y., Lee, H., Canny, J., and Guadarrama,
  S.
\newblock Predictive information accelerates learning in rl.
\newblock \emph{arXiv preprint arXiv:2007.12401}, 2020.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Ng et~al.(2000)Ng, Russell, et~al.]{ng2000algorithms}
Ng, A.~Y., Russell, S.~J., et~al.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Icml}, volume~1, pp.\ ~2, 2000.

\bibitem[Paine et~al.(2020)Paine, Paduraru, Michi, Gulcehre, Zolna, Novikov,
  Wang, and de~Freitas]{paine2020hyperparameter}
Paine, T.~L., Paduraru, C., Michi, A., Gulcehre, C., Zolna, K., Novikov, A.,
  Wang, Z., and de~Freitas, N.
\newblock Hyperparameter selection for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.09055}, 2020.

\bibitem[Piot et~al.(2013)Piot, Geist, and Pietquin]{piot2013learning}
Piot, B., Geist, M., and Pietquin, O.
\newblock Learning from demonstrations: Is it worth estimating a reward
  function?
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  17--32. Springer, 2013.

\bibitem[Piot et~al.(2014)Piot, Geist, and Pietquin]{piot2014boosted}
Piot, B., Geist, M., and Pietquin, O.
\newblock Boosted bellman residual minimization handling expert demonstrations.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  549--564. Springer, 2014.

\bibitem[Pomerleau(1991)]{pomerleau1991efficient}
Pomerleau, D.~A.
\newblock Efficient training of artificial neural networks for autonomous
  navigation.
\newblock \emph{Neural computation}, 3\penalty0 (1):\penalty0 88--97, 1991.

\bibitem[Popov et~al.(2017)Popov, Heess, Lillicrap, Hafner, Barth-Maron,
  Vecerik, Lampe, Tassa, Erez, and Riedmiller]{popov2017data}
Popov, I., Heess, N., Lillicrap, T., Hafner, R., Barth-Maron, G., Vecerik, M.,
  Lampe, T., Tassa, Y., Erez, T., and Riedmiller, M.
\newblock Data-efficient deep reinforcement learning for dexterous
  manipulation.
\newblock \emph{arXiv preprint arXiv:1704.03073}, 2017.

\bibitem[Riedmiller(2005)]{riedmiller2005neural}
Riedmiller, M.
\newblock Neural fitted q iteration--first experiences with a data efficient
  neural reinforcement learning method.
\newblock In \emph{European Conference on Machine Learning}, pp.\  317--328.
  Springer, 2005.

\bibitem[Russell(1998)]{russell1998learning}
Russell, S.
\newblock Learning agents for uncertain environments.
\newblock In \emph{Conference on Computational learning theory}, 1998.

\bibitem[Schaal(1999)]{SCHAAL1999233}
Schaal, S.
\newblock Is imitation learning the route to humanoid robots?
\newblock \emph{Trends in Cognitive Sciences}, 3\penalty0 (6):\penalty0 233 --
  242, 1999.
\newblock ISSN 1364-6613.
\newblock \doi{https://doi.org/10.1016/S1364-6613(99)01327-3}.
\newblock URL
  \url{http://www.sciencedirect.com/science/article/pii/S1364661399013273}.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Sims(1994)]{sims1994evolving}
Sims, K.
\newblock Evolving virtual creatures.
\newblock In \emph{Proceedings of the 21st annual conference on Computer
  graphics and interactive techniques}, pp.\  15--22, 1994.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (56):\penalty0 1929--1958, 2014.
\newblock URL \url{http://jmlr.org/papers/v15/srivastava14a.html}.

\bibitem[Stooke et~al.(2020)Stooke, Lee, Abbeel, and Laskin]{atc}
Stooke, A., Lee, K., Abbeel, P., and Laskin, M.
\newblock Decoupling representation learning from reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2009.08319}, 2020.

\bibitem[Tesauro(1995)]{tesauro1995temporal}
Tesauro, G.
\newblock Temporal difference learning and td-gammon.
\newblock \emph{Communications of the ACM}, 38\penalty0 (3):\penalty0 58--68,
  1995.

\bibitem[Villani(2008)]{villani2008optimal}
Villani, C.
\newblock \emph{Optimal transport: old and new}.
\newblock 2008.

\bibitem[Vinyals et~al.(2017)Vinyals, Ewalds, Bartunov, Georgiev, Vezhnevets,
  Yeo, Makhzani, K{\"u}ttler, Agapiou, Schrittwieser,
  et~al.]{vinyals2017starcraft}
Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A.~S., Yeo,
  M., Makhzani, A., K{\"u}ttler, H., Agapiou, J., Schrittwieser, J., et~al.
\newblock Starcraft ii: A new challenge for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1708.04782}, 2017.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{starcraft}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang et~al.(2019)Wang, Ciliberto, Amadori, and
  Demiris]{wang2019random}
Wang, R., Ciliberto, C., Amadori, P.~V., and Demiris, Y.
\newblock Random expert distillation: Imitation learning via expert policy
  support estimation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6536--6544. PMLR, 2019.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Aaai}, volume~8, pp.\  1433--1438. Chicago, IL, USA, 2008.

\end{thebibliography}
