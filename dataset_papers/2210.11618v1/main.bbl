\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson et~al.(2021)Anderson, Grundy, Grady, Craik, and
  Bialystok]{anderson2021bilingualism}
John~AE Anderson, John~G Grundy, Cheryl~L Grady, Fergus~IM Craik, and Ellen
  Bialystok.
\newblock Bilingualism contributes to reserve and working memory efficiency:
  Evidence from structural and functional neuroimaging.
\newblock \emph{Neuropsychologia}, 163:\penalty0 108071, 2021.

\bibitem[Gold et~al.(2013)Gold, Johnson, and Powell]{gold2013lifelong}
Brian~T Gold, Nathan~F Johnson, and David~K Powell.
\newblock Lifelong bilingualism contributes to cognitive reserve against white
  matter integrity declines in aging.
\newblock \emph{Neuropsychologia}, 51\penalty0 (13):\penalty0 2841--2846, 2013.

\bibitem[Bialystok et~al.(2007)Bialystok, Craik, and
  Freedman]{bialystok2007bilingualism}
Ellen Bialystok, Fergus~IM Craik, and Morris Freedman.
\newblock Bilingualism as a protection against the onset of symptoms of
  dementia.
\newblock \emph{Neuropsychologia}, 45\penalty0 (2):\penalty0 459--464, 2007.

\bibitem[Craik et~al.(2010)Craik, Bialystok, and Freedman]{craik2010delaying}
Fergus~IM Craik, Ellen Bialystok, and Morris Freedman.
\newblock Delaying the onset of alzheimer disease: Bilingualism as a form of
  cognitive reserve.
\newblock \emph{Neurology}, 75\penalty0 (19):\penalty0 1726--1729, 2010.

\bibitem[Barulli and Stern(2013)]{barulli2013efficiency}
Daniel Barulli and Yaakov Stern.
\newblock Efficiency, capacity, compensation, maintenance, plasticity: emerging
  concepts in cognitive reserve.
\newblock \emph{Trends in cognitive sciences}, 17\penalty0 (10):\penalty0
  502--509, 2013.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han}
Song Han, Jeff Pool, John Tran, and William~J. Dally.
\newblock Learning both weights and connections for efficient neural networks.
\newblock In \emph{Proceedings of the 28th International Conference on Neural
  Information Processing Systems - Volume 1}, NIPS'15, page 1135–1143,
  Cambridge, MA, USA, 2015. MIT Press.

\bibitem[Maurer et~al.(2016)Maurer, Pontil, and Romera-Paredes]{Maurer}
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes.
\newblock The benefit of multitask representation learning.
\newblock \emph{J. Mach. Learn. Res.}, 17\penalty0 (1):\penalty0 2853–2884,
  jan 2016.
\newblock ISSN 1532-4435.

\bibitem[Du et~al.(2021)Du, Hu, Kakade, Lee, and Lei]{du2020few}
Simon~Shaolei Du, Wei Hu, Sham~M. Kakade, Jason~D. Lee, and Qi~Lei.
\newblock Few-shot learning via learning the representation, provably.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=pW2Q2xLwIMD}.

\bibitem[Golub and Van~Loan(1996)]{golub1996matrix}
Gene~H Golub and Charles~F Van~Loan.
\newblock Matrix computations. edition, 1996.

\bibitem[Blum et~al.(2020)Blum, Hopcroft, and Kannan]{blum2020foundations}
Avrim Blum, John Hopcroft, and Ravindran Kannan.
\newblock \emph{Foundations of data science}.
\newblock Cambridge University Press, 2020.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL}, 2019.

\bibitem[de~Vries and Nissim(2021)]{gpt_recycling}
Wietse de~Vries and Malvina Nissim.
\newblock As good as new. how to successfully recycle english gpt-2 to make
  models for other languages.
\newblock \emph{Findings of the Association for Computational Linguistics:
  ACL-IJCNLP 2021}, 2021.
\newblock \doi{10.18653/v1/2021.findings-acl.74}.
\newblock URL \url{http://dx.doi.org/10.18653/v1/2021.findings-acl.74}.

\bibitem[{Ortiz Su{\'a}rez} et~al.(2019){Ortiz Su{\'a}rez}, Sagot, and
  Romary]{OrtizSuarezSagotRomary2019}
Pedro~Javier {Ortiz Su{\'a}rez}, Beno{\^i}t Sagot, and Laurent Romary.
\newblock Asynchronous pipelines for processing huge corpora on medium to low
  resource infrastructures.
\newblock Proceedings of the Workshop on Challenges in the Management of Large
  Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019, pages 9 -- 16, Mannheim,
  2019. Leibniz-Institut f{\"u}r Deutsche Sprache.
\newblock \doi{10.14618/ids-pub-9021}.
\newblock URL \url{http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215}.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and
  Potts]{maas-etal-2011-learning}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th Annual Meeting of the Association
  for Computational Linguistics: Human Language Technologies}, pages 142--150,
  Portland, Oregon, USA, June 2011. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/P11-1015}.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,
  Pezzelle, Baroni, Boleda, and Fernández]{paperno2016lambada}
Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham,
  Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
  Fernández.
\newblock The lambada dataset: Word prediction requiring a broad discourse
  context, 2016.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{GLUE}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
  Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock \emph{Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing
  and Interpreting Neural Networks for NLP}, 2018.
\newblock \doi{10.18653/v1/w18-5446}.
\newblock URL \url{http://dx.doi.org/10.18653/v1/w18-5446}.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{sst}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1631--1642, 2013.

\bibitem[Cer et~al.(2017)Cer, Diab, Agirre, Lopez-Gazpio, and Specia]{stsb}
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia.
\newblock Semeval-2017 task 1: Semantic textual similarity multilingual and
  crosslingual focused evaluation.
\newblock \emph{Proceedings of the 11th International Workshop on Semantic
  Evaluation (SemEval-2017)}, 2017.
\newblock \doi{10.18653/v1/s17-2001}.
\newblock URL \url{http://dx.doi.org/10.18653/v1/S17-2001}.

\bibitem[Iyer et~al.(2017)Iyer, Dandekar, and Csernai]{qqp}
Shankar Iyer, Nikhil Dandekar, and Kornel Csernai.
\newblock First quora dataset release: Question pairs, 2017.
\newblock URL
  \url{https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs}.

\bibitem[Levesque et~al.(2012)Levesque, Davis, and Morgenstern]{wnli}
Hector Levesque, Ernest Davis, and Leora Morgenstern.
\newblock The winograd schema challenge.
\newblock In \emph{Thirteenth International Conference on the Principles of
  Knowledge Representation and Reasoning}, 2012.

\bibitem[Dagan et~al.(2005)Dagan, Glickman, and Magnini]{rte1}
Ido Dagan, Oren Glickman, and Bernardo Magnini.
\newblock The pascal recognising textual entailment challenge.
\newblock In \emph{Machine Learning Challenges Workshop}, pages 177--190.
  Springer, 2005.

\bibitem[Bar-Haim et~al.(2006)Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo,
  Magnini, and Szpektor]{rte2}
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo
  Magnini, and Idan Szpektor.
\newblock The second pascal recognising textual entailment challenge.
\newblock In \emph{Proceedings of the second PASCAL challenges workshop on
  recognising textual entailment}, volume~6, pages 6--4. Venice, 2006.

\bibitem[Giampiccolo et~al.(2007)Giampiccolo, Magnini, Dagan, and Dolan]{rte3}
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan.
\newblock The third pascal recognizing textual entailment challenge.
\newblock In \emph{Proceedings of the ACL-PASCAL workshop on textual entailment
  and paraphrasing}, pages 1--9. Association for Computational Linguistics,
  2007.

\bibitem[Bentivogli et~al.(2009)Bentivogli, Clark, Dagan, and
  Giampiccolo]{rte4}
Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo.
\newblock The fifth pascal recognizing textual entailment challenge.
\newblock In \emph{TAC}, 2009.

\bibitem[Dolan and Brockett(2005)]{mrpc}
William~B Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{Proceedings of the Third International Workshop on
  Paraphrasing (IWP2005)}, 2005.

\bibitem[Warstadt et~al.(2019)Warstadt, Singh, and Bowman]{cola}
Alex Warstadt, Amanpreet Singh, and Samuel~R Bowman.
\newblock Neural network acceptability judgments.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  7:\penalty0 625--641, 2019.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock \emph{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, 2016.
\newblock \doi{10.18653/v1/d16-1264}.
\newblock URL \url{http://dx.doi.org/10.18653/v1/D16-1264}.

\bibitem[Mao et~al.(2020)Mao, Gupta, Nitin, Ray, Song, Yang, and
  Vondrick]{multitask_adv}
Chengzhi Mao, Amogh Gupta, Vikram Nitin, Baishakhi Ray, Shuran Song, Junfeng
  Yang, and Carl Vondrick.
\newblock Multitask learning strengthens adversarial robustness.
\newblock \emph{Lecture Notes in Computer Science}, page 158–174, 2020.
\newblock ISSN 1611-3349.
\newblock \doi{10.1007/978-3-030-58536-5_10}.
\newblock URL \url{http://dx.doi.org/10.1007/978-3-030-58536-5_10}.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{wu2020adversarial}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2958--2969, 2020.

\bibitem[Ghamizi et~al.(2021)Ghamizi, Cordy, Papadakis, and
  Traon]{ghamizi2021adversarial}
Salah Ghamizi, Maxime Cordy, Mike Papadakis, and Yves~Le Traon.
\newblock Adversarial robustness in multi-task learning: Promises and
  illusions, 2021.

\bibitem[LeCun et~al.(1990)LeCun, Denker, and Solla]{NIPS1989_6c9882bb}
Yann LeCun, John Denker, and Sara Solla.
\newblock Optimal brain damage.
\newblock In D.~Touretzky, editor, \emph{Advances in Neural Information
  Processing Systems}, volume~2. Morgan-Kaufmann, 1990.

\bibitem[Frankle and Carbin(2019)]{frankle2018the}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Chen et~al.(2021)Chen, Zhang, Liu, Chang, and Wang]{chen2021long}
Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang.
\newblock Long live the lottery: The existence of winning tickets in lifelong
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Ledoux(2001)]{ledoux2001concentration}
Michel Ledoux.
\newblock \emph{The concentration of measure phenomenon}.
\newblock Number~89. American Mathematical Soc., 2001.

\bibitem[Hamborg et~al.(2017)Hamborg, Meuschke, Breitinger, and Gipp]{cc_news}
Felix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp.
\newblock news-please: A generic news crawler and extractor.
\newblock In \emph{Proceedings of the 15th International Symposium of
  Information Science}, pages 218--223, March 2017.
\newblock \doi{10.5281/zenodo.4120316}.

\bibitem[Bender et~al.(2021)Bender, Gebru, McMillan-Major, and
  Shmitchell]{bender2021dangers}
Emily~M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret
  Shmitchell.
\newblock On the dangers of stochastic parrots: Can language models be too big?
\newblock In \emph{Proceedings of the 2021 ACM Conference on Fairness,
  Accountability, and Transparency}, pages 610--623, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 32}, pages 8024--8035. Curran Associates,
  Inc., 2019.
\newblock URL
  \url{http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf2019huggingfaces}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing, 2019.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary,
  Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye
  Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\end{thebibliography}
