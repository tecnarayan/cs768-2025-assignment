% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{adlerRandomFieldsGeometry2007}{book}{}
      \name{author}{2}{}{%
        {{hash=6405a345f93041f5d5c00c3c6e5e6bb5}{%
           family={Adler},
           familyi={A\bibinitperiod},
           given={Robert\bibnamedelima J.},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=440b5b5ff563f3aa740ea8ae5dc66c6f}{%
           family={Taylor},
           familyi={T\bibinitperiod},
           given={Jonathan\bibnamedelima E.},
           giveni={J\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New York, NY}%
      }
      \list{publisher}{1}{%
        {Springer New York}%
      }
      \strng{namehash}{a74345724fd11c7691df92d67c947595}
      \strng{fullhash}{a74345724fd11c7691df92d67c947595}
      \strng{bibnamehash}{a74345724fd11c7691df92d67c947595}
      \strng{authorbibnamehash}{a74345724fd11c7691df92d67c947595}
      \strng{authornamehash}{a74345724fd11c7691df92d67c947595}
      \strng{authorfullhash}{a74345724fd11c7691df92d67c947595}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{978-0-387-48112-8}
      \field{issn}{1439-7382}
      \field{langid}{english}
      \field{series}{Springer {{Monographs}} in {{Mathematics}}}
      \field{title}{Random {{Fields}} and {{Geometry}}}
      \field{year}{2007}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.1007/978-0-387-48116-6
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2007_Adler_Taylor/Adler_Taylor_2007_Applications of Random Fields and Geometry Foundations and Case Studies.pdf;/Users/felix/paper/2007_Adler_Taylor/Adler_Taylor_2007_Random Fields and Geometry_old_notes.pdf;/Users/felix/paper/2007_Adler_Taylor/Adler_Taylor_2007_Random Fields and Geometry.pdf
      \endverb
      \keyw{Geometry,Mathematical Methods in Physics,Mathematics,Mathematics and Statistics,Probability Theory and Stochastic Processes,Statistics general}
    \endentry
    \entry{agnihotriExploringBayesianOptimization2020}{article}{}
      \name{author}{2}{}{%
        {{hash=57ec9a719c3e8f1a28b5c5bb68867e09}{%
           family={Agnihotri},
           familyi={A\bibinitperiod},
           given={Apoorv},
           giveni={A\bibinitperiod}}}%
        {{hash=6e0ed92e4f89cb3723b371ca56db6fde}{%
           family={Batra},
           familyi={B\bibinitperiod},
           given={Nipun},
           giveni={N\bibinitperiod}}}%
      }
      \strng{namehash}{8c1116655bbbe2cd57c3b4488587791d}
      \strng{fullhash}{8c1116655bbbe2cd57c3b4488587791d}
      \strng{bibnamehash}{8c1116655bbbe2cd57c3b4488587791d}
      \strng{authorbibnamehash}{8c1116655bbbe2cd57c3b4488587791d}
      \strng{authornamehash}{8c1116655bbbe2cd57c3b4488587791d}
      \strng{authorfullhash}{8c1116655bbbe2cd57c3b4488587791d}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{How to tune hyperparameters for your machine learning model using Bayesian optimization.}
      \field{day}{5}
      \field{issn}{2476-0757}
      \field{journaltitle}{Distill}
      \field{langid}{english}
      \field{month}{5}
      \field{number}{5}
      \field{shortjournal}{Distill}
      \field{title}{Exploring {{Bayesian Optimization}}}
      \field{volume}{5}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{pages}{e26}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.23915/distill.00026
      \endverb
      \verb{file}
      \verb /Users/felix/Zotero/storage/XV7HBLLR/distill-bayesian-optimization.png;/Users/felix/Zotero/storage/YXIIIXCJ/bayesian-optimization.html
      \endverb
    \endentry
    \entry{anEnsembleSimpleConvolutional2020}{online}{}
      \name{author}{5}{}{%
        {{hash=ec3e94c9a6fa7655f35a1faac83a709d}{%
           family={An},
           familyi={A\bibinitperiod},
           given={Sanghyeon},
           giveni={S\bibinitperiod}}}%
        {{hash=761ddbd06b721d05b98e1ff7d96ff408}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Minjun},
           giveni={M\bibinitperiod}}}%
        {{hash=d9730f4c920b674f1b84c99a703b797e}{%
           family={Park},
           familyi={P\bibinitperiod},
           given={Sanglee},
           giveni={S\bibinitperiod}}}%
        {{hash=164495751deac883eb2c56aa3db5ac5f}{%
           family={Yang},
           familyi={Y\bibinitperiod},
           given={Heerin},
           giveni={H\bibinitperiod}}}%
        {{hash=f463ebb21b49a0a93666c7254b0a49fb}{%
           family={So},
           familyi={S\bibinitperiod},
           given={Jungmin},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{c9a0af631b7f33929b8a52219b998925}
      \strng{fullhash}{5f2afbafd16d8fc21b2d8f457f28ab17}
      \strng{bibnamehash}{c9a0af631b7f33929b8a52219b998925}
      \strng{authorbibnamehash}{c9a0af631b7f33929b8a52219b998925}
      \strng{authornamehash}{c9a0af631b7f33929b8a52219b998925}
      \strng{authorfullhash}{5f2afbafd16d8fc21b2d8f457f28ab17}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We report that a very high accuracy on the MNIST test set can be achieved by using simple convolutional neural network (CNN) models. We use three different models with 3x3, 5x5, and 7x7 kernel size in the convolution layers. Each model consists of a set of convolution layers followed by a single fully connected layer. Every convolution layer uses batch normalization and ReLU activation, and pooling is not used. Rotation and translation is used to augment training data, which is frequently used in most image classification tasks. A majority voting using the three models independently trained on the training data set can achieve up to 99.87\% accuracy on the test set, which is one of the state-of-the-art results. A two-layer ensemble, a heterogeneous ensemble of three homogeneous ensemble networks, can achieve up to 99.91\% test accuracy. The results can be reproduced by using the code at: https://github.com/ansh941/MnistSimpleCNN}
      \field{day}{4}
      \field{month}{10}
      \field{pubstate}{prepublished}
      \field{title}{An {{Ensemble}} of {{Simple Convolutional Neural Network Models}} for {{MNIST Digit Recognition}}}
      \field{year}{2020}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2008.10400
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2020_An et al/An et al_2020_An Ensemble of Simple Convolutional Neural Network Models for MNIST Digit.pdf;/Users/felix/Zotero/storage/MMVJEDFF/2008.html
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
    \endentry
    \entry{auffingerComplexityGaussianRandom2023}{article}{}
      \name{author}{2}{}{%
        {{hash=52074dabe25be1576d00a5bedd1980fa}{%
           family={Auffinger},
           familyi={A\bibinitperiod},
           given={Antonio},
           giveni={A\bibinitperiod}}}%
        {{hash=cbc415fc36c599555cb13b0e962f1c25}{%
           family={Zeng},
           familyi={Z\bibinitperiod},
           given={Qiang},
           giveni={Q\bibinitperiod}}}%
      }
      \strng{namehash}{e1f69444a8684cf9850f885725a7d260}
      \strng{fullhash}{e1f69444a8684cf9850f885725a7d260}
      \strng{bibnamehash}{e1f69444a8684cf9850f885725a7d260}
      \strng{authorbibnamehash}{e1f69444a8684cf9850f885725a7d260}
      \strng{authornamehash}{e1f69444a8684cf9850f885725a7d260}
      \strng{authorfullhash}{e1f69444a8684cf9850f885725a7d260}
      \field{sortinit}{A}
      \field{sortinithash}{2f401846e2029bad6b3ecc16d50031e2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study the energy landscape of a model of a single particle on a random potential, that is, we investigate the topology of level sets of smooth random fields on \$\$\{\textbackslash mathbb \{R\}\}\textasciicircum\{N\}\$\$of the form \$\$X\_N(x) +\textbackslash frac\{\textbackslash mu \}\{2\} \textbackslash Vert x\textbackslash Vert \textasciicircum 2,\$\$where \$\$X\_\{N\}\$\$is a Gaussian process with isotropic increments. We derive asymptotic formulas for the mean number of critical points with critical values in an open set as the dimension N goes to infinity. In a companion paper, we provide the same analysis for the number of critical points with a given index.}
      \field{day}{1}
      \field{issn}{1432-0916}
      \field{journaltitle}{Communications in Mathematical Physics}
      \field{langid}{english}
      \field{month}{8}
      \field{number}{1}
      \field{shortjournal}{Commun. Math. Phys.}
      \field{title}{Complexity of {{Gaussian Random Fields}} with {{Isotropic Increments}}}
      \field{volume}{402}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{pages}{951\bibrangedash 993}
      \range{pages}{43}
      \verb{doi}
      \verb 10.1007/s00220-023-04739-0
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2023_Auffinger_Zeng/Auffinger_Zeng_2023_Complexity of Gaussian Random Fields with Isotropic Increments.pdf
      \endverb
    \endentry
    \entry{benningGradientSpanAlgorithms2024}{online}{}
      \name{author}{2}{}{%
        {{hash=f65f3ef30b2c05fb994c108111f042a7}{%
           family={Benning},
           familyi={B\bibinitperiod},
           given={Felix},
           giveni={F\bibinitperiod}}}%
        {{hash=7ee3f9d39a90b80f2587712d132a6b6d}{%
           family={Döring},
           familyi={D\bibinitperiod},
           given={Leif},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{d4e6b9c1abae09fdf4c4960d6e5dc54d}
      \strng{fullhash}{d4e6b9c1abae09fdf4c4960d6e5dc54d}
      \strng{bibnamehash}{d4e6b9c1abae09fdf4c4960d6e5dc54d}
      \strng{authorbibnamehash}{d4e6b9c1abae09fdf4c4960d6e5dc54d}
      \strng{authornamehash}{d4e6b9c1abae09fdf4c4960d6e5dc54d}
      \strng{authorfullhash}{d4e6b9c1abae09fdf4c4960d6e5dc54d}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We prove that all 'gradient span algorithms' have asymptotically deterministic behavior on scaled Gaussian random functions as the dimension tends to infinity. In particular, this result explains the counterintuitive phenomenon that different training runs of many large machine learning models result in approximately equal cost curves despite random initialization on a complicated non-convex landscape. The distributional assumption of (non-stationary) isotropic Gaussian random functions we use is sufficiently general to serve as realistic model for machine learning training but also encompass spin glasses and random quadratic functions.}
      \field{day}{13}
      \field{month}{10}
      \field{pubstate}{prepublished}
      \field{title}{Gradient {{Span Algorithms Make Predictable Progress}} in {{High Dimension}}}
      \field{year}{2024}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2410.09973
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2024_Benning_Döring/Benning_Döring_2024_Gradient Span Algorithms Make Predictable Progress in High Dimension.pdf;/Users/felix/Zotero/storage/B2SP78C5/2410.html
      \endverb
      \keyw{Computer Science - Machine Learning,Mathematics - Optimization and Control,Mathematics - Probability,Statistics - Machine Learning}
    \endentry
    \entry{borgwardtSimplexMethodProbabilistic1986}{book}{}
      \name{author}{1}{}{%
        {{hash=4441bfcc76985883a97c03b116e19eb1}{%
           family={Borgwardt},
           familyi={B\bibinitperiod},
           given={Karl\bibnamedelima Heinz},
           giveni={K\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Berlin Heidelberg}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{4441bfcc76985883a97c03b116e19eb1}
      \strng{fullhash}{4441bfcc76985883a97c03b116e19eb1}
      \strng{bibnamehash}{4441bfcc76985883a97c03b116e19eb1}
      \strng{authorbibnamehash}{4441bfcc76985883a97c03b116e19eb1}
      \strng{authornamehash}{4441bfcc76985883a97c03b116e19eb1}
      \strng{authorfullhash}{4441bfcc76985883a97c03b116e19eb1}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{For more than 35 years now, George B. Dantzig's Simplex-Method has been the most efficient mathematical tool for solving linear programming problems. It is proba­ bly that mathematical algorithm for which the most computation time on computers is spent. This fact explains the great interest of experts and of the public to understand the method and its efficiency. But there are linear programming problems which will not be solved by a given variant of the Simplex-Method in an acceptable time. The discrepancy between this (negative) theoretical result and the good practical behaviour of the method has caused a great fascination for many years. While the "worst-case analysis" of some variants of the method shows that this is not a "good" algorithm in the usual sense of complexity theory, it seems to be useful to apply other criteria for a judgement concerning the quality of the algorithm. One of these criteria is the average computation time, which amounts to an anal­ ysis of the average number of elementary arithmetic computations and of the number of pivot steps. A rigid analysis of the average behaviour may be very helpful for the decision which algorithm and which variant shall be used in practical applications. The subject and purpose of this book is to explain the great efficiency in prac­ tice by assuming certain distributions on the "real-world" -problems. Other stochastic models are realistic as well and so this analysis should be considered as one of many possibilities.}
      \field{day}{1}
      \field{edition}{Softcover reprint of the original 1st ed. 1987 edition}
      \field{isbn}{978-3-540-17096-9}
      \field{langid}{english}
      \field{month}{11}
      \field{pagetotal}{282}
      \field{shorttitle}{The {{Simplex Method}}}
      \field{title}{The {{Simplex Method}}: {{A Probabilistic Analysis}}}
      \field{year}{1986}
      \field{dateera}{ce}
      \verb{file}
      \verb /Users/felix/paper/1986_Borgwardt/Borgwardt_1986_The Simplex Method.pdf
      \endverb
    \endentry
    \entry{bubeckUniversalLawRobustness2021}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=54fdec6da88ea6f8ac33ca7d8ba4651a}{%
           family={Bubeck},
           familyi={B\bibinitperiod},
           given={Sebastien},
           giveni={S\bibinitperiod}}}%
        {{hash=1b878d86cd8e2f2fbba6b5592fb42608}{%
           family={Sellke},
           familyi={S\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Virtual Event}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{92187df755bb42ac5d7d27c3828b3ed3}
      \strng{fullhash}{92187df755bb42ac5d7d27c3828b3ed3}
      \strng{bibnamehash}{92187df755bb42ac5d7d27c3828b3ed3}
      \strng{authorbibnamehash}{92187df755bb42ac5d7d27c3828b3ed3}
      \strng{authornamehash}{92187df755bb42ac5d7d27c3828b3ed3}
      \strng{authorfullhash}{92187df755bb42ac5d7d27c3828b3ed3}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{title}{A {{Universal Law}} of {{Robustness}} via {{Isoperimetry}}}
      \field{urlday}{22}
      \field{urlmonth}{9}
      \field{urlyear}{2023}
      \field{volume}{34}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{28811\bibrangedash 28822}
      \range{pages}{12}
      \verb{eprint}
      \verb 2105.12806
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2021_Bubeck_Sellke/[Arxiv] Bubeck_Sellke_2021_A Universal Law of Robustness via Isoperimetry.pdf;/Users/felix/paper/2021_Bubeck_Sellke/[Supplemental] Bubeck_Sellke_2021_A Universal Law of Robustness via Isoperimetry.pdf;/Users/felix/paper/2021_Bubeck_Sellke/Bubeck_Sellke_2021_A Universal Law of Robustness via Isoperimetry.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2021/hash/f197002b9a0853eca5e046d9ca4663d5-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2021/hash/f197002b9a0853eca5e046d9ca4663d5-Abstract.html
      \endverb
    \endentry
    \entry{choKernelMethodsDeep2009}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=f0a835a38700b8665b6910c550d051a5}{%
           family={Cho},
           familyi={C\bibinitperiod},
           given={Youngmin},
           giveni={Y\bibinitperiod}}}%
        {{hash=b4b914bc0b7ce87893ad1306ee6be5ba}{%
           family={Saul},
           familyi={S\bibinitperiod},
           given={Lawrence},
           giveni={L\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{13fcfcf199eca6c9c3f09b5296c854d1}
      \strng{fullhash}{13fcfcf199eca6c9c3f09b5296c854d1}
      \strng{bibnamehash}{13fcfcf199eca6c9c3f09b5296c854d1}
      \strng{authorbibnamehash}{13fcfcf199eca6c9c3f09b5296c854d1}
      \strng{authornamehash}{13fcfcf199eca6c9c3f09b5296c854d1}
      \strng{authorfullhash}{13fcfcf199eca6c9c3f09b5296c854d1}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets. These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Kernel {{Methods}} for {{Deep Learning}}}
      \field{urlday}{3}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{volume}{22}
      \field{year}{2009}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/felix/paper/2009_Cho_Saul/Cho_Saul_2009_Kernel Methods for Deep Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2009/hash/5751ec3e9a4feab575962e78e006250d-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2009/hash/5751ec3e9a4feab575962e78e006250d-Abstract.html
      \endverb
    \endentry
    \entry{cunhaOnlyTailsMatter2022}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=9090bd5cb4082d7abe373cb1e7cfa2eb}{%
           family={Cunha},
           familyi={C\bibinitperiod},
           given={Leonardo},
           giveni={L\bibinitperiod}}}%
        {{hash=9bf237a59d859c698045c3e46e4abc83}{%
           family={Gidel},
           familyi={G\bibinitperiod},
           given={Gauthier},
           giveni={G\bibinitperiod}}}%
        {{hash=bab4e5caee2d67831e464ce575022b37}{%
           family={Pedregosa},
           familyi={P\bibinitperiod},
           given={Fabian},
           giveni={F\bibinitperiod}}}%
        {{hash=cec256cc503bc0f0752bca34d0445248}{%
           family={Scieur},
           familyi={S\bibinitperiod},
           given={Damien},
           giveni={D\bibinitperiod}}}%
        {{hash=8da8a91d96c88a39bd8ae5015c9fe2c5}{%
           family={Paquette},
           familyi={P\bibinitperiod},
           given={Courtney},
           giveni={C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{3b91a3f93fd04de2a04fd3e74c6f2247}
      \strng{fullhash}{4e8f82bbf7cefe0284de399f772f2859}
      \strng{bibnamehash}{3b91a3f93fd04de2a04fd3e74c6f2247}
      \strng{authorbibnamehash}{3b91a3f93fd04de2a04fd3e74c6f2247}
      \strng{authornamehash}{3b91a3f93fd04de2a04fd3e74c6f2247}
      \strng{authorfullhash}{4e8f82bbf7cefe0284de399f772f2859}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{The recently developed average-case analysis of optimization methods allows a more fine-grained and representative convergence analysis than usual worst-case results. In exchange, this analysis requires a more precise hypothesis over the data generating process, namely assuming knowledge of the expected spectral distribution (ESD) of the random matrix associated with the problem. This work shows that the concentration of eigenvalues near the edges of the ESD determines a problem’s asymptotic average complexity. This a priori information on this concentration is a more grounded assumption than complete knowledge of the ESD. This approximate concentration is effectively a middle ground between the coarseness of the worst-case scenario convergence and the restrictive previous average-case analysis. We also introduce the Generalized Chebyshev method, asymptotically optimal under a hypothesis on this concentration and globally optimal when the ESD follows a Beta distribution. We compare its performance to classical optimization algorithms, such as gradient descent or Nesterov’s scheme, and we show that, in the average-case context, Nesterov’s method is universally nearly optimal asymptotically.}
      \field{booktitle}{Proceedings of the 39th {{International Conference}} on {{Machine Learning}}}
      \field{day}{28}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{6}
      \field{shorttitle}{Only Tails Matter}
      \field{title}{Only Tails Matter: {{Average-Case Universality}} and {{Robustness}} in the {{Convex Regime}}}
      \field{urlday}{9}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{4474\bibrangedash 4491}
      \range{pages}{18}
      \verb{file}
      \verb /Users/felix/paper/2022_Cunha et al/Cunha et al_2022_Only tails matter.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v162/cunha22a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v162/cunha22a.html
      \endverb
    \endentry
    \entry{dauphinIdentifyingAttackingSaddle2014}{inproceedings}{}
      \name{author}{6}{}{%
        {{hash=ea1ca71b064fbb7ec15bd2e49e287ea9}{%
           family={Dauphin},
           familyi={D\bibinitperiod},
           given={Yann\bibnamedelima N},
           giveni={Y\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
        {{hash=7045b009b04d57bd2e19b5dfa0864d4f}{%
           family={Pascanu},
           familyi={P\bibinitperiod},
           given={Razvan},
           giveni={R\bibinitperiod}}}%
        {{hash=2adc0c92c308f233c731321d55efe58f}{%
           family={Gulcehre},
           familyi={G\bibinitperiod},
           given={Caglar},
           giveni={C\bibinitperiod}}}%
        {{hash=3da7501a79d9346572c7fd6e41b615df}{%
           family={Cho},
           familyi={C\bibinitperiod},
           given={Kyunghyun},
           giveni={K\bibinitperiod}}}%
        {{hash=05b3e391388084df874ede60f2210c12}{%
           family={Ganguli},
           familyi={G\bibinitperiod},
           given={Surya},
           giveni={S\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Montréal, Canada}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{91d61729ce47b72febc33bd9db02bdac}
      \strng{fullhash}{ed383fe3b9caf384b339905974cb1155}
      \strng{bibnamehash}{91d61729ce47b72febc33bd9db02bdac}
      \strng{authorbibnamehash}{91d61729ce47b72febc33bd9db02bdac}
      \strng{authornamehash}{91d61729ce47b72febc33bd9db02bdac}
      \strng{authorfullhash}{ed383fe3b9caf384b339905974cb1155}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization}
      \field{urlday}{10}
      \field{urlmonth}{6}
      \field{urlyear}{2022}
      \field{volume}{27}
      \field{year}{2014}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/felix/paper/2014_Dauphin et al/Dauphin et al_2014_Identifying and attacking the saddle point problem in high-dimensional.pdf;/Users/felix/paper/2014_Dauphin et al/Dauphin et al_2014_Identifying and attacking the saddle point problem in high-dimensional2.pdf;/Users/felix/Zotero/storage/ZICT2QCI/1406.html
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2014/hash/17e23e50bedc63b4095e3d8204ce063b-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2014/hash/17e23e50bedc63b4095e3d8204ce063b-Abstract.html
      \endverb
    \endentry
    \entry{defazioLearningRateFreeLearningDAdaptation2023}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=a5551dfae979b8c2befb546fac59ffee}{%
           family={Defazio},
           familyi={D\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod}}}%
        {{hash=dbe35b250bff9f2ca1bdf7d01177b0d9}{%
           family={Mishchenko},
           familyi={M\bibinitperiod},
           given={Konstantin},
           giveni={K\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{a1ee5f43b476e8cd209466ab74860898}
      \strng{fullhash}{a1ee5f43b476e8cd209466ab74860898}
      \strng{bibnamehash}{a1ee5f43b476e8cd209466ab74860898}
      \strng{authorbibnamehash}{a1ee5f43b476e8cd209466ab74860898}
      \strng{authornamehash}{a1ee5f43b476e8cd209466ab74860898}
      \strng{authorfullhash}{a1ee5f43b476e8cd209466ab74860898}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The speed of gradient descent for convex Lipschitz functions is highly dependent on the choice of learning rate. Setting the learning rate to achieve the optimal convergence rate requires knowing the distance D from the initial point to the solution set. In this work, we describe a single-loop method, with no back-tracking or line searches, which does not require knowledge of D yet asymptotically achieves the optimal rate of convergence for the complexity class of convex Lipschitz functions. Our approach is the first parameter-free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems. Our method is practical, efficient and requires no additional function value or gradient evaluations each step. An implementation is provided in the supplementary material.}
      \field{booktitle}{Proceedings of the 40th {{International Conference}} on {{Machine Learning}}}
      \field{day}{3}
      \field{eprintclass}{cs.LG}
      \field{eprinttype}{arXiv}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{7}
      \field{title}{Learning-{{Rate-Free Learning}} by {{D-Adaptation}}}
      \field{urlday}{31}
      \field{urlmonth}{3}
      \field{urlyear}{2024}
      \field{year}{2023}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{7449\bibrangedash 7479}
      \range{pages}{31}
      \verb{eprint}
      \verb 2301.07733
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2023_Defazio_Mishchenko/Defazio_Mishchenko_2023_Learning-Rate-Free Learning by D-Adaptation.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v202/defazio23a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v202/defazio23a.html
      \endverb
    \endentry
    \entry{deiftConjugateGradientAlgorithm2021}{article}{}
      \name{author}{2}{}{%
        {{hash=3b46a9e4076cb2e3664e72f6c20e01bb}{%
           family={Deift},
           familyi={D\bibinitperiod},
           given={Percy},
           giveni={P\bibinitperiod}}}%
        {{hash=d3052222f56223c9897a048b4ebd5cb2}{%
           family={Trogdon},
           familyi={T\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{5c0699b21506793ed922170558bd8853}
      \strng{fullhash}{5c0699b21506793ed922170558bd8853}
      \strng{bibnamehash}{5c0699b21506793ed922170558bd8853}
      \strng{authorbibnamehash}{5c0699b21506793ed922170558bd8853}
      \strng{authornamehash}{5c0699b21506793ed922170558bd8853}
      \strng{authorfullhash}{5c0699b21506793ed922170558bd8853}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We prove that the number of iterations required to solve a random positive definite linear system with the conjugate gradient algorithm is almost deterministic for large matrices. We treat the case of Wishart matrices \$W = XX\textasciicircum *\$ where \$X\$ is \$n \textbackslash times m\$ and \$n/m \textbackslash sim d\$ for \$0 {$<$} d {$<$} 1\$. Precisely, we prove that for most choices of error tolerance, as the matrix increases in size, the probability that the iteration count deviates from an explicit deterministic value tends to zero. In addition, for a fixed iteration count, we show that the norm of the error vector and the norm of the residual converge exponentially fast in probability, converge in mean, and converge almost surely.}
      \field{eprintclass}{cs, math}
      \field{eprinttype}{arXiv}
      \field{issn}{0033-569X, 1552-4485}
      \field{journaltitle}{Quarterly of Applied Mathematics}
      \field{langid}{english}
      \field{month}{3}
      \field{number}{1}
      \field{shortjournal}{Quart. Appl. Math.}
      \field{title}{The Conjugate Gradient Algorithm on Well-Conditioned {{Wishart}} Matrices Is Almost Deterministic}
      \field{volume}{79}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{pages}{125\bibrangedash 161}
      \range{pages}{37}
      \verb{doi}
      \verb 10.1090/qam/1574
      \endverb
      \verb{eprint}
      \verb 1901.09007
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2021_Deift_Trogdon/Deift_Trogdon_2021_The conjugate gradient algorithm on well-conditioned Wishart matrices is almost.pdf
      \endverb
    \endentry
    \entry{deuflhardAffineInvariantConvergence1979}{article}{}
      \name{author}{2}{}{%
        {{hash=9c16313884836d624cb51b3010b713d2}{%
           family={Deuflhard},
           familyi={D\bibinitperiod},
           given={P.},
           giveni={P\bibinitperiod}}}%
        {{hash=343046ce1dfbb6a82c7c31f11a38584e}{%
           family={Heindl},
           familyi={H\bibinitperiod},
           given={G.},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Society for Industrial and Applied Mathematics}%
      }
      \strng{namehash}{07cc9d1bba4b1bd22c2bdb370eab1ebc}
      \strng{fullhash}{07cc9d1bba4b1bd22c2bdb370eab1ebc}
      \strng{bibnamehash}{07cc9d1bba4b1bd22c2bdb370eab1ebc}
      \strng{authorbibnamehash}{07cc9d1bba4b1bd22c2bdb370eab1ebc}
      \strng{authornamehash}{07cc9d1bba4b1bd22c2bdb370eab1ebc}
      \strng{authorfullhash}{07cc9d1bba4b1bd22c2bdb370eab1ebc}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A classical algorithm for solving the system of nonlinear equations \$F(x) = 0\$ is Newton’s method \textbackslash [ x\_\{k + 1\} = x\_k + s\_k ,\textbackslash quad \{\textbackslash text\{where \}\}F'(x\_k )s\_k = - F(x\_k ),\textbackslash quad x\_0 \{\textbackslash text\{ given\}\}.\textbackslash ] The method is attractive because it converges rapidly from any sufficiently good initial guess \$x\_0 \$. However, solving a system of linear equations (the Newton equations) at each stage can be expensive if the number of unknowns is large and may not be justified when \$x\_k \$ is far from a solution. Therefore, we consider the class of inexact Newton methods: \textbackslash [ x\_\{k + 1\} = x\_k + s\_k ,\textbackslash quad \{\textbackslash text\{where \}\}F'(x\_k )s\_k = - F(x\_k ) + r\_k ,\textbackslash quad \{\{\textbackslash left\textbackslash | \{r\_k \} \textbackslash right\textbackslash |\} / \{\textbackslash left\textbackslash | \{F(x\_k )\} \textbackslash right\textbackslash |\}\} \textbackslash leqq \textbackslash eta \_k \textbackslash ] which solve the Newton equations only approximately and in some unspecified manner. Under the natural assumption that the forcing sequence \$\textbackslash\{ n\_k \textbackslash\} \$ is uniformly less than one, we show that all such methods are locally convergent and characterize the order of convergence in terms of the rate of convergence of the relative residuals \$\textbackslash\{ \{\{\textbackslash |r\_k \textbackslash |\} / \{\textbackslash |F(x\_k )\textbackslash |\}\}\textbackslash\} \$.Finally, we indicate how these general results can be used to construct and analyze specific methods for solving systems of nonlinear equations.}
      \field{issn}{0036-1429}
      \field{journaltitle}{SIAM Journal on Numerical Analysis}
      \field{month}{2}
      \field{number}{1}
      \field{shortjournal}{SIAM J. Numer. Anal.}
      \field{title}{Affine {{Invariant Convergence Theorems}} for {{Newton}}’s {{Method}} and {{Extensions}} to {{Related Methods}}}
      \field{volume}{16}
      \field{year}{1979}
      \field{dateera}{ce}
      \field{pages}{1\bibrangedash 10}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1137/0716001
      \endverb
      \verb{file}
      \verb /Users/felix/paper/1979_Deuflhard_Heindl/Deuflhard_Heindl_1979_Affine Invariant Convergence Theorems for Newton’s Method and Extensions to.pdf
      \endverb
    \endentry
    \entry{duchiAdaptiveSubgradientMethods2011}{article}{}
      \name{author}{3}{}{%
        {{hash=b8fbef1897da5bf46822ced31bc865c6}{%
           family={Duchi},
           familyi={D\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
        {{hash=0b3d32703edd2b7248e8e8f33c5893db}{%
           family={Hazan},
           familyi={H\bibinitperiod},
           given={Elad},
           giveni={E\bibinitperiod}}}%
        {{hash=300d4990e626d975e0c28630444f63c3}{%
           family={Singer},
           familyi={S\bibinitperiod},
           given={Yoram},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{b8b34bd771a4e5e8cbaec4bd55417929}
      \strng{fullhash}{21039f1d6e97f13dbd40c187a8131cf0}
      \strng{bibnamehash}{21039f1d6e97f13dbd40c187a8131cf0}
      \strng{authorbibnamehash}{21039f1d6e97f13dbd40c187a8131cf0}
      \strng{authornamehash}{b8b34bd771a4e5e8cbaec4bd55417929}
      \strng{authorfullhash}{21039f1d6e97f13dbd40c187a8131cf0}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.}
      \field{day}{1}
      \field{issn}{1532-4435}
      \field{journaltitle}{The Journal of Machine Learning Research}
      \field{month}{7}
      \field{shortjournal}{J. Mach. Learn. Res.}
      \field{title}{Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}}
      \field{volume}{12}
      \field{year}{2011}
      \field{dateera}{ce}
      \field{pages}{2121\bibrangedash 2159}
      \range{pages}{39}
      \verb{file}
      \verb /Users/felix/paper/2011_Duchi et al/Duchi et al_2011_Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf
      \endverb
    \endentry
    \entry{elalaouiOptimizationMeanfieldSpin2021}{article}{}
      \name{author}{3}{}{%
        {{hash=058520b457c5e352bf1c64bb506723ee}{%
           family={El\bibnamedelima Alaoui},
           familyi={E\bibinitperiod\bibinitdelim A\bibinitperiod},
           given={Ahmed},
           giveni={A\bibinitperiod}}}%
        {{hash=484a542ca834affa98f81afbd90fba25}{%
           family={Montanari},
           familyi={M\bibinitperiod},
           given={Andrea},
           giveni={A\bibinitperiod}}}%
        {{hash=1b878d86cd8e2f2fbba6b5592fb42608}{%
           family={Sellke},
           familyi={S\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Institute of Mathematical Statistics}%
      }
      \strng{namehash}{30467be5ee5aa8b984b7353e94fd18dd}
      \strng{fullhash}{32b16d9ddeffc1e1c9257fd9aa0a51cd}
      \strng{bibnamehash}{32b16d9ddeffc1e1c9257fd9aa0a51cd}
      \strng{authorbibnamehash}{32b16d9ddeffc1e1c9257fd9aa0a51cd}
      \strng{authornamehash}{30467be5ee5aa8b984b7353e94fd18dd}
      \strng{authorfullhash}{32b16d9ddeffc1e1c9257fd9aa0a51cd}
      \field{sortinit}{E}
      \field{sortinithash}{8da8a182d344d5b9047633dfc0cc9131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{The Annals of Probability}
      \field{month}{11}
      \field{number}{6}
      \field{title}{Optimization of Mean-Field Spin Glasses}
      \field{volume}{49}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{pages}{2922\bibrangedash 2960}
      \range{pages}{39}
      \verb{doi}
      \verb 10.1214/21-AOP1519
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2021_El Alaoui et al/[Arxiv] El Alaoui et al_2021_Optimization of mean-field spin glasses.pdf;/Users/felix/paper/2021_El Alaoui et al/El Alaoui et al_2021_Optimization of mean-field spin glasses.pdf
      \endverb
    \endentry
    \entry{frazierBayesianOptimization2018}{incollection}{}
      \name{author}{1}{}{%
        {{hash=f59904428dedd78a02ae194a44057c63}{%
           family={Frazier},
           familyi={F\bibinitperiod},
           given={Peter\bibnamedelima I.},
           giveni={P\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Phoenix, Arizona, USA}%
      }
      \list{publisher}{1}{%
        {INFORMS}%
      }
      \strng{namehash}{f59904428dedd78a02ae194a44057c63}
      \strng{fullhash}{f59904428dedd78a02ae194a44057c63}
      \strng{bibnamehash}{f59904428dedd78a02ae194a44057c63}
      \strng{authorbibnamehash}{f59904428dedd78a02ae194a44057c63}
      \strng{authornamehash}{f59904428dedd78a02ae194a44057c63}
      \strng{authorfullhash}{f59904428dedd78a02ae194a44057c63}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Recent {{Advances}} in {{Optimization}} and {{Modeling}} of {{Contemporary Problems}}}
      \field{eprintclass}{cs, math, stat}
      \field{eprinttype}{arXiv}
      \field{isbn}{978-0-9906153-2-3}
      \field{month}{10}
      \field{series}{{{INFORMS TutORials}} in {{Operations Research}}}
      \field{title}{Bayesian {{Optimization}}}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{pages}{255\bibrangedash 278}
      \range{pages}{24}
      \verb{doi}
      \verb 10.1287/educ.2018.0188
      \endverb
      \verb{eprint}
      \verb 1807.02811
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2018_Frazier/Frazier_2018_A Tutorial on Bayesian Optimization.pdf;/Users/felix/paper/2018_Frazier/Frazier_2018_Bayesian Optimization.pdf
      \endverb
      \keyw{Bayesian optimization,Computer Science - Machine Learning,derivative-free optimization,entropy search,expected improvement,Gaussian processes,knowledge-gradient methods,Mathematics - Optimization and Control,optimization of expensive functions,Statistics - Machine Learning,surrogate-based optimization}
    \endentry
    \entry{gaoImplementingNelderMeadSimplex2012}{article}{}
      \name{author}{2}{}{%
        {{hash=7a0b1a1f91828e25a65fe807d6e1c651}{%
           family={Gao},
           familyi={G\bibinitperiod},
           given={Fuchang},
           giveni={F\bibinitperiod}}}%
        {{hash=f97c7b110779475ebbc54bf6f131e753}{%
           family={Han},
           familyi={H\bibinitperiod},
           given={Lixing},
           giveni={L\bibinitperiod}}}%
      }
      \strng{namehash}{06e9e94332657d5a1d7c5393513fd38d}
      \strng{fullhash}{06e9e94332657d5a1d7c5393513fd38d}
      \strng{bibnamehash}{06e9e94332657d5a1d7c5393513fd38d}
      \strng{authorbibnamehash}{06e9e94332657d5a1d7c5393513fd38d}
      \strng{authornamehash}{06e9e94332657d5a1d7c5393513fd38d}
      \strng{authorfullhash}{06e9e94332657d5a1d7c5393513fd38d}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this paper, we first prove that the expansion and contraction steps of the Nelder-Mead simplex algorithm possess a descent property when the objective function is uniformly convex. This property provides some new insights on why the standard Nelder-Mead algorithm becomes inefficient in high dimensions. We then propose an implementation of the Nelder-Mead method in which the expansion, contraction, and shrink parameters depend on the dimension of the optimization problem. Our numerical experiments show that the new implementation outperforms the standard Nelder-Mead method for high dimensional problems.}
      \field{day}{1}
      \field{issn}{1573-2894}
      \field{journaltitle}{Computational Optimization and Applications}
      \field{langid}{english}
      \field{month}{1}
      \field{number}{1}
      \field{shortjournal}{Comput Optim Appl}
      \field{title}{Implementing the {{Nelder-Mead}} Simplex Algorithm with~Adaptive Parameters}
      \field{volume}{51}
      \field{year}{2012}
      \field{dateera}{ce}
      \field{pages}{259\bibrangedash 277}
      \range{pages}{19}
      \verb{doi}
      \verb 10.1007/s10589-010-9329-3
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2012_Gao_Han/Gao_Han_2012_Implementing the Nelder-Mead simplex algorithm with adaptive parameters.pdf
      \endverb
      \keyw{Adaptive parameter,Nelder-Mead method,Optimization,Polytope,Simplex}
    \endentry
    \entry{glorotUnderstandingDifficultyTraining2010}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=0b3943c3bfdbb5867b3760f7c7d488c2}{%
           family={Glorot},
           familyi={G\bibinitperiod},
           given={Xavier},
           giveni={X\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Sardinia, Italy}%
      }
      \list{publisher}{1}{%
        {JMLR Workshop and Conference Proceedings}%
      }
      \strng{namehash}{02af15243279c938a0a5ca766835bcd4}
      \strng{fullhash}{02af15243279c938a0a5ca766835bcd4}
      \strng{bibnamehash}{02af15243279c938a0a5ca766835bcd4}
      \strng{authorbibnamehash}{02af15243279c938a0a5ca766835bcd4}
      \strng{authornamehash}{02af15243279c938a0a5ca766835bcd4}
      \strng{authorfullhash}{02af15243279c938a0a5ca766835bcd4}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
      \field{booktitle}{Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}
      \field{day}{31}
      \field{eventtitle}{Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}}
      \field{issn}{1938-7228}
      \field{langid}{english}
      \field{month}{3}
      \field{title}{Understanding the Difficulty of Training Deep Feedforward Neural Networks}
      \field{urlday}{11}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{year}{2010}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{249\bibrangedash 256}
      \range{pages}{8}
      \verb{file}
      \verb /Users/felix/paper/2010_Glorot_Bengio/Glorot_Bengio_2010_Understanding the difficulty of training deep feedforward neural networks.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v9/glorot10a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v9/glorot10a.html
      \endverb
    \endentry
    \entry{goodfellowDeepLearning2016}{book}{}
      \name{author}{3}{}{%
        {{hash=5d2585c11210cf1d4512e6e0a03ec315}{%
           family={Goodfellow},
           familyi={G\bibinitperiod},
           given={Ian},
           giveni={I\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
        {{hash=ccec1ccd2e1aa86960eb2e872c6b7020}{%
           family={Courville},
           familyi={C\bibinitperiod},
           given={Aaron},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{d17e6557c5836d2d978179999ea1037f}
      \strng{fullhash}{3ae53fe582e8a815b118d26947eaa326}
      \strng{bibnamehash}{3ae53fe582e8a815b118d26947eaa326}
      \strng{authorbibnamehash}{3ae53fe582e8a815b118d26947eaa326}
      \strng{authornamehash}{d17e6557c5836d2d978179999ea1037f}
      \strng{authorfullhash}{3ae53fe582e8a815b118d26947eaa326}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{An introduction to a broad range of topics in deep learning, covering mathematical and conceptual background, deep learning techniques used in industry, and research perspectives.“Written by three experts in the field, Deep Learning is the only comprehensive book on the subject.”—Elon Musk, cochair of OpenAI; cofounder and CEO of Tesla and SpaceXDeep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and videogames. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors.}
      \field{day}{10}
      \field{eprinttype}{googlebooks}
      \field{isbn}{978-0-262-33737-3}
      \field{langid}{english}
      \field{month}{11}
      \field{pagetotal}{801}
      \field{title}{Deep {{Learning}}}
      \field{year}{2016}
      \field{dateera}{ce}
      \verb{eprint}
      \verb omivDQAAQBAJ
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2016_Goodfellow et al/Goodfellow et al_2016_Deep Learning.pdf
      \endverb
      \keyw{Computers / Artificial Intelligence / General,Computers / Computer Science,Computers / Data Science / Machine Learning}
    \endentry
    \entry{goyalAccurateLargeMinibatch2018}{report}{}
      \name{author}{9}{}{%
        {{hash=d87be7c19fa16049347907c9820816c6}{%
           family={Goyal},
           familyi={G\bibinitperiod},
           given={Priya},
           giveni={P\bibinitperiod}}}%
        {{hash=ecd149fdcb3e0503881d49e545744c3d}{%
           family={Dollár},
           familyi={D\bibinitperiod},
           given={Piotr},
           giveni={P\bibinitperiod}}}%
        {{hash=bd5dadbe57bedc5957c19a3154c4d424}{%
           family={Girshick},
           familyi={G\bibinitperiod},
           given={Ross},
           giveni={R\bibinitperiod}}}%
        {{hash=c3ca2ee41cf25a9ed55b70a29aca9c9a}{%
           family={Noordhuis},
           familyi={N\bibinitperiod},
           given={Pieter},
           giveni={P\bibinitperiod}}}%
        {{hash=fe2c91d7f7f1eb9fba5b4b2349a02fbb}{%
           family={Wesolowski},
           familyi={W\bibinitperiod},
           given={Lukasz},
           giveni={L\bibinitperiod}}}%
        {{hash=fb0c5ef5bd0623316e6c5b00f3202e24}{%
           family={Kyrola},
           familyi={K\bibinitperiod},
           given={Aapo},
           giveni={A\bibinitperiod}}}%
        {{hash=ead37c072da8d4bf054810a1c3011177}{%
           family={Tulloch},
           familyi={T\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod}}}%
        {{hash=9fce03efe6b3331a1b93ed2e7c0da9d5}{%
           family={Jia},
           familyi={J\bibinitperiod},
           given={Yangqing},
           giveni={Y\bibinitperiod}}}%
        {{hash=6b4b60e909e78633945f3f9c9dc83e01}{%
           family={He},
           familyi={H\bibinitperiod},
           given={Kaiming},
           giveni={K\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {arXiv}%
      }
      \strng{namehash}{cf278d9271bb4e6539a9e3cdc5225acb}
      \strng{fullhash}{ffe8205ee34aca8734032bf6b02a24f1}
      \strng{bibnamehash}{cf278d9271bb4e6539a9e3cdc5225acb}
      \strng{authorbibnamehash}{cf278d9271bb4e6539a9e3cdc5225acb}
      \strng{authornamehash}{cf278d9271bb4e6539a9e3cdc5225acb}
      \strng{authorfullhash}{ffe8205ee34aca8734032bf6b02a24f1}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textasciitilde 90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.}
      \field{day}{30}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{4}
      \field{shorttitle}{Accurate, {{Large Minibatch SGD}}}
      \field{title}{Accurate, {{Large Minibatch SGD}}: {{Training ImageNet}} in 1 {{Hour}}}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{year}{2018}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 1706.02677
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2018_Goyal et al/Goyal et al_2018_Accurate, Large Minibatch SGD.pdf;/Users/felix/Zotero/storage/GVK5FVSB/1706.html
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1706.02677
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1706.02677
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning}
    \endentry
    \entry{hanssonOptimizationLearningControl2023}{book}{}
      \name{author}{1}{}{%
        {{hash=95df626b3ad08232819bb326cc985642}{%
           family={Hansson},
           familyi={H\bibinitperiod},
           given={Anders},
           giveni={A\bibinitperiod}}}%
      }
      \name{namea}{1}{}{%
        {{hash=389fa1c9fc349acdb559154199052582}{%
           family={Andersen},
           familyi={A\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Hoboken, New Jersey}%
      }
      \list{publisher}{1}{%
        {John Wiley \& Sons, Inc.}%
      }
      \strng{namehash}{95df626b3ad08232819bb326cc985642}
      \strng{fullhash}{95df626b3ad08232819bb326cc985642}
      \strng{bibnamehash}{95df626b3ad08232819bb326cc985642}
      \strng{authorbibnamehash}{95df626b3ad08232819bb326cc985642}
      \strng{authornamehash}{95df626b3ad08232819bb326cc985642}
      \strng{authorfullhash}{95df626b3ad08232819bb326cc985642}
      \strng{nameabibnamehash}{389fa1c9fc349acdb559154199052582}
      \strng{nameanamehash}{389fa1c9fc349acdb559154199052582}
      \strng{nameafullhash}{389fa1c9fc349acdb559154199052582}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Optimization for Learning and Control Comprehensive resource providing a masters’ level introduction to optimization theory and algorithms for learning and control. Optimization for Learning and Control describes how optimization is used in these domains, giving a thorough introduction to both Unsupervised Learning, Supervised Learning, and Reinforcement Learning, with an emphasis on optimization methods for large-scale learning and control problems. Several applications areas are also discussed, including signal processing, system identification, optimal control, and Machine Learning. Today, most of the material on the optimization aspects of Deep Learning that is accessible for students at a Masters’ level is focused on surface-level computer programming; deeper knowledge about the optimization methods and the trade-offs that are behind these methods is not provided. The objective of this book is to make this scattered knowledge, currently mainly available in publications in academic journals, accessible for Masters’ students in a coherent way. The focus is on basic algorithmic principles and trade-offs. We are now going to discuss Unsupervised Learning. This is about finding lower-dimensional descriptions of a set of data \{x1, … , xN\}. One simple such lower-dimensional description is the mean of the data. Another one could be to find a probability function from which the data are the outcome. We will see that there are many more lower-dimensional descriptions of data. We will start the chapter by defining entropy, and we will see that many of the probability density functions that are of interest in learning can be derived from the so-called “maximum entropy principle.” Specifically, we will derive the categorical distribution, the Ising distribution, and the normal distribution. There is a close relationship between the Lagrange dual function of the maximum entropy problem and maximum likelihood (ML) estimation, which will also be investigated. Other topics that we cover are prediction, graphical models, cross entropy, the expectation maximization algorithm, the Boltzmann machine, principal component analysis, mutual information, and cluster analysis. As a prelude to entropy we will start by discussing the so-called Chebyshev bounds. The CVX modeling package for MATLAB has pioneered what is referred to as disciplined convex programming. It requires that user inputs a problem in a form that allows the software to verify convexity via a number of known composition rules. The problem is then reformulated as a conic optimization problem and passed to one of several possible solvers. The software packages CVXPY, Convex.jl, and CVXR make similar modeling functionality available in the programming languages Python, Julia, and R, respectively. Optimization for Learning and Control covers sample topics such as: Optimization theory and optimization methods, covering classes of optimization problems like least squares problems, quadratic problems, conic optimization problems and rank optimization. First-order methods, second-order methods, variable metric methods, and methods for nonlinear least squares problems. Stochastic optimization methods, augmented Lagrangian methods, interior-point methods, and conic optimization methods. Dynamic programming for solving optimal control problems and its generalization to Reinforcement learning. How optimization theory is used to develop theory and tools of statistics and learning, e.g., the maximum likelihood method, expectation maximization, k-means clustering, and support vector machines. How calculus of variations is used in optimal control and for deriving the family of exponential distributions. Optimization for Learning and Control is an ideal resource on the subject for scientists and engineers learning about which optimization methods are useful for learning and control problems; the text will also appeal to industry professionals using Machine Learning for different practical applications.}
      \field{isbn}{978-1-119-80914-2}
      \field{langid}{english}
      \field{nameatype}{collaborator}
      \field{title}{Optimization for Learning and Control}
      \field{year}{2023}
      \field{dateera}{ce}
      \verb{file}
      \verb /Users/felix/paper/2023_Hansson/Hansson_2023_Optimization for learning and control.pdf
      \endverb
      \keyw{Mathematics; Mathematical optimization; Machine learning,Mathematics; MATLAB,Mathematics; Signal processing,System analysis}
    \endentry
    \entry{hintonNeuralNetworksMachine2012}{unpublished}{}
      \name{author}{1}{}{%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
      }
      \name{namea}{2}{}{%
        {{hash=c7447cc1d8288530b6713c6dbff02c99}{%
           family={Sirvastava},
           familyi={S\bibinitperiod},
           given={Nitish},
           giveni={N\bibinitperiod}}}%
        {{hash=fe49ec9b6f902b79d166c1f25405c088}{%
           family={Swersky},
           familyi={S\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{fullhash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{bibnamehash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{authorbibnamehash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{authornamehash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{authorfullhash}{9a8750ccdb2a4cf14d2655face1ce016}
      \strng{nameabibnamehash}{ac842ad1fd9f2fb1545a8a6030d6ade9}
      \strng{nameanamehash}{ac842ad1fd9f2fb1545a8a6030d6ade9}
      \strng{nameafullhash}{ac842ad1fd9f2fb1545a8a6030d6ade9}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{annotation}{Lecture 6 Slides: http://www.cs.toronto.edu/\textasciitilde tijmen/csc321/slides/lecture\_slides\_lec6.pdf}
      \field{nameatype}{collaborator}
      \field{title}{Neural {{Networks}} for {{Machine Learning}}}
      \field{type}{Massive Open Online Course}
      \field{urlday}{16}
      \field{urlmonth}{11}
      \field{urlyear}{2021}
      \field{venue}{Coursera}
      \field{year}{2012}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/felix/paper/2012_Hinton/Hinton_2012_Neural Networks for Machine Learning.pdf
      \endverb
      \verb{urlraw}
      \verb https://www.cs.toronto.edu/~hinton/coursera_lectures.html
      \endverb
      \verb{url}
      \verb https://www.cs.toronto.edu/~hinton/coursera_lectures.html
      \endverb
    \endentry
    \entry{hoareQuicksort1962}{article}{}
      \name{author}{1}{}{%
        {{hash=d6facc8397f631d3ef01dd70be993640}{%
           family={Hoare},
           familyi={H\bibinitperiod},
           given={C.\bibnamedelimi A.\bibnamedelimi R.},
           giveni={C\bibinitperiod\bibinitdelim A\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
      }
      \strng{namehash}{d6facc8397f631d3ef01dd70be993640}
      \strng{fullhash}{d6facc8397f631d3ef01dd70be993640}
      \strng{bibnamehash}{d6facc8397f631d3ef01dd70be993640}
      \strng{authorbibnamehash}{d6facc8397f631d3ef01dd70be993640}
      \strng{authornamehash}{d6facc8397f631d3ef01dd70be993640}
      \strng{authorfullhash}{d6facc8397f631d3ef01dd70be993640}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A description is given of a new method of sorting in the random-access store of a computer. The method compares very favourably with other known methods in speed, in economy of storage, and in ease of programming. Certain refinements of the method, which may be useful in the optimization of inner loops, are described in the second part of the paper.}
      \field{day}{1}
      \field{issn}{0010-4620}
      \field{journaltitle}{The Computer Journal}
      \field{month}{1}
      \field{number}{1}
      \field{shortjournal}{The Computer Journal}
      \field{title}{Quicksort}
      \field{volume}{5}
      \field{year}{1962}
      \field{dateera}{ce}
      \field{pages}{10\bibrangedash 16}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1093/comjnl/5.1.10
      \endverb
      \verb{file}
      \verb /Users/felix/paper/1962_Hoare/Hoare_1962_Quicksort.pdf;/Users/felix/Zotero/storage/NH6FLD4F/395338.html
      \endverb
    \endentry
    \entry{huangTightLipschitzHardness2022}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=4ac79ec5e47587c282ef7303a3d59669}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Brice},
           giveni={B\bibinitperiod}}}%
        {{hash=1b878d86cd8e2f2fbba6b5592fb42608}{%
           family={Sellke},
           familyi={S\bibinitperiod},
           given={Mark},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{444f558b8eb5ac3fefee2d25d3f00dfc}
      \strng{fullhash}{444f558b8eb5ac3fefee2d25d3f00dfc}
      \strng{bibnamehash}{444f558b8eb5ac3fefee2d25d3f00dfc}
      \strng{authorbibnamehash}{444f558b8eb5ac3fefee2d25d3f00dfc}
      \strng{authornamehash}{444f558b8eb5ac3fefee2d25d3f00dfc}
      \strng{authorfullhash}{444f558b8eb5ac3fefee2d25d3f00dfc}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study the problem of algorithmically optimizing the Hamiltonian of a spherical or Ising mean field spin glass. The maximum asymptotic value OPT of this random function is characterized by a variational principle known as the Parisi formula, proved first by Talagrand and in more generality by Panchenko. Recently developed approximate message passing algorithms efficiently optimize these functions up to a value ALG given by an extended Parisi formula, which minimizes over a larger space of functional order parameters. These two objectives are equal for spin glasses exhibiting a no overlap gap property. However, ALG can be strictly smaller than OPT, and no efficient algorithm producing a value exceeding ALG is known. We prove that when all interactions have even degree, no algorithm satisfying an overlap concentration property can produce an objective larger than ALG with non-negligible probability. This property holds for all algorithms with suitably Lipschitz dependence on the random disorder coefficients of the objective. It encompasses natural formulations of gradient descent, approximate message passing, and Langevin dynamics run for bounded time and in particular includes the algorithms achieving ALG mentioned above. To prove this result, we substantially generalize the overlap gap property framework introduced by Gamarnik and Sudan to arbitrary ultrametric forbidden structures of solutions.}
      \field{booktitle}{2022 {{IEEE}} 63rd {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} ({{FOCS}})}
      \field{eventtitle}{2022 {{IEEE}} 63rd {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} ({{FOCS}})}
      \field{issn}{2575-8454}
      \field{month}{10}
      \field{title}{Tight {{Lipschitz Hardness}} for Optimizing {{Mean Field Spin Glasses}}}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{pages}{312\bibrangedash 322}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1109/FOCS54457.2022.00037
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2022_Huang_Sellke/[ArXivExtended] Huang_Sellke_2022_Tight Lipschitz Hardness for optimizing Mean Field Spin Glasses.pdf;/Users/felix/paper/2022_Huang_Sellke/Huang_Sellke_2022_Tight Lipschitz Hardness for optimizing Mean Field Spin Glasses.pdf;/Users/felix/Zotero/storage/7SY7GREN/9996802.html
      \endverb
      \keyw{Approximation algorithms,Computer science,Glass,Heuristic algorithms,Message passing,non-convex optimization,Optimized production technology,overlap gap property,spin glass,statistical physics}
    \endentry
    \entry{ioffeBatchNormalizationAccelerating2015}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=5543e82359e26b035efc009cb3efff9d}{%
           family={Ioffe},
           familyi={I\bibinitperiod},
           given={Sergey},
           giveni={S\bibinitperiod}}}%
        {{hash=ed568d9c3bb059e6bf22899fbf170f86}{%
           family={Szegedy},
           familyi={S\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{7e8dee717d54c2984b1c6bd3f3c0561f}
      \strng{fullhash}{7e8dee717d54c2984b1c6bd3f3c0561f}
      \strng{bibnamehash}{7e8dee717d54c2984b1c6bd3f3c0561f}
      \strng{authorbibnamehash}{7e8dee717d54c2984b1c6bd3f3c0561f}
      \strng{authornamehash}{7e8dee717d54c2984b1c6bd3f3c0561f}
      \strng{authorfullhash}{7e8dee717d54c2984b1c6bd3f3c0561f}
      \field{sortinit}{I}
      \field{sortinithash}{8d291c51ee89b6cd86bf5379f0b151d8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.}
      \field{booktitle}{Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}}
      \field{day}{1}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{1938-7228}
      \field{langid}{english}
      \field{month}{6}
      \field{shorttitle}{Batch {{Normalization}}}
      \field{title}{Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}}
      \field{urlday}{6}
      \field{urlmonth}{10}
      \field{urlyear}{2021}
      \field{year}{2015}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{448\bibrangedash 456}
      \range{pages}{9}
      \verb{eprint}
      \verb 1502.03167
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2015_Ioffe_Szegedy/Ioffe_Szegedy_2015_Batch Normalization.pdf;/Users/felix/paper/2015_Ioffe_Szegedy/Ioffe_Szegedy_2015_Batch Normalization2.pdf;/Users/felix/Zotero/storage/56DUMX6R/1502.html
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v37/ioffe15.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v37/ioffe15.html
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{jaynesInformationTheoryStatistical1957}{article}{}
      \name{author}{1}{}{%
        {{hash=750189cbf905cceea863a5e1aed9afaf}{%
           family={Jaynes},
           familyi={J\bibinitperiod},
           given={E.\bibnamedelimi T.},
           giveni={E\bibinitperiod\bibinitdelim T\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {American Physical Society}%
      }
      \strng{namehash}{750189cbf905cceea863a5e1aed9afaf}
      \strng{fullhash}{750189cbf905cceea863a5e1aed9afaf}
      \strng{bibnamehash}{750189cbf905cceea863a5e1aed9afaf}
      \strng{authorbibnamehash}{750189cbf905cceea863a5e1aed9afaf}
      \strng{authornamehash}{750189cbf905cceea863a5e1aed9afaf}
      \strng{authorfullhash}{750189cbf905cceea863a5e1aed9afaf}
      \field{sortinit}{J}
      \field{sortinithash}{b2f54a9081ace9966a7cb9413811edb4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum-entropy estimate. It is the least biased estimate possible on the given information; i.e., it is maximally noncommittal with regard to missing information. If one considers statistical mechanics as a form of statistical inference rather than as a physical theory, it is found that the usual computational rules, starting with the determination of the partition function, are an immediate consequence of the maximum-entropy principle. In the resulting "subjective statistical mechanics," the usual rules are thus justified independently of any physical argument, and in particular independently of experimental verification; whether or not the results agree with experiment, they still represent the best estimates that could have been made on the basis of the information available.}
      \field{day}{15}
      \field{journaltitle}{Physical Review}
      \field{month}{5}
      \field{number}{4}
      \field{shortjournal}{Phys. Rev.}
      \field{title}{Information {{Theory}} and {{Statistical Mechanics}}}
      \field{volume}{106}
      \field{year}{1957}
      \field{dateera}{ce}
      \field{pages}{620\bibrangedash 630}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1103/PhysRev.106.620
      \endverb
      \verb{file}
      \verb /Users/felix/paper/1957_Jaynes/Jaynes_1957_Information Theory and Statistical Mechanics.pdf;/Users/felix/Zotero/storage/KP5UJW3J/PhysRev.106.html
      \endverb
    \endentry
    \entry{johnsonAppliedMultivariateStatistical2007}{book}{}
      \name{author}{2}{}{%
        {{hash=a4b1107dc3afce2369248dc2eac538aa}{%
           family={Johnson},
           familyi={J\bibinitperiod},
           given={Richard\bibnamedelima Arnold},
           giveni={R\bibinitperiod\bibinitdelim A\bibinitperiod}}}%
        {{hash=6bf1256920bca6ae536f5b087628170d}{%
           family={Wichern},
           familyi={W\bibinitperiod},
           given={Dean\bibnamedelima W.},
           giveni={D\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Upper Saddle River, N.J}%
      }
      \list{publisher}{1}{%
        {Pearson College Div}%
      }
      \strng{namehash}{cd97fa67bcdbfef855e67f3f7479ee40}
      \strng{fullhash}{cd97fa67bcdbfef855e67f3f7479ee40}
      \strng{bibnamehash}{cd97fa67bcdbfef855e67f3f7479ee40}
      \strng{authorbibnamehash}{cd97fa67bcdbfef855e67f3f7479ee40}
      \strng{authornamehash}{cd97fa67bcdbfef855e67f3f7479ee40}
      \strng{authorfullhash}{cd97fa67bcdbfef855e67f3f7479ee40}
      \field{sortinit}{J}
      \field{sortinithash}{b2f54a9081ace9966a7cb9413811edb4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This market-leading book offers a readable introduction to the statistical analysis of multivariate observations. Its overarching goal is to provide readers with the knowledge necessary to make proper interpretations and select appropriate techniques for analyzing multivariate data. Chapter topics include aspects of multivariate analysis, matrix algebra and random vectors, sample geometry and random sampling, the multivariate normal distribution, inferences about a mean vector, comparisons of several multivariate means, multivariate linear regression models, principal components, factor analysis and inference for structured covariance matrices, canonical correlation analysis, and discrimination and classification. For experimental scientists in a variety of disciplines.}
      \field{edition}{6}
      \field{isbn}{978-0-13-187715-3}
      \field{langid}{english}
      \field{pagetotal}{767}
      \field{title}{Applied {{Multivariate Statistical Analysis}}}
      \field{year}{2007}
      \field{dateera}{ce}
    \endentry
    \entry{kayFundamentalsStatisticalSignal1993}{book}{}
      \name{author}{1}{}{%
        {{hash=19c66c9bb8003666a723e9405355c007}{%
           family={Kay},
           familyi={K\bibinitperiod},
           given={Steven\bibnamedelima M.},
           giveni={S\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {USA}%
      }
      \list{publisher}{1}{%
        {Prentice-Hall, Inc.}%
      }
      \strng{namehash}{19c66c9bb8003666a723e9405355c007}
      \strng{fullhash}{19c66c9bb8003666a723e9405355c007}
      \strng{bibnamehash}{19c66c9bb8003666a723e9405355c007}
      \strng{authorbibnamehash}{19c66c9bb8003666a723e9405355c007}
      \strng{authornamehash}{19c66c9bb8003666a723e9405355c007}
      \strng{authorfullhash}{19c66c9bb8003666a723e9405355c007}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{isbn}{978-0-13-345711-7}
      \field{month}{2}
      \field{pagetotal}{595}
      \field{shorttitle}{Fundamentals of Statistical Signal Processing}
      \field{title}{Fundamentals of Statistical Signal Processing: Estimation Theory}
      \field{year}{1993}
      \field{dateera}{ce}
      \verb{file}
      \verb /Users/felix/paper/1993_Kay/Kay_1993_Fundamentals of statistical signal processing.pdf
      \endverb
    \endentry
    \entry{kingmaAdamMethodStochastic2015}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=b6fbd171848aad4edf3925543f1f1522}{%
           family={Kingma},
           familyi={K\bibinitperiod},
           given={Diederik\bibnamedelima P.},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=8aa66e8231cc2fdbe67aa4f18ca970c6}{%
           family={Ba},
           familyi={B\bibinitperiod},
           given={Jimmy},
           giveni={J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {San Diego}%
      }
      \strng{namehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{fullhash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{bibnamehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{authorbibnamehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{authornamehash}{a09df9f123146b8e2c7f1134c9496932}
      \strng{authorfullhash}{a09df9f123146b8e2c7f1134c9496932}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.}
      \field{booktitle}{Proceedings of the 3rd {{International Conference}} on {{Learning Representations}}}
      \field{eprinttype}{arXiv}
      \field{eventtitle}{{{ICLR}}}
      \field{shorttitle}{Adam}
      \field{title}{Adam: {{A Method}} for {{Stochastic Optimization}}}
      \field{year}{2015}
      \field{dateera}{ce}
      \verb{eprint}
      \verb 1412.6980
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2015_Kingma_Ba/Kingma_Ba_2015_Adam.pdf;/Users/felix/Zotero/storage/6WJBPWQT/1412.html
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{klenkeProbabilityTheoryComprehensive2014}{book}{}
      \name{author}{1}{}{%
        {{hash=0a05d830003b0065009dc105922410e3}{%
           family={Klenke},
           familyi={K\bibinitperiod},
           given={Achim},
           giveni={A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {London}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{0a05d830003b0065009dc105922410e3}
      \strng{fullhash}{0a05d830003b0065009dc105922410e3}
      \strng{bibnamehash}{0a05d830003b0065009dc105922410e3}
      \strng{authorbibnamehash}{0a05d830003b0065009dc105922410e3}
      \strng{authornamehash}{0a05d830003b0065009dc105922410e3}
      \strng{authorfullhash}{0a05d830003b0065009dc105922410e3}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{isbn}{978-1-4471-5360-3 978-1-4471-5361-0}
      \field{langid}{english}
      \field{series}{Universitext}
      \field{shorttitle}{Probability {{Theory}}}
      \field{title}{Probability {{Theory}}: {{A Comprehensive Course}}}
      \field{year}{2014}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.1007/978-1-4471-5361-0
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2014_Klenke/Klenke_2014_Probability Theory.pdf
      \endverb
      \keyw{Brownian Motion,Integration Theory,Limit Theorems,Markov Chains,Martingales,Measure Theory,Percolation,Poisson Point Process,Statistical Physics,Stochastic Differential Equations,Stochastic Integral,Stochastic Processes}
    \endentry
    \entry{krizhevskyLearningMultipleLayers2009}{report}{}
      \name{author}{1}{}{%
        {{hash=c5e3a676e2ac1164b3afcd539c131fc9}{%
           family={Krizhevsky},
           familyi={K\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{c5e3a676e2ac1164b3afcd539c131fc9}
      \strng{fullhash}{c5e3a676e2ac1164b3afcd539c131fc9}
      \strng{bibnamehash}{c5e3a676e2ac1164b3afcd539c131fc9}
      \strng{authorbibnamehash}{c5e3a676e2ac1164b3afcd539c131fc9}
      \strng{authornamehash}{c5e3a676e2ac1164b3afcd539c131fc9}
      \strng{authorfullhash}{c5e3a676e2ac1164b3afcd539c131fc9}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Learning Multiple Layers of Features from Tiny Images}
      \field{urlday}{21}
      \field{urlmonth}{5}
      \field{urlyear}{2024}
      \field{year}{2009}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/felix/Zotero/storage/W25N4ZA2/1370861707142497920.html
      \endverb
      \verb{urlraw}
      \verb https://www.cs.toronto.edu/ kriz/learning-features-2009-TR.pdf
      \endverb
      \verb{url}
      \verb https://www.cs.toronto.edu/%20kriz/learning-features-2009-TR.pdf
      \endverb
    \endentry
    \entry{kushnerNewMethodLocating1964}{article}{}
      \name{author}{1}{}{%
        {{hash=1435729142d7ff2b491681a967128ce5}{%
           family={Kushner},
           familyi={K\bibinitperiod},
           given={H.\bibnamedelimi J.},
           giveni={H\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{1435729142d7ff2b491681a967128ce5}
      \strng{fullhash}{1435729142d7ff2b491681a967128ce5}
      \strng{bibnamehash}{1435729142d7ff2b491681a967128ce5}
      \strng{authorbibnamehash}{1435729142d7ff2b491681a967128ce5}
      \strng{authornamehash}{1435729142d7ff2b491681a967128ce5}
      \strng{authorfullhash}{1435729142d7ff2b491681a967128ce5}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A versatile and practical method of searching a parameter space is presented. Theoretical and experimental results illustrate the usefulness of the method for such problems as the experimental optimization of the performance of a system with a very general multipeak performance function when the only available information is noise-distributed samples of the function. At present, its usefulness is restricted to optimization with respect to one system parameter. The observations are taken sequentially; but, as opposed to the gradient method, the observation may be located anywhere on the parameter interval. A sequence of estimates of the location of the curve maximum is generated. The location of the next observation may be interpreted as the location of the most likely competitor (with the current best estimate) for the location of the curve maximum. A Brownian motion stochastic process is selected as a model for the unknown function, and the observations are interpreted with respect to the model. The model gives the results a simple intuitive interpretation and allows the use of simple but efficient sampling procedures. The resulting process possesses some powerful convergence properties in the presence of noise; it is nonparametric and, despite its generality, is efficient in the use of observations. The approach seems quite promising as a solution to many of the problems of experimental system optimization.}
      \field{day}{1}
      \field{issn}{0021-9223}
      \field{journaltitle}{Journal of Basic Engineering}
      \field{month}{3}
      \field{number}{1}
      \field{shortjournal}{Journal of Basic Engineering}
      \field{title}{A {{New Method}} of {{Locating}} the {{Maximum Point}} of an {{Arbitrary Multipeak Curve}} in the {{Presence}} of {{Noise}}}
      \field{volume}{86}
      \field{year}{1964}
      \field{dateera}{ce}
      \field{pages}{97\bibrangedash 106}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1115/1.3653121
      \endverb
      \verb{file}
      \verb /Users/felix/paper/1964_Kushner/Kushner_1964_A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in.pdf;/Users/felix/Zotero/storage/SHIJUFR9/A-New-Method-of-Locating-the-Maximum-Point-of-an.html
      \endverb
    \endentry
    \entry{lacotteOptimalRandomizedFirstOrder2020}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=6bb769e7f1b865f9ab5e348c9806d1a3}{%
           family={Lacotte},
           familyi={L\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod}}}%
        {{hash=5d26b754e75f4129f15bf3882c34f70e}{%
           family={Pilanci},
           familyi={P\bibinitperiod},
           given={Mert},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{cf92837d211867f26a6cb5221fcced16}
      \strng{fullhash}{cf92837d211867f26a6cb5221fcced16}
      \strng{bibnamehash}{cf92837d211867f26a6cb5221fcced16}
      \strng{authorbibnamehash}{cf92837d211867f26a6cb5221fcced16}
      \strng{authornamehash}{cf92837d211867f26a6cb5221fcced16}
      \strng{authorfullhash}{cf92837d211867f26a6cb5221fcced16}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We provide an exact analysis of a class of randomized algorithms for solving overdetermined least-squares problems. We consider first-order methods, where the gradients are pre-conditioned by an approximation of the Hessian, based on a subspace embedding of the data matrix. This class of algorithms encompasses several randomized methods among the fastest solvers for least-squares problems. We focus on two classical embeddings, namely, Gaussian projections and subsampled randomized Hadamard transforms (SRHT). Our key technical innovation is the derivation of the limiting spectral density of SRHT embeddings. Leveraging this novel result, we derive the family of normalized orthogonal polynomials of the SRHT density and we find the optimal pre-conditioned first-order method along with its rate of convergence. Our analysis of Gaussian embeddings proceeds similarly, and leverages classical random matrix theory results. In particular, we show that for a given sketch size, SRHT embeddings exhibits a faster rate of convergence than Gaussian embeddings. Then, we propose a new algorithm by optimizing the computational complexity over the choice of the sketching dimension. To our knowledge, our resulting algorithm yields the best known complexity for solving least-squares problems with no condition number dependence.}
      \field{booktitle}{Proceedings of the 37th {{International Conference}} on {{Machine Learning}}}
      \field{day}{21}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{11}
      \field{title}{Optimal {{Randomized First-Order Methods}} for {{Least-Squares Problems}}}
      \field{urlday}{9}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{5587\bibrangedash 5597}
      \range{pages}{11}
      \verb{file}
      \verb /Users/felix/paper/2020_Lacotte_Pilanci/[Supplemental] Lacotte_Pilanci_2020_Optimal Randomized First-Order Methods for Least-Squares Problems.pdf;/Users/felix/paper/2020_Lacotte_Pilanci/Lacotte_Pilanci_2020_Optimal Randomized First-Order Methods for Least-Squares Problems.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v119/lacotte20a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v119/lacotte20a.html
      \endverb
    \endentry
    \entry{lecunMNISTDATABASEHandwritten2010}{dataset}{}
      \name{author}{3}{}{%
        {{hash=6a1aa6b7eab12b931ca7c7e3f927231d}{%
           family={LeCun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod}}}%
        {{hash=17acda211a651e90e228f1776ee07818}{%
           family={Cortes},
           familyi={C\bibinitperiod},
           given={Corinna},
           giveni={C\bibinitperiod}}}%
        {{hash=57683730ceb22e5d170051dedfc04a8e}{%
           family={Burges},
           familyi={B\bibinitperiod},
           given={Christopher\bibnamedelima J.C.},
           giveni={C\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{9e4c6012409dc8dd9b2aa198a2059804}
      \strng{fullhash}{efcd7df77681651155e8e7a4553de729}
      \strng{bibnamehash}{efcd7df77681651155e8e7a4553de729}
      \strng{authorbibnamehash}{efcd7df77681651155e8e7a4553de729}
      \strng{authornamehash}{9e4c6012409dc8dd9b2aa198a2059804}
      \strng{authorfullhash}{efcd7df77681651155e8e7a4553de729}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from MNIST. The digits have been size-normalized and centered in a fixed-size image}
      \field{title}{{{THE MNIST DATABASE}} of Handwritten Digits}
      \field{urlday}{24}
      \field{urlmonth}{1}
      \field{urlyear}{2024}
      \field{year}{2010}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/felix/Zotero/storage/BD62568A/mnist.html
      \endverb
      \verb{urlraw}
      \verb http://yann.lecun.com/exdb/mnist/
      \endverb
      \verb{url}
      \verb http://yann.lecun.com/exdb/mnist/
      \endverb
    \endentry
    \entry{lizottePracticalBayesianOptimization2008}{thesis}{}
      \name{author}{1}{}{%
        {{hash=e281ed451824019b77760f820c063087}{%
           family={Lizotte},
           familyi={L\bibinitperiod},
           given={Daniel\bibnamedelima James},
           giveni={D\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {University of Alberta}%
      }
      \list{location}{1}{%
        {Alberta, Canada}%
      }
      \strng{namehash}{e281ed451824019b77760f820c063087}
      \strng{fullhash}{e281ed451824019b77760f820c063087}
      \strng{bibnamehash}{e281ed451824019b77760f820c063087}
      \strng{authorbibnamehash}{e281ed451824019b77760f820c063087}
      \strng{authornamehash}{e281ed451824019b77760f820c063087}
      \strng{authorfullhash}{e281ed451824019b77760f820c063087}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Global optimization of non-convex functions over real vector spaces is a problem of widespread theoretical and practical interest. In the past fifty years, research in global optimization has produced many important approaches including Lipschitz optimization, simulated annealing, homotopy methods, genetic algorithms, and Bayesian response-surface methods. This work examines the last of these approaches. The Bayesian response-surface approach to global optimization maintains a posterior model of the function being optimized by combining a prior over functions with accumulating function evaluations. The model is then used to compute which point the method should acquire next in its search for the optimum of the function. Bayesian methods can be some of the most efficient approaches to optimization in terms of the number of function evaluations required, but they have significant drawbacks: Current approaches are needlessly data-inefficient, approximations to the Bayes-optimal acquisition criterion are poorly studied, and current approaches do not take advantage of the small-scale properties of differentiable functions near local optima. This work addresses each of these problems to make Bayesian methods more widely applicable.}
      \field{pagetotal}{168}
      \field{title}{Practical Bayesian Optimization}
      \field{type}{phdthesis}
      \field{year}{2008}
      \field{dateera}{ce}
      \verb{file}
      \verb /Users/felix/paper/2008_Lizotte/Lizotte_2008_Practical bayesian optimization.pdf
      \endverb
    \endentry
    \entry{matheronIntrinsicRandomFunctions1973}{article}{}
      \name{author}{1}{}{%
        {{hash=ea0a6a067aea85f0014b448311fbd6c0}{%
           family={Matheron},
           familyi={M\bibinitperiod},
           given={G.},
           giveni={G\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{ea0a6a067aea85f0014b448311fbd6c0}
      \strng{fullhash}{ea0a6a067aea85f0014b448311fbd6c0}
      \strng{bibnamehash}{ea0a6a067aea85f0014b448311fbd6c0}
      \strng{authorbibnamehash}{ea0a6a067aea85f0014b448311fbd6c0}
      \strng{authornamehash}{ea0a6a067aea85f0014b448311fbd6c0}
      \strng{authorfullhash}{ea0a6a067aea85f0014b448311fbd6c0}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The intrinsic random functions (IRF) are a particular case of the Guelfand generalized processes with stationary increments. They constitute a much wider class than the stationary RF, and are used in practical applications for representing non-stationary phenomena. The most important topics are: existence of a generalized covariance (GC) for which statistical inference is possible from a unique realization; theory of the best linear intrinsic estimator (BLIE) used for contouring and estimating problems; the turning bands method for simulating IRF; and the models with polynomial GC, for which statistical inference may be performed by automatic procedures.}
      \field{issn}{0001-8678, 1475-6064}
      \field{journaltitle}{Advances in Applied Probability}
      \field{langid}{english}
      \field{month}{12}
      \field{number}{3}
      \field{title}{The Intrinsic Random Functions and Their Applications}
      \field{volume}{5}
      \field{year}{1973}
      \field{dateera}{ce}
      \field{pages}{439\bibrangedash 468}
      \range{pages}{30}
      \verb{doi}
      \verb 10.2307/1425829
      \endverb
      \verb{file}
      \verb /Users/felix/paper/1973_Matheron/Matheron_1973_The intrinsic random functions and their applications.pdf
      \endverb
      \keyw{GENERALIZED COVARIANCES,GUELFAND GENERALIZED STOCHASTIC PROCESS,INTRINSIC RANDOM FUNCTION,POLYNOMIAL COVARIANCE,TURNING BANDS METHOD}
    \endentry
    \entry{montanariOptimizationSherringtonKirkpatrick2021}{article}{}
      \name{author}{1}{}{%
        {{hash=484a542ca834affa98f81afbd90fba25}{%
           family={Montanari},
           familyi={M\bibinitperiod},
           given={Andrea},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Society for Industrial and Applied Mathematics}%
      }
      \strng{namehash}{484a542ca834affa98f81afbd90fba25}
      \strng{fullhash}{484a542ca834affa98f81afbd90fba25}
      \strng{bibnamehash}{484a542ca834affa98f81afbd90fba25}
      \strng{authorbibnamehash}{484a542ca834affa98f81afbd90fba25}
      \strng{authornamehash}{484a542ca834affa98f81afbd90fba25}
      \strng{authorfullhash}{484a542ca834affa98f81afbd90fba25}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multigrid modeling algorithms are a technique used to accelerate iterative method models running on a hierarchy of similar graphlike structures. We introduce and demonstrate a new method for training neural networks which uses multilevel methods. Using an objective function derived from a graph-distance metric, we perform orthogonally-constrained optimization to find optimal prolongation and restriction maps between graphs. We compare and contrast several methods for performing this numerical optimization, and additionally present some new theoretical results on upper bounds of this type of objective function. Once calculated, these optimal maps between graphs form the core of multiscale artificial neural network (MsANN) training, a new procedure we present which simultaneously trains a hierarchy of neural network models of varying spatial resolution. Parameter information is passed between members of this hierarchy according to standard coarsening and refinement schedules from the multiscale modeling literature. In our machine learning experiments, these models are able to learn faster than training at the fine scale alone, achieving a comparable level of error with fewer weight updates (by an order of magnitude).}
      \field{day}{7}
      \field{issn}{0097-5397}
      \field{journaltitle}{SIAM Journal on Computing}
      \field{month}{1}
      \field{pagetotal}{FOCS19-38}
      \field{shortjournal}{SIAM J. Comput.}
      \field{title}{Optimization of the {{Sherrington--Kirkpatrick Hamiltonian}}}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{pages}{FOCS19\bibrangedash 1}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.1137/20M132016X
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2021_Montanari/Montanari_2021_Optimization of the Sherrington--Kirkpatrick Hamiltonian.pdf
      \endverb
    \endentry
    \entry{nesterovLecturesConvexOptimization2018}{book}{}
      \name{author}{1}{}{%
        {{hash=826ae4a383e24ca852de5196ad5fb432}{%
           family={Nesterov},
           familyi={N\bibinitperiod},
           given={Yurii\bibnamedelima Evgen'evič},
           giveni={Y\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cham}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{826ae4a383e24ca852de5196ad5fb432}
      \strng{fullhash}{826ae4a383e24ca852de5196ad5fb432}
      \strng{bibnamehash}{826ae4a383e24ca852de5196ad5fb432}
      \strng{authorbibnamehash}{826ae4a383e24ca852de5196ad5fb432}
      \strng{authornamehash}{826ae4a383e24ca852de5196ad5fb432}
      \strng{authorfullhash}{826ae4a383e24ca852de5196ad5fb432}
      \field{sortinit}{N}
      \field{sortinithash}{22369a73d5f88983a108b63f07f37084}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This book provides a comprehensive, modern introduction to convex optimization, a field that is becoming increasingly important in applied mathematics, economics and finance, engineering, and computer science, notably in data science and machine learning. Written by a leading expert in the field, this book includes recent advances in the algorithmic theory of convex optimization, naturally complementing the existing literature. It contains a unified and rigorous presentation of the acceleration techniques for minimization schemes of first- and second-order. It provides readers with a full treatment of the smoothing technique, which has tremendously extended the abilities of gradient-type methods. Several powerful approaches in structural optimization, including optimization in relative scale and polynomial-time interior-point methods, are also discussed in detail. Researchers in theoretical optimization as well as professionals working on optimization problems will find this book very useful. It presents many successful examples of how to develop very fast specialized minimization algorithms. Based on the author’s lectures, it can naturally serve as the basis for introductory and advanced courses in convex optimization for students in engineering, economics, computer science and mathematics, Introduction -- Part I Black-Box Optimization -- 1 Nonlinear Optimization -- 2 Smooth Convex Optimization -- 3 Nonsmooth Convex Optimization -- 4 Second-Order Methods -- Part II Structural Optimization -- 5 Polynomial-time Interior-Point Methods -- 6 Primal-Dual Model of Objective Function -- 7 Optimization in Relative Scale -- Bibliographical Comments -- Appendix A. Solving some Auxiliary Optimization Problems -- References -- Index}
      \field{edition}{Second edition}
      \field{isbn}{978-3-319-91578-4}
      \field{langid}{english}
      \field{series}{Springer Optimization and {{Its}} Applications; Volume 137}
      \field{title}{Lectures on {{Convex Optimization}}}
      \field{year}{2018}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.1007/978-3-319-91578-4
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2018_Nesterov/Nesterov_2018_Lectures on Convex Optimization.pdf
      \endverb
      \keyw{Computer software; Optimization; Mathematical optimization; Algorithms,Konvexe Optimierung}
    \endentry
    \entry{padidarScalingGaussianProcesses2021}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=3cdfbc5800a2ea88d010684b0aa7c70b}{%
           family={Padidar},
           familyi={P\bibinitperiod},
           given={Misha},
           giveni={M\bibinitperiod}}}%
        {{hash=7cfeb85dce9e83c26ebd06783ce09b54}{%
           family={Zhu},
           familyi={Z\bibinitperiod},
           given={Xinran},
           giveni={X\bibinitperiod}}}%
        {{hash=31c590f47d103d2dc1f832840fffdb3c}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Leo},
           giveni={L\bibinitperiod}}}%
        {{hash=627bb6a2d8d8a84e33faddfbc299827a}{%
           family={Gardner},
           familyi={G\bibinitperiod},
           given={Jacob},
           giveni={J\bibinitperiod}}}%
        {{hash=ca0e0045640f03b0eb8d1779f669001c}{%
           family={Bindel},
           familyi={B\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{a1cc276877a8b861cd98cf49df53e8e7}
      \strng{fullhash}{21f8fd396f1a7a3b5d3c41513195f7db}
      \strng{bibnamehash}{a1cc276877a8b861cd98cf49df53e8e7}
      \strng{authorbibnamehash}{a1cc276877a8b861cd98cf49df53e8e7}
      \strng{authornamehash}{a1cc276877a8b861cd98cf49df53e8e7}
      \strng{authorfullhash}{21f8fd396f1a7a3b5d3c41513195f7db}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Scaling {{Gaussian Processes}} with {{Derivative Information Using Variational Inference}}}
      \field{urlday}{17}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{volume}{34}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{6442\bibrangedash 6453}
      \range{pages}{12}
      \verb{file}
      \verb /Users/felix/paper/2021_Padidar et al/Padidar et al_2021_Scaling Gaussian Processes with Derivative Information Using Variational.pdf;/Users/felix/paper/2021_Padidar et al/Supplemental_Padidar et al_2021_Scaling Gaussian Processes with Derivative Information Using Variational.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2021/hash/32bbf7b2bc4ed14eb1e9c2580056a989-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2021/hash/32bbf7b2bc4ed14eb1e9c2580056a989-Abstract.html
      \endverb
    \endentry
    \entry{paquetteHaltingTimePredictable2022}{article}{}
      \name{author}{4}{}{%
        {{hash=8da8a91d96c88a39bd8ae5015c9fe2c5}{%
           family={Paquette},
           familyi={P\bibinitperiod},
           given={Courtney},
           giveni={C\bibinitperiod}}}%
        {{useprefix=true,hash=f0f9993391f26a73b94828bfc6fb784f}{%
           family={Merriënboer},
           familyi={M\bibinitperiod},
           given={Bart},
           giveni={B\bibinitperiod},
           prefix={van},
           prefixi={v\bibinitperiod}}}%
        {{hash=b39e81d521bfb9d0c1f7f75697d57957}{%
           family={Paquette},
           familyi={P\bibinitperiod},
           given={Elliot},
           giveni={E\bibinitperiod}}}%
        {{hash=bab4e5caee2d67831e464ce575022b37}{%
           family={Pedregosa},
           familyi={P\bibinitperiod},
           given={Fabian},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{4d97e0e44ec659ca409a44550867ab8f}
      \strng{fullhash}{7e6b8688fca15a1d987a16b6dfe5ea4e}
      \strng{bibnamehash}{4d97e0e44ec659ca409a44550867ab8f}
      \strng{authorbibnamehash}{4d97e0e44ec659ca409a44550867ab8f}
      \strng{authornamehash}{4d97e0e44ec659ca409a44550867ab8f}
      \strng{authorfullhash}{7e6b8688fca15a1d987a16b6dfe5ea4e}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Average-case analysis computes the complexity of an algorithm averaged over all possible inputs. Compared to worst-case analysis, it is more representative of the typical behavior of an algorithm, but remains largely unexplored in optimization. One difficulty is that the analysis can depend on the probability distribution of the inputs to the model. However, we show that this is not the case for a class of large-scale problems trained with first-order methods including random least squares and one-hidden layer neural networks with random weights. In fact, the halting time exhibits a universality property: it is independent of the probability distribution. With this barrier for average-case analysis removed, we provide the first explicit average-case convergence rates showing a tighter complexity not captured by traditional worst-case analysis. Finally, numerical simulations suggest this universality property holds for a more general class of algorithms and problems.}
      \field{eprintclass}{math, stat}
      \field{eprinttype}{arXiv}
      \field{issn}{1615-3383}
      \field{journaltitle}{Foundations of Computational Mathematics}
      \field{langid}{english}
      \field{month}{2}
      \field{number}{2}
      \field{shortjournal}{Found Comput Math}
      \field{shorttitle}{Halting {{Time}} Is {{Predictable}} for {{Large Models}}}
      \field{title}{Halting {{Time}} Is {{Predictable}} for {{Large Models}}: {{A Universality Property}} and {{Average-Case Analysis}}}
      \field{volume}{23}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{pages}{597\bibrangedash 673}
      \range{pages}{77}
      \verb{doi}
      \verb 10.1007/s10208-022-09554-y
      \endverb
      \verb{eprint}
      \verb 2006.04299
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2022_Paquette et al/[ArXiv] Paquette et al_2022_Halting Time is Predictable for Large Models.pdf;/Users/felix/paper/2022_Paquette et al/Paquette et al_2022_Halting Time is Predictable for Large Models.pdf
      \endverb
      \keyw{60B20,65K10,68T07,90C06,90C25,Optimization,Random matrix theory,Universality}
    \endentry
    \entry{paquetteUniversalityConjugateGradient2022}{article}{}
      \name{author}{2}{}{%
        {{hash=b39e81d521bfb9d0c1f7f75697d57957}{%
           family={Paquette},
           familyi={P\bibinitperiod},
           given={Elliot},
           giveni={E\bibinitperiod}}}%
        {{hash=d3052222f56223c9897a048b4ebd5cb2}{%
           family={Trogdon},
           familyi={T\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
      }
      \strng{namehash}{3307f6fa2a392b9fb317c96915962403}
      \strng{fullhash}{3307f6fa2a392b9fb317c96915962403}
      \strng{bibnamehash}{3307f6fa2a392b9fb317c96915962403}
      \strng{authorbibnamehash}{3307f6fa2a392b9fb317c96915962403}
      \strng{authornamehash}{3307f6fa2a392b9fb317c96915962403}
      \strng{authorfullhash}{3307f6fa2a392b9fb317c96915962403}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a probabilistic analysis of two Krylov subspace methods for solving linear systems. We prove a central limit theorem for norms of the residual vectors that are produced by the conjugate gradient and MINRES algorithms when applied to a wide class of sample covariance matrices satisfying some standard moment conditions. The proof involves establishing a four-moment theorem for the so-called spectral measure, implying, in particular, universality for the matrix produced by the Lanczos iteration. The central limit theorem then implies an almost-deterministic iteration count for the iterative methods in question. © 2022 Wiley Periodicals LLC.}
      \field{day}{1}
      \field{issn}{1097-0312}
      \field{journaltitle}{Communications on Pure and Applied Mathematics}
      \field{langid}{english}
      \field{month}{9}
      \field{number}{5}
      \field{title}{Universality for the {{Conjugate Gradient}} and {{MINRES Algorithms}} on {{Sample Covariance Matrices}}}
      \field{volume}{76}
      \field{year}{2022}
      \field{dateera}{ce}
      \field{pages}{1085\bibrangedash 1136}
      \range{pages}{52}
      \verb{doi}
      \verb 10.1002/cpa.22081
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2022_Paquette_Trogdon/Paquette_Trogdon_2022_Universality for the Conjugate Gradient and MINRES Algorithms on Sample.pdf;/Users/felix/Zotero/storage/5TA4BUYX/cpa.html
      \endverb
      \keyw{conjugate gradient,MINRES,Sample covariance matrices,Wishart distribution}
    \endentry
    \entry{pascanuDifficultyTrainingRecurrent2013}{inproceedings}{}
      \name{author}{3}{}{%
        {{hash=7045b009b04d57bd2e19b5dfa0864d4f}{%
           family={Pascanu},
           familyi={P\bibinitperiod},
           given={Razvan},
           giveni={R\bibinitperiod}}}%
        {{hash=a2d359b12ca2fadf0b40136a73f021bb}{%
           family={Mikolov},
           familyi={M\bibinitperiod},
           given={Tomas},
           giveni={T\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Atlanta}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{ddea4a14df69745e0b708bc055e9ad09}
      \strng{fullhash}{219d705befe0e5043ea8e48c4048f89a}
      \strng{bibnamehash}{219d705befe0e5043ea8e48c4048f89a}
      \strng{authorbibnamehash}{219d705befe0e5043ea8e48c4048f89a}
      \strng{authornamehash}{ddea4a14df69745e0b708bc055e9ad09}
      \strng{authorfullhash}{219d705befe0e5043ea8e48c4048f89a}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.}
      \field{booktitle}{Proceedings of the 30th {{International Conference}} on {{Machine Learning}}}
      \field{day}{26}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{1938-7228}
      \field{langid}{english}
      \field{month}{5}
      \field{title}{On the Difficulty of Training Recurrent Neural Networks}
      \field{urlday}{2}
      \field{urlmonth}{4}
      \field{urlyear}{2024}
      \field{year}{2013}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{1310\bibrangedash 1318}
      \range{pages}{9}
      \verb{file}
      \verb /Users/felix/paper/2013_Pascanu et al/[Supplemental] Pascanu et al_2013_On the difficulty of training recurrent neural networks.pdf;/Users/felix/paper/2013_Pascanu et al/Pascanu et al_2013_On the difficulty of training recurrent neural networks.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v28/pascanu13.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v28/pascanu13.html
      \endverb
    \endentry
    \entry{pedregosaAccelerationSpectralDensity2020}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=bab4e5caee2d67831e464ce575022b37}{%
           family={Pedregosa},
           familyi={P\bibinitperiod},
           given={Fabian},
           giveni={F\bibinitperiod}}}%
        {{hash=cec256cc503bc0f0752bca34d0445248}{%
           family={Scieur},
           familyi={S\bibinitperiod},
           given={Damien},
           giveni={D\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Virtual Event (formerly Vienna)}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{5fec6bfd9690f0837f880f048bfc9b21}
      \strng{fullhash}{5fec6bfd9690f0837f880f048bfc9b21}
      \strng{bibnamehash}{5fec6bfd9690f0837f880f048bfc9b21}
      \strng{authorbibnamehash}{5fec6bfd9690f0837f880f048bfc9b21}
      \strng{authornamehash}{5fec6bfd9690f0837f880f048bfc9b21}
      \strng{authorfullhash}{5fec6bfd9690f0837f880f048bfc9b21}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We develop a framework for the average-case analysis of random quadratic problems and derive algorithms that are optimal under this analysis. This yields a new class of methods that achieve acceleration given a model of the Hessian’s eigenvalue distribution. We develop explicit algorithms for the uniform, Marchenko-Pastur, and exponential distributions. These methods have a simple momentum-like update, in which each update only makes use on the current gradient and previous two iterates. Furthermore, the momentum and step-size parameters can be estimated without knowledge of the Hessian’s smallest singular value, in contrast with classical accelerated methods like Nesterov acceleration and Polyak momentum. Through empirical benchmarks on quadratic and logistic regression problems, we identify regimes in which the the proposed methods improve over classical (worst-case) accelerated methods.}
      \field{booktitle}{Proceedings of the 37th {{International Conference}} on {{Machine Learning}}}
      \field{day}{21}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{11}
      \field{title}{Acceleration through Spectral Density Estimation}
      \field{urlday}{9}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{year}{2020}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{7553\bibrangedash 7562}
      \range{pages}{10}
      \verb{file}
      \verb /Users/felix/paper/2020_Pedregosa_Scieur/Pedregosa_Scieur_2020_Acceleration through spectral density estimation.pdf;/Users/felix/Zotero/storage/IFQE5S5M/Pedregosa and Scieur - 2020 - Acceleration through spectral density estimation.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v119/pedregosa20a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v119/pedregosa20a.html
      \endverb
    \endentry
    \entry{rasmussenGaussianProcessesMachine2006}{book}{}
      \name{author}{2}{}{%
        {{hash=58d90ed7b7348c7a5a9b4a2a8f46df7b}{%
           family={Rasmussen},
           familyi={R\bibinitperiod},
           given={Carl\bibnamedelima Edward},
           giveni={C\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=d34fdfcbfcb5412397b946351370db22}{%
           family={Williams},
           familyi={W\bibinitperiod},
           given={Christopher\bibnamedelima K.I.},
           giveni={C\bibinitperiod\bibinitdelim K\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cambridge, Massachusetts}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{f0ccf4a09a3a6a91719405724e3ae5da}
      \strng{fullhash}{f0ccf4a09a3a6a91719405724e3ae5da}
      \strng{bibnamehash}{f0ccf4a09a3a6a91719405724e3ae5da}
      \strng{authorbibnamehash}{f0ccf4a09a3a6a91719405724e3ae5da}
      \strng{authornamehash}{f0ccf4a09a3a6a91719405724e3ae5da}
      \strng{authorfullhash}{f0ccf4a09a3a6a91719405724e3ae5da}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{edition}{2}
      \field{isbn}{0-262-18253-X}
      \field{langid}{english}
      \field{number}{3}
      \field{pagetotal}{248}
      \field{series}{Adaptive Computation and Machine Learning}
      \field{title}{Gaussian Processes for Machine Learning}
      \field{urlday}{6}
      \field{urlmonth}{4}
      \field{urlyear}{2023}
      \field{year}{2006}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/felix/paper/2006_Rasmussen_Williams/Rasmussen_Williams_2006_Gaussian processes for machine learning.pdf
      \endverb
      \verb{urlraw}
      \verb http://gaussianprocess.org/gpml/chapters/RW.pdf
      \endverb
      \verb{url}
      \verb http://gaussianprocess.org/gpml/chapters/RW.pdf
      \endverb
    \endentry
    \entry{roosHighDimensionalGaussianProcess2021}{inproceedings}{}
      \name{author}{3}{}{%
        {{useprefix=false,hash=b4772660ced9e6ae5b5fd5c58f367f67}{%
           family={Roos},
           familyi={R\bibinitperiod},
           given={Filip},
           giveni={F\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
        {{hash=aa869df2799bcd9045db40ccc0b3f2af}{%
           family={Gessner},
           familyi={G\bibinitperiod},
           given={Alexandra},
           giveni={A\bibinitperiod}}}%
        {{hash=0fc3fa397d36e1fb833fdafa98a2b96f}{%
           family={Hennig},
           familyi={H\bibinitperiod},
           given={Philipp},
           giveni={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{5834de487767a2ef534b7c0299584dde}
      \strng{fullhash}{0d30c55a4f8d65a26b8e1f0094981cac}
      \strng{bibnamehash}{0d30c55a4f8d65a26b8e1f0094981cac}
      \strng{authorbibnamehash}{0d30c55a4f8d65a26b8e1f0094981cac}
      \strng{authornamehash}{5834de487767a2ef534b7c0299584dde}
      \strng{authorfullhash}{0d30c55a4f8d65a26b8e1f0094981cac}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Although it is widely known that Gaussian processes can be conditioned on observations of the gradient, this functionality is of limited use due to the prohibitive computational cost of O(N\textasciicircum 3D\textasciicircum 3) in data points N and dimension D. The dilemma of gradient observations is that a single one of them comes at the same cost as D independent function evaluations, so the latter are often preferred. Careful scrutiny reveals, however, that derivative observations give rise to highly structured kernel Gram matrices for very general classes of kernels (inter alia, stationary kernels). We show that in the low-data regime N {$<$} D, the Gram matrix can be decomposed in a manner that reduces the cost of inference to O(N\textasciicircum 2D +(N\textasciicircum 2)\textasciicircum 3) (i.e., linear in the number of dimensions) and, in special cases, to O(N\textasciicircum 2D+N\textasciicircum 3). This reduction in complexity opens up new use-cases for inference with gradients especially in the high-dimensional regime, where the information-to-cost ratio of gradient observations significantly increases. We demonstrate this potential in a variety of tasks relevant for machine learning, such as optimization and Hamiltonian Monte Carlo with predictive gradients.}
      \field{booktitle}{Proceedings of the 38th {{International Conference}} on {{Machine Learning}}}
      \field{day}{1}
      \field{eventtitle}{International {{Conference}} on {{Machine Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{7}
      \field{title}{High-{{Dimensional Gaussian Process Inference}} with {{Derivatives}}}
      \field{urlday}{15}
      \field{urlmonth}{5}
      \field{urlyear}{2023}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \field{pages}{2535\bibrangedash 2545}
      \range{pages}{11}
      \verb{file}
      \verb /Users/felix/paper/2021_Roos et al/Roos et al_2021_High-Dimensional Gaussian Process Inference with Derivatives.pdf;/Users/felix/paper/2021_Roos et al/Supplemental_Roos et al_2021_High-Dimensional Gaussian Process Inference with Derivatives.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.mlr.press/v139/de-roos21a.html
      \endverb
      \verb{url}
      \verb https://proceedings.mlr.press/v139/de-roos21a.html
      \endverb
    \endentry
    \entry{salimansWeightNormalizationSimple2016}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=e6f76e1a4d058df028530916774ad3a7}{%
           family={Salimans},
           familyi={S\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
        {{hash=4058157a9701d06f0def43a257c6c2db}{%
           family={Kingma},
           familyi={K\bibinitperiod},
           given={Durk\bibnamedelima P},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Barcelona, Spain}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{ea0644660facdc121e091561d7db9eb7}
      \strng{fullhash}{ea0644660facdc121e091561d7db9eb7}
      \strng{bibnamehash}{ea0644660facdc121e091561d7db9eb7}
      \strng{authorbibnamehash}{ea0644660facdc121e091561d7db9eb7}
      \strng{authornamehash}{ea0644660facdc121e091561d7db9eb7}
      \strng{authorfullhash}{ea0644660facdc121e091561d7db9eb7}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{shorttitle}{Weight {{Normalization}}}
      \field{title}{Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}}
      \field{urlday}{16}
      \field{urlmonth}{10}
      \field{urlyear}{2023}
      \field{volume}{29}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/felix/paper/2016_Salimans_Kingma/Salimans_Kingma_2016_Weight Normalization.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2016/hash/ed265bc903a5a097f61d3ec064d96d2e-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2016/hash/ed265bc903a5a097f61d3ec064d96d2e-Abstract.html
      \endverb
    \endentry
    \entry{shahriariTakingHumanOut2016}{article}{}
      \name{author}{5}{}{%
        {{hash=4b01bc2ebef2b134ce8110e0431af923}{%
           family={Shahriari},
           familyi={S\bibinitperiod},
           given={Bobak},
           giveni={B\bibinitperiod}}}%
        {{hash=fe49ec9b6f902b79d166c1f25405c088}{%
           family={Swersky},
           familyi={S\bibinitperiod},
           given={Kevin},
           giveni={K\bibinitperiod}}}%
        {{hash=05bc8d503a2c310ef0976ace7f9d2734}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Ziyu},
           giveni={Z\bibinitperiod}}}%
        {{hash=c1552be0c6aa9c6e0fd91a130a682853}{%
           family={Adams},
           familyi={A\bibinitperiod},
           given={Ryan\bibnamedelima P.},
           giveni={R\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{useprefix=true,hash=cf269f9a5106a41ad53847a68a27db1c}{%
           family={Freitas},
           familyi={F\bibinitperiod},
           given={Nando},
           giveni={N\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
      }
      \strng{namehash}{e93053cadf8e1ed30669480ff76591f7}
      \strng{fullhash}{4b7419f24eab32e4907cf382d3adfad1}
      \strng{bibnamehash}{e93053cadf8e1ed30669480ff76591f7}
      \strng{authorbibnamehash}{e93053cadf8e1ed30669480ff76591f7}
      \strng{authornamehash}{e93053cadf8e1ed30669480ff76591f7}
      \strng{authorfullhash}{4b7419f24eab32e4907cf382d3adfad1}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Big Data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involve many tunable configuration parameters. These parameters are often specified and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in significant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.}
      \field{eventtitle}{Proceedings of the {{IEEE}}}
      \field{issn}{1558-2256}
      \field{journaltitle}{Proceedings of the IEEE}
      \field{month}{1}
      \field{number}{1}
      \field{shorttitle}{Taking the {{Human Out}} of the {{Loop}}}
      \field{title}{Taking the {{Human Out}} of the {{Loop}}: {{A Review}} of {{Bayesian Optimization}}}
      \field{volume}{104}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{pages}{148\bibrangedash 175}
      \range{pages}{28}
      \verb{doi}
      \verb 10.1109/JPROC.2015.2494218
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2016_Shahriari et al/Shahriari et al_2016_Taking the Human Out of the Loop.pdf;/Users/felix/Zotero/storage/HNUAJ9FM/7352306.html
      \endverb
      \keyw{Bayes methods,Big data,decision making,Decision making,design of experiments,Design of experiments,Genomes,genomic medicine,Linear programming,optimization,Optimization,response surface methodology,Statistical analysis,statistical learning}
    \endentry
    \entry{smithDisciplinedApproachNeural2018}{online}{}
      \name{author}{1}{}{%
        {{hash=eb6a39a428446862c3179c02093cb54c}{%
           family={Smith},
           familyi={S\bibinitperiod},
           given={Leslie\bibnamedelima N.},
           giveni={L\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \strng{namehash}{eb6a39a428446862c3179c02093cb54c}
      \strng{fullhash}{eb6a39a428446862c3179c02093cb54c}
      \strng{bibnamehash}{eb6a39a428446862c3179c02093cb54c}
      \strng{authorbibnamehash}{eb6a39a428446862c3179c02093cb54c}
      \strng{authornamehash}{eb6a39a428446862c3179c02093cb54c}
      \strng{authorfullhash}{eb6a39a428446862c3179c02093cb54c}
      \field{extraname}{1}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Although deep learning has produced dazzling successes for applications of image, speech, and video processing in the past few years, most trainings are with suboptimal hyper-parameters, requiring unnecessarily long training times. Setting the hyper-parameters remains a black art that requires years of experience to acquire. This report proposes several efficient ways to set the hyper-parameters that significantly reduce training time and improves performance. Specifically, this report shows how to examine the training validation/test loss function for subtle clues of underfitting and overfitting and suggests guidelines for moving toward the optimal balance point. Then it discusses how to increase/decrease the learning rate/momentum to speed up training. Our experiments show that it is crucial to balance every manner of regularization for each dataset and architecture. Weight decay is used as a sample regularizer to show how its optimal value is tightly coupled with the learning rates and momentums. Files to help replicate the results reported here are available.}
      \field{day}{24}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{month}{4}
      \field{pubstate}{prepublished}
      \field{shorttitle}{A Disciplined Approach to Neural Network Hyper-Parameters}
      \field{title}{A Disciplined Approach to Neural Network Hyper-Parameters: {{Part}} 1 -- Learning Rate, Batch Size, Momentum, and Weight Decay}
      \field{year}{2018}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1803.09820
      \endverb
      \verb{eprint}
      \verb 1803.09820
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2018_Smith/Smith_2018_A disciplined approach to neural network hyper-parameters.pdf;/Users/felix/Zotero/storage/APBXGKDL/1803.html
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning}
    \endentry
    \entry{smithCyclicalLearningRates2017}{inproceedings}{}
      \name{author}{1}{}{%
        {{hash=eb6a39a428446862c3179c02093cb54c}{%
           family={Smith},
           familyi={S\bibinitperiod},
           given={Leslie\bibnamedelima N.},
           giveni={L\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \strng{namehash}{eb6a39a428446862c3179c02093cb54c}
      \strng{fullhash}{eb6a39a428446862c3179c02093cb54c}
      \strng{bibnamehash}{eb6a39a428446862c3179c02093cb54c}
      \strng{authorbibnamehash}{eb6a39a428446862c3179c02093cb54c}
      \strng{authornamehash}{eb6a39a428446862c3179c02093cb54c}
      \strng{authorfullhash}{eb6a39a428446862c3179c02093cb54c}
      \field{extraname}{2}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" - linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.}
      \field{booktitle}{2017 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})}
      \field{eventtitle}{2017 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})}
      \field{month}{3}
      \field{title}{Cyclical {{Learning Rates}} for {{Training Neural Networks}}}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{pages}{464\bibrangedash 472}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1109/WACV.2017.58
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2017_Smith/Smith_2017_Cyclical Learning Rates for Training Neural Networks.pdf;/Users/felix/Zotero/storage/F6RH6K44/7926641.html
      \endverb
      \keyw{Computational efficiency,Computer architecture,Neural networks,Schedules,Training,Tuning}
    \endentry
    \entry{steinInterpolationSpatialData1999}{book}{}
      \name{author}{1}{}{%
        {{hash=145b78a38e70132548362a1b1caa4367}{%
           family={Stein},
           familyi={S\bibinitperiod},
           given={Michael\bibnamedelima L.},
           giveni={M\bibinitperiod\bibinitdelim L\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {New York, NY}%
      }
      \list{publisher}{1}{%
        {Springer}%
      }
      \strng{namehash}{145b78a38e70132548362a1b1caa4367}
      \strng{fullhash}{145b78a38e70132548362a1b1caa4367}
      \strng{bibnamehash}{145b78a38e70132548362a1b1caa4367}
      \strng{authorbibnamehash}{145b78a38e70132548362a1b1caa4367}
      \strng{authornamehash}{145b78a38e70132548362a1b1caa4367}
      \strng{authorfullhash}{145b78a38e70132548362a1b1caa4367}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{978-1-4612-7166-6 978-1-4612-1494-6}
      \field{series}{Springer {{Series}} in {{Statistics}}}
      \field{title}{Interpolation of {{Spatial Data}}}
      \field{year}{1999}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.1007/978-1-4612-1494-6
      \endverb
      \verb{file}
      \verb /Users/felix/paper/1999_Stein/Stein_1999_Interpolation of Spatial Data.pdf
      \endverb
      \keyw{digital elevation model,geographic data,Kriging,Likelihood,linear optimization,Normal distribution,Spatial Data,Spatial Statistics,STATISTICA,Variance}
    \endentry
    \entry{subagFollowingGroundStates2021}{article}{}
      \name{author}{1}{}{%
        {{hash=c8339d2579147672f5f2ceff9b80272d}{%
           family={Subag},
           familyi={S\bibinitperiod},
           given={Eliran},
           giveni={E\bibinitperiod}}}%
      }
      \strng{namehash}{c8339d2579147672f5f2ceff9b80272d}
      \strng{fullhash}{c8339d2579147672f5f2ceff9b80272d}
      \strng{bibnamehash}{c8339d2579147672f5f2ceff9b80272d}
      \strng{authorbibnamehash}{c8339d2579147672f5f2ceff9b80272d}
      \strng{authornamehash}{c8339d2579147672f5f2ceff9b80272d}
      \strng{authorfullhash}{c8339d2579147672f5f2ceff9b80272d}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We focus on spherical spin glasses whose Parisi distribution has support of the form [0, q]. For such models we construct paths from the origin to the sphere that consistently remain close to the ground-state energy on the sphere of corresponding radius. The construction uses a greedy strategy, which always follows a direction corresponding to the most negative eigenvalues of the Hessian of the Hamiltonian. For finite mixtures ξ(x) it provides an algorithm of time complexity O(Ndeg(ξ)) to find w.h.p. points with the ground-state energy, up to a small error. For the pure spherical models, the same algorithm reaches the energy −E∞, the conjectural terminal energy for gradient descent. Using the TAP formula for the free energy, for full-RSB models with support [0, q], we are able to prove the correct lower bound on the free energy (namely, prove the lower bound from Parisi's formula), assuming the correctness of the Parisi formula only in the replica symmetric case.}
      \field{issn}{1097-0312}
      \field{journaltitle}{Communications on Pure and Applied Mathematics}
      \field{langid}{english}
      \field{number}{5}
      \field{title}{Following the {{Ground States}} of {{Full-RSB Spherical Spin Glasses}}}
      \field{volume}{74}
      \field{year}{2021}
      \field{dateera}{ce}
      \field{pages}{1021\bibrangedash 1044}
      \range{pages}{24}
      \verb{doi}
      \verb 10.1002/cpa.21922
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2021_Subag/Subag_2021_Following the Ground States of Full-RSB Spherical Spin Glasses.pdf;/Users/felix/Zotero/storage/JG5Y44BS/cpa.html
      \endverb
    \endentry
    \entry{wangBayesianOptimizationBillion2016}{article}{}
      \name{author}{5}{}{%
        {{hash=05bc8d503a2c310ef0976ace7f9d2734}{%
           family={Wang},
           familyi={W\bibinitperiod},
           given={Ziyu},
           giveni={Z\bibinitperiod}}}%
        {{hash=528d4af87fd2ecf5fb8a22db913ce088}{%
           family={Hutter},
           familyi={H\bibinitperiod},
           given={Frank},
           giveni={F\bibinitperiod}}}%
        {{hash=4ef20846e7a2262bf573dd15a0b4466d}{%
           family={Zoghi},
           familyi={Z\bibinitperiod},
           given={Masrour},
           giveni={M\bibinitperiod}}}%
        {{hash=569edd646af5cdcdcea0ea9b2f82b9a6}{%
           family={Matheson},
           familyi={M\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod}}}%
        {{useprefix=false,hash=e2c851c1e98e0213d665f3a3218c390f}{%
           family={Feitas},
           familyi={F\bibinitperiod},
           given={Nando},
           giveni={N\bibinitperiod},
           prefix={de},
           prefixi={d\bibinitperiod}}}%
      }
      \strng{namehash}{ce0f3d71b18ee7367558a7b6d6e6973e}
      \strng{fullhash}{322e7cfb866d034190153f10baf7e535}
      \strng{bibnamehash}{ce0f3d71b18ee7367558a7b6d6e6973e}
      \strng{authorbibnamehash}{ce0f3d71b18ee7367558a7b6d6e6973e}
      \strng{authornamehash}{ce0f3d71b18ee7367558a7b6d6e6973e}
      \strng{authorfullhash}{322e7cfb866d034190153f10baf7e535}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high-dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple, has important invariance properties, and applies to domains with both categorical and continuous variables. We present a thorough theoretical analysis of REMBO. Empirical results confirm that REMBO can effectively solve problems with billions of dimensions, provided the intrinsic dimensionality is low. They also show that REMBO achieves state-of-the-art performance in optimizing the 47 discrete parameters of a popular mixed integer linear programming solver.}
      \field{day}{19}
      \field{issn}{1076-9757}
      \field{journaltitle}{Journal of Artificial Intelligence Research}
      \field{langid}{english}
      \field{month}{2}
      \field{title}{Bayesian {{Optimization}} in a {{Billion Dimensions}} via {{Random Embeddings}}}
      \field{volume}{55}
      \field{year}{2016}
      \field{dateera}{ce}
      \field{pages}{361\bibrangedash 387}
      \range{pages}{27}
      \verb{doi}
      \verb 10.1613/jair.4806
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2016_Wang et al/Wang et al_2016_Bayesian Optimization in a Billion Dimensions via Random Embeddings.pdf
      \endverb
    \endentry
    \entry{williamsBayesianClassificationGaussian1998}{article}{}
      \name{author}{2}{}{%
        {{hash=03fe732f745576c10e0c06a73741997b}{%
           family={Williams},
           familyi={W\bibinitperiod},
           given={C.K.I.},
           giveni={C\bibinitperiod}}}%
        {{hash=63af0569e3be3ec55e9e2661756e9e2c}{%
           family={Barber},
           familyi={B\bibinitperiod},
           given={D.},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{e4d04cc8f2508c98cd9a4db5ef73d159}
      \strng{fullhash}{e4d04cc8f2508c98cd9a4db5ef73d159}
      \strng{bibnamehash}{e4d04cc8f2508c98cd9a4db5ef73d159}
      \strng{authorbibnamehash}{e4d04cc8f2508c98cd9a4db5ef73d159}
      \strng{authornamehash}{e4d04cc8f2508c98cd9a4db5ef73d159}
      \strng{authorfullhash}{e4d04cc8f2508c98cd9a4db5ef73d159}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We consider the problem of assigning an input vector to one of m classes by predicting P(c|x) for c=1,...,m. For a two-class problem, the probability of class one given x is estimated by /spl sigma/(y(x)), where /spl sigma/(y)=1/(1+e/sup -y/). A Gaussian process prior is placed on y(x), and is combined with the training data to obtain predictions for new x points. We provide a Bayesian treatment, integrating over uncertainty in y and in the parameters that control the Gaussian process prior the necessary integration over y is carried out using Laplace's approximation. The method is generalized to multiclass problems (m{$>$}2) using the softmax function. We demonstrate the effectiveness of the method on a number of datasets.}
      \field{eventtitle}{{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}}
      \field{issn}{1939-3539}
      \field{journaltitle}{IEEE Transactions on Pattern Analysis and Machine Intelligence}
      \field{month}{12}
      \field{number}{12}
      \field{title}{Bayesian Classification with {{Gaussian}} Processes}
      \field{volume}{20}
      \field{year}{1998}
      \field{dateera}{ce}
      \field{pages}{1342\bibrangedash 1351}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1109/34.735807
      \endverb
      \verb{file}
      \verb /Users/felix/paper/1998_Williams_Barber/Williams_Barber_1998_Bayesian classification with Gaussian processes.pdf;/Users/felix/Zotero/storage/EJVE6YL8/735807.html
      \endverb
    \endentry
    \entry{williamsComputationInfiniteNeural1998}{article}{}
      \name{author}{1}{}{%
        {{hash=d34fdfcbfcb5412397b946351370db22}{%
           family={Williams},
           familyi={W\bibinitperiod},
           given={Christopher\bibnamedelimb K.\bibnamedelimi I.},
           giveni={C\bibinitperiod\bibinitdelim K\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
      }
      \strng{namehash}{d34fdfcbfcb5412397b946351370db22}
      \strng{fullhash}{d34fdfcbfcb5412397b946351370db22}
      \strng{bibnamehash}{d34fdfcbfcb5412397b946351370db22}
      \strng{authorbibnamehash}{d34fdfcbfcb5412397b946351370db22}
      \strng{authornamehash}{d34fdfcbfcb5412397b946351370db22}
      \strng{authorfullhash}{d34fdfcbfcb5412397b946351370db22}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{For neural networks with a wide class of weight priors, it can be shown that in the limit of an infinite number of hidden units, the prior over functions tends to a gaussian process. In this article, analytic forms are derived for the covariance function of the gaussian processes corresponding to networks with sigmoidal and gaussian hidden units. This allows predictions to be made efficiently using networks with an infinite number of hidden units and shows, somewhat paradoxically, that it may be easier to carry out Bayesian prediction with infinite networks rather than finite ones.}
      \field{day}{1}
      \field{issn}{0899-7667}
      \field{journaltitle}{Neural Computation}
      \field{month}{7}
      \field{number}{5}
      \field{shortjournal}{Neural Computation}
      \field{title}{Computation with {{Infinite Neural Networks}}}
      \field{volume}{10}
      \field{year}{1998}
      \field{dateera}{ce}
      \field{pages}{1203\bibrangedash 1216}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1162/089976698300017412
      \endverb
      \verb{file}
      \verb /Users/felix/paper/1998_Williams/Williams_1998_Computation with Infinite Neural Networks.pdf;/Users/felix/Zotero/storage/LZEA6EBK/Computation-with-Infinite-Neural-Networks.html
      \endverb
    \endentry
    \entry{wuBayesianOptimizationGradients2017}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=bb371597af3d6d97a656e6f879c0189d}{%
           family={Wu},
           familyi={W\bibinitperiod},
           given={Jian},
           giveni={J\bibinitperiod}}}%
        {{hash=2776ab61e5bbbd0a0795eb8c67bb4674}{%
           family={Poloczek},
           familyi={P\bibinitperiod},
           given={Matthias},
           giveni={M\bibinitperiod}}}%
        {{hash=278ffe43354cf506bc6cf64565fff14e}{%
           family={Wilson},
           familyi={W\bibinitperiod},
           given={Andrew\bibnamedelima G},
           giveni={A\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=2395ef596a2897c9dd06f57367b1b397}{%
           family={Frazier},
           familyi={F\bibinitperiod},
           given={Peter},
           giveni={P\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{b4c5ad116ade0274f969846362aa2873}
      \strng{fullhash}{96f73c80ed86f980eb36407ecb0d73a0}
      \strng{bibnamehash}{b4c5ad116ade0274f969846362aa2873}
      \strng{authorbibnamehash}{b4c5ad116ade0274f969846362aa2873}
      \strng{authornamehash}{b4c5ad116ade0274f969846362aa2873}
      \strng{authorfullhash}{96f73c80ed86f980eb36407ecb0d73a0}
      \field{sortinit}{W}
      \field{sortinithash}{4315d78024d0cea9b57a0c6f0e35ed0d}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Bayesian optimization has shown success in global optimization of expensive-to-evaluate multimodal objective functions. However, unlike most optimization methods, Bayesian optimization typically does not use derivative information. In this paper we show how Bayesian optimization can exploit derivative information to find good solutions with fewer objective function evaluations. In particular, we develop a novel Bayesian optimization algorithm, the derivative-enabled knowledge-gradient (dKG), which is one-step Bayes-optimal, asymptotically consistent, and provides greater one-step value of information than in the derivative-free setting. dKG accommodates noisy and incomplete derivative information, comes in both sequential and batch forms, and can optionally reduce the computational cost of inference through automatically selected retention of a single directional derivative. We also compute the dKG acquisition function and its gradient using a novel fast discretization-free technique. We show dKG provides state-of-the-art performance compared to a wide range of optimization procedures with and without gradients, on benchmarks including logistic regression, deep learning, kernel learning, and k-nearest neighbors.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Bayesian {{Optimization}} with {{Gradients}}}
      \field{urlday}{2}
      \field{urlmonth}{6}
      \field{urlyear}{2022}
      \field{volume}{30}
      \field{year}{2017}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/felix/paper/2017_Wu et al/[Supplemental] Wu et al_2017_Bayesian Optimization with Gradients.pdf;/Users/felix/paper/2017_Wu et al/Wu et al_2017_Bayesian Optimization with Gradients.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2017/hash/64a08e5f1e6c39faeb90108c430eb120-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2017/hash/64a08e5f1e6c39faeb90108c430eb120-Abstract.html
      \endverb
    \endentry
    \entry{xiaoFashionMNISTNovelImage2017}{online}{}
      \name{author}{3}{}{%
        {{hash=1af11a48cd2fa4be0cc93269e719b54c}{%
           family={Xiao},
           familyi={X\bibinitperiod},
           given={Han},
           giveni={H\bibinitperiod}}}%
        {{hash=3c17107e356e9e329a5b82ae2f7cd441}{%
           family={Rasul},
           familyi={R\bibinitperiod},
           given={Kashif},
           giveni={K\bibinitperiod}}}%
        {{hash=626b3b250f5889797be5d59aab9e0ac5}{%
           family={Vollgraf},
           familyi={V\bibinitperiod},
           given={Roland},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{baf27447d7d65975cdce75ac5efbf71b}
      \strng{fullhash}{5290b4700ff446de9ce062ea732ebd9a}
      \strng{bibnamehash}{5290b4700ff446de9ce062ea732ebd9a}
      \strng{authorbibnamehash}{5290b4700ff446de9ce062ea732ebd9a}
      \strng{authornamehash}{baf27447d7d65975cdce75ac5efbf71b}
      \strng{authorfullhash}{5290b4700ff446de9ce062ea732ebd9a}
      \field{sortinit}{X}
      \field{sortinithash}{1965c258adceecf23ce3d67b05113442}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist}
      \field{day}{15}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{month}{9}
      \field{pubstate}{prepublished}
      \field{shorttitle}{Fashion-{{MNIST}}}
      \field{title}{Fashion-{{MNIST}}: A {{Novel Image Dataset}} for {{Benchmarking Machine Learning Algorithms}}}
      \field{year}{2017}
      \field{dateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1708.07747
      \endverb
      \verb{eprint}
      \verb 1708.07747
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2017_Xiao et al/Xiao et al_2017_Fashion-MNIST.pdf;/Users/felix/Zotero/storage/TC8E9NTG/1708.html
      \endverb
      \keyw{Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{zeilerADADELTAAdaptiveLearning2012}{unpublished}{}
      \name{author}{1}{}{%
        {{hash=90d345b65e166a9cfc0c6ddec4ddc3d6}{%
           family={Zeiler},
           familyi={Z\bibinitperiod},
           given={Matthew\bibnamedelima D.},
           giveni={M\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \strng{namehash}{90d345b65e166a9cfc0c6ddec4ddc3d6}
      \strng{fullhash}{90d345b65e166a9cfc0c6ddec4ddc3d6}
      \strng{bibnamehash}{90d345b65e166a9cfc0c6ddec4ddc3d6}
      \strng{authorbibnamehash}{90d345b65e166a9cfc0c6ddec4ddc3d6}
      \strng{authornamehash}{90d345b65e166a9cfc0c6ddec4ddc3d6}
      \strng{authorfullhash}{90d345b65e166a9cfc0c6ddec4ddc3d6}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.}
      \field{day}{22}
      \field{eprintclass}{cs}
      \field{eprinttype}{arXiv}
      \field{month}{12}
      \field{shorttitle}{{{ADADELTA}}}
      \field{title}{{{ADADELTA}}: {{An Adaptive Learning Rate Method}}}
      \field{year}{2012}
      \field{dateera}{ce}
      \verb{eprint}
      \verb 1212.5701
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2012_Zeiler/Zeiler_2012_ADADELTA.pdf;/Users/felix/Zotero/storage/KEXJ9289/1212.html
      \endverb
      \keyw{Computer Science - Machine Learning}
    \endentry
    \entry{zhangWhichAlgorithmicChoices2019}{inproceedings}{}
      \name{author}{8}{}{%
        {{hash=67cf1f71a516120793970cb0772e206e}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Guodong},
           giveni={G\bibinitperiod}}}%
        {{hash=0bde480b669967bb3a864e27dbd502cf}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Lala},
           giveni={L\bibinitperiod}}}%
        {{hash=f42b0f2cefb8f54e220b8df5fc3a3ce1}{%
           family={Nado},
           familyi={N\bibinitperiod},
           given={Zachary},
           giveni={Z\bibinitperiod}}}%
        {{hash=6811d5cc72244219e1b98cf2bb1b64f1}{%
           family={Martens},
           familyi={M\bibinitperiod},
           given={James},
           giveni={J\bibinitperiod}}}%
        {{hash=58071d26b9ea8e6a87fe158890d7c57a}{%
           family={Sachdeva},
           familyi={S\bibinitperiod},
           given={Sushant},
           giveni={S\bibinitperiod}}}%
        {{hash=ba040554cec389bc8b5dc4b20d44218c}{%
           family={Dahl},
           familyi={D\bibinitperiod},
           given={George},
           giveni={G\bibinitperiod}}}%
        {{hash=8217f686e554cb86b6cad9ebba97bff4}{%
           family={Shallue},
           familyi={S\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=66cef787d7dbe5bcb5c1447fa4cf547c}{%
           family={Grosse},
           familyi={G\bibinitperiod},
           given={Roger\bibnamedelima B},
           giveni={R\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{c5aa66bde9226d217fe996d81cafa36a}
      \strng{fullhash}{59e972d712e6813c4fe67eeace65917e}
      \strng{bibnamehash}{c5aa66bde9226d217fe996d81cafa36a}
      \strng{authorbibnamehash}{c5aa66bde9226d217fe996d81cafa36a}
      \strng{authornamehash}{c5aa66bde9226d217fe996d81cafa36a}
      \strng{authorfullhash}{59e972d712e6813c4fe67eeace65917e}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Increasing the batch size is a popular way to speed up neural network training, but beyond some critical batch size, larger batch sizes yield diminishing returns. In this work, we study how the critical batch size changes based on properties of the optimization algorithm, including acceleration and preconditioning, through two different lenses: large scale experiments and analysis using a simple noisy quadratic model (NQM). We experimentally demonstrate that optimization algorithms that employ preconditioning, specifically Adam and K-FAC, result in much larger critical batch sizes than stochastic gradient descent with momentum. We also demonstrate that the NQM captures many of the essential features of real neural network training, despite being drastically simpler to work with. The NQM predicts our results with preconditioned optimizers, previous results with accelerated gradient descent, and other results around optimal learning rates and large batch training, making it a useful tool to generate testable predictions about neural network optimization. We demonstrate empirically that the simple noisy quadratic model (NQM) displays many similarities to neural networks in terms of large-batch training. We prove analytical convergence results for the NQM model that predict such behavior and hence provide possible explanations and a better understanding for many large-batch training phenomena.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{eprintclass}{cs, stat}
      \field{eprinttype}{arXiv}
      \field{shorttitle}{Which {{Algorithmic Choices Matter}} at {{Which Batch Sizes}}?}
      \field{title}{Which {{Algorithmic Choices Matter}} at {{Which Batch Sizes}}? {{Insights From}} a {{Noisy Quadratic Model}}}
      \field{urlday}{9}
      \field{urlmonth}{11}
      \field{urlyear}{2023}
      \field{volume}{32}
      \field{year}{2019}
      \field{dateera}{ce}
      \field{urldateera}{ce}
      \verb{eprint}
      \verb 1907.04164
      \endverb
      \verb{file}
      \verb /Users/felix/paper/2019_Zhang et al/[ArXiv] Zhang et al_2019_Which Algorithmic Choices Matter at Which Batch Sizes.pdf;/Users/felix/paper/2019_Zhang et al/Zhang et al_2019_Which Algorithmic Choices Matter at Which Batch Sizes.pdf
      \endverb
      \verb{urlraw}
      \verb https://proceedings.neurips.cc/paper/2019/hash/e0eacd983971634327ae1819ea8b6214-Abstract.html
      \endverb
      \verb{url}
      \verb https://proceedings.neurips.cc/paper/2019/hash/e0eacd983971634327ae1819ea8b6214-Abstract.html
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

