\begin{thebibliography}{10}

\bibitem{aljundi2019task}
R.~Aljundi, K.~Kelchtermans, and T.~Tuytelaars.
\newblock Task-free continual learning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 11254--11263, 2019.

\bibitem{NIPS2019_9354}
R.~Aljundi, M.~Lin, B.~Goujaud, and Y.~Bengio.
\newblock Gradient based sample selection for online continual learning.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d~Alch\'{e}-Buc,
  E.~Fox, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 32}, pages 11816--11825. Curran Associates, Inc., 2019.

\bibitem{arora2019}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, R.~R. Salakhutdinov, and R.~Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d~Alch\'{e}-Buc,
  E.~Fox, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 32}, pages 8139--8148. Curran Associates, Inc., 2019.

\bibitem{arthur2006k}
D.~ARTHUR.
\newblock k-means++: The advantages of careful seeding.
\newblock In {\em Proceedings of the eighteenth annual ACM-SIAM symposium on
  Discrete algorithms, 2007}, pages 1027--1035, 2007.

\bibitem{Bard1998}
J.~F. Bard.
\newblock {\em Practical Bilevel Optimization: Algorithms and Applications}.
\newblock Springer, 1998.

\bibitem{Bian2017}
A.~A. Bian, J.~M. Buhmann, A.~Krause, and S.~Tschiatschek.
\newblock Guarantees for greedy maximization of non-submodular functions with
  applications.
\newblock In D.~Precup and Y.~W. Teh, editors, {\em Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of {\em Proceedings
  of Machine Learning Research}, pages 498--507, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem{campbell2019automated}
T.~Campbell and T.~Broderick.
\newblock Automated scalable bayesian inference via hilbert coresets.
\newblock {\em The Journal of Machine Learning Research}, 20(1):551--588, 2019.

\bibitem{Chaloner1995}
K.~Chaloner and I.~Verdinelli.
\newblock Bayesian experimental design: A review.
\newblock {\em Statist. Sci.}, 10(3):273--304, 08 1995.

\bibitem{chaudhry2019continual}
A.~Chaudhry, M.~Rohrbach, M.~Elhoseiny, T.~Ajanthan, P.~K. Dokania, P.~H. Torr,
  and M.~Ranzato.
\newblock Continual learning with tiny episodic memories.
\newblock {\em arXiv preprint arXiv:1902.10486}, 2019.

\bibitem{chazelle1996linear}
B.~Chazelle and J.~Matou{\v{s}}ek.
\newblock On linear-time deterministic algorithms for optimization problems in
  fixed dimension.
\newblock {\em Journal of Algorithms}, 21(3):579--597, 1996.

\bibitem{chrysakis2020online}
A.~Chrysakis and M.-F. Moens.
\newblock Online continual learning from imbalanced data.
\newblock {\em Proceedings of Machine Learning Research}, 2020.

\bibitem{coleman2020selection}
C.~Coleman, C.~Yeh, S.~Mussmann, B.~Mirzasoleiman, P.~Bailis, P.~Liang,
  J.~Leskovec, and M.~Zaharia.
\newblock Selection via proxy: Efficient data selection for deep learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{cook1980characterizations}
R.~D. Cook and S.~Weisberg.
\newblock Characterizations of an empirical influence function for detecting
  influential cases in regression.
\newblock {\em Technometrics}, 22(4):495--508, 1980.

\bibitem{cook1982residuals}
R.~D. Cook and S.~Weisberg.
\newblock {\em Residuals and influence in regression}.
\newblock New York: Chapman and Hall, 1982.

\bibitem{Das2011}
A.~Das and D.~Kempe.
\newblock Submodular meets spectral: greedy algorithms for subset selection,
  sparse approximation and dictionary selection.
\newblock In {\em Proceedings of the 28th International Conference on
  International Conference on Machine Learning}, pages 1057--1064, 2011.

\bibitem{fanello2013icub}
S.~Fanello, C.~Ciliberto, M.~Santoro, L.~Natale, G.~Metta, L.~Rosasco, and
  F.~Odone.
\newblock icub world: Friendly robots help building good vision data-sets.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 700--705, 2013.

\bibitem{farquhar2018towards}
S.~Farquhar and Y.~Gal.
\newblock Towards robust evaluations of continual learning.
\newblock {\em arXiv preprint arXiv:1805.09733}, 2018.

\bibitem{Fedorov1972}
V.~V. Fedorov.
\newblock {\em Theory of optimal experiments}.
\newblock Probability and mathematical statistics. Academic Press, New York,
  NY, USA, 1972.

\bibitem{feldman2011unified}
D.~Feldman and M.~Langberg.
\newblock A unified framework for approximating and clustering data.
\newblock In {\em Proceedings of the forty-third annual ACM symposium on Theory
  of computing}, pages 569--578. ACM, 2011.

\bibitem{finn2017model}
C.~Finn, P.~Abbeel, and S.~Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1126--1135. JMLR. org, 2017.

\bibitem{franceschi2018bilevel}
L.~Franceschi, P.~Frasconi, S.~Salzo, R.~Grazzi, and M.~Pontil.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock {\em arXiv preprint arXiv:1806.04910}, 2018.

\bibitem{Frank1956}
M.~Frank and P.~Wolfe.
\newblock An algorithm for quadratic programming.
\newblock {\em Naval research logistics quarterly}, 3(1-2):95--110, 1956.

\bibitem{french1999catastrophic}
R.~M. French.
\newblock Catastrophic forgetting in connectionist networks.
\newblock {\em Trends in cognitive sciences}, 3(4):128--135, 1999.

\bibitem{Ghadimi2018}
S.~Ghadimi and W.~Mengdi.
\newblock Approximation methods for bilevel programming.
\newblock {\em arXiv:1802.02246}, 2018.

\bibitem{goodfellow2013empirical}
I.~J. Goodfellow, M.~Mirza, D.~Xiao, A.~Courville, and Y.~Bengio.
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks, 2014.

\bibitem{Harshaw2019}
C.~Harshaw, M.~Feldman, J.~Ward, and A.~Karbasi.
\newblock Submodular maximization beyond non-negativity: Guarantees, fast
  algorithms, and applications.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, {\em Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of {\em
  Proceedings of Machine Learning Research}, pages 2634--2643, Long Beach,
  California, USA, 09--15 Jun 2019. PMLR.

\bibitem{hayes2019memory}
T.~L. Hayes, N.~D. Cahill, and C.~Kanan.
\newblock Memory efficient experience replay for streaming learning.
\newblock In {\em International Conference on Robotics and Automation (ICRA)}.
  IEEE, 2019.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{huggins2016coresets}
J.~Huggins, T.~Campbell, and T.~Broderick.
\newblock Coresets for scalable bayesian logistic regression.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4080--4088, 2016.

\bibitem{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem{DBLP:journals/corr/KingmaB14}
D.~P. Kingma and J.~Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em 3rd International Conference on Learning Representations,
  {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
  Proceedings}, 2015.

\bibitem{kirkpatrick2017overcoming}
J.~Kirkpatrick, R.~Pascanu, N.~Rabinowitz, J.~Veness, G.~Desjardins, A.~A.
  Rusu, K.~Milan, J.~Quan, T.~Ramalho, A.~Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the national academy of sciences},
  114(13):3521--3526, 2017.

\bibitem{kirsch2019batchbald}
A.~Kirsch, J.~van Amersfoort, and Y.~Gal.
\newblock Batchbald: Efficient and diverse batch acquisition for deep bayesian
  active learning.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d~Alch\'{e}-Buc,
  E.~Fox, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems 32}, pages 7024--7035. Curran Associates, Inc., 2019.

\bibitem{koh2017understanding}
P.~W. Koh and P.~Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1885--1894. JMLR. org, 2017.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky et~al.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem{liu1989limited}
D.~C. Liu and J.~Nocedal.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock {\em Mathematical programming}, 45(1-3):503--528, 1989.

\bibitem{liu2018darts}
H.~Liu, K.~Simonyan, and Y.~Yang.
\newblock {DARTS}: Differentiable architecture search.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{Locatello2017a}
F.~Locatello, M.~Tschannen, G.~R{\"a}tsch, and M.~Jaggi.
\newblock Greedy algorithms for cone constrained optimization with convergence
  guarantees.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  773--784, 2017.

\bibitem{lopez2017gradient}
D.~Lopez-Paz and M.~Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6467--6476, 2017.

\bibitem{lorraine2019optimizing}
J.~Lorraine, P.~Vicol, and D.~Duvenaud.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1540--1552. PMLR, 2020.

\bibitem{lucic2017training}
M.~Lucic, M.~Faulkner, A.~Krause, and D.~Feldman.
\newblock Training gaussian mixture models at scale via coresets.
\newblock {\em The Journal of Machine Learning Research}, 18(1):5885--5909,
  2017.

\bibitem{mccloskey1989catastrophic}
M.~McCloskey and N.~J. Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock In {\em Psychology of learning and motivation}, volume~24, pages
  109--165. Elsevier, 1989.

\bibitem{v.2018variational}
C.~V. Nguyen, Y.~Li, T.~D. Bui, and R.~E. Turner.
\newblock Variational continual learning.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{neuraltangents2020}
R.~Novak, L.~Xiao, J.~Hron, J.~Lee, A.~A. Alemi, J.~Sohl-Dickstein, and S.~S.
  Schoenholz.
\newblock Neural tangents: Fast and easy infinite neural networks in python.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{pedregosa2016hyperparameter}
F.~Pedregosa.
\newblock Hyperparameter optimization with approximate gradient.
\newblock In {\em International Conference on Machine Learning}, pages
  737--746, 2016.

\bibitem{rebuffi2017icarl}
S.-A. Rebuffi, A.~Kolesnikov, G.~Sperl, and C.~H. Lampert.
\newblock icarl: Incremental classifier and representation learning.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 2001--2010, 2017.

\bibitem{ren2018learning}
M.~Ren, W.~Zeng, B.~Yang, and R.~Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4331--4340, 2018.

\bibitem{rusu2016progressive}
A.~A. Rusu, N.~C. Rabinowitz, G.~Desjardins, H.~Soyer, J.~Kirkpatrick,
  K.~Kavukcuoglu, R.~Pascanu, and R.~Hadsell.
\newblock Progressive neural networks.
\newblock {\em arXiv preprint arXiv:1606.04671}, 2016.

\bibitem{sener2018active}
O.~Sener and S.~Savarese.
\newblock Active learning for convolutional neural networks: A core-set
  approach.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{shin2017continual}
H.~Shin, J.~K. Lee, J.~Kim, and J.~Kim.
\newblock Continual learning with deep generative replay.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2990--2999, 2017.

\bibitem{tapia2019}
J.~Tapia, E.~Knoop, M.~Mutn{\`y}, M.~A. Otaduy, and M.~B{\"a}cher.
\newblock Makesense: Automated sensor design for proprioceptive soft robots.
\newblock {\em Soft robotics}, 7(3):332--345, 2020.

\bibitem{tibshirani1996regression}
R.~Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock {\em Journal of the Royal Statistical Society: Series B
  (Methodological)}, 58(1):267--288, 1996.

\bibitem{titsias2020functional}
M.~K. Titsias, J.~Schwarz, A.~G. de~G.~Matthews, R.~Pascanu, and Y.~W. Teh.
\newblock Functional regularisation for continual learning with gaussian
  processes.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{vicente1994bilevel}
L.~N. Vicente and P.~H. Calamai.
\newblock Bilevel and multilevel programming: A bibliography review.
\newblock {\em Journal of Global optimization}, 5(3):291--306, 1994.

\bibitem{vitter1985random}
J.~S. Vitter.
\newblock Random sampling with a reservoir.
\newblock {\em ACM Transactions on Mathematical Software (TOMS)}, 11(1):37--57,
  1985.

\bibitem{pmlr-v37-wei15}
K.~Wei, R.~Iyer, and J.~Bilmes.
\newblock Submodularity in data subset selection and active learning.
\newblock In F.~Bach and D.~Blei, editors, {\em Proceedings of the 32nd
  International Conference on Machine Learning}, volume~37 of {\em Proceedings
  of Machine Learning Research}, pages 1954--1963, Lille, France, 07--09 Jul
  2015. PMLR.

\bibitem{pmlr-v70-zenke17a}
F.~Zenke, B.~Poole, and S.~Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock In D.~Precup and Y.~W. Teh, editors, {\em Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of {\em Proceedings
  of Machine Learning Research}, pages 3987--3995, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\end{thebibliography}
