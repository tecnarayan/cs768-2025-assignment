\begin{thebibliography}{10}

\bibitem{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In {\em European conference on computer vision}, pages 630--645.
  Springer, 2016.

\bibitem{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock {\em arXiv preprint arXiv:1312.6199}, 2013.

\bibitem{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{moosavi2016deepfool}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard.
\newblock Deepfool: a simple and accurate method to fool deep neural networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2574--2582, 2016.

\bibitem{papernot2016distillation}
Nicolas Papernot, Patrick McDaniel, Xi~Wu, Somesh Jha, and Ananthram Swami.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In {\em Security and Privacy (SP), 2016 IEEE Symposium on}, pages
  582--597. IEEE, 2016.

\bibitem{kurakin2016adversarial}
Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
\newblock Adversarial machine learning at scale.
\newblock {\em arXiv preprint arXiv:1611.01236}, 2016.

\bibitem{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In {\em Security and Privacy (SP), 2017 IEEE Symposium on}, pages
  39--57. IEEE, 2017.

\bibitem{brendel2017decision}
Wieland Brendel, Jonas Rauber, and Matthias Bethge.
\newblock Decision-based adversarial attacks: Reliable attacks against
  black-box machine learning models.
\newblock {\em arXiv preprint arXiv:1712.04248}, 2017.

\bibitem{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock {\em arXiv preprint arXiv:1802.00420}, 2018.

\bibitem{hein2017formal}
Matthias Hein and Maksym Andriushchenko.
\newblock Formal guarantees on the robustness of a classifier against
  adversarial manipulation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2266--2276, 2017.

\bibitem{raghunathan2018certified}
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.
\newblock Certified defenses against adversarial examples.
\newblock {\em arXiv preprint arXiv:1801.09344}, 2018.

\bibitem{kolter2017provable}
J~Zico Kolter and Eric Wong.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock {\em arXiv preprint arXiv:1711.00851}, 2017.

\bibitem{weng2018towards}
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning,
  Inderjit~S Dhillon, and Luca Daniel.
\newblock Towards fast computation of certified robustness for relu networks.
\newblock {\em arXiv preprint arXiv:1804.09699}, 2018.

\bibitem{zhang2018efficient}
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel.
\newblock Efficient neural network robustness certification with general
  activation functions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4944--4953, 2018.

\bibitem{dvijotham2018dual}
Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and
  Pushmeet Kohli.
\newblock A dual approach to scalable verification of deep networks.
\newblock {\em arXiv preprint arXiv:1803.06567}, 2018.

\bibitem{wong2018scaling}
Eric Wong, Frank Schmidt, Jan~Hendrik Metzen, and J~Zico Kolter.
\newblock Scaling provable adversarial defenses.
\newblock {\em arXiv preprint arXiv:1805.12514}, 2018.

\bibitem{lecuyer2018certified}
Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman
  Jana.
\newblock Certified robustness to adversarial examples with differential
  privacy.
\newblock {\em arXiv preprint arXiv:1802.03471}, 2018.

\bibitem{van2014renyi}
Tim Van~Erven and Peter Harremos.
\newblock R{\'e}nyi divergence and kullback-leibler divergence.
\newblock {\em IEEE Transactions on Information Theory}, 60(7):3797--3820,
  2014.

\bibitem{anonymous2020ell}
Anonymous.
\newblock {\$}{\textbackslash}ell{\_}1{\$} adversarial robustness certificates:
  a randomized smoothing approach.
\newblock In {\em Submitted to International Conference on Learning
  Representations}, 2020.
\newblock under review.

\bibitem{namkoong2017variance}
Hongseok Namkoong and John~C Duchi.
\newblock Variance-based regularization with convex objectives.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2971--2980, 2017.

\bibitem{sinha2017certifiable}
Aman Sinha, Hongseok Namkoong, and John Duchi.
\newblock Certifiable distributional robustness with principled adversarial
  training.
\newblock {\em arXiv preprint arXiv:1710.10571}, 2017.

\bibitem{fawzi2016robustness}
Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard.
\newblock Robustness of classifiers: from adversarial to random noise.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1632--1640, 2016.

\bibitem{ford2019adversarial}
Nic Ford, Justin Gilmer, Nicolas Carlini, and Dogus Cubuk.
\newblock Adversarial examples are a natural consequence of test error in
  noise.
\newblock {\em arXiv preprint arXiv:1901.10513}, 2019.

\bibitem{unrestricted_advex_2018}
T.~B. {Brown}, N.~{Carlini}, C.~{Zhang}, C.~{Olsson}, P.~{Christiano}, and
  I.~{Goodfellow}.
\newblock Unrestricted adversarial examples.
\newblock {\em arXiv preprint arXiv:1809.08352}, 2018.

\bibitem{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock {\em arXiv preprint arXiv:1706.06083}, 2017.

\bibitem{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric~P Xing, Laurent~El Ghaoui, and
  Michael~I Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock {\em arXiv preprint arXiv:1901.08573}, 2019.

\bibitem{zantedeschi2017efficient}
Valentina Zantedeschi, Maria-Irina Nicolae, and Ambrish Rawat.
\newblock Efficient defenses against adversarial attacks.
\newblock In {\em Proceedings of the 10th ACM Workshop on Artificial
  Intelligence and Security}, pages 39--49. ACM, 2017.

\bibitem{carlini2017magnet}
Nicholas Carlini and David Wagner.
\newblock Magnet and" efficient defenses against adversarial attacks" are not
  robust to adversarial examples.
\newblock {\em arXiv preprint arXiv:1711.08478}, 2017.

\bibitem{xie2012image}
Junyuan Xie, Linli Xu, and Enhong Chen.
\newblock Image denoising and inpainting with deep neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  341--349, 2012.

\bibitem{zhang2017beyond}
Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang.
\newblock Beyond a gaussian denoiser: Residual learning of deep cnn for image
  denoising.
\newblock {\em IEEE Transactions on Image Processing}, 26(7):3142--3155, 2017.

\bibitem{bachman2014learning}
Philip Bachman, Ouais Alsharif, and Doina Precup.
\newblock Learning with pseudo-ensembles.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3365--3373, 2014.

\bibitem{zheng2016improving}
Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow.
\newblock Improving the robustness of deep neural networks via stability
  training.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4480--4488, 2016.

\bibitem{kannan2018adversarial}
Harini Kannan, Alexey Kurakin, and Ian Goodfellow.
\newblock Adversarial logit pairing.
\newblock {\em arXiv preprint arXiv:1803.06373}, 2018.

\bibitem{engstrom2018evaluating}
Logan Engstrom, Andrew Ilyas, and Anish Athalye.
\newblock Evaluating and understanding the robustness of adversarial logit
  pairing.
\newblock {\em arXiv preprint arXiv:1807.10272}, 2018.

\bibitem{tramer2017ensemble}
Florian Tram{\`e}r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan
  Boneh, and Patrick McDaniel.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock {\em arXiv preprint arXiv:1705.07204}, 2017.

\bibitem{imagenet_cvpr09}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In {\em CVPR09}, 2009.

\bibitem{carlini2019evaluating}
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas
  Rauber, Dimitris Tsipras, Ian Goodfellow, and Aleksander Madry.
\newblock On evaluating adversarial robustness.
\newblock {\em arXiv preprint arXiv:1902.06705}, 2019.

\bibitem{1905.06455}
Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin.
\newblock On norm-agnostic robustness of adversarial training, 2019.

\bibitem{cohen2019certified}
Jeremy~M Cohen, Elan Rosenfeld, and J~Zico Kolter.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock {\em arXiv preprint arXiv:1902.02918}, 2019.

\end{thebibliography}
