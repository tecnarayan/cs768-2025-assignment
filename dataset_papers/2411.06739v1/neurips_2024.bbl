\begin{thebibliography}{}

\bibitem[Abbeel and Ng, 2005]{abbeel2005exploration}
Abbeel, P. and Ng, A.~Y. (2005).
\newblock Exploration and apprenticeship learning in reinforcement learning.
\newblock In {\em Proceedings of the 22nd international conference on Machine learning}, pages 1--8.

\bibitem[Agarwal et~al., 2020]{agarwal2020flambe}
Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W. (2020).
\newblock Flambe: Structural complexity and representation learning of low rank mdps.
\newblock {\em Advances in neural information processing systems}, 33:20095--20107.

\bibitem[Ayoub et~al., 2020]{ayoub2020model}
Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L. (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In {\em International Conference on Machine Learning}, pages 463--474. PMLR.

\bibitem[Cai et~al., 2020]{cai2020provably}
Cai, Q., Yang, Z., Jin, C., and Wang, Z. (2020).
\newblock Provably efficient exploration in policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages 1283--1294. PMLR.

\bibitem[Chen et~al., 2022a]{chen2022unified}
Chen, F., Mei, S., and Bai, Y. (2022a).
\newblock Unified algorithms for rl with decision-estimation coefficients: No-regret, pac, and reward-free learning.
\newblock {\em arXiv preprint arXiv:2209.11745}.

\bibitem[Chen et~al., 2022b]{chen2022statistical}
Chen, J., Modi, A., Krishnamurthy, A., Jiang, N., and Agarwal, A. (2022b).
\newblock On the statistical efficiency of reward-free exploration in non-linear rl.
\newblock {\em Advances in Neural Information Processing Systems}, 35:20960--20973.

\bibitem[Chen and Luo, 2021]{chen2021finding}
Chen, L. and Luo, H. (2021).
\newblock Finding the stochastic shortest path with low regret: The adversarial cost and unknown transition case.
\newblock In {\em International Conference on Machine Learning}, pages 1651--1660. PMLR.

\bibitem[Cheng et~al., 2023]{cheng2023improved}
Cheng, Y., Huang, R., Yang, J., and Liang, Y. (2023).
\newblock Improved sample complexity for reward-free reinforcement learning under low-rank mdps.
\newblock {\em arXiv preprint arXiv:2303.10859}.

\bibitem[Dai et~al., 2022]{dai2022follow}
Dai, Y., Luo, H., and Chen, L. (2022).
\newblock Follow-the-perturbed-leader for adversarial markov decision processes with bandit feedback.
\newblock {\em Advances in Neural Information Processing Systems}, 35:11437--11449.

\bibitem[Dai et~al., 2023]{dai2023refined}
Dai, Y., Luo, H., Wei, C.-Y., and Zimmert, J. (2023).
\newblock Refined regret for adversarial mdps with linear function approximation.
\newblock In {\em International Conference on Machine Learning}, pages 6726--6759. PMLR.

\bibitem[Dann et~al., 2023]{dann2023best}
Dann, C., Wei, C.-Y., and Zimmert, J. (2023).
\newblock Best of both worlds policy optimization.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Du et~al., 2021]{du2021bilinear}
Du, S., Kakade, S., Lee, J., Lovett, S., Mahajan, G., Sun, W., and Wang, R. (2021).
\newblock Bilinear classes: A structural framework for provable generalization in rl.
\newblock In {\em International Conference on Machine Learning}, pages 2826--2836. PMLR.

\bibitem[Foster et~al., 2021]{foster2021statistical}
Foster, D.~J., Kakade, S.~M., Qian, J., and Rakhlin, A. (2021).
\newblock The statistical complexity of interactive decision making.
\newblock {\em arXiv preprint arXiv:2112.13487}.

\bibitem[Foster et~al., 2022]{foster2022complexity}
Foster, D.~J., Rakhlin, A., Sekhari, A., and Sridharan, K. (2022).
\newblock On the complexity of adversarial decision making.
\newblock {\em Advances in Neural Information Processing Systems}, 35:35404--35417.

\bibitem[He et~al., 2022]{he2022near}
He, J., Zhou, D., and Gu, Q. (2022).
\newblock Near-optimal policy optimization algorithms for learning adversarial linear mixture mdps.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 4259--4280. PMLR.

\bibitem[Huang et~al., 2023]{huang2023reinforcement}
Huang, A., Chen, J., and Jiang, N. (2023).
\newblock Reinforcement learning in low-rank mdps with density features.
\newblock In {\em International Conference on Machine Learning}, pages 13710--13752. PMLR.

\bibitem[Jiang et~al., 2017]{jiang2017contextual}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E. (2017).
\newblock Contextual decision processes with low bellman rank are pac-learnable.
\newblock In {\em International Conference on Machine Learning}, pages 1704--1713. PMLR.

\bibitem[Jin et~al., 2020a]{jin2020learning}
Jin, C., Jin, T., Luo, H., Sra, S., and Yu, T. (2020a).
\newblock Learning adversarial markov decision processes with bandit feedback and unknown transition.
\newblock In {\em International Conference on Machine Learning}, pages 4860--4869. PMLR.

\bibitem[Jin et~al., 2021a]{jin2021bellman}
Jin, C., Liu, Q., and Miryoosefi, S. (2021a).
\newblock Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms.
\newblock {\em Advances in neural information processing systems}, 34:13406--13418.

\bibitem[Jin et~al., 2020b]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I. (2020b).
\newblock Provably efficient reinforcement learning with linear function approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143. PMLR.

\bibitem[Jin et~al., 2021b]{jin2021best}
Jin, T., Huang, L., and Luo, H. (2021b).
\newblock The best of both worlds: stochastic and adversarial episodic mdps with unknown transition.
\newblock {\em Advances in Neural Information Processing Systems}, 34:20491--20502.

\bibitem[Kakade and Langford, 2002]{kakade2002approximately}
Kakade, S. and Langford, J. (2002).
\newblock Approximately optimal approximate reinforcement learning.
\newblock In {\em Proceedings of the Nineteenth International Conference on Machine Learning}, pages 267--274.

\bibitem[Kong et~al., 2023]{kong2023improved}
Kong, F., Zhang, X., Wang, B., and Li, S. (2023).
\newblock Improved regret bounds for linear adversarial mdps via linear optimization.
\newblock {\em arXiv preprint arXiv:2302.06834}.

\bibitem[Lattimore and Szepesv{\'a}ri, 2020]{lattimore2020bandit}
Lattimore, T. and Szepesv{\'a}ri, C. (2020).
\newblock {\em Bandit algorithms}.
\newblock Cambridge University Press.

\bibitem[Lee et~al., 2020]{lee2020bias}
Lee, C.-W., Luo, H., Wei, C.-Y., and Zhang, M. (2020).
\newblock Bias no more: high-probability data-dependent regret bounds for adversarial bandits and mdps.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Liu et~al., 2023]{liu2023towards}
Liu, H., Wei, C.-Y., and Zimmert, J. (2023).
\newblock Towards optimal regret in adversarial linear mdps with bandit feedback.
\newblock {\em arXiv preprint arXiv:2310.11550}.

\bibitem[Luo et~al., 2021]{luo2021policy}
Luo, H., Wei, C.-Y., and Lee, C.-W. (2021).
\newblock Policy optimization in adversarial mdps: Improved exploration via dilated bonuses.
\newblock {\em Advances in Neural Information Processing Systems}, 34:22931--22942.

\bibitem[Mhammedi et~al., 2023]{mhammedi2023efficient}
Mhammedi, Z., Block, A., Foster, D.~J., and Rakhlin, A. (2023).
\newblock Efficient model-free exploration in low-rank mdps.
\newblock {\em arXiv preprint arXiv:2307.03997}.

\bibitem[Mhammedi et~al., 2024a]{mhammedi2024efficient}
Mhammedi, Z., Block, A., Foster, D.~J., and Rakhlin, A. (2024a).
\newblock Efficient model-free exploration in low-rank mdps.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Mhammedi et~al., 2024b]{mhammedi2024power}
Mhammedi, Z., Foster, D.~J., and Rakhlin, A. (2024b).
\newblock The power of resets in online reinforcement learning.
\newblock {\em arXiv preprint arXiv:2404.15417}.

\bibitem[Modi et~al., 2024]{modi2024model}
Modi, A., Chen, J., Krishnamurthy, A., Jiang, N., and Agarwal, A. (2024).
\newblock Model-free representation learning and exploration in low-rank mdps.
\newblock {\em Journal of Machine Learning Research}, 25(6):1--76.

\bibitem[Padakandla et~al., 2020]{padakandla2020reinforcement}
Padakandla, S., KJ, P., and Bhatnagar, S. (2020).
\newblock Reinforcement learning algorithm for non-stationary environments.
\newblock {\em Applied Intelligence}, 50(11):3590--3606.

\bibitem[Ren et~al., 2022]{ren2022free}
Ren, T., Zhang, T., Szepesv{\'a}ri, C., and Dai, B. (2022).
\newblock A free lunch from the noise: Provable and practical exploration for representation learning.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 1686--1696. PMLR.

\bibitem[Rosenberg and Mansour, 2019]{rosenberg2019online}
Rosenberg, A. and Mansour, Y. (2019).
\newblock Online stochastic shortest path with bandit feedback and unknown transition function.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Shani et~al., 2020]{shani2020optimistic}
Shani, L., Efroni, Y., Rosenberg, A., and Mannor, S. (2020).
\newblock Optimistic policy optimization with bandit feedback.
\newblock In {\em International Conference on Machine Learning}, pages 8604--8613. PMLR.

\bibitem[Sherman et~al., 2023a]{sherman2023rate}
Sherman, U., Cohen, A., Koren, T., and Mansour, Y. (2023a).
\newblock Rate-optimal policy optimization for linear markov decision processes.
\newblock {\em arXiv preprint arXiv:2308.14642}.

\bibitem[Sherman et~al., 2023b]{sherman2023improved}
Sherman, U., Koren, T., and Mansour, Y. (2023b).
\newblock Improved regret for efficient online reinforcement learning with linear function approximation.
\newblock In {\em International Conference on Machine Learning}.

\bibitem[Uehara et~al., 2021]{uehara2021representation}
Uehara, M., Zhang, X., and Sun, W. (2021).
\newblock Representation learning for online and offline rl in low-rank mdps.
\newblock {\em arXiv preprint arXiv:2110.04652}.

\bibitem[Xie et~al., 2022]{xie2022role}
Xie, T., Foster, D.~J., Bai, Y., Jiang, N., and Kakade, S.~M. (2022).
\newblock The role of coverage in online reinforcement learning.
\newblock {\em arXiv preprint arXiv:2210.04157}.

\bibitem[Zhang et~al., 2022a]{zhang2022making}
Zhang, T., Ren, T., Yang, M., Gonzalez, J., Schuurmans, D., and Dai, B. (2022a).
\newblock Making linear mdps practical via contrastive representation learning.
\newblock In {\em International Conference on Machine Learning}, pages 26447--26466. PMLR.

\bibitem[Zhang et~al., 2022b]{zhang2022efficient}
Zhang, X., Song, Y., Uehara, M., Wang, M., Agarwal, A., and Sun, W. (2022b).
\newblock Efficient reinforcement learning in block mdps: A model-free representation learning approach.
\newblock In {\em International Conference on Machine Learning}, pages 26517--26547. PMLR.

\bibitem[Zhao et~al., 2022]{zhao2022mixture}
Zhao, C., Yang, R., Wang, B., and Li, S. (2022).
\newblock Learning adversarial linear mixture markov decision processes with bandit feedback and unknown transition.
\newblock In {\em The Eleventh International Conference on Learning Representations}.

\bibitem[Zhao et~al., 2024]{zhao2024learning}
Zhao, C., Yang, R., Wang, B., Zhang, X., and Li, S. (2024).
\newblock Learning adversarial low-rank markov decision processes with unknown transition and full-information feedback.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Zhong et~al., 2022]{zhong2022gec}
Zhong, H., Xiong, W., Zheng, S., Wang, L., Wang, Z., Yang, Z., and Zhang, T. (2022).
\newblock Gec: A unified framework for interactive decision making in mdp, pomdp, and beyond.
\newblock {\em arXiv preprint arXiv:2211.01962}.

\end{thebibliography}
