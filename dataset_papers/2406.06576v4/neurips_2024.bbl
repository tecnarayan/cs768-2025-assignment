\begin{thebibliography}{10}

\bibitem{translation_1}
Biao Zhang, Barry Haddow, and Alexandra Birch.
\newblock Prompting large language model for machine translation: A case study, 2023.

\bibitem{translation_multi}
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li.
\newblock Multilingual machine translation with large language models: Empirical results and analysis, 2023.

\bibitem{sa_1}
Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner, and Michael Bendersky.
\newblock Llms to the moon? reddit market sentiment analysis with large language models.
\newblock In {\em Companion Proceedings of the ACM Web Conference 2023}, WWW '23 Companion, page 1014–1019, New York, NY, USA, 2023. Association for Computing Machinery.

\bibitem{sa_2}
Zengzhi {Wang}, Qiming {Xie}, Yi~{Feng}, Zixiang {Ding}, Zinong {Yang}, and Rui {Xia}.
\newblock {Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study}.
\newblock {\em arXiv e-prints}, page arXiv:2304.04339, April 2023.

\bibitem{sa_3}
Wenxuan Zhang, Yue Deng, Bing Liu, Sinno~Jialin Pan, and Lidong Bing.
\newblock Sentiment analysis in the era of large language models: A reality check, 2023.

\bibitem{gpt4}
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, ..., and Barret Zoph.
\newblock Gpt-4 technical report, 2024.

\bibitem{gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, ..., and Oriol Vinyals.
\newblock Gemini: A family of highly capable multimodal models, 2024.

\bibitem{math401}
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang.
\newblock How well do large language models perform in arithmetic tasks?, 2023.

\bibitem{multiagent_1}
Joon~Sung Park, Joseph~C. O'Brien, Carrie~J. Cai, Meredith~Ringel Morris, Percy Liang, and Michael~S. Bernstein.
\newblock Generative agents: Interactive simulacra of human behavior, 2023.

\bibitem{multiagent_2}
Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh~V. Chawla, Olaf Wiest, and Xiangliang Zhang.
\newblock Large language model based multi-agents: A survey of progress and challenges, 2024.

\bibitem{OccamNet}
Owen {Dugan}, Rumen {Dangovski}, Allan {Costa}, Samuel {Kim}, Pawan {Goyal}, Joseph {Jacobson}, and Marin {Solja{\v{c}}i{\'c}}.
\newblock {OccamNet: A Fast Neural Model for Symbolic Regression at Scale}.
\newblock {\em arXiv e-prints}, page arXiv:2007.10784, July 2020.

\bibitem{OccamNetSocialSci}
Julia {Balla}, Sihao {Huang}, Owen {Dugan}, Rumen {Dangovski}, and Marin {Soljacic}.
\newblock {AI-Assisted Discovery of Quantitative and Formal Models in Social Science}.
\newblock {\em arXiv e-prints}, page arXiv:2210.00563, October 2022.

\bibitem{catastrophic}
Michael McCloskey and Neal~J. Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential learning problem.
\newblock volume~24 of {\em Psychology of Learning and Motivation}, pages 109--165. Academic Press, 1989.

\bibitem{2023arXiv230801154M}
Davide {Maltoni} and Matteo {Ferrara}.
\newblock {Arithmetic with Language Models: from Memorization to Computation}.
\newblock {\em arXiv e-prints}, page arXiv:2308.01154, August 2023.

\bibitem{2023arXiv230903241Y}
Zhen {Yang}, Ming {Ding}, Qingsong {Lv}, Zhihuan {Jiang}, Zehai {He}, Yuyi {Guo}, Jinfeng {Bai}, and Jie {Tang}.
\newblock {GPT Can Solve Mathematical Problems Without a Calculator}.
\newblock {\em arXiv e-prints}, page arXiv:2309.03241, September 2023.

\bibitem{ft_full}
Yixin Liu, Avi Singh, C.~Daniel Freeman, John~D. Co-Reyes, and Peter~J. Liu.
\newblock Improving large language model fine-tuning for solving math problems, 2023.

\bibitem{wang2023mathcoder}
Ke~Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li.
\newblock Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning, 2023.

\bibitem{ft_adapters}
Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee.
\newblock Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models, 2023.

\bibitem{french1999catastrophic}
Robert~M French.
\newblock Catastrophic forgetting in connectionist networks.
\newblock {\em Trends in cognitive sciences}, 3(4):128--135, 1999.

\bibitem{peft_1}
Haolin Chen and Philip~N. Garner.
\newblock Bayesian parameter-efficient fine-tuning for overcoming catastrophic forgetting, 2024.

\bibitem{peft_2}
Shuo Liu, Jacky Keung, Zhen Yang, Fang Liu, Qilin Zhou, and Yihan Liao.
\newblock Delving into parameter-efficient fine-tuning in code change learning: An empirical study, 2024.

\bibitem{kadlčík2023calcx}
Marek Kadlčík, Michal Štefánik, Ondřej Sotolář, and Vlastimil Martinek.
\newblock Calc-x and calcformers: Empowering arithmetical chain-of-thought through interaction with symbolic systems, 2023.

\bibitem{komeili-etal-2022-internet}
Mojtaba Komeili, Kurt Shuster, and Jason Weston.
\newblock {I}nternet-augmented dialogue generation.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 8460--8478, Dublin, Ireland, May 2022. Association for Computational Linguistics.

\bibitem{nakano2022webgpt}
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu~Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.
\newblock Webgpt: Browser-assisted question-answering with human feedback, 2022.

\bibitem{thoppilan2022lamda}
Romal Thoppilan, Daniel~De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du, YaGuang Li, Hongrae Lee, Huaixiu~Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith~Ringel Morris, Tulsee Doshi, Renelito~Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed~Chi, and Quoc Le.
\newblock Lamda: Language models for dialog applications, 2022.

\bibitem{schick2023toolformer}
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
\newblock Toolformer: Language models can teach themselves to use tools, 2023.

\bibitem{wei2023chainofthought}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed~Chi, Quoc Le, and Denny Zhou.
\newblock Chain-of-thought prompting elicits reasoning in large language models, 2023.

\bibitem{gao2023pal}
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig.
\newblock Pal: Program-aided language models, 2023.

\bibitem{chen2023program}
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William~W. Cohen.
\newblock Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2023.

\bibitem{zhou2023solving}
Aojun Zhou, Ke~Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li.
\newblock Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification, 2023.

\bibitem{attentionAllYouNeed}
Ashish {Vaswani}, Noam {Shazeer}, Niki {Parmar}, Jakob {Uszkoreit}, Llion {Jones}, Aidan~N. {Gomez}, Lukasz {Kaiser}, and Illia {Polosukhin}.
\newblock {Attention Is All You Need}.
\newblock {\em arXiv e-prints}, page arXiv:1706.03762, June 2017.

\bibitem{rnns}
Robin~M. {Schmidt}.
\newblock {Recurrent Neural Networks (RNNs): A gentle Introduction and Overview}.
\newblock {\em arXiv e-prints}, page arXiv:1912.05911, November 2019.

\bibitem{multiarith}
Subhro Roy and Dan Roth.
\newblock Solving general arithmetic word problems, 2016.

\bibitem{reinforce}
Ronald~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist reinforcement learning.
\newblock {\em Mach. Learn.}, 8(3–4):229–256, May 1992.

\bibitem{llama3modelcard}
AI@Meta.
\newblock Llama 3 model card.
\newblock 2024.

\bibitem{llama2}
Hugo {Touvron}, Louis {Martin}, Kevin {Stone}, Peter {Albert}, Amjad {Almahairi}, Yasmine {Babaei}, Nikolay {Bashlykov}, Soumya {Batra}, Prajjwal {Bhargava}, Shruti {Bhosale}, Dan {Bikel}, Lukas {Blecher}, Cristian {Canton Ferrer}, Moya {Chen}, Guillem {Cucurull}, David {Esiobu}, Jude {Fernandes}, Jeremy {Fu}, Wenyin {Fu}, Brian {Fuller}, Cynthia {Gao}, Vedanuj {Goswami}, Naman {Goyal}, Anthony {Hartshorn}, Saghar {Hosseini}, Rui {Hou}, Hakan {Inan}, Marcin {Kardas}, Viktor {Kerkez}, Madian {Khabsa}, Isabel {Kloumann}, Artem {Korenev}, Punit {Singh Koura}, Marie-Anne {Lachaux}, Thibaut {Lavril}, Jenya {Lee}, Diana {Liskovich}, Yinghai {Lu}, Yuning {Mao}, Xavier {Martinet}, Todor {Mihaylov}, Pushkar {Mishra}, Igor {Molybog}, Yixin {Nie}, Andrew {Poulton}, Jeremy {Reizenstein}, Rashi {Rungta}, Kalyan {Saladi}, Alan {Schelten}, Ruan {Silva}, Eric~Michael {Smith}, Ranjan {Subramanian}, Xiaoqing~Ellen {Tan}, Binh {Tang}, Ross {Taylor}, Adina {Williams}, Jian~Xiang {Kuan}, Puxin {Xu}, Zheng {Yan}, Iliyan {Zarov}, Yuchen {Zhang}, Angela {Fan}, Melanie {Kambadur}, Sharan {Narang}, Aurelien {Rodriguez}, Robert {Stojnic}, Sergey {Edunov}, and Thomas {Scialom}.
\newblock {Llama 2: Open Foundation and Fine-Tuned Chat Models}.
\newblock {\em arXiv e-prints}, page arXiv:2307.09288, July 2023.

\bibitem{gpt4o}
OpenAI.
\newblock Hello gpt-4o.
\newblock 2024.

\bibitem{spec_decoding}
Yaniv {Leviathan}, Matan {Kalman}, and Yossi {Matias}.
\newblock {Fast Inference from Transformers via Speculative Decoding}.
\newblock {\em arXiv e-prints}, page arXiv:2211.17192, November 2022.

\bibitem{addsub}
Mohammad~Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman.
\newblock Learning to solve arithmetic word problems with verb categorization.
\newblock In Alessandro Moschitti, Bo~Pang, and Walter Daelemans, editors, {\em Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})}, pages 523--533, Doha, Qatar, October 2014. Association for Computational Linguistics.

\bibitem{gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
\newblock Training verifiers to solve math word problems.
\newblock {\em ArXiv}, abs/2110.14168, 2021.

\bibitem{singleeq}
Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena~Dumas Ang.
\newblock Parsing algebraic word problems into equations.
\newblock {\em Transactions of the Association for Computational Linguistics}, 3:585--597, 2015.

\bibitem{svamp}
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
\newblock Are {NLP} models really able to solve simple math word problems?
\newblock In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz~Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, {\em Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 2080--2094, Online, June 2021. Association for Computational Linguistics.

\bibitem{staged_spec_decoding}
Benjamin {Spector} and Chris {Re}.
\newblock {Accelerating LLM Inference with Staged Speculative Decoding}.
\newblock {\em arXiv e-prints}, page arXiv:2308.04623, August 2023.

\bibitem{shi2022languagemodelsmultilingualchainofthought}
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung~Won Chung, Yi~Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei.
\newblock Language models are multilingual chain-of-thought reasoners, 2022.

\bibitem{EQLOriginal}
Georg {Martius} and Christoph~H. {Lampert}.
\newblock {Extrapolation and learning equations}.
\newblock {\em arXiv e-prints}, page arXiv:1610.02995, October 2016.

\bibitem{EQLWithDivision}
Subham Sahoo, Christoph Lampert, and Georg Martius.
\newblock Learning equations for extrapolation and control.
\newblock In Jennifer Dy and Andreas Krause, editors, {\em Proceedings of the 35th International Conference on Machine Learning}, volume~80 of {\em Proceedings of Machine Learning Research}, pages 4442--4450, Stockholmsmässan, Stockholm Sweden, 10--15 Jul 2018. PMLR.

\bibitem{ppo}
John {Schulman}, Filip {Wolski}, Prafulla {Dhariwal}, Alec {Radford}, and Oleg {Klimov}.
\newblock {Proximal Policy Optimization Algorithms}.
\newblock {\em arXiv e-prints}, page arXiv:1707.06347, July 2017.

\end{thebibliography}
