@article{gato,
title={A Generalist Agent},
author={Scott Reed and Konrad Zolna and Emilio Parisotto and Sergio G{\'o}mez Colmenarejo and Alexander Novikov and Gabriel Barth-maron and Mai Gim{\'e}nez and Yury Sulsky and Jackie Kay and Jost Tobias Springenberg and Tom Eccles and Jake Bruce and Ali Razavi and Ashley Edwards and Nicolas Heess and Yutian Chen and Raia Hadsell and Oriol Vinyals and Mahyar Bordbar and Nando de Freitas},
journal={Transactions on Machine Learning Research},
year={2022},
url={https://openreview.net/forum?id=1ikK0kHjvj},
note={Featured Certification}
}

@misc{ofa,
  doi = {10.48550/ARXIV.2202.03052},
  
  url = {https://arxiv.org/abs/2202.03052},
  
  author = {Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
  
  keywords = {Computer Vision and Pattern Recognition (cs.CV), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}
@misc{4_bit_scaling,
  doi = {10.48550/ARXIV.2212.09720},
  
  url = {https://arxiv.org/abs/2212.09720},
  
  author = {Dettmers, Tim and Zettlemoyer, Luke},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The case for 4-bit precision: k-bit Inference Scaling Laws},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
@misc{scaling_vs_arch,
  doi = {10.48550/ARXIV.2207.10551},
  
  url = {https://arxiv.org/abs/2207.10551},
  
  author = {Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Chung, Hyung Won and Fedus, William and Rao, Jinfeng and Narang, Sharan and Tran, Vinh Q. and Yogatama, Dani and Metzler, Donald},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{taming,
      title={Taming Transformers for High-Resolution Image Synthesis},
      author={Patrick Esser and Robin Rombach and Björn Ommer},
      year={2020},
      eprint={2012.09841},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{megatron,
  doi = {10.48550/ARXIV.1909.08053},
  
  url = {https://arxiv.org/abs/1909.08053},
  
  author = {Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{bf16,
  doi = {10.48550/ARXIV.1905.12322},
  
  url = {https://arxiv.org/abs/1905.12322},
  
  author = {Kalamkar, Dhiraj and Mudigere, Dheevatsa and Mellempudi, Naveen and Das, Dipankar and Banerjee, Kunal and Avancha, Sasikanth and Vooturi, Dharma Teja and Jammalamadaka, Nataraj and Huang, Jianyu and Yuen, Hector and Yang, Jiyan and Park, Jongsoo and Heinecke, Alexander and Georganas, Evangelos and Srinivasan, Sudarshan and Kundu, Abhisek and Smelyanskiy, Misha and Kaul, Bharat and Dubey, Pradeep},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Study of BFLOAT16 for Deep Learning Training},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@InProceedings{Merlotreserved,
    author    = {Zellers, Rowan and Lu, Jiasen and Lu, Ximing and Yu, Youngjae and Zhao, Yanpeng and Salehi, Mohammadreza and Kusupati, Aditya and Hessel, Jack and Farhadi, Ali and Choi, Yejin},
    title     = {MERLOT Reserve: Neural Script Knowledge Through Vision and Language and Sound},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {16375-16387}
}

@ARTICLE{hubert,
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units}, 
  year={2021},
  volume={29},
  number={},
  pages={3451-3460},
  doi={10.1109/TASLP.2021.3122291}}

@inproceedings{donahue-etal-2020-enabling,
  title={Enabling language models to fill in the blanks},
  author={Donahue, Chris and Lee, Mina and Liang, Percy},
  booktitle={ACL},
  year={2020}
}

@inproceedings{gulwani2017synthesis,
  title={Program synthesis},
  author={Sumit Gulwani and Oleksandr Polozov and Rishabh Singh},
  booktitle={Foundations and Trends in Programming Languages},
  year={2017}
}

@inproceedings{manna1971synthesis,
  title={Toward automatic program synthesis},
  author={Zohar Manna and Richard J Waldinger},
  booktitle={Communications of the ACM},
  year={1971}
}

@inproceedings{lee2021deduplicating,
  title={Deduplicating training data makes language models better},
  author={Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and Carlini, Nicholas},
  booktitle={ACL},
  year={2022}
}

@article{kandpal2022deduplicating,
  title={Deduplicating training data mitigates privacy risks in language models},
  author={Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
  journal={arXiv preprint arXiv:2202.06539},
  year={2022}
}

@inproceedings{svyatkovskiy2021fast,
  title={Fast and memory-efficient neural code completion},
  author={Svyatkovskiy, Alexey and Lee, Sebastian and Hadjitofi, Anna and Riechert, Maik and Franco, Juliana Vicente and Allamanis, Miltiadis},
  booktitle={ACM MSR},
  year={2021},
}

@inproceedings{feng2020codebert,
  title={{CodeBERT}: {A} pre-trained model for programming and natural languages},
  author={Feng, Zhangyin and Guo, Daya and Tang, Duyu and Duan, Nan and Feng, Xiaocheng and Gong, Ming and Shou, Linjun and Qin, Bing and Liu, Ting and Jiang, Daxin and others},
  booktitle={EMNLP Findings},
  year={2020}
}

@inproceedings{xu2021ide,
  title={{In-IDE} code generation from natural language: {Promise} and challenges},
  author={Xu, Frank F and Vasilescu, Bogdan and Neubig, Graham},
  booktitle={ACM TOSEM},
  year={2022}
}


@inproceedings{hellendoorn2019completion,
  author={Hellendoorn, Vincent J. and Proksch, Sebastian and Gall, Harald C. and Bacchelli, Alberto},
  booktitle={ICSE}, 
  title={When Code Completion Fails: {A} Case Study on Real-World Completions}, 
  year={2019},
 }

@inproceedings{pradel2020typewriter,
  title={{TypeWriter}: {Neural} type prediction with search-based validation},
  author={Pradel, Michael and Gousios, Georgios and Liu, Jason and Chandra, Satish},
  booktitle={ACM SIGSOFT},
  year={2020}
}

@inproceedings{ahmad2021unified,
  title={Unified pre-training for program understanding and generation},
  author={Ahmad, Wasi Uddin and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
  booktitle={NAACL},
  year={2021}
}

@inproceedings{lu2021codexglue,
  title={{CodeXGlue}: A machine learning benchmark dataset for code understanding and generation},
  author={Lu, Shuai and Guo, Daya and Ren, Shuo and Huang, Junjie and Svyatkovskiy, Alexey and Blanco, Ambrosio and Clement, Colin and Drain, Dawn and Jiang, Daxin and Tang, Duyu and others},
  booktitle={NeurIPS},
  year={2021}
}


@article{li2022competition,
  title={Competition-Level Code Generation with {AlphaCode}},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Lago, Agustin Dal and others},
  journal={arXiv preprint arXiv:2203.07814},
  year={2022}
}

@inproceedings{wang2021codet5,
  title={{CodeT5}: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  booktitle={EMNLP},
  year={2021}
}

@inproceedings{izadi2022codefill,
  title={{CodeFill}: Multi-token Code Completion by Jointly Learning from Structure and Naming Sequences},
  author={Izadi, Maliheh and Gismondi, Roberta and Gousios, Georgios},
  booktitle={ICSE},
  year={2022}
}

@article{xu2022systematic,
  title={A Systematic Evaluation of Large Language Models of Code},
  author={Xu, Frank F and Alon, Uri and Neubig, Graham and Hellendoorn, Vincent J},
  journal={arXiv preprint arXiv:2202.13169},
  year={2022}
}


@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{chan2019kermit,
  title={{KERMIT}: Generative insertion-based modeling for sequences},
  author={Chan, William and Kitaev, Nikita and Guu, Kelvin and Stern, Mitchell and Uszkoreit, Jakob},
  journal={arXiv preprint arXiv:1906.01604},
  year={2019}
}

@inproceedings{Fabbri2021ImprovingZA,
  title={Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation},
  author={A. R. Fabbri and Simeng Han and Haoyuan Li and Haoran Li and Marjan Ghazvininejad and Shafiq R. Joty and Dragomir Radev and Yashar Mehdad},
  booktitle={NAACL},
  year={2021}
}
@article{adapet,
  title={Improving and Simplifying Pattern Exploiting Training},
  author={Derek Tam and R. R. Menon and M. Bansal and Shashank Srivastava and Colin Raffel},
  journal={ArXiv},
  year={2021},
  volume={abs/2103.11955}
}
@misc{unifiedqa,
      title={UnifiedQA: Crossing Format Boundaries With a Single QA System}, 
      author={Daniel Khashabi and Sewon Min and Tushar Khot and Ashish Sabharwal and Oyvind Tafjord and Peter Clark and Hannaneh Hajishirzi},
      year={2020},
      eprint={2005.00700},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{fastfood,
  title={Fastfood-approximating kernel expansions in loglinear time},
  author={Le, Quoc and Sarl{\'o}s, Tam{\'a}s and Smola, Alex},
  booktitle={Proceedings of the international conference on machine learning},
  volume={85},
  year={2013}
}
@article{ROBERTA,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{BART,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}
@misc{MARGE,
    title={Pre-training via Paraphrasing},
    author={Mike Lewis and Marjan Ghazvininejad and Gargi Ghosh and Armen Aghajanyan and Sida Wang and Luke Zettlemoyer},
    year={2020},
    eprint={2006.15020},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{revisiting_bert,
  title={Revisiting Few-sample BERT Fine-tuning},
  author={Zhang, Tianyi and Wu, Felix and Katiyar, Arzoo and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:2006.05987},
  year={2020}
}
@inproceedings{SMART,
  title={{SMART}: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization},
  author={Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Zhao, Tuo},
  booktitle={ACL},
  year={2020}
}
@article{what_does_bert_look_at,
  title={What does bert look at? an analysis of bert's attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal={arXiv preprint arXiv:1906.04341},
  year={2019}
}
@incollection{catastrophic_interference,
  title={Catastrophic interference in connectionist networks: The sequential learning problem},
  author={McCloskey, Michael and Cohen, Neal J},
  booktitle={Psychology of learning and motivation},
  volume={24},
  pages={109--165},
  year={1989},
  publisher={Elsevier}
}
@article{mut_info_max,
  title={On mutual information maximization for representation learning},
  author={Tschannen, Michael and Djolonga, Josip and Rubenstein, Paul K and Gelly, Sylvain and Lucic, Mario},
  journal={arXiv preprint arXiv:1907.13625},
  year={2019}
}
@inproceedings{trust_region,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015}
}
@article{revisiting_natural_gradient,
  title={Revisiting natural gradient for deep networks},
  author={Pascanu, Razvan and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1301.3584},
  year={2013}
}
@article{fine_tuning_data_order_dodge,
  title={Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping},
  author={Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah},
  journal={arXiv preprint arXiv:2002.06305},
  year={2020}
}
@inproceedings{freelb,
  title={Freelb: Enhanced adversarial training for natural language understanding},
  author={Zhu, Chen and Cheng, Yu and Gan, Zhe and Sun, Siqi and Goldstein, Tom and Liu, Jingjing},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{GPT2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@inproceedings{GLUE,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}
@article{spectral_normalization,
  title={Spectral normalization for generative adversarial networks},
  author={Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  journal={arXiv preprint arXiv:1802.05957},
  year={2018}
}
@inproceedings{cnndailymail,
  title={Teaching machines to read and comprehend},
  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  booktitle={Advances in neural information processing systems},
  pages={1693--1701},
  year={2015}
}
@inproceedings{gigaword,
  title={Annotated gigaword},
  author={Napoles, Courtney and Gormley, Matthew R and Van Durme, Benjamin},
  booktitle={Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX)},
  pages={95--100},
  year={2012}
}
@article{reddittifu,
  title={Abstractive summarization of reddit posts with multi-level memory networks},
  author={Kim, Byeongchang and Kim, Hyunwoo and Kim, Gunhee},
  journal={arXiv preprint arXiv:1811.00783},
  year={2018}
}
@article{pegasus,
  title={Pegasus: Pre-training with extracted gap-sentences for abstractive summarization},
  author={Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J},
  journal={arXiv preprint arXiv:1912.08777},
  year={2019}
}
@article{xnli,
  title={XNLI: Evaluating cross-lingual sentence representations},
  author={Conneau, Alexis and Lample, Guillaume and Rinott, Ruty and Williams, Adina and Bowman, Samuel R and Schwenk, Holger and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1809.05053},
  year={2018}
}
@article{mirror_descent_natural_gradient,
  title={The information geometry of mirror descent},
  author={Raskutti, Garvesh and Mukherjee, Sayan},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={3},
  pages={1451--1457},
  year={2015},
  publisher={IEEE}
}
@InProceedings{mnli,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}
@inproceedings{sst2,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}
@inproceedings{mrpc,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, William B and Brockett, Chris},
  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},
  year={2005}
}
@misc{qqp,
  author = {Iyer, Shankar and Dandekar, Nikhil and Csernai, Kornel},
  title = {First Quora Dataset Release: Question Pairs},
  year = 2017,
  url = {https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs},
  urldate = {2019-04-03}
}
@article{stsb,
  title={Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation},
  author={Cer, Daniel and Diab, Mona and Agirre, Eneko and Lopez-Gazpio, Inigo and Specia, Lucia},
  journal={arXiv preprint arXiv:1708.00055},
  year={2017}
}
@article{qnli,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}
@inproceedings{rte,
  title={The Fifth PASCAL Recognizing Textual Entailment Challenge.},
  author={Bentivogli, Luisa and Clark, Peter and Dagan, Ido and Giampiccolo, Danilo},
  booktitle={TAC},
  year={2009}
}
@article{cola,
  title={Neural Network Acceptability Judgments},
  author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1805.12471},
  year={2018}
}
@misc{infoxlm,
    title={InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training},
    author={Zewen Chi and Li Dong and Furu Wei and Nan Yang and Saksham Singhal and Wenhui Wang and Xia Song and Xian-Ling Mao and Heyan Huang and Ming Zhou},
    year={2020},
    eprint={2007.07834},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{anli,
  title={Adversarial nli: A new benchmark for natural language understanding},
  author={Nie, Yixin and Williams, Adina and Dinan, Emily and Bansal, Mohit and Weston, Jason and Kiela, Douwe},
  journal={arXiv preprint arXiv:1910.14599},
  year={2019}
}
@article{snli,
  title={A large annotated corpus for learning natural language inference},
  author={Bowman, Samuel R and Angeli, Gabor and Potts, Christopher and Manning, Christopher D},
  journal={arXiv preprint arXiv:1508.05326},
  year={2015}
}
@article{prophetnet,
  title={Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training},
  author={Yan, Yu and Qi, Weizhen and Gong, Yeyun and Liu, Dayiheng and Duan, Nan and Chen, Jiusheng and Zhang, Ruofei and Zhou, Ming},
  journal={arXiv preprint arXiv:2001.04063},
  year={2020}
}
@inproceedings{uniform_convergence,
  title={Uniform convergence may be unable to explain generalization in deep learning},
  author={Nagarajan, Vaishnavh and Kolter, J Zico},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11615--11626},
  year={2019}
}
@article{layer_wise_learning_rate_finetuning,
  title={Layer-wise Pruning and Auto-tuning of Layer-wise Learning Rates in Fine-tuning of Deep Networks},
  author={Ro, Youngmin and Choi, Jin Young},
  journal={arXiv preprint arXiv:2002.06048},
  year={2020}
}
@inproceedings{BERT,
  title={{BERT}: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL},
  year={2019}
}
@article{erniegen,
  title={ERNIE-GEN: An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation},
  author={Xiao, Dongling and Zhang, Han and Li, Yukun and Sun, Yu and Tian, Hao and Wu, Hua and Wang, Haifeng},
  journal={arXiv preprint arXiv:2001.11314},
  year={2020}
}
@article{stability_bert,
  title={On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines},
  author={Mosbach, Marius and Andriushchenko, Maksym and Klakow, Dietrich},
  journal={arXiv preprint arXiv:2006.04884},
  year={2020}
}
@inproceedings{trpo,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015}
}
@article{bert_layers,
  title={What's so special about BERT's layers? A closer look at the NLP pipeline in monolingual and multilingual models},
  author={de Vries, Wietse and van Cranenburgh, Andreas and Nissim, Malvina},
  journal={arXiv preprint arXiv:2004.06499},
  year={2020}
}
@article{huggingface,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={ArXiv},
  pages={arXiv--1910},
  year={2019}
}
@article{RXF,
  title={Better fine-tuning by reducing representational collapse},
  author={Aghajanyan, Armen and Shrivastava, Akshat and Gupta, Anchit and Goyal, Naman and Zettlemoyer, Luke and Gupta, Sonal},
  journal={arXiv preprint arXiv:2008.03156},
  year={2020}
}
@article{min_desc_length,
  title={Autoencoders, minimum description length and Helmholtz free energy},
  author={Hinton, Geoffrey E and Zemel, Richard},
  journal={Advances in neural information processing systems},
  volume={6},
  pages={3--10},
  year={1993}
}
@article{yelp_polarity,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.01626},
  primaryClass = {cs},
  title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},
  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
  journal = {arXiv:1509.01626 [cs]},
  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  month = sep,
  year = {2015},
}
@article{bert_lottery_ticket,
  title={The lottery ticket hypothesis for pre-trained bert networks},
  author={Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@article{bert_lottery_all_winners,
  title={When BERT Plays the Lottery, All Tickets Are Winning},
  author={Prasanna, Sai and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2005.00561},
  year={2020}
}
@article{compression_generalization_gap,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  journal={arXiv preprint arXiv:1802.05296},
  year={2018}
}
@inproceedings{XLNET,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={5753--5763},
  year={2019}
}
@article{ALBERT,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@inproceedings{stern2019insertion,
  title={Insertion transformer: Flexible sequence generation via insertion operations},
  author={Stern, Mitchell and Chan, William and Kiros, Jamie and Uszkoreit, Jakob},
  booktitle={ICML},
  year={2019},
}

@article{XLMR,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}
@article{ELECTRA,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}

@inproceedings{west2020reflective,
  title={Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models},
  author={West, Peter and Lu, Ximing and Holtzman, Ari and Bhagavatula, Chandra and Hwang, Jena and Choi, Yejin},
  booktitle={ACL},
  year={2021}
}

@inproceedings{T5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  booktitle={JMLR},
  year={2020}
}
@article{adapter_network,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  journal={arXiv preprint arXiv:1902.00751},
  year={2019}
}
@article{hongyuan_lotter_ticket,
  title={Evaluating lottery tickets under distributional shifts},
  author={Desai, Shrey and Zhan, Hongyuan and Aly, Ahmed},
  journal={arXiv preprint arXiv:1910.12708},
  year={2019}
}
@article{pc_grad,
  title={Gradient surgery for multi-task learning},
  author={Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  journal={arXiv preprint arXiv:2001.06782},
  year={2020}
}
@article{MT_DNN,
  title={Multi-task deep neural networks for natural language understanding},
  author={Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv preprint arXiv:1901.11504},
  year={2019}
}
@inproceedings{gradnorm_mtl,
  title={Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks},
  author={Chen, Zhao and Badrinarayanan, Vijay and Lee, Chen-Yu and Rabinovich, Andrew},
  booktitle={International Conference on Machine Learning},
  pages={794--803},
  year={2018},
  organization={PMLR}
}
@article{mtl_active_sampling,
  title={Learning to multi-task by active sampling},
  author={Sharma, Sahil and Jha, Ashutosh and Hegde, Parikshit and Ravindran, Balaraman},
  journal={arXiv preprint arXiv:1702.06053},
  year={2017}
}

@misc{inception_label_smoothing,
  title={Rethinking the inception architecture for computer vision. CoRR abs/1512.00567 (2015)},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  year={2015}
}

@article{bidaf,
  title={Bidirectional attention flow for machine comprehension},
  author={Seo, Minjoon and Kembhavi, Aniruddha and Farhadi, Ali and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1611.01603},
  year={2016}
}
@inproceedings{ADAM,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={ICLR},
  year={2015}
}
@article{DROPOUT,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}
@article{SQUAD,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}
@article{RECORD,
  title={Record: Bridging the gap between human and machine commonsense reading comprehension},
  author={Zhang, Sheng and Liu, Xiaodong and Liu, Jingjing and Gao, Jianfeng and Duh, Kevin and Van Durme, Benjamin},
  journal={arXiv preprint arXiv:1810.12885},
  year={2018}
}

@article{wang2019superglue,
   title={Super{GLUE}: A Stickier Benchmark for General-Purpose Language Understanding Systems},
   author={Alex Wang and Yada Pruksachatkun and Nikita Nangia and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
   journal={arXiv preprint 1905.00537},
   year={2019}
 }
 @inproceedings{clark2019boolq,
   title={{B}ool{Q}: Exploring the Surprising Difficulty of Natural Yes/No Questions},
   author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
   booktitle={Proceedings of NAACL-HLT 2019},
   year={2019}
 }
 @inproceedings{demarneffe:cb,
   title={{The CommitmentBank}: Investigating projection in naturally occurring discourse},
   author={De Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},
   note={To appear in proceedings of Sinn und Bedeutung 23. Data can be found at https://github.com/mcdm/CommitmentBank/},
   year={2019}
 }
 @inproceedings{roemmele2011choice,
   title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},
   author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S.},
   booktitle={2011 AAAI Spring Symposium Series},
   year={2011}
 }
 @inproceedings{khashabi2018looking,
   title={Looking beyond the surface: A challenge set for reading comprehension over multiple sentences},
   author={Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
   booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
   pages={252--262},
   year={2018}
 }
 @article{zhang2018record,
   title={{ReCoRD}: Bridging the Gap between Human and Machine Commonsense Reading Comprehension},
   author={Sheng Zhang and Xiaodong Liu and Jingjing Liu and Jianfeng Gao and Kevin Duh and Benjamin Van Durme},
   journal={arXiv preprint 1810.12885},
   year={2018}
 }
 @incollection{dagan2006pascal,
   title={The {PASCAL} recognising textual entailment challenge},
   author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
   booktitle={Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment},
   pages={177--190},
   year={2006},
   publisher={Springer}
 }
 @article{bar2006second,
   title={The second {PASCAL} recognising textual entailment challenge},
   author={Bar Haim, Roy and Dagan, Ido and Dolan, Bill and Ferro, Lisa and Giampiccolo, Danilo and Magnini, Bernardo and Szpektor, Idan},
   year={2006}
 }
 @inproceedings{giampiccolo2007third,
   title={The third {PASCAL} recognizing textual entailment challenge},
   author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, Bill},
   booktitle={Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},
   pages={1--9},
   year={2007},
   organization={Association for Computational Linguistics},
 }
 @article{bentivogli2009fifth,
   title={The Fifth {PASCAL} Recognizing Textual Entailment Challenge},
   author={Bentivogli, Luisa and Dagan, Ido and Dang, Hoa Trang and Giampiccolo, Danilo and Magnini, Bernardo},
   booktitle={TAC},
   year={2009}
 }
 @inproceedings{pilehvar2018wic,
   title={{WiC}: The Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations},
   author={Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
   booktitle={Proceedings of NAACL-HLT},
   year={2019}
 }
 @inproceedings{rudinger2018winogender,
   title={Gender Bias in Coreference Resolution},
   author={Rudinger, Rachel  and  Naradowsky, Jason  and  Leonard, Brian  and  {Van Durme}, Benjamin},
   booktitle={Proceedings of NAACL-HLT},
   year={2018}
 }
 @inproceedings{poliak2018dnc,
   title={Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation},
   author={Poliak, Adam and Haldar, Aparajita and Rudinger, Rachel and Hu, J. Edward and Pavlick, Ellie and White, Aaron Steven and {Van Durme}, Benjamin},
   booktitle={Proceedings of EMNLP},
   year={2018}
 }
 @inproceedings{levesque2011winograd,
   title={The {W}inograd schema challenge},
   author={Levesque, Hector J and Davis, Ernest and Morgenstern, Leora},
   booktitle={{AAAI} Spring Symposium: Logical Formalizations of Commonsense Reasoning},
   volume={46},
   pages={47},
   year={2011}
 }

@inproceedings{scitail,
     Author = {Tushar Khot and Ashish Sabharwal and Peter Clark},
     Booktitle = {AAAI},
     Title = {{SciTail}: A Textual Entailment Dataset from Science Question Answering},
     Year = {2018}
}

@inproceedings{li-roth-2002-learning,
    title = "Learning Question Classifiers",
    author = "Li, Xin  and
      Roth, Dan",
    booktitle = "{COLING} 2002: The 19th International Conference on Computational Linguistics",
    year = "2002",
    url = "https://www.aclweb.org/anthology/C02-1150",
}
@inproceedings{hovy-etal-2001-toward,
    title = "Toward Semantics-Based Answer Pinpointing",
    author = "Hovy, Eduard  and
      Gerber, Laurie  and
      Hermjakob, Ulf  and
      Lin, Chin-Yew  and
      Ravichandran, Deepak",
    booktitle = "Proceedings of the First International Conference on Human Language Technology Research",
    year = "2001",
    url = "https://www.aclweb.org/anthology/H01-1069",
}

@inproceedings{EMOTION,
    title = "{CARER}: Contextualized Affect Representations for Emotion Recognition",
    author = "Saravia, Elvis  and
      Liu, Hsien-Chi Toby  and
      Huang, Yen-Hao  and
      Wu, Junlin  and
      Chen, Yi-Shin",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1404",
    doi = "10.18653/v1/D18-1404",
    pages = "3687--3697",
    abstract = "Emotions are expressed in nuanced ways, which varies by collective or individual experiences, knowledge, and beliefs. Therefore, to understand emotion, as conveyed through text, a robust mechanism capable of capturing and modeling different linguistic nuances and phenomena is needed. We propose a semi-supervised, graph-based algorithm to produce rich structural descriptors which serve as the building blocks for constructing contextualized affect representations from text. The pattern-based representations are further enriched with word embeddings and evaluated through several emotion recognition tasks. Our experimental results demonstrate that the proposed method outperforms state-of-the-art techniques on emotion recognition tasks.",
}

@InProceedings{RottenTomatoes,
  author =       {Bo Pang and Lillian Lee},
  title =        {Seeing stars: Exploiting class relationships for sentiment
                  categorization with respect to rating scales},
  booktitle =    {Proceedings of the ACL},
  year =         2005
}
@inproceedings{snli:emnlp2015,
	Author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher, and Manning, Christopher D.},
	Booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	Publisher = {Association for Computational Linguistics},
	Title = {A large annotated corpus for learning natural language inference},
	Year = {2015}
}

@article{HANS,
  author    = {R. Thomas McCoy and
               Ellie Pavlick and
               Tal Linzen},
  title     = {Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural
               Language Inference},
  journal   = {CoRR},
  volume    = {abs/1902.01007},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.01007},
  archivePrefix = {arXiv},
  eprint    = {1902.01007},
  timestamp = {Tue, 21 May 2019 18:03:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-01007.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{MultiNLI,
  author = {Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel},
  title = {A Broad-Coverage Challenge Corpus for
           Sentence Understanding through Inference},
  booktitle = {Proceedings of the 2018 Conference of
               the North American Chapter of the
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)},
  year = {2018},
  publisher = {Association for Computational Linguistics},
  pages = {1112--1122},
  location = {New Orleans, Louisiana},
  url = {http://aclweb.org/anthology/N18-1101}
  }

@InProceedings{IMDB,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}

@inproceedings{Zhang2015CharacterlevelCN,
  title={Character-level Convolutional Networks for Text Classification},
  author={Xiang Zhang and Junbo Jake Zhao and Yann LeCun},
  booktitle={NIPS},
  year={2015}
}
@article{race,
  title={Race: Large-scale reading comprehension dataset from examinations},
  author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
  journal={arXiv preprint arXiv:1704.04683},
  year={2017}
}
@article{hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}
@inproceedings{winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning},
  year={2012},
  organization={Citeseer}
}
@article{swag,
  title={Swag: A large-scale adversarial dataset for grounded commonsense inference},
  author={Zellers, Rowan and Bisk, Yonatan and Schwartz, Roy and Choi, Yejin},
  journal={arXiv preprint arXiv:1808.05326},
  year={2018}
}

@inproceedings{Assael2019RestoringAT,
  title={Restoring ancient text using deep learning: a case study on {Greek} epigraphy},
  author={Yannis M. Assael and Thea Sommerschield and J. Prag},
  booktitle={EMNLP},
  year={2019}
}


@misc{allocine,




 author = {Blard, Theophile},


 title = {french-sentiment-analysis-with-bert},


 year = {2020},


 publisher = {GitHub},


 journal = {GitHub repository},


 howpublished={\\url{https://github.com/TheophileBlard/french-sentiment-analysis-with-bert}},


}

@article{Shih2019XLEditorPS,
  title={{XL-Editor}: Post-editing Sentences with {XLNet}},
  author={Yong-Siang Shih and Wei-Cheng Chang and Yiming Yang},
  journal={arXiv preprint arXiv:1910.10479},
  year={2019},
}

@inproceedings{Yu2018SpiderAL,
  title={Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-{SQL} Task},
  author={Tao Yu and Rui Zhang and Kai-Chou Yang and Michihiro Yasunaga and Dongxu Wang and Zifan Li and James Ma and Irene Z Li and Qingning Yao and Shanelle Roman and Zilin Zhang and Dragomir R. Radev},
  booktitle={EMNLP},
  year={2018}
}


@article{bavishi2019autopandas,
  title={{AutoPandas}: neural-backed generators for program synthesis},
  author={Bavishi, Rohan and Lemieux, Caroline and Fox, Roy and Sen, Koushik and Stoica, Ion},
  journal={PACMPL},
  year={2019},
}

@inproceedings{hendrycks2021measuring,
  title={Measuring coding challenge competence with {APPS}},
  author={Hendrycks, Dan and Basart, Steven and Kadavath, Saurav and Mazeika, Mantas and Arora, Akul and Guo, Ethan and Burns, Collin and Puranik, Samir and He, Horace and Song, Dawn and Steinhardt, Jacob},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{Yasunaga2021BreakItFixItUL,
  title={Break-It-Fix-It: Unsupervised Learning for Program Repair},
  author={Michihiro Yasunaga and Percy Liang},
  booktitle={ICML},
  year={2021}
}

@article{Chen2021SequenceRSL,
  title={{SequenceR}: Sequence-to-Sequence Learning for End-to-End Program Repair},
  author={Zimin Chen and Steve Kommrusch and Michele Tufano and Louis-No{\"e}l Pouchet and Denys Poshyvanyk and Monperrus Martin},
  journal={IEEE Transactions on Software Engineering},
  year={2021},
}

@inproceedings{Ryan2020CLN2INVLL,
  title={{CLN2INV}: Learning Loop Invariants with Continuous Logic Networks},
  author={Gabriel Ryan and Justin Wong and Jianan Yao and Ronghui Gu and Suman Sekhar Jana},
  booktitle={ICLR},
  year={2020},
}



@InProceedings{WikiQA,
       author = {{Yi}, Yang and {Wen-tau},  Yih and {Christopher} Meek},
        title = "{WikiQA: A Challenge Dataset for Open-Domain Question Answering}",
      journal = {Association for Computational Linguistics},
         year = 2015,
          doi = {10.18653/v1/D15-1237},
        pages = {2013–2018},
}

@unpublished{eraser,
    title = {ERASER: A Benchmark to Evaluate Rationalized NLP Models},
    author = {Jay DeYoung and Sarthak Jain and Nazneen Fatema Rajani and Eric Lehman and Caiming Xiong and Richard Socher and Byron C. Wallace}
}
@inproceedings{MultiRC2018,
    author = {Daniel Khashabi and Snigdha Chaturvedi and Michael Roth and Shyam Upadhyay and Dan Roth},
    title = {Looking Beyond the Surface:A Challenge Set for Reading Comprehension over Multiple Sentences},
    booktitle = {NAACL},
    year = {2018}
}

@inproceedings{yin2018learning,
  title={Learning to mine aligned code and natural language pairs from stack overflow},
  author={Yin, Pengcheng and Deng, Bowen and Chen, Edgar and Vasilescu, Bogdan and Neubig, Graham},
  booktitle={ACM MSR},
  year={2018},
}

@inproceedings{kulal2019spoc,
  title={{SPoC}: Search-based pseudocode to code},
  author={Kulal, Sumith and Pasupat, Panupong and Chandra, Kartik and Lee, Mina and Padon, Oded and Aiken, Alex and Liang, Percy S},
  booktitle={NeurIPS},
  year={2019}
}

@article{YelpPolarity,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.01626},
  primaryClass = {cs},
  title = {Character-Level {{Convolutional Networks}} for {{Text Classification}}},
  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
  journal = {arXiv:1509.01626 [cs]},
  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  month = sep,
  year = {2015},
}

@inproceedings{evosuite, 
    author = {Fraser, Gordon and Arcuri, Andrea}, 
    title = {{EvoSuite}: Automatic Test Suite Generation for Object-Oriented Software}, 
    year = {2011}, 
    booktitle = {ACM SIGSOFT}
}

@inproceedings{zelle1996learning,
  title={Learning to parse database queries using inductive logic programming},
  author={Zelle, John M and Mooney, Raymond J},
  booktitle={AAAI},
  year={1996}
}

@inproceedings{chen2021spreadsheetcoder,
  title={{SpreadsheetCoder}: Formula prediction from semi-structured context},
  author={Chen, Xinyun and Maniatis, Petros and Singh, Rishabh and Sutton, Charles and Dai, Hanjun and Lin, Max and Zhou, Denny},
  booktitle={ICML},
  year={2021},
}

@inproceedings{wikiqa,
  title={Wikiqa: A challenge dataset for open-domain question answering},
  author={Yang, Yi and Yih, Wen-tau and Meek, Christopher},
  booktitle={Proceedings of the 2015 conference on empirical methods in natural language processing},
  pages={2013--2018},
  year={2015}
}

@inproceedings{yasunaga2020graph,
  title={Graph-based, self-supervised program repair from diagnostic feedback},
  author={Yasunaga, Michihiro and Liang, Percy},
  booktitle={ICML},
  year={2020},
  organization={PMLR}
}

@inproceedings{gupta2017deepfix,
  title={{DeepFix}: Fixing common {C} language errors by deep learning},
  author={Gupta, Rahul and Pal, Soham and Kanade, Aditya and Shevade, Shirish},
  booktitle={AAAI},
  year={2017}
}


@article{TriviaQA,
       author = {{Joshi}, Mandar and {Choi}, Eunsol and {Weld},
                 Daniel and {Zettlemoyer}, Luke},
        title = "{triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension}",
      journal = {arXiv e-prints},
         year = 2017,
          eid = {arXiv:1705.03551},
        pages = {arXiv:1705.03551},
archivePrefix = {arXiv},
       eprint = {1705.03551},
}
@article{NaturalQuestions,
title	= {Natural Questions: a Benchmark for Question Answering Research},
author	= {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Matthew Kelcey and Jacob Devlin and Kenton Lee and Kristina N. Toutanova and Llion Jones and Ming-Wei Chang and Andrew Dai and Jakob Uszkoreit and Quoc Le and Slav Petrov},
year	= {2019},
journal	= {Transactions of the Association of Computational Linguistics}
}

@inproceedings{chen2018execution,
  title={Execution-guided neural program synthesis},
  author={Chen, Xinyun and Liu, Chang and Song, Dawn},
  booktitle={ICLR},
  year={2018}
}

@inproceedings{Balog2017DeepCoderLT,
  title={{DeepCoder}: Learning to Write Programs},
  author={Matej Balog and Alexander L. Gaunt and Marc Brockschmidt and Sebastian Nowozin and Daniel Tarlow},
  booktitle={ICLR},
  year={2017}
}

@article{socialqa,
  title={Social QA in non-CQA platforms},
  author={Herrera, Jos{\'e} and Parra, Denis and Poblete, Barbara},
  journal={Future Generation Computer Systems},
  volume={105},
  pages={631--649},
  year={2020},
  publisher={Elsevier}
}
@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@inproceedings{Zhong2021AdaptingLM,
  title={Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections},
  author={Ruiqi Zhong and Kristy Lee and Zheng Zhang and Dan Klein},
  booktitle={EMNLP},
  year={2021}
}

@article{Ouyang2022TrainingLM,
  title={Training language models to follow instructions with human feedback},
  author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Francis Christiano and Jan Leike and Ryan J. Lowe},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022},
}

@inproceedings{Wei2021FinetunedLM,
  title={Finetuned Language Models Are Zero-Shot Learners},
  author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
  booktitle={ICLR},
  year={2022},
}

@article{mathqa,
  title={Mathqa: Towards interpretable math word problem solving with operation-based formalisms},
  author={Amini, Aida and Gabriel, Saadia and Lin, Peter and Koncel-Kedziorski, Rik and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1905.13319},
  year={2019}
}

@inproceedings{DROP,
  author={Dheeru Dua and Yizhong Wang and Pradeep Dasigi and Gabriel Stanovsky and Sameer Singh and Matt Gardner},
  title={DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},
  booktitle={Proc. of NAACL},
  year={2019}
}
@article{openbookqa,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}


@inproceedings{HotpotQA,
  title={{HotpotQA}: A Dataset for Diverse, Explainable Multi-hop Question Answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W. and Salakhutdinov, Ruslan and Manning, Christopher D.},
  booktitle={Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  year={2018}
}
@article{multinews,
  title={Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model},
  author={Fabbri, Alexander R and Li, Irene and She, Tianwei and Li, Suyi and Radev, Dragomir R},
  journal={arXiv preprint arXiv:1906.01749},
  year={2019}
}
@article{sciq,
  title={Crowdsourcing multiple choice science questions},
  author={Welbl, Johannes and Liu, Nelson F and Gardner, Matt},
  journal={arXiv preprint arXiv:1707.06209},
  year={2017}
}

@inproceedings{kanade2020learning,
  title={Learning and evaluating contextual embedding of source code},
  author={Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
  booktitle={ICML},
  year={2020},
}

@article{arc,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}
@article{Austin2021ProgramSW,
  title={Program Synthesis with Large Language Models},
  author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie J. Cai and Michael Terry and Quoc V. Le and Charles Sutton},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{cosmosqa,
  title={Cosmos qa: Machine reading comprehension with contextual commonsense reasoning},
  author={Huang, Lifu and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={arXiv preprint arXiv:1909.00277},
  year={2019}
}
@article{commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}
@article{xsum,
  title={Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1808.08745},
  year={2018}
}
@inproceedings{billsum,
  title={Billsum: A corpus for automatic summarization of us legislation},
  author={Eidelman, Vladimir},
  booktitle={Proceedings of the 2nd Workshop on New Frontiers in Summarization},
  pages={48--56},
  year={2019}
}
@article{Pei2020TrexLE,
  title={Trex: Learning Execution Semantics from Micro-Traces for Binary Similarity},
  author={Kexin Pei and Zhou Xuan and Junfeng Yang and Suman Sekhar Jana and Baishakhi Ray},
  journal={ArXiv},
  year={2020},
  volume={abs/2012.08680}
}

@InProceedings{aelsc,
  author =      "Rui Zhang and Joel Tetreault",
  title =       "This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation",
  booktitle =   "Proceedings of The 57th Annual Meeting of the Association for Computational Linguistics",
  year =        "2019",
  address =     "Florence, Italy"
}

@article{ALUM,
  title={Adversarial Training for Large Neural Language Models},
  author={X. Liu and Hao Cheng and Pengcheng He and W. Chen and Yu Wang and Hoifung Poon and Jianfeng Gao},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.08994}
}

@inproceedings{Schneider2019wav2vecUP,
  title={wav2vec: Unsupervised Pre-training for Speech Recognition},
  author={Steffen Schneider and Alexei Baevski and Ronan Collobert and Michael Auli},
  booktitle={INTERSPEECH},
  year={2019}
}

@inproceedings{
infobert,
title={Info{\{}BERT{\}}: Improving Robustness of Language Models from An Information Theoretic Perspective},
author={Boxin Wang and Shuohang Wang and Yu Cheng and Zhe Gan and Ruoxi Jia and Bo Li and Jingjing Liu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=hpH98mK5Puk}
}
@inproceedings{hyperpartisan,
  title={Semeval-2019 task 4: Hyperpartisan news detection},
  author={Kiesel, Johannes and Mestre, Maria and Shukla, Rishabh and Vincent, Emmanuel and Adineh, Payam and Corney, David and Stein, Benno and Potthast, Martin},
  booktitle={Proceedings of the 13th International Workshop on Semantic Evaluation},
  pages={829--839},
  year={2019}
}

@inproceedings{Mikolov2013EfficientEO,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Tomas Mikolov and Kai Chen and Gregory S. Corrado and Jeffrey Dean},
  booktitle={ICLR},
  year={2013}
}

@article{Elnaggar2021ProtTransTC,
  title={{ProtTrans}: Towards Cracking the Language of Lifes Code Through Self-Supervised Deep Learning and High Performance Computing},
  author={Ahmed Elnaggar and Michael Heinzinger and Christian Dallago and Ghalia Rehawi and Wang Yu and Llion Jones and Tom Gibbs and Tamas B. Feh{\'e}r and Christoph Angerer and Martin Steinegger and Debsindhu Bhowmik and Burkhard Rost},
  journal={TPAMI},
  year={2021},
}

@article{arxiv,
  title={Long document classification from local word glimpses via recurrent attention learning},
  author={He, Jun and Wang, Liqun and Liu, Liu and Feng, Jiao and Wu, Hao},
  journal={IEEE Access},
  volume={7},
  pages={40707--40718},
  year={2019},
  publisher={IEEE}
}
@article{bigpatent,
  title={Bigpatent: A large-scale dataset for abstractive and coherent summarization},
  author={Sharma, Eva and Li, Chen and Wang, Lu},
  journal={arXiv preprint arXiv:1906.03741},
  year={2019}
}
@article{pubmed,
  title={A discourse-aware attention model for abstractive summarization of long documents},
  author={Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
  journal={arXiv preprint arXiv:1804.05685},
  year={2018}
}
@article{penn,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  year={1993}
}
@inproceedings{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={NeurIPS},
  year={2020}
}

@article{itsnotjustsize,
  title={It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2009.07118},
  year={2020}
}
@inproceedings{pet,
    title = "Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference",
    author = {Schick, Timo  and
      Sch{\"u}tze, Hinrich},
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.eacl-main.20",
    pages = "255--269",
    abstract = "Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with {``}task descriptions{''} in natural language (e.g., Radford et al., 2019). While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases to help language models understand a given task. These phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, standard supervised training is performed on the resulting training set. For several tasks and languages, PET outperforms supervised training and strong semi-supervised approaches in low-resource settings by a large margin.",
}

@article{prefixtuning,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}
@article{intrinsic_dimensionality_finetuning,
  title={Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},
  author={Aghajanyan, Armen and Zettlemoyer, Luke and Gupta, Sonal},
  journal={arXiv preprint arXiv:2012.13255},
  year={2020}
}
@inproceedings{how_many_datapoints,
    title = "How many data points is a prompt worth?",
    author = "Le Scao, Teven  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.208",
    doi = "10.18653/v1/2021.naacl-main.208",
    pages = "2627--2636",
    abstract = "When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting task-specific guidance, which is beneficial in low-data regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks.",
}
@inproceedings{rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}
@inproceedings{etc_google,
  title={ETC: Encoding Long and Structured Inputs in Transformers},
  author={Ainslie, Joshua and Ontanon, Santiago and Alberti, Chris and Cvicek, Vaclav and Fisher, Zachary and Pham, Philip and Ravula, Anirudh and Sanghai, Sumit and Wang, Qifan and Yang, Li},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={268--284},
  year={2020}
}
@article{true_few_shot_learning,
  title={True Few-Shot Learning with Language Models},
  author={Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:2105.11447},
  year={2021}
}
@article{performer,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}
@article{linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}
@article{longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}
@article{improving_few_shot_fb,
  title={Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation},
  author={Fabbri, Alexander R and Han, Simeng and Li, Haoyuan and Li, Haoran and Ghazvininejad, Marjan and Joty, Shafiq and Radev, Dragomir and Mehdad, Yashar},
  journal={arXiv preprint arXiv:2010.12836},
  year={2020}
}
@article{fasttext,
  title={Fasttext. zip: Compressing text classification models},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Douze, Matthijs and J{\'e}gou, H{\'e}rve and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1612.03651},
  year={2016}
}
@article{e2e_nlg,
  title={The E2E dataset: New challenges for end-to-end generation},
  author={Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Rieser, Verena},
  journal={arXiv preprint arXiv:1706.09254},
  year={2017}
}
@inproceedings{webnlg,
  title={The WebNLG challenge: Generating text from RDF data},
  author={Gardent, Claire and Shimorina, Anastasia and Narayan, Shashi and Perez-Beltrachini, Laura},
  booktitle={Proceedings of the 10th International Conference on Natural Language Generation},
  pages={124--133},
  year={2017}
}
@article{dartnlg,
  title={Dart: Open-domain structured data record to text generation},
  author={Nan, Linyong and Radev, Dragomir and Zhang, Rui and Rau, Amrit and Sivaprasad, Abhinand and Hsieh, Chiachun and Tang, Xiangru and Vyas, Aadit and Verma, Neha and Krishna, Pranav and others},
  journal={arXiv preprint arXiv:2007.02871},
  year={2020}
}
@inproceedings{bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}
@inproceedings{nist,
  title={Comparing automatic and human evaluation of NLG systems},
  author={Belz, Anja and Reiter, Ehud},
  booktitle={11th conference of the european chapter of the association for computational linguistics},
  year={2006}
}
@inproceedings{meteor,
  title={METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments},
  author={Lavie, Alon and Agarwal, Abhaya},
  booktitle={Proceedings of the second workshop on statistical machine translation},
  pages={228--231},
  year={2007}
}
@inproceedings{cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}
@inproceedings{ter,
  title={A study of translation error rate with targeted human annotation},
  author={Snover, Mathew and Dorr, Bonnie and Schwartz, Richard and Makhoul, John and Micciulla, Linnea and Weischedel, Ralph},
  booktitle={Proceedings of the 7th Conference of the Association for Machine Translation in the Americas (AMTA 06)},
  pages={223--231},
  year={2005}
}
@article{laion_critic,
  title={Multimodal datasets: misogyny, pornography, and malignant stereotypes},
  author={Birhane, Abeba and Prabhu, Vinay Uday and Kahembwe, Emmanuel},
  journal={arXiv preprint arXiv:2110.01963},
  year={2021}
}
@inproceedings{vqvae_gan,
  title={Taming transformers for high-resolution image synthesis},
  author={Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12873--12883},
  year={2021}
}

@inproceedings{pytorch,
  title={{PyTorch}: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={NeurIPS},
  year={2019}
}
@article{fairseq,
  title={{Fairseq}: A fast, extensible toolkit for sequence modeling},
  author={Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1904.01038},
  year={2019}
}
@Misc{fairscale,
  author =       {Mandeep Baines and Shruti Bhosale and Vittorio Caggiano and Naman Goyal and Siddharth Goyal and Myle Ott and Benjamin Lefaudeux and Vitaliy Liptchinsky and Mike Rabbat and Sam Sheiffer and Anjali Sridhar and Min Xu},
  title =        {{FairScale}:  A general purpose modular {PyTorch} library for high performance and large scale training},
  howpublished = {\url{https://github.com/facebookresearch/fairscale}},
  year =         {2021}
}
@article{language_zipf,
  title={Zipf’s word frequency law in natural language: A critical review and future directions},
  author={Piantadosi, Steven T},
  journal={Psychonomic bulletin \& review},
  volume={21},
  number={5},
  pages={1112--1130},
  year={2014},
  publisher={Springer}
}
@article{scaling_laws_lm,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@inproceedings{HTLM1,
  title={{HTLM}: Hyper-text pre-training and prompting of language models},
  author={Aghajanyan, Armen and Okhonko, Dmytro and Lewis, Mike and Joshi, Mandar and Xu, Hu and Ghosh, Gargi and Zettlemoyer, Luke},
  booktitle={ICLR},
  year={2022}
}

@article{DALLE,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2102.12092},
  year={2021}
}

@article{improving_entity_disambiguation,
  title={Improving entity linking by modeling latent relations between mentions},
  author={Le, Phong and Titov, Ivan},
  journal={arXiv preprint arXiv:1804.10637},
  year={2018}
}
@article{GENRE,
  title={Autoregressive entity retrieval},
  author={De Cao, Nicola and Izacard, Gautier and Riedel, Sebastian and Petroni, Fabio},
  journal={arXiv preprint arXiv:2010.00904},
  year={2020}
}
@article{zero_shot_entity_linking,
  title={Scalable zero-shot entity linking with dense entity retrieval},
  author={Wu, Ledell and Petroni, Fabio and Josifoski, Martin and Riedel, Sebastian and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1911.03814},
  year={2019}
}
@inproceedings{robust_entity_disambiguation,
  title={Robust disambiguation of named entities in text},
  author={Hoffart, Johannes and Yosef, Mohamed Amir and Bordino, Ilaria and F{\"u}rstenau, Hagen and Pinkal, Manfred and Spaniol, Marc and Taneva, Bilyana and Thater, Stefan and Weikum, Gerhard},
  booktitle={Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
  pages={782--792},
  year={2011}
}
@article{robust_entity_disambiguation_random_walk,
  title={Robust named entity disambiguation with random walks},
  author={Guo, Zhaochen and Barbosa, Denilson},
  journal={Semantic Web},
  volume={9},
  number={4},
  pages={459--479},
  year={2018},
  publisher={IOS Press}
}
@article{ganea2017deep,
  title={Deep joint entity disambiguation with local neural attention},
  author={Ganea, Octavian-Eugen and Hofmann, Thomas},
  journal={arXiv preprint arXiv:1704.04920},
  year={2017}
}
@article{yang2018collective,
  title={Collective entity disambiguation with structured gradient tree boosting},
  author={Yang, Yi and Irsoy, Ozan and Rahman, Kazi Shefaet},
  journal={arXiv preprint arXiv:1802.10229},
  year={2018}
}
@article{shahbazi2019entity,
  title={Entity-aware elmo: Learning contextual entity representation for entity disambiguation},
  author={Shahbazi, Hamed and Fern, Xiaoli Z and Ghaeini, Reza and Obeidat, Rasha and Tadepalli, Prasad},
  journal={arXiv preprint arXiv:1908.05762},
  year={2019}
}
@article{yang2019learning,
  title={Learning dynamic context augmentation for global entity linking},
  author={Yang, Xiyuan and Gu, Xiaotao and Lin, Sheng and Tang, Siliang and Zhuang, Yueting and Wu, Fei and Chen, Zhigang and Hu, Guoping and Ren, Xiang},
  journal={arXiv preprint arXiv:1909.02117},
  year={2019}
}
@article{le2019boosting,
  title={Boosting entity linking performance by leveraging unlabeled documents},
  author={Le, Phong and Titov, Ivan},
  journal={arXiv preprint arXiv:1906.01250},
  year={2019}
}
@inproceedings{fang2019joint,
  title={Joint entity linking with deep reinforcement learning},
  author={Fang, Zheng and Cao, Yanan and Li, Qian and Zhang, Dongjie and Zhang, Zhenyu and Liu, Yanbing},
  booktitle={The World Wide Web Conference},
  pages={438--447},
  year={2019}
}
@article{attngan,
    Author = {Tao Xu and Pengchuan Zhang and Qiuyuan Huang and Han Zhang and Zhe Gan and Xiaolei Huang and Xiaodong He},
    Title = {AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks},
    Year = {2017},
    Journal = {\href{https://arxiv.org/abs/1711.10485}{arXiv:1711.10485}},
}

@article{dmgan,
    Author = {Minfeng Zhu and Pingbo Pan and Wei Chen and Yi Yang},
    Title = {DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis},
    Year = {2019},
    Journal = {\href{https://arxiv.org/abs/1904.01310}{arXiv:1904.01310}},
}

@article{dfgan,
    Author = {Ming Tao and Hao Tang and Songsong Wu and Nicu Sebe and Xiao-Yuan Jing and Fei Wu and Bingkun Bao},
    Title = {DF-GAN: Deep Fusion Generative Adversarial Networks for Text-to-Image Synthesis},
    Year = {2020},
    Journal = {\href{https://arxiv.org/abs/2008.05865}{arXiv:2008.05865}},
}

@article{xmcgan,
    Author = {Han Zhang and Jing Yu Koh and Jason Baldridge and Honglak Lee and Yinfei Yang},
    Title = {Cross-Modal Contrastive Learning for Text-to-Image Generation},
    Year = {2021},
    Journal = {\href{https://arxiv.org/abs/2101.04702}{arXiv:2101.04702}},
}

@article{textstylegan,
    Author = {David Stap and Maurits Bleeker and Sarah Ibrahimi and Maartje ter Hoeve},
    Title = {Conditional Image Generation and Manipulation for User-Specified Content},
    Year = {2020},
    Journal = {\href{https://arxiv.org/abs/2005.04909}{arXiv:2005.04909}},
}

@article{stylegannada,
    Author = {Rinon Gal and Or Patashnik and Haggai Maron and Gal Chechik and Daniel Cohen-Or},
    Title = {StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators},
    Year = {2021},
    Journal = {\href{https://arxiv.org/abs/2108.00946}{arXiv:2108.00946}},
}

@article{diffusionclip,
    Author = {Gwanghyun Kim and Jong Chul Ye},
    Title = {DiffusionCLIP: Text-guided Image Manipulation Using Diffusion Models},
    Year = {2021},
    Journal = {\href{https://arxiv.org/abs/2110.02711}{arXiv:2110.02711}},
}

@article{lafite,
    Author = {Yufan Zhou and Ruiyi Zhang and Changyou Chen and Chunyuan Li and Chris Tensmeyer and Tong Yu and Jiuxiang Gu and Jinhui Xu and Tong Sun},
    Title = {LAFITE: Towards Language-Free Training for Text-to-Image Generation},
    Year = {2021},
    Journal = {\href{https://arxiv.org/abs/2111.13792}{arXiv:2111.13792}},
}
@article{textcl,
    Author = {Hui Ye and Xiulong Yang and Martin Takac and Rajshekhar Sunderraman and Shihao Ji},
    Title = {Improving Text-to-Image Synthesis Using Contrastive Learning},
    Year = {2021},
    Journal = {\href{https://arxiv.org/abs/2107.02423}{arXiv:2107.02423}},
}
@article{GLIDE,
  title={GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models},
  author={Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  journal={arXiv preprint arXiv:2112.10741},
  year={2021}
}
@misc{FID_implementation,
  author={Maximilian Seitzer},
  title={{pytorch-fid: FID Score for PyTorch}},
  month={August},
  year={2020},
  note={Version 0.2.1},
  howpublished={\url{https://github.com/mseitzer/pytorch-fid}},
}
@inproceedings{steinmetz2013semantic,
  title={Semantic multimedia information retrieval based on contextual descriptions},
  author={Steinmetz, Nadine and Sack, Harald},
  booktitle={Extended Semantic Web Conference},
  pages={382--396},
  year={2013},
  organization={Springer}
}
@article{moro2014entity,
  title={Entity linking meets word sense disambiguation: a unified approach},
  author={Moro, Andrea and Raganato, Alessandro and Navigli, Roberto},
  journal={Transactions of the Association for Computational Linguistics},
  volume={2},
  pages={231--244},
  year={2014},
  publisher={MIT Press}
}
@article{broscheit2020investigating,
  title={Investigating entity knowledge in BERT with simple neural end-to-end entity linking},
  author={Broscheit, Samuel},
  journal={arXiv preprint arXiv:2003.05473},
  year={2020}
}
@article{martins2019joint,
  title={Joint learning of named entity recognition and entity linking},
  author={Martins, Pedro Henrique and Marinho, Zita and Martins, Andr{\'e} FT},
  journal={arXiv preprint arXiv:1907.08243},
  year={2019}
}
@inproceedings{van2020rel,
  title={Rel: An entity linker standing on the shoulders of giants},
  author={van Hulst, Johannes M and Hasibi, Faegheh and Dercksen, Koen and Balog, Krisztian and de Vries, Arjen P},
  booktitle={Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={2197--2200},
  year={2020}
}
@article{two_time_gan,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{mscoco,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}
@article{efficient_lm_xlm_g,
  title={Efficient Large Scale Language Modeling with Mixtures of Experts},
  author={Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Lin, Xi Victoria and Du, Jingfei and Iyer, Srinivasan and Pasunuru, Ramakanth and others},
  journal={arXiv preprint arXiv:2112.10684},
  year={2021}
}
@article{Caliskan2017SemanticsDA,
  title={Semantics derived automatically from language corpora contain human-like biases},
  author={Aylin Caliskan and Joanna J. Bryson and Arvind Narayanan},
  journal={Science},
  year={2017},
  volume={356},
  pages={183 - 186}
}
@inproceedings{Tan2019AssessingSA,
  title={Assessing Social and Intersectional Biases in Contextualized Word Representations},
  author={Yi Chern Tan and L. Elisa Celis},
  booktitle={NeurIPS},
  year={2019}
}
@article{May2019OnMS,
  title={On Measuring Social Biases in Sentence Encoders},
  author={Chandler May and Alex Wang and Shikha Bordia and Samuel R. Bowman and Rachel Rudinger},
  journal={ArXiv},
  year={2019},
  volume={abs/1903.10561}
}
@article{Ross2021MeasuringSB,
  title={Measuring Social Biases in Grounded Vision and Language Embeddings},
  author={Candace Ross and Boris Katz and Andrei Barbu},
  journal={ArXiv},
  year={2021},
  volume={abs/2002.08911}
}
@article{CLIP,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021}
}
@article{kolitsas2018end,
  title={End-to-end neural entity linking},
  author={Kolitsas, Nikolaos and Ganea, Octavian-Eugen and Hofmann, Thomas},
  journal={arXiv preprint arXiv:1808.07699},
  year={2018}
}
@article{roder2018gerbil,
  title={Gerbil--benchmarking named entity recognition and linking consistently},
  author={R{\"o}der, Michael and Usbeck, Ricardo and Ngonga Ngomo, Axel-Cyrille},
  journal={Semantic Web},
  volume={9},
  number={5},
  pages={605--625},
  year={2018},
  publisher={IOS Press}
}
@inproceedings{hoffart2012kore,
  title={KORE: keyphrase overlap relatedness for entity disambiguation},
  author={Hoffart, Johannes and Seufert, Stephan and Nguyen, Dat Ba and Theobald, Martin and Weikum, Gerhard},
  booktitle={Proceedings of the 21st ACM international conference on Information and knowledge management},
  pages={545--554},
  year={2012}
}
@article{derczynski2015analysis,
  title={Analysis of named entity recognition and linking for tweets},
  author={Derczynski, Leon and Maynard, Diana and Rizzo, Giuseppe and Van Erp, Marieke and Gorrell, Genevieve and Troncy, Rapha{\"e}l and Petrak, Johann and Bontcheva, Kalina},
  journal={Information Processing \& Management},
  volume={51},
  number={2},
  pages={32--49},
  year={2015},
  publisher={Elsevier}
}
@inproceedings{nuzzolese2015open,
  title={Open knowledge extraction challenge},
  author={Nuzzolese, Andrea Giovanni and Gentile, Anna Lisa and Presutti, Valentina and Gangemi, Aldo and Garigliotti, Dar{\'\i}o and Navigli, Roberto},
  booktitle={Semantic Web Evaluation Challenges},
  pages={3--15},
  year={2015},
  organization={Springer}
}
@inproceedings{roder2014n3,
  title={N$^3$-A Collection of Datasets for Named Entity Recognition and Disambiguation in the NLP Interchange Format.},
  author={R{\"o}der, Michael and Usbeck, Ricardo and Hellmann, Sebastian and Gerber, Daniel and Both, Andreas},
  booktitle={LREC},
  pages={3529--3533},
  year={2014}
}
@article{visualbert,
  title={Visualbert: A simple and performant baseline for vision and language},
  author={Li, Liunian Harold and Yatskar, Mark and Yin, Da and Hsieh, Cho-Jui and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:1908.03557},
  year={2019}
}
@article{vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  journal={arXiv preprint arXiv:1908.02265},
  year={2019}
}
@article{bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}
@article{vq-wav2vec,
  title={vq-wav2vec: Self-supervised learning of discrete speech representations},
  author={Baevski, Alexei and Schneider, Steffen and Auli, Michael},
  journal={arXiv preprint arXiv:1910.05453},
  year={2019}
}
@article{jukebox,
  title={Jukebox: A generative model for music},
  author={Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2005.00341},
  year={2020}
}

@article{Aghajanyan2022cm,
  title={{CM3}: A Causal Masked Multimodal Model of the {Internet}},
  author={Aghajanyan, Armen and Huang, Bernie and Ross, Candace and Karpukhin, Vladimir and Xu, Hu and Goyal, Naman and Okhonko, Dmytro and Joshi, Mandar and Ghosh, Gargi and Lewis, Mike and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2201.07520},
  year={2022}
}

@inproceedings{roziere2021dobf,
  title={{DOBF}: A Deobfuscation Pre-Training Objective for Programming Languages},
  author={Roziere, Baptiste and Lachaux, Marie-Anne and Szafraniec, Marc and Lample, Guillaume},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{maskpredict,
    title = "{Mask-Predict}: Parallel Decoding of Conditional Masked Language Models",
    author = "Ghazvininejad, Marjan  and
      Levy, Omer  and
      Liu, Yinhan  and
      Zettlemoyer, Luke",
    booktitle = {EMNLP},
    year = "2019",
}

@article{bloom1970space,
  title={Space/time trade-offs in hash coding with allowable errors},
  author={Bloom, Burton H},
  journal={Communications of the ACM},
  year={1970},
}

@misc{rivest1992rfc1321,
  title={{RFC1321}: The {MD5} message-digest algorithm},
  author={Rivest, Ronald},
  year={1992},
  publisher={RFC Editor}
}

@inproceedings{holtzman2020nucleus,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  booktitle={ICLR},
  year={2020}
}

@article{gulwani2011automating,
  title={Automating string processing in spreadsheets using input-output examples},
  author={Gulwani, Sumit},
  journal={ACM SIGPLAN Notices},
  year={2011},
}

@inproceedings{solar2006combinatorial,
  title={Combinatorial sketching for finite programs},
  author={Solar-Lezama, Armando and Tancau, Liviu and Bodik, Rastislav and Seshia, Sanjit and Saraswat, Vijay},
  booktitle={ASPLOS},
  year={2006}
}

@inproceedings{allamanis2019adverse,
  title={The adverse effects of code duplication in machine learning models of code},
  author={Allamanis, Miltiadis},
  booktitle={SPLASH},
  year={2019}
}

@article{nijkamp2022conversational,
  title={A Conversational Paradigm for Program Synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={NeurIPS},
  year={2017}
}

@article{gao2020pile,
  title={The {Pile}: An 800{GB} dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{askell2021general,
  title={A General Language Assistant as a Laboratory for Alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}

@article{gpt-neox-20b,
  title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
  year={2022}
}

@book{tunstall2022natural,
  title={Natural Language Processing with Transformers},
  author={Tunstall, Lewis and von Werra, Leandro and Wolf, Thomas},
  year={2022},
  publisher={O'Reilly Media, Inc.}
}

@article{husain2019codesearchnet,
  title={{CodeSearchNet} challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}

@inproceedings{sennrich-etal-2016-neural,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "ACL",
    year = "2016",
    address = "Berlin, Germany",
    url = "https://aclanthology.org/P16-1162",
}

@techreport{Radford2019LanguageMA,
  title={Language Models are Unsupervised Multitask Learners},
  author={Alec Radford and Jeff Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
  year={2019},
  institution={OpenAI},
}

@article{chowdhery2022palm,
  title={{PaLM}: Scaling Language Modeling with Pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{keskar2019ctrl,
  title={{CTRL}: A conditional transformer language model for controllable generation},
  author={Keskar, Nitish Shirish and McCann, Bryan and Varshney, Lav R and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:1909.05858},
  year={2019}
}

@inproceedings{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{agashe-etal-2019-juice,
    title = "{J}u{IC}e: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation",
    author = "Agashe, Rajas  and
      Iyer, Srinivasan  and
      Zettlemoyer, Luke",
    booktitle = {Proceedings of EMNLP},
    year = "2019",
    url = "https://aclanthology.org/D19-1546",
}

@article{thoppilan2022lamda,
  title={{LaMDA}: Language Models for Dialog Applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@inproceedings{hellendoorn2018deep,
  title={Deep learning type inference},
  author={Hellendoorn, Vincent J and Bird, Christian and Barr, Earl T and Allamanis, Miltiadis},
  booktitle={ESEC/FSE 2018},
  year={2018}
}

@inproceedings{wei2020lambdanet,
  title={{LambdaNet}: Probabilistic type inference using graph neural networks},
  author={Wei, Jiayi and Goyal, Maruth and Durrett, Greg and Dillig, Isil},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{xu2016python,
  title={Python probabilistic type inference with natural language support},
  author={Xu, Zhaogui and Zhang, Xiangyu and Chen, Lin and Pei, Kexin and Xu, Baowen},
  booktitle={SIGSOFT},
  year={2016}
}

@inproceedings{allamanis2020typilus,
  title={Typilus: Neural Type Hints},
  author={Allamanis, Miltiadis and Barr, Earl T and Ducousso, Soline and Gao, Zheng},
  booktitle={PLDI},
  year={2020}
}

@article{mir2021type4py,
  title={Type4Py: Practical Deep Similarity Learning-Based Type Inference for Python},
  author={Mir, Amir M and Latoskinas, Evaldas and Proksch, Sebastian and Gousios, Georgios},
  journal={arXiv preprint arXiv:2101.04470},
  year={2021}
}

@inproceedings{jesse2021typebert,
author = {Jesse, Kevin and Devanbu, Premkumar T. and Ahmed, Toufique},
title = {Learning Type Annotation: Is Big Data Enough?},
year = {2021},
booktitle = {ESEC/FSE},
}

@inproceedings{alon2020structural,
  title={Structural language models of code},
  author={Alon, Uri and Sadaka, Roy and Levy, Omer and Yahav, Eran},
  booktitle={ICML},
  pages={245--256},
  year={2020},
  organization={PMLR}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@inproceedings{clement-etal-2020-pymt5,
    title = "{P}y{MT}5: multi-mode translation of natural language and Python code with transformers",
    author = "Clement, Colin  and
      Drain, Dawn  and
      Timcheck, Jonathan  and
      Svyatkovskiy, Alexey  and
      Sundaresan, Neel",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.728",
    doi = "10.18653/v1/2020.emnlp-main.728",
    pages = "9052--9065",
}

@article{bavarian2022efficient,
  title={Efficient Training of Language Models to Fill in the Middle},
  author={Bavarian, Mohammad and Jun, Heewoo and Tezak, Nikolas and Schulman, John and McLeavey, Christine and Tworek, Jerry and Chen, Mark},
  journal={arXiv preprint arXiv:2207.14255},
  year={2022}
}

@article{OPT,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}
@article{CM3,
  title={Cm3: A causal masked multimodal model of the internet},
  author={Aghajanyan, Armen and Huang, Bernie and Ross, Candace and Karpukhin, Vladimir and Xu, Hu and Goyal, Naman and Okhonko, Dmytro and Joshi, Mandar and Ghosh, Gargi and Lewis, Mike and others},
  journal={arXiv preprint arXiv:2201.07520},
  year={2022}
}
@article{INCODER,
  title={Incoder: A generative model for code infilling and synthesis},
  author={Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:2204.05999},
  year={2022}
}
@article{make_a_scene,
  title={Make-a-scene: Scene-based text-to-image generation with human priors},
  author={Gafni, Oran and Polyak, Adam and Ashual, Oron and Sheynin, Shelly and Parikh, Devi and Taigman, Yaniv},
  journal={arXiv preprint arXiv:2203.13131},
  year={2022}
}
@article{laion,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal={arXiv preprint arXiv:2210.08402},
  year={2022}
}
@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{polu2020generative,
  title={Generative language modeling for automated theorem proving},
  author={Polu, Stanislas and Sutskever, Ilya},
  journal={arXiv preprint arXiv:2009.03393},
  year={2020}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{borsos2022audiolm,
  title={Audiolm: a language modeling approach to audio generation},
  author={Borsos, Zal{\'a}n and Marinier, Rapha{\"e}l and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and Zeghidour, Neil},
  journal={arXiv preprint arXiv:2209.03143},
  year={2022}
}

@article{nguyen2022generative,
  title={Generative Spoken Dialogue Language Modeling},
  author={Nguyen, Tu Anh and Kharitonov, Eugene and Copet, Jade and Adi, Yossi and Hsu, Wei-Ning and Elkahky, Ali and Tomasello, Paden and Algayres, Robin and Sagot, Benoit and Mohamed, Abdelrahman and others},
  journal={arXiv preprint arXiv:2203.16502},
  year={2022}
}

@article{zeghidour2021soundstream,
  title={Soundstream: An end-to-end neural audio codec},
  author={Zeghidour, Neil and Luebs, Alejandro and Omran, Ahmed and Skoglund, Jan and Tagliasacchi, Marco},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={30},
  pages={495--507},
  year={2021},
  publisher={IEEE}
}

@article{hsu2021hubert,
  title={Hubert: Self-supervised speech representation learning by masked prediction of hidden units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={29},
  pages={3451--3460},
  year={2021},
  publisher={IEEE}
}

@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}

@article{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{jang2016categorical,
  title={Categorical reparameterization with gumbel-softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal={arXiv preprint arXiv:1611.01144},
  year={2016}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}

@inproceedings{kahn2020libri,
  title={Libri-light: A benchmark for asr with limited or no supervision},
  author={Kahn, Jacob and Riviere, Morgane and Zheng, Weiyi and Kharitonov, Evgeny and Xu, Qiantong and Mazar{\'e}, Pierre-Emmanuel and Karadayi, Julien and Liptchinsky, Vitaliy and Collobert, Ronan and Fuegen, Christian and others},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7669--7673},
  year={2020},
  organization={IEEE}
}

@article{lakhotia2021generative,
  title={On generative spoken language modeling from raw audio},
  author={Lakhotia, Kushal and Kharitonov, Eugene and Hsu, Wei-Ning and Adi, Yossi and Polyak, Adam and Bolte, Benjamin and Nguyen, Tu-Anh and Copet, Jade and Baevski, Alexei and Mohamed, Abdelrahman and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1336--1354},
  year={2021},
  publisher={MIT Press}
}

@article{lin2022language,
  title={Language models of protein sequences at the scale of evolution enable accurate structure prediction},
  author={Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and dos Santos Costa, Allan and Fazel-Zarandi, Maryam and Sercu, Tom and Candido, Sal and others},
  journal={bioRxiv},
  year={2022},
  publisher={Cold Spring Harbor Laboratory}
}
@article{rives2021biological,
  title={Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
  author={Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C Lawrence and Ma, Jerry and others},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={15},
  pages={e2016239118},
  year={2021},
  publisher={National Acad Sciences}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{han2021unsupervised,
  title={Unsupervised neural machine translation with generative language models only},
  author={Han, Jesse Michael and Babuschkin, Igor and Edwards, Harrison and Neelakantan, Arvind and Xu, Tao and Polu, Stanislas and Ray, Alex and Shyam, Pranav and Ramesh, Aditya and Radford, Alec and others},
  journal={arXiv preprint arXiv:2110.05448},
  year={2021}
}
@article{BARTSMILES,
  title={BARTSmiles: Generative Masked Language Models for Molecular Representations},
  author={Chilingaryan, Gayane and Tamoyan, Hovhannes and Tevosyan, Ani and Babayan, Nelly and Khondkaryan, Lusine and Hambardzumyan, Karen and Navoyan, Zaven and Khachatrian, Hrant and Aghajanyan, Armen},
  journal={arXiv preprint arXiv:2211.16349},
  year={2022}
}
@article{RA_CM3,
  title={Retrieval-Augmented Multimodal Language Modeling},
  author={Yasunaga, Michihiro and Aghajanyan, Armen and Shi, Weijia and James, Rich and Leskovec, Jure and Liang, Percy and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2211.12561},
  year={2022}
}
@article{hsu2022learning,
	author = {Hsu, Chloe and Verkuil, Robert and Liu, Jason and Lin, Zeming and Hie, Brian and Sercu, Tom and Lerer, Adam and Rives, Alexander},
	title = {Learning inverse folding from millions of predicted structures},
	year = {2022},
	doi = {10.1101/2022.04.10.487779},
	url = {https://www.biorxiv.org/content/early/2022/04/10/2022.04.10.487779},
	journal = {bioRxiv}
}
@article{makeascene,
  title={Make-a-scene: Scene-based text-to-image generation with human priors},
  author={Gafni, Oran and Polyak, Adam and Ashual, Oron and Sheynin, Shelly and Parikh, Devi and Taigman, Yaniv},
  journal={arXiv preprint arXiv:2203.13131},
  year={2022}
}
@article{codex,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@article{pratap2020mls,
  title={Mls: A large-scale multilingual dataset for speech research},
  author={Pratap, Vineel and Xu, Qiantong and Sriram, Anuroop and Synnaeve, Gabriel and Collobert, Ronan},
  journal={arXiv preprint arXiv:2012.03411},
  year={2020}
}
@article{wang2021voxpopuli,
  title={Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation},
  author={Wang, Changhan and Riviere, Morgane and Lee, Ann and Wu, Anne and Talnikar, Chaitanya and Haziza, Daniel and Williamson, Mary and Pino, Juan and Dupoux, Emmanuel},
  journal={arXiv preprint arXiv:2101.00390},
  year={2021}
}
@article{ardila2019common,
  title={Common voice: A massively-multilingual speech corpus},
  author={Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M and Weber, Gregor},
  journal={arXiv preprint arXiv:1912.06670},
  year={2019}
}
@article{lee2021direct,
  title={Direct speech-to-speech translation with discrete units},
  author={Lee, Ann and Chen, Peng-Jen and Wang, Changhan and Gu, Jiatao and Ma, Xutai and Polyak, Adam and Adi, Yossi and He, Qing and Tang, Yun and Pino, Juan and others},
  journal={arXiv preprint arXiv:2107.05604},
  year={2021}
}
@software{AIM,
author = {Arakelyan, Gor and Soghomonyan, Gevorg and {The Aim team}},
doi = {10.5281/zenodo.6536395},
license = {Apache-2.0},
month = {6},
title = {{Aim}},
url = {https://github.com/aimhubio/aim},
version = {3.9.3},
year = {2020}
}
@article{Chinchilla,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}
@inproceedings{BPE,
    title = "Neural Machine Translation of Rare Words with Subword Units",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1162",
    doi = "10.18653/v1/P16-1162",
    pages = "1715--1725",
}
@article{deep_learning_scaling_2017,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md and Ali, Mostofa and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}
@inproceedings{moe_scaling_laws,
  title={Unified scaling laws for routed language models},
  author={Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and others},
  booktitle={International Conference on Machine Learning},
  pages={4057--4086},
  year={2022},
  organization={PMLR}
}
@article{int8,
  title={Llm. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}
@inproceedings{scaling_laws_nmt,
    title = "Data and Parameter Scaling Laws for Neural Machine Translation",
    author = "Gordon, Mitchell A  and
      Duh, Kevin  and
      Kaplan, Jared",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.478",
    doi = "10.18653/v1/2021.emnlp-main.478",
    pages = "5915--5922",
    abstract = "We observe that the development cross-entropy loss of supervised neural machine translation models scales like a power law with the amount of training data and the number of non-embedding parameters in the model. We discuss some practical implications of these results, such as predicting BLEU achieved by large scale models and predicting the ROI of labeling data in low-resource language pairs.",
}
@article{scaling_laws_nmt_ghorbani,
  title={Scaling laws for neural machine translation},
  author={Ghorbani, Behrooz and Firat, Orhan and Freitag, Markus and Bapna, Ankur and Krikun, Maxim and Garcia, Xavier and Chelba, Ciprian and Cherry, Colin},
  journal={arXiv preprint arXiv:2109.07740},
  year={2021}
}
@article{image_text_scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}
@Inproceedings{speech_alexa_scaling_laws,
 author = {Jasha Droppo and Oguz Elibol},
 title = {Scaling laws for acoustic models},
 year = {2021},
 url = {https://www.amazon.science/publications/scaling-laws-for-acoustic-models},
 booktitle = {Interspeech 2021},
}
@article{conneau2019unsupervised,
  title={Unsupervised cross-lingual representation learning at scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}
@article{goyal2021larger,
  title={Larger-scale transformers for multilingual masked language modeling},
  author={Goyal, Naman and Du, Jingfei and Ott, Myle and Anantharaman, Giri and Conneau, Alexis},
  journal={arXiv preprint arXiv:2105.00572},
  year={2021}
}
@article{interference_translation,
  title={Causes and Cures for Interference in Multilingual Translation},
  author={Shaham, Uri and Elbayad, Maha and Goswami, Vedanuj and Levy, Omer and Bhosale, Shruti},
  journal={arXiv preprint arXiv:2212.07530},
  year={2022}
}
@article{CLIP_scaling_law,
  title={Reproducible scaling laws for contrastive language-image learning},
  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  journal={arXiv preprint arXiv:2212.07143},
  year={2022}
}