\begin{thebibliography}{}

\bibitem[Aljundi et~al., 2017]{aljundi2017expert}
Aljundi, R., Chakravarty, P., and Tuytelaars, T. (2017).
\newblock Expert gate: Lifelong learning with a network of experts.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3366--3375.

\bibitem[Arani et~al., 2021]{arani2022learning}
Arani, E., Sarfraz, F., and Zonooz, B. (2021).
\newblock Learning fast, learning slow: A general continual learning method
  based on complementary learning system.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Bhat et~al., 2022]{bhat}
Bhat, P.~S., Zonooz, B., and Arani, E. (2022).
\newblock Consistency is the key to further mitigating catastrophic forgetting
  in continual learning.
\newblock In {\em Conference on Lifelong Learning Agents}, pages 1195--1212.
  PMLR.

\bibitem[Bhat et~al., 2023]{bhat2023taskaware}
Bhat, P.~S., Zonooz, B., and Arani, E. (2023).
\newblock Task-aware information routing from common representation space in
  lifelong learning.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}.

\bibitem[Caccia et~al., 2020]{caccia2020online}
Caccia, L., Belilovsky, E., Caccia, M., and Pineau, J. (2020).
\newblock Online learned continual compression with adaptive quantization
  modules.
\newblock In {\em International conference on machine learning}, pages
  1240--1250. PMLR.

\bibitem[Chaudhry et~al., 2018]{chaudhry2018riemannian}
Chaudhry, A., Dokania, P.~K., Ajanthan, T., and Torr, P.~H. (2018).
\newblock Riemannian walk for incremental learning: Understanding forgetting
  and intransigence.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 532--547.

\bibitem[Chen et~al., 2021]{chen2021noise}
Chen, P., Chen, G., Ye, J., Heng, P.-A., et~al. (2021).
\newblock Noise against noise: stochastic label noise helps combat inherent
  label noise.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[d'Ascoli et~al., 2021]{DBLP:journals/corr/abs-2103-10697}
d'Ascoli, S., Touvron, H., Leavitt, M.~L., Morcos, A.~S., Biroli, G., and
  Sagun, L. (2021).
\newblock Convit: Improving vision transformers with soft convolutional
  inductive biases.
\newblock {\em CoRR}, abs/2103.10697.

\bibitem[De~Lange et~al., 2021]{de2021continual}
De~Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A.,
  Slabaugh, G., and Tuytelaars, T. (2021).
\newblock A continual learning survey: Defying forgetting in classification
  tasks.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  44(7):3366--3385.

\bibitem[Deng et~al., 2009]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009).
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee.

\bibitem[Dosovitskiy et~al., 2020]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
  (2020).
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}.

\bibitem[Douillard and Lesort, 2021]{douillardlesort2021continuum}
Douillard, A. and Lesort, T. (2021).
\newblock Continuum: Simple management of complex continual learning scenarios.

\bibitem[Douillard et~al., 2021]{douillard2021dytox}
Douillard, A., Ram{\'e}, A., Couairon, G., and Cord, M. (2021).
\newblock Dytox: Transformers for continual learning with dynamic token
  expansion.
\newblock {\em arXiv preprint arXiv:2111.11326}.

\bibitem[Ebrahimi et~al., 2021]{ebrahimi2021remembering}
Ebrahimi, S., Petryk, S., Gokul, A., Gan, W., Gonzalez, J.~E., Rohrbach, M.,
  and Darrell, T. (2021).
\newblock Remembering for the right reasons: Explanations reduce catastrophic
  forgetting.
\newblock {\em Applied AI Letters}, 2(4):e44.

\bibitem[Ermis et~al., 2022]{ermis2022continual}
Ermis, B., Zappella, G., Wistuba, M., Rawal, A., and Archambeau, C. (2022).
\newblock Continual learning with transformers for image classification.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3774--3781.

\bibitem[Faisal et~al., 2008]{faisal2008noise}
Faisal, A.~A., Selen, L.~P., and Wolpert, D.~M. (2008).
\newblock Noise in the nervous system.
\newblock {\em Nature reviews neuroscience}, 9(4):292--303.

\bibitem[Farquhar and Gal, 2018]{farquhar2018towards}
Farquhar, S. and Gal, Y. (2018).
\newblock Towards robust evaluations of continual learning.
\newblock {\em arXiv preprint arXiv:1805.09733}.

\bibitem[Fernando et~al., 2017]{fernando2017pathnet}
Fernando, C., Banarse, D., Blundell, C., Zwols, Y., Ha, D., Rusu, A.~A.,
  Pritzel, A., and Wierstra, D. (2017).
\newblock Pathnet: Evolution channels gradient descent in super neural
  networks.
\newblock {\em arXiv preprint arXiv:1701.08734}.

\bibitem[Gao et~al., 2022]{gao2022r}
Gao, Q., Zhao, C., Ghanem, B., and Zhang, J. (2022).
\newblock R-dfcil: Relation-guided representation learning for data-free class
  incremental learning.
\newblock In {\em Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXIII}, pages 423--439.
  Springer.

\bibitem[Hassabis et~al., 2017]{hassabis2017neuroscience}
Hassabis, D., Kumaran, D., Summerfield, C., and Botvinick, M. (2017).
\newblock Neuroscience-inspired artificial intelligence.
\newblock {\em Neuron}, 95(2):245--258.

\bibitem[Hayes et~al., 2019]{hayes2019memory}
Hayes, T.~L., Cahill, N.~D., and Kanan, C. (2019).
\newblock Memory efficient experience replay for streaming learning.
\newblock In {\em 2019 International Conference on Robotics and Automation
  (ICRA)}, pages 9769--9776. IEEE.

\bibitem[Hayes et~al., 2020]{hayes2020remind}
Hayes, T.~L., Kafle, K., Shrestha, R., Acharya, M., and Kanan, C. (2020).
\newblock Remind your neural network to prevent catastrophic forgetting.
\newblock In {\em European Conference on Computer Vision}, pages 466--483.
  Springer.

\bibitem[Hendrycks and Dietterich, 2019]{hendrycks2019benchmarking}
Hendrycks, D. and Dietterich, T. (2019).
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock {\em arXiv preprint arXiv:1903.12261}.

\bibitem[Hinton et~al., 2015]{hinton2015KD}
Hinton, G., Vinyals, O., and Dean, J. (2015).
\newblock Distilling the knowledge in a neural network.

\bibitem[Hou et~al., 2019]{hou2019learning}
Hou, S., Pan, X., Loy, C.~C., Wang, Z., and Lin, D. (2019).
\newblock Learning a unified classifier incrementally via rebalancing.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 831--839.

\bibitem[Iscen et~al., 2020]{iscen2020memory}
Iscen, A., Zhang, J., Lazebnik, S., and Schmid, C. (2020).
\newblock Memory-efficient incremental learning through feature adaptation.
\newblock In {\em European conference on computer vision}, pages 699--715.
  Springer.

\bibitem[Jeeveswaran. et~al., 2022]{jeeveswaran2022comprehensive}
Jeeveswaran., K., Kathiresan., S., Varma., A., Magdy., O., Zonooz., B., and
  Arani., E. (2022).
\newblock A comprehensive study of vision transformers on dense prediction
  tasks.
\newblock In {\em Proceedings of the 17th International Joint Conference on
  Computer Vision, Imaging and Computer Graphics Theory and Applications -
  Volume 4: VISAPP,}, pages 213--223. INSTICC, SciTePress.

\bibitem[Ji and Wilson, 2007]{ji2007coordinated}
Ji, D. and Wilson, M.~A. (2007).
\newblock Coordinated memory replay in the visual cortex and hippocampus during
  sleep.
\newblock {\em Nature neuroscience}, 10(1):100--107.

\bibitem[Kemker and Kanan, 2017]{kemker2017fearnet}
Kemker, R. and Kanan, C. (2017).
\newblock Fearnet: Brain-inspired model for incremental learning.
\newblock {\em arXiv preprint arXiv:1711.10563}.

\bibitem[Khan et~al., 2022]{khan2022susceptibility}
Khan, H., Shah, P.~M., Zaidi, S. F.~A., et~al. (2022).
\newblock Susceptibility of continual learning against adversarial attacks.
\newblock {\em arXiv preprint arXiv:2207.05225}.

\bibitem[Kim, 2020]{kim2020torchattacks}
Kim, H. (2020).
\newblock Torchattacks: A pytorch repository for adversarial attacks.
\newblock {\em arXiv preprint arXiv:2010.01950}.

\bibitem[Kirkpatrick et~al., 2017]{kirkpatrick2017overcoming}
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu,
  A.~A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et~al.
  (2017).
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the national academy of sciences},
  114(13):3521--3526.

\bibitem[Krizhevsky et~al., 2009]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al. (2009).
\newblock Learning multiple layers of features from tiny images.

\bibitem[Kudithipudi et~al., 2022]{kudithipudi2022biological}
Kudithipudi, D., Aguilar-Simon, M., Babb, J., Bazhenov, M., Blackiston, D.,
  Bongard, J., Brna, A.~P., Chakravarthi~Raja, S., Cheney, N., Clune, J.,
  et~al. (2022).
\newblock Biological underpinnings for lifelong learning machines.
\newblock {\em Nature Machine Intelligence}, 4(3):196--210.

\bibitem[Kumaran et~al., 2016]{kumaran2016learning}
Kumaran, D., Hassabis, D., and McClelland, J.~L. (2016).
\newblock What learning systems do intelligent agents need? complementary
  learning systems theory updated.
\newblock {\em Trends in cognitive sciences}, 20(7):512--534.

\bibitem[Lao et~al., 2020]{lao2020continuous}
Lao, Q., Jiang, X., Havaei, M., and Bengio, Y. (2020).
\newblock Continuous domain adaptation with variational domain-agnostic feature
  replay.
\newblock {\em arXiv preprint arXiv:2003.04382}.

\bibitem[Le and Yang, 2015]{le2015tiny}
Le, Y. and Yang, X. (2015).
\newblock Tiny imagenet visual recognition challenge.
\newblock {\em CS 231N}, 7(7):3.

\bibitem[Li and Hoiem, 2017]{li2017learning}
Li, Z. and Hoiem, D. (2017).
\newblock Learning without forgetting.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  40(12):2935--2947.

\bibitem[Lindsay, 2020]{lindsay2020attention}
Lindsay, G.~W. (2020).
\newblock Attention in psychology, neuroscience, and machine learning.
\newblock {\em Frontiers in computational neuroscience}, page~29.

\bibitem[Liu et~al., 2022]{liu2022robust}
Liu, S., Zhu, Z., Qu, Q., and You, C. (2022).
\newblock Robust training under label noise by over-parameterization.
\newblock In {\em International Conference on Machine Learning}, pages
  14153--14172. PMLR.

\bibitem[Liu et~al., 2019]{liu2019human}
Liu, Y., Dolan, R.~J., Kurth-Nelson, Z., and Behrens, T.~E. (2019).
\newblock Human replay spontaneously reorganizes experience.
\newblock {\em Cell}, 178(3):640--652.

\bibitem[Lopez-Paz and Ranzato, 2017]{lopez2017gradient}
Lopez-Paz, D. and Ranzato, M. (2017).
\newblock Gradient episodic memory for continual learning.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Maass, 2014]{maass2014noise}
Maass, W. (2014).
\newblock Noise as a resource for computation and learning in networks of
  spiking neurons.
\newblock {\em Proceedings of the IEEE}, 102(5):860--880.

\bibitem[Mai et~al., 2022]{mai2022online}
Mai, Z., Li, R., Jeong, J., Quispe, D., Kim, H., and Sanner, S. (2022).
\newblock Online continual learning in image classification: An empirical
  survey.
\newblock {\em Neurocomputing}, 469:28--51.

\bibitem[Masana et~al., 2020]{masana2020class}
Masana, M., Liu, X., Twardowski, B., Menta, M., Bagdanov, A.~D., and van~de
  Weijer, J. (2020).
\newblock Class-incremental learning: survey and performance evaluation on
  image classification.
\newblock {\em arXiv preprint arXiv:2010.15277}.

\bibitem[McDonnell and Ward, 2011]{mcdonnell2011benefits}
McDonnell, M.~D. and Ward, L.~M. (2011).
\newblock The benefits of noise in neural systems: bridging theory and
  experiment.
\newblock {\em Nature Reviews Neuroscience}, 12(7):415--425.

\bibitem[McNaughton and O'Reilly, 1995]{mcnaughton1995there}
McNaughton, B.~L. and O'Reilly, R.~C. (1995).
\newblock Why there are complementary learning systems in the hippocampus and
  neocortex: Insights from the successes and failures of.
\newblock {\em Psychological Review}, 102(3):419--457.

\bibitem[Parisi et~al., 2019]{parisi2019continual}
Parisi, G.~I., Kemker, R., Part, J.~L., Kanan, C., and Wermter, S. (2019).
\newblock Continual lifelong learning with neural networks: A review.
\newblock {\em Neural Networks}, 113:54--71.

\bibitem[Pellegrini et~al., 2020]{pellegrini2020latent}
Pellegrini, L., Graffieti, G., Lomonaco, V., and Maltoni, D. (2020).
\newblock Latent replay for real-time continual learning.
\newblock In {\em 2020 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pages 10203--10209. IEEE.

\bibitem[Pelosin et~al., 2022]{pelosin2022towards}
Pelosin, F., Jha, S., Torsello, A., Raducanu, B., and van~de Weijer, J. (2022).
\newblock Towards exemplar-free continual learning in vision transformers: an
  account of attention, functional and weight regularization.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 3820--3829.

\bibitem[Peng et~al., 2021]{peng2021overcoming}
Peng, J., Tang, B., Jiang, H., Li, Z., Lei, Y., Lin, T., and Li, H. (2021).
\newblock Overcoming long-term catastrophic forgetting through adversarial
  neural pruning and synaptic consolidation.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems}.

\bibitem[Raghu et~al., 2021]{raghu2021vision}
Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C., and Dosovitskiy, A.
  (2021).
\newblock Do vision transformers see like convolutional neural networks?
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Ratcliff, 1990]{ratcliff1990connectionist}
Ratcliff, R. (1990).
\newblock Connectionist models of recognition memory: constraints imposed by
  learning and forgetting functions.
\newblock {\em Psychological review}, 97(2):285.

\bibitem[Rebuffi et~al., 2017]{rebuffi2017icarl}
Rebuffi, S.-A., Kolesnikov, A., Sperl, G., and Lampert, C.~H. (2017).
\newblock icarl: Incremental classifier and representation learning.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 2001--2010.

\bibitem[Robins, 1995]{robins1995catastrophic}
Robins, A. (1995).
\newblock Catastrophic forgetting, rehearsal and pseudorehearsal.
\newblock {\em Connection Science}, 7(2):123--146.

\bibitem[Rusu et~al., 2016]{rusu2016progressive}
Rusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,
  Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016).
\newblock Progressive neural networks.
\newblock {\em arXiv preprint arXiv:1606.04671}.

\bibitem[Sarfraz et~al., 2022]{sarfraz2022synergy}
Sarfraz, F., Arani, E., and Zonooz, B. (2022).
\newblock Synergy between synaptic consolidation and experience replay for
  general continual learning.
\newblock In {\em Conference on Lifelong Learning Agents}, pages 920--936.
  PMLR.

\bibitem[Smith et~al., 2021]{smith2021always}
Smith, J., Hsu, Y.-C., Balloch, J., Shen, Y., Jin, H., and Kira, Z. (2021).
\newblock Always be dreaming: A new approach for data-free class-incremental
  learning.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9374--9384.

\bibitem[Srivastava et~al., 2014]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R. (2014).
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958.

\bibitem[Touvron et~al., 2021]{touvron2021training}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou,
  H. (2021).
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In {\em International Conference on Machine Learning}, pages
  10347--10357. PMLR.

\bibitem[van~de Ven et~al., 2020]{van2020brain}
van~de Ven, G.~M., Siegelmann, H.~T., and Tolias, A.~S. (2020).
\newblock Brain-inspired replay for continual learning with artificial neural
  networks.
\newblock {\em Nature communications}, 11(1):1--14.

\bibitem[Van~de Ven and Tolias, 2019]{van2019three}
Van~de Ven, G.~M. and Tolias, A.~S. (2019).
\newblock Three scenarios for continual learning.
\newblock {\em arXiv preprint arXiv:1904.07734}.

\bibitem[Vaswani et~al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008.

\bibitem[Verma et~al., 2019]{verma2019manifold}
Verma, V., Lamb, A., Beckham, C., Najafi, A., Mitliagkas, I., Lopez-Paz, D.,
  and Bengio, Y. (2019).
\newblock Manifold mixup: Better representations by interpolating hidden
  states.
\newblock In {\em International Conference on Machine Learning}, pages
  6438--6447. PMLR.

\bibitem[Wang et~al., 2022a]{wang2022continual}
Wang, Z., Liu, L., Duan, Y., Kong, Y., and Tao, D. (2022a).
\newblock Continual learning with lifelong vision transformer.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 171--181.

\bibitem[Wang et~al., 2022b]{wang2022dualprompt}
Wang, Z., Zhang, Z., Ebrahimi, S., Sun, R., Zhang, H., Lee, C.-Y., Ren, X., Su,
  G., Perot, V., Dy, J., et~al. (2022b).
\newblock Dualprompt: Complementary prompting for rehearsal-free continual
  learning.
\newblock {\em arXiv preprint arXiv:2204.04799}.

\bibitem[Wang et~al., 2022c]{wang2022learning}
Wang, Z., Zhang, Z., Lee, C.-Y., Zhang, H., Sun, R., Ren, X., Su, G., Perot,
  V., Dy, J., and Pfister, T. (2022c).
\newblock Learning to prompt for continual learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 139--149.

\bibitem[Yin et~al., 2020]{yin2020dreaming}
Yin, H., Molchanov, P., Alvarez, J.~M., Li, Z., Mallya, A., Hoiem, D., Jha,
  N.~K., and Kautz, J. (2020).
\newblock Dreaming to distill: Data-free knowledge transfer via deepinversion.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 8715--8724.

\bibitem[Yoon et~al., 2019]{yoon2019scalable}
Yoon, J., Kim, S., Yang, E., and Hwang, S.~J. (2019).
\newblock Scalable and order-robust continual learning with additive parameter
  decomposition.
\newblock {\em arXiv preprint arXiv:1902.09432}.

\bibitem[Yu et~al., 2021]{yu2021improving}
Yu, P., Chen, Y., Jin, Y., and Liu, Z. (2021).
\newblock Improving vision transformers for incremental learning.
\newblock {\em arXiv preprint arXiv:2112.06103}.

\bibitem[Zenke et~al., 2017]{zenke2017continual}
Zenke, F., Poole, B., and Ganguli, S. (2017).
\newblock Continual learning through synaptic intelligence.
\newblock In {\em International Conference on Machine Learning}, pages
  3987--3995. PMLR.

\bibitem[Zhou et~al., 2019]{zhou2019toward}
Zhou, M., Liu, T., Li, Y., Lin, D., Zhou, E., and Zhao, T. (2019).
\newblock Toward understanding the importance of noise in training neural
  networks.
\newblock In {\em International Conference on Machine Learning}, pages
  7594--7602. PMLR.

\end{thebibliography}
