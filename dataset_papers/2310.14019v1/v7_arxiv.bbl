\begin{thebibliography}{10}

\bibitem{cazenavette2022distillation}
G.~Cazenavette, T.~Wang, A.~Torralba, A.~A. Efros, and J.-Y. Zhu.
\newblock Dataset distillation by matching training trajectories.
\newblock In {\em Proc. IEEE Conf. Comput. Vis. Pattern Recog.}, 2022.

\bibitem{chitta2021training}
K.~Chitta, J.~M. {\'A}lvarez, E.~Haussmann, and C.~Farabet.
\newblock Training data subset search with ensemble active learning.
\newblock {\em IEEE Transactions on Intelligent Transportation Systems}, 23(9):14741--14752, 2021.

\bibitem{coleman2020selection}
C.~Coleman, C.~Yeh, S.~Mussmann, B.~Mirzasoleiman, P.~Bailis, P.~Liang, J.~Leskovec, and M.~Zaharia.
\newblock Selection via proxy: Efficient data selection for deep learning.
\newblock In {\em Proc. Int. Conf. Learn. Represent.}, 2020.

\bibitem{cui2022scaling}
J.~Cui, R.~Wang, S.~Si, and C.-J. Hsieh.
\newblock Scaling up dataset distillation to imagenet-1k with constant memory.
\newblock {\em arXiv preprint arXiv:2211.10586}, 2022.

\bibitem{imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em Proc. IEEE Conf. Comput. Vis. Pattern Recog.}, pages 248--255, 2009.

\bibitem{deng2022remember}
Z.~Deng and O.~Russakovsky.
\newblock Remember the past: Distilling datasets into addressable memories for neural networks.
\newblock In {\em Proc. Adv. Neural Inform. Process. Syst.}, 2022.

\bibitem{du2022minimizing}
J.~Du, Y.~Jiang, V.~T. Tan, J.~T. Zhou, and H.~Li.
\newblock Minimizing the accumulated trajectory error to improve dataset distillation.
\newblock In {\em Proc. IEEE Conf. Comput. Vis. Pattern Recog.}, 2023.

\bibitem{ducoffe2018adversarial}
M.~Ducoffe and F.~Precioso.
\newblock Adversarial active learning for deep networks: a margin based approach.
\newblock {\em arXiv preprint arXiv:1802.09841}, 2018.

\bibitem{feldman2011unified}
D.~Feldman and M.~Langberg.
\newblock A unified framework for approximating and clustering data.
\newblock In {\em Proceedings of the Forty-Third Annual ACM Symposium on Theory of Computing}, page 569–578, 2011.

\bibitem{feldman2020turning}
D.~Feldman, M.~Schmidt, and C.~Sohler.
\newblock Turning big data into tiny data: Constant-size coresets for k-means, pca, and projective clustering.
\newblock {\em SIAM Journal on Computing}, 49(3):601--657, 2020.

\bibitem{feldman2020neural}
V.~Feldman and C.~Zhang.
\newblock What neural networks memorize and why: Discovering the long tail via influence estimation.
\newblock In {\em Proc. Adv. Neural Inform. Process. Syst.}, pages 2881--2891, 2020.

\bibitem{gidaris2018dynamic}
S.~Gidaris and N.~Komodakis.
\newblock Dynamic few-shot visual learning without forgetting.
\newblock In {\em Proc. IEEE Conf. Comput. Vis. Pattern Recog.}, pages 4367--4375, 2018.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proc. IEEE Conf. Comput. Vis. Pattern Recog.}, pages 770--778, 2016.

\bibitem{huang2017densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em Proc. IEEE Conf. Comput. Vis. Pattern Recog.}, pages 4700--4708, 2017.

\bibitem{huang2020importance}
L.~Huang and N.~K. Vishnoi.
\newblock Coresets for clustering in euclidean spaces: Importance sampling is nearly optimal.
\newblock In {\em Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing}, page 1416–1429, 2020.

\bibitem{jiang2022delving}
Z.~Jiang, J.~Gu, M.~Liu, and D.~Z. Pan.
\newblock Delving into effective gradient matching for dataset condensation.
\newblock {\em arXiv preprint arXiv:2208.00311}, 2022.

\bibitem{killamsetty2021grad}
K.~Killamsetty, S.~Durga, G.~Ramakrishnan, A.~De, and R.~Iyer.
\newblock Grad-match: Gradient matching based data subset selection for efficient deep model training.
\newblock In {\em Proc. Int. Conf. Mach. Learn.}, pages 5464--5474, 2021.

\bibitem{killamsetty2021glister}
K.~Killamsetty, D.~Sivasubramanian, G.~Ramakrishnan, and R.~Iyer.
\newblock Glister: Generalization based data subset selection for efficient and robust learning.
\newblock In {\em Proc. AAAI Conf. Artif. Intell.}, pages 8110--8118, 2021.

\bibitem{kimICML22}
J.-H. Kim, J.~Kim, S.~J. Oh, S.~Yun, H.~Song, J.~Jeong, J.-W. Ha, and H.~O. Song.
\newblock Dataset condensation via efficient synthetic-data parameterization.
\newblock In {\em Proc. Int. Conf. Mach. Learn.}, 2022.

\bibitem{cifar}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{lee2022dataset}
S.~Lee, S.~Chun, S.~Jung, S.~Yun, and S.~Yoon.
\newblock Dataset condensation with contrastive signals.
\newblock In {\em Proc. Int. Conf. Mach. Learn.}, pages 12352--12364, 2022.

\bibitem{lewis1995sequential}
D.~D. Lewis.
\newblock A sequential algorithm for training text classifiers: Corrigendum and additional data.
\newblock In {\em Acm Sigir Forum}, pages 13--19, 1995.

\bibitem{liu2022dataset}
S.~Liu, K.~Wang, X.~Yang, J.~Ye, and X.~Wang.
\newblock Dataset distillation via factorization.
\newblock In {\em Proc. Adv. Neural Inform. Process. Syst.}, 2022.

\bibitem{liu2023dream}
Y.~Liu, J.~Gu, K.~Wang, Z.~Zhu, W.~Jiang, and Y.~You.
\newblock {DREAM}: Efficient dataset distillation by representative matching.
\newblock {\em arXiv preprint arXiv:2302.14416}, 2023.

\bibitem{loo2022efficient}
N.~Loo, R.~Hasani, A.~Amini, and D.~Rus.
\newblock Efficient dataset distillation using random feature approximation.
\newblock In {\em Proc. Adv. Neural Inform. Process. Syst.}, 2022.

\bibitem{loo2023dataset}
N.~Loo, R.~Hasani, M.~Lechner, and D.~Rus.
\newblock Dataset distillation with convexified implicit gradients.
\newblock {\em arXiv preprint arXiv:2302.06755}, 2023.

\bibitem{lorraine2020optimizing}
J.~Lorraine, P.~Vicol, and D.~Duvenaud.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 1540--1552, 2020.

\bibitem{margatina-etal-2021-active}
K.~Margatina, G.~Vernikos, L.~Barrault, and N.~Aletras.
\newblock Active learning by acquiring contrastive examples.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 650--663, 2021.

\bibitem{meding2022trivial}
K.~Meding, L.~M.~S. Buschoff, R.~Geirhos, and F.~A. Wichmann.
\newblock Trivial or impossible --- dichotomous data difficulty masks model differences (on imagenet and beyond).
\newblock In {\em Proc. Int. Conf. Learn. Represent.}, 2022.

\bibitem{mirzasoleiman2020coresets}
B.~Mirzasoleiman, J.~Bilmes, and J.~Leskovec.
\newblock Coresets for data-efficient training of machine learning models.
\newblock In {\em Proc. Int. Conf. Mach. Learn.}, pages 6950--6960, 2020.

\bibitem{mohri2008rademacher}
M.~Mohri and A.~Rostamizadeh.
\newblock Rademacher complexity bounds for non-iid processes.
\newblock In {\em Proc. Adv. Neural Inform. Process. Syst.}, 2008.

\bibitem{nguyen2021dataset}
T.~Nguyen, Z.~Chen, and J.~Lee.
\newblock Dataset meta-learning from kernel ridge-regression.
\newblock In {\em Proc. Int. Conf. Learn. Represent.}, 2021.

\bibitem{nguyen2021datasetKIP}
T.~Nguyen, R.~Novak, L.~Xiao, and J.~Lee.
\newblock Dataset distillation with infinitely wide convolutional networks.
\newblock In {\em Proc. Adv. Neural Inform. Process. Syst.}, pages 5186--5198, 2021.

\bibitem{paul2021deep}
M.~Paul, S.~Ganguli, and G.~K. Dziugaite.
\newblock Deep learning on a data diet: Finding important examples early in training.
\newblock In {\em Proc. Adv. Neural Inform. Process. Syst.}, pages 20596--20607, 2021.

\bibitem{pleiss2020identifying}
G.~Pleiss, T.~Zhang, E.~Elenberg, and K.~Q. Weinberger.
\newblock Identifying mislabeled data using the area under the margin ranking.
\newblock In {\em Proc. Adv. Neural Inform. Process. Syst.}, pages 17044--17056, 2020.

\bibitem{pooladzandi2022adaptive}
O.~Pooladzandi, D.~Davini, and B.~Mirzasoleiman.
\newblock Adaptive second order coresets for data-efficient machine learning.
\newblock In {\em Proc. Int. Conf. Mach. Learn.}, pages 17848--17869, 2022.

\bibitem{rensurvey2021}
P.~Ren, Y.~Xiao, X.~Chang, P.-Y. Huang, Z.~Li, B.~B. Gupta, X.~Chen, and X.~Wang.
\newblock A survey of deep active learning.
\newblock {\em ACM Comput. Surv.}, 54(9), oct 2021.

\bibitem{settles2012}
B.~Settles.
\newblock {\em Active Learning}.
\newblock Springer International Publishing, 2012.

\bibitem{shin2023loss}
S.~Shin, H.~Bae, D.~Shin, W.~Joo, and I.-C. Moon.
\newblock Loss-curvature matching for dataset selection and condensation.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 8606--8628, 2023.

\bibitem{sorscher2022beyond}
B.~Sorscher, R.~Geirhos, S.~Shekhar, S.~Ganguli, and A.~Morcos.
\newblock Beyond neural scaling laws: beating power law scaling via data pruning.
\newblock In {\em Proc. Adv. Neural Inform. Process. Syst.}, pages 19523--19536, 2022.

\bibitem{toneva2018an}
M.~Toneva, A.~Sordoni, R.~T. des Combes, A.~Trischler, Y.~Bengio, and G.~J. Gordon.
\newblock An empirical study of example forgetting during deep neural network learning.
\newblock In {\em Proc. Int. Conf. Learn. Represent.}, 2019.

\bibitem{vicol2022implicit}
P.~Vicol, J.~P. Lorraine, F.~Pedregosa, D.~Duvenaud, and R.~B. Grosse.
\newblock On implicit bias in overparameterized bilevel optimization.
\newblock In {\em Proc. Int. Conf. Mach. Learn.}, pages 22234--22259, 2022.

\bibitem{wang2023dim}
K.~Wang, J.~Gu, D.~Zhou, Z.~Zhu, W.~Jiang, and Y.~You.
\newblock Dim: Distilling dataset into generative model.
\newblock {\em arXiv preprint arXiv:2303.04707}, 2023.

\bibitem{wang2022cafe}
K.~Wang, B.~Zhao, X.~Peng, Z.~Zhu, S.~Yang, S.~Wang, G.~Huang, H.~Bilen, X.~Wang, and Y.~You.
\newblock Cafe: Learning to condense dataset by aligning features.
\newblock In {\em Proc. IEEE Conf. Comput. Vis. Pattern Recog.}, pages 12196--12205, 2022.

\bibitem{wang2018dataset}
T.~Wang, J.-Y. Zhu, A.~Torralba, and A.~A. Efros.
\newblock Dataset distillation.
\newblock {\em arXiv preprint arXiv:1811.10959}, 2018.

\bibitem{welling2009herding}
M.~Welling.
\newblock Herding dynamical weights to learn.
\newblock In {\em Proc. Int. Conf. Mach. Learn.}, pages 1121--1128, 2009.

\bibitem{xia2023moderate}
X.~Xia, J.~Liu, J.~Yu, X.~Shen, B.~Han, and T.~Liu.
\newblock Moderate coreset: A universal method of data selection for real-world data-efficient deep learning.
\newblock In {\em Proc. Int. Conf. Learn. Represent.}, 2023.

\bibitem{yang2023dataset}
S.~Yang, Z.~Xie, H.~Peng, M.~Xu, M.~Sun, and P.~Li.
\newblock Dataset pruning: Reducing training data by examining generalization influence.
\newblock In {\em Proc. Int. Conf. Learn. Represent.}, 2023.

\bibitem{zhang2022accelerating}
L.~Zhang, J.~Zhang, B.~Lei, S.~Mukherjee, X.~Pan, B.~Zhao, C.~Ding, Y.~Li, and D.~Xu.
\newblock Accelerating dataset distillation via model augmentation.
\newblock In {\em Proc. IEEE Conf. Comput. Vis. Pattern Recog.}, 2023.

\bibitem{zhao2021dataset}
B.~Zhao and H.~Bilen.
\newblock Dataset condensation with differentiable siamese augmentation.
\newblock In {\em Proc. Int. Conf. Mach. Learn.}, pages 12674--12685, 2021.

\bibitem{zhao2022synthesizing}
B.~Zhao and H.~Bilen.
\newblock Synthesizing informative training samples with {GAN}.
\newblock In {\em NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research}, 2022.

\bibitem{zhao2023dataset}
B.~Zhao and H.~Bilen.
\newblock Dataset condensation with distribution matching.
\newblock In {\em Proc. IEEE Winter Conf. Appl. Comput. Vis.}, pages 6514--6523, 2023.

\bibitem{zhao2021datasetGM}
B.~Zhao, K.~R. Mopuri, and H.~Bilen.
\newblock Dataset condensation with gradient matching.
\newblock In {\em Proc. Int. Conf. Learn. Represent.}, 2021.

\bibitem{zheng2023coveragecentric}
H.~Zheng, R.~Liu, F.~Lai, and A.~Prakash.
\newblock Coverage-centric coreset selection for high pruning rates.
\newblock In {\em Proc. Int. Conf. Learn. Represent.}, 2023.

\bibitem{zhou2022dataset}
Y.~Zhou, E.~Nezhadarya, and J.~Ba.
\newblock Dataset distillation using neural feature regression.
\newblock In {\em Proc. Adv. Neural Inform. Process. Syst.}, 2022.

\end{thebibliography}
