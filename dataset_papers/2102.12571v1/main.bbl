\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abel \& Winder(2019)Abel and Winder]{abel2019expected}
Abel, D. and Winder, J.
\newblock The expected-length model of options.
\newblock In \emph{IJCAI}, 2019.

\bibitem[Alpern \& Schneider(1987)Alpern and Schneider]{alpern1987recognizing}
Alpern, B. and Schneider, F.~B.
\newblock Recognizing safety and liveness.
\newblock \emph{Distributed computing}, 2\penalty0 (3):\penalty0 117--126,
  1987.

\bibitem[Andreas et~al.(2017)Andreas, Klein, and Levine]{andreas2017modular}
Andreas, J., Klein, D., and Levine, S.
\newblock Modular multitask reinforcement learning with policy sketches.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  166--175, 2017.

\bibitem[Araki et~al.(2019)Araki, Vodrahalli, Leech, Vasile, Donahue, and
  Rus]{araki2019learning}
Araki, B., Vodrahalli, K., Leech, T., Vasile, C.~I., Donahue, M., and Rus, D.
\newblock Learning to plan with logical automata.
\newblock In \emph{Proceedings of Robotics: Science and Systems},
  FreiburgimBreisgau, Germany, June 2019.
\newblock \doi{10.15607/RSS.2019.XV.064}.

\bibitem[Araki et~al.(2020)Araki, Vodrahalli, Leech, Vasile, Donahue, and
  Rus]{araki2020deep}
Araki, B., Vodrahalli, K., Leech, T., Vasile, C.~I., Donahue, M., and Rus, D.
\newblock Deep bayesian nonparametric learning of rules and plans from
  demonstrations with a learned automaton prior.
\newblock In \emph{AAAI}, pp.\  10026--10034, 2020.

\bibitem[Baier \& Katoen(2008)Baier and Katoen]{Baier08}
Baier, C. and Katoen, J.
\newblock \emph{{Principles of model checking}}.
\newblock MIT Press, 2008.
\newblock ISBN 978-0-262-02649-9.

\bibitem[Bhatia et~al.(2010)Bhatia, Kavraki, and Vardi]{bhatia2010sampling}
Bhatia, A., Kavraki, L.~E., and Vardi, M.~Y.
\newblock Sampling-based motion planning with temporal goals.
\newblock In \emph{2010 IEEE International Conference on Robotics and
  Automation}, pp.\  2689--2696. IEEE, 2010.

\bibitem[Camacho et~al.(2019)Camacho, Icarte, Klassen, Valenzano, and
  McIlraith]{camacho2019ltl}
Camacho, A., Icarte, R.~T., Klassen, T.~Q., Valenzano, R.~A., and McIlraith,
  S.~A.
\newblock Ltl and beyond: Formal languages for reward function specification in
  reinforcement learning.
\newblock In \emph{IJCAI}, volume~19, pp.\  6065--6073, 2019.

\bibitem[Clarke et~al.(2001)Clarke, Grumberg, and Peled]{Clark01}
Clarke, E.~M., Grumberg, O., and Peled, D.
\newblock \emph{Model Checking}.
\newblock MIT Press, 2001.
\newblock ISBN 978-0-262-03270-4.

\bibitem[Dayan \& Hinton(1993)Dayan and Hinton]{dayan1993feudal}
Dayan, P. and Hinton, G.~E.
\newblock Feudal reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  271--278, 1993.

\bibitem[Dietterich(2000)]{dietterich2000hierarchical}
Dietterich, T.~G.
\newblock Hierarchical reinforcement learning with the maxq value function
  decomposition.
\newblock \emph{Journal of artificial intelligence research}, 13:\penalty0
  227--303, 2000.

\bibitem[Diuk et~al.(2008)Diuk, Cohen, and Littman]{diuk2008object}
Diuk, C., Cohen, A., and Littman, M.~L.
\newblock An object-oriented representation for efficient reinforcement
  learning.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  240--247, 2008.

\bibitem[Duret-Lutz et~al.(2016)Duret-Lutz, Lewkowicz, Fauchille, Michaud,
  Renault, and Xu]{Duret16}
Duret-Lutz, A., Lewkowicz, A., Fauchille, A., Michaud, T., Renault, E., and Xu,
  L.
\newblock Spot 2.0 --- a framework for {LTL} and $\omega$-automata
  manipulation.
\newblock In \emph{Proceedings of the 14th International Symposium on Automated
  Technology for Verification and Analysis (ATVA'16)}, volume 9938 of
  \emph{Lecture Notes in Computer Science}, pp.\  122--129. Springer, October
  2016.
\newblock \doi{10.1007/978-3-319-46520-3_8}.

\bibitem[Eysenbach et~al.(2019)Eysenbach, Salakhutdinov, and
  Levine]{eysenbach2019search}
Eysenbach, B., Salakhutdinov, R.~R., and Levine, S.
\newblock Search on the replay buffer: Bridging planning and reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  15220--15231, 2019.

\bibitem[Faust et~al.(2018)Faust, Oslund, Ramirez, Francis, Tapia, Fiser, and
  Davidson]{faust2018prm}
Faust, A., Oslund, K., Ramirez, O., Francis, A., Tapia, L., Fiser, M., and
  Davidson, J.
\newblock Prm-rl: Long-range robotic navigation tasks by combining
  reinforcement learning and sampling-based planning.
\newblock In \emph{2018 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  5113--5120. IEEE, 2018.

\bibitem[Ghosh et~al.(2018)Ghosh, Gupta, and Levine]{ghosh2018learning}
Ghosh, D., Gupta, A., and Levine, S.
\newblock Learning actionable representations with goal-conditioned policies.
\newblock \emph{arXiv preprint arXiv:1811.07819}, 2018.

\bibitem[Gopalan et~al.(2017)Gopalan, Littman, MacGlashan, Squire, Tellex,
  Winder, Wong, et~al.]{gopalan2017planning}
Gopalan, N., Littman, M.~L., MacGlashan, J., Squire, S., Tellex, S., Winder,
  J., Wong, L.~L., et~al.
\newblock Planning with abstract markov decision processes.
\newblock In \emph{Twenty-Seventh International Conference on Automated
  Planning and Scheduling}, 2017.

\bibitem[Gordon et~al.(2019)Gordon, Fox, and Farhadi]{gordon2019should}
Gordon, D., Fox, D., and Farhadi, A.
\newblock What should i do now? marrying reinforcement learning and symbolic
  planning.
\newblock \emph{arXiv preprint arXiv:1901.01492}, 2019.

\bibitem[Hasanbeig et~al.(2018)Hasanbeig, Abate, and
  Kroening]{hasanbeig2018logically}
Hasanbeig, M., Abate, A., and Kroening, D.
\newblock Logically-constrained reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1801.08099}, 2018.

\bibitem[Icarte et~al.(2018)Icarte, Klassen, Valenzano, and
  McIlraith]{icarte2018using}
Icarte, R.~T., Klassen, T., Valenzano, R., and McIlraith, S.
\newblock Using reward machines for high-level task specification and
  decomposition in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2107--2116, 2018.

\bibitem[Icarte et~al.(2019)Icarte, Waldie, Klassen, Valenzano, Castro, and
  McIlraith]{icarte2019learning}
Icarte, R.~T., Waldie, E., Klassen, T., Valenzano, R., Castro, M., and
  McIlraith, S.
\newblock Learning reward machines for partially observable reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  15523--15534, 2019.

\bibitem[Illanes et~al.(2020)Illanes, Yan, Icarte, and
  McIlraith]{illanes2020symbolic}
Illanes, L., Yan, X., Icarte, R.~T., and McIlraith, S.~A.
\newblock Symbolic plans as high-level instructions for reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Automated
  Planning and Scheduling}, volume~30, pp.\  540--550, 2020.

\bibitem[James et~al.(2019)James, Freese, and Davison]{james2019pyrep}
James, S., Freese, M., and Davison, A.~J.
\newblock Pyrep: Bringing v-rep to deep robot learning.
\newblock \emph{arXiv preprint arXiv:1906.11176}, 2019.

\bibitem[Jothimurugan et~al.(2019)Jothimurugan, Alur, and
  Bastani]{jothimurugan2019composable}
Jothimurugan, K., Alur, R., and Bastani, O.
\newblock A composable specification language for reinforcement learning tasks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  13041--13051, 2019.

\bibitem[Kansou(2019)]{kansou2019converting}
Kansou, B. K.~A.
\newblock Converting asubset of ltl formula to buchi automata.
\newblock \emph{International Journal of Software Engineering \& Applications
  (IJSEA)}, 10\penalty0 (2), 2019.

\bibitem[Kuo et~al.(2020)Kuo, Katz, and Barbu]{kuo2020encoding}
Kuo, Y.-L., Katz, B., and Barbu, A.
\newblock Encoding formulas as deep networks: Reinforcement learning for
  zero-shot execution of ltl formulas.
\newblock \emph{arXiv preprint arXiv:2006.01110}, 2020.

\bibitem[Li et~al.(2017)Li, Vasile, and Belta]{li2017reinforcement}
Li, X., Vasile, C.-I., and Belta, C.
\newblock Reinforcement learning with temporal logic rewards.
\newblock In \emph{2017 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  3834--3839. IEEE, 2017.

\bibitem[Li et~al.(2019)Li, Serlin, Yang, and Belta]{li2019formal}
Li, X., Serlin, Z., Yang, G., and Belta, C.
\newblock A formal methods approach to interpretable reinforcement learning for
  robotic planning.
\newblock \emph{Science Robotics}, 4\penalty0 (37), 2019.

\bibitem[Lyu et~al.(2019)Lyu, Yang, Liu, and Gustafson]{lyu2019sdrl}
Lyu, D., Yang, F., Liu, B., and Gustafson, S.
\newblock Sdrl: interpretable and data-efficient deep reinforcement learning
  leveraging symbolic planning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  2970--2977, 2019.

\bibitem[Mason et~al.(2017)Mason, Calinescu, Kudenko, and
  Banks]{mason2017assured}
Mason, G.~R., Calinescu, R.~C., Kudenko, D., and Banks, A.
\newblock Assured reinforcement learning with formally verified abstract
  policies.
\newblock In \emph{9th International Conference on Agents and Artificial
  Intelligence (ICAART)}. York, 2017.

\bibitem[Oh et~al.(2019)Oh, Patel, Nguyen, Huang, Pavlick, and
  Tellex]{oh2019planning}
Oh, Y., Patel, R., Nguyen, T., Huang, B., Pavlick, E., and Tellex, S.
\newblock Planning with state abstractions for non-markovian task
  specifications.
\newblock \emph{arXiv preprint arXiv:1905.12096}, 2019.

\bibitem[Parr \& Russell(1998)Parr and Russell]{parr1998reinforcement}
Parr, R. and Russell, S.~J.
\newblock Reinforcement learning with hierarchies of machines.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1043--1049, 1998.

\bibitem[Paxton et~al.(2017)Paxton, Raman, Hager, and
  Kobilarov]{paxton2017combining}
Paxton, C., Raman, V., Hager, G.~D., and Kobilarov, M.
\newblock Combining neural networks and tree search for task and motion
  planning in challenging environments.
\newblock In \emph{2017 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  6059--6066. IEEE, 2017.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shah et~al.(2020)Shah, Li, and Shah]{shah2020planning}
Shah, A., Li, S., and Shah, J.
\newblock Planning with uncertain specifications (puns).
\newblock \emph{IEEE Robotics and Automation Letters}, 5\penalty0 (2):\penalty0
  3414--3421, 2020.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton1999between}
Sutton, R.~S., Precup, D., and Singh, S.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in
  reinforcement learning.
\newblock \emph{Artificial intelligence}, 112\penalty0 (1-2):\penalty0
  181--211, 1999.

\bibitem[Yuan et~al.(2019)Yuan, Hasanbeig, Abate, and
  Kroening]{yuan2019modular}
Yuan, L.~Z., Hasanbeig, M., Abate, A., and Kroening, D.
\newblock Modular deep reinforcement learning with temporal logic
  specifications.
\newblock \emph{arXiv preprint arXiv:1909.11591}, 2019.

\bibitem[Zhang \& Sridharan(2020)Zhang and Sridharan]{zhang2020survey}
Zhang, S. and Sridharan, M.
\newblock A survey of knowledge-based sequential decision making under
  uncertainty.
\newblock \emph{arXiv preprint arXiv:2008.08548}, 2020.

\end{thebibliography}
