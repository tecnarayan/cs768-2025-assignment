\begin{thebibliography}{88}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Aghajanyan et~al.(2022)Aghajanyan, Huang, Ross, Karpukhin, Xu, Goyal, Okhonko, Joshi, Ghosh, Lewis et~al.}]{aghajanyan2022cm3}
Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu~Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et~al. 2022.
\newblock Cm3: A causal masked multimodal model of the internet.
\newblock \emph{arXiv preprint arXiv:2201.07520}.

\bibitem[{Aghajanyan et~al.(2023)Aghajanyan, Yu, Conneau, Hsu, Hambardzumyan, Zhang, Roller, Goyal, Levy, and Zettlemoyer}]{aghajanyan2023scaling}
Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. 2023.
\newblock Scaling laws for generative mixed-modal language models.
\newblock In \emph{International Conference on Machine Learning}, pages 265--279. PMLR.

\bibitem[{Allal et~al.(2023)Allal, Li, Kocetkov, Mou, Akiki, Ferrandis, Muennighoff, Mishra, Gu, Dey et~al.}]{allal2023santacoder}
Loubna~Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos~Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et~al. 2023.
\newblock SantaCoder: don't reach for the stars!
\newblock \emph{arXiv preprint arXiv:2301.03988}.

\bibitem[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Debbah, Goffinet, Hesslow, Launay, Malartic et~al.}]{almazrouei2023falcon}
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M{\'e}rouane Debbah, {\'E}tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et~al. 2023.
\newblock The falcon series of open language models.
\newblock \emph{arXiv preprint arXiv:2311.16867}.

\bibitem[{Azerbayev et~al.(2023)Azerbayev, Schoelkopf, Paster, Santos, McAleer, Jiang, Deng, Biderman, and Welleck}]{azerbayev2023llemma}
Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco~Dos Santos, Stephen McAleer, Albert~Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023.
\newblock Llemma: An open language model for mathematics.
\newblock \emph{arXiv preprint arXiv:2310.10631}.

\bibitem[{Bahri et~al.(2021)Bahri, Dyer, Kaplan, Lee, and Sharma}]{bahri2021explaining}
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. 2021.
\newblock Explaining neural scaling laws.
\newblock \emph{arXiv preprint arXiv:2102.06701}.

\bibitem[{Bi et~al.(2024)Bi, Chen, Chen, Chen, Dai, Deng, Ding, Dong, Du, Fu, Gao, Gao, Gao, Ge, Guan, Guo, Guo, Hao, Hao, He, Hu, Huang, Li, Li, Li, Li, Li, Liang, Lin, Liu, Liu, Liu, Liu, Liu, Liu, Lu, Lu, Luo, Ma, Nie, Pei, Piao, Qiu, Qu, Ren, Ren, Ruan, Sha, Shao, Song, Su, Sun, Sun, Tang, Wang, Wang, Wang, Wang, Wang, Wu, Wu, Xie, Xie, Xie, Xiong, Xu, Xu, Xu, Yang, You, Yu, Yu, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhao, Zhao, Zhou, Zhou, Zhu, and Zou}]{deepseekllm2024}
Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y.~K. Li, Wenfeng Liang, Fangyun Lin, Alex~X. Liu, Bo~Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y.~Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R.~X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B.~Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao
  Zhu, and Yuheng Zou. 2024.
\newblock \href {https://doi.org/10.48550/ARXIV.2401.02954} {DeepSeek {LLM:} Scaling Open-Source Language Models with Longtermism}.
\newblock \emph{CoRR}, abs/2401.02954.

\bibitem[{Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O’Brien, Hallahan, Khan, Purohit, Prashanth, Raff et~al.}]{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, et~al. 2023.
\newblock Pythia: A suite for analyzing large language models across training and scaling.
\newblock In \emph{International Conference on Machine Learning}, pages 2397--2430. PMLR.

\bibitem[{Bisk et~al.(2020)Bisk, Zellers, Gao, Choi et~al.}]{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al. 2020.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 7432--7439.

\bibitem[{Blevins et~al.(2024)Blevins, Limisiewicz, Gururangan, Li, Gonen, Smith, and Zettlemoyer}]{blevins2024breaking}
Terra Blevins, Tomasz Limisiewicz, Suchin Gururangan, Margaret Li, Hila Gonen, Noah~A Smith, and Luke Zettlemoyer. 2024.
\newblock Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models.
\newblock \emph{arXiv preprint arXiv:2401.10440}.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al. 2020.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:1877--1901.

\bibitem[{Cai et~al.(2024)Cai, Cao, Chen, Chen, Chen, Chen, Chen, Chen, Chen, Chu, Dong, Duan, Fan, Fei, Gao, Ge, Gu, Gu, Gui, Guo, Guo, He, Hu, Huang, Jiang, Jiao, Jin, Lei, Li, Li, Li, Li, Li, Li, Liu, Liu, Hong, Liu, Liu, Liu, Lv, Lv, Lv, Ma, Ma, Ma, Ning, Ouyang, Qiu, Qu, Shang, Shao, Song, Song, Sui, Sun, Sun, Tang, Wang, Wang, Wang, Wang, Wang, Wang, Wang, Wei, Weng, Wu, Xiong, Xu, Xu, Yan, Yan, Yang, Ye, Ying, Yu, Yu, Zang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhang, Zhao, Zhao, Zhao, Zhou, Zhou, Zhuo, Zou, Qiu, Qiao, and Lin}]{cai2024internlm2technicalreport}
Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi~Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, Aijia Guo, Qipeng Guo, Conghui He, Yingfan Hu, Ting Huang, Tao Jiang, Penglong Jiao, Zhenjiang Jin, Zhikai Lei, Jiaxing Li, Jingwen Li, Linyang Li, Shuaibin Li, Wei Li, Yining Li, Hongwei Liu, Jiangning Liu, Jiawei Hong, Kaiwen Liu, Kuikun Liu, Xiaoran Liu, Chengqi Lv, Haijun Lv, Kai Lv, Li~Ma, Runyuan Ma, Zerun Ma, Wenchang Ning, Linke Ouyang, Jiantao Qiu, Yuan Qu, Fukai Shang, Yunfan Shao, Demin Song, Zifan Song, Zhihao Sui, Peng Sun, Yu~Sun, Huanze Tang, Bin Wang, Guoteng Wang, Jiaqi Wang, Jiayu Wang, Rui Wang, Yudong Wang, Ziyi Wang, Xingjian Wei, Qizhen Weng, Fan Wu, Yingtong Xiong, Chao Xu, Ruiliang Xu, Hang Yan, Yirong Yan, Xiaogui Yang, Haochen Ye, Huaiyuan Ying, Jia Yu, Jing Yu, Yuhang Zang, Chuyu Zhang, Li~Zhang, Pan Zhang, Peng Zhang, Ruijie Zhang, Shuo Zhang, Songyang Zhang, Wenjian Zhang,
  Wenwei Zhang, Xingcheng Zhang, Xinyue Zhang, Hui Zhao, Qian Zhao, Xiaomeng Zhao, Fengzhe Zhou, Zaida Zhou, Jingming Zhuo, Yicheng Zou, Xipeng Qiu, Yu~Qiao, and Dahua Lin. 2024.
\newblock \href {http://arxiv.org/abs/2403.17297} {InternLM2 Technical Report}.
\newblock \emph{arxiv}.

\bibitem[{Cherti et~al.(2023)Cherti, Beaumont, Wightman, Wortsman, Ilharco, Gordon, Schuhmann, Schmidt, and Jitsev}]{cherti2023reproducible}
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. 2023.
\newblock Reproducible scaling laws for contrastive language-image learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 2818--2829.

\bibitem[{Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann et~al.}]{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al. 2023.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24(240):1--113.

\bibitem[{Chung et~al.(2020)Chung, Garrette, Tan, and Riesa}]{chung-etal-2020-improving}
Hyung~Won Chung, Dan Garrette, Kiat~Chuan Tan, and Jason Riesa. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.emnlp-main.367} {Improving Multilingual Models with Language-Clustered Vocabularies}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 4536--4546, Online. Association for Computational Linguistics.

\bibitem[{Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and Toutanova}]{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019.
\newblock BoolQ: Exploring the surprising difficulty of natural yes/no questions.
\newblock \emph{arXiv preprint arXiv:1905.10044}.

\bibitem[{Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord}]{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}.

\bibitem[{Dagan et~al.(2024)Dagan, Synnaeve, and Rozi{\`{e}}re}]{dagan2024gettingthemost}
Gautier Dagan, Gabriel Synnaeve, and Baptiste Rozi{\`{e}}re. 2024.
\newblock \href {https://doi.org/10.48550/ARXIV.2402.01035} {Getting the most out of your tokenizer for pre-training and domain adaptation}.
\newblock \emph{CoRR}, abs/2402.01035.

\bibitem[{Dou et~al.(2024)Dou, Liu, Zeng, Guo, Zhou, Lu, and Lin}]{dou2024sailor}
Longxu Dou, Qian Liu, Guangtao Zeng, Jia Guo, Jiahui Zhou, Wei Lu, and Min Lin. 2024.
\newblock \href {https://doi.org/10.48550/ARXIV.2404.03608} {Sailor: Open Language Models for South-East Asia}.
\newblock \emph{CoRR}, abs/2404.03608.

\bibitem[{Du et~al.(2021)Du, Qian, Liu, Ding, Qiu, Yang, and Tang}]{du2021glm}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2021.
\newblock Glm: General language model pretraining with autoregressive blank infilling.
\newblock \emph{arXiv preprint arXiv:2103.10360}.

\bibitem[{Enarvi et~al.(2017)Enarvi, Smit, Virpioja, and Kurimo}]{enarvi2017automatic}
Seppo Enarvi, Peter Smit, Sami Virpioja, and Mikko Kurimo. 2017.
\newblock Automatic speech recognition with very large conversational finnish and estonian vocabularies.
\newblock \emph{IEEE/ACM Transactions on audio, speech, and language processing}, 25(11):2085--2097.

\bibitem[{Esser et~al.(2021)Esser, Rombach, and Ommer}]{esser2021taming}
Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021.
\newblock Taming transformers for high-resolution image synthesis.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 12873--12883.

\bibitem[{Gadre et~al.(2024)Gadre, Smyrnis, Shankar, Gururangan, Wortsman, Shao, Mercat, Fang, Li, Keh et~al.}]{gadre2024language}
Samir~Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, et~al. 2024.
\newblock Language models scale reliably with over-training and on downstream tasks.
\newblock \emph{arXiv preprint arXiv:2403.08540}.

\bibitem[{Gao et~al.(2019)Gao, He, Tan, Qin, Wang, and Liu}]{gao2019representation}
Jun Gao, Di~He, Xu~Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. 2019.
\newblock Representation degeneration problem in training natural language generation models.
\newblock \emph{arXiv preprint arXiv:1907.12009}.

\bibitem[{Groeneveld et~al.(2024)Groeneveld, Beltagy, Walsh, Bhagia, Kinney, Tafjord, Jha, Ivison, Magnusson, Wang et~al.}]{groeneveld2024olmo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et~al. 2024.
\newblock Olmo: Accelerating the science of language models.
\newblock \emph{arXiv preprint arXiv:2402.00838}.

\bibitem[{Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark et~al.}]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al. 2022.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}.

\bibitem[{Huang et~al.(2024)Huang, Zhang, Shan, and He}]{compression2024}
Yuzhen Huang, Jinghan Zhang, Zifei Shan, and Junxian He. 2024.
\newblock \href {https://doi.org/10.48550/ARXIV.2404.09937} {Compression Represents Intelligence Linearly}.
\newblock \emph{CoRR}, abs/2404.09937.

\bibitem[{Isik et~al.(2024)Isik, Ponomareva, Hazimeh, Paparas, Vassilvitskii, and Koyejo}]{isik2024scaling}
Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi Koyejo. 2024.
\newblock Scaling Laws for Downstream Task Performance of Large Language Models.
\newblock \emph{arXiv preprint arXiv:2402.04177}.

\bibitem[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier et~al.}]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al. 2023.
\newblock Mistral 7B.
\newblock \emph{arXiv preprint arXiv:2310.06825}.

\bibitem[{Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}.

\bibitem[{Le~Scao et~al.(2023)Le~Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon, Gall{\'e} et~al.}]{le2023bloom}
Teven Le~Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, Matthias Gall{\'e}, et~al. 2023.
\newblock Bloom: A 176b-parameter open-access multilingual language model.

\bibitem[{Li et~al.(2024)Li, Fang, Smyrnis, Ivgi, Jordan, Gadre, Bansal, Guha, Keh, Arora, Garg, Xin, Muennighoff, Heckel, Mercat, Chen, Gururangan, Wortsman, Albalak, Bitton, Nezhurina, Abbas, Hsieh, Ghosh, Gardner, Kilian, Zhang, Shao, Pratt, Sanyal, Ilharco, Daras, Marathe, Gokaslan, Zhang, Chandu, Nguyen, Vasiljevic, Kakade, Song, Sanghavi, Faghri, Oh, Zettlemoyer, Lo, El-Nouby, Pouransari, Toshev, Wang, Groeneveld, Soldaini, Koh, Jitsev, Kollar, Dimakis, Carmon, Dave, Schmidt, and Shankar}]{li2024datacomplmsearchgenerationtraining}
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu, Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang~Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros~G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar. 2024.
\newblock \href {http://arxiv.org/abs/2406.11794} {DataComp-LM: In search of the next generation of training sets for language models}.

\bibitem[{Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim et~al.}]{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al. 2023.
\newblock Starcoder: may the source be with you!
\newblock \emph{arXiv preprint arXiv:2305.06161}.

\bibitem[{Liang et~al.(2023)Liang, Gonen, Mao, Hou, Goyal, Ghazvininejad, Zettlemoyer, and Khabsa}]{liang-etal-2023-xlm}
Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, and Madian Khabsa. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.emnlp-main.813} {{XLM}-{V}: Overcoming the Vocabulary Bottleneck in Multilingual Masked Language Models}.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 13142--13152, Singapore. Association for Computational Linguistics.

\bibitem[{Liao et~al.(2021)Liao, Huang, Wei, Zhang, Deng, and Yi}]{liao2021efficient}
Xianwen Liao, Yongzhong Huang, Changfu Wei, Chenhao Zhang, Yongqing Deng, and Ke~Yi. 2021.
\newblock Efficient estimate of low-frequency words’ embeddings based on the dictionary: a case study on Chinese.
\newblock \emph{Applied Sciences}, 11(22):11018.

\bibitem[{Lin et~al.(2024)Lin, Xu, Wu, Cui, Zhang, Mou, Song, Sun, and Wei}]{lin2024duquant}
Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, and Ying Wei. 2024.
\newblock DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs.
\newblock \emph{arXiv preprint arXiv:2406.01721}.

\bibitem[{Loshchilov and Hutter(2017)}]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter. 2017.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}.

\bibitem[{Lozhkov et~al.(2024)Lozhkov, Li, Allal, Cassano, Lamy-Poirier, Tazi, Tang, Pykhtar, Liu, Wei et~al.}]{lozhkov2024starcoder}
Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et~al. 2024.
\newblock StarCoder 2 and The Stack v2: The Next Generation.
\newblock \emph{arXiv preprint arXiv:2402.19173}.

\bibitem[{Luukkonen et~al.(2023)Luukkonen, Komulainen, Luoma, Eskelinen, Kanerva, Kupari, Ginter, Laippala, Muennighoff, Piktus et~al.}]{luukkonen2023fingpt}
Risto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-Mari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus, et~al. 2023.
\newblock Fingpt: Large generative models for a small language.
\newblock \emph{arXiv preprint arXiv:2311.05640}.

\bibitem[{Mei et~al.(2024)Mei, Tu, Delbracio, Talebi, Patel, and Milanfar}]{mei2024bigger}
Kangfu Mei, Zhengzhong Tu, Mauricio Delbracio, Hossein Talebi, Vishal~M Patel, and Peyman Milanfar. 2024.
\newblock Bigger is not Always Better: Scaling Properties of Latent Diffusion Models.
\newblock \emph{arXiv preprint arXiv:2404.01367}.

\bibitem[{{Meta AI}(2024)}]{metallama3blog}
{Meta AI}. 2024.
\newblock {Meta LLaMA-3: The most capable openly available LLM to date}.
\newblock \url{https://ai.meta.com/blog/meta-llama-3/}.

\bibitem[{Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal}]{mihaylov2018can}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock \emph{arXiv preprint arXiv:1809.02789}.

\bibitem[{Muennighoff et~al.(2023{\natexlab{a}})Muennighoff, Liu, Zebaze, Zheng, Hui, Zhuo, Singh, Tang, Von~Werra, and Longpre}]{muennighoff2023octopack}
Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry~Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von~Werra, and Shayne Longpre. 2023{\natexlab{a}}.
\newblock Octopack: Instruction tuning code large language models.
\newblock \emph{arXiv preprint arXiv:2308.07124}.

\bibitem[{Muennighoff et~al.(2024)Muennighoff, Rush, Barak, Le~Scao, Tazi, Piktus, Pyysalo, Wolf, and Raffel}]{muennighoff2024scaling}
Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le~Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin~A Raffel. 2024.
\newblock Scaling data-constrained language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Muennighoff et~al.(2023{\natexlab{b}})Muennighoff, Wang, Sutawika, Roberts, Biderman, Scao, Bari, Shen, Yong, Schoelkopf, Tang, Radev, Aji, Almubarak, Albanie, Alyafeai, Webson, Raff, and Raffel}]{muennighoff2023crosslingualgeneralizationmultitaskfinetuning}
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven~Le Scao, M~Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham~Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2211.01786} {Crosslingual Generalization through Multitask Finetuning}.

\bibitem[{Nvidia et~al.(2024)Nvidia, :, Adler, Agarwal, Aithal, Anh, Bhattacharya, Brundyn, Casper, Catanzaro, Clay, Cohen, Das, Dattagupta, Delalleau, Derczynski, Dong, Egert, Evans, Ficek, Fridman, Ghosh, Ginsburg, Gitman, Grzegorzek, Hero, Huang, Jawa, Jennings, Jhunjhunwala, Kamalu, Khan, Kuchaiev, LeGresley, Li, Liu, Liu, Long, Mahabaleshwarkar, Majumdar, Maki, Martinez, de~Melo, Moshkov, Narayanan, Narenthiran, Navarro, Nguyen, Nitski, Noroozi, Nutheti, Parisien, Parmar, Patwary, Pawelec, Ping, Prabhumoye, Roy, Saar, Sabavat, Satheesh, Scowcroft, Sewall, Shamis, Shen, Shoeybi, Sizer, Smelyanskiy, Soares, Sreedhar, Su, Subramanian, Sun, Toshniwal, Wang, Wang, You, Zeng, Zhang, Zhang, Zhang, Zhang, and Zhu}]{nvidia2024nemotron4}
Nvidia, :, Bo~Adler, Niket Agarwal, Ashwath Aithal, Dong~H. Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, Sirshak Das, Ayush Dattagupta, Olivier Delalleau, Leon Derczynski, Yi~Dong, Daniel Egert, Ellie Evans, Aleksander Ficek, Denys Fridman, Shaona Ghosh, Boris Ginsburg, Igor Gitman, Tomasz Grzegorzek, Robert Hero, Jining Huang, Vibhu Jawa, Joseph Jennings, Aastha Jhunjhunwala, John Kamalu, Sadaf Khan, Oleksii Kuchaiev, Patrick LeGresley, Hui Li, Jiwei Liu, Zihan Liu, Eileen Long, Ameya~Sunil Mahabaleshwarkar, Somshubra Majumdar, James Maki, Miguel Martinez, Maer~Rodrigues de~Melo, Ivan Moshkov, Deepak Narayanan, Sean Narenthiran, Jesus Navarro, Phong Nguyen, Osvald Nitski, Vahid Noroozi, Guruprasad Nutheti, Christopher Parisien, Jupinder Parmar, Mostofa Patwary, Krzysztof Pawelec, Wei Ping, Shrimai Prabhumoye, Rajarshi Roy, Trisha Saar, Vasanth Rao~Naik Sabavat, Sanjeev Satheesh, Jane~Polak Scowcroft, Jason Sewall, Pavel Shamis, Gerald Shen, Mohammad
  Shoeybi, Dave Sizer, Misha Smelyanskiy, Felipe Soares, Makesh~Narsimhan Sreedhar, Dan Su, Sandeep Subramanian, Shengyang Sun, Shubham Toshniwal, Hao Wang, Zhilin Wang, Jiaxuan You, Jiaqi Zeng, Jimmy Zhang, Jing Zhang, Vivienne Zhang, Yian Zhang, and Chen Zhu. 2024.
\newblock \href {http://arxiv.org/abs/2406.11704} {Nemotron-4 340B Technical Report}.
\newblock \emph{arxiv}.

\bibitem[{OpenAI et~al.(2023)OpenAI, Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat et~al.}]{achiam2023gpt}
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al. 2023.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray et~al.}]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al. 2022.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in neural information processing systems}, 35:27730--27744.

\bibitem[{Peng et~al.(2023)Peng, Alcaide, Anthony, Albalak, Arcadinho, Cao, Cheng, Chung, Grella, GV et~al.}]{peng2023rwkv}
Bo~Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi~Kiran GV, et~al. 2023.
\newblock Rwkv: Reinventing rnns for the transformer era.
\newblock \emph{arXiv preprint arXiv:2305.13048}.

\bibitem[{Peng et~al.(2024)Peng, Goldstein, Anthony, Albalak, Alcaide, Biderman, Cheah, Ferdinan, Hou, Kazienko et~al.}]{peng2024eagle}
Bo~Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemys{\l}aw Kazienko, et~al. 2024.
\newblock Eagle and Finch: RWKV with matrix-valued states and dynamic recurrence.
\newblock \emph{arXiv preprint arXiv:2404.05892}.

\bibitem[{Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever et~al.}]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et~al. 2018.
\newblock Improving language understanding by generative pre-training.

\bibitem[{Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song, Aslanides, Henderson, Ring, Young et~al.}]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et~al. 2021.
\newblock Scaling language models: Methods, analysis \& insights from training gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}.

\bibitem[{Ren et~al.(2023)Ren, Zhou, Meng, Huang, Wang, Wang, Li, Zhang, Podolskiy, Arshinov et~al.}]{ren2023pangu}
Xiaozhe Ren, Pingyi Zhou, Xinfan Meng, Xinjing Huang, Yadao Wang, Weichao Wang, Pengfei Li, Xiaoda Zhang, Alexander Podolskiy, Grigory Arshinov, et~al. 2023.
\newblock Pangu-$\{$$\backslash$Sigma$\}$: Towards trillion parameter language model with sparse heterogeneous computing.
\newblock \emph{arXiv preprint arXiv:2303.10845}.

\bibitem[{Roh et~al.(2020)Roh, Oh, and Lee}]{roh2020unigram}
Jihyeon Roh, Sang-Hoon Oh, and Soo-Young Lee. 2020.
\newblock Unigram-normalized perplexity as a language model performance measure with different vocabulary sizes.
\newblock \emph{arXiv preprint arXiv:2011.13220}.

\bibitem[{Ruan et~al.(2024)Ruan, Maddison, and Hashimoto}]{ruan2024observational}
Yangjun Ruan, Chris~J. Maddison, and Tatsunori Hashimoto. 2024.
\newblock \href {http://arxiv.org/abs/2405.10938} {Observational Scaling Laws and the Predictability of Language Model Performance}.

\bibitem[{Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi}]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64(9):99--106.

\bibitem[{Sardana and Frankle(2023)}]{sardana2023beyond}
Nikhil Sardana and Jonathan Frankle. 2023.
\newblock Beyond chinchilla-optimal: Accounting for inference in language model scaling laws.
\newblock \emph{arXiv preprint arXiv:2401.00448}.

\bibitem[{Scao et~al.(2022)Scao, Wang, Hesslow, Saulnier, Bekman, Bari, Biderman, Elsahar, Muennighoff, Phang et~al.}]{scao2022language}
Teven~Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M~Saiful Bari, Stella Biderman, Hady Elsahar, Niklas Muennighoff, Jason Phang, et~al. 2022.
\newblock What language model to train if you have one million gpu hours?
\newblock \emph{arXiv preprint arXiv:2210.15424}.

\bibitem[{Sennrich et~al.(2016)Sennrich, Haddow, and Birch}]{bpe2016}
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.
\newblock \href {https://doi.org/10.18653/V1/P16-1162} {Neural Machine Translation of Rare Words with Subword Units}.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, {ACL} 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers}. The Association for Computer Linguistics.

\bibitem[{Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro}]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}.

\bibitem[{Soboleva et~al.(2023)Soboleva, Al-Khateeb, Myers, Steeves, Hestness, and Dey}]{soboleva2023slimpajama}
Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob~R Steeves, Joel Hestness, and Nolan Dey. 2023.
\newblock SlimPajama: A 627B token cleaned and deduplicated version of RedPajama.

\bibitem[{Soldaini et~al.(2024)Soldaini, Kinney, Bhagia, Schwenk, Atkinson, Authur, Bogin, Chandu, Dumas, Elazar et~al.}]{soldaini2024dolma}
Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et~al. 2024.
\newblock Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research.
\newblock \emph{arXiv preprint arXiv:2402.00159}.

\bibitem[{Takahashi and Tanaka-Ishii(2017)}]{takahashi2017neural}
Shuntaro Takahashi and Kumiko Tanaka-Ishii. 2017.
\newblock Do neural nets learn statistical laws behind natural language?
\newblock \emph{PloS one}, 12(12):e0189326.

\bibitem[{Tao et~al.(2023)Tao, Hou, Bai, Wei, Jiang, Liu, Luo, and Wong}]{tao2023structured}
Chaofan Tao, Lu~Hou, Haoli Bai, Jiansheng Wei, Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong. 2023.
\newblock Structured pruning for efficient generative pre-trained language models.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pages 10880--10895.

\bibitem[{Tao et~al.(2022)Tao, Hou, Zhang, Shang, Jiang, Liu, Luo, and Wong}]{tao2022compression}
Chaofan Tao, Lu~Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, and Ngai Wong. 2022.
\newblock Compression of generative pre-trained language models via quantization.
\newblock \emph{arXiv preprint arXiv:2203.10705}.

\bibitem[{Tay et~al.(2023)Tay, Dehghani, Abnar, Chung, Fedus, Rao, Narang, Tran, Yogatama, and Metzler}]{tay-etal-2023-scaling}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Hyung Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Tran, Dani Yogatama, and Donald Metzler. 2023.
\newblock \href {https://doi.org/10.18653/v1/2023.findings-emnlp.825} {Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?}
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 12342--12364, Singapore. Association for Computational Linguistics.

\bibitem[{Team et~al.(2024)Team, Mesnard, Hardin, Dadashi, Bhupatiraju, Pathak, Sifre, Rivi{\`e}re, Kale, Love et~al.}]{team2024gemma}
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale, Juliette Love, et~al. 2024.
\newblock Gemma: Open models based on gemini research and technology.
\newblock \emph{arXiv preprint arXiv:2403.08295}.

\bibitem[{Tian et~al.(2024)Tian, Jiang, Yuan, Peng, and Wang}]{tian2024visual}
Keyu Tian, Yi~Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. 2024.
\newblock Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction.
\newblock \emph{arXiv preprint arXiv:2404.02905}.

\bibitem[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama1}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}.

\bibitem[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages 5998--6008.

\bibitem[{Villalobos et~al.(2022)Villalobos, Sevilla, Heim, Besiroglu, Hobbhahn, and Ho}]{villalobos2022will}
Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and Anson Ho. 2022.
\newblock Will we run out of data? an analysis of the limits of scaling datasets in machine learning.
\newblock \emph{arXiv preprint arXiv:2211.04325}.

\bibitem[{Wan et~al.(2023)Wan, Wang, Liu, Alam, Zheng, Qu, Yan, Zhu, Zhang, Chowdhury et~al.}]{wan2023efficient}
Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu~Zheng, Zhongnan Qu, Shen Yan, Yi~Zhu, Quanlu Zhang, Mosharaf Chowdhury, et~al. 2023.
\newblock Efficient large language models: A survey.
\newblock \emph{arXiv preprint arXiv:2312.03863}, 1.

\bibitem[{Wan et~al.(2024)Wan, Wu, Zhang, Xin, Tao, Zhu, Wang, Luo, Xiong, and Zhang}]{wan2024d2o}
Zhongwei Wan, Xinjian Wu, Yu~Zhang, Yi~Xin, Chaofan Tao, Zhihong Zhu, Xin Wang, Siqi Luo, Jing Xiong, and Mi~Zhang. 2024.
\newblock D2O: Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models.
\newblock \emph{arXiv preprint arXiv:2406.13035}.

\bibitem[{Wang et~al.(2019)Wang, Yu, Sun, Chen, and Yu}]{wang2019improving}
Hai Wang, Dian Yu, Kai Sun, Janshu Chen, and Dong Yu. 2019.
\newblock Improving pre-trained multilingual models with vocabulary expansion.
\newblock \emph{arXiv preprint arXiv:1909.12440}.

\bibitem[{Wang et~al.(2023)Wang, Yuan, Liu, Shen, Yin, Xiong, Xie, Shi, Li, Li et~al.}]{wang2023dt}
Haiming Wang, Ye~Yuan, Zhengying Liu, Jianhao Shen, Yichun Yin, Jing Xiong, Enze Xie, Han Shi, Yujun Li, Lin Li, et~al. 2023.
\newblock Dt-solver: Automated theorem proving with dynamic-tree sampling guided by proof-level value function.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 12632--12646.

\bibitem[{Wang et~al.(2024)Wang, Gangavarapu, Yan, and Rush}]{wang2024mambabyte}
Junxiong Wang, Tushaar Gangavarapu, Jing~Nathan Yan, and Alexander~M Rush. 2024.
\newblock Mambabyte: Token-free selective state space model.
\newblock \emph{arXiv preprint arXiv:2401.13660}.

\bibitem[{Wu et~al.(2024)Wu, Tao, Wang, Zhao, and Wong}]{wu2024rethinking}
Taiqiang Wu, Chaofan Tao, Jiahao Wang, Zhe Zhao, and Ngai Wong. 2024.
\newblock Rethinking Kullback-Leibler Divergence in Knowledge Distillation for Large Language Models.
\newblock \emph{arXiv preprint arXiv:2404.02657}.

\bibitem[{Xu et~al.(2020)Xu, Zhou, Gan, Zheng, and Li}]{xu2020vocabulary}
Jingjing Xu, Hao Zhou, Chun Gan, Zaixiang Zheng, and Lei Li. 2020.
\newblock Vocabulary learning via optimal transport for neural machine translation.
\newblock \emph{arXiv preprint arXiv:2012.15671}.

\bibitem[{Xue et~al.(2024)Xue, Fu, Zhou, Zheng, and You}]{xue2024repeat}
Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You. 2024.
\newblock To repeat or not to repeat: Insights from scaling llm under token-crisis.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Yang et~al.(2024)Yang, Yang, Hui, Zheng, Yu, Zhou, Li, Li, Liu, Huang, Dong, Wei, Lin, Tang, Wang, Yang, Tu, Zhang, Ma, Xu, Zhou, Bai, He, Lin, Dang, Lu, Chen, Yang, Li, Xue, Ni, Zhang, Wang, Peng, Men, Gao, Lin, Wang, Bai, Tan, Zhu, Li, Liu, Ge, Deng, Zhou, Ren, Zhang, Wei, Ren, Fan, Yao, Zhang, Wan, Chu, Liu, Cui, Zhang, and Fan}]{yang2024qwen2technicalreport}
An~Yang, Baosong Yang, Binyuan Hui, Bo~Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na~Ni, Pei Zhang, Peng Wang, Ru~Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu~Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024.
\newblock \href {http://arxiv.org/abs/2407.10671} {Qwen2 Technical Report}.
\newblock \emph{arxiv}.

\bibitem[{Yu et~al.(2024)Yu, Simig, Flaherty, Aghajanyan, Zettlemoyer, and Lewis}]{yu2024megabyte}
Lili Yu, D{\'a}niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. 2024.
\newblock Megabyte: Predicting million-byte sequences with multiscale transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 36.

\bibitem[{Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi}]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}.

\bibitem[{Zhang et~al.(2024)Zhang, Zeng, Wang, and Lu}]{zhang2024tinyllama}
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024.
\newblock Tinyllama: An open-source small language model.
\newblock \emph{arXiv preprint arXiv:2401.02385}.

\bibitem[{Zheng et~al.(2021)Zheng, Dong, Huang, Singhal, Che, Liu, Song, and Wei}]{zheng-etal-2021-allocating}
Bo~Zheng, Li~Dong, Shaohan Huang, Saksham Singhal, Wanxiang Che, Ting Liu, Xia Song, and Furu Wei. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.emnlp-main.257} {Allocating Large Vocabulary Capacity for Cross-Lingual Language Model Pre-Training}.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 3203--3215, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

\bibitem[{Zhuo et~al.(2024{\natexlab{a}})Zhuo, Vu, Chim, Hu, Yu, Widyasari, Yusuf, Zhan, He, Paul, Brunner, Gong, Hoang, Zebaze, Hong, Li, Kaddour, Xu, Zhang, Yadav, Jain, Gu, Cheng, Liu, Liu, Wang, Lo, Hui, Muennighoff, Fried, Du, de~Vries, and Werra}]{zhuo2024bigcodebenchbenchmarkingcodegeneration}
Terry~Yue Zhuo, Minh~Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur~Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, Simon Brunner, Chen Gong, Thong Hoang, Armel~Randy Zebaze, Xiaoheng Hong, Wen-Ding Li, Jean Kaddour, Ming Xu, Zhihan Zhang, Prateek Yadav, Naman Jain, Alex Gu, Zhoujun Cheng, Jiawei Liu, Qian Liu, Zijian Wang, David Lo, Binyuan Hui, Niklas Muennighoff, Daniel Fried, Xiaoning Du, Harm de~Vries, and Leandro~Von Werra. 2024{\natexlab{a}}.
\newblock \href {http://arxiv.org/abs/2406.15877} {BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions}.

\bibitem[{Zhuo et~al.(2024{\natexlab{b}})Zhuo, Zebaze, Suppattarachai, von Werra, de~Vries, Liu, and Muennighoff}]{zhuo2024astraiosparameterefficientinstructiontuning}
Terry~Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de~Vries, Qian Liu, and Niklas Muennighoff. 2024{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2401.00788} {Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models}.

\bibitem[{Üstün et~al.(2024)Üstün, Aryabumi, Yong, Ko, D'souza, Onilude, Bhandari, Singh, Ooi, Kayid, Vargus, Blunsom, Longpre, Muennighoff, Fadaee, Kreutzer, and Hooker}]{üstün2024ayamodelinstructionfinetuned}
Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D'souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024.
\newblock \href {http://arxiv.org/abs/2402.07827} {Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model}.

\end{thebibliography}
