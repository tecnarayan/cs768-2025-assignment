\begin{thebibliography}{90}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abrahams and Thomas(1984)]{abrahams1984note}
J.~Abrahams and J.~B. Thomas.
\newblock A note on the characterization of bivariate densities by conditional
  densities.
\newblock \emph{Communications in Statistics-Theory and Methods}, 13\penalty0
  (3):\penalty0 395--400, 1984.

\bibitem[Alain and Bengio(2014)]{alain2014regularized}
G.~Alain and Y.~Bengio.
\newblock What regularized auto-encoders learn from the data-generating
  distribution.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 3563--3593, 2014.

\bibitem[Ardizzone et~al.(2019)Ardizzone, Kruse, Rother, and
  K{\"o}the]{ardizzone2019analyzing}
L.~Ardizzone, J.~Kruse, C.~Rother, and U.~K{\"o}the.
\newblock Analyzing inverse problems with invertible neural networks.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR 2019)}, 2019.

\bibitem[Arnold and Press(1989)]{arnold1989compatible}
B.~C. Arnold and S.~J. Press.
\newblock Compatible conditional distributions.
\newblock \emph{Journal of the American Statistical Association}, 84\penalty0
  (405):\penalty0 152--156, 1989.

\bibitem[Arnold et~al.(2001)Arnold, Castillo, Sarabia,
  et~al.]{arnold2001conditionally}
B.~C. Arnold, E.~Castillo, J.~M. Sarabia, et~al.
\newblock Conditionally specified distributions: an introduction.
\newblock \emph{Statistical Science}, 16\penalty0 (3):\penalty0 249--274, 2001.

\bibitem[Arnold et~al.(2012)Arnold, Castillo, and
  Alegria]{arnold2012conditionally}
B.~C. Arnold, E.~Castillo, and J.-M.~S. Alegria.
\newblock \emph{Conditionally specified distributions}, volume~73.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Baldi and Hornik(1989)]{baldi1989neural}
P.~Baldi and K.~Hornik.
\newblock Neural networks and principal component analysis: Learning from
  examples without local minima.
\newblock \emph{Neural Networks}, 2\penalty0 (1):\penalty0 53--58, 1989.

\bibitem[Behrmann et~al.(2019)Behrmann, Grathwohl, Chen, Duvenaud, and
  Jacobsen]{behrmann2019invertible}
J.~Behrmann, W.~Grathwohl, R.~T. Chen, D.~Duvenaud, and J.-H. Jacobsen.
\newblock Invertible residual networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  573--582. PMLR, 2019.

\bibitem[Bengio et~al.(2013)Bengio, Yao, Alain, and
  Vincent]{bengio2013generalized}
Y.~Bengio, L.~Yao, G.~Alain, and P.~Vincent.
\newblock Generalized denoising auto-encoders as generative models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2013.

\bibitem[Bengio et~al.(2014)Bengio, Laufer, Alain, and
  Yosinski]{bengio2014deep}
Y.~Bengio, E.~Laufer, G.~Alain, and J.~Yosinski.
\newblock Deep generative stochastic networks trainable by backprop.
\newblock In \emph{International Conference on Machine Learning}, pages
  226--234, 2014.

\bibitem[Berti et~al.(2014)Berti, Dreassi, and Rigo]{berti2014compatibility}
P.~Berti, E.~Dreassi, and P.~Rigo.
\newblock Compatibility results for conditional distributions.
\newblock \emph{Journal of Multivariate Analysis}, 125:\penalty0 190--203,
  2014.

\bibitem[Bhattacharyya(1943)]{bhattacharyya1943some}
A.~Bhattacharyya.
\newblock On some sets of sufficient conditions leading to the normal bivariate
  distribution.
\newblock \emph{Sankhy{\=a}: The Indian Journal of Statistics}, pages 399--406,
  1943.

\bibitem[Billingsley(2012)]{billingsley2012probability}
P.~Billingsley.
\newblock \emph{Probability and Measure}.
\newblock John Wiley \& Sons, New Jersey, 2012.
\newblock ISBN 978-1-118-12237-2.

\bibitem[Bornschein et~al.(2016)Bornschein, Shabanian, Fischer, and
  Bengio]{bornschein2016bidirectional}
J.~Bornschein, S.~Shabanian, A.~Fischer, and Y.~Bengio.
\newblock Bidirectional {H}elmholtz machines.
\newblock In \emph{International Conference on Machine Learning}, pages
  2511--2519. PMLR, 2016.

\bibitem[Bowman et~al.(2016)Bowman, Vilnis, Vinyals, Dai, Jozefowicz, and
  Bengio]{bowman2016generating}
S.~Bowman, L.~Vilnis, O.~Vinyals, A.~Dai, R.~Jozefowicz, and S.~Bengio.
\newblock Generating sentences from a continuous space.
\newblock In \emph{Proceedings of The 20th SIGNLL Conference on Computational
  Natural Language Learning}, pages 10--21, 2016.

\bibitem[Brehmer and Cranmer(2020)]{brehmer2020flows}
J.~Brehmer and K.~Cranmer.
\newblock Flows for simultaneous manifold learning and density estimation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[Chen et~al.(2018)Chen, Zhang, Wang, Li, and Chen]{chen2018unified}
C.~Chen, R.~Zhang, W.~Wang, B.~Li, and L.~Chen.
\newblock A unified particle-optimization framework for scalable {B}ayesian
  sampling.
\newblock In \emph{Proceedings of the Conference on Uncertainty in Artificial
  Intelligence (UAI 2018)}, Monterey, California USA, 2018. Association for
  Uncertainty in Artificial Intelligence.

\bibitem[Chen et~al.(2019)Chen, Behrmann, Duvenaud, and
  Jacobsen]{chen2019residual}
R.~T. Chen, J.~Behrmann, D.~Duvenaud, and J.-H. Jacobsen.
\newblock Residual flows for invertible generative modeling.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Chen et~al.(2014)Chen, Fox, and Guestrin]{chen2014stochastic}
T.~Chen, E.~Fox, and C.~Guestrin.
\newblock Stochastic gradient {H}amiltonian {M}onte {C}arlo.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning (ICML 2014)}, pages 1683--1691, Beijing, China, 2014. IMLS.

\bibitem[Coors et~al.(2018)Coors, Condurache, and Geiger]{coors2018spherenet}
B.~Coors, A.~P. Condurache, and A.~Geiger.
\newblock {SphereNet}: Learning spherical representations for detection and
  classification in omnidirectional images.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 518--533, 2018.

\bibitem[Dai and Wipf(2019)]{dai2019diagnosing}
B.~Dai and D.~Wipf.
\newblock Diagnosing and enhancing {VAE} models.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR 2019)}, 2019.

\bibitem[Davidson et~al.(2018)Davidson, Falorsi, De~Cao, Kipf, and
  Tomczak]{davidson2018hyperspherical}
T.~R. Davidson, L.~Falorsi, N.~De~Cao, T.~Kipf, and J.~M. Tomczak.
\newblock Hyperspherical variational auto-encoders.
\newblock In \emph{Proceedings of the Conference on Uncertainty in Artificial
  Intelligence (UAI 2018)}, 2018.

\bibitem[Ding et~al.(2014)Ding, Fang, Babbush, Chen, Skeel, and
  Neven]{ding2014bayesian}
N.~Ding, Y.~Fang, R.~Babbush, C.~Chen, R.~D. Skeel, and H.~Neven.
\newblock {B}ayesian sampling using stochastic gradient thermostats.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3203--3211, Montréal, Canada, 2014. NIPS Foundation.

\bibitem[Dinh et~al.(2015)Dinh, Krueger, and Bengio]{dinh2015nice}
L.~Dinh, D.~Krueger, and Y.~Bengio.
\newblock {NICE}: Non-linear independent components estimation.
\newblock In \emph{Workshop on the International Conference on Learning
  Representations}, 2015.

\bibitem[Donahue et~al.(2017)Donahue, Kr{\"a}henb{\"u}hl, and
  Darrell]{donahue2017adversarial}
J.~Donahue, P.~Kr{\"a}henb{\"u}hl, and T.~Darrell.
\newblock Adversarial feature learning.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR 2017)}, 2017.

\bibitem[Du and Mordatch(2019)]{du2019implicit}
Y.~Du and I.~Mordatch.
\newblock Implicit generation and modeling with energy based models.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.

\bibitem[Dumoulin et~al.(2017)Dumoulin, Belghazi, Poole, Mastropietro, Lamb,
  Arjovsky, and Courville]{dumoulin2017adversarially}
V.~Dumoulin, I.~Belghazi, B.~Poole, O.~Mastropietro, A.~Lamb, M.~Arjovsky, and
  A.~Courville.
\newblock Adversarially learned inference.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR 2017)}, 2017.

\bibitem[Falorsi et~al.(2018)Falorsi, de~Haan, Davidson, De~Cao, Weiler,
  Forr{\'e}, and Cohen]{falorsi2018explorations}
L.~Falorsi, P.~de~Haan, T.~R. Davidson, N.~De~Cao, M.~Weiler, P.~Forr{\'e}, and
  T.~S. Cohen.
\newblock Explorations in homeomorphic variational auto-encoding.
\newblock \emph{arXiv preprint arXiv:1807.04689}, 2018.

\bibitem[Galambos(1995)]{galambos1995advanced}
J.~Galambos.
\newblock \emph{Advanced probability theory}, volume~10.
\newblock CRC Press, 1995.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2672--2680, Montréal, Canada, 2014. NIPS Foundation.

\bibitem[Grathwohl et~al.(2019)Grathwohl, Chen, Bettencourt, Sutskever, and
  Duvenaud]{grathwohl2019ffjord}
W.~Grathwohl, R.~T. Chen, J.~Bettencourt, I.~Sutskever, and D.~Duvenaud.
\newblock {FFJORD}: Free-form continuous dynamics for scalable reversible
  generative models.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR 2019)}, 2019.

\bibitem[Grover and Ermon(2019)]{grover2019uncertainty}
A.~Grover and S.~Ermon.
\newblock Uncertainty autoencoders: Learning compressed representations via
  variational information maximization.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 2514--2524. PMLR, 2019.

\bibitem[He et~al.(2016)He, Xia, Qin, Wang, Yu, Liu, and Ma]{he2016dual}
D.~He, Y.~Xia, T.~Qin, L.~Wang, N.~Yu, T.-Y. Liu, and W.-Y. Ma.
\newblock Dual learning for machine translation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  820--828, 2016.

\bibitem[He et~al.(2019)He, Spokoyny, Neubig, and
  Berg-Kirkpatrick]{he2019lagging}
J.~He, D.~Spokoyny, G.~Neubig, and T.~Berg-Kirkpatrick.
\newblock Lagging inference networks and posterior collapse in variational
  autoencoders.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR 2019)}, 2019.

\bibitem[Heckerman et~al.(2000)Heckerman, Chickering, Meek, Rounthwaite, and
  Kadie]{heckerman2000dependency}
D.~Heckerman, D.~M. Chickering, C.~Meek, R.~Rounthwaite, and C.~Kadie.
\newblock Dependency networks for inference, collaborative filtering, and data
  visualization.
\newblock \emph{Journal of Machine Learning Research}, 1\penalty0
  (Oct):\penalty0 49--75, 2000.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
M.~Heusel, H.~Ramsauer, T.~Unterthiner, B.~Nessler, and S.~Hochreiter.
\newblock {GANs} trained by a two time-scale update rule converge to a local
  {N}ash equilibrium.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30. Curran Associates, Inc., 2017.

\bibitem[Higgins et~al.(2017)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{higgins2017beta}
I.~Higgins, L.~Matthey, A.~Pal, C.~Burgess, X.~Glorot, M.~Botvinick,
  S.~Mohamed, and A.~Lerchner.
\newblock Beta-{VAE}: Learning basic visual concepts with a constrained
  variational framework.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR 2017)}, 2017.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
J.~Ho, A.~Jain, and P.~Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Hofmann and Tresp(1998)]{hofmann1998nonlinear}
R.~Hofmann and V.~Tresp.
\newblock Nonlinear {M}arkov networks for continuous variables.
\newblock \emph{Advances in Neural Information Processing Systems}, pages
  521--527, 1998.

\bibitem[Hutchinson(1989)]{hutchinson1989stochastic}
M.~F. Hutchinson.
\newblock A stochastic estimator of the trace of the influence matrix for
  laplacian smoothing splines.
\newblock \emph{Communications in Statistics-Simulation and Computation},
  18\penalty0 (3):\penalty0 1059--1076, 1989.

\bibitem[Kalatzis et~al.(2020)Kalatzis, Eklund, Arvanitidis, and
  Hauberg]{kalatzis2020variational}
D.~Kalatzis, D.~Eklund, G.~Arvanitidis, and S.~Hauberg.
\newblock Variational autoencoders with riemannian brownian motion priors.
\newblock In \emph{International Conference on Machine Learning}, pages
  5053--5066. PMLR, 2020.

\bibitem[Kim et~al.(2017)Kim, Cha, Kim, Lee, and Kim]{kim2017learning}
T.~Kim, M.~Cha, H.~Kim, J.~K. Lee, and J.~Kim.
\newblock Learning to discover cross-domain relations with generative
  adversarial networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1857--1865. JMLR.org, 2017.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma and Dhariwal(2018)]{kingma2018glow}
D.~P. Kingma and P.~Dhariwal.
\newblock Glow: generative flow with invertible 1$\times$ 1 convolutions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10236--10245, 2018.

\bibitem[Kingma and Welling(2014)]{kingma2014auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational {B}ayes.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR 2014)}, Banff, Canada, 2014. ICLR Committee.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, Jozefowicz, Chen, Sutskever, and
  Welling]{kingma2016improved}
D.~P. Kingma, T.~Salimans, R.~Jozefowicz, X.~Chen, I.~Sutskever, and
  M.~Welling.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4743--4751, Barcelona, Spain, 2016. NIPS Foundation.

\bibitem[Klenke(2013)]{klenke2013probability}
A.~Klenke.
\newblock \emph{Probability theory: a comprehensive course}.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Kocaoglu et~al.(2018)Kocaoglu, Snyder, Dimakis, and
  Vishwanath]{kocaoglu2018causalgan}
M.~Kocaoglu, C.~Snyder, A.~G. Dimakis, and S.~Vishwanath.
\newblock {CausalGAN}: Learning causal implicit generative models with
  adversarial training.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2018.

\bibitem[Kong et~al.(2020)Kong, Ping, Huang, Zhao, and
  Catanzaro]{kong2020diffwave}
Z.~Kong, W.~Ping, J.~Huang, K.~Zhao, and B.~Catanzaro.
\newblock {DiffWave}: A versatile diffusion model for audio synthesis.
\newblock \emph{arXiv preprint arXiv:2009.09761}, 2020.

\bibitem[Lamb et~al.(2017)Lamb, Hjelm, Ganin, Cohen, Courville, and
  Bengio]{lamb2017gibbsnet}
A.~M. Lamb, D.~Hjelm, Y.~Ganin, J.~P. Cohen, A.~C. Courville, and Y.~Bengio.
\newblock {GibbsNet}: Iterative adversarial inference for deep graphical
  models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5089--5098, 2017.

\bibitem[LeCun et~al.(1989)LeCun, Boser, Denker, Henderson, Howard, Hubbard,
  and Jackel]{lecun1989backpropagation}
Y.~LeCun, B.~Boser, J.~S. Denker, D.~Henderson, R.~E. Howard, W.~Hubbard, and
  L.~D. Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock \emph{Neural computation}, 1\penalty0 (4):\penalty0 541--551, 1989.

\bibitem[Li et~al.(2018)Li, Welling, Zhu, and Zhang]{li2018graphical}
C.~Li, M.~Welling, J.~Zhu, and B.~Zhang.
\newblock Graphical generative adversarial networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Lin et~al.(2019)Lin, Chen, Xia, Liu, Qin, and Luo]{lin2019exploring}
J.~Lin, Z.~Chen, Y.~Xia, S.~Liu, T.~Qin, and J.~Luo.
\newblock Exploring explicit domain supervision for latent space
  disentanglement in unpaired image-to-image translation.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 2019.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, Zhuo, Cheng, Zhang, Zhu, and
  Carin]{liu2019understanding_a}
C.~Liu, J.~Zhuo, P.~Cheng, R.~Zhang, J.~Zhu, and L.~Carin.
\newblock Understanding and accelerating particle-based variational inference.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97, pages
  4082--4092, Long Beach, California USA, 2019{\natexlab{a}}. IMLS, PMLR.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, Zhuo, and
  Zhu]{liu2019understanding_b}
C.~Liu, J.~Zhuo, and J.~Zhu.
\newblock Understanding {MCMC} dynamics as flows on the {W}asserstein space.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97, pages
  4093--4103, Long Beach, California USA, 2019{\natexlab{b}}. IMLS, PMLR.

\bibitem[Liu and Wang(2016)]{liu2016stein}
Q.~Liu and D.~Wang.
\newblock {S}tein variational gradient descent: A general purpose {B}ayesian
  inference algorithm.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2370--2378, Barcelona, Spain, 2016. NIPS Foundation.

\bibitem[Ma et~al.(2021)Ma, Chatterji, Cheng, Flammarion, Bartlett, and
  Jordan]{ma2021there}
Y.-A. Ma, N.~S. Chatterji, X.~Cheng, N.~Flammarion, P.~L. Bartlett, and M.~I.
  Jordan.
\newblock Is there an analog of {N}esterov acceleration for gradient-based
  {MCMC}?
\newblock \emph{Bernoulli}, 27\penalty0 (3):\penalty0 1942--1992, 2021.

\bibitem[Nakagawa et~al.(2021)Nakagawa, Kato, and
  Suzuki]{nakagawa2021quantitative}
A.~Nakagawa, K.~Kato, and T.~Suzuki.
\newblock Quantitative understanding of {VAE} as a non-linearly scaled
  isometric embedding.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, 2021.

\bibitem[Pang et~al.(2020)Pang, Han, Nijkamp, Zhu, and Wu]{pang2020learning}
B.~Pang, T.~Han, E.~Nijkamp, S.-C. Zhu, and Y.~N. Wu.
\newblock Learning latent space energy-based prior model.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[Papamakarios et~al.(2017)Papamakarios, Pavlakou, and
  Murray]{papamakarios2017masked}
G.~Papamakarios, T.~Pavlakou, and I.~Murray.
\newblock Masked autoregressive flow for density estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2335--2344, 2017.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, et~al.
\newblock {PyTorch}: An imperative style, high-performance deep learning
  library.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 8026--8037, 2019.

\bibitem[Peters et~al.(2014)Peters, Mooij, Janzing, and
  Sch{\"o}lkopf]{peters2014causal}
J.~Peters, J.~M. Mooij, D.~Janzing, and B.~Sch{\"o}lkopf.
\newblock Causal discovery with continuous additive noise models.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 2009--2053, 2014.

\bibitem[Ranzato et~al.(2007)Ranzato, Boureau, and LeCun]{ranzato2007sparse}
M.~Ranzato, Y.-L. Boureau, and Y.~LeCun.
\newblock Sparse feature learning for deep belief networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  20:\penalty0 1185--1192, 2007.

\bibitem[Razavi et~al.(2019)Razavi, Oord, Poole, and
  Vinyals]{razavi2019preventing}
A.~Razavi, A.~v.~d. Oord, B.~Poole, and O.~Vinyals.
\newblock Preventing posterior collapse with delta-{VAEs}.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR 2019)}, 2019.

\bibitem[Ren et~al.(2020)Ren, Hu, Tan, Qin, Zhao, Zhao, and
  Liu]{ren2020fastspeech}
Y.~Ren, C.~Hu, X.~Tan, T.~Qin, S.~Zhao, Z.~Zhao, and T.-Y. Liu.
\newblock Fastspeech 2: Fast and high-quality end-to-end text to speech.
\newblock \emph{arXiv preprint arXiv:2006.04558}, 2020.

\bibitem[Rezende and Mohamed(2015)]{rezende2015variational}
D.~Rezende and S.~Mohamed.
\newblock Variational inference with normalizing flows.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning (ICML 2015)}, pages 1530--1538, Lille, France, 2015. IMLS.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
D.~J. Rezende, S.~Mohamed, and D.~Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{International Conference on Machine Learning}, pages
  1278--1286, 2014.

\bibitem[Rifai et~al.(2011)Rifai, Vincent, Muller, Glorot, and
  Bengio]{rifai2011contractive}
S.~Rifai, P.~Vincent, X.~Muller, X.~Glorot, and Y.~Bengio.
\newblock Contractive auto-encoders: Explicit invariance during feature
  extraction.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2011.

\bibitem[Rifai et~al.(2012)Rifai, Bengio, Dauphin, and
  Vincent]{rifai2012generative}
S.~Rifai, Y.~Bengio, Y.~N. Dauphin, and P.~Vincent.
\newblock A generative process for sampling contractive auto-encoders.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, pages 1811--1818, 2012.

\bibitem[Rinaldo(2018)]{rinaldo2018advanced}
A.~Rinaldo.
\newblock Advanced probability, February 2018.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{rumelhart1986learning}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Nature}, 323\penalty0 (6088):\penalty0 533--536, 1986.

\bibitem[Seitzer(2020)]{seitzer2020fid}
M.~Seitzer.
\newblock {pytorch-fid: FID Score for PyTorch}.
\newblock \url{https://github.com/mseitzer/pytorch-fid}, August 2020.
\newblock Version 0.2.1.

\bibitem[Shao et~al.(2018)Shao, Kumar, and Thomas~Fletcher]{shao2018riemannian}
H.~Shao, A.~Kumar, and P.~Thomas~Fletcher.
\newblock The {R}iemannian geometry of deep generative models.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 315--323, 2018.

\bibitem[Sohl-Dickstein et~al.(2015)Sohl-Dickstein, Weiss, Maheswaranathan, and
  Ganguli]{sohl2015deep}
J.~Sohl-Dickstein, E.~Weiss, N.~Maheswaranathan, and S.~Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In \emph{International Conference on Machine Learning}, pages
  2256--2265. PMLR, 2015.

\bibitem[Song and Ermon(2019)]{song2019generative}
Y.~Song and S.~Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.

\bibitem[Song et~al.(2021)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and
  Poole]{song2021score}
Y.~Song, J.~Sohl-Dickstein, D.~P. Kingma, A.~Kumar, S.~Ermon, and B.~Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR 2021)}, 2021.

\bibitem[Taghvaei and Mehta(2019)]{taghvaei2019accelerated}
A.~Taghvaei and P.~Mehta.
\newblock Accelerated flow for probability distributions.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pages 6076--6085. PMLR, 2019.

\bibitem[Teshima et~al.(2020)Teshima, Ishikawa, Tojo, Oono, Ikeda, and
  Sugiyama]{teshima2020coupling}
T.~Teshima, I.~Ishikawa, K.~Tojo, K.~Oono, M.~Ikeda, and M.~Sugiyama.
\newblock Coupling-based invertible neural networks are universal
  diffeomorphism approximators.
\newblock \emph{arXiv preprint arXiv:2006.11469}, 2020.

\bibitem[Vahdat and Kautz(2020)]{vahdat2020nvae}
A.~Vahdat and J.~Kautz.
\newblock {NVAE}: A deep hierarchical variational autoencoder.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Van Den~Berg et~al.(2018)Van Den~Berg, Hasenclever, Tomczak, and
  Welling]{van2018sylvester}
R.~Van Den~Berg, L.~Hasenclever, J.~M. Tomczak, and M.~Welling.
\newblock Sylvester normalizing flows for variational inference.
\newblock In \emph{Proceedings of the Conference on Uncertainty in Artificial
  Intelligence}, pages 393--402. Association For Uncertainty in Artificial
  Intelligence (AUAI), 2018.

\bibitem[Vincent(2011)]{vincent2011connection}
P.~Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural Computation}, 23\penalty0 (7):\penalty0 1661--1674,
  2011.

\bibitem[Vincent et~al.(2008)Vincent, Larochelle, Bengio, and
  Manzagol]{vincent2008extracting}
P.~Vincent, H.~Larochelle, Y.~Bengio, and P.-A. Manzagol.
\newblock Extracting and composing robust features with denoising autoencoders.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, pages 1096--1103, 2008.

\bibitem[Welling and Teh(2011)]{welling2011bayesian}
M.~Welling and Y.~W. Teh.
\newblock {B}ayesian learning via stochastic gradient {L}angevin dynamics.
\newblock In \emph{Proceedings of the 28th International Conference on Machine
  Learning (ICML 2011)}, pages 681--688, Bellevue, Washington USA, 2011. IMLS.

\bibitem[Xia et~al.(2017{\natexlab{a}})Xia, Bian, Qin, Yu, and
  Liu]{xia2017duali}
Y.~Xia, J.~Bian, T.~Qin, N.~Yu, and T.-Y. Liu.
\newblock Dual inference for machine learning.
\newblock In \emph{Proceedings of the 26th International Joint Conference on
  Artificial Intelligence (IJCAI-17)}, pages 3112--3118, 2017{\natexlab{a}}.

\bibitem[Xia et~al.(2017{\natexlab{b}})Xia, Qin, Chen, Bian, Yu, and
  Liu]{xia2017duals}
Y.~Xia, T.~Qin, W.~Chen, J.~Bian, N.~Yu, and T.-Y. Liu.
\newblock Dual supervised learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3789--3798. JMLR.org, 2017{\natexlab{b}}.

\bibitem[Xiao et~al.(2020)Xiao, Zheng, Liu, Wang, He, Ke, Bian, Lin, and
  Liu]{xiao2020invertible}
M.~Xiao, S.~Zheng, C.~Liu, Y.~Wang, D.~He, G.~Ke, J.~Bian, Z.~Lin, and T.-Y.
  Liu.
\newblock Invertible image rescaling.
\newblock In \emph{European Conference on Computer Vision}, pages 126--144.
  Springer, 2020.

\bibitem[Xiao et~al.(2021)Xiao, Kreis, Kautz, and Vahdat]{xiao2021vaebm}
Z.~Xiao, K.~Kreis, J.~Kautz, and A.~Vahdat.
\newblock {VAEBM}: A symbiosis between variational autoencoders and
  energy-based models.
\newblock In \emph{International Conference on Learning Representations (ICLR
  2021)}, 2021.

\bibitem[Yi et~al.(2017)Yi, Zhang, Tan, and Gong]{yi2017dualgan}
Z.~Yi, H.~Zhang, P.~Tan, and M.~Gong.
\newblock {DualGAN}: Unsupervised dual learning for image-to-image translation.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 2849--2857, 2017.

\bibitem[Zhang and Hyv{\"a}rinen(2009)]{zhang2009identifiability}
K.~Zhang and A.~Hyv{\"a}rinen.
\newblock On the identifiability of the post-nonlinear causal model.
\newblock In \emph{Proceedings of the 25th Conference on Uncertainty in
  Artificial Intelligence (UAI 2009)}, pages 647--655. AUAI Press, 2009.

\bibitem[Zhu et~al.(2017)Zhu, Park, Isola, and Efros]{zhu2017unpaired}
J.-Y. Zhu, T.~Park, P.~Isola, and A.~A. Efros.
\newblock Unpaired image-to-image translation using cycle-consistent
  adversarial networks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 2223--2232, 2017.

\end{thebibliography}
