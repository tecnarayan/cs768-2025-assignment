
@inproceedings{bahdanau2015neural,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sun2022length,
  title={A Length-Extrapolatable Transformer},
  author={Sun, Yutao and Dong, Li and Patra, Barun and Ma, Shuming and Huang, Shaohan and Benhaim, Alon and Chaudhary, Vishrav and Song, Xia and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10554},
  year={2022}
}

@inproceedings{
garg2022lisa,
title={{LISA}: Learning Interpretable Skill Abstractions from Language},
author={Divyansh Garg and Skanda Vaidyanath and Kuno Kim and Jiaming Song and Stefano Ermon},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=XZhipvOUBB}
}

@inproceedings{
keysers2020measuring,
title={Measuring Compositional Generalization: A Comprehensive Method on Realistic Data},
author={Daniel Keysers and Nathanael Sch{\"a}rli and Nathan Scales and Hylke Buisman and Daniel Furrer and Sergii Kashubin and Nikola Momchev and Danila Sinopalnikov and Lukasz Stafiniak and Tibor Tihon and Dmitry Tsarkov and Xiao Wang and Marc van Zee and Olivier Bousquet},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SygcCnNKwr}
}



@article{Carrasco1995,
  doi = {10.3758/bf03208380},
  url = {https://doi.org/10.3758/bf03208380},
  year = {1995},
  month = nov,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {57},
  number = {8},
  pages = {1241--1261},
  author = {Marisa Carrasco and Denise L. Evert and Irene Chang and Svetlana M. Katz},
  title = {The eccentricity effect: Target eccentricity affects performance on conjunction searches},
  journal = {Perception {\&} Psychophysics}
}

@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{raffel2020exploring,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@misc{kim2022uncontrolled,
  doi = {10.48550/ARXIV.2212.10769},
  
  url = {https://arxiv.org/abs/2212.10769},
  
  author = {Kim, Najoung and Linzen, Tal and Smolensky, Paul},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Uncontrolled Lexical Exposure Leads to Overestimation of Compositional Generalization in Pretrained Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International}
}

@inproceedings{
dehghani2018universal,
title={Universal Transformers},
author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Lukasz Kaiser},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyzdRiR9Y7},
}


@InProceedings{lake2018generalization,
  title = 	 {Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks},
  author =       {Lake, Brenden and Baroni, Marco},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {2873--2882},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/lake18a/lake18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/lake18a.html},
  abstract = 	 {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks’ notorious training data thirst.}
}

@article{liska2018memorize,
  author    = {Adam Liska and
               Germ{\'{a}}n Kruszewski and
               Marco Baroni},
  title     = {Memorize or generalize? Searching for a compositional {RNN} in a haystack},
  journal   = {ICML Workshop},
  volume    = {abs/1802.06467},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.06467},
  eprinttype = {arXiv},
  eprint    = {1802.06467},
  timestamp = {Mon, 13 Aug 2018 16:47:12 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-06467.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{
csordas2022the,
title={The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization},
author={R{\'o}bert Csord{\'a}s and Kazuki Irie and J{\"u}rgen Schmidhuber},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=KBQP4A_J1K}
}

@inproceedings{
anonymous2023compositional,
title={Compositional Semantic Parsing with Large Language Models},
author={Anonymous},
booktitle={Submitted to The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=gJW8hSGBys8},
note={under review}
}

@inproceedings{
anil2022exploring,
title={Exploring Length Generalization in Large Language Models},
author={Cem Anil and Yuhuai Wu and Anders Johan Andreassen and Aitor Lewkowycz and Vedant Misra and Vinay Venkatesh Ramasesh and Ambrose Slone and Guy Gur-Ari and Ethan Dyer and Behnam Neyshabur},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=zSkYVeX7bC4}
}

@article{Su2021Ro,
  author    = {Jianlin Su and
               Yu Lu and
               Shengfeng Pan and
               Bo Wen and
               Yunfeng Liu},
  title     = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  journal   = {ArXiv},
  volume    = {abs/2104.09864},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.09864},
  eprinttype = {arXiv},
  eprint    = {2104.09864},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-09864.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sutskever2014seq2seq,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Sequence to Sequence Learning with Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf},
 volume = {27},
 year = {2014}
}

@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@inproceedings{shaw-etal-2018-self,
    title = "Self-Attention with Relative Position Representations",
    author = "Shaw, Peter  and
      Uszkoreit, Jakob  and
      Vaswani, Ashish",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2074",
    doi = "10.18653/v1/N18-2074",
    pages = "464--468",
    abstract = "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.",
}

@inproceedings{
press2022train,
title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
author={Ofir Press and Noah Smith and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=R8sQPpGCv0}
}

@article{chung2014gru,
  author    = {Junyoung Chung and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               KyungHyun Cho and
               Yoshua Bengio},
  title     = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
               Modeling},
  journal   = {NeurIPS Workshop},
  volume    = {abs/1412.3555},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.3555},
  eprinttype = {arXiv},
  eprint    = {1412.3555},
  timestamp = {Mon, 13 Aug 2018 16:47:38 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChungGCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{banio2021ponder,
  author    = {Andrea Banino and
               Jan Balaguer and
               Charles Blundell},
  title     = {PonderNet: Learning to Ponder},
  journal   = {ICML Workshop},
  volume    = {abs/2107.05407},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.05407},
  eprinttype = {arXiv},
  eprint    = {2107.05407},
  timestamp = {Tue, 20 Jul 2021 15:08:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-05407.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{graves2016adaptive,
  author    = {Alex Graves},
  title     = {Adaptive Computation Time for Recurrent Neural Networks},
  journal   = {ArXiv},
  volume    = {abs/1603.08983},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.08983},
  eprinttype = {arXiv},
  eprint    = {1603.08983},
  timestamp = {Mon, 13 Aug 2018 16:47:31 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Graves16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bai2019deq,
 author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Deep Equilibrium Models},
 url = {https://proceedings.neurips.cc/paper/2019/file/01386bd6d8e091c2ab4c7c7de644d37b-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{Schmidhuber2012delimiting,
  author    = {J{\"{u}}rgen Schmidhuber},
  title     = {Self-Delimiting Neural Networks},
  journal   = {arXiv},
  volume    = {abs/1210.0118},
  year      = {2012},
  url       = {http://arxiv.org/abs/1210.0118},
  eprinttype = {arXiv},
  eprint    = {1210.0118},
  timestamp = {Mon, 13 Aug 2018 16:47:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1210-0118.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{maxwell2021show,
  author    = {Maxwell I. Nye and
               Anders Johan Andreassen and
               Guy Gur{-}Ari and
               Henryk Michalewski and
               Jacob Austin and
               David Bieber and
               David Dohan and
               Aitor Lewkowycz and
               Maarten Bosma and
               David Luan and
               Charles Sutton and
               Augustus Odena},
  title     = {Show Your Work: Scratchpads for Intermediate Computation with Language
               Models},
  journal   = {ArXiv},
  volume    = {abs/2112.00114},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.00114},
  eprinttype = {arXiv},
  eprint    = {2112.00114},
  timestamp = {Fri, 29 Apr 2022 17:42:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-00114.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chang-etal-2021-convolutions,
    title = "Convolutions and Self-Attention: {R}e-interpreting Relative Positions in Pre-trained Language Models",
    author = "Chang, Tyler  and
      Xu, Yifan  and
      Xu, Weijian  and
      Tu, Zhuowen",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.333",
    doi = "10.18653/v1/2021.acl-long.333",
    pages = "4322--4333",
    abstract = "In this paper, we detail the relationship between convolutions and self-attention in natural language tasks. We show that relative position embeddings in self-attention layers are equivalent to recently-proposed dynamic lightweight convolutions, and we consider multiple new ways of integrating convolutions into Transformer self-attention. Specifically, we propose composite attention, which unites previous relative position encoding methods under a convolutional framework. We conduct experiments by training BERT with composite attention, finding that convolutions consistently improve performance on multiple downstream tasks, replacing absolute position embeddings. To inform future work, we present results comparing lightweight convolutions, dynamic convolutions, and depthwise-separable convolutions in language model pre-training, considering multiple injection points for convolutions in self-attention layers.",
}

@inproceedings{wennberg-henter-2021-case,
    title = "The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models",
    author = "Wennberg, Ulme  and
      Henter, Gustav Eje",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.18",
    doi = "10.18653/v1/2021.acl-short.18",
    pages = "130--140",
    abstract = "Mechanisms for encoding positional information are central for transformer-based language models. In this paper, we analyze the position embeddings of existing language models, finding strong evidence of translation invariance, both for the embeddings themselves and for their effect on self-attention. The degree of translation invariance increases during training and correlates positively with model performance. Our findings lead us to propose translation-invariant self-attention (TISA), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings. Our proposal has several theoretical advantages over existing position-representation approaches. Proof-of-concept experiments show that it improves on regular ALBERT on GLUE tasks, while only adding orders of magnitude less positional parameters.",
}

@inproceedings{wu-etal-2021-da,
    title = "{DA}-Transformer: Distance-aware Transformer",
    author = "Wu, Chuhan  and
      Wu, Fangzhao  and
      Huang, Yongfeng",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.166",
    doi = "10.18653/v1/2021.naacl-main.166",
    pages = "2059--2068",
    abstract = "Transformer has achieved great success in the NLP field by composing various advanced models like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually cannot keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distance-aware Transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the relevance between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long- or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting self-attention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the ReLU function to keep non-negativity and introduce sparsity, and then multiply them with the re-scaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that DA-Transformer can effectively improve the performance of many tasks and outperform the vanilla Transformer and its several variants.",
}

@inproceedings{qu-etal-2021-explore,
    title = "Explore Better Relative Position Embeddings from Encoding Perspective for Transformer Models",
    author = "Qu, Anlin  and
      Niu, Jianwei  and
      Mo, Shasha",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.237",
    doi = "10.18653/v1/2021.emnlp-main.237",
    pages = "2989--2997",
    abstract = "Relative position embedding (RPE) is a successful method to explicitly and efficaciously encode position information into Transformer models. In this paper, we investigate the potential problems in Shaw-RPE and XL-RPE, which are the most representative and prevalent RPEs, and propose two novel RPEs called Low-level Fine-grained High-level Coarse-grained (LFHC) RPE and Gaussian Cumulative Distribution Function (GCDF) RPE. LFHC-RPE is an improvement of Shaw-RPE, which enhances the perception ability at medium and long relative positions. GCDF-RPE utilizes the excellent properties of the Gaussian function to amend the prior encoding mechanism in XL-RPE. Experimental results on nine authoritative datasets demonstrate the effectiveness of our methods empirically. Furthermore, GCDF-RPE achieves the best overall performance among five different RPEs.",
}

@inproceedings{luo2021stable,
 author = {Luo, Shengjie and Li, Shanda and Cai, Tianle and He, Di and Peng, Dinglan and Zheng, Shuxin and Ke, Guolin and Wang, Liwei and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {22795--22807},
 publisher = {Curran Associates, Inc.},
 title = {Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding},
 url = {https://proceedings.neurips.cc/paper/2021/file/c0f168ce8900fa56e57789e2a2f2c9d0-Paper.pdf},
 volume = {34},
 year = {2021}
}



@inproceedings{
ke2021rethinking,
title={Rethinking Positional Encoding in Language Pre-training},
author={Guolin Ke and Di He and Tie-Yan Liu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=09-528y2Fgf}
}

@inproceedings{likhomenko2021cape,
 author = {Likhomanenko, Tatiana and Xu, Qiantong and Synnaeve, Gabriel and Collobert, Ronan and Rogozhnikov, Alex},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {16079--16092},
 publisher = {Curran Associates, Inc.},
 title = {CAPE: Encoding Relative Positions with Continuous Augmented Positional Embeddings},
 url = {https://proceedings.neurips.cc/paper/2021/file/865bf46435bd84fa5d89f64cf3ba7347-Paper.pdf},
 volume = {34},
 year = {2021}
}



@inproceedings{
luo2022your,
title={Your Transformer May Not be as Powerful as You Expect},
author={Shengjie Luo and Shanda Li and Shuxin Zheng and Tie-Yan Liu and Liwei Wang and Di He},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=NQFFNdsOGD}
}

@article{duffer2022pi,
    author = {Dufter, Philipp and Schmitt, Martin and Schütze, Hinrich},
    title = "{Position Information in Transformers: An Overview}",
    journal = {Computational Linguistics},
    volume = {48},
    number = {3},
    pages = {733-763},
    year = {2022},
    month = {09},
    abstract = "{Transformers are arguably the main workhorse in recent natural language processing research. By definition, a Transformer is invariant with respect to reordering of the input. However, language is inherently sequential and word order is essential to the semantics and syntax of an utterance. In this article, we provide an overview and theoretical comparison of existing methods to incorporate position information into Transformer models. The objectives of this survey are to (1) showcase that position information in Transformer is a vibrant and extensive research area; (2) enable the reader to compare existing methods by providing a unified notation and systematization of different approaches along important model dimensions; (3) indicate what characteristics of an application should be taken into account when selecting a position encoding; and (4) provide stimuli for future research.}",
    issn = {0891-2017},
    doi = {10.1162/coli_a_00445},
    url = {https://doi.org/10.1162/coli\_a\_00445},
    eprint = {https://direct.mit.edu/coli/article-pdf/48/3/733/2040503/coli\_a\_00445.pdf},
}



@inproceedings{
Wang2020Encoding,
title={Encoding word order in complex embeddings},
author={Benyou Wang and Donghao Zhao and Christina Lioma and Qiuchi Li and Peng Zhang and Jakob Grue Simonsen},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Hke-WTVtwr}
}

@inproceedings{
wu2018pay,
title={Pay Less Attention with Lightweight and Dynamic Convolutions},
author={Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=SkVhlh09tX},
}

@inproceedings{yang-etal-2018-modeling,
    title = "Modeling Localness for Self-Attention Networks",
    author = "Yang, Baosong  and
      Tu, Zhaopeng  and
      Wong, Derek F.  and
      Meng, Fandong  and
      Chao, Lidia S.  and
      Zhang, Tong",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1475",
    doi = "10.18653/v1/D18-1475",
    pages = "4449--4458",
    abstract = "Self-attention networks have proven to be of profound value for its strength of capturing global dependencies. In this work, we propose to model localness for self-attention networks, which enhances the ability of capturing useful local context. We cast localness modeling as a learnable Gaussian bias, which indicates the central and scope of the local region to be paid more attention. The bias is then incorporated into the original attention distribution to form a revised distribution. To maintain the strength of capturing long distance dependencies while enhance the ability of capturing short-range dependencies, we only apply localness modeling to lower layers of self-attention networks. Quantitative and qualitative analyses on Chinese-English and English-German translation tasks demonstrate the effectiveness and universality of the proposed approach.",
}

@inproceedings{Kaiser2016neuralgpu,
  author={Lukasz Kaiser and Ilya Sutskever},
  title={Neural GPUs Learn Algorithms},
  year={2016},
  cdate={1451606400000},
  url={http://arxiv.org/abs/1511.08228},
  booktitle={ICLR (Poster)}
}

@book{nltk,
author = {Bird, Steven and Klein, Ewan and Loper, Edward},
title = {Natural Language Processing with Python},
year = {2009},
isbn = {0596516495},
publisher = {O'Reilly Media, Inc.},
edition = {1st},
abstract = {This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication. Packed with examples and exercises, Natural Language Processing with Python will help you: Extract information from unstructured text, either to guess the topic or identify "named entities" Analyze linguistic structure in text, including parsing and semantic analysis Access popular linguistic databases, including WordNet and treebanks Integrate techniques drawn from fields as diverse as linguistics and artificial intelligence This book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you're interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages -- or if you're simply curious to have a programmer's perspective on how human language works -- you'll find Natural Language Processing with Python both fascinating and immensely useful.}
}

@misc{
price2017extensions,
title={Extensions and Limitations of the Neural {GPU}},
author={Eric Price and Wojciech Zaremba and Ilya Sutskever},
year={2017},
url={https://openreview.net/forum?id=ryjp1c9xg}
}

@inproceedings{Liang2021OutofDistributionGW,
  title={Out-of-Distribution Generalization with Deep Equilibrium Models},
  author={Kai-Ping Liang and Cem Anil and Yuhuai Wu and Roger Baker Grosse},
  booktitle={ICML Workshop},
  year={2021}
}

@inproceedings{luong-etal-2015-effective,
    title = "Effective Approaches to Attention-based Neural Machine Translation",
    author = "Luong, Thang  and
      Pham, Hieu  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1166",
    doi = "10.18653/v1/D15-1166",
    pages = "1412--1421",
}

@article{graves2014ntm,
  author    = {Alex Graves and
               Greg Wayne and
               Ivo Danihelka},
  title     = {Neural Turing Machines},
  journal   = {ArXiv},
  volume    = {abs/1410.5401},
  year      = {2014},
  url       = {http://arxiv.org/abs/1410.5401},
  eprinttype = {arXiv},
  eprint    = {1410.5401},
  timestamp = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/GravesWD14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Newman2020TheED,
  title={The EOS Decision and Length Extrapolation},
  author={Benjamin Newman and John Hewitt and Percy Liang and Christopher D. Manning},
  booktitle={BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  year={2020}
}

@inproceedings{dai-etal-2019-transformer,
    title = "Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context",
    author = "Dai, Zihang  and
      Yang, Zhilin  and
      Yang, Yiming  and
      Carbonell, Jaime  and
      Le, Quoc  and
      Salakhutdinov, Ruslan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1285",
    doi = "10.18653/v1/P19-1285",
    pages = "2978--2988",
    abstract = "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80{\%} longer than RNNs and 450{\%} longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",
}

@inproceedings{dubois-etal-2020-location,
    title = "{L}ocation {A}ttention for {E}xtrapolation to {L}onger {S}equences",
    author = "Dubois, Yann  and
      Dagan, Gautier  and
      Hupkes, Dieuwke  and
      Bruni, Elia",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.39",
    doi = "10.18653/v1/2020.acl-main.39",
    pages = "403--413",
    abstract = "Neural networks are surprisingly good at interpolating and perform remarkably well when the training set examples resemble those in the test set. However, they are often unable to extrapolate patterns beyond the seen data, even when the abstractions required for such patterns are simple. In this paper, we first review the notion of extrapolation, why it is important and how one could hope to tackle it. We then focus on a specific type of extrapolation which is especially useful for natural language processing: generalization to sequences that are longer than the training ones. We hypothesize that models with a separate content- and location-based attention are more likely to extrapolate than those with common attention mechanisms. We empirically support our claim for recurrent seq2seq models with our proposed attention on variants of the Lookup Table task. This sheds light on some striking failures of neural models for sequences and on possible methods to approaching such issues.",
}

