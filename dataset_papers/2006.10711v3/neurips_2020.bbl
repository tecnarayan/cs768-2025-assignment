\begin{thebibliography}{}

\bibitem[Avelin and Nystr{\"o}m, 2019]{avelin2019neural}
Avelin, B. and Nystr{\"o}m, K. (2019).
\newblock Neural odes as the deep limit of resnets with constant weights.
\newblock {\em arXiv preprint arXiv:1906.12183}.

\bibitem[Brugnano et~al., 2011]{brugnano2011fifty}
Brugnano, L., Mazzia, F., and Trigiante, D. (2011).
\newblock Fifty years of stiffness.
\newblock In {\em Recent Advances in Computational and Applied Mathematics},
  pages 1--21. Springer.

\bibitem[Chapra et~al., 2010]{chapra2010numerical}
Chapra, S.~C., Canale, R.~P., et~al. (2010).
\newblock {\em Numerical methods for engineers}.
\newblock Boston: McGraw-Hill Higher Education,.

\bibitem[Chen et~al., 2018]{chen2018neural}
Chen, T.~Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D.~K. (2018).
\newblock Neural ordinary differential equations.
\newblock In {\em Advances in neural information processing systems}, pages
  6571--6583.

\bibitem[Cuchiero et~al., 2019]{cuchiero2019deep}
Cuchiero, C., Larsson, M., and Teichmann, J. (2019).
\newblock Deep neural networks, generic universal interpolation, and controlled
  odes.
\newblock {\em arXiv preprint arXiv:1908.07838}.

\bibitem[Curtiss and Hirschfelder, 1952]{curtiss1952integration}
Curtiss, C.~F. and Hirschfelder, J.~O. (1952).
\newblock Integration of stiff equations.
\newblock {\em Proceedings of the National Academy of Sciences of the United
  States of America}, 38(3):235.

\bibitem[Dahlquist, 1976]{dahlquist1976error}
Dahlquist, G. (1976).
\newblock Error analysis for a class of methods for stiff non-linear initial
  value problems.
\newblock In {\em Numerical analysis}, pages 60--72. Springer.

\bibitem[Dahlquist, 1985]{dahlquist198533}
Dahlquist, G. (1985).
\newblock 33 years of numerical instability, part i.
\newblock {\em BIT Numerical Mathematics}, 25(1):188--204.

\bibitem[Davis et~al., 2020]{davis2020time}
Davis, J.~Q., Choromanski, K., Varley, J., Lee, H., Slotine, J.-J.,
  Likhosterov, V., Weller, A., Makadia, A., and Sindhwani, V. (2020).
\newblock Time dependence in non-autonomous neural odes.
\newblock {\em arXiv preprint arXiv:2005.01906}.

\bibitem[Deng et~al., 2009]{Deng09ImageNet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009).
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.

\bibitem[Dinh et~al., 2016]{dinh2016density}
Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2016).
\newblock Density estimation using real nvp.
\newblock {\em arXiv preprint arXiv:1605.08803}.

\bibitem[Dormand and Prince, 1980]{dormand1980family}
Dormand, J.~R. and Prince, P.~J. (1980).
\newblock A family of embedded runge-kutta formulae.
\newblock {\em Journal of computational and applied mathematics}, 6(1):19--26.

\bibitem[Du et~al., 2020]{du2020model}
Du, J., Futoma, J., and Doshi-Velez, F. (2020).
\newblock Model-based reinforcement learning for semi-markov decision processes
  with neural odes.
\newblock {\em arXiv preprint arXiv:2006.16210}.

\bibitem[Dupont et~al., 2019]{dupont2019augmented}
Dupont, E., Doucet, A., and Teh, Y.~W. (2019).
\newblock Augmented neural odes.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3134--3144.

\bibitem[Farrell et~al., 2013]{farrell2013automated}
Farrell, P.~E., Ham, D.~A., Funke, S.~W., and Rognes, M.~E. (2013).
\newblock Automated derivation of the adjoint of high-level transient finite
  element programs.
\newblock {\em SIAM Journal on Scientific Computing}, 35(4):C369--C393.

\bibitem[Finlay et~al., 2020]{finlay2020train}
Finlay, C., Jacobsen, J.-H., Nurbekyan, L., and Oberman, A.~M. (2020).
\newblock How to train your neural ode.
\newblock In {\em International Conference in Machine Learning}.

\bibitem[Ghosh et~al., 2019]{ghosh2019interactive}
Ghosh, A., Zhang, R., Dokania, P.~K., Wang, O., Efros, A.~A., Torr, P.~H., and
  Shechtman, E. (2019).
\newblock Interactive sketch \& fill: Multiclass sketch-to-image translation.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1171--1180.

\bibitem[Grathwohl et~al., 2018]{grathwohl2018ffjord}
Grathwohl, W., Chen, R.~T., Bettencourt, J., Sutskever, I., and Duvenaud, D.
  (2018).
\newblock Ffjord: Free-form continuous dynamics for scalable reversible
  generative models.
\newblock {\em arXiv preprint arXiv:1810.01367}.

\bibitem[Haber and Ruthotto, 2017]{haber2017stable}
Haber, E. and Ruthotto, L. (2017).
\newblock Stable architectures for deep neural networks.
\newblock {\em Inverse Problems}, 34(1):014004.

\bibitem[Hanshu et~al., 2019]{hanshu2019robustness}
Hanshu, Y., Jiawei, D., Vincent, T., and Jiashi, F. (2019).
\newblock On robustness of neural ordinary differential equations.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[He et~al., 2016]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J. (2016).
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778.

\bibitem[Hodgkinson et~al., 2020]{hodgkinson2020stochastic}
Hodgkinson, L., van~der Heide, C., Roosta, F., and Mahoney, M.~W. (2020).
\newblock Stochastic normalizing flows.
\newblock {\em arXiv preprint arXiv:2002.09547}.

\bibitem[Huang et~al., 2016]{huang2016deep}
Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger, K.~Q. (2016).
\newblock Deep networks with stochastic depth.
\newblock In {\em European conference on computer vision}, pages 646--661.
  Springer.

\bibitem[Ioffe and Szegedy, 2015]{ioffe2015batch}
Ioffe, S. and Szegedy, C. (2015).
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em arXiv preprint arXiv:1502.03167}.

\bibitem[Jabir et~al., 2019]{jabir2019mean}
Jabir, J.-F., {\v{S}}i{\v{s}}ka, D., and Szpruch, {\L}. (2019).
\newblock Mean-field neural odes via relaxed optimal control.
\newblock {\em arXiv preprint arXiv:1912.05475}.

\bibitem[Jia and Benson, 2019]{jia2019neural}
Jia, J. and Benson, A.~R. (2019).
\newblock Neural jump stochastic differential equations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9843--9854.

\bibitem[Kanaa et~al., 2019]{kanaasimple}
Kanaa, D., Voleti, V., Kahou, S., and Pal, C. (2019).
\newblock Simple video generation using neural odes.

\bibitem[Kelly et~al., 2020]{kelly2020learning}
Kelly, J., Bettencourt, J., Johnson, M.~J., and Duvenaud, D. (2020).
\newblock Learning differential equations that are easy to solve.
\newblock {\em arXiv preprint arXiv:2007.04504}.

\bibitem[Kingma and Dhariwal, 2018]{kingma2018glow}
Kingma, D.~P. and Dhariwal, P. (2018).
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10215--10224.

\bibitem[Krizhevsky et~al., 2009]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al. (2009).
\newblock Learning multiple layers of features from tiny images.

\bibitem[Lagaris et~al., 1998]{lagaris1998artificial}
Lagaris, I.~E., Likas, A., and Fotiadis, D.~I. (1998).
\newblock Artificial neural networks for solving ordinary and partial
  differential equations.
\newblock {\em IEEE transactions on neural networks}, 9(5):987--1000.

\bibitem[LeCun et~al., 1990]{lecun1990handwritten}
LeCun, Y., Boser, B.~E., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard,
  W.~E., and Jackel, L.~D. (1990).
\newblock Handwritten digit recognition with a back-propagation network.
\newblock In {\em Advances in neural information processing systems}, pages
  396--404.

\bibitem[Li et~al., 2020a]{li2020scalable}
Li, X., Wong, T.-K.~L., Chen, R.~T., and Duvenaud, D. (2020a).
\newblock Scalable gradients for stochastic differential equations.
\newblock {\em arXiv preprint arXiv:2001.01328}.

\bibitem[Li et~al., 2020b]{li2020exchangeable}
Li, Y., Yi, H., Bender, C.~M., Shan, S., and Oliva, J.~B. (2020b).
\newblock Exchangeable neural ode for set modeling.
\newblock {\em arXiv preprint arXiv:2008.02676}.

\bibitem[Liu et~al., 2019]{liu2019neural}
Liu, X., Xiao, T., Si, S., Cao, Q., Kumar, S., and Hsieh, C.-J. (2019).
\newblock Neural sde: Stabilizing neural ode networks with stochastic noise.
\newblock {\em arXiv preprint arXiv:1906.02355}.

\bibitem[Lu et~al., 2017]{lu2017beyond}
Lu, Y., Zhong, A., Li, Q., and Dong, B. (2017).
\newblock Beyond finite layer neural networks: Bridging deep architectures and
  numerical differential equations.
\newblock {\em arXiv preprint arXiv:1710.10121}.

\bibitem[Massaroli et~al., 2020a]{massaroli2020stable}
Massaroli, S., Poli, M., Bin, M., Park, J., Yamashita, A., and Asama, H.
  (2020a).
\newblock Stable neural flows.
\newblock {\em arXiv preprint arXiv:2003.08063}.

\bibitem[Massaroli et~al., 2020b]{massaroli2020dissecting}
Massaroli, S., Poli, M., Park, J., Yamashita, A., and Asama, H. (2020b).
\newblock Dissecting neural odes.
\newblock {\em arXiv preprint arXiv:2002.08071}.

\bibitem[Netzer et~al., 2011]{Netzer11SVHN}
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and N, A.~Y. (2011).
\newblock Reading digits in natural images with unsupervised feature learning.

\bibitem[Norcliffe et~al., 2020]{norcliffe2020second}
Norcliffe, A., Bodnar, C., Day, B., Simidjievski, N., and Li{\`o}, P. (2020).
\newblock On second order behaviour in augmented neural odes.
\newblock {\em arXiv preprint arXiv:2006.07220}.

\bibitem[Oganesyan et~al., 2020]{oganesyan2020stochasticity}
Oganesyan, V., Volokhova, A., and Vetrov, D. (2020).
\newblock Stochasticity in neural odes: An empirical study.
\newblock {\em arXiv preprint arXiv:2002.09779}.

\bibitem[Papamakarios et~al., 2019]{papamakarios2019normalizing}
Papamakarios, G., Nalisnick, E., Rezende, D.~J., Mohamed, S., and
  Lakshminarayanan, B. (2019).
\newblock Normalizing flows for probabilistic modeling and inference.
\newblock {\em arXiv preprint arXiv:1912.02762}.

\bibitem[Papamakarios et~al., 2017]{papamakarios2017masked}
Papamakarios, G., Pavlakou, T., and Murray, I. (2017).
\newblock Masked autoregressive flow for density estimation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2338--2347.

\bibitem[Pearlmutter, 1995]{pearlmutter1995gradient}
Pearlmutter, B.~A. (1995).
\newblock Gradient calculations for dynamic recurrent neural networks: A
  survey.
\newblock {\em IEEE Transactions on Neural networks}, 6(5):1212--1228.

\bibitem[Rackauckas and Nie, 2017]{rackauckas2017differentialequations}
Rackauckas, C. and Nie, Q. (2017).
\newblock Differentialequations. jl--a performant and feature-rich ecosystem
  for solving differential equations in julia.
\newblock {\em Journal of Open Research Software}, 5(1).

\bibitem[Rubanova et~al., 2019]{rubanova2019latent}
Rubanova, Y., Chen, T.~Q., and Duvenaud, D.~K. (2019).
\newblock Latent ordinary differential equations for irregularly-sampled time
  series.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5321--5331.

\bibitem[Ruthotto and Haber, 2019]{ruthotto2019deep}
Ruthotto, L. and Haber, E. (2019).
\newblock Deep neural networks motivated by partial differential equations.
\newblock {\em Journal of Mathematical Imaging and Vision}, pages 1--13.

\bibitem[Silva et~al., 2012]{silva2012predicting}
Silva, I., Moody, G., Scott, D.~J., Celi, L.~A., and Mark, R.~G. (2012).
\newblock Predicting in-hospital mortality of icu patients: The
  physionet/computing in cardiology challenge 2012.
\newblock In {\em 2012 Computing in Cardiology}, pages 245--248. IEEE.

\bibitem[S{\"o}derlind et~al., 2015]{soderlind2015stiffness}
S{\"o}derlind, G., Jay, L., and Calvo, M. (2015).
\newblock Stiffness 1952--2012: Sixty years in search of a definition.
\newblock {\em BIT Numerical Mathematics}, 55(2):531--558.

\bibitem[Veit et~al., 2016]{veit2016residual}
Veit, A., Wilber, M.~J., and Belongie, S. (2016).
\newblock Residual networks behave like ensembles of relatively shallow
  networks.
\newblock In {\em NIPS}, pages 550--558.

\bibitem[Wang et~al., 2019]{wang2019resnets}
Wang, B., Shi, Z., and Osher, S. (2019).
\newblock Resnets ensemble via the feynman-kac formalism to improve natural and
  robust accuracies.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1655--1665.

\bibitem[Wanner and Hairer, 1996]{wanner1996solving}
Wanner, G. and Hairer, E. (1996).
\newblock {\em Solving ordinary differential equations II}.
\newblock Springer Berlin Heidelberg.

\bibitem[Weinan, 2017]{weinan2017proposal}
Weinan, E. (2017).
\newblock A proposal on machine learning via dynamical systems.
\newblock {\em Communications in Mathematics and Statistics}, 5(1):1--11.

\bibitem[Wu et~al., 2020]{wu2020stochastic}
Wu, H., K{\"o}hler, J., and No{\'e}, F. (2020).
\newblock Stochastic normalizing flows.
\newblock {\em arXiv preprint arXiv:2002.06707}.

\bibitem[Xhonneux et~al., 2019]{xhonneux2019continuous}
Xhonneux, L.-P.~A., Qu, M., and Tang, J. (2019).
\newblock Continuous graph neural networks.
\newblock {\em arXiv preprint arXiv:1912.00967}.

\bibitem[Yao et~al., 2007]{yao2007early}
Yao, Y., Rosasco, L., and Caponnetto, A. (2007).
\newblock On early stopping in gradient descent learning.
\newblock {\em Constructive Approximation}, 26(2):289--315.

\bibitem[Yildiz et~al., 2019]{yildiz2019ode2vae}
Yildiz, C., Heinonen, M., and Lahdesmaki, H. (2019).
\newblock Ode2vae: Deep generative second order odes with bayesian neural
  networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  13412--13421.

\bibitem[Zhang et~al., 2019]{zhang2019approximation}
Zhang, H., Gao, X., Unterman, J., and Arodz, T. (2019).
\newblock Approximation capabilities of neural ordinary differential equations.
\newblock {\em arXiv preprint arXiv:1907.12998}.

\end{thebibliography}
