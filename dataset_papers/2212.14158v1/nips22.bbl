\begin{thebibliography}{10}

\bibitem{bacciu2020tensor}
Davide Bacciu and Danilo~P Mandic.
\newblock Tensor decompositions in deep learning.
\newblock {\em arXiv preprint arXiv:2002.11835}, 2020.

\bibitem{bethge2020meliusnet}
Joseph Bethge, Christian Bartz, Haojin Yang, Ying Chen, and Christoph Meinel.
\newblock {MeliusNet}: Can binary neural networks achieve mobilenet-level
  accuracy?
\newblock In {\em WACV}, 2021.

\bibitem{chen2021cyclemlp}
Shoufa Chen, Enze Xie, Chongjian Ge, Ding Liang, and Ping Luo.
\newblock {CycleMLP}: A mlp-like architecture for dense prediction.
\newblock In {\em ICLR}, 2022.

\bibitem{chmiel2020robust}
Brian Chmiel, Ron Banner, Gil Shomron, Yury Nahshan, Alex Bronstein, Uri
  Weiser, et~al.
\newblock Robust quantization: One model to rule them all.
\newblock {\em NeurIPS}, 33:5308--5317, 2020.

\bibitem{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In {\em CVPR Workshops}, pages 702--703, 2020.

\bibitem{darabi2018bnn}
Sajad Darabi, Mouloud Belbahri, Matthieu Courbariaux, and Vahid~Partovi Nia.
\newblock Bnn+: Improved binary network training.
\newblock 2018.

\bibitem{ding2022scaling}
Xiaohan Ding, Xiangyu Zhang, Yizhuang Zhou, Jungong Han, Guiguang Ding, and
  Jian Sun.
\newblock Scaling up your kernels to 31x31: Revisiting large kernel design in
  cnns.
\newblock In {\em CVPR}, 2022.

\bibitem{gong2019differentiable}
Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin,
  Fengwei Yu, and Junjie Yan.
\newblock Differentiable soft quantization: Bridging full-precision and low-bit
  neural networks.
\newblock In {\em ICCV}, 2019.

\bibitem{guo2022cmt}
Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and Chang
  Xu.
\newblock Cmt: Convolutional neural networks meet vision transformers.
\newblock In {\em CVPR}, pages 12175--12185, 2022.

\bibitem{guo2022hire}
Jianyuan Guo, Yehui Tang, Kai Han, Xinghao Chen, Han Wu, Chao Xu, Chang Xu, and
  Yunhe Wang.
\newblock Hire-mlp: Vision mlp via hierarchical rearrangement.
\newblock In {\em CVPR}, pages 826--836, 2022.

\bibitem{han2020ghostnet}
Kai Han, Yunhe Wang, Qi~Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu.
\newblock Ghostnet: More features from cheap operations.
\newblock In {\em CVPR}, pages 1580--1589, 2020.

\bibitem{han2020training}
Kai Han, Yunhe Wang, Yixing Xu, Chunjing Xu, Enhua Wu, and Chang Xu.
\newblock Training binary neural networks through learning with noisy
  supervision.
\newblock In {\em ICML}, pages 4017--4026. PMLR, 2020.

\bibitem{he2015spatial}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Spatial pyramid pooling in deep convolutional networks for visual
  recognition.
\newblock {\em IEEE T-PAMI}, 37(9):1904--1916, 2015.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, pages 770--778, 2016.

\bibitem{he2018soft}
Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi~Yang.
\newblock Soft filter pruning for accelerating deep convolutional neural
  networks.
\newblock In {\em IJCAI}, 2018.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2(7), 2015.

\bibitem{mindspore}
Huawei.
\newblock Mindspore.
\newblock \url{https://www.mindspore.cn/}, 2020.

\bibitem{kim2021improving}
Hyungjun Kim, Jihoon Park, Changhun Lee, and Jae-Joon Kim.
\newblock Improving accuracy of binary neural networks using unbalanced
  activation distribution.
\newblock In {\em CVPR}, pages 7862--7871, 2021.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em NeurIPS}, 25, 2012.

\bibitem{li2022equal}
Yunqiang Li, Silvia-Laura Pintea, and Jan~C van Gemert.
\newblock Equal bits: Enforcing equally distributed binary network weights.
\newblock In {\em AAAI}, volume~36, pages 1491--1499, 2022.

\bibitem{li2019feedback}
Zhen Li, Jinglei Yang, Zheng Liu, Xiaomin Yang, Gwanggil Jeon, and Wei Wu.
\newblock Feedback network for image super-resolution.
\newblock In {\em CVPR}, pages 3867--3876, 2019.

\bibitem{lian2021mlp}
Dongze Lian, Zehao Yu, Xing Sun, and Shenghua Gao.
\newblock {AS-MLP}: An axial shifted mlp architecture for vision.
\newblock In {\em ICLR}, 2022.

\bibitem{lin2020rotated}
Mingbao Lin, Rongrong Ji, Zihan Xu, Baochang Zhang, Yan Wang, Yongjian Wu,
  Feiyue Huang, and Chia-Wen Lin.
\newblock Rotated binary neural network.
\newblock {\em NeurIPS}, 33:7474--7485, 2020.

\bibitem{lin2017towards}
Xiaofan Lin, Cong Zhao, and Wei Pan.
\newblock Towards accurate binary convolutional neural network.
\newblock {\em NeurIPS}, 30, 2017.

\bibitem{liu2021pay}
Hanxiao Liu, Zihang Dai, David So, and Quoc~V Le.
\newblock Pay attention to mlps.
\newblock In {\em NeurIPS}, volume~34, pages 9204--9215, 2021.

\bibitem{liu2020reactnet}
Zechun Liu, Zhiqiang Shen, Marios Savvides, and Kwang-Ting Cheng.
\newblock Reactnet: Towards precise binary neural network with generalized
  activation functions.
\newblock In {\em ECCV}, pages 143--159. Springer, 2020.

\bibitem{liu2018bi}
Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng.
\newblock Bi-real net: Enhancing the performance of 1-bit cnns with improved
  representational capability and advanced training algorithm.
\newblock In {\em ECCV}, pages 722--737, 2018.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{luo2017thinet}
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In {\em ICCV}, pages 5058--5066, 2017.

\bibitem{martinez2020training}
Brais Martinez, Jing Yang, Adrian Bulat, and Georgios Tzimiropoulos.
\newblock Training binary neural networks with real-to-binary convolutions.
\newblock {\em arXiv preprint arXiv:2003.11535}, 2020.

\bibitem{niemulti}
Ying Nie, Kai Han, and Yunhe Wang.
\newblock Multi-bit adaptive distillation for binary neural networks.
\newblock In {\em BMVC}, 2021.

\bibitem{noh2015learning}
Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.
\newblock Learning deconvolution network for semantic segmentation.
\newblock In {\em ICCV}, pages 1520--1528, 2015.

\bibitem{rastegari2016xnor}
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In {\em ECCV}, pages 525--542. Springer, 2016.

\bibitem{redmon2016you}
Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi.
\newblock You only look once: Unified, real-time object detection.
\newblock In {\em CVPR}, pages 779--788, 2016.

\bibitem{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock {\em NeurIPS}, 28, 2015.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em IJCV}, 115(3):211--252, 2015.

\bibitem{shang2022lipschitz}
Yuzhang Shang, Dan Xu, Bin Duan, Ziliang Zong, Liqiang Nie, and Yan Yan.
\newblock Lipschitz continuity retained binary neural network.
\newblock In {\em ECCV}, 2022.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em ICML}, pages 6105--6114. PMLR, 2019.

\bibitem{tang2021image}
Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Yanxi Li, Chao Xu, and Yunhe Wang.
\newblock An image patch is a wave: Phase-aware vision mlp.
\newblock In {\em CVPR}, 2022.

\bibitem{tang2021manifold}
Yehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, Chao Xu, Dacheng Tao, and Chang
  Xu.
\newblock Manifold regularized dynamic network pruning.
\newblock In {\em CVPR}, pages 5018--5028, 2021.

\bibitem{tang2020scop}
Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, and Chang
  Xu.
\newblock Scop: Scientific control for reliable neural network pruning.
\newblock {\em NeurIPS}, 33:10936--10947, 2020.

\bibitem{tolstikhin2021mlp}
Ilya~O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua
  Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers,
  Jakob Uszkoreit, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock {\em NeurIPS}, 34, 2021.

\bibitem{touvron2021resmlp}
Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin
  El-Nouby, Edouard Grave, Gautier Izacard, Armand Joulin, Gabriel Synnaeve,
  Jakob Verbeek, et~al.
\newblock Resmlp: Feedforward networks for image classification with
  data-efficient training.
\newblock {\em arXiv preprint arXiv:2105.03404}, 2021.

\bibitem{tu2022adabin}
Zhijun Tu, Xinghao Chen, Pengju Ren, and Yunhe Wang.
\newblock {AdaBin}: Improving binary neural networks with adaptive binary sets.
\newblock In {\em ECCV}, 2022.

\bibitem{wang2022tensor}
Yinan Wang, Weihong~“Grace” Guo, and Xiaowei Yue.
\newblock Tensor decomposition to compress convolutional layers in deep
  learning.
\newblock {\em IISE Transactions}, 54(5):481--495, 2022.

\bibitem{xu2021learning}
Yixing Xu, Kai Han, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe Wang.
\newblock Learning frequency domain approximation for binary neural networks.
\newblock {\em NeurIPS}, 34:25553--25565, 2021.

\bibitem{xu2019positive}
Yixing Xu, Yunhe Wang, Hanting Chen, Kai Han, Chunjing Xu, Dacheng Tao, and
  Chang Xu.
\newblock Positive-unlabeled compression on the cloud.
\newblock {\em NeurIPS}, 32, 2019.

\bibitem{xu2020kernel}
Yixing Xu, Chang Xu, Xinghao Chen, Wei Zhang, Chunjing Xu, and Yunhe Wang.
\newblock Kernel based progressive distillation for adder neural networks.
\newblock {\em NeurIPS}, 33:12322--12333, 2020.

\bibitem{xu2021recu}
Zihan Xu, Mingbao Lin, Jianzhuang Liu, Jie Chen, Ling Shao, Yue Gao, Yonghong
  Tian, and Rongrong Ji.
\newblock Recu: Reviving the dead weights in binary neural networks.
\newblock In {\em ICCV}, pages 5198--5208, 2021.

\bibitem{yun2019cutmix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In {\em ICCV}, 2019.

\bibitem{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In {\em ICLR}, 2018.

\bibitem{zhou2016dorefa}
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He~Wen, and Yuheng Zou.
\newblock Dorefa-net: Training low bitwidth convolutional neural networks with
  low bitwidth gradients.
\newblock {\em arXiv preprint arXiv:1606.06160}, 2016.

\bibitem{zhou2018adaptive}
Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, Ngai-Man Cheung, and Pascal
  Frossard.
\newblock Adaptive quantization for deep neural network.
\newblock In {\em AAAI}, volume~32, 2018.

\end{thebibliography}
