% introduction

@inproceedings{le2017using,
  title={Using synthetic data to train neural networks is model-based reasoning},
  author={Le, Tuan Anh and Baydin, Atilim Giine{\c{s}} and Zinkov, Robert and Wood, Frank},
  booktitle={2017 international joint conference on neural networks (IJCNN)},
  pages={3514--3521},
  year={2017},
  organization={IEEE}
}
@article{han2022card,
  title={CARD: Classification and Regression Diffusion Models},
  author={Han, Xizewen and Zheng, Huangjie and Zhou, Mingyuan},
  journal={arXiv preprint arXiv:2206.07275},
  year={2022}
}

@article{baydinAutomaticDifferentiationMachine2017,
  title = {Automatic {{Differentiation}} in {{Machine Learning}}: A {{Survey}}},
  author = {Baydin, Atilim Gunes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
  year = {2017},
  journal = {Journal of Machine Learning Research},
  volume = {18},
  pages = {153:1--153:43},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/jmlr/BaydinPRS17.bib},
}

@article{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  journal = {arXiv:2005.14165 [cs]},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  primaryclass = {cs}
}

@misc{chenEvaluatingLargeLanguage2021,
  title = {Evaluating {{Large Language Models Trained}} on {{Code}}},
  author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and {Herbert-Voss}, Ariel and Guss, William Hebgen and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang, Jie and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Saunders, William and Hesse, Christopher and Carr, Andrew N. and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
  year = {2021},
  month = jul,
  number = {arXiv:2107.03374},
  eprint = {2107.03374},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.03374}
}

@book{hutterAutomaticMachineLearning2018,
  title = {Automatic {{Machine Learning}}: {{Methods}}, {{Systems}}, {{Challenges}}},
  editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
  year = {2018},
  publisher = {{Springer}}
}

@inproceedings{hoogeboom2022equivariant,
  title={Equivariant diffusion for molecule generation in 3d},
  author={Hoogeboom, Emiel and Satorras, V{\i}ctor Garcia and Vignac, Cl{\'e}ment and Welling, Max},
  booktitle={International Conference on Machine Learning},
  pages={8867--8887},
  year={2022},
  organization={PMLR}
}


@inproceedings{ho2019flow++,
  title={Flow++: Improving flow-based generative models with variational dequantization and architecture design},
  author={Ho, Jonathan and Chen, Xi and Srinivas, Aravind and Duan, Yan and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={2722--2730},
  year={2019},
  organization={PMLR}
}


@article{austin2021structured,
  title={Structured denoising diffusion models in discrete state-spaces},
  author={Austin, Jacob and Johnson, Daniel D and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17981--17993},
  year={2021}
}


@article{hoogeboom2021autoregressive,
  title={Autoregressive diffusion models},
  author={Hoogeboom, Emiel and Gritsenko, Alexey A and Bastings, Jasmijn and Poole, Ben and Berg, Rianne van den and Salimans, Tim},
  journal={arXiv preprint arXiv:2110.02037},
  year={2021}
}


@book{cormenIntroductionAlgorithms2009,
  title = {Introduction to Algorithms},
  editor = {Cormen, Thomas H.},
  year = {2009},
  edition = {3rd ed},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  isbn = {978-0-262-03384-8 978-0-262-53305-8},
  langid = {english},
  lccn = {QA76.6 .C662 2009},
  keywords = {Computer algorithms,Computer programming},
  annotation = {OCLC: ocn311310321},
}

@book{marslandMachineLearningAlgorithmic2009,
  title = {Machine Learning - an Algorithmic Perspective},
  author = {Marsland, Stephen},
  year = {2009},
  series = {Chapman and {{Hall}} / {{CRC}} Machine Learning and Pattern Recognition Series},
  publisher = {{CRC Press}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/books/daglib/0029547.bib},
  isbn = {978-1-4200-6718-7},
  timestamp = {Mon, 15 Oct 2012 13:17:49 +0200}
}

@book{stuartrussellArtificialIntelligenceModern2010,
  title = {Artificial Intelligence: {{A Modern Approach}}},
  author = {Russell, Stuart and Norvig, Peter},
  year = {2010},
  series = {Prentice Hall Series in Artificial Intelligence},
  edition = {3rd},
  publisher = {{Prentice Hall}},
  isbn = {0-13-604259-7 978-0-13-604259-4}
}

@book{williamsonDesignApproximationAlgorithms2011a,
  title = {The {{Design}} of {{Approximation Algorithms}}},
  author = {Williamson, David P. and Shmoys, David B.},
  year = {2011},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511921735},
  isbn = {978-0-511-92173-5},
  langid = {english},
}

@article{freivalds2022denoising,
  title={Denoising Diffusion for Sampling SAT Solutions},
  author={Freivalds, Karlis and Kozlovics, Sergejs},
  journal={arXiv preprint arXiv:2212.00121},
  year={2022}
}


@article{zhou2020graph,
  title={Graph neural networks: A review of methods and applications},
  author={Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  journal={AI open},
  volume={1},
  pages={57--81},
  year={2020},
  publisher={Elsevier}
}


 @misc{karpathy_2017, url={https://karpathy.medium.com/software-2-0-a64152b37c35}, journal={Medium}, author={Karpathy, Andrej}, year={2017}, month={Nov}} 



@article{maillardDijkstraMonadsAll2019a,
  title = {Dijkstra Monads for All},
  author = {Maillard, Kenji and Ahman, Danel and Atkey, Robert and Mart{\'i}nez, Guido and Hri{\c t}cu, C{\u a}t{\u a}lin and Rivas, Exequiel and Tanter, {\'E}ric},
  year = {2019},
  month = jul,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {3},
  number = {ICFP},
  pages = {104:1--104:29},
  doi = {10.1145/3341708},
  abstract = {This paper proposes a general semantic framework for verifying programs with arbitrary monadic side-effects using Dijkstra monads, which we define as monad-like structures indexed by a specification monad. We prove that any monad morphism between a computational monad and a specification monad gives rise to a Dijkstra monad, which provides great flexibility for obtaining Dijkstra monads tailored to the verification task at hand. We moreover show that a large variety of specification monads can be obtained by applying monad transformers to various base specification monads, including predicate transformers and Hoare-style pre- and postconditions. For defining correct monad transformers, we propose a language inspired by Moggi's monadic metalanguage that is parameterized by a dependent type theory. We also develop a notion of algebraic operations for Dijkstra monads, and start to investigate two ways of also accommodating effect handlers. We implement our framework in both Coq and F*, and illustrate that it supports a wide variety of verification styles for effects such as exceptions, nondeterminism, state, input-output, and general recursion.},
  keywords = {dependent types,foundations,monads,program verification,side-effects}
}

@inproceedings{mouraLeanTheoremProver2021,
  title = {The {{Lean}} 4 {{Theorem Prover}} and {{Programming Language}}},
  booktitle = {Automated {{Deduction}} \textendash{} {{CADE}} 28},
  author = {de Moura, Leonardo and Ullrich, Sebastian},
  editor = {Platzer, Andr{\'e} and Sutcliffe, Geoff},
  year = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {625--635},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-79876-5_37},
  abstract = {Lean 4 is a reimplementation of the Lean interactive theorem prover (ITP) in Lean itself. It addresses many shortcomings of the previous versions and contains many new features. Lean 4 is fully extensible: users can modify and extend the parser, elaborator, tactics, decision procedures, pretty printer, and code generator. The new system has a hygienic macro system custom-built for ITPs. It contains a new typeclass resolution procedure based on tabled resolution, addressing significant performance problems reported by the growing user base. Lean 4 is also an efficient functional programming language based on a novel programming paradigm called functional but in-place. Efficient code generation is crucial for Lean users because many write custom proof automation procedures in Lean itself.},
  isbn = {978-3-030-79876-5},
  langid = {english}
}

@inproceedings{coquandConstructionsHigherOrder1985,
  title = {Constructions: {{A}} Higher Order Proof System for Mechanizing Mathematics},
  shorttitle = {Constructions},
  booktitle = {{{EUROCAL}} '85},
  author = {Coquand, Thierry and Huet, G{\'e}rard},
  editor = {Buchberger, Bruno},
  year = {1985},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {151--184},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-15983-5_13},
  abstract = {We present an extensive set of mathematical propositions and proofs in order to demonstrate the power of expression of the theory of constructions.},
  isbn = {978-3-540-39684-0},
  langid = {english},
  keywords = {Abstract Syntax,Combinatory Logic,Concrete Syntax,Natural Deduction,Type Theory},
  file = {/home/christian/Zotero/storage/JDJAHZDC/Coquand and Huet - 1985 - Constructions A higher order proof system for mec.pdf}
}

@misc{harveyFlexibleDiffusionModeling2022,
  title = {Flexible {{Diffusion Modeling}} of {{Long Videos}}},
  author = {Harvey, William and Naderiparizi, Saeid and Masrani, Vaden and Weilbach, Christian and Wood, Frank},
  year = {2022},
  month = may,
  number = {arXiv:2205.11495},
  eprint = {2205.11495},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.11495},
  abstract = {We present a framework for video modeling based on denoising diffusion probabilistic models that produces long-duration video completions in a variety of realistic environments. We introduce a generative model that can at test-time sample any arbitrary subset of video frames conditioned on any other subset and present an architecture adapted for this purpose. Doing so allows us to efficiently compare and optimize a variety of schedules for the order in which frames in a long video are sampled and use selective sparse and long-range conditioning on previously sampled frames. We demonstrate improved video modeling over prior work on a number of datasets and sample temporally coherent videos over 25 minutes in length. We additionally release a new video modeling dataset and semantically meaningful metrics based on videos generated in the CARLA self-driving car simulator.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{hoVideoDiffusionModels2022,
  title = {Video {{Diffusion Models}}},
  author = {Ho, Jonathan and Salimans, Tim and Gritsenko, Alexey and Chan, William and Norouzi, Mohammad and Fleet, David J.},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.03458 [cs]},
  eprint = {2204.03458},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Generating temporally coherent high fidelity video is an important milestone in generative modeling research. We make progress towards this milestone by proposing a diffusion model for video generation that shows very promising initial results. Our model is a natural extension of the standard image diffusion architecture, and it enables jointly training from image and video data, which we find to reduce the variance of minibatch gradients and speed up optimization. To generate long and higher resolution videos we introduce a new conditional sampling technique for spatial and temporal video extension that performs better than previously proposed methods. We present the first results on a large text-conditioned video generation task, as well as state-of-the-art results on an established unconditional video generation benchmark. Supplementary material is available at https://video-diffusion.github.io/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

% background

@inproceedings{vaswaniAttentionAllYou2017a,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{hoDenoisingDiffusionProbabilistic2020a,
  title = {Denoising Diffusion Probabilistic Models},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {{Curran Associates, Inc.}}
}

@article{songScoreBasedGenerativeModeling2021,
  title = {Score-{{Based Generative Modeling}} through {{Stochastic Differential Equations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2021},
  month = feb,
  journal = {arXiv:2011.13456 [cs, stat]},
  eprint = {2011.13456},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{sohl-dicksteinDeepUnsupervisedLearning2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  month = jun,
  pages = {2256--2265},
  publisher = {{PMLR}},
  issn = {1938-7228}
}

% methods

@inproceedings{kingmaVariationalDiffusionModels2021,
  title = {Variational {{Diffusion Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kingma, Diederik and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  year = {2021},
  volume = {34},
  pages = {21696--21707},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{songMaximumLikelihoodTraining2021a,
  title = {Maximum {{Likelihood Training}} of {{Score-Based Diffusion Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
  year = {2021},
  volume = {34},
  pages = {1415--1428},
  publisher = {{Curran Associates, Inc.}}
}

@inproceedings{nicholImprovedDenoisingDiffusion2021,
  title = {Improved {{Denoising Diffusion Probabilistic Models}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  year = {2021},
  month = jul,
  pages = {8162--8171},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-05-29},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.},
  langid = {english}
}

@misc{hendrycksGaussianErrorLinear2020,
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  year = {2020},
  month = jul,
  number = {arXiv:1606.08415},
  eprint = {1606.08415},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1606.08415},
  urldate = {2022-06-25},
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x\textbackslash Phi(x)\$, where \$\textbackslash Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x\textbackslash mathbf\{1\}\_\{x{$>$}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning}
}


@article{songDenoisingDiffusionImplicit2021,
  title = {Denoising {{Diffusion Implicit Models}}},
  author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  year = {2021},
  month = nov,
  journal = {arXiv:2010.02502 [cs]},
  eprint = {2010.02502},
  primaryclass = {cs},
  urldate = {2022-04-04},
  abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples \$10 \textbackslash times\$ to \$50 \textbackslash times\$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}

@article{huangVariationalPerspectiveDiffusionBased2021,
  title = {A {{Variational Perspective}} on {{Diffusion-Based Generative Models}} and {{Score Matching}}},
  author = {Huang, Chin-Wei and Lim, Jae Hyun and Courville, Aaron},
  year = {2021},
  month = sep,
  journal = {arXiv:2106.02808 [cs]},
  eprint = {2106.02808},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Discrete-time diffusion-based generative models and score matching methods have shown promising results in modeling high-dimensional image data. Recently, Song et al. (2021) show that diffusion processes that transform data into noise can be reversed via learning the score function, i.e. the gradient of the logdensity of the perturbed data. They propose to plug the learned score function into an inverse formula to define a generative diffusion process. Despite the empirical success, a theoretical underpinning of this procedure is still lacking. In this work, we approach the (continuous-time) generative diffusion directly and derive a variational framework for likelihood estimation, which includes continuous-time normalizing flows as a special case, and can be seen as an infinitely deep variational autoencoder. Under this framework, we show that minimizing the score-matching loss is equivalent to maximizing a lower bound of the likelihood of the plug-in reverse SDE proposed by Song et al. (2021), bridging the theoretical gap.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}, {{CVPR}} 2016, {{Las Vegas}}, {{NV}}, {{USA}}, {{June}} 27-30, 2016},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778},
  doi = {10.1109/CVPR.2016.90}
}

@article{ritchieDeepAmortizedInference2016,
  title = {Deep {{Amortized Inference}} for {{Probabilistic Programs}}},
  author = {Ritchie, Daniel and Horsfall, Paul and Goodman, Noah D.},
  year = {2016},
  month = oct,
  journal = {arXiv:1610.05735 [cs, stat]},
  eprint = {1610.05735},
  eprinttype = {arxiv},
  primaryclass = {cs, stat}
}

@inproceedings{leInferenceCompilationUniversal2017,
  title = {Inference {{Compilation}} and {{Universal Probabilistic Programming}}},
  booktitle = {Proceedings of the 20th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}})},
  author = {Le, Tuan Anh and Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and Wood, Frank},
  year = {2017},
  series = {Proceedings of {{Machine Learning Research}}},
  volume = {54},
  pages = {1338--1348},
  publisher = {{PMLR}},
  address = {{Fort Lauderdale, FL, USA}}
}

@inproceedings{gershmanAmortizedInferenceProbabilistic2014,
  title = {Amortized Inference in Probabilistic Reasoning},
  booktitle = {Proceedings of the {{Cognitive Science Society}}},
  author = {Gershman, Samuel and Goodman, Noah},
  year = {2014},
  volume = {36}
}

@book{kallenbergFoundationsModernProbability2021,
  title = {Foundations of {{Modern Probability}}},
  author = {Kallenberg, Olav},
  year = {2021},
  series = {Probability {{Theory}} and {{Stochastic Modelling}}},
  volume = {99},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-61871-1},
  isbn = {978-3-030-61870-4 978-3-030-61871-1},
  langid = {english}
}

@book{kallenbergProbabilisticSymmetriesInvariance2005,
  title = {Probabilistic Symmetries and Invariance Principles},
  author = {Kallenberg, Olav},
  year = {2005},
  series = {Probability and Its Applications},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-25115-8},
  langid = {english},
  lccn = {QA273 .K288 2005},
  keywords = {Probabilities,Symmetry (Physics)},
  file = {/home/christian/Zotero/storage/VWGF4C38/Kallenberg - 2005 - Probabilistic symmetries and invariance principles.pdf}
}

@article{bloem-reddyProbabilisticSymmetriesInvariant2020,
  title = {Probabilistic {{Symmetries}} and {{Invariant Neural Networks}}},
  author = {{Bloem-Reddy}, Benjamin and Teh, Yee Whye},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {90},
  pages = {1--61},
  issn = {1533-7928},
  abstract = {Treating neural network inputs and outputs as random variables, we characterize the structure of neural networks that can be used to model data that are invariant or equivariant under the action of a compact group. Much recent research has been devoted to encoding invariance under symmetry transformations into neural network architectures, in an effort to improve the performance of deep neural networks in data-scarce, non-i.i.d., or unsupervised settings. By considering group invariance from the perspective of probabilistic symmetry, we establish a link between functional and probabilistic symmetry, and obtain generative functional representations of probability distributions that are invariant or equivariant under the action of a compact group. Our representations completely characterize the structure of neural networks that can be used to model such distributions and yield a general program for constructing invariant stochastic or deterministic neural networks. We demonstrate that examples from the recent literature are special cases, and develop the details of the general program for exchangeable sequences and arrays.}
}


@InProceedings{weilbachStructuredConditionalContinuous2020,
  title = 	 {Structured Conditional Continuous Normalizing Flows for Efficient Amortized Inference in Graphical Models},
  author =       {Weilbach, Christian and Beronov, Boyan and Wood, Frank and Harvey, William},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {4441--4451},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/weilbach20a/weilbach20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/weilbach20a.html},
  abstract = 	 {We exploit minimally faithful inversion of graphical model structures to specify sparse continuous normalizing flows (CNFs) for amortized inference. We find that the sparsity of this factorization can be exploited to reduce the numbers of parameters in the neural network, adaptive integration steps of the flow, and consequently FLOPs at both training and inference time without decreasing performance in comparison to unconstrained flows. By expressing the structure inversion as a compilation pass in a probabilistic programming language, we are able to apply it in a novel way to models as complex as convolutional neural networks. Furthermore, we extend the training objective for CNFs in the context of inference amortization to the symmetric Kullback-Leibler divergence, and demonstrate its theoretical and practical advantages.}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={arXiv preprint arXiv:2203.02155},
  year={2022}
}


@article{webbFaithfulInversionGenerative2017,
  title = {Faithful {{Inversion}} of {{Generative Models}} for {{Effective Amortized Inference}}},
  author = {Webb, Stefan and Golinski, Adam and Zinkov, Robert and Siddharth, N. and Rainforth, Tom and Teh, Yee Whye and Wood, Frank},
  year = {2017},
  month = dec
}

@inproceedings{tashiro2021csdi,
  title = {{{CSDI}}: {{Conditional}} Score-Based Diffusion Models for Probabilistic Time Series Imputation},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Tashiro, Yusuke and Song, Jiaming and Song, Yang and Ermon, Stefano},
  year = {2021}
}

@book{cooperEngineeringCompiler2012,
  title = {Engineering a Compiler},
  author = {Cooper, Keith D. and Torczon, Linda},
  year = {2012},
  edition = {2nd ed},
  publisher = {{Elsevier/Morgan Kaufmann}},
  address = {{Amsterdam ; Boston}},
  isbn = {978-0-12-088478-0},
  lccn = {QA76.76.C65 C675 2012},
  keywords = {Compilers (Computer programs)}
}

@book{kollerProbabilisticGraphicalModels2009a,
  title = {Probabilistic Graphical Models: Principles and Techniques},
  shorttitle = {Probabilistic Graphical Models},
  author = {Koller, Daphne and Friedman, Nir},
  year = {2009},
  series = {Adaptive Computation and Machine Learning},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}},
  isbn = {978-0-262-01319-2},
  langid = {english},
  lccn = {QA279.5 .K65 2009},
  keywords = {Bayesian statistical decision theory,Graphic methods,Graphical modeling (Statistics)},
  file = {/home/christian/Zotero/storage/KNCYGHTT/Koller and Friedman - 2009 - Probabilistic graphical models principles and tec.pdf}
}

@article{loeligerIntroductionFactorGraphs2004,
  title = {An {{Introduction}} to Factor Graphs},
  author = {Loeliger, H.},
  year = {2004},
  month = jan,
  journal = {IEEE Signal Processing Magazine},
  volume = {21},
  number = {1},
  pages = {28--41},
  issn = {1053-5888},
  doi = {10.1109/MSP.2004.1267047},
  langid = {english}
}

@inproceedings{freyExtendingFactorGraphs2002,
  title = {Extending Factor Graphs so as to Unify Directed and Undirected Graphical Models},
  booktitle = {Proceedings of the {{Nineteenth}} Conference on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Frey, Brendan J.},
  year = {2002},
  month = aug,
  series = {{{UAI}}'03},
  pages = {257--264},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  abstract = {The two most popular types of graphical model are Bayesian networks (BNs) and Markov random fields (MRFs). These types of model offer complementary properties in model construction, expressing conditional independencies, expressing arbitrary factorizations of joint distributions, and formulating messagepassing inference algorithms. We show how the notation and semantics of factor graphs (a relatively new type of graphical model) can be extended so as to combine the strengths of BNs and MRFs. Every BN or MRF can be easily converted to a factor graph that expresses the same conditional independencies, expresses the same factorization of the joint distribution, and can be used for probabilistic inference through application of a single, simple message-passing algorithm. We describe a modified "Bayes-ball" algorithm for establishing conditional independence in factor graphs, and we show that factor graphs form a strict superset of BNs and MRFs. In particular, we give an example of a commonly-used model fragment, whose independencies cannot be represented in a BN or an MRF, but can be represented in a factor graph. For readers who use chain graphs, we describe a further extension of factor graphs that enables them to represent properties of chain graphs.},
  isbn = {978-0-12-705664-7}
}

@article{schulmanGradientEstimationUsing2016,
  title = {Gradient {{Estimation Using Stochastic Computation Graphs}}},
  author = {Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  year = {2016},
  month = jan,
  journal = {arXiv:1506.05254 [cs]},
  eprint = {1506.05254},
  eprinttype = {arxiv},
  primaryclass = {cs}
}

% experiments

@article{wingateLightweightImplementationsProbabilistic,
  title = {Lightweight {{Implementations}} of {{Probabilistic Programming Languages Via Transformational Compilation}}},
  author = {Wingate, David and Stuhlm{\"u}ller, Andreas and Goodman, Noah D},
  pages = {9},
  year = {2011},
}


@inproceedings{zhang2007binary,
  title={Binary matrix factorization with applications},
  author={Zhang, Zhongyuan and Li, Tao and Ding, Chris and Zhang, Xiangsun},
  booktitle={Seventh IEEE international conference on data mining (ICDM 2007)},
  pages={391--400},
  year={2007},
  organization={IEEE}
}

@article{prelic2006systematic,
  title={A systematic comparison and evaluation of biclustering methods for gene expression data},
  author={Preli{\'c}, Amela and Bleuler, Stefan and Zimmermann, Philip and Wille, Anja and B{\"u}hlmann, Peter and Gruissem, Wilhelm and Hennig, Lars and Thiele, Lothar and Zitzler, Eckart},
  journal={Bioinformatics},
  volume={22},
  number={9},
  pages={1122--1129},
  year={2006},
  publisher={Oxford University Press}
}

% related work

% generative models

@article{grathwohlFFJORDFreeformContinuous2018a,
  title = {{{FFJORD}}: {{Free-form Continuous Dynamics}} for {{Scalable Reversible Generative Models}}},
  shorttitle = {{{FFJORD}}},
  author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
  year = {2018},
  month = oct,
  journal = {arXiv:1810.01367 [cs, stat]},
  eprint = {1810.01367},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
}

@inproceedings{austinStructuredDenoisingDiffusion2021,
  title = {Structured {{Denoising Diffusion Models}} in {{Discrete State-Spaces}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
  year = {2021},
  month = may,
  abstract = {We explore diffusion models for discrete data, exploiting structured transition matrices to achieve strong results on image and text generation.},
  langid = {english},
  file = {/home/christian/Zotero/storage/2LD7MDTT/Austin et al. - 2021 - Structured Denoising Diffusion Models in Discrete .pdf;/home/christian/Zotero/storage/NQJCMSRC/forum.html}
}


@inproceedings{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis With Latent Diffusion Models}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2022},
  pages = {10684--10695},
  langid = {english},
}

% programming languages

@inproceedings{weissThinkingTransformers2021a,
  title = {Thinking like Transformers},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning, {{ICML}} 2021, 18-24 July 2021, Virtual Event},
  author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  editor = {Meila, Marina and Zhang, Tong},
  year = {2021},
  series = {Proceedings of Machine Learning Research},
  volume = {139},
  pages = {11080--11090},
  publisher = {{PMLR}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/icml/WeissGY21.bib},
  timestamp = {Wed, 25 Aug 2021 17:11:17 +0200}
}


@article{vandemeentIntroductionProbabilisticProgramming2018,
  title = {An {{Introduction}} to {{Probabilistic Programming}}},
  author = {{van de Meent}, Jan-Willem and Paige, Brooks and Yang, Hongseok and Wood, Frank},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.10756 [cs, stat]},
  eprint = {1809.10756},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning as a foundational computation is central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a simple first-order probabilistic programming language (PPL) whose programs define static-computation-graph, finite-variable-cardinality models. In the context of this restricted PPL we introduce fundamental inference algorithms and describe how they can be implemented in the context of models denoted by probabilistic programs. In the second part of this document, we introduce a higher-order probabilistic programming language, with a functionality analogous to that of established programming languages. This affords the opportunity to define models with dynamic computation graphs, at the cost of requiring inference methods that generate samples by repeatedly executing the program. Foundational inference algorithms for this kind of probabilistic programming language are explained in the context of an interface between program executions and an inference controller. This document closes with a chapter on advanced topics which we believe to be, at the time of writing, interesting directions for probabilistic programming research; directions that point towards a tight integration with deep neural network research and the development of systems for next-generation artificial intelligence applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Programming Languages,Statistics - Machine Learning}
}

% conditioning

@article{ivanovVariationalAutoencoderArbitrary2019,
  title = {Variational {{Autoencoder}} with {{Arbitrary Conditioning}}},
  author = {Ivanov, Oleg and Figurnov, Michael and Vetrov, Dmitry},
  year = {2019},
  month = jun,
  journal = {arXiv:1806.02382 [cs, stat]},
  eprint = {1806.02382},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a single neural probabilistic model based on variational autoencoder that can be conditioned on an arbitrary subset of observed features and then sample the remaining features in ``one shot''. The features may be both real-valued and categorical. Training of the model is performed by stochastic variational Bayes. The experimental evaluation on synthetic data, as well as feature imputation and image inpainting problems, shows the effectiveness of the proposed approach and diversity of the generated samples.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
}

% attention


@misc{tayEfficientTransformersSurvey2022,
  title = {Efficient {{Transformers}}: {{A Survey}}},
  shorttitle = {Efficient {{Transformers}}},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  year = {2022},
  month = mar,
  number = {arXiv:2009.06732},
  eprint = {2009.06732},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.06732},
  abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Retrieval,Computer Science - Machine Learning}
}

% memory

@misc{daiTransformerXLAttentiveLanguage2019,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed-Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  number = {arXiv:1901.02860},
  eprint = {1901.02860},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1901.02860},
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{royEfficientContentBasedSparse2020,
  title = {Efficient {{Content-Based Sparse Attention}} with {{Routing Transformers}}},
  author = {Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  year = {2020},
  month = oct,
  number = {arXiv:2003.05997},
  eprint = {2003.05997},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2003.05997},
  abstract = {Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to \$O\textbackslash left(n\^\{1.5\}d\textbackslash right)\$ from \$O\textbackslash left(n\^2d\textbackslash right)\$ for sequence length \$n\$ and hidden dimension \$d\$. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning}
}

@article{kitaevReformerEfficientTransformer2020,
  title = {Reformer: {{The Efficient Transformer}}},
  shorttitle = {Reformer},
  author = {Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  year = {2020},
  month = feb,
  journal = {arXiv:2001.04451 [cs, stat]},
  eprint = {2001.04451},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L2) to O(L log L), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/christian/Zotero/storage/8V2PCIWR/Kitaev et al. - 2020 - Reformer The Efficient Transformer.pdf}
}

% compute

@misc{childGeneratingLongSequences2019,
  title = {Generating {{Long Sequences}} with {{Sparse Transformers}}},
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  year = {2019},
  month = apr,
  number = {arXiv:1904.10509},
  eprint = {1904.10509},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1904.10509},
  abstract = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n \textbackslash sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
  archiveprefix = {arXiv}
}

@misc{beltagyLongformerLongDocumentTransformer2020,
  title = {Longformer: {{The Long-Document Transformer}}},
  shorttitle = {Longformer},
  author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
  year = {2020},
  month = dec,
  number = {arXiv:2004.05150},
  eprint = {2004.05150},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2004.05150},
  abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{zaheerBigBirdTransformers2021,
  title = {Big {{Bird}}: {{Transformers}} for {{Longer Sequences}}},
  shorttitle = {Big {{Bird}}},
  author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  year = {2021},
  month = jan,
  number = {arXiv:2007.14062},
  eprint = {2007.14062},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2007.14062},
  abstract = {Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning}
}

% neurosymbolism

@article{chaudhuriNeurosymbolicProgramming2021,
  title = {Neurosymbolic {{Programming}}},
  author = {Chaudhuri, Swarat and Ellis, Kevin and Polozov, Oleksandr and Singh, Rishabh and {Solar-Lezama}, Armando and Yue, Yisong},
  year = {2021},
  month = dec,
  journal = {Foundations and Trends\textregistered{} in Programming Languages},
  volume = {7},
  number = {3},
  pages = {158--243},
  publisher = {{Now Publishers, Inc.}},
  issn = {2325-1107, 2325-1131},
  doi = {10.1561/2500000049},
  abstract = {Neurosymbolic Programming},
  langid = {english}
}


% Sudoku

@article{yatoComplexityCompletenessFinding2003,
  title = {Complexity and {{Completeness}} of {{Finding Another Solution}} and {{Its Application}} to {{Puzzles}}},
  author = {Yato, Takayuki and Seta, Takahiro},
  year = {2003},
  pages = {8},
  abstract = {The Another Solution Problem (ASP) of a problem {$\Pi$} is the following problem: for a given instance x of {$\Pi$} and a solution s to it, find a solution to x other than s. (The notion of ASP as a new class of problems was first introduced by Ueda and Nagao.) In this paper we consider n-ASP, the problem to find another solution when n solutions are given. In particular we consider ASP-completeness, the completeness with respect to the parsimonious reductions which allow polynomial-time transformation of solutions.},
  langid = {english}
}

@article{wangSATNetBridgingDeep2019,
  title = {{{SATNet}}: {{Bridging}} Deep Learning and Logical Reasoning Using a Differentiable Satisfiability Solver},
  shorttitle = {{{SATNet}}},
  author = {Wang, Po-Wei and Donti, Priya L. and Wilder, Bryan and Kolter, Zico},
  year = {2019},
  month = may,
  journal = {arXiv:1905.12149 [cs, stat]},
  eprint = {1905.12149},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9 \texttimes{} 9 Sudoku solely from examples. We also solve a ``visual Sudoku'' problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.},
  archiveprefix = {arXiv},
  langid = {english}
}

@inproceedings{palmRecurrentRelationalNetworks2018,
  title = {Recurrent {{Relational Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Palm, Rasmus and Paquet, Ulrich and Winther, Ole},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {This paper is concerned with learning to solve tasks that require a chain of interde- pendent steps of relational inference, like answering complex questions about the relationships between objects, or solving puzzles where the smaller elements of a solution mutually constrain each other. We introduce the recurrent relational net- work, a general purpose module that operates on a graph representation of objects. As a generalization of Santoro et al. [2017]'s relational network, it can augment any neural network model with the capacity to do many-step relational reasoning. We achieve state of the art results on the bAbI textual question-answering dataset with the recurrent relational network, consistently solving 20/20 tasks. As bAbI is not particularly challenging from a relational reasoning point of view, we introduce Pretty-CLEVR, a new diagnostic dataset for relational reasoning. In the Pretty- CLEVR set-up, we can vary the question to control for the number of relational reasoning steps that are required to obtain the answer. Using Pretty-CLEVR, we probe the limitations of multi-layer perceptrons, relational and recurrent relational networks. Finally, we show how recurrent relational networks can learn to solve Sudoku puzzles from supervised training data, a challenging task requiring upwards of 64 steps of relational reasoning. We achieve state-of-the-art results amongst comparable methods by solving 96.6\% of the hardest Sudoku puzzles.}
}

@article{tonshoffOneModelAny2022,
  title = {One {{Model}}, {{Any CSP}}: {{Graph Neural Networks}} as {{Fast Global Search Heuristics}} for {{Constraint Satisfaction}}},
  shorttitle = {One {{Model}}, {{Any CSP}}},
  author = {T{\"o}nshoff, Jan and Kisin, Berke and Lindner, Jakob and Grohe, Martin},
  year = {2022},
  month = aug,
  doi = {10.48550/arXiv.2208.10227},
  abstract = {We propose a universal Graph Neural Network architecture which can be trained as an end-2-end search heuristic for any Constraint Satisfaction Problem (CSP). Our architecture can be trained unsupervised with policy gradient descent to generate problem specific heuristics for any CSP in a purely data driven manner. The approach is based on a novel graph representation for CSPs that is both generic and compact and enables us to process every possible CSP instance with one GNN, regardless of constraint arity, relations or domain size. Unlike previous RL-based methods, we operate on a global search action space and allow our GNN to modify any number of variables in every step of the stochastic search. This enables our method to properly leverage the inherent parallelism of GNNs. We perform a thorough empirical evaluation where we learn heuristics for well known and important CSPs from random data, including graph coloring, MaxCut, 3-SAT and MAX-k-SAT. Our approach outperforms prior approaches for neural combinatorial optimization by a substantial margin. It can compete with, and even improve upon, conventional search heuristics on test instances that are several orders of magnitude larger and structurally more complex than those seen during training.},
  langid = {english}
}

% conclusion

@article{selsamLearningSATSolver2019,
  title = {Learning a {{SAT Solver}} from {{Single-Bit Supervision}}},
  author = {Selsam, Daniel and Lamm, Matthew and B{\"u}nz, Benedikt and Liang, Percy and {de Moura}, Leonardo and Dill, David L.},
  year = {2019},
  month = mar,
  journal = {arXiv:1802.03685 [cs]},
  eprint = {1802.03685},
  eprinttype = {arxiv},
  primaryclass = {cs},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning}
}

# appendix

@inproceedings{kingmaAdamMethodStochastic2015,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {{ICLR}} 2015, San Diego, {{CA}}, {{USA}}, May 7-9, 2015, Conference Track Proceedings},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2015},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200}
}

@inproceedings{leeOptimizationGPUbasedSparse2020,
  title = {Optimization of {{GPU-based Sparse Matrix Multiplication}} for {{Large Sparse Networks}}},
  booktitle = {2020 {{IEEE}} 36th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Lee, Jeongmyung and Kang, Seokwon and Yu, Yongseung and Jo, Yong-Yeon and Kim, Sang-Wook and Park, Yongjun},
  year = {2020},
  month = apr,
  pages = {925--936},
  issn = {2375-026X},
  doi = {10.1109/ICDE48307.2020.00085}
}


@misc{mcguireThereNo16Clue2013,
  title = {There Is No 16-{{Clue Sudoku}}: {{Solving}} the {{Sudoku Minimum Number}} of {{Clues Problem}}},
  shorttitle = {There Is No 16-{{Clue Sudoku}}},
  author = {McGuire, Gary and Tugemann, Bastian and Civario, Gilles},
  year = {2013},
  month = sep,
  number = {arXiv:1201.0749},
  eprint = {1201.0749},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1201.0749}
}

@misc{chatgpt,
  author = {OpenAI},
  title = {ChatGPT},
  howpublished = {\url{https://chat.openai.com}},
  year = {2021},
  note = {Accessed on March 17, 2023}
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={the Journal of machine Learning research},
  volume={12},
  pages={2825--2830},
  year={2011},
  publisher={JMLR. org}
}

@article{salimans2022progressive,
  title={Progressive distillation for fast sampling of diffusion models},
  author={Salimans, Tim and Ho, Jonathan},
  journal={arXiv preprint arXiv:2202.00512},
  year={2022}
}

@article{karras2022elucidating,
  title={Elucidating the design space of diffusion-based generative models},
  author={Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
  journal={arXiv preprint arXiv:2206.00364},
  year={2022}
}

@article{kreuzer2021rethinking,
  title={Rethinking graph transformers with spectral attention},
  author={Kreuzer, Devin and Beaini, Dominique and Hamilton, Will and L{\'e}tourneau, Vincent and Tossou, Prudencio},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={21618--21629},
  year={2021}
}