\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Battaglia et~al.(2016)Battaglia, Pascanu, Lai, Rezende,
  et~al.]{BPLR16}
Battaglia, P., Pascanu, R., Lai, M., Rezende, D.~J., et~al.
\newblock Interaction networks for learning about objects, relations and
  physics.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4502--4510, 2016.

\bibitem[Brutzkus \& Globerson(2017)Brutzkus and Globerson]{BG17}
Brutzkus, A. and Globerson, A.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  605--614. JMLR. org, 2017.

\bibitem[Cao \& Gu(2020)Cao and Gu]{CG20}
Cao, Y. and Gu, Q.
\newblock Generalization error bounds of gradient descent for learning
  overparameterized deep relu networks.
\newblock 2020.

\bibitem[Chizat \& Bach(2018)Chizat and Bach]{CB18}
Chizat, L. and Bach, F.
\newblock A note on lazy training in supervised differentiable programming.
\newblock \emph{arXiv preprint arXiv:1812.07956}, 8, 2018.

\bibitem[Du et~al.(2017)Du, Lee, and Tian]{DLT17}
Du, S.~S., Lee, J.~D., and Tian, Y.
\newblock When is a convolutional filter easy to learn?
\newblock \emph{arXiv preprint, http://arxiv.org/abs/1709.06129}, 2017.

\bibitem[Du et~al.(2018)Du, Lee, Tian, Singh, and Poczos]{DLTPS17}
Du, S.~S., Lee, J.~D., Tian, Y., Singh, A., and Poczos, B.
\newblock Gradient descent learns one-hidden-layer cnn: Don't be afraid of
  spurious local minima.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1338--1347, 2018.

\bibitem[Du et~al.(2019)Du, Hou, Salakhutdinov, Poczos, Wang, and Xu]{DHPSWX19}
Du, S.~S., Hou, K., Salakhutdinov, R.~R., Poczos, B., Wang, R., and Xu, K.
\newblock Graph neural tangent kernel: Fusing graph neural networks with graph
  kernels.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5724--5734, 2019.

\bibitem[Duvenaud et~al.(2015)Duvenaud, Maclaurin, Iparraguirre, Bombarell,
  Hirzel, Aspuru-Guzik, and Adams]{DMIBH15}
Duvenaud, D.~K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T.,
  Aspuru-Guzik, A., and Adams, R.~P.
\newblock Convolutional networks on graphs for learning molecular fingerprints.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2224--2232, 2015.

\bibitem[Fu et~al.(2018)Fu, Chi, and Liang]{FCL19}
Fu, H., Chi, Y., and Liang, Y.
\newblock Guaranteed recovery of one-hidden-layer neural networks via cross
  entropy.
\newblock \emph{arXiv preprint arXiv:1802.06463}, 2018.

\bibitem[Ge et~al.(2018)Ge, Lee, and Ma]{GLM17}
Ge, R., Lee, J.~D., and Ma, T.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=BkwHObbRZ}.

\bibitem[Gilbert et~al.(2005)Gilbert, Muthukrishnan, and Strauss]{GMS05}
Gilbert, A.~C., Muthukrishnan, S., and Strauss, M.
\newblock Improved time bounds for near-optimal sparse fourier representation
  via sampling.
\newblock In \emph{Proc. SPIE}, 2005.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{GSRVD17}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1263--1272. JMLR. org, 2017.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and Leskovec]{HYL17}
Hamilton, W., Ying, Z., and Leskovec, J.
\newblock Inductive representation learning on large graphs.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1024--1034, 2017.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{JGH18}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8571--8580, 2018.

\bibitem[Janson(2004)]{J04}
Janson, S.
\newblock Large deviations for sums of partly dependent random variables.
\newblock \emph{Random Structures \& Algorithms}, 24\penalty0 (3):\penalty0
  234--248, 2004.

\bibitem[Kipf \& Welling(2017)Kipf and Welling]{KW17}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{Proc. International Conference on Learning (ICLR)}, 2017.

\bibitem[Kuleshov et~al.(2015)Kuleshov, Chaganty, and Liang]{KCL15}
Kuleshov, V., Chaganty, A., and Liang, P.
\newblock Tensor factorization via matrix factorization.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  507--516,
  2015.

\bibitem[Li et~al.(2016)Li, Tarlow, Brockschmidt, and Zemel]{LTBZ16}
Li, Y., Tarlow, D., Brockschmidt, M., and Zemel, R.
\newblock Gated graph sequence neural networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2016.

\bibitem[Monfardini et~al.(2006)Monfardini, Di~Massa, Scarselli, and
  Gori]{MDSG06}
Monfardini, G., Di~Massa, V., Scarselli, F., and Gori, M.
\newblock Graph neural networks for object localization.
\newblock \emph{Frontiers in Artificial Intelligence and Applications},
  141:\penalty0 665, 2006.

\bibitem[Morris et~al.(2019)Morris, Ritzert, Fey, Hamilton, Lenssen, Rattan,
  and Grohe]{MRFMW19}
Morris, C., Ritzert, M., Fey, M., Hamilton, W.~L., Lenssen, J.~E., Rattan, G.,
  and Grohe, M.
\newblock Weisfeiler and leman go neural: Higher-order graph neural networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  4602--4609, 2019.

\bibitem[Nitanda \& Suzuki(2019)Nitanda and Suzuki]{NS19}
Nitanda, A. and Suzuki, T.
\newblock Refined generalization analysis of gradient descent for
  over-parameterized two-layer neural networks with smooth activations on
  classification problems.
\newblock \emph{arXiv preprint arXiv:1905.09870}, 2019.

\bibitem[Polyak(1987)]{P87}
Polyak, B.~T.
\newblock Introduction to optimization.
\newblock \emph{New York: Optimization Software, Inc}, 1987.

\bibitem[Safran \& Shamir(2018)Safran and Shamir]{SS17}
Safran, I. and Shamir, O.
\newblock Spurious local minima are common in two-layer relu neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4430--4438, 2018.

\bibitem[Scarselli et~al.(2008)Scarselli, Gori, Tsoi, Hagenbuchner, and
  Monfardini]{SGAHM08}
Scarselli, F., Gori, M., Tsoi, A.~C., Hagenbuchner, M., and Monfardini, G.
\newblock The graph neural network model.
\newblock \emph{IEEE Transactions on Neural Networks}, 20\penalty0
  (1):\penalty0 61--80, 2008.

\bibitem[Shamir(2018)]{Sh18}
Shamir, O.
\newblock Distribution-specific hardness of learning neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 1135--1163, 2018.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2018)Veli{\v{c}}kovi{\'c}, Cucurull,
  Casanova, Romero, Lio, and Bengio]{VCCRLB18}
Veli{\v{c}}kovi{\'c}, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and
  Bengio, Y.
\newblock Graph attention networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Verma \& Zhang(2019)Verma and Zhang]{VZ19}
Verma, S. and Zhang, Z.-L.
\newblock Stability and generalization of graph convolutional neural networks.
\newblock In \emph{Proceedings of the 25th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  1539--1548, 2019.

\bibitem[Vershynin(2010)]{V2010}
Vershynin, R.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint arXiv:1011.3027}, 2010.

\bibitem[Wu et~al.(2019)Wu, Souza, Zhang, Fifty, Yu, and Weinberger]{WSZFYW19}
Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Weinberger, K.
\newblock Simplifying graph convolutional networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6861--6871, 2019.

\bibitem[Xu et~al.(2018)Xu, Li, Tian, Sonobe, Kawarabayashi, and
  Jegelka]{XLTSTKKJ18}
Xu, K., Li, C., Tian, Y., Sonobe, T., Kawarabayashi, K.-i., and Jegelka, S.
\newblock Representation learning on graphs with jumping knowledge networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5453--5462, 2018.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{XHLJ19}
Xu, K., Hu, W., Leskovec, J., and Jegelka, S.
\newblock How powerful are graph neural networks?
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Ying et~al.(2018)Ying, He, Chen, Eksombatchai, Hamilton, and
  Leskovec]{YHCEHL18}
Ying, R., He, R., Chen, K., Eksombatchai, P., Hamilton, W.~L., and Leskovec, J.
\newblock Graph convolutional neural networks for web-scale recommender
  systems.
\newblock In \emph{Proceedings of the 24th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  974--983, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Wang, and Gu]{ZYWG18}
Zhang, X., Yu, Y., Wang, L., and Gu, Q.
\newblock Learning one-hidden-layer relu networks via gradient descent.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  1524--1534, 2019.

\bibitem[Zhong et~al.(2017{\natexlab{a}})Zhong, Song, and Dhillon]{ZSD17}
Zhong, K., Song, Z., and Dhillon, I.~S.
\newblock Learning non-overlapping convolutional neural networks with multiple
  kernels.
\newblock \emph{arXiv preprint arXiv:1711.03440}, 2017{\natexlab{a}}.

\bibitem[Zhong et~al.(2017{\natexlab{b}})Zhong, Song, Jain, Bartlett, and
  Dhillon]{ZSJB17}
Zhong, K., Song, Z., Jain, P., Bartlett, P.~L., and Dhillon, I.~S.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  4140--4149. JMLR. org,
  https://arxiv.org/abs/1706.03175, 2017{\natexlab{b}}.

\bibitem[Zhong et~al.(2017{\natexlab{c}})Zhong, Song, Jain, Bartlett, and
  Dhillon]{ZSJB17_v2}
Zhong, K., Song, Z., Jain, P., Bartlett, P.~L., and Dhillon, I.~S.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock \emph{arXiv preprint, http://arxiv.org/abs/1706.03175},
  2017{\natexlab{c}}.

\end{thebibliography}
