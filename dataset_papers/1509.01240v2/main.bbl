\begin{thebibliography}{10}

\bibitem{Arora15}
S.~Arora, R.~Ge, T.~Ma, and A.~Moitra.
\newblock Simple, efficient, and neural algorithms for sparse coding.
\newblock In {\em Proceedings of the Conference on Learning Theory (COLT)},
  2015.

\bibitem{Bottou98}
L.~Bottou.
\newblock Online algorithms and stochastic approximations.
\newblock In D.~Saad, editor, {\em Online Learning and Neural Networks}.
  Cambridge University Press, Cambridge, UK, 1998.

\bibitem{Bottou08}
L.~Bottou and O.~Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In {\em Neural Information Processing Systems}, 2008.

\bibitem{BousquetE02}
O.~Bousquet and A.~Elisseeff.
\newblock Stability and generalization.
\newblock {\em Journal of Machine Learning Research}, 2:499--526, 2002.

\bibitem{bucklew1993weak}
J.~Bucklew, T.~G. Kurtz, and W.~Sethares.
\newblock Weak convergence and local stability properties of fixed step size
  recursive algorithms.
\newblock {\em IEEE Transactions on Information Theory}, 39(3):966--978, 1993.

\bibitem{Combettes05}
P.~L. Combettes and V.~R. Wajs.
\newblock Signal recovery by proximal forward-backward splitting.
\newblock {\em Multiscale Modeling and Simulation}, 4(4):1168--1200, 2005.

\bibitem{devolder2014first}
O.~Devolder, F.~Glineur, and Y.~Nesterov.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock {\em Mathematical Programming}, 146(1-2):37--75, 2014.

\bibitem{DevroyeW79}
L.~P. Devroye and T.~Wagner.
\newblock Distribution-free performance bounds for potential function rules.
\newblock {\em Information Theory, IEEE Transactions on}, 25(5):601--604, Sep
  1979.

\bibitem{Elisseeff05}
A.~Elisseeff, T.~Evgeniou, and M.~Pontil.
\newblock Stability of randomized learning algorithms.
\newblock {\em Journal of Machine Learning Research}, 6:55--79, 2005.

\bibitem{FreundSc99}
Y.~Freund and R.~E. Schapire.
\newblock Large margin classification using the perceptron algorithm.
\newblock {\em Machine Learning}, 37(3):277--296, 1999.

\bibitem{frostig15competing}
R.~Frostig, R.~Ge, S.~M. Kakade, and A.~Sidford.
\newblock Competing with the empirical risk minimizer in a single pass.
\newblock In {\em Conference on Learning Theory (COLT)}, 2015.

\bibitem{ghadimi2013stochastic}
S.~Ghadimi and G.~Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{Hazan06}
E.~Hazan, A.~Kalai, S.~Kale, and A.~Agarwal.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock In {\em Proceedings of the 19th Annual Conference on Learning
  Theory}, 2006.

\bibitem{HazanKale14}
E.~Hazan and S.~Kale.
\newblock Beyond the regret minimization barrier: optimal algorithms for
  stochastic strongly-convex optimization.
\newblock {\em Journal of Machine Learning Research}, 15(1):2489--2512, 2014.

\bibitem{Jamieson15}
K.~Jamieson and A.~Talwalkar.
\newblock Non-stochastic best arm identification and hyperparameter
  optimization.
\newblock Preprint available at \url{arXiv:1502.07943v1}, 2015.

\bibitem{Janzamin15}
M.~Janzamin, H.~Sedghi, and A.~Anandkumar.
\newblock Generalization bounds for neural networks through tensor
  factorization.
\newblock Preprint available at \url{arXiv:1506.08473}, 2015.

\bibitem{karnin2013almost}
Z.~Karnin, T.~Koren, and O.~Somekh.
\newblock Almost optimal exploration in multi-armed bandits.
\newblock In {\em Proceedings of the 30th International Conference on Machine
  Learning (ICML-13)}, 2013.

\bibitem{KearnsR99}
M.~J. Kearns and D.~Ron.
\newblock Algorithmic stability and sanity-check bounds for leave-one-out
  cross-validation.
\newblock {\em Neural Computation}, 11(6):1427--1453, 1999.

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Proc.~NIPS}, pages 1097--1105, 2012.

\bibitem{KroghHe92}
A.~Krogh and J.~A. Hertz.
\newblock A simple weight decay can improve generalization.
\newblock In {\em Proc.~NIPS}, pages 950--957, 1992.

\bibitem{KushnerBook}
H.~J. Kushner and G.~G. Yin.
\newblock {\em Stochastic Approximation and Recursive Algorithms and
  Applications}.
\newblock Springer-Verlag, New York, second edition, 2003.

\bibitem{Lan12}
G.~Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock {\em Mathematical Programming}, 133(1-2):365--397, 2012.

\bibitem{Lessard14}
L.~Lessard, B.~Recht, and A.~Packard.
\newblock Analysis and design of optimization algorithms via integral quadratic
  constraints.
\newblock To appear in \emph{SIAM Journal on Optimization}. Preprint available
  at \url{arxiv:1408.3595}., 2014.

\bibitem{livni2014computational}
R.~Livni, S.~Shalev-Shwartz, and O.~Shamir.
\newblock On the computational efficiency of training neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  855--863, 2014.

\bibitem{marcus1993building}
M.~P. Marcus, M.~A. Marcinkiewicz, and B.~Santorini.
\newblock Building a large annotated corpus of {E}nglish: {The Penn Treebank}.
\newblock {\em Computational linguistics}, 19(2):313--330, 1993.

\bibitem{MukherjeeNPR06}
S.~Mukherjee, P.~Niyogi, T.~Poggio, and R.~M. Rifkin.
\newblock Learning theory: stability is sufficient for generalization and
  necessary and sufficient for consistency of empirical risk minimization.
\newblock {\em Adv. Comput. Math.}, 25(1-3):161--193, 2006.

\bibitem{Nemirovski09}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em SIAM Journal on Optimization}, 19(4):1574--1609, 2009.

\bibitem{NY78}
A.~Nemirovski and D.~B. Yudin.
\newblock On {C}ezari's convergence of the steepest descent method for
  approximating saddle point of convex-concave functions.
\newblock In {\em Soviet Mathetmatics Doklady}, volume~19, 1978.

\bibitem{NY83}
A.~Nemirovski and D.~B. Yudin.
\newblock {\em Problem complexity and method efficiency in optimization}.
\newblock Wiley Interscience, 1983.

\bibitem{NesterovBook}
Y.~Nesterov.
\newblock {\em Introductory lectures on convex optimization}, volume~87 of {\em
  Applied Optimization}.
\newblock Kluwer Academic Publishers, Boston, MA, 2004.
\newblock A basic course.

\bibitem{NeyshaburTS15}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock In {\em In Proc.~$3$rd ICLR}, 2015.

\bibitem{NissimStemmer15}
K.~Nissim and U.~Stemmer.
\newblock On the generalization properties of differential privacy.
\newblock Preprint available at \url{arXiv:1504.05800}, 2015.

\bibitem{parikh2013proximal}
N.~Parikh and S.~P. Boyd.
\newblock Proximal algorithms.
\newblock {\em Foundations and Trends in optimization}, 1(3):123--231, 2013.

\bibitem{PolyakBook}
B.~T. Polyak.
\newblock Introduction to optimization.
\newblock {\em Optimization Software, Inc.}, 1987.

\bibitem{Rakhlin11}
A.~Rakhlin, O.~Shamir, and K.~Sridharan.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In {\em Proceedings of the 29th International Conference on Machine
  Learning}, 2012.
\newblock Extended version at \url{arxiv:1109.5647}.

\bibitem{Rockafellar76}
R.~T. Rockafellar.
\newblock Monotone operators and the proximal point algorithm.
\newblock {\em {SIAM} Journal on Control and Optimization}, 14(5):877--898,
  1976.

\bibitem{Rosasco14}
L.~Rosasco and S.~Villa.
\newblock Learning with incremental iterative regularization.
\newblock Preprint available at \url{arXiv:1405.0042v2}, 2014.

\bibitem{Rumelhart85}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams.
\newblock Learning internal representations by error propagation.
\newblock In {\em Parallel Distributed Processing--Explorations in the
  Microstructure of Cognition}, pages 318--362. MIT Press, Cambridge, MA, 1986.

\bibitem{SSSS10}
S.~Shalev{-}Shwartz, O.~Shamir, N.~Srebro, and K.~Sridharan.
\newblock Learnability, stability and uniform convergence.
\newblock {\em Journal of Machine Learning Research}, 11:2635--2670, 2010.

\bibitem{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em The Journal of Machine Learning Research}, 15(1):1929--1958,
  2014.

\bibitem{SzegedyInception}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich.
\newblock Going deeper with convolutions.
\newblock In {\em Proceedings of the 2015 Conference on Computer Vision and
  Pattern Recognition}, 2015.

\bibitem{zaremba2014recurrent}
W.~Zaremba, I.~Sutskever, and O.~Vinyals.
\newblock Recurrent neural network regularization.
\newblock Technical report, 2014.
\newblock Preprint available at \url{arxiv:1409.2329}.

\end{thebibliography}
