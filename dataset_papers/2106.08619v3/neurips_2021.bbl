\begin{thebibliography}{10}

\bibitem{luxburg2004distance}
Ulrike~von Luxburg and Olivier Bousquet.
\newblock Distance-based classification with lipschitz functions.
\newblock {\em Journal of Machine Learning Research}, 5(Jun):669--695, 2004.

\bibitem{hestness2017deep}
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,
  Hassan Kianinejad, Md~Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou.
\newblock Deep learning scaling is predictable, empirically.
\newblock {\em arXiv preprint arXiv:1712.00409}, 2017.

\bibitem{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock {\em Advances in neural information processing systems},
  25:1097--1105, 2012.

\bibitem{spigler2019asymptotic}
Stefano Spigler, Mario Geiger, and Matthieu Wyart.
\newblock Asymptotic learning curves of kernel methods: empirical data versus
  teacherâ€“student paradigm.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2020(12):124001, December 2020.
\newblock Publisher: IOP Publishing.

\bibitem{biederman1987recognition}
Irving Biederman.
\newblock Recognition-by-components: a theory of human image understanding.
\newblock {\em Psychological review}, 94(2):115, 1987.

\bibitem{poggio2017and}
Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, and Qianli
  Liao.
\newblock Why and when can deep-but not shallow-networks avoid the curse of
  dimensionality: a review.
\newblock {\em International Journal of Automation and Computing},
  14(5):503--519, 2017.

\bibitem{deza2020hierarchically}
Arturo Deza, Qianli Liao, Andrzej Banburski, and Tomaso Poggio.
\newblock Hierarchically compositional tasks and deep convolutional networks,
  2020.

\bibitem{bietti2021approximation}
Alberto Bietti.
\newblock On approximation in deep convolutional networks: a kernel
  perspective, 2021.

\bibitem{Neyshabur2020towards}
Behnam Neyshabur.
\newblock Towards learning convolutions from scratch.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 8078--8088. Curran Associates, Inc., 2020.

\bibitem{fukushima1975cognitron}
Kunihiko Fukushima.
\newblock Cognitron: A self-organizing multilayered neural network.
\newblock {\em Biological cybernetics}, 20(3):121--136, 1975.

\bibitem{lecun1989generalization}
Yann LeCun et~al.
\newblock Generalization and network design strategies.
\newblock {\em Connectionism in perspective}, 19:143--155, 1989.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Proceedings of the 32Nd International Conference on Neural
  Information Processing Systems}, NIPS'18, pages 8580--8589, USA, 2018. Curran
  Associates Inc.

\bibitem{Du2019}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide {Neural} {Networks} of {Any} {Depth} {Evolve} as {Linear}
  {Models} {Under} {Gradient} {Descent}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}
  32}, pages 8572--8583. Curran Associates, Inc., 2019.

\bibitem{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ~R Salakhutdinov, and
  Ruosong Wang.
\newblock On {Exact} {Computation} with an {Infinitely} {Wide} {Neural} {Net}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}
  32}, pages 8141--8150. Curran Associates, Inc., 2019.

\bibitem{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2937--2947, 2019.

\bibitem{bordelon2020spectrum}
Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan.
\newblock Spectrum {Dependent} {Learning} {Curves} in {Kernel} {Regression} and
  {Wide} {Neural} {Networks}.
\newblock In {\em International {Conference} on {Machine} {Learning}}, pages
  1024--1034. PMLR, November 2020.
\newblock ISSN: 2640-3498.

\bibitem{canatar2021spectral}
Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan.
\newblock Spectral bias and task-model alignment explain generalization in
  kernel regression and infinitely wide neural networks.
\newblock {\em Nature Communications}, 12(1):1--12, 2021.

\bibitem{loureiro2021capturing}
Bruno Loureiro, C{\'e}dric Gerbelot, Hugo Cui, Sebastian Goldt, Florent
  Krzakala, Marc M{\'e}zard, and Lenka Zdeborov{\'a}.
\newblock Capturing the learning curves of generic features maps for realistic
  data sets with a teacher-student model.
\newblock {\em arXiv preprint arXiv:2102.08127}, 2021.

\bibitem{micchelli1979design}
Charles~A Micchelli and Grace Wahba.
\newblock Design problems for optimal surface interpolation.
\newblock Technical report, WISCONSIN UNIV-MADISON DEPT OF STATISTICS, 1979.

\bibitem{jacot2020kernel}
Arthur Jacot, Berfin Simsek, Francesco Spadaro, Clement Hongler, and Franck
  Gabriel.
\newblock Kernel alignment risk estimator: Risk prediction from training data.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 15568--15578. Curran Associates, Inc., 2020.

\bibitem{kondor2018generalization}
Risi Kondor and Shubhendu Trivedi.
\newblock On the generalization of equivariance and convolution in neural
  networks to the action of compact groups.
\newblock In {\em International Conference on Machine Learning}, pages
  2747--2755. PMLR, 2018.

\bibitem{zhou2018building}
H.~H. Zhou, Y.~Xiong, and V.~Singh.
\newblock Building bayesian neural networks with blocks: On structure,
  interpretability and uncertainty.
\newblock {\em arXiv preprint, arXiv:1806.03563}, 2018.

\bibitem{Poggio2020theo}
Tommaso Poggio, Andrzej Banburski, and Qianli Liao.
\newblock Theoretical issues in deep networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  117(48):30039--30045, 2020.

\bibitem{malach2021computational}
Eran Malach and Shai Shalev-Shwartz.
\newblock Computational separation between convolutional and fully-connected
  networks.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{mairal2016end}
Julien Mairal.
\newblock End-to-end kernel learning with supervised convolutional kernel
  networks.
\newblock {\em arXiv preprint arXiv:1605.06265}, 2016.

\bibitem{bietti2019inductive}
Alberto Bietti and Julien Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock {\em arXiv preprint arXiv:1905.12173}, 2019.

\bibitem{mei2021learning}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Learning with invariances in random features and kernel models, 2021.

\bibitem{kanagawa2018gaussian}
Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath~K
  Sriperumbudur.
\newblock Gaussian processes and kernel methods: A review on connections and
  equivalences.
\newblock {\em arXiv preprint arXiv:1807.02582}, 2018.

\bibitem{bach2017breaking}
Francis Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock {\em The Journal of Machine Learning Research}, 18(1):629--681, 2017.

\bibitem{paccolat2020isotropic}
Jonas Paccolat, Stefano Spigler, and Matthieu Wyart.
\newblock How isotropic kernels perform on simple invariants.
\newblock {\em Machine Learning: Science and Technology}, 2(2):025020, March
  2021.
\newblock Publisher: IOP Publishing.

\bibitem{geifman2020similarity}
Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Basri
  Ronen.
\newblock On the similarity between the laplace and neural tangent kernels.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 1451--1461. Curran Associates, Inc., 2020.

\bibitem{Neal1996}
Radford~M. Neal.
\newblock {\em Bayesian Learning for Neural Networks}.
\newblock Springer-Verlag New York, Inc., Secaucus, NJ, USA, 1996.

\bibitem{williams1997computing}
Christopher~KI Williams.
\newblock Computing with infinite networks.
\newblock {\em Advances in neural information processing systems}, pages
  295--301, 1997.

\bibitem{lee2017deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S Schoenholz, Jeffrey
  Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock {\em arXiv preprint arXiv:1711.00165}, 2017.

\bibitem{cho2009kernel}
Youngmin Cho and Lawrence Saul.
\newblock Kernel methods for deep learning.
\newblock In Y.~Bengio, D.~Schuurmans, J.~Lafferty, C.~Williams, and
  A.~Culotta, editors, {\em Advances in Neural Information Processing Systems},
  volume~22. Curran Associates, Inc., 2009.

\bibitem{scholkopf2001learning}
Bernhard Scholkopf and Alexander~J Smola.
\newblock {\em Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2001.

\bibitem{widom1964asymptotic}
Harold Widom.
\newblock Asymptotic behavior of the eigenvalues of certain integral equations.
  ii.
\newblock {\em Archive for Rational Mechanics and Analysis}, 17(3):215--229,
  1964.

\bibitem{Mezard87}
Marc {M{\'e}zard}, Giorgio Parisi, and Miguel Virasoro.
\newblock {\em Spin glass theory and beyond: An Introduction to the Replica
  Method and Its Applications}, volume~9.
\newblock World Scientific Publishing Company, 1987.

\bibitem{potters2020first}
Marc Potters and Jean-Philippe Bouchaud.
\newblock {\em A First Course in Random Matrix Theory: For Physicists,
  Engineers and Data Scientists}.
\newblock Cambridge University Press, 2020.

\bibitem{jacot2020implicit}
Arthur Jacot, Berfin Simsek, Francesco Spadaro, Cl\'ement Hongler, and Franck
  Gabriel.
\newblock Implicit regularization of random feature models.
\newblock In {\em International Conference on Machine Learning}, pages
  4631--4640, 2020.

\end{thebibliography}
