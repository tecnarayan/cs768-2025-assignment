@article{EgoSchema,
  title={Egoschema: A diagnostic benchmark for very long-form video language understanding},
  author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}
@article{sevila,
  title={Self-chained image-language model for video localization and question answering},
  author={Yu, Shoubin and Cho, Jaemin and Yadav, Prateek and Bansal, Mohit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}


@inproceedings{li2023blip,
    author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  booktitle={International conference on machine learning},
  pages={19730-19742},
  year={2023},
  organization={PMLR}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{wang2021simvlm,
  title={Simvlm: Simple visual language model pretraining with weak supervision},
  author={Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
  journal={arXiv preprint arXiv:2108.10904},
  year={2021}
}


% Image-Languge Model
@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}





% video-language model

@inproceedings{xu2023mplug,
  title={{mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video}},
  author={Xu, Haiyang and Ye, Qinghao and Yan, Ming and Shi, Yaya and Ye, Jiabo and Xu, Yuanhong and Li, Chenliang and Bi, Bin and Qian, Qi and Wang, Wei and others},
  journal={arXiv preprint arXiv:2302.00402},
  year={2023},
  booktitle={ICML}
}

@inproceedings{frozenbilm,
title = {Zero-Shot Video Question Answering via Frozen Bidirectional Language Models},
author = {Antoine Yang and Antoine Miech and Josef Sivic and Ivan Laptev and Cordelia Schmid},
booktitle={NeurIPS},
year = {2022}}




@article{wang2023vamos,
  title={Vamos: Versatile Action Models for Video Understanding},
  author={Wang, Shijie and Zhao, Qi and Do, Minh Quan and Agarwal, Nakul and Lee, Kwonjoon and Sun, Chen},
  journal={arXiv preprint arXiv:2311.13627},
  year={2023}
}

@article{LLovi,
  title={A simple llm framework for long-range video question-answering},
  author={Zhang, Ce and Lu, Taixi and Islam, Md Mohaiminul and Wang, Ziyang and Yu, Shoubin and Bansal, Mohit and Bertasius, Gedas},
  journal={arXiv preprint arXiv:2312.17235},
  year={2023}
}


@article{agent_proviq,
  title={Zero-Shot Video Question Answering with Procedural Programs},
  author={Choudhury, Rohan and Niinuma, Koichiro and Kitani, Kris M and Jeni, L{\'a}szl{\'o} A},
  journal={arXiv preprint arXiv:2312.00937},
  year={2023}
}



@article{gemini1.5,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Reid, Machel and Savinov, Nikolay and Teplyashin, Denis and Lepikhin, Dmitry and Lillicrap, Timothy and Alayrac, Jean-baptiste and Soricut, Radu and Lazaridou, Angeliki and Firat, Orhan and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@inproceedings{nextqa,
  title={{NExT-QA: Next phase of question-answering to explaining temporal actions}},
  author={Xiao, Junbin and Shang, Xindi and Yao, Angela and Chua, Tat-Seng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9777--9786},
  year={2021}
}

@inproceedings{star,
  title={Star: A benchmark for situated reasoning in real-world videos},
  author={Wu, Bo and Yu, Shoubin and Chen, Zhenfang and Tenenbaum, Joshua B and Gan, Chuang},
  booktitle={Thirty-fifth conference on neural information processing systems datasets and benchmarks track (Round 2)},
  year={2021}
}

@article{howtoqa,
  title={Hero: Hierarchical encoder for video+ language omni-representation pre-training},
  author={Li, Linjie and Chen, Yen-Chun and Cheng, Yu and Gan, Zhe and Yu, Licheng and Liu, Jingjing},
  journal={arXiv preprint arXiv:2005.00200},
  year={2020}
}



@article{imagegrid,
  title={An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM},
  author={Kim, Wonkyun and Choi, Changin and Lee, Wonseok and Rhee, Wonjong},
  journal={arXiv preprint arXiv:2403.18406},
  year={2024}
}

@inproceedings{lavila,
  title={Learning video representations from large language models},
  author={Zhao, Yue and Misra, Ishan and Kr{\"a}henb{\"u}hl, Philipp and Girdhar, Rohit},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6586--6597},
  year={2023}
}

@inproceedings{vipergpt,
  title={Vipergpt: Visual inference via python execution for reasoning},
  author={Sur{\'\i}s, D{\'\i}dac and Menon, Sachit and Vondrick, Carl},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11888--11898},
  year={2023}
}

@inproceedings{li2023decap,
  title={DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training},
  author={Li, Wei and Zhu, Linchao and Wen, Longyin and Yang, Yi},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@article{mindthegap,
  title={Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning},
  author={Liang, Victor Weixin and Zhang, Yuhui and Kwon, Yongchan and Yeung, Serena and Zou, James Y},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17612--17625},
  year={2022}
}

@inproceedings{gapnoimages,
  title={I Can't Believe There's No Images! Learning Visual Tasks Using only Language Supervision},
  author={Gu, Sophia and Clark, Christopher and Kembhavi, Aniruddha},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2672--2683},
  year={2023}
}

@inproceedings{gapC3,
  title={Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal Data},
  author={Zhang, Yuhui and Sui, Elaine and Yeung-Levy, Serena},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@inproceedings{gapcapdec,
  title={Text-Only Training for Image Captioning using Noise-Injected CLIP},
  author={Nukrai, David and Mokady, Ron and Globerson, Amir},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={4055--4063},
  year={2022}
}


@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18995--19012},
  year={2022}
}


@article{laion5b,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25278--25294},
  year={2022}
}


@inproceedings{miech2019howto100m,
  title={Howto100m: Learning a text-video embedding by watching hundred million narrated video clips},
  author={Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={2630--2640},
  year={2019}
}


@inproceedings{
wang2023internvid,
title={InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation},
author={Yi Wang and Yinan He and Yizhuo Li and Kunchang Li and Jiashuo Yu and Xin Ma and Xinhao Li and Guo Chen and Xinyuan Chen and Yaohui Wang and Ping Luo and Ziwei Liu and Yali Wang and Limin Wang and Yu Qiao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}


@inproceedings{webvid,
  title={Frozen in time: A joint video and image encoder for end-to-end retrieval},
  author={Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1728--1738},
  year={2021}
}

@article{llava,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2023}
}

@article{cliprewrire,
  title={Improving clip training with language rewrites},
  author={Fan, Lijie and Krishnan, Dilip and Isola, Phillip and Katabi, Dina and Tian, Yonglong},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@inproceedings{instructpix2pix,
  title={Instructpix2pix: Learning to follow image editing instructions},
  author={Brooks, Tim and Holynski, Aleksander and Efros, Alexei A},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18392--18402},
  year={2023}
}

@inproceedings{minigpt,
  title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}



@article{chen2023videollm,
  title={{VideoLLM}: Modeling video sequence with large language models},
  author={Chen, Guo and Zheng, Yin-Dong and Wang, Jiahao and Xu, Jilan and Huang, Yifei and Pan, Junting and Wang, Yi and Wang, Yali and Qiao, Yu and Lu, Tong and others},
  journal={arXiv preprint arXiv:2305.13292},
  year={2023}
}

@article{agent_chatvideo,
  title={Chatvideo: A tracklet-centric multimodal and versatile video understanding system},
  author={Wang, Junke and Chen, Dongdong and Luo, Chong and Dai, Xiyang and Yuan, Lu and Wu, Zuxuan and Jiang, Yu-Gang},
  journal={arXiv preprint arXiv:2304.14407},
  year={2023}
}



@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}



@inproceedings{coco,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European Conference Computer Vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@inproceedings{align,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International conference on machine learning},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}

@inproceedings{cc12m,
  title={Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts},
  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3558--3568},
  year={2021}
}

@article{yu2022coca,
  title={CoCa: Contrastive captioners are image-text foundation models},
  author={Yu, Jiahui and Wang, Zirui and Vasudevan, Vijay and Yeung, Legg and Seyedhosseini, Mojtaba and Wu, Yonghui},
  journal={arXiv preprint arXiv:2205.01917},
  year={2022}
}

@article{yan2022videococa,
  title={{VideoCoCa}: Video-text modeling with zero-shot transfer from contrastive captioners},
  author={Yan, Shen and Zhu, Tao and Wang, Zirui and Cao, Yuan and Zhang, Mi and Ghosh, Soham and Wu, Yonghui and Yu, Jiahui},
  journal={arXiv preprint arXiv:2212.04979},
  year={2022}
}

@inproceedings{cc3m,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year={2018}
}


@inproceedings{xu2016msrvtt,
  title={{MSR-VTT}: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5288--5296},
  year={2016}
}

@inproceedings{wang2019vatex,
  title={Vatex: A large-scale, high-quality multilingual dataset for video-and-language research},
  author={Wang, Xin and Wu, Jiawei and Chen, Junkun and Li, Lei and Wang, Yuan-Fang and Wang, William Yang},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4581--4591},
  year={2019}
}

@inproceedings{caba2015activitynet,
  title={{ActivityNet}: A large-scale video benchmark for human activity understanding},
  author={Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={961--970},
  year={2015}
}

@inproceedings{blip1,
  title={{BLIP}: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@article{gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{miller1995wordnet,
  title={{WordNet}: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  year={1995},
  publisher={ACM New York, NY, USA}
}


@article{anil2023palm,
  title={{PaLM} 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}


@article{eva,
  title={{EVA-CLIP-18B}: Scaling CLIP to 18 Billion Parameters},
  author={Sun, Quan and Wang, Jinsheng and Yu, Qiying and Cui, Yufeng and Zhang, Fan and Zhang, Xiaosong and Wang, Xinlong},
  journal={arXiv preprint arXiv:2402.04252},
  year={2024}
}


@inproceedings{song2023moviechat,
  title={{MovieChat}: From dense token to sparse memory for long video understanding},
  author={Song, Enxin and Chai, Wenhao and Wang, Guanhong and Zhang, Yucheng and Zhou, Haoyang and Wu, Feiyang and Guo, Xun and Ye, Tian and Lu, Yan and Hwang, Jenq-Neng and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={18221-18232},
  year={2024}
}

@article{li2023videochat,
  title={{VideoChat}: Chat-centric video understanding},
  author={Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2305.06355},
  year={2023}
}




@inproceedings{zhu2020actbert,
  title={Actbert: Learning global-local video-text representations},
  author={Zhu, Linchao and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8746--8755},
  year={2020}
}

@article{dai2024instructblip,
  title={Instructblip: Towards general-purpose vision-language models with instruction tuning},
  author={Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{otter,
  title={Mimic-it: Multi-modal in-context instruction tuning},
  author={Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Pu, Fanyi and Yang, Jingkang and Li, Chunyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2306.05425},
  year={2023}
}

@article{ye2023mplug-owl,
  title={mplug-owl: Modularization empowers large language models with multimodality},
  author={Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal={arXiv preprint arXiv:2304.14178},
  year={2023}
}

@article{videochatgpt,
  title={Video-chatgpt: Towards detailed video understanding via large vision and language models},
  author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
  journal={arXiv preprint arXiv:2306.05424},
  year={2023}
}

@inproceedings{li2023mvbench,
  title={{MVBench}: {A} Comprehensive Multi-modal Video Understanding Benchmark},
  author={Kunchang Li and
                  Yali Wang and
                  Yinan He and
                  Yizhuo Li and
                  Yi Wang and
                  Yi Liu and
                  Zun Wang and
                  Jilan Xu and
                  Guo Chen and
                  Ping Lou and
                  Limin Wang and
                  Yu Qiao},
  booktitle    = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition},
  pages        = {22195--22206},
  year={2024}
}



@article{paxion,
  title={Paxion: Patching action knowledge in video-language foundation models},
  author={Wang, Zhenhailong and Blume, Ansel and Li, Sha and Liu, Genglin and Cho, Jaemin and Tang, Zineng and Bansal, Mohit and Ji, Heng},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{mit,
  title={Moments in time dataset: one million videos for event understanding},
  author={Monfort, Mathew and Andonian, Alex and Zhou, Bolei and Ramakrishnan, Kandan and Bargal, Sarah Adel and Yan, Tom and Brown, Lisa and Fan, Quanfu and Gutfreund, Dan and Vondrick, Carl and others},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={42},
  number={2},
  pages={502--508},
  year={2019},
  publisher={IEEE}
}



@inproceedings{charades_sta,
  title={Tall: Temporal activity localization via language query},
  author={Gao, Jiyang and Sun, Chen and Yang, Zhenheng and Nevatia, Ram},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5267--5275},
  year={2017}
}



@inproceedings{movienet,
  title        = {{MovieNet}: {A} Holistic Dataset for Movie Understanding},
  author={Huang, Qingqiu and Xiong, Yu and Rao, Anyi and Wang, Jiaze and Lin, Dahua},
  booktitle={European Conference Computer Vision},
  series       = {Lecture Notes in Computer Science},
  volume       = {12357},
  pages={709--727},
  year={2020},
  organization={Springer}
}


@article{ntu_rgbd,
  title={Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding},
  author={Liu, Jun and Shahroudy, Amir and Perez, Mauricio and Wang, Gang and Duan, Ling-Yu and Kot, Alex C},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={42},
  number={10},
  pages={2684--2701},
  year={2019},
  publisher={IEEE}
}

@inproceedings{scalevln,
  title={Scaling data generation in vision-and-language navigation},
  author={Wang, Zun and Li, Jialu and Hong, Yicong and Wang, Yi and Wu, Qi and Bansal, Mohit and Gould, Stephen and Tan, Hao and Qiao, Yu},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={12009--12020},
  year={2023}
}

@inproceedings{clevrer,
  title={CLEVRER: Collision Events for Video Representation and Reasoning},
  author={Yi, Kexin and Gan, Chuang and Li, Yunzhu and Kohli, Pushmeet and Wu, Jiajun and Torralba, Antonio and Tenenbaum, Joshua B},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


@article{perception_test,
  title={Perception test: A diagnostic benchmark for multimodal video models},
  author={Patraucean, Viorica and Smaira, Lucas and Gupta, Ankush and Recasens, Adria and Markeeva, Larisa and Banarse, Dylan and Koppula, Skanda and Malinowski, Mateusz and Yang, Yi and Doersch, Carl and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}



@article{luo2023valley_videoinstruct,
  title={Valley: Video assistant with large language model enhanced ability},
  author={Luo, Ruipu and Zhao, Ziwang and Yang, Min and Dong, Junwei and Qiu, Minghui and Lu, Pengcheng and Wang, Tao and Wei, Zhongyu},
  journal={arXiv preprint arXiv:2306.07207},
  year={2023}
}

@article{T5,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}

@article{vicuna,
  title={Instruction tuning with gpt-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}





@article{wang2024lifelongmemory,
  title={LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos},
  author={Ying Wang and Yanlai Yang and Mengye Ren},
  journal={arXiv preprint arXiv:2312.05269},
  year={2024}
}

@inproceedings{videoagent2,
  title={Videoagent: A memory-augmented multimodal agent for video understanding},
  author={Fan, Yue and Ma, Xiaojian and Wu, Rujie and Du, Yuntao and Li, Jiaqi and Gao, Zhi and Li, Qing},
  booktitle={European Conference on Computer Vision},
  series       = {Lecture Notes in Computer Science},
  volume       = {15080},
  pages={75--92},
  year={2025},
  organization={Springer}
}

@article{langrepo,
  title={Language repository for long video understanding},
  author={Kahatapitiya, Kumara and Ranasinghe, Kanchana and Park, Jongwoo and Ryoo, Michael S},
  journal={arXiv preprint arXiv:2403.14622},
  year={2024}
}

@article{onepass,
  title={Understanding Long Videos in One Multimodal Language Model Pass},
  author={Ranasinghe, Kanchana and Li, Xiang and Kahatapitiya, Kumara and Ryoo, Michael S},
  journal={arXiv preprint arXiv:2403.16998},
  year={2024}
}

@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@inproceedings{han2022temporal_align,
  title={Temporal alignment networks for long-term video},
  author={Han, Tengda and Xie, Weidi and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2906--2916},
  year={2022}
}

@article{wang2024lvbench,
  title={LVBench: An Extreme Long Video Understanding Benchmark},
  author={Wang, Weihan and He, Zehai and Hong, Wenyi and Cheng, Yean and Zhang, Xiaohan and Qi, Ji and Huang, Shiyu and Xu, Bin and Dong, Yuxiao and Ding, Ming and others},
  journal={arXiv preprint arXiv:2406.08035},
  year={2024}
}

@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}

@misc{wu2024longvideobench,
      title={LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding}, 
      author={Haoning Wu and Dongxu Li and Bei Chen and Junnan Li},
      year={2024},
      eprint={2407.15754},
      archivePrefix={arXiv},
      primaryClass={cs.CV},

}


@inproceedings{
MC:MemoryConsolidation,
title={Memory Consolidation Enables Long-Context Video Understanding},
author={Ivana Balazevic and Yuge Shi and Pinelopi Papalampidi and Rahma Chaabouni and Skanda Koppula and Olivier J Henaff},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
}

@inproceedings{
chen2022pali,
title={Pa{LI}: A Jointly-Scaled Multilingual Language-Image Model},
author={Xi Chen and Xiao Wang and Soravit Changpinyo and AJ Piergiovanni and Piotr Padlewski and Daniel Salz and Sebastian Goodman and Adam Grycner and Basil Mustafa and Lucas Beyer and Alexander Kolesnikov and Joan Puigcerver and Nan Ding and Keran Rong and Hassan Akbari and Gaurav Mishra and Linting Xue and Ashish V Thapliyal and James Bradbury and Weicheng Kuo and Mojtaba Seyedhosseini and Chao Jia and Burcu Karagol Ayan and Carlos Riquelme Ruiz and Andreas Peter Steiner and Anelia Angelova and Xiaohua Zhai and Neil Houlsby and Radu Soricut},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
}

@inproceedings{
LLamavqa,
title={Large Language Models are Temporal and Causal Reasoners for Video Question Answering},
author={Dohwan Ko and Ji Soo Lee and Woo-Young Kang and Byungseok Roh and Hyunwoo J. Kim},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
}

@inproceedings{tvqa,
  title={TVQA: Localized, Compositional Video Question Answering},
  author={Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L},
  booktitle={EMNLP},
  year={2018}
}

@article{video-llava,
  title={Video-LLaVA: Learning United Visual Representation by Alignment Before Projection},
  author={Lin, Bin and Zhu, Bin and Ye, Yang and Ning, Munan and Jin, Peng and Yuan, Li},
  journal={arXiv preprint arXiv:2311.10122},
  year={2023}
}

@inproceedings{lin2024noise_align,
   title={Multi-granularity Correspondence Learning from Long-term Noisy Videos},
   author={Lin, Yijie and Zhang, Jie and Huang, Zhenyu and Liu, Jia and Wen, Zujie and Peng, Xi},
   booktitle={Proceedings of the International Conference on Learning Representations},
   month={May},
   year={2024}
}

@inproceedings{adamw,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
}

@inproceedings{morevqa,
  author    = {Min, Juhong and Buch, Shyamal and Nagrani, Arsha and Cho, Minsu and Schmid, Cordelia},
  title     = {MoReVQA: Exploring Modular Reasoning Models for Video Question Answering},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2024},
}

@inproceedings{longvivit,
  title={A simple recipe for contrastively pre-training video-first encoders beyond 16 frames},
  author={Papalampidi, Pinelopi and Koppula, Skanda and Pathak, Shreya and Chiu, Justin and Heyward, Joe and Patraucean, Viorica and Shen, Jiajun and Miech, Antoine and Zisserman, Andrew and Nematzdeh, Aida},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14386--14397},
  year={2024}
}

@inproceedings{qian2024momentorvideo,
  title={Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning},
  author={Qian, Long and Li, Juncheng and Wu, Yu and Ye, Yaobo and Fei, Hao and Chua, Tat-Seng and Zhuang, Yueting and Tang, Siliang},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{laion400m,
  title={LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs},
  author={Schuhmann, Christoph and Kaczmarczyk, Robert and Komatsuzaki, Aran and Katta, Aarush and Vencu, Richard and Beaumont, Romain and Jitsev, Jenia and Coombes, Theo and Mullis, Clayton},
  booktitle={NeurIPS Workshop Datacentric AI},
  year={2021},
}

@inproceedings{agent_video,
  title={VideoAgent: Long-form Video Understanding with Large Language Model as Agent},
  author={Wang, Xiaohan and Zhang, Yuhui and Zohar, Orr and Yeung-Levy, Serena},
  booktitle={European Conference on Computer Vision (ECCV)},
  year={2024}
}


@article{wang2022internvideo,
  title={Internvideo: General video foundation models via generative and discriminative learning},
  author={Wang, Yi and Li, Kunchang and Li, Yizhuo and He, Yinan and Huang, Bingkun and Zhao, Zhiyu and Zhang, Hongjie and Xu, Jilan and Liu, Yi and Wang, Zun and others},
  journal={arXiv preprint arXiv:2212.03191},
  year={2022}
}

@article{internvideo2,
  title={InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding},
  author={Wang, Yi and Li, Kunchang and Li, Xinhao and Yu, Jiashuo and He, Yinan and Chen, Guo and Pei, Baoqi and Zheng, Rongkun and Xu, Jilan and Wang, Zun and others},
  journal={CoRR},
  year={2024}
}

@inproceedings{funqa,
  title={Funqa: Towards surprising video comprehension},
  author={Xie, Binzhu and Zhang, Sicheng and Zhou, Zitang and Li, Bo and Zhang, Yuanhan and Hessel, Jack and Yang, Jingkang and Liu, Ziwei},
  booktitle={European Conference on Computer Vision},
  pages={39--57},
  year={2025},
  organization={Springer}
}

@inproceedings{agent_dlam,
  title={Doraemongpt: Toward understanding dynamic scenes with large language models (exemplified as a video agent)},
  author={Yang, Zongxin and Chen, Guikun and Li, Xiaodi and Wang, Wenguan and Yang, Yi},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{videollama,
  title={Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={543--553},
  year={2023}
}

@inproceedings{
zhang2023llamaadapter,
title={{LL}a{MA}-Adapter: Efficient Fine-tuning of Large Language Models with Zero-initialized Attention},
author={Renrui Zhang and Jiaming Han and Chris Liu and Aojun Zhou and Pan Lu and Yu Qiao and Hongsheng Li and Peng Gao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}

@inproceedings{zhao2024videoprism,
  title={VideoPrism: A Foundational Visual Encoder for Video Understanding},
  author={Zhao, Long and Gundavarapu, Nitesh Bharadwaj and Yuan, Liangzhe and Zhou, Hao and Yan, Shen and Sun, Jennifer J and Friedman, Luke and Qian, Rui and Weyand, Tobias and Zhao, Yue and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

@inproceedings{zhao2024distilling,
  title={Distilling vision-language models on millions of videos},
  author={Zhao, Yue and Zhao, Long and Zhou, Xingyi and Wu, Jialin and Chu, Chun-Te and Miao, Hui and Schroff, Florian and Adam, Hartwig and Liu, Ting and Gong, Boqing and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13106--13116},
  year={2024}
}