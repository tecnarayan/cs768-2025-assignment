\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2016)Agarwal, Bullins, and Hazan]{agarwal2016second}
Agarwal, N., Bullins, B., and Hazan, E.
\newblock Second-order stochastic optimization in linear time.
\newblock \emph{stat}, 1050:\penalty0 15, 2016.

\bibitem[Ali et~al.(2019)Ali, Kolter, and Tibshirani]{ali2019continuous}
Ali, A., Kolter, J.~Z., and Tibshirani, R.~J.
\newblock A continuous-time view of early stopping for least squares
  regression.
\newblock In \emph{The 22nd international conference on artificial intelligence
  and statistics}, pp.\  1370--1378. PMLR, 2019.

\bibitem[Amari et~al.(2020)Amari, Ba, Grosse, Li, Nitanda, Suzuki, Wu, and
  Xu]{amari2020does}
Amari, S.-i., Ba, J., Grosse, R., Li, X., Nitanda, A., Suzuki, T., Wu, D., and
  Xu, J.
\newblock When does preconditioning help or hurt generalization?
\newblock \emph{arXiv preprint arXiv:2006.10732}, 2020.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Arora, S., Du, S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  322--332. PMLR, 2019.

\bibitem[Ash \& Adams(2020)Ash and Adams]{ash2020warm}
Ash, J. and Adams, R.~P.
\newblock On warm-starting neural network training.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3884--3894, 2020.

\bibitem[Barshan et~al.(2020)Barshan, Brunet, and
  Dziugaite]{barshan2020relatif}
Barshan, E., Brunet, M.-E., and Dziugaite, G.~K.
\newblock Relat{IF}: Identifying explanatory training samples via relative
  influence.
\newblock In Chiappa, S. and Calandra, R. (eds.), \emph{Proceedings of the
  Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of \emph{Proceedings of Machine Learning Research},
  pp.\  1899--1909. PMLR, 26--28 Aug 2020.
\newblock URL \url{https://proceedings.mlr.press/v108/barshan20a.html}.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2020benign}
Bartlett, P.~L., Long, P.~M., Lugosi, G., and Tsigler, A.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30063--30070, 2020.

\bibitem[Basu et~al.(2020{\natexlab{a}})Basu, Pope, and
  Feizi]{basu2020influence}
Basu, S., Pope, P., and Feizi, S.
\newblock Influence functions in deep learning are fragile.
\newblock \emph{arXiv preprint arXiv:2006.14651}, 2020{\natexlab{a}}.

\bibitem[Basu et~al.(2020{\natexlab{b}})Basu, You, and Feizi]{basu2020second}
Basu, S., You, X., and Feizi, S.
\newblock On second-order group influence functions for black-box predictions.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  715--724. PMLR, 2020{\natexlab{b}}.

\bibitem[Bengio(2012)]{bengio2012practical}
Bengio, Y.
\newblock Practical recommendations for gradient-based training of deep
  architectures.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  437--478.
  Springer, 2012.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brunet et~al.(2019)Brunet, Alkalay-Houlihan, Anderson, and
  Zemel]{brunet2019understanding}
Brunet, M.-E., Alkalay-Houlihan, C., Anderson, A., and Zemel, R.
\newblock Understanding the origins of bias in word embeddings.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  803--811. PMLR, 09--15
  Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/brunet19a.html}.

\bibitem[Charpiat et~al.(2019)Charpiat, Girard, Felardos, and
  Tarabalka]{charpiat2019similarity}
Charpiat, G., Girard, N., Felardos, L., and Tarabalka, Y.
\newblock Input similarity from the neural network perspective.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/c61f571dbd2fb949d3fe5ae1608dd48b-Paper.pdf}.

\bibitem[Cook(1979)]{cook1979influential}
Cook, R.~D.
\newblock Influential observations in linear regression.
\newblock \emph{Journal of the American Statistical Association}, 74\penalty0
  (365):\penalty0 169--174, 1979.

\bibitem[Demmel(1997)]{demmel1997applied}
Demmel, J.~W.
\newblock \emph{Applied numerical linear algebra}.
\newblock SIAM, 1997.

\bibitem[Deng(2012)]{deng2012mnist}
Deng, L.
\newblock The mnist database of handwritten digit images for machine learning
  research.
\newblock \emph{IEEE Signal Processing Magazine}, 29\penalty0 (6):\penalty0
  141--142, 2012.

\bibitem[Dua \& Graff(2017)Dua and Graff]{Dua2019}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Farnia \& Ozdaglar(2020)Farnia and Ozdaglar]{farnia2020gans}
Farnia, F. and Ozdaglar, A.
\newblock Do gans always have nash equilibria?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3029--3039. PMLR, 2020.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Abid, and
  Zou]{ghorbani2019interpretation}
Ghorbani, A., Abid, A., and Zou, J.
\newblock Interpretation of neural networks is fragile.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  33\penalty0 (01):\penalty0 3681--3688, Jul. 2019.
\newblock \doi{10.1609/aaai.v33i01.33013681}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/4252}.

\bibitem[Griewank \& Walther(2008)Griewank and Walther]{griewank2008evaluating}
Griewank, A. and Walther, A.
\newblock \emph{Evaluating derivatives: principles and techniques of
  algorithmic differentiation}.
\newblock SIAM, 2008.

\bibitem[Guo et~al.(2021)Guo, Rajani, Hase, Bansal, and Xiong]{guo2021fastif}
Guo, H., Rajani, N., Hase, P., Bansal, M., and Xiong, C.
\newblock {F}ast{IF}: Scalable influence functions for efficient model
  interpretation and debugging.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  10333--10350, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.808}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.808}.

\bibitem[Hampel(1974)]{hampel1974influence}
Hampel, F.~R.
\newblock The influence curve and its role in robust estimation.
\newblock \emph{Journal of the american statistical association}, 69\penalty0
  (346):\penalty0 383--393, 1974.

\bibitem[Han et~al.(2020)Han, Wallace, and Tsvetkov]{xiaochuang2020explaining}
Han, X., Wallace, B.~C., and Tsvetkov, Y.
\newblock Explaining black box predictions and unveiling data artifacts through
  influence functions.
\newblock In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J.~R. (eds.),
  \emph{Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics, {ACL} 2020, Online, July 5-10, 2020}, pp.\
  5553--5563. Association for Computational Linguistics, 2020.
\newblock \doi{10.18653/v1/2020.acl-main.492}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.acl-main.492}.

\bibitem[Hanawa et~al.(2021)Hanawa, Yokoi, Hara, and
  Inui]{hanawa2021evaluation}
Hanawa, K., Yokoi, S., Hara, S., and Inui, K.
\newblock Evaluation of similarity-based explanations.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=9uvhpyQwzM_}.

\bibitem[Hastie et~al.(2022)Hastie, Montanari, Rosset, and
  Tibshirani]{hastie2022surprises}
Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R.~J.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{The Annals of Statistics}, 50\penalty0 (2):\penalty0 949--986,
  2022.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock \emph{arXiv preprint arXiv:1512.03385}, 2015.

\bibitem[Izzo et~al.(2021)Izzo, Smart, Chaudhuri, and Zou]{izzo2021approximate}
Izzo, Z., Smart, M.~A., Chaudhuri, K., and Zou, J.
\newblock Approximate data deletion from machine learning models.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2008--2016. PMLR, 2021.

\bibitem[K \& S{\o}gaard(2021)K and S{\o}gaard]{karthikeyan2021revisting}
K, K. and S{\o}gaard, A.
\newblock Revisiting methods for finding influential examples.
\newblock \emph{CoRR}, abs/2111.04683, 2021.
\newblock URL \url{https://arxiv.org/abs/2111.04683}.

\bibitem[Khanna et~al.(2019)Khanna, Kim, Ghosh, and
  Koyejo]{khanna2019interpreting}
Khanna, R., Kim, B., Ghosh, J., and Koyejo, S.
\newblock Interpreting black box predictions using fisher kernels.
\newblock In Chaudhuri, K. and Sugiyama, M. (eds.), \emph{Proceedings of the
  Twenty-Second International Conference on Artificial Intelligence and
  Statistics}, volume~89 of \emph{Proceedings of Machine Learning Research},
  pp.\  3382--3390. PMLR, 16--18 Apr 2019.
\newblock URL \url{https://proceedings.mlr.press/v89/khanna19a.html}.

\bibitem[Kim et~al.(2016)Kim, Khanna, and Koyejo]{kim2016examples}
Kim, B., Khanna, R., and Koyejo, O.~O.
\newblock Examples are not enough, learn to criticize! criticism for
  interpretability.
\newblock In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2016/file/5680522b8e2bb01943234bce7bf84534-Paper.pdf}.

\bibitem[Koh \& Liang(2017)Koh and Liang]{koh2017understanding}
Koh, P.~W. and Liang, P.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International conference on machine learning}, pp.\
  1885--1894. PMLR, 2017.

\bibitem[Koh et~al.(2022)Koh, Steinhardt, and Liang]{koh2022poisoning}
Koh, P.~W., Steinhardt, J., and Liang, P.
\newblock Stronger data poisoning attacks break data sanitization defenses.
\newblock \emph{Machine Learning}, 111\penalty0 (1):\penalty0 1--47, Jan 2022.
\newblock ISSN 1573-0565.
\newblock \doi{10.1007/s10994-021-06119-y}.
\newblock URL \url{https://doi.org/10.1007/s10994-021-06119-y}.

\bibitem[Koh et~al.(2019)Koh, Ang, Teo, and Liang]{koh2019accuracy}
Koh, P. W.~W., Ang, K.-S., Teo, H., and Liang, P.~S.
\newblock On the accuracy of influence functions for measuring group effects.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Kong et~al.(2021)Kong, Shen, and Huang]{kong2021resolving}
Kong, S., Shen, Y., and Huang, L.
\newblock Resolving training biases via influence-based data relabeling.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Krantz \& Parks(2002)Krantz and Parks]{krantz2002implicit}
Krantz, S.~G. and Parks, H.~R.
\newblock \emph{The implicit function theorem: history, theory, and
  applications}.
\newblock Springer Science \& Business Media, 2002.

\bibitem[Krizhevsky(2009)]{Krizhevsky2009learning}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Proceedings of the 25th International Conference on Neural
  Information Processing Systems - Volume 1}, NIPS'12, pp.\  1097–1105, Red
  Hook, NY, USA, 2012. Curran Associates Inc.

\bibitem[Lecun et~al.(1998)Lecun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.
\newblock \doi{10.1109/5.726791}.

\bibitem[Lee et~al.(2020)Lee, Park, Pham, and Yoo]{lee2020learning}
Lee, D., Park, H., Pham, T., and Yoo, C.~D.
\newblock Learning augmentation network via influence functions.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  10961--10970, 2020.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Liu \& Nocedal(1989)Liu and Nocedal]{liu1989limited}
Liu, D.~C. and Nocedal, J.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock \emph{Mathematical programming}, 45\penalty0 (1):\penalty0 503--528,
  1989.

\bibitem[Marcus et~al.(1993)Marcus, Santorini, and
  Marcinkiewicz]{marcus1993building}
Marcus, M.~P., Santorini, B., and Marcinkiewicz, M.~A.
\newblock Building a large annotated corpus of {E}nglish: The {P}enn
  {T}reebank.
\newblock \emph{Computational Linguistics}, 19\penalty0 (2):\penalty0 313--330,
  1993.
\newblock URL \url{https://aclanthology.org/J93-2004}.

\bibitem[Martens(2014)]{martens2014new}
Martens, J.
\newblock New insights and perspectives on the natural gradient method.
\newblock \emph{arXiv preprint arXiv:1412.1193}, 2014.

\bibitem[Martens \& Grosse(2015)Martens and Grosse]{martens2015optimizing}
Martens, J. and Grosse, R.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{International conference on machine learning}, pp.\
  2408--2417. PMLR, 2015.

\bibitem[Martens et~al.(2010)]{martens2010deep}
Martens, J. et~al.
\newblock Deep learning via hessian-free optimization.
\newblock In \emph{ICML}, volume~27, pp.\  735--742, 2010.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019_9015}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 32}, pp.\  8024--8035. Curran Associates,
  Inc., 2019.
\newblock URL
  \url{http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}.

\bibitem[Pruthi et~al.(2020)Pruthi, Liu, Kale, and
  Sundararajan]{pruthi2020estimating}
Pruthi, G., Liu, F., Kale, S., and Sundararajan, M.
\newblock Estimating training data influence by tracing gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 19920--19930, 2020.

\bibitem[Schioppa et~al.(2021)Schioppa, Zablotskaia, Vilar, and
  Sokolov]{schioppa2021scaling}
Schioppa, A., Zablotskaia, P., Vilar, D., and Sokolov, A.
\newblock Scaling up influence functions.
\newblock \emph{CoRR}, abs/2112.03052, 2021.
\newblock URL \url{https://arxiv.org/abs/2112.03052}.

\bibitem[Schulam \& Saria(2019)Schulam and Saria]{schulam2019trust}
Schulam, P. and Saria, S.
\newblock Can you trust this prediction? {A}uditing pointwise reliability after
  learning.
\newblock In Chaudhuri, K. and Sugiyama, M. (eds.), \emph{Proceedings of the
  Twenty-Second International Conference on Artificial Intelligence and
  Statistics}, volume~89 of \emph{Proceedings of Machine Learning Research},
  pp.\  1022--1031. PMLR, 16--18 Apr 2019.
\newblock URL \url{https://proceedings.mlr.press/v89/schulam19a.html}.

\bibitem[Sedgwick(2012)]{sedgwick2012pearson}
Sedgwick, P.
\newblock Pearson’s correlation coefficient.
\newblock \emph{Bmj}, 345, 2012.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Spearman(1961)]{spearman1961proof}
Spearman, C.
\newblock The proof and measurement of association between two things.
\newblock 1961.

\bibitem[Teso et~al.(2021)Teso, Bontempelli, Giunchiglia, and
  Passerini]{teso2021interactive}
Teso, S., Bontempelli, A., Giunchiglia, F., and Passerini, A.
\newblock Interactive label cleaning with example-based explanations.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Van~der Vaart(2000)]{van2000asymptotic}
Van~der Vaart, A.~W.
\newblock \emph{Asymptotic statistics}, volume~3.
\newblock Cambridge university press, 2000.

\bibitem[Vicol et~al.(2022{\natexlab{a}})Vicol, Lorraine, Duvenaud, and
  Grosse]{vicol2021implicit}
Vicol, P., Lorraine, J., Duvenaud, D., and Grosse, R.
\newblock Implicit regularization in overparameterized bilevel optimization.
\newblock 2022{\natexlab{a}}.

\bibitem[Vicol et~al.(2022{\natexlab{b}})Vicol, Lorraine, Pedregosa, Duvenaud,
  and Grosse]{vicol2022implicit}
Vicol, P., Lorraine, J., Pedregosa, F., Duvenaud, D., and Grosse, R.
\newblock On implicit bias in overparameterized bilevel optimization.
\newblock In \emph{International Conference on Machine Learning}. PMLR,
  2022{\natexlab{b}}.

\bibitem[Wachter et~al.(2018)Wachter, Mittelstadt, and
  Russell]{wachter2018counterfactual}
Wachter, S., Mittelstadt, B.~D., and Russell, C.
\newblock Counterfactual explanations without opening the black box automated
  decisions and the {GDPR}.
\newblock \emph{Harvard Journal of Law \& Technology}, 31\penalty0 (2), 2018.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem[Yang \& Chaudhuri(2022)Yang and Chaudhuri]{yang2022understanding}
Yang, Y.-Y. and Chaudhuri, K.
\newblock Understanding rare spurious correlations in neural networks.
\newblock \emph{arXiv preprint arXiv:2202.05189}, 2022.

\bibitem[Yeh et~al.(2018)Yeh, Kim, Yen, and Ravikumar]{yeh2018representer}
Yeh, C.-K., Kim, J., Yen, I. E.-H., and Ravikumar, P.~K.
\newblock Representer point selection for explaining deep neural networks.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/8a7129b8f3edd95b7d969dfc2c8e9d9d-Paper.pdf}.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\end{thebibliography}
