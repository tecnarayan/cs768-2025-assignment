\begin{thebibliography}{}

\bibitem[Abbeel and Ng, 2004]{abbeel2004irl}
Abbeel, P. and Ng, A. (2004).
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In {\em ICML}.

\bibitem[Achiam et~al., 2017]{achiam2017constrained}
Achiam, J., Held, D., Tamar, A., and Abbeel, P. (2017).
\newblock Constrained policy optimization.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 22--31. JMLR. org.

\bibitem[Agarwal et~al., 2019]{agarwal2019optimality}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G. (2019).
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock {\em arXiv preprint arXiv:1908.00261}.

\bibitem[Akametalu et~al., 2014]{akametalu2014reachability}
Akametalu, A.~K., Fisac, J.~F., Gillula, J.~H., Kaynama, S., Zeilinger, M.~N.,
  and Tomlin, C.~J. (2014).
\newblock Reachability-based safe learning with gaussian processes.
\newblock In {\em 53rd IEEE Conference on Decision and Control}, pages
  1424--1431. IEEE.

\bibitem[Alshiekh et~al., 2018]{alshiekh2018shielding}
Alshiekh, M., Bloem, R., Ehlers, R., Könighofer, B., Niekum, S., and Topcu, U.
  (2018).
\newblock Safe reinforcement learning via shielding.
\newblock In {\em AAAI Conference on Artificial Intelligence}.

\bibitem[Altman, 1999]{altman1999constrained}
Altman, E. (1999).
\newblock {\em Constrained Markov decision processes}, volume~7.
\newblock CRC Press.

\bibitem[Authors, 2016]{gpyopt2016}
Authors, T.~G. (2016).
\newblock {GPyOpt}: A bayesian optimization framework in python.
\newblock \url{http://github.com/SheffieldML/GPyOpt}.

\bibitem[Berkenkamp et~al., 2016]{berkenkamp2016safe}
Berkenkamp, F., Schoellig, A.~P., and Krause, A. (2016).
\newblock Safe controller optimization for quadrotors with {G}aussian
  processes.
\newblock In {\em Proc. of the IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 493--496.

\bibitem[Berkenkamp et~al., 2017]{berkenkamp2017safe}
Berkenkamp, F., Turchetta, M., Schoellig, A.~P., and Krause, A. (2017).
\newblock Safe model-based reinforcement learning with stability guarantees.
\newblock In {\em NiPS}.

\bibitem[Brockman et~al., 2016]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W. (2016).
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}.

\bibitem[Chen and Jiang, 2019]{chen2019information}
Chen, J. and Jiang, N. (2019).
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock {\em ICML}.

\bibitem[Chow et~al., 2018]{chow2018lyapunov}
Chow, Y., Nachum, O., Duenez-Guzman, E., and Ghavamzadeh, M. (2018).
\newblock A lyapunov-based approach to safe reinforcement learning.
\newblock In {\em Advances in neural information processing systems}, pages
  8092--8101.

\bibitem[Chow et~al., 2019]{chow2019lyapunov}
Chow, Y., Nachum, O., Faust, A., Duenez-Guzman, E., and Ghavamzadeh, M. (2019).
\newblock Lyapunov-based safe policy optimization for continuous control.
\newblock {\em arXiv preprint arXiv:1901.10031}.

\bibitem[Clement et~al., 2015]{clement2015}
Clement, B., Roy, D., Oudeyer, P.-Y., and Lopes, M. (2015).
\newblock Multi-armed bandits for intelligent tutoring systems.
\newblock {\em Journal of Educational Data Mining}, 7.

\bibitem[D.Argall et~al., 2009]{argall2009ras}
D.Argall, B., Chernova, S., Veloso, M., and Browning, B. (2009).
\newblock A survey of robot learning from demonstration.
\newblock {\em Robotics and Autonomous Systems}, 57.

\bibitem[Dulac-Arnold et~al., 2019]{dulac2019challenges}
Dulac-Arnold, G., Mankowitz, D., and Hester, T. (2019).
\newblock Challenges of real-world reinforcement learning.

\bibitem[El~Chamie et~al., 2016]{el2016convex}
El~Chamie, M., Yu, Y., and A{\c{c}}{\i}kme{\c{s}}e, B. (2016).
\newblock Convex synthesis of randomized policies for controlled markov chains
  with density safety upper bound constraints.
\newblock In {\em 2016 American Control Conference (ACC)}, pages 6290--6295.
  IEEE.

\bibitem[Eysenbach et~al., 2018]{eysenbach2017leave}
Eysenbach, B., Gu, S., Ibarz, J., and Levine, S. (2018).
\newblock Leave no trace: Learning to reset for safe and autonomous
  reinforcement learning.
\newblock In {\em ICLR}.

\bibitem[Fachantidis et~al., 2013]{fachantidis2013transferring}
Fachantidis, A., Partalas, I., Tsoumakas, G., and Vlahavas, I. (2013).
\newblock Transferring task models in reinforcement learning agents.
\newblock {\em Neurocomputing}, 107:23--32.

\bibitem[Fern{\'a}ndez et~al., 2010]{fernandez2010probabilistic}
Fern{\'a}ndez, F., Garc{\'\i}a, J., and Veloso, M. (2010).
\newblock Probabilistic policy reuse for inter-task transfer learning.
\newblock {\em Robotics and Autonomous Systems}, 58(7):866--871.

\bibitem[Florensa et~al., 2018]{florensa2017automatic}
Florensa, C., Held, D., Geng, X., and Abbeel, P. (2018).
\newblock Automatic goal generation for reinforcement learning agents.
\newblock In {\em ICML}.

\bibitem[Florensa et~al., 2017]{florensa2017reverse}
Florensa, C., Held, D., Wulfmeier, M., Zhang, M., and Abbeel, P. (2017).
\newblock Reverse curriculum generation for reinforcement learning.
\newblock In {\em CoRL}.

\bibitem[Garc{\i}a and Fern{\'a}ndez, 2015]{garcia2015comprehensive}
Garc{\i}a, J. and Fern{\'a}ndez, F. (2015).
\newblock A comprehensive survey on safe reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 16(1):1437--1480.

\bibitem[Graves et~al., 2017]{graves2017automated}
Graves, A., Bellemare, M.~G., Menick, J., Munos, R., and Kavukcuoglu, K.
  (2017).
\newblock Automated curriculum learning for neural networks.
\newblock In {\em ICML}.

\bibitem[Hill et~al., 2018]{stable-baselines}
Hill, A., Raffin, A., Ernestus, M., Gleave, A., Kanervisto, A., Traore, R.,
  Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A.,
  Schulman, J., Sidor, S., and Wu, Y. (2018).
\newblock Stable baselines.
\newblock \url{https://github.com/hill-a/stable-baselines}.

\bibitem[Kendall et~al., 2019]{kendall2019driving}
Kendall, A., Hawke, J., Janz, D., Mazur, P., Reda, D., Allen, J.-M., Lam,
  V.-D., Bewley, A., and Shah, A. (2019).
\newblock Learning to drive in a day.
\newblock In {\em ICRA}.

\bibitem[Kivinen and Warmuth, 1997]{kivinen1997exponentiated}
Kivinen, J. and Warmuth, M.~K. (1997).
\newblock Exponentiated gradient versus gradient descent for linear predictors.
\newblock {\em information and computation}, 132(1):1--63.

\bibitem[Koller et~al., 2018]{koller2018learning}
Koller, T., Berkenkamp, F., Turchetta, M., and Krause, A. (2018).
\newblock Learning-based model predictive control for safe exploration.
\newblock In {\em 2018 IEEE Conference on Decision and Control (CDC)}, pages
  6059--6066. IEEE.

\bibitem[Lazaric and Restelli, 2011]{lazaric2011transfer}
Lazaric, A. and Restelli, M. (2011).
\newblock Transfer from multiple mdps.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1746--1754.

\bibitem[Le et~al., 2019]{le2019batch}
Le, H.~M., Voloshin, C., and Yue, Y. (2019).
\newblock Batch policy learning under constraints.
\newblock {\em arXiv preprint arXiv:1903.08738}.

\bibitem[Matiisen et~al., 2019]{matiisen2017teacherstudent}
Matiisen, T., Oliver, A., Cohen, T., and Schulman, J. (2019).
\newblock Teacher-student curriculum learning.
\newblock In {\em IEEE Transactions on Neural Networks and Learning Systems}.

\bibitem[Mockus et~al., 1978]{mockus1978application}
Mockus, J., Tiesis, V., and Zilinskas, A. (1978).
\newblock The application of bayesian methods for seeking the extremum.
\newblock {\em Towards global optimization}, 2(117-129):2.

\bibitem[Narvekar et~al., 2016]{narvekar2016source}
Narvekar, S., Sinapov, J., Leonetti, M., and Stone, P. (2016).
\newblock Source task creation for curriculum learning.
\newblock In {\em AAMAS}.

\bibitem[Narvekar and Stone, 2019]{narvekar2019learning}
Narvekar, S. and Stone, P. (2019).
\newblock Learning curriculum policies for reinforcement learning.
\newblock In {\em Proceedings of the 18th International Conference on
  Autonomous Agents and MultiAgent Systems}, AAMAS '19, page 25–33, Richland,
  SC. International Foundation for Autonomous Agents and Multiagent Systems.

\bibitem[Osa et~al., 2018]{Osa_2018}
Osa, T., Pajarinen, J., Neumann, G., Bagnell, J.~A., Abbeel, P., and Peters, J.
  (2018).
\newblock An algorithmic perspective on imitation learning.
\newblock {\em Foundations and Trends in Robotics}, 7(1-2):1--179.

\bibitem[Pan et~al., 2018]{pan2017agile}
Pan, Y., Cheng, C.-A., Saigol, K., Lee, K., Yan, X., Theodorou, E., and Boots,
  B. (2018).
\newblock Agile autonomous driving using end-to-end deep imitation learning.
\newblock In {\em RSS}.

\bibitem[Pomerleau, 1989]{pomerleau89alvinn}
Pomerleau, D. (1989).
\newblock {ALVINN}: An autonomous land vehicle in a neural network.
\newblock In {\em Advances in Neural Information Processing Systems}.

\bibitem[Portelas et~al., 2019]{portelas2019cts}
Portelas, R., Colas, C., Hofmann, K., and Oudeyer, P.-Y. (2019).
\newblock Teacher algorithms for curriculum learning of deep {RL} in
  continuously parameterized environments.
\newblock In {\em CoRL}.

\bibitem[Portelas et~al., 2020]{portelas2020automatic}
Portelas, R., Colas, C., Weng, L., Hofmann, K., and Oudeyer, P.-Y. (2020).
\newblock Automatic curriculum learning for deep rl: A short survey.

\bibitem[Ray et~al., 2019]{ray2019benchmarking}
Ray, A., Achiam, J., and Amodei, D. (2019).
\newblock Benchmarking safe exploration in deep reinforcement learning.

\bibitem[Riedmiller et~al., 2018]{riedmiller2018learning}
Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., Degrave, J., Van~de Wiele,
  T., Mnih, V., Heess, N., and Springenberg, J.~T. (2018).
\newblock Learning by playing -- solving sparse reward tasks from scratch.
\newblock In {\em ICML}.

\bibitem[Scherrer, 2014]{scherrer2014approximate}
Scherrer, B. (2014).
\newblock Approximate policy iteration schemes: a comparison.
\newblock In {\em International Conference on Machine Learning}, pages
  1314--1322.

\bibitem[Schulman et~al., 2017]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}.

\bibitem[Srinivas et~al., 2010]{srinivas2009gaussian}
Srinivas, N., Krause, A., Kakade, S.~M., and Seeger, M. (2010).
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock In {\em ICML}.

\bibitem[Sutton and Barto, 2018]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G. (2018).
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[Taylor and Stone, 2005]{taylor2005behavior}
Taylor, M.~E. and Stone, P. (2005).
\newblock Behavior transfer for value-function-based reinforcement learning.
\newblock In {\em Proceedings of the fourth international joint conference on
  Autonomous agents and multiagent systems}, pages 53--59.

\bibitem[Thananjeyan et~al., 2020]{thananjeyan2020safety}
Thananjeyan, B., Balakrishna, A., Rosolia, U., Li, F., McAllister, R.,
  Gonzalez, J.~E., Levine, S., Borrelli, F., and Goldberg, K. (2020).
\newblock Safety augmented value estimation from demonstrations (saved): Safe
  deep model-based rl for sparse cost robotic tasks.
\newblock {\em IEEE Robotics and Automation Letters}, 5(2):3612--3619.

\bibitem[Turchetta et~al., 2016]{turchetta2016safe}
Turchetta, M., Berkenkamp, F., and Krause, A. (2016).
\newblock Safe exploration in finite markov decision processes with gaussian
  processes.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4312--4320.

\bibitem[Turchetta et~al., 2019]{turchetta2019safe}
Turchetta, M., Berkenkamp, F., and Krause, A. (2019).
\newblock Safe exploration for interactive machine learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2887--2897.

\bibitem[Vanschoren, 2018]{vanschoren2018meta}
Vanschoren, J. (2018).
\newblock Meta-learning: A survey.
\newblock {\em arXiv preprint arXiv:1810.03548}.

\bibitem[Wang et~al., 2019]{wang2019poet}
Wang, R., Lehman, J., Clune, J., and Stanley, K.~O. (2019).
\newblock Paired open-ended trailblazer {(POET):} endlessly generating
  increasingly complex and diverse learning environments and their solutions.
\newblock {\em CoRR}, abs/1901.01753.

\bibitem[Wu and Tian, 2017]{wu2016training}
Wu, Y. and Tian, Y. (2017).
\newblock Training agent for first-person shooter game with actor-critic
  curriculum learning.
\newblock In {\em ICLR}.

\end{thebibliography}
