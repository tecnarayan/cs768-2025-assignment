@article{weissman2003inequalities,
  title={Inequalities for the L1 deviation of the empirical distribution},
  author={Weissman, Tsachy and Ordentlich, Erik and Seroussi, Gadiel and Verdu, Sergio and Weinberger, Marcelo J},
  journal={Hewlett-Packard Labs, Tech. Rep},
  year={2003}
}

@inproceedings{zhang2021reinforcement,
  title={Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon},
  author={Zhang, Zihan and Ji, Xiangyang and Du, Simon},
  booktitle={Conference on Learning Theory},
  pages={4528--4531},
  year={2021},
  organization={PMLR}
}

@article{auer2002nonstochastic,
  title={The nonstochastic multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Freund, Yoav and Schapire, Robert E},
  journal={SIAM journal on computing},
  volume={32},
  number={1},
  pages={48--77},
  year={2002},
  publisher={SIAM}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{jin2021bellman,
  title={Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms},
  author={Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={13406--13418},
  year={2021}
}

@inproceedings{munos1999influence,
  title={Influence and variance of a Markov chain: Application to adaptive discretization in optimal control},
  author={Munos, R{\'e}mi and Moore, Andrew},
  booktitle={Proceedings of the 38th IEEE Conference on Decision and Control (Cat. No. 99CH36304)},
  volume={2},
  pages={1464--1469},
  year={1999},
  organization={IEEE}
}

@inproceedings{zhang2021model,
  title={Model-free reinforcement learning: from clipped pseudo-regret to sample complexity},
  author={Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
  booktitle={International Conference on Machine Learning},
  pages={12653--12662},
  year={2021},
  organization={PMLR}
}

@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}

@article{o2021variational,
  title={Variational bayesian reinforcement learning with regret bounds},
  author={O'Donoghue, Brendan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{osband2017posterior,
  title={Why is posterior sampling better than optimism for reinforcement learning?},
  author={Osband, Ian and Van Roy, Benjamin},
  booktitle={International conference on machine learning},
  pages={2701--2710},
  year={2017},
  organization={PMLR}
}

@article{osband2013more,
  title={(More) efficient reinforcement learning via posterior sampling},
  author={Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@article{wang2020reinforcement,
  title={Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension},
  author={Wang, Ruosong and Salakhutdinov, Russ R and Yang, Lin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6123--6135},
  year={2020}
}

@article{malik2021generalizable,
  title={When Is Generalizable Reinforcement Learning Tractable?},
  author={Malik, Dhruv and Li, Yuanzhi and Ravikumar, Pradeep},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@article{jin2018q,
  title={Is Q-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{tirinzoni2020sequential,
  title={Sequential transfer in reinforcement learning with a generative model},
  author={Tirinzoni, Andrea and Poiani, Riccardo and Restelli, Marcello},
  booktitle={International Conference on Machine Learning},
  pages={9481--9492},
  year={2020},
  organization={PMLR}
}

@article{lu2021power,
  title={On the power of multitask representation learning in linear mdp},
  author={Lu, Rui and Huang, Gao and Du, Simon S},
  journal={arXiv preprint arXiv:2106.08053},
  year={2021}
}

@article{zhang2021provably,
  title={Provably efficient multi-task reinforcement learning with model transfer},
  author={Zhang, Chicheng and Wang, Zhi},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{hu2021near,
  title={Near-optimal representation learning for linear bandits and linear rl},
  author={Hu, Jiachen and Chen, Xiaoyu and Jin, Chi and Li, Lihong and Wang, Liwei},
  booktitle={International Conference on Machine Learning},
  pages={4349--4358},
  year={2021},
  organization={PMLR}
}

@inproceedings{wang2019generalization,
  title={On the generalization gap in reparameterizable reinforcement learning},
  author={Wang, Huan and Zheng, Stephan and Xiong, Caiming and Socher, Richard},
  booktitle={International Conference on Machine Learning},
  pages={6648--6658},
  year={2019},
  organization={PMLR}
}

@article{packer2018assessing,
  title={Assessing generalization in deep reinforcement learning},
  author={Packer, Charles and Gao, Katelyn and Kos, Jernej and Kr{\"a}henb{\"u}hl, Philipp and Koltun, Vladlen and Song, Dawn},
  journal={arXiv preprint arXiv:1810.12282},
  year={2018}
}

@article{kirk2021survey,
  title={A survey of generalisation in deep reinforcement learning},
  author={Kirk, Robert and Zhang, Amy and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  journal={arXiv preprint arXiv:2111.09794},
  year={2021}
}

@article{zhang2020learning,
  title={Learning invariant representations for reinforcement learning without reconstruction},
  author={Zhang, Amy and McAllister, Rowan and Calandra, Roberto and Gal, Yarin and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.10742},
  year={2020}
}

@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={The Journal of Machine Learning Research},
  volume={2},
  pages={499--526},
  year={2002},
  publisher={JMLR. org}
}

@article{kawaguchi2017generalization,
  title={Generalization in deep learning},
  author={Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.05468},
  year={2017}
}

@article{mitchell1986explanation,
  title={Explanation-based generalization: A unifying view},
  author={Mitchell, Tom M and Keller, Richard M and Kedar-Cabelli, Smadar T},
  journal={Machine learning},
  volume={1},
  number={1},
  pages={47--80},
  year={1986},
  publisher={Springer}
}

@article{andrychowicz2020learning,
  title={Learning dexterous in-hand manipulation},
  author={Andrychowicz, OpenAI: Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and others},
  journal={The International Journal of Robotics Research},
  volume={39},
  number={1},
  pages={3--20},
  year={2020},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{peng2018sim,
  title={Sim-to-real transfer of robotic control with dynamics randomization},
  author={Peng, Xue Bin and Andrychowicz, Marcin and Zaremba, Wojciech and Abbeel, Pieter},
  booktitle={2018 IEEE international conference on robotics and automation (ICRA)},
  pages={3803--3810},
  year={2018},
  organization={IEEE}
}

@inproceedings{james2019sim,
  title={Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks},
  author={James, Stephen and Wohlhart, Paul and Kalakrishnan, Mrinal and Kalashnikov, Dmitry and Irpan, Alex and Ibarz, Julian and Levine, Sergey and Hadsell, Raia and Bousmalis, Konstantinos},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12627--12637},
  year={2019}
}

@article{rajeswaran2016epopt,
  title={Epopt: Learning robust neural network policies using model ensembles},
  author={Rajeswaran, Aravind and Ghotra, Sarvjeet and Ravindran, Balaraman and Levine, Sergey},
  journal={arXiv preprint arXiv:1610.01283},
  year={2016}
}

@inproceedings{rusu2017sim,
  title={Sim-to-real robot learning from pixels with progressive nets},
  author={Rusu, Andrei A and Ve{\v{c}}er{\'\i}k, Matej and Roth{\"o}rl, Thomas and Heess, Nicolas and Pascanu, Razvan and Hadsell, Raia},
  booktitle={Conference on Robot Learning},
  pages={262--270},
  year={2017},
  organization={PMLR}
}

@article{sallab2017deep,
  title={Deep reinforcement learning framework for autonomous driving},
  author={Sallab, Ahmad EL and Abdou, Mohammed and Perot, Etienne and Yogamani, Senthil},
  journal={Electronic Imaging},
  volume={2017},
  number={19},
  pages={70--76},
  year={2017},
  publisher={Society for Imaging Science and Technology}
}

@article{shalev2016safe,
  title={Safe, multi-agent, reinforcement learning for autonomous driving},
  author={Shalev-Shwartz, Shai and Shammah, Shaked and Shashua, Amnon},
  journal={arXiv preprint arXiv:1610.03295},
  year={2016}
}

@article{yu2021reinforcement,
  title={Reinforcement learning in healthcare: A survey},
  author={Yu, Chao and Liu, Jiming and Nemati, Shamim and Yin, Guosheng},
  journal={ACM Computing Surveys (CSUR)},
  volume={55},
  number={1},
  pages={1--36},
  year={2021},
  publisher={ACM New York, NY}
}

@inproceedings{mao2016resource,
  title={Resource management with deep reinforcement learning},
  author={Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
  booktitle={Proceedings of the 15th ACM workshop on hot topics in networks},
  pages={50--56},
  year={2016}
}

@inproceedings{cai2017real,
  title={Real-time bidding by reinforcement learning in display advertising},
  author={Cai, Han and Ren, Kan and Zhang, Weinan and Malialis, Kleanthis and Wang, Jun and Yu, Yong and Guo, Defeng},
  booktitle={Proceedings of the Tenth ACM International Conference on Web Search and Data Mining},
  pages={661--670},
  year={2017}
}

@article{farebrother2018generalization,
  title={Generalization and regularization in DQN},
  author={Farebrother, Jesse and Machado, Marlos C and Bowling, Michael},
  journal={arXiv preprint arXiv:1810.00123},
  year={2018}
}

@article{sutton1995generalization,
  title={Generalization in reinforcement learning: Successful examples using sparse coarse coding},
  author={Sutton, Richard S},
  journal={Advances in neural information processing systems},
  volume={8},
  year={1995}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@inproceedings{zheng2018drn,
  title={DRN: A deep reinforcement learning framework for news recommendation},
  author={Zheng, Guanjie and Zhang, Fuzheng and Zheng, Zihan and Xiang, Yang and Yuan, Nicholas Jing and Xie, Xing and Li, Zhenhui},
  booktitle={Proceedings of the 2018 World Wide Web Conference},
  pages={167--176},
  year={2018}
}

@article{kormushev2013reinforcement,
  title={Reinforcement learning in robotics: Applications and real-world challenges},
  author={Kormushev, Petar and Calinon, Sylvain and Caldwell, Darwin G},
  journal={Robotics},
  volume={2},
  number={3},
  pages={122--148},
  year={2013},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{kober2013reinforcement,
  title={Reinforcement learning in robotics: A survey},
  author={Kober, Jens and Bagnell, J Andrew and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1238--1274},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{shani2005mdp,
  title={An MDP-based recommender system.},
  author={Shani, Guy and Heckerman, David and Brafman, Ronen I and Boutilier, Craig},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={9},
  year={2005}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@inproceedings{du2019provably,
  title={Provably efficient RL with rich observations via latent state decoding},
  author={Du, Simon and Krishnamurthy, Akshay and Jiang, Nan and Agarwal, Alekh and Dudik, Miroslav and Langford, John},
  booktitle={International Conference on Machine Learning},
  pages={1665--1674},
  year={2019},
  organization={PMLR}
}

@article{chen2021near,
  title={Near-Optimal Reward-Free Exploration for Linear Mixture MDPs with Plug-in Solver},
  author={Chen, Xiaoyu and Hu, Jiachen and Yang, Lin F and Wang, Liwei},
  journal={arXiv preprint arXiv:2110.03244},
  year={2021}
}

@book{banditbook,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}
@article{ghosh2021generalization,
  title={Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability},
  author={Ghosh, Dibya and Rahme, Jad and Kumar, Aviral and Zhang, Amy and Adams, Ryan P and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{azizzadenesheli2016reinforcement,
  title={Reinforcement learning of POMDPs using spectral methods},
  author={Azizzadenesheli, Kamyar and Lazaric, Alessandro and Anandkumar, Animashree},
  booktitle={Conference on Learning Theory},
  pages={193--256},
  year={2016},
  organization={PMLR}
}

@article{cui2020plug,
  title={Is Plug-in Solver Sample-Efficient for Feature-based Reinforcement Learning?},
  author={Cui, Qiwen and Yang, Lin},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6015--6026},
  year={2020}
}

@inproceedings{guo2016pac,
  title={A pac rl algorithm for episodic pomdps},
  author={Guo, Zhaohan Daniel and Doroudi, Shayan and Brunskill, Emma},
  booktitle={Artificial Intelligence and Statistics},
  pages={510--518},
  year={2016},
  organization={PMLR}
}

@article{jin2020sample,
  title={Sample-efficient reinforcement learning of undercomplete POMDPs},
  author={Jin, Chi and Kakade, Sham and Krishnamurthy, Akshay and Liu, Qinghua},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18530--18539},
  year={2020}
}

@article{kwon2021reinforcement,
  title={Reinforcement Learning in Reward-Mixing MDPs},
  author={Kwon, Jeongyeol and Efroni, Yonathan and Caramanis, Constantine and Mannor, Shie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{kwon2021rl,
  title={RL for latent MDPs: Regret guarantees and a lower bound},
  author={Kwon, Jeongyeol and Efroni, Yonathan and Caramanis, Constantine and Mannor, Shie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{brunskill2013sample,
  title={Sample complexity of multi-task reinforcement learning},
  author={Brunskill, Emma and Li, Lihong},
  journal={arXiv preprint arXiv:1309.6821},
  year={2013}
}

@article{agarwal2020flambe,
  title={Flambe: Structural complexity and representation learning of low rank mdps},
  author={Agarwal, Alekh and Kakade, Sham and Krishnamurthy, Akshay and Sun, Wen},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={20095--20107},
  year={2020}
}

@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020},
  organization={PMLR}
}

@article{auer2010ucb,
  title={UCB revisited: Improved regret bounds for the stochastic multi-armed bandit problem},
  author={Auer, Peter and Ortner, Ronald},
  journal={Periodica Mathematica Hungarica},
  volume={61},
  number={1-2},
  pages={55--65},
  year={2010},
  publisher={Akad{\'e}miai Kiad{\'o}, co-published with Springer Science+ Business Media BV~â€¦}
}

@article{dann2017unifying,
  title={Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{riedmiller2018learning,
  title={Learning by playing solving sparse reward tasks from scratch},
  author={Riedmiller, Martin and Hafner, Roland and Lampe, Thomas and Neunert, Michael and Degrave, Jonas and Wiele, Tom and Mnih, Vlad and Heess, Nicolas and Springenberg, Jost Tobias},
  booktitle={International conference on machine learning},
  pages={4344--4353},
  year={2018},
  organization={PMLR}
}

@article{vecerik2017leveraging,
  title={Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards},
  author={Vecerik, Mel and Hester, Todd and Scholz, Jonathan and Wang, Fumin and Pietquin, Olivier and Piot, Bilal and Heess, Nicolas and Roth{\"o}rl, Thomas and Lampe, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1707.08817},
  year={2017}
}

@inproceedings{li2022settling,
  title={Settling the horizon-dependence of sample complexity in reinforcement learning},
  author={Li, Yuanzhi and Wang, Ruosong and Yang, Lin F},
  booktitle={2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)},
  pages={965--976},
  year={2022},
  organization={IEEE}
}

@article{ren2021nearly,
  title={Nearly horizon-free offline reinforcement learning},
  author={Ren, Tongzheng and Li, Jialian and Dai, Bo and Du, Simon S and Sanghavi, Sujay},
  journal={Advances in neural information processing systems},
  volume={34},
  year={2021}
}


@article{chen2021understanding,
  title={Understanding Domain Randomization for Sim-to-real Transfer},
  author={Chen, Xiaoyu and Hu, Jiachen and Jin, Chi and Li, Lihong and Wang, Liwei},
  journal={arXiv preprint arXiv:2110.03239},
  year={2021}
}
@inproceedings{duan2021risk,
  title={Risk bounds and rademacher complexity in batch reinforcement learning},
  author={Duan, Yaqi and Jin, Chi and Li, Zhiyuan},
  booktitle={International Conference on Machine Learning},
  pages={2892--2902},
  year={2021},
  organization={PMLR}
}