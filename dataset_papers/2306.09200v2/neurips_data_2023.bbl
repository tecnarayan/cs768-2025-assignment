\begin{thebibliography}{10}

\bibitem{playwright}
Playwright.
\newblock \url{https://github.com/microsoft/playwright}.

\bibitem{ahn2022can}
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron
  David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog,
  et~al.
\newblock {Do as I can, not as I say: Grounding language in robotic
  affordances}.
\newblock {\em arXiv preprint arXiv:2204.01691}, 2022.

\bibitem{gpt4all}
Yuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy
  Mulyar.
\newblock {GPT4All: Training an Assistant-style Chatbot with Large Scale Data
  Distillation from GPT-3.5-Turbo}.
\newblock \url{https://github.com/nomic-ai/gpt4all}, 2023.

\bibitem{baker2022video}
Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien
  Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune.
\newblock Video pretraining (vpt): Learning to act by watching unlabeled online
  videos.
\newblock {\em Advances in Neural Information Processing Systems},
  35:24639--24654, 2022.

\bibitem{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle
  O'Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai
  Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van~der
  Wal.
\newblock {Pythia: A Suite for Analyzing Large Language Models Across Training
  and Scaling}, 2023.

\bibitem{black2022gpt}
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
  Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et~al.
\newblock {GPT-NeoX-20B: An Open-Source Autoregressive Language Model}.
\newblock {\em arXiv preprint arXiv:2204.06745}, 2022.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock {Sparks of artificial general intelligence: Early experiments with
  GPT-4}.
\newblock {\em arXiv preprint arXiv:2303.12712}, 2023.

\bibitem{campbell2002DeepBlue}
Murray Campbell, A.~Joseph Hoane, and Feng-hsiung Hsu.
\newblock Deep {{Blue}}.
\newblock {\em Artificial Intelligence}, 134(1):57--83, January 2002.

\bibitem{ccrl}
{CCRL}.
\newblock \url{https://www.computerchess.org.uk/ccrl/}.

\bibitem{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em Advances in neural information processing systems},
  34:15084--15097, 2021.

\bibitem{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing.
\newblock {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT
  Quality}, March 2023.

\bibitem{christiano2017deep}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{dolly2}
Dahoas.
\newblock {D}olly2.
\newblock
  \url{https://huggingface.co/datasets/databricks/databricks-dolly-15k}.

\bibitem{du2023guiding}
Yuqing Du, Olivia Watkins, Zihan Wang, C{\'e}dric Colas, Trevor Darrell, Pieter
  Abbeel, Abhishek Gupta, and Jacob Andreas.
\newblock Guiding pretraining in reinforcement learning with large language
  models.
\newblock {\em arXiv preprint arXiv:2302.06692}, 2023.

\bibitem{du2022glm}
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and
  Jie Tang.
\newblock Glm: General language model pretraining with autoregressive blank
  infilling.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 320--335, 2022.

\bibitem{fan2022minedojo}
Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu,
  Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar.
\newblock {Minedojo: Building open-ended embodied agents with internet-scale
  knowledge}.
\newblock {\em arXiv preprint arXiv:2206.08853}, 2022.

\bibitem{wikidump}
Wikimedia Foundation.
\newblock {Wikimedia Downloads}.
\newblock \url{https://dumps.wikimedia.org}.

\bibitem{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The {P}ile: An 800{GB} dataset of diverse text for language modeling.
\newblock {\em arXiv preprint arXiv:2101.00027}, 2020.

\bibitem{he2016resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep {{Residual Learning}} for {{Image Recognition}}.
\newblock In {\em 2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern
  Recognition}} ({{CVPR}})}, pages 770--778, {Las Vegas, NV, USA}, June 2016.
  {IEEE}.

\bibitem{hong2023metagpt}
Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang,
  Steven Ka~Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et~al.
\newblock Metagpt: Meta programming for multi-agent collaborative framework.
\newblock {\em arXiv preprint arXiv:2308.00352}, 2023.

\bibitem{janner2021offline}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock {Offline reinforcement learning as one big sequence modeling
  problem}.
\newblock {\em Advances in neural information processing systems},
  34:1273--1286, 2021.

\bibitem{safe_rlhf}
Dai Juntao, Ji~Jiaming, Pan Xuehai, Sun Ruiyang, Wang Yizhou, and Yang Yaodong.
\newblock {Constrained Value-Aligned LLM via Safe RLHF}, May 2023.

\bibitem{kopf2023openassistant}
Andreas K{\"o}pf, Yannic Kilcher, Dimitri von R{\"u}tte, Sotiris Anagnostidis,
  Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen~Minh Duc, Oliver
  Stanley, Rich{\'a}rd Nagyfi, et~al.
\newblock {OpenAssistant Conversations--Democratizing Large Language Model
  Alignment}.
\newblock {\em arXiv preprint arXiv:2304.07327}, 2023.

\bibitem{bigscience2022roots}
Hugo Lauren{\c{c}}on, Lucile Saulnier, Thomas Wang, Christopher Akiki,
  Albert~Villanova del Moral, Teven~Le Scao, Leandro~Von Werra, Chenghao Mou,
  Eduardo~Gonz{\'a}lez Ponferrada, Huu Nguyen, J{\"o}rg Frohberg, Mario
  {\v{S}}a{\v{s}}ko, Quentin Lhoest, Angelina McMillan-Major, G{\'e}rard
  Dupont, Stella Biderman, Anna Rogers, Loubna~Ben allal, Francesco~De Toni,
  Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre
  Colombo, Javier de~la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre,
  Sebastian Nagel, Leon Weber, Manuel~Romero Mu{\~n}oz, Jian Zhu, Daniel~Van
  Strien, Zaid Alyafeai, Khalid Almubarak, Vu~Minh Chien, Itziar Gonzalez-Dios,
  Aitor Soroa, Kyle Lo, Manan Dey, Pedro~Ortiz Suarez, Aaron Gokaslan, Shamik
  Bose, David~Ifeoluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny
  Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, and
  Yacine Jernite.
\newblock The bigscience {ROOTS} corpus: A 1.6{TB} composite multilingual
  dataset.
\newblock In {\em Thirty-sixth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2022.

\bibitem{LeelaChessZero}
Leela {{Chess Zero}}.
\newblock \url{https://lczero.org/}.

\bibitem{levine2016end}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock {\em The Journal of Machine Learning Research}, 17(1):1334--1373,
  2016.

\bibitem{li2021grounded}
Liunian~Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan
  Li, Yiwu Zhong, Lijuan Wang, Lu~Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei
  Chang, and Jianfeng Gao.
\newblock Grounded language-image pre-training.
\newblock In {\em CVPR}, 2022.

\bibitem{opening}
Lichess chess opening names.
\newblock \url{https://github.com/lichess-org/chess-openings}.

\bibitem{lichess}
{Lichess Developers}.
\newblock Lichess.
\newblock \url{https://lichess.org/}.

\bibitem{liu2023agentbench}
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu~Gu,
  Hangliang Ding, Kaiwen Men, Kejuan Yang, et~al.
\newblock Agentbench: Evaluating llms as agents.
\newblock {\em arXiv preprint arXiv:2308.03688}, 2023.

\bibitem{mcgrathAcquisitionChessKnowledge2022}
Thomas McGrath, Andrei Kapishnikov, Nenad Toma{\v s}ev, Adam Pearce, Martin
  Wattenberg, Demis Hassabis, Been Kim, Ulrich Paquet, and Vladimir Kramnik.
\newblock Acquisition of chess knowledge in {{AlphaZero}}.
\newblock {\em Proceedings of the National Academy of Sciences},
  119(47):e2206625119, November 2022.

\bibitem{young2020maia}
Reid {McIlroy-Young}, Siddhartha Sen, Jon Kleinberg, and Ashton Anderson.
\newblock Aligning {{Superhuman AI}} with {{Human Behavior}}: {{Chess}} as a
  {{Model System}}.
\newblock In {\em Proceedings of the 26th {{ACM SIGKDD International
  Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}}, {{KDD}} '20,
  pages 1677--1687, {New York, NY, USA}, August 2020. {Association for
  Computing Machinery}.

\bibitem{mishra2021cross}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.
\newblock Cross-task generalization via natural language crowdsourcing
  instructions.
\newblock {\em arXiv preprint arXiv:2104.08773}, 2021.

\bibitem{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock {Playing Atari with deep reinforcement learning}.
\newblock {\em arXiv preprint arXiv:1312.5602}, 2013.

\bibitem{noever2020chess}
David Noever, Matt Ciolino, and Josh Kalin.
\newblock The chess transformer: Mastering play using generative language
  models.
\newblock {\em arXiv preprint arXiv:2008.04057}, 2020.

\bibitem{OrtizSuarezSagotRomary2019}
Pedro~Javier {Ortiz Su{'a}rez}, Benoit Sagot, and Laurent Romary.
\newblock Asynchronous pipelines for processing huge corpora on medium to low
  resource infrastructures.
\newblock In Piotr Ba≈Ñski, Adrien Barbaresi, Hanno Biber, Evelyn Breiteneder,
  Simon Clematide, Marc Kupietz, Harald L{"u}ngen, and Caroline Iliadi,
  editors, {\em Proceedings of the Workshop on Challenges in the Management of
  Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019}, pages 9 -- 16,
  Mannheim, 2019. Leibniz-Institut f{"u}r Deutsche Sprache.

\bibitem{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in Neural Information Processing Systems},
  35:27730--27744, 2022.

\bibitem{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock {Instruction Tuning with GPT-4}.
\newblock {\em arXiv preprint arXiv:2304.03277}, 2023.

\bibitem{perez2018film}
Ethan Perez, Florian Strub, Harm {de Vries}, Vincent Dumoulin, and Aaron
  Courville.
\newblock {{FiLM}}: Visual reasoning with a general conditioning layer.
\newblock In {\em Proceedings of the {{Thirty-Second AAAI Conference}} on
  {{Artificial Intelligence}} and {{Thirtieth Innovative Applications}} of
  {{Artificial Intelligence Conference}} and {{Eighth AAAI Symposium}} on
  {{Educational Advances}} in {{Artificial Intelligence}}},
  {{AAAI}}'18/{{IAAI}}'18/{{EAAI}}'18, pages 3942--3951, {New Orleans,
  Louisiana, USA}, February 2018. {AAAI Press}.

\bibitem{python_chess}
python-chess: a chess library for python.
\newblock \url{https://github.com/niklasf/python-chess}.

\bibitem{radford2021clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever.
\newblock Learning {{Transferable Visual Models From Natural Language
  Supervision}}, February 2021.

\bibitem{2019t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock {\em arXiv e-prints}, 2019.

\bibitem{reed2022generalist}
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio~Gomez Colmenarejo, Alexander
  Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay,
  Jost~Tobias Springenberg, et~al.
\newblock A generalist agent.
\newblock {\em arXiv preprint arXiv:2205.06175}, 2022.

\bibitem{schrittwieser2020mastering}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan,
  Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis,
  Thore Graepel, et~al.
\newblock {M}astering {A}tari, {G}o, chess and shogi by planning with a learned
  model.
\newblock {\em Nature}, 588(7839):604--609, 2020.

\bibitem{sharegpt}
{ShareGPT}.
\newblock
  \url{https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered}.

\bibitem{silver2018az}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, Timothy Lillicrap, Karen Simonyan, and {Demis Hassabis}.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and {{Go}} through self-play.
\newblock {\em Science (New York, N.Y.)}, 362(6419):1140--1144, 2018.

\bibitem{selenium}
Alexei~Barantsev Simon~Stewart.
\newblock Selenium, March 2023.

\bibitem{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar
  Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a}
  Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock {\em arXiv preprint arXiv:2206.04615}, 2022.

\bibitem{stockl2021watching}
Andreas St{\"o}ckl.
\newblock Watching a language model learning chess.
\newblock In {\em Proceedings of the International Conference on Recent
  Advances in Natural Language Processing (RANLP 2021)}, pages 1369--1379,
  2021.

\bibitem{moss}
Tianxiang Sun and Xipeng Qiu.
\newblock Moss, March 2023.

\bibitem{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock {Stanford Alpaca: An Instruction-following LLaMA model}.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem{together2023redpajama}
{Together Computer}.
\newblock {RedPajama: An Open Source Recipe to Reproduce LLaMA training
  dataset}.
\newblock \url{https://github.com/togethercomputer/RedPajama-Data}, april 2023.

\bibitem{toshniwal2022chess}
Shubham Toshniwal, Sam Wiseman, Karen Livescu, and Kevin Gimpel.
\newblock {Chess as a Testbed for Language Model State Tracking}.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2022.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock {Llama: Open and efficient foundation language models}.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{turing1953digital}
Alan Turing.
\newblock Digital computers applied to games.
\newblock {\em Faster than thought}, 1953.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2023voyager}
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu,
  Linxi Fan, and Anima Anandkumar.
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock {\em arXiv preprint arXiv: Arxiv-2305.16291}, 2023.

\bibitem{selfinstruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock {Self-Instruct: Aligning Language Model with Self Generated
  Instructions}, 2022.

\bibitem{wang2023describe}
Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang.
\newblock Describe, explain, plan and select: Interactive planning with large
  language models enables open-world multi-task agents.
\newblock {\em arXiv preprint arXiv:2302.01560}, 2023.

\bibitem{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock {\em arXiv preprint arXiv:2109.01652}, 2021.

\bibitem{yang2023auto}
Hui Yang, Sifu Yue, and Yunzhong He.
\newblock Auto-gpt for online decision making: Benchmarks and additional
  opinions.
\newblock {\em arXiv preprint arXiv:2306.02224}, 2023.

\bibitem{yujian2007normalized}
Li~Yujian and Liu Bo.
\newblock A normalized levenshtein distance metric.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  29(6):1091--1095, 2007.

\bibitem{chatglm}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
  Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et~al.
\newblock {ChatGLM-6B}, March 2023.

\end{thebibliography}
