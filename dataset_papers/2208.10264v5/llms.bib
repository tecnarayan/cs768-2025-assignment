
@article{Cobbe2021T,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={"Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{roziere2021leveraging,
  title={Leveraging automated unit tests for unsupervised code translation},
  author={Roziere, Baptiste and Zhang, Jie M and Charton, Francois and Harman, Mark and Synnaeve, Gabriel and Lample, Guillaume},
  journal={arXiv preprint arXiv:2110.06773},
  year={2021}
}

@misc{umapcode,
  author = {{McInnes, Leland}},
  title = {GitHub - lmcinnes/umap: Uniform Manifold Approximation and Projection (UMAP)},
  publisher = {GitHub},
  year = {2020},
  howpublished = {\url{https://github.com/lmcinnes/umap}},
}

@article{Turing1950ComputingMA,
  title={Computing Machinery and Intelligence},
  author={Alan M. Turing},
  journal={Mind},
  year={1950},
  volume={LIX},
  pages={433-460}
}

@Article{tSNE,
  author =       "Laurens van der Maaten and Geoffrey Hinton",
  title =        "Visualizing data using t-{SNE}",
  journal =      "Journal of Machine Learning Research",
  year =         "2008",
  volume =       "9",
  pages =        "2579--2605"
}

@ARTICLE{UMAP,
       author = {{McInnes}, Leland and {Healy}, John and {Melville}, James},
        title = "{UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}",
      journal = {arXiv e-prints},
     keywords = {Statistics - Machine Learning, Computer Science - Computational Geometry, Computer Science - Machine Learning},
         year = 2018,
        month = feb,
          eid = {arXiv:1802.03426},
        pages = {arXiv:1802.03426},
archivePrefix = {arXiv},
       eprint = {1802.03426},
 primaryClass = {stat.ML},
}

@inproceedings{bender2020climbing,
  title={Climbing towards NLU: On meaning, form, and understanding in the age of data},
  author={Bender, Emily M and Koller, Alexander},
  booktitle={Proceedings of the 58th annual meeting of the association for computational linguistics},
  pages={5185--5198},
  year={2020}
}

@article{influence13,
    doi = {10.1371/journal.pone.0078433},
    author = {Moussaïd, Mehdi AND Kämmer, Juliane E. AND Analytis, Pantelis P. AND Neth, Hansjörg},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Social Influence and the Collective Dynamics of Opinion Formation},
    year = {2013},
    month = {11},
    volume = {8},
    url = {https://doi.org/10.1371/journal.pone.0078433},
    pages = {1-8},
    number = {11},
}

@article{DBLP:journals/corr/HintonVD15,
  author    = {Geoffrey E. Hinton and
               Oriol Vinyals and
               Jeffrey Dean},
  title     = {Distilling the Knowledge in a Neural Network},
  journal   = {CoRR},
  volume    = {abs/1503.02531},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.02531},
  eprinttype = {arXiv},
  eprint    = {1503.02531},
  timestamp = {Mon, 13 Aug 2018 16:48:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HintonVD15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{jones2022capturing,
  title={Capturing failures of large language models via human cognitive biases},
  author={Jones, Erik and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2202.12299},
  year={2022}
}


@misc{rytting2021reasoning,
  doi = {10.48550/ARXIV.2110.02370},
  
  url = {https://arxiv.org/abs/2110.02370},
  
  author = {Rytting, Christopher Michael and Wingate, David},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Leveraging the Inductive Bias of Large Language Models for Abstract Textual Reasoning},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{wikibias,
   author = "Wikipedia",
   title = "{Wikipedia:Systemic bias} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2022",
   howpublished = {\url{http://en.wikipedia.org/w/index.php?title=Wikipedia\%3ASystemic\%20bias&oldid=1102157003}},
   note = "[Online; accessed 30-August-2022]"
 }

@inproceedings{messias2017white,
  title={White, man, and highly followed: Gender and race inequalities in Twitter},
  author={Messias, Johnnatan and Vikatos, Pantelis and Benevenuto, Fabr{\'\i}cio},
  booktitle={Proceedings of the International Conference on Web Intelligence},
  pages={266--274},
  year={2017}
}

@article{vig2020investigating,
  title={Investigating gender bias in language models using causal mediation analysis},
  author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12388--12401},
  year={2020}
}

@inproceedings{sheng-etal-2021-societal,
    title = "Societal Biases in Language Generation: Progress and Challenges",
    author = "Sheng, Emily  and
      Chang, Kai-Wei  and
      Natarajan, Prem  and
      Peng, Nanyun",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.330",
    doi = "10.18653/v1/2021.acl-long.330",
    pages = "4275--4293",
    abstract = "Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.",
}
@inproceedings{blodgett2020language,
  title={Language (Technology) is Power: A Critical Survey of “Bias” in NLP},
  author={Blodgett, Su Lin and Barocas, Solon and Daum{\'e} III, Hal and Wallach, Hanna},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={5454--5476},
  year={2020}
}


@incollection{darling2016extending,
  title={Extending legal protection to social robots: The effects of anthropomorphism, empathy, and violent behavior towards robotic objects},
  author={Darling, Kate},
  booktitle={Robot law},
  year={2016},
  publisher={Edward Elgar Publishing}
}
@article{hartog2000simple,
  title={On a simple measure of individual risk aversion},
  author={Hartog, Joop and Ferrer-i-Carbonell, Ada and Jonker, Nicole},
  journal={Tinbergen Institute},
  year={2000},
  publisher={Citeseer}
}
@article{byrnes1999gender,
  title={Gender differences in risk taking: A meta-analysis.},
  author={Byrnes, James P and Miller, David C and Schafer, William D},
  journal={Psychological bulletin},
  volume={125},
  number={3},
  pages={367},
  year={1999},
  publisher={American Psychological Association}
}

@article{milgram1963behavioral,
  title={Behavioral study of obedience.},
  author={Milgram, Stanley},
  journal={The Journal of abnormal and social psychology},
  volume={67},
  number={4},
  pages={371},
  year={1963},
  publisher={American Psychological Association}
}
@article{christianson2001thematic,
  title={Thematic roles assigned along the garden path linger},
  author={Christianson, Kiel and Hollingworth, Andrew and Halliwell, John F and Ferreira, Fernanda},
  journal={Cognitive psychology},
  volume={42},
  number={4},
  pages={368--407},
  year={2001},
  publisher={Elsevier}
}
@article{Patson2009LingeringMI,
  title={Lingering misinterpretations in garden-path sentences: evidence from a paraphrasing task.},
  author={Nikole D. Patson and Emily S. Darowski and Nicole Moon and Fernanda Ferreira},
  journal={Journal of experimental psychology. Learning, memory, and cognition},
  year={2009},
  volume={35 1},
  pages={
          280-5
        }
}
@misc{chainOfThought,
  doi = {10.48550/ARXIV.2201.11903},
  
  url = {https://arxiv.org/abs/2201.11903},
  
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{macal2010tutorial,
  title={Tutorial on agent-based modelling and simulation},
  author={Macal, CM and North, MJ},
  journal={Journal of Simulation},
  volume={4},
  number={3},
  pages={151--162},
  year={2010}
}
@article{Cong2022PsycholinguisticDO,
  title={Psycholinguistic Diagnosis of Language Models’ Commonsense Reasoning},
  author={Yan Cong},
  journal={Proceedings of the First Workshop on Commonsense Representation and Reasoning (CSRR 2022)},
  year={2022}
}

@article{binz2023using,
  title={Using cognitive psychology to understand GPT-3},
  author={Binz, Marcel and Schulz, Eric},
  journal={Proceedings of the National Academy of Sciences},
  volume={120},
  number={6},
  pages={e2218523120},
  year={2023},
  publisher={National Acad Sciences}
}

@article{hendrycks2021MMLU,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  journal={Proceedings of the International Conference on Learning Representations (ICLR)},
  year={2021}
}



@article{Jiang2021Delphi,
  title={Delphi: Towards Machine Ethics and Norms},
  author={Liwei Jiang and Jena D. Hwang and Chandrasekhar Bhagavatula and Ronan Le Bras and Maxwell Forbes and Jon Borchardt and Jenny Liang and Oren Etzioni and Maarten Sap and Yejin Choi},
  journal={ArXiv},
  url = {https://arxiv.org/abs/2110.07574},
  year={2021},
  volume={abs/2110.07574}
}

@article{palm22,
  url = {https://arxiv.org/abs/2204.02311},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  title = {PaLM: Scaling Language Modeling with Pathways},
  publisher = {arXiv},
  eprint={2204.02311},
  archivePrefix={arXiv},
  journal = {arXiv preprint arXiv:2204.02311},
  primaryClass={cs.CL},
  year = {2022}
}


@misc{GPT4,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2023-05-17},
	publisher = {arXiv},
	author = {OpenAI},
	month = mar,
	year = {2023},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@inproceedings{GPT3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{turingBench,
    title = "{TURINGBENCH}: A Benchmark Environment for {T}uring Test in the Age of Neural Text Generation",
    author = "Uchendu, Adaku  and
      Ma, Zeyu  and
      Le, Thai  and
      Zhang, Rui  and
      Lee, Dongwon",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.172",
    doi = "10.18653/v1/2021.findings-emnlp.172",
    pages = "2001--2016"
}

@inproceedings{shin19synthetic,
title	= {Synthetic Datasets for Neural Program Synthesis},
author	= {Richard Shin and Neel Kant and Kavi Gupta and Chris Bender and Brandon Trabucco and Rishabh Singh and Dawn Song},
year	= {2019},
URL	= {https://openreview.net/pdf?id=ryeOSnAqYm}
}

@misc{bigbench,
  doi = {10.48550/ARXIV.2206.04615},
  
  url = {https://arxiv.org/abs/2206.04615},
  
  author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and (422-others)},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Computers and Society (cs.CY), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  
  publisher = {arXiv},
  
  year = {2022},
  
}


@InProceedings{alet21,
  title = 	 {A large-scale benchmark for few-shot program induction and synthesis}, 
  author =       {Alet, Ferran and Lopez-Contreras, Javier and Koppel, James and Nye, Maxwell and Solar-Lezama, Armando and Lozano-Perez, Tomas and Kaelbling, Leslie and Tenenbaum, Joshua},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {175--186},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/alet21a/alet21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/alet21a.html},
  abstract = 	 {A landmark challenge for AI is to learn flexible, powerful representations from small numbers of examples. On an important class of tasks, hypotheses in the form of programs provide extreme generalization capabilities from surprisingly few examples. However, whereas large natural few-shot learning image benchmarks have spurred progress in meta-learning for deep networks, there is no comparably big, natural program-synthesis dataset that can play a similar role. This is because, whereas images are relatively easy to label from internet meta-data or annotated by non-experts, generating meaningful input-output examples for program induction has proven hard to scale. In this work, we propose a new way of leveraging unit tests and natural inputs for small programs as meaningful input-output examples for each sub-program of the overall program. This allows us to create a large-scale naturalistic few-shot program-induction benchmark and propose new challenges in this domain. The evaluation of multiple program induction and synthesis algorithms points to shortcomings of current methods and suggests multiple avenues for future work.}
}


@inbook{dreamcoder2021,
author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and Sabl\'{e}-Meyer, Mathias and Morales, Lucas and Hewitt, Luke and Cary, Luc and Solar-Lezama, Armando and Tenenbaum, Joshua B.},
title = {DreamCoder: Bootstrapping Inductive Program Synthesis with Wake-Sleep Library Learning},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454080},
abstract = {We present a system for inductive program synthesis called DreamCoder, which inputs a corpus of synthesis problems each specified by one or a few examples, and automatically derives a library of program components and a neural search policy that can be used to efficiently solve other similar synthesis problems. The library and search policy bootstrap each other iteratively through a variant of "wake-sleep" approximate Bayesian learning. A new refactoring algorithm based on E-graph matching identifies common sub-components across synthesized programs, building a progressively deepening library of abstractions capturing the structure of the input domain. We evaluate on eight domains including classic program synthesis areas and AI tasks such as planning, inverse graphics, and equation discovery. We show that jointly learning the library and neural search policy leads to solving more problems, and solving them more quickly.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {835–850},
numpages = {16}
}

@article{edunov2018understanding,
  title={Understanding back-translation at scale},
  author={Edunov, Sergey and Ott, Myle and Auli, Michael and Grangier, David},
  journal={arXiv preprint arXiv:1808.09381},
  year={2018}
}

@article{gao2020pile,
  title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@misc{alphacode2022,
    title={Competition-Level Code Generation with AlphaCode},
    author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and
    Schrittwieser, Julian and Leblond, Rémi and Eccles, Tom and
    Keeling, James and Gimeno, Felix and Dal Lago, Agustin and
    Hubert, Thomas and Choy, Peter and de Masson d'Autume, Cyprien and
    Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and
    Gowal, Sven and Cherepanov, Alexey and Molloy, James and
    Mankowitz, Daniel and Sutherland Robson, Esme and Kohli, Pushmeet and
    de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
    year={2022},
    month={Feb},
    url={https://storage.googleapis.com/deepmind-media/AlphaCode/competition_level_code_generation_with_alphacode.pdf}}


@article{austin2021program,
      title={Program Synthesis with Large Language Models}, 
      author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie Cai and Michael Terry and Quoc Le and Charles Sutton},
      year={2021},
      eprint={2108.07732},
      archivePrefix={arXiv},
      journal = {arXiv preprint arXiv:2108.07732},
      primaryClass={cs.PL},
      url={https://arxiv.org/abs/2108.07732}
}

@article{codex2021,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  year={2021},
  eprint={2107.03374},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  journal = {arXiv preprint arXiv:2107.03374},
  url={https://arxiv.org/abs/2107.03374}
}

@inproceedings{
puzzles2021,
title={Programming Puzzles},
author={Tal Schuster and Ashwin Kalyan and Alex Polozov and Adam Tauman Kalai},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems},
year={2021},
url={https://openreview.net/forum?id=fe_hCc4RBrg}
}

@software{gpt-neo,
  author       = {Black, Sid and
                  Gao, Leo and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language 
                   Modeling with Mesh-Tensorflow}},
  month        = mar,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5551208},
  url          = {https://doi.org/10.5281/zenodo.5551208}
}

@article{tang2019reinforcement,
  title={Reinforcement learning for integer programming: Learning to cut},
  author={Tang, Yunhao and Agrawal, Shipra and Faenza, Yuri},
  journal={arXiv preprint arXiv:1906.04859},
  year={2019}
}

@inproceedings{ding2020accelerating,
  title={Accelerating Primal Solution Findings for Mixed Integer Programs Based on Solution Prediction.},
  author={Ding, Jian-Ya and Zhang, Chao and Shen, Lei and Li, Shengyin and Wang, Bing and Xu, Yinghui and Song, Le},
  booktitle={AAAI},
  pages={1452--1459},
  year={2020}
}

@inproceedings{prates2019learning,
  title={Learning to solve np-complete problems: A graph neural network for decision tsp},
  author={Prates, Marcelo and Avelar, Pedro HC and Lemos, Henrique and Lamb, Luis C and Vardi, Moshe Y},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={4731--4738},
  year={2019}
}

@article{bello2016neural,
  title={Neural combinatorial optimization with reinforcement learning},
  author={Bello, Irwan and Pham, Hieu and Le, Quoc V and Norouzi, Mohammad and Bengio, Samy},
  journal={arXiv preprint arXiv:1611.09940},
  year={2016}
}

@article{selsam2018learning,
  title={Learning a SAT solver from single-bit supervision},
  author={Selsam, Daniel and Lamm, Matthew and B{\"u}nz, Benedikt and Liang, Percy and de Moura, Leonardo and Dill, David L},
  journal={arXiv preprint arXiv:1802.03685},
  year={2018}
}

@article{kurin2019improving,
  title={Improving SAT solver heuristics with graph networks and reinforcement learning},
  author={Kurin, Vitaly and Godil, Saad and Whiteson, Shimon and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.11830},
  year={2019}
}

@article{minsky1961steps,
  title={Steps toward artificial intelligence},
  author={Minsky, Marvin},
  journal={Proceedings of the IRE},
  volume={49},
  number={1},
  pages={8--30},
  year={1961},
  publisher={IEEE}
}

@article{chollet2019measure,
  title={On the measure of intelligence},
  author={Chollet, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1911.01547},
  year={2019}
}

@article{samuel1959some,
  title={Some studies in machine learning using the game of checkers},
  author={Samuel, Arthur L},
  journal={IBM Journal of research and development},
  volume={3},
  number={3},
  pages={210--229},
  year={1959},
  publisher={IBM}
}

@article{galton1907vox,
  title={Vox populi},
  author={Galton, Francis},
  journal={Nature},
  volume={75},
  number={7},
  pages={450--451},
  year={1907}
}
@book{page2008difference,
  title={The Difference: How the Power of Diversity Creates Better Groups, Firms, Schools, and Societies.},
  author={Page, Scott},
  year={2007},
  publisher={Princeton University Press}
}
@book{Surowiecki2005Wisdom,
  author = {Surowiecki, James},
  publisher = {Doubleday},
  title = {The Wisdom of Crowds},
  year = 2004
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{bommasani2021foundation,
  author = {Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and Shyamal Buch and Dallas Card and Rodrigo Castellon and Niladri Chatterji and Annie Chen and Kathleen Creel and Jared Quincy Davis and Dorottya Demszky and ... and Percy Liang},
  journal = {arXiv preprint arXiv:2108.07258},
  title = {On the Opportunities and Risks of Foundation Models},
  year = {2021},
}

@inproceedings{valls2017graph,
  title={Graph grammar-based controllable generation of puzzles for a learning game about parallel programming},
  author={Valls-Vargas, Josep and Zhu, Jichen and Onta{\~n}{\'o}n, Santiago},
  booktitle={Proceedings of the 12th International Conference on the Foundations of Digital Games},
  pages={1--10},
  year={2017}
}

@inproceedings{park2019generating,
  title={Generating educational game levels with multistep deep convolutional generative adversarial networks},
  author={Park, Kyungjin and Mott, Bradford W and Min, Wookhee and Boyer, Kristy Elizabeth and Wiebe, Eric N and Lester, James C},
  booktitle={2019 IEEE Conference on Games (CoG)},
  pages={1--8},
  year={2019},
  organization={IEEE}
}

@article{mott2019controllable,
  title={Controllable and Coherent Level Generation: A Two-Pronged Approach},
  author={Mott, Justin and Nandi, Saujas and Zeller, Luke},
  year={2019}
}

@article{falkner2010puzzle,
  title={Puzzle-based learning for engineering and computer science},
  author={Falkner, Nickolas and Sooriamurthi, Raja and Michalewicz, Zbigniew},
  journal={Computer},
  volume={43},
  number={4},
  pages={20--28},
  year={2010},
  publisher={IEEE}
}

@inproceedings{parsons2006parson,
  title={Parson's programming puzzles: a fun and effective learning tool for first programming courses},
  author={Parsons, Dale and Haden, Patricia},
  booktitle={Proceedings of the 8th Australasian Conference on Computing Education-Volume 52},
  pages={157--163},
  year={2006}
}

@inproceedings{ellis2019write,
  title={Write, execute, assess: Program synthesis with a repl},
  author={Ellis, Kevin and Nye, Maxwell and Pu, Yewen and Sosa, Felix and Tenenbaum, Josh and Solar-Lezama, Armando},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9165--9174},
  year={2019}
}

@article{lample2019deep,
  title={Deep learning for symbolic mathematics},
  author={Lample, Guillaume and Charton, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1912.01412},
  year={2019}
}

@inproceedings{perez2018film,
  title={Film: Visual reasoning with a general conditioning layer},
  author={Perez, Ethan and Strub, Florian and De Vries, Harm and Dumoulin, Vincent and Courville, Aaron},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{campbell2002deep,
  title={Deep blue},
  author={Campbell, Murray and Hoane Jr, A Joseph and Hsu, Feng-hsiung},
  journal={Artificial intelligence},
  volume={134},
  number={1-2},
  pages={57--83},
  year={2002},
  publisher={Elsevier}
}

@article{brockschmidt2019gnn,
  title={{GNN-FiLM}: Graph Neural Networks with Feature-wise Linear Modulation},
  author={Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1906.12192},
  year={2019}
}


@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{silver1994mathematical,
  title={On mathematical problem posing},
  author={Silver, Edward A},
  journal={For the learning of mathematics},
  volume={14},
  number={1},
  pages={19--28},
  year={1994},
  publisher={JSTOR}
}

@inproceedings{johnson2017clevr,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2901--2910},
  year={2017}
}

@article{barrett2018measuring,
  title={Measuring abstract reasoning in neural networks},
  author={Barrett, David GT and Hill, Felix and Santoro, Adam and Morcos, Ari S and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1807.04225},
  year={2018}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{christakopoulou2018glass,
  title={Glass-Box Program Synthesis: A Machine Learning Approach},
  author={Christakopoulou, Konstantina and Kalai, Adam Tauman},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{gulwani2012synthesis,
  title={Synthesis from examples: Interaction models and algorithms},
  author={Gulwani, Sumit},
  pages={8--14},
  year={2012},
  booktitle={SYNASC}
  }
@book{skiena,
  title={The algorithm design manual: Text},
  author={Skiena, Steven S},
  volume={1},
  year={1998},
  publisher={Springer Science \& Business Media}
}
@phdthesis{solar,
  title={Program synthesis by sketching.},
  author={Solar-Lezama, Armando.},
  school={EECS Department, University of California, Berkeley},
  year={2008}
}
@inproceedings{menon2013machine,
  title={A Machine Learning Framework for Programming by Example.},
  author={Menon, Aditya Krishna and Tamuz, Omer and Gulwani, Sumit and Lampson, Butler W and Kalai, Adam},
  booktitle={ICML},
  pages={187--195},
  year={2013}
}
@inproceedings{yessenov2013colorful,
  title={A colorful approach to text processing by example},
  author={Yessenov, Kuat and Tulsiani, Shubham and Menon, Aditya and Miller, Robert C and Gulwani, Sumit and Lampson, Butler and Kalai, Adam},
  booktitle={UIST},
  pages={495--504},
  year={2013}
}
@inproceedings{lavrac1994inductive,
  title={Inductive Logic Programming.},
  author={Lavrac, Nada and Dzeroski, Saso},
  booktitle={WLP},
  pages={146--160},
  year={1994},
  organization={Springer}
}
@article{singh2012automatically,
  title={Automatically Generating Algebra Problems.},
  author={Singh, Rohit and Gulwani, Sumit and Rajamani, Sriram K},
  booktitle={AAAI},
  year={2012}
}
@inproceedings{raza2015compositional,
  title={Compositional program synthesis from natural language and examples},
  author={Raza, Mohammad and Gulwani, Sumit and Milic-Frayling, Natasa},
  booktitle={IJCAI},
  pages={792--800},
  year={2015}
}
@article{lake2015human,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  volume={350},
  number={6266},
  pages={1332--1338},
  year={2015},
  publisher={American Association for the Advancement of Science}
}
@inproceedings{dechter2013bootstrap,
  title={Bootstrap Learning via Modular Concept Discovery.},
  author={Dechter, Eyal and Malmaud, Jonathan and Adams, Ryan P and Tenenbaum, Joshua B},
    pages={1302--1309},
  booktitle={IJCAI},
  year={2013}
}


@article{Svyatkovskiy_2020,
   title={{IntelliCode} {Compose}: code generation using {Transformers}},
   ISBN={9781450370431},
   url={http://dx.doi.org/10.1145/3368089.3417058},
   DOI={10.1145/3368089.3417058},
   journal={Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
   publisher={ACM},
   author={Svyatkovskiy, Alexey and Deng, Shao Kun and Fu, Shengyu and Sundaresan, Neel},
   year={2020},
   month={Nov}
}
@article{hendrycks2016gaussian,
    title={Gaussian Error Linear Units (GELUs)},
    author={Dan Hendrycks and Kevin Gimpel},
    year={2016},
    eprint={1606.08415},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{ioffe2015batch,
    title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
    author={Sergey Ioffe and Christian Szegedy},
    year={2015},
    eprint={1502.03167},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{Gou2021,
author = {Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
doi = {10.1007/s11263-021-01453-z},
issn = {1573-1405},
journal = {International Journal of Computer Vision},
number = {6},
pages = {1789--1819},
title = {Knowledge Distillation: A Survey},
url = {https://doi.org/10.1007/s11263-021-01453-z},
volume = {129},
year = {2021}
}


@article{loshchilov2018fixing,
  title={Fixing weight decay regularization in adam},
  author={Loshchilov, Ilya and Hutter, Frank},
  year={2018}
}


@article{west2021symbolic,
      title={Symbolic Knowledge Distillation: from General Language Models to Commonsense Models}, 
      author={Peter West and Chandra Bhagavatula and Jack Hessel and Jena D. Hwang and Liwei Jiang and Ronan Le Bras and Ximing Lu and Sean Welleck and Yejin Choi},
      year={2021},
      eprint={2110.07178},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{husain2019codesearchnet,
  title={{CodeSearchNet} challenge: Evaluating the state of semantic code search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  journal={arXiv preprint arXiv:1909.09436},
  year={2019}
}
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
@article{liu2019roberta,
    title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
    year={2019},
    eprint={1907.11692},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{alon2020structural,
  title={Structural language models of code},
  author={Alon, Uri and Sadaka, Roy and Levy, Omer and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={245--256},
  year={2020},
  organization={PMLR}
}

@inproceedings{cubert,
author    = {Aditya Kanade and
             Petros Maniatis and
             Gogul Balakrishnan and
             Kensen Shi},
title     = {Learning and evaluating contextual embedding of source code},
booktitle = {Proceedings of the 37th International Conference on Machine Learning,
               {ICML} 2020, 12-18 July 2020},
series    = {Proceedings of Machine Learning Research},
publisher = {{PMLR}},
year      = {2020},
}

@misc{lu2021codexglue,
      title={CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation}, 
      author={Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and Alexey Svyatkovskiy and Ambrosio Blanco and Colin Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Shengyu Fu and Shujie Liu},
      year={2021},
      eprint={2102.04664},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{ren2020codebleu,
      title={CodeBLEU: a Method for Automatic Evaluation of Code Synthesis}, 
      author={Shuo Ren and Daya Guo and Shuai Lu and Long Zhou and Shujie Liu and Duyu Tang and Neel Sundaresan and Ming Zhou and Ambrosio Blanco and Shuai Ma},
      year={2020},
      eprint={2009.10297},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@inproceedings{feng-etal-2020-codebert,
    title = "{C}ode{BERT}: A Pre-Trained Model for Programming and Natural Languages",
    author = "Feng, Zhangyin  and
      Guo, Daya  and
      Tang, Duyu  and
      Duan, Nan  and
      Feng, Xiaocheng  and
      Gong, Ming  and
      Shou, Linjun  and
      Qin, Bing  and
      Liu, Ting  and
      Jiang, Daxin  and
      Zhou, Ming",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.139",
    doi = "10.18653/v1/2020.findings-emnlp.139",
    pages = "1536--1547",
    abstract = "We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both {``}bimodal{''} data of NL-PL pairs and {``}unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.",
}

@inproceedings{transformer2017,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{vincent2020openai,
    title={The Ultimate Autocomplete},
    author={James Vincent},
    year={2020},
    month={July},
    url={https://www.theverge.com/21346343/gpt-3-explainer-openai-examples-errors-agi-potential},
    publisher={The Verge}
}

@article{polu2020generative,
  author    = {Stanislas Polu and
               Ilya Sutskever},
  title     = {Generative Language Modeling for Automated Theorem Proving},
  journal   = {CoRR},
  volume    = {abs/2009.03393},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.03393},
  archivePrefix = {arXiv},
  eprint    = {2009.03393},
  timestamp = {Sat, 23 Jan 2021 01:12:25 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-03393.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{andrychowicz2016learning,
  title={Learning Efficient Algorithms with Hierarchical Attentive Memory},
  author={Andrychowicz, Marcin and Kurach, Karol},
  journal={arXiv preprint arXiv:1602.03218},
  year={2016}
}
@article{thornton2013auto,
  title={Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms},
  author={Thornton, Chris and Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  journal={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={847--855},
  year={2013},
  organization={ACM}
}
@article{hutter2011sequential,
  title={Sequential model-based optimization for general algorithm configuration},
  author={Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  booktitle={International Conference on Learning and Intelligent Optimization},
  pages={507--523},
  year={2011},
  organization={Springer}
}
@article{andrychowicz2016gradient,
  title={Learning to learn by gradient descent by gradient descent},
  author={Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and de Freitas, Nando},
  journal={arXiv preprint arXiv:1606.04474},
  year={2016}
}
@inproceedings{joulin2015inferring,
  title={Inferring algorithmic patterns with stack-augmented recurrent nets},
  author={Joulin, Armand and Mikolov, Tomas},
  booktitle={NIPS},
  pages={190--198},
  year={2015}
}
@InProceedings{balog2016deepcoder,
  author    = {Matej Balog and
               Alexander L. Gaunt and
               Marc Brockschmidt and
               Sebastian Nowozin and
               Daniel Tarlow},
  title     = {DeepCoder: Learning to Write Programs},
  booktitle = {International Conference on Representation Learning (ICLR)},
  year      = {2017}
}
@article{zaremba2014learning,
  title={Learning to execute},
  author={Zaremba, Wojciech and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1410.4615},
  year={2014}
}
@article{neelakantan2015neural,
  title={Neural programmer: Inducing latent programs with gradient descent},
  author={Neelakantan, Arvind and Le, Quoc V and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1511.04834},
  year={2015}
}
@article{reed2015neural,
  title={Neural programmer-interpreters},
  author={Reed, Scott and de Freitas, Nando},
  journal={arXiv preprint arXiv:1511.06279},
  year={2015}
}
@article{graves2014neural,
  title={Neural turing machines},
  author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  journal={arXiv preprint arXiv:1410.5401},
  year={2014}
}
@article{mikolov2015roadmap,
  title={A roadmap towards machine intelligence},
  author={Mikolov, Tomas and Joulin, Armand and Baroni, Marco},
  journal={arXiv preprint arXiv:1511.08130},
  year={2015}
}
@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}
@article{weston2015towards,
  title={Towards ai-complete question answering: A set of prerequisite toy tasks},
  author={Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M and van Merri{\"e}nboer, Bart and Joulin, Armand and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1502.05698},
  year={2015}
}
@article{shotton2013real,
  title={Real-time human pose recognition in parts from single depth images},
  author={Shotton, Jamie and Sharp, Toby and Kipman, Alex and Fitzgibbon, Andrew and Finocchio, Mark and Blake, Andrew and Cook, Mat and Moore, Richard},
  journal={Communications of the ACM},
  volume={56},
  number={1},
  pages={116--124},
  year={2013},
  publisher={ACM}
}
@article{johnson2016malmo,
  title={The Malmo platform for artificial intelligence experimentation},
  author={Johnson, Matthew and Hofmann, Katja and Hutton, Tim and Bignell, David},
  booktitle={International joint conference on artificial intelligence (IJCAI)},
  year={2016}
}
@article{krishnamurthy2015learning,
  title={Learning to search better than your teacher},
  author={Krishnamurthy, Akshay and EDU, CMU and Daum{\'e} III, Hal and EDU, UMD},
  journal={arXiv preprint arXiv:1502.02206},
  year={2015}
}
@article{schmidhuber1993self,
  title={A ‘Self-Referential’Weight Matrix},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={ICANN’93},
  pages={446--450},
  year={1993},
  publisher={Springer}
}
@article{Lloyd2014-ABCD,
  title         = "Automatic Construction and {Natural-Language} Description of
                   Nonparametric Regression Models",
  author        = {Lloyd, James Robert and Duvenaud, David and Grosse, Roger
                   and Tenenbaum, Joshua B. and Ghahramani, Zoubin},
  booktitle     = "Association for the Advancement of Artificial Intelligence
                   (AAAI)",
  year          = 2014
}
@article{grosse2012exploiting,
  title={Exploiting compositionality to explore a large space of model structures},
  author={Grosse, R.B. and Salakhutdinov, R. and Freeman, W.T. and Tenenbaum, J.B.},
  booktitle={Uncertainty in Artificial Intelligence},
  year={2012}
}
@article{feurer2015efficient,
  title={Efficient and robust automated machine learning},
  author={Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2962--2970},
  year={2015}
}
@article{graves2016hybrid,
  title={Hybrid computing using a neural network with dynamic external memory},
  author={Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'n}ska, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and others},
  journal={Nature},
  volume={538},
  number={7626},
  pages={471--476},
  year={2016},
  publisher={Nature Research}
}
@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}
@article{samulowitz2014cognitive,
  title={Cognitive automation of data science},
  author={Samulowitz, Horst and Sabharwal, A and Reddy, C},
  booktitle={ICML AutoML workshop},
  year={2014}
}
@article{gulwani2012spreadsheet,
  title={Spreadsheet data manipulation using examples},
  author={Gulwani, Sumit and Harris, William R and Singh, Rishabh},
  journal={Communications of the ACM},
  volume={55},
  number={8},
  pages={97--105},
  year={2012},
  publisher={ACM}
}
@article{lake2016building,
  title={Building machines that learn and think like people},
  author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={arXiv preprint arXiv:1604.00289},
  year={2016}
}
@article{clark2015elementary,
  title={Elementary school science and math tests as a driver for AI: take the aristo challenge!},
  author={Clark, Peter},
  booktitle={AAAI},
  pages={4019--4021},
  year={2015}
}
@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}
@article{rumelhart1988learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={Cognitive modeling},
  volume={5},
  number={3},
  pages={1},
  year={1988}
}
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}
@article{taskar2005learning,
  title={Learning structured prediction models: A large margin approach},
  author={Taskar, Ben and Chatalbashev, Vassil and Koller, Daphne and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd international conference on Machine learning},
  pages={896--903},
  year={2005},
  organization={ACM}
}
@book{sutton1998reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  volume={1},
  number={1},
  year={1998},
  publisher={MIT press Cambridge}
}

@article{he2015deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  journal={arXiv preprint arXiv:1512.03385},
  year={2015}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}
@article{cai2017making,
  title={Making neural programming architectures generalize via recursion},
  author={Cai, Jonathon and Shin, Richard and Song, Dawn},
  journal={arXiv preprint arXiv:1704.06611},
  year={2017}
}
@book{russell2003artificial,
  title={Artificial intelligence: a modern approach},
  author={Russell, Stuart Jonathan and Norvig, Peter and Canny, John F and Malik, Jitendra M and Edwards, Douglas D},
  volume={2},
  year={2003},
  publisher={Prentice hall Upper Saddle River}
}
@inproceedings{bello2017neural,
  title={Neural Optimizer Search with Reinforcement Learning},
  author={Bello, Irwan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc V},
  booktitle={International Conference on Machine Learning},
  pages={459--468},
  year={2017}
}
@inproceedings{chen2017learning,
  title={Learning to learn without gradient descent by gradient descent},
  author={Chen, Yutian and Hoffman, Matthew W and Colmenarejo, Sergio G{\'o}mez and Denil, Misha and Lillicrap, Timothy P and Botvinick, Matt and Freitas, Nando},
  booktitle={International Conference on Machine Learning},
  pages={748--756},
  year={2017}
}
@inproceedings{gaunt2017differentiable,
  title={Differentiable Programs with Neural Libraries},
  author={Gaunt, Alexander L and Brockschmidt, Marc and Kushman, Nate and Tarlow, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={1213--1222},
  year={2017}
}
@article{gaunt2016lifelong,
  title={Lifelong Perceptual Programming By Example},
  author={Gaunt, Alexander L and Brockschmidt, Marc and Kushman, Nate and Tarlow, Daniel},
  journal={arXiv preprint arXiv:1611.02109},
  year={2016}
}
@article{feser2016neural,
  title={Neural Functional Programming},
  author={Feser, John K and Brockschmidt, Marc and Gaunt, Alexander L and Tarlow, Daniel},
  journal={arXiv preprint arXiv:1611.01988},
  year={2016}
}
@article{li2016learning,
  title={Learning to optimize},
  author={Li, Ke and Malik, Jitendra},
  journal={arXiv preprint arXiv:1606.01885},
  year={2016}
}
@article{parisotto2016neuro,
  title={Neuro-symbolic program synthesis},
  author={Parisotto, Emilio and Mohamed, Abdel-rahman and Singh, Rishabh and Li, Lihong and Zhou, Dengyong and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:1611.01855},
  year={2016}
}
@article{devlin2017robustfill,
  title={RobustFill: Neural Program Learning under Noisy I/O},
  author={Devlin, Jacob and Uesato, Jonathan and Bhupatiraju, Surya and Singh, Rishabh and Mohamed, Abdel-rahman and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:1703.07469},
  year={2017}
}

@article{hendrycks2021measuring,
      title={Measuring Coding Challenge Competence With {APPS}}, 
      author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2105.09938},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}
@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}
@article{gaunt2016terpret,
  title={Terpret: A probabilistic programming language for program induction},
  author={Gaunt, Alexander L and Brockschmidt, Marc and Singh, Rishabh and Kushman, Nate and Kohli, Pushmeet and Taylor, Jonathan and Tarlow, Daniel},
  journal={arXiv preprint arXiv:1608.04428},
  year={2016}
}
@article{manna1980deductive,
  title={A deductive approach to program synthesis},
  author={Manna, Zohar and Waldinger, Richard},
  journal={TOPLAS},
  volume={2},
  number={1},
  pages={90--121},
  year={1980},
  publisher={ACM}
}

@book{coq,
  title={Interactive Theorem Proving and Program Development: Coq’Art: The Calculus of Inductive Constructions},
  author={Bertot, Yves and Cast{\'e}ran, Pierre},
  year={2004},
  publisher={Springer Science \& Business Media}
}

@book{isabelle,
  title={Isabelle: A generic theorem prover},
  author={Paulson, Lawrence C},
  volume={828},
  year={1994},
  publisher={Springer Science \& Business Media}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{lean,
  title={The Lean theorem prover (system description)},
  author={de Moura, Leonardo and Kong, Soonho and Avigad, Jeremy and Van Doorn, Floris and von Raumer, Jakob},
  booktitle={International Conference on Automated Deduction},
  pages={378--388},
  year={2015},
  organization={Springer}
}

@inproceedings{merz2000model,
  title={Model checking: A tutorial overview},
  author={Merz, Stephan},
  booktitle={Summer School on Modeling and Verification of Parallel Processes},
  pages={3--38},
  year={2000},
  organization={Springer}
}

@book{lamport2002specifying,
  title={Specifying systems: the TLA+ language and tools for hardware and software engineers},
  author={Lamport, Leslie},
  year={2002},
  publisher={Addison-Wesley Longman Publishing Co., Inc.}
}

@inproceedings{vampire,
  title={First-order theorem proving and Vampire},
  author={Kov{\'a}cs, Laura and Voronkov, Andrei},
  booktitle={International Conference on Computer Aided Verification},
  pages={1--35},
  year={2013},
  organization={Springer}
}

@inproceedings{z3,
  title={Z3: An efficient SMT solver},
  author={De Moura, Leonardo and Bj{\o}rner, Nikolaj},
  booktitle={International conference on Tools and Algorithms for the Construction and Analysis of Systems},
  pages={337--340},
  year={2008},
  organization={Springer}
}

@book{satbook,
  title={Handbook of satisfiability},
  author={Biere, Armin and Heule, Marijn and van Maaren, Hans},
  volume={185},
  year={2009},
  publisher={IOS press}
}

@inproceedings{theoremgeneration,
  title={The HR program for theorem generation},
  author={Colton, Simon},
  booktitle={International Conference on Automated Deduction},
  pages={285--289},
  year={2002},
  organization={Springer}
}

@misc{nearblog,
    author= {Illia Polosukhin},
    year  = {2018},
    month = {August},
    day = {23},
    title = {{NEAR.ai} $\to$ {NEAR Protocol}},
    note  = {\url{https://nearprotocol.com/blog/near-ai-near-protocol/}, 
             Last accessed on 2019-09-24}
}

@misc{copilot,
    author= {Albert Ziegler},
    year  = {2021},
    month = {June},
    title = {Research recitation: A first look at rote learning in GitHub Copilot suggestions.},
    note  = {\url{https://docs.github.com/en/github/copilot/research-recitation},
            Last accessed on 2021-11-01}
}


@article{welbl2018constructing,
  title={Constructing datasets for multi-hop reading comprehension across documents},
  author={Welbl, Johannes and Stenetorp, Pontus and Riedel, Sebastian},
  journal={Transactions of the Association of Computational Linguistics},
  volume={6},
  pages={287--302},
  year={2018},
  publisher={MIT Press}
}

@article{Weston2016TowardsAQ,
  title={Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks},
  author={Jason Weston and Antoine Bordes and Sumit Chopra and Tomas Mikolov},
  journal={CoRR},
  year={2016},
  volume={abs/1502.05698}
}

@article{textworld,
  title={TextWorld: A Learning Environment for Text-based Games},
  author={C{\^o}t{\'e}, Marc-Alexandre and K{\'a}d{\'a}r, {\'A}kos and Yuan, Xingdi and Kybartas, Ben and Barnes, Tavian and Fine, Emery and Moore, James and Hausknecht, Matthew and El Asri, Layla and Adada, Mahmoud and others},
  journal={arXiv preprint arXiv:1806.11532},
  year={2018}
}

@article{blocksworld,
  title={Blocks world revisited},
  author={Slaney, John and Thi{\'e}baux, Sylvie},
  journal={Artificial Intelligence},
  volume={125},
  number={1-2},
  pages={119--153},
  year={2001},
  publisher={Elsevier}
}

@inproceedings{minecraft,
  title={A Deep Hierarchical Approach to Lifelong Learning in Minecraft.},
  author={Tessler, Chen and Givony, Shahar and Zahavy, Tom and Mankowitz, Daniel J and Mannor, Shie},
  booktitle={AAAI},
  volume={3},
  pages={6},
  year={2017}
}

@article{lipton2018mythos,
  title={The mythos of model interpretability},
  author={Lipton, Zachary C},
  journal={Queue},
  volume={16},
  number={3},
  pages={31--57},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@inproceedings{kaliszyk2018reinforcement,
  title={Reinforcement Learning of Theorem Proving},
  author={Kaliszyk, Cezary and Urban, Josef and Michalewski, Henryk and Ol{\v{s}}{\'a}k, Mirek},
  booktitle={NeurIPS},
  year={2018}
}

@article{deepMath,
  author    = {David Saxton and
               Edward Grefenstette and
               Felix Hill and
               Pushmeet Kohli},
  title     = {Analysing Mathematical Reasoning Abilities of Neural Models},
  journal   = {CoRR},
  volume    = {abs/1904.01557},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.01557},
  archivePrefix = {arXiv},
  eprint    = {1904.01557},
  timestamp = {Wed, 24 Apr 2019 12:21:25 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1904-01557},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{goldreich2010p,
  title={P, NP, and NP-Completeness: The Basics of Computational Complexity},
  author={Goldreich, O.},
  isbn={9781139490092},
  year={2010},
  publisher={Cambridge University Press}
}

@article{alphaZero,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{blumNP,
title = "Training a 3-node neural network is NP-complete",
journal = "Neural Networks",
volume = "5",
number = "1",
pages = "117 - 127",
year = "1992",
issn = "0893-6080",
doi = "https://doi.org/10.1016/S0893-6080(05)80010-3",
url = "http://www.sciencedirect.com/science/article/pii/S0893608005800103",
author = "Avrim L. Blum and Ronald L. Rivest",
keywords = "Neural networks, Computational complexity, NP-completeness, Intractability, Learning, Training, Multilayer perceptron, Representation",
abstract = "We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for this network so that it produces output consistent with a given set of training examples. We extend the result to other simple networks. We also present a network for which training is hard but where switching to a more powerful representation makes training easier. These results suggest that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. They also suggest the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with nonlinear functions such as sigmoids."
}

@inproceedings{prose_automating,
author = {Gulwani, Sumit},
title = {Automating String Processing in Spreadsheets Using Input-Output Examples},
year = {2011},
isbn = {9781450304900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1926385.1926423},
doi = {10.1145/1926385.1926423},
booktitle = {Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {317–330},
numpages = {14},
keywords = {spreadsheet programming, string manipulation, user intent, program synthesis, version space algebra, programming by example (pbe)},
location = {Austin, Texas, USA},
series = {POPL '11}
}



@article{naps2018,
  author    = {Maksym Zavershynskyi and
               Alexander Skidanov and
               Illia Polosukhin},
  title     = {{NAPS:} Natural Program Synthesis Dataset},
  journal   = {CoRR},
  volume    = {abs/1807.03168},
  year      = {2018},
  url       = {http://arxiv.org/abs/1807.03168},
  archivePrefix = {arXiv},
  eprint    = {1807.03168},
  timestamp = {Mon, 13 Aug 2018 16:47:15 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1807-03168},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{synthesisSurvey2017,
  title={Program synthesis},
  author={Gulwani, Sumit and Polozov, Oleksandr and Singh, Rishabh and others},
  journal={Foundations and Trends{\textregistered} in Programming Languages},
  volume={4},
  number={1-2},
  pages={1--119},
  year={2017},
  publisher={Now Publishers, Inc.}
}

@article{manna1971toward,
  title={Toward automatic program synthesis},
  author={Manna, Zohar and Waldinger, Richard J},
  journal={Communications of the ACM},
  volume={14},
  number={3},
  pages={151--165},
  year={1971},
  publisher={ACM}
}

@article{lamport1999specifying,
  title={Specifying Concurrent Systems with TLA\^{}+},
  author={Lamport, Leslie},
  journal={NATO ASI SERIES F COMPUTER AND SYSTEMS SCIENCES},
  volume={173},
  pages={183--250},
  year={1999},
  publisher={SPRINGER VERLAG}
}

@article{Liang:2016:LES:2991470.2866568,
 author = {Liang, Percy},
 title = {Learning Executable Semantic Parsers for Natural Language Understanding},
 journal = {Commun. ACM},
 issue_date = {September 2016},
 volume = {59},
 number = {9},
 month = aug,
 year = {2016},
 issn = {0001-0782},
 pages = {68--76},
 numpages = {9},
 doi = {10.1145/2866568},
 acmid = {2866568},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Bornholt:2017:SMM:3062341.3062353,
 author = {Bornholt, James and Torlak, Emina},
 title = {Synthesizing Memory Models from Framework Sketches and Litmus Tests},
 booktitle = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation},
 series = {PLDI 2017},
 year = {2017},
 isbn = {978-1-4503-4988-8},
 location = {Barcelona, Spain},
 pages = {467--481},
 numpages = {15},
 doi = {10.1145/3062341.3062353},
 acmid = {3062353},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {program synthesis, weak memory models},
}

@article{alur2018search,
  title={Search-based program synthesis},
  author={Alur, Rajeev and Singh, Rishabh and Fisman, Dana and Solar-Lezama, Armando},
  journal={Communications of the ACM},
  volume={61},
  number={12},
  pages={84--93},
  year={2018},
  publisher={ACM}
}

@inproceedings{
polosukhin2018neural,
title={Neural Program Search: Solving Data Processing Tasks from Description and Examples},
author={Illia Polosukhin, Alexander Skidanov},
year={2018},
 booktitle={ICLR Workshop Acceptance Decision},
url={https://openreview.net/forum?id=B1KJJf-R-},
}

@article{alur2019syguscomp,
      title={{SyGuS-Comp} 2018: Results and Analysis}, 
      author={Rajeev Alur and Dana Fisman and Saswat Padhi and Rishabh Singh and Abhishek Udupa},
      year={2019},
      eprint={1904.07146},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@inproceedings{shin2019synthetic,
  title={Synthetic Datasets for Neural Program Synthesis},
  booktitle={International Conference on Learning Representations},
  author={Shin, Richard and Kant, Neel and Gupta, Kavi and Bender, Chris and Trabucco, Brandon and Singh, Rishabh and Song, Dawn},
  year={2019}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={Proceedings of the 26th annual international conference on machine learning},
  pages={41--48},
  year={2009},
  organization={ACM}
}

@inproceedings{graves2017automated,
  title={Automated curriculum learning for neural networks},
  author={Graves, Alex and Bellemare, Marc G and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1311--1320},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{sukhbaatar2017intrinsic,
  title={Intrinsic motivation and automatic curricula via asymmetric self-play},
  author={Sukhbaatar, Sainbayar and Lin, Zeming and Kostrikov, Ilya and Synnaeve, Gabriel and Szlam, Arthur and Fergus, Rob},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}

@inproceedings{pinto2017robust,
  title={Robust adversarial reinforcement learning},
  author={Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2817--2826},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{durugkar2018adversarial,
  title={Adversarial Goal Generation for Intrinsic Motivation},
  author={Durugkar, Ishan and Stone, Peter},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{arc,
  title={The Measure of Intelligence},
  author={Chollet, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1911.01547},
  year={2019}
}  


@inproceedings{zinkevich2003online,
  title={Online convex programming and generalized infinitesimal gradient ascent},
  author={Zinkevich, Martin},
  booktitle={Proceedings of the 20th International Conference on Machine Learning (ICML-03)},
  pages={928--936},
  year={2003}
}

@article{neumann1928theorie,
  title={Zur theorie der gesellschaftsspiele},
  author={Neumann, J v},
  journal={Mathematische annalen},
  volume={100},
  number={1},
  pages={295--320},
  year={1928},
  publisher={Springer}
}

@book{myerson2013game,
  title={Game theory},
  author={Myerson, Roger B},
  year={2013},
  publisher={Harvard university press}
}

@article{allamanis2017learning,
  title={Learning to represent programs with graphs},
  author={Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
  journal={arXiv preprint arXiv:1711.00740},
  year={2017}
}

@article{brockschmidt2018generative,
  title={Generative code modeling with graphs},
  author={Brockschmidt, Marc and Allamanis, Miltiadis and Gaunt, Alexander L and Polozov, Oleksandr},
  journal={arXiv preprint arXiv:1805.08490},
  year={2018}
}

@article{kalyan2018neural,
  title={Neural-guided deductive search for real-time program synthesis from examples},
  author={Kalyan, Ashwin and Mohta, Abhishek and Polozov, Oleksandr and Batra, Dhruv and Jain, Prateek and Gulwani, Sumit},
  journal={arXiv preprint arXiv:1804.01186},
  year={2018}
}

@article{rabin1986randomized,
  title={Randomized algorithms in number theory},
  author={Rabin, Michael O and Shallit, Jeffery O},
  journal={Communications on Pure and Applied Mathematics},
  volume={39},
  number={S1},
  pages={S239--S256},
  year={1986},
  publisher={Wiley Online Library}
}

@Book{norman96,
author = { Biggs, Norman L. },
title = { Finite groups of automorphisms; course given at the University of Southampton, October-December 1969 },
isbn = { 0521082153 },
publisher = { University Press Cambridge [Eng.] },
pages = { iii, 117 p. },
year = { 1971 },
type = { Book },
language = { English },
subjects = { Finite groups.; Graph theory. },
life-dates = { 1971 -  },
catalogue-url = { https://nla.gov.au/nla.cat-vn169693 },
}

@article{silver2017mastering,
    title={Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
    author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
    year={2017},
    eprint={1712.01815},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}

@inproceedings {schuster2021autocomplete,
author = {Roei Schuster and Congzheng Song and Eran Tromer and Vitaly Shmatikov},
title = {You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion},
booktitle = {30th {USENIX} Security Symposium ({USENIX} Security 21)},
year = {2021},
url = {https://www.usenix.org/conference/usenixsecurity21/presentation/schuster},
publisher = {{USENIX} Association},
month = aug,
}

@article {ivanona2020computer,
article_type = {journal},
title = {Comprehension of computer code relies primarily on domain-general executive brain regions},
author = {Ivanova, Anna A and Srikant, Shashank and Sueoka, Yotaro and Kean, Hope H and Dhamala, Riva and O'Reilly, Una-May and Bers, Marina U and Fedorenko, Evelina},
editor = {Martin, Andrea E and Behrens, Timothy E and Matchin, William and Bornkessel-Schlesewsky, Ina},
volume = 9,
year = 2020,
month = {dec},
pub_date = {2020-12-15},
pages = {e58906},
citation = {eLife 2020;9:e58906},
doi = {10.7554/eLife.58906},
url = {https://doi.org/10.7554/eLife.58906},
abstract = {Computer programming is a novel cognitive tool that has transformed modern society. What cognitive and neural mechanisms support this skill? Here, we used functional magnetic resonance imaging to investigate two candidate brain systems: the multiple demand (MD) system, typically recruited during math, logic, problem solving, and executive tasks, and the language system, typically recruited during linguistic processing. We examined MD and language system responses to code written in Python, a text-based programming language (Experiment 1) and in ScratchJr, a graphical programming language (Experiment 2); for both, we contrasted responses to code problems with responses to content-matched sentence problems. We found that the MD system exhibited strong bilateral responses to code in both experiments, whereas the language system responded strongly to sentence problems, but weakly or not at all to code problems. Thus, the MD system supports the use of novel cognitive tools even when the input is structurally similar to natural language.},
keywords = {fMRI, multiple demand, language, programming, computer code},
journal = {eLife},
issn = {2050-084X},
publisher = {eLife Sciences Publications, Ltd},
}

@inproceedings{kulal_spoc,
 author = {Kulal, Sumith and Pasupat, Panupong and Chandra, Kartik and Lee, Mina and Padon, Oded and Aiken, Alex and Liang, Percy S},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {{SPoC}: Search-based Pseudocode to Code},
 url = {https://proceedings.neurips.cc/paper/2019/file/7298332f04ac004a0ca44cc69ecf6f6b-Paper.pdf},
 volume = {32},
 year = {2019}
}



@article{alphago,
	Abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
	Author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	Da = {2017/10/01},
	Date-Added = {2021-05-07 14:21:11 +0000},
	Date-Modified = {2021-05-07 14:21:11 +0000},
	Doi = {10.1038/nature24270},
	Id = {Silver2017},
	Isbn = {1476-4687},
	Journal = {Nature},
	Number = {7676},
	Pages = {354--359},
	Title = {Mastering the game of Go without human knowledge},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/nature24270},
	Volume = {550},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature24270}}

@misc{karel_the_robot,
   author = "Chris Piech and Eric Roberts",
   title = "Karel the Robot Learns Python",
   year = "2020",
   howpublished = {\url{https://compedu.stanford.edu/karel-reader/docs/python/en/intro.html}},
   note = "[Online; accessed 07-Jun-2021]"
 }

@misc{gpt3kearsdemo,
   author = "Matt Shumer",
   title = "Generate Keras machine learning models with GPT-3",
   year = "2020",
   howpublished = {\url{https://gpt3demo.com/apps/text-to-keras}},
   note = "[Online; accessed 07-May-2021]"
 }

@misc{wikiConway,
   author = "Wikipedia",
   title = "{Conway's 99-graph problem} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2020",
   howpublished = {\url{http://en.wikipedia.org/w/index.php?title=Conway's\%2099-graph\%20problem&oldid=977185522}},
   note = "[Online; accessed 12-Nov-2020]"
 }
 
@misc{wikiBallot,
   author = "Wikipedia",
   title = "{Bertrand's ballot theorem} --- {W}ikipedia{,} The Free Encyclopedia",
   year = "2020",
   howpublished = {\url{http://en.wikipedia.org/w/index.php?title=Bertrand's\%20ballot\%20theorem&oldid=956464877}},
   note = "[Online; accessed 12-Nov-2020]"
 } 
 

@misc{wikiQuine,
   author = "Wikipedia",
   title = "{Quine (computing)}",
   year = "2021",
   howpublished = {\url{https://en.wikipedia.org/wiki/Quine_(computing)}},
   note = "[Online; accessed 10-May-2021]"
 }

@misc{wikiCompProg,
   author = "Wikipedia",
   title = "Competitive programming",
   year = "2021",
   howpublished = {\url{https://en.wikipedia.org/wiki/Competitive_programming}},
   note = "[Online; accessed 10-May-2021]"
 }

 

@article{bertrand1887,
  title={Solution d'un problème},
  author={Bertrand, Joseph},
  journal={Comptes Rendus de l'Académie des Sciences},
  volume={105},
  pages={369},
  year={1887}
}
 
 
@Inbook{ballot2008,
author="Addario-Berry, L.
and Reed, B. A.",
editor="Gy{\H{o}}ri, Ervin
and Katona, Gyula O. H.
and Lov{\'a}sz, L{\'a}szl{\'o}
and S{\'a}gi, G{\'a}bor",
title="Ballot Theorems, Old and New",
bookTitle="Horizons of Combinatorics",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="9--35",
abstract="We begin by sketching the development of the classical ballot theorem as it first appeared in the Comptes Rendus de 1'Academie des Sciences. The statement that is fairly called the first Ballot Theorem was due to Bertrand: Theorem 1 ([8]). We suppose that two candidates have been submitted to a vote in which the number of voters is $\mu$. Candidate A obtains n votes and is elected; candidate B obtains m = $\mu$ − n votes. We ask for the probability that during the counting of the votes, the number of votes for A is at all times greater than the number of votes for B. This probability is (2n − $\mu$)/$\mu$ = (n − m)/(n + m).",
isbn="978-3-540-77200-2",
doi="10.1007/978-3-540-77200-2_1",
url="https://doi.org/10.1007/978-3-540-77200-2_1"
}


@misc{chinchilla,
  doi = {10.48550/ARXIV.2203.15556},
  
  url = {https://arxiv.org/abs/2203.15556},
  
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Training Compute-Optimal Large Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{zeroShotChainOfThought,
  doi = {10.48550/ARXIV.2205.11916},
  
  url = {https://arxiv.org/abs/2205.11916},
  
  author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Large Language Models are Zero-Shot Reasoners},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{promptsurvey,
  doi = {10.48550/ARXIV.2107.13586},
  
  url = {https://arxiv.org/abs/2107.13586},
  
  author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@inproceedings{autoprompt,
    title = "{A}uto{P}rompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
    author = "Shin, Taylor  and
      Razeghi, Yasaman  and
      Logan IV, Robert L.  and
      Wallace, Eric  and
      Singh, Sameer",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.346",
    doi = "10.18653/v1/2020.emnlp-main.346",
    pages = "4222--4235",
}


@article{wiles1995modular,
  title={Modular elliptic curves and Fermat's last theorem},
  author={Wiles, Andrew},
  journal={Annals of mathematics},
  volume={141},
  number={3},
  pages={443--551},
  year={1995},
  publisher={JSTOR}
}

@inproceedings{reynolds2021prompt,
author = {Reynolds, Laria and McDonell, Kyle},
title = {Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm},
year = {2021},
isbn = {9781450380959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411763.3451760},
doi = {10.1145/3411763.3451760},
booktitle = {Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {314},
numpages = {7},
keywords = {semiotics, language models, serial reasoning, metaprompts, GPT-3, transformers, prompt programming, few-shot learning},
location = {Yokohama, Japan},
series = {CHI EA '21}
}

@article{tam2021improving,
    title={Improving and Simplifying Pattern Exploiting Training},
    author={Derek Tam and Rakesh R Menon and Mohit Bansal and Shashank Srivastava and Colin Raffel},
    year={2021},
    eprint={2103.11955},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@book{pythonOneLiners,
  title={Python One-Liners: Write Concise, Eloquent Python Like a Professional},
  author={Mayer, C.},
  isbn={9781718500501},
  lccn={2020001449},
  url={https://books.google.com/books?id=jVv6DwAAQBAJ},
  year={2020},
  publisher={No Starch Press, Incorporated}
}

@inproceedings{gao2021making,
   title={Making Pre-trained Language Models Better Few-shot Learners},
   author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
   booktitle={Association for Computational Linguistics (ACL)},
   year={2021}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  url={https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf}
}

@article{Conway99,
author={John Horton Conway},
year={2017}, 
title={Five \$1,000 problems (Update 2017)},
url={https://oeis.org/A248380/a248380.pdf},
note={[Online; accessed 12/15/2020]}
}

@article{collatzConjecture,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2322189},
 author = {Jeffrey C. Lagarias},
 journal = {The American Mathematical Monthly},
 number = {1},
 pages = {3--23},
 publisher = {Mathematical Association of America},
 title = {The 3x + 1 Problem and Its Generalizations},
 volume = {92},
 year = {1985}
}


@book{biggs1971finite,
  title={Finite Groups of Automorphisms: Course Given at the University of Southampton, October-December 1969},
  author={Biggs, N. and Biggs, P.M.L.S.E.N.L. and London Mathematical Society and London Mathematical Society lecture note series and Hitchin, S.P.G.N.J.},
  isbn={9780521082150},
  lccn={lc74154510},
  series={Lecture note series},
  url={https://books.google.com/books?id=flA4AAAAIAAJ},
  year={1971},
  publisher={Cambridge University Press}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@misc{ScottBuild2020,
        year = {2020},
        title = {The Future of Technology with {Microsoft}'s {CTO}},
        organization = {Build2020 video on Youtube},
        author = {Scott, Kevin},
        url = {https://youtu.be/eNhYTLWQFeg?t=30m10s},
        note = {See minute 30. [Online; accessed 15-May-2021]}
    }

@misc{Shameem2020,
        year = {2020},
        title = {Tweet},
        organization = {Twitter},
        author = {Shameem, Sharif},
        url = {https://twitter.com/sharifshameem/status/1282676454690451457},
        note = {[Online; accessed 15-May-2021]}
    }



@misc{swyx2020,
  author = {Wang, Shawn},
  title = {{GPT3}-list},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/sw-yx/gpt3-list#code}},
  commit = {5184e6c028c77ff2449dc096e7f89e0698050a3f},
  note = {[Online; accessed 15-May-2021]}
}

@article{LempelZiv1977,
  title={A universal algorithm for sequential data compression},
  author={J. Ziv and A. Lempel},
  journal={IEEE Trans. Inf. Theory},
  year={1977},
  volume={23},
  pages={337-343}
}

@inproceedings{dagiene2008bebras,
  title={Bebras international contest on informatics and computer literacy: Criteria for good tasks},
  author={Dagien{\.e}, Valentina and Futschek, Gerald},
  booktitle={International conference on informatics in secondary schools-evolution and perspectives},
  pages={19--30},
  year={2008},
  organization={Springer}
}

@inproceedings{Arora2009ComputationalCA,
  title={Computational Complexity: A Modern Approach},
  author={Sanjeev Arora and B. Barak},
  year={2009}
}


@book{theory_of_numbers_book,
	Abstract = {The sixth edition of the classic undergraduate text in elementary number theory includes a new chapter on elliptic curves and their role in the proof of Fermat's Last Theorem, a foreword by Andrew Wiles and extensively revised and updated end-of-chapter notes.},
	Address = {Oxford; New York},
	Author = {Hardy, G. H. and Wright, E. M. and Heath-Brown, D. R. and Silverman, Joseph H.},
	Date = {2008///},
	Date-Added = {2021-06-08 03:38:10 +0000},
	Date-Modified = {2021-06-08 03:38:10 +0000},
	Db = {/z-wcorg/},
	Dp = {http://worldcat.org},
	Id = {214305907},
	Isbn = {9780199219858 0199219850 9780199219865 0199219869},
	La = {English},
	Publisher = {Oxford University Press},
	Title = {An introduction to the theory of numbers},
	Ty = {BOOK},
	Year = {2008}}


@article{blum2003,
author = {Blum, Avrim and Kalai, Adam and Wasserman, Hal},
title = {Noise-Tolerant Learning, the Parity Problem, and the Statistical Query Model},
year = {2003},
issue_date = {July 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0004-5411},
url = {https://doi.org/10.1145/792538.792543},
doi = {10.1145/792538.792543},
abstract = {We describe a slightly subexponential time algorithm for learning parity functions in the presence of random classification noise, a problem closely related to several cryptographic and coding problems. Our algorithm runs in polynomial time for the case of parity functions that depend on only the first O(log n log log n) bits of input, which provides the first known instance of an efficient noise-tolerant algorithm for a concept class that is not learnable in the Statistical Query model of Kearns [1998]. Thus, we demonstrate that the set of problems learnable in the statistical query model is a strict subset of those problems learnable in the presence of noise in the PAC model.In coding-theory terms, what we give is a poly(n)-time algorithm for decoding linear k \texttimes{} n codes in the presence of random noise for the case of k = c log n log log n for some c > 0. (The case of k = O(log n) is trivial since one can just individually check each of the 2k possible messages and choose the one that yields the closest codeword.)A natural extension of the statistical query model is to allow queries about statistical properties that involve t-tuples of examples, as opposed to just single examples. The second result of this article is to show that any class of functions learnable (strongly or weakly) with t-wise queries for t = O(log n) is also weakly learnable with standard unary queries. Hence, this natural extension to the statistical query model does not increase the set of weakly learnable functions.},
journal = {J. ACM},
month = jul,
pages = {506–519},
numpages = {14},
keywords = {Computational learning theory, machine learning, statistical queries}
}

@misc{personalityLLM,
  doi = {10.48550/ARXIV.2204.12000},
  
  url = {https://arxiv.org/abs/2204.12000},
  
  author = {Karra, Saketh Reddy and Nguyen, Son and Tulabandhula, Theja},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {AI Personification: Estimating the Personality of Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{kiela-etal-2021-dynabench,
    title = "Dynabench: Rethinking Benchmarking in {NLP}",
    author = "Kiela, Douwe  and
      Bartolo, Max  and
      Nie, Yixin  and
      Kaushik, Divyansh  and
      Geiger, Atticus  and
      Wu, Zhengxuan  and
      Vidgen, Bertie  and
      Prasad, Grusha  and
      Singh, Amanpreet  and
      Ringshia, Pratik  and
      Ma, Zhiyi  and
      Thrush, Tristan  and
      Riedel, Sebastian  and
      Waseem, Zeerak  and
      Stenetorp, Pontus  and
      Jia, Robin  and
      Bansal, Mohit  and
      Potts, Christopher  and
      Williams, Adina",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.324",
    doi = "10.18653/v1/2021.naacl-main.324",
    pages = "4110--4124"
}

@article{Lourie2021UNICORNOR,
  title={UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark},
  author={Nicholas Lourie and Ronan {Le Bras} and Chandra Bhagavatula and Yejin Choi},
  journal={AAAI},
  year={2021}
}


@inproceedings{rajpurkar-etal-2016-squad,
    title = "{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text",
    author = "Rajpurkar, Pranav  and
      Zhang, Jian  and
      Lopyrev, Konstantin  and
      Liang, Percy",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1264",
    doi = "10.18653/v1/D16-1264",
    pages = "2383--2392",
}

@incollection{krawczyk2018,
title = {Chapter 12 - Social Cognition: Reasoning With Others},
editor = {Daniel C. Krawczyk},
booktitle = {Reasoning},
publisher = {Academic Press},
pages = {283-311},
year = {2018},
isbn = {978-0-12-809285-9},
doi = {https://doi.org/10.1016/B978-0-12-809285-9.00012-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128092859000120},
author = {Daniel C. Krawczyk}
}

@incollection{houser2014,
title = {Chapter 2 - Experimental Economics and Experimental Game Theory},
editor = {Paul W. Glimcher and Ernst Fehr},
booktitle = {Neuroeconomics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {19-34},
year = {2014},
isbn = {978-0-12-416008-8},
doi = {https://doi.org/10.1016/B978-0-12-416008-8.00002-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160088000024},
author = {Daniel Houser and Kevin McCabe}
}


@inproceedings{sap-etal-2019-social,
    title = "Social {IQ}a: Commonsense Reasoning about Social Interactions",
    author = "Sap, Maarten  and
      Rashkin, Hannah  and
      Chen, Derek  and
      Le Bras, Ronan  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1454",
    doi = "10.18653/v1/D19-1454",
    pages = "4463--4473"
}

@inproceedings{zellers-etal-2018-swag,
    title = "{SWAG}: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference",
    author = "Zellers, Rowan  and
      Bisk, Yonatan  and
      Schwartz, Roy  and
      Choi, Yejin",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1009",
    doi = "10.18653/v1/D18-1009",
    pages = "93--104"
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355"
}

@article{instructOpenAI,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{fantasticPrompts,
  author    = {Yao Lu and
               Max Bartolo and
               Alastair Moore and
               Sebastian Riedel and
               Pontus Stenetorp},
  title     = {Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot
               Prompt Order Sensitivity},
  journal   = {CoRR},
  volume    = {abs/2104.08786},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.08786},
  eprinttype = {arXiv},
  eprint    = {2104.08786},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08786.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ultimatumExp,
title = {An experimental analysis of ultimatum bargaining},
journal = {Journal of Economic Behavior \& Organization},
volume = {3},
number = {4},
pages = {367-388},
year = {1982},
issn = {0167-2681},
doi = {https://doi.org/10.1016/0167-2681(82)90011-7},
url = {https://www.sciencedirect.com/science/article/pii/0167268182900117},
author = {Werner Güth and Rolf Schmittberger and Bernd Schwarze},
abstract = {There are many experimental studies of bargaining behavior, but suprisingly enough nearly no attempt has been made to investigate the so-called ultimatum bargaining behavior experimentally. The special property of ultimatum bargaining games is that on every stage of the bargaining process only one player has to decide and that before the last stage the set of outcomes is already restricted to only two results. To make the ultimatum aspect obvious we concentrated on situations with two players and two stages. In the ‘easy games’ a given amount c has to be distributed among the two players, whereas in the ‘complicated games’ the players have to allocate a bundle of black and white chips with different values for both players. We performed two main experiments for easy games as well as for complicated games. By a special experiment it was investigated how the demands of subjects as player 1 are related to their acceptance decisions as player 2.}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}
@inbook{crain85,
title = "On not being led up the garden path: the use of context by the psychological syntax processor",
author = "Stephen Crain and Mark Steedman",
note = "Cambridge Books Online",
year = "1985",
language = "English",
isbn = "9780521262033",
pages = "320--358",
booktitle = "Natural language parsing",
publisher = "Cambridge University Press",
address = "United States",
}
@inproceedings{petroni-etal-2019-language,
    title = "Language Models as Knowledge Bases?",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1250",
    doi = "10.18653/v1/D19-1250",
    pages = "2463--2473"
}

@inproceedings{schuster-etal-2021-get,
    title = "Get Your Vitamin {C}! Robust Fact Verification with Contrastive Evidence",
    author = "Schuster, Tal  and
      Fisch, Adam  and
      Barzilay, Regina",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.52",
    doi = "10.18653/v1/2021.naacl-main.52",
    pages = "624--643",
    abstract = "Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness{---}improving accuracy by 10{\%} on adversarial fact verification and 6{\%} on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation.",
}

@inproceedings{zellers2019vcr,
    author = {Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    title = {From Recognition to Cognition: Visual Commonsense Reasoning},
    booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2019}
}

@article{ILSVRC15,
Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
Title = {{ImageNet Large Scale Visual Recognition Challenge}},
Year = {2015},
journal   = {International Journal of Computer Vision (IJCV)},
doi = {10.1007/s11263-015-0816-y},
volume={115},
number={3},
pages={211-252}
}

@inproceedings{talmor-etal-2019-commonsenseqa,
    title = "{C}ommonsense{QA}: A Question Answering Challenge Targeting Commonsense Knowledge",
    author = "Talmor, Alon  and
      Herzig, Jonathan  and
      Lourie, Nicholas  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1421",
    doi = "10.18653/v1/N19-1421",
    pages = "4149--4158",
    abstract = "When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56{\%} accuracy, well below human performance, which is 89{\%}.",
}

@article{andreas-etal-2020-task,
    title = "Task-Oriented Dialogue as Dataflow Synthesis",
    author = "Andreas, Jacob  and
      Bufe, John  and
      Burkett, David  and
      Chen, Charles  and
      Clausman, Josh  and
      Crawford, Jean  and
      Crim, Kate  and
      DeLoach, Jordan  and
      Dorner, Leah  and
      Eisner, Jason  and
      Fang, Hao  and
      Guo, Alan  and
      Hall, David  and
      Hayes, Kristin  and
      Hill, Kellie  and
      Ho, Diana  and
      Iwaszuk, Wendy  and
      Jha, Smriti  and
      Klein, Dan  and
      Krishnamurthy, Jayant  and
      Lanman, Theo  and
      Liang, Percy  and
      Lin, Christopher H.  and
      Lintsbakh, Ilya  and
      McGovern, Andy  and
      Nisnevich, Aleksandr  and
      Pauls, Adam  and
      Petters, Dmitrij  and
      Read, Brent  and
      Roth, Dan  and
      Roy, Subhro  and
      Rusak, Jesse  and
      Short, Beth  and
      Slomin, Div  and
      Snyder, Ben  and
      Striplin, Stephon  and
      Su, Yu  and
      Tellman, Zachary  and
      Thomson, Sam  and
      Vorobev, Andrei  and
      Witoszko, Izabela  and
      Wolfe, Jason  and
      Wray, Abby  and
      Zhang, Yuchen  and
      Zotov, Alexander",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://aclanthology.org/2020.tacl-1.36",
    doi = "10.1162/tacl_a_00333",
    pages = "556--571",
    abstract = "We describe an approach to task-oriented dialogue in which dialogue state is represented as a dataflow graph. A dialogue agent maps each user utterance to a program that extends this graph. Programs include metacomputation operators for reference and revision that reuse dataflow fragments from previous turns. Our graph-based state enables the expression and manipulation of complex user intents, and explicit metacomputation makes these intents easier for learned models to predict. We introduce a new dataset, SMCalFlow, featuring complex dialogues about events, weather, places, and people. Experiments show that dataflow graphs and metacomputation substantially improve representability and predictability in these natural dialogues. Additional experiments on the MultiWOZ dataset show that our dataflow representation enables an otherwise off-the-shelf sequence-to-sequence model to match the best existing task-specific state tracking model. The SMCalFlow dataset, code for replicating experiments, and a public leaderboard are available at https://www.microsoft.com/en-us/research/project/dataflow-based-dialogue-semantic-machines.",
}

@ARTICLE{berlekamp1978,
  author={Berlekamp, E. and McEliece, R. and van Tilborg, H.},
  journal={IEEE Transactions on Information Theory}, 
  title={On the inherent intractability of certain coding problems (Corresp.)}, 
  year={1978},
  volume={24},
  number={3},
  pages={384-386},
  doi={10.1109/TIT.1978.1055873}}
  
@Inbook{Kaliski2005,
author="Kaliski, Burt",
title="{RSA} factoring challenge",
bookTitle="Encyclopedia of Cryptography and Security",
year="2005",
publisher="Springer US",
address="Boston, MA",
pages="531--532",
isbn="978-0-387-23483-0",
doi="10.1007/0-387-23483-7_362",
url="https://doi.org/10.1007/0-387-23483-7_362"
}

@article{tesauro1995temporal,
  title={Temporal difference learning and {TD-Gammon}},
  author={Tesauro, Gerald},
  journal={Communications of the ACM},
  volume={38},
  number={3},
  pages={58--68},
  year={1995}
}
@article{dantzig1951proof,
  title={A proof of the equivalence of the programming problem and the game problem},
  author={Dantzig, George B},
  journal={Activity analysis of production and allocation},
  volume={13},
  pages={330--338},
  year={1951}
}

@inproceedings{helmuth2015general,
  title={General program synthesis benchmark suite},
  author={Helmuth, Thomas and Spector, Lee},
  booktitle={Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
  pages={1039--1046},
  year={2015}
}

@inproceedings{polozov2015flashmeta,
  title={{FlashMeta}: A framework for inductive program synthesis},
  author={Polozov, Oleksandr and Gulwani, Sumit},
  booktitle={Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
  pages={107--126},
  year={2015}
}

@misc{langston_2021,
  title={From conversation to code: {Microsoft} introduces its first product features powered by {GPT-3}},
  url={https://blogs.microsoft.com/ai/from-conversation-to-code-microsoft-introduces-its-first-product-features-powered-by-gpt-3/},
  journal={Microsoft - The AI Blog},
  author={Langston, Jennifer},
  year={2021},
  month={May}
} 

@book{laaksonen2020guide,
  title={Guide to Competitive Programming: Learning and Improving Algorithms Through Contests},
  author={Laaksonen, A.},
  isbn={9783030393571},
  series={Undergraduate Topics in Computer Science},
  url={https://books.google.com/books?id=3JbiDwAAQBAJ},
  year={2020},
  publisher={Springer International Publishing}
}


@Article{eckel01,
  author={Eckel, Catherine C and Grossman, Philip J},
  title={{Chivalry and Solidarity in Ultimatum Games}},
  journal={Economic Inquiry},
  year=2001,
  volume={39},
  number={2},
  pages={171-188},
  month={April},
  keywords={},
  doi={},
  abstract={ We report the results of ultimatum game experiments designed to test for differences in the behavior of women and men. Women's proposals are on average more generous than men's, regardless of the sex of the partner, and women respondents are more likely to accept an offer of a given amount. A given offer is more likely to be accepted if it comes from a woman; we term this result chivalry. Women paired with women almost never fail to reach an agreement; we term this result solidarity. Age, earnings, and race also significantly affect proposals and the rates of rejection. Copyright 2001 by Oxford University Press.},
  url={https://ideas.repec.org/a/oup/ecinqu/v39y2001i2p171-88.html}
}


@article{treebank,
  added-at = {2017-12-13T19:08:11.000+0100},
  author = {Marcus, Mitchell P. and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  biburl = {https://www.bibsonomy.org/bibtex/2cddb0eaf7c9424de4c56b47316292ccb/slicside},
  description = {Nur für Referenzzwecke verwendet (Penn Tree Bank)},
  interhash = {e15687d066c2619723bd00c8d536efdb},
  intrahash = {cddb0eaf7c9424de4c56b47316292ccb},
  journal = {Computational Linguistics},
  keywords = {dropout final uw_ws17_ml},
  month = {June},
  number = 2,
  pages = {313--330},
  timestamp = {2017-12-14T18:03:30.000+0100},
  title = {Building a Large Annotated Corpus of {E}nglish: {T}he {P}enn
  Treebank},
  volume = 19,
  year = 1993
}


@misc{helmLiang2022,
  doi = {10.48550/ARXIV.2211.09110},
  
  url = {https://arxiv.org/abs/2211.09110},
  
  author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and Ré, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
  
  title = {Holistic Evaluation of Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
}

@inproceedings{gender2016,
 title = {Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings},
 author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 year = {2016}
}

@article {Boltonh4978,
	author = {Bolton, James M and Gunnell, David and Turecki, Gustavo},
	title = {Suicide risk assessment and intervention in people with mental illness},
	volume = {351},
	elocation-id = {h4978},
	year = {2015},
	doi = {10.1136/bmj.h4978},
	publisher = {BMJ Publishing Group Ltd},
	abstract = {Suicide is the 15th most common cause of death worldwide. Although relatively uncommon in the general population, suicide rates are much higher in people with mental health problems. Clinicians often have to assess and manage suicide risk. Risk assessment is challenging for several reasons, not least because conventional approaches to risk assessment rely on patient self reporting and suicidal patients may wish to conceal their plans. Accurate methods of predicting suicide therefore remain elusive and are actively being studied. Novel approaches to risk assessment have shown promise, including empirically derived tools and implicit association tests. Service provision for suicidal patients is often substandard, particularly at times of highest need, such as after discharge from hospital or the emergency department. Although several drug based and psychotherapy based treatments exist, the best approaches to reducing the risk of suicide are still unclear. Some of the most compelling evidence supports long established treatments such as lithium and cognitive behavioral therapy. Emerging options include ketamine and internet based psychotherapies. This review summarizes the current science in suicide risk assessment and provides an overview of the interventions shown to reduce the risk of suicide, with a focus on the clinical management of people with mental disorders.},
	URL = {https://www.bmj.com/content/351/bmj.h4978},
	eprint = {https://www.bmj.com/content/351/bmj.h4978.full.pdf},
	journal = {BMJ}
}

@article{warwick2016can,
  title={Can machines think? A report on Turing test experiments at the Royal Society},
  author={Warwick, Kevin and Shah, Huma},
  journal={Journal of experimental \& Theoretical artificial Intelligence},
  volume={28},
  number={6},
  pages={989--1007},
  year={2016},
  publisher={Taylor \& Francis}
}

@techreport{korinek2023language,
  title={Language models and cognitive automation for economic research},
  author={Korinek, Anton},
  year={2023},
  institution={National Bureau of Economic Research}
}

@article{dasgupta2022language,
  title={Language models show human-like content effects on reasoning},
  author={Dasgupta, Ishita and Lampinen, Andrew K and Chan, Stephanie CY and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L and Hill, Felix},
  journal={arXiv preprint arXiv:2207.07051},
  year={2022}
}

@article{hagendorff2022machine,
  title={Machine intuition: Uncovering human-like intuitive decision-making in GPT-3.5},
  author={Hagendorff, Thilo and Fabi, Sarah and Kosinski, Michal},
  journal={arXiv preprint arXiv:2212.05206},
  year={2022}
}

@inproceedings{park2022social,
  title={Social Simulacra: Creating Populated Prototypes for Social Computing Systems},
  author={Park, Joon Sung and Popowski, Lindsay and Cai, Carrie and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S},
  booktitle={Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology},
  pages={1--18},
  year={2022}
}

@article{caron2022identifying,
  title={Identifying and Manipulating the Personality Traits of Language Models},
  author={Caron, Graham and Srivastava, Shashank},
  journal={arXiv preprint arXiv:2212.10276},
  year={2022}
}

@article{jiang2022mpi,
  title={MPI: Evaluating and Inducing Personality in Pre-trained Language Models},
  author={Jiang, Guangyuan and Xu, Manjie and Zhu, Song-Chun and Han, Wenjuan and Zhang, Chi and Zhu, Yixin},
  journal={arXiv preprint arXiv:2206.07550},
  year={2022}
}

@article{karra2022ai,
  title={AI Personification: Estimating the Personality of Language Models},
  author={Karra, Saketh Reddy and Nguyen, Son and Tulabandhula, Theja},
  journal={arXiv preprint arXiv:2204.12000},
  year={2022}
}

@article{argyle2023out, 
    title={Out of One, Many: Using Language Models to Simulate Human Samples}, 
    DOI={10.1017/pan.2023.2}, journal={Political Analysis},
    publisher={Cambridge University Press}, 
    author={Argyle, Lisa P. and Busby, Ethan C. and Fulda, Nancy and Gubler, Joshua R. and Rytting, Christopher and Wingate, David}, 
    year={2023}, 
    pages={1–15}
}

@techreport{NBERw31122,
 title = "Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?",
 author = "Horton, John J",
 institution = "National Bureau of Economic Research",
 type = "Working Paper",
 series = "Working Paper Series",
 number = "31122",
 year = "2023",
 month = "April",
 doi = {10.3386/w31122},
 URL = "http://www.nber.org/papers/w31122",
 abstract = {Newly-developed large language models (LLM)—because of how they are trained and designed—are implicit computational models of humans—a homo silicus. LLMs can be used like economists use homo economicus: they can be given endowments, information, preferences, and so on, and then their behavior can be explored in scenarios via simulation. Experiments using this approach, derived from Charness and Rabin (2002), Kahneman, Knetsch and Thaler (1986), and Samuelson and Zeckhauser (1988) show qualitatively similar results to the original, but it is also easy to try variations for fresh insights. LLMs could allow researchers to pilot studies via simulation first, searching for novel social science insights to test in the real world.},
}

@article{ullman2023large,
  title={Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks},
  author={Ullman, Tomer},
  journal={arXiv preprint arXiv:2302.08399},
  year={2023}
}

@article{kosinski2023theory,
  title={Theory of mind may have spontaneously emerged in large language models},
  author={Kosinski, Michal},
  journal={arXiv preprint arXiv:2302.02083},
  year={2023}
}

@article{bakker2022fine,
  title={Fine-tuning language models to find agreement among humans with diverse preferences},
  author={Bakker, Michiel and Chadwick, Martin and Sheahan, Hannah and Tessler, Michael and Campbell-Gillingham, Lucy and Balaguer, Jan and McAleese, Nat and Glaese, Amelia and Aslanides, John and Botvinick, Matt and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38176--38189},
  year={2022}
}
