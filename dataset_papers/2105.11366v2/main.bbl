\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and Bottou]{arjovsky17}
Arjovsky, M., Chintala, S., and Bottou, L.
\newblock Wasserstein gan.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, 2017.

\bibitem[Barth-Maron et~al.(2018)Barth-Maron, Hoffman, Budden, Dabney, Horgan,
  TB, Muldal, Heess, and Lillicrap]{barth-maron18}
Barth-Maron, G., Hoffman, M.~W., Budden, D., Dabney, W., Horgan, D., TB, D.,
  Muldal, A., Heess, N., and Lillicrap, T.
\newblock Distributed distributional deterministic policy gradients.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[{Bellemare} et~al.(2013){Bellemare}, {Naddaf}, {Veness}, and
  {Bowling}]{ale}
{Bellemare}, M.~G., {Naddaf}, Y., {Veness}, J., and {Bowling}, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bellemare et~al.(2017{\natexlab{a}})Bellemare, Dabney, and
  Munos]{bellemare17}
Bellemare, M.~G., Dabney, W., and Munos, R.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, 2017{\natexlab{a}}.

\bibitem[Bellemare et~al.(2017{\natexlab{b}})Bellemare, Danihelka, Dabney,
  Mohamed, Lakshminarayanan, Hoyer, and Munos]{bellemare17cramer}
Bellemare, M.~G., Danihelka, I., Dabney, W., Mohamed, S., Lakshminarayanan, B.,
  Hoyer, S., and Munos, R.
\newblock The cramer distance as a solution to biased wasserstein gradients.
\newblock \emph{arXiv preprint arXiv:1705.10743}, 2017{\natexlab{b}}.

\bibitem[Bellemare et~al.(2019)Bellemare, Roux, Castro, and
  Moitra]{bellemare19}
Bellemare, M.~G., Roux, N.~L., Castro, P.~S., and Moitra, S.
\newblock Distributional reinforcement learning with linear function
  approximation.
\newblock In \emph{Artificial Intelligence and Statistics}, volume~89 of
  \emph{Proceedings of Machine Learning Research}, pp.\  2203--2211, 2019.

\bibitem[Bellman(1957)]{bellman58}
Bellman, R.
\newblock \emph{{Dynamic Programming}}.
\newblock Dover Publications, 1957.

\bibitem[Bertsekas \& Tsitsiklis(1996)Bertsekas and Tsitsiklis]{bertsekas96}
Bertsekas, D.~P. and Tsitsiklis, J.~N.
\newblock \emph{Neuro-Dynamic Programming}.
\newblock Athena Scientific, 1st edition, 1996.

\bibitem[Burda et~al.(2019)Burda, Edwards, Storkey, and Klimov]{burda19}
Burda, Y., Edwards, H., Storkey, A.~J., and Klimov, O.
\newblock Exploration by random network distillation.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.

\bibitem[{Choi} et~al.(2019){Choi}, {Lee}, and {Oh}]{choi19}
{Choi}, Y., {Lee}, K., and {Oh}, S.
\newblock Distributional deep reinforcement learning with a mixture of
  gaussians.
\newblock In \emph{2019 International Conference on Robotics and Automation
  (ICRA)}, pp.\  9791--9797, 2019.

\bibitem[Cichosz(1995)]{cichosz95}
Cichosz, P.
\newblock Truncating temporal differences: On the efficient implementation of
  {TD($\lambda$)} for reinforcement learning.
\newblock \emph{Journal on Artificial Intelligence}, 2:\penalty0 287--318,
  1995.

\bibitem[Coumans \& Bai(2016--2020)Coumans and Bai]{pybullet}
Coumans, E. and Bai, Y.
\newblock Pybullet, a python module for physics simulation for games, robotics
  and machine learning.
\newblock \url{http://pybullet.org}, 2016--2020.

\bibitem[Dabney et~al.(2018{\natexlab{a}})Dabney, Ostrovski, Silver, and
  Munos]{dabney18}
Dabney, W., Ostrovski, G., Silver, D., and Munos, R.
\newblock Implicit quantile networks for distributional reinforcement learning.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, 2018{\natexlab{a}}.

\bibitem[Dabney et~al.(2018{\natexlab{b}})Dabney, Rowland, Bellemare, and
  Munos]{dabney17}
Dabney, W., Rowland, M., Bellemare, M.~G., and Munos, R.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{AAAI}, pp.\  2892--2901, 2018{\natexlab{b}}.

\bibitem[Dearden et~al.(1998)Dearden, Friedman, and Russell]{dearden98}
Dearden, R., Friedman, N., and Russell, S.
\newblock Bayesian q-learning.
\newblock \emph{Proceedings of the fifteenth national/tenth conference on
  Artificial intelligence/Innovative applications of artificial intelligence},
  pp.\  761 -- 768, 1998.

\bibitem[Duan et~al.(2020)Duan, Guan, Li, Ren, and Cheng]{duan20}
Duan, J., Guan, Y., Li, S.~E., Ren, Y., and Cheng, B.
\newblock Distributional soft actor-critic: Off-policy reinforcement learning
  for addressing value estimation errors.
\newblock \emph{arXiv preprint arXiv:2001.02811}, 2020.

\bibitem[Gruslys et~al.(2017)Gruslys, Dabney, Azar, Piot, Bellemare, and
  Munos]{gruslys17}
Gruslys, A., Dabney, W., Azar, M.~G., Piot, B., Bellemare, M., and Munos, R.
\newblock The reactor: A fast and sample-efficient actor-critic agent for
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representation (ICLR)},
  2017.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{Hessel2018RainbowCI}
Hessel, M., Modayil, J., Hasselt, H.~V., Schaul, T., Ostrovski, G., Dabney, W.,
  Horgan, D., Piot, B., Azar, M.~G., and Silver, D.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{AAAI}, 2018.

\bibitem[Huber(1964)]{huber1964}
Huber, P.~J.
\newblock Robust estimation of a location parameter.
\newblock \emph{The Annals of Mathematical Statistics}, 35\penalty0
  (1):\penalty0 73--101, 1964.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade02}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{Proceedings of the 19th International Conference on Machine
  Learning (ICML)}, 2002.

\bibitem[Kuznetsov et~al.(2020)Kuznetsov, Shvechikov, Grishin, and
  Vetrov]{kuznetsov20}
Kuznetsov, A., Shvechikov, P., Grishin, A., and Vetrov, D.~P.
\newblock Controlling overestimation bias with truncated mixture of continuous
  distributional quantile critics.
\newblock \emph{arXiv preprint arXiv:2005.04269}, 2020.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine16}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{Journal of Machine Learning Research}, 17:\penalty0
  39:1--39:40, 2016.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap16}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{Proceedings of the 33rd International Conference on Learning
  Representations (ICML)}, 2016.

\bibitem[Ma et~al.(2020)Ma, Zhang, Xia, Zhou, Yang, and Zhao]{ma20}
Ma, X., Zhang, Q., Xia, L., Zhou, Z., Yang, J., and Zhao, Q.
\newblock Distributional soft actor critic for risk sensitive learning.
\newblock \emph{arXiv preprint arXiv:2004.14547}, 2020.

\bibitem[Mavrin et~al.(2019)Mavrin, Yao, Kong, Wu, and Yu]{mavrin19a}
Mavrin, B., Yao, H., Kong, L., Wu, K., and Yu, Y.
\newblock Distributional reinforcement learning for efficient exploration.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, 2019.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{mnih15}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Morimura et~al.(2010{\natexlab{a}})Morimura, Sugiyama, Kashima,
  Hachiya, and Tanaka]{morimura10a}
Morimura, T., Sugiyama, M., Kashima, H., Hachiya, H., and Tanaka, T.
\newblock Parametric return density estimation for reinforcement learning.
\newblock In \emph{UAI}, pp.\  368--375. AUAI Press, 2010{\natexlab{a}}.

\bibitem[Morimura et~al.(2010{\natexlab{b}})Morimura, Sugiyama, Kashima,
  Hachiya, and Tanaka]{morimura10b}
Morimura, T., Sugiyama, M., Kashima, H., Hachiya, H., and Tanaka, T.
\newblock Nonparametric return distribution approximation for reinforcement
  learning.
\newblock In \emph{Proceedings of the 27th International Conference on Machine
  Learning (ICML)}, 2010{\natexlab{b}}.

\bibitem[Nikolov et~al.(2019)Nikolov, Kirschner, Berkenkamp, and
  Krause]{nikolov18}
Nikolov, N., Kirschner, J., Berkenkamp, F., and Krause, A.
\newblock Information-directed exploration for deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Puterman(1994)]{puterman94}
Puterman, M.~L.
\newblock \emph{Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc., 1st edition, 1994.

\bibitem[Qu et~al.(2019)Qu, Mannor, and Xu]{qu19}
Qu, C., Mannor, S., and Xu, H.
\newblock Nonlinear distributional gradient temporal-difference learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\
   5251--5260. PMLR, 09--15 Jun 2019.

\bibitem[Rowland et~al.(2018)Rowland, Bellemare, Dabney, Munos, and
  Teh]{rowland18}
Rowland, M., Bellemare, M.~G., Dabney, W., Munos, R., and Teh, Y.~W.
\newblock An analysis of categorical distributional reinforcement learning.
\newblock In Storkey, A.~J. and Pérez-Cruz, F. (eds.), \emph{Artificial
  Intelligence and Statistics (AISTATS)}, volume~84 of \emph{Proceedings of
  Machine Learning Research}, pp.\  29--37. PMLR, 2018.

\bibitem[Rowland et~al.(2019)Rowland, Dadashi, Kumar, Munos, Bellemare, and
  Dabney]{rowland19}
Rowland, M., Dadashi, R., Kumar, S., Munos, R., Bellemare, M.~G., and Dabney,
  W.
\newblock Statistics and samples in distributional reinforcement learning.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman15}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning (ICML)}, 2015.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{schulman16}
Schulman, J., Moritz, P., Levine, S., Jordan, M.~I., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman17}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver16}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Singh et~al.(2020)Singh, Lee, and Chen]{singh20}
Singh, R., Lee, K., and Chen, Y.
\newblock Sample-based distributional policy gradient.
\newblock \emph{arXiv preprint arXiv:2001.02652}, 2020.

\bibitem[Sutton(1988)]{sutton88}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine Learning}, 3\penalty0 (1):\penalty0 9–44, aug 1988.
\newblock ISSN 0885-6125.
\newblock \doi{10.1023/A:1022633531479}.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton98}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Introduction to Reinforcement Learning}.
\newblock MIT Press, Cambridge, MA, USA, 1st edition, 1998.
\newblock ISBN 0262193981.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and Mansour]{sutton99}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Proceedings of the 12th International Conference on Neural
  Information Processing Systems (NeurIPS)}, Cambridge, MA, USA, 1999.

\bibitem[Székely(2002)]{szekely}
Székely, G.
\newblock E-statistics: The energy of statistical samples.
\newblock 10 2002.
\newblock \doi{10.13140/RG.2.1.5063.9761}.

\bibitem[van Seijen et~al.(2011)van Seijen, Whiteson, van Hasselt, and
  Wiering]{seijen11}
van Seijen, H., Whiteson, S., van Hasselt, H., and Wiering, M.
\newblock Exploiting best-match equations for efficient reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2045--2094,
  2011.

\bibitem[Watkins(1989)]{watkins89}
Watkins, C. J. C.~H.
\newblock \emph{Learning from Delayed Rewards}.
\newblock PhD thesis, King's College, Oxford, 1989.

\bibitem[Williams(1988)]{williams88}
Williams, R.~J.
\newblock Toward a theory of reinforcement-learning connectionist systems.
\newblock Technical Report NU-CCS-88-3, College of Comp. Sci., Northeastern
  University, Boston, MA, 1988.

\bibitem[Williams(1992)]{williams92}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine Learning}, 8:\penalty0 229--256, 1992.

\bibitem[Yang et~al.(2019)Yang, Zhao, Lin, Qin, Bian, and Liu]{yang19}
Yang, D., Zhao, L., Lin, Z., Qin, T., Bian, J., and Liu, T.-Y.
\newblock Fully parameterized quantile function for distributional
  reinforcement learning.
\newblock In \emph{33rd Annual Conference on Neural Information Processing
  Systems (NeurIPS)}, pp.\  6190--6199, 2019.

\end{thebibliography}
