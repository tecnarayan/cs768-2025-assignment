\begin{thebibliography}{10}

\bibitem{agostinelli2021transferability}
Andrea Agostinelli, Jasper Uijlings, Thomas Mensink, and Vittorio Ferrari.
\newblock Transferability metrics for selecting source model ensembles.
\newblock {\em arXiv preprint arXiv:2111.13011}, 2021.

\bibitem{bansal2021revisiting}
Yamini Bansal, Preetum Nakkiran, and Boaz Barak.
\newblock Revisiting model stitching to compare neural representations.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{bao2019information}
Yajie Bao, Yang Li, Shao-Lun Huang, Lin Zhang, Lizhong Zheng, Amir Zamir, and
  Leonidas Guibas.
\newblock An information-theoretic approach to transferability in task transfer
  learning.
\newblock In {\em 2019 IEEE International Conference on Image Processing
  (ICIP)}, pages 2309--2313. IEEE, 2019.

\bibitem{bolya2021scalable}
Daniel Bolya, Rohit Mittapalli, and Judy Hoffman.
\newblock Scalable diverse model selection for accessible transfer learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{buldygin2000metric}
Valeri\u\i~Vladimirovich Buldygin and IU~V Kozachenko.
\newblock {\em Metric characterization of random variables and random
  processes}, volume 188.
\newblock American Mathematical Soc., 2000.

\bibitem{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem{chen2020tenas}
Wuyang Chen, Xinyu Gong, and Zhangyang Wang.
\newblock Neural architecture search on imagenet in four gpu hours: A
  theoretically inspired perspective.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{chen2020improved}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock {\em arXiv preprint arXiv:2003.04297}, 2020.

\bibitem{chen2021empirical}
Xinlei Chen, Saining Xie, and Kaiming He.
\newblock An empirical study of training self-supervised vision transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9640--9649, 2021.

\bibitem{elements1992}
B.~Choudhary.
\newblock {\em The Elements of Complex Analysis}.
\newblock New Age International Publishers, 1992.

\bibitem{cimpoi14describing}
M.~Cimpoi, S.~Maji, I.~Kokkinos, S.~Mohamed, , and A.~Vedaldi.
\newblock Describing textures in the wild.
\newblock In {\em Proceedings of the {IEEE} Conf. on Computer Vision and
  Pattern Recognition ({CVPR})}, 2014.

\bibitem{Cohen2022xrv}
Joseph~Paul Cohen, Joseph~D. Viviano, Paul Bertin, Paul Morrison, Parsa
  Torabian, Matteo Guarrera, Matthew~P Lungren, Akshay Chaudhari, Rupert
  Brooks, Mohammad Hashir, and Hadrien Bertrand.
\newblock {TorchXRayVision: A library of chest X-ray datasets and models}.
\newblock In {\em Medical Imaging with Deep Learning}, 2022.

\bibitem{csiszarik2021similarity}
Adri{\'a}n Csisz{\'a}rik, P{\'e}ter K{\H{o}}r{\"o}si-Szab{\'o}, {\'A}kos
  Matszangosz, Gergely Papp, and D{\'a}niel Varga.
\newblock Similarity and matching of neural network representations.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 702--703, 2020.

\bibitem{dai2011greedy}
Dong Dai and Tong Zhang.
\newblock Greedy model averaging.
\newblock {\em Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem{dietterich2000ensemble}
Thomas~G Dietterich.
\newblock Ensemble methods in machine learning.
\newblock In {\em International workshop on multiple classifier systems}, pages
  1--15. Springer, 2000.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{feder1999complexity}
Tomas Feder, Pavol Hell, Sulamita Klein, and Rajeev Motwani.
\newblock Complexity of graph partition problems.
\newblock In {\em Proceedings of the thirty-first annual ACM symposium on
  Theory of computing}, pages 464--472, 1999.

\bibitem{fei2004learning}
Li~Fei-Fei, Rob Fergus, and Pietro Perona.
\newblock Learning generative visual models from few training examples: An
  incremental bayesian approach tested on 101 object categories.
\newblock In {\em 2004 conference on computer vision and pattern recognition
  workshop}, pages 178--178. IEEE, 2004.

\bibitem{fiduccia1982linear}
Charles~M Fiduccia and Robert~M Mattheyses.
\newblock A linear-time heuristic for improving network partitions.
\newblock In {\em 19th design automation conference}, pages 175--181. IEEE,
  1982.

\bibitem{grill2020bootstrap}
Jean-Bastien Grill, Florian Strub, Florent Altch{\'e}, Corentin Tallec, Pierre
  Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila~Pires, Zhaohan
  Guo, Mohammad Gheshlaghi~Azar, et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:21271--21284, 2020.

\bibitem{gross2018graph}
Jonathan~L Gross, Jay Yellen, and Mark Anderson.
\newblock {\em Graph theory and its applications}.
\newblock Chapman and Hall/CRC, 2018.

\bibitem{hanin2019complexity}
Boris Hanin and David Rolnick.
\newblock Complexity of linear regions in deep networks.
\newblock In {\em International Conference on Machine Learning}, pages
  2596--2604. PMLR, 2019.

\bibitem{hardoon2004canonical}
David~R Hardoon, Sandor Szedmak, and John Shawe-Taylor.
\newblock Canonical correlation analysis: An overview with application to
  learning methods.
\newblock {\em Neural computation}, 16(12):2639--2664, 2004.

\bibitem{hartmanis1982computers}
Juris Hartmanis.
\newblock Computers and intractability: a guide to the theory of
  np-completeness (michael r. garey and david s. johnson).
\newblock {\em Siam Review}, 24(1):90, 1982.

\bibitem{MaskedAutoencoders2021}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock {\em arXiv:2111.06377}, 2021.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2(7), 2015.

\bibitem{hochba1997approximation}
Dorit~S Hochba.
\newblock Approximation algorithms for np-hard problems.
\newblock {\em ACM Sigact News}, 28(2):40--52, 1997.

\bibitem{hou2021coordinate}
Qibin Hou, Daquan Zhou, and Jiashi Feng.
\newblock Coordinate attention for efficient mobile network design.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 13713--13722, 2021.

\bibitem{howard2019searching}
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo~Chen, Mingxing
  Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et~al.
\newblock Searching for mobilenetv3.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1314--1324, 2019.

\bibitem{howard2018universal}
Jeremy Howard and Sebastian Ruder.
\newblock Universal language model fine-tuning for text classification.
\newblock {\em arXiv preprint arXiv:1801.06146}, 2018.

\bibitem{karypis2000multilevel}
George Karypis and Vipin Kumar.
\newblock Multilevel k-way hypergraph partitioning.
\newblock {\em VLSI design}, 11(3):285--300, 2000.

\bibitem{kendall1938new}
Maurice~G Kendall.
\newblock A new measure of rank correlation.
\newblock {\em Biometrika}, 30(1/2):81--93, 1938.

\bibitem{kernighan1970efficient}
Brian~W Kernighan and Shen Lin.
\newblock An efficient heuristic procedure for partitioning graphs.
\newblock {\em The Bell system technical journal}, 49(2):291--307, 1970.

\bibitem{kolesnikov2020big}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock Big transfer (bit): General visual representation learning.
\newblock In {\em European conference on computer vision}, pages 491--507.
  Springer, 2020.

\bibitem{kornblith2019similarity}
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton.
\newblock Similarity of neural network representations revisited.
\newblock In {\em International Conference on Machine Learning}, pages
  3519--3529. PMLR, 2019.

\bibitem{kornblith2019better}
Simon Kornblith, Jonathon Shlens, and Quoc~V Le.
\newblock Do better imagenet models transfer better?
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 2661--2671, 2019.

\bibitem{KrauseStarkDengFei-Fei_3DRR2013}
Jonathan Krause, Michael Stark, Jia Deng, and Li~Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In {\em 4th International IEEE Workshop on 3D Representation and
  Recognition (3dRR-13)}, Sydney, Australia, 2013.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lenc2015understanding}
Karel Lenc and Andrea Vedaldi.
\newblock Understanding image representations by measuring their equivariance
  and equivalence.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 991--999, 2015.

\bibitem{li2019delta}
Xingjian Li, Haoyi Xiong, Hanchao Wang, Yuxuan Rao, Liping Liu, Zeyu Chen, and
  Jun Huan.
\newblock Delta: Deep learning transfer using feature map with attention for
  convolutional networks.
\newblock {\em arXiv preprint arXiv:1901.09229}, 2019.

\bibitem{LiuHuihui21}
Huihui Liu, Yiding Yang, and Xinchao Wang.
\newblock Overcoming catastrophic forgetting in graph neural networks.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, 2021.

\bibitem{liu2021Swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock {\em International Conference on Computer Vision (ICCV)}, 2021.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{macqueen1967some}
James MacQueen et~al.
\newblock Some methods for classification and analysis of multivariate
  observations.
\newblock In {\em Proceedings of the fifth Berkeley symposium on mathematical
  statistics and probability}, volume~1, pages 281--297. Oakland, CA, USA,
  1967.

\bibitem{maji13fine-grained}
S.~Maji, J.~Kannala, E.~Rahtu, M.~Blaschko, and A.~Vedaldi.
\newblock Fine-grained visual classification of aircraft.
\newblock Technical report, 2013.

\bibitem{mehta2021mobilevit}
Sachin Mehta and Mohammad Rastegari.
\newblock Mobilevit: light-weight, general-purpose, and mobile-friendly vision
  transformer.
\newblock {\em arXiv preprint arXiv:2110.02178}, 2021.

\bibitem{mellor2021neural}
Joe Mellor, Jack Turner, Amos Storkey, and Elliot~J Crowley.
\newblock Neural architecture search without training.
\newblock In {\em International Conference on Machine Learning}, pages
  7588--7598. PMLR, 2021.

\bibitem{montufar2014number}
Guido~F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio.
\newblock On the number of linear regions of deep neural networks.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem{nguyen2020leep}
Cuong Nguyen, Tal Hassner, Matthias Seeger, and Cedric Archambeau.
\newblock Leep: A new measure to evaluate transferability of learned
  representations.
\newblock In {\em International Conference on Machine Learning}, pages
  7294--7305. PMLR, 2020.

\bibitem{nguyen2021model}
Dang Nguyen, Khai Nguyen, Dinh Phung, Hung Bui, and Nhat Ho.
\newblock Model fusion of heterogeneous neural networks via cross-layer
  alignment.
\newblock {\em arXiv preprint arXiv:2110.15538}, 2021.

\bibitem{nilsback2008automated}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In {\em 2008 Sixth Indian Conference on Computer Vision, Graphics \&
  Image Processing}, pages 722--729. IEEE, 2008.

\bibitem{papadimitriou1998combinatorial}
Christos~H Papadimitriou and Kenneth Steiglitz.
\newblock {\em Combinatorial optimization: algorithms and complexity}.
\newblock Courier Corporation, 1998.

\bibitem{parkhi12a}
O.~M. Parkhi, A.~Vedaldi, A.~Zisserman, and C.~V. Jawahar.
\newblock Cats and dogs.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  2012.

\bibitem{radosavovic2020designing}
Ilija Radosavovic, Raj~Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr
  Doll{\'a}r.
\newblock Designing network design spaces.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 10428--10436, 2020.

\bibitem{raghu2017svcca}
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein.
\newblock Svcca: Singular vector canonical correlation analysis for deep
  learning dynamics and interpretability.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{ramsay1984matrix}
JO~Ramsay, Jos ten Berge, and GPH Styan.
\newblock Matrix correlation.
\newblock {\em Psychometrika}, 49(3):403--423, 1984.

\bibitem{ridnik2021imagenet21k}
Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor.
\newblock Imagenet-21k pretraining for the masses, 2021.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International journal of computer vision}, 115(3):211--252,
  2015.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{shu2021zoo}
Yang Shu, Zhi Kou, Zhangjie Cao, Jianmin Wang, and Mingsheng Long.
\newblock Zoo-tuning: Adaptive transfer from a zoo of models.
\newblock In {\em International Conference on Machine Learning}, pages
  9626--9637. PMLR, 2021.

\bibitem{singh2020model}
Sidak~Pal Singh and Martin Jaggi.
\newblock Model fusion via optimal transport.
\newblock {\em Advances in Neural Information Processing Systems},
  33:22045--22055, 2020.

\bibitem{SongJieNuerIPS19}
Jie Song, Yixin Chen, Xinchao Wang, Chengchao Shen, and Mingli Song.
\newblock Deep model transferability from attribution maps.
\newblock In {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{steiner2021train}
Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob
  Uszkoreit, and Lucas Beyer.
\newblock How to train your vit? data, augmentation, and regularization in
  vision transformers.
\newblock {\em arXiv preprint arXiv:2106.10270}, 2021.

\bibitem{tran2019transferability}
Anh~T Tran, Cuong~V Nguyen, and Tal Hassner.
\newblock Transferability and hardness of supervised classification tasks.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1395--1405, 2019.

\bibitem{van2021benchmarking}
Grant Van~Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and
  Oisin Mac~Aodha.
\newblock Benchmarking representation learning for natural world image
  collections.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 12884--12893, 2021.

\bibitem{wang2020federated}
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and
  Yasaman Khazaeni.
\newblock Federated learning with matched averaging.
\newblock {\em arXiv preprint arXiv:2002.06440}, 2020.

\bibitem{cubbird}
Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff,
  Serge Belongie, and Pietro Perona.
\newblock Caltech-ucsd birds 200.
\newblock Technical Report CNS-TR-201, Caltech, 2010.

\bibitem{williams2021generalized}
Alex Williams, Erin Kunz, Simon Kornblith, and Scott Linderman.
\newblock Generalized shape metrics on neural representations.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{xiao2021early}
Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Doll{\'a}r, and
  Ross Girshick.
\newblock Early convolutions help transformers see better.
\newblock {\em Advances in Neural Information Processing Systems},
  34:30392--30400, 2021.

\bibitem{xie2017aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1492--1500, 2017.

\bibitem{xuhong2018explicit}
LI~Xuhong, Yves Grandvalet, and Franck Davoine.
\newblock Explicit inductive bias for transfer learning with convolutional
  networks.
\newblock In {\em International Conference on Machine Learning}, pages
  2825--2834. PMLR, 2018.

\bibitem{yamins2014performance}
Daniel~LK Yamins, Ha~Hong, Charles~F Cadieu, Ethan~A Solomon, Darren Seibert,
  and James~J DiCarlo.
\newblock Performance-optimized hierarchical models predict neural responses in
  higher visual cortex.
\newblock {\em Proceedings of the national academy of sciences},
  111(23):8619--8624, 2014.

\bibitem{yang2020transfer}
Xingyi Yang, Xuehai He, Yuxiao Liang, Yue Yang, Shanghang Zhang, and Pengtao
  Xie.
\newblock Transfer learning or self-supervised learning? a tale of two
  pretraining paradigms.
\newblock {\em arXiv preprint arXiv:2007.04234}, 2020.

\bibitem{yang2022factorizing}
Xingyi Yang, Jingwen Ye, and Xinchao Wang.
\newblock Factorizing knowledge in neural networks.
\newblock {\em European Conference on Computer Vision}, 2022.

\bibitem{yang2020factorizable}
Yiding Yang, Zunlei Feng, Mingli Song, and Xinchao Wang.
\newblock Factorizable graph convolutional networks.
\newblock {\em Advances in Neural Information Processing Systems},
  33:20286--20296, 2020.

\bibitem{Yang_2020_CVPR}
Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xinchao Wang.
\newblock Distilling knowledge from graph convolutional networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2020.

\bibitem{ye2019student}
Jingwen Ye, Yixin Ji, Xinchao Wang, Kairi Ou, Dapeng Tao, and Mingli Song.
\newblock Student becoming the master: Knowledge amalgamation for joint scene
  parsing, depth estimation, and more.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2829--2838, 2019.

\bibitem{you2021logme}
Kaichao You, Yong Liu, Jianmin Wang, and Mingsheng Long.
\newblock Logme: Practical assessment of pre-trained models for transfer
  learning.
\newblock In {\em International Conference on Machine Learning}, pages
  12133--12143. PMLR, 2021.

\bibitem{yun2019cutmix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 6023--6032, 2019.

\bibitem{zhang2021quantifying}
Guojun Zhang, Han Zhao, Yaoliang Yu, and Pascal Poupart.
\newblock Quantifying and improving transferability in domain generalization.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv preprint arXiv:1710.09412}, 2017.

\bibitem{zhou2020rethinking}
Daquan Zhou, Qibin Hou, Yunpeng Chen, Jiashi Feng, and Shuicheng Yan.
\newblock Rethinking bottleneck structure for efficient mobile network design.
\newblock In {\em European Conference on Computer Vision}, pages 680--697.
  Springer, 2020.

\bibitem{zhou2021autospace}
Daquan Zhou, Xiaojie Jin, Xiaochen Lian, Linjie Yang, Yujing Xue, Qibin Hou,
  and Jiashi Feng.
\newblock Autospace: Neural architecture search with less human interference.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 337--346, 2021.

\bibitem{zhou2021deepvit}
Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang
  Jiang, Qibin Hou, and Jiashi Feng.
\newblock Deepvit: Towards deeper vision transformer.
\newblock {\em arXiv preprint arXiv:2103.11886}, 2021.

\bibitem{zhou2021ensemble}
Zhi-Hua Zhou.
\newblock Ensemble learning.
\newblock In {\em Machine learning}, pages 181--210. Springer, 2021.

\end{thebibliography}
