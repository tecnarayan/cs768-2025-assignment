@article{lu2022bridging,
title={Bridging the Gap Between Vision Transformers and Convolutional Neural Networks on Small Datasets},
author={Zhiying Lu and Hongtao Xie and Chuanbin Liu and Yongdong Zhang},
journal={Advances in Neural Information Processing Systems},
year={2022},
}

@article{xu2021vitae,
  title={Vitae: Vision transformer advanced by exploring intrinsic inductive bias},
  author={Xu, Yufei and Zhang, Qiming and Zhang, Jing and Tao, Dacheng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{andriushchenko2023modern,
  title={A modern look at the relationship between sharpness and generalization},
  author={Andriushchenko, Maksym and Croce, Francesco and M{\"u}ller, Maximilian and Hein, Matthias and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2302.07011},
  year={2023}
}

@inproceedings{kwon2021asam,
  title={Asam: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks},
  author={Kwon, Jungmin and Kim, Jeongseop and Park, Hyunseo and Choi, In Kwon},
  booktitle={International Conference on Machine Learning},
  pages={5905--5914},
  year={2021},
  organization={PMLR}
}

@article{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}

@article{salimans2016weight,
  title={Weight normalization: A simple reparameterization to accelerate training of deep neural networks},
  author={Salimans, Tim and Kingma, Durk P},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{zhuang2022surrogate,
  title={Surrogate gap minimization improves sharpness-aware training},
  author={Zhuang, Juntang and Gong, Boqing and Yuan, Liangzhe and Cui, Yin and Adam, Hartwig and Dvornek, Nicha and Tatikonda, Sekhar and Duncan, James and Liu, Ting},
  journal={arXiv preprint arXiv:2203.08065},
  year={2022}
}

@article{chatterji2019intriguing,
  title={The intriguing role of module criticality in the generalization of deep networks},
  author={Chatterji, Niladri S and Neyshabur, Behnam and Sedghi, Hanie},
  journal={arXiv preprint arXiv:1912.00528},
  year={2019}
}

@inproceedings{andriushchenko2022towards,
  title={Towards understanding sharpness-aware minimization},
  author={Andriushchenko, Maksym and Flammarion, Nicolas},
  booktitle={International Conference on Machine Learning},
  pages={639--668},
  year={2022},
  organization={PMLR}
}

@article{liu2022random,
  title={Random sharpness-aware minimization},
  author={Liu, Yong and Mai, Siqi and Cheng, Minhao and Chen, Xiangning and Hsieh, Cho-Jui and You, Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24543--24556},
  year={2022}
}

@article{liu2023over,
  title={Over-training with mixup may hurt generalization},
  author={Liu, Zixuan and Wang, Ziqiao and Guo, Hongyu and Mao, Yongyi},
  journal={arXiv preprint arXiv:2303.01475},
  year={2023}
}

@article{mordido2023lookbehind,
  title={Lookbehind Optimizer: k steps back, 1 step forward},
  author={Mordido, Gon{\c{c}}alo and Malviya, Pranshu and Baratin, Aristide and Chandar, Sarath},
  journal={arXiv preprint arXiv:2307.16704},
  year={2023}
}

@article{mollenhoff2022sam,
  title={SAM as an Optimal Relaxation of Bayes},
  author={M{\"o}llenhoff, Thomas and Khan, Mohammad Emtiyaz},
  journal={arXiv preprint arXiv:2210.01620},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{tiwari2023overcoming,
  title={Overcoming simplicity bias in deep networks using a feature sieve},
  author={Tiwari, Rishabh and Shenoy, Pradeep},
  booktitle={International Conference on Machine Learning},
  pages={34330--34343},
  year={2023},
  organization={PMLR}
}

@inproceedings{
zhou2022fortuitous,
title={Fortuitous Forgetting in Connectionist Networks},
author={Hattie Zhou and Ankit Vani and Hugo Larochelle and Aaron Courville},
booktitle={International Conference on Learning Representations},
year={2022}
}

@article{beyer2020we,
  title={Are we done with imagenet?},
  author={Beyer, Lucas and H{\'e}naff, Olivier J and Kolesnikov, Alexander and Zhai, Xiaohua and Oord, A{\"a}ron van den},
  journal={arXiv preprint arXiv:2006.07159},
  year={2020}
}

@article{korpelevich1976extragradient,
  title={The extragradient method for finding saddle points and other problems},
  author={Korpelevich, Galina M},
  journal={Matecon},
  volume={12},
  pages={747--756},
  year={1976}
}

@article{juditsky2011solving,
  title={Solving variational inequalities with stochastic mirror-prox algorithm},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011},
  publisher={INFORMS}
}

@inproceedings{mishchenko2020revisiting,
  title={Revisiting stochastic extragradient},
  author={Mishchenko, Konstantin and Kovalev, Dmitry and Shulgin, Egor and Richt{\'a}rik, Peter and Malitsky, Yura},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4573--4582},
  year={2020},
  organization={PMLR}
}

@article{bahri2021sharpness,
  title={Sharpness-aware minimization improves language model generalization},
  author={Bahri, Dara and Mobahi, Hossein and Tay, Yi},
  journal={arXiv preprint arXiv:2110.08529},
  year={2021}
}

@article{kawaguchi2023does,
  title={How Does Information Bottleneck Help Deep Learning?},
  author={Kawaguchi, Kenji and Deng, Zhun and Ji, Xu and Huang, Jiaoyang},
  journal={arXiv preprint arXiv:2305.18887},
  year={2023}
}

@article{ujvary2022rethinking,
  title={Rethinking Sharpness-Aware Minimization as Variational Inference},
  author={Ujv{\'a}ry, Szilvia and Telek, Zsigmond and Kerekes, Anna and M{\'e}sz{\'a}ros, Anna and Husz{\'a}r, Ferenc},
  journal={arXiv preprint arXiv:2210.10452},
  year={2022}
}

@inproceedings{kim2022fisher,
  title={Fisher sam: Information geometry and sharpness aware minimisation},
  author={Kim, Minyoung and Li, Da and Hu, Shell X and Hospedales, Timothy},
  booktitle={International Conference on Machine Learning},
  pages={11148--11161},
  year={2022},
  organization={PMLR}
}

@inproceedings{wen2022sharpness,
  title={How Sharpness-Aware Minimization Minimizes Sharpness?},
  author={Wen, Kaiyue and Ma, Tengyu and Li, Zhiyuan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{taha2021knowledge,
  title={Knowledge evolution in neural networks},
  author={Taha, Ahmed and Shrivastava, Abhinav and Davis, Larry S},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12843--12852},
  year={2021}
}

@inproceedings{deng2009imagenet,
  title = {{ImageNet}: A large-scale hierarchical image database},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  year = 2009,
  booktitle = {2009 IEEE conference on computer vision and pattern recognition},
  pages = {248--255},
  organization = {{IEEE}}
}

@article{andriushchenko2023sharpness,
  title={Sharpness-Aware Minimization Leads to Low-Rank Features},
  author={Andriushchenko, Maksym and Bahri, Dara and Mobahi, Hossein and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2305.16292},
  year={2023}
}

@article{si2023practical,
  title={Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima},
  author={Si, Dongkuk and Yun, Chulhee},
  journal={arXiv preprint arXiv:2306.09850},
  year={2023}
}

@article{chen2023does,
  title={Why Does Sharpness-Aware Minimization Generalize Better Than SGD?},
  author={Chen, Zixiang and Zhang, Junkai and Kou, Yiwen and Chen, Xiangning and Hsieh, Cho-Jui and Gu, Quanquan},
  journal={arXiv preprint arXiv:2310.07269},
  year={2023}
}

@inproceedings{kaur2023maximum,
  title={On the maximum hessian eigenvalue and generalization},
  author={Kaur, Simran and Cohen, Jeremy and Lipton, Zachary Chase},
  booktitle={Proceedings on},
  pages={51--65},
  year={2023},
  organization={PMLR}
}

@article{tishby2000information,
  title={The information bottleneck method},
  author={Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  journal={arXiv preprint physics/0004057},
  year={2000}
}

@article{jiang2019fantastic,
  title={Fantastic generalization measures and where to find them},
  author={Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  journal={arXiv preprint arXiv:1912.02178},
  year={2019}
}

@article{chen2021vision,
  title={When vision transformers outperform resnets without pre-training or strong data augmentations},
  author={Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing},
  journal={arXiv preprint arXiv:2106.01548},
  year={2021}
}

@article{krizhevsky2009learning,
  title = {Learning multiple layers of features from tiny images},
  author = {Krizhevsky, Alex and Hinton, Geoffrey and others},
  year = 2009,
  journal = {Technical report},
  publisher = {Toronto, ON, Canada}
}

@inproceedings{recht2019imagenet,
  title={Do imagenet classifiers generalize to imagenet?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle={International conference on machine learning},
  pages={5389--5400},
  year={2019},
  organization={PMLR}
}

@inproceedings{hendrycks2021many,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8340--8349},
  year={2021}
}

@article{hendrycks2019benchmarking,
  title={Benchmarking neural network robustness to common corruptions and perturbations},
  author={Hendrycks, Dan and Dietterich, Thomas},
  journal={arXiv preprint arXiv:1903.12261},
  year={2019}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{bartlett2023dynamics,
  title={The dynamics of sharpness-aware minimization: Bouncing across ravines and drifting towards wide minima},
  author={Bartlett, Peter L and Long, Philip M and Bousquet, Olivier},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={316},
  pages={1--36},
  year={2023}
}

@article{behdin2023sharpness,
  title={Sharpness-aware minimization: An implicit regularization perspective},
  author={Behdin, Kayhan and Mazumder, Rahul},
  journal={arXiv preprint arXiv:2302.11836},
  year={2023}
}

@article{jin2022pruning,
  title={Pruning’s effect on generalization through the lens of training and regularization},
  author={Jin, Tian and Carbin, Michael and Roy, Dan and Frankle, Jonathan and Dziugaite, Gintare Karolina},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37947--37961},
  year={2022}
}

@article{ren2020compositional,
  title={Compositional languages emerge in a neural iterated learning model},
  author={Ren, Yi and Guo, Shangmin and Labeau, Matthieu and Cohen, Shay B and Kirby, Simon},
  journal={arXiv preprint arXiv:2002.01365},
  year={2020}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{wen2023sharpness,
  title={Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization},
  author={Wen, Kaiyue and Ma, Tengyu and Li, Zhiyuan},
  journal={arXiv preprint arXiv:2307.11007},
  year={2023}
}

@article{mueller2023normalization,
  title={Normalization Layers Are All That Sharpness-Aware Minimization Needs},
  author={Mueller, Maximilian and Vlaar, Tiffany and Rolnick, David and Hein, Matthias},
  journal={arXiv preprint arXiv:2306.04226},
  year={2023}
}

@article{chen2022compressing,
  title={Compressing features for learning with noisy labels},
  author={Chen, Yingyi and Hu, Shell Xu and Shen, Xi and Ai, Chunrong and Suykens, Johan AK},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@article{saxe2019information,
  title={On the information bottleneck theory of deep learning},
  author={Saxe, Andrew M and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D and Cox, David D},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124020},
  year={2019},
  publisher={IOP Publishing}
}
@inproceedings{wang2019learning,
        title={Learning Robust Global Representations by Penalizing Local Predictive Power},
        author={Wang, Haohan and Ge, Songwei and Lipton, Zachary and Xing, Eric P},
        booktitle={Advances in Neural Information Processing Systems},
        pages={10506--10518},
        year={2019}
}
@inproceedings{kim2023fantastic,
  title={Fantastic Robustness Measures: The Secrets of Robust Generalization},
  author={Kim, Hoki and Park, Jinseong and Choi, Yujin and Lee, Jaewook},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}

@inproceedings{mollenhoff2023sam,
title={{SAM} as an Optimal Relaxation of Bayes},
author={Thomas M{\"o}llenhoff and Mohammad Emtiyaz Khan},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}
}

@inproceedings{baek2024why,
title={Why is {SAM} Robust to Label Noise?},
author={Christina Baek and J Zico Kolter and Aditi Raghunathan},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}

@inproceedings{du2022sharpnessaware,
title={Sharpness-Aware Training for Free},
author={Jiawei Du and Zhou Daquan and Jiashi Feng and Vincent Tan and Joey Tianyi Zhou},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022}
}

@article{ash2020warm,
  title={On warm-starting neural network training},
  author={Ash, Jordan and Adams, Ryan P},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={3884--3894},
  year={2020}
}

@inproceedings{elsayed2023utility,
  title={Utility-based Perturbed Gradient Descent: An Optimizer for Continual Learning},
  author={Elsayed, Mohamed and Mahmood, A Rupam},
  booktitle={OPT 2023: Optimization for Machine Learning},
  year={2023}
}

@article{dohare2023maintaining,
  title={Maintaining plasticity in deep continual learning},
  author={Dohare, Shibhansh and Hernandez-Garcia, J Fernando and Rahman, Parash and Sutton, Richard S and Mahmood, A Rupam},
  journal={arXiv preprint arXiv:2306.13812},
  year={2023}
}

@article{kumar2023maintaining,
  title={Maintaining plasticity via regenerative regularization},
  author={Kumar, Saurabh and Marklund, Henrik and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:2308.11958},
  year={2023}
}

@inproceedings{doro2023sampleefficient,
title={Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier},
author={Pierluca D'Oro and Max Schwarzer and Evgenii Nikishin and Pierre-Luc Bacon and Marc G Bellemare and Aaron Courville},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}
}