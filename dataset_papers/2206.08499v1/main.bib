@article{huber64,
 author = {Peter J. Huber},
 journal = {The Annals of Mathematical Statistics},
 number = {1},
 pages = {73--101},
 publisher = {Institute of Mathematical Statistics},
 title = {Robust Estimation of a Location Parameter},
 volume = {35},
 year = {1964}
}

@article{mnih15,
 author = {Mnih, V., Kavukcuoglu, K., Silver, D. et al.},
 journal = {Nature},
 number = {518},
 pages = {529–533},
 title = { Human-level control through deep reinforcement learning},
 year = {2015}
}

@inproceedings{chen19,
  author    = {Minmin Chen and
               Alex Beutel and
               Paul Covington and
               Sagar Jain and
               Francois Belletti and
               Ed H. Chi},
  title     = {Top-K Off-Policy Correction for a {REINFORCE} Recommender System},
  booktitle = {Proceedings of the Twelfth {ACM} International Conference on Web Search
               and Data Mining, {WSDM} 2019, Melbourne, VIC, Australia, February
               11-15, 2019},
  pages     = {456--464},
  publisher = {{ACM}},
  year      = {2019},
}

@misc{parkerholder2022automated,
      title={Automated Reinforcement Learning (AutoRL): A Survey and Open Problems}, 
      author={Jack Parker-Holder and Raghu Rajan and Xingyou Song and André Biedenkapp and Yingjie Miao and Theresa Eimer and Baohe Zhang and Vu Nguyen and Roberto Calandra and Aleksandra Faust and Frank Hutter and Marius Lindauer},
      year={2022},
      eprint={2201.03916},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{impala,
	title={IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures}, 
	author={Lasse Espeholt and Hubert Soyer and Remi Munos and Karen Simonyan and Volodymir Mnih and Tom Ward and Yotam Doron and Vlad Firoiu and Tim Harley and Iain Dunning and Shane Legg and Koray Kavukcuoglu},
	year={2018},
	journal = {ArXiv:1802.01561},
	note = "\href{https://arxiv.org/abs/1802.01561}{URL}"
}


@article{acme,
	title={Acme: {A} Research Framework for Distributed Reinforcement Learning}, 
	author={
	 Matt Hoffman and
               Bobak Shahriari and
               John Aslanides and
               Gabriel Barth{-}Maron and
               Feryal Behbahani and
               Tamara Norman and
               Abbas Abdolmaleki and
               Albin Cassirer and
               Fan Yang and
               Kate Baumli and
               Sarah Henderson and
               Alexander Novikov and
               Sergio G{\'{o}}mez Colmenarejo and
               Serkan Cabi and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Tom Le Paine and
               Andrew Cowie and
               Ziyu Wang and
               Bilal Piot and
               Nando de Freitas
	 },
	year={2020},
	journal = {ArXiv:2006.00979},
	note = "\href{https://arxiv.org/abs/2006.00979}{URL}"
}

@article{disentangle,
	title={Disentangling Adaptive Gradient Methods from Learning Rates}, 
	author={Naman Agarwal and
               Rohan Anil and
               Elad Hazan and
               Tomer Koren and
               Cyril Zhang},
	year={2020},
	journal = {ArXiv:2002.11803},
	note = "\href{https://arxiv.org/abs/2002.11803}{URL}"
}


@article{warmuth20,
	title={Interpolating Between Gradient Descent and Exponentiated Gradient
               Using Reparameterized Gradient Descent}, 
	author={Ehsan Amid and
               Manfred K. Warmuth},
	year={2020},
	journal = {ArXiv:2002.10487},
	note = "\href{https://arxiv.org/abs/2002.10487}{URL}"
}

Reparameterizing Mirror Descent
as Gradient Descent

@article{huibelkin,
	title={Evaluation of Neural Architectures Trained with Square Loss vs Cross-Entropy in Classification Tasks}, 
	author={Like Hui and Mikhail Belkin},
	year={2020},
	journal = {ArXiv:2006.07322},
	note = "\href{https://arxiv.org/abs/2006.07322}{URL}"
}

@article{controlasinference,
	title={Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}, 
	author={Sergey Levine},
	year={2018},
	journal = {ArXiv:1805.00909},
	note = "\href{https://arxiv.org/abs/1805.00909}{URL}"
}

@article{agrawal20,
Author = {Naman Agarwal and Rohan Anil and Elad Hazan and Tomer Koren and Cyril Zhang},
Title = {Disentangling Adaptive Gradient Methods from Learning Rates},
journal = {ArXiv:2002.11803},
Year = {2020},
note = "\href{https://arxiv.org/pdf/2002.11803.pdf}{URL}"
}

@phdthesis{dabney14,
author = {William Dabney},
title = {Adaptive step-sizes for reinforcement learning},
publisher = {PhD Thesis University of Massachusetts at Amherst},
year = {2014}
}

@inproceedings{sil,
  title={Self-Imitation Learning},
  author={Junhyuk Oh and Yijie Guo and Satinder Singh and Honglak Lee},
  booktitle={ICML},
  year={2018},
  note = "\href{https://arxiv.org/pdf/1806.05635.pdf}{URL}"
}

@inproceedings{actionbaseline1, 
	title={Variance Reduction for Policy Gradient with Action-Dependent Factorized Baselines}, 
	author={Cathy Wu and Aravind Rajeswaran and Yan Duan and Vikash Kumar 
	 and Alexandre M. Bayen and Sham M. Kakade and Igor Mordatch and Pieter Abbeel}, 
	year={2018}, 
	booktitle={ICLR},
	note="\href{https://arxiv.org/pdf/1803.07246.pdf}{URL}"
}

@inproceedings{actionbaseline2,
	title={Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic}, 
	author={Shixiang Gu and Timothy Lillicrap and Zoubin Ghahramani and Richard E. Turner and Sergey Levine},
	year={2017},
	booktitle={ICLR},
	note = "\href{https://arxiv.org/pdf/1611.02247.pdf}{URL}"
}

@inproceedings{actionbaseline3,
	title={The Mirage of Action-Dependent Baselines in Reinforcement Learning}, 
	author={George Tucker and Surya Bhupatiraju and Shixiang Gu and Richard E. Turner and Zoubin Ghahramani and Sergey Levine},
	year={2018},
	booktitle={ICML},
	note = "\href{https://arxiv.org/abs/1802.10031}{URL}"
}

@inproceedings{baseline1,
	title={The Optimal Reward Baseline for Gradient-Based Reinforcement Learning}, 
	author={Lex Weaver and Nigel Tao},
	year={2001},
	booktitle={UAI},
	note = "\href{https://arxiv.org/pdf/1301.2315.pdf}{URL}"
}

@inproceedings{baseline2,
	author={Evan Greensmith and Peter L. Bartlett and Jonathan Baxter},
	title={Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning},
	year={2001},
	note = "\href{http://papers.nips.cc/paper/2052-variance-reduction-techniques-for-gradient-estimates-in-reinforcement-learning.pdf}{URL}",
	booktitle={Advances in Neural Information Processing Systems},
}



@inproceedings{mapo,
  title={Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing},
  author={Liang, Chen and Norouzi, Mohammad and Berant, Jonathan and Le, Quoc V and Lao, Ni},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018},
  note = "\href{https://arxiv.org/pdf/1807.02322.pdf}{URL}"
}

@inproceedings{pcl,
	title={Bridging the Gap Between Value and Policy Based Reinforcement Learning}, 
	author={Ofir Nachum and Mohammad Norouzi and Kelvin Xu and Dale Schuurmans},
	year={2017},
	booktitle={Advances in Neural Information Processing Systems},
	note = "\href{https://arxiv.org/pdf/1702.08892.pdf}{URL}"
}


@inproceedings{pgqcombine,
	title={Combining policy gradient and Q-learning}, 
	author={Brendan O'Donoghue and Remi Munos and Koray Kavukcuoglu and Volodymyr Mnih},
  	booktitle={ICLR},
	year={2017},
	note = "\href{https://openreview.net/forum?id=B1kJ6H9ex}{URL}"
}

@inproceedings{pgoriginal,
	title={Policy Gradient Methods for Reinforcement Learning with Function Approximation}, 
	author={Richard Sutton and David McAllester and Satinder Singh and Yishay Mansour},
	year={1999},
	booktitle={Advances in Neural Information Processing Systems},
	note = "\href{https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf}{URL}"
}

@inproceedings{bridge,
	title={Bridging the Gap Between Value and Policy Based Reinforcement Learning}, 
	author={Ofir Nachum and Mohammad Norouzi and Kelvin Xu and Dale Schuurmans},
	year={2017},
	booktitle={Advances in Neural Information Processing Systems},
	note = "\href{https://arxiv.org/pdf/1702.08892.pdf}{URL}"
}

@article{cql,
	title={Conservative Q-Learning for Offline Reinforcement Learning}, 
	author={Aviral Kumar and Aurick Zhou and George Tucker and Sergey Levine},
	year={2020},
	journal = {ArXiv:2006.04779},
	note = "\href{https://arxiv.org/pdf/2006.04779.pdf}{URL}"
}

@article{pgtheory,
	title={On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift}, 
	author={Alekh Agarwal and Sham M. Kakade and Jason D. Lee and Gaurav Mahajan},
	year={2020},
	journal = {ArXiv:1908.00261},
	note = "\href{https://arxiv.org/pdf/1908.00261.pdf}{URL}"
}

@article{pgequiv,
    title={Equivalence Between Policy Gradients and Soft Q-Learning},
    author={John Schulman and Xi Chen and Pieter Abbeel},
    year={2018},
    journal = {ArXiv:1704.06440},
    note = "\href{https://arxiv.org/pdf/1704.06440.pdf}{URL}"
}

@inproceedings{ours21,
	title = {Characterizing the Gap Between Actor-Critic and Policy Gradient},
	author = {Wen, Junfeng and Kumar, Saurabh and Gummadi, Ramki and Schuurmans, Dale},
	booktitle = {ICML},
	year = {2021}
}

@inproceedings{surrogate,
	title = {Surrogate Objectives for Batch Policy Optimization in One-step Decision Making},
	author = {Chen, Minmin and Gummadi, Ramki and Harris, Chris and Schuurmans, Dale},
	booktitle = {Advances in Neural Information Processing Systems},
	year = {2019},
	note = "\href{https://papers.nips.cc/paper/9086-surrogate-objectives-for-batch-policy-optimization-in-one-step-decision-making}{URL}"
}

@article{brac,
	title={Behavior Regularized Offline Reinforcement Learning}, 
	author={Yifan Wu and George Tucker and Ofir Nachum},
	year={2019},
	journal = {ArXiv:1911.11361},
	note = "\href{https://arxiv.org/pdf/1911.11361.pdf}{URL}"
}

@article{ppo,
	title={Proximal Policy Optimization Algorithms}, 
	author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
	journal = {ArXiv:1707.06347},
	year = {2017},
	note = "\href{https://arxiv.org/pdf/1707.06347.pdf}{URL}"
}

@inproceedings{gpi,
title = {Generalized Policy Updates for Policy Optimization},
author = {Saurabh Kumar and Robert Dadashi and Zafarali Ahmed and Dale Schuurmans and Marc G. Bellemare},
note = "\href{https://optrl2019.github.io/assets/accepted_papers/68.pdf}{URL}",
booktitle = {NeurIPS 2019 Optimization Foundations for Reinforcement Learning Workshop},
year = {2019},
}

% MDP book
@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}


@inproceedings{liu2020off,
  title={Off-Policy Policy Gradient with Stationary Distribution Correction},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={1180--1190},
  year={2020},
  organization={PMLR}
}

@article{dabney2020distributional,
  title={A distributional code for value in dopamine-based reinforcement learning},
  author={Dabney, Will and Kurth-Nelson, Zeb and Uchida, Naoshige and Starkweather, Clara Kwon and Hassabis, Demis and Munos, R{\'e}mi and Botvinick, Matthew},
  journal={Nature},
  volume={577},
  number={7792},
  pages={671--675},
  year={2020},
  publisher={Nature Publishing Group}
}

@inproceedings{sherstan2018comparing,
  title={Comparing Direct and Indirect Temporal-Difference Methods for Estimating the Variance of the Return.},
  author={Sherstan, Craig and Ashley, Dylan R and Bennett, Brendan and Young, Kenny and White, Adam and White, Martha and Sutton, Richard S},
  booktitle={UAI},
  pages={63--72},
  year={2018}
}


% two time-scale convergence analysis
@inproceedings{wu2020finite,
 author = {Wu, Yue Frank and Zhang, Weitong and Xu, Pan and Gu, Quanquan},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {17617--17628},
 title = {A Finite-Time Analysis of Two Time-Scale Actor-Critic Methods},
 volume = {33},
 year = {2020}
}

@inproceedings{bridge,
	title={Bridging the Gap Between Value and Policy Based Reinforcement Learning}, 
	author={Ofir Nachum and Mohammad Norouzi and Kelvin Xu and Dale Schuurmans},
	year={2017},
	booktitle={Advances in Neural Information Processing Systems},
}

@inproceedings{transformer,
title	= {Attention is All You Need},
author	= {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
year	= {2017},
URL	= {https://arxiv.org/pdf/1706.03762.pdf}
}

% TRPO
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

@inproceedings{reparamgd,
  author    = {Ehsan Amid and Manfred Warmuth},
  title     = {Reparameterizing Mirror Descent as Gradient Descent},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020}
}

@inproceedings{GMR20,
  author    = {Dibya Ghosh and
               Marlos C. Machado and
               Nicolas Le Roux},
  title     = {An operator view of policy gradient methods},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
}

@inproceedings{Williams92simplestatistical,
    author = {Ronald J. Williams},
    title = {Simple statistical gradient-following algorithms for connectionist reinforcement learning},
    booktitle = {Machine Learning},
    year = {1992},
    pages = {229--256}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

% softmax
@article{mei2020global,
  title={On the Global Convergence Rates of Softmax Policy Gradient Methods},
  author={Mei, Jincheng and Xiao, Chenjun and Szepesvari, Csaba and Schuurmans, Dale},
  journal={arXiv preprint arXiv:2005.06392},
  year={2020}
}

% softmax gradient
@inproceedings{agarwal2020optimality,
  title={Optimality and approximation with policy gradient methods in markov decision processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  booktitle={Conference on Learning Theory},
  pages={64--66},
  year={2020},
  organization={PMLR}
}

% dual notations
@inproceedings{wang2007dual,
  title={Dual representations for dynamic programming and reinforcement learning},
  author={Wang, Tao and Bowling, Michael and Schuurmans, Dale},
  booktitle={2007 IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning},
  pages={44--51},
  year={2007},
  organization={IEEE}
}

% PG
@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

% SAC
@inproceedings{haarnoja2018soft,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1861--1870},
  year={2018}
}

% A3C
@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}


@inproceedings{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Hoof, Herke and Meger, David},
  booktitle={International Conference on Machine Learning},
  pages={1587--1596},
  year={2018},
  organization={PMLR}
}

% different form of average reward
@incollection{singh1994learning,
  title={Learning without state-estimation in partially observable Markovian decision processes},
  author={Singh, Satinder P and Jaakkola, Tommi and Jordan, Michael I},
  booktitle={Machine Learning Proceedings 1994},
  pages={284--292},
  year={1994},
  publisher={Elsevier}
}

% log stationary der
@article{morimura2010derivatives,
  title={Derivatives of logarithmic stationary distributions for policy gradient reinforcement learning},
  author={Morimura, Tetsuro and Uchibe, Eiji and Yoshimoto, Junichiro and Peters, Jan and Doya, Kenji},
  journal={Neural computation},
  volume={22},
  number={2},
  pages={342--376},
  year={2010},
  publisher={MIT Press}
}

% stackelberg
@inproceedings{fiez2020implicit,
  title={Implicit learning dynamics in stackelberg games: Equilibria characterization, convergence analysis, and empirical study},
  author={Fiez, Tanner and Chasnov, Benjamin and Ratliff, Lillian},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2020}
}

% Stack model-based
@inproceedings{rajeswaran2020game,
  title = {A Game Theoretic Framework for Model-Based Reinforcement Learning},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Rajeswaran, Aravind and Mordatch, Igor and Kumar, Vikash},
  year = {2020}
}

% BR versus TD
@inproceedings{scherrer2010should,
  title={Should one compute the temporal difference fix point or minimize the Bellman Residual? The unified oblique projection view},
  author={Scherrer, Bruno},
  booktitle={Proceedings of the 27th International Conference on International Conference on Machine Learning},
  pages={959--966},
  year={2010}
}

@inproceedings{dai2018sbeed,
  title={Sbeed: Convergent reinforcement learning with nonlinear function approximation},
  author={Dai, Bo and Shaw, Albert and Li, Lihong and Xiao, Lin and He, Niao and Liu, Zhen and Chen, Jianshu and Song, Le},
  booktitle={International Conference on Machine Learning},
  pages={1125--1134},
  year={2018},
  organization={PMLR}
}

% double sample
@incollection{baird1995residual,
  title={Residual algorithms: Reinforcement learning with function approximation},
  author={Baird, Leemon},
  booktitle={Machine Learning Proceedings 1995},
  pages={30--37},
  year={1995},
  publisher={Elsevier}
}

% resnet
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

%MuJoCo
@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

% Stackelberg review
@article{sinha2017review,
  title={A review on bilevel optimization: from classical to evolutionary approaches and applications},
  author={Sinha, Ankur and Malo, Pekka and Deb, Kalyanmoy},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={22},
  number={2},
  pages={276--295},
  year={2017},
  publisher={IEEE}
}

% DQN
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

% DDPG
@inproceedings{lillicrap2016continuous,
  title={Continuous control with deep reinforcement learning.},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  booktitle={ICLR},
  year={2016}
}


@inproceedings{flet21,
  title={Learning Value Functions in Deep Policy Gradients using Residual Variance},
  author={Y. Flet-Berliac and R. Ouhamma and O.-A. Maillard and P. Preux},
  booktitle={ICLR},
  year={2021}
}

% GAE
@inproceedings{schulman2016high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  booktitle={ICLR},
  year={2016}
}

% prioritized replay
@inproceedings{schaul2016prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  booktitle={ICLR},
  year={2016}
}

% First appearance of PG (according to Sutton's book)
@article{marbach2001simulation,
  title={Simulation-based optimization of Markov reward processes},
  author={Marbach, Peter and Tsitsiklis, John N},
  journal={IEEE Transactions on Automatic Control},
  volume={46},
  number={2},
  pages={191--209},
  year={2001},
  publisher={IEEE}
}

% AC
@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle={Advances in neural information processing systems},
  pages={1008--1014},
  year={2000},
  organization={Citeseer}
}

% Off-policy AC
@inproceedings{degris2012off,
  title={Off-policy actor-critic},
  author={Degris, Thomas and White, Martha and Sutton, Richard S},
  booktitle={Proceedings of the 29th International Coference on International Conference on Machine Learning},
  pages={179--186},
  year={2012}
}

% Reparametrization trick
@inproceedings{schulman2015gradient,
  title={Gradient estimation using stochastic computation graphs},
  author={Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  booktitle={Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 2},
  pages={3528--3536},
  year={2015}
}

% DPG
@inproceedings{silver2014deterministic,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={International conference on machine learning},
  pages={387--395},
  year={2014},
  organization={PMLR}
}

% AC finite sample convergence
@article{wu2020finite,
  title={A finite time analysis of two time-scale actor critic methods},
  author={Wu, Yue and Zhang, Weitong and Xu, Pan and Gu, Quanquan},
  journal={Advances in neural information processing systems},
  year={2020}
}

% GTD2 and TDC
@inproceedings{sutton2009fast,
  title={Fast gradient-descent methods for temporal-difference learning with linear function approximation},
  author={Sutton, Richard S and Maei, Hamid Reza and Precup, Doina and Bhatnagar, Shalabh and Silver, David and Szepesv{\'a}ri, Csaba and Wiewiora, Eric},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={993--1000},
  year={2009}
}

% GTD2 and TDC with (non-linear) smooth approximator
@inproceedings{maei2009convergent,
  title={Convergent temporal-difference learning with arbitrary smooth function approximation.},
  author={Maei, Hamid Reza and Szepesvari, Csaba and Bhatnagar, Shalabh and Precup, Doina and Silver, David and Sutton, Richard S},
  booktitle={NIPS},
  pages={1204--1212},
  year={2009}
}

% TD as reward to expand basis in the linear case
@inproceedings{sun2011incremental,
  title={Incremental basis construction from temporal difference error},
  author={Sun, Yi and Gomez, Faustino and Ring, Mark and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 28th International Conference on International Conference on Machine Learning},
  pages={481--488},
  year={2011}
}

% Prioritized sweeping
@inproceedings{van2013planning,
  title={Planning by prioritized sweeping with small backups},
  author={Van Seijen, Harm and Sutton, Rich},
  booktitle={International Conference on Machine Learning},
  pages={361--369},
  year={2013},
  organization={PMLR}
}

@article{haarnoja2018soft2,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

% Beta policy
@inproceedings{chou2017improving,
  title={Improving stochastic policy gradients in continuous control with deep reinforcement learning using the beta distribution},
  author={Chou, Po-Wei and Maturana, Daniel and Scherer, Sebastian},
  booktitle={International conference on machine learning},
  pages={834--843},
  year={2017},
  organization={PMLR}
}

@article{peters2008natural,
  title={Natural actor-critic},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neurocomputing},
  volume={71},
  number={7-9},
  pages={1180--1190},
  year={2008},
  publisher={Elsevier}
}

% MaxEnt RL
@inproceedings{haarnoja2017reinforcement,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1352--1361},
  year={2017},
  organization={PMLR}
}

% FourRoom
@book{precup2000temporal,
  title={Temporal abstraction in reinforcement learning},
  author={Precup, Doina},
  year={2000},
  publisher={University of Massachusetts Amherst}
}

% Connection between loss and sampling
@article{fujimoto2020equivalence,
  title={An Equivalence between Loss Functions and Non-Uniform Sampling in Experience Replay},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

% logistic QL
@inproceedings{serrano2021logistic,
  title={Logistic Q-Learning},
  author={Serrano, Joan Bas and Curi, Sebastian and Krause, Andreas and Neu, Gergely},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3610--3618},
  year={2021},
  organization={PMLR}
}

% general mirror descent
@article{vaswani2021general,
  title={A general class of surrogate functions for stable and efficient reinforcement learning},
  author={Vaswani, Sharan and Bachem, Olivier and Totaro, Simone and Mueller, Robert and Garg, Shivam and Geist, Matthieu and Machado, Marlos C and Castro, Pablo Samuel and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:2108.05828},
  year={2021}
}

% momentum
@inproceedings{vieillard2020momentum,
  title={Momentum in reinforcement learning},
  author={Vieillard, Nino and Scherrer, Bruno and Pietquin, Olivier and Geist, Matthieu},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2529--2538},
  year={2020},
  organization={PMLR}
}

@article{vieillard2020leverage,
  title={Leverage the Average: an Analysis of KL Regularization in Reinforcement Learning},
  author={Vieillard, Nino and Kozuno, Tadashi and Scherrer, Bruno and Pietquin, Olivier and Munos, Remi and Geist, Matthieu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}