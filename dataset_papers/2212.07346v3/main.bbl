\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andriushchenko et~al.(2022)Andriushchenko, Varre, Pillaud-Vivien, and
  Flammarion]{andriushchenko2022sgd}
Maksym Andriushchenko, Aditya Varre, Loucas Pillaud-Vivien, and Nicolas
  Flammarion.
\newblock Sgd with large step sizes learns sparse features.
\newblock \emph{arXiv preprint arXiv:2210.05337}, 2022.

\bibitem[Arjovsky et~al.(2020)Arjovsky, Bottou, Gulrajani, and Lopez-Paz]{irm}
Martin Arjovsky, L\'{e}on Bottou, Ishaan Gulrajani, and David Lopez-Paz.
\newblock Invariant risk minimization.
\newblock \emph{arXiv}, 2020.

\bibitem[Bandi et~al.(2018)Bandi, Geessink, Manson, Van~Dijk, Balkenhol,
  Hermsen, Bejnordi, Lee, Paeng, Zhong, et~al.]{bandi2018detection}
Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van~Dijk, Maschenka
  Balkenhol, Meyke Hermsen, Babak~Ehteshami Bejnordi, Byungjae Lee, Kyunghyun
  Paeng, Aoxiao Zhong, et~al.
\newblock From detection of individual metastases to classification of lymph
  node status at the patient level: the camelyon17 challenge.
\newblock \emph{IEEE Transactions on Medical Imaging}, 2018.

\bibitem[Bilen and Vedaldi(2017)]{bilen2017universal}
Hakan Bilen and Andrea Vedaldi.
\newblock Universal representations: The missing link between faces, text,
  planktons, and cat breeds.
\newblock \emph{arXiv preprint arXiv:1701.07275}, 2017.

\bibitem[Blanc et~al.(2020)Blanc, Gupta, Valiant, and
  Valiant]{blanc2020implicit}
Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant.
\newblock Implicit regularization for deep neural networks driven by an
  ornstein-uhlenbeck like process.
\newblock In \emph{Conference on learning theory}, pages 483--513. PMLR, 2020.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, al. Russ~Altman,
  Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, Brynjolfsson, Buch,
  Card, Castellon, Chatterji, Chen, Creel, Davis, Demszky, Donahue, Doumbouya,
  Durmus, Ermon, Etchemendy, Ethayarajh, Fei{-}Fei, Finn, Gale, Gillespie,
  Goel, Goodman, Grossman, Guha, Hashimoto, Henderson, Hewitt, Ho, Hong, Hsu,
  Huang, Icard, Jain, Jurafsky, Kalluri, Karamcheti, Keeling, Khani, Khattab,
  Koh, Krass, Krishna, Kuditipudi, and et~al.]{bommasani-2021}
Rishi Bommasani, Drew~A. Hudson, Ehsan Adeli, al. Russ~Altman, Simran Arora,
  Sydney von Arx, Michael~S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon,
  Niladri~S. Chatterji, Annie~S. Chen, Kathleen Creel, Jared~Quincy Davis,
  Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano
  Ermon, John Etchemendy, Kawin Ethayarajh, Li~Fei{-}Fei, Chelsea Finn, Trevor
  Gale, Lauren Gillespie, Karan Goel, Noah~D. Goodman, Shelby Grossman, Neel
  Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel~E. Ho, Jenny
  Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky,
  Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar
  Khattab, Pang~Wei Koh, Mark~S. Krass, Ranjay Krishna, Rohith Kuditipudi, and
  et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{CoRR}, abs/2108.07258, 2021.
\newblock URL \url{https://arxiv.org/abs/2108.07258}.

\bibitem[Bottou(2011)]{tr-bottou-2011}
L\'eon Bottou.
\newblock From machine learning to machine reasoning.
\newblock Technical report, arXiv:1102.1808, February 2011.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{Siam Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Caron et~al.(2020)Caron, Misra, Mairal, Goyal, Bojanowski, and
  Joulin]{caron2020unsupervised}
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
  Armand Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 9912--9924, 2020.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem[Chen et~al.(2019)Chen, Liu, Kira, Wang, and Huang]{closelookatfewshot}
Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang~Frank Wang, and Jia-Bin
  Huang.
\newblock A closer look at few-shot classification.
\newblock \emph{arXiv preprint arXiv:1904.04232}, 2019.

\bibitem[Chen and He(2020)]{chen2020simsiam}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock \emph{arXiv preprint arXiv:2011.10566}, 2020.

\bibitem[Chowdhury et~al.(2021)Chowdhury, Jiang, Chaudhuri, and
  Jermaine]{chowdhury2021few}
Arkabandhu Chowdhury, Mingchao Jiang, Swarat Chaudhuri, and Chris Jermaine.
\newblock Few-shot image classification: Just use a library of pre-trained
  feature extractors and a simple classifier.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9445--9454, 2021.

\bibitem[Collobert et~al.(2011)Collobert, Weston, Bottou, Karlen, Kavukcuoglu,
  and Kuksa]{collobert-2011}
Ronan Collobert, Jason Weston, L\'eon Bottou, Michael Karlen, Koray
  Kavukcuoglu, and Pavel Kuksa.
\newblock Natural language processing (almost) from scratch.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2493--2537,
  Aug 2011.

\bibitem[Denevi et~al.(2022)Denevi, Pontil, and
  Ciliberto]{denevi2022conditional}
Giulia Denevi, Massimiliano Pontil, and Carlo Ciliberto.
\newblock Conditional meta-learning of linear representations.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 253--266, 2022.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{imagenet_cvpr09}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{CVPR09}, 2009.

\bibitem[Dietterich(2000)]{dietterich2000ensemble}
Thomas~G Dietterich.
\newblock Ensemble methods in machine learning.
\newblock In \emph{International workshop on multiple classifier systems},
  pages 1--15. Springer, 2000.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Dvornik et~al.(2020)Dvornik, Schmid, and Mairal]{dvornik2020selecting}
Nikita Dvornik, Cordelia Schmid, and Julien Mairal.
\newblock Selecting relevant features from a multi-domain representation for
  few-shot classification.
\newblock In \emph{European Conference on Computer Vision}, pages 769--786.
  Springer, 2020.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International conference on machine learning}, pages
  1126--1135. PMLR, 2017.

\bibitem[Ganaie et~al.(2021)Ganaie, Hu, et~al.]{ganaie2021ensemble}
Mudasir~A Ganaie, Minghui Hu, et~al.
\newblock Ensemble deep learning: A review.
\newblock \emph{arXiv preprint arXiv:2104.02395}, 2021.

\bibitem[Geirhos et~al.(2020)Geirhos, Jacobsen, Michaelis, Zemel, Brendel,
  Bethge, and Wichmann]{geirhos-2020}
Robert Geirhos, J{\"o}rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel,
  Wieland Brendel, Matthias Bethge, and Felix~A Wichmann.
\newblock Shortcut learning in deep neural networks.
\newblock \emph{Nature Machine Intelligence}, 2\penalty0 (11):\penalty0
  665--673, 2020.

\bibitem[Gontijo-Lopes et~al.(2021)Gontijo-Lopes, Dauphin, and
  Cubuk]{gontijo2021no}
Raphael Gontijo-Lopes, Yann Dauphin, and Ekin~D Cubuk.
\newblock No one representation to rule them all: Overlapping features of
  training methods.
\newblock \emph{arXiv preprint arXiv:2110.12899}, 2021.

\bibitem[Gontijo-Lopes et~al.(2022)Gontijo-Lopes, Dauphin, and
  Cubuk]{gontijo-lopes2022no}
Raphael Gontijo-Lopes, Yann Dauphin, and Ekin~Dogus Cubuk.
\newblock No one representation to rule them all: Overlapping features of
  training methods.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=BK-4qbGgIE3}.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Goyal et~al.(2022)Goyal, Duval, Seessel, Caron, Singh, Misra, Sagun,
  Joulin, and Bojanowski]{goyal2022vision}
Priya Goyal, Quentin Duval, Isaac Seessel, Mathilde Caron, Mannat Singh, Ishan
  Misra, Levent Sagun, Armand Joulin, and Piotr Bojanowski.
\newblock Vision models are more robust and fair when pretrained on uncurated
  images without supervision.
\newblock \emph{arXiv preprint arXiv:2202.08360}, 2022.

\bibitem[Gulrajani and Lopez-Paz(2021)]{gulrajani2021in}
Ishaan Gulrajani and David Lopez-Paz.
\newblock In search of lost domain generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=lQdXeXDoWtI}.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{he-2016}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 770--778, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016{\natexlab{b}}.

\bibitem[Hebert-Johnson et~al.(2018)Hebert-Johnson, Kim, Reingold, and
  Rothblum]{hebertjohnson18a}
Ursula Hebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum.
\newblock Multicalibration: Calibration for the
  ({C}omputationally-identifiable) masses.
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 1939--1948. PMLR,
  10--15 Jul 2018.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, Dean,
  et~al.]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2\penalty0 (7), 2015.

\bibitem[Huang et~al.(2020)Huang, Wang, Xing, and Huang]{huang2020self}
Zeyi Huang, Haohan Wang, Eric~P Xing, and Dong Huang.
\newblock Self-challenging improves cross-domain generalization.
\newblock In \emph{European Conference on Computer Vision}, pages 124--140.
  Springer, 2020.

\bibitem[Ilharco et~al.(2022)Ilharco, Ribeiro, Wortsman, Gururangan, Schmidt,
  Hajishirzi, and Farhadi]{ilharco2022editing}
Gabriel Ilharco, Marco~Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan,
  Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock Editing models with task arithmetic.
\newblock \emph{arXiv preprint arXiv:2212.04089}, 2022.

\bibitem[Jing et~al.(2021)Jing, Vincent, LeCun, and
  Tian]{jing2021understanding}
Li~Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian.
\newblock Understanding dimensional collapse in contrastive self-supervised
  learning.
\newblock \emph{arXiv preprint arXiv:2110.09348}, 2021.

\bibitem[Kirichenko et~al.(2022)Kirichenko, Izmailov, and
  Wilson]{kirichenko2022last}
Polina Kirichenko, Pavel Izmailov, and Andrew~Gordon Wilson.
\newblock Last layer re-training is sufficient for robustness to spurious
  correlations.
\newblock In \emph{ICML 2022: Workshop on Spurious Correlations, Invariance and
  Stability}, 2022.
\newblock URL \url{https://openreview.net/forum?id=THOOBy1uWVH}.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Gao, et~al.]{koh2021wilds}
Pang~Wei Koh, Shiori Sagawa, Henrik Marklund, Sang~Michael Xie, Marvin Zhang,
  Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard~Lanas Phillips,
  Irena Gao, et~al.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning}, pages
  5637--5664. PMLR, 2021.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.
\newblock URL
  \url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}.

\bibitem[Krueger et~al.(2020)Krueger, Caballero, Jacobsen, Zhang, Binas, Priol,
  and Courville]{vrex}
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
  Binas, Remi~Le Priol, and Aaron Courville.
\newblock Out-of-distribution generalization via risk extrapolation (rex).
\newblock \emph{arXiv}, 2020.

\bibitem[Kumar et~al.(2022)Kumar, Raghunathan, Jones, Ma, and
  Liang]{kumar2022finetuning}
Ananya Kumar, Aditi Raghunathan, Robbie~Matthew Jones, Tengyu Ma, and Percy
  Liang.
\newblock Fine-tuning can distort pretrained features and underperform
  out-of-distribution.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=UYneFzXSJWh}.

\bibitem[Li et~al.(2021)Li, Liu, and Bilen]{li2021universal}
Wei-Hong Li, Xialei Liu, and Hakan Bilen.
\newblock Universal representation learning from multiple domains for few-shot
  classification.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9526--9535, 2021.

\bibitem[Li et~al.(2022)Li, Liu, and Bilen]{li2022universal}
Wei-Hong Li, Xialei Liu, and Hakan Bilen.
\newblock Universal representations: A unified look at multiple task and domain
  learning.
\newblock \emph{arXiv preprint arXiv:2204.02744}, 2022.

\bibitem[Minsky and Papert(1969)]{minsky-papert-1969}
M.~Minsky and S.~Papert.
\newblock \emph{Perceptrons}.
\newblock MIT Press, Cambridge, MA, 1969.

\bibitem[Oquab et~al.(2014)Oquab, Bottou, Laptev, and Sivic]{oquab-2014}
Maxime Oquab, L\'eon Bottou, Ivan Laptev, and Josef Sivic.
\newblock Learning and transferring mid-level image representations using
  convolutional neural networks.
\newblock In \emph{Proceedings of Computer Vision and Pattern Recognition
  (CVPR)}. IEEE, 2014.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{papyan-2020}
Vardan Papyan, X.~Y. Han, and David~L. Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, 2020.
\newblock \doi{10.1073/pnas.2015509117}.
\newblock URL \url{https://www.pnas.org/doi/abs/10.1073/pnas.2015509117}.

\bibitem[Pezeshki et~al.(2021)Pezeshki, Kaba, Bengio, Courville, Precup, and
  Lajoie]{pezeshki2021gradient}
Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron~C Courville, Doina Precup,
  and Guillaume Lajoie.
\newblock Gradient starvation: A learning proclivity in neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 1256--1272, 2021.

\bibitem[Ram{\'e} et~al.(2022)Ram{\'e}, Ahuja, Zhang, Cord, Bottou, and
  Lopez-Paz]{rame2022recycling}
Alexandre Ram{\'e}, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, L{\'e}on Bottou,
  and David Lopez-Paz.
\newblock Recycling diverse models for out-of-distribution generalization.
\newblock \emph{arXiv preprint arXiv:2212.10445}, 2022.

\bibitem[Rame et~al.(2022{\natexlab{a}})Rame, Dancette, and
  Cord]{rame2021fishr}
Alexandre Rame, Corentin Dancette, and Matthieu Cord.
\newblock Fishr: Invariant gradient variances for out-of-distribution
  generalization.
\newblock In \emph{International Conference on Machine Learning}, pages
  18347--18377. PMLR, 2022{\natexlab{a}}.

\bibitem[Rame et~al.(2022{\natexlab{b}})Rame, Kirchmeyer, Rahier,
  Rakotomamonjy, Gallinari, and Cord]{rame2022diverse}
Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy,
  Patrick Gallinari, and Matthieu Cord.
\newblock Diverse weight averaging for out-of-distribution generalization.
\newblock \emph{arXiv preprint arXiv:2205.09739}, 2022{\natexlab{b}}.

\bibitem[Rosenblatt(1957)]{rosenblatt-1957}
F.~Rosenblatt.
\newblock The perceptron: A perceiving and recognizing automaton.
\newblock Technical Report 85-460-1, Project PARA, Cornell Aeronautical Lab,
  1957.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and Williams]{rumelhart-1986}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams.
\newblock Learning internal representations by error propagation.
\newblock In \emph{Parallel distributed processing: Explorations in the
  microstructure of cognition}, volume~I, pages 318--362. Bradford Books,
  Cambridge, MA, 1986.

\bibitem[Rusu et~al.(2018)Rusu, Rao, Sygnowski, Vinyals, Pascanu, Osindero, and
  Hadsell]{rusu2018meta}
Andrei~A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu,
  Simon Osindero, and Raia Hadsell.
\newblock Meta-learning with latent embedding optimization.
\newblock \emph{arXiv preprint arXiv:1807.05960}, 2018.

\bibitem[Shwartz-Ziv and Tishby(2017)]{shwartz2017opening}
Ravid Shwartz-Ziv and Naftali Tishby.
\newblock Opening the black box of deep neural networks via information.
\newblock \emph{arXiv preprint arXiv:1703.00810}, 2017.

\bibitem[Simon(1989)]{simon-1989}
J.C. Simon.
\newblock \emph{From Pixels to Features}.
\newblock North Holland, August 1989.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{protonet}
Jake Snell, Kevin Swersky, and Richard Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/cb8da6767461f2812ae4290eac7cbc42-Paper.pdf}.

\bibitem[Steiner et~al.(2021)Steiner, Kolesnikov, Zhai, Wightman, Uszkoreit,
  and Beyer]{steiner2021train}
Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob
  Uszkoreit, and Lucas Beyer.
\newblock How to train your vit? data, augmentation, and regularization in
  vision transformers.
\newblock \emph{arXiv preprint arXiv:2106.10270}, 2021.

\bibitem[Sung et~al.(2018)Sung, Yang, Zhang, Xiang, Torr, and
  Hospedales]{sung2018learning}
Flood Sung, Yongxin Yang, Li~Zhang, Tao Xiang, Philip~HS Torr, and Timothy~M
  Hospedales.
\newblock Learning to compare: Relation network for few-shot learning.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1199--1208, 2018.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy-2014}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2014.
\newblock URL \url{http://arxiv.org/abs/1312.6199}.

\bibitem[Teney et~al.(2022)Teney, Abbasnejad, Lucey, and van~den
  Hengel]{teney2022evading}
Damien Teney, Ehsan Abbasnejad, Simon Lucey, and Anton van~den Hengel.
\newblock Evading the simplicity bias: Training a diverse set of models
  discovers solutions with superior ood generalization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 16761--16772, 2022.

\bibitem[Triantafillou et~al.(2019)Triantafillou, Zhu, Dumoulin, Lamblin, Evci,
  Xu, Goroshin, Gelada, Swersky, Manzagol, et~al.]{triantafillou2019meta}
Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci,
  Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine
  Manzagol, et~al.
\newblock Meta-dataset: A dataset of datasets for learning to learn from few
  examples.
\newblock \emph{arXiv preprint arXiv:1903.03096}, 2019.

\bibitem[Van~Horn et~al.(2018)Van~Horn, Mac~Aodha, Song, Cui, Sun, Shepard,
  Adam, Perona, and Belongie]{van2018inaturalist}
Grant Van~Horn, Oisin Mac~Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
  Hartwig Adam, Pietro Perona, and Serge Belongie.
\newblock The inaturalist species classification and detection dataset.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 8769--8778, 2018.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, koray Kavukcuoglu,
  and Wierstra]{matchingnet}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray Kavukcuoglu, and Daan
  Wierstra.
\newblock {Matching Networks for One Shot Learning}.
\newblock In D~Lee, M~Sugiyama, U~Luxburg, I~Guyon, and R~Garnett, editors,
  \emph{Advances in Neural Information Processing Systems}, volume~29. Curran
  Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf}.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and
  Belongie]{wah2011caltech}
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge
  Belongie.
\newblock The {Caltech-UCSD} {Birds}-200-2011 dataset.
\newblock Technical report, California Institute of Technology, 2011.

\bibitem[Wald et~al.(2021)Wald, Feder, Greenfeld, and
  Shalit]{wald2021calibration}
Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit.
\newblock On calibration and out-of-domain generalization.
\newblock \emph{arXiv preprint arXiv:2102.10395}, 2021.

\bibitem[Wang et~al.(2022)Wang, Frank, Pfahringer, Mayo, and
  Holmes]{wang2022cross}
Hongyu Wang, Eibe Frank, Bernhard Pfahringer, Michael Mayo, and Geoffrey
  Holmes.
\newblock Cross-domain few-shot meta-learning using stacking.
\newblock \emph{arXiv preprint arXiv:2205.05831}, 2022.

\bibitem[Wang et~al.(2020)Wang, Demiris, and Ciliberto]{wang2020structured}
Ruohan Wang, Yiannis Demiris, and Carlo Ciliberto.
\newblock Structured prediction for conditional meta-learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2587--2598, 2020.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes,
  Morcos, Namkoong, Farhadi, Carmon, Kornblith, et~al.]{wortsman2022model}
Mitchell Wortsman, Gabriel Ilharco, Samir~Ya Gadre, Rebecca Roelofs, Raphael
  Gontijo-Lopes, Ari~S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon,
  Simon Kornblith, et~al.
\newblock Model soups: averaging weights of multiple fine-tuned models improves
  accuracy without increasing inference time.
\newblock In \emph{International Conference on Machine Learning}, pages
  23965--23998. PMLR, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Lopez-Paz, and Bottou]{zhang2022rich}
Jianyu Zhang, David Lopez-Paz, and Leon Bottou.
\newblock Rich feature construction for the optimization-generalization
  dilemma.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pages 26397--26411. PMLR,
  17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/zhang22u.html}.

\end{thebibliography}
