\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andreas et~al.(2017)Andreas, Neil, Gal, Ivan, Abhinav, and
  Serge~J.]{Veit:2017}
Andreas, V., Neil, A., Gal, C., Ivan, K., Abhinav, G., and Serge~J., B.
\newblock Learning from noisy large-scale datasets with minimal supervision.
\newblock In \emph{CVPR}, pp.\  6575--6583, 2017.

\bibitem[Belkin et~al.(2018)Belkin, Hsu, and Mitra]{belkin:2018}
Belkin, M., Hsu, D.~J., and Mitra, P.
\newblock Overfitting or perfect fitting? risk bounds for classification and
  regression rules that interpolate.
\newblock In \emph{NeurIPS}, pp.\  2300--2311, 2018.

\bibitem[Brodley \& Friedl(1999)Brodley and Friedl]{Brodley:1999}
Brodley, C.~E. and Friedl, M.~A.
\newblock Identifying mislabeled training data.
\newblock \emph{Journal of Artificial Intelligence Research}, 11:\penalty0
  131--167, 1999.

\bibitem[Chaudhuri \& Dasgupta(2014)Chaudhuri and Dasgupta]{chaud:2014}
Chaudhuri, K. and Dasgupta, S.
\newblock Rates of convergence for nearest neighbor classification.
\newblock In \emph{NeurIPS}, pp.\  3437--3445, 2014.

\bibitem[Chen \& Sun(2006)Chen and Sun]{chen:2006}
Chen, D.-R. and Sun, T.
\newblock Consistency of multiclass empirical risk minimization methods based
  on convex loss.
\newblock \emph{Journal of Machine Learning Research}, 7\penalty0
  (11):\penalty0 2435--2447, 2006.

\bibitem[Cheng et~al.(2020)Cheng, Liu, Ramamohanarao, and
  Tao]{cheng2020learning}
Cheng, J., Liu, T., Ramamohanarao, K., and Tao, D.
\newblock Learning with bounded instance-and label-dependent label noise.
\newblock In \emph{ICML}, 2020.

\bibitem[Crammer \& Lee(2010)Crammer and Lee]{Crammer:2010}
Crammer, K. and Lee, D.~D.
\newblock Learning via gaussian herding.
\newblock In \emph{NeurIPS}, pp.\  451--459. 2010.

\bibitem[Crammer et~al.(2009)Crammer, Kulesza, and Dredze]{Crammer:2009}
Crammer, K., Kulesza, A., and Dredze, M.
\newblock Adaptive regularization of weight vectors.
\newblock In \emph{NeurIPS}, pp.\  414--422, 2009.

\bibitem[Dan et~al.(2019)Dan, Kimin, and Mantas]{Hendrycks:19}
Dan, H., Kimin, L., and Mantas, M.
\newblock Using pre-training can improve model robustness and uncertainty.
\newblock In \emph{ICML}, pp.\  2712--2721, 2019.

\bibitem[Devansh et~al.(2017)Devansh, Stanislaw, Nicolas, David, Emmanuel,
  Maxinder, Tegan, Asja, Aaron, Yoshua, and Simon]{Arpit_Memorization_ICML2017}
Devansh, A., Stanislaw, K.~J., Nicolas, B., David, K., Emmanuel, B., Maxinder,
  S.~K., Tegan, M., Asja, F., Aaron, C.~C., Yoshua, B., and Simon, L.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{ICML}, pp.\  233--242, 2017.

\bibitem[Frénay \& Verleysen(2014)Frénay and Verleysen]{Frenay:2014}
Frénay, B. and Verleysen, M.
\newblock Classification in the presence of label noise: A survey.
\newblock \emph{Neural Networks and Learning Systems, IEEE Transactions on},
  25\penalty0 (5):\penalty0 845--869, 2014.

\bibitem[Gao et~al.(2016)Gao, Yang, and Zhou]{gao:2016}
Gao, W., Yang, B.-B., and Zhou, Z.-H.
\newblock On the resistance of nearest neighbor to random noisy labels.
\newblock \emph{arXiv}, pp.\  arXiv--1607, 2016.

\bibitem[Ghosh et~al.(2015)Ghosh, Manwani, and
  Sastry]{Ghosh_riskmini_2015_NIPS}
Ghosh, A., Manwani, N., and Sastry, P.
\newblock Making risk minimization tolerant to label noise.
\newblock \emph{Neurocomput}, 160:\penalty0 93--107, 2015.

\bibitem[Goldberger \& Ben-Reuven(2017)Goldberger and
  Ben-Reuven]{goldberger_Adaptation_ICLR2017}
Goldberger, J. and Ben-Reuven, E.
\newblock Training deep neural-networks using a noise adaptation layer.
\newblock In \emph{ICLR}, 2017.

\bibitem[Han et~al.(2018)Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{Han:2018}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I.~W., and Sugiyama,
  M.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In \emph{NeurIPS}, pp.\  8536--8546, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016identity}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{ECCV}, pp.\  630--645, 2016.

\bibitem[Jacob et~al.(2017)Jacob, Pang~Wei, and
  Percy~S.]{Steinhardt_DataPoison_NIPS2017}
Jacob, S., Pang~Wei, K., and Percy~S., L.
\newblock Certified defenses for data poisoning attacks.
\newblock In \emph{NeurIPS}, pp.\  3520--3532, 2017.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and Li]{jiang:2018}
Jiang, L., Zhou, Z., Leung, T., Li, J., and Li, F.-F.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In \emph{ICML}, pp.\  2304--2313, 2018.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{cifar100}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.
\newblock URL
  \url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{mnist}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.
\newblock URL
  \url{https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=726791}.

\bibitem[Lee et~al.(2019)Lee, Yun, Lee, Lee, Li, and Shin]{lee2019robust}
Lee, K., Yun, S., Lee, K., Lee, H., Li, B., and Shin, J.
\newblock Robust inference via generative classifiers for handling noisy
  labels.
\newblock In \emph{ICML}, 2019.

\bibitem[Liu et~al.(2019)Liu, Jiang, He, Chen, Liu, Gao, and Han]{Radam}
Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and Han, J.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock In \emph{ICLR}, 2019.

\bibitem[Long \& Servedio(2010)Long and Servedio]{long_2010_random_JMLR}
Long, P.~M. and Servedio, R.~A.
\newblock Random classification noise defeats all convex potential boosters.
\newblock \emph{Machine learning}, 78\penalty0 (3):\penalty0 287--304, 2010.

\bibitem[Malach \& Shalev-Shwartz(2017)Malach and Shalev-Shwartz]{Malach:2017}
Malach, E. and Shalev-Shwartz, S.
\newblock Decoupling "when to update" from "how to update".
\newblock In \emph{NeurIPS}, pp.\  960--970, 2017.

\bibitem[Nagarajan et~al.(2013)Nagarajan, Ambuj, Inderjit~S., and
  Pradeep]{natarajan:2013}
Nagarajan, N., Ambuj, T., Inderjit~S., D., and Pradeep, R.
\newblock Learning with noisy labels.
\newblock In \emph{NeurIPS}, 2013.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Menon, Nock, and Qu]{Patrini:2017}
Patrini, G., Rozza, A., Menon, A., Nock, R., and Qu, L.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{CVPR}, pp.\  2233--2241, 2017.

\bibitem[Qi et~al.(2017)Qi, Su, Mo, and Guibas]{PointNet}
Qi, C.~R., Su, H., Mo, K., and Guibas, L.~J.
\newblock Pointnet: Deep learning on point sets for 3d classification and
  segmentation.
\newblock In \emph{CVPR}, pp.\  652--660, 2017.

\bibitem[Qiao et~al.(2019)Qiao, Duan, and Cheng]{qiao:2019}
Qiao, X., Duan, J., and Cheng, G.
\newblock Rates of convergence for large-scale nearest neighbor classification.
\newblock In \emph{NeurIPS}, pp.\  10768--10779, 2019.

\bibitem[Reed et~al.(2014)Reed, Lee, Anguelov, Szegedy, Erhan, and
  Rabinovich]{reed2014training}
Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and Rabinovich, A.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock \emph{arXiv}, pp.\  arXiv--1412, 2014.

\bibitem[Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun]{ren2018reweight}
Ren, M., Zeng, W., Yang, B., and Urtasun, R.
\newblock Learning to reweight examples for robust deep learning.
\newblock In \emph{ICML}, pp.\  4334--4343, 2018.

\bibitem[Shen \& Sanghavi(2019)Shen and Sanghavi]{shen2019trim}
Shen, Y. and Sanghavi, S.
\newblock Learning with bad training data via iterative trimmed loss
  minimization.
\newblock In \emph{ICML}, pp.\  5739--5748, 2019.

\bibitem[Smyth et~al.(1994)Smyth, Fayyad, Burl, Perona, and Baldi]{Smyth:1994}
Smyth, P., Fayyad, U., Burl, M., Perona, P., and Baldi, P.
\newblock Inferring ground truth from subjective labelling of venus images.
\newblock In \emph{NeurIPS}, pp.\  1085--1092, 1994.

\bibitem[Tanaka et~al.(2018)Tanaka, Ikami, Yamasaki, and
  Aizawa]{tanaka2018jointlearn}
Tanaka, D., Ikami, D., Yamasaki, T., and Aizawa, K.
\newblock Joint optimization framework for learning with noisy labels.
\newblock In \emph{CVPR}, pp.\  5552--5560, 2018.

\bibitem[Thulasidasan et~al.(2019)Thulasidasan, Bhattacharya, Bilmes,
  Chennupati, and Mohd-Yusof]{sunil:2019}
Thulasidasan, S., Bhattacharya, T., Bilmes, J., Chennupati, G., and Mohd-Yusof,
  J.
\newblock Combating label noise in deep learning using abstention.
\newblock In \emph{ICML}, pp.\  6234--6243, 2019.

\bibitem[Tsybakov(2004)]{tsybakov2004optimal}
Tsybakov, A.~B.
\newblock Optimal aggregation of classifiers in statistical learning.
\newblock \emph{The Annals of Statistics}, 32\penalty0 (1):\penalty0 135--166,
  2004.

\bibitem[Van~Rooyen et~al.(2015)Van~Rooyen, Menon, and Williamson]{Rooyen:2015}
Van~Rooyen, B., Menon, A., and Williamson, R.~C.
\newblock Learning with symmetric label noise: The importance of being
  unhinged.
\newblock In \emph{NeurIPS}, pp.\  10--18, 2015.

\bibitem[Volodymyr \& Geoffrey~E.(2012)Volodymyr and
  Geoffrey~E.]{Mnih_Aerial_Image_ICML2012}
Volodymyr, M. and Geoffrey~E., H.
\newblock Learning to label aerial images from noisy data.
\newblock In \emph{ICML}, pp.\  567--574, 2012.

\bibitem[Wang \& Chaudhuri(2018)Wang and Chaudhuri]{Yizhen:2018}
Wang, J. and Chaudhuri.
\newblock Analyzing the robustness of nearest neighbors to adversarial
  examples.
\newblock In \emph{ICML}, pp.\  5133--5142, 2018.

\bibitem[Wang et~al.(2018)Wang, Liu, Ma, Bailey, Zha, Song, and
  Xia]{wang2018openset}
Wang, Y., Liu, W., Ma, X., Bailey, J., Zha, H., Song, L., and Xia, S.-T.
\newblock Iterative learning with open-set noisy labels.
\newblock In \emph{CVPR}, pp.\  8688--8696, 2018.

\bibitem[{Wu} et~al.(2018){Wu}, {He}, {Sun}, and
  {Tan}]{Wu_Face_Noise_IEEETrans2018}
{Wu}, X., {He}, R., {Sun}, Z., and {Tan}, T.
\newblock A light {CNN} for deep face representation with noisy labels.
\newblock \emph{IEEE Transactions on Information Forensics and Security},
  13:\penalty0 2884--2896, 2018.

\bibitem[{Wu} et~al.(2015){Wu}, {Song}, {Khosla}, {Yu}, {Zhang}, {Tang}, and
  {Xiao}]{modelnet40}
{Wu}, Z., {Song}, S., {Khosla}, A., {Yu}, F., {Zhang}, L., {Tang}, X., and
  {Xiao}, J.
\newblock 3d shapenets: A deep representation for volumetric shape modeling.
\newblock In \emph{CVPR}, pp.\  1912--1920, 2015.

\bibitem[Xiao et~al.(2015)Xiao, Xia, Yang, Huang, and Wang]{Xiao:2015}
Xiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In \emph{CVPR}, pp.\  2691--2699, 2015.

\bibitem[Yan et~al.(2014)Yan, Rosales, Fung, Subramanian, and
  Jennifer]{yan:2014}
Yan, Y., Rosales, R., Fung, G., Subramanian, R., and Jennifer, D.
\newblock Learning from multiple annotators with varying expertise.
\newblock \emph{Machine learning}, 95\penalty0 (3):\penalty0 291--327, 2014.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{Zhang_noise_ICLR2017}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zhang \& Sabuncu(2018)Zhang and Sabuncu]{zhang2018general_loss}
Zhang, Z. and Sabuncu, M.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In \emph{NeurIPS}, pp.\  8778--8788, 2018.

\end{thebibliography}
