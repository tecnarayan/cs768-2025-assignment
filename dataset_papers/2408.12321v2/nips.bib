


% incollection (has an editor, title, and possibly a booktitle)

% and incol coz we have a BLANK chapter - due to presence of editor
%atIncollection{Kong:2006:IEC:887006.887011,
%  author =     {Kong, Wei-Chang},
%  editor =     {Theerasak Thanasankit},
%  title =      "The title"
%  booktitle =  {E-commerce and cultural values (Incol-coz-blank-chap)},
%  year =       {2006},
%  address =    {Hershey, PA, USA},
%  publisher =  {IGI Publishing},
%  url =        {http://portal.acm.org/citation.cfm?id=887006.887010},
%  type =       {Type!},
%  chapter =    {},
%  pages =      {51--74},
%  numpages =   {24},
%  acmid =      {887010},
%  isbn =       {1-59140-056-2},
%  number =     "",
%  month =      "",
%  note =       "",
%}


@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}
@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author = {Ji, Ziwei and Lee, Nayeon and Frieske, Rita and others},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  year={2023}
}



@article{zhang2023siren,
  title={Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models},
  author = {Zhang, Yue and Li, Yafu and Cui, Leyang and others},
  journal={arXiv preprint arXiv:2309.01219},
  year={2023}
}



@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and others},
  journal={arXiv preprint arXiv:2311.05232},
  year={2023}
}



@article{li2023otter,
  title={Otter: A multi-modal model with in-context instruction tuning},
  author = {Li, Bo and Zhang, Yuanhan and Chen, Liangyu and others},
  journal={arXiv preprint arXiv:2305.03726},
  year={2023}
}



@inproceedings{CHAIR,
  title={Object Hallucination in Image Captioning},
  author = {Rohrbach, Anna and Hendricks, Lisa Anne and Burns, Kaylee and others},
  booktitle={EMNLP},
  year={2018}
}



@inproceedings{lu2018neural,
  title={Neural baby talk},
  author = {Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and others},
  booktitle={CVPR},
  year={2018}
}



@article{FAITHScore,
  title={FAITHSCORE: Evaluating Hallucinations in Large Vision-Language Models},
  author = {Jing, Liqiang and Li, Ruosen and Chen, Yunmo and others},
  journal={arXiv preprint arXiv:2311.01477},
  year={2023}
}



@inproceedings{factscore,
  title={FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation},
  author = {Min, Sewon and Krishna, Kalpesh and Lyu, Xinxi and others},
  booktitle={EMNLP},
  year={2023}
}



@article{GPT-4,
      title={GPT-4 Technical Report},
      author = {OpenAI},
      journal={arXiv preprint arXiv:2303.08774},
      year={2023}
}



@article{HallE-Switch,
  title={HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision Language Models for Detailed Caption},
  author = {Zhai, Bohan and Yang, Shijia and Zhao, Xiangchen and others},
  journal={arXiv preprint arXiv:2310.01779},
  year={2023}
}



@article{liu2023mitigating,
  title={Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning},
  author = {Liu, Fuxiao and Lin, Kevin and Li, Linjie and others},
  journal={arXiv preprint arXiv:2306.14565},
  year={2023}
}



@article{gunjal2023detecting,
  title={Detecting and preventing hallucinations in large vision language models},
  author = {Gunjal, Anisha and Yin, Jihan and Bas, Erhan},
  journal={arXiv preprint arXiv:2308.06394},
  year={2023}
}



@article{wang2023evaluation,
  title={Evaluation and analysis of hallucination in large vision-language models},
  author = {Wang, Junyang and Zhou, Yiyang and Xu, Guohai and others},
  journal={arXiv preprint arXiv:2308.15126},
  year={2023}
}



@article{InstructBLIP,
      title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
      author = {Wenliang Dai and Junnan Li and Dongxu Li and others},
      year={2023},
      journal={arXiv preprint arXiv:2305.06500}
}



@article{LLaMA,
  title={Llama: Open and efficient foundation language models},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}



@inproceedings{LoRA,
  author = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  others},
  title        = {LoRA: Low-Rank Adaptation of Large Language Models},
  booktitle    = {ICLR},
  year         = {2022}
}



@inproceedings{POPE,
  title={Evaluating object hallucination in large vision-language models},
  author = {Li, Yifan and Du, Yifan and Zhou, Kun and others},
  booktitle={EMNLP},
  year={2023}
}



@article{NOPE,
  title={Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models},
  author = {Lovenia, Holy and Dai, Wenliang and Cahyawijaya, Samuel and others},
  journal={arXiv preprint arXiv:2310.05338},
  year={2023}
}



@inproceedings{CIEM,
  title={CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning},
  author = {Hu, Hongyu and Zhang, Jiyuan and Zhao, Minyi and others},
  booktitle={NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
  year={2023}
}



@article{HallusionBench,
      title={HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination \& Visual Illusion in Large Vision-Language Models},
      author = {Tianrui Guan and Fuxiao Liu and Xiyang Wu and others},
      journal={arXiv preprint arXiv:2310.14566},
      year={2023}
}



@article{Bingo,
  title={Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges},
  author = {Cui, Chenhang and Zhou, Yiyang and Yang, Xinyu and others},
  journal={arXiv preprint arXiv:2311.03287},
  year={2023}
}



@article{LLaVA-1.5,
  title={Improved baselines with visual instruction tuning},
  author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and others},
  journal={arXiv preprint arXiv:2310.03744},
  year={2023}
}



@misc{GPT-4V,
  author = {OpenAI},
  title = {GPT-4V(ision) System Card},
  year = {2023},
  howpublished = {\url{https://cdn.openai.com/papers/GPTV_System_Card.pdf}}
}



@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}



@article{lee2023volcano,
  title={Volcano: Mitigating Multimodal Hallucination through Self-Feedback Guided Revision},
  author = {Lee, Seongyun and Park, Sue Hyun and Jo, Yongrae and others},
  journal={arXiv preprint arXiv:2311.07362},
  year={2023}
}



@article{yin2023woodpecker,
  title={Woodpecker: Hallucination correction for multimodal large language models},
  author = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and others},
  journal={arXiv preprint arXiv:2310.16045},
  year={2023}
}


@inproceedings{
zhou2023analyzing,
title={Analyzing and Mitigating Object Hallucination in Large Vision-Language Models},
author={Yiyang Zhou and Chenhang Cui and Jaehong Yoon and others},
booktitle={NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
year={2023}
}



@article{MiniGPT-4,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and others},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}



@article{wang2023vigc,
  title={Vigc: Visual instruction generation and correction},
  author = {Wang, Bin and Wu, Fan and Han, Xiao and others},
  journal={arXiv preprint arXiv:2308.12714},
  year={2023}
}



@article{sun2023aligning,
  title={Aligning large multimodal models with factually augmented rlhf},
  author = {Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and others},
  journal={arXiv preprint arXiv:2309.14525},
  year={2023}
}



@article{lu2023evaluation,
  title={Evaluation and mitigation of agnosia in multimodal large language models},
  author = {Lu, Jiaying and Rao, Jinmeng and Chen, Kezhen and others},
  journal={arXiv preprint arXiv:2309.04041},
  year={2023}
}



@article{you2023ferret,
      title={Ferret: Refer and Ground Anything Anywhere at Any Granularity},
      author = {Haoxuan You and Haotian Zhang and Zhe Gan and others},
      year={2023},
      journal={arXiv preprint arXiv:2310.07704}
}



@article{zhao2023beyond,
  title={Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization},
  author = {Zhao, Zhiyuan and Wang, Bin and Ouyang, Linke and others},
  journal={arXiv preprint arXiv:2311.16839},
  year={2023}
}



@article{leng2023mitigating,
  title={Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding},
  author = {Leng, Sicong and Zhang, Hang and Chen, Guanzheng and others},
  journal={arXiv preprint arXiv:2311.16922},
  year={2023}
}



@article{liu2023mmbench,
  title={MMBench: Is Your Multi-modal Model an All-around Player?},
  author = {Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and others},
  journal={arXiv preprint arXiv:2307.06281},
  year={2023}
}



@inproceedings{LLaVA,
  title={Visual instruction tuning},
  author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and others},
  booktitle={NeurIPS},
  year={2023}
}



@inproceedings{RLHF,
 author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and others},
 booktitle = {NeurIPS},
 title = {Learning to summarize with human feedback},
 volume = {33},
 year = {2020}
}



@article{DPO,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and others},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}



@inproceedings{ViT,
  author = {Alexey Dosovitskiy and
                  Lucas Beyer and
                  Alexander Kolesnikov and
                  others},
  title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition
                  at Scale},
  booktitle    = {ICLR},
  year         = {2021}
}



@inproceedings{CLIP,
  title={Learning transferable visual models from natural language supervision},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and others},
  booktitle={ICML},
  year={2021}
}



@inproceedings{dai2023plausible,
  title={Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training},
  author = {Dai, Wenliang and Liu, Zihan and Ji, Ziwei and others},
  booktitle={EACL},
  year={2023}
}



@article{chen2023minigpt,
  title={Minigpt-v2: large language model as a unified interface for vision-language multi-task learning},
  author = {Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and others},
  journal={arXiv preprint arXiv:2310.09478},
  year={2023}
}



@inproceedings{BLIP2,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and others},
  booktitle={ICML},
  year={2023}
}



@inproceedings{stahlberg2019nmt,
  title={On NMT Search Errors and Model Errors: Cat Got Your Tongue?},
  author = {Stahlberg, Felix and Byrne, Bill},
  booktitle={EMNLP-IJCNLP},
  year={2019}
}



@inproceedings{holtzman2019curious,
  title={The Curious Case of Neural Text Degeneration},
  author = {Holtzman, Ari and Buys, Jan and Du, Li and others},
  booktitle={ICLR},
  year={2020}
}



@inproceedings{fan2018hierarchical,
  title={Hierarchical Neural Story Generation},
  author = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
  booktitle={ACL},
  year={2018}
}



@article{chuang2023dola,
  title={Dola: Decoding by contrasting layers improves factuality in large language models},
  author = {Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and others},
  journal={arXiv preprint arXiv:2309.03883},
  year={2023}
}



@article{wang2023data,
  title={Data Management For Large Language Models: A Survey},
  author = {Wang, Zige and Zhong, Wanjun and Wang, Yufei and others},
  journal={arXiv preprint arXiv:2312.01700},
  year={2023}
}



@inproceedings{Flamingo,
 author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and others},
 booktitle = {NeurIPS},
 title = {Flamingo: a Visual Language Model for Few-Shot Learning},
 volume = {35},
 year = {2022}
}



@article{gao2023llama,
  title={Llama-adapter v2: Parameter-efficient visual instruction model},
  author = {Gao, Peng and Han, Jiaming and Zhang, Renrui and others},
  journal={arXiv preprint arXiv:2304.15010},
  year={2023}
}



@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}



@inproceedings{dziri2021neural,
  title={Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding},
  author = {Dziri, Nouha and Madotto, Andrea and Zaiane, Osmar R and others},
  booktitle={EMNLP},
  year={2021}
}



@inproceedings{long2022vision,
  title={Vision-and-language pretrained models: A survey},
  author = {Long, Siqu and Cao, Feiqi and Han, Soyeon Caren and others},
  booktitle={IJCAI},
  year={2022}
}



@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and others},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}



@article{wang2023llm,
  title={An llm-free multi-dimensional benchmark for mllms hallucination evaluation},
  author = {Wang, Junyang and Wang, Yuhang and Xu, Guohai and others},
  journal={arXiv preprint arXiv:2311.07397},
  year={2023}
}



@article{huang2023opera,
  title={OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation},
  author = {Huang, Qidong and Dong, Xiaoyi and Zhang, Pan and others},
  journal={arXiv preprint arXiv:2311.17911},
  year={2023}
}



@article{yu2023rlhf,
  title={RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback},
  author = {Yu, Tianyu and Yao, Yuan and Zhang, Haoye and others},
  journal={arXiv preprint arXiv:2312.00849},
  year={2023}
}



@article{wang2023mitigating,
  title={Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites},
  author = {Wang, Lei and He, Jiabang and Li, Shenshen and others},
  journal={arXiv preprint arXiv:2312.01701},
  year={2023}
}



@article{chen2023mitigating,
  title={Mitigating Hallucination in Visual Language Models with Visual Supervision},
  author = {Chen, Zhiyang and Zhu, Yousong and Zhan, Yufei and others},
  journal={arXiv preprint arXiv:2311.16479},
  year={2023}
}



@article{yu2023hallucidoctor,
  title={HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data},
  author = {Yu, Qifan and Li, Juncheng and Wei, Longhui and others},
  journal={arXiv preprint arXiv:2311.13614},
  year={2023}
}



@article{jiang2023hallucination,
  title={Hallucination Augmented Contrastive Learning for Multimodal Large Language Model},
  author = {Jiang, Chaoya and Xu, Haiyang and Dong, Mengfan and others},
  journal={arXiv preprint arXiv:2312.06968},
  year={2023}
}



@article{Chen2023PVIT,
  author = {Chi Chen and
                  Ruoyu Qin and
                  Fuwen Luo and
                  others},
  title        = {Position-Enhanced Visual Instruction Tuning for Multimodal Large Language
                  Models},
  journal      = {arXiv preprint arXiv:2308.13437},
  year         = {2023}
}



@inproceedings{Girdhar2023ImageBind,
  author = {Rohit Girdhar and
                  Alaaeldin El{-}Nouby and
                  Zhuang Liu and
                  others},
  title        = {ImageBind One Embedding Space to Bind Them All},
  booktitle    = {{CVPR}},
  year         = {2023}
}



@article{Han2023ImageBindLLM,
  author = {Jiaming Han and
                  Renrui Zhang and
                  Wenqi Shao and
                  others
                  },
  title        = {ImageBind-LLM: Multi-modality Instruction Tuning},
  journal      = {arXiv preprint arXiv:2309.03905},
  year         = {2023}
}



@article{Lin2023VideoLLaVA,
  title={Video-LLaVA: Learning United Visual Representation by Alignment Before Projection},
  author = {Lin, Bin and Zhu, Bin and Ye, Yang and others},
  journal={arXiv preprint arXiv:2311.10122},
  year={2023}
}



@article{Liu2023LLaVAPlus,
  author = {Shilong Liu and
                  Hao Cheng and
                  Haotian Liu and
                  others},
  title        = {LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents},
  journal      = {arXiv preprint arXiv:2311.05437},
  year         = {2023}
}



@inproceedings{Cheng23MMEidt,
  author = {Siyuan Cheng and
                  Bozhong Tian and
                  Qingbin Liu and others},
  title        = {Can We Edit Multimodal Large Language Models?},
  booktitle    = {{EMNLP}},
  year         = {2023}
}



@inproceedings{andonian2022robust,
  title={Robust cross-modal representation learning with progressive self-distillation},
  author = {Andonian, Alex and Chen, Shixing and Hamid, Raffay},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}



@article{jain2023vcoder,
  title={VCoder: Versatile Vision Encoders for Multimodal Large Language Models},
  author = {Jain, Jitesh and Yang, Jianwei and Shi, Humphrey},
  journal={arXiv preprint arXiv:2312.14233},
  year={2023}
}



@inproceedings{cho2022fine,
  title={Fine-grained image captioning with clip reward},
  author = {Cho, Jaemin and Yoon, Seunghyun and Kale, Ajinkya and others},
  journal={arXiv preprint arXiv:2205.13115},
  booktitle={Findings of NAACL},
  year={2022}
}



@article{yin2023survey,
  title={A Survey on Multimodal Large Language Models},
  author = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and others},
  journal={arXiv preprint arXiv:2306.13549},
  year={2023}
}



@article{bai2023qwen,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author = {Bai, Jinze and Bai, Shuai and Yang, Shusheng and others},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}



article{li2023monkey,
  title={Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models},
  author = {Li, Zhang and Yang, Biao and Liu, Qiang and others},
  journal={arXiv preprint arXiv:2311.06607},
  year={2023}
}

@article{Gemini,
  title={Gemini: A Family of Highly Capable Multimodal Models},
  author={Gemini Team Google Rohan Anil and Sebastian Borgeaud and Yonghui Wu and Jean-Baptiste Alayrac and Jiahui Yu and Radu Soricut and Johan Schalkwyk...},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.11805},
}

@article{chen2023internvl,
  title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
  author = {Chen, Zhe and Wu, Jiannan and Wang, Wenhai and others},
  journal={arXiv preprint arXiv:2312.14238},
  year={2023}
}



@article{zhao2023enhancing,
  title={Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model},
  author = {Zhao, Yongqiang and Li, Zhenyu and Jin, Zhi and others},
  journal={arXiv preprint arXiv:2310.20357},
  year={2023}
}

@article{zhang2023recognize,
  title={Recognize Anything: A Strong Image Tagging Model},
  author={Zhang, Youcai and Huang, Xinyu and Ma, Jinyu and others},
  journal={arXiv preprint arXiv:2306.03514},
  year={2023}
}

@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})


@article{mplugdocowl,
  author       = {Jiabo Ye and
                  Anwen Hu and
                  Haiyang Xu and
                  Qinghao Ye and
                  Ming Yan and
                  Yuhao Dan and
                  Chenlin Zhao and
                  Guohai Xu and
                  Chenliang Li and
                  Junfeng Tian and
                  Qian Qi and
                  Ji Zhang and
                  Fei Huang},
  title        = {mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document
                  Understanding},
  journal      = {CoRR},
  volume       = {abs/2307.02499},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.02499},
  doi          = {10.48550/ARXIV.2307.02499},
  eprinttype    = {arXiv},
  eprint       = {2307.02499},
  timestamp    = {Mon, 11 Sep 2023 12:05:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2307-02499.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Brown2020gpt3,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and T. J. Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.14165},
  url={https://api.semanticscholar.org/CorpusID:218971783}
}

@article{OpenAI2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.08774},
  url={https://api.semanticscholar.org/CorpusID:257532815}
}

@inproceedings{2023GPT4VisionSC,
  title={GPT-4V(ision) System Card},
  author={OpenAI},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:263218031}
}

@article{SurveyHAL,
  title={A Survey on Hallucination in Large Vision-Language Models},
  author={Hanchao Liu and Wenyuan Xue and Yifei Chen and Dapeng Chen and Xiutian Zhao and Ke Wang and Liping Hou and Rong-Zhi Li and Wei Peng},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.00253},
  url={https://api.semanticscholar.org/CorpusID:267365472}
}
@article{Touvron2023Llama2,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Hugo Touvron and Louis Martin and Kevin R. Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Daniel M. Bikel and Lukas Blecher and Cristian Cant{\'o}n Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony S. Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel M. Kloumann and A. V. Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and R. Subramanian and Xia Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zhengxu Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  journal={ArXiv},
  year={2023},
  volume={abs/2307.09288},
  url={https://api.semanticscholar.org/CorpusID:259950998}
}

@article{Li2023BLIP2,
  title={BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author={Junnan Li and Dongxu Li and Silvio Savarese and Steven C. H. Hoi},
  journal={ArXiv},
  year={2023},
  volume={abs/2301.12597},
  url={https://api.semanticscholar.org/CorpusID:256390509}
}

@article{Dai2023InstructBLIP,
  title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning},
  author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Albert Li and Pascale Fung and Steven C. H. Hoi},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.06500},
  url={https://api.semanticscholar.org/CorpusID:258615266}
}

@article{ye2023mplugowl,
  title={mplug-owl: Modularization empowers large language models with multimodality},
  author={Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal={arXiv preprint arXiv:2304.14178},
  year={2023}
}

@article{Gao2023LLaMAAdapterV2,
  title={LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model},
  author={Peng Gao and Jiaming Han and Renrui Zhang and Ziyi Lin and Shijie Geng and Aojun Zhou and W. Zhang and Pan Lu and Conghui He and Xiangyu Yue and Hongsheng Li and Yu Jiao Qiao},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.15010},
  url={https://api.semanticscholar.org/CorpusID:258418343}
}

@article{Zhu2023MiniGPT4,
  title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
  author={Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.10592},
  url={https://api.semanticscholar.org/CorpusID:258291930}
}

@article{Liu2023Llava,
  title={Visual Instruction Tuning},
  author={Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.08485},
  url={https://api.semanticscholar.org/CorpusID:258179774}
}

@article{Liu2023LLava15,
  title={Improved Baselines with Visual Instruction Tuning},
  author={Haotian Liu and Chunyuan Li and Yuheng Li and Yong Jae Lee},
  journal={ArXiv},
  year={2023},
  volume={abs/2310.03744},
  url={https://api.semanticscholar.org/CorpusID:263672058}
}

@article{Sun2023LLavaRlhf,
  title={Aligning Large Multimodal Models with Factually Augmented RLHF},
  author={Zhiqing Sun and Sheng Shen and Shengcao Cao and Haotian Liu and Chunyuan Li and Yikang Shen and Chuang Gan and Liangyan Gui and Yu-Xiong Wang and Yiming Yang and Kurt Keutzer and Trevor Darrell},
  journal={ArXiv},
  year={2023},
  volume={abs/2309.14525},
  url={https://api.semanticscholar.org/CorpusID:262824780}
}

@article{Chen2023Shikra,
  title={Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic},
  author={Ke Chen and Zhao Zhang and Weili Zeng and Richong Zhang and Feng Zhu and Rui Zhao},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.15195},
  url={https://api.semanticscholar.org/CorpusID:259262082}
}

@inproceedings{Driess2023PaLME,
  title={PaLM-E: An Embodied Multimodal Language Model},
  author={Danny Driess and F. Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Ho Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Peter R. Florence},
  booktitle={International Conference on Machine Learning},
  year={2023},
  url={https://api.semanticscholar.org/CorpusID:257364842}
}


@inproceedings{xu2023mplug2,
  title={mPLUG-2: A modularized multi-modal foundation model across text, image and video},
  author={Xu, Haiyang and Ye, Qinghao and Yan, Ming and Shi, Yaya and Ye, Jiabo and Xu, Yuanhong and Li, Chenliang and Bi, Bin and Qian, Qi and Wang, Wei and others},
  booktitle={International Conference on Machine Learning},
  year={2023},
}

@article{liang2022gap,
  title={Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning},
  author={Liang, Victor Weixin and Zhang, Yuhui and Kwon, Yongchan and Yeung, Serena and Zou, James Y},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17612--17625},
  year={2022}
}


@article{Chowdhery2022PaLM,
  title={PaLM: Scaling Language Modeling with Pathways},
  author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam M. Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Benton C. Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garc{\'i}a and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark D{\'i}az and Orhan Firat and Michele Catasta and Jason Wei and Kathleen S. Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
  journal={J. Mach. Learn. Res.},
  year={2022},
  volume={24},
  pages={240:1-240:113},
  url={https://api.semanticscholar.org/CorpusID:247951931}
}

@article{wang2022git,
  title={Git: A generative image-to-text transformer for vision and language},
  author={Wang, Jianfeng and Yang, Zhengyuan and Hu, Xiaowei and Li, Linjie and Lin, Kevin and Gan, Zhe and Liu, Zicheng and Liu, Ce and Wang, Lijuan},
  journal={arXiv preprint arXiv:2205.14100},
  year={2022}
}
@misc{rajbhandari2020zero,
      title={ZeRO: Memory Optimizations Toward Training Trillion Parameter Models}, 
      author={Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He},
      year={2020},
      eprint={1910.02054},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{chen2022pali,
  title={Pali: A jointly-scaled multilingual language-image model},
  author={Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others},
  journal={arXiv preprint arXiv:2209.06794},
  year={2022}
}
@article{bao2022vlmo,
  title={Vlmo: Unified vision-language pre-training with mixture-of-modality-experts},
  author={Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit and Piao, Songhao and Wei, Furu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32897--32912},
  year={2022}
}
@article{zeng2021multi,
  title={Multi-grained vision language pre-training: Aligning texts with visual concepts},
  author={Zeng, Yan and Zhang, Xinsong and Li, Hang},
  journal={arXiv preprint arXiv:2111.08276},
  year={2021}
}
@inproceedings{Li2022BLIPBL,
  title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation},
  author={Junnan Li and Dongxu Li and Caiming Xiong and Steven C. H. Hoi},
  booktitle={International Conference on Machine Learning},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:246411402}
}
@inproceedings{BLEU,
  title={Bleu: a Method for Automatic Evaluation of Machine Translation},
  author={Kishore Papineni and Salim Roukos and Todd Ward and Wei-Jing Zhu},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2002},
  url={https://api.semanticscholar.org/CorpusID:11080756}
}
@inproceedings{ROUGE,
  title={ROUGE: A Package for Automatic Evaluation of Summaries},
  author={Chin-Yew Lin},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2004},
  url={https://api.semanticscholar.org/CorpusID:964287}
}
@inproceedings{yang2022triple,
  title={Vision-language pre-training with triple contrastive learning},
  author={Yang, Jinyu and Duan, Jiali and Tran, Son and Xu, Yi and Chanda, Sampath and Chen, Liqun and Zeng, Belinda and Chilimbi, Trishul and Huang, Junzhou},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15671--15680},
  year={2022}
}

@article{Lu2022UnifiedIO,
  title={Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks},
  author={Jiasen Lu and Christopher Clark and Rowan Zellers and Roozbeh Mottaghi and Aniruddha Kembhavi},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.08916},
  url={https://api.semanticscholar.org/CorpusID:249848272}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@inproceedings{yang2022frozenbilm,
title = {Zero-Shot Video Question Answering via Frozen Bidirectional Language Models},
author = {Antoine Yang and Antoine Miech and Josef Sivic and Ivan Laptev and Cordelia Schmid},
booktitle={NeurIPS},
year = {2022}
}

@article{li2023videochat,
  title={Videochat: Chat-centric video understanding},
  author={Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
  journal={arXiv preprint arXiv:2305.06355},
  year={2023}
}

@article{Maaz2023VideoChatGPT,
  title={Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models},
  author={Muhammad Maaz and Hanoona Rasheed and Salman Khan and Fahad Shahbaz Khan},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.05424},
  url={https://api.semanticscholar.org/CorpusID:259108333}
}

@article{Bai2023QwenVL,
  title={Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities},
  author={Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.12966},
  url={https://api.semanticscholar.org/CorpusID:263875678}
}

@inproceedings{ye2023hitea,
  title={Hitea: Hierarchical temporal-aware video-language pre-training},
  author={Ye, Qinghao and Xu, Guohai and Yan, Ming and Xu, Haiyang and Qian, Qi and Zhang, Ji and Huang, Fei},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15405--15416},
  year={2023}
}

@article{Zhang2023VideoLLaMA,
  title={Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding},
  author={Hang Zhang and Xin Li and Lidong Bing},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.02858},
  url={https://api.semanticscholar.org/CorpusID:259075356}
}

@article{Huang2023Kosmos1,
  title={Language Is Not All You Need: Aligning Perception with Language Models},
  author={Shaohan Huang and Li Dong and Wenhui Wang and Yaru Hao and Saksham Singhal and Shuming Ma and Tengchao Lv and Lei Cui and Owais Khan Mohammed and Qiang Liu and Kriti Aggarwal and Zewen Chi and Johan Bjorck and Vishrav Chaudhary and Subhojit Som and Xia Song and Furu Wei},
  journal={ArXiv},
  year={2023},
  volume={abs/2302.14045},
  url={https://api.semanticscholar.org/CorpusID:257219775}
}

@article{Peng2023Kosmos2GM,
  title={Kosmos-2: Grounding Multimodal Large Language Models to the World},
  author={Zhiliang Peng and Wenhui Wang and Li Dong and Yaru Hao and Shaohan Huang and Shuming Ma and Furu Wei},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.14824},
  url={https://api.semanticscholar.org/CorpusID:259262263}
}

@inproceedings{carion2020detr,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@article{shazeer2020swiglu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{Xu2023WizardLM,
  title={WizardLM: Empowering Large Language Models to Follow Complex Instructions},
  author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.12244},
  url={https://api.semanticscholar.org/CorpusID:258298159}
}

@misc{zheng2023vicuna,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{liu2023lrv,
  title={Aligning Large Multi-Modal Model with Robust Instruction Tuning},
  author={Liu, Fuxiao and Lin, Kevin and Li, Linjie and Wang, Jianfeng and Yacoob, Yaser and Wang, Lijuan},
  journal={arXiv preprint arXiv:2306.14565},
  year={2023}
}

@article{zhao2023svit,
  title={Svit: Scaling up visual instruction tuning},
  author={Zhao, Bo and Wu, Boya and Huang, Tiejun},
  journal={arXiv preprint arXiv:2307.04087},
  year={2023}
}

@inproceedings{radford2021clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@misc{ShareGPT2023,
    title = {ShareGPT},
    year = {2023},
    howpublished = {\url{http://sharegpt.com}}
}

@misc{SlimOrca,
  title = {SlimOrca: An Open Dataset of GPT-4 Augmented FLAN Reasoning Traces, with Verification},
  author = {Wing Lian and Guan Wang and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong and "Teknium"},
  year = {2023},
  publisher = {HuggingFace},
  url = {https://https://huggingface.co/Open-Orca/SlimOrca}
}
@inproceedings{SBU,
  title={Im2Text: Describing Images Using 1 Million Captioned Photographs},
  author={Vicente Ordonez and Girish Kulkarni and Tamara L. Berg},
  booktitle={Neural Information Processing Systems},
  year={2011},
  url={https://api.semanticscholar.org/CorpusID:14579301}
}
@article{Chen2023ShareGPT4V,
  title={ShareGPT4V: Improving Large Multi-Modal Models with Better Captions},
  author={Lin Chen and Jinsong Li and Xiao-wen Dong and Pan Zhang and Conghui He and Jiaqi Wang and Feng Zhao and Dahua Lin},
  journal={ArXiv},
  year={2023},
  volume={abs/2311.12793},
  url={https://api.semanticscholar.org/CorpusID:265308687}
}

@article{fu2023mme,
  title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
  author={Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Qiu, Zhenyu and Lin, Wei and Yang, Jinrui and Zheng, Xiawu and others},
  journal={arXiv preprint arXiv:2306.13394},
  year={2023}
}

@article{li2023seedbench,
  title={Seed-bench: Benchmarking multimodal llms with generative comprehension},
  author={Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
  journal={arXiv preprint arXiv:2307.16125},
  year={2023}
}


@article{yu2023mmvet,
  title={Mm-vet: Evaluating large multimodal models for integrated capabilities},
  author={Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  journal={arXiv preprint arXiv:2308.02490},
  year={2023}
}

@article{wu2023qbench,
    title={Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision},
    author={Wu, Haoning and Zhang, Zicheng and Zhang, Erli and Chen, Chaofeng and Liao, Liang and Wang, Annan and Li, Chunyi and Sun, Wenxiu and Yan, Qiong and Zhai, Guangtao and Lin, Weisi},
    journal={arXiv preprint arXiv:2309.14181},
    year={2023},
}

@article{Li2023pope,
  title={Evaluating Object Hallucination in Large Vision-Language Models},
  author={Yifan Li and Yifan Du and Kun Zhou and Jinpeng Wang and Wayne Xin Zhao and Ji-rong Wen},
  journal={ArXiv},
  year={2023},
  volume={abs/2305.10355},
  url={https://api.semanticscholar.org/CorpusID:258740697}
}

@article{hendrycks2020mmlu,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{suzgun2022bbh,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{zhong2023agieval,
  title={Agieval: A human-centric benchmark for evaluating foundation models},
  author={Zhong, Wanjun and Cui, Ruixiang and Guo, Yiduo and Liang, Yaobo and Lu, Shuai and Wang, Yanlin and Saied, Amin and Chen, Weizhu and Duan, Nan},
  journal={arXiv preprint arXiv:2304.06364},
  year={2023}
}

@article{clark2018arc,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@inproceedings{xu2017msrvttqa,
  title={Video question answering via gradually refined attention over appearance and motion},
  author={Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  booktitle={Proceedings of the 25th ACM international conference on Multimedia},
  pages={1645--1653},
  year={2017}
}

@inproceedings{jang2017tgif,
  title={Tgif-qa: Toward spatio-temporal reasoning in visual question answering},
  author={Jang, Yunseok and Song, Yale and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2758--2766},
  year={2017}
}

@misc{laurencon2023idefics,
      title={OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents},
      author={Hugo Laurençon and Lucile Saulnier and Léo Tronchon and Stas Bekman and Amanpreet Singh and Anton Lozhkov and Thomas Wang and Siddharth Karamcheti and Alexander M. Rush and Douwe Kiela and Matthieu Cord and Victor Sanh},
      year={2023},
      eprint={2306.16527},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{ye2023ureader,
  title={UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model},
  author={Ye, Jiabo and Hu, Anwen and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Xu, Guohai and Li, Chenliang and Tian, Junfeng and Qian, Qi and Zhang, Ji and others},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023},
}

@inproceedings{he2022mae,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16000--16009},
  year={2022}
}

@article{loshchilov2018adamW,
  title={Fixing weight decay regularization in adam},
  author={Loshchilov, Ilya and Hutter, Frank},
  year={2018}
}

@inproceedings{changpinyo2021cc3m12m,
  title={Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts},
  author={Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3558--3568},
  year={2021}
}

@article{schuhmann2022laion,
  title={Laion-5b: An open large-scale dataset for training next generation image-text models},
  author={Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25278--25294},
  year={2022}
}

@article{gadre2023datacomp,
  title={DataComp: In search of the next generation of multimodal datasets},
  author={Gadre, Samir Yitzhak and Ilharco, Gabriel and Fang, Alex and Hayase, Jonathan and Smyrnis, Georgios and Nguyen, Thao and Marten, Ryan and Wortsman, Mitchell and Ghosh, Dhruba and Zhang, Jieyu and others},
  journal={arXiv preprint arXiv:2304.14108},
  year={2023}
}

@misc{kakaobrain2022coyo-700m,
  title         = {COYO-700M: Image-Text Pair Dataset},
  author        = {Byeon, Minwoo and Park, Beomhee and Kim, Haecheon and Lee, Sungjun and Baek, Woonhyuk and Kim, Saehoon},
  year          = {2022},
  howpublished  = {\url{https://github.com/kakaobrain/coyo-dataset}},
}

@inproceedings{lin2014coco,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@InProceedings{balanced_vqa_v2,
    author = {Yash Goyal and Tejas Khot and Douglas Summers{-}Stay and Dhruv Batra and Devi Parikh},
    title = {Making the {V} in {VQA} Matter: Elevating the Role of Image Understanding in {V}isual {Q}uestion {A}nswering},
    booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
    year = {2017}, 
}


@inproceedings{mishra2019ocrvqa,
  title={Ocr-vqa: Visual question answering by reading text in images},
  author={Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
  booktitle={2019 international conference on document analysis and recognition (ICDAR)},
  pages={947--952},
  year={2019},
  organization={IEEE}
}

@inproceedings{hudson2019gqa,
  title={Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  author={Hudson, Drew A and Manning, Christopher D},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={6700--6709},
  year={2019}
}

@inproceedings{singh2019textvqa,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8317--8326},
  year={2019}
}

@inproceedings{sidorov2020textcaps,
  title={Textcaps: a dataset for image captioning with reading comprehension},
  author={Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16},
  pages={742--758},
  year={2020},
  organization={Springer}
}

@inproceedings{marino2019okvqa,
  title={Ok-vqa: A visual question answering benchmark requiring external knowledge},
  author={Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
  booktitle={Proceedings of the IEEE/cvf conference on computer vision and pattern recognition},
  pages={3195--3204},
  year={2019}
}

@inproceedings{schwenk2022aokvqa,
  title={A-okvqa: A benchmark for visual question answering using world knowledge},
  author={Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh},
  booktitle={European Conference on Computer Vision},
  pages={146--162},
  year={2022},
  organization={Springer}
}
@inproceedings{nlvr2,
  title={A corpus of natural language for visual reasoning},
  author={Suhr, Alane and Lewis, Mike and Yeh, James and Artzi, Yoav},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={217--223},
  year={2017}
}
@article{ren2024grounded,
  title={Grounded sam: Assembling open-world models for diverse visual tasks},
  author={Ren, Tianhe and Liu, Shilong and Zeng, Ailing and Lin, Jing and Li, Kunchang and Cao, He and Chen, Jiayu and Huang, Xinyu and Chen, Yukang and Yan, Feng and others},
  journal={arXiv preprint arXiv:2401.14159},
  year={2024}
}
@article{VG,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={International journal of computer vision},
  volume={123},
  pages={32--73},
  year={2017},
  publisher={Springer}
}


@inproceedings{yu2016refcoco,
  title={Modeling context in referring expressions},
  author={Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C and Berg, Tamara L},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pages={69--85},
  year={2016},
  organization={Springer}
}

@article{awadalla2023openflamingo,
  title={Openflamingo: An open-source framework for training large autoregressive vision-language models},
  author={Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others},
  journal={arXiv preprint arXiv:2308.01390},
  year={2023}
}

@article{gong2023mmgpt,
  title={Multimodal-gpt: A vision and language model for dialogue with humans},
  author={Gong, Tao and Lyu, Chengqi and Zhang, Shilong and Wang, Yudong and Zheng, Miao and Zhao, Qian and Liu, Kuikun and Zhang, Wenwei and Luo, Ping and Chen, Kai},
  journal={arXiv preprint arXiv:2305.04790},
  year={2023}
}

@misc{2023opencompass,
    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},
    author={OpenCompass Contributors},
    howpublished = {\url{https://github.com/open-compass/opencompass}},
    year={2023}
}


@article{kwon2022masked,
  title={Masked vision and language modeling for multi-modal representation learning},
  author={Kwon, Gukyeong and Cai, Zhaowei and Ravichandran, Avinash and Bas, Erhan and Bhotika, Rahul and Soatto, Stefano},
  journal={arXiv preprint arXiv:2208.02131},
  year={2022}
}


@inproceedings{wang2023beit3,
  title={Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks},
  author={Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19175--19186},
  year={2023}
}
@inproceedings{jiang2023copa,
  title={COPA: Efficient Vision-Language Pre-training through Collaborative Object-and Patch-Text Alignment},
  author={Jiang, Chaoya and Xu, Haiyang and Ye, Wei and Ye, Qinghao and Li, Chenliang and Yan, Ming and Bi, Bin and Zhang, Shikun and Huang, Fei and Zhang, Ji},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={4480--4491},
  year={2023}
}
@InProceedings{jiangbus,
    author    = {Jiang, Chaoya and Xu, Haiyang and Ye, Wei and Ye, Qinghao and Li, Chenliang and Yan, Ming and Bi, Bin and Zhang, Shikun and Huang, Fei and Huang, Songfang},
    title     = {BUS: Efficient and Effective Vision-Language Pre-Training with Bottom-Up Patch Summarization.},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {2900-2910}
}
@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{chen2020moco,
  title={Improved baselines with momentum contrastive learning},
  author={Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming},
  journal={arXiv preprint arXiv:2003.04297},
  year={2020}
}
@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9729--9738},
  year={2020}
}
@inproceedings{jiangtrips,
    title = "{TRIPS}: Efficient Vision-and-Language Pre-training with Text-Relevant Image Patch Selection",
    author = "Jiang, Chaoya  and
      Xu, Haiyang  and
      Li, Chenliang  and
      Yan, Ming  and
      Ye, Wei  and
      Zhang, Shikun  and
      Bi, Bin  and
      Huang, Songfang",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.273",
    doi = "10.18653/v1/2022.emnlp-main.273",
    pages = "4084--4096",
}
@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International Conference on Machine Learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}
@article{li2021align,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={9694--9705},
  year={2021}
}
@article{Wang2023VIGCVI,
  title={VIGC: Visual Instruction Generation and Correction},
  author={Bin Wang and Fan Wu and Xiao Han and Jiahui Peng and Huaping Zhong and Pan Zhang and Xiao-wen Dong and Weijia Li and Wei Li and Jiaqi Wang and Conghui He},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.12714},
  url={https://api.semanticscholar.org/CorpusID:261100735}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{demon,
  title={Fine-tuning multimodal llms to follow zero-shot demonstrative instructions},
  author={Li, Juncheng and Pan, Kaihang and Ge, Zhiqi and Gao, Minghe and Ji, Wei and Zhang, Wenqiao and Chua, Tat-Seng and Tang, Siliang and Zhang, Hanwang and Zhuang, Yueting},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{Tan2019EfficientNetRM,
  title={EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  author={Mingxing Tan and Quoc V. Le},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.11946},
  url={https://api.semanticscholar.org/CorpusID:167217261}
}

@article{MMC4,
  title={Multimodal c4: An open, billion-scale corpus of images interleaved with text},
  author={Zhu, Wanrong and Hessel, Jack and Awadalla, Anas and Gadre, Samir Yitzhak and Dodge, Jesse and Fang, Alex and Yu, Youngjae and Schmidt, Ludwig and Wang, William Yang and Choi, Yejin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{goodfellow2020generative,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}
@article{He2015DeepRL,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={770-778},
  url={https://api.semanticscholar.org/CorpusID:206594692}
}
@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@article{zhao2023retrieving,
  title={Retrieving multimodal information for augmented generation: A survey},
  author={Zhao, Ruochen and Chen, Hailin and Wang, Weishi and Jiao, Fangkai and Do, Xuan Long and Qin, Chengwei and Ding, Bosheng and Guo, Xiaobao and Li, Minzhi and Li, Xingxuan and others},
  journal={arXiv preprint arXiv:2303.10868},
  year={2023}
}

@article{zhao2023mmicl,
  title={Mmicl: Empowering vision-language model with multi-modal in-context learning},
  author={Zhao, Haozhe and Cai, Zefan and Si, Shuzheng and Ma, Xiaojian and An, Kaikai and Chen, Liang and Liu, Zixuan and Wang, Sheng and Han, Wenjuan and Chang, Baobao},
  journal={arXiv preprint arXiv:2309.07915},
  year={2023}
}

@article{laurenccon2024obelics,
  title={Obelics: An open web-scale filtered dataset of interleaved image-text documents},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Tronchon, L{\'e}o and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander and Kiela, Douwe and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
} 

@inproceedings{VQGAN,
  title={Vector quantized diffusion model for text-to-image synthesis},
  author={Gu, Shuyang and Chen, Dong and Bao, Jianmin and Wen, Fang and Zhang, Bo and Chen, Dongdong and Yuan, Lu and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10696--10706},
  year={2022}
}
@article{VQVAE,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{evaclip,
  title={Eva-clip: Improved training techniques for clip at scale},
  author={Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},
  journal={arXiv preprint arXiv:2303.15389},
  year={2023}
}
@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{bao2021beit,
  title={Beit: Bert pre-training of image transformers},
  author={Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu},
  journal={arXiv preprint arXiv:2106.08254},
  year={2021}
}
@article{SEED,
  title={Planting a seed of vision in large language model},
  author={Ge, Yuying and Ge, Yixiao and Zeng, Ziyun and Wang, Xintao and Shan, Ying},
  journal={arXiv preprint arXiv:2307.08041},
  year={2023}
}

@article{liang2022mind,
  title={Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning},
  author={Liang, Victor Weixin and Zhang, Yuhui and Kwon, Yongchan and Yeung, Serena and Zou, James Y},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17612--17625},
  year={2022}
}
@article{Li2023LargeMM,
  title={Large Multimodal Models: Notes on CVPR 2023 Tutorial},
  author={Chunyuan Li},
  journal={ArXiv},
  year={2023},
  volume={abs/2306.14895},
  url={https://api.semanticscholar.org/CorpusID:259262448}
}
@article{Zhang2015ASO,
  title={A Survey of Sparse Representation: Algorithms and Applications},
  author={Zheng Zhang and Yong Xu and Jian Yang and Xuelong Li and David Dian Zhang},
  journal={IEEE Access},
  year={2015},
  volume={3},
  pages={490-530},
  url={https://api.semanticscholar.org/CorpusID:206485791}
}