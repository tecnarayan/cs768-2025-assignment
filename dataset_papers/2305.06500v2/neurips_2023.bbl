\begin{thebibliography}{10}

\bibitem{chatgpt}
Chatgpt.
\newblock \url{https://openai.com/blog/chatgpt}, 2023.

\bibitem{vicuna}
Vicuna.
\newblock \url{https://github.com/lm-sys/FastChat}, 2023.

\bibitem{nocaps}
Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson.
\newblock nocaps: novel object captioning at scale.
\newblock In {\em ICCV}, pages 8948--8957, 2019.

\bibitem{flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob~L Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Miko\l~aj Bi\'{n}kowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Kar\'{e}n Simonyan.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock In S.~Koyejo, S.~Mohamed, A.~Agarwal, D.~Belgrave, K.~Cho, and A.~Oh, editors, {\em NeurIPS}, 2022.

\bibitem{GPT3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}, 2020.

\bibitem{VL_T5}
Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal.
\newblock Unifying vision-and-language tasks via text generation.
\newblock {\em arXiv preprint arXiv:2102.02779}, 2021.

\bibitem{flanT5}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang~Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent~Y. Zhao, Yanping Huang, Andrew~M. Dai, Hongkun Yu, Slav Petrov, Ed~H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc~V. Le, and Jason Wei.
\newblock Scaling instruction-finetuned language models.
\newblock {\em arXiv preprint arXiv:2210.11416}, 2022.

\bibitem{visdial}
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose M.~F. Moura, Devi Parikh, and Dhruv Batra.
\newblock Visual dialog.
\newblock In {\em CVPR}, 2017.

\bibitem{palme}
Danny Driess, Fei Xia, Mehdi S.~M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence.
\newblock Palm-e: An embodied multimodal language model, 2023.

\bibitem{eva}
Yuxin Fang, Wen Wang, Binhui Xie, Quan-Sen Sun, Ledell~Yu Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao.
\newblock Eva: Exploring the limits of masked visual representation learning at scale.
\newblock {\em ArXiv}, abs/2211.07636, 2022.

\bibitem{vqav2}
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
\newblock Making the v in vqa matter: Elevating the role of image understanding in visual question answering.
\newblock In {\em CVPR}, July 2017.

\bibitem{vizwiz}
Danna Gurari, Qing Li, Abigale~J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey~P. Bigham.
\newblock Vizwiz grand challenge: Answering visual questions from blind people.
\newblock In {\em CVPR}, 2018.

\bibitem{unnatural-instruct}
Or~Honovich, Thomas Scialom, Omer Levy, and Timo Schick.
\newblock Unnatural instructions: Tuning language models with (almost) no human labor.
\newblock {\em ArXiv}, abs/2212.09689, 2022.

\bibitem{lora}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen{-}Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock In {\em {ICLR}}, 2022.

\bibitem{promptcap}
Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah~A. Smith, and Jiebo Luo.
\newblock Promptcap: Prompt-guided task-aware image captioning, 2023.

\bibitem{gqa}
Drew~A. Hudson and Christopher~D. Manning.
\newblock Gqa: A new dataset for real-world visual reasoning and compositional question answering.
\newblock In {\em CVPR}, 2019.

\bibitem{karpathy_split}
Andrej Karpathy and Li~Fei-Fei.
\newblock Deep visual-semantic alignments for generating image descriptions.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, June 2015.

\bibitem{hatefulmemes}
Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine.
\newblock The hateful memes challenge: Detecting hate speech in multimodal memes.
\newblock In {\em NeurIPS}, 2020.

\bibitem{lavis}
Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and Steven C.~H. Hoi.
\newblock Lavis: A library for language-vision intelligence, 2022.

\bibitem{blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In {\em ICML}, 2023.

\bibitem{blip1}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
\newblock In {\em ICML}, 2022.

\bibitem{albef}
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu~Hong Hoi.
\newblock Align before fuse: Vision and language representation learning with momentum distillation.
\newblock In {\em NeurIPS}, 2021.

\bibitem{coco}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C.~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em ECCV}, 2014.

\bibitem{vsr}
Fangyu Liu, Guy Edward~Toh Emerson, and Nigel Collier.
\newblock Visual spatial reasoning.
\newblock {\em Transactions of the Association for Computational Linguistics}, 2023.

\bibitem{llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock 2023.

\bibitem{adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em ICLR}, 2019.

\bibitem{12in1}
Jiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee.
\newblock 12-in-1: Multi-task vision and language representation learning.
\newblock In {\em {CVPR}}, 2020.

\bibitem{scienceqa}
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.
\newblock Learn to explain: Multimodal reasoning via thought chains for science question answering.
\newblock In {\em NeurIPS}, 2022.

\bibitem{iconqa}
Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-Chun Zhu.
\newblock Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning.
\newblock In {\em NeurIPS Track on Datasets and Benchmarks}, 2021.

\bibitem{okvqa}
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
\newblock Ok-vqa: A visual question answering benchmark requiring external knowledge.
\newblock In {\em CVPR}, 2019.

\bibitem{ocrvqa}
Anand Mishra, Shashank Shekhar, Ajeet~Kumar Singh, and Anirban Chakraborty.
\newblock Ocr-vqa: Visual question answering by reading text in images.
\newblock In {\em ICDAR}, 2019.

\bibitem{visdial_pretrain}
Vishvak Murahari, Dhruv Batra, Devi Parikh, and Abhishek Das.
\newblock Large-scale pretraining for visual dialog: A simple state-of-the-art baseline.
\newblock In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, {\em ECCV}, 2020.

\bibitem{gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock {\em ArXiv}, abs/2303.08774, 2023.

\bibitem{T5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em The Journal of Machine Learning Research}, 2020.

\bibitem{t0}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M~Saiful Bari, Canwen Xu, Urmish Thakker, Shanya~Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal~V. Nayak, Debajyoti Datta, Jonathan Chang, Mike~Tian{-}Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng~Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault F{\'{e}}vry, Jason~Alan Fries, Ryan Teehan, Teven~Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander~M. Rush.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In {\em {ICLR}}, 2022.

\bibitem{aokvqa}
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.
\newblock A-okvqa: A benchmark forÂ visual question answering using world knowledge.
\newblock In Shai Avidan, Gabriel Brostow, Moustapha Ciss{\'e}, Giovanni~Maria Farinella, and Tal Hassner, editors, {\em ECCV}, 2022.

\bibitem{prophet}
Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu.
\newblock Prompting large language models with answer heuristics for knowledge-based visual question answering.
\newblock {\em Computer Vision and Pattern Recognition (CVPR)}, 2023.

\bibitem{textcaps}
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh.
\newblock Textcaps: a dataset for image captioningwith reading comprehension.
\newblock 2020.

\bibitem{textvqa}
Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu~Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach.
\newblock Towards vqa models that can read.
\newblock In {\em CVPR}, pages 8317--8326, 2019.

\bibitem{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem{llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{cider}
Ramakrishna Vedantam, C.~Lawrence Zitnick, and Devi Parikh.
\newblock Cider: Consensus-based image description evaluation.
\newblock In {\em 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 4566--4575, 2015.

\bibitem{git}
Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce~Liu, and Lijuan Wang.
\newblock Git: A generative image-to-text transformer for vision and language, 2022.

\bibitem{self-instruct}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language model with self generated instructions.
\newblock {\em ArXiv}, abs/2212.10560, 2022.

\bibitem{super-natural-instruct}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut~Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal~Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani~Rohitha Kaza, Pulkit Verma, Ravsehaj~Singh Puri, Rushang Karia, Savan Doshi, Shailaja~Keyur Sampat, Siddhartha Mishra, Sujan Reddy~A, Sumanta Patro, Tanay Dixit, and Xudong Shen.
\newblock Super-{N}atural{I}nstructions: Generalization via declarative instructions on 1600+ {NLP} tasks.
\newblock In {\em EMNLP}, 2022.

\bibitem{flan}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In {\em {ICLR}}, 2022.

\bibitem{msvdqa}
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang.
\newblock Video question answering via gradually refined attention over appearance and motion.
\newblock In {\em Proceedings of the 25th ACM International Conference on Multimedia}, page 1645â1653, 2017.

\bibitem{multi-instruct}
Zhiyang Xu, Ying Shen, and Lifu Huang.
\newblock Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning.
\newblock {\em ArXiv}, abs/2212.10773, 2022.

\bibitem{ivqa}
Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.
\newblock Just ask: Learning to answer questions from millions of narrated videos.
\newblock In {\em ICCV}, pages 1686--1697, 2021.

\bibitem{mplug-owl}
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi~Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji~Chao Zhang, and Feiyan Huang.
\newblock mplug-owl: Modularization empowers large language models with multimodality.
\newblock 2023.

\bibitem{flickr30k}
Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
\newblock From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.
\newblock {\em Transactions of the Association for Computational Linguistics}, 2, 2014.

\bibitem{zhu2022minigpt4}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023.

\end{thebibliography}
