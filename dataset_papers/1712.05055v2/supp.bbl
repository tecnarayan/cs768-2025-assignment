\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Arpit, D., Jastrz{k{e}}bski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal,
  M.~S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Black \& Anandan(1996)Black and Anandan]{black1996robust}
Black, M.~J. and Anandan, P.
\newblock The robust estimation of multiple motions: Parametric and
  piecewise-smooth flow fields.
\newblock \emph{Computer vision and image understanding}, 63\penalty0
  (1):\penalty0 75--104, 1996.

\bibitem[Candes et~al.(2008)Candes, Wakin, and Boyd]{candes2008enhancing}
Candes, E.~J., Wakin, M.~B., and Boyd, S.~P.
\newblock Enhancing sparsity by reweighted l1 minimization.
\newblock \emph{Journal of Fourier analysis and applications}, 14\penalty0
  (5-6):\penalty0 877--905, 2008.

\bibitem[Chang et~al.(2017)Chang, Learned-Miller, and
  McCallum]{chang2017active}
Chang, H.-S., Learned-Miller, E., and McCallum, A.
\newblock Active bias: Training a more accurate neural network by emphasizing
  high variance samples.
\newblock \emph{NIPS}, 2017.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Felzenszwalb et~al.(2010)Felzenszwalb, Girshick, McAllester, and
  Ramanan]{felzenszwalb2010object}
Felzenszwalb, P.~F., Girshick, R.~B., McAllester, D., and Ramanan, D.
\newblock Object detection with discriminatively trained part-based models.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 32\penalty0 (9):\penalty0 1627--1645, 2010.

\bibitem[Goldberger \& Ben-Reuven(2017)Goldberger and
  Ben-Reuven]{goldberger2017training}
Goldberger, J. and Ben-Reuven, E.
\newblock Training deep neural-networks using a noise adaptation layer.
\newblock In \emph{ICLR}, 2017.

\bibitem[Gong et~al.(2013)Gong, Zhang, Lu, Huang, and Ye]{gong2013general}
Gong, P., Zhang, C., Lu, Z., Huang, J.~Z., and Ye, J.
\newblock A general iterative shrinkage and thresholding algorithm for
  non-convex regularized optimization problems.
\newblock In \emph{ICML}, 2013.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[Huber et~al.(1964)]{huber1964robust}
Huber, P.~J. et~al.
\newblock Robust estimation of a location parameter.
\newblock \emph{The Annals of Mathematical Statistics}, 35\penalty0
  (1):\penalty0 73--101, 1964.

\bibitem[Jiang et~al.(2014)Jiang, Meng, Yu, Lan, Shan, and
  Hauptmann]{jiang2014self}
Jiang, L., Meng, D., Yu, S.-I., Lan, Z., Shan, S., and Hauptmann, A.
\newblock Self-paced learning with diversity.
\newblock In \emph{NIPS}, 2014.

\bibitem[Jiang et~al.(2015)Jiang, Meng, Zhao, Shan, and
  Hauptmann]{jiang2015self}
Jiang, L., Meng, D., Zhao, Q., Shan, S., and Hauptmann, A.~G.
\newblock Self-paced curriculum learning.
\newblock In \emph{AAAI}, 2015.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2015adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{NIPS}, 2012.

\bibitem[Kumar et~al.(2010)Kumar, Packer, and Koller]{kumar2010self}
Kumar, M.~P., Packer, B., and Koller, D.
\newblock Self-paced learning for latent variable models.
\newblock In \emph{NIPS}, 2010.

\bibitem[Lin et~al.(2017)Lin, Goyal, Girshick, He, and
  Doll{\'a}r]{lin2017focal}
Lin, T.-Y., Goyal, P., Girshick, R., He, K., and Doll{\'a}r, P.
\newblock Focal loss for dense object detection.
\newblock \emph{ICCV}, 2017.

\bibitem[Mairal(2013)]{mairal2013stochastic}
Mairal, J.
\newblock Stochastic majorization-minimization algorithms for large-scale
  optimization.
\newblock In \emph{NIPS}, 2013.

\bibitem[Meng et~al.(2015)Meng, Zhao, and Jiang]{meng2015objective}
Meng, D., Zhao, Q., and Jiang, L.
\newblock What objective does self-paced learning indeed optimize?
\newblock \emph{arXiv preprint arXiv:1511.06049}, 2015.

\bibitem[Reed et~al.(2014)Reed, Lee, Anguelov, Szegedy, Erhan, and
  Rabinovich]{reed2014training}
Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and Rabinovich, A.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock \emph{arXiv preprint arXiv:1412.6596}, 2014.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G.~E., Krizhevsky, A., Sutskever, I., and
  Salakhutdinov, R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of machine learning research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{CVPR}, 2016.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{BMVC}, 2016.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zhang(2010)]{zhang2010nearly}
Zhang, C.-H.
\newblock Nearly unbiased variable selection under minimax concave penalty.
\newblock \emph{The Annals of Statistics}, pp.\  894--942, 2010.

\end{thebibliography}
