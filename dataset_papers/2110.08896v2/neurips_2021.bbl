% Generated by IEEEtran.bst, version: 1.12 (2007/01/11)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto, \emph{Reinforcement learning: An
  introduction}.\hskip 1em plus 0.5em minus 0.4em\relax MIT press, 2018.

\bibitem{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski \emph{et~al.},
  ``Human-level control through deep reinforcement learning,'' \emph{Nature},
  vol. 518, no. 7540, pp. 529--533, 2015.

\bibitem{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, S.~Dieleman,
  D.~Grewe, J.~Nham, N.~Kalchbrenner, I.~Sutskever, T.~Lillicrap, M.~Leach,
  K.~Kavukcuoglu, T.~Graepel, and D.~Hassabis, ``Mastering the game of go with
  deep neural networks and tree search,'' \emph{Nature}, vol. 529, no. 7587,
  pp. 484--489, 2016.

\bibitem{mavrin2019exploration}
B.~Mavrin, S.~Zhang, H.~Yao, and L.~Kong, ``Exploration in the face of
  parametric and intrinsic uncertainties,'' in \emph{Proceedings of the 18th
  International Conference on Autonomous Agents and MultiAgent Systems}, 2019,
  pp. 2117--2119.

\bibitem{mirowski2016learning}
P.~Mirowski, R.~Pascanu, F.~Viola, H.~Soyer, A.~J. Ballard, A.~Banino,
  M.~Denil, R.~Goroshin, L.~Sifre, K.~Kavukcuoglu \emph{et~al.}, ``Learning to
  navigate in complex environments,'' \emph{arXiv preprint arXiv:1611.03673},
  2016.

\bibitem{hasselt2010double}
H.~Hasselt, ``Double q-learning,'' \emph{Advances in Neural Information
  Processing Systems}, vol.~23, pp. 2613--2621, 2010.

\bibitem{wang2016dueling}
Z.~Wang, T.~Schaul, M.~Hessel, H.~Hasselt, M.~Lanctot, and N.~Freitas,
  ``Dueling network architectures for deep reinforcement learning,'' in
  \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2016, pp. 1995--2003.

\bibitem{lillicrap2015continuous}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra, ``Continuous control with deep reinforcement
  learning,'' \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem{haarnoja2018soft}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine, ``Soft actor-critic: Off-policy
  maximum entropy deep reinforcement learning with a stochastic actor,'' in
  \emph{International Conference on Machine Learning}.\hskip 1em plus 0.5em
  minus 0.4em\relax PMLR, 2018, pp. 1861--1870.

\bibitem{bellemare2017distributional}
M.~G. Bellemare, W.~Dabney, and R.~Munos, ``A distributional perspective on
  reinforcement learning,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2017, pp. 449--458.

\bibitem{mavrin2019distributional}
B.~Mavrin, H.~Yao, L.~Kong, K.~Wu, and Y.~Yu, ``Distributional reinforcement
  learning for efficient exploration,'' in \emph{International Conference on
  Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2019, pp.
  4424--4434.

\bibitem{zhang2019quota}
S.~Zhang, B.~Mavrin, L.~Kong, B.~Liu, and H.~Yao, ``Quota: The quantile option
  architecture for reinforcement learning,'' in \emph{Proceedings of the AAAI
  Conference on Artificial Intelligence}, vol.~33, no.~01, 2019, pp.
  5797--5804.

\bibitem{puterman1990markov}
M.~L. Puterman, ``Markov decision processes,'' \emph{Handbooks in Operations
  Research and Management Science}, vol.~2, pp. 331--434, 1990.

\bibitem{puterman1978analytic}
M.~L. Puterman and S.~L. Brumelle, ``The analytic theory of policy iteration,''
  \emph{Dynamic Programming and Its Applications}, pp. 91--113, 1978.

\bibitem{anschel2017averaged}
O.~Anschel, N.~Baram, and N.~Shimkin, ``Averaged-dqn: Variance reduction and
  stabilization for deep reinforcement learning,'' in \emph{International
  Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR,
  2017, pp. 176--185.

\bibitem{bubeck2014convex}
S.~Bubeck, ``Convex optimization: Algorithms and complexity,'' \emph{arXiv
  preprint arXiv:1405.4980}, 2014.

\bibitem{walker2011anderson}
H.~F. Walker and P.~Ni, ``Anderson acceleration for fixed-point iterations,''
  \emph{SIAM Journal on Numerical Analysis}, vol.~49, no.~4, pp. 1715--1735,
  2011.

\bibitem{fu2020anderson}
A.~Fu, J.~Zhang, and S.~Boyd, ``Anderson accelerated douglas--rachford
  splitting,'' \emph{SIAM Journal on Scientific Computing}, vol.~42, no.~6, pp.
  A3560--A3583, 2020.

\bibitem{geist2018anderson}
M.~Geist and B.~Scherrer, ``Anderson acceleration for reinforcement learning,''
  \emph{arXiv preprint arXiv:1809.09501}, 2018.

\bibitem{peng2018anderson}
Y.~Peng, B.~Deng, J.~Zhang, F.~Geng, W.~Qin, and L.~Liu, ``Anderson
  acceleration for geometry optimization and physics simulation,'' \emph{ACM
  Transactions on Graphics (TOG)}, vol.~37, no.~4, pp. 1--14, 2018.

\bibitem{an2017anderson}
H.~An, X.~Jia, and H.~F. Walker, ``Anderson acceleration and application to the
  three-temperature energy equations,'' \emph{Journal of Computational
  Physics}, vol. 347, pp. 1--19, 2017.

\bibitem{li2018accelerated}
Y.~Li, C.~Ni, G.~Xie, W.~Yang, S.~Zhou, and Z.~Zhang, ``Accelerated value
  iteration via anderson mixing,'' 2018.

\bibitem{shi2019regularized}
W.~Shi, S.~Song, H.~Wu, Y.-C. Hsu, C.~Wu, and G.~Huang, ``Regularized anderson
  acceleration for off-policy deep reinforcement learning,'' \emph{Advances in
  Neural Information Processing Systems}, 2019.

\bibitem{fang2009two}
H.-r. Fang and Y.~Saad, ``Two classes of multisecant methods for nonlinear
  acceleration,'' \emph{Numerical Linear Algebra with Applications}, vol.~16,
  no.~3, pp. 197--221, 2009.

\bibitem{evans2020proof}
C.~Evans, S.~Pollock, L.~G. Rebholz, and M.~Xiao, ``A proof that anderson
  acceleration improves the convergence rate in linearly converging fixed-point
  methods (but not in those converging quadratically),'' \emph{SIAM Journal on
  Numerical Analysis}, vol.~58, no.~1, pp. 788--810, 2020.

\bibitem{asadi2017alternative}
K.~Asadi and M.~L. Littman, ``An alternative softmax operator for reinforcement
  learning,'' in \emph{International Conference on Machine Learning}.\hskip 1em
  plus 0.5em minus 0.4em\relax PMLR, 2017, pp. 243--252.

\bibitem{azar2012dynamic}
M.~G. Azar, V.~G{\'o}mez, and H.~J. Kappen, ``Dynamic policy programming,''
  \emph{The Journal of Machine Learning Research}, vol.~13, no.~1, pp.
  3207--3245, 2012.

\bibitem{watkins1989learning}
C.~J. C.~H. Watkins, ``Learning from delayed rewards,'' 1989.

\bibitem{van2016deep}
H.~Van~Hasselt, A.~Guez, and D.~Silver, ``Deep reinforcement learning with
  double q-learning,'' in \emph{Proceedings of the AAAI Conference on
  Artificial Intelligence}, vol.~30, no.~1, 2016.

\bibitem{pan2019reinforcement}
L.~Pan, Q.~Cai, Q.~Meng, W.~Chen, L.~Huang, and T.-Y. Liu, ``Reinforcement
  learning with dynamic boltzmann softmax updates,'' \emph{International Joint
  Conference on Artificial Intelligence (IJCAI)}, 2020.

\bibitem{song2019revisiting}
Z.~Song, R.~Parr, and L.~Carin, ``Revisiting the softmax bellman operator: New
  benefits and new perspective,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2019, pp. 5916--5925.

\bibitem{fujimoto2018addressing}
S.~Fujimoto, H.~Hoof, and D.~Meger, ``Addressing function approximation error
  in actor-critic methods,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2018, pp. 1587--1596.

\bibitem{kim2019deepmellow}
S.~Kim, K.~Asadi, M.~Littman, and G.~Konidaris, ``Deepmellow: removing the need
  for a target network in deep q-learning,'' in \emph{Proceedings of the Twenty
  Eighth International Joint Conference on Artificial Intelligence}, 2019.

\end{thebibliography}
