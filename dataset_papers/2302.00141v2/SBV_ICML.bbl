\begin{thebibliography}{85}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng]{tf2015}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp,
  A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
  Levenberg, J., Man\'{e}, D., Monga, R., Moore, S., Murray, D., Olah, C.,
  Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Vi\'{e}gas, F., Vinyals, O., Warden, P.,
  Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{https://www.tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Agarwal et~al.(2022)Agarwal, Jiang, Kakade, and Sun]{Agarwal2022}
Agarwal, A., Jiang, N., Kakade, S.~M., and Sun, W.
\newblock Reinforcement learning: Theory and algorithms, 2022.
\newblock URL \url{https://rltheorybook.github.io/}.
\newblock A working draft.

\bibitem[Agarwal et~al.(2020)Agarwal, Schuurmans, and Norouzi]{Agarwal2020}
Agarwal, R., Schuurmans, D., and Norouzi, M.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML 2020)}, pp.\  104--114, 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/agarwal20c.html}.

\bibitem[Antos et~al.(2007)Antos, Munos, and Szepesv{\'a}ri]{Antos2007fqi}
Antos, A., Munos, R., and Szepesv{\'a}ri, C.~a.
\newblock Fitted {Q}-iteration in continuous action-space {MDP}s.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS
  2007)}, volume~20, pp.\  9--16, 2007.
\newblock URL
  \url{https://papers.nips.cc/paper_files/paper/2007/file/da0d1111d2dc5d489242e60ebcbaf988-Paper.pdf}.

\bibitem[Arulkumaran et~al.(2017)Arulkumaran, Deisenroth, Brundage, and
  Bharath]{Arulkumaran2017}
Arulkumaran, K., Deisenroth, M.~P., Brundage, M., and Bharath, A.~A.
\newblock Deep reinforcement learning: A brief survey.
\newblock \emph{IEEE Signal Processing Magazine}, 34\penalty0 (6):\penalty0
  26--38, 2017.
\newblock \doi{10.1109/MSP.2017.2743240}.

\bibitem[Baird(1995)]{Baird1995}
Baird, L.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{Proceedings of the 12th International Conference on Machine
  Learning (ICML 1995)}, pp.\  30--37, 1995.
\newblock \doi{10.1016/B978-1-55860-377-6.50013-X}.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{Bellemare2013}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.
\newblock \doi{10.1613/jair.3912}.

\bibitem[Busoniu et~al.(2010)Busoniu, Babuska, Schutter, and
  Ernst]{Busoniu2010}
Busoniu, L., Babuska, R., Schutter, B.~D., and Ernst, D.
\newblock \emph{Reinforcement Learning and Dynamic Programming Using Function
  Approximators}.
\newblock CRC Press, 2010.
\newblock ISBN 9781439821084.

\bibitem[Castro et~al.(2018)Castro, Moitra, Gelada, Kumar, and
  Bellemare]{dopamine}
Castro, P.~S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M.~G.
\newblock Dopamine: A research framework for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1812.06110v1}, 2018.

\bibitem[Chen \& Jiang(2019)Chen and Jiang]{Chen2019}
Chen, J. and Jiang, N.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML 2019)}, pp.\  1042--1051, 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/chen19e.html}.

\bibitem[Chen \& Jiang(2022)Chen and Jiang]{Chen2022}
Chen, J. and Jiang, N.
\newblock On well-posedness and minimax optimal rates of nonparametric
  {Q}-function estimation in off-policy evaluation.
\newblock In \emph{Proceedings of the 39th International Conference on Machine
  Learning (ICML 2022)}, pp.\  3558--3582, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/chen22u.html}.

\bibitem[Chollet(2017)]{Chollet2017}
Chollet, F.
\newblock Xception: Deep learning with depthwise separable convolutions.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR 2017)}, 2017.
\newblock \doi{10.1109/CVPR.2017.195}.

\bibitem[Dabney et~al.(2018)Dabney, Rowland, Bellemare, and Munos]{Dabney2018}
Dabney, W., Rowland, M., Bellemare, M., and Munos, R.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{Proceedings of the 32nd AAAI Conference on Artificial
  Intelligence (AAAI-18)}, 2018.
\newblock \doi{10.1609/aaai.v32i1.11791}.

\bibitem[Dozat(2016)]{Dozat2016}
Dozat, T.
\newblock Incorporating {N}esterov momentum into {A}dam.
\newblock In \emph{The 4th International Conference on Learning Representations
  (ICLR 2016)}, 2016.
\newblock URL \url{https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf}.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{Ernst2005}
Ernst, D., Geurts, P., and Wehenkel, L.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 6:\penalty0 503--556,
  2005.
\newblock URL \url{http://jmlr.org/papers/v6/ernst05a.html}.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, Legg, and Kavukcuoglu]{Espeholt2018}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu, K.
\newblock {IMPALA}: Scalable distributed deep-{RL} with importance weighted
  actor-learner architectures.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML 2018)}, pp.\  1407--1416, 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/espeholt18a.html}.

\bibitem[Farahmand \& Szepesvari(2010)Farahmand and Szepesvari]{Farahmand2010}
Farahmand, A.~M. and Szepesvari, C.
\newblock Model selection in reinforcement learning.
\newblock \emph{Machine Learning}, 85:\penalty0 299--332, 2010.
\newblock \doi{10.1007/s10994-011-5254-7}.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{Fu2020}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock {D4RL}: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fu et~al.(2021)Fu, Norouzi, Nachum, Tucker, Wang, Novikov, Yang,
  Zhang, Chen, Kumar, Paduraru, Levine, and Paine]{Fu2021}
Fu, J., Norouzi, M., Nachum, O., Tucker, G., Wang, Z., Novikov, A., Yang, M.,
  Zhang, M.~R., Chen, Y., Kumar, A., Paduraru, C., Levine, S., and Paine, T.~L.
\newblock Benchmarks for deep off-policy evaluation.
\newblock In \emph{The 9th International Conference on Learning Representations
  (ICLR 2021)}, 2021.
\newblock URL \url{https://openreview.net/pdf?id=kWSeGEeHvF8}.

\bibitem[Fujimoto et~al.(2022)Fujimoto, Meger, Precup, Nachum, and
  Gu]{Fujimoto2022}
Fujimoto, S., Meger, D., Precup, D., Nachum, O., and Gu, S.~S.
\newblock Why should {I} trust you, {B}ellman? the {B}ellman error is a poor
  replacement for value error.
\newblock \emph{arXiv preprint arXiv:2201.12417}, 2022.

\bibitem[G{\'e}ron(2019)]{Geron2019}
G{\'e}ron, A.
\newblock \emph{Hands-On Machine Learning with {S}cikit-Learn, {K}eras and
  {T}ensor{F}low}.
\newblock O’Reilly, second edition, 2019.
\newblock ISBN 9781492032649.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{Glorot2010}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics (AISTATS 2010)}, pp.\  249--256, 2010.
\newblock URL \url{https://proceedings.mlr.press/v9/glorot10a.html}.

\bibitem[Gulcehre et~al.(2020)Gulcehre, Wang, Novikov, Paine, G{\'o}mez, Zolna,
  Agarwal, Merel, Mankowitz, Paduraru, et~al.]{gulcehre2020}
Gulcehre, C., Wang, Z., Novikov, A., Paine, T., G{\'o}mez, S., Zolna, K.,
  Agarwal, R., Merel, J.~S., Mankowitz, D.~J., Paduraru, C., et~al.
\newblock {RL} {U}nplugged: A suite of benchmarks for offline reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS
  2020)}, volume~33, pp.\  7248--7259, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/51200d29d1fc15f5a71c1dab4bb54f7c-Paper.pdf}.

\bibitem[Hasselt et~al.(2016)Hasselt, Guez, and Silver]{Hasselt2016}
Hasselt, H.~v., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double {Q}-learning.
\newblock In \emph{Proceedings of the 30th AAAI Conference on Artificial
  Intelligence (AAAI-2016)}, pp.\  2094–2100, 2016.
\newblock \doi{10.1609/aaai.v30i1.10295}.

\bibitem[Hastie et~al.(2009)Hastie, Tibshirani, and Friedman]{Hastie2009}
Hastie, T., Tibshirani, R., and Friedman, J.
\newblock \emph{The Elements of Statistical Learning}.
\newblock Springer, 2009.
\newblock \doi{10.1007/978-0-387-84858-7}.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{He2015}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  {I}mage{N}et classification.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV 2015)}, pp.\  1026--1034, 2015.
\newblock \doi{10.1109/ICCV.2015.123}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He2016}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR 2016)}, 2016.
\newblock \doi{10.1109/CVPR.2016.90}.

\bibitem[Hu et~al.(2018)Hu, Shen, and Sun]{Hu2018}
Hu, J., Shen, L., and Sun, G.
\newblock {S}queeze-and-{E}xcitation networks.
\newblock In \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pp.\  7132--7141, 2018.
\newblock \doi{10.1109/CVPR.2018.00745}.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{Ioffe2015}
Ioffe, S. and Szegedy, C.
\newblock {B}atch {N}ormalization: Accelerating deep network training by
  reducing internal covariate shift.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning (ICML 2015)}, pp.\  448--456, 2015.
\newblock URL \url{http://proceedings.mlr.press/v37/ioffe15.html}.

\bibitem[Irpan et~al.(2019)Irpan, Rao, Bousmalis, Harris, Ibarz, and
  Levine]{Irpan2019}
Irpan, A., Rao, K., Bousmalis, K., Harris, C., Ibarz, J., and Levine, S.
\newblock Off-policy evaluation via off-policy classification.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS
  2019)}, volume~32, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/b5b03f06271f8917685d14cea7c6c50a-Paper.pdf}.

\bibitem[Janner et~al.(2019)Janner, Fu, Zhang, and Levine]{Janner2019}
Janner, M., Fu, J., Zhang, M., and Levine, S.
\newblock When to trust your model: Model-based policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems
  (NeurIPS2019)}, 32, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2019/file/5faf461eff3099671ad63c6f3f094f7f-Paper.pdf}.

\bibitem[Jiang \& Li(2016)Jiang and Li]{Jiang2016}
Jiang, N. and Li, L.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning (ICML 2016)}, pp.\  652--661, 2016.
\newblock URL \url{http://proceedings.mlr.press/v48/jiang16.html}.

\bibitem[Kahn et~al.(2021)Kahn, Abbeel, and Levine]{Khan2021}
Kahn, G., Abbeel, P., and Levine, S.
\newblock {BADGR}: An autonomous self-supervised learning-based navigation
  system.
\newblock \emph{IEEE Robotics and Automation Letters}, 6\penalty0 (2):\penalty0
  1312--1319, 2021.
\newblock \doi{10.1109/LRA.2021.3057023}.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, et~al.]{Kalashnikov2018}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E.,
  Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et~al.
\newblock {QT}-{O}pt: Scalable deep reinforcement learning for vision-based
  robotic manipulation.
\newblock In \emph{2nd Conference on Robot Learning (CoRL 2018)}, 2018.
\newblock \doi{10.48550/arXiv.1806.10293}.

\bibitem[Kazdin et~al.(2000)Kazdin, Association, et~al.]{Kazdin2000}
Kazdin, A.~E., Association, A.~P., et~al.
\newblock \emph{Encyclopedia of Psychology}, volume~8.
\newblock American Psychological Association Washington, DC, 2000.
\newblock ISBN 9781557981875.

\bibitem[Klasnja et~al.(2015)Klasnja, Hekler, Shiffman, Boruvka, Almirall,
  Tewari, and Murphy]{Klasnja2015}
Klasnja, P., Hekler, E.~B., Shiffman, S., Boruvka, A., Almirall, D., Tewari,
  A., and Murphy, S.~A.
\newblock Microrandomized trials: An experimental design for developing
  just-in-time adaptive interventions.
\newblock \emph{Health Psychology}, 34\penalty0 (S):\penalty0 1220, 2015.
\newblock \doi{10.1037/hea0000305}.

\bibitem[Kostrikov \& Nachum(2020)Kostrikov and Nachum]{Kostrikov2020}
Kostrikov, I. and Nachum, O.
\newblock Statistical bootstrapping for uncertainty estimation in off-policy
  evaluation.
\newblock \emph{arXiv preprint arXiv:2007.13609v1}, 2020.

\bibitem[Kotz et~al.(2006)Kotz, Read, Balakrishnan, Vidakovic, and
  Johnson]{Kotz2006}
Kotz, S., Read, C.~B., Balakrishnan, N., Vidakovic, B., and Johnson, N.~L.
\newblock \emph{Encyclopedia of Statistical Sciences}.
\newblock John Wiley {\&} Sons, 2006.
\newblock \doi{10.1002/0471667196}.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and Levine]{Kumar2020}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative {Q}-learning for offline reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS
  2020)}, volume~33, pp.\  1179--1191, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/0d2b2061826a5df3221116a5085a6052-Paper.pdf}.

\bibitem[Kumar et~al.(2021{\natexlab{a}})Kumar, Agarwal, Ghosh, and
  Levine]{kumar2021implicit}
Kumar, A., Agarwal, R., Ghosh, D., and Levine, S.
\newblock Implicit under-parameterization inhibits data-efficient deep
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=O9bnihsFfXU}.

\bibitem[Kumar et~al.(2021{\natexlab{b}})Kumar, Singh, Tian, Finn, and
  Levine]{Kumar2021}
Kumar, A., Singh, A., Tian, S., Finn, C., and Levine, S.
\newblock A workflow for offline model-free robotic reinforcement learning.
\newblock In \emph{5th Conference on Robot Learning (CoRL 2021)},
  2021{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2109.10813}.

\bibitem[Lagoudakis \& Parr(2003)Lagoudakis and Parr]{Lagoudakis2003}
Lagoudakis, M.~G. and Parr, R.~E.
\newblock Least-squares policy iteration.
\newblock \emph{Journal of Machine Learning Research}, 4:\penalty0 1107--1149,
  2003.
\newblock URL \url{https://www.jmlr.org/papers/v4/lagoudakis03a.html}.

\bibitem[Le et~al.(2019)Le, Voloshin, and Yue]{Le2019}
Le, H., Voloshin, C., and Yue, Y.
\newblock Batch policy learning under constraints.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning (ICML 2019)}, pp.\  3703--3712, 2019.
\newblock URL \url{http://proceedings.mlr.press/v97/le19a.html}.

\bibitem[Lee et~al.(2022)Lee, Tucker, Nachum, Dai, and Brunskill]{Lee2022}
Lee, J.~N., Tucker, G., Nachum, O., Dai, B., and Brunskill, E.
\newblock Oracle inequalities for model selection in offline reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2211.02016}, 2022.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{Levine2020}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Liao et~al.(2021)Liao, Klasnja, and Murphy]{Liao2021}
Liao, P., Klasnja, P., and Murphy, S.
\newblock Off-policy estimation of long-term average outcomes with applications
  to mobile health.
\newblock \emph{Journal of the American Statistical Association}, 116\penalty0
  (533):\penalty0 382--391, 2021.
\newblock \doi{10.1080/01621459.2020.1807993}.

\bibitem[Liao et~al.(2022)Liao, Qi, Wan, Klasnja, and Murphy]{Liao2022}
Liao, P., Qi, Z., Wan, R., Klasnja, P., and Murphy, S.~A.
\newblock Batch policy learning in average reward {M}arkov decision processes.
\newblock \emph{The Annals of Statistics}, 50\penalty0 (6):\penalty0 3364 --
  3387, 2022.
\newblock \doi{10.1214/22-AOS2231}.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{Lillicrap2016}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N. M.~O., Erez, T., Tassa,
  Y., Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{The 4th International Conference on Learning Representations
  (ICLR 2016)}, 2016.
\newblock URL \url{https://arxiv.org/pdf/1509.02971v6.pdf}.

\bibitem[Luckett et~al.(2020)Luckett, Laber, Kahkoska, Maahs, Mayer‐Davis,
  and Kosorok]{Luckett2020}
Luckett, D.~J., Laber, E.~B., Kahkoska, A.~R., Maahs, D.~M., Mayer‐Davis,
  E.~J., and Kosorok, M.~R.
\newblock Estimating dynamic treatment regimes in mobile health using
  {V}-learning.
\newblock \emph{Journal of the American Statistical Association}, 115:\penalty0
  692--706, 2020.
\newblock \doi{10.1080/01621459.2018.1537919}.

\bibitem[Miyaguchi(2022)]{Miyaguchi2022}
Miyaguchi, K.
\newblock Hyperparameter selection methods for fitted {Q}-evaluation with error
  guarantee.
\newblock \emph{arXiv preprint arXiv:2201.02300v2}, 2022.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{Mnih2015}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M.~A., Fidjeland, A., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518:\penalty0 529--533, 2015.
\newblock \doi{10.1038/nature14236}.

\bibitem[Munos(2005)]{Munos2005}
Munos, R.
\newblock Error bounds for approximate value iteration.
\newblock In \emph{Proceedings of the 20th National Conference on Artificial
  Intelligence (AAAI-05)}, pp.\  1006–--1011, 2005.
\newblock URL \url{https://www.aaaipress.org/Papers/AAAI/2005/AAAI05-159.pdf}.

\bibitem[Munos \& Szepesv{{\'a}}ri(2008)Munos and Szepesv{{\'a}}ri]{Munos2008}
Munos, R. and Szepesv{{\'a}}ri, C.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0
  (27):\penalty0 815--857, 2008.
\newblock URL \url{http://jmlr.org/papers/v9/munos08a.html}.

\bibitem[Nie et~al.(2022)Nie, Flet-Berliac, Jordan, Steenbergen, and
  Brunskill]{Nie2022}
Nie, A., Flet-Berliac, Y., Jordan, D., Steenbergen, W., and Brunskill, E.
\newblock Data-efficient pipeline for offline reinforcement learning with
  limited data.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS
  2022)}, volume~35, pp.\  14810--14823, 2022.

\bibitem[Osa et~al.(2018)Osa, Pajarinen, Neumann, Bagnell, Abbeel, Peters,
  et~al.]{Osa2018}
Osa, T., Pajarinen, J., Neumann, G., Bagnell, J.~A., Abbeel, P., Peters, J.,
  et~al.
\newblock An algorithmic perspective on imitation learning.
\newblock \emph{Foundations and Trends{\textregistered} in Robotics},
  7\penalty0 (1-2):\penalty0 1--179, 2018.
\newblock \doi{10.1561/2300000053}.

\bibitem[Paine et~al.(2020)Paine, Paduraru, Michi, Gulcehre, Zolna, Novikov,
  Wang, and de~Freitas]{Paine2020}
Paine, T.~L., Paduraru, C., Michi, A., Gulcehre, C., Zolna, K., Novikov, A.,
  Wang, Z., and de~Freitas, N.
\newblock Hyperparameter selection for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.09055}, 2020.

\bibitem[Precup et~al.(2000)Precup, Sutton, and Singh]{Precup2000}
Precup, D., Sutton, R.~S., and Singh, S.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock In \emph{Proceedings of the 17th International Confrence on Machine
  Learning (ICML 2000)}, pp.\ ~80, 2000.
\newblock URL \url{https://scholarworks.umass.edu/cs_faculty_pubs/80}.

\bibitem[Prudencio et~al.(2023)Prudencio, Maximo, and Colombini]{Prudencio2023}
Prudencio, R.~F., Maximo, M. R. O.~A., and Colombini, E.~L.
\newblock A survey on offline reinforcement learning: Taxonomy, review, and
  open problems.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems
  (Early Access)}, pp.\  1--0, 2023.
\newblock \doi{10.1109/TNNLS.2023.3250269}.

\bibitem[Puterman(1994)]{Putterman1994}
Puterman, M.~L.
\newblock \emph{{M}arkov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, 1994.
\newblock \doi{10.1002/9780470316887}.

\bibitem[Rafailov et~al.(2021)Rafailov, Yu, Rajeswaran, and Finn]{Rafailov2021}
Rafailov, R., Yu, T., Rajeswaran, A., and Finn, C.
\newblock Offline reinforcement learning from images with latent space models.
\newblock In \emph{Proceedings of the 3rd Conference on Learning for Dynamics
  and Control (L4DC 2021)}, pp.\  1154--1168, 2021.
\newblock URL \url{https://proceedings.mlr.press/v144/rafailov21a.html}.

\bibitem[Randl\o{}v \& Alstr\o{}m(1998)Randl\o{}v and Alstr\o{}m]{Randlov1998}
Randl\o{}v, J. and Alstr\o{}m, P.
\newblock Learning to drive a bicycle using reinforcement learning and shaping.
\newblock In \emph{Proceedings of the 15th International Conference on Machine
  Learning (ICML 1998)}, pp.\  463–471, 1998.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock {ImageNet} large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.

\bibitem[Schrittwieser et~al.(2021)Schrittwieser, Hubert, Mandhane, Barekatain,
  Antonoglou, and Silver]{schrittwieser2021}
Schrittwieser, J., Hubert, T., Mandhane, A., Barekatain, M., Antonoglou, I.,
  and Silver, D.
\newblock Online and offline reinforcement learning by planning with a learned
  model.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS
  2021)}, 34:\penalty0 27580--27591, 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/e8258e5140317ff36c7f8225a3bf9590-Paper.pdf}.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{Schulman2015}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning (ICML 2015)}, pp.\  1889--1897, 2015.
\newblock URL \url{https://proceedings.mlr.press/v37/schulman15.html}.

\bibitem[Shi et~al.(2021)Shi, Zhang, Lu, and Song]{Shi2021}
Shi, C., Zhang, S., Lu, W., and Song, R.
\newblock Statistical inference of the value function for reinforcement
  learning in infinite-horizon settings.
\newblock \emph{Journal of the Royal Statistical Society Series B: Statistical
  Methodology}, 84\penalty0 (3):\penalty0 765--793, 2021.
\newblock \doi{10.1111/rssb.12465}.

\bibitem[Shi et~al.(2022)Shi, Zhu, Ye, Luo, Zhu, and Song]{Shi2022}
Shi, C., Zhu, J., Ye, S., Luo, S., Zhu, H., and Song, R.
\newblock Off-policy confidence interval estimation with confounded markov
  decision process.
\newblock \emph{Journal of the American Statistical Association}, 0\penalty0
  (0):\penalty0 1--12, 2022.
\newblock \doi{10.1080/01621459.2022.2110878}.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{Sutton2018}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock The MIT Press, second edition, 2018.
\newblock ISBN 9780262039246.

\bibitem[Tang \& Wiens(2021)Tang and Wiens]{Tang2021}
Tang, S. and Wiens, J.
\newblock Model selection for offline reinforcement learning: Practical
  considerations for healthcare settings.
\newblock In \emph{Proceedings of the 6th Machine Learning for Healthcare
  Conference (MLHC 2021)}, pp.\  2--35, 2021.
\newblock URL \url{https://proceedings.mlr.press/v149/tang21a.html}.

\bibitem[Thomas \& Brunskill(2016)Thomas and Brunskill]{Thomas2016}
Thomas, P. and Brunskill, E.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning (ICML 2016)}, pp.\  2139--2148, 2016.
\newblock URL \url{https://proceedings.mlr.press/v48/thomasa16.html}.

\bibitem[Thomas et~al.(2015)Thomas, Theocharous, and Ghavamzadeh]{Thomas2015}
Thomas, P., Theocharous, G., and Ghavamzadeh, M.
\newblock High-confidence off-policy evaluation.
\newblock In \emph{Proceedings of the 29th AAAI Conference on Artificial
  Intelligence (AAAI-15)}, pp.\  2094–2100, 2015.
\newblock \doi{10.1609/aaai.v29i1.9541}.

\bibitem[Tsiatis et~al.(2019)Tsiatis, Davidian, Holloway, and
  Laber]{Tsiatis2021}
Tsiatis, A.~A., Davidian, M., Holloway, S.~T., and Laber, E.~B.
\newblock \emph{Dynamic Treatment Regimes: Statistical Methods for Precision
  Medicine}.
\newblock CRC Press, first edition, 2019.
\newblock \doi{10.1201/9780429192692}.

\bibitem[Uehara et~al.(2021)Uehara, Imaizumi, Jiang, Kallus, Sun, and
  Xie]{Uehara2022}
Uehara, M., Imaizumi, M., Jiang, N., Kallus, N., Sun, W., and Xie, T.
\newblock Finite sample analysis of minimax offline reinforcement learning:
  Completeness, fast rates and first-order efficiency.
\newblock \emph{arXiv preprint arXiv:2102.02981}, 2021.

\bibitem[Voloshin et~al.(2021{\natexlab{a}})Voloshin, Jiang, and
  Yue]{Voloshin2021model}
Voloshin, C., Jiang, N., and Yue, Y.
\newblock Minimax model learning.
\newblock In \emph{Proceedings of The 24th International Conference on
  Artificial Intelligence and Statistics (AISTATS 2021)}, pp.\  1612--1620,
  2021{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v130/voloshin21a.html}.

\bibitem[Voloshin et~al.(2021{\natexlab{b}})Voloshin, Le, Jiang, and
  Yue]{Voloshin2021}
Voloshin, C., Le, H.~M., Jiang, N., and Yue, Y.
\newblock Empirical study of off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{35th Conference on Neural Information Processing Systems
  (NeurIPS 2021) Track on Datasets and Benchmarks}, 2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/pdf?id=IsK8iKbL-I}.

\bibitem[Wang et~al.(2016)Wang, Schaul, Hessel, Hasselt, Lanctot, and
  Freitas]{Wang2016}
Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., and Freitas, N.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning (ICML 2016)}, pp.\  1995--2003, 2016.
\newblock URL \url{http://proceedings.mlr.press/v48/wangf16.html}.

\bibitem[Weltz et~al.(2022)Weltz, Volfovsky, and Laber]{Weltz2022}
Weltz, J., Volfovsky, A., and Laber, E.~B.
\newblock Reinforcement learning methods in public health.
\newblock \emph{Clinical Therapeutics}, 44\penalty0 (1):\penalty0 139--154,
  2022.
\newblock \doi{10.1016/j.clinthera.2021.11.002}.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{Wu2019}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Xie \& Jiang(2021)Xie and Jiang]{Xie2021}
Xie, T. and Jiang, N.
\newblock Batch value-function approximation with only realizability.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning (ICML 2021)}, pp.\  11404--11413, 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/xie21d.html}.

\bibitem[Xie et~al.(2019)Xie, Ma, and Wang]{Xie2019}
Xie, T., Ma, Y., and Wang, Y.-X.
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS
  2019)}, volume~32, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/4ffb0d2ba92f664c2281970110a2e071-Paper.pdf}.

\bibitem[Yang et~al.(2020)Yang, Nachum, Dai, Li, and Schuurmans]{Yang2020}
Yang, M., Nachum, O., Dai, B., Li, L., and Schuurmans, D.
\newblock Off-policy evaluation via the regularized {L}agrangian.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS
  2020)}, volume~33, pp.\  6551--6561, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc//paper/2020/file/488e4104520c6aab692863cc1dba45af-Paper.pdf}.

\bibitem[Yu et~al.(2021)Yu, Liu, Nemati, and Yin]{Yu2021}
Yu, C., Liu, J., Nemati, S., and Yin, G.
\newblock Reinforcement learning in healthcare: A survey.
\newblock \emph{ACM Computing Surveys (CSUR)}, 55\penalty0 (1):\penalty0 1--36,
  2021.
\newblock \doi{10.1145/3477600}.

\bibitem[Yu et~al.(2020)Yu, Chen, Wang, Xian, Chen, Liu, Madhavan, and
  Darrell]{Yu2020}
Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., and
  Darrell, T.
\newblock {BDD100K}: A diverse driving dataset for heterogeneous multitask
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR 2020)}, 2020.
\newblock \doi{10.1109/CVPR42600.2020.00271}.

\bibitem[Zhang et~al.(2021)Zhang, Paine, Nachum, Paduraru, Tucker, ziyu wang,
  and Norouzi]{Zhang2021}
Zhang, M.~R., Paine, T., Nachum, O., Paduraru, C., Tucker, G., ziyu wang, and
  Norouzi, M.
\newblock Autoregressive dynamics models for offline policy evaluation and
  optimization.
\newblock In \emph{The 9th International Conference on Learning Representations
  (ICLR 2021)}, 2021.
\newblock URL \url{https://openreview.net/pdf?id=kmqjgSNXby}.

\bibitem[Zhang \& Jiang(2021)Zhang and Jiang]{Zhang2021ps}
Zhang, S. and Jiang, N.
\newblock Towards hyperparameter-free policy selection for offline
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS
  2021)}, volume~34, pp.\  12864--12875, 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/6add07cf50424b14fdf649da87843d01-Paper.pdf}.

\bibitem[Zhu et~al.(2020)Zhu, Lu, and Song]{Zhu2020}
Zhu, L., Lu, W., and Song, R.
\newblock Causal effect estimation and optimal dose suggestions in mobile
  health.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML 2020)}, pp.\  11588--11598, 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/zhu20c.html}.

\end{thebibliography}
