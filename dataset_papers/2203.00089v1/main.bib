% Proximal Point Method
@article{moreau1962fonctions,
  title={{Fonctions convexes duales et points proximaux dans un espace Hilbertien}},
  author={Moreau, Jean Jacques},
  journal={Comptes Rendus Hebdomadaires des S{\'e}ances de l'Acad{\'e}mie des Sciences},
  volume={255},
  pages={2897--2899},
  year={1962}
}

@article{mishkin2018slang,
  title={{Slang: Fast structured covariance approximations for Bayesian deep learning with natural gradient}},
  author={Mishkin, Aaron and Kunstner, Frederik and Nielsen, Didrik and Schmidt, Mark and Khan, Mohammad Emtiyaz},
  journal={arXiv preprint arXiv:1811.04504},
  year={2018}
}

@article{moreau1965proximite,
  title={{Proximit{\'e} et dualit{\'e} dans un espace Hilbertien}},
  author={Moreau, Jean-Jacques},
  journal={Bulletin de la Soci{\'e}t{\'e} Math{\'e}matique de France},
  volume={93},
  pages={273--299},
  year={1965}
}

@inproceedings{gupta2018shampoo,
  title={{Shampoo: Preconditioned stochastic tensor optimization}},
  author={Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1842--1850},
  year={2018}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@article{jacot2018neural,
  title={{Neural tangent kernel: Convergence and generalization in neural networks}},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1806.07572},
  year={2018}
}

@book{martens2016second,
  title={Second-order optimization for neural networks. PhD Thesis.},
  author={Martens, James},
  year={2016},
  publisher={University of Toronto}
}

@article{jiang2019smart,
  title={{Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization}},
  author={Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Zhao, Tuo},
  journal={arXiv preprint arXiv:1911.03437},
  year={2019}
}

@inproceedings{cettolo2014report,
  title={{Report on the 11th IWSLT evaluation campaign}},
  author={Cettolo, Mauro and Niehues, Jan and St{\"u}ker, Sebastian and Bentivogli, Luisa and Federico, Marcello},
  booktitle={Proceedings of the International Workshop on Spoken Language Translation, Hanoi, Vietnam},
  volume={57},
  year={2014}
}

@inproceedings{ott2019fairseq,
  title = {{fairseq: A fast, extensible toolkit for sequence modeling}},
  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
  year = {2019},
}

@article{eckstein1993nonlinear,
  title={{Nonlinear proximal point algorithms using Bregman functions, with applications to convex programming}},
  author={Eckstein, Jonathan},
  journal={Mathematics of Operations Research},
  volume={18},
  number={1},
  pages={202--226},
  year={1993}
}

@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@article{benjamin2018measuring,
  title={Measuring and regularizing networks in function space},
  author={Benjamin, Ari S and Rolnick, David and Kording, Konrad},
  journal={arXiv preprint arXiv:1805.08289},
  year={2018}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={1139--1147},
  year={2013}
}

@article{goldfarb2020practical,
  title={{Practical quasi-Newton methods for training deep neural networks}},
  author={Goldfarb, Donald and Ren, Yi and Bahamou, Achraf},
  journal={arXiv preprint arXiv:2006.08877},
  year={2020}
}

@article{amari1998natural,
  title={Natural gradient works efficiently in learning},
  author={Amari, Shun-Ichi},
  journal={Neural Computation},
  volume={10},
  number={2},
  pages={251--276},
  year={1998}
}

@article{setiono1995use,
  title={{Use of a quasi-Newton method in a feedforward neural network construction algorithm}},
  author={Setiono, Rudy and Hui, Lucas Chi Kwong},
  journal={IEEE Transactions on Neural Networks},
  volume={6},
  number={1},
  pages={273--277},
  year={1995}
}

@inproceedings{rolinek2018l4,
  title={{L4: Practical loss-based stepsize adaptation for deep learning}},
  author={Rolinek, Michal and Martius, Georg},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6433--6443},
  year={2018}
}

@inproceedings{wichrowska2017learned,
  title={Learned optimizers that scale and generalize},
  author={Wichrowska, Olga and Maheswaranathan, Niru and Hoffman, Matthew W and Colmenarejo, Sergio Gomez and Denil, Misha and de Freitas, Nando and Sohl-Dickstein, Jascha},
  booktitle={International Conference on Machine Learning},
  pages={3751--3760},
  year={2017}
}

@article{ochs2016techniques,
  title={Techniques for gradient-based bilevel optimization with non-smooth lower level problems},
  author={Ochs, Peter and Ranftl, Ren{\'e} and Brox, Thomas and Pock, Thomas},
  journal={Journal of Mathematical Imaging and Vision},
  volume={56},
  number={2},
  pages={175--194},
  year={2016}
}

@article{metz2018learned,
  title={{Learned optimizers that outperform SGD on wall-clock and validation loss}},
  author={Metz, Luke and Maheswaranathan, Niru and Nixon, Jeremy and Freeman, C Daniel and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1810.10180},
  year={2018}
}

@inproceedings{franceschi2017forward,
  title={Forward and reverse gradient-based hyperparameter optimization},
  author={Franceschi, Luca and Donini, Michele and Frasconi, Paolo and Pontil, Massimiliano},
  booktitle={International Conference on Machine Learning},
  pages={1165--1173},
  year={2017}
}

@inproceedings{domke2012generic,
  title={Generic methods for optimization-based modeling},
  author={Domke, Justin},
  booktitle={Artificial Intelligence and Statistics},
  pages={318--326},
  year={2012}
}

@inproceedings{duchi2010composite,
  title={Composite Objective Mirror Descent},
  author={Duchi, John C and Shalev-Shwartz, Shai and Singer, Yoram and Tewari, Ambuj},
  booktitle={COLT},
  pages={14--26},
  year={2010}
}

@article{beck2003mirror,
  title={Mirror descent and nonlinear projected subgradient methods for convex optimization},
  author={Beck, Amir and Teboulle, Marc},
  journal={Operations Research Letters},
  volume={31},
  number={3},
  pages={167--175},
  year={2003}
}

@article{parikh2014proximal,
  title={Proximal algorithms},
  author={Parikh, Neal and Boyd, Stephen and others},
  journal={Foundations and Trends{\textregistered} in Optimization},
  volume={1},
  number={3},
  pages={127--239},
  year={2014}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={1139--1147},
  year={2013}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{dauphin2014identifying,
  title={Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
  author={Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2933--2941},
  year={2014}
}

@article{martens2014new,
  title={New insights and perspectives on the natural gradient method},
  author={Martens, James},
  journal={arXiv preprint arXiv:1412.1193},
  year={2014}
}

@misc{Tieleman2012,
  title={{Lecture 6.5---RMSprop: Divide the gradient by a running average of its recent magnitude}},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  howpublished={{COURSERA: Neural Networks for Machine Learning}},
  year={2012}
}

@misc{GrosseNNTDChapter4,
  title={{University of Toronto CSC2541, Topics in Machine Learning: Neural Net Training Dynamics, Chapter 4: Second-Order Optimization}},
  author={Grosse, Roger},
  howpublished={{Lecture Notes}},
  year={2021},
  url={https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021/readings/L04_second_order.pdf},
}

@inproceedings{kingma2014adam,
  title={Adam: {A} method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={International Conference on Learning Representations},
  year={2015}
}

@inproceedings{smith2017don,
  title={Don't Decay the Learning Rate, Increase the Batch Size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

@inproceedings{martens2010deep,
  title={{Deep learning via Hessian-free optimization}},
  author={Martens, James},
  booktitle={International Conference on Machine Learning},
  year={2010}
}

@inproceedings{wu2018understanding,
  title={Understanding Short-Horizon Bias in Stochastic Meta-Optimization},
  author={Wu, Yuhuai and Ren, Mengye and Liao, Renjie and Grosse, Roger},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={1889--1897},
  year={2015}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}


@article{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1703.04933},
  year={2017}
}

@article{li2017visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Goldstein, Tom},
  journal={arXiv preprint arXiv:1712.09913},
  year={2017}
}

@article{mandt2017stochastic,
  title={Stochastic gradient descent as approximate {B}ayesian inference},
  author={Mandt, Stephan and Hoffman, Matthew D and Blei, David M},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={4873--4907},
  year={2017}
}

@article{smith2017understanding,
  title={Understanding Generalization and Stochastic Gradient Descent},
  author={Smith, Samuel L and Le, Quoc V},
  journal={International Conference on Learning Representations},
  year={2018}
}


@article{twolayerlinear,
  title={Reflections on random kitchen sinks},
  author={Recht, Benjamin and Rahimi, Ali},
  url={http://www.argmin.net/2017/12/05/kitchen-sinks/},
  year={2017}
  }

@inproceedings{maclaurin2015gradient,
  title={Gradient-based hyperparameter optimization through reversible learning},
  author={Maclaurin, Dougal and Duvenaud, David and Adams, Ryan},
  booktitle={International Conference on Machine Learning},
  pages={2113--2122},
  year={2015}
}

@article{bergstra2012random,
  title={Random search for hyper-parameter optimization},
  author={Bergstra, James and Bengio, Yoshua},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={Feb},
  pages={281--305},
  year={2012}
}

@inproceedings{schaul2013no,
  title={No more pesky learning rates},
  author={Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  booktitle={International Conference on Machine Learning},
  pages={343--351},
  year={2013}
}

@inproceedings{schraudolph1999local,
  title={Local Gain Adaptation in Stochastic Gradient Descent},
  author={Schraudolph, Nicol N},
  booktitle={Ninth International Conference on Artificial Neural Networks},
  volume={2},
  pages={569--574},
  year={1999}
}

@article{baydin2017online,
  title={Online learning rate adaptation with hypergradient descent},
  author={Baydin, Atilim G and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank},
  journal={arXiv preprint arXiv:1703.04782},
  year={2017}
}

@article{donini2019marthe,
  title={{MARTHE: Scheduling the learning rate via online hypergradients}},
  author={Donini, Michele and Franceschi, Luca and Pontil, Massimiliano and Majumder, Orchid and Frasconi, Paolo},
  journal={arXiv preprint arXiv:1910.08525},
  year={2019}
}

@article{michal2018l4,
  title={L4: {P}ractical loss-based stepsize adaptation for deep learning},
  author={Michal, Rolinek and Georg, Martius},
  journal={arXiv preprint arXiv:1802.05074},
  year={2018}
}


@article{threewdecay,
  title={Three Mechanisms of Weight Decay Regularization},
  author={Zhang, Guodong and Wang, Chaoqi and Xu, Bowen and Grosse, Roger},
  journal={arXiv preprint arXiv:1810.12281},
  year={2018}
}


@article{xiao2017fashion,
  title={{Fashion-MNIST: A} Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}


@inproceedings{krizhevsky2012imagenet,
  title={{ImageNet} classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1097--1105},
  year={2012}
}

@inproceedings{deng2009imagenet,
  title={{ImageNet: A large-scale hierarchical image database}},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={Computer Vision and Pattern Recognition},
  pages={248--255},
  year={2009}
}

@article{goyal2017accurate,
  title={{Accurate, large minibatch SGD: Training ImageNet in 1 hour}},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@incollection{bengio2012practical,
  title={Practical recommendations for gradient-based training of deep architectures},
  author={Bengio, Yoshua},
  booktitle={{Neural Networks: Tricks of the Trade}},
  pages={437--478},
  year={2012},
  publisher={Springer}
}


% Bayesian optimization
% ---------------------

@inproceedings{snoek2012practical,
  title={Practical {B}ayesian optimization of machine learning algorithms},
  author={Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2951--2959},
  year={2012}
}

@inproceedings{snoek2015scalable,
  title={Scalable {B}ayesian optimization using deep neural networks},
  author={Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Mostofa and Prabhat, Prabhat and Adams, Ryan},
  booktitle={International Conference on Machine Learning},
  pages={2171--2180},
  year={2015}
}

@inproceedings{jamieson2016non,
  title={Non-stochastic best arm identification and hyperparameter optimization},
  author={Jamieson, Kevin and Talwalkar, Ameet},
  booktitle={19th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  volume={41},
  pages={240--248},
  year={2016}
}

@article{swersky2014freeze,
  title={Freeze-thaw {B}ayesian optimization},
  author={Swersky, Kevin and Snoek, Jasper and Adams, Ryan P},
  journal={arXiv preprint arXiv:1406.3896},
  year={2014}
}


# Hyperband and PBT
# -----------------

@article{li2016hyperband,
  title={{Hyperband: Bandit-based configuration evaluation for hyperparameter optimization}},
  author={Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:1603.06560},
  year={2016}
}

@article{jaderberg2017population,
  title={Population-based training of neural networks},
  author={Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and others},
  journal={arXiv preprint arXiv:1711.09846},
  year={2017}
}


# Literature for learning optimization algorithms
# -----------------------------------------------
@article{li2016learning,
  title={Learning to optimize},
  author={Li, Ke and Malik, Jitendra},
  journal={arXiv preprint arXiv:1606.01885},
  year={2016}
}

@article{li2017learning,
  title={Learning to optimize neural nets},
  author={Li, Ke and Malik, Jitendra},
  journal={arXiv preprint arXiv:1703.00441},
  year={2017}
}

@inproceedings{andrychowicz2016learning,
  title={Learning to learn by gradient descent by gradient descent},
  author={Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3981--3989},
  year={2016}
}

@inproceedings{sutton2000policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1057--1063},
  year={2000}
}

# Batch norm references
# ---------------------

@article{ioffe2015batch,
  title={Batch normalization: {A}ccelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

@article{van2017l2,
  title={L2 regularization versus batch and weight normalization},
  author={van Laarhoven, Twan},
  journal={arXiv preprint arXiv:1706.05350},
  year={2017}
}

@article{hoffer2018norm,
  title={Norm matters: {E}fficient and accurate normalization schemes in deep networks},
  author={Hoffer, Elad and Banner, Ron and Golan, Itay and Soudry, Daniel},
  journal={arXiv preprint arXiv:1803.01814},
  year={2018}
}


@inproceedings{
lucas2018aggregated,
title={{Aggregated momentum: Stability through passive damping}},
author={James Lucas and Shengyang Sun and Richard Zemel and Roger  Grosse},
booktitle={International Conference on Learning Representations},
year={2019}
}


# Architecture and dataset references
# -----------------------------------

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  booktitle={NIPS Workshop on Deep Learning and Unsupervised Feature Learning},
  year={2011}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@techreport{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey},
  year={2009},
  institution={University of Toronto}
}


# RNN references
# ----------------

@inproceedings{gal2016theoretically,
  title={A theoretically grounded application of dropout in recurrent neural networks},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1019--1027},
  year={2016}
}

@inproceedings{merity2017regularizing,
  title={{Regularizing and optimizing LSTM language models}},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{marcus1993building,
  title={{Building a large annotated corpus of English: The Penn Treebank}},
  author={Marcus, Mitchell P and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
  journal={Computational Linguistics},
  volume={19},
  number={2},
  pages={313--330},
  year={1993},
  publisher={MIT Press}
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}

@book{nesterov2013introductory,
  title={Introductory Lectures on Convex Optimization: A Basic Course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}


% Second-order optimizers
% =======================

@article{yao2020adahessian,
  title={{ADAHESSIAN: An adaptive second order optimizer for machine learning}},
  author={Yao, Zhewei and Gholami, Amir and Shen, Sheng and Mustafa, Mustafa and Keutzer, Kurt and Mahoney, Michael W},
  journal={arXiv preprint arXiv:2006.00719},
  year={2020}
}

@inproceedings{martens2011learning,
  title={{Learning recurrent neural networks with Hessian-free optimization}},
  author={Martens, James and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  year={2011}
}

% KFAC
@inproceedings{martens2015optimizing,
  title={{Optimizing neural networks with Kronecker-factored approximate curvature}},
  author={Martens, James and Grosse, Roger},
  booktitle={International Conference on Machine Learning},
  pages={2408--2417},
  year={2015}
}

% KFAC for CNNs
@inproceedings{grosse2016kronecker,
  title={{A Kronecker-factored approximate Fisher matrix for convolution layers}},
  author={Grosse, Roger and Martens, James},
  booktitle={International Conference on Machine Learning},
  pages={573--582},
  year={2016}
}

% KFAC for RNNs
@inproceedings{martens2018kronecker,
  title={Kronecker-factored curvature approximations for recurrent neural networks},
  author={Martens, James and Ba, Jimmy and Johnson, Matt},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

% Distributed KFAC
@inproceedings{ba2016distributed,
  title={{Distributed second-order optimization using Kronecker-factored approximations}},
  author={Ba, Jimmy and Grosse, Roger and Martens, James},
  booktitle={International Conference on Learning Representations},
  year={2017}
}

% EKFAC
@article{george2018fast,
  title={{Fast approximate natural gradient descent in a Kronecker-factored eigenbasis}},
  author={George, Thomas and Laurent, C{\'e}sar and Bouthillier, Xavier and Ballas, Nicolas and Vincent, Pascal},
  journal={arXiv preprint arXiv:1806.03884},
  year={2018}
}

% SKFAC
@inproceedings{tang2021skfac,
  title={{SKFAC: Training neural networks with faster Kronecker-factored approximate curvature}},
  author={Tang, Zedong and Jiang, Fenlong and Gong, Maoguo and Li, Hao and Wu, Yue and Yu, Fan and Wang, Zidong and Wang, Min},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  pages={13479--13487},
  year={2021}
}

@inproceedings{osawa2019large,
  title={{Large-scale distributed second-order optimization using Kronecker-factored approximate curvature for deep convolutional neural networks}},
  author={Osawa, Kazuki and Tsuji, Yohei and Ueno, Yuichiro and Naruse, Akira and Yokota, Rio and Matsuoka, Satoshi},
  booktitle={Conference on Computer Vision and Pattern Recognition},
  pages={12359--12367},
  year={2019}
}

@article{karakida2020understanding,
  title={{Understanding approximate Fisher information for fast convergence of natural gradient descent in wide neural networks}},
  author={Karakida, Ryo and Osawa, Kazuki},
  journal={arXiv preprint arXiv:2010.00879},
  year={2020}
}

@article{asi2020minibatch,
  title={Minibatch Stochastic Approximate Proximal Point Methods},
  author={Asi, Hilal and Chadha, Karan and Cheng, Gary and Duchi, John C},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

% Natural gradient
@inproceedings{le2007topmoumoute,
  title={Topmoumoute Online Natural Gradient Algorithm.},
  author={Le Roux, Nicolas and Manzagol, Pierre-Antoine and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={849--856},
  year={2007}
}

@inproceedings{becker1988improving,
  title={Improving the Convergence of Back-Propagation Learning with Second Order Methods},
  author={Becker, Sue and Le Cun, Yann},
  booktitle={Proceedings of the 1988 Connecticut Summer School},
  year={1988}
}

@article{bae2018eigenvalue,
  title={Eigenvalue corrected noisy natural gradient},
  author={Bae, Juhan and Zhang, Guodong and Grosse, Roger},
  journal={arXiv preprint arXiv:1811.12565},
  year={2018}
}

% Deep Kernel Shaping
@article{martens2021rapid,
  title={Rapid training of deep neural networks without skip connections or normalization layers using deep kernel shaping},
  author={Martens, James and Ballard, Andy and Desjardins, Guillaume and Swirszcz, Grzegorz and Dalibard, Valentin and Sohl-Dickstein, Jascha and Schoenholz, Samuel S},
  journal={arXiv preprint arXiv:2110.01765},
  year={2021}
}

@article{heskes2000natural,
  title={On “natural” learning and pruning in multilayered perceptrons},
  author={Heskes, Tom},
  journal={Neural Computation},
  volume={12},
  number={4},
  pages={881--901},
  year={2000}
}

@inproceedings{lee2018gradient,
  title={Gradient-based meta-learning with learned layerwise metric and subspace},
  author={Lee, Yoonho and Choi, Seungjin},
  booktitle={International Conference on Machine Learning},
  pages={2927--2936},
  year={2018}
}


@article{hinton2006reducing,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={Science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006}
}

@article{amari2020does,
  title={When Does Preconditioning Help or Hurt Generalization?},
  author={Amari, Shun-ichi and Ba, Jimmy and Grosse, Roger and Li, Xuechen and Nitanda, Atsushi and Suzuki, Taiji and Wu, Denny and Xu, Ji},
  journal={arXiv preprint arXiv:2006.10732},
  year={2020}
}

@incollection{feurer2019hyperparameter,
  title={Hyperparameter optimization},
  author={Feurer, Matthias and Hutter, Frank},
  booktitle={Automated Machine Learning},
  pages={3--33},
  year={2019}
}



@inproceedings{yang2019proxsgd,
  title={{ProxSGD: Training structured neural networks under regularization and constraints}},
  author={Yang, Yang and Yuan, Yaxiong and Chatzimichailidis, Avraam and van Sloun, Ruud JG and Lei, Lei and Chatzinotas, Symeon},
  booktitle={International Conference on Learning Representations},
  year={2019}
}


@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}











% SOFTWARE
@misc{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX: Composable transformations of Python+NumPy programs}},
  url = {http://github.com/google/jax},
  year = {2018},
}

@article{wang2013incremental,
  title={Incremental constraint projection-proximal methods for nonsmooth convex optimization},
  author={Wang, Mengdi and Bertsekas, Dimitri P}
}

@article{asi2019stochastic,
  title={{Stochastic (approximate) proximal point methods: Convergence, optimality, and adaptivity}},
  author={Asi, Hilal and Duchi, John C},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={3},
  pages={2257--2290},
  year={2019}
}


% Truncation Bias and Real-Time Recurrent Learning
@inproceedings{metz2019understanding,
  title={Understanding and correcting pathologies in the training of learned optimizers},
  author={Metz, Luke and Maheswaranathan, Niru and Nixon, Jeremy and Freeman, Daniel and Sohl-Dickstein, Jascha},
  booktitle={International Conference on Machine Learning},
  pages={4556--4565},
  year={2019}
}

@article{zhang2019fast,
  title={Fast convergence of natural gradient descent for overparameterized neural networks},
  author={Zhang, Guodong and Martens, James and Grosse, Roger},
  journal={arXiv preprint arXiv:1905.10961},
  year={2019}
}

@InProceedings{bojar-EtAl:2014:W14-33,
  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale
{s} },
  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {12--58},
  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}
}

@article{vaswani2020adaptive,
  title={Adaptive Gradient Methods Converge Faster with Over-Parameterization (but you should do a line-search)},
  author={Vaswani, Sharan and Laradji, Issam and Kunstner, Frederik and Meng, Si Yi and Schmidt, Mark and Lacoste-Julien, Simon},
  journal={arXiv preprint arXiv:2006.06835},
  year={2020}
}

@article{zhang2019algorithmic,
  title={Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model},
  author={Zhang, Guodong and Li, Lala and Nado, Zachary and Martens, James and Sachdeva, Sushant and Dahl, George and Shallue, Chris and Grosse, Roger B},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={8196--8207},
  year={2019}
}

@inproceedings{loizou2021stochastic,
  title={Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence},
  author={Loizou, Nicolas and Vaswani, Sharan and Laradji, Issam Hadj and Lacoste-Julien, Simon},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1306--1314},
  year={2021},
  organization={PMLR}
}

@misc{
  laurent2018an,
  title={{An evaluation of Fisher approximations beyond Kronecker factorization}},
  author={César Laurent and Thomas George and Xavier Bouthillier and Nicolas Ballas and Pascal Vincent},
  year={2018},
  url={https://openreview.net/forum?id=ryVC6tkwG}
}

@inproceedings{vaswani2019painless,
  title={Painless stochastic gradient: Interpolation, line-search, and convergence rates},
  author={Vaswani, Sharan and Mishkin, Aaron and Laradji, Issam and Schmidt, Mark and Gidel, Gauthier and Lacoste-Julien, Simon},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3727--3740},
  year={2019}
}

@article{hazan2019revisiting,
  title={Revisiting the Polyak step size},
  author={Hazan, Elad and Kakade, Sham},
  journal={arXiv preprint arXiv:1905.00313},
  year={2019}
}

@article{loshchilov2016sgdr,
  title={{SGDR: Stochastic gradient descent with warm restarts}},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{lecun1998mnist,
  title={{The MNIST database of handwritten digits}},
  author={LeCun, Yann},
    year      = {1988},
  journal={http://yann. lecun. com/exdb/mnist/}
}

@article{arora2018theoretical,
  title={Theoretical analysis of auto rate-tuning by batch normalization},
  author={Arora, Sanjeev and Li, Zhiyuan and Lyu, Kaifeng},
  journal={arXiv preprint arXiv:1812.03981},
  year={2018}
}


@inproceedings{schaul2013no,
  title={No more pesky learning rates},
  author={Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  booktitle={International Conference on Machine Learning},
  pages={343--351},
  year={2013}
}

@article{micaelli2020non,
  title={Non-greedy gradient-based hyperparameter optimization over long horizons},
  author={Micaelli, Paul and Storkey, Amos},
  journal={arXiv preprint arXiv:2007.07869},
  year={2020}
}

@article{tallec2017unbiased,
  title={Unbiased online recurrent optimization},
  author={Tallec, Corentin and Ollivier, Yann},
  journal={arXiv preprint arXiv:1702.05043},
  year={2017}
}

@article{mujika2018approximating,
  title={{Approximating real-time recurrent learning with random Kronecker factors}},
  author={Mujika, Asier and Meier, Florian and Steger, Angelika},
  journal={arXiv preprint arXiv:1805.10842},
  year={2018}
}

@inproceedings{benzing2019optimal,
  title={{Optimal Kronecker-sum approximation of real time recurrent learning}},
  author={Benzing, Frederik and Gauy, Marcelo Matheus and Mujika, Asier and Martinsson, Anders and Steger, Angelika},
  booktitle={International Conference on Machine Learning},
  pages={604--613},
  year={2019}
}

@inproceedings{vicol2021unbiased,
  title={Unbiased gradient estimation in unrolled computation graphs with persistent evolution strategies},
  author={Vicol, Paul and Metz, Luke and Sohl-Dickstein, Jascha},
  booktitle={International Conference on Machine Learning},
  pages={10553--10563},
  year={2021}
}

@article{menick2020practical,
  title={A practical sparse approximation for real time recurrent learning},
  author={Menick, Jacob and Elsen, Erich and Evci, Utku and Osindero, Simon and Simonyan, Karen and Graves, Alex},
  journal={arXiv preprint arXiv:2006.07232},
  year={2020}
}



@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@misc{rahimi_recht_2017, title={Reflections on random kitchen sinks}, url={http://www.argmin.net/2017/12/05/kitchen-sinks/}, author={Rahimi, Ali and Recht, Ben}, year={2017}} 

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={1889--1897},
  year={2015}
}

@article{zhang2018three,
  title={Three mechanisms of weight decay regularization},
  author={Zhang, Guodong and Wang, Chaoqi and Xu, Bowen and Grosse, Roger},
  journal={arXiv preprint arXiv:1810.12281},
  year={2018}
}

@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019}
}

@incollection{byrd1996towards,
  title={{Towards a discrete Newton method with memory for large-scale optimization}},
  author={Byrd, Richard H and Nocedal, Jorge and Zhu, Ciyou},
  booktitle={Nonlinear Optimization and Applications},
  pages={1--12},
  year={1996},
  publisher={Springer}
}

@book{fletcher2013practical,
  booktitle={Practical Methods of Optimization},
  author={Fletcher, Roger},
  year={2013},
  publisher={John Wiley \& Sons}
}

@inproceedings{lecun1991second,
  title={{Second order properties of error surfaces: Learning time and generalization}},
  author={LeCun, Yann and Kanter, Ido and Solla, Sara A},
  booktitle={Advances in Neural Information Processing Systems},
  pages={918--924},
  year={1991}
}

@article{flennerhag2019meta,
  title={Meta-learning with warped gradient descent},
  author={Flennerhag, Sebastian and Rusu, Andrei A and Pascanu, Razvan and Visin, Francesco and Yin, Hujun and Hadsell, Raia},
  journal={arXiv preprint arXiv:1909.00025},
  year={2019}
}

@article{moskovitz2019first,
  title={First-Order Preconditioning via Hypergradient Descent},
  author={Moskovitz, Ted and Wang, Rui and Lan, Janice and Kapoor, Sanyam and Miconi, Thomas and Yosinski, Jason and Rawal, Aditya},
  journal={arXiv preprint arXiv:1910.08461},
  year={2019}
}

@article{park2019meta,
  title={Meta-curvature},
  author={Park, Eunbyung and Oliva, Junier B},
  journal={arXiv preprint arXiv:1902.03356},
  year={2019}
}

@misc{asuncion2007uci,
  title={{UCI machine learning repository}},
  author={Asuncion, Arthur and Newman, David},
  year={2007},
  publisher={Irvine, CA, USA}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018}
}

@book{wright1999numerical,
  title={Numerical Optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={1999},
  publisher={Springer}
}

@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The Annals of Mathematical Statistics},
  pages={400--407},
  year={1951}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={7},
  year={2011}
}

@article{hinton2012neural,
  title={Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  journal={},
  volume={14},
  number={8},
  pages={2},
  year={2012}
}


@inproceedings{wadia2021whitening,
  title={Whitening and second order optimization both make information in the dataset unusable during training, and can reduce or prevent generalization},
  author={Wadia, Neha and Duckworth, Daniel and Schoenholz, Samuel S and Dyer, Ethan and Sohl-Dickstein, Jascha},
  booktitle={International Conference on Machine Learning},
  pages={10617--10629},
  year={2021}
}
