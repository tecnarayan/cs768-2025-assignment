\begin{thebibliography}{10}

\bibitem{aghasi2017nettrim}
A.~Aghasi, A.~Abdi, N.~Nguyen, and J.~Romberg.
\newblock {Net-Trim}: Convex pruning of deep neural networks with performance
  guarantee.
\newblock In {\em NeurIPS}, 2017.

\bibitem{alfalou2003recurrent}
A.~Al-Falou and D.~Trummer.
\newblock Identifiability of recurrent neural networks.
\newblock {\em Econometric Theory}, 2003.

\bibitem{albertini1993form}
F.~Albertini and E.~Sontag.
\newblock For neural networks, function determines form.
\newblock {\em Neural Networks}, 1993.

\bibitem{albertini1993identifiability}
F.~Albertini and E.~Sontag.
\newblock Identifiability of discrete-time neural networks.
\newblock In {\em ECC}, 1993.

\bibitem{amodei2018aicompute}
D.~Amodei, D.~Hernandez, G.~Sastry, J.~Clark, G.~Brockman, and I.~Sutskever.
\newblock {AI} and compute.
\newblock \url{https://openai.com/blog/ai-and-compute/}, 2018.
\newblock Accessed: 2020-12-23.

\bibitem{huchette2020mp}
R.~Anderson, J.~Huchette, W.~Ma, C.~Tjandraatmadja, and J.~Vielma.
\newblock Strong mixed-integer programming formulations for trained neural
  networks.
\newblock {\em Mathematical Programming}, 2020.

\bibitem{huchette2019ipco}
R.~Anderson, J.~Huchette, C.~Tjandraatmadja, and J.~Vielma.
\newblock Strong mixed-integer programming formulations for trained neural
  networks.
\newblock In {\em IPCO}, 2019.

\bibitem{arora2018bounds}
S.~Arora, R.~Ge, B.~Neyshabur, and Y.~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In {\em ICML}, 2018.

\bibitem{bergman2019janos}
D.~Bergman, T.~Huang, P.~Brooks, A.~Lodi, and A.~Raghunathan.
\newblock {JANOS}: An integrated predictive and prescriptive modeling
  framework.
\newblock {\em arXiv}, 1911.09461, 2019.

\bibitem{berner2019degeneracy}
J.~Berner, D.~Elbrächter, and P.~Grohs.
\newblock How degenerate is the parametrization of neural networks with the
  relu activation function?
\newblock In {\em NeurIPS}, 2019.

\bibitem{blalock2020survey}
D.~Blalock, J.~Ortiz, J.~Frankle, and J.~Guttag.
\newblock What is the state of neural network pruning?
\newblock In {\em MLSys}, 2020.

\bibitem{botoeva2020dependency}
E.~Botoeva, P.~Kouvaros, J.~Kronqvist, A.~Lomuscio, and R.~Misener.
\newblock Efficient verification of relu-based neural networks via dependency
  analysis.
\newblock In {\em AAAI}, 2020.

\bibitem{bridle1990softmax}
J.~Bridle.
\newblock Probabilistic interpretation of feedforward classification network
  outputs, with relationships to statistical pattern recognition.
\newblock In {\em Neurocomputing}. 1990.

\bibitem{Bunel2017}
R.~Bunel, I.~Turkaslan, P.~Torr, P.~Kohli, and M.~Pawan Kumar.
\newblock Piecewise linear neural network verification: {A} comparative study.
\newblock {\em CoRR}, abs/1711.00455, 2017.

\bibitem{buning2020equivalence}
M.~B\"uning, P.~Kern, and C.~Sinz.
\newblock Verifying equivalence properties of neural networks with relu
  activation functions.
\newblock In {\em CP}, 2020.

\bibitem{chen1993geometry}
A.~Chen, {H.} Lu, and R.~Hecht-Nielsen.
\newblock On the geometry of feedforward neural network error surfaces.
\newblock {\em Neural Computation}, 1993.

\bibitem{cheng2017mip}
C.~Cheng, G.~N{\"u}hrenberg, and H.~Ruess.
\newblock Maximum resilience of artificial neural networks.
\newblock In {\em ATVA}, 2017.

\bibitem{cheng2018survey}
Y.~Cheng, D.~Wang, P.~Zhou, and T.~Zhang.
\newblock Model compression and acceleration for deep neural networks: The
  principles, progress, and challenges.
\newblock {\em IEEE Signal Processing Magazine}, 2018.

\bibitem{cook1971feasibility}
S.~Cook.
\newblock The complexity of theorem-proving procedures.
\newblock In {\em STOC}, 1971.

\bibitem{dantzig1954tsp}
G.~Dantzig, D.~Fulkerson, and S.~Johnson.
\newblock Solution of a large scale traveling salesman problem.
\newblock Technical Report P-510, RAND Corporation, Santa Monica, California,
  USA, 1954.

\bibitem{delarue2020rlvrp}
A.~Delarue, R.~Anderson, and C.~Tjandraatmadja.
\newblock Reinforcement learning with combinatorial actions: An application to
  vehicle routing.
\newblock In {\em NeurIPS}, 2020.

\bibitem{delong2012fast}
A.~Delong, A.~Osokin, H.~Isack, and Y.~Boykov.
\newblock Fast approximate energy minimization with label costs.
\newblock {\em IJCV}, 2012.

\bibitem{denil2013parameters}
M.~Denil, B.~Shakibi, L.~Dinh, M.~Ranzato, and N.~Freitas.
\newblock Predicting parameters in deep learning.
\newblock In {\em NeurIPS}, 2013.

\bibitem{denton2014linear}
E.~Denton, W.~Zaremba, J.~Bruna, Y.~LeCun, and R.~Fergus.
\newblock Exploiting linear structure within convolutional networks for
  efficient evaluation.
\newblock In {\em NeurIPS}, 2014.

\bibitem{dong2017layerwise}
X.~Dong, S.~Chen, and S.~Pan.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon.
\newblock In {\em NeurIPS}, 2017.

\bibitem{elaraby2020importance}
M.~ElAraby, G.~Wolf, and M.~Carvalho.
\newblock Identifying efficient sub-networks using mixed integer programming.
\newblock In {\em OPT Workshop}, 2020.

\bibitem{fefferman1994recovering}
C.~Fefferman and S.~Markel.
\newblock Recovering a feed-forward net from its output.
\newblock In {\em NeurIPS}, 1994.

\bibitem{fischetti2018mip}
M.~Fischetti and J.~Jo.
\newblock Deep neural networks and mixed integer linear optimization.
\newblock {\em Constraints}, 2018.

\bibitem{frankle2019lottery}
J.~Frankle and M.~Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em ICLR}, 2019.

\bibitem{frankle2021initialization}
J.~Frankle, G.~Dziugaite, D.~Roy, and M.~Carbin.
\newblock Pruning neural networks at initialization: Why are we missing the
  mark?
\newblock In {\em ICLR}, 2021.

\bibitem{gordon2020bert}
M.~Gordon, K.~Duh, and N.~Andrews.
\newblock Compressing {BERT}: Studying the effects of weight pruning on
  transfer learning.
\newblock In {\em Rep4NLP Workshop}, 2020.

\bibitem{gurobi2020gurobi}
LLC Gurobi~Optimization.
\newblock Gurobi optimizer reference manual, 2020.
\newblock Version 9.1.

\bibitem{hahnloser2000relu}
R.~Hahnloser, R.~Sarpeshkar, M.~Mahowald, R.~Douglas, and S.~Seung.
\newblock Digital selection and analogue amplification coexist in a
  cortex-inspired silicon circuit.
\newblock {\em Nature}, 2000.

\bibitem{han2016deepcompression}
S.~Han, H.~Mao, and W.~Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and {Huffman} coding.
\newblock In {\em ICLR}, 2016.

\bibitem{han2015connections}
S.~Han, J.~Pool, J.~Tran, and W.~Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In {\em NeurIPS}, 2015.

\bibitem{hanin2019complexity}
B.~Hanin and D.~Rolnick.
\newblock Complexity of linear regions in deep networks.
\newblock In {\em ICML}, 2019.

\bibitem{hanin2019deep}
B.~Hanin and D.~Rolnick.
\newblock Deep {ReLU} networks have surprisingly few activation patterns.
\newblock In {\em NeurIPS}, 2019.

\bibitem{hanson1988minimal}
S.~Hanson and L.~Pratt.
\newblock Comparing biases for minimal network construction with
  back-propagation.
\newblock In {\em NeurIPS}, 1988.

\bibitem{hassibi1993surgeon}
B.~Hassibi, D.~Stork, and G.~Wolff.
\newblock Optimal brain surgeon and general network pruning.
\newblock In {\em IEEE International Conference on Neural Networks}, 1993.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, 2016.

\bibitem{he2017cnn}
Y.~He, X.~Zhang, and J.~Sun.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In {\em ICCV}, 2017.

\bibitem{hechtnielsen1990weightspaces}
R.~Hecht-Nielsen.
\newblock On the algebraic structure of feedforward network weight spaces.
\newblock In {\em Advanced Neural Computers}. 1990.

\bibitem{hooker2019forget}
S.~Hooker, A.~Courville, G.~Clark, Y.~Dauphin, and A.~Frome.
\newblock What do compressed deep neural networks forget?
\newblock {\em arXiv}, 1911.05248, 2019.

\bibitem{hooker2020bias}
S.~Hooker, N.~Moorosi, G.~Clark, S.~Bengio, and E.~Denton.
\newblock Characterising bias in compressed models.
\newblock {\em arXiv}, 2010.03058, 2020.

\bibitem{ioffe2015batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em ICML}, 2015.

\bibitem{jaderberg2014lowrank}
M.~Jaderberg, A.~Vedaldi, and A.~Zisserman.
\newblock Speeding up convolutional neural networks with low rank expansions.
\newblock In {\em BMVC}, 2014.

\bibitem{janowsky1989prunning}
S.~Janowsky.
\newblock Pruning versus clipping in neural networks.
\newblock {\em Physical Review A}, 1989.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{kurkova1994equivalent}
V.~{Kůrková} and P.~{Kainen}.
\newblock Functionally equivalent feedforward neural networks.
\newblock {\em Neural Computation}, 1994.

\bibitem{kumar2019transformations}
A.~Kumar, T.~Serra, and S.~Ramalingam.
\newblock Equivalent and approximate transformations of deep neural networks.
\newblock {\em arXiv}, 1905.11428, 2019.

\bibitem{lebedev2015decomposition}
V.~Lebedev, Y.~Ganin, M.~Rakhuba, I.~Oseledets, and V.~Lempitsky.
\newblock Speeding-up convolutional neural networks using fine-tuned
  {CP}-decomposition.
\newblock In {\em ICLR}, 2015.

\bibitem{lebedev2016braindamage}
V.~Lebedev and V.~Lempitsky.
\newblock Fast {ConvNets} using group-wise brain damage.
\newblock In {\em CVPR}, 2016.

\bibitem{lecun1998mnist}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 1998.

\bibitem{lecun1989damage}
Y.~LeCun, J.~Denker, and S.~Solla.
\newblock Optimal brain damage.
\newblock In {\em NeurIPS}, 1989.

\bibitem{lee2020pretraining}
N.~Lee, T.~Ajanthan, S.~Gould, and P.~Torr.
\newblock A signal propagation perspective for pruning neural networks at
  initialization.
\newblock In {\em ICLR}, 2020.

\bibitem{lee2019pretraining}
N.~Lee, T.~Ajanthan, and P.~Torr.
\newblock {SNIP}: Single-shot network pruning based on connection sensitivity.
\newblock In {\em ICLR}, 2019.

\bibitem{li2017cnn}
H.~Li, A.~Kadav, I.~Durdanovic, H.~Samet, and H.~Graf.
\newblock Pruning filters for efficient convnets.
\newblock In {\em ICLR}, 2017.

\bibitem{li2020understanding}
J.~Li, Y.~Sun, J.~Su, T.~Suzuki, and F.~Huang.
\newblock Understanding generalization in deep learning via tensor methods,
  2020.

\bibitem{lost}
L.~Liebenwein, C.~Baykal, B.~Carter, D.~Gifford, and D.~Rus.
\newblock Lost in pruning: The effects of pruning neural networks beyond test
  accuracy.
\newblock In {\em MLSys}, 2021.

\bibitem{Changliu2019}
C.~Liu, T.~Arnon, C.~Lazarus, C.~Barrett, and M.~Kochenderfer.
\newblock Algorithms for verifying deep neural networks.
\newblock {\em CoRR}, 2019.

\bibitem{liu2019structure}
Z.~Liu, M.~Sun, T.~Zhou, G.~Huang, and T.~Darrell.
\newblock Rethinking the value of network pruning.
\newblock In {\em ICLR}, 2019.

\bibitem{luo2017thinet}
J.~Luo, J.~Wu, and W.~Lin.
\newblock {ThiNet}: A filter level pruning method for deep neural network
  compression.
\newblock In {\em ICCV}, 2017.

\bibitem{mariet2016diversity}
Z.~Mariet and S.~Sra.
\newblock Diversity networks: Neural network compression using determinantal
  point processes.
\newblock In {\em ICLR}, 2016.

\bibitem{molchanov2017taylor}
P.~Molchanov, S.~Tyree, T.~Karras, T.~Aila, and J.~Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock In {\em ICLR}, 2017.

\bibitem{montufar2014linear}
G.~Mont\'{u}far, R.~Pascanu, K.~Cho, and Y.~Bengio.
\newblock On the number of linear regions of deep neural networks.
\newblock In {\em NeurIPS}, 2014.

\bibitem{mozer1989relevance}
M.~Mozer and P.~Smolensky.
\newblock Using relevance to reduce network size automatically.
\newblock {\em Connection Science}, 1989.

\bibitem{narodytska2018verifying}
N.~Narodytska, S.~Kasiviswanathan, L.~Ryzhyk, M.~Sagiv, and T.~Walsh.
\newblock Verifying properties of binarized deep neural networks.
\newblock In {\em AAAI}, 2018.

\bibitem{paganini2020forget}
M.~Paganini.
\newblock Prune responsibly.
\newblock {\em arXiv}, 2009.09936, 2020.

\bibitem{pascanu2013regions}
R.~Pascanu, G.~Mont\'{u}far, and Y.~Bengio.
\newblock On the number of response regions of deep feedforward networks with
  piecewise linear activations.
\newblock In {\em ICLR}, 2014.

\bibitem{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito,
  M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and
  S.~Chintala.
\newblock {PyTorch}: An imperative style, high-performance deep learning
  library.
\newblock In {\em NeurIPS}, 2019.

\bibitem{petersen2020topological}
P.~Petersen, M.~Raslan, and F.~Voigtlaender.
\newblock Topological properties of the set of functions generated by neural
  networks of fixed size.
\newblock {\em Foundations of Computational Mathematics}, 2020.

\bibitem{phuong2020equivalence}
M.~Phuong and C.~Lampert.
\newblock Functional vs. parametric equivalence of {ReLU} networks.
\newblock In {\em ICLR}, 2020.

\bibitem{raghu2017expressive}
M.~Raghu, B.~Poole, J.~Kleinberg, S.~Ganguli, and J.~Dickstein.
\newblock On the expressive power of deep neural networks.
\newblock In {\em ICML}, 2017.

\bibitem{renda2020retraining}
A.~Renda, J.~Frankle, and M.~Carbin.
\newblock Comparing rewinding and fine-tuning in neural network pruning.
\newblock In {\em ICLR}, 2020.

\bibitem{rolnick2020engineering}
D.~Rolnick and K.~Kording.
\newblock Reverse-engineering deep {ReLU} networks.
\newblock In {\em ICML}, 2020.

\bibitem{rosenfeld2020predictability}
J.~Rosenfeld, J.~Frankle, M.~Carbin, and N.~Shavit.
\newblock On the predictability of pruning across scales.
\newblock {\em arXiv}, 2006.10621, 2020.

\bibitem{rossig2020verification}
A.~R\"{o}ssig and M.~Petkovic.
\newblock Advances in verification of {ReLU} neural networks.
\newblock {\em Journal of Global Optimization}, 2020.

\bibitem{sanner2017planning}
B.~Say, G.~Wu, Y.~Zhou, and S.~Sanner.
\newblock Nonlinear hybrid planning with deep net learned transition models and
  mixed-integer linear programming.
\newblock In {\em IJCAI}, 2017.

\bibitem{serra2020lossless}
T.~Serra, A.~Kumar, and S.~Ramalingam.
\newblock Lossless compression of deep neural networks.
\newblock In {\em CPAIOR}, 2020.

\bibitem{serra2020empirical}
T.~Serra and S.~Ramalingam.
\newblock Empirical bounds on linear regions of deep rectifier networks.
\newblock In {\em AAAI}, 2020.

\bibitem{serra2018bounding}
T.~Serra, C.~Tjandraatmadja, and S.~Ramalingam.
\newblock Bounding and counting linear regions of deep neural networks.
\newblock In {\em ICML}, 2018.

\bibitem{woodfisher}
S.~Singh and D.~Alistarh.
\newblock {WoodFisher:} efficient second-order approximation for neural network
  compression.
\newblock In {\em NeurIPS}, 2020.

\bibitem{sourek2021lossless}
G.~Sourek and F.~Zelezny.
\newblock Lossless compression of structured convolutional models via lifting.
\newblock In {\em ICLR}, 2021.

\bibitem{srinivas2015datafree}
S.~Srinivas and R.~Venkatesh Babu.
\newblock Data-free parameter pruning for deep neural networks.
\newblock In {\em BMVC}, 2015.

\bibitem{su2018tensorial}
J.~Su, J.~Li, B.~Bhattacharjee, and F.~Huang.
\newblock Tensorial neural networks: Generalization of neural networks and
  application to model compression, 2018.

\bibitem{suau2020distillation}
X.~Suau, L.~Zappella, and N.~Apostoloff.
\newblock Filter distillation for network compression.
\newblock In {\em WACV}, 2020.

\bibitem{suzuki2020spectral}
T.~Suzuki, H.~Abe, T.~Murata, S.~Horiuchi, K.~Ito, T.~Wachi, S.~Hirai,
  M.~Yukishima, and T.~Nishimura.
\newblock Spectral-pruning: Compressing deep neural network via spectral
  analysis.
\newblock In {\em IJCAI}, 2020.

\bibitem{suzuki2020bounds}
T.~Suzuki, H.~Abe, and T.~Nishimura.
\newblock Compression based bound for non-compressed network: Unified
  generalization error analysis of large compressible deep neural network.
\newblock In {\em ICLR}, 2020.

\bibitem{tanaka2020synapticflow}
H.~Tanaka, D.~Kunin, D.~Yamins, and S.~Ganguli.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock In {\em NeurIPS}, 2020.

\bibitem{tjeng2019stability}
V.~Tjeng, K.~Xiao, and R.~Tedrake.
\newblock Evaluating robustness of neural networks with mixed integer
  programming.
\newblock In {\em ICLR}, 2019.

\bibitem{wang2019eigendamage}
C.~Wang, R.~Grosse, S.~Fidler, and G.~Zhang.
\newblock {EigenDamage}: Structured pruning in the {Kronecker}-factored
  eigenbasis.
\newblock In {\em ICML}, 2019.

\bibitem{wang2020tickets}
C.~Wang, G.~Zhang, and R.~Grosse.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In {\em ICLR}, 2020.

\bibitem{wang2018wide}
W.~Wang, Y.~Sun, B.~Eriksson, W.~Wang, and V.~Aggarwal.
\newblock Wide compression: Tensor ring nets, 2018.

\bibitem{xiao2019training}
K.~Xiao, V.~Tjeng, N.~Shafiullah, and A.~Madry.
\newblock Training for faster adversarial robustness verification via inducing
  {ReLU} stability.
\newblock {\em ICLR}, 2019.

\bibitem{xing2020lossless}
X.~Xing, L.~Sha, P.~Hong, Z.~Shang, and J.~Liu.
\newblock Probabilistic connection importance inference and lossless
  compression of deep neural networks.
\newblock In {\em ICLR}, 2020.

\bibitem{yu2018importance}
R.~Yu, A.~Li, C.~Chen, J.~Lai, V.~Morariu, X.~Han, M.~Gao, C.~Lin, and
  L.~Davis.
\newblock {NISP}: Pruning networks using neuron importance score propagation.
\newblock In {\em CVPR}, 2018.

\bibitem{zeng2018mlprune}
W.~Zeng and R.~Urtasun.
\newblock {MLPrune}: Multi-layer pruning for automated neural network
  compression.
\newblock 2018.

\bibitem{zhou2019bounds}
W.~Zhou, V.~Veitch, M.~Austern, R.~Adams, and P.~Orbanz.
\newblock Non-vacuous generalization bounds at the {ImageNet} scale: A
  {PAC-Bayesian} compression approach.
\newblock In {\em ICLR}, 2019.

\end{thebibliography}
