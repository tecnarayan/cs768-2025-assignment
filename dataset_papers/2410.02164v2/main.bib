@article{jain2024scaling,
  title={Scaling laws for learning with real and surrogate data},
  author={Jain, Ayush and Montanari, Andrea and Sasoglu, Eren},
  journal={arXiv preprint arXiv:2402.04376},
  year={2024}
}

@article{zhuang2020comprehensive,
  title={A comprehensive survey on transfer learning},
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE},
  volume={109},
  number={1},
  pages={43--76},
  year={2020},
  publisher={IEEE}
}

@article{dar2021common,
  title={The common intuition to transfer learning can win or lose: Case studies for linear regression},
  author={Dar, Yehuda and LeJeune, Daniel and Baraniuk, Richard G},
  journal={arXiv preprint arXiv:2103.05621},
  year={2021}
}

@article{hastie2022surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={Annals of statistics},
  volume={50},
  number={2},
  pages={949},
  year={2022},
  publisher={NIH Public Access}
}

@inproceedings{tan2018survey,
  title={A survey on deep transfer learning},
  author={Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
  booktitle={Artificial Neural Networks and Machine Learning--ICANN 2018: 27th International Conference on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part III 27},
  pages={270--279},
  year={2018},
  organization={Springer}
}

@inproceedings{azizan2018stochastic,
  title={Stochastic Gradient/Mirror Descent: Minimax Optimality and Implicit Regularization},
  author={Azizan, Navid and Hassibi, Babak},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@INPROCEEDINGS{10206966,
  author={Varma, K Nithin and Lale, Sahin and Hassibi, Babak},
  booktitle={2023 IEEE International Symposium on Information Theory (ISIT)}, 
  title={The Asymptotic Distribution of the Stochastic Mirror Descent Iterates in Linear Models}, 
  year={2023},
  volume={},
  number={},
  pages={346-351},
  keywords={Deep learning;Training;Analytical models;Interpolation;Machine learning algorithms;Computational modeling;Stochastic processes},
  doi={10.1109/ISIT54713.2023.10206966}}


@InProceedings{pmlr-v80-gunasekar18a,
  title = 	 {Characterizing Implicit Bias in Terms of Optimization Geometry},
  author =       {Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1832--1841},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/gunasekar18a/gunasekar18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/gunasekar18a.html},
  abstract = 	 {We study the bias of generic optimization methods, including Mirror Descent, Natural Gradient Descent and Steepest Descent with respect to different potentials and norms, when optimizing underdetermined linear models or separable linear classification problems. We ask the question of whether the global minimum (among the many possible global minima) reached by optimization can be characterized in terms of the potential or norm, and indecently of hyper-parameter choices, such as stepsize and momentum.}
}

@article{akhtiamov2024novel,
  title={A Novel Gaussian Min-Max Theorem and its Applications},
  author={Akhtiamov, Danil and Bosch, David and Ghane, Reza and Varma, K Nithin and Hassibi, Babak},
  journal={arXiv preprint arXiv:2402.07356},
  year={2024}
}

@article{gerace2022probing,
  title={Probing transfer learning with a model of synthetic correlated datasets},
  author={Gerace, Federica and Saglietti, Luca and Mannelli, Stefano Sarao and Saxe, Andrew and Zdeborov{\'a}, Lenka},
  journal={Machine Learning: Science and Technology},
  volume={3},
  number={1},
  pages={015030},
  year={2022},
  publisher={IOP Publishing}
}

@article{panahi2017universal,
  title={A universal analysis of large-scale regularized least squares solutions},
  author={Panahi, Ashkan and Hassibi, Babak},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{bosch2023precise,
  title={Precise asymptotic analysis of deep random feature models},
  author={Bosch, David and Panahi, Ashkan and Hassibi, Babak},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={4132--4179},
  year={2023},
  organization={PMLR}
}

@article{hu2022universality,
  title={Universality laws for high-dimensional learning with random features},
  author={Hu, Hong and Lu, Yue M},
  journal={IEEE Transactions on Information Theory},
  volume={69},
  number={3},
  pages={1932--1964},
  year={2022},
  publisher={IEEE}
}

@article{abbasi2019universality,
  title={Universality in learning from linear measurements},
  author={Abbasi, Ehsan and Salehi, Fariborz and Hassibi, Babak},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{han2023universality,
  title={Universality of regularized regression estimators in high dimensions},
  author={Han, Qiyang and Shen, Yandi},
  journal={The Annals of Statistics},
  volume={51},
  number={4},
  pages={1799--1823},
  year={2023},
  publisher={Institute of Mathematical Statistics}
}

@article{bozinovski2020reminder,
  title={Reminder of the first paper on transfer learning in neural networks, 1976},
  author={Bozinovski, Stevo},
  journal={Informatica},
  volume={44},
  number={3},
  year={2020}
}

@inproceedings{dai2007boosting,
  title={Boosting for transfer learning},
  author={Dai, Wenyuan and Yang, Qiang and Xue, Gui-Rong and Yu, Yong},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={193--200},
  year={2007}
}

@article{lindeberg1922neue,
  title={Eine neue Herleitung des Exponentialgesetzes in der Wahrscheinlichkeitsrechnung},
  author={Lindeberg, Jarl Waldemar},
  journal={Mathematische Zeitschrift},
  volume={15},
  number={1},
  pages={211--225},
  year={1922},
  publisher={Springer}
}

@article{chatterjee2006generalization,
  title={A generalization of the Lindeberg principle},
  author={Chatterjee, Sourav},
  year={2006}
}

@incollection{liese2008statistical,
  title={Statistical decision theory},
  author={Liese, Friedrich and Miescke, Klaus-J},
  booktitle={Statistical Decision Theory: Estimation, Testing, and Selection},
  pages={1--52},
  year={2008},
  publisher={Springer}
}

@article{akhtiamov2023regularized,
  title={Regularized linear regression for binary classification},
  author={Akhtiamov, Danil and Ghane, Reza and Hassibi, Babak},
  journal={arXiv preprint arXiv:2311.02270},
  year={2023}
}

@article{ghane2024one,
  title={One-Bit Quantization and Sparsification for Multiclass Linear Classification via Regularized Regression},
  author={Ghane, Reza and Akhtiamov, Danil and Hassibi, Babak},
  journal={arXiv preprint arXiv:2402.10474},
  year={2024}
}
@inproceedings{seddik2020random,
  title={Random matrix theory proves that deep learning representations of gan-data behave as gaussian mixtures},
  author={Seddik, Mohamed El Amine and Louart, Cosme and Tamaazousti, Mohamed and Couillet, Romain},
  booktitle={International Conference on Machine Learning},
  pages={8573--8582},
  year={2020},
  organization={PMLR}
}

@misc{cvx,
  author       = {Michael Grant and Stephen Boyd},
  title        = {{CVX}: Matlab Software for Disciplined Convex Programming, version 2.1},
  month        = mar,
  year         = 2014
}
@article{agrawal2018rewriting,
  author  = {Agrawal, Akshay and Verschueren, Robin and Diamond, Steven and Boyd, Stephen},
  title   = {A rewriting system for convex optimization problems},
  journal = {Journal of Control and Decision},
  year    = {2018},
  volume  = {5},
  number  = {1},
  pages   = {42--60},
}

@article{dandi2024universality,
  title={Universality laws for gaussian mixtures in generalized linear models},
  author={Dandi, Yatin and Stephan, Ludovic and Krzakala, Florent and Loureiro, Bruno and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{schroder2023deterministic,
  title={Deterministic equivalent and error universality of deep random features learning},
  author={Schr{\"o}der, Dominik and Cui, Hugo and Dmitriev, Daniil and Loureiro, Bruno},
  booktitle={International Conference on Machine Learning},
  pages={30285--30320},
  year={2023},
  organization={PMLR}
}

@inproceedings{montanari2022universality,
  title={Universality of empirical risk minimization},
  author={Montanari, Andrea and Saeed, Basil N},
  booktitle={Conference on Learning Theory},
  pages={4310--4312},
  year={2022},
  organization={PMLR}
}
@inproceedings{montanari2017universality,
  title={Universality of the elastic net error},
  author={Montanari, Andrea and Nguyen, Phan-Minh},
  booktitle={2017 IEEE International Symposium on Information Theory (ISIT)},
  pages={2338--2342},
  year={2017},
  organization={IEEE}
}

@article{oymak2018universality,
  title={Universality laws for randomized dimension reduction, with applications},
  author={Oymak, Samet and Tropp, Joel A},
  journal={Information and Inference: A Journal of the IMA},
  volume={7},
  number={3},
  pages={337--446},
  year={2018},
  publisher={Oxford University Press}
}

@article{lahiry2023universality,
  title={Universality in block dependent linear models with applications to nonparametric regression},
  author={Lahiry, Samriddha and Sur, Pragya},
  journal={arXiv preprint arXiv:2401.00344},
  year={2023}
}

@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}
@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}

@article{bobkov2003concentration,
  title={On concentration of distributions of random weighted sums},
  author={Bobkov, Sergey G},
  journal={Annals of probability},
  pages={195--215},
  year={2003},
  publisher={JSTOR}
}

@incollection{bai2008limit,
  title={Limit of the smallest eigenvalue of a large dimensional sample covariance matrix},
  author={Bai, Zhi-Dong and Yin, Yong-Qua},
  booktitle={Advances In Statistics},
  pages={108--127},
  year={2008},
  publisher={World Scientific}
}
