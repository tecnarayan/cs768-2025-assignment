\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  de~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  de~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{ICML}, 2019.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL}, 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{JMLR}, 2020.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Howard and Ruder(2018)]{howard-2018-ulmfit}
Jeremy Howard and Sebastian Ruder.
\newblock {Universal Language Model Fine-tuning for Text Classification}.
\newblock In \emph{ACL}, 2018.

\bibitem[Peters et~al.(2019)Peters, Ruder, and Smith]{peters-2019-tune}
Matthew~E Peters, Sebastian Ruder, and Noah~A Smith.
\newblock To tune or not to tune? adapting pretrained representations to
  diverse tasks.
\newblock In \emph{RepL4NLP}, 2019.

\bibitem[Dodge et~al.(2020)Dodge, Ilharco, Schwartz, Farhadi, Hajishirzi, and
  Smith]{Dodge2020fine-tuning}
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi,
  and Noah Smith.
\newblock {Fine-Tuning Pretrained Language Models: Weight Initializations, Data
  Orders, and Early Stopping}.
\newblock \emph{arXiv preprint arXiv:2002.06305}, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Perez et~al.(2021)Perez, Kiela, and Cho]{Perez2021true}
Ethan Perez, Douwe Kiela, and Kyunghyun Cho.
\newblock {True Few-Shot Learning with Language Models}.
\newblock \emph{arXiv preprint arXiv:2105.11447}, 2021.
\newblock URL \url{http://arxiv.org/abs/2105.11447}.

\bibitem[Li and Liang(2021)]{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock \emph{ACL}, 2021.

\bibitem[Hambardzumyan et~al.(2021)Hambardzumyan, Khachatrian, and
  May]{hambardzumyan2021warp}
Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May.
\newblock Warp: Word-level adversarial reprogramming.
\newblock \emph{ACL}, 2021.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock \emph{EMNLP}, 2021.

\bibitem[Li et~al.(2018)Li, Farkhoor, Liu, and Yosinski]{li2018measuring}
Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski.
\newblock Measuring the intrinsic dimension of objective landscapes.
\newblock In \emph{ICLR}, 2018.

\bibitem[Aghajanyan et~al.(2021)Aghajanyan, Zettlemoyer, and
  Gupta]{aghajanyan2020intrinsic}
Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta.
\newblock Intrinsic dimensionality explains the effectiveness of language model
  fine-tuning.
\newblock \emph{ACL}, 2021.

\bibitem[Rebuffi et~al.(2018)Rebuffi, Bilen, and Vedaldi]{rebuffi2018efficient}
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
\newblock Efficient parametrization of multi-domain deep neural networks.
\newblock In \emph{CVPR}, 2018.

\bibitem[Lin et~al.(2020)Lin, Madotto, and Fung]{linz-etal-2020-exploring}
Zhaojiang Lin, Andrea Madotto, and Pascale Fung.
\newblock Exploring versatile generative language model via parameter-efficient
  transfer learning.
\newblock In \emph{EMNLP Findings}, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Tay, Zhang, Chan, Luu, Hui, and
  Fu]{zhang2021beyond}
Aston Zhang, Yi~Tay, SHUAI Zhang, Alvin Chan, Anh~Tuan Luu, Siu Hui, and Jie
  Fu.
\newblock Beyond fully-connected layers with quaternions: Parameterization of
  hypercomplex multiplications with 1/n parameters.
\newblock In \emph{ICLR}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{ICLR}, 2019{\natexlab{a}}.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Pruksachatkun, Nangia, Singh,
  Michael, Hill, Levy, and Bowman]{wang2019superglue}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
  Felix Hill, Omer Levy, and Samuel~R Bowman.
\newblock Superglue: a stickier benchmark for general-purpose language
  understanding systems.
\newblock In \emph{NeurIPS}, 2019{\natexlab{b}}.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Wu, Katiyar, Weinberger, and
  Artzi]{Zhang2021revisiting}
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian~Q Weinberger, and Yoav Artzi.
\newblock {Revisiting Few-sample BERT Fine-tuning}.
\newblock In \emph{ICLR}, 2021{\natexlab{b}}.

\bibitem[Chung et~al.(2021)Chung, F{\'{e}}vry, Tsai, Johnson, and
  Ruder]{Chung2021rembert}
Hyung~Won Chung, Thibault F{\'{e}}vry, Henry Tsai, Melvin Johnson, and
  Sebastian Ruder.
\newblock {Rethinking Embedding Coupling in Pre-trained Language Models}.
\newblock In \emph{ICLR}, 2021.

\bibitem[R{\"{u}}ckl{\'{e}} et~al.(2021)R{\"{u}}ckl{\'{e}}, Geigle, Glockner,
  Beck, Pfeiffer, Reimers, and Gurevych]{Ruckle2020adapterdrop}
Andreas R{\"{u}}ckl{\'{e}}, Gregor Geigle, Max Glockner, Tilman Beck, Jonas
  Pfeiffer, Nils Reimers, and Iryna Gurevych.
\newblock {AdapterDrop: On the Efficiency of Adapters in Transformers}.
\newblock \emph{EMNLP}, 2021.

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{arora2018optimization}
Sanjeev Arora, Nadav Cohen, and Elad Hazan.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In \emph{ICML}, 2018.

\bibitem[Wen et~al.(2020)Wen, Tran, and Ba]{Wen2020batchensemble}
Yeming Wen, Dustin Tran, and Jimmy Ba.
\newblock {BatchEnsemble: An Alternative Approach to Efficient Ensemble and
  Lifelong Learning}.
\newblock In \emph{ICLR}, 2020.

\bibitem[Levesque et~al.(2012)Levesque, Davis, and
  Morgenstern]{levesque2012winograd}
Hector Levesque, Ernest Davis, and Leora Morgenstern.
\newblock The winograd schema challenge.
\newblock In \emph{KR}, 2012.

\bibitem[Zhang et~al.(2021{\natexlab{c}})Zhang, Wu, Katiyar, Weinberger, and
  Artzi]{zhang2020revisiting}
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian~Q Weinberger, and Yoav Artzi.
\newblock Revisiting few-sample bert fine-tuning.
\newblock In \emph{ICLR}, 2021{\natexlab{c}}.

\bibitem[Wolf et~al.(2020{\natexlab{a}})Wolf, Debut, Sanh, Chaumond, Delangue,
  Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma,
  Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and
  Rush]{wolf-etal2020transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{EMNLP: System Demonstrations}, 2020{\natexlab{a}}.

\bibitem[Le et~al.(2021)Le, Bertolini, Noé, and Clevert]{le2021parameterized}
Tuan Le, Marco Bertolini, Frank Noé, and Djork-Arné Clevert.
\newblock Parameterized hypercomplex graph neural networks for graph
  classification.
\newblock \emph{ICANN}, 2021.

\bibitem[Mahabadi et~al.(2021)Mahabadi, Ruder, Dehghani, and
  Henderson]{karimi2021parameter-efficient}
Rabeeh~Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson.
\newblock Parameter-efficient multi-task fine-tuning for transformers via
  shared hypernetworks.
\newblock In \emph{ACL}, 2021.

\bibitem[Pfeiffer et~al.(2021)Pfeiffer, Kamath, R{\"{u}}ckĺe, Kyunghyun, and
  Gurevych]{Pfeiffer2021adapterfusion}
Jonas Pfeiffer, Aishwarya Kamath, Andreas R{\"{u}}ckĺe, Cho Kyunghyun, and
  Iryna Gurevych.
\newblock {AdapterFusion: Non-destructive task composition for transfer
  learning}.
\newblock In \emph{EACL}, 2021.

\bibitem[Le et~al.(2013)Le, Sarl{\'o}s, and Smola]{le2013fastfood}
Quoc Le, Tam{\'a}s Sarl{\'o}s, and Alex Smola.
\newblock Fastfood-approximating kernel expansions in loglinear time.
\newblock In \emph{ICML}, 2013.

\bibitem[Cai et~al.(2020)Cai, Gan, Zhu, and Han]{cai2020tinytl}
Han Cai, Chuang Gan, Ligeng Zhu, and Song Han.
\newblock Tinytl: Reduce memory, not parameters for efficient on-device
  learning.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Ravfogel et~al.()Ravfogel, Ben-Zaken, and
  Goldberg]{ravfogel2021bitfit}
Shauli Ravfogel, Elad Ben-Zaken, and Yoav Goldberg.
\newblock Bitfit: Simple parameter-efficient fine-tuning for transformer-based
  masked languagemodels.
\newblock \emph{arXiv preprint arXiv:2106.10199}.

\bibitem[Brock et~al.(2021)Brock, De, Smith, and Simonyan]{brock2021high}
Andrew Brock, Soham De, Samuel~L Smith, and Karen Simonyan.
\newblock High-performance large-scale image recognition without normalization.
\newblock \emph{ICML}, 2021.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[{\"U}st{\"u}n et~al.(2020){\"U}st{\"u}n, Bisazza, Bouma, and van
  Noord]{ustun2020udapter}
Ahmet {\"U}st{\"u}n, Arianna Bisazza, Gosse Bouma, and Gertjan van Noord.
\newblock Udapter: Language adaptation for truly universal dependency parsing.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Platanios et~al.(2018)Platanios, Sachan, Neubig, and
  Mitchell]{platanios2018contextual}
Emmanouil~Antonios Platanios, Mrinmaya Sachan, Graham Neubig, and Tom Mitchell.
\newblock Contextual parameter generation for universal neural machine
  translation.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Pilault et~al.(2021)Pilault, hattami, and
  Pal]{pilault2021conditionally}
Jonathan Pilault, Amine~El hattami, and Christopher Pal.
\newblock Conditionally adaptive multi-task learning: Improving transfer
  learning in {NLP} using fewer parameters \& less data.
\newblock In \emph{ICLR}, 2021.

\bibitem[Gaudet and Maida(2018)]{gaudet2018deep}
Chase~J Gaudet and Anthony~S Maida.
\newblock Deep quaternion networks.
\newblock In \emph{IJCNN}, 2018.

\bibitem[Parcollet et~al.(2018{\natexlab{a}})Parcollet, Zhang, Morchid,
  Trabelsi, Linar{\`e}s, de~Mori, and Bengio]{parcollet2018quaternion}
Titouan Parcollet, Ying Zhang, Mohamed Morchid, Chiheb Trabelsi, Georges
  Linar{\`e}s, Renato de~Mori, and Yoshua Bengio.
\newblock Quaternion convolutional neural networks for end-to-end automatic
  speech recognition.
\newblock In \emph{Interspeech}, 2018{\natexlab{a}}.

\bibitem[Parcollet et~al.(2018{\natexlab{b}})Parcollet, Ravanelli, Morchid,
  Linar{\`e}s, Trabelsi, De~Mori, and Bengio]{parcollet2018quaternion_b}
Titouan Parcollet, Mirco Ravanelli, Mohamed Morchid, Georges Linar{\`e}s,
  Chiheb Trabelsi, Renato De~Mori, and Yoshua Bengio.
\newblock Quaternion recurrent neural networks.
\newblock In \emph{ICLR}, 2018{\natexlab{b}}.

\bibitem[Zhu et~al.(2018)Zhu, Xu, Xu, and Chen]{zhu2018quaternion}
Xuanyu Zhu, Yi~Xu, Hongteng Xu, and Changjian Chen.
\newblock Quaternion convolutional neural networks.
\newblock In \emph{ECCV}, 2018.

\bibitem[Tay et~al.(2019)Tay, Zhang, Luu, Rao, Zhang, Wang, Fu, and
  Hui]{tay2019lightweight}
Yi~Tay, Aston Zhang, Anh~Tuan Luu, Jinfeng Rao, Shuai Zhang, Shuohang Wang, Jie
  Fu, and Siu~Cheung Hui.
\newblock Lightweight and efficient neural natural language processing with
  quaternion networks.
\newblock In \emph{ACL}, 2019.

\bibitem[Chen et~al.(2020)Chen, Frankle, Chang, Liu, Zhang, Wang, and
  Carbin]{chen2020lottery}
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang
  Wang, and Michael Carbin.
\newblock The lottery ticket hypothesis for pre-trained bert networks.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Prasanna et~al.(2020)Prasanna, Rogers, and
  Rumshisky]{prasannaetal2020bert}
Sai Prasanna, Anna Rogers, and Anna Rumshisky.
\newblock {W}hen {BERT} {P}lays the {L}ottery, {A}ll {T}ickets {A}re {W}inning.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Desai et~al.(2019)Desai, Zhan, and Aly]{desaietal2019evaluating}
Shrey Desai, Hongyuan Zhan, and Ahmed Aly.
\newblock Evaluating lottery tickets under distributional shifts.
\newblock In \emph{DeepLo}, 2019.

\bibitem[Blalock et~al.(2020)Blalock, Ortiz, Frankle, and
  Guttag]{blalock2020state}
Davis Blalock, Jose Javier~Gonzalez Ortiz, Jonathan Frankle, and John Guttag.
\newblock What is the state of neural network pruning?
\newblock \emph{arXiv preprint arXiv:2003.03033}, 2020.

\bibitem[Warstadt et~al.(2019)Warstadt, Singh, and
  Bowman]{warstadt-etal-2019-neural}
Alex Warstadt, Amanpreet Singh, and Samuel~R. Bowman.
\newblock Neural network acceptability judgments.
\newblock \emph{TACL}, 2019.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher-etal-2013-recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning,
  Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{EMNLP}, 2013.

\bibitem[Dolan and Brockett(2005)]{dolan-brockett-2005-automatically}
William~B. Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{IWP}, 2005.

\bibitem[Cer et~al.(2017)Cer, Diab, Agirre, Lopez-Gazpio, and
  Specia]{cer-etal-2017-semeval}
Daniel Cer, Mona Diab, Eneko Agirre, I{\~n}igo Lopez-Gazpio, and Lucia Specia.
\newblock {S}em{E}val-2017 task 1: Semantic textual similarity multilingual and
  crosslingual focused evaluation.
\newblock In \emph{{S}em{E}val}, 2017.

\bibitem[Williams et~al.(2018)Williams, Nangia, and
  Bowman]{williams-etal-2018-broad}
Adina Williams, Nikita Nangia, and Samuel Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{NAACL}, 2018.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar-etal-2016-squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In \emph{EMNLP}, 2016.

\bibitem[Dagan et~al.(2005)Dagan, Glickman, and Magnini]{dagan2005pascal}
Ido Dagan, Oren Glickman, and Bernardo Magnini.
\newblock The pascal recognising textual entailment challenge.
\newblock In \emph{Machine Learning Challenges Workshop}, 2005.

\bibitem[Bar-Haim et~al.(2006)Bar-Haim, Dagan, Dolan, Ferro, and
  Giampiccolo]{rte2}
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, and Danilo Giampiccolo.
\newblock The second pascal recognising textual entailment challenge.
\newblock \emph{Second PASCAL Challenges Workshop on Recognising Textual
  Entailment}, 2006.

\bibitem[Giampiccolo et~al.(2007)Giampiccolo, Magnini, Dagan, and
  Dolan]{giampiccolo-etal-2007-third}
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan.
\newblock The third {PASCAL} recognizing textual entailment challenge.
\newblock In \emph{{ACL}-{PASCAL} Workshop on Textual Entailment and
  Paraphrasing}, 2007.

\bibitem[Bentivogli et~al.(2009)Bentivogli, Dagan, Dang, Giampiccolo, and
  Magnini]{Bentivogli09thefifth}
Luisa Bentivogli, Ido Dagan, Hoa~Trang Dang, Danilo Giampiccolo, and Bernardo
  Magnini.
\newblock The fifth pascal recognizing textual entailment challenge.
\newblock In \emph{TAC}, 2009.

\bibitem[Roemmele et~al.(2011)Roemmele, Bejan, and Gordon]{roemmele2011choice}
Melissa Roemmele, Cosmin~Adrian Bejan, and Andrew~S Gordon.
\newblock Choice of plausible alternatives: An evaluation of commonsense causal
  reasoning.
\newblock In \emph{AAAI Spring Symposium Series}, 2011.

\bibitem[De~Marneffe et~al.(2019)De~Marneffe, Simons, and
  Tonhauser]{de2019commitmentbank}
Marie-Catherine De~Marneffe, Mandy Simons, and Judith Tonhauser.
\newblock The commitmentbank: Investigating projection in naturally occurring
  discourse.
\newblock In \emph{proceedings of Sinn und Bedeutung}, 2019.

\bibitem[Khashabi et~al.(2018)Khashabi, Chaturvedi, Roth, Upadhyay, and
  Roth]{khashabi2018looking}
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan
  Roth.
\newblock Looking beyond the surface: A challenge set for reading comprehension
  over multiple sentences.
\newblock In \emph{NAACL}, 2018.

\bibitem[Zhang et~al.(2018)Zhang, Liu, Liu, Gao, Duh, and
  Van~Durme]{zhang2018record}
Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin
  Van~Durme.
\newblock Record: Bridging the gap between human and machine commonsense
  reading comprehension.
\newblock \emph{arXiv preprint arXiv:1810.12885}, 2018.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and
  Toutanova]{clark-etal-2019-boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
  Collins, and Kristina Toutanova.
\newblock {B}ool{Q}: Exploring the surprising difficulty of natural yes/no
  questions.
\newblock In \emph{NAACL}, 2019.

\bibitem[Pilehvar and Camacho-Collados(2019)]{pilehvar2019wic}
Mohammad~Taher Pilehvar and Jose Camacho-Collados.
\newblock Wic: the word-in-context dataset for evaluating context-sensitive
  meaning representations.
\newblock In \emph{NAACL}, 2019.

\bibitem[Schuler(2005)]{schuler2005verbnet}
Karin~Kipper Schuler.
\newblock Verbnet: A broad-coverage, comprehensive verb lexicon.
\newblock \emph{PhD Thesis}, 2005.

\bibitem[Miller(1995)]{miller1995wordnet}
George~A Miller.
\newblock Wordnet: a lexical database for english.
\newblock \emph{Communications of the ACM}, 1995.

\bibitem[Wolf et~al.(2020{\natexlab{b}})Wolf, Lhoest, von Platen, Jernite,
  Drame, Plu, Chaumond, Delangue, Ma, Thakur, Patil, Davison, Scao, Sanh, Xu,
  Patry, McMillan-Major, Brandeis, Gugger, Lagunas, Debut, Funtowicz, Moi,
  Rush, Schmidd, Cistac, Muštar, Boudier, and
  Tordjmann]{2020HuggingFace-datasets}
Thomas Wolf, Quentin Lhoest, Patrick von Platen, Yacine Jernite, Mariama Drame,
  Julien Plu, Julien Chaumond, Clement Delangue, Clara Ma, Abhishek Thakur,
  Suraj Patil, Joe Davison, Teven~Le Scao, Victor Sanh, Canwen Xu, Nicolas
  Patry, Angie McMillan-Major, Simon Brandeis, Sylvain Gugger, François
  Lagunas, Lysandre Debut, Morgan Funtowicz, Anthony Moi, Sasha Rush, Philipp
  Schmidd, Pierric Cistac, Victor Muštar, Jeff Boudier, and Anna Tordjmann.
\newblock Datasets.
\newblock \emph{GitHub. Note: https://github.com/huggingface/datasets},
  2020{\natexlab{b}}.

\end{thebibliography}
