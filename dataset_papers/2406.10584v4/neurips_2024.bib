@article{schick2020exploiting,
  title={Exploiting cloze questions for few shot text classification and natural language inference},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2001.07676},
  year={2020}
}

@article{schick2020s,
  title={It's not just size that matters: Small language models are also few-shot learners},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2009.07118},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{jiang2020can,
  title={How can we know what language models know?},
  author={Jiang, Zhengbao and Xu, Frank F and Araki, Jun and Neubig, Graham},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={423--438},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@article{yuan2021bartscore,
  title={Bartscore: Evaluating generated text as text generation},
  author={Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={27263--27277},
  year={2021}
}

@article{haviv2021bertese,
  title={BERTese: Learning to speak to BERT},
  author={Haviv, Adi and Berant, Jonathan and Globerson, Amir},
  journal={arXiv preprint arXiv:2103.05327},
  year={2021}
}

@inproceedings{davison2019commonsense,
  title={Commonsense knowledge mining from pretrained models},
  author={Davison, Joe and Feldman, Joshua and Rush, Alexander M},
  booktitle={Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)},
  pages={1173--1178},
  year={2019}
}

@inproceedings{li2024dialogue,
  title={Dialogue for Prompting: A Policy-Gradient-Based Discrete Prompt Generation for Few-Shot Learning},
  author={Li, Chengzhengxu and Liu, Xiaoming and Wang, Yichen and Li, Duyi and Lan, Yu and Shen, Chao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={18481--18489},
  year={2024}
}


@inproceedings{sun2023query,
  title={Query-Dependent Prompt Evaluation and Optimization with Offline Inverse RL},
  author={Sun, Hao and H{\"u}y{\"u}k, Alihan and van der Schaar, Mihaela},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{wu2022adversarial,
  title={Adversarial soft prompt tuning for cross-domain sentiment analysis},
  author={Wu, Hui and Shi, Xiaodong},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2438--2447},
  year={2022}
}

@inproceedings{zhao2022adpl,
  title={Adpl: Adversarial prompt-based domain adaptation for dialogue summarization with knowledge disentanglement},
  author={Zhao, Lulu and Zheng, Fujia and Zeng, Weihao and He, Keqing and Geng, Ruotong and Jiang, Huixing and Wu, Wei and Xu, Weiran},
  booktitle={Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={245--255},
  year={2022}
}


@article{gu2021ppt,
  title={Ppt: Pre-trained prompt tuning for few-shot learning},
  author={Gu, Yuxian and Han, Xu and Liu, Zhiyuan and Huang, Minlie},
  journal={arXiv preprint arXiv:2109.04332},
  year={2021}
}



@article{vu2021spot,
  title={Spot: Better frozen model adaptation through soft prompt transfer},
  author={Vu, Tu and Lester, Brian and Constant, Noah and Al-Rfou, Rami and Cer, Daniel},
  journal={arXiv preprint arXiv:2110.07904},
  year={2021}
}

@article{liu2024stablept,
  title={StablePT: Towards Stable Prompting for Few-shot Learning via Input Separation},
  author={Liu, Xiaoming and Liu, Chen and Zhang, Zhaohan and Li, Chengzhengxu and Wang, Longtian and Lan, Yu and Shen, Chao},
  journal={arXiv preprint arXiv:2404.19335},
  year={2024}
}


@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@inproceedings{hu2004mining,
  title={Mining and summarizing customer reviews},
  author={Hu, Minqing and Liu, Bing},
  booktitle={Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={168--177},
  year={2004}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{deng2022rlprompt,
  title={Rlprompt: Optimizing discrete text prompts with reinforcement learning},
  author={Deng, Mingkai and Wang, Jianyu and Hsieh, Cheng-Ping and Wang, Yihan and Guo, Han and Shu, Tianmin and Song, Meng and Xing, Eric P and Hu, Zhiting},
  journal={arXiv preprint arXiv:2205.12548},
  year={2022}
}


@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{an2022input,
  title={Input-tuning: Adapting unfamiliar inputs to frozen pretrained models},
  author={An, Shengnan and Li, Yifei and Lin, Zeqi and Liu, Qian and Chen, Bei and Fu, Qiang and Chen, Weizhu and Zheng, Nanning and Lou, Jian-Guang},
  journal={arXiv preprint arXiv:2203.03131},
  year={2022}
}

@article{qian2022controllable,
  title={Controllable natural language generation with contrastive prefixes},
  author={Qian, Jing and Dong, Li and Shen, Yelong and Wei, Furu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2202.13257},
  year={2022}
}

@article{perez2021true,
  title={True few-shot learning with language models},
  author={Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={11054--11070},
  year={2021}
}


@inproceedings{wang2023label,
  title={Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning},
  author={Wang, Lean and Li, Lei and Dai, Damai and Chen, Deli and Zhou, Hao and Meng, Fandong and Zhou, Jie and Sun, Xu},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@article{prasad2022grips,
  title={Grips: Gradient-free, edit-based instruction search for prompting large language models},
  author={Prasad, Archiki and Hase, Peter and Zhou, Xiang and Bansal, Mohit},
  journal={arXiv preprint arXiv:2203.07281},
  year={2022}
}

@article{lu2022dynamic,
  title={Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning},
  author={Lu, Pan and Qiu, Liang and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Rajpurohit, Tanmay and Clark, Peter and Kalyan, Ashwin},
  journal={arXiv preprint arXiv:2209.14610},
  year={2022}
}

@article{pang2005seeing,
  title={Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales},
  author={Pang, Bo and Lee, Lillian},
  journal={arXiv preprint cs/0506075},
  year={2005}
}

@misc{bach2022promptsource,
      title={PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts},
      author={Stephen H. Bach and Victor Sanh and Zheng-Xin Yong and Albert Webson and Colin Raffel and Nihal V. Nayak and Abheesht Sharma and Taewoon Kim and M Saiful Bari and Thibault Fevry and Zaid Alyafeai and Manan Dey and Andrea Santilli and Zhiqing Sun and Srulik Ben-David and Canwen Xu and Gunjan Chhablani and Han Wang and Jason Alan Fries and Maged S. Al-shaibani and Shanya Sharma and Urmish Thakker and Khalid Almubarak and Xiangru Tang and Xiangru Tang and Mike Tian-Jian Jiang and Alexander M. Rush},
      year={2022},
      eprint={2202.01279},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{liu2021p,
  title={P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2110.07602},
  year={2021}
}

@article{OpenAI2023GPT4TR,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.08774}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{yu2022surprising,
  title={The surprising effectiveness of ppo in cooperative multi-agent games},
  author={Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24611--24624},
  year={2022}
}


@article{guo2022improving,
  title={Improving the sample efficiency of prompt tuning with domain adaptation},
  author={Guo, Xu and Li, Boyang and Yu, Han},
  journal={arXiv preprint arXiv:2210.02952},
  year={2022}
}

@article{ge2023domain,
  title={Domain adaptation via prompt learning},
  author={Ge, Chunjiang and Huang, Rui and Xie, Mixue and Lai, Zihang and Song, Shiji and Li, Shuang and Huang, Gao},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023},
  publisher={IEEE}
}


@article{dulac2019challenges,
  title={Challenges of real-world reinforcement learning},
  author={Dulac-Arnold, Gabriel and Mankowitz, Daniel and Hester, Todd},
  journal={arXiv preprint arXiv:1904.12901},
  year={2019}
}

@inproceedings{mao2023cross,
  title={Cross-entropy loss functions: Theoretical analysis and applications},
  author={Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
  booktitle={International Conference on Machine Learning},
  pages={23803--23828},
  year={2023},
  organization={PMLR}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{vig-2019-multiscale,
    title = "A Multiscale Visualization of Attention in the Transformer Model",
    author = "Vig, Jesse",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-3007",
    doi = "10.18653/v1/P19-3007",
    pages = "37--42",
}

@inproceedings{xu2023making,
  title={Making pre-trained language models end-to-end few-shot learners with contrastive prompt tuning},
  author={Xu, Ziyun and Wang, Chengyu and Qiu, Minghui and Luo, Fuli and Xu, Runxin and Huang, Songfang and Huang, Jun},
  booktitle={Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
  pages={438--446},
  year={2023}
}


@article{liu2023gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={AI Open},
  year={2023},
  publisher={Elsevier}
}

@article{lee1998stability,
  title={The stability, scalability and performance of multi-agent systems},
  author={Lee, Lyndon C and Nwana, Hyacinth S and Ndumu, Divine T and De Wilde, Philippe},
  journal={BT Technology Journal},
  volume={16},
  number={3},
  pages={94--103},
  year={1998},
  publisher={Springer}
}

@article{bai2024reducing,
  title={Reducing Redundant Computation in Multi-Agent Coordination through Locally Centralized Execution},
  author={Bai, Yidong and Sugawara, Toshiharu},
  journal={arXiv preprint arXiv:2404.13096},
  year={2024}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{zhang2022tempera,
  title={Tempera: Test-time prompting via reinforcement learning},
  author={Zhang, Tianjun and Wang, Xuezhi and Zhou, Denny and Schuurmans, Dale and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2211.11890},
  year={2022}
}

@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}


@article{lu2021fantastically,
  title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  journal={arXiv preprint arXiv:2104.08786},
  year={2021}
}

@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@article{clark2019does,
  title={What does bert look at? an analysis of bert's attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal={arXiv preprint arXiv:1906.04341},
  year={2019}
}

@article{wang2022paying,
  title={Paying more attention to self-attention: Improving pre-trained language models via attention guiding},
  author={Wang, Shanshan and Chen, Zhumin and Ren, Zhaochun and Liang, Huasheng and Yan, Qiang and Ren, Pengjie},
  journal={arXiv preprint arXiv:2204.02922},
  year={2022}
}

@article{htut2019attention,
  title={Do attention heads in BERT track syntactic dependencies?},
  author={Htut, Phu Mon and Phang, Jason and Bordia, Shikha and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1911.12246},
  year={2019}
}

@article{lin2019open,
  title={Open sesame: Getting inside BERT's linguistic knowledge},
  author={Lin, Yongjie and Tan, Yi Chern and Frank, Robert},
  journal={arXiv preprint arXiv:1906.01698},
  year={2019}
}


@article{qin2021learning,
  title={Learning how to ask: Querying LMs with mixtures of soft prompts},
  author={Qin, Guanghui and Eisner, Jason},
  journal={arXiv preprint arXiv:2104.06599},
  year={2021}
}

@article{workrethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Work, What Makes In-Context Learning}
}

@article{gao2020making,
  title={Making pre-trained language models better few-shot learners},
  author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  journal={arXiv preprint arXiv:2012.15723},
  year={2020}
}


@inproceedings{bai2024prompt,
  title={Prompt-based distribution alignment for unsupervised domain adaptation},
  author={Bai, Shuanghao and Zhang, Min and Zhou, Wanqi and Huang, Siteng and Luan, Zhirong and Wang, Donglin and Chen, Badong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={2},
  pages={729--737},
  year={2024}
}

@inproceedings{liu2022prompt,
  title={Prompt learning with cross-modal feature alignment for visual domain adaptation},
  author={Liu, Jinxing and Xiao, Junjin and Ma, Haokai and Li, Xiangxian and Qi, Zhuang and Meng, Xiangxu and Meng, Lei},
  booktitle={CAAI International Conference on Artificial Intelligence},
  pages={416--428},
  year={2022},
  organization={Springer}
}

@article{chen2024multi,
  title={Multi-prompt alignment for multi-source unsupervised domain adaptation},
  author={Chen, Haoran and Han, Xintong and Wu, Zuxuan and Jiang, Yu-Gang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}

@article{wang2023tram,
  title={Tram: Benchmarking temporal reasoning for large language models},
  author={Wang, Yuqing and Zhao, Yun},
  journal={arXiv preprint arXiv:2310.00835},
  year={2023}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}