\begin{thebibliography}{10}

\bibitem{blum}
L.~C. Blum and J.-L. Reymond.
\newblock 970 million druglike small molecules for virtual screening in the
  chemical universe database {GDB-13}.
\newblock {\em J. Am. Chem. Soc.}, 131:8732, 2009.

\bibitem{bragman2019stochastic}
Felix~JS Bragman, Ryutaro Tanno, Sebastien Ourselin, Daniel~C Alexander, and
  Jorge Cardoso.
\newblock Stochastic filter groups for multi-task cnns: Learning specialist and
  generalist convolution kernels.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1385--1394, 2019.

\bibitem{bruggemann2020automated}
David Bruggemann, Menelaos Kanakis, Stamatios Georgoulis, and Luc Van~Gool.
\newblock Automated search for resource-efficient branched multi-task networks.
\newblock {\em arXiv preprint arXiv:2008.10292}, 2020.

\bibitem{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock {\em arXiv preprint arXiv:2303.12712}, 2023.

\bibitem{caruana1997multitask}
Rich Caruana.
\newblock Multitask learning.
\newblock {\em Machine learning}, 28(1):41--75, 1997.

\bibitem{chen2018gradnorm}
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich.
\newblock Gradnorm: Gradient normalization for adaptive loss balancing in deep
  multitask networks.
\newblock In {\em International Conference on Machine Learning}, pages
  794--803. PMLR, 2018.

\bibitem{chen2020just}
Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning
  Chai, and Dragomir Anguelov.
\newblock Just pick a sign: Optimizing deep multitask models with gradient sign
  dropout.
\newblock {\em arXiv preprint arXiv:2010.06808}, 2020.

\bibitem{cordts2016cityscapes}
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
  Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
\newblock The cityscapes dataset for semantic urban scene understanding.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3213--3223, 2016.

\bibitem{desideri2012multiple}
Jean-Antoine D{\'e}sid{\'e}ri.
\newblock Multiple-gradient descent algorithm (mgda) for multiobjective
  optimization.
\newblock {\em Comptes Rendus Mathematique}, 350(5-6):313--318, 2012.

\bibitem{fey2019fast}
Matthias Fey and Jan~Eric Lenssen.
\newblock Fast graph representation learning with pytorch geometric.
\newblock {\em arXiv preprint arXiv:1903.02428}, 2019.

\bibitem{fifty2021efficiently}
Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn.
\newblock Efficiently identifying task groupings for multi-task learning.
\newblock {\em Advances in Neural Information Processing Systems},
  34:27503--27516, 2021.

\bibitem{gao2020mtl}
Yuan Gao, Haoping Bai, Zequn Jie, Jiayi Ma, Kui Jia, and Wei Liu.
\newblock Mtl-nas: Task-agnostic neural architecture search towards
  general-purpose multi-task learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on computer vision and
  pattern recognition}, pages 11543--11552, 2020.

\bibitem{guo2018dynamic}
Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li~Fei-Fei.
\newblock Dynamic task prioritization for multitask learning.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 270--287, 2018.

\bibitem{guo2020learning}
Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht.
\newblock Learning to branch for multi-task learning.
\newblock In {\em International Conference on Machine Learning}, pages
  3854--3863. PMLR, 2020.

\bibitem{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em International Conference on Machine Learning}, pages
  1861--1870. PMLR, 2018.

\bibitem{javaloy2021rotograd}
Adri{\'a}n Javaloy and Isabel Valera.
\newblock Rotograd: Dynamic gradient homogenization for multi-task learning.
\newblock {\em arXiv preprint arXiv:2103.02631}, 2021.

\bibitem{katrutsa2020follow}
Alexandr Katrutsa, Daniil Merkulov, Nurislam Tursynbek, and Ivan Oseledets.
\newblock Follow the bisector: a simple method for multi-objective
  optimization.
\newblock {\em arXiv preprint arXiv:2007.06937}, 2020.

\bibitem{kendall2018multi}
Alex Kendall, Yarin Gal, and Roberto Cipolla.
\newblock Multi-task learning using uncertainty to weigh losses for scene
  geometry and semantics.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7482--7491, 2018.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kirillov2023segment}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
  Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C Berg, Wan-Yen Lo, et~al.
\newblock Segment anything.
\newblock {\em arXiv preprint arXiv:2304.02643}, 2023.

\bibitem{kokkinos2017ubernet}
Iasonas Kokkinos.
\newblock Ubernet: Training a universal convolutional neural network for low-,
  mid-, and high-level vision using diverse datasets and limited memory.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 6129--6138, 2017.

\bibitem{kurin2022defense}
Vitaly Kurin, Alessandro De~Palma, Ilya Kostrikov, Shimon Whiteson, and Pawan~K
  Mudigonda.
\newblock In defense of the unitary scalarization for deep multi-task learning.
\newblock {\em Advances in Neural Information Processing Systems},
  35:12169--12183, 2022.

\bibitem{lin2021closer}
Baijiong Lin, Feiyang Ye, and Yu~Zhang.
\newblock A closer look at loss weighting in multi-task learning.
\newblock {\em arXiv preprint arXiv:2111.10603}, 2021.

\bibitem{liu2021conflict}
Bo~Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu.
\newblock Conflict-averse gradient descent for multi-task learning.
\newblock {\em Advances in Neural Information Processing Systems},
  34:18878--18890, 2021.

\bibitem{liu2020towards}
Liyang Liu, Yi~Li, Zhanghui Kuang, Jing-Hao Xue, Yimin Chen, Wenming Yang,
  Qingmin Liao, and Wayne Zhang.
\newblock Towards impartial multi-task learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{liu2022auto}
Shikun Liu, Stephen James, Andrew~J Davison, and Edward Johns.
\newblock Auto-lambda: Disentangling dynamic task relationships.
\newblock {\em arXiv preprint arXiv:2202.03091}, 2022.

\bibitem{liu2019end}
Shikun Liu, Edward Johns, and Andrew~J Davison.
\newblock End-to-end multi-task learning with attention.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1871--1880, 2019.

\bibitem{liu2015faceattributes}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In {\em Proceedings of International Conference on Computer Vision
  (ICCV)}, December 2015.

\bibitem{long2017learning}
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip~S Yu.
\newblock Learning multiple tasks with multilinear relationship networks.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{misra2016cross}
Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert.
\newblock Cross-stitch networks for multi-task learning.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3994--4003, 2016.

\bibitem{SilbermanECCV12}
Pushmeet~Kohli Nathan~Silberman, Derek~Hoiem and Rob Fergus.
\newblock Indoor segmentation and support inference from rgbd images.
\newblock In {\em ECCV}, 2012.

\bibitem{navon2022multi}
Aviv Navon, Aviv Shamsian, Idan Achituve, Haggai Maron, Kenji Kawaguchi, Gal
  Chechik, and Ethan Fetaya.
\newblock Multi-task learning as a bargaining game.
\newblock {\em arXiv preprint arXiv:2202.01017}, 2022.

\bibitem{pascal2021improved}
Lucas Pascal, Pietro Michiardi, Xavier Bost, Benoit Huet, and Maria~A Zuluaga.
\newblock Improved optimization strategies for deep multi-task networks.
\newblock {\em arXiv preprint arXiv:2109.11678}, 2021.

\bibitem{ruder2019latent}
Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders S{\o}gaard.
\newblock Latent multi-task architecture learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 4822--4829, 2019.

\bibitem{sener2018multi}
Ozan Sener and Vladlen Koltun.
\newblock Multi-task learning as multi-objective optimization.
\newblock {\em arXiv preprint arXiv:1810.04650}, 2018.

\bibitem{shen2021variational}
Jiayi Shen, Xiantong Zhen, Marcel Worring, and Ling Shao.
\newblock Variational multi-task learning with gumbel-softmax priors.
\newblock {\em Advances in Neural Information Processing Systems},
  34:21031--21042, 2021.

\bibitem{sodhani2021multi}
Shagun Sodhani, Amy Zhang, and Joelle Pineau.
\newblock Multi-task reinforcement learning with context-based representations.
\newblock {\em arXiv preprint arXiv:2102.06177}, 2021.

\bibitem{standley2020tasks}
Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and
  Silvio Savarese.
\newblock Which tasks should be learned together in multi-task learning?
\newblock In {\em International Conference on Machine Learning}, pages
  9120--9132. PMLR, 2020.

\bibitem{thrun1996discovering}
Sebastian Thrun and Joseph O'Sullivan.
\newblock Discovering structure in multiple learning tasks: The tc algorithm.
\newblock In {\em ICML}, volume~96, pages 489--497, 1996.

\bibitem{vandenhende2021multi}
Simon Vandenhende, Stamatios Georgoulis, Wouter Van~Gansbeke, Marc Proesmans,
  Dengxin Dai, and Luc Van~Gool.
\newblock Multi-task learning for dense prediction tasks: A survey.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2021.

\bibitem{xin2022current}
Derrick Xin, Behrooz Ghorbani, Justin Gilmer, Ankush Garg, and Orhan Firat.
\newblock Do current multi-task optimization methods in deep learning even
  help?
\newblock {\em Advances in Neural Information Processing Systems},
  35:13597--13609, 2022.

\bibitem{yang2020multi}
Ruihan Yang, Huazhe Xu, Yi~Wu, and Xiaolong Wang.
\newblock Multi-task reinforcement learning with soft modularization.
\newblock {\em arXiv preprint arXiv:2003.13661}, 2020.

\bibitem{yu2020gradient}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and
  Chelsea Finn.
\newblock Gradient surgery for multi-task learning.
\newblock {\em arXiv preprint arXiv:2001.06782}, 2020.

\bibitem{yu2020meta}
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea
  Finn, and Sergey Levine.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In {\em Conference on Robot Learning}, pages 1094--1100. PMLR, 2020.

\bibitem{zamir2018taskonomy}
Amir~R Zamir, Alexander Sax, William Shen, Leonidas~J Guibas, Jitendra Malik,
  and Silvio Savarese.
\newblock Taskonomy: Disentangling task transfer learning.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3712--3722, 2018.

\bibitem{zhou2022convergence}
Shiji Zhou, Wenpeng Zhang, Jiyan Jiang, Wenliang Zhong, Jinjie Gu, and Wenwu
  Zhu.
\newblock On the convergence of stochastic multi-objective gradient
  manipulation and beyond.
\newblock {\em Advances in Neural Information Processing Systems},
  35:38103--38115, 2022.

\bibitem{zhugradient}
Shijie Zhu, Hui Zhao, Pengjie Wang, Hongbo Deng, Jian Xu, and Bo~Zheng.
\newblock Gradient deconfliction via orthogonal projections onto subspaces for
  multi-task learning.

\end{thebibliography}
