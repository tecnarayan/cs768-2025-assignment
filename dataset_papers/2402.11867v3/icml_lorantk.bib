@article{lee2016gradient,
  title={Gradient descent only converges to minimizers},
  author={Lee, Jason D and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  journal={Conference on Learning Theory},
  year=2016
}

@article{bach2008matrix,
  title={Convex sparse matrix factorizations},
  author={Bach, Francis and Mairal, Julien and Ponce, Jean},
  journal={arXiv preprint arXiv:0812.1869},
  year={2008}
}

@article{haeffele14lowrank,
  title={Structured low-rank matrix factorization: optimality, algorithm, and applications to image processing},
  author={Haeffele, Benjamin and Young, Eric and Vidal, Rene},
  journal={International Conference on Machine Learning},
  year=2014
}

@article{pataki1998,
  title={On the rank of extreme matrices in semidefinite programs and the multiplicity of optimal eigenvalues},
  author={Pataki, G{\'a}bor},
  journal={Mathematics of Operations Research},
  volume={23},
  number={2},
  pages={339--358},
  year={1998},
  publisher={INFORMS}
}

@article{burer2003nonlinear,
  title={A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization},
  author={Burer, Samuel and Monteiro, Renato DC},
  journal={Mathematical Programming},
  volume={95},
  number={2},
  pages={329--357},
  year={2003},
  publisher={Springer}
}

@article{boumal2016non,
  title={The non-convex {B}urer--{M}onteiro approach works on smooth semidefinite programs},
  author={Boumal, Nicolas and Voroninski, Vlad and Bandeira, Afonso},
  journal={Neural Information Processing Systems},
  volume={29},
  year={2016}
}


@article{du2018power,
  title={On the power of over-parametrization in neural networks with quadratic activation},
  author={Du, Simon and Lee, Jason},
  journal={International Conference on Machine Learning},
  year={2018},
}



@article{liu2021p,
  title={P-tuning v2: prompt tuning can be comparable to fine-tuning universally across scales and tasks},
  author={Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
  journal={Annual Meeting of the Association for Computational Linguistics},
  year={2022}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={Empirical Methods in Natural Language Processing},
  year={2021}
}

@article{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  journal={International Conference on Machine Learning},
  year={2019},
}

@article{hu2021lora,
  title={Lo{RA}: low-rank adaptation of large language models},
  author={Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={International Conference on Learning Representations},
  year={2021}
}


@article{malladi2023kernel,
  title={A kernel-based view of language model fine-tuning},
  author={Malladi, Sadhika and Wettig, Alexander and Yu, Dingli and Chen, Danqi and Arora, Sanjeev},
  journal={International Conference on Machine Learning},
  year={2023}
}

@article{jacot2018neural,
  title={Neural tangent kernel: convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Neural Information Processing Systems},
  year={2018}
}

@article{mcdiarmid1989method,
  title={On the method of bounded differences},
  author={McDiarmid, Colin and others},
  journal={Surveys in Combinatorics},
  volume={141},
  number={1},
  pages={148--188},
  year={1989},
  publisher={Norwich}
}

@article{bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  pages={463--482},
  year={2002}
}

@book{bach2021learning,
  title={Learning Theory from First Principles},
  author={Bach, Francis},
  publisher={Draft},
  year={2023}
}

@article{meir2003generalization,
  title={Generalization error bounds for Bayesian mixture algorithms},
  author={Meir, Ron and Zhang, Tong},
  journal={Journal of Machine Learning Research},
  volume={4},
  pages={839--860},
  year={2003}
}

@article{sridharan2008fast,
  title={Fast rates for regularized objectives},
  author={Sridharan, Karthik and Shalev-Shwartz, Shai and Srebro, Nathan},
  journal={Neural Information Processing Systems},
  volume={21},
  year={2008}
}

@article{bartlett2005local,
  title={Local Rademacher complexities},
  author={Bartlett, Peter L and Bousquet, Olivier and Mendelson, Shahar},
  journal={The Annals of Statistics},
  volume={33},
  number={4},
  pages={1497--1537},
  year={2005}
}

@incollection{pataki2000geometry,
  author    = {Pataki, G{\'a}bor},
  title     = {The Geometry of Semidefinite Programming},
  booktitle = {Handbook of Semidefinite Programming: Theory, Algorithms, and Applications},
  publisher = {Springer},
  year      = 2000,
  pages     = {29-65},
  editor    = {Wolkowicz, Henry and Saigal, Romesh and Vandenberghe, Lieven}
}

@article{barvinok1995problems,
  title={Problems of distance geometry and convex properties of quadratic maps},
  author={Barvinok, Alexander I.},
  journal={Discrete \& Computational Geometry},
  volume={13},
  pages={189--202},
  year={1995},
  publisher={Springer}
}

@article{maurer2016vector,
  title={A vector-contraction inequality for Rademacher complexities},
  author={Maurer, Andreas},
  journal={Algorithmic Learning Theory},
  year={2016},
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of Control, Signals and Systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}

@article{hornik1990universal,
  title={Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural Networks},
  volume={3},
  number={5},
  pages={551--560},
  year={1990},
  publisher={Elsevier}
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}

@article{delalleau2011shallow,
  title={Shallow vs. deep sum-product networks},
  author={Delalleau, Olivier and Bengio, Yoshua},
  journal={Neural Information Processing Systems},
  year={2011}
}

@article{bengio2011expressive,
  title={On the expressive power of deep architectures},
  author={Bengio, Yoshua and Delalleau, Olivier},
  journal={Algorithmic Learning Theory},
  year={2011}
}

@article{lu2017expressive,
  title={The expressive power of neural networks: a view from the width},
  author={Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  journal={Neural Information Processing Systems},
  year={2017}
}

@article{lu2020universal,
  title={A universal approximation theorem of deep neural networks for expressing probability distributions},
  author={Lu, Yulong and Lu, Jianfeng},
  journal={Neural Information Processing Systems},
  year={2020}
}

@article{lee2017ability,
  title={On the ability of neural nets to express distributions},
  author={Lee, Holden and Ge, Rong and Ma, Tengyu and Risteski, Andrej and Arora, Sanjeev},
  journal={Conference on Learning Theory},
  year={2017}
}

@article{duan2023minimum,
  title={Minimum width of leaky-relu neural networks for uniform universal approximation},
  author={Duan, Yifei and Ji, Guanghua and Cai, Yongqiang and others},
  journal={International Conference on Machine Learning},
  year={2023}
}

@article{du2017gradient,
  title={Gradient descent can take exponential time to escape saddle points},
  author={Du, Simon S and Jin, Chi and Lee, Jason D and Jordan, Michael I and Singh, Aarti and Poczos, Barnabas},
  journal={Neural Information Processing Systems},
  year={2017}
}

@article{ge2015escaping,
  title={Escaping from saddle pointsâ€”online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  journal={Conference on Learning Theory},
  year={2015}
}

@article{jin2017escape,
  title={How to escape saddle points efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  journal={International Conference on Machine Learning},
  year={2017}
}

@article{soltanolkotabi2018theoretical,
  title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author={Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D},
  journal={IEEE Transactions on Information Theory},
  volume={65},
  number={2},
  pages={742--769},
  year={2018},
  publisher={IEEE}
}

@article{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  journal={International Conference on Machine Learning},
  year={2019}
}

@article{zou2018stochastic,
  title={Stochastic gradient descent optimizes over-parameterized deep relu networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={Machine Learning},
  year={2020},
  number = {3},
  volume ={109},
  pages = {467--492},
  publisher = {Springer}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@article{ge2017no,
  title={No spurious local minima in nonconvex low rank problems: a unified geometric analysis},
  author={Ge, Rong and Jin, Chi and Zheng, Yi},
  booktitle={International Conference on Machine Learning},
  year={2017}
}

@article{no2021wgan,
  title={WGAN with an infinitely wide generator has no spurious stationary points},
  author={No, Albert and Yoon, TaeHo and Sehyun, Kwon and Ryu, Ernest K},
  booktitle={International Conference on Machine Learning},
  year={2021},
}

@article{hardt2016train,
  title={Train faster, generalize better: stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  journal={International Conference on Machine Learning},
  year={2016}
}

@article{dinh2017sharp,
  title={Sharp minima can generalize for deep nets},
  author={Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  journal={International Conference on Machine Learning},
  year={2017}
}

@article{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
  journal={Neural Information Processing Systems},
  year={2017}
}

@article{bartlett2002model,
  title={Model selection and error estimation},
  author={Bartlett, Peter L and Boucheron, St{\'e}phane and Lugosi, G{\'a}bor},
  journal={Machine Learning},
  volume={48},
  pages={85--113},
  year={2002},
  publisher={Springer}
}

@incollection{koltchinskii2000rademacher,
  title={Rademacher Processes and Bounding the Risk of Function Learning},
  author={Koltchinskii, Vladimir and Panchenko, Dmitriy},
  booktitle={High Dimensional Probability II},
  pages={443--457},
  year={2000},
  publisher={Springer},
  editor = {Gin{\'e}, Evarist and Mason, David M. and Wellner, Jon A.}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={The Journal of Machine Learning Research},
  volume={2},
  pages={499--526},
  year={2002},
  publisher={JMLR}
}

@article{wu2017towards,
  title={Towards understanding generalization of deep learning: perspective of loss landscapes},
  author={Wu, Lei and Zhu, Zhanxing and others},
  journal={arXiv preprint arXiv:1706.10239},
  year={2017}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM}
}

@article{chen2020generalized,
  title={A generalized neural tangent kernel analysis for two-layer neural networks},
  author={Chen, Zixiang and Cao, Yuan and Gu, Quanquan and Zhang, Tong},
  journal={Neural Information Processing Systems},
  year={2020}
}

@article{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  journal={Neural Information Processing Systems},
  year={2019}
}

@article{wei2022more,
  title={More than a toy: random matrix models predict how real-world neural representations generalize},
  author={Wei, Alexander and Hu, Wei and Steinhardt, Jacob},
  journal={International Conference on Machine Learning},
  year={2022},
}

@article{fu2023effectiveness,
  title={On the effectiveness of parameter-efficient fine-tuning},
  author={Fu, Zihao and Yang, Haoran and So, Anthony Man-Cho and Lam, Wai and Bing, Lidong and Collier, Nigel},
  journal={AAAI Conference on Artificial Intelligence},
  year={2023}
}

@article{dettmers2023qlora,
  title={Q{L}o{RA}: efficient finetuning of quantized llms},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Neural Information Processing Systems},
  year={2023}
}

@article{lialin2023relora,
  title={Re{L}o{RA}: high-rank training through low-rank updates},
  author={Lialin, Vladislav and Muckatira, Sherin and Shivagunde, Namrata and Rumshisky, Anna},
  journal={Workshop on Advancing Neural Network Training (WANT): Computational Efficiency, Scalability, and Resource Optimization},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Neural Information Processing Systems},
  year={2017}
}

@article{yun2019transformers,
  title={Are transformers universal approximators of sequence-to-sequence functions?},
  author={Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  journal={International Conference on Learning Representations},
  year={2019}
}

@article{giannou2023looped,
  title={Looped transformers as programmable computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  journal={International Conference on Machine Learning},
  year={2023}
}

@article{wei2022statistically,
  title={Statistically meaningful approximation: a case study on approximating turing machines with transformers},
  author={Wei, Colin and Chen, Yining and Ma, Tengyu},
  journal={Neural Information Processing Systems},
  year={2022}
}


@article{zhang2020adaptive,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Neural Information Processing Systems},
  year={2020}
}

@article{liu2020understanding,
  title={Understanding the difficulty of training transformers},
  author={Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  journal={Empirical Methods in Natural Language Processing},
  year={2020}
}

@article{boix2023transformers,
  title={Transformers learn through gradual rank increase},
  author={Boix-Adsera, Enric and Littwin, Etai and Abbe, Emmanuel and Bengio, Samy and Susskind, Joshua},
  journal={Neural Information Processing Systems},
  year={2023}
}

@article{aghajanyan2020intrinsic,
  title={Intrinsic dimensionality explains the effectiveness of language model fine-tuning},
  author={Aghajanyan, Armen and Zettlemoyer, Luke and Gupta, Sonal},
  journal={Association for Computational Linguistics},
  year={2021}
}

@article{ge2016matrix,
  title={Matrix completion has no spurious local minimum},
  author={Ge, Rong and Lee, Jason D and Ma, Tengyu},
  journal={Neural Information Processing Systems},
  year={2016}
}

@article{jin2023understanding,
  title={Understanding incremental learning of gradient descent: A fine-grained analysis of matrix sensing},
  author={Jin, Jikai and Li, Zhiyuan and Lyu, Kaifeng and Du, Simon Shaolei and Lee, Jason D},
  journal={International Conference on Machine Learning},
  year={2023},
}

@article{schick2020exploiting,
  title={Exploiting cloze questions for few shot text classification and natural language inference},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={Association for Computational Linguistics},
  year={2021}
}

@article{gao2020making,
  title={Making pre-trained language models better few-shot learners},
  author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  journal={Association for Computational Linguistics},
  year={2021}
}

@book{polyak1987introduction,
  title={Introduction to Optimization},
  author={Polyak, Boris T},
  year={1987},
  publisher={New York, Optimization Software}
}

@article{liu2019roberta,
  title={Ro{BERT}a: a robustly optimized {BERT} pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{smith2023continual,
  title={Continual diffusion: continual customization of text-to-image diffusion with c-lora},
  author={Smith, James Seale and Hsu, Yen-Chang and Zhang, Lingyu and Hua, Ting and Kira, Zsolt and Shen, Yilin and Jin, Hongxia},
  journal={arXiv preprint arXiv:2304.06027},
  year={2023}
}

@article{choilora,
  title={Lo{RA} can replace time and class embeddings in diffusion probabilistic models},
  author={Choi, Joo Young and Park, Jaesung and Park, Inkyu and Cho, Jaewoong and No, Albert and Ryu, Ernest K},
  journal = {NeurIPS 2023 Workshop on Diffusion Models},
 year = {2023}
}

@misc{ryu2023low,
  title={Low-rank adaptation for fast text-to-image diffusion fine-tuning},
  author={Ryu, Simo},
  year = {2023},
  url = {https://github.com/cloneofsimo/lora}
}

@misc{beansdata,
    author="{Makerere AI Lab}",
    title="Bean disease dataset",
    year="2020",
    url="https://github.com/AI-Lab-Makerere/ibean/"
}

@article{yeh2023navigating,
  title={Navigating Text-To-Image Customization: from {L}y{CORIS} Fine-Tuning to Model Evaluation},
  author={Yeh, Shin-Ying and Hsieh, Yu-Guan and Gao, Zhidong and Yang, Bernard BW and Oh, Giyeong and Gong, Yanmin},
  journal={International Conference on Learning Representations},
  year={2024}
}

@article{johnson1984extension,
author = {Johnson, William and Lindenstrauss, Joram},
year = {1984},
pages = {189-206},
title = {Extensions of Lipschitz maps into a Hilbert space},
volume = {26},
journal = {Contemporary Mathematics},
}

@article{recht2010guaranteed,
  title={Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization},
  author={Recht, Benjamin and Fazel, Maryam and Parrilo, Pablo A},
  journal={SIAM review},
  volume={52},
  number={3},
  pages={471--501},
  year={2010},
  publisher={SIAM}
}

@article{
zeng2023expressive,
title={The Expressive Power of Low-Rank Adaptation},
author={Zeng, Yuchen and Lee, Kangwook},
journal={International Conference on Learning Representations},
year={2024}
}

@article{helmke1995critical,
  title={Critical points of matrix least squares distance functions},
  author={Helmke, Uwe and Shayman, Mark A},
  journal={Linear Algebra and its Applications},
  volume={215},
  pages={1--19},
  year={1995},
  publisher={Elsevier}
}

@article{bhojanapalli2018smoothed,
  title={Smoothed analysis for low-rank solutions to semidefinite programs in quadratic penalty form},
  author={Bhojanapalli, Srinadh and Boumal, Nicolas and Jain, Prateek and Netrapalli, Praneeth},
  journal={Conference On Learning Theory},
  year={2018}
}

@article{cabral2013unifying,
  title={Unifying nuclear norm and bilinear factorization approaches for low-rank matrix decomposition},
  author={Cabral, Ricardo and De la Torre, Fernando and Costeira, Jo{\~a}o P and Bernardino, Alexandre},
  journal={International Conference on Computer Vision},
  year={2013}
}

@article{pilanci2020neural,
  title={Neural networks are convex regularizers: exact polynomial-time convex optimization formulations for two-layer networks},
  author={Pilanci, Mert and Ergen, Tolga},
  journal={International Conference on Machine Learning},
  year={2020},
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={International Conference on Learning Representations},
  year={2021}
}

@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Neural Information Processing Systems},
  year={2020}
}

@article{yang2021superb,
  title={Superb: Speech processing universal performance benchmark},
  author={Yang, Shu-wen and Chi, Po-Han and Chuang, Yung-Sung and Lai, Cheng-I Jeff and Lakhotia, Kushal and Lin, Yist Y and Liu, Andy T and Shi, Jiatong and Chang, Xuankai and Lin, Guan-Ting and others},
  journal={Interspeech},
  year={2021}
}

@article{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen--Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={International Conference on Machine Learning},
  year={2019},
}

@article{allen2019convergence2,
  title={On the convergence rate of training recurrent neural networks},
  author={Allen--Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={Neural Information Processing Systems},
  year={2019}
}