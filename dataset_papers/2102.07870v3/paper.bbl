\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, et~al.]{tensorflow2015-whitepaper}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., et~al.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
\newblock \emph{arXiv preprint arXiv:1603.04467}, 2016.

\bibitem[Andrica \& Rohan(2010)Andrica and Rohan]{andrica2010image}
Andrica, D. and Rohan, R.-A.
\newblock The image of the exponential map and some applications.
\newblock In \emph{Proc. 8th Joint Conference on Mathematics and Computer
  Science MaCS, Komarno, Slovakia}, pp.\  3--14, 2010.

\bibitem[Bauer(1974)]{Computational_Graphs}
Bauer, F.~L.
\newblock Computational graphs and rounding error.
\newblock \emph{SIAM Journal on Numerical Analysis}, 11\penalty0 (1):\penalty0
  87--96, 1974.

\bibitem[Baydin et~al.(2018)Baydin, Pearlmutter, Radul, and
  Siskind]{baydin2015automatic}
Baydin, A.~G., Pearlmutter, B.~A., Radul, A.~A., and Siskind, J.~M.
\newblock Automatic differentiation in machine learning: a survey.
\newblock \emph{Journal of machine learning research}, 18, 2018.

\bibitem[Behrmann et~al.(2019)Behrmann, Grathwohl, Chen, Duvenaud, and
  Jacobsen]{behrmann2019invertible}
Behrmann, J., Grathwohl, W., Chen, R.~T., Duvenaud, D., and Jacobsen, J.-H.
\newblock Invertible residual networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  573--582. PMLR, 2019.

\bibitem[Chang et~al.(2018)Chang, Meng, Haber, Ruthotto, Begert, and
  Holtham]{chang2017reversible}
Chang, B., Meng, L., Haber, E., Ruthotto, L., Begert, D., and Holtham, E.
\newblock Reversible architectures for arbitrarily deep residual neural
  networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and
  Duvenaud]{chen2018neural}
Chen, T.~Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D.~K.
\newblock Neural ordinary differential equations.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  6571--6583, 2018.

\bibitem[Chun et~al.(2020)Chun, Huang, Lim, and Fessler]{chun2020momentum}
Chun, I.~Y., Huang, Z., Lim, H., and Fessler, J.
\newblock Momentum-net: Fast and convergent iterative neural network for
  inverse problems.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2020.

\bibitem[Conway(2012)]{conway2012functions}
Conway, J.~B.
\newblock \emph{Functions of one complex variable II}, volume 159.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Craven \& Csordas(2002)Craven and Csordas]{craven2002iterated}
Craven, T. and Csordas, G.
\newblock Iterated laguerre and tur{\'a}n inequalities.
\newblock \emph{J. Inequal. Pure Appl. Math}, 3\penalty0 (3):\penalty0 14,
  2002.

\bibitem[Culver(1966)]{culver1966existence}
Culver, W.~J.
\newblock On the existence and uniqueness of the real logarithm of a matrix.
\newblock \emph{Proceedings of the American Mathematical Society}, 17\penalty0
  (5):\penalty0 1146--1151, 1966.

\bibitem[Cybenko(1989)]{Cybenkot2006ApproximationBS}
Cybenko, G.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of control, signals and systems}, 2\penalty0
  (4):\penalty0 303--314, 1989.

\bibitem[Daubechies et~al.(2004)Daubechies, Defrise, and
  De~Mol]{daubechies2004iterative}
Daubechies, I., Defrise, M., and De~Mol, C.
\newblock An iterative thresholding algorithm for linear inverse problems with
  a sparsity constraint.
\newblock \emph{Communications on Pure and Applied Mathematics: A Journal
  Issued by the Courant Institute of Mathematical Sciences}, 57\penalty0
  (11):\penalty0 1413--1457, 2004.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Dryanov \& Rahman(1999)Dryanov and Rahman]{dryanov1999approximation}
Dryanov, D. and Rahman, Q.
\newblock Approximation by entire functions belonging to the laguerre--polya
  class.
\newblock \emph{Methods and Applications of Analysis}, 6\penalty0 (1):\penalty0
  21--38, 1999.

\bibitem[Gantmacher(1959)]{gant}
Gantmacher, F.~R.
\newblock The theory of matrices.
\newblock \emph{Chelsea, New York}, Vol. I, 1959.

\bibitem[Gholami et~al.(2019)Gholami, Keutzer, and Biros]{gholami2019anode}
Gholami, A., Keutzer, K., and Biros, G.
\newblock Anode: Unconditionally accurate memory-efficient gradients for neural
  odes.
\newblock \emph{arXiv preprint arXiv:1902.10298}, 2019.

\bibitem[Gomez et~al.(2017)Gomez, Ren, Urtasun, and
  Grosse]{gomez2017reversible}
Gomez, A.~N., Ren, M., Urtasun, R., and Grosse, R.~B.
\newblock The reversible residual network: Backpropagation without storing
  activations.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}. Curran Associates, Inc., 2017.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, Courville, and
  Bengio]{Goodfellow-et-al-2016}
Goodfellow, I., Bengio, Y., Courville, A., and Bengio, Y.
\newblock \emph{Deep learning}, volume~1.
\newblock MIT press Cambridge, 2016.

\bibitem[Gregor \& LeCun(2010)Gregor and LeCun]{gregor2010learning}
Gregor, K. and LeCun, Y.
\newblock Learning fast approximations of sparse coding.
\newblock In \emph{Proceedings of the 27th international conferenceon machine
  learning}, pp.\  399--406, 2010.

\bibitem[Griewank \& Walther(2008)Griewank and
  Walther]{doi:10.1137/1.9780898717761}
Griewank, A. and Walther, A.
\newblock \emph{Evaluating derivatives: principles and techniques of
  algorithmic differentiation}.
\newblock SIAM, 2008.

\bibitem[Gusak et~al.(2020)Gusak, Markeeva, Daulbaev, Katrutsa, Cichocki, and
  Oseledets]{gusak2020towards}
Gusak, J., Markeeva, L., Daulbaev, T., Katrutsa, A., Cichocki, A., and
  Oseledets, I.
\newblock Towards understanding normalization in neural odes.
\newblock \emph{arXiv preprint arXiv:2004.09222}, 2020.

\bibitem[Haber \& Ruthotto(2017)Haber and Ruthotto]{Haber_2017}
Haber, E. and Ruthotto, L.
\newblock Stable architectures for deep neural networks.
\newblock \emph{Inverse Problems}, 34\penalty0 (1):\penalty0 014004, 2017.

\bibitem[Hairer et~al.(2006)Hairer, Lubich, and Wanner]{Hairer:1250576}
Hairer, E., Lubich, C., and Wanner, G.
\newblock \emph{Geometric numerical integration: structure-preserving
  algorithms for ordinary differential equations}, volume~31.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Hartfiel(1995)]{10.2307/2160975}
Hartfiel, D.~J.
\newblock Dense sets of diagonalizable matrices.
\newblock \emph{Proceedings of the American Mathematical Society}, 123\penalty0
  (6):\penalty0 1669--1672, 1995.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2015deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  9729--9738, 2020.

\bibitem[Jacobsen et~al.(2018)Jacobsen, Smeulders, and
  Oyallon]{jacobsen2018irevnet}
Jacobsen, J.-H., Smeulders, A.~W., and Oyallon, E.
\newblock i-revnet: Deep invertible networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Kolesnikov et~al.(2019)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{alex2019big}
Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and
  Houlsby, N.
\newblock Big transfer (bit): General visual representation learning.
\newblock \emph{arXiv preprint arXiv:1912.11370}, 6\penalty0 (2):\penalty0 8,
  2019.

\bibitem[Krizhevsky et~al.(2010)Krizhevsky, Nair, and Hinton]{CIFAR}
Krizhevsky, A., Nair, V., and Hinton, G.
\newblock Cifar-10 (canadian institute for advanced research).
\newblock \emph{URL http://www. cs. toronto. edu/kriz/cifar. html}, 5, 2010.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{cite-key}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Levin(1996)]{levin1996lectures}
Levin, B.~Y.
\newblock \emph{Lectures on entire functions}, volume 150.
\newblock American Mathematical Soc., 1996.

\bibitem[Li et~al.(2018)Li, Yang, Chen, and Lin]{li2018optimization}
Li, H., Yang, Y., Chen, D., and Lin, Z.
\newblock Optimization algorithm inspired deep neural network structure design.
\newblock In \emph{Asian Conference on Machine Learning}, pp.\  614--629. PMLR,
  2018.

\bibitem[Li et~al.(2019)Li, Lin, and Shen]{li2019deep}
Li, Q., Lin, T., and Shen, Z.
\newblock Deep learning via dynamical systems: An approximation perspective.
\newblock \emph{arXiv preprint arXiv:1912.10382}, 2019.

\bibitem[Lu et~al.(2018)Lu, Zhong, Li, and Dong]{lu2017finite}
Lu, Y., Zhong, A., Li, Q., and Dong, B.
\newblock Beyond finite layer neural networks: Bridging deep architectures and
  numerical differential equations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3276--3285. PMLR, 2018.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{10.5555/3045118.3045343}
Maclaurin, D., Duvenaud, D., and Adams, R.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{International conference on machine learning}, pp.\
  2113--2122. PMLR, 2015.

\bibitem[Martens \& Sutskever(2012)Martens and Sutskever]{martens2012training}
Martens, J. and Sutskever, I.
\newblock Training deep and recurrent networks with hessian-free optimization.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  479--535.
  Springer, 2012.

\bibitem[Massaroli et~al.(2020)Massaroli, Poli, Park, Yamashita, and
  Asama]{massaroli2020dissecting}
Massaroli, S., Poli, M., Park, J., Yamashita, A., and Asama, H.
\newblock Dissecting neural odes.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  3952--3963. Curran Associates, Inc., 2020.

\bibitem[Nguyen et~al.(2020)Nguyen, Baraniuk, Bertozzi, Osher, and
  Wang]{nguyen2020momentumrnn}
Nguyen, T.~M., Baraniuk, R.~G., Bertozzi, A.~L., Osher, S.~J., and Wang, B.
\newblock Momentumrnn: Integrating momentum into recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:2006.06919}, 2020.

\bibitem[Norcliffe et~al.(2020)Norcliffe, Bodnar, Day, Simidjievski, and
  Li{\`o}]{norcliffe2020second}
Norcliffe, A., Bodnar, C., Day, B., Simidjievski, N., and Li{\`o}, P.
\newblock On second order behaviour in augmented neural odes.
\newblock \emph{arXiv preprint arXiv:2006.07220}, 2020.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Peng et~al.(2017)Peng, Zhang, Yu, Luo, and Sun]{peng2017large}
Peng, C., Zhang, X., Yu, G., Luo, G., and Sun, J.
\newblock Large kernel matters--improve semantic segmentation by global
  convolutional network.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  4353--4361, 2017.

\bibitem[Perko(2013)]{perko2013differential}
Perko, L.
\newblock \emph{Differential equations and dynamical systems}, volume~7.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Pontryagin(2018)]{Pontryagin:234445}
Pontryagin, L.~S.
\newblock \emph{Mathematical theory of optimal processes}.
\newblock Routledge, 2018.

\bibitem[Queiruga et~al.(2020)Queiruga, Erichson, Taylor, and
  Mahoney]{queiruga2020continuous}
Queiruga, A.~F., Erichson, N.~B., Taylor, D., and Mahoney, M.~W.
\newblock Continuous-in-depth neural networks.
\newblock \emph{arXiv preprint arXiv:2008.02389}, 2020.

\bibitem[Ruder(2016)]{ruder2016overview}
Ruder, S.
\newblock An overview of gradient descent optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1609.04747}, 2016.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{1986Natur.323..533R}
Rumelhart, D.~E., Hinton, G.~E., and Williams, R.~J.
\newblock Learning representations by back-propagating errors.
\newblock \emph{nature}, 323\penalty0 (6088):\penalty0 533--536, 1986.

\bibitem[Runckel(1969)]{runckel1969zeros}
Runckel, H.-J.
\newblock Zeros of entire functions.
\newblock \emph{Transactions of the American Mathematical Society},
  143:\penalty0 343--362, 1969.

\bibitem[Rusch \& Mishra(2021)Rusch and Mishra]{rusch2020coupled}
Rusch, T.~K. and Mishra, S.
\newblock Coupled oscillatory recurrent neural network (co{\{}rnn{\}}): An
  accurate and (gradient) stable architecture for learning long time
  dependencies.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Ruthotto \& Haber(2019)Ruthotto and Haber]{ruthotto2018deep}
Ruthotto, L. and Haber, E.
\newblock Deep neural networks motivated by partial differential equations.
\newblock \emph{Journal of Mathematical Imaging and Vision}, pp.\  1--13, 2019.

\bibitem[Sun et~al.(2018)Sun, Tao, and Du]{sun2018stochastic}
Sun, Q., Tao, Y., and Du, Q.
\newblock Stochastic training of residual networks: a differential equation
  viewpoint.
\newblock \emph{arXiv preprint arXiv:1812.00174}, 2018.

\bibitem[Tajbakhsh et~al.(2016)Tajbakhsh, Shin, Gurudu, Hurst, Kendall, Gotway,
  and Liang]{tajbakhsh2016convolutional}
Tajbakhsh, N., Shin, J.~Y., Gurudu, S.~R., Hurst, R.~T., Kendall, C.~B.,
  Gotway, M.~B., and Liang, J.
\newblock Convolutional neural networks for medical image analysis: Full
  training or fine tuning?
\newblock \emph{IEEE transactions on medical imaging}, 35\penalty0
  (5):\penalty0 1299--1312, 2016.

\bibitem[Teh et~al.(2019)Teh, Doucet, and Dupont]{dupont2019augmented}
Teh, Y., Doucet, A., and Dupont, E.
\newblock Augmented neural odes.
\newblock \emph{Advances in Neural Information Processing Systems 32 (NIPS
  2019)}, 32\penalty0 (2019), 2019.

\bibitem[Teshima et~al.(2020)Teshima, Tojo, Ikeda, Ishikawa, and
  Oono]{teshima2020universal}
Teshima, T., Tojo, K., Ikeda, M., Ishikawa, I., and Oono, K.
\newblock Universal approximation property of neural ordinary differential
  equations.
\newblock \emph{arXiv preprint arXiv:2012.02414}, 2020.

\bibitem[Tibshirani(1996)]{tibshirani1996regression}
Tibshirani, R.
\newblock Regression shrinkage and selection via the lasso.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 58\penalty0 (1):\penalty0 267--288, 1996.

\bibitem[Touvron et~al.(2019)Touvron, Vedaldi, Douze, and
  Jegou]{touvron2020fixing}
Touvron, H., Vedaldi, A., Douze, M., and Jegou, H.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}. Curran Associates, Inc., 2019.

\bibitem[Verma(2000)]{verma2000introduction}
Verma, A.
\newblock An introduction to automatic differentiation.
\newblock \emph{Current Science}, pp.\  804--807, 2000.

\bibitem[Wang et~al.(2018)Wang, Ye, Zhao, Wu, Li, Song, Xu, and
  Kraska]{wang2018superneurons}
Wang, L., Ye, J., Zhao, Y., Wu, W., Li, A., Song, S.~L., Xu, Z., and Kraska, T.
\newblock Superneurons: Dynamic gpu memory management for training deep neural
  networks.
\newblock In \emph{Proceedings of the 23rd ACM SIGPLAN Symposium on Principles
  and Practice of Parallel Programming}, pp.\  41--53, 2018.

\bibitem[Weinan(2017)]{E_2017}
Weinan, E.
\newblock A proposal on machine learning via dynamical systems.
\newblock \emph{Communications in Mathematics and Statistics}, 5\penalty0
  (1):\penalty0 1--11, 2017.

\bibitem[Weinan et~al.(2019)Weinan, Han, and Li]{E_2018}
Weinan, E., Han, J., and Li, Q.
\newblock A mean-field optimal control formulation of deep learning.
\newblock \emph{Research in the Mathematical Sciences}, 6\penalty0
  (1):\penalty0 10, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Gao, Unterman, and
  Arodz]{zhang2019approximation}
Zhang, H., Gao, X., Unterman, J., and Arodz, T.
\newblock Approximation capabilities of neural odes and invertible residual
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11086--11095. PMLR, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Yao, Gholami, Keutzer, Gonzalez, Biros, and
  Mahoney]{zhang2019anodev2}
Zhang, T., Yao, Z., Gholami, A., Keutzer, K., Gonzalez, J., Biros, G., and
  Mahoney, M.
\newblock Anodev2: A coupled neural ode evolution framework.
\newblock \emph{arXiv preprint arXiv:1906.04596}, 2019.

\bibitem[Zhu et~al.(2017)Zhu, Park, Isola, and Efros]{zhu2017unpaired}
Zhu, J.-Y., Park, T., Isola, P., and Efros, A.~A.
\newblock Unpaired image-to-image translation using cycle-consistent
  adversarial networks.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  2223--2232, 2017.

\end{thebibliography}
