@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}
@article{shepard2015representation,
  title={The representation and parametrization of orthogonal matrices},
  author={Shepard, Ron and Brozell, Scott R and Gidofalvi, Gergely},
  journal={The Journal of Physical Chemistry A},
  volume={119},
  number={28},
  pages={7924--7939},
  year={2015},
  publisher={ACS Publications}
}
@article{krawczuk2020gg,
  title={GG-GAN: A Geometric Graph Generative Adversarial Network},
  author={Krawczuk, Igor and Abranches, Pedro and Loukas, Andreas and Cevher, Volkan},
  year={2020}
}
@article{de2018molgan,
  title={MolGAN: An implicit generative model for small molecular graphs},
  author={De Cao, Nicola and Kipf, Thomas},
  journal={arXiv preprint arXiv:1805.11973},
  year={2018}
}

@inproceedings{morris2019weisfeiler,
  title={Weisfeiler and leman go neural: Higher-order graph neural networks},
  author={Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={4602--4609},
  year={2019}
}
@inproceedings{segol2019universal,
  title={On Universal Equivariant Set Networks},
  author={Segol, Nimrod and Lipman, Yaron},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{qi2017pointnet,
  title={Pointnet: Deep learning on point sets for 3d classification and segmentation},
  author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={652--660},
  year={2017}
}
@inproceedings{ng2002spectral,
  title={On spectral clustering: Analysis and an algorithm},
  author={Ng, Andrew Y and Jordan, Michael I and Weiss, Yair},
  booktitle={Advances in neural information processing systems},
  pages={849--856},
  year={2002}
}
@article{oord2016wavenet,
  title={Wavenet: A generative model for raw audio},
  author={Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1609.03499},
  year={2016}
}

@inproceedings{oord2016conditional,
author = {Oord, A\"{a}ron van den and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
title = {Conditional Image Generation with PixelCNN Decoders},
year = {2016},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
series = {NIPS'16}
}

@article{peixoto2019bayesian,
  title={Bayesian stochastic blockmodeling},
  author={Peixoto, Tiago P},
  journal={Advances in network clustering and blockmodeling},
  pages={289--332},
  year={2019},
  publisher={Wiley Online Library}
}
@article{peixoto2020merge,
  title={Merge-split Markov chain Monte Carlo for community detection},
  author={Peixoto, Tiago P},
  journal={Physical Review E},
  volume={102},
  number={1},
  pages={012305},
  year={2020},
  publisher={APS}
}
@article{peixoto2014graph,
  title={The graph-tool python library},
  author={Peixoto, Tiago P},
  year={2014}
}
@book{fahrmeir2007regression,
  title={Regression},
  author={Fahrmeir, Ludwig and Kneib, Thomas and Lang, Stefan and Marx, Brian},
  year={2007},
  publisher={Springer}
}
@inproceedings{cordella2001improved,
  title={An improved algorithm for matching large graphs},
  author={Cordella, Luigi Pietro and Foggia, Pasquale and Sansone, Carlo and Vento, Mario},
  booktitle={3rd IAPR-TC15 workshop on graph-based representations in pattern recognition},
  pages={149--159},
  year={2001},
  organization={Citeseer}
}

@book{chung1997spectral,
  title={Spectral graph theory},
  author={Chung, Fan RK and Graham, Fan Chung},
  number={92},
  year={1997},
  publisher={American Mathematical Soc.}
}

@article{shi2000normalized,
  title={Normalized cuts and image segmentation},
  author={Shi, Jianbo and Malik, Jitendra},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={22},
  number={8},
  pages={888--905},
  year={2000},
  publisher={Ieee}
}

@article{belkin2003laplacian,
  title={Laplacian eigenmaps for dimensionality reduction and data representation},
  author={Belkin, Mikhail and Niyogi, Partha},
  journal={Neural computation},
  volume={15},
  number={6},
  pages={1373--1396},
  year={2003},
  publisher={MIT Press}
}
@article{dobson2003distinguishing,
  title={Distinguishing enzyme structures from non-enzymes without alignments},
  author={Dobson, Paul D and Doig, Andrew J},
  journal={Journal of molecular biology},
  volume={330},
  number={4},
  pages={771--783},
  year={2003},
  publisher={Elsevier}
}
@inproceedings{petzka2018regularization,
  title={On the regularization of Wasserstein GANs},
  author={Petzka, Henning and Fischer, Asja and Lukovnikov, Denis},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{mahmood2021masked,
  title={Masked graph modeling for molecule generation},
  author={Mahmood, Omar and Mansimov, Elman and Bonneau, Richard and Cho, Kyunghyun},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={1--12},
  year={2021},
  publisher={Nature Publishing Group}
}

@inproceedings{simonovsky2018graphvae,
  title={Graphvae: Towards generation of small graphs using variational autoencoders},
  author={Simonovsky, Martin and Komodakis, Nikos},
  booktitle={International conference on artificial neural networks},
  pages={412--422},
  year={2018},
  organization={Springer}
}

@inproceedings{mitton2021graph,
  title={A Graph {VAE} and Graph Transformer Approach to Generating Molecular Graphs},
  author={Mitton, Joshua and Senn, Hans M and Wynne, Klaas and Murray-Smith, Roderick},
  booktitle={Graph Representation learning and beyong (ICLR Workshop)},
  year={2021}
}

@inproceedings{jin2018junction,
  title={Junction tree variational autoencoder for molecular graph generation},
  author={Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  booktitle={International conference on machine learning},
  pages={2323--2332},
  year={2018},
  organization={PMLR}
}


@InProceedings{pmlr-v119-jin20a,
  title = 	 {Hierarchical Generation of Molecular Graphs using Structural Motifs},
  author =       {Jin, Wengong and Barzilay, Dr.Regina and Jaakkola, Tommi},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4839--4848},
  year = 	 {2020},
  NOeditor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/jin20a/jin20a.pdf},
  NOurl = 	 {https://proceedings.mlr.press/v119/jin20a.html},
  abstract = 	 {Graph generation techniques are increasingly being adopted for drug discovery. Previous graph generation approaches have utilized relatively small molecular building blocks such as atoms or simple cycles, limiting their effectiveness to smaller molecules. Indeed, as we demonstrate, their performance degrades significantly for larger molecules. In this paper, we propose a new hierarchical graph encoder-decoder that employs significantly larger and more flexible graph motifs as basic building blocks. Our encoder produces a multi-resolution representation for each molecule in a fine-to-coarse fashion, from atoms to connected motifs. Each level integrates the encoding of constituents below with the graph at that level. Our autoregressive coarse-to-fine decoder adds one motif at a time, interleaving the decision of selecting a new motif with the process of resolving its attachments to the emerging molecule. We evaluate our model on multiple molecule generation tasks, including polymers, and show that our model significantly outperforms previous state-of-the-art baselines.}
}

@inproceedings{you2018graph,
    author = {You, Jiaxuan and Liu, Bowen and Ying, Rex and Pande, Vijay and Leskovec, Jure},
    title = {Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation},
    year = {2018},
    booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
    series = {NIPS'18}
}

@inproceedings{
anonymous2022dataefficient,
title={Data-Efficient Graph Grammar Learning for Molecular Generation},
author={Anonymous},
booktitle={Submitted to The Tenth International Conference on Learning Representations },
year={2022},
NOurl={https://openreview.net/forum?id=l4IHywGq6a},
note={under review}
}

@inproceedings{liu2018constrained,
  title={Constrained graph variational autoencoders for molecule design},
  author={Liu, Qi and Allamanis, Miltiadis and Brockschmidt, Marc and Gaunt, Alexander L},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={7806--7815},
  year={2018}
}



@article{alon1985lambda1,
  title={$\lambda$1, isoperimetric inequalities for graphs, and superconcentrators},
  author={Alon, Noga and Milman, Vitali D},
  journal={Journal of Combinatorial Theory, Series B},
  volume={38},
  number={1},
  pages={73--88},
  year={1985},
  publisher={Elsevier}
}

@article{alon1986eigenvalues,
  title={Eigenvalues and expanders},
  author={Alon, Noga},
  journal={Combinatorica},
  volume={6},
  number={2},
  pages={83--96},
  year={1986},
  publisher={Springer}
}

@article{SINCLAIR198993,
title = {Approximate counting, uniform generation and rapidly mixing Markov chains},
journal = {Information and Computation},
volume = {82},
number = {1},
pages = {93-133},
year = {1989},
issn = {0890-5401},
doi = {https://doi.org/10.1016/0890-5401(89)90067-9},
NOurl = {https://www.sciencedirect.com/science/article/pii/0890540189900679},
author = {Alistair Sinclair and Mark Jerrum},
abstract = {The paper studies effective approximate solutions to combinatorial counting and unform generation problems. Using a technique based on the simulation of ergodic Markov chains, it is shown that, for self-reducible structures, almost uniform generation is possible in polynomial time provided only that randomised approximate counting to within some arbitrary polynomial factor is possible in polynomial time. It follows that, for self-reducible structures, polynomial time randomised algorithms for counting to within factors of the form (1 + nâˆ’Î²) are available either for all Î² Ïµ R or for no Î² Ïµ R. A substantial part of the paper is devoted to investigating the rate of convergence of finite ergodic Markov chains, and a simple but powerful characterisation of rapid convergence for a broad class of chains based on a structural property of the underlying graph is established. Finally, the general techniques of the paper are used to derive an almost uniform generation procedure for labelled graphs with a given degree sequence which is valid over a much wider range of degrees than previous methods: this in turn leads to randomised approximate counting algorithms for these graphs with very good asymptotic behaviour.}
}

@inproceedings{10.1145/3308558.3313631,
author = {Shine, Alana and Kempe, David},
title = {Generative Graph Models Based on Laplacian Spectra?},
year = {2019},
booktitle = {The World Wide Web Conference},
pages = {1691–1701},
series = {WWW '19}
}

@inproceedings{baldesi2018spectral,
  title={Spectral graph forge: Graph generation targeting modularity},
  author={Baldesi, Luca and Butts, Carter T and Markopoulou, Athina},
  booktitle={IEEE INFOCOM 2018-IEEE Conference on Computer Communications},
  pages={1727--1735},
  year={2018},
  organization={IEEE}
}
@article{lee2014multiway,
  title={Multiway spectral partitioning and higher-order cheeger inequalities},
  author={Lee, James R and Gharan, Shayan Oveis and Trevisan, Luca},
  journal={Journal of the ACM (JACM)},
  volume={61},
  number={6},
  pages={1--30},
  year={2014},
  publisher={ACM New York, NY, USA}
}

@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@inproceedings{kour2014real,
  title={Real-time segmentation of on-line handwritten arabic script},
  author={Kour, George and Saabne, Raid},
  booktitle={Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  pages={417--422},
  year={2014},
  organization={IEEE}
}

@inproceedings{kour2014fast,
  title={Fast classification of handwritten on-line Arabic characters},
  author={Kour, George and Saabne, Raid},
  booktitle={Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  pages={312--318},
  year={2014},
  organization={IEEE}
}

@article{hadash2018estimate,
  title={Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications},
  author={Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal={arXiv preprint arXiv:1804.09028},
  year={2018}
}
@article{tabakFamilyNonparametricDensity2013,
  langid = {english},
  title = {A {{Family}} of {{Nonparametric Density Estimation Algorithms}}},
  volume = {66},
  issn = {1097-0312},
  noNOurl = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21423},
  doi = {10.1002/cpa.21423},
  abstract = {A new methodology for density estimation is proposed. The methodology, which builds on the one developed by Tabak and Vanden-Eijnden, normalizes the data points through the composition of simple maps. The parameters of each map are determined through the maximization of a local quadratic approximation to the log-likelihood. Various candidates for the elementary maps of each step are proposed; criteria for choosing one includes robustness, computational simplicity, and good behavior in high-dimensional settings. A good choice is that of localized radial expansions, which depend on a single parameter: all the complexity of arbitrary, possibly convoluted probability densities can be built through the composition of such simple maps. © 2012 Wiley Periodicals, Inc.},
  number = {2},
  journaltitle = {Communications on Pure and Applied Mathematics},
  NOurldate = {2019-11-19},
  date = {2013},
  pages = {145-164},
  author = {Tabak, E. G. and Turner, Cristina V.},
  file = {files/38833/Tabak and Turner - 2013 - A Family of Nonparametric Density Estimation Algor.pdf;files/38831/cpa.html},
  note = {00047}
}

@article{tabakDensityEstimationDual2010,
  langid = {english},
  title = {Density Estimation by Dual Ascent of the Log-Likelihood},
  volume = {8},
  issn = {1539-6746, 1945-0796},
  noNOurl = {https://projecteuclid.org/euclid.cms/1266935020},
  abstract = {A methodology is developed to assign, from an observed sample, a joint-probability distribution to a set of continuous variables. The algorithm proposed performs this assignment by mapping the original variables onto a jointly-Gaussian set. The map is built iteratively, ascending the log-likelihood of the observations, through a series of steps that move the marginal distributions along a random set of orthogonal directions towards normality.},
  number = {1},
  journaltitle = {Communications in Mathematical Sciences},
  shortjournal = {Commun. Math. Sci.},
  NOurldate = {2019-11-19},
  date = {2010-03},
  pages = {217-233},
  keywords = {machine learning,Density estimation,maximum likelihood},
  author = {Tabak, Esteban G. and Vanden-Eijnden, Eric},
  file = {files/38830/Tabak and Vanden-Eijnden - 2010 - Density estimation by dual ascent of the log-likel.pdf;files/38832/1266935020.html},
  mrnumber = {MR2655907},
  zmnumber = {1189.62063},
  note = {00046}
}

@book{vempala2005random,
  title={The random projection method},
  author={Vempala, Santosh S},
  volume={65},
  year={2005},
  publisher={American Mathematical Soc.}
}


@inproceedings{taggen,
author = {Zhou, Dawei and Zheng, Lecheng and Han, Jiawei and He, Jingrui},
title = {A Data-Driven Graph Generative Model for Temporal Interaction Networks},
year = {2020},
isbn = {9781450379984},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
NOurl = {https://doi.org/10.1145/3394486.3403082},
doi = {10.1145/3394486.3403082},
abstract = {Deep graph generative models have recently received a surge of attention due to its superiority of modeling realistic graphs in a variety of domains, including biology, chemistry, and social science. Despite the initial success, most, if not all, of the existing works are designed for static networks. Nonetheless, many realistic networks are intrinsically dynamic and presented as a collection of system logs (i.e., timestamped interactions/edges between entities), which pose a new research direction for us: how can we synthesize realistic dynamic networks by directly learning from the system logs? In addition, how can we ensure the generated graphs preserve both the structural and temporal characteristics of the real data?To address these challenges, we propose an end-to-end deep generative framework named TagGen. In particular, we start with a novel sampling strategy for jointly extracting structural and temporal context information from temporal networks. On top of that, TagGen parameterizes a bi-level self-attention mechanism together with a family of local operations to generate temporal random walks. At last, a discriminator gradually selects generated temporal random walks, that are plausible in the input data, and feeds them to an assembling module for generating temporal networks. The experimental results in seven real-world data sets across a variety of metrics demonstrate that (1) TagGen outperforms all baselines in the temporal interaction network generation problem, and (2) TagGen significantly boosts the performance of the prediction models in the tasks of anomaly detection and link prediction.},
booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {401–411},
numpages = {11},
keywords = {graph generative model, transformer model, temporal networks},
location = {Virtual Event, CA, USA},
series = {KDD '20}
}

@article{bachlechner2020rezero,
  title={Rezero is all you need: Fast convergence at large depth},
  author={Bachlechner, Thomas and Majumder, Bodhisattwa Prasad and Mao, Huanru Henry and Cottrell, Garrison W and McAuley, Julian},
  journal={arXiv preprint arXiv:2003.04887},
  year={2020}
}

@article{heusel2017gans,
  title={GANs Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium.},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Klambauer, G{\"u}nter and Hochreiter, Sepp},
  year={2017}
}
@article{lin2020pacgan,
  title={Pacgan: The power of two samples in generative adversarial networks},
  author={Lin, Zinan and Khetan, Ashish and Fanti, Giulia and Oh, Sewoong},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={324--335},
  year={2020},
  publisher={IEEE}
}

@inproceedings{you2018graphrnn,
  title={GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models},
  author={You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William L and Leskovec, Jure},
  booktitle={ICML},
  year={2018}
}

@inproceedings{kingma2014adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015}
}

@misc{chordalg,
  author = {Brendan McKay},
  title = {chordal graphs},
  noNOurl = {https://users.cecs.anu.edu.au/~bdm/data/graphs.html},
  year={2020}
}

@article{gidel2018variational,
  title={A variational inequality perspective on generative adversarial networks},
  author={Gidel, Gauthier and Berard, Hugo and Vignoud, Ga{\"e}tan and Vincent, Pascal and Lacoste-Julien, Simon},
  journal={arXiv preprint arXiv:1802.10551},
  year={2018}
}


@InProceedings{pmlr-v108-niu20a,
  title = 	 {Permutation Invariant Graph Generation via Score-Based Generative Modeling},
  author =       {Niu, Chenhao and Song, Yang and Song, Jiaming and Zhao, Shengjia and Grover, Aditya and Ermon, Stefano},
  pages = 	 {4474--4484},
  year = 	 {2020},
  NOeditor = 	 {Silvia Chiappa and Roberto Calandra},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Online},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/niu20a/niu20a.pdf},
  noNOurl = 	 {http://proceedings.mlr.press/v108/niu20a.html},
  abstract = 	 {Learning generative models for graph-structured data is challenging because graphs are discrete, combinatorial, and the underlying data distribution is invariant to the ordering of nodes. However, most of the existing generative models for graphs are not invariant to the chosen ordering, which might lead to an undesirable bias in the learned distribution. To address this difficulty, we propose a permutation invariant approach to modeling graphs, using the recent framework of score-based generative modeling. In particular, we design a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution at the input graph (a.k.a., the score function). This permutation equivariant model of gradients implicitly defines a permutation invariant distribution for graphs. We train this graph neural network with score matching and sample from it with annealed Langevin dynamics. In our experiments, we first demonstrate the capacity of this new architecture in learning discrete graph algorithms. For graph generation, we find that our learning approach achieves better or comparable results to existing models on benchmark datasets.}
}

@inproceedings{kipf2016variational,
  title={Variational graph auto-encoders},
  author={Kipf, Thomas N and Welling, Max},
  booktitle={NIPS Workshop onBayesian Deep Learning},
  year={2016}
}

@inproceedings{yang2019conditional,
  title={Conditional structure generation through graph variational generative adversarial nets},
  author={Yang, Carl and Zhuang, Peiye and Shi, Wenhan and Luu, Alan and Li, Pan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1340--1351},
  year={2019}
}
@article{dwivedi2020benchmarking,
  title={Benchmarking graph neural networks},
  author={Dwivedi, Vijay Prakash and Joshi, Chaitanya K and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  journal={arXiv preprint arXiv:2003.00982},
  year={2020}
}


@inproceedings{maron2019provably,
  title={Provably powerful graph networks},
  author={Maron, Haggai and Ben-Hamu, Heli and Serviansky, Hadar and Lipman, Yaron},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2156--2167},
  year={2019}
}

@article{chen2020can,
  title={Can graph neural networks count substructures?},
  author={Chen, Zhengdao and Chen, Lei and Villar, Soledad and Bruna, Joan},
  journal={arXiv preprint arXiv:2002.04025},
  year={2020}
}


@inproceedings{
xu2018how,
title={How Powerful are Graph Neural Networks?},
author={Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
booktitle={International Conference on Learning Representations},
year={2019},
noNOurl={https://openreview.net/forum?id=ryGs6iA5Km},
}

@misc{chaillou2019archigan,
  title={ArchiGAN: a Generative Stack for Apartment Building Design},
  author={Chaillou, S},
  year={2019}
}

@inproceedings{loukas2019graph,
  title={What graph neural networks cannot learn: depth vs width},
  author={Loukas, Andreas},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{serviansky2020set2graph,
 author = {Serviansky, Hadar and Segol, Nimrod and Shlomi, Jonathan and Cranmer, Kyle and Gross, Eilam and Maron, Haggai and Lipman, Yaron},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {22080--22091},
 title = {Set2Graph: Learning Graphs From Sets},
 volume = {33},
 year = {2020}
}



@inproceedings{liao2019efficient,
  title={Efficient graph generation with graph recurrent attention networks},
  author={Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Hamilton, Will and Duvenaud, David K and Urtasun, Raquel and Zemel, Richard},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4255--4265},
  year={2019}
}

@article{rezendeVariationalInferenceNormalizing2016a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1505.05770},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  noNOurl = {http://arxiv.org/abs/1505.05770},
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  NOurldate = {2019-11-19},
  date = {2016-06-14},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Statistics - Methodology,Statistics - Computation},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  file = {files/38834/Rezende and Mohamed - 2016 - Variational Inference with Normalizing Flows.pdf},
  note = {00667}
}

@article{kingmaFastGradientBasedInference2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1306.0733},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Fast {{Gradient}}-{{Based Inference}} with {{Continuous Latent Variable Models}} in {{Auxiliary Form}}},
  noNOurl = {http://arxiv.org/abs/1306.0733},
  abstract = {We propose a technique for increasing the efficiency of gradient-based inference and learning in Bayesian networks with multiple layers of continuous latent variables. We show that, in many cases, it is possible to express such models in an auxiliary form, where continuous latent variables are conditionally deterministic given their parents and a set of independent auxiliary variables. Variables of models in this auxiliary form have much larger Markov blankets, leading to significant speedups in gradient-based inference, e.g. rapid mixing Hybrid Monte Carlo and efficient gradient-based optimization. The relative efficiency is confirmed in experiments.},
  NOurldate = {2019-11-19},
  date = {2013-06-04},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Kingma, Diederik P.},
  file = {files/38837/Kingma - 2013 - Fast Gradient-Based Inference with Continuous Late.pdf},
  note = {00017}
}

@inproceedings{Fey/Lenssen/2019,
  title={Fast Graph Representation Learning with {PyTorch Geometric}},
  author={Fey, Matthias and Lenssen, Jan E.},
  booktitle={ICLR Workshop on Representation Learning on Graphs and Manifolds},
  year={2019},
}
@article{falcon2019pytorch,
  title={PyTorch Lightning},
  author={Falcon, WA},
  journal={GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning},
  volume={3},
  year={2019}
}

@InProceedings{klaus_greff-proc-scipy-2017,
  author    = { {K}laus {G}reff and {A}aron {K}lein and {M}artin {C}hovanec and {F}rank {H}utter and {J}\"urgen {S}chmidhuber },
  title     = { {T}he {S}acred {I}nfrastructure for {C}omputational {R}esearch },
  booktitle = { {P}roceedings of the 16th {P}ython in {S}cience {C}onference },
  pages     = { 49 - 56 },
  year      = { 2017 },
  NOeditor    = { {K}aty {H}uff and {D}avid {L}ippa and {D}illon {N}iederhut and {M} {P}acer },
  doi       = { 10.25080/shinma-7f4c6e7-008 }
}
@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
NOeditor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@misc{huang2018densely,
      title={Densely Connected Convolutional Networks}, 
      author={Gao Huang and Zhuang Liu and Laurens van der Maaten and Kilian Q. Weinberger},
      year={2018},
      eprint={1608.06993},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European conference on computer vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}
@misc{ulyanov2017instance,
      title={Instance Normalization: The Missing Ingredient for Fast Stylization}, 
      author={Dmitry Ulyanov and Andrea Vedaldi and Victor Lempitsky},
      year={2017},
      eprint={1607.08022},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{gulrajani2017improved,
      title={Improved Training of Wasserstein GANs}, 
      author={Ishaan Gulrajani and Faruk Ahmed and Martin Arjovsky and Vincent Dumoulin and Aaron Courville},
      year={2017},
      eprint={1704.00028},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{li2018learning,
  title={Learning Deep Generative Models of Graphs},
  author={Li, Yujia and Vinyals, Oriol and Dyer, Chris and Pascanu, Razvan and Battaglia, Peter},
  year={2018}
}
@misc{bengio2013estimating,
      title={Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation}, 
      author={Yoshua Bengio and Nicholas Léonard and Aaron Courville},
      year={2013},
      eprint={1308.3432},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{doi:10.1146/annurev-statistics-010814-020120,
author = {Salakhutdinov, Ruslan},
title = {Learning Deep Generative Models},
journal = {Annual Review of Statistics and Its Application},
volume = {2},
number = {1},
pages = {361-385},
year = {2015},
doi = {10.1146/annurev-statistics-010814-020120},
}


@misc{bahdanau2014neural,
  abstract = {Neural machine translation is a recently proposed approach to machine
translation. Unlike the traditional statistical machine translation, the neural
machine translation aims at building a single neural network that can be
jointly tuned to maximize the translation performance. The models proposed
recently for neural machine translation often belong to a family of
encoder-decoders and consists of an encoder that encodes a source sentence into
a fixed-length vector from which a decoder generates a translation. In this
paper, we conjecture that the use of a fixed-length vector is a bottleneck in
improving the performance of this basic encoder-decoder architecture, and
propose to extend this by allowing a model to automatically (soft-)search for
parts of a source sentence that are relevant to predicting a target word,
without having to form these parts as a hard segment explicitly. With this new
approach, we achieve a translation performance comparable to the existing
state-of-the-art phrase-based system on the task of English-to-French
translation. Furthermore, qualitative analysis reveals that the
(soft-)alignments found by the model agree well with our intuition.},
  added-at = {2020-06-07T20:24:58.000+0200},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  bibnonoNOurl = {https://www.bibsonomy.org/bibtex/2713375898fd7d2477f6ab6dc3dd66c2c/jan.hofmann1},
  description = {[1409.0473] Neural Machine Translation by Jointly Learning to Align and Translate},
  interhash = {bb2ca011eeafccb0bd2505c9476dcd10},
  intrahash = {713375898fd7d2477f6ab6dc3dd66c2c},
  keywords = {thema:pyramid_scene_parsing},
  note = {cite arxiv:1409.0473Comment: Accepted at ICLR 2015 as oral presentation},
  timestamp = {2020-06-07T20:24:58.000+0200},
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  nonoNOurl = {http://arxiv.org/abs/1409.0473},
  year = 2014
}




@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein Generative Adversarial Networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International Conference on Machine Learning},
  pages={214--223},
  year={2017}
}

@inproceedings{NIPS2014_5423,
title = {Generative Adversarial Nets},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in Neural Information Processing Systems 27},
NOeditor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {2672--2680},
year = {2014},
publisher = {Curran Associates, Inc.},
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT press Cambridge}
}
@article{huang2016coming,
  title={The coming of age of de novo protein design},
  author={Huang, Po-Ssu and Boyken, Scott E and Baker, David},
  journal={Nature},
  volume={537},
  number={7620},
  pages={320--327},
  year={2016},
  publisher={Nature Publishing Group}
}

@inproceedings{maddison2016concrete,
  title={The concrete distribution: A continuous relaxation of discrete random variables},
  author={Maddison, C and Mnih, A and Teh, Y},
  booktitle={Proceedings of the international conference on learning Representations},
  year={2017},
  organization={International Conference on Learning Representations}
}

@article{jang2016categorical,
  title={Categorical Reparameterization with Gumbel-Softmax},
  author={Jang, Eric and Gu, Shixiang and Poole, Ben},
  year={2016}
}

@inproceedings{guo2019circuit,
  title={Circuit synthesis using generative adversarial networks (GANs)},
  author={Guo, Tinghao and Herber, Daniel and Allison, James T},
  booktitle={AIAA Scitech 2019 Forum},
  pages={2350},
  year={2019}
}

@book{foster2019generative,
  title={Generative deep learning: teaching machines to paint, write, compose, and play},
  author={Foster, David},
  year={2019},
  publisher={O'Reilly Media}
}

@online{NormalizingFlowNetwork2019,
  title = {The {{Normalizing Flow Network}}},
  nonoNOurl = {https://siboehm.com/articles/19/normalizing-flow-network},
  abstract = {The Normalizing Flow Network (NFN) is a normalizing-flow based regression model, great at modelling complex conditional densities.Look at our recent paper on...},
  NOurldate = {2019-11-19},
  date = {2019-08-08T15:06:04+02:00},
  file = {files/38840/normalizing-flow-network.html},
  note = {00000}
}

@article{anonymousLearningLikelihoodsConditional2019,
  title = {Learning {{Likelihoods}} with {{Conditional Normalizing Flows}}},
  nonoNOurl = {https://openreview.net/forum?id=rJg3zxBYwH},
  abstract = {Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an...},
  NOurldate = {2019-11-19},
  date = {2019-09-25},
  author = {Anonymous},
  file = {files/38843/Anonymous - 2019 - Learning Likelihoods with Conditional Normalizing .pdf;files/38845/forum.html},
  note = {00000}
}

@online{vinkSculptingDistributionsNormalizing,
  langid = {english},
  title = {Sculpting Distributions with {{Normalizing Flows}} - {{Ritchie Vink}}},
  nonoNOurl = {https://www.ritchievink.com/blog/2019/10/11/sculpting-distributions-with-normalizing-flows/},
  NOurldate = {2019-11-19},
  author = {Vink, Ritchie},
  file = {files/38844/sculpting-distributions-with-normalizing-flows.html},
  note = {00000}
}

@article{maehara1990dimension,
  title={On the dimension to represent a graph by a unit distance graph},
  author={Maehara, Hiroshi and R{\"o}dl, Vojtech},
  journal={Graphs and Combinatorics},
  volume={6},
  number={4},
  pages={365--367},
  year={1990},
  publisher={Springer}
}

@article{DBLP:journals/corr/abs-1903-01939,
  author = {Akiyoshi Sannai and
               Yuuki Takai and
               Matthieu Cordonnier},
  title = {Universal approximations of permutation invariant / equivariant functions by deep neural networks},
  journal = {CoRR},
  volume = {abs/1903.01939},
  year = {2019},
  nonoNOurl = {http://arxiv.org/abs/1903.01939},
}

@inproceedings{keriven2019universal,
  title={Universal invariant and equivariant graph neural networks},
  author={Keriven, Nicolas and Peyr{\'e}, Gabriel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7092--7101},
  year={2019}
}



@book{lovasz2012large,
  title={Large networks and graph limits},
  author={Lov{\'a}sz, L{\'a}szl{\'o}},
  volume={60},
  year={2012},
  publisher={American Mathematical Soc.}
}

@article{keriven2020convergence,
  title={Convergence and Stability of Graph Convolutional Networks on Large Random Graphs},
  author={Keriven, Nicolas and Bietti, Alberto and Vaiter, Samuel},
  journal={arXiv preprint arXiv:2006.01868},
  year={2020}
}

@book{penrose2003random,
  title={Random geometric graphs},
  author={Penrose, Mathew and others},
  volume={5},
  year={2003},
  publisher={Oxford university press}
}

@article{bollobas2007phase,
  title={The phase transition in inhomogeneous random graphs},
  author={Bollob{\'a}s, B{\'e}la and Janson, Svante and Riordan, Oliver},
  journal={Random Structures \& Algorithms},
  volume={31},
  number={1},
  pages={3--122},
  year={2007},
  publisher={Wiley Online Library}
}

@article{glasscock2015graphon,
  title={a Graphon?},
  author={Glasscock, Daniel},
  journal={Notices of the AMS},
  volume={62},
  number={1},
  year={2015}
}
@inproceedings{huson1995broadcast,
  title={Broadcast scheduling algorithms for radio networks},
  author={Huson, Mark L and Sen, Arunabha},
  booktitle={Proceedings of MILCOM'95},
  volume={2},
  pages={647--651},
  year={1995},
  organization={IEEE}
}

@article{alon2014two,
  title={Two notions of unit distance graphs},
  author={Alon, Noga and Kupavskii, Andrey},
  journal={Journal of Combinatorial Theory, Series A},
  volume={125},
  pages={1--17},
  year={2014},
  publisher={Elsevier}
}

@article{erdos1959r,
  title={A. Rényi,“On random graphs,”},
  author={Erd{\"o}s, Paul},
  journal={Publicationes Mathematicae},
  volume={6},
  pages={290--297},
  year={1959}
}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{dai2020scalable,
  title={Scalable deep generative modeling for sparse graphs},
  author={Dai, Hanjun and Nazi, Azade and Li, Yujia and Dai, Bo and Schuurmans, Dale},
  booktitle={International Conference on Machine Learning},
  pages={2302--2312},
  year={2020},
  organization={PMLR}
}


@misc{Rohatgi2020,
  nonoNOurl = {https://automeris.io/WebPlotDigitizer},
  author = {Rohatgi,  Ankit},
  title = {Webplotdigitizer: Version 4.3},
  year = {2020}
}


@misc{wang2020linformer,
    title   = {Linformer: Self-Attention with Linear Complexity},
    author  = {Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
    year    = {2020},
    eprint  = {2006.04768}
}
@article{shen2019efficient,
  author    = {Zhuoran Shen and
               Mingyuan Zhang and
               Haiyu Zhao and
               Shuai Yi and
               Hongsheng Li},
  title     = {Efficient Attention: Attention with Linear Complexities},
  journal   = {CoRR},
  volume    = {abs/1812.01243},
  year      = {2018},
  NOurl       = {http://arxiv.org/abs/1812.01243}
}
@inproceedings{katharopoulos-et-al-2020,
  author    = {Katharopoulos, A. and Vyas, A. and Pappas, N. and Fleuret, F.},
  title     = {Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention},
  booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
  year      = {2020},
  NOurl       = {https://arxiv.org/abs/2006.16236}
}

@inproceedings{bojchevski2018netgan,
  author    = {Aleksandar Bojchevski and
               Oleksandr Shchur and
               Daniel Z{\"{u}}gner and
               Stephan G{\"{u}}nnemann},
  title     = {NetGAN: Generating Graphs via Random Walks},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning,
               {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
               10-15, 2018},
  pages     = {609--618},
  year      = {2018},
}

@article{theis2015note,
  title={A note on the evaluation of generative models},
  author={Theis, Lucas and Oord, A{\"a}ron van den and Bethge, Matthias},
  journal={arXiv preprint arXiv:1511.01844},
  year={2015}
}

@article{liu2019graph,
  title={Graph Normalizing Flows},
  author={Liu, Jenny and Kumar, Aviral and Ba, Jimmy and Kiros, Jamie and Swersky, Kevin},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={13578--13588},
  year={2019}
}


@inproceedings{brock2018large,
  title={Large Scale GAN Training for High Fidelity Natural Image Synthesis},
  author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{coifman2005geometric,
  title={Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps},
  author={Coifman, Ronald R and Lafon, Stephane and Lee, Ann B and Maggioni, Mauro and Nadler, Boaz and Warner, Frederick and Zucker, Steven W},
  journal={Proceedings of the national academy of sciences},
  volume={102},
  number={21},
  pages={7426--7431},
  year={2005},
  publisher={National Acad Sciences}
}

@article{coifman2006diffusion,
  title={Diffusion maps},
  author={Coifman, Ronald R and Lafon, St{\'e}phane},
  journal={Applied and computational harmonic analysis},
  volume={21},
  number={1},
  pages={5--30},
  year={2006},
  publisher={Elsevier}
}

@article{bevilacqua2021equivariant,
  title={Equivariant Subgraph Aggregation Networks},
  author={Bevilacqua, Beatrice and Frasca, Fabrizio and Lim, Derek and Srinivasan, Balasubramaniam and Cai, Chen and Balamurugan, Gopinath and Bronstein, Michael M and Maron, Haggai},
  journal={arXiv preprint arXiv:2110.02910},
  year={2021}
}

@inproceedings{
papp2021dropgnn,
title={Drop{GNN}: Random Dropouts Increase the Expressiveness of Graph Neural Networks},
  author={Papp, P{\'a}l Andr{\'a}s and Martinkus, Karolis and Faber, Lukas and Wattenhofer, Roger},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={21997--22009},
NOeditor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
NOurl={https://openreview.net/forum?id=fpQojkIV5q8}
}

@inproceedings{NEURIPS2020_f81dee42,
 author = {Morris, Christopher and Rattan, Gaurav and Mutzel, Petra},
 booktitle = {Advances in Neural Information Processing Systems},
 NOeditor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {21824--21840},
 publisher = {Curran Associates, Inc.},
 title = {Weisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings},
 NOurl = {https://proceedings.neurips.cc/paper/2020/file/f81dee42585b3814de199b2e88757f5c-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{bouritsas2020improving,
  title={Improving graph neural network expressivity via subgraph isomorphism counting},
  author={Bouritsas, Giorgos and Frasca, Fabrizio and Zafeiriou, Stefanos and Bronstein, Michael M},
  journal={arXiv preprint arXiv:2006.09252},
  year={2020}
}

@article{zhang2021nested,
  title={Nested Graph Neural Networks},
  author={Zhang, Muhan and Li, Pan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{sandfelder2021ego,
  title={Ego-GNNs: Exploiting Ego Structures in Graph Neural Networks},
  author={Sandfelder, Dylan and Vijayan, Priyesh and Hamilton, William L},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8523--8527},
  year={2021},
  organization={IEEE}
}
@inproceedings{liu2021graphebm,
title={Graph{EBM}: Molecular Graph Generation with Energy-Based Models},
author={Meng Liu and Keqiang Yan and Bora Oztekin and Shuiwang Ji},
booktitle={Energy Based Models Workshop - ICLR 2021},
year={2021},
NOurl={https://openreview.net/forum?id=Gc51PtL_zYw}
}
@inproceedings{vignac2020building,
  title={Building powerful and equivariant graph neural networks with structural message-passing},
  author={Vignac, Cl{\'e}ment and Loukas, Andreas and Frossard, Pascal},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{bresson2019two,
 author = {Bresson, Xavier and Laurent, Thomas},
 booktitle = {NeurIPS Workshop on Machine Learning and the Physical Sciences},
 title = {A Two-Step Graph Convolutional Decoder for Molecule Generation},
 year = {2019}
}

@inproceedings{wang2018graphgan,
  title={Graphgan: Graph representation learning with generative adversarial nets},
  author={Wang, Hongwei and Wang, Jia and Wang, Jialin and Zhao, Miao and Zhang, Weinan and Zhang, Fuzheng and Xie, Xing and Guo, Minyi},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}
@article{garcia2021n,
  title={E (n) Equivariant Normalizing Flows},
  author={Garcia Satorras, Victor and Hoogeboom, Emiel and Fuchs, Fabian and Posner, Ingmar and Welling, Max},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{vignac2021top,
  title={Top-N: Equivariant set and graph generation without exchangeability},
  author={Vignac, Clement and Frossard, Pascal},
  journal={arXiv preprint arXiv:2110.02096},
  year={2021}
}
@article{chakrabarti2006graph,
  title={Graph mining: Laws, generators, and algorithms},
  author={Chakrabarti, Deepayan and Faloutsos, Christos},
  journal={ACM computing surveys (CSUR)},
  volume={38},
  number={1},
  pages={2--es},
  year={2006},
  publisher={ACM New York, NY, USA}
}
@article{goldenberg2010survey,
  title={A survey of statistical network models},
  author={Goldenberg, Anna and Zheng, Alice X and Fienberg, Stephen E and Airoldi, Edoardo M},
  year={2010},
  publisher={Now Publishers Inc}
}

@incollection{mohar1997some,
  title={Some applications of Laplace eigenvalues of graphs},
  author={Mohar, Bojan},
  booktitle={Graph symmetry},
  pages={225--275},
  year={1997},
  publisher={Springer}
}
@inproceedings{eldridge2016graphons,
  title={Graphons, mergeons, and so on!},
  author={Eldridge, Justin and Belkin, Mikhail and Wang, Yusu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2307--2315},
  year={2016}
}
@article{holland1983stochastic,
  title={Stochastic blockmodels: First steps},
  author={Holland, Paul W and Laskey, Kathryn Blackmond and Leinhardt, Samuel},
  journal={Social networks},
  volume={5},
  number={2},
  pages={109--137},
  year={1983},
  publisher={Elsevier}
}

@article{barabasi2013network,
  title={Network science},
  author={Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  journal={Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume={371},
  number={1987},
  pages={20120375},
  year={2013},
  publisher={The Royal Society Publishing}
}
@article{erdos1960evolution,
  title={On the evolution of random graphs},
  author={Erdos, Paul and R{\'e}nyi, Alfr{\'e}d and others},
  journal={Publ. Math. Inst. Hung. Acad. Sci},
  volume={5},
  number={1},
  pages={17--60},
  year={1960},
  publisher={Citeseer}
}

@inproceedings{hammond2013graph,
  title={Graph diffusion distance: A difference measure for weighted graphs based on the graph Laplacian exponential kernel},
  author={Hammond, David K and Gur, Yaniv and Johnson, Chris R},
  booktitle={2013 IEEE Global Conference on Signal and Information Processing},
  pages={419--422},
  year={2013},
  organization={IEEE}
}
@inproceedings{guo2019circuit,
  title={Circuit synthesis using generative adversarial networks (Gans)},
  author={Guo, Tinghao and Herber, Daniel and Allison, James T},
  booktitle={AIAA Scitech 2019 Forum},
  pages={2350},
  year={2019}
}
@article{mirhoseini2021graph,
  title={A graph placement methodology for fast chip design},
  author={Mirhoseini, Azalia and Goldie, Anna and Yazgan, Mustafa and Jiang, Joe Wenjie and Songhori, Ebrahim and Wang, Shen and Lee, Young-Joon and Johnson, Eric and Pathak, Omkar and Nazi, Azade and others},
  journal={Nature},
  volume={594},
  number={7862},
  pages={207--212},
  year={2021},
  publisher={Nature Publishing Group}
}
@article{mirhoseini2020chip,
  title={Chip placement with deep reinforcement learning},
  author={Mirhoseini, Azalia and Goldie, Anna and Yazgan, Mustafa and Jiang, Joe and Songhori, Ebrahim and Wang, Shen and Lee, Young-Joon and Johnson, Eric and Pathak, Omkar and Bae, Sungmin and others},
  journal={arXiv preprint arXiv:2004.10746},
  year={2020}
}
@inproceedings{donahue2018adversarial,
  title={Adversarial Audio Synthesis},
  author={Donahue, Chris and McAuley, Julian and Puckette, Miller},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{edelman1998geometry,
  title={The geometry of algorithms with orthogonality constraints},
  author={Edelman, Alan and Arias, Tom{\'a}s A and Smith, Steven T},
  journal={SIAM journal on Matrix Analysis and Applications},
  volume={20},
  number={2},
  pages={303--353},
  year={1998},
  publisher={SIAM}
}
@article{ramakrishnan2014quantum,
  title={Quantum chemistry structures and properties of 134 kilo molecules},
  author={Ramakrishnan, Raghunathan and Dral, Pavlo O and Rupp, Matthias and Von Lilienfeld, O Anatole},
  journal={Scientific data},
  volume={1},
  number={1},
  pages={1--7},
  year={2014},
  publisher={Nature Publishing Group}
}

@inproceedings{donnat2018learning,
  title={Learning structural node embeddings via diffusion wavelets},
  author={Donnat, Claire and Zitnik, Marinka and Hallac, David and Leskovec, Jure},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={1320--1329},
  year={2018}
}

@phdthesis{perraudin2017graph,
  title={Graph-based structures in data science: fundamental limits and applications to machine learning},
  author={Perraudin, Nathana{\"e}l},
  year={2017},
  school={Ecole Polytechnique F{\'e}d{\'e}rale de Lausanne}
}

@misc{pygsp,
  title = {PyGSP: Graph Signal Processing in Python},
  author = {Defferrard, Micha\"el and Martin, Lionel and Pena, Rodrigo and Perraudin, Nathana\"el},
  doi = {10.5281/zenodo.1003157},
  NOurl = {https://github.com/epfl-lts2/pygsp/},
}
@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}
@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={Advances in NIPS 2016 Deep Learning Symposium},
  year={2016}
}
@inproceedings{shi2020effective,
  title={Effective decoding in graph auto-encoder using triadic closure},
  author={Shi, Han and Fan, Haozheng and Kwok, James T},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={01},
  pages={906--913},
  year={2020}
}

@article{hammond2011wavelets,
  title={Wavelets on graphs via spectral graph theory},
  author={Hammond, David K and Vandergheynst, Pierre and Gribonval, R{\'e}mi},
  journal={Applied and Computational Harmonic Analysis},
  volume={30},
  number={2},
  pages={129--150},
  year={2011},
  publisher={Elsevier}
}