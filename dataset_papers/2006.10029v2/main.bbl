\begin{thebibliography}{71}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock \emph{arXiv preprint arXiv:2002.05709}, 2020{\natexlab{a}}.

\bibitem[Pham et~al.(2020)Pham, Xie, Dai, and Le]{pham2020meta}
Hieu Pham, Qizhe Xie, Zihang Dai, and Quoc~V Le.
\newblock Meta pseudo labels.
\newblock \emph{arXiv preprint arXiv:2003.10580}, 2020.

\bibitem[Hinton et~al.(2006)Hinton, Osindero, and Teh]{hinton2006fast}
Geoffrey~E Hinton, Simon Osindero, and Yee-Whye Teh.
\newblock A fast learning algorithm for deep belief nets.
\newblock \emph{Neural computation}, 18\penalty0 (7):\penalty0 1527--1554,
  2006.

\bibitem[Bengio et~al.(2007)Bengio, Lamblin, Popovici, and
  Larochelle]{bengio2007greedy}
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle.
\newblock Greedy layer-wise training of deep networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  153--160, 2007.

\bibitem[Dai and Le(2015)]{dai2015semi}
Andrew~M Dai and Quoc~V Le.
\newblock Semi-supervised sequence learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  3079--3087, 2015.

\bibitem[Kiros et~al.(2015)Kiros, Zhu, Salakhutdinov, Zemel, Urtasun, Torralba,
  and Fidler]{kiros2015skip}
Ryan Kiros, Yukun Zhu, Russ~R Salakhutdinov, Richard Zemel, Raquel Urtasun,
  Antonio Torralba, and Sanja Fidler.
\newblock Skip-thought vectors.
\newblock In \emph{Advances in neural information processing systems}, pages
  3294--3302, 2015.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Peters et~al.(2018)Peters, Neumann, Iyyer, Gardner, Clark, Lee, and
  Zettlemoyer]{peters2018deep}
Matthew~E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
  Kenton Lee, and Luke Zettlemoyer.
\newblock Deep contextualized word representations.
\newblock In \emph{Proc. of NAACL}, 2018.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Radford et~al.()Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.

\bibitem[Lee()]{lee2013pseudo}
Dong-Hyun Lee.
\newblock Pseudo-label: The simple and efficient semi-supervised learning
  method for deep neural networks.

\bibitem[Xie et~al.(2019{\natexlab{a}})Xie, Hovy, Luong, and Le]{xie2019self}
Qizhe Xie, Eduard Hovy, Minh-Thang Luong, and Quoc~V Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock \emph{arXiv preprint arXiv:1911.04252}, 2019{\natexlab{a}}.

\bibitem[Berthelot et~al.(2019)Berthelot, Carlini, Goodfellow, Papernot,
  Oliver, and Raffel]{berthelot2019mixmatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
  Oliver, and Colin~A Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5050--5060, 2019.

\bibitem[Xie et~al.(2019{\natexlab{b}})Xie, Dai, Hovy, Luong, and
  Le]{xie2019unsupervised}
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc~V Le.
\newblock Unsupervised data augmentation.
\newblock \emph{arXiv preprint arXiv:1904.12848}, 2019{\natexlab{b}}.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Li, Zhang, Carlini, Cubuk, Kurakin,
  Zhang, and Raffel]{sohn2020fixmatch}
Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini,
  Ekin~D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock \emph{arXiv preprint arXiv:2001.07685}, 2020.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Wu et~al.(2018)Wu, Xiong, Yu, and Lin]{wu2018unsupervised}
Zhirong Wu, Yuanjun Xiong, Stella~X Yu, and Dahua Lin.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3733--3742, 2018.

\bibitem[Bachman et~al.(2019)Bachman, Hjelm, and
  Buchwalter]{bachman2019learning}
Philip Bachman, R~Devon Hjelm, and William Buchwalter.
\newblock Learning representations by maximizing mutual information across
  views.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  15509--15519, 2019.

\bibitem[H{\'e}naff et~al.(2019)H{\'e}naff, Razavi, Doersch, Eslami, and
  Oord]{henaff2019data}
Olivier~J H{\'e}naff, Ali Razavi, Carl Doersch, SM~Eslami, and Aaron van~den
  Oord.
\newblock Data-efficient image recognition with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1905.09272}, 2019.

\bibitem[He et~al.(2019{\natexlab{a}})He, Fan, Wu, Xie, and
  Girshick]{he2019momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock \emph{arXiv preprint arXiv:1911.05722}, 2019{\natexlab{a}}.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Buciluǎ et~al.(2006)Buciluǎ, Caruana, and
  Niculescu-Mizil]{bucilua2006model}
Cristian Buciluǎ, Rich Caruana, and Alexandru Niculescu-Mizil.
\newblock Model compression.
\newblock In \emph{Proceedings of the 12th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 535--541, 2006.

\bibitem[Yalniz et~al.(2019)Yalniz, J{\'e}gou, Chen, Paluri, and
  Mahajan]{yalniz2019billion}
I~Zeki Yalniz, Herv{\'e} J{\'e}gou, Kan Chen, Manohar Paluri, and Dhruv
  Mahajan.
\newblock Billion-scale semi-supervised learning for image classification.
\newblock \emph{arXiv preprint arXiv:1905.00546}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Becker and Hinton(1992)]{becker1992self}
Suzanna Becker and Geoffrey~E Hinton.
\newblock Self-organizing neural network that discovers surfaces in random-dot
  stereograms.
\newblock \emph{Nature}, 355\penalty0 (6356):\penalty0 161--163, 1992.

\bibitem[Kolesnikov et~al.(2019{\natexlab{a}})Kolesnikov, Zhai, and
  Beyer]{kolesnikov2019revisiting}
Alexander Kolesnikov, Xiaohua Zhai, and Lucas Beyer.
\newblock Revisiting self-supervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 1920--1929, 2019{\natexlab{a}}.

\bibitem[Li et~al.(2019)Li, Wang, Hu, and Yang]{li2019selective}
Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang.
\newblock Selective kernel networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 510--519, 2019.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Fan, Girshick, and
  He]{chen2020improved}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock \emph{arXiv preprint arXiv:2003.04297}, 2020{\natexlab{b}}.

\bibitem[Zhai et~al.(2019)Zhai, Oliver, Kolesnikov, and Beyer]{zhai2019s4l}
Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer.
\newblock S4l: Self-supervised semi-supervised learning.
\newblock In \emph{The IEEE International Conference on Computer Vision
  (ICCV)}, October 2019.

\bibitem[Zhang et~al.(2016)Zhang, Isola, and Efros]{zhang2016colorful}
Richard Zhang, Phillip Isola, and Alexei~A Efros.
\newblock Colorful image colorization.
\newblock In \emph{European conference on computer vision}, pages 649--666.
  Springer, 2016.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017large}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}, 2017.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167}, 2015.

\bibitem[He et~al.(2019{\natexlab{b}})He, Zhang, Zhang, Zhang, Xie, and
  Li]{he2019bag}
Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu~Li.
\newblock Bag of tricks for image classification with convolutional neural
  networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 558--567, 2019{\natexlab{b}}.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2019autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 113--123, 2019.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem[Grandvalet and Bengio(2005)]{grandvalet2005semi}
Yves Grandvalet and Yoshua Bengio.
\newblock Semi-supervised learning by entropy minimization.
\newblock In \emph{Advances in neural information processing systems}, pages
  529--536, 2005.

\bibitem[Miyato et~al.(2018)Miyato, Maeda, Koyama, and
  Ishii]{miyato2018virtual}
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii.
\newblock Virtual adversarial training: a regularization method for supervised
  and semi-supervised learning.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 41\penalty0 (8):\penalty0 1979--1993, 2018.

\bibitem[Tarvainen and Valpola(2017)]{tarvainen2017mean}
Antti Tarvainen and Harri Valpola.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock In \emph{Advances in neural information processing systems}, pages
  1195--1204, 2017.

\bibitem[French et~al.(2020)French, Oliver, and Salimans]{french2020milking}
Geoff French, Avital Oliver, and Tim Salimans.
\newblock Milking cowmask for semi-supervised image classification.
\newblock \emph{arXiv preprint arXiv:2003.12022}, 2020.

\bibitem[Donahue and Simonyan(2019)]{donahue2019large}
Jeff Donahue and Karen Simonyan.
\newblock Large scale adversarial representation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10541--10551, 2019.

\bibitem[Misra and van~der Maaten(2019)]{misra2019self}
Ishan Misra and Laurens van~der Maaten.
\newblock Self-supervised learning of pretext-invariant representations.
\newblock \emph{arXiv preprint arXiv:1912.01991}, 2019.

\bibitem[Grill et~al.(2020)Grill, Strub, Altché, Tallec, Richemond,
  Buchatskaya, Doersch, Pires, Guo, Azar, Piot, Kavukcuoglu, Munos, and
  Valko]{grill2020bootstrap}
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre~H.
  Richemond, Elena Buchatskaya, Carl Doersch, Bernardo~Avila Pires,
  Zhaohan~Daniel Guo, Mohammad~Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu,
  Rémi Munos, and Michal Valko.
\newblock Bootstrap your own latent: A new approach to self-supervised
  learning, 2020.

\bibitem[Dosovitskiy et~al.(2014)Dosovitskiy, Springenberg, Riedmiller, and
  Brox]{dosovitskiy2014discriminative}
Alexey Dosovitskiy, Jost~Tobias Springenberg, Martin Riedmiller, and Thomas
  Brox.
\newblock Discriminative unsupervised feature learning with convolutional
  neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  766--774, 2014.

\bibitem[Hjelm et~al.(2018)Hjelm, Fedorov, Lavoie-Marchildon, Grewal, Bachman,
  Trischler, and Bengio]{hjelm2018learning}
R~Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil
  Bachman, Adam Trischler, and Yoshua Bengio.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock \emph{arXiv preprint arXiv:1808.06670}, 2018.

\bibitem[Tian et~al.(2019)Tian, Krishnan, and Isola]{tian2019contrastive}
Yonglong Tian, Dilip Krishnan, and Phillip Isola.
\newblock Contrastive multiview coding.
\newblock \emph{arXiv preprint arXiv:1906.05849}, 2019.

\bibitem[Li et~al.(2020)Li, Zhou, Xiong, Socher, and Hoi]{li2020prototypical}
Junnan Li, Pan Zhou, Caiming Xiong, Richard Socher, and Steven~CH Hoi.
\newblock Prototypical contrastive learning of unsupervised representations.
\newblock \emph{arXiv preprint arXiv:2005.04966}, 2020.

\bibitem[Tian et~al.(2020)Tian, Sun, Poole, Krishnan, Schmid, and
  Isola]{tian2020makes}
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and
  Phillip Isola.
\newblock What makes for good views for contrastive learning.
\newblock \emph{arXiv preprint arXiv:2005.10243}, 2020.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pages
  2672--2680, 2014.

\bibitem[Chen et~al.(2019)Chen, Zhai, Ritter, Lucic, and Houlsby]{chen2019self}
Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby.
\newblock Self-supervised gans via auxiliary rotation loss.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 12154--12163, 2019.

\bibitem[Doersch et~al.(2015)Doersch, Gupta, and
  Efros]{doersch2015unsupervised}
Carl Doersch, Abhinav Gupta, and Alexei~A Efros.
\newblock Unsupervised visual representation learning by context prediction.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1422--1430, 2015.

\bibitem[Noroozi and Favaro(2016)]{noroozi2016unsupervised}
Mehdi Noroozi and Paolo Favaro.
\newblock Unsupervised learning of visual representations by solving jigsaw
  puzzles.
\newblock In \emph{European Conference on Computer Vision}, pages 69--84.
  Springer, 2016.

\bibitem[Gidaris et~al.(2018)Gidaris, Singh, and
  Komodakis]{gidaris2018unsupervised}
Spyros Gidaris, Praveer Singh, and Nikos Komodakis.
\newblock Unsupervised representation learning by predicting image rotations.
\newblock \emph{arXiv preprint arXiv:1803.07728}, 2018.

\bibitem[Goyal et~al.(2019)Goyal, Mahajan, Gupta, and Misra]{goyal2019scaling}
Priya Goyal, Dhruv Mahajan, Abhinav Gupta, and Ishan Misra.
\newblock Scaling and benchmarking self-supervised visual representation
  learning.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 6391--6400, 2019.

\bibitem[Chapelle et~al.(2006)Chapelle, Scholkopf, and Zien]{chapelle2006semi}
Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien.
\newblock Semi-supervised learning.
\newblock \emph{MIT Press}, 2006.

\bibitem[Zhu and Goldberg(2009)]{zhu2009introduction}
Xiaojin Zhu and Andrew~B Goldberg.
\newblock Introduction to semi-supervised learning.
\newblock \emph{Synthesis lectures on artificial intelligence and machine
  learning}, 3\penalty0 (1):\penalty0 1--130, 2009.

\bibitem[Oliver et~al.(2018)Oliver, Odena, Raffel, Cubuk, and
  Goodfellow]{oliver2018realistic}
Avital Oliver, Augustus Odena, Colin~A Raffel, Ekin~Dogus Cubuk, and Ian
  Goodfellow.
\newblock Realistic evaluation of deep semi-supervised learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3235--3246, 2018.

\bibitem[Zou et~al.(2019)Zou, Yu, Liu, Kumar, and Wang]{zou2019confidence}
Yang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar, and Jinsong Wang.
\newblock Confidence regularized self-training.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 5982--5991, 2019.

\bibitem[Bachman et~al.(2014)Bachman, Alsharif, and
  Precup]{bachman2014learning}
Philip Bachman, Ouais Alsharif, and Doina Precup.
\newblock Learning with pseudo-ensembles.
\newblock In \emph{Advances in neural information processing systems}, pages
  3365--3373, 2014.

\bibitem[Laine and Aila(2016)]{laine2016temporal}
Samuli Laine and Timo Aila.
\newblock Temporal ensembling for semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1610.02242}, 2016.

\bibitem[Sajjadi et~al.(2016)Sajjadi, Javanmardi, and
  Tasdizen]{sajjadi2016regularization}
Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen.
\newblock Regularization with stochastic transformations and perturbations for
  deep semi-supervised learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  1163--1171, 2016.

\bibitem[Verma et~al.(2019)Verma, Lamb, Kannala, Bengio, and
  Lopez-Paz]{verma2019interpolation}
Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz.
\newblock Interpolation consistency training for semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1903.03825}, 2019.

\bibitem[Sun et~al.(2017)Sun, Shrivastava, Singh, and Gupta]{sun2017revisiting}
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 843--852, 2017.

\bibitem[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun,
  Kianinejad, Patwary, Ali, Yang, and Zhou]{hestness2017deep}
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,
  Hassan Kianinejad, Md~Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou.
\newblock Deep learning scaling is predictable, empirically.
\newblock \emph{arXiv preprint arXiv:1712.00409}, 2017.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Mahajan et~al.(2018)Mahajan, Girshick, Ramanathan, He, Paluri, Li,
  Bharambe, and van~der Maaten]{mahajan2018exploring}
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
  Yixuan Li, Ashwin Bharambe, and Laurens van~der Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 181--196, 2018.

\bibitem[Kolesnikov et~al.(2019{\natexlab{b}})Kolesnikov, Beyer, Zhai,
  Puigcerver, Yung, Gelly, and Houlsby]{kolesnikov2019large}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock Large scale learning of general visual representations for transfer.
\newblock \emph{arXiv preprint arXiv:1912.11370}, 2019{\natexlab{b}}.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1--9, 2015.

\end{thebibliography}
