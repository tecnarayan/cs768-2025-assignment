\begin{thebibliography}{78}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and
  Song]{ConvergenceDeepLearningAllenZhu}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages 242--252,
  Long Beach, California, USA, 09--15 Jun 2019. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v97/allen-zhu19a.html}.

\bibitem[Alquier and Ridgway(2017)]{Tempered}
P.~Alquier and J.~Ridgway.
\newblock Concentration of tempered posteriors and of their variational
  approximations.
\newblock \emph{arXiv preprint arXiv:1706.09293}, 2017.

\bibitem[Alquier et~al.(2016)Alquier, Ridgway, and
  Chopin]{alquier2016properties}
P.~Alquier, J.~Ridgway, and N.~Chopin.
\newblock On the properties of variational approximations of {G}ibbs
  posteriors.
\newblock \emph{JMLR}, 17\penalty0 (239):\penalty0 1--41, 2016.

\bibitem[Baldi and Hornik(1989)]{BaldiOptiNNPCA1989}
Pierre Baldi and Kurt Hornik.
\newblock Neural networks and principal component analysis: Learning from
  examples without local minima'', ne.
\newblock \emph{Neural Networks}, 2:\penalty0 53--58, 12 1989.
\newblock \doi{10.1016/0893-6080(89)90014-2}.

\bibitem[Barron(1993)]{Barron93NN}
Andrew Barron.
\newblock Barron, a.e.: Universal approximation bounds for superpositions of a
  sigmoidal function. ieee trans. on information theory 39, 930-945.
\newblock \emph{Information Theory, IEEE Transactions on}, 39:\penalty0 930 --
  945, 06 1993.
\newblock \doi{10.1109/18.256500}.

\bibitem[Barron(1994)]{Barron94estimation}
Andrew~R Barron.
\newblock Approximation and estimation bounds for artificial neural networks.
\newblock \emph{Machine Learning}, 14\penalty0 (1):\penalty0 115--133, 1994.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{Bartlett2017MarginBoundsNNs}
Peter~L Bartlett, Dylan~J Foster, and Matus~J Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 30}, pages 6240--6249. Curran Associates,
  Inc., 2017.
\newblock URL
  \url{http://papers.nips.cc/paper/7204-spectrally-normalized-margin-bounds-for-neural-networks.pdf}.

\bibitem[Baum and Petrie(1966)]{HiddenMarkovModel1966}
Leonard~E. Baum and Ted Petrie.
\newblock Statistical inference for probabilistic functions of finite state
  markov chains.
\newblock \emph{Ann. Math. Statist.}, 37\penalty0 (6):\penalty0 1554--1563, 12
  1966.
\newblock \doi{10.1214/aoms/1177699147}.
\newblock URL \url{https://doi.org/10.1214/aoms/1177699147}.

\bibitem[Behrens et~al.(2012)Behrens, Friel, and Hurn]{behrens2012tuning}
G.~Behrens, N.~Friel, and M.~Hurn.
\newblock Tuning tempered transitions.
\newblock \emph{Statistics and computing}, 22\penalty0 (1):\penalty0 65--78,
  2012.

\bibitem[Bellec et~al.(2018)Bellec, Kappel, Maass, and
  Legenstein]{DeepRewiring2018}
Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=BJ_wN01C-}.

\bibitem[Bengio and Delalleau(2011)]{BengioDelalleau2011}
Yoshua Bengio and Olivier Delalleau.
\newblock On the expressive power of deep architectures.
\newblock In \emph{Proceedings of the 22Nd International Conference on
  Algorithmic Learning Theory}, ALT'11, pages 18--36, Berlin, Heidelberg, 2011.
  Springer-Verlag.
\newblock ISBN 978-3-642-24411-7.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=2050345.2050349}.

\bibitem[Bhattacharya et~al.(2016)Bhattacharya, Pati, and
  Yang]{bhattacharya2016bayesian}
A.~Bhattacharya, D.~Pati, and Y.~Yang.
\newblock Bayesian fractional posteriors.
\newblock \emph{arXiv preprint arXiv:1611.01125, to appear in the Annals of
  Statistics}, 2016.

\bibitem[Bhattacharya et~al.(2018)Bhattacharya, Pati, and Yang]{Plage}
A.~Bhattacharya, D.~Pati, and Y.~Yang.
\newblock On statistical optimality of variational {Bayes}.
\newblock \emph{Proceedings of Machine Learning Research}, 84 - AISTAT, 2018.

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{Blei2017ReviewVB}
David~M Blei, Alp Kucukelbir, and Jon~D McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock \emph{Journal of the American Statistical Association}, 112\penalty0
  (518):\penalty0 859--877, 2017.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{Blundell2015}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural networks.
\newblock In \emph{Proceedings of the 32Nd International Conference on
  International Conference on Machine Learning - Volume 37}, ICML'15, pages
  1613--1622. JMLR.org, 2015.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=3045118.3045290}.

\bibitem[Boucheron et~al.(2003)Boucheron, Lugosi, and Massart]{boucheron2003}
Stéphane Boucheron, Gábor Lugosi, and Pascal Massart.
\newblock Concentration inequalities using the entropy method.
\newblock \emph{Ann. Probab.}, 31\penalty0 (3):\penalty0 1583--1614, 07 2003.
\newblock \doi{10.1214/aop/1055425791}.
\newblock URL \url{https://doi.org/10.1214/aop/1055425791}.

\bibitem[Buchholz et~al.(2018)Buchholz, Wenzel, and
  Mandt]{QuasiMonteCarloBuchholz18a}
Alexander Buchholz, Florian Wenzel, and Stephan Mandt.
\newblock Quasi-{M}onte {C}arlo variational inference.
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 668--677,
  Stockholmsmässan, Stockholm Sweden, 10--15 Jul 2018. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/buchholz18a.html}.

\bibitem[Campbell and Li(2019)]{UBVICampbell2019}
Trevor Campbell and Xinglong Li.
\newblock Universal boosting variational inference.
\newblock volume arXiv:1903.05220, 2019.

\bibitem[Castillo et~al.(2015)Castillo, Schmidt-Hieber, and van~der
  Vaart]{Castillo2015SS}
Ismaël Castillo, Johannes Schmidt-Hieber, and Aad van~der Vaart.
\newblock Bayesian linear regression with sparse priors.
\newblock \emph{Ann. Statist.}, 43\penalty0 (5):\penalty0 1986--2018, 10 2015.
\newblock \doi{10.1214/15-AOS1334}.
\newblock URL \url{https://doi.org/10.1214/15-AOS1334}.

\bibitem[Catoni(2007)]{CatoniThermo}
O.~Catoni.
\newblock \emph{{PAC}-{B}ayesian supervised classification: the thermodynamics
  of statistical learning}.
\newblock Institute of Mathematical Statistics Lecture Notes---Monograph
  Series, 56. Institute of Mathematical Statistics, Beachwood, OH, 2007.

\bibitem[Ch\'erief-Abdellatif and Alquier(2018)]{cherief2018consistency}
B.~Ch\'erief-Abdellatif and P.~Alquier.
\newblock Consistency of variational bayes inference for estimation and model
  selection in mixtures.
\newblock \emph{Electronic Journal of Statistics}, 12\penalty0 (2):\penalty0
  2995--3035, 2018.
\newblock ISSN 1935-7524.
\newblock \doi{10.1214/18-EJS1475}.

\bibitem[Ch{\'e}rief-Abdellatif et~al.(2019)Ch{\'e}rief-Abdellatif, Alquier,
  and Khan]{cheriefAlquierKhan2019}
B.-E. Ch{\'e}rief-Abdellatif, P.~Alquier, and M.E. Khan.
\newblock A generalization bound for online variational inference.
\newblock Preprint arXiv:1904.03920v1, 2019.

\bibitem[Cherief-Abdellatif(2019)]{cherief2018consistency2}
Badr-Eddine Cherief-Abdellatif.
\newblock Consistency of elbo maximization for model selection.
\newblock In Francisco Ruiz, Cheng Zhang, Dawen Liang, and Thang Bui, editors,
  \emph{Proceedings of The 1st Symposium on Advances in Approximate Bayesian
  Inference}, volume~96 of \emph{Proceedings of Machine Learning Research},
  pages 11--31. PMLR, 02 Dec 2019.
\newblock URL
  \url{http://proceedings.mlr.press/v96/cherief-abdellatif19a.html}.

\bibitem[Cybenko(1989)]{Cybenko89ShallowNN}
G.~Cybenko.
\newblock {Approximation by superpositions of a sigmoidal function}.
\newblock \emph{Mathematics of Control, Signals, and Systems (MCSS)},
  2\penalty0 (4):\penalty0 303--314, December 1989.
\newblock ISSN 0932-4194.
\newblock \doi{10.1007/BF02551274}.
\newblock URL \url{http://dx.doi.org/10.1007/BF02551274}.

\bibitem[Doucet and Johansen(2009)]{DoucetParticleFiltering}
A.~Doucet and A.~Johansen.
\newblock A tutorial on particle filtering and smoothing: Fifteen years later.
\newblock \emph{Handbook of Nonlinear Filtering}, 12, 01 2009.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{GlobalMinimaDNNDu}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  1675--1685, Long Beach, California, USA, 09--15 Jun 2019. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v97/du19c.html}.

\bibitem[Gal(2016)]{YarinGalThesis}
Yarin Gal.
\newblock \emph{Uncertainty in Deep Learning}.
\newblock PhD thesis, University of Cambridge, 2016.

\bibitem[Ghosal et~al.(2000)Ghosal, Ghosh, and van~der
  Vaart]{ghosal2000convergence}
Subhashis Ghosal, Jayanta~K. Ghosh, and Aad~W. van~der Vaart.
\newblock Convergence rates of posterior distributions.
\newblock \emph{Ann. Statist.}, 28\penalty0 (2):\penalty0 500--531, 04 2000.
\newblock \doi{10.1214/aos/1016218228}.
\newblock URL \url{https://doi.org/10.1214/aos/1016218228}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{GAN-Goodfellow}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~D. Lawrence, and K.~Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing Systems
  27}, pages 2672--2680. Curran Associates, Inc., 2014.
\newblock URL
  \url{http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{Goodfellow-et-al-2016}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock \emph{Deep Learning}.
\newblock MIT Press, 2016.
\newblock \url{http://www.deeplearningbook.org}.

\bibitem[Graves(2011)]{GravesVI2011}
Alex Graves.
\newblock Practical variational inference for neural networks.
\newblock In J.~Shawe-Taylor, R.~S. Zemel, P.~L. Bartlett, F.~Pereira, and
  K.~Q. Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems 24}, pages 2348--2356. Curran Associates, Inc., 2011.
\newblock URL
  \url{http://papers.nips.cc/paper/4329-practical-variational-inference-for-neural-networks.pdf}.

\bibitem[Grohs et~al.(2019)Grohs, Perekrestenko, Elbrächter, and
  Bölcskei]{DNNApproximationTheory2019}
Philipp Grohs, Dmytro Perekrestenko, Dennis Elbrächter, and Helmut Bölcskei.
\newblock Deep neural network approximation theory, 01 2019.

\bibitem[Gr{\"u}nwald and Van~Ommen(2017)]{grunwaldmisspecifiation}
P.~D. Gr{\"u}nwald and T.~Van~Ommen.
\newblock Inconsistency of {B}ayesian inference for misspecified linear models,
  and a proposal for repairing it.
\newblock \emph{Bayesian Analysis}, 12\penalty0 (4):\penalty0 1069--1103, 2017.

\bibitem[Guedj(2019)]{guedj2019primer}
B.~Guedj.
\newblock A primer on pac-bayesian learning.
\newblock \emph{arXiv preprint arXiv:1901.05353}, 2019.

\bibitem[Hayakawa and Suzuki(2019)]{Suzuki2019Superiority}
Satoshi Hayakawa and Taiji Suzuki.
\newblock On the minimax optimality and superiority of deep neural network
  learning over sparse parameter spaces.
\newblock \emph{arXiv preprint arXiv:1905.09195}, 2019.

\bibitem[Hinton and van Camp(1993)]{HintonVanCamp1993}
Geoffrey~E. Hinton and Drew van Camp.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{Proceedings of the Sixth Annual Conference on Computational
  Learning Theory}, COLT '93, pages 5--13, New York, NY, USA, 1993. ACM.
\newblock ISBN 0-89791-611-5.
\newblock \doi{10.1145/168304.168306}.
\newblock URL \url{http://doi.acm.org/10.1145/168304.168306}.

\bibitem[Hoffman et~al.(2013)Hoffman, Blei, Wang, and
  Paisley]{hoffman2013stochastic}
M.~D. Hoffman, D.~M. Blei, C.~Wang, and J.~Paisley.
\newblock Stochastic variational inference.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 1303--1347, 2013.

\bibitem[Huggins et~al.(2018)Huggins, Campbell, Kasprzak, and
  Broderick]{HugginsBroderick2018PracticalBounds}
Jonathan~H. Huggins, Trevor Campbell, Mikolaj Kasprzak, and Tamara Broderick.
\newblock Practical bounds on the error of bayesian posterior approximations: A
  nonasymptotic approach.
\newblock \emph{ArXiv}, abs/1809.09505, 2018.

\bibitem[Imaizumi and Fukumizu(2019)]{Imaizumi19DNN}
Masaaki Imaizumi and Kenji Fukumizu.
\newblock Deep neural networks learn non-smooth functions effectively.
\newblock In Kamalika Chaudhuri and Masashi Sugiyama, editors,
  \emph{Proceedings of Machine Learning Research}, volume~89 of
  \emph{Proceedings of Machine Learning Research}, pages 869--878. PMLR, 16--18
  Apr 2019.
\newblock URL \url{http://proceedings.mlr.press/v89/imaizumi19a.html}.

\bibitem[Jaiswal et~al.(2019{\natexlab{a}})Jaiswal, Rao, and
  Honnappa]{jaiswal2019asymptotic}
P.~Jaiswal, V.~A. Rao, and H.~Honnappa.
\newblock Asymptotic consistency of $\alpha$-r\'enyi-approximate posteriors.
\newblock Preprint arXiv:1902.01902, 2019{\natexlab{a}}.

\bibitem[Jaiswal et~al.(2019{\natexlab{b}})Jaiswal, Honnappa, and
  Rao]{Jaiswal2019RiskSensitiveVB}
Prateek Jaiswal, Harsha Honnappa, and Vinayak~A. Rao.
\newblock Risk-sensitive variational bayes: Formulations and bounds.
\newblock volume arXiv:1906.01235, 2019{\natexlab{b}}.

\bibitem[Jordan et~al.(1999)Jordan, Ghahramani, Jaakkola, and
  Saul]{VIJordan1999}
M.~I. Jordan, Z.~Ghahramani, T.~S. Jaakkola, and L.~K. Saul.
\newblock An introduction to variational methods for graphical models.
\newblock \emph{Machine Learning}, 37:\penalty0 183--233, 1999.

\bibitem[Kawaguchi(2016)]{KawaguchiNoPoorLocalMinima2016}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems 29}, pages
  586--594. Curran Associates, Inc., 2016.
\newblock URL
  \url{http://papers.nips.cc/paper/6112-deep-learning-without-poor-local-minima.pdf}.

\bibitem[Kawaguchi et~al.(2019)Kawaguchi, Huang, and
  Kaelbling]{Kawaguchi2018EffectOfDepth}
Kenji Kawaguchi, Jiaoyang Huang, and Leslie~Pack Kaelbling.
\newblock Effect of depth and width on local minima in deep learning.
\newblock \emph{Neural Computation}, 31\penalty0 (6):\penalty0 1462--1498,
  2019.

\bibitem[Khan et~al.(2018)Khan, Nielsen, Tangkaratt, Lin, Gal, and
  Srivastava]{KhanBayesianDeepLearning2018}
Mohammad Khan, Didrik Nielsen, Voot Tangkaratt, Wu~Lin, Yarin Gal, and Akash
  Srivastava.
\newblock Fast and scalable {B}ayesian deep learning by weight-perturbation in
  {A}dam.
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 2611--2620,
  Stockholmsmässan, Stockholm Sweden, 10--15 Jul 2018. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v80/khan18a.html}.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Lecun et~al.(1998)Lecun, Bottou, Bengio, and
  Haffner]{CNNreference98lecun}
Yann Lecun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock In \emph{Proceedings of the IEEE}, pages 2278--2324, 1998.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{deeplearningbooklecun}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 5 2015.
\newblock ISSN 0028-0836.
\newblock \doi{10.1038/nature14539}.

\bibitem[Louizos et~al.(2018)Louizos, Welling, and Kingma]{Louizos2018SparseDL}
Christos Louizos, Max Welling, and Diederik~P. Kingma.
\newblock Learning sparse neural networks through $l_0$-regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=H1Y8hhg0b}.

\bibitem[MacKay(1992{\natexlab{a}})]{McKay1992BNN}
David J.~C. MacKay.
\newblock A practical bayesian framework for backpropagation networks.
\newblock \emph{Neural Computation}, 4\penalty0 (3):\penalty0 448--472,
  1992{\natexlab{a}}.
\newblock \doi{10.1162/neco.1992.4.3.448}.
\newblock URL \url{https://doi.org/10.1162/neco.1992.4.3.448}.

\bibitem[MacKay(1992{\natexlab{b}})]{McKayPhD}
David J.~C. MacKay.
\newblock \emph{Bayesian methods for adaptive models}.
\newblock PhD thesis, California Institute of Technology, 1992{\natexlab{b}}.

\bibitem[Minka(2001)]{MinkaEP}
T.~P. Minka.
\newblock Expectation propagation for approximate bayesian inference.
\newblock In \emph{Proceedings of the 17th Conference in Uncertainty in
  Artificial Intelligence}, UAI '01, pages 362--369, San Francisco, CA, USA,
  2001. Morgan Kaufmann Publishers Inc.
\newblock ISBN 1-55860-800-1.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=647235.720257}.

\bibitem[Mishkin et~al.(2018)Mishkin, Kunstner, Nielsen, Schmidt, and
  Khan]{SlangKhan2018}
Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark Schmidt, and
  Mohammad~Emtiyaz Khan.
\newblock Slang: Fast structured covariance approximations for bayesian deep
  learning with natural gradient.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 6245--6255. Curran Associates, Inc., 2018.

\bibitem[Neal(1995)]{NealPhD}
Radford.~M. Neal.
\newblock \emph{Bayesian learning for neural networks}.
\newblock PhD thesis, University of Toronto, 1995.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, and
  Srebro]{Srebro2018PACMarginBoundsNNs}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
\newblock A {PAC}-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=Skz_WfbCZ}.

\bibitem[Nguyen et~al.(2019)Nguyen, Mukkamala, and
  Hein]{Nguyen2019LandscapeDNN}
Quynh Nguyen, Mahesh~Chandra Mukkamala, and Matthias Hein.
\newblock On the loss landscape of a class of deep neural networks with no bad
  local valleys.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HJgXsjA5tQ}.

\bibitem[Opper and Archambeau(2008)]{OpperArchambeau2009}
Manfred Opper and Cedric Archambeau.
\newblock The variational gaussian approximation revisited.
\newblock \emph{Neural computation}, 21:\penalty0 786--92, 10 2008.
\newblock \doi{10.1162/neco.2008.08-07-592}.

\bibitem[Osawa et~al.(2019)Osawa, Swaroop, Jain, Eschenhagen, Turner, Yokota,
  and Khan]{KhanBayesianDeepLearning2019}
Kazuki Osawa, Siddharth Swaroop, Anirudh Jain, Runa Eschenhagen, Richard~E.
  Turner, Rio Yokota, and Mohammad~Emtiyaz Khan.
\newblock Practical deep learning with bayesian principles, 2019.
\newblock URL \url{http://arxiv.org/abs/1906.02506}.
\newblock cite arxiv:1906.02506Comment: Under review.

\bibitem[Petersen and Voigtländer(2017)]{Petersen2018Approximation}
Philipp Petersen and Felix Voigtländer.
\newblock Optimal approximation of piecewise smooth functions using deep relu
  neural networks.
\newblock \emph{Neural Networks}, 09 2017.
\newblock \doi{10.1016/j.neunet.2018.08.019}.

\bibitem[Rockova and Polson(2018)]{Rockova2018}
Veronika Rockova and nicholas Polson.
\newblock Posterior concentration for sparse deep learning.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 930--941. Curran Associates, Inc., 2018.

\bibitem[Rolnick and Tegmark(2018)]{RolnickTegmark2018PowerOfDepth}
David Rolnick and Max Tegmark.
\newblock The power of deeper networks for expressing natural functions.
\newblock In \emph{6th International Conference on Learning Representations,
  {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
  Proceedings}, 2018.
\newblock URL \url{https://openreview.net/forum?id=SyProzZAW}.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{RumelhartRNN1986}
David~E. Rumelhart, Geoffrey~E. Hinton, and Ronald~J. Williams.
\newblock {Learning Representations by Back-propagating Errors}.
\newblock \emph{Nature}, 323\penalty0 (6088):\penalty0 533--536, 1986.
\newblock \doi{10.1038/323533a0}.
\newblock URL \url{http://www.nature.com/articles/323533a0}.

\bibitem[Schmidt-Hieber(2017)]{SchmidtHieberDNN}
Johannes Schmidt-Hieber.
\newblock Nonparametric regression using deep neural networks with relu
  activation function.
\newblock \emph{ArXiv}, arxiv:1708.06633, 2017.

\bibitem[Sheth and Khardon(2017)]{RoniKhardonExcessRiskBounds2017}
Rishit Sheth and Roni Khardon.
\newblock Excess risk bounds for the bayes risk using variational inference in
  latent gaussian models.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 30}, pages 5151--5161. Curran Associates,
  Inc., 2017.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, van~den
  Driessche, Graepel, and Hassabis]{alphago}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van~den
  Driessche, Thore Graepel, and Demis Hassabis.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 550:\penalty0 354--, October 2017.
\newblock URL \url{http://dx.doi.org/10.1038/nature24270}.

\bibitem[Soudry and Carmon(2016)]{SoudryLocalMinima2016}
Daniel Soudry and Yair Carmon.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock 05 2016.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{DropoutSrivastava2014}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 1929--1958,
  2014.
\newblock URL \url{http://jmlr.org/papers/v15/srivastava14a.html}.

\bibitem[Stanford et~al.(2000)Stanford, Giardina, Gerhardt, Fukumizu, and
  Amari]{Amari2000PlateausDNN}
J.A. Stanford, K~Giardina, G.A. Gerhardt, Kenji Fukumizu, and Shun-ichi Amari.
\newblock Local minima and plateaus in hierarchical structures of multilayer
  perceptrons.
\newblock \emph{Neural Networks}, 13, 05 2000.
\newblock \doi{10.1016/S0893-6080(00)00009-5}.

\bibitem[Suzuki(2018)]{Suzuki18DNNkerels}
Taiji Suzuki.
\newblock Fast generalization error bound of deep learning from a kernel
  perspective.
\newblock In Amos Storkey and Fernando Perez-Cruz, editors, \emph{Proceedings
  of the Twenty-First International Conference on Artificial Intelligence and
  Statistics}, volume~84 of \emph{Proceedings of Machine Learning Research},
  pages 1397--1406, Playa Blanca, Lanzarote, Canary Islands, 09--11 Apr 2018.
  PMLR.
\newblock URL \url{http://proceedings.mlr.press/v84/suzuki18a.html}.

\bibitem[Suzuki(2019)]{suzuki2019adaptivity}
Taiji Suzuki.
\newblock Adaptivity of deep re{LU} network for learning in besov and mixed
  smooth besov spaces: optimal rate and curse of dimensionality.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=H1ebTsActm}.

\bibitem[Titsias and L\'{a}zaro-Gredilla(2011)]{SpikeAndSlabVITitsias2011}
Michalis~K. Titsias and Miguel L\'{a}zaro-Gredilla.
\newblock Spike and slab variational inference for multi-task and multiple
  kernel learning.
\newblock In J.~Shawe-Taylor, R.~S. Zemel, P.~L. Bartlett, F.~Pereira, and
  K.~Q. Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems 24}, pages 2339--2347. Curran Associates, Inc., 2011.

\bibitem[Tonolini et~al.(2019)Tonolini, Jensen, and
  Murray-Smith]{VariationalSparseCoding2019}
Francesco Tonolini, Bjorn~Sand Jensen, and Roderick Murray-Smith.
\newblock Variational sparse coding, 2019.
\newblock URL \url{https://openreview.net/forum?id=SkeJ6iR9Km}.

\bibitem[Tsybakov(2008)]{tsybakov2008}
Alexandre~B. Tsybakov.
\newblock \emph{Introduction to Nonparametric Estimation}.
\newblock Springer Publishing Company, Incorporated, 1st edition, 2008.
\newblock ISBN 0387790519, 9780387790510.

\bibitem[Vladimirova et~al.(2019)Vladimirova, Verbeek, Mesejo, and
  Arbel]{Vladimirova2019PriorsBNNsUnits}
Mariia Vladimirova, Jakob Verbeek, Pablo Mesejo, and Julyan Arbel.
\newblock Understanding priors in {B}ayesian neural networks at the unit level.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  6458--6467, Long Beach, California, USA, 09--15 Jun 2019. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v97/vladimirova19a.html}.

\bibitem[Wang and Blei(2018)]{wang2018frequentist}
Y.~Wang and D.~M. Blei.
\newblock Frequentist consistency of variational {B}ayes.
\newblock Journal of the American Statistical Association (to appear), 2018.

\bibitem[Yarotsky(2016)]{Yarotsky2017}
Dmitry Yarotsky.
\newblock Error bounds for approximations with deep relu networks.
\newblock \emph{Neural Networks}, 94, 10 2016.
\newblock \doi{10.1016/j.neunet.2017.07.002}.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{ZhangUnderstandingDL2017}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock 2017.
\newblock URL \url{https://arxiv.org/abs/1611.03530}.

\bibitem[Zhang and Gao(2017)]{Chicago}
F.~Zhang and C.~Gao.
\newblock Convergence rates of variational posterior distributions.
\newblock \emph{arXiv preprint arXiv:1712.02519v1}, 2017.

\end{thebibliography}
