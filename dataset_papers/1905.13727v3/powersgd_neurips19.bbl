\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017quantized}
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M.
\newblock {QSGD}: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Arbenz(2016)]{arbenz2012lecture}
Arbenz, P.
\newblock Lecture notes on solving large scale eigenvalue problems.
\newblock \emph{D-MATH, ETH Z{\"u}rich}, 2, 2016.

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{arora2018stronger}
Arora, S., Ge, R., Neyshabur, B., and Zhang, Y.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Awan et~al.(2018)Awan, Chu, Subramoni, and Panda]{awan2018optimized}
Awan, A.~A., Chu, C.-H., Subramoni, H., and Panda, D.~K.
\newblock Optimized broadcast for deep learning workloads on dense-{GPU}
  infiniband clusters: {MPI} or {NCCL}?
\newblock In \emph{European MPI Users' Group Meeting (EuroMPI)}, 2018.

\bibitem[Baevski \& Auli(2019)Baevski and Auli]{baevski2019adaptive}
Baevski, A. and Auli, M.
\newblock Adaptive input representations for neural language modeling.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and
  Anandkumar]{bernstein2018signsgd}
Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anandkumar, A.
\newblock {signSGD}: compressed optimisation for non-convex problems.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Bernstein et~al.(2019)Bernstein, Zhao, Azizzadenesheli, and
  Anandkumar]{bernstein2019iclr}
Bernstein, J., Zhao, J., Azizzadenesheli, K., and Anandkumar, A.
\newblock {signSGD} with majority vote is communication efficient and fault
  tolerant.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Caldas et~al.(2018)Caldas, Konecn{\'{y}}, McMahan, and
  Talwalkar]{calas2018expanding}
Caldas, S., Konecn{\'{y}}, J., McMahan, H.~B., and Talwalkar, A.
\newblock Expanding the reach of federated learning by reducing client resource
  requirements.
\newblock \emph{arXiv}, abs/1812.07210, 2018.

\bibitem[Carlson et~al.(2015)Carlson, Cevher, and Carin]{Carlson:2015to}
Carlson, D., Cevher, V., and Carin, L.
\newblock {Stochastic Spectral Descent for Restricted {Boltzmann} Machines}.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2015.

\bibitem[Collins et~al.(2018)Collins, Bigdeli, and
  S{\"{u}}sstrunk]{collins2018memorization}
Collins, E., Bigdeli, S.~A., and S{\"{u}}sstrunk, S.
\newblock Detecting memorization in {ReLU} networks.
\newblock \emph{arXiv}, abs/1810.03372, 2018.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Senior,
  Tucker, Yang, Le, et~al.]{dean2012large}
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Senior, A.,
  Tucker, P., Yang, K., Le, Q.~V., et~al.
\newblock Large scale distributed deep networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2012.

\bibitem[Ghadimi \& Lan(2016)Ghadimi and Lan]{ghadimi2016accelerated}
Ghadimi, S. and Lan, G.
\newblock Accelerated gradient methods for nonconvex nonlinear and stochastic
  programming.
\newblock \emph{Mathematical Programming}, 156\penalty0 (1-2):\penalty0 59--99,
  2016.

\bibitem[Goyal et~al.(2017)Goyal, Dollar, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Goyal, P., Dollar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A.,
  Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch {SGD}: training imagenet in 1 hour.
\newblock \emph{arXiv}, abs/1706.02677, 2017.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Gunasekar, S., Lee, J., Soudry, D., and Srebro, N.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Iandola et~al.(2015)Iandola, Ashraf, Moskewicz, and
  Keutzer]{iandola2015firecaffe}
Iandola, F.~N., Ashraf, K., Moskewicz, M.~W., and Keutzer, K.
\newblock {FireCaffe}: near-linear acceleration of deep neural network training
  on compute clusters. corr abs/1511.00175 (2015), 2015.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Rebjock, Stich, and
  Jaggi]{karimireddy2019error}
Karimireddy, S.~P., Rebjock, Q., Stich, S.~U., and Jaggi, M.
\newblock Error feedback fixes {SignSGD} and other gradient compression
  schemes.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Yu,
  Richt{\'a}rik, Suresh, and Bacon]{konevcny2016bfederated}
Kone{\v{c}}n{\`y}, J., McMahan, H.~B., Yu, F.~X., Richt{\'a}rik, P., Suresh,
  A.~T., and Bacon, D.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock \emph{arXiv}, abs/1610.05492, 2016.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{li2017algorithmic}
Li, Y., Ma, T., and Zhang, H.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2018.

\bibitem[Lin et~al.(2018)Lin, Han, Mao, Wang, and Dally]{lin2017deep}
Lin, Y., Han, S., Mao, H., Wang, Y., and Dally, W.~J.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Martin \& Mahoney(2018)Martin and Mahoney]{martin2018implicit}
Martin, C.~H. and Mahoney, M.~W.
\newblock Implicit self-regularization in deep neural networks: Evidence from
  random matrix theory and implications for learning.
\newblock \emph{arXiv}, abs/1810.01075, 2018.

\bibitem[Mazumder et~al.(2010)Mazumder, Hastie, and
  Tibshirani]{mazumder2010spectral}
Mazumder, R., Hastie, T., and Tibshirani, R.
\newblock Spectral regularization algorithms for learning large incomplete
  matrices.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Aug):\penalty0 2287--2322, 2010.

\bibitem[Oja(1982)]{oja1982simplified}
Oja, E.
\newblock Simplified neuron model as a principal component analyzer.
\newblock \emph{Journal of Mathematical Biology}, 15\penalty0 (3):\penalty0
  267--273, 1982.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott2019fairseq}
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and
  Auli, M.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{Proceedings of NAACL-HLT 2019: Demonstrations}, 2019.

\bibitem[Panda et~al.(2019)Panda, Subramoni, and Awan]{panda2019high}
Panda, D. K.~D., Subramoni, H., and Awan, A.~A.
\newblock High performance distributed deep learning: A beginnerâ€™s guide.
\newblock In \emph{Symposium on Principles and Practice of Parallel Programming
  (PPoPP)}, 2019.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951stochastic}
Robbins, H. and Monro, S.
\newblock {A Stochastic Approximation Method}.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400--407, September 1951.

\bibitem[Seide et~al.(2014)Seide, Fu, Droppo, Li, and Yu]{seide20141}
Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In \emph{Annual Conference of the International Speech Communication
  Association (INTERSPEECH)}, 2014.

\bibitem[Shallue et~al.(2018)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2018measuring}
Shallue, C.~J., Lee, J., Antognini, J., Sohl-Dickstein, J., Frostig, R., and
  Dahl, G.~E.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{arXiv}, abs/1811.03600, 2018.

\bibitem[Stewart(1976)]{stewart1976simultaneous}
Stewart, G.
\newblock Simultaneous iteration for computing invariant subspaces of
  non-{Hermitian} matrices.
\newblock \emph{Numerische Mathematik}, 25\penalty0 (2):\penalty0 123--136,
  1976.

\bibitem[Stewart \& Miller(1975)Stewart and Miller]{stewart1975methods}
Stewart, G. and Miller, J.
\newblock Methods of simultaneous iteration for calculating eigenvectors of
  matrices.
\newblock \emph{Topics in Numerical Analysis II}, pp.\  169--185, 1975.

\bibitem[Stich \& Karimireddy(2019)Stich and
  Karimireddy]{stich2019errorfeedback}
Stich, S.~U. and Karimireddy, S.~P.
\newblock The error-feedback framework: Better rates for sgd with delayed
  gradients and compressed communication.
\newblock \emph{arXiv}, abs/1909.05350, 2019.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{stich2018sparsified}
Stich, S.~U., Cordonnier, J.-B., and Jaggi, M.
\newblock Sparsified {SGD} with memory.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Wang et~al.(2018)Wang, Sievert, Liu, Charles, Papailiopoulos, and
  Wright]{wang2018atomo}
Wang, H., Sievert, S., Liu, S., Charles, Z., Papailiopoulos, D., and Wright, S.
\newblock {ATOMO}: Communication-efficient learning via atomic sparsification.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Wangni et~al.(2018)Wangni, Wang, Liu, and Zhang]{wangni2018gradient}
Wangni, J., Wang, J., Liu, J., and Zhang, T.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Wen et~al.(2017)Wen, Xu, Yan, Wu, Wang, Chen, and Li]{wen2017terngrad}
Wen, W., Xu, C., Yan, F., Wu, C., Wang, Y., Chen, Y., and Li, H.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  1509--1519, 2017.

\bibitem[Yoshida \& Miyato(2017)Yoshida and Miyato]{yoshida2017spectral}
Yoshida, Y. and Miyato, T.
\newblock Spectral norm regularization for improving the generalizability of
  deep learning.
\newblock \emph{arXiv}, abs/1705.10941, 2017.

\bibitem[Yu et~al.(2018)Yu, Lin, Narra, Li, Li, Kim, Schwing, Annavaram, and
  Avestimehr]{yu2018gradiveq}
Yu, M., Lin, Z., Narra, K., Li, S., Li, Y., Kim, N.~S., Schwing, A.~G.,
  Annavaram, M., and Avestimehr, S.
\newblock Gradiveq: Vector quantization for bandwidth-efficient gradient
  aggregation in distributed {CNN} training.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Yurtsever et~al.(2017)Yurtsever, Udell, Tropp, and
  Cevher]{yurtsever2017sketchy}
Yurtsever, A., Udell, M., Tropp, J.~A., and Cevher, V.
\newblock Sketchy decisions: Convex low-rank matrix optimization with optimal
  storage.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2017.

\bibitem[Zhao(2019)]{github2019signsgd}
Zhao, J.
\newblock {signSGD} with majority vote.
\newblock \url{github.com/PermiJW/signSGD-with-Majority-Vote}, 2019.
\newblock [Online; accessed 12-May-2019].

\end{thebibliography}
