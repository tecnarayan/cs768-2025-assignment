\begin{thebibliography}{10}

\bibitem{McMahan_aistats17}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
  Blaise~Ag{\"{u}}era y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em {AISTATS}}, Proceedings of Machine Learning Research, pages
  1273--1282. {PMLR}, 2017.

\bibitem{Gboard}
Kallista~A. Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
  Ingerman, Vladimir Ivanov, Chlo{\'{e}} Kiddon, Jakub Kone{\v{c}}n{\'y},
  Stefano Mazzocchi, Brendan McMahan, Timon~Van Overveldt, David Petrou, Daniel
  Ramage, and Jason Roselander.
\newblock Towards federated learning at scale: System design.
\newblock In {\em MLSys}. mlsys.org, 2019.

\bibitem{li_osdi14}
Mu~Li, David~G. Andersen, Jun~Woo Park, Alexander~J. Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J. Shekita, and Bor{-}Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In {\em {OSDI}}, pages 583--598. {USENIX} Association, 2014.

\bibitem{lian_icml18}
Xiangru Lian, Wei Zhang, Ce~Zhang, and Ji~Liu.
\newblock Asynchronous decentralized parallel stochastic gradient descent.
\newblock In {\em {ICML}}, Proceedings of Machine Learning Research, pages
  3049--3058. {PMLR}, 2018.

\bibitem{tang_icml18}
Hanlin Tang, Xiangru Lian, Ming Yan, Ce~Zhang, and Ji~Liu.
\newblock D\({}^{\mbox{2}}\): Decentralized training over decentralized data.
\newblock In {\em {ICML}}, Proceedings of Machine Learning Research, pages
  4855--4863. {PMLR}, 2018.

\bibitem{Yu_icml19}
Hao Yu and Rong Jin.
\newblock On the computation and communication complexity of parallel {SGD}
  with dynamic batch sizes for stochastic non-convex optimization.
\newblock In {\em {ICML}}, Proceedings of Machine Learning Research, pages
  7174--7183. {PMLR}, 2019.

\bibitem{khaled_corr19}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'{a}}rik.
\newblock First analysis of local {GD} on heterogeneous data.
\newblock {\em CoRR}, abs/1909.04715, 2019.

\bibitem{Stich_corr19}
Sebastian~U. Stich and Sai~Praneeth Karimireddy.
\newblock The error-feedback framework: Better rates for {SGD} with delayed
  gradients and compressed communication.
\newblock {\em CoRR}, abs/1909.05350, 2019.

\bibitem{Stich_iclr19}
Sebastian~U. Stich.
\newblock Local {SGD} converges fast and communicates little.
\newblock In {\em {ICLR}}. OpenReview.net, 2019.

\bibitem{wang_corr18}
Jianyu Wang and Gauri Joshi.
\newblock Cooperative {SGD:} {A} unified framework for the design and analysis
  of local-update {SGD} algorithms.
\newblock {\em J. Mach. Learn. Res.}, 22:213:1--213:50, 2021.

\bibitem{yu_aaai19}
Hao Yu, Sen Yang, and Shenghuo Zhu.
\newblock Parallel restarted {SGD} with faster convergence and less
  communication: Demystifying why model averaging works for deep learning.
\newblock In {\em {AAAI}}, pages 5693--5700. {AAAI} Press, 2019.

\bibitem{cho_corr20}
Yae~Jee Cho, Jianyu Wang, and Gauri Joshi.
\newblock Client selection in federated learning: Convergence analysis and
  power-of-choice selection strategies.
\newblock {\em CoRR}, abs/2010.01243, 2020.

\bibitem{Li_iclr20}
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock In {\em {ICLR}}. OpenReview.net, 2020.

\bibitem{Karimireddy_icml20}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank~J. Reddi,
  Sebastian~U. Stich, and Ananda~Theertha Suresh.
\newblock {SCAFFOLD:} stochastic controlled averaging for federated learning.
\newblock In {\em {ICML}}, Proceedings of Machine Learning Research, pages
  5132--5143. {PMLR}, 2020.

\bibitem{avdi_icml21}
Dmitrii Avdiukhin and Shiva~Prasad Kasiviswanathan.
\newblock Federated learning under arbitrary communication patterns.
\newblock In {\em {ICML}}, Proceedings of Machine Learning Research, pages
  425--435. {PMLR}, 2021.

\bibitem{frab_icml21}
Yann Fraboni, Richard Vidal, Laetitia Kameni, and Marco Lorenzi.
\newblock Clustered sampling: Low-variance and improved representativity for
  clients selection in federated learning.
\newblock In {\em {ICML}}, Proceedings of Machine Learning Research, pages
  3407--3416. {PMLR}, 2021.

\bibitem{li_mlsys20}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock In {\em MLSys}. mlsys.org, 2020.

\bibitem{scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank~J. Reddi,
  Sebastian~U. Stich, and Ananda~Theertha Suresh.
\newblock {SCAFFOLD:} stochastic controlled averaging for federated learning.
\newblock In {\em {ICML}}, Proceedings of Machine Learning Research, pages
  5132--5143. {PMLR}, 2020.

\bibitem{niu_mobicom20}
Chaoyue Niu, Fan Wu, Shaojie Tang, Lifeng Hua, Rongfei Jia, Chengfei Lv, Zhihua
  Wu, and Guihai Chen.
\newblock Billion-scale federated learning on mobile clients: a submodel design
  with tunable privacy.
\newblock In {\em MobiCom}, pages 31:1--31:14. {ACM}, 2020.

\bibitem{preconditioner}
Yann~N. Dauphin, Harm de~Vries, and Yoshua Bengio.
\newblock Equilibrated adaptive learning rates for non-convex optimization.
\newblock In {\em {NeurIPS}}, pages 1504--1512, 2015.

\bibitem{yu_icml19_FL}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  {SGD} for distributed non-convex optimization.
\newblock In {\em {ICML}}, Proceedings of Machine Learning Research, pages
  7184--7193. {PMLR}, 2019.

\bibitem{movielens}
F.~Maxwell Harper and Joseph~A. Konstan.
\newblock The movielens datasets: History and context.
\newblock {\em {ACM} Trans. Interact. Intell. Syst.}, 5(4):19:1--19:19, 2016.

\bibitem{sent140}
Tapan Sahni, Chinmay Chandak, Naveen~Reddy Chedeti, and Manish Singh.
\newblock Efficient twitter sentiment classification using subjective distant
  supervision.
\newblock In {\em {COMSNETS}}, pages 548--553. {IEEE}, 2017.

\bibitem{din}
Guorui Zhou, Xiaoqiang Zhu, Chengru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui
  Yan, Junqi Jin, Han Li, and Kun Gai.
\newblock Deep interest network for click-through rate prediction.
\newblock In {\em {KDD}}, pages 1059--1068. {ACM}, 2018.

\bibitem{afo}
Sashank~J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Kone{\v{c}}n{\`y}, Sanjiv Kumar, and Hugh~Brendan McMahan.
\newblock Adaptive federated optimization.
\newblock In {\em {ICLR}}. OpenReview.net, 2020.

\bibitem{warner_1965}
Stanley~L Warner.
\newblock Randomized response: A survey technique for eliminating evasive
  answer bias.
\newblock {\em J. Am. Stat. Assoc.}, 60(309):63--69, 1965.

\bibitem{secure_aggregation}
Kallista~A. Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone,
  H.~Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.
\newblock Practical secure aggregation for privacy-preserving machine learning.
\newblock In {\em {CCS}}, pages 1175--1191. {ACM}, 2017.

\end{thebibliography}
