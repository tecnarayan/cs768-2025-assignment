\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Angelopoulos et~al.(2021)Angelopoulos, Bates, Cand{\`e}s, Jordan, and
  Lei]{angelopoulos2021learn}
Anastasios~N. Angelopoulos, Stephen Bates, Emmanuel~J. Cand{\`e}s, Michael~I.
  Jordan, and Lihua Lei.
\newblock Learn then test: Calibrating predictive algorithms to achieve risk
  control.
\newblock \emph{arXiv preprint}, 2021.
\newblock arXiv:2110.01052.

\bibitem[Bates et~al.(2021)Bates, Angelopoulos, Lei, Malik, and
  Jordan]{bates2021distribution}
Stephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and
  Michael~I. Jordan.
\newblock Distribution-free, risk-controlling prediction sets.
\newblock \emph{Journal of the ACM}, 68\penalty0 (6), September 2021.
\newblock \doi{10.1145/3478535}.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{ben2010theory}
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
  Jennifer~Wortman Vaughan.
\newblock A theory of learning from different domains.
\newblock \emph{Machine learning}, 79\penalty0 (1):\penalty0 151--175, 2010.

\bibitem[Blitzer et~al.(2007)Blitzer, Crammer, Kulesza, Pereira, and
  Wortman]{blitzer2007learning}
John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer
  Wortman.
\newblock Learning bounds for domain adaptation.
\newblock \emph{Advances in neural information processing systems}, 20, 2007.

\bibitem[Cauchois et~al.(2020)Cauchois, Gupta, Ali, and
  Duchi]{cauchois2020robust}
Maxime Cauchois, Suyash Gupta, Alnur Ali, and John~C Duchi.
\newblock Robust validation: Confident predictions even when distributions
  shift.
\newblock \emph{arXiv preprint arXiv:2008.04267}, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Gal and Ghahramani(2016)]{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{international conference on machine learning}, pages
  1050--1059. PMLR, 2016.

\bibitem[Gal et~al.(2017)Gal, Hron, and Kendall]{gal2017concrete}
Yarin Gal, Jiri Hron, and Alex Kendall.
\newblock Concrete dropout.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Guillory et~al.(2021)Guillory, Shankar, Ebrahimi, Darrell, and
  Schmidt]{guillory2021predicting}
Devin Guillory, Vaishaal Shankar, Sayna Ebrahimi, Trevor Darrell, and Ludwig
  Schmidt.
\newblock Predicting with confidence on unseen distributions.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1134--1144, 2021.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1321--1330. PMLR, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{arXiv preprint arXiv:1903.12261}, 2019.

\bibitem[Hendrycks and Gimpel(2016)]{hendrycks2016baseline}
Dan Hendrycks and Kevin Gimpel.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock \emph{arXiv preprint arXiv:1610.02136}, 2016.

\bibitem[Hendrycks et~al.(2019{\natexlab{a}})Hendrycks, Mazeika, Kadavath, and
  Song]{hendrycks2019using}
Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song.
\newblock Using self-supervised learning can improve model robustness and
  uncertainty.
\newblock \emph{Advances in Neural Information Processing Systems}, 32,
  2019{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2019{\natexlab{b}})Hendrycks, Mu, Cubuk, Zoph,
  Gilmer, and Lakshminarayanan]{hendrycks2019augmix}
Dan Hendrycks, Norman Mu, Ekin~D Cubuk, Barret Zoph, Justin Gilmer, and Balaji
  Lakshminarayanan.
\newblock Augmix: A simple data processing method to improve robustness and
  uncertainty.
\newblock \emph{arXiv preprint arXiv:1912.02781}, 2019{\natexlab{b}}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo,
  Desai, Zhu, Parajuli, Guo, Song, Steinhardt, and Gilmer]{hendrycks2021many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob
  Steinhardt, and Justin Gilmer.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock \emph{ICCV}, 2021.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Gao, et~al.]{koh2021wilds}
Pang~Wei Koh, Shiori Sagawa, Henrik Marklund, Sang~Michael Xie, Marvin Zhang,
  Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard~Lanas Phillips,
  Irena Gao, et~al.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning}, pages
  5637--5664. PMLR, 2021.

\bibitem[Kolesnikov et~al.(2020)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{kolesnikov2020big}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock Big transfer (bit): General visual representation learning.
\newblock In \emph{European conference on computer vision}, pages 491--507.
  Springer, 2020.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{lakshminarayanan2017simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Lee et~al.(2022)Lee, Huang, Hassani, and Dobriban]{lee2022tcal}
Donghwan Lee, Xinmeng Huang, Hamed Hassani, and Edgar Dobriban.
\newblock T-cal: An optimal test for the calibration of predictive models.
\newblock \emph{arXiv preprint arXiv:2203.01850}, 2022.

\bibitem[Minderer et~al.(2021)Minderer, Djolonga, Romijnders, Hubis, Zhai,
  Houlsby, Tran, and Lucic]{minderer2021revisiting}
Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai,
  Neil Houlsby, Dustin Tran, and Mario Lucic.
\newblock Revisiting the calibration of modern neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and
  Talwalkar]{mohri2018foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock \emph{Foundations of machine learning}.
\newblock 2018.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and
  Hauskrecht]{naeini2015obtaining}
Mahdi~Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht.
\newblock Obtaining well calibrated probabilities using bayesian binning.
\newblock In \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019can}
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian
  Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Park et~al.(2020)Park, Bastani, Weimer, and Lee]{park2020calibrated}
Sangdon Park, Osbert Bastani, James Weimer, and Insup Lee.
\newblock Calibrated prediction with covariate shift via unsupervised domain
  adaptation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3219--3229. PMLR, 2020.

\bibitem[Park et~al.(2021)Park, Dobriban, Lee, and Bastani]{park2021pac}
Sangdon Park, Edgar Dobriban, Insup Lee, and Osbert Bastani.
\newblock Pac prediction sets under covariate shift.
\newblock \emph{arXiv preprint arXiv:2106.09848}, 2021.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion,
  Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos,
  Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel,
  M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos,
  D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830,
  2011.

\bibitem[Platt et~al.(1999)]{platt1999probabilistic}
John Platt et~al.
\newblock Probabilistic outputs for support vector machines and comparisons to
  regularized likelihood methods.
\newblock \emph{Advances in large margin classifiers}, 10\penalty0
  (3):\penalty0 61--74, 1999.

\bibitem[Podkopaev and Ramdas(2021)]{podkopaev2021distribution}
Aleksandr Podkopaev and Aaditya Ramdas.
\newblock Distribution-free uncertainty quantification for classification under
  label shift.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 844--853.
  PMLR, 2021.

\bibitem[Qui{\~n}onero-Candela et~al.(2008)Qui{\~n}onero-Candela, Sugiyama,
  Schwaighofer, and Lawrence]{quinonero2008dataset}
Joaquin Qui{\~n}onero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil~D
  Lawrence.
\newblock \emph{Dataset shift in machine learning}.
\newblock Mit Press, 2008.

\bibitem[Tan and Le(2019)]{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{International conference on machine learning}, pages
  6105--6114. PMLR, 2019.

\bibitem[Thulasidasan et~al.(2019)Thulasidasan, Chennupati, Bilmes,
  Bhattacharya, and Michalak]{thulasidasan2019mixup}
Sunil Thulasidasan, Gopinath Chennupati, Jeff~A Bilmes, Tanmoy Bhattacharya,
  and Sarah Michalak.
\newblock On mixup training: Improved calibration and predictive uncertainty
  for deep neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Tibshirani et~al.(2019)Tibshirani, Foygel~Barber, Candes, and
  Ramdas]{tibshirani2019conformal}
Ryan~J Tibshirani, Rina Foygel~Barber, Emmanuel Candes, and Aaditya Ramdas.
\newblock Conformal prediction under covariate shift.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Vovk et~al.(2005)Vovk, Gammerman, and Shafer]{vovk2005algorithmic}
Vladimir Vovk, Alexander Gammerman, and Glenn Shafer.
\newblock \emph{Algorithmic learning in a random world}.
\newblock Springer Science \& Business Media, 2005.

\bibitem[Wald et~al.(2021)Wald, Feder, Greenfeld, and
  Shalit]{wald2021calibration}
Yoav Wald, Amir Feder, Daniel Greenfeld, and Uri Shalit.
\newblock On calibration and out-of-domain generalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Wang et~al.(2020)Wang, Long, Wang, and Jordan]{wang2020transferable}
Ximei Wang, Mingsheng Long, Jianmin Wang, and Michael Jordan.
\newblock Transferable calibration with lower bias and variance in domain
  adaptation.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 19212--19223, 2020.

\bibitem[Weyand et~al.(2020)Weyand, Araujo, Cao, and Sim]{weyand2020google}
Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.
\newblock Google landmarks dataset v2-a large-scale benchmark for
  instance-level recognition and retrieval.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 2575--2584, 2020.

\bibitem[Xie et~al.(2020)Xie, Tan, Gong, Wang, Yuille, and
  Le]{xie2020adversarial}
Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan~L Yuille, and Quoc~V
  Le.
\newblock Adversarial examples improve image recognition.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 819--828, 2020.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2017aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1492--1500, 2017.

\bibitem[Zadrozny and Elkan(2001)]{zadrozny2001obtaining}
Bianca Zadrozny and Charles Elkan.
\newblock Obtaining calibrated probability estimates from decision trees and
  naive bayesian classifiers.
\newblock In \emph{Icml}, volume~1, pages 609--616. Citeseer, 2001.

\bibitem[Zadrozny and Elkan(2002)]{zadrozny2002transforming}
Bianca Zadrozny and Charles Elkan.
\newblock Transforming classifier scores into accurate multiclass probability
  estimates.
\newblock In \emph{Proceedings of the eighth ACM SIGKDD international
  conference on Knowledge discovery and data mining}, pages 694--699, 2002.

\bibitem[Zhao et~al.(2018)Zhao, Zhang, Wu, Moura, Costeira, and
  Gordon]{zhao2018adversarial}
Han Zhao, Shanghang Zhang, Guanhang Wu, Jos{\'e}~MF Moura, Joao~P Costeira, and
  Geoffrey~J Gordon.
\newblock Adversarial multiple source domain adaptation.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\end{thebibliography}
