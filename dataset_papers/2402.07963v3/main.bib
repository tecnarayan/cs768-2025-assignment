\begin{filecontents}{test.bib}

@article{silver2016mastering,
  title={Mastering the game of {Go} with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den
  Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot,
  Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{silver2017mastering,
  title={Mastering the game of go without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{anthony2017thinking,
  title={Thinking fast and slow with deep learning and tree search},
  author={Anthony, Thomas and Tian, Zheng and Barber, David},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{boxobanlevels,
author = {Arthur Guez and Mehdi Mirza and Karol Gregor and Rishabh Kabra and Sebastien Racaniere and Theophane Weber and David Raposo and Adam Santoro and Laurent Orseau and Tom Eccles and Greg Wayne and David Silver and Timothy Lillicrap and Victor Valdes},
title = {An investigation of Model-free planning: boxoban levels},
howpublished= {https://github.com/deepmind/boxoban-levels/},
year = "2018"
}

@inproceedings{bonnet2023jumanji,
  title={Jumanji: a Diverse Suite of Scalable Reinforcement Learning Environments in JAX},
  author={Bonnet, Cl{\'e}ment and Luo, Daniel and Byrne, Donal John and Surana, Shikha and Duckworth, Paul and Coyette, Vincent and Midgley, Laurence Illing and Abramowitz, Sasha and Tegegn, Elshadai and Kalloniatis, Tristan and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@inproceedings{piche2018probabilistic,
  title={Probabilistic planning with sequential monte carlo methods},
  author={Pich{\'e}, Alexandre and Thomas, Valentin and Ibrahim, Cyril and Bengio, Yoshua and Pal, Chris},
  booktitle={International Conference on Learning Representations},
  year={2018}
}


@inproceedings{danihelka2021policy,
  title={Policy improvement by planning with Gumbel},
  author={Danihelka, Ivo and Guez, Arthur and Schrittwieser, Julian and Silver, David},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@misc{brax2021github,
  author = {C. Daniel Freeman and Erik Frey and Anton Raichuk and Sertan Girgin and Igor Mordatch and Olivier Bachem},
  title = {Brax - A Differentiable Physics Engine for Large Scale Rigid Body Simulation},
  url = {http://github.com/google/brax},
  version = {0.9.4},
  year = {2021},
}

@inproceedings{todorov2012mujoco,
  title={MuJoCo: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE},
  doi={10.1109/IROS.2012.6386109}
}



@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@article{vieillard2020munchausen,
  title={Munchausen reinforcement learning},
  author={Vieillard, Nino and Pietquin, Olivier and Geist, Matthieu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={4235--4246},
  year={2020}
}


%%%%%%%%%%VARIANCE PAPERS%%%%%
@misc{engstrom2020implementation,
	title        = {Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO},
	author       = {Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and Larry Rudolph and Aleksander Madry},
	year         = 2020,
	eprint       = {2005.12729},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@misc{henderson2019deep,
	title        = {{Deep Reinforcement Learning that Matters}},
	author       = {Peter Henderson and Riashat Islam and Philip Bachman and Joelle Pineau and Doina Precup and David Meger},
	year         = 2018,
	eprint       = {1709.06560},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@misc{jordan2020evaluating,
  title         = {{Evaluating the Performance of Reinforcement Learning Algorithms}},
  author        = {Scott M. Jordan and Yash Chandak and Daniel Cohen and Mengxue Zhang and Philip S. Thomas},
  year          = 2020,
  eprint        = {2006.16958},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{agarwal2021deep,
  title={Deep reinforcement learning at the edge of the statistical precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={29304--29320},
  year={2021}
}

@inproceedings{schulman2016high,
  author       = {John Schulman and
                  Philipp Moritz and
                  Sergey Levine and
                  Michael I. Jordan and
                  Pieter Abbeel},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  booktitle    = {4th International Conference on Learning Representations, {ICLR} 2016,
                  San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year         = {2016},
  url          = {http://arxiv.org/abs/1506.02438},
  timestamp    = {Thu, 25 Jul 2019 14:25:38 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SchulmanMLJA15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{schrittwieser2020mastering,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{moerland2018a0c,
  title={A0c: Alpha zero in continuous action space},
  author={Moerland, Thomas M and Broekens, Joost and Plaat, Aske and Jonker, Catholijn M},
  journal={arXiv preprint arXiv:1805.09613},
  year={2018}
}

@inproceedings{hubert2021learning,
  title={Learning and planning in complex action spaces},
  author={Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Barekatain, Mohammadamin and Schmitt, Simon and Silver, David},
  booktitle={International Conference on Machine Learning},
  pages={4476--4486},
  year={2021},
  organization={PMLR}
}

@inproceedings{tang2020discretizing,
  title={Discretizing continuous action space for on-policy optimization},
  author={Tang, Yunhao and Agrawal, Shipra},
  booktitle={Proceedings of the aaai conference on artificial intelligence},
  volume={34},
  number={04},
  pages={5981--5988},
  year={2020}
}

@article{agostinelli2019solving,
  title={Solving the Rubik’s cube with deep reinforcement learning and search},
  author={Agostinelli, Forest and McAleer, Stephen and Shmakov, Alexander and Baldi, Pierre},
  journal={Nature Machine Intelligence},
  volume={1},
  number={8},
  pages={356--363},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K and others},
  booktitle={Aaai},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}


@inproceedings{bellemare2017distributional,
  title={A distributional perspective on reinforcement learning},
  author={Bellemare, Marc G and Dabney, Will and Munos, R{\'e}mi},
  booktitle={International conference on machine learning},
  pages={449--458},
  year={2017},
  organization={PMLR}
}

@inproceedings{imani2018improving,
  title={Improving regression performance with distributional losses},
  author={Imani, Ehsan and White, Martha},
  booktitle={International conference on machine learning},
  pages={2157--2166},
  year={2018},
  organization={PMLR}
}

@article{pitt2001auxiliary,
  title={Auxiliary variable based particle filters},
  author={Pitt, Michael K and Shephard, Neil},
  journal={Sequential Monte Carlo methods in practice},
  pages={273--293},
  year={2001},
  publisher={Springer}
}

@article{stuhlmuller2015coarse,
  title={Coarse-to-fine sequential monte carlo for probabilistic programs},
  author={Stuhlm{\"u}ller, Andreas and Hawkins, Robert XD and Siddharth, N and Goodman, Noah D},
  journal={arXiv preprint arXiv:1509.02962},
  year={2015}
}

@article{bresler1986two,
  title={Two-filter formulae for discrete-time non-linear Bayesian smoothing},
  author={Bresler, Yoram},
  journal={International Journal of Control},
  volume={43},
  number={2},
  pages={629--641},
  year={1986},
  publisher={Taylor \& Francis}
}

@article{kitagawa1994two,
  title={The two-filter formula for smoothing and an implementation of the Gaussian-sum smoother},
  author={Kitagawa, Genshiro},
  journal={Annals of the Institute of Statistical Mathematics},
  volume={46},
  pages={605--623},
  year={1994},
  publisher={Springer}
}

@inproceedings{gordon1993novel,
  title={Novel approach to nonlinear/non-Gaussian Bayesian state estimation},
  author={Gordon, Neil J and Salmond, David J and Smith, Adrian FM},
  booktitle={IEE proceedings F (radar and signal processing)},
  volume={140},
  number={2},
  pages={107--113},
  year={1993},
  organization={IET}
}

@article{ziebart2010modeling,
  title={Modeling interaction via the principle of maximum causal entropy},
  author={Ziebart, Brian D and Bagnell, J Andrew and Dey, Anind K},
  year={2010},
  publisher={Carnegie Mellon University}
}

@inproceedings{neumann2011variational,
  title={Variational inference for policy search in changing situations},
  author={Neumann, Gerhard and others},
  booktitle={Proceedings of the 28th International Conference on Machine Learning, ICML 2011},
  pages={817--824},
  year={2011}
}

@article{levine2018reinforcement,
  title={Reinforcement learning and control as probabilistic inference: Tutorial and review},
  author={Levine, Sergey},
  journal={arXiv preprint arXiv:1805.00909},
  year={2018}
}

@article{sun2018dual,
  title={Dual policy iteration},
  author={Sun, Wen and Gordon, Geoffrey J and Boots, Byron and Bagnell, J},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{moerland2023model,
  title={Model-based reinforcement learning: A survey},
  author={Moerland, Thomas M and Broekens, Joost and Plaat, Aske and Jonker, Catholijn M and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={16},
  number={1},
  pages={1--118},
  year={2023},
  publisher={Now Publishers, Inc.}
}


@inproceedings{coulom2006efficient,
  title={Efficient selectivity and backup operators in {Monte-Carlo} tree search},
  author={Coulom, R{\'e}mi},
  booktitle={International conference on computers and games},
  pages={72--83},
  year={2006},
  organization={Springer}
}

@article{auer2002finite,
  title={Finite-time analysis of the multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  journal={Machine learning},
  volume={47},
  number={2},
  pages={235--256},
  year={2002},
  publisher={Springer}
}


@inproceedings{grill2020monte,
  title={Monte-Carlo tree search as regularized policy optimization},
  author={Grill, Jean-Bastien and Altch{\'e}, Florent and Tang, Yunhao and Hubert, Thomas and Valko, Michal and Antonoglou, Ioannis and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={3769--3778},
  year={2020},
  organization={PMLR}
}

@article{ye2021mastering,
  title={Mastering atari games with limited data},
  author={Ye, Weirui and Liu, Shaohuai and Kurutach, Thanard and Abbeel, Pieter and Gao, Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={25476--25488},
  year={2021}
}

@article{lai2015giraffe,
  title={Giraffe: Using deep reinforcement learning to play chess},
  author={Lai, Matthew},
  journal={arXiv preprint arXiv:1509.01549},
  year={2015}
}

@article{yang2020continuous,
  title={Continuous control for searching and planning with a learned model},
  author={Yang, Xuxi and Duvaud, Werner and Wei, Peng},
  journal={arXiv preprint arXiv:2006.07430},
  year={2020}
}

@article{kappen2012optimal,
  title={Optimal control as a graphical model inference problem},
  author={Kappen, Hilbert J and G{\'o}mez, Vicen{\c{c}} and Opper, Manfred},
  journal={Machine learning},
  volume={87},
  pages={159--182},
  year={2012},
  publisher={Springer}
}

@inproceedings{toussaint2006probabilistic,
  title={Probabilistic inference for solving discrete and continuous state Markov Decision Processes},
  author={Toussaint, Marc and Storkey, Amos},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={945--952},
  year={2006}
}

@inproceedings{toussaint2007probabilistic,
  title={Probabilistic inference for structured planning in robotics},
  author={Toussaint, Marc and Goerick, Christian},
  booktitle={2007 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={3068--3073},
  year={2007},
  organization={IEEE}
}

@article{kitagawa1996monte,
  title={Monte Carlo filter and smoother for non-Gaussian nonlinear state space models},
  author={Kitagawa, Genshiro},
  journal={Journal of computational and graphical statistics},
  volume={5},
  number={1},
  pages={1--25},
  year={1996},
  publisher={Taylor \& Francis}
}

@article{liu1998sequential,
  title={Sequential Monte Carlo methods for dynamic systems},
  author={Liu, Jun S and Chen, Rong},
  journal={Journal of the American statistical association},
  volume={93},
  number={443},
  pages={1032--1044},
  year={1998},
  publisher={Taylor \& Francis}
}

@article{pitt1999filtering,
  title={Filtering via simulation: Auxiliary particle filters},
  author={Pitt, Michael K and Shephard, Neil},
  journal={Journal of the American statistical association},
  volume={94},
  number={446},
  pages={590--599},
  year={1999},
  publisher={Taylor \& Francis}
}

@article{andrieu2004particle,
  title={Particle methods for change detection, system identification, and control},
  author={Andrieu, Christophe and Doucet, Arnaud and Singh, Sumeetpal S and Tadic, Vladislav B},
  journal={Proceedings of the IEEE},
  volume={92},
  number={3},
  pages={423--438},
  year={2004},
  publisher={IEEE}
}

@article{lazaric2007reinforcement,
  title={Reinforcement learning in continuous action spaces through sequential monte carlo methods},
  author={Lazaric, Alessandro and Restelli, Marcello and Bonarini, Andrea},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}

@article{gu2015neural,
  title={Neural adaptive sequential monte carlo},
  author={Gu, Shixiang Shane and Ghahramani, Zoubin and Turner, Richard E},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}


@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{kool2018attention,
  title={Attention, Learn to Solve Routing Problems!},
  author={Kool, Wouter and van Hoof, Herke and Welling, Max},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}

@article{hamrick2019combining,
  title={Combining q-learning and search with amortized value estimates},
  author={Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Pfaff, Tobias and Weber, Theophane and Buesing, Lars and Battaglia, Peter W},
  journal={arXiv preprint arXiv:1912.02807},
  year={2019}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@misc{alphastarblog,
  title="{AlphaStar: Mastering the Real-Time Strategy Game StarCraft II}",
  author={Vinyals, Oriol and Babuschkin, Igor and Chung, Junyoung and Mathieu, Michael and Jaderberg, Max and Czarnecki, Wojtek and Dudzik, Andrew and Huang, Aja and Georgiev, Petko and Powell, Richard and Ewalds, Timo and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Agapiou, John and Oh, Junhyuk and Dalibard, Valentin and Choi, David and Sifre, Laurent and Sulsky, Yury and Vezhnevets, Sasha and Molloy, James and Cai, Trevor and Budden, David and Paine, Tom and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Pohlen, Toby and Yogatama, Dani and Cohen, Julia and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Apps, Chris and Kavukcuoglu, Koray and Hassabis, Demis and Silver, David},
  howpublished={\url{https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/}},
  year={2019}
}

@article{degrave2022magnetic,
  title={Magnetic control of tokamak plasmas through deep reinforcement learning},
  author={Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and de Las Casas, Diego and others},
  journal={Nature},
  volume={602},
  number={7897},
  pages={414--419},
  year={2022},
  publisher={Nature Publishing Group}
}

@Article{AlphaTensor2022,
  author  = {Fawzi, Alhussein and Balog, Matej and Huang, Aja and Hubert, Thomas and Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Ruiz, Francisco J. R. and Schrittwieser, Julian and Swirszcz, Grzegorz and Silver, David and Hassabis, Demis and Kohli, Pushmeet},
  journal = {Nature},
  title   = {Discovering faster matrix multiplication algorithms with reinforcement learning},
  year    = {2022},
  volume  = {610},
  number  = {7930},
  pages   = {47--53},
  doi     = {10.1038/s41586-022-05172-4}
}

@article{sutton2019bitter,
  title={The bitter lesson},
  author={Sutton, Richard},
  journal={Incomplete Ideas (blog)},
  volume={13},
  number={1},
  year={2019}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@article{mirhoseini2021graph,
  title={A graph placement methodology for fast chip design},
  author={Mirhoseini, Azalia and Goldie, Anna and Yazgan, Mustafa and Jiang, Joe Wenjie and Songhori, Ebrahim and Wang, Shen and Lee, Young-Joon and Johnson, Eric and Pathak, Omkar and Nazi, Azade and others},
  journal={Nature},
  volume={594},
  number={7862},
  pages={207--212},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{watkins1989learning,
  title={Learning from delayed rewards},
  author={Watkins, Christopher John Cornish Hellaby},
  year={1989},
  publisher={King's College, Cambridge United Kingdom}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@book{rummery1994line,
  title={On-line Q-learning using connectionist systems},
  author={Rummery, Gavin A and Niranjan, Mahesan},
  volume={37},
  year={1994},
  publisher={University of Cambridge, Department of Engineering Cambridge, UK}
}

@article{arulampalam2002tutorial,
  title={A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking},
  author={Arulampalam, M Sanjeev and Maskell, Simon and Gordon, Neil and Clapp, Tim},
  journal={IEEE Transactions on signal processing},
  volume={50},
  number={2},
  pages={174--188},
  year={2002},
  publisher={Ieee}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@misc{deepmind2020jax,
  title = {The {D}eep{M}ind {JAX} {E}cosystem},
  author = {DeepMind and Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu, Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou, Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Sartran, Laurent and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stanojevi\'{c}, Milo\v{s} and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and Viola, Fabio},
  url = {http://github.com/deepmind},
  year = {2020},
}

@article{freeman2021brax,
  title={Brax--A Differentiable Physics Engine for Large Scale Rigid Body Simulation},
  author={Freeman, C Daniel and Frey, Erik and Raichuk, Anton and Girgin, Sertan and Mordatch, Igor and Bachem, Olivier},
  journal={arXiv preprint arXiv:2106.13281},
  year={2021}
}

@misc{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}

@article{gorsane2022towards,
  title={Towards a standardised performance evaluation protocol for cooperative marl},
  author={Gorsane, Rihab and Mahjoub, Omayma and de Kock, Ruan John and Dubb, Roland and Singh, Siddarth and Pretorius, Arnu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={5510--5521},
  year={2022}
}

@article{bouthillier2021accounting,
  title={Accounting for variance in machine learning benchmarks},
  author={Bouthillier, Xavier and Delaunay, Pierre and Bronzi, Mirko and Trofimov, Assya and Nichyporuk, Brennan and Szeto, Justin and Mohammadi Sepahvand, Nazanin and Raff, Edward and Madan, Kanika and Voleti, Vikram and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={747--769},
  year={2021}
}

@inproceedings{
hafner2021mastering,
title={Mastering Atari with Discrete World Models},
author={Danijar Hafner and Timothy P Lillicrap and Mohammad Norouzi and Jimmy Ba},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=0oabwyZbOu}
}

@article{hafner2023mastering,
  title={Mastering diverse domains through world models},
  author={Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:2301.04104},
  year={2023}
}

@inproceedings{young2023the,
author = {Young, Kenny and Ramesh, Aditya and Kirsch, Louis and Schmidhuber, J\"{u}rgen},
title = {The benefits of model-based generalization in reinforcement learning},
year = {2023},
publisher = {JMLR.org},
abstract = {Model-Based Reinforcement Learning (RL) is widely believed to have the potential to improve sample efficiency by allowing an agent to synthesize large amounts of imagined experience. Experience Replay (ER) can be considered a simple kind of model, which has proved effective at improving the stability and efficiency of deep RL. In principle, a learned parametric model could improve on ER by generalizing from real experience to augment the dataset with additional plausible experience. However, given that learned value functions can also generalize, it is not immediately obvious why model generalization should be better. Here, we provide theoretical and empirical insight into when, and how, we can expect data generated by a learned model to be useful. First, we provide a simple theorem motivating how learning a model as an intermediate step can narrow down the set of possible value functions more than learning a value function directly from data using the Bellman equation. Second, we provide an illustrative example showing empirically how a similar effect occurs in a more concrete setting with neural network function approximation. Finally, we provide extensive experiments showing the benefit of model-based learning for online RL in environments with combinatorial complexity, but factored structure that allows a learned model to generalize. In these experiments, we take care to control for other factors in order to isolate, insofar as possible, the benefit of using experience generated by a learned model relative to ER alone.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1683},
numpages = {23},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@article{konda1999actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay and Tsitsiklis, John},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

@article{wu2019accelerating,
  title={Accelerating self-play learning in go},
  author={Wu, David J},
  journal={arXiv preprint arXiv:1902.10565},
  year={2019}
}

@misc{flashbax,
    title={Flashbax: Streamlining Experience Replay Buffers for Reinforcement Learning with JAX},
    author={Edan Toledo and Laurence Midgley and Donal Byrne and Callum Rhys Tilbury and
    Matthew Macfarlane and Cyprien Courtot and Alexandre Laterre},
    year={2023},
    url={https://github.com/instadeepai/flashbax/},
}

@article{hoffman2020acme,
    title={Acme: A Research Framework for Distributed Reinforcement Learning},
    author={
        Matthew W. Hoffman and Bobak Shahriari and John Aslanides and
        Gabriel Barth-Maron and Nikola Momchev and Danila Sinopalnikov and
        Piotr Sta\'nczyk and Sabela Ramos and Anton Raichuk and
        Damien Vincent and L\'eonard Hussenot and Robert Dadashi and
        Gabriel Dulac-Arnold and Manu Orsini and Alexis Jacq and
        Johan Ferret and Nino Vieillard and Seyed Kamyar Seyed Ghasemipour and
        Sertan Girgin and Olivier Pietquin and Feryal Behbahani and
        Tamara Norman and Abbas Abdolmaleki and Albin Cassirer and
        Fan Yang and Kate Baumli and Sarah Henderson and Abe Friesen and
        Ruba Haroun and Alex Novikov and Sergio G\'omez Colmenarejo and
        Serkan Cabi and Caglar Gulcehre and Tom Le Paine and
        Srivatsan Srinivasan and Andrew Cowie and Ziyu Wang and Bilal Piot and
        Nando de Freitas
    },
    year={2020},
    journal={arXiv preprint arXiv:2006.00979},
    url={https://arxiv.org/abs/2006.00979},
}

@article{dulac2015deep,
  title={Deep reinforcement learning in large discrete action spaces},
  author={Dulac-Arnold, Gabriel and Evans, Richard and van Hasselt, Hado and Sunehag, Peter and Lillicrap, Timothy and Hunt, Jonathan and Mann, Timothy and Weber, Theophane and Degris, Thomas and Coppin, Ben},
  journal={arXiv preprint arXiv:1512.07679},
  year={2015}
}

@article{racaniere2017imagination,
  title={Imagination-augmented agents for deep reinforcement learning},
  author={Racani{\`e}re, S{\'e}bastien and Weber, Th{\'e}ophane and Reichert, David and Buesing, Lars and Guez, Arthur and Jimenez Rezende, Danilo and Puigdom{\`e}nech Badia, Adri{\`a} and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{hottung2021efficient,
  title={Efficient active search for combinatorial optimization problems},
  author={Hottung, Andr{\'e} and Kwon, Yeong-Dae and Tierney, Kevin},
  journal={arXiv preprint arXiv:2106.05126},
  year={2021}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{compass2023,
  title={Combinatorial Optimization with Policy Adaptation using Latent Space Search},
  author={Felix Chalumeau and Shikha Surana and Clément Bonnet and Nathan Grinsztajn and 
  Arnu Pretorius and Alexandre Laterre and Thomas D. Barrett},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023}
}

@article{fearnhead2004particle,
  title={Particle filters for mixture models with an unknown number of components},
  author={Fearnhead, Paul},
  journal={Statistics and Computing},
  volume={14},
  pages={11--21},
  year={2004},
  publisher={Springer}
}

@inproceedings{guez2019investigation,
  title={An investigation of model-free planning},
  author={Guez, Arthur and Mirza, Mehdi and Gregor, Karol and Kabra, Rishabh and Racani{\`e}re, S{\'e}bastien and Weber, Th{\'e}ophane and Raposo, David and Santoro, Adam and Orseau, Laurent and Eccles, Tom and others},
  booktitle={International Conference on Machine Learning},
  pages={2464--2473},
  year={2019},
  organization={PMLR}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@article{rosenbluth1955monte,
  title={Monte Carlo calculation of the average extension of molecular chains},
  author={Rosenbluth, Marshall N and Rosenbluth, Arianna W},
  journal={The Journal of Chemical Physics},
  volume={23},
  number={2},
  pages={356--359},
  year={1955},
  publisher={American Institute of Physics}
}

@article{hammersley1954poor,
  title={Poor man’s monte carlo},
  author={Hammersley, John M and Morton, K William},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={16},
  number={1},
  pages={23--38},
  year={1954},
  publisher={Oxford University Press}
}

@incollection{liu2001theoretical,
  title={A theoretical framework for sequential importance sampling with resampling},
  author={Liu, Jun S and Chen, Rong and Logvinenko, Tanya},
  booktitle={Sequential Monte Carlo methods in practice},
  pages={225--246},
  year={2001},
  publisher={Springer}
}

@article{chung2023thinker,
  title={Thinker: Learning to Plan and Act},
  author={Chung, Stephen and Anokhin, Ivan and Krueger, David},
  journal={arXiv preprint arXiv:2307.14993},
  year={2023}
}

@inproceedings{guez2018learning,
  title={Learning to search with MCTSnets},
  author={Guez, Arthur and Weber, Th{\'e}ophane and Antonoglou, Ioannis and Simonyan, Karen and Vinyals, Oriol and Wierstra, Daan and Munos, R{\'e}mi and Silver, David},
  booktitle={International conference on machine learning},
  pages={1822--1831},
  year={2018},
  organization={PMLR}
}

@article{ghosh2020operator,
  title={An operator view of policy gradient methods},
  author={Ghosh, Dibya and C Machado, Marlos and Le Roux, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={3397--3406},
  year={2020}
}

@article{cappe2007overview,
  title={An overview of existing methods and recent advances in sequential Monte Carlo},
  author={Capp{\'e}, Olivier and Godsill, Simon J and Moulines, Eric},
  journal={Proceedings of the IEEE},
  volume={95},
  number={5},
  pages={899--924},
  year={2007},
  publisher={IEEE}
}

@article{de2000sequential,
  title={Sequential Monte Carlo methods to train neural network models},
  author={de Freitas, Joao FG and Niranjan, Mahesan and Gee, Andrew H. and Doucet, Arnaud},
  journal={Neural computation},
  volume={12},
  number={4},
  pages={955--993},
  year={2000},
  publisher={MIT Press}
}

@article{creal2012survey,
  title={A survey of sequential Monte Carlo methods for economics and finance},
  author={Creal, Drew},
  journal={Econometric reviews},
  volume={31},
  number={3},
  pages={245--296},
  year={2012},
  publisher={Taylor \& Francis}
}

@article{thrun2001robust,
  title={Robust Monte Carlo localization for mobile robots},
  author={Thrun, Sebastian and Fox, Dieter and Burgard, Wolfram and Dellaert, Frank},
  journal={Artificial intelligence},
  volume={128},
  number={1-2},
  pages={99--141},
  year={2001},
  publisher={Elsevier}
}

@inproceedings{berkenkamp2017,
 author = {Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela and Krause, Andreas},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Safe Model-based Reinforcement Learning with Stability Guarantees},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/766ebcd59621e305170616ba3d3dac32-Paper.pdf},
 volume = {30},
 year = {2017}
}

@InProceedings{pmlr-v119-sekar20a,
  title = 	 {Planning to Explore via Self-Supervised World Models},
  author =       {Sekar, Ramanan and Rybkin, Oleh and Daniilidis, Kostas and Abbeel, Pieter and Hafner, Danijar and Pathak, Deepak},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {8583--8592},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/sekar20a/sekar20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/sekar20a.html},
  abstract = 	 {Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code: https://ramanans1.github.io/plan2explore/}
}


@article{weber2017imagination,
  title={Imagination-augmented agents for deep reinforcement learning},
  author={Weber, Th{\'e}ophane and Racaniere, S{\'e}bastien and Reichert, David P and Buesing, Lars and Guez, Arthur and Rezende, Danilo Jimenez and Badia, Adria Puigdomenech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and others},
  journal={arXiv preprint arXiv:1707.06203},
  year={2017}
}

@inproceedings{antonoglou2021planning,
  title={Planning in stochastic environments with a learned model},
  author={Antonoglou, Ioannis and Schrittwieser, Julian and Ozair, Sherjil and Hubert, Thomas K and Silver, David},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{levy1992stochastic,
  title={Stochastic Dominance and Expected Utility: Survey and Analysis},
  author={Levy, Haim},
  journal={Management Science},
  pages={555--593},
  year={1992},
  publisher={JSTOR}
}

@inproceedings{dror2019deep,
  title={Deep dominance-how to properly compare deep neural models},
  author={Dror, Rotem and Shlomov, Segev and Reichart, Roi},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2773--2785},
  year={2019}
}

@book{bain2009fundamentals,
  title={Fundamentals of stochastic filtering},
  author={Bain, Alan and Crisan, Dan},
  volume={3},
  year={2009},
  publisher={Springer}
}

@article{maskell2001tutorial,
  title={A tutorial on particle filters for on-line nonlinear/non-Gaussian Bayesian tracking},
  author={Maskell, Simon and Gordon, Neil},
  journal={IEE Target Tracking: Algorithms and Applications (Ref. No. 2001/174)},
  pages={2--1},
  year={2001},
  publisher={IET}
}

@article{doucet2000sequential,
  title={On sequential Monte Carlo sampling methods for Bayesian filtering},
  author={Doucet, Arnaud and Godsill, Simon and Andrieu, Christophe},
  journal={Statistics and computing},
  volume={10},
  pages={197--208},
  year={2000},
  publisher={Springer}
}

@article{peng2019advantage,
  title={Advantage-weighted regression: Simple and scalable off-policy reinforcement learning},
  author={Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey},
  journal={arXiv preprint arXiv:1910.00177},
  year={2019}
}

@article{mann1947test,
  title={On a test of whether one of two random variables is stochastically larger than the other},
  author={Mann, Henry B and Whitney, Donald R},
  journal={The annals of mathematical statistics},
  pages={50--60},
  year={1947},
  publisher={JSTOR}
}

@inproceedings{haarnoja2017reinforcement,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1352--1361},
  year={2017},
  organization={PMLR}
}

@inproceedings{lioutas2022critic,
  title={Critic Sequential Monte Carlo},
  author={Lioutas, Vasileios and Lavington, Jonathan Wilder and Sefas, Justice and Niedoba, Matthew and Liu, Yunpeng and Zwartsenberg, Berend and Dabiri, Setareh and Wood, Frank and Scibior, Adam},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{dayan1997using,
  title={Using expectation-maximization for reinforcement learning},
  author={Dayan, Peter and Hinton, Geoffrey E},
  journal={Neural Computation},
  volume={9},
  number={2},
  pages={271--278},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{levine2013variational,
  title={Variational policy search via trajectory optimization},
  author={Levine, Sergey and Koltun, Vladlen},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}



@article{fellows2019virel,
  title={Virel: A variational inference framework for reinforcement learning},
  author={Fellows, Matthew and Mahajan, Anuj and Rudner, Tim GJ and Whiteson, Shimon},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{hoffman2008trans,
  title={Trans-dimensional MCMC for Bayesian policy learning},
  author={Hoffman, Matt and Doucet, Arnaud and De Freitas, Nando and Jasra, Ajay},
  booktitle={Neural Information Processing Systems},
  volume={20},
  pages={1--8},
  year={2008},
  organization={Citeseer}
}

@inproceedings{furmston2010variational,
  title={Variational methods for reinforcement learning},
  author={Furmston, Thomas and Barber, David},
  booktitle={Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages={241--248},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{peters2010relative,
  title={Relative entropy policy search},
  author={Peters, Jan and Mulling, Katharina and Altun, Yasemin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={24},
  number={1},
  pages={1607--1612},
  year={2010}
}

@article{rawlik2013stochastic,
  title={On stochastic optimal control and reinforcement learning by approximate inference},
  author={Rawlik, Konrad and Toussaint, Marc and Vijayakumar, Sethu},
  year={2013}
}

@article{fox2015taming,
  title={Taming the noise in reinforcement learning via soft updates},
  author={Fox, Roy and Pakman, Ari and Tishby, Naftali},
  journal={arXiv preprint arXiv:1512.08562},
  year={2015}
}

@article{dempster1977maximum,
  title={Maximum likelihood from incomplete data via the EM algorithm},
  author={Dempster, Arthur P and Laird, Nan M and Rubin, Donald B},
  journal={Journal of the royal statistical society: series B (methodological)},
  volume={39},
  number={1},
  pages={1--22},
  year={1977},
  publisher={Wiley Online Library}
}

@inproceedings{
abdolmaleki2018maximum,
title={Maximum a Posteriori Policy Optimisation},
author={Abbas Abdolmaleki and Jost Tobias Springenberg and Yuval Tassa and Remi Munos and Nicolas Heess and Martin Riedmiller},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=S1ANxQW0b},
}

@inproceedings{wirth2016model,
  title={Model-free preference-based reinforcement learning},
  author={Wirth, Christian and F{\"u}rnkranz, Johannes and Neumann, Gerhard},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}

@article{sutton1988learning,
  title={Learning to predict by the methods of temporal differences},
  author={Sutton, Richard S},
  journal={Machine learning},
  volume={3},
  pages={9--44},
  year={1988},
  publisher={Springer}
}

@inproceedings{peters2007reinforcement,
  title={Reinforcement learning by reward-weighted regression for operational space control},
  author={Peters, Jan and Schaal, Stefan},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={745--750},
  year={2007}
}

@inproceedings{song2019v,
  title={V-MPO: On-Policy Maximum a Posteriori Policy Optimization for Discrete and Continuous Control},
  author={Song, H Francis and Abdolmaleki, Abbas and Springenberg, Jost Tobias and Clark, Aidan and Soyer, Hubert and Rae, Jack W and Noury, Seb and Ahuja, Arun and Liu, Siqi and Tirumala, Dhruva and others},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{nair2020awac,
  title={Awac: Accelerating online reinforcement learning with offline datasets},
  author={Nair, Ashvin and Gupta, Abhishek and Dalal, Murtaza and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.09359},
  year={2020}
}

@article{furuta2021co,
  title={Co-adaptation of algorithmic and implementational innovations in inference-based deep reinforcement learning},
  author={Furuta, Hiroki and Kozuno, Tadashi and Matsushima, Tatsuya and Matsuo, Yutaka and Gu, Shixiang Shane},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={9828--9842},
  year={2021}
}

@book{anderson2007optimal,
  title={Optimal control: linear quadratic methods},
  author={Anderson, Brian DO and Moore, John B},
  year={2007},
  publisher={Courier Corporation}
}

@article{schulman2017equivalence,
  title={Equivalence between policy gradients and soft q-learning},
  author={Schulman, John and Chen, Xi and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1704.06440},
  year={2017}
}

@book{doucet2001sequential,
  title={Sequential Monte Carlo methods in practice},
  author={Doucet, Arnaud and De Freitas, Nando and Gordon, Neil James and others},
  volume={1},
  number={2},
  year={2001},
  publisher={Springer}
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@inproceedings{segal2010scalability,
  title={On the scalability of parallel UCT},
  author={Segal, Richard B},
  booktitle={International Conference on Computers and Games},
  pages={36--47},
  year={2010},
  organization={Springer}
}

@inproceedings{liu2018watch,
  title={Watch the Unobserved: A Simple Approach to Parallelizing Monte Carlo Tree Search},
  author={Liu, Anji and Chen, Jianshu and Yu, Mingze and Zhai, Yu and Zhou, Xuewen and Liu, Ji},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{li2017approximate,
  title={Approximate inference with amortised mcmc},
  author={Li, Yingzhen and Turner, Richard E and Liu, Qiang},
  journal={arXiv preprint arXiv:1702.08343},
  year={2017}
}

@book{gilks1995markov,
  title={Markov chain Monte Carlo in practice},
  author={Gilks, Walter R and Richardson, Sylvia and Spiegelhalter, David},
  year={1995},
  publisher={CRC press}
}

@inproceedings{liu2022constrained,
  title={Constrained variational policy optimization for safe reinforcement learning},
  author={Liu, Zuxin and Cen, Zhepeng and Isenbaev, Vladislav and Liu, Wei and Wu, Steven and Li, Bo and Zhao, Ding},
  booktitle={International Conference on Machine Learning},
  pages={13644--13668},
  year={2022},
  organization={PMLR}
}

@article{montgomery2016guided,
  title={Guided policy search as approximate mirror descent},
  author={Montgomery, William and Levine, Sergey},
  journal={arXiv preprint arXiv:1607.04614},
  year={2016}
}

@article{beck2003mirror,
  title={Mirror descent and nonlinear projected subgradient methods for convex optimization},
  author={Beck, Amir and Teboulle, Marc},
  journal={Operations Research Letters},
  volume={31},
  number={3},
  pages={167--175},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{hachiya2009efficient,
  title={Efficient sample reuse in EM-based policy search},
  author={Hachiya, Hirotaka and Peters, Jan and Sugiyama, Masashi},
  booktitle={Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2009, Bled, Slovenia, September 7-11, 2009, Proceedings, Part I 20},
  pages={469--484},
  year={2009},
  organization={Springer}
}

@inproceedings{colas2018gep,
  title={Gep-pg: Decoupling exploration and exploitation in deep reinforcement learning algorithms},
  author={Colas, C{\'e}dric and Sigaud, Olivier and Oudeyer, Pierre-Yves},
  booktitle={International conference on machine learning},
  pages={1039--1048},
  year={2018},
  organization={PMLR}
}

@article{silver2018general,
  title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={Science},
  volume={362},
  number={6419},
  pages={1140--1144},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{dalal2021improve,
  title={Improve agents without retraining: Parallel tree search with off-policy correction},
  author={Dalal, Gal and Hallak, Assaf and Dalton, Steven and Mannor, Shie and Chechik, Gal and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5518--5530},
  year={2021}
}


@article{vlassis2009learning,
  title={Learning model-free robot control by a Monte Carlo EM algorithm},
  author={Vlassis, Nikos and Toussaint, Marc and Kontes, Georgios and Piperidis, Savas},
  journal={Autonomous Robots},
  volume={27},
  pages={123--130},
  year={2009},
  publisher={Springer}
}

@inproceedings{levine2013guided,
  title={Guided policy search},
  author={Levine, Sergey and Koltun, Vladlen},
  booktitle={International conference on machine learning},
  pages={1--9},
  year={2013},
  organization={PMLR}
}

@misc{toledo2024stoix,
    title={Stoix: Distributed Single-Agent Reinforcement Learning End-to-End in JAX},
    doi = {10.5281/zenodo.10916257},
    author={Edan Toledo},
    month = apr,
    year = {2024},
    url = {https://github.com/EdanToledo/Stoix},
}

@article{schulman2015trust,
  title={Trust Region Policy Optimization},
  author={Schulman, John},
  journal={arXiv preprint arXiv:1502.05477},
  year={2015}
}

@inproceedings{adkinsmethod,
  title={A Method for Evaluating Hyperparameter Sensitivity in Reinforcement Learning},
  author={Adkins, Jacob and Bowling, Michael and White, Adam},
  booktitle={Finding the Frame: An RLC Workshop for Examining Conceptual Frameworks}
}

@article{fountas2020deep,
  title={Deep active inference agents using Monte-Carlo methods},
  author={Fountas, Zafeirios and Sajid, Noor and Mediano, Pedro and Friston, Karl},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={11662--11675},
  year={2020}
}

@article{friston2010free,
  title={The free-energy principle: a unified brain theory?},
  author={Friston, Karl},
  journal={Nature reviews neuroscience},
  volume={11},
  number={2},
  pages={127--138},
  year={2010},
  publisher={Nature publishing group}
}

@article{amos2023tutorial,
  title={Tutorial on amortized optimization},
  author={Amos, Brandon and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={16},
  number={5},
  pages={592--732},
  year={2023},
  publisher={Now Publishers, Inc.}
}

@inproceedings{li2004iterative,
  title={Iterative linear quadratic regulator design for nonlinear biological movement systems},
  author={Li, Weiwei and Todorov, Emanuel},
  booktitle={First International Conference on Informatics in Control, Automation and Robotics},
  volume={2},
  pages={222--229},
  year={2004},
  organization={SciTePress}
}

@article{pang2023language,
  title={Language model self-improvement by reinforcement learning contemplation},
  author={Pang, Jing-Cheng and Wang, Pengyuan and Li, Kaiyuan and Chen, Xiong-Hui and Xu, Jiacheng and Zhang, Zongzhang and Yu, Yang},
  journal={arXiv preprint arXiv:2305.14483},
  year={2023}
}