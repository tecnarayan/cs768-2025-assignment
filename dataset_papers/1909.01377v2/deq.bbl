\begin{thebibliography}{10}

\bibitem{al2018character}
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones.
\newblock Character-level language modeling with deeper self-attention.
\newblock {\em arXiv:1808.04444}, 2018.

\bibitem{almeida1990learning}
Luis~B Almeida.
\newblock A learning rule for asynchronous perceptrons with feedback in a
  combinatorial environment.
\newblock In {\em Artificial Neural Networks}. 1990.

\bibitem{amos2017optnet}
Brandon Amos and J~Zico Kolter.
\newblock {OptNet}: Differentiable optimization as a layer in neural networks.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2017.

\bibitem{arjovsky2016unitary}
Martin Arjovsky, Amar Shah, and Yoshua Bengio.
\newblock Unitary evolution recurrent neural networks.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2016.

\bibitem{Ba2016layer}
Lei~Jimmy Ba, Ryan Kiros, and Geoffrey~E. Hinton.
\newblock Layer normalization.
\newblock {\em arXiv:1607.06450}, 2016.

\bibitem{baevski2019adaptive}
Alexei Baevski and Michael Auli.
\newblock Adaptive input representations for neural language modeling.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{bai2018empirical}
Shaojie Bai, J.~Zico Kolter, and Vladlen Koltun.
\newblock An empirical evaluation of generic convolutional and recurrent
  networks for sequence modeling.
\newblock {\em arXiv:1803.01271}, 2018.

\bibitem{bai2018trellis}
Shaojie Bai, J.~Zico Kolter, and Vladlen Koltun.
\newblock Trellis networks for sequence modeling.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{bradbury2016quasi}
James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher.
\newblock Quasi-recurrent neural networks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{broyden1965class}
Charles~G Broyden.
\newblock A class of methods for solving nonlinear simultaneous equations.
\newblock {\em Mathematics of Computation}, 1965.

\bibitem{chen2018neural}
Tian~Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In {\em Neural Information Processing Systems}, 2018.

\bibitem{chen2016training}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock {\em arXiv:1604.06174}, 2016.

\bibitem{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em arXiv:1904.10509}, 2019.

\bibitem{choGRU}
Kyunghyun Cho, Bart Van~Merri{\"e}nboer, Dzmitry Bahdanau, and Yoshua Bengio.
\newblock On the properties of neural machine translation: Encoder-decoder
  approaches.
\newblock {\em arXiv:1409.1259}, 2014.

\bibitem{dabre2019recurrent}
Raj Dabre and Atsushi Fujita.
\newblock Recurrent stacking of layers for compact neural machine translation
  models.
\newblock In {\em AAAI Conference on Artificial Intelligence}, 2019.

\bibitem{dai2018transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc~V. Le, and Ruslan
  Salakhutdinov.
\newblock {Transformer-XL}: Attentive language models beyond a fixed-length
  context.
\newblock In {\em Annual Meeting of the Association for Computational
  Linguistics (ACL)}, 2019.

\bibitem{dauphinGatedConv}
Yann~N. Dauphin, Angela Fan, Michael Auli, and David Grangier.
\newblock Language modeling with gated convolutional networks.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2017.

\bibitem{dehghani2018universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and {\L}ukasz
  Kaiser.
\newblock Universal transformers.
\newblock {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL-HLT}, 2019.

\bibitem{elghaoui2019implicit}
Laurent {El Ghaoui}, Fangda {Gu}, Bertrand {Travacca}, and Armin {Askari}.
\newblock Implicit deep learning.
\newblock {\em arXiv:1908.06315}, 2019.

\bibitem{Elman90findstructure}
Jeffrey~L Elman.
\newblock Finding structure in time.
\newblock {\em Cognitive Science}, 14(2), 1990.

\bibitem{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock A theoretically grounded application of dropout in recurrent neural
  networks.
\newblock In {\em Neural Information Processing Systems}, 2016.

\bibitem{gomez2017reversible}
Aidan~N Gomez, Mengye Ren, Raquel Urtasun, and Roger~B Grosse.
\newblock The reversible residual network: Backpropagation without storing
  activations.
\newblock In {\em Neural Information Processing Systems}, 2017.

\bibitem{haber2017stable}
Eldad Haber and Lars Ruthotto.
\newblock Stable architectures for deep neural networks.
\newblock {\em Inverse Problems}, 2017.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Computer Vision and Pattern Recognition (CVPR)}, 2016.

\bibitem{hochreiterLSTM}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural Computation}, 9(8), 1997.

\bibitem{kazi2017implicitly}
Michaeel Kazi and Brian Thompson.
\newblock Implicitly-defined neural networks for sequence labeling.
\newblock In {\em Annual Meeting of the Association for Computational
  Linguistics (Short Papers)}, 2017.

\bibitem{liao2018reviving}
Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Xaq Pitkow,
  Raquel Urtasun, and Richard Zemel.
\newblock Reviving and improving recurrent back-propagation.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2018.

\bibitem{liu2018darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock {DARTS}: Differentiable architecture search.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{MacKay2018}
Matthew MacKay, Paul Vicol, Jimmy Ba, and Roger~B. Grosse.
\newblock Reversible recurrent neural networks.
\newblock In {\em Neural Information Processing Systems}, 2018.

\bibitem{Marcus93buildinga}
Mitchell~P Marcus, Mary~Ann Marcinkiewicz, and Beatrice Santorini.
\newblock Building a large annotated corpus of {English}: The {Penn} treebank.
\newblock {\em Computational Linguistics}, 19(2), 1993.

\bibitem{Melis2018}
G{\'{a}}bor Melis, Chris Dyer, and Phil Blunsom.
\newblock On the state of the art of evaluation in neural language models.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{merity2018analysis}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock An analysis of neural language modeling at multiple scales.
\newblock {\em arXiv:1803.08240}, 2018.

\bibitem{merityRegOpt}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock Regularizing and optimizing {LSTM} language models.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{miller2018recurrent}
John Miller and Moritz Hardt.
\newblock When recurrent models don't need to be recurrent.
\newblock {\em arXiv:1805.10369}, 2018.

\bibitem{niculae2018sparsemap}
Vlad Niculae, Andre Martins, Mathieu Blondel, and Claire Cardie.
\newblock {SparseMAP}: Differentiable sparse structured inference.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2018.

\bibitem{pineda1988generalization}
Fernando~J Pineda.
\newblock Generalization of back propagation to recurrent and higher order
  neural networks.
\newblock In {\em Neural Information Processing Systems}, 1988.

\bibitem{Salimans2016}
Tim Salimans and Diederik~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In {\em Neural Information Processing Systems}, 2016.

\bibitem{santoro2018relational}
Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski,
  Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy
  Lillicrap.
\newblock Relational recurrent neural networks.
\newblock In {\em Neural Information Processing Systems}, 2018.

\bibitem{scellier2017equilibrium}
Benjamin Scellier and Yoshua Bengio.
\newblock Equilibrium propagation: Bridging the gap between energy-based models
  and backpropagation.
\newblock {\em Frontiers in Computational Neuroscience}, 2017.

\bibitem{sherman1950adjustment}
Jack Sherman and Winifred~J Morrison.
\newblock Adjustment of an inverse matrix corresponding to a change in one
  element of a given matrix.
\newblock {\em The Annals of Mathematical Statistics}, 1950.

\bibitem{simard1989fixed}
Patrice~Y Simard, Mary~B Ottaway, and Dana~H Ballard.
\newblock Fixed point analysis for recurrent networks.
\newblock In {\em Neural Information Processing Systems}, 1989.

\bibitem{srivastava2014dropout}
Nitish Srivastava, Geoffrey~E Hinton, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research (JMLR)}, 15(1), 2014.

\bibitem{Steiner2019}
Benoit Steiner, Zachary DeVito, Soumith Chintala, Sam Gross, Adam Paszke,
  Francisco Massa, Adam Lerer, Gregory Chanan, Zeming Lin, Edward Yang, et~al.
\newblock {PyTorch}: An imperative style, high-performance deep learning
  library.
\newblock In {\em Neural Information Processing Systems}, 2019.

\bibitem{trinh2018learning}
Trieu~H Trinh, Andrew~M Dai, Thang Luong, and Quoc~V Le.
\newblock Learning longer-term dependencies in {RNNs} with auxiliary losses.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2018.

\bibitem{waveNet}
A{\"{a}}ron van~den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol
  Vinyals, Alex Graves, Nal Kalchbrenner, Andrew~W. Senior, and Koray
  Kavukcuoglu.
\newblock {WaveNet}: {A} generative model for raw audio.
\newblock {\em arXiv:1609.03499}, 2016.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Neural Information Processing Systems}, 2017.

\bibitem{waibel}
Alex Waibel, Toshiyuki Hanazawa, Geoffrey Hinton, Kiyohiro Shikano, and Kevin~J
  Lang.
\newblock Phoneme recognition using time-delay neural networks.
\newblock {\em IEEE Transactions on Acoustics, Speech, and Signal Processing},
  37(3), 1989.

\bibitem{wang2019satnet}
Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter.
\newblock {SATNet}: Bridging deep learning and logical reasoning using a
  differentiable satisfiability solver.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2019.

\bibitem{Werbos1990}
Paul~J Werbos.
\newblock Backpropagation through time: What it does and how to do it.
\newblock {\em Proceedings of the IEEE}, 78(10), 1990.

\bibitem{yang2018breaking}
Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William~W. Cohen.
\newblock Breaking the softmax bottleneck: A high-rank {RNN} language model.
\newblock {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{Zhang2016NIPS}
Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan~R
  Salakhutdinov, and Yoshua Bengio.
\newblock Architectural complexity measures of recurrent neural networks.
\newblock In {\em Neural Information Processing Systems}, 2016.

\bibitem{zhang2019equilibrated}
Ziming Zhang, Anil Kag, Alan Sullivan, and Venkatesh Saligrama.
\newblock Equilibrated recurrent neural network: Neuronal time-delayed
  self-feedback improves accuracy and stability.
\newblock {\em arXiv:1903.00755}, 2019.

\bibitem{zoph2017neural}
Barret Zoph and Quoc~V Le.
\newblock Neural architecture search with reinforcement learning.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\end{thebibliography}
