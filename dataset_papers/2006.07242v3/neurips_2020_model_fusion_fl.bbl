\begin{thebibliography}{10}

\bibitem{ahn2019variational}
S.~Ahn, S.~X. Hu, A.~Damianou, N.~D. Lawrence, and Z.~Dai.
\newblock Variational information distillation for knowledge transfer.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 9163--9171, 2019.

\bibitem{anil2018large}
R.~Anil, G.~Pereyra, A.~Passos, R.~Ormandi, G.~E. Dahl, and G.~E. Hinton.
\newblock Large scale distributed neural network training through online
  distillation.
\newblock {\em arXiv preprint arXiv:1804.03235}, 2018.

\bibitem{ben2010theory}
S.~Ben-David, J.~Blitzer, K.~Crammer, A.~Kulesza, F.~Pereira, and J.~W.
  Vaughan.
\newblock A theory of learning from different domains.
\newblock {\em Machine learning}, 79(1-2):151--175, 2010.

\bibitem{bengio2013estimating}
Y.~Bengio, N.~Léonard, and A.~Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation, 2013.

\bibitem{bonawitz2019federated}
K.~Bonawitz, H.~Eichner, W.~Grieskamp, D.~Huba, A.~Ingerman, V.~Ivanov,
  C.~Kiddon, J.~Konečný, S.~Mazzocchi, H.~B. McMahan, T.~V. Overveldt,
  D.~Petrou, D.~Ramage, and J.~Roselander.
\newblock Towards federated learning at scale: System design, 2019.

\bibitem{brock2018large}
A.~Brock, J.~Donahue, and K.~Simonyan.
\newblock Large scale {GAN} training for high fidelity natural image synthesis.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{bucilua2006model}
C.~Buciluǎ, R.~Caruana, and A.~Niculescu-Mizil.
\newblock Model compression.
\newblock In {\em Proceedings of the 12th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 535--541, 2006.

\bibitem{caldas2018leaf}
S.~Caldas, P.~Wu, T.~Li, J.~Kone{\v{c}}n{\`y}, H.~B. McMahan, V.~Smith, and
  A.~Talwalkar.
\newblock Leaf: A benchmark for federated settings.
\newblock {\em arXiv preprint arXiv:1812.01097}, 2018.

\bibitem{chang2019cronus}
H.~Chang, V.~Shejwalkar, R.~Shokri, and A.~Houmansadr.
\newblock Cronus: Robust and heterogeneous collaborative learning with
  black-box knowledge transfer.
\newblock {\em arXiv preprint arXiv:1912.11279}, 2019.

\bibitem{chen2020feddistill}
H.-Y. Chen and W.-L. Chao.
\newblock Feddistill: Making bayesian model ensemble applicable to federated
  learning.
\newblock {\em arXiv preprint arXiv:2009.01974}, 2020.

\bibitem{chrabaszcz2017downsampled}
P.~Chrabaszcz, I.~Loshchilov, and F.~Hutter.
\newblock A downsampled variant of imagenet as an alternative to the cifar
  datasets.
\newblock {\em arXiv preprint arXiv:1707.08819}, 2017.

\bibitem{deng2020adaptive}
Y.~Deng, M.~M. Kamani, and M.~Mahdavi.
\newblock Adaptive personalized federated learning.
\newblock {\em arXiv preprint arXiv:2003.13461}, 2020.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{Dvornik_2019_ICCV}
N.~Dvornik, C.~Schmid, and J.~Mairal.
\newblock Diversity with cooperation: Ensemble methods for few-shot
  classification.
\newblock In {\em The IEEE International Conference on Computer Vision (ICCV)},
  October 2019.

\bibitem{furlanello2018born}
T.~Furlanello, Z.~C. Lipton, M.~Tschannen, L.~Itti, and A.~Anandkumar.
\newblock Born again neural networks.
\newblock {\em arXiv preprint arXiv:1805.04770}, 2018.

\bibitem{geyer2017differentially}
R.~C. Geyer, T.~Klein, and M.~Nabi.
\newblock Differentially private federated learning: A client level
  perspective.
\newblock {\em arXiv preprint arXiv:1712.07557}, 2017.

\bibitem{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, pages
  2672--2680, 2014.

\bibitem{guha2019one}
N.~Guha, A.~Talwlkar, and V.~Smith.
\newblock One-shot federated learning.
\newblock {\em arXiv preprint arXiv:1902.11175}, 2019.

\bibitem{he2020group}
C.~He, S.~Avestimehr, and M.~Annavaram.
\newblock Group knowledge transfer: Collaborative training of large cnns on the
  edge.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hintonestimator}
G.~Hinton.
\newblock Neural networks for machine learning, 2012.

\bibitem{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{hoffman2018algorithms}
J.~Hoffman, M.~Mohri, and N.~Zhang.
\newblock Algorithms and theory for multiple-source adaptation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8246--8256, 2018.

\bibitem{hsieh2019non}
K.~Hsieh, A.~Phanishayee, O.~Mutlu, and P.~B. Gibbons.
\newblock The non-iid data quagmire of decentralized machine learning.
\newblock {\em arXiv preprint arXiv:1910.00189}, 2019.

\bibitem{hsu2019measuring}
T.-M.~H. Hsu, H.~Qi, and M.~Brown.
\newblock Measuring the effects of non-identical data distribution for
  federated visual classification.
\newblock {\em arXiv preprint arXiv:1909.06335}, 2019.

\bibitem{hsu2020federated}
T.-M.~H. Hsu, H.~Qi, and M.~Brown.
\newblock Federated visual classification with real-world data distribution.
\newblock In {\em European Conference on Computer Vision (ECCV)}, 2020.

\bibitem{huang2017densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem{huang2017like}
Z.~Huang and N.~Wang.
\newblock Like what you like: Knowledge distill via neuron selectivity
  transfer.
\newblock {\em arXiv preprint arXiv:1707.01219}, 2017.

\bibitem{hubara2016binarized}
I.~Hubara, M.~Courbariaux, D.~Soudry, R.~El-Yaniv, and Y.~Bengio.
\newblock Binarized neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  4107--4115, 2016.

\bibitem{hubara2017quantized}
I.~Hubara, M.~Courbariaux, D.~Soudry, R.~El-Yaniv, and Y.~Bengio.
\newblock Quantized neural networks: Training neural networks with low
  precision weights and activations.
\newblock {\em The Journal of Machine Learning Research}, 18(1):6869--6898,
  2017.

\bibitem{ioffe2015batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em arXiv preprint arXiv:1502.03167}, 2015.

\bibitem{izmailov2018averaging}
P.~Izmailov, D.~Podoprikhin, T.~Garipov, D.~Vetrov, and A.~G. Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In {\em Appears at the Conference on Uncertainty in Artificial
  Intelligence (UAI)}, 2018.

\bibitem{jeong2018communication}
E.~Jeong, S.~Oh, H.~Kim, J.~Park, M.~Bennis, and S.-L. Kim.
\newblock Communication-efficient on-device machine learning: Federated
  distillation and augmentation under non-iid private data.
\newblock {\em arXiv preprint arXiv:1811.11479}, 2018.

\bibitem{kairouz2019advances}
P.~Kairouz, H.~B. McMahan, B.~Avent, A.~Bellet, M.~Bennis, A.~N. Bhagoji,
  K.~Bonawitz, Z.~Charles, G.~Cormode, R.~Cummings, R.~G.~L. D'Oliveira, S.~E.
  Rouayheb, D.~Evans, J.~Gardner, Z.~Garrett, A.~Gascón, B.~Ghazi, P.~B.
  Gibbons, M.~Gruteser, Z.~Harchaoui, C.~He, L.~He, Z.~Huo, B.~Hutchinson,
  J.~Hsu, M.~Jaggi, T.~Javidi, G.~Joshi, M.~Khodak, J.~Konečný, A.~Korolova,
  F.~Koushanfar, S.~Koyejo, T.~Lepoint, Y.~Liu, P.~Mittal, M.~Mohri, R.~Nock,
  A.~Özgür, R.~Pagh, M.~Raykova, H.~Qi, D.~Ramage, R.~Raskar, D.~Song,
  W.~Song, S.~U. Stich, Z.~Sun, A.~T. Suresh, F.~Tramèr, P.~Vepakomma,
  J.~Wang, L.~Xiong, Z.~Xu, Q.~Yang, F.~X. Yu, H.~Yu, and S.~Zhao.
\newblock Advances and open problems in federated learning, 2019.

\bibitem{karimireddy2019scaffold}
S.~P. Karimireddy, S.~Kale, M.~Mohri, S.~J. Reddi, S.~U. Stich, and A.~T.
  Suresh.
\newblock Scaffold: Stochastic controlled averaging for on-device federated
  learning.
\newblock {\em arXiv preprint arXiv:1910.06378}, 2019.

\bibitem{kim2018paraphrasing}
J.~Kim, S.~Park, and N.~Kwak.
\newblock Paraphrasing complex network: Network compression via factor
  transfer.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2760--2769, 2018.

\bibitem{koratana19a}
A.~Koratana, D.~Kang, P.~Bailis, and M.~Zaharia.
\newblock {LIT}: Learned intermediate representation training for model
  compression.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, {\em Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of {\em
  Proceedings of Machine Learning Research}, pages 3509--3518, Long Beach,
  California, USA, 09--15 Jun 2019. PMLR.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem{kuncheva2003measures}
L.~I. Kuncheva and C.~J. Whitaker.
\newblock Measures of diversity in classifier ensembles and their relationship
  with the ensemble accuracy.
\newblock {\em Machine learning}, 51(2):181--207, 2003.

\bibitem{li2019fedmd}
D.~Li and J.~Wang.
\newblock Fedmd: Heterogenous federated learning via model distillation.
\newblock {\em arXiv preprint arXiv:1910.03581}, 2019.

\bibitem{li2019federated}
T.~Li, A.~K. Sahu, A.~Talwalkar, and V.~Smith.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock {\em arXiv preprint arXiv:1908.07873}, 2019.

\bibitem{li2018federated}
T.~Li, A.~K. Sahu, M.~Zaheer, M.~Sanjabi, A.~Talwalkar, and V.~Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock {\em arXiv preprint arXiv:1812.06127}, 2018.

\bibitem{li2020fair}
T.~Li, M.~Sanjabi, A.~Beirami, and V.~Smith.
\newblock Fair resource allocation in federated learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{Lin2020Dynamic}
T.~Lin, S.~U. Stich, L.~Barba, D.~Dmitriev, and M.~Jaggi.
\newblock Dynamic model pruning with feedback.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{lin2020dont}
T.~Lin, S.~U. Stich, K.~K. Patel, and M.~Jaggi.
\newblock Don't use large mini-batches, use local {SGD}.
\newblock In {\em ICLR - International Conference on Learning Representations},
  2020.

\bibitem{liu2019knowledge}
I.-J. Liu, J.~Peng, and A.~G. Schwing.
\newblock Knowledge flow: Improve upon your teachers.
\newblock {\em arXiv preprint arXiv:1904.05878}, 2019.

\bibitem{ma2018shufflenet}
N.~Ma, X.~Zhang, H.-T. Zheng, and J.~Sun.
\newblock Shufflenet v2: Practical guidelines for efficient cnn architecture
  design.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 116--131, 2018.

\bibitem{maddox2019simple}
W.~J. Maddox, P.~Izmailov, T.~Garipov, D.~P. Vetrov, and A.~G. Wilson.
\newblock A simple baseline for bayesian uncertainty in deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  13153--13164, 2019.

\bibitem{mansour2009domain}
Y.~Mansour, M.~Mohri, and A.~Rostamizadeh.
\newblock Domain adaptation with multiple sources.
\newblock In {\em Advances in neural information processing systems}, pages
  1041--1048, 2009.

\bibitem{mcmahan2016communication}
H.~B. McMahan, E.~Moore, D.~Ramage, S.~Hampson, et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock {\em arXiv preprint arXiv:1602.05629}, 2016.

\bibitem{micaelli2019zero}
P.~Micaelli and A.~J. Storkey.
\newblock Zero-shot knowledge transfer via adversarial belief matching.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9547--9557, 2019.

\bibitem{mohri2019agnostic}
M.~Mohri, G.~Sivek, and A.~T. Suresh.
\newblock Agnostic federated learning.
\newblock {\em arXiv preprint arXiv:1902.00146}, 2019.

\bibitem{nayak2019zero}
G.~K. Nayak, K.~R. Mopuri, V.~Shaj, R.~V. Babu, and A.~Chakraborty.
\newblock Zero-shot knowledge distillation in deep networks.
\newblock {\em arXiv preprint arXiv:1905.08114}, 2019.

\bibitem{nedic2020review}
A.~Nedic.
\newblock Distributed gradient methods for convex machine learning problems in
  networks: Distributed optimization.
\newblock {\em {IEEE} Signal Processing Magazine}, 37(3):92--101, 2020.

\bibitem{park2019feed}
S.~Park and N.~Kwak.
\newblock Feed: Feature-level ensemble for knowledge distillation.
\newblock {\em arXiv preprint arXiv:1909.10754}, 2019.

\bibitem{rastegari2016xnor}
M.~Rastegari, V.~Ordonez, J.~Redmon, and A.~Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In {\em European conference on computer vision}, pages 525--542.
  Springer, 2016.

\bibitem{reddi2020adaptive}
S.~Reddi, Z.~Charles, M.~Zaheer, Z.~Garrett, K.~Rush, J.~Kone{\v{c}}n{\`y},
  S.~Kumar, and H.~B. McMahan.
\newblock Adaptive federated optimization.
\newblock {\em arXiv preprint arXiv:2003.00295}, 2020.

\bibitem{romero2014fitnets}
A.~Romero, N.~Ballas, S.~E. Kahou, A.~Chassang, C.~Gatta, and Y.~Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{sanh2019distilbert}
V.~Sanh, L.~Debut, J.~Chaumond, and T.~Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{shalev2014understanding}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem{shoham2019overcoming}
N.~Shoham, T.~Avidor, A.~Keren, N.~Israel, D.~Benditkis, L.~Mor-Yosef, and
  I.~Zeitak.
\newblock Overcoming forgetting in federated learning on non-iid data.
\newblock {\em arXiv preprint arXiv:1910.07796}, 2019.

\bibitem{shokri2015privacy}
R.~Shokri and V.~Shmatikov.
\newblock Privacy-preserving deep learning.
\newblock In {\em Proceedings of the 22nd ACM SIGSAC conference on computer and
  communications security}, pages 1310--1321, 2015.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{singh2019model}
S.~P. Singh and M.~Jaggi.
\newblock Model fusion via optimal transport.
\newblock In {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{smith2017federated}
V.~Smith, C.-K. Chiang, M.~Sanjabi, and A.~S. Talwalkar.
\newblock Federated multi-task learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4424--4434, 2017.

\bibitem{socher-etal-2013-recursive}
R.~Socher, A.~Perelygin, J.~Wu, J.~Chuang, C.~D. Manning, A.~Ng, and C.~Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In {\em Proceedings of the 2013 Conference on Empirical Methods in
  Natural Language Processing}, pages 1631--1642, Seattle, Washington, USA,
  Oct. 2013. Association for Computational Linguistics.

\bibitem{sollich1996learning}
P.~Sollich and A.~Krogh.
\newblock Learning with ensembles: How overfitting can be useful.
\newblock In {\em Advances in neural information processing systems}, pages
  190--196, 1996.

\bibitem{sun2020federated}
L.~Sun and L.~Lyu.
\newblock Federated model distillation with noise-free differential privacy.
\newblock {\em arXiv preprint arXiv:2009.05537}, 2020.

\bibitem{tan2019efficientnet}
M.~Tan and Q.~V. Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock {\em arXiv preprint arXiv:1905.11946}, 2019.

\bibitem{tian2019contrastive}
Y.~Tian, D.~Krishnan, and P.~Isola.
\newblock Contrastive representation distillation.
\newblock {\em arXiv preprint arXiv:1910.10699}, 2019.

\bibitem{tung2019similarity}
F.~Tung and G.~Mori.
\newblock Similarity-preserving knowledge distillation.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1365--1374, 2019.

\bibitem{wang2019superglue}
A.~Wang, Y.~Pruksachatkun, N.~Nangia, A.~Singh, J.~Michael, F.~Hill, O.~Levy,
  and S.~Bowman.
\newblock Superglue: A stickier benchmark for general-purpose language
  understanding systems.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3261--3275, 2019.

\bibitem{wang2018glue}
A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In {\em Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}:
  Analyzing and Interpreting Neural Networks for {NLP}}, pages 353--355,
  Brussels, Belgium, Nov. 2018. Association for Computational Linguistics.

\bibitem{Wang2020Federated}
H.~Wang, M.~Yurochkin, Y.~Sun, D.~Papailiopoulos, and Y.~Khazaeni.
\newblock Federated learning with matched averaging.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{wu2019personid}
A.~{Wu}, W.~{Zheng}, X.~{Guo}, and J.~{Lai}.
\newblock Distilled person re-identification: Towards a more scalable system.
\newblock In {\em 2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2019.

\bibitem{wu2018group}
Y.~Wu and K.~He.
\newblock Group normalization.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 3--19, 2018.

\bibitem{Shan2017learning}
S.~You, C.~Xu, C.~Xu, and D.~Tao.
\newblock Learning from multiple teacher networks.
\newblock In {\em Proceedings of the 23rd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, KDD ’17, page 1285–1294, New
  York, NY, USA, 2017. Association for Computing Machinery.

\bibitem{yurochkin2019bayesian}
M.~Yurochkin, M.~Agarwal, S.~Ghosh, K.~Greenewald, T.~N. Hoang, and
  Y.~Khazaeni.
\newblock Bayesian nonparametric federated learning of neural networks.
\newblock {\em arXiv preprint arXiv:1905.12022}, 2019.

\bibitem{zagoruyko2016paying}
S.~Zagoruyko and N.~Komodakis.
\newblock Paying more attention to attention: Improving the performance of
  convolutional neural networks via attention transfer.
\newblock {\em arXiv preprint arXiv:1612.03928}, 2016.

\bibitem{zhang2015character}
X.~Zhang, J.~Zhao, and Y.~LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock In {\em Advances in neural information processing systems}, pages
  649--657, 2015.

\bibitem{zhou2020distilled}
Y.~Zhou, G.~Pu, X.~Ma, X.~Li, and D.~Wu.
\newblock Distilled one-shot federated learning.
\newblock {\em 2009.07999}, 2020.

\end{thebibliography}
