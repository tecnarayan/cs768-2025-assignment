\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016{\natexlab{a}})Abadi, Barham, Chen, Chen, Davis,
  Dean, Devin, Ghemawat, Irving, Isard, Kudlur, Levenberg, Monga, Moore,
  Murray, Steiner, Tucker, Vasudevan, Warden, Wicke, Yu, and
  Zheng]{Abadi:2016:TSL:3026877.3026899}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R.,
  Moore, S., Murray, D.~G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P.,
  Wicke, M., Yu, Y., and Zheng, X.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{Proceedings of the 12th USENIX Conference on Operating
  Systems Design and Implementation}, OSDI'16, pp.\  265--283, Berkeley, CA,
  USA, 2016{\natexlab{a}}. USENIX Association.
\newblock ISBN 978-1-931971-33-1.

\bibitem[Abadi et~al.(2016{\natexlab{b}})Abadi, Barham, Chen, Chen, Davis,
  Dean, Devin, Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al.
\newblock Tensorflow: a system for large-scale machine learning.
\newblock In \emph{OSDI}, volume~16, pp.\  265--283, 2016{\natexlab{b}}.

\bibitem[Agarwal \& Duchi(2011)Agarwal and Duchi]{NIPS2011_4247}
Agarwal, A. and Duchi, J.~C.
\newblock Distributed delayed stochastic optimization.
\newblock In Shawe-Taylor, J., Zemel, R.~S., Bartlett, P.~L., Pereira, F., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems 24}, pp.\  873--881. Curran Associates, Inc., 2011.

\bibitem[Agarwal et~al.(2018)Agarwal, Suresh, Yu, Kumar, and
  McMahan]{NIPS2018_7984}
Agarwal, N., Suresh, A.~T., Yu, F. X.~X., Kumar, S., and McMahan, B.
\newblock cpsgd: Communication-efficient and differentially-private distributed
  sgd.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  7575--7586. Curran Associates,
  Inc., 2018.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{DBLP:conf/nips/AlistarhG0TV17}
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M.
\newblock {QSGD:} communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H.~M., Fergus,
  R., Vishwanathan, S. V.~N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30: Annual Conference on Neural Information
  Processing Systems 2017, 4-9 December 2017, Long Beach, CA, {USA}}, pp.\
  1707--1718, 2017.

\bibitem[Alistarh et~al.(2018)Alistarh, Hoefler, Johansson, Konstantinov,
  Khirirat, and Renggli]{NIPS2018_7837}
Alistarh, D., Hoefler, T., Johansson, M., Konstantinov, N., Khirirat, S., and
  Renggli, C.
\newblock The convergence of sparsified gradient methods.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  5977--5987. Curran Associates,
  Inc., 2018.

\bibitem[Bernstein et~al.(2018{\natexlab{a}})Bernstein, Wang, Azizzadenesheli,
  and Anandkumar]{Bernstein2018signSGDCO}
Bernstein, J., Wang, Y.-X., Azizzadenesheli, K., and Anandkumar, A.
\newblock signsgd: compressed optimisation for non-convex problems.
\newblock In \emph{ICML}, 2018{\natexlab{a}}.

\bibitem[Bernstein et~al.(2018{\natexlab{b}})Bernstein, Zhao, Azizzadenesheli,
  and Anandkumar]{Bernstein:2018aa}
Bernstein, J., Zhao, J., Azizzadenesheli, K., and Anandkumar, A.
\newblock signsgd with majority vote is communication efficient and byzantine
  fault tolerant.
\newblock 10 2018{\natexlab{b}}.

\bibitem[Bottou \& Bottou(2010)Bottou and Bottou]{Bottou:2010aa}
Bottou, L. and Bottou, L.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock \emph{IN COMPSTAT}, 2010.
\newblock \doi{10.1.1.419.462}.

\bibitem[Chen(2018)]{Chen:2018aa}
Chen, C.-Y.
\newblock \emph{AdaComp : Adaptive Residual Gradient Compression for
  Data-Parallel Distributed Training.; AAAI}.
\newblock 2018.

\bibitem[Chen et~al.(2018)Chen, Giannakis, Sun, and Yin]{NIPS2018_7752}
Chen, T., Giannakis, G., Sun, T., and Yin, W.
\newblock Lag: Lazily aggregated gradient for communication-efficient
  distributed learning.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  5055--5065. Curran Associates,
  Inc., 2018.

\bibitem[Cutkosky \& Busa-Fekete(2018)Cutkosky and Busa-Fekete]{NIPS2018_7461}
Cutkosky, A. and Busa-Fekete, R.
\newblock Distributed stochastic optimization via adaptive sgd.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  1914--1923. Curran Associates,
  Inc., 2018.

\bibitem[Garber et~al.(2017)Garber, Shamir, and Srebro]{pmlr-v70-garber17a}
Garber, D., Shamir, O., and Srebro, N.
\newblock Communication-efficient algorithms for distributed stochastic
  principal component analysis.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1203--1212, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2018{\natexlab{a}})He, Bian, and Jaggi]{NIPS2018_7705}
He, L., Bian, A., and Jaggi, M.
\newblock Cola: Decentralized linear learning.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  4541--4551. Curran Associates,
  Inc., 2018{\natexlab{a}}.

\bibitem[He et~al.(2018{\natexlab{b}})He, Bian, and Jaggi]{he2018cola}
He, L., Bian, A., and Jaggi, M.
\newblock Cola: Decentralized linear learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4541--4551, 2018{\natexlab{b}}.

\bibitem[Hong et~al.(2017)Hong, Hajinezhad, and Zhao]{pmlr-v70-hong17a}
Hong, M., Hajinezhad, D., and Zhao, M.-M.
\newblock Prox-{PDA}: The proximal primal-dual algorithm for fast distributed
  nonconvex optimization and learning over networks.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1529--1538, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[Huo et~al.(2018)Huo, Gu, qian Yang, and Huang]{pmlr-v80-huo18a}
Huo, Z., Gu, B., qian Yang, and Huang, H.
\newblock Decoupled parallel backpropagation with convergence guarantee.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  2098--2106, Stockholmsm{\"a}ssan,
  Stockholm Sweden, 10--15 Jul 2018. PMLR.

\bibitem[Jayaraman et~al.(2018)Jayaraman, Wang, Evans, and Gu]{NIPS2018_7871}
Jayaraman, B., Wang, L., Evans, D., and Gu, Q.
\newblock Distributed learning without distress: Privacy-preserving empirical
  risk minimization.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  6346--6357. Curran Associates,
  Inc., 2018.

\bibitem[Jiang \& Agrawal(2018)Jiang and Agrawal]{NIPS2018_7519}
Jiang, P. and Agrawal, G.
\newblock A linear speedup analysis of distributed deep learning with sparse
  and quantized communication.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  2530--2541. Curran Associates,
  Inc., 2018.

\bibitem[Jin et~al.(2016)Jin, Yuan, Iandola, and Keutzer]{jin2016scale}
Jin, P.~H., Yuan, Q., Iandola, F., and Keutzer, K.
\newblock How to scale distributed deep learning?
\newblock \emph{arXiv preprint arXiv:1611.04581}, 2016.

\bibitem[Li et~al.(2014)Li, Andersen, Park, Smola, Ahmed, Josifovski, Long,
  Shekita, and Su]{li2014scaling}
Li, M., Andersen, D.~G., Park, J.~W., Smola, A.~J., Ahmed, A., Josifovski, V.,
  Long, J., Shekita, E.~J., and Su, B.-Y.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{OSDI}, volume~14, pp.\  583--598, 2014.

\bibitem[Li et~al.(2018)Li, Yu, Li, Avestimehr, Kim, and
  Schwing]{NIPS2018_8028}
Li, Y., Yu, M., Li, S., Avestimehr, S., Kim, N.~S., and Schwing, A.
\newblock Pipe-sgd: A decentralized pipelined sgd framework for distributed
  deep net training.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  8056--8067. Curran Associates,
  Inc., 2018.

\bibitem[Lian et~al.(2015)Lian, Huang, Li, and Liu]{NIPS2015_5751}
Lian, X., Huang, Y., Li, Y., and Liu, J.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In Cortes, C., Lawrence, N.~D., Lee, D.~D., Sugiyama, M., and
  Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems
  28}, pp.\  2737--2745. Curran Associates, Inc., 2015.

\bibitem[Lian et~al.(2017{\natexlab{a}})Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{NIPS2017_7117}
Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30}, pp.\  5330--5340. Curran Associates,
  Inc., 2017{\natexlab{a}}.

\bibitem[Lian et~al.(2017{\natexlab{b}})Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5330--5340, 2017{\natexlab{b}}.

\bibitem[Nedi{\'c} \& Olshevsky(2015)Nedi{\'c} and
  Olshevsky]{nedic2015distributed}
Nedi{\'c}, A. and Olshevsky, A.
\newblock Distributed optimization over time-varying directed graphs.
\newblock \emph{IEEE Transactions on Automatic Control}, 60\penalty0
  (3):\penalty0 601--615, 2015.

\bibitem[Nedic et~al.(2017)Nedic, Olshevsky, and Shi]{nedic2017achieving}
Nedic, A., Olshevsky, A., and Shi, W.
\newblock Achieving geometric convergence for distributed optimization over
  time-varying graphs.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2597--2633, 2017.

\bibitem[Recht et~al.(2011)Recht, Re, Wright, and Niu]{NIPS2011_4390}
Recht, B., Re, C., Wright, S., and Niu, F.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In Shawe-Taylor, J., Zemel, R.~S., Bartlett, P.~L., Pereira, F., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems 24}, pp.\  693--701. Curran Associates, Inc., 2011.

\bibitem[Renggli et~al.(2018)Renggli, Alistarh, and
  Hoefler]{renggli2018sparcml}
Renggli, C., Alistarh, D., and Hoefler, T.
\newblock Sparcml: High-performance sparse communication for machine learning.
\newblock \emph{arXiv preprint arXiv:1802.08021}, 2018.

\bibitem[Saparbayeva et~al.(2018)Saparbayeva, Zhang, and Lin]{NIPS2018_7616}
Saparbayeva, B., Zhang, M., and Lin, L.
\newblock Communication efficient parallel algorithms for optimization on
  manifolds.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  3578--3588. Curran Associates,
  Inc., 2018.

\bibitem[Scaman et~al.(2018)Scaman, Bach, Bubeck, Massouli\'{e}, and
  Lee]{NIPS2018_7539}
Scaman, K., Bach, F., Bubeck, S., Massouli\'{e}, L., and Lee, Y.~T.
\newblock Optimal algorithms for non-smooth distributed optimization in
  networks.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  2745--2754. Curran Associates,
  Inc., 2018.

\bibitem[Seide \& Agarwal(2016)Seide and Agarwal]{seide2016cntk}
Seide, F. and Agarwal, A.
\newblock Cntk: Microsoft's open-source deep-learning toolkit.
\newblock In \emph{Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pp.\  2135--2135. ACM, 2016.

\bibitem[Seide et~al.(2014)Seide, Fu, Droppo, Li, and Yu]{1-bitexp}
Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D.
\newblock 1-bit stochastic gradient descent and application to data-parallel
  distributed training of speech dnns.
\newblock In \emph{Interspeech 2014}, September 2014.

\bibitem[Shen et~al.(2018)Shen, Mokhtari, Zhou, Zhao, and
  Qian]{pmlr-v80-shen18a}
Shen, Z., Mokhtari, A., Zhou, T., Zhao, P., and Qian, H.
\newblock Towards more efficient stochastic decentralized learning: Faster
  convergence and sparse communication.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  4624--4633, Stockholmsm{\"a}ssan,
  Stockholm Sweden, 10--15 Jul 2018. PMLR.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{NIPS2018_7697}
Stich, S.~U., Cordonnier, J.-B., and Jaggi, M.
\newblock Sparsified sgd with memory.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  4452--4463. Curran Associates,
  Inc., 2018.

\bibitem[Suresh et~al.(2017)Suresh, Yu, Kumar, and McMahan]{pmlr-v70-suresh17a}
Suresh, A.~T., Yu, F.~X., Kumar, S., and McMahan, H.~B.
\newblock Distributed mean estimation with limited communication.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  3329--3337, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[Tang et~al.(2018{\natexlab{a}})Tang, Gan, Zhang, Zhang, and
  Liu]{NIPS2018_7992}
Tang, H., Gan, S., Zhang, C., Zhang, T., and Liu, J.
\newblock Communication compression for decentralized training.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  7663--7673. Curran Associates,
  Inc., 2018{\natexlab{a}}.

\bibitem[Tang et~al.(2018{\natexlab{b}})Tang, Lian, Yan, Zhang, and
  Liu]{pmlr-v80-tang18a}
Tang, H., Lian, X., Yan, M., Zhang, C., and Liu, J.
\newblock $d^2$: Decentralized training over decentralized data.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  4848--4856, Stockholmsm{\"a}ssan,
  Stockholm Sweden, 10--15 Jul 2018{\natexlab{b}}. PMLR.

\bibitem[Tang et~al.(2018{\natexlab{c}})Tang, Lian, Yan, Zhang, and
  Liu]{tang2018d}
Tang, H., Lian, X., Yan, M., Zhang, C., and Liu, J.
\newblock D2: Decentralized training over decentralized data.
\newblock \emph{arXiv preprint arXiv:1803.07068}, 2018{\natexlab{c}}.

\bibitem[Thakur et~al.(2005)Thakur, Rabenseifner, and
  Gropp]{thakur2005optimization}
Thakur, R., Rabenseifner, R., and Gropp, W.
\newblock Optimization of collective communication operations in mpich.
\newblock \emph{The International Journal of High Performance Computing
  Applications}, 19\penalty0 (1):\penalty0 49--66, 2005.

\bibitem[Wang et~al.(2018)Wang, Sievert, Liu, Charles, Papailiopoulos, and
  Wright]{NIPS2018_8191}
Wang, H., Sievert, S., Liu, S., Charles, Z., Papailiopoulos, D., and Wright, S.
\newblock Atomo: Communication-efficient learning via atomic sparsification.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  9872--9883. Curran Associates,
  Inc., 2018.

\bibitem[Wang et~al.(2017)Wang, Kolar, Srebro, and Zhang]{pmlr-v70-wang17f}
Wang, J., Kolar, M., Srebro, N., and Zhang, T.
\newblock Efficient distributed learning with sparsity.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  3636--3645, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[Wangni et~al.(2018)Wangni, Wang, Liu, and Zhang]{NIPS2018_7405}
Wangni, J., Wang, J., Liu, J., and Zhang, T.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  1306--1316. Curran Associates,
  Inc., 2018.

\bibitem[Wen et~al.(2017)Wen, Xu, Yan, Wu, Wang, Chen, and Li]{NIPS2017_6749}
Wen, W., Xu, C., Yan, F., Wu, C., Wang, Y., Chen, Y., and Li, H.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30}, pp.\  1509--1519. Curran Associates,
  Inc., 2017.

\bibitem[Wu et~al.(2018)Wu, Huang, Huang, and Zhang]{pmlr-v80-wu18d}
Wu, J., Huang, W., Huang, J., and Zhang, T.
\newblock Error compensated quantized {SGD} and its applications to large-scale
  distributed optimization.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  5325--5333, Stockholmsm{\"a}ssan,
  Stockholm Sweden, 10--15 Jul 2018. PMLR.

\bibitem[Xu et~al.(2017)Xu, Taylor, Li, Figueiredo, Yuan, and
  Goldstein]{pmlr-v70-xu17c}
Xu, Z., Taylor, G., Li, H., Figueiredo, M. A.~T., Yuan, X., and Goldstein, T.
\newblock Adaptive consensus {ADMM} for distributed optimization.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  3841--3850, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[Zhang et~al.(2017{\natexlab{a}})Zhang, Li, Kara, Alistarh, Liu, and
  Zhang]{pmlr-v70-zhang17e}
Zhang, H., Li, J., Kara, K., Alistarh, D., Liu, J., and Zhang, C.
\newblock {Z}ip{ML}: Training linear models with end-to-end low precision, and
  a little bit of deep learning.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  4035--4043, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017{\natexlab{a}}. PMLR.

\bibitem[Zhang et~al.(2017{\natexlab{b}})Zhang, Zhao, Zhu, Hoi, and
  Zhang]{pmlr-v70-zhang17g}
Zhang, W., Zhao, P., Zhu, W., Hoi, S. C.~H., and Zhang, T.
\newblock Projection-free distributed online learning in networks.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  4054--4062, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017{\natexlab{b}}. PMLR.

\bibitem[Zhang et~al.(2018)Zhang, Khalili, and Liu]{pmlr-v80-zhang18f}
Zhang, X., Khalili, M.~M., and Liu, M.
\newblock Improving the privacy and accuracy of {ADMM}-based distributed
  algorithms.
\newblock In Dy, J. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of \emph{Proceedings
  of Machine Learning Research}, pp.\  5796--5805, Stockholmsm{\"a}ssan,
  Stockholm Sweden, 10--15 Jul 2018. PMLR.

\end{thebibliography}
