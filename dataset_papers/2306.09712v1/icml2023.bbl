\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bahdanau et~al.(2016)Bahdanau, Brakel, Xu, Goyal, Lowe, Pineau,
  Courville, and Bengio]{bahdanau2016actor}
Bahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville,
  A., and Bengio, Y.
\newblock An actor-critic algorithm for sequence prediction.
\newblock In \emph{ICLR}, 2016.

\bibitem[Banerjee \& Lavie(2005)Banerjee and Lavie]{Banerjee2005METEORAA}
Banerjee, S. and Lavie, A.
\newblock Meteor: An automatic metric for mt evaluation with improved
  correlation with human judgments.
\newblock In \emph{IEEvaluation@ACL}, 2005.

\bibitem[Bengio et~al.(2015)Bengio, Vinyals, Jaitly, and
  Shazeer]{bengio2015scheduled}
Bengio, S., Vinyals, O., Jaitly, N., and Shazeer, N.
\newblock Scheduled sampling for sequence prediction with recurrent neural
  networks.
\newblock \emph{NeurIPS}, 28, 2015.

\bibitem[Chen et~al.(2022)Chen, Wang, Yi, Wu, Xie, and
  Yan]{Chen2022PersonalizedCG}
Chen, C.~H., Wang, X., Yi, X., Wu, F., Xie, X., and Yan, R.
\newblock Personalized chit-chat generation for recommendation using external
  chat corpora.
\newblock \emph{KDD}, 2022.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL}, 2019.

\bibitem[Ding \& Soricut(2017)Ding and Soricut]{spg}
Ding, N. and Soricut, R.
\newblock Cold-start reinforcement learning with softmax policy gradient.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Feng et~al.(2022)Feng, Lian, Wang, Liu, Xie, and
  Chen]{Feng2022ReinforcementRO}
Feng, C., Lian, D., Wang, X., Liu, Z., Xie, X., and Chen, E.
\newblock Reinforcement routing on proximity graph for efficient
  recommendation.
\newblock \emph{TOIS}, 41:\penalty0 1--27, 2022.

\bibitem[Fran{\c{c}}ois-Lavet et~al.(2019)Fran{\c{c}}ois-Lavet, Rabusseau,
  Pineau, Ernst, and Fonteneau]{franccois2019overfitting}
Fran{\c{c}}ois-Lavet, V., Rabusseau, G., Pineau, J., Ernst, D., and Fonteneau,
  R.
\newblock On overfitting and asymptotic bias in batch reinforcement learning
  with partial observability.
\newblock \emph{JAIR}, 65:\penalty0 1--30, 2019.

\bibitem[Gliwa et~al.(2019)Gliwa, Mochol, Biesek, and
  Wawer]{gliwa-etal-2019-samsum}
Gliwa, B., Mochol, I., Biesek, M., and Wawer, A.
\newblock Samsum corpus: A human-annotated dialogue dataset for abstractive
  summarization.
\newblock \emph{EMNLP-IJCNLP 2019}, pp.\ ~70, 2019.

\bibitem[Goodrich et~al.(2019)Goodrich, Rao, Liu, and
  Saleh]{goodrich2019assessing}
Goodrich, B., Rao, V., Liu, P.~J., and Saleh, M.
\newblock Assessing the factual accuracy of generated text.
\newblock In \emph{KDD}, pp.\  166--175, 2019.

\bibitem[Hermann et~al.(2015)Hermann, Kocisky, Grefenstette, Espeholt, Kay,
  Suleyman, and Blunsom]{cnn}
Hermann, K.~M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman,
  M., and Blunsom, P.
\newblock Teaching machines to read and comprehend.
\newblock \emph{NeurIPS}, 28, 2015.

\bibitem[Hyun et~al.(2022)Hyun, Wang, Park, Xie, and Yu]{Hyun2022GeneratingMS}
Hyun, D., Wang, X., Park, C., Xie, X., and Yu, H.
\newblock Generating multiple-length summaries via reinforcement learning for
  unsupervised sentence summarization.
\newblock In \emph{EMNLP}, 2022.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2019way}
Jaques, N., Ghandeharioun, A., Shen, J.~H., Ferguson, C., Lapedriza, A., Jones,
  N., Gu, S., and Picard, R.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \emph{arXiv:1907.00456}, 2019.

\bibitem[Konda \& Tsitsiklis(1999)Konda and Tsitsiklis]{konda1999actor}
Konda, V. and Tsitsiklis, J.
\newblock Actor-critic algorithms.
\newblock \emph{NeurIPS}, 12, 1999.

\bibitem[Laban et~al.(2021)Laban, Schnabel, Bennett, and Hearst]{factmodel}
Laban, P., Schnabel, T., Bennett, P.~N., and Hearst, M.~A.
\newblock Summac: Re-visiting nli-based models for inconsistency detection in
  summarization.
\newblock \emph{ACL}, 10:\penalty0 163--177, 2021.

\bibitem[Le et~al.(2022)Le, Wang, Gotmare, Savarese, and Hoi]{Le2022CodeRLMC}
Le, H., Wang, Y., Gotmare, A.~D., Savarese, S., and Hoi, S.
\newblock Code{RL}: Mastering code generation through pretrained models and
  deep reinforcement learning.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{NeurIPS}, 2022.

\bibitem[Lewis et~al.(2020)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{lewis2020bart}
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O.,
  Stoyanov, V., and Zettlemoyer, L.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In \emph{ACL}, pp.\  7871--7880, 2020.

\bibitem[Li et~al.(2019)Li, Lei, Qin, and Wang]{selfcriticranker}
Li, S., Lei, D., Qin, P., and Wang, W.~Y.
\newblock Deep reinforcement learning with distributional semantic rewards for
  abstractive summarization.
\newblock In \emph{EMNLP}, pp.\  6038--6044, 2019.

\bibitem[Lin \& Hovy(2003)Lin and Hovy]{lin2003automatic}
Lin, C.-Y. and Hovy, E.
\newblock Automatic evaluation of summaries using n-gram co-occurrence
  statistics.
\newblock In \emph{HLT-NAACL}, pp.\  150--157, 2003.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv:1907.11692}, 2019.

\bibitem[Liu et~al.(2022)Liu, Liu, Radev, and Neubig]{liu2022brio}
Liu, Y., Liu, P., Radev, D., and Neubig, G.
\newblock Brio: Bringing order to abstractive summarization.
\newblock In \emph{ACL}, pp.\  2890--2903, 2022.

\bibitem[Mihaylova \& Martins(2019)Mihaylova and
  Martins]{mihaylova2019scheduled}
Mihaylova, T. and Martins, A.~F.
\newblock Scheduled sampling for transformers.
\newblock \emph{ACL 2019}, pp.\  351, 2019.

\bibitem[Narayan et~al.(2018)Narayan, Cohen, and Lapata]{narayan2018don}
Narayan, S., Cohen, S.~B., and Lapata, M.
\newblock Don’t give me the details, just the summary! topic-aware
  convolutional neural networks for extreme summarization.
\newblock In \emph{EMNLP}, pp.\  1797--1807, 2018.

\bibitem[Norouzi et~al.(2016)Norouzi, Bengio, Jaitly, Schuster, Wu, Schuurmans,
  et~al.]{norouzi2016reward}
Norouzi, M., Bengio, S., Jaitly, N., Schuster, M., Wu, Y., Schuurmans, D.,
  et~al.
\newblock Reward augmented maximum likelihood for neural structured prediction.
\newblock \emph{NeurIPS}, 29, 2016.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyangtraining}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang,
  C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{NeurIPS}, 35:\penalty0 27730--27744, 2022.

\bibitem[Pang \& He(2020)Pang and He]{pang2020text}
Pang, R.~Y. and He, H.
\newblock Text generation by learning from demonstrations.
\newblock In \emph{ICLR}, 2020.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu]{papineni2002bleu}
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{ACL}, pp.\  311--318, 2002.

\bibitem[Paulus et~al.(2018)Paulus, Xiong, and Socher]{selfcritic}
Paulus, R., Xiong, C., and Socher, R.
\newblock A deep reinforced model for abstractive summarization.
\newblock In \emph{ICLR}, 2018.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, Liu, et~al.]{t5}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., Liu, P.~J., et~al.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{SQuAD}
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock In \emph{EMNLP}, pp.\  2383--2392, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv:1707.06347}, 2017.

\bibitem[Serban et~al.(2017)Serban, Sankar, Germain, Zhang, Lin, Subramanian,
  Kim, Pieper, Chandar, Ke, et~al.]{serban2017deep}
Serban, I.~V., Sankar, C., Germain, M., Zhang, S., Lin, Z., Subramanian, S.,
  Kim, T., Pieper, M., Chandar, S., Ke, N.~R., et~al.
\newblock A deep reinforcement learning chatbot.
\newblock \emph{arXiv:1709.02349}, 2017.

\bibitem[Snell et~al.(2022)Snell, Kostrikov, Su, Yang, and
  Levine]{snell2022offline}
Snell, C., Kostrikov, I., Su, Y., Yang, M., and Levine, S.
\newblock Offline rl for natural language generation with implicit language q
  learning.
\newblock \emph{arXiv preprint arXiv:2206.11871}, 2022.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{sutton1999policy}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock \emph{NeurIPS}, 12, 1999.

\bibitem[Tan et~al.(2018)Tan, Hu, Yang, Salakhutdinov, and Xing]{erpo}
Tan, B., Hu, Z., Yang, Z., Salakhutdinov, R., and Xing, E.~P.
\newblock Connecting the dots between mle and rl for sequence generation.
\newblock \emph{ArXiv}, abs/1811.09740, 2018.

\bibitem[Ushio et~al.(2022)Ushio, Alva-Manchego, and
  Camacho-Collados]{ushio-etal-2022-generative}
Ushio, A., Alva-Manchego, F., and Camacho-Collados, J.
\newblock {G}enerative {L}anguage {M}odels for {P}aragraph-{L}evel {Q}uestion
  {G}eneration.
\newblock In \emph{EMNLP}, Abu Dhabi, U.A.E., December 2022.

\bibitem[Vijayakumar et~al.(2016)Vijayakumar, Cogswell, Selvaraju, Sun, Lee,
  Crandall, and Batra]{Vijayakumar2016DiverseBS}
Vijayakumar, A.~K., Cogswell, M., Selvaraju, R.~R., Sun, Q., Lee, S., Crandall,
  D.~J., and Batra, D.
\newblock Diverse beam search: Decoding diverse solutions from neural sequence
  models.
\newblock \emph{ArXiv}, abs/1610.02424, 2016.

\bibitem[Wang et~al.(2018)Wang, Chen, Yang, Wu, Wu, and Xie]{Wang2018ARL}
Wang, X., Chen, Y., Yang, J., Wu, L., Wu, Z., and Xie, X.
\newblock A reinforcement learning framework for explainable recommendation.
\newblock \emph{ICDM}, pp.\  587--596, 2018.

\bibitem[Wang et~al.(2021)Wang, Gu, Cao, Zhao, Yan, Middha, and Xie]{umpg}
Wang, X., Gu, X., Cao, J., Zhao, Z., Yan, Y., Middha, B., and Xie, X.
\newblock Reinforcing pretrained models for generating attractive text
  advertisements.
\newblock In \emph{KDD}, pp.\  3697–3707, 2021.

\bibitem[Wang et~al.(2022)Wang, Liu, Wang, Wu, Fu, and
  Xie]{Wang2022MultilevelRR}
Wang, X., Liu, K., Wang, D., Wu, L., Fu, Y., and Xie, X.
\newblock Multi-level recommendation reasoning over knowledge graphs with
  reinforcement learning.
\newblock \emph{Proceedings of the ACM Web Conference 2022}, 2022.

\bibitem[Williams(1992)]{williams1992policygradient}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 229--256, 1992.

\bibitem[Yang et~al.(2022)Yang, Wang, Jin, Li, Lian, and
  Xie]{Yang2022ReinforcementSR}
Yang, R., Wang, X., Jin, Y., Li, C., Lian, J., and Xie, X.
\newblock Reinforcement subgraph reasoning for fake news detection.
\newblock \emph{KDD}, 2022.

\bibitem[Zhang et~al.(2020)Zhang, Zhao, Saleh, and Liu]{zhang2020pegasus}
Zhang, J., Zhao, Y., Saleh, M., and Liu, P.
\newblock Pegasus: Pre-training with extracted gap-sentences for abstractive
  summarization.
\newblock In \emph{ICML}, pp.\  11328--11339. PMLR, 2020.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Kishore, Wu, Weinberger, and
  Artzi]{bertscore}
Zhang, T., Kishore, V., Wu, F., Weinberger, K.~Q., and Artzi, Y.
\newblock Bertscore: Evaluating text generation with bert.
\newblock \emph{ArXiv}, abs/1904.09675, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Pu, Zhuang, Zhou, and
  Li]{zhang2019continuous}
Zhang, Z., Pu, J., Zhuang, L., Zhou, W., and Li, H.
\newblock Continuous sign language recognition via reinforcement learning.
\newblock In \emph{ICIP}, pp.\  285--289. IEEE, 2019{\natexlab{b}}.

\bibitem[Zhao et~al.(2020)Zhao, Wang, Zhang, Zhao, Liu, Xing, and
  Xie]{Zhao2020LeveragingDF}
Zhao, K., Wang, X., Zhang, Y., Zhao, L., Liu, Z., Xing, C., and Xie, X.
\newblock Leveraging demonstrations for reinforcement recommendation reasoning
  over knowledge graphs.
\newblock \emph{SIGIR}, 2020.

\bibitem[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang,
  Dong, et~al.]{zhao2023survey}
Zhao, W.~X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B.,
  Zhang, J., Dong, Z., et~al.
\newblock A survey of large language models.
\newblock \emph{arXiv preprint arXiv:2303.18223}, 2023.

\bibitem[Zhu et~al.(2023)Zhu, Dang, and Grover]{zhuscaling}
Zhu, B., Dang, M., and Grover, A.
\newblock Scaling pareto-efficient decision making via offline multi-objective
  rl.
\newblock In \emph{ICLR}, 2023.

\end{thebibliography}
