\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[AI(2024)]{llama3}
Meta AI.
\newblock Introducing meta llama 3: The most capable openly available llm to date, 2024.
\newblock URL \url{https://ai.meta.com/blog/meta-llama-3/}.

\bibitem[Bai et~al.(2022)Bai, Kadavath, Kundu, Askell, Kernion, Jones, Chen, Goldie, Mirhoseini, McKinnon, et~al.]{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock \emph{arXiv preprint arXiv:2212.08073}, 2022.

\bibitem[Bianchi et~al.(2023)Bianchi, Suzgun, Attanasio, R{\"o}ttger, Jurafsky, Hashimoto, and Zou]{bianchi2023safety}
Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R{\"o}ttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou.
\newblock Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions.
\newblock \emph{arXiv preprint arXiv:2309.07875}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Bsharat et~al.(2023)Bsharat, Myrzakhan, and Shen]{bsharat2023principled}
Sondos~Mahmoud Bsharat, Aidar Myrzakhan, and Zhiqiang Shen.
\newblock Principled instructions are all you need for questioning llama-1/2, gpt-3.5/4.
\newblock \emph{arXiv preprint arXiv:2312.16171}, 2023.

\bibitem[Carlini et~al.(2024)Carlini, Nasr, Choquette-Choo, Jagielski, Gao, Koh, Ippolito, Tramer, and Schmidt]{carlini2024aligned}
Nicholas Carlini, Milad Nasr, Christopher~A Choquette-Choo, Matthew Jagielski, Irena Gao, Pang Wei~W Koh, Daphne Ippolito, Florian Tramer, and Ludwig Schmidt.
\newblock Are aligned neural networks adversarially aligned?
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Chao et~al.(2023)Chao, Robey, Dobriban, Hassani, Pappas, and Wong]{chao2023jailbreaking}
Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J Pappas, and Eric Wong.
\newblock Jailbreaking black box large language models in twenty queries.
\newblock \emph{arXiv preprint arXiv:2310.08419}, 2023.

\bibitem[Chao et~al.(2024)Chao, Debenedetti, Robey, Andriushchenko, Croce, Sehwag, Dobriban, Flammarion, Pappas, Tramer, et~al.]{chao2024jailbreakbench}
Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George~J Pappas, Florian Tramer, et~al.
\newblock Jailbreakbench: An open robustness benchmark for jailbreaking large language models.
\newblock \emph{arXiv preprint arXiv:2404.01318}, 2024.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dinan et~al.(2019)Dinan, Humeau, Chintagunta, and Weston]{dinan2019build}
Emily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston.
\newblock Build it break it fix it for dialogue safety: Robustness from adversarial human attack.
\newblock \emph{arXiv preprint arXiv:1908.06083}, 2019.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{International Conference on Machine Learning}, pages 1019--1028. PMLR, 2017.

\bibitem[Ganguli et~al.(2022)Ganguli, Lovitt, Kernion, Askell, Bai, Kadavath, Mann, Perez, Schiefer, Ndousse, et~al.]{ganguli2022red}
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et~al.
\newblock Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.
\newblock \emph{arXiv preprint arXiv:2209.07858}, 2022.

\bibitem[Goldstein and Studer(2018)]{goldstein2018phasemax}
Tom Goldstein and Christoph Studer.
\newblock Phasemax: Convex phase retrieval via basis pursuit.
\newblock \emph{IEEE Transactions on Information Theory}, 64\penalty0 (4):\penalty0 2675--2689, 2018.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi]{holtzman2019curious}
Ari Holtzman, Jan Buys, Li~Du, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock \emph{arXiv preprint arXiv:1904.09751}, 2019.

\bibitem[Howard and Ruder(2018)]{howard2018universal}
Jeremy Howard and Sebastian Ruder.
\newblock Universal language model fine-tuning for text classification.
\newblock \emph{arXiv preprint arXiv:1801.06146}, 2018.

\bibitem[Hsu et~al.(2024)Hsu, Tsai, Lin, Chen, Yu, and Huang]{hsu2024safe}
Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang.
\newblock {Safe LoRA}: the silver lining of reducing safety risks when fine-tuning large language models.
\newblock \emph{arXiv preprint arXiv:2405.16833}, 2024.

\bibitem[Huang et~al.(2024{\natexlab{a}})Huang, Bhattacharya, Joshi, Kimball, and Liu]{huang2024antidote}
Tiansheng Huang, Gautam Bhattacharya, Pratik Joshi, Josh Kimball, and Ling Liu.
\newblock Antidote: Post-fine-tuning safety alignment for large language models against harmful fine-tuning.
\newblock \emph{arXiv preprint arXiv:2408.09600}, 2024{\natexlab{a}}.

\bibitem[Huang et~al.(2024{\natexlab{b}})Huang, Hu, Ilhan, Tekin, and Liu]{huang2024booster}
Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim~Furkan Tekin, and Ling Liu.
\newblock Booster: Tackling harmful fine-tuing for large language models via attenuating harmful perturbation.
\newblock \emph{arXiv preprint arXiv:2409.01586}, 2024{\natexlab{b}}.

\bibitem[Huang et~al.(2024{\natexlab{c}})Huang, Hu, Ilhan, Tekin, and Liu]{huang2024lazy}
Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim~Furkan Tekin, and Ling Liu.
\newblock Lazy safety alignment for large language models against harmful fine-tuning.
\newblock \emph{arXiv preprint arXiv:2405.18641}, 2024{\natexlab{c}}.

\bibitem[Huang et~al.(2024{\natexlab{d}})Huang, Hu, and Liu]{huang2024vaccine}
Tiansheng Huang, Sihao Hu, and Ling Liu.
\newblock Vaccine: Perturbation-aware alignment for large language model.
\newblock \emph{arXiv preprint arXiv:2402.01109}, 2024{\natexlab{d}}.

\bibitem[Ilharco et~al.(2022)Ilharco, Ribeiro, Wortsman, Gururangan, Schmidt, Hajishirzi, and Farhadi]{ilharco2022editing}
Gabriel Ilharco, Marco~Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock Editing models with task arithmetic.
\newblock \emph{arXiv preprint arXiv:2212.04089}, 2022.

\bibitem[Imani et~al.(2023)Imani, Du, and Shrivastava]{imani2023mathprompter}
Shima Imani, Liang Du, and Harsh Shrivastava.
\newblock Mathprompter: Mathematical reasoning using large language models.
\newblock \emph{arXiv preprint arXiv:2303.05398}, 2023.

\bibitem[Inan et~al.(2023)Inan, Upasani, Chi, Rungta, Iyer, Mao, Tontchev, Hu, Fuller, Testuggine, et~al.]{inan2023llama}
Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et~al.
\newblock Llama guard: Llm-based input-output safeguard for human-ai conversations.
\newblock \emph{arXiv preprint arXiv:2312.06674}, 2023.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Jin et~al.(2023)Jin, Chen, Chen, and Wang]{jin2023quack}
Haibo Jin, Ruoxi Chen, Jinyin Chen, and Haohan Wang.
\newblock Quack: Automatic jailbreaking large language models via role-playing.
\newblock 2023.

\bibitem[Jones et~al.(2023)Jones, Dragan, Raghunathan, and Steinhardt]{jones2023automatically}
Erik Jones, Anca Dragan, Aditi Raghunathan, and Jacob Steinhardt.
\newblock Automatically auditing large language models via discrete optimization.
\newblock In \emph{International Conference on Machine Learning}, pages 15307--15329. PMLR, 2023.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Liu et~al.(2024)Liu, Lin, Huang, Mo, Mu, and Shen]{liu2024targeted}
Guozhi Liu, Weiwei Lin, Tiansheng Huang, Ruichao Mo, Qi~Mu, and Li~Shen.
\newblock Targeted vaccine: Safety alignment for large language models against harmful fine-tuning via layer-wise perturbation.
\newblock \emph{arXiv preprint arXiv:2410.09760}, 2024.

\bibitem[Liu et~al.(2023)Liu, Zhao, Joshi, Khalman, Saleh, Liu, and Liu]{liu2023statistical}
Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter~J Liu, and Jialu Liu.
\newblock Statistical rejection sampling improves preference optimization.
\newblock \emph{arXiv preprint arXiv:2309.06657}, 2023.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Peng et~al.(2023)Peng, Wu, Allard, Kilpatrick, and Heidel]{peng2023gpt}
Andrew Peng, Michael Wu, John Allard, Logan Kilpatrick, and Steven Heidel.
\newblock Gpt-3.5 turbo fine-tuning and api updates, 2023.
\newblock URL \url{https://openai.com/index/gpt-3-5-turbo-fine-tuning-and-api-updates}.

\bibitem[Qi et~al.(2023)Qi, Zeng, Xie, Chen, Jia, Mittal, and Henderson]{qi2023fine}
Xiangyu Qi, Yi~Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson.
\newblock Fine-tuning aligned language models compromises safety, even when users do not intend to!
\newblock \emph{arXiv preprint arXiv:2310.03693}, 2023.

\bibitem[Qi et~al.(2024)Qi, Huang, Panda, Henderson, Wang, and Mittal]{qi2024visual}
Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi Wang, and Prateek Mittal.
\newblock Visual adversarial examples jailbreak aligned large language models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 21527--21536, 2024.

\bibitem[Rafailov et~al.(2024)Rafailov, Sharma, Mitchell, Manning, Ermon, and Finn]{rafailov2024direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Rimsky et~al.(2023)Rimsky, Gabrieli, Schulz, Tong, Hubinger, and Turner]{rimsky2023steering}
Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander~Matt Turner.
\newblock Steering llama 2 via contrastive activation addition.
\newblock \emph{arXiv preprint arXiv:2312.06681}, 2023.

\bibitem[Sun et~al.(2024)Sun, Shen, Zhou, Zhang, Chen, Cox, Yang, and Gan]{sun2024principle}
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan.
\newblock Principle-driven self-alignment of language models from scratch with minimal human supervision.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Tatro et~al.(2020)Tatro, Chen, Das, Melnyk, Sattigeri, and Lai]{tatro2020optimizing}
Norman Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, and Rongjie Lai.
\newblock Optimizing mode connectivity via neuron alignment.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 15300--15311, 2020.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama1}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models (2023).
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Wei et~al.(2024)Wei, Haghtalab, and Steinhardt]{wei2024jailbroken}
Alexander Wei, Nika Haghtalab, and Jacob Steinhardt.
\newblock Jailbroken: How does llm safety training fail?
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock \emph{arXiv preprint arXiv:2109.01652}, 2021.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Yang et~al.(2023)Yang, Wang, Zhang, Petzold, Wang, Zhao, and Lin]{yang2023shadow}
Xianjun Yang, Xiao Wang, Qi~Zhang, Linda Petzold, William~Yang Wang, Xun Zhao, and Dahua Lin.
\newblock Shadow alignment: The ease of subverting safely-aligned language models.
\newblock \emph{arXiv preprint arXiv:2310.02949}, 2023.

\bibitem[Zhan et~al.(2023)Zhan, Fang, Bindu, Gupta, Hashimoto, and Kang]{zhan2023removing}
Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, and Daniel Kang.
\newblock Removing rlhf protections in gpt-4 via fine-tuning.
\newblock \emph{arXiv preprint arXiv:2311.05553}, 2023.

\bibitem[Zhao et~al.(2020)Zhao, Chen, Das, Ramamurthy, and Lin]{zhao2020bridging}
Pu~Zhao, Pin-Yu Chen, Payel Das, Karthikeyan~Natesan Ramamurthy, and Xue Lin.
\newblock Bridging mode connectivity in loss landscapes and adversarial robustness.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zheng et~al.(2024{\natexlab{a}})Zheng, Yin, Zhou, Meng, Zhou, Chang, Huang, and Peng]{zheng2024prompt}
Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng.
\newblock On prompt-driven safeguarding for large language models.
\newblock In \emph{ICLR 2024 Workshop on Secure and Trustworthy Large Language Models}, 2024{\natexlab{a}}.

\bibitem[Zheng et~al.(2024{\natexlab{b}})Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, et~al.]{zheng2024judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{b}}.

\bibitem[Zou et~al.(2023)Zou, Wang, Kolter, and Fredrikson]{zou2023universal}
Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language models.
\newblock \emph{arXiv preprint arXiv:2307.15043}, 2023.

\end{thebibliography}
