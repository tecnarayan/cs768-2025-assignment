\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alghunaim \& Yuan(2021)Alghunaim and Yuan]{alghunaim2021unified}
Alghunaim, S.~A. and Yuan, K.
\newblock A unified and refined convergence analysis for non-convex
  decentralized learning.
\newblock \emph{arXiv preprint arXiv:2110.09993}, 2021.

\bibitem[Assran et~al.(2019)Assran, Loizou, Ballas, and
  Rabbat]{assran2019stochastic}
Assran, M., Loizou, N., Ballas, N., and Rabbat, M.
\newblock Stochastic gradient push for distributed deep learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  344--353, 2019.

\bibitem[Bar-Noy et~al.(1993)Bar-Noy, Kipnis, and Schieber]{bar1993optimal}
Bar-Noy, A., Kipnis, S., and Schieber, B.
\newblock An optimal algorithm for computing census functions in
  message-passing systems.
\newblock \emph{Parallel Processing Letters}, 3\penalty0 (01):\penalty0 19--23,
  1993.

\bibitem[Ben-Nun \& Hoefler(2019)Ben-Nun and Hoefler]{ben2019demystifying}
Ben-Nun, T. and Hoefler, T.
\newblock Demystifying parallel and distributed deep learning: An in-depth
  concurrency analysis.
\newblock \emph{ACM Computing Surveys (CSUR)}, 52\penalty0 (4):\penalty0 1--43,
  2019.

\bibitem[Benjamini et~al.(2014)Benjamini, Kozma, and
  Wormald]{benjamini2014mixing}
Benjamini, I., Kozma, G., and Wormald, N.
\newblock The mixing time of the giant component of a random graph.
\newblock \emph{Random Structures \& Algorithms}, 45\penalty0 (3):\penalty0
  383--407, 2014.

\bibitem[Beveridge \& Youngblood(2016)Beveridge and
  Youngblood]{beveridge2016best}
Beveridge, A. and Youngblood, J.
\newblock The best mixing time for random walks on trees.
\newblock \emph{Graphs and Combinatorics}, 32\penalty0 (6):\penalty0
  2211--2239, 2016.

\bibitem[Boyd et~al.(2005)Boyd, Ghosh, Prabhakar, and Shah]{boyd2005mixing}
Boyd, S.~P., Ghosh, A., Prabhakar, B., and Shah, D.
\newblock Mixing times for random walks on geometric random graphs.
\newblock In \emph{ALENEX/ANALCO}, pp.\  240--249, 2005.

\bibitem[Chen \& Sayed(2012)Chen and Sayed]{chen2012diffusion}
Chen, J. and Sayed, A.~H.
\newblock Diffusion adaptation strategies for distributed optimization and
  learning over networks.
\newblock \emph{IEEE Transactions on Signal Processing}, 60\penalty0
  (8):\penalty0 4289--4305, 2012.

\bibitem[Di~Lorenzo \& Scutari(2016)Di~Lorenzo and Scutari]{di2016next}
Di~Lorenzo, P. and Scutari, G.
\newblock Next: In-network nonconvex optimization.
\newblock \emph{IEEE Transactions on Signal and Information Processing over
  Networks}, 2\penalty0 (2):\penalty0 120--136, 2016.

\bibitem[Dimakis et~al.(2010)Dimakis, Kar, Moura, Rabbat, and
  Scaglione]{dimakis2010gossip}
Dimakis, A.~G., Kar, S., Moura, J.~M., Rabbat, M.~G., and Scaglione, A.
\newblock Gossip algorithms for distributed signal processing.
\newblock \emph{Proceedings of the IEEE}, 98\penalty0 (11):\penalty0
  1847--1864, 2010.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  770--778, 2016.

\bibitem[Jiang et~al.(2020)Jiang, Zhu, Lan, Yi, Cui, and Guo]{jiang2020unified}
Jiang, Y., Zhu, Y., Lan, C., Yi, B., Cui, Y., and Guo, C.
\newblock A unified architecture for accelerating distributed dnn training in
  heterogeneous gpu/cpu clusters.
\newblock In \emph{Proceedings of the 14th USENIX Conference on Operating
  Systems Design and Implementation}, pp.\  463--479, 2020.

\bibitem[Koloskova et~al.(2019)Koloskova, Stich, and
  Jaggi]{koloskova2019decentralized}
Koloskova, A., Stich, S., and Jaggi, M.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3478--3487, 2019.

\bibitem[Koloskova et~al.(2020)Koloskova, Loizou, Boreiri, Jaggi, and
  Stich]{koloskova2020unified}
Koloskova, A., Loizou, N., Boreiri, S., Jaggi, M., and Stich, S.~U.
\newblock A unified theory of decentralized sgd with changing topology and
  local updates.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1--12, 2020.

\bibitem[Kong et~al.(2021)Kong, Lin, Koloskova, Jaggi, and
  Stich]{kong2021consensus}
Kong, L., Lin, T., Koloskova, A., Jaggi, M., and Stich, S.~U.
\newblock Consensus control for decentralized deep learning.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{Krizhevsky09}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
LeCun, Y., Cortes, C., and Burges, C.
\newblock {MNIST} handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available:
  http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem[Li et~al.(2019)Li, Shi, and Yan]{li2017decentralized}
Li, Z., Shi, W., and Yan, M.
\newblock A decentralized proximal-gradient method with network independent
  step-sizes and separated convergence rates.
\newblock \emph{IEEE Transactions on Signal Processing}, July 2019.
\newblock early acces. Also available on arXiv:1704.07807.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017can}
Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J.
\newblock Can decentralized algorithms outperform centralized algorithms? {A}
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5330--5340, 2017.

\bibitem[Lian et~al.(2018)Lian, Zhang, Zhang, and Liu]{lian2018asynchronous}
Lian, X., Zhang, W., Zhang, C., and Liu, J.
\newblock Asynchronous decentralized parallel stochastic gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3043--3052, 2018.

\bibitem[Lin et~al.(2021)Lin, Karimireddy, Stich, and Jaggi]{lin2021quasi}
Lin, T., Karimireddy, S.~P., Stich, S.~U., and Jaggi, M.
\newblock Quasi-global momentum: Accelerating decentralized deep learning on
  heterogeneous data.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Liu(2021)]{kuangliu}
Liu, K.
\newblock Train {CIFAR10} with {PyTorch}.
\newblock \nolinkurl{https://github.com/kuangliu/pytorch-cifar}, 2021.
\newblock Accessed: 2023-01.

\bibitem[Lopes \& Sayed(2008)Lopes and Sayed]{lopes2008diffusion}
Lopes, C.~G. and Sayed, A.~H.
\newblock Diffusion least-mean squares over adaptive networks: Formulation and
  performance analysis.
\newblock \emph{IEEE Transactions on Signal Processing}, 56\penalty0
  (7):\penalty0 3122--3136, 2008.

\bibitem[Nachmias \& Peres(2008)Nachmias and Peres]{nachmias2008critical}
Nachmias, A. and Peres, Y.
\newblock Critical random graphs: diameter and mixing time.
\newblock \emph{The Annals of Probability}, 36\penalty0 (4):\penalty0
  1267--1286, 2008.

\bibitem[Nedic \& Ozdaglar(2009)Nedic and Ozdaglar]{nedic2009distributed}
Nedic, A. and Ozdaglar, A.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (1):\penalty0 48--61, 2009.

\bibitem[Nedic et~al.(2017)Nedic, Olshevsky, and Shi]{nedic2017achieving}
Nedic, A., Olshevsky, A., and Shi, W.
\newblock Achieving geometric convergence for distributed optimization over
  time-varying graphs.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2597--2633, 2017.

\bibitem[Nedi{\'c} et~al.(2018)Nedi{\'c}, Olshevsky, and
  Rabbat]{nedic2018network}
Nedi{\'c}, A., Olshevsky, A., and Rabbat, M.~G.
\newblock Network topology and communication-computation tradeoffs in
  decentralized optimization.
\newblock \emph{Proceedings of the IEEE}, 106\penalty0 (5):\penalty0 953--976,
  2018.

\bibitem[Niwa et~al.(2021)Niwa, Zhang, Kleijn, Harada, Sawada, and
  Fujino]{niwa2021asynchronous}
Niwa, K., Zhang, G., Kleijn, W.~B., Harada, N., Sawada, H., and Fujino, A.
\newblock Asynchronous decentralized optimization with implicit stochastic
  variance reduction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8195--8204. PMLR, 2021.

\bibitem[Patarasuk \& Yuan(2009)Patarasuk and Yuan]{patarasuk2009bandwidth}
Patarasuk, P. and Yuan, X.
\newblock Bandwidth optimal all-reduce algorithms for clusters of workstations.
\newblock \emph{Journal of Parallel and Distributed Computing}, 69\penalty0
  (2):\penalty0 117--124, 2009.

\bibitem[Pu et~al.(2019)Pu, Olshevsky, and Paschalidis]{pu2019sharp}
Pu, S., Olshevsky, A., and Paschalidis, I.~C.
\newblock A sharp estimate on the transient time of distributed stochastic
  gradient descent.
\newblock \emph{arXiv preprint arXiv:1906.02702}, 2019.

\bibitem[Qu \& Li(2018)Qu and Li]{qu2018harnessing}
Qu, G. and Li, N.
\newblock Harnessing smoothness to accelerate distributed optimization.
\newblock \emph{IEEE Transactions on Control of Network Systems}, 5\penalty0
  (3):\penalty0 1245--1260, 2018.

\bibitem[Shi et~al.(2014)Shi, Ling, Yuan, Wu, and Yin]{shi2014linear}
Shi, W., Ling, Q., Yuan, K., Wu, G., and Yin, W.
\newblock On the linear convergence of the admm in decentralized consensus
  optimization.
\newblock \emph{IEEE Transactions on Signal Processing}, 62\penalty0
  (7):\penalty0 1750--1761, 2014.

\bibitem[Shi et~al.(2015)Shi, Ling, Wu, and Yin]{shi2015extra}
Shi, W., Ling, Q., Wu, G., and Yin, W.
\newblock {EXTRA}: An exact first-order algorithm for decentralized consensus
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  944--966, 2015.

\bibitem[Song et~al.(2022)Song, Li, Jin, Shi, Yan, Yin, and
  Yuan]{song2022communication}
Song, Z., Li, W., Jin, K., Shi, L., Yan, M., Yin, W., and Yuan, K.
\newblock Communication-efficient topologies for decentralized learning with $
  o (1) $ consensus rate.
\newblock \emph{arXiv preprint arXiv:2210.07881}, 2022.

\bibitem[Tang et~al.(2018)Tang, Lian, Yan, Zhang, and Liu]{tang2018d}
Tang, H., Lian, X., Yan, M., Zhang, C., and Liu, J.
\newblock $ d^2$: Decentralized training over decentralized data.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4848--4856, 2018.

\bibitem[Trevisan(2017)]{trevisan2017lecture}
Trevisan, L.
\newblock Lecture notes on graph partitioning, expanders and spectral methods.
\newblock \emph{University of California, Berkeley, https://people. eecs.
  berkeley. edu/\~{} luca/books/expanders-2016. pdf}, 2017.

\bibitem[Tsitsiklis et~al.(1986)Tsitsiklis, Bertsekas, and
  Athans]{tsitsiklis1986distributed}
Tsitsiklis, J., Bertsekas, D., and Athans, M.
\newblock Distributed asynchronous deterministic and stochastic gradient
  optimization algorithms.
\newblock \emph{IEEE transactions on automatic control}, 31\penalty0
  (9):\penalty0 803--812, 1986.

\bibitem[Xin et~al.(2020)Xin, Khan, and Kar]{xin2020improved}
Xin, R., Khan, U.~A., and Kar, S.
\newblock An improved convergence analysis for decentralized online stochastic
  non-convex optimization.
\newblock \emph{arXiv preprint arXiv:2008.04195}, 2020.

\bibitem[Xu et~al.(2015)Xu, Zhu, Soh, and Xie]{xu2015augmented}
Xu, J., Zhu, S., Soh, Y.~C., and Xie, L.
\newblock Augmented distributed gradient methods for multi-agent optimization
  under uncoordinated constant stepsizes.
\newblock In \emph{IEEE Conference on Decision and Control (CDC)}, pp.\
  2055--2060, Osaka, Japan, 2015.

\bibitem[Ying et~al.(2021{\natexlab{a}})Ying, Yuan, Chen, Hu, Pan, and
  Yin]{ying2021exponential}
Ying, B., Yuan, K., Chen, Y., Hu, H., Pan, P., and Yin, W.
\newblock Exponential graph is provably efficient for decentralized deep
  training.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 13975--13987, 2021{\natexlab{a}}.

\bibitem[Ying et~al.(2021{\natexlab{b}})Ying, Yuan, Hu, Chen, and Yin]{bluefog}
Ying, B., Yuan, K., Hu, H., Chen, Y., and Yin, W.
\newblock Bluefog: Make decentralized algorithms practical for optimization and
  deep learning.
\newblock \emph{arXiv preprint arXiv:2111.04287}, 2021{\natexlab{b}}.

\bibitem[Yuan \& Alghunaim(2021)Yuan and Alghunaim]{yuan2021removing}
Yuan, K. and Alghunaim, S.~A.
\newblock Removing data heterogeneity influence enhances network topology
  dependence of decentralized sgd.
\newblock \emph{arXiv preprint arXiv:2105.08023}, 2021.

\bibitem[Yuan et~al.(2016)Yuan, Ling, and Yin]{yuan2016convergence}
Yuan, K., Ling, Q., and Yin, W.
\newblock On the convergence of decentralized gradient descent.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (3):\penalty0
  1835--1854, 2016.

\bibitem[Yuan et~al.(2019)Yuan, Ying, Zhao, and Sayed]{yuan2017exact1}
Yuan, K., Ying, B., Zhao, X., and Sayed, A.~H.
\newblock Exact dffusion for distributed optimization and learning -- {Part I:
  Algorithm development}.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (3):\penalty0 708 -- 723, 2019.

\bibitem[Yuan et~al.(2020)Yuan, Alghunaim, Ying, and Sayed]{yuan2020influence}
Yuan, K., Alghunaim, S.~A., Ying, B., and Sayed, A.~H.
\newblock On the influence of bias-correction on distributed stochastic
  optimization.
\newblock \emph{IEEE Transactions on Signal Processing}, 2020.

\bibitem[Yuan et~al.(2021)Yuan, Chen, Huang, Zhang, Pan, Xu, and
  Yin]{yuan2021decentlam}
Yuan, K., Chen, Y., Huang, X., Zhang, Y., Pan, P., Xu, Y., and Yin, W.
\newblock {DecentLaM}: Decentralized momentum {SGD} for large-batch deep
  training.
\newblock \emph{arXiv preprint arXiv:2104.11981}, 2021.

\end{thebibliography}
