\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aky{\"u}rek et~al.(2023)Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou]{akyurek2023_incontext}
Aky{\"u}rek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D.
\newblock What learning algorithm is in-context learning? investigations with linear models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=0g0X4H8yN4I}.

\bibitem[Baldi \& Vershynin(2023)Baldi and Vershynin]{baldi2023_quarks}
Baldi, P. and Vershynin, R.
\newblock The quarks of attention: Structure and capacity of neural attention building blocks.
\newblock \emph{Artificial Intelligence}, 319:\penalty0 103901, 2023.
\newblock ISSN 0004-3702.
\newblock \doi{https://doi.org/10.1016/j.artint.2023.103901}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S0004370223000474}.

\bibitem[Bartlett et~al.(2005)Bartlett, Bousquet, and Mendelson]{Bartlett_2005}
Bartlett, P.~L., Bousquet, O., and Mendelson, S.
\newblock Local rademacher complexities.
\newblock \emph{The Annals of Statistics}, 33\penalty0 (4), August 2005.
\newblock ISSN 0090-5364.
\newblock \doi{10.1214/009053605000000282}.
\newblock URL \url{http://dx.doi.org/10.1214/009053605000000282}.

\bibitem[Bi et~al.(2023)Bi, Yin, and Weng]{bi2023low}
Bi, S., Yin, Z., and Weng, Y.
\newblock A low-rank spectral method for learning markov models.
\newblock \emph{Optimization Letters}, 17\penalty0 (1):\penalty0 143--162, 2023.

\bibitem[Billingsley(1961)]{billingsley1961statistical}
Billingsley, P.
\newblock Statistical methods in markov chains.
\newblock \emph{The annals of mathematical statistics}, pp.\  12--40, 1961.

\bibitem[Block et~al.(2023)Block, Simchowitz, and Tedrake]{block2023smoothed}
Block, A., Simchowitz, M., and Tedrake, R.
\newblock Smoothed online learning for prediction in piecewise affine systems.
\newblock \emph{arXiv preprint arXiv:2301.11187}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020_gpt3}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33, pp.\  1877--1901. Curran Associates, Inc., 2020.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Chan et~al.(2021)Chan, Ding, and Li]{pmlr-v132-chan21a}
Chan, S.~O., Ding, Q., and Li, S.~H.
\newblock Learning and testing irreducible {M}arkov chains via the $k$-cover time.
\newblock In Feldman, V., Ligett, K., and Sabato, S. (eds.), \emph{Proceedings of the 32nd International Conference on Algorithmic Learning Theory}, volume 132 of \emph{Proceedings of Machine Learning Research}, pp.\  458--480. PMLR, 16--19 Mar 2021.

\bibitem[Chen \& Jiang(2019)Chen and Jiang]{chen2019informationtheoretic_coverage}
Chen, J. and Jiang, N.
\newblock Information-theoretic considerations in batch reinforcement learning, 2019.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022_palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Dean et~al.(2020)Dean, Mania, Matni, Recht, and Tu]{dean2020sample}
Dean, S., Mania, H., Matni, N., Recht, B., and Tu, S.
\newblock On the sample complexity of the linear quadratic regulator.
\newblock \emph{Foundations of Computational Mathematics}, 20\penalty0 (4):\penalty0 633--679, 2020.

\bibitem[Edelman et~al.(2022)Edelman, Goel, Kakade, and Zhang]{edelman2022_inductive}
Edelman, B.~L., Goel, S., Kakade, S., and Zhang, C.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  5793--5831. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/edelman22a.html}.

\bibitem[Fan et~al.(2018)Fan, Lewis, and Dauphin]{fan2018hierarchical}
Fan, A., Lewis, M., and Dauphin, Y.
\newblock Hierarchical neural story generation.
\newblock \emph{arXiv preprint arXiv:1805.04833}, 2018.

\bibitem[Foster et~al.(2020)Foster, Sarkar, and Rakhlin]{foster2020learning}
Foster, D., Sarkar, T., and Rakhlin, A.
\newblock Learning nonlinear dynamical systems from a single trajectory.
\newblock In \emph{Learning for Dynamics and Control}, pp.\  851--861. PMLR, 2020.

\bibitem[Foster et~al.(2022)Foster, Krishnamurthy, Simchi-Levi, and Xu]{foster2022offline_coverage}
Foster, D.~J., Krishnamurthy, A., Simchi-Levi, D., and Xu, Y.
\newblock Offline reinforcement learning: Fundamental barriers for value function approximation, 2022.

\bibitem[Fu et~al.(2023)Fu, Guo, Bai, and Mei]{fu2023_randomfeatures}
Fu, H., Guo, T., Bai, Y., and Mei, S.
\newblock What can a single attention layer learn? a study through the random features lens.
\newblock \emph{arXiv preprint arXiv:2307.11353}, 2023.

\bibitem[Fu et~al.(2021)Fu, Lam, So, and Shi]{fu2021theoretical}
Fu, Z., Lam, W., So, A. M.-C., and Shi, B.
\newblock A theoretical analysis of the repetition problem in text generation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pp.\  12848--12856, 2021.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022_incontext}
Garg, S., Tsipras, D., Liang, P.~S., and Valiant, G.
\newblock What can transformers learn in-context? a case study of simple function classes.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~35, pp.\  30583--30598. Curran Associates, Inc., 2022.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/c529dba08a146ea8d6cf715ae8930cbe-Paper-Conference.pdf}.

\bibitem[Hao et~al.(2018)Hao, Orlitsky, and Pichapati]{hao2018learning}
Hao, Y., Orlitsky, A., and Pichapati, V.
\newblock On learning markov chains.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and Choi]{holtzman2019curious}
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.
\newblock The curious case of neural text degeneration.
\newblock \emph{arXiv preprint arXiv:1904.09751}, 2019.

\bibitem[Jelassi et~al.(2022)Jelassi, Sander, and Li]{jelassi2022_transformer}
Jelassi, S., Sander, M.~E., and Li, Y.
\newblock Vision transformers provably learn spatial structure.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=eMW9AkXaREI}.

\bibitem[Jin et~al.(2022)Jin, Yang, and Wang]{jin2022pessimism_coverage}
Jin, Y., Yang, Z., and Wang, Z.
\newblock Is pessimism provably efficient for offline rl?, 2022.

\bibitem[Kuznetsov \& Mohri(2014)Kuznetsov and Mohri]{kuznetsov2014generalization}
Kuznetsov, V. and Mohri, M.
\newblock Generalization bounds for time series prediction with non-stationary processes.
\newblock In \emph{International conference on algorithmic learning theory}. Springer, 2014.

\bibitem[Kuznetsov \& Mohri(2016)Kuznetsov and Mohri]{kuznetsov2016time}
Kuznetsov, V. and Mohri, M.
\newblock Time series prediction and online learning.
\newblock In \emph{Conference on Learning Theory}, pp.\  1190--1213. PMLR, 2016.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Wang, Liu, and Chen]{li2023_transformer}
Li, H., Wang, M., Liu, S., and Chen, P.-Y.
\newblock A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=jClGv3Qjhb}.

\bibitem[Li et~al.(2018)Li, Wang, and Zhang]{li2018estimation}
Li, X., Wang, M., and Zhang, A.
\newblock Estimation of markov chain via rank-constrained likelihood.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3033--3042. PMLR, 2018.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Ildiz, Papailiopoulos, and Oymak]{li2023_incontext}
Li, Y., Ildiz, M.~E., Papailiopoulos, D., and Oymak, S.
\newblock Transformers as algorithms: Generalization and stability in in-context learning.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  19565--19594. PMLR, 23--29 Jul 2023{\natexlab{b}}.
\newblock URL \url{https://proceedings.mlr.press/v202/li23l.html}.

\bibitem[Li et~al.(2024)Li, Huang, Ildiz, Rawat, and Oymak]{anonymous}
Li, Y., Huang, Y., Ildiz, M.~E., Rawat, A.~S., and Oymak, S.
\newblock Mechanics of next token prediction with self-attention.
\newblock \emph{In International Conference on Artificial Intelligence and Statistics (AISTATS)}, 2024.

\bibitem[Lin et~al.(2021)Lin, Han, and Joty]{lin2021straight}
Lin, X., Han, S., and Joty, S.
\newblock Straight to the gradient: Learning to use novel tokens for neural text generation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  6642--6653. PMLR, 2021.

\bibitem[Makkuva et~al.(2024)Makkuva, Bondaschi, Girish, Nagle, Jaggi, Kim, and Gastpar]{makkuva2024attention}
Makkuva, A.~V., Bondaschi, M., Girish, A., Nagle, A., Jaggi, M., Kim, H., and Gastpar, M.
\newblock Attention with markov: A framework for principled analysis of transformers via markov chains, 2024.

\bibitem[Mania et~al.(2020)Mania, Jordan, and Recht]{mania2020active}
Mania, H., Jordan, M.~I., and Recht, B.
\newblock Active learning for nonlinear system identification with guarantees.
\newblock \emph{arXiv preprint arXiv:2006.10277}, 2020.

\bibitem[Matni \& Tu(2019)Matni and Tu]{matni2019tutorial}
Matni, N. and Tu, S.
\newblock A tutorial on concentration bounds for system identification.
\newblock In \emph{2019 IEEE 58th Conference on Decision and Control (CDC)}, pp.\  3741--3749. IEEE, 2019.

\bibitem[Mohri \& Rostamizadeh(2008)Mohri and Rostamizadeh]{mohri2008rademacher}
Mohri, M. and Rostamizadeh, A.
\newblock Rademacher complexity bounds for non-iid processes.
\newblock \emph{Advances in Neural Information Processing Systems}, 21, 2008.

\bibitem[Oymak(2019)]{oymak2019stochastic}
Oymak, S.
\newblock Stochastic gradient descent learns state equations with nonlinear activations.
\newblock In \emph{conference on Learning Theory}, pp.\  2551--2579. PMLR, 2019.

\bibitem[Oymak \& Ozay(2019)Oymak and Ozay]{oymak2019non}
Oymak, S. and Ozay, N.
\newblock Non-asymptotic identification of lti systems from a single trajectory.
\newblock In \emph{2019 American control conference (ACC)}, pp.\  5655--5661. IEEE, 2019.

\bibitem[Oymak \& Ozay(2021)Oymak and Ozay]{oymak2021revisiting}
Oymak, S. and Ozay, N.
\newblock Revisiting ho--kalman-based system identification: Robustness and finite-sample analysis.
\newblock \emph{IEEE Transactions on Automatic Control}, 67\penalty0 (4):\penalty0 1914--1928, 2021.

\bibitem[Oymak et~al.(2023)Oymak, Rawat, Soltanolkotabi, and Thrampoulidis]{oymak23a_prompt}
Oymak, S., Rawat, A.~S., Soltanolkotabi, M., and Thrampoulidis, C.
\newblock On the role of attention in prompt-tuning.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  26724--26768. PMLR, 23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/oymak23a.html}.

\bibitem[Press \& Wolf(2017)Press and Wolf]{press2017using}
Press, O. and Wolf, L.
\newblock Using the output embedding to improve language models, 2017.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever, et~al.]{radford2018_gpt1}
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et~al.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{OpenAI blog}, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{radford2019_gpt2}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Rashidinejad et~al.(2023)Rashidinejad, Zhu, Ma, Jiao, and Russell]{rashidinejad2023bridging_coverage}
Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S.
\newblock Bridging offline reinforcement learning and imitation learning: A tale of pessimism, 2023.

\bibitem[Sarkar \& Rakhlin(2019)Sarkar and Rakhlin]{sarkar2019near}
Sarkar, T. and Rakhlin, A.
\newblock Near optimal finite time identification of arbitrary linear dynamical systems.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5610--5618. PMLR, 2019.

\bibitem[Sattar \& Oymak(2022)Sattar and Oymak]{sattar2022non}
Sattar, Y. and Oymak, S.
\newblock Non-asymptotic and accurate learning of nonlinear dynamical systems.
\newblock \emph{The Journal of Machine Learning Research}, 23\penalty0 (1):\penalty0 6248--6296, 2022.

\bibitem[See et~al.(2017)See, Liu, and Manning]{see2017get}
See, A., Liu, P.~J., and Manning, C.~D.
\newblock Get to the point: Summarization with pointer-generator networks.
\newblock \emph{arXiv preprint arXiv:1704.04368}, 2017.

\bibitem[Shah et~al.(2020)Shah, Song, Xu, and Yang]{shah2020sample}
Shah, D., Song, D., Xu, Z., and Yang, Y.
\newblock Sample efficient reinforcement learning via low-rank matrix estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 12092--12103, 2020.

\bibitem[Simchowitz et~al.(2018)Simchowitz, Mania, Tu, Jordan, and Recht]{simchowitz2018learning}
Simchowitz, M., Mania, H., Tu, S., Jordan, M.~I., and Recht, B.
\newblock Learning without mixing: Towards a sharp analysis of linear system identification.
\newblock In \emph{Conference On Learning Theory}, pp.\  439--473. PMLR, 2018.

\bibitem[Srebro et~al.(2010)Srebro, Sridharan, and Tewari]{srebro_FastRate}
Srebro, N., Sridharan, K., and Tewari, A.
\newblock Smoothness, low noise and fast rates.
\newblock In Lafferty, J., Williams, C., Shawe-Taylor, J., Zemel, R., and Culotta, A. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~23. Curran Associates, Inc., 2010.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2010/file/76cf99d3614e23eabab16fb27e944bf9-Paper.pdf}.

\bibitem[Stojanovic et~al.(2023)Stojanovic, Jedra, and Proutiere]{stojanovic2023spectral}
Stojanovic, S., Jedra, Y., and Proutiere, A.
\newblock Spectral entry-wise matrix estimation for low-rank reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2310.06793}, 2023.

\bibitem[Sun et~al.(2022)Sun, Oymak, and Fazel]{sun2022finite}
Sun, Y., Oymak, S., and Fazel, M.
\newblock Finite sample identification of low-order lti systems via nuclear norm regularization.
\newblock \emph{IEEE Open Journal of Control Systems}, 1:\penalty0 237--254, 2022.

\bibitem[Tarzanagh et~al.(2023{\natexlab{a}})Tarzanagh, Li, Thrampoulidis, and Oymak]{tarzanagh2023transformers}
Tarzanagh, D.~A., Li, Y., Thrampoulidis, C., and Oymak, S.
\newblock Transformers as support vector machines.
\newblock \emph{arXiv preprint arXiv:2308.16898}, 2023{\natexlab{a}}.

\bibitem[Tarzanagh et~al.(2023{\natexlab{b}})Tarzanagh, Li, Zhang, and Oymak]{tarzanagh2023max}
Tarzanagh, D.~A., Li, Y., Zhang, X., and Oymak, S.
\newblock Max-margin token selection in attention mechanism.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023{\natexlab{b}}.

\bibitem[Tian et~al.(2023)Tian, Wang, Chen, and Du]{tian2023_scan}
Tian, Y., Wang, Y., Chen, B., and Du, S.
\newblock Scan and snap: Understanding training dynamics and token composition in 1-layer transformer.
\newblock \emph{arXiv preprint arXiv:2305.16380}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023_llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Tsiamis et~al.(2022)Tsiamis, Ziemann, Matni, and Pappas]{tsiamis2022statistical}
Tsiamis, A., Ziemann, I., Matni, N., and Pappas, G.~J.
\newblock Statistical learning theory for control: A finite sample perspective.
\newblock \emph{arXiv preprint arXiv:2209.05423}, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L.~u., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Vershynin(2018)]{vershynin2018high}
Vershynin, R.
\newblock \emph{High-dimensional probability: An introduction with applications in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Von~Oswald et~al.(2023)Von~Oswald, Niklasson, Randazzo, Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov]{oswald2023_incontext}
Von~Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M.
\newblock Transformers learn in-context by gradient descent.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), \emph{Proceedings of the 40th International Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  35151--35174. PMLR, 23--29 Jul 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/von-oswald23a.html}.

\bibitem[Welleck et~al.(2019)Welleck, Kulikov, Roller, Dinan, Cho, and Weston]{welleck2019neural}
Welleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., and Weston, J.
\newblock Neural text generation with unlikelihood training.
\newblock \emph{arXiv preprint arXiv:1908.04319}, 2019.

\bibitem[Welleck et~al.(2020)Welleck, Kulikov, Kim, Pang, and Cho]{welleck2020consistency}
Welleck, S., Kulikov, I., Kim, J., Pang, R.~Y., and Cho, K.
\newblock Consistency of a recurrent language model with respect to incomplete decoding.
\newblock \emph{arXiv preprint arXiv:2002.02492}, 2020.

\bibitem[Wolfer \& Kontorovich(2019)Wolfer and Kontorovich]{wolfer2019minimax}
Wolfer, G. and Kontorovich, A.
\newblock Minimax learning of ergodic markov chains.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  904--930. PMLR, 2019.

\bibitem[Wolfer \& Kontorovich(2021)Wolfer and Kontorovich]{wolfer2021statistical}
Wolfer, G. and Kontorovich, A.
\newblock Statistical estimation of ergodic markov chain kernel over discrete state space.
\newblock \emph{Bernoulli}, 27\penalty0 (1):\penalty0 532--553, 2021.

\bibitem[Xie et~al.(2022)Xie, Raghunathan, Liang, and Ma]{xie2022_incontext}
Xie, S.~M., Raghunathan, A., Liang, P., and Ma, T.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=RdJVFCHjUMI}.

\bibitem[Xie \& Jiang(2020)Xie and Jiang]{xie2020q_coverage}
Xie, T. and Jiang, N.
\newblock Q* approximation schemes for batch reinforcement learning: A theoretical comparison, 2020.

\bibitem[Xu et~al.(2022)Xu, Liu, Yan, Cai, Li, and Li]{xu2022learning}
Xu, J., Liu, X., Yan, J., Cai, D., Li, H., and Li, J.
\newblock Learning to break the loop: Analyzing and mitigating repetitions for neural text generation.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 3082--3095, 2022.

\bibitem[Yu(1994)]{yu1994rates}
Yu, B.
\newblock Rates of convergence for empirical processes of stationary mixing sequences.
\newblock \emph{The Annals of Probability}, pp.\  94--116, 1994.

\bibitem[Yun et~al.(2020)Yun, Bhojanapalli, Rawat, Reddi, and Kumar]{yun2020_universal}
Yun, C., Bhojanapalli, S., Rawat, A.~S., Reddi, S., and Kumar, S.
\newblock Are transformers universal approximators of sequence-to-sequence functions?
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=ByxRM0Ntvr}.

\bibitem[Zhan et~al.(2022)Zhan, Huang, Huang, Jiang, and Lee]{zhan2022offline_coverage}
Zhan, W., Huang, B., Huang, A., Jiang, N., and Lee, J.~D.
\newblock Offline reinforcement learning with realizability and single-policy concentrability, 2022.

\bibitem[Zhang \& Wang(2019)Zhang and Wang]{zhang2019spectral}
Zhang, A. and Wang, M.
\newblock Spectral state compression of markov processes.
\newblock \emph{IEEE transactions on information theory}, 66\penalty0 (5):\penalty0 3202--3231, 2019.

\bibitem[Zhu et~al.(2022)Zhu, Li, Wang, and Zhang]{zhu2022learning}
Zhu, Z., Li, X., Wang, M., and Zhang, A.
\newblock Learning markov models via low-rank optimization.
\newblock \emph{Operations Research}, 70\penalty0 (4):\penalty0 2384--2398, 2022.

\bibitem[Ziemann \& Tu(2022{\natexlab{a}})Ziemann and Tu]{ziemann2022learning}
Ziemann, I. and Tu, S.
\newblock Learning with little mixing.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 4626--4637, 2022{\natexlab{a}}.

\bibitem[Ziemann \& Tu(2022{\natexlab{b}})Ziemann and Tu]{ziemannlearning}
Ziemann, I. and Tu, S.
\newblock Learning with little mixing.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{b}}.

\bibitem[Ziemann et~al.(2024)Ziemann, Tu, Pappas, and Matni]{ziemann2024sharp}
Ziemann, I., Tu, S., Pappas, G.~J., and Matni, N.
\newblock Sharp rates in dependent learning theory: Avoiding sample size deflation for the square loss.
\newblock \emph{arXiv preprint arXiv:2402.05928}, 2024.

\bibitem[Ziemann et~al.(2022)Ziemann, Sandberg, and Matni]{ziemann2022single}
Ziemann, I.~M., Sandberg, H., and Matni, N.
\newblock Single trajectory nonparametric learning of nonlinear dynamics.
\newblock In \emph{conference on Learning Theory}, pp.\  3333--3364. PMLR, 2022.

\end{thebibliography}
