@article{arnold2019maml,
  title={When MAML Can Adapt Fast and How to Assist When It Cannot},
  author={Arnold, S{\'e}bastien MR and Iqbal, Shariq and Sha, Fei},
  journal={arXiv preprint arXiv:1910.13603},
  year={2019}
}

@article{yao2020don,
  title={Don't Overlook the Support Set: Towards Improving Generalization in Meta-learning},
  author={Yao, Huaxiu and Huang, Longkai and Wei, Ying and Tian, Li and Huang, Junzhou and Li, Zhenhui},
  journal={arXiv preprint arXiv:2007.13040},
  year={2020}
}

@misc{setlur2020support,
      title={Is Support Set Diversity Necessary for Meta-Learning?}, 
      author={Amrith Setlur and Oscar Li and Virginia Smith},
      year={2020},
      eprint={2011.14048},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{wang2020globalmaml,
  title={On the global optimality of model-agnostic meta-learning},
  author={Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={9837--9846},
  year={2020},
  organization={PMLR}
}

@article{collins2020does,
  title={Why Does MAML Outperform ERM? An Optimization Perspective},
  author={Collins, Liam and Mokhtari, Aryan and Shakkottai, Sanjay},
  journal={arXiv preprint arXiv:2010.14672},
  year={2020}
}

@article{gao2020modeling,
  title={Modeling and Optimization Trade-off in Meta-learning},
  author={Gao, Katelyn and Sener, Ozan},
  journal={arXiv preprint arXiv:2010.12916},
  year={2020}
}

@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}

@article{ji2020convergence,
  title={Convergence of Meta-Learning with Task-Specific Adaptation over Partial Parameters},
  author={Ji, Kaiyi and Lee, Jason D and Liang, Yingbin and Poor, H Vincent},
  journal={arXiv preprint arXiv:2006.09486},
  year={2020}
}

@article{bai2019beyond,
  title={Beyond linearization: On quadratic and higher-order approximation of wide neural networks},
  author={Bai, Yu and Lee, Jason D},
  journal={arXiv preprint arXiv:1910.01619},
  year={2019}
}

@article{wang2020global,
  title={Global convergence and induced kernels of gradient-based meta-learning with neural nets},
  author={Wang, Haoxiang and Sun, Ruoyu and Li, Bo},
  journal={arXiv preprint arXiv:2006.14606},
  year={2020}
}

@article{tripuraneni2020theory,
  title={On the Theory of Transfer Learning: The Importance of Task Diversity},
  author={Tripuraneni, Nilesh and Jordan, Michael I and Jin, Chi},
  journal={arXiv preprint arXiv:2006.11650},
  year={2020}
}

@article{goldblum2020unraveling,
  title={Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks},
  author={Goldblum, Micah and Reich, Steven and Fowl, Liam and Ni, Renkun and Cherepanova, Valeriia and Goldstein, Tom},
  journal={arXiv preprint arXiv:2002.06753},
  year={2020}
}

@inproceedings{raghu2020rapid,
title={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},
author={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgMkCEtPB}
}

@article{wang2020global,
	Author = {Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
	Date-Added = {2020-10-01 23:58:53 -0400},
	Date-Modified = {2020-10-01 23:58:53 -0400},
	Journal = {arXiv preprint arXiv:2006.13182},
	Title = {On the Global Optimality of Model-Agnostic Meta-Learning},
	Year = {2020}}

@article{nichol2018first,
	Author = {Nichol, Alex and Achiam, Joshua and Schulman, John},
	Date-Added = {2020-10-01 23:57:57 -0400},
	Date-Modified = {2020-10-01 23:57:57 -0400},
	Journal = {arXiv preprint arXiv:1803.02999},
	Title = {On first-order meta-learning algorithms},
	Year = {2018}}

@inproceedings{fallah2020convergence,
	Author = {Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
	Booktitle = {International Conference on Artificial Intelligence and Statistics},
	Date-Added = {2020-10-01 23:57:32 -0400},
	Date-Modified = {2020-10-01 23:57:32 -0400},
	Pages = {1082--1092},
	Title = {On the convergence theory of gradient-based model-agnostic meta-learning algorithms},
	Year = {2020}}

@inproceedings{xie2019meta,
	Author = {Xie, Yujia and Jiang, Haoming and Liu, Feng and Zhao, Tuo and Zha, Hongyuan},
	Booktitle = {Advances in Neural Information Processing Systems},
	Date-Added = {2020-10-01 23:53:23 -0400},
	Date-Modified = {2020-10-01 23:53:23 -0400},
	Pages = {9904--9915},
	Title = {Meta Learning with Relational Information for Short Sequences},
	Year = {2019}}

@article{gu2018meta,
	Author = {Gu, Jiatao and Wang, Yong and Chen, Yun and Cho, Kyunghyun and Li, Victor OK},
	Date-Added = {2020-10-01 23:52:36 -0400},
	Date-Modified = {2020-10-01 23:52:36 -0400},
	Journal = {arXiv preprint arXiv:1808.08437},
	Title = {Meta-learning for low-resource neural machine translation},
	Year = {2018}}

@article{franceschi2018bilevel,
	Author = {Franceschi, Luca and Frasconi, Paolo and Salzo, Saverio and Grazzi, Riccardo and Pontil, Massimilano},
	Date-Added = {2020-10-01 23:50:27 -0400},
	Date-Modified = {2020-10-01 23:50:27 -0400},
	Journal = {arXiv preprint arXiv:1806.04910},
	Title = {Bilevel programming for hyperparameter optimization and meta-learning},
	Year = {2018}}

@phdthesis{schmidhuber1987evolutionary,
	Author = {Schmidhuber, J{\"u}rgen},
	Date-Added = {2020-10-01 23:48:32 -0400},
	Date-Modified = {2020-10-01 23:48:32 -0400},
	School = {Technische Universit{\"a}t M{\"u}nchen},
	Title = {Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook},
	Year = {1987}}

@article{ruder2017overview,
	Author = {Ruder, Sebastian},
	Date-Added = {2020-10-01 23:45:36 -0400},
	Date-Modified = {2020-10-01 23:45:36 -0400},
	Journal = {arXiv preprint arXiv:1706.05098},
	Title = {An overview of multi-task learning in deep neural networks},
	Year = {2017}}

@article{liu2015calibrated,
	Author = {Liu, Han and Wang, Lie and Zhao, Tuo},
	Date-Added = {2020-10-01 23:37:50 -0400},
	Date-Modified = {2020-10-01 23:37:50 -0400},
	Journal = {Journal of machine learning research: JMLR},
	Pages = {1579},
	Publisher = {NIH Public Access},
	Title = {Calibrated multivariate regression with application to neural semantic basis discovery},
	Volume = {16},
	Year = {2015}}

@inproceedings{liu2009blockwise,
	Author = {Liu, Han and Palatucci, Mark and Zhang, Jian},
	Booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	Date-Added = {2020-10-01 23:36:55 -0400},
	Date-Modified = {2020-10-01 23:36:55 -0400},
	Pages = {649--656},
	Title = {Blockwise coordinate descent procedures for the multi-task lasso, with applications to neural semantic basis discovery},
	Year = {2009}}

@article{evgeniou2005learning,
	Author = {Evgeniou, Theodoros and Micchelli, Charles A and Pontil, Massimiliano},
	Date-Added = {2020-10-01 23:36:03 -0400},
	Date-Modified = {2020-10-01 23:36:03 -0400},
	Journal = {Journal of machine learning research},
	Number = {Apr},
	Pages = {615--637},
	Title = {Learning multiple tasks with kernel methods},
	Volume = {6},
	Year = {2005}}

@inproceedings{argyriou2007multi,
	Author = {Argyriou, Andreas and Evgeniou, Theodoros and Pontil, Massimiliano},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2020-10-01 23:35:07 -0400},
	Date-Modified = {2020-10-01 23:35:07 -0400},
	Pages = {41--48},
	Title = {Multi-task feature learning},
	Year = {2007}}

@article{caruana1997,
	Abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
	Author = {Caruana, Rich},
	Day = {01},
	Doi = {10.1023/A:1007379606734},
	Issn = {1573-0565},
	Journal = {Machine Learning},
	Month = {Jul},
	Number = {1},
	Pages = {41--75},
	Title = {Multitask Learning},
	Url = {https://doi.org/10.1023/A:1007379606734},
	Volume = {28},
	Year = {1997},
	Bdsk-Url-1 = {https://doi.org/10.1023/A:1007379606734}}

@article{saunshi2020sample,
	Author = {Saunshi, Nikunj and Zhang, Yi and Khodak, Mikhail and Arora, Sanjeev},
	Journal = {arXiv preprint arXiv:2002.11172},
	Title = {A Sample Complexity Separation between Non-Convex and Convex Meta-Learning},
	Year = {2020}}

@inbook{thrun1998,
	Abstract = {Over the past three decades or so, research on machine learning and data mining has led to a wide variety of algorithms that learn general functions from experience. As machine learning is maturing, it has begun to make the successful transition from academic research to various practical applications. Generic techniques such as decision trees and artificial neural networks, for example, are now being used in various commercial and industrial applications (see e.g., [Langley, 1992; Widrow et al., 1994]).},
	Address = {Boston, MA},
	Author = {Thrun, Sebastian and Pratt, Lorien},
	Booktitle = {Learning to Learn},
	Doi = {10.1007/978-1-4615-5529-2_1},
	Editor = {Thrun, Sebastian and Pratt, Lorien},
	Isbn = {978-1-4615-5529-2},
	Pages = {3--17},
	Publisher = {Springer US},
	Title = {Learning to Learn: Introduction and Overview},
	Url = {https://doi.org/10.1007/978-1-4615-5529-2_1},
	Year = {1998},
	Bdsk-Url-1 = {https://doi.org/10.1007/978-1-4615-5529-2_1}}

@article{tripuraneni2020provable,
	Author = {Tripuraneni, Nilesh and Jin, Chi and Jordan, Michael I},
	Journal = {arXiv preprint arXiv:2002.11684},
	Title = {Provable Meta-Learning of Linear Representations},
	Year = {2020}}

@article{baxter2000model,
	Author = {Baxter, Jonathan},
	Journal = {J. Artif. Int. Res.},
	Publisher = {AI Access Foundation},
	Title = {A Model of Inductive Bias Learning},
	Year = {2000}}

@inproceedings{finn2019online,
	Author = {Finn, Chelsea and Rajeswaran, Aravind and Kakade, Sham and Levine, Sergey},
	Booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	Title = {Online Meta-Learning},
	Year = {2019}}

@inproceedings{bullins2019generalize,
	Author = {Bullins, Brian and Hazan, Elad and Kalai, Adam and Livni, Roi},
	Booktitle = {Proceedings of the 30th International Conference on Algorithmic Learning Theory},
	Title = {Generalize Across Tasks: Efficient Algorithms for Linear Representation Learning},
	Year = {2019}}

@article{khodak2019adaptive,
	Author = {Khodak, Mikhail and Balcan, Maria-Florina and Talwalkar, Ameet},
	Journal = {arXiv preprint arXiv:1906.02717},
	Title = {Adaptive Gradient-Based Meta-Learning Methods},
	Year = {2019}}

@article{maurer2016benefit,
	Author = {Maurer, Andreas and Pontil, Massimiliano and Romera-Paredes, Bernardino},
	Journal = {The Journal of Machine Learning Research},
	Number = {1},
	Pages = {2853--2884},
	Publisher = {JMLR. org},
	Title = {The benefit of multitask representation learning},
	Volume = {17},
	Year = {2016}}

@article{wang2020guarantees,
	Author = {Wang, Xiang and Yuan, Shuai and Wu, Chenwei and Ge, Rong},
	Journal = {arXiv preprint arXiv:2006.16495},
	Title = {Guarantees for Tuning the Step Size using a Learning-to-Learn Approach},
	Year = {2020}}

@article{galanti2016theoretical,
	Author = {Galanti, Tomer and Wolf, Lior and Hazan, Tamir},
	Journal = {Information and Inference: A Journal of the IMA},
	Number = {2},
	Pages = {159--209},
	Publisher = {Oxford University Press},
	Title = {A theoretical framework for deep transfer learning},
	Volume = {5},
	Year = {2016}}

@article{alquier2016regret,
	Author = {Alquier, Pierre and Mai, The Tien and Pontil, Massimiliano},
	Journal = {arXiv preprint arXiv:1610.08628},
	Title = {Regret bounds for lifelong learning},
	Year = {2016}}

@inproceedings{mcnamara2017risk,
	Author = {McNamara, Daniel and Balcan, Maria-Florina},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	Organization = {JMLR. org},
	Pages = {2373--2381},
	Title = {Risk bounds for transferring representations with and without fine-tuning},
	Year = {2017}}

@article{raghu2019rapid,
	Author = {Raghu, Aniruddh and Raghu, Maithra and Bengio, Samy and Vinyals, Oriol},
	Journal = {arXiv preprint arXiv:1909.09157},
	Title = {Rapid learning or feature reuse? towards understanding the effectiveness of maml},
	Year = {2019}}

@article{denevi2018incremental,
	Author = {Denevi, Giulia and Ciliberto, Carlo and Stamos, Dimitris and Pontil, Massimiliano},
	Journal = {arXiv preprint arXiv:1803.08089},
	Title = {Incremental learning-to-learn with statistical guarantees},
	Year = {2018}}

@article{du2020few,
	Author = {Du, Simon S and Hu, Wei and Kakade, Sham M and Lee, Jason D and Lei, Qi},
	Journal = {arXiv preprint arXiv:2002.09434},
	Title = {Few-shot learning via learning the representation, provably},
	Year = {2020}}

@inproceedings{rajeswaran2019meta,
	Author = {Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham M and Levine, Sergey},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {113--124},
	Title = {Meta-learning with implicit gradients},
	Year = {2019}}

@inproceedings{snell2017prototypical,
	Author = {J. Snell and K. Swersky and R. Zemel},
	Booktitle = nips,
	Pages = {4077-4087},
	Title = {Prototypical networks for few-shot learning},
	Year = {2017}}

@inproceedings{deng2009imagenet,
	Author = {J. Deng and W. Dong and R. Socher and L. Li and K. Li and F. Li},
	Booktitle = cvpr,
	Pages = {248--255},
	Title = {Imagenet: A large-scale hierarchical image database},
	Year = {2009}}

@article{russakovsky2015imagenet,
	Author = {O. Russakovsky and J. Deng and H. Su and J. Krause and S. Satheesh and S. Ma and Z. Huang and A. Karpathy and A. Khosla and M. Bernstein},
	Journal = ijcv,
	Number = {3},
	Pages = {211--252},
	Title = {Imagenet large scale visual recognition challenge},
	Volume = {115},
	Year = {2015}}

@inproceedings{ravi2016optimization,
	Author = {S. Ravi and H. Larochelle},
	Booktitle = iclr,
	Title = {Optimization as a model for few-shot learning},
	Year = {2017}}

@article{nichol2018reptile,
	Author = {A. Nichol and J. Schulman},
	Journal = {arXiv preprint arXiv:1803.02999},
	Title = {Reptile: a scalable metalearning algorithm},
	Volume = {2},
	Year = {2018}}

@inproceedings{krizhevsky2012imagenet,
	Author = {A. Krizhevsky and I. Sutskever and G. Hinton},
	Booktitle = nips,
	Pages = {1097--1105},
	Title = {Imagenet classification with deep convolutional neural networks},
	Year = {2012}}

@article{ren2018meta,
	Author = {M. Ren and E. Triantafillou and S. Ravi and J. Snell and K. Swersky and J. Tenenbaum and H. Larochelle and R. Zemel},
	Journal = {arXiv preprint arXiv:1803.00676},
	Title = {Meta-learning for semi-supervised few-shot classification},
	Year = {2018}}

@inproceedings{ravi2016optimization,
	Author = {S. Ravi and H. Larochelle},
	Booktitle = iclr,
	Title = {Optimization as a model for few-shot learning},
	Year = {2017}}

@book{anderson2010introduction,
	Author = {Anderson, Greg W and Guionnet, Alice and Zeitouni, Ofer},
	Publisher = {Cambridge university press},
	Title = {An introduction to random matrices},
	Volume = {118},
	Year = {2010}}

@book{bai2010spectral,
	Author = {Bai, Zhidong and Silverstein, Jack W},
	Publisher = {Springer},
	Title = {Spectral analysis of large dimensional random matrices},
	Volume = {20},
	Year = {2010}}

@inproceedings{denevi2018learning,
	Author = {Denevi, Giulia and Ciliberto, Carlo and Stamos, Dimitris and Pontil, Massimiliano},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {10169--10179},
	Title = {Learning to learn around a common mean},
	Year = {2018}}

@article{dobriban2018high,
	Author = {Dobriban, Edgar and Wager, Stefan and others},
	Journal = {The Annals of Statistics},
	Number = {1},
	Pages = {247--279},
	Publisher = {Institute of Mathematical Statistics},
	Title = {High-dimensional asymptotics of prediction: Ridge regression and classification},
	Volume = {46},
	Year = {2018}}

@inproceedings{finn2017model,
	Author = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	Pages = {1126--1135},
	Title = {Model-agnostic meta-learning for fast adaptation of deep networks},
	Year = {2017}}

@inproceedings{hochreiter2001learning,
	Author = {Hochreiter, Sepp and Younger, A Steven and Conwell, Peter R},
	Booktitle = {International Conference on Artificial Neural Networks},
	Organization = {Springer},
	Pages = {87--94},
	Title = {Learning to learn using gradient descent},
	Year = {2001}}

@misc{liang2016cs229t,
	Author = {Liang, Percy},
	Title = {CS229T/STAT231: Statistical Learning Theory (Winter 2016)},
	Year = {2016}}

@inproceedings{lee2019meta,
	Author = {Lee, Kwonjoon and Maji, Subhransu and Ravichandran, Avinash and Soatto, Stefano},
	Booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	Pages = {10657--10665},
	Title = {Meta-learning with differentiable convex optimization},
	Year = {2019}}

@book{van2000asymptotic,
	Author = {Van der Vaart, Aad W},
	Publisher = {Cambridge university press},
	Title = {Asymptotic statistics},
	Volume = {3},
	Year = {2000}}

@inproceedings{zhou2019efficient,
	Author = {Zhou, Pan and Yuan, Xiaotong and Xu, Huan and Yan, Shuicheng and Feng, Jiashi},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {1534--1544},
	Title = {Efficient meta learning via minibatch proximal update},
	Year = {2019}}
