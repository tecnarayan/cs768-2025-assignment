\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2018{\natexlab{a}})Allen-Zhu, Li, and
  Liang]{Allen-Zhu2018-sv}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock November 2018{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1811.04918}.

\bibitem[Allen-Zhu et~al.(2018{\natexlab{b}})Allen-Zhu, Li, and
  Song]{Allen-Zhu2018-bz}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via {Over-Parameterization}.
\newblock November 2018{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1811.03962}.

\bibitem[Ayinde et~al.(2019)Ayinde, Inanc, and Zurada]{Ayinde2019-ne}
Babajide~O Ayinde, Tamer Inanc, and Jacek~M Zurada.
\newblock Redundant feature pruning for accelerated inference in deep neural
  networks.
\newblock \emph{Neural networks: the official journal of the International
  Neural Network Society}, May 2019.
\newblock ISSN 0893-6080.
\newblock \doi{10.1016/j.neunet.2019.04.021}.
\newblock URL
  \url{http://www.sciencedirect.com/science/article/pii/S0893608019301273}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{imagenet_cvpr09}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{CVPR09}, 2009.

\bibitem[Du and Lee(2018)]{Du2018-ig}
Simon~S Du and Jason~D Lee.
\newblock On the power of over-parametrization in neural networks with
  quadratic activation.
\newblock March 2018.
\newblock URL \url{http://arxiv.org/abs/1803.01206}.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{Du2018-hu}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock October 2018.
\newblock URL \url{http://arxiv.org/abs/1810.02054}.

\bibitem[Frankle and Carbin(2019)]{Frankle2019-hp}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{http://arxiv.org/abs/1803.03635}.

\bibitem[Frankle et~al.(2019)Frankle, Dziugaite, Roy, and
  Carbin]{Frankle2019-wj}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel~M Roy, and Michael Carbin.
\newblock The lottery ticket hypothesis at scale.
\newblock March 2019.
\newblock URL \url{http://arxiv.org/abs/1903.01611}.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{Gale2019-zf}
Trevor Gale, Erich Elsen, and Sara Hooker.
\newblock The state of sparsity in deep neural networks.
\newblock February 2019.
\newblock URL \url{http://arxiv.org/abs/1902.09574}.

\bibitem[Glorot and Bengio(2010)]{Glorot2010-yu}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock \emph{Aistats}, 9:\penalty0 249--256, 2010.
\newblock ISSN 1532-4435.
\newblock \doi{10.1.1.207.2059}.
\newblock URL
  \url{http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf}.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{guo_network_surgery}
Yiwen Guo, Anbang Yao, and Yurong Chen.
\newblock Dynamic network surgery for efficient dnns.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems 29}, pages
  1379--1387. Curran Associates, Inc., 2016.
\newblock URL
  \url{http://papers.nips.cc/paper/6165-dynamic-network-surgery-for-efficient-dnns.pdf}.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in neural information processing systems}, pages
  1135--1143, 2015.

\bibitem[Hanin and Rolnick(2018)]{hanin_init}
Boris Hanin and David Rolnick.
\newblock How to start training: The effect of initialization and architecture.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 571--581. Curran Associates, Inc., 2018.
\newblock URL
  \url{http://papers.nips.cc/paper/7338-how-to-start-training-the-effect-of-initialization-and-architecture.pdf}.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Kornblith et~al.(2018)Kornblith, Shlens, and Le]{Kornblith2018-qw}
Simon Kornblith, Jonathon Shlens, and Quoc~V Le.
\newblock Do better {ImageNet} models transfer better?
\newblock May 2018.
\newblock URL \url{http://arxiv.org/abs/1805.08974}.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Li et~al.(2016)Li, Kadav, Durdanovic, Samet, and Graf]{li2016pruning}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock \emph{arXiv preprint arXiv:1608.08710}, 2016.

\bibitem[Liu et~al.(2019)Liu, Sun, Zhou, Huang, and Darrell]{Liu2019-ue}
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
\newblock Rethinking the value of network pruning.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{http://arxiv.org/abs/1810.05270}.

\bibitem[Molchanov et~al.(2017)Molchanov, Ashukha, and
  Vetrov]{molchanov2017variational}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 2498--2507. JMLR. org, 2017.

\bibitem[Molchanov et~al.(2016)Molchanov, Tyree, Karras, Aila, and
  Kautz]{Molchanov2016-rx}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock November 2016.
\newblock URL \url{http://arxiv.org/abs/1611.06440}.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and
  Srebro]{Neyshabur2014-yn}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock December 2014.
\newblock URL \url{http://arxiv.org/abs/1412.6614}.

\bibitem[Neyshabur et~al.(2019)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{Neyshabur2019-gk}
Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan`
  Srebro.
\newblock The role of over-parametrization in generalization of neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL
  \url{https://openreview.net/forum?id=BygfghAcYX&noteId=BygfghAcYX}.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in pytorch.
\newblock In \emph{NIPS-W}, 2017.

\bibitem[Pretorius et~al.(2018)Pretorius, van Biljon, Kroon, and
  Kamper]{pretorius_critical_init}
Arnu Pretorius, Elan van Biljon, Steve Kroon, and Herman Kamper.
\newblock Critical initialisation for deep signal propagation in noisy
  rectifier neural networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 5717--5726. Curran Associates, Inc., 2018.

\bibitem[Qin et~al.(2019)Qin, Yu, Liu, and Chen]{qin2019interpretable}
Zhuwei Qin, Fuxun Yu, Chenchen Liu, and Xiang Chen.
\newblock {INTERPRETABLE} {CONVOLUTIONAL} {FILTER} {PRUNING}, 2019.
\newblock URL \url{https://openreview.net/forum?id=BJ4BVhRcYX}.

\bibitem[Schoenholz et~al.(2016)Schoenholz, Gilmer, Ganguli, and
  Sohl-Dickstein]{Schoenholz2016-nk}
Samuel~S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein.
\newblock Deep information propagation.
\newblock In \emph{International Conference on Learning Representations}, 2016.
\newblock URL \url{http://arxiv.org/abs/1611.01232}.

\bibitem[Simonyan and Zisserman(2015)]{Simonyan2015VeryDC}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{International Conference on Learning Representations}, 2015.
\newblock URL \url{https://arxiv.org/abs/1409.1556}.

\bibitem[Tan et~al.(2018)Tan, Sun, Kong, Zhang, Yang, and Liu]{Tan2018-bt}
Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang, and Chunfang Liu.
\newblock A survey on deep transfer learning.
\newblock In \emph{ICANN}, 2018.
\newblock URL \url{http://arxiv.org/abs/1808.01974}.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017/online}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem[Yang and Schoenholz(2018)]{yang2018deep}
Greg Yang and Sam~S. Schoenholz.
\newblock Deep mean field theory: Layerwise variance and width variation as
  methods to control gradient explosion.
\newblock In \emph{International Conference on Learning Representations
  Workshop Track}, 2018.
\newblock URL \url{https://openreview.net/forum?id=rJGY8GbR-}.

\bibitem[Yosinski et~al.(2014)Yosinski, Clune, Bengio, and
  Lipson]{Yosinski2014-cz}
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
\newblock How transferable are features in deep neural networks?
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~D. Lawrence, and K.~Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing Systems
  27}, pages 3320--3328. Curran Associates, Inc., 2014.
\newblock URL
  \url{http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf}.

\bibitem[Zhang et~al.(2019)Zhang, Dauphin, and Ma]{zhang2018residual}
Hongyi Zhang, Yann~N. Dauphin, and Tengyu Ma.
\newblock Residual learning without normalization via better initialization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=H1gsz30cKX}.

\bibitem[Zhou et~al.(2017)Zhou, Lapedriza, Khosla, Oliva, and
  Torralba]{zhou2017places}
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba.
\newblock Places: A 10 million image database for scene recognition.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2017.

\bibitem[Zhu and Gupta(2018)]{Zhu2018-km}
Michael~H Zhu and Suyog Gupta.
\newblock To prune, or not to prune: Exploring the efficacy of pruning for
  model compression.
\newblock In \emph{International Conference on Learning Representations
  Workshop Track}, February 2018.
\newblock URL \url{https://openreview.net/pdf?id=Sy1iIDkPM}.

\bibitem[Zoph et~al.(2017)Zoph, Vasudevan, Shlens, and Le]{Zoph2017-ac}
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc~V Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock July 2017.
\newblock URL \url{http://arxiv.org/abs/1707.07012}.

\end{thebibliography}
