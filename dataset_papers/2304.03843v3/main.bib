@inproceedings{wei2022chain,
    title={Chain of Thought Prompting Elicits Reasoning in Large Language Models},
    author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and brian ichter and Fei Xia and Ed H. Chi and Quoc V Le and Denny Zhou},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=_VjQlMeSB_J}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}

@inproceedings{zelikman2022star,
    title={{ST}a{R}: Bootstrapping Reasoning With Reasoning},
    author={Eric Zelikman and Yuhuai Wu and Jesse Mu and Noah Goodman},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=_3ELRdg2sgI}
}

@inproceedings{chan2022data,
    title={Data Distributional Properties Drive Emergent In-Context Learning in Transformers},
    author={Stephanie C.Y. Chan and Adam Santoro and Andrew Kyle Lampinen and Jane X Wang and Aaditya K Singh and Pierre Harvey Richemond and James McClelland and Felix Hill},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=lHj-q9BSRjF}
}

@article{shepard2008step,
  title={The step to rationality: The efficacy of thought experiments in science, ethics, and free will},
  author={Shepard, Roger N},
  journal={Cognitive Science},
  volume={32},
  number={1},
  pages={3--35},
  year={2008},
  publisher={Wiley Online Library}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@book{dennett2013intuition,
  title={Intuition pumps and other tools for thinking},
  author={Dennett, Daniel C},
  year={2013},
  publisher={WW Norton \& Company}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{dohan2022language,
  title={Language Model Cascades},
  author={Dohan, David and Xu, Winnie and Lewkowycz, Aitor and Austin, Jacob and Bieber, David and Lopes, Raphael Gontijo and Wu, Yuhuai and Michalewski, Henryk and Saurous, Rif A and Sohl-dickstein, Jascha and others},
  journal={arXiv preprint arXiv:2207.10342},
  year={2022}
}

@article{lampinen2022can,
  title={Can language models learn from explanations in context?},
  author={Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie CY and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L and Wang, Jane X and Hill, Felix},
  journal={arXiv preprint arXiv:2204.02329},
  year={2022}
}

@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={993--1022},
  year={2003}
}

@article{blei2007correlated,
author = {David M. Blei and John D. Lafferty},
title = {{A correlated topic model of Science}},
volume = {1},
journal = {The Annals of Applied Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {17 -- 35},
keywords = {approximate posterior inference, hierarchical models, text analysis, variational methods},
year = {2007},
doi = {10.1214/07-AOAS114},
URL = {https://doi.org/10.1214/07-AOAS114}
}

@article{bubic2010prediction,
  title={Prediction, cognition and the brain},
  author={Bubic, Andreja and Von Cramon, D Yves and Schubotz, Ricarda I},
  journal={Frontiers in human neuroscience},
  pages={25},
  year={2010},
  publisher={Frontiers}
}

@article{griffiths2006optimal,
  title={Optimal predictions in everyday cognition},
  author={Griffiths, Thomas L and Tenenbaum, Joshua B},
  journal={Psychological science},
  volume={17},
  number={9},
  pages={767--773},
  year={2006},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{loshchilov2019decoupled,
    title={Decoupled Weight Decay Regularization},
    author={Ilya Loshchilov and Frank Hutter},
    booktitle={International Conference on Learning Representations},
    year={2019},
    url={https://openreview.net/forum?id=Bkg6RiCqY7},
}

@article{Gershman2014AmortizedII,
  title={Amortized Inference in Probabilistic Reasoning},
  author={Samuel J. Gershman and Noah D. Goodman},
  journal={Cognitive Science},
  year={2014},
  volume={36}
}

@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations},
  pages={38--45},
  year={2020}
}

@inproceedings{sennrich2016neural,
  title={Neural Machine Translation of Rare Words with Subword Units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1715--1725},
  year={2016}
}

@inproceedings{kingma2015adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  booktitle={Proceedings of the 3rd International Conference on Learning Representations (ICLR)},
  year={2015}
}

@article{zhu2023computation,
  title={Computation-Limited Bayesian Updating},
  author={Zhu, Jian-Qiao and Sanborn, Adam and Chater, Nick and Griffiths, Thomas L},
  publisher={PsyArXiv},
  year={2023}
}

@inproceedings{wang2022self,
  title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc V and Chi, Ed H and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{liu2023federated,
  title={Federated Prompting and Chain-of-Thought Reasoning for Improving LLMs Answering},
  author={Liu, Xiangyang and Pang, Tianqi and Fan, Chenyou},
  journal={arXiv preprint arXiv:2304.13911},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{srivastava2023beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={BIG-bench authors},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  url={https://openreview.net/forum?id=uyTL5Bvosj},
  note={}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2305.10601},
  year={2023}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@article{evans2003two,
  title={In two minds: dual-process accounts of reasoning},
  author={Evans, Jonathan St BT},
  journal={Trends in cognitive sciences},
  volume={7},
  number={10},
  pages={454--459},
  year={2003},
  publisher={Elsevier}
}
