@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})




@article{recovery_guarantee,
  author    = {Kai Zhong and
               Zhao Song and
               Prateek Jain and
               Peter L. Bartlett and
               Inderjit S. Dhillon},
  title     = {Recovery Guarantees for One-hidden-layer Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1706.03175},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03175},
  archivePrefix = {arXiv},
  eprint    = {1706.03175},
  timestamp = {Mon, 13 Aug 2018 16:48:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ZhongS0BD17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{computer_vision,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@ARTICLE{nlp, 
author={G. Hinton and L. Deng and D. Yu and G. E. Dahl and A. Mohamed and N. Jaitly and A. Senior and V. Vanhoucke and P. Nguyen and T. N. Sainath and B. Kingsbury}, 
journal={IEEE Signal Processing Magazine}, 
title={Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups}, 
year={2012}, 
volume={29}, 
number={6}, 
pages={82-97}, 
keywords={feedforward neural nets;Gaussian processes;hidden Markov models;speech recognition;deep neural networks;acoustic modeling;speech recognition;hidden Markov models;temporal variability;Gaussian mixture models;feed-forward neural network;posterior probabilities;HMM states;Automatic speech recognition;Speech recognition;Hidden Markov models;Training;Gaussian processes;Acoustics;Neural networks;Data models}, 
doi={10.1109/MSP.2012.2205597}, 
ISSN={1053-5888}, 
month={Nov},}

@article{zhang_onelayer_relu,
  title={Learning One-hidden-layer ReLU Networks via Gradient Descent},
  author={Zhang, Xiao and Yu, Yaodong and Wang, Lingxiao and Gu, Quanquan},
  journal={arXiv preprint arXiv:1806.07808},
  year={2018}
}

@article{brutzkus_localminima,
  title={Globally optimal gradient descent for a convnet with gaussian inputs},
  author={Brutzkus, Alon and Globerson, Amir},
  journal={arXiv preprint arXiv:1702.07966},
  year={2017}
}

@article{du_onelayer_cnn,
  title={When is a Convolutional Filter Easy to Learn?},
  author={Du, Simon S and Lee, Jason D and Tian, Yuandong},
  journal={arXiv preprint arXiv:1709.06129},
  year={2017}
}

@InProceedings{du_onelayer_cnn2,
  title = 	 {Gradient Descent Learns One-hidden-layer {CNN}: Don’t be Afraid of Spurious Local Minima},
  author = 	 {Du, Simon and Lee, Jason and Tian, Yuandong and Singh, Aarti and Poczos, Barnabas},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1339--1348},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/du18b/du18b.pdf},
  url = 	 {http://proceedings.mlr.press/v80/du18b.html},
  abstract = 	 {We consider the problem of learning an one-hidden-layer neural network with non-overlapping convolutional layer and ReLU activation function, i.e., $f(Z; w, a) = \sum_j a_j\sigma(w^\top Z_j)$, in which both the convolutional weights $w$ and the output weights $a$ are parameters to be learned. We prove that with Gaussian input $\mathbf{Z}$ there is a spurious local minimizer. Surprisingly, in the presence of the spurious local minimizer, starting from randomly initialized weights, gradient descent with weight normalization can still be proven to recover the true parameters with constant probability (which can be boosted to probability $1$ with multiple restarts). We also show that with constant probability, the same procedure could also converge to the spurious local minimum, showing that the local minimum plays a non-trivial role in the dynamics of gradient descent. Furthermore, a quantitative analysis shows that the gradient descent dynamics has two phases: it starts off slow, but converges much faster after several iterations.}
}


@article{tian_onelayer_relu_gaussian,
  title={An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis},
  author={Tian, Yuandong},
  journal={arXiv preprint arXiv:1703.00560},
  year={2017}
}

@article{zhong_onelayer_smooth,
  title={Recovery guarantees for one-hidden-layer neural networks},
  author={Zhong, Kai and Song, Zhao and Jain, Prateek and Bartlett, Peter L and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:1706.03175},
  year={2017}
}

@article{soltanolkotabi_overparameterized,
  title={Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
  author={Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D},
  journal={IEEE Transactions on Information Theory},
  year={2018},
  publisher={IEEE}
}


@article{fu_onelayer_smooth,
  title={Local Geometry of One-Hidden-Layer Neural Networks for Logistic Regression},
  author={Fu, Haoyu and Chi, Yuejie and Liang, Yingbin},
  journal={arXiv preprint arXiv:1802.06463},
  year={2018}
}


@article{shallow_power,
title = "Approximation capabilities of multilayer feedforward networks",
journal = "Neural Networks",
volume = "4",
number = "2",
pages = "251 - 257",
year = "1991",
issn = "0893-6080",
doi = "https://doi.org/10.1016/0893-6080(91)90009-T",
url = "http://www.sciencedirect.com/science/article/pii/089360809190009T",
author = "Kurt Hornik",
keywords = "Multilayer feedforward networks, Activation function, Universal approximation capabilities, Input environment measure, () approximation, Uniform approximation, Sobolev spaces, Smooth approximation",
abstract = "We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives."
}

@article{du_overparameterized,
  author    = {Simon S. Du and
               Xiyu Zhai and
               Barnab{\'{a}}s P{\'{o}}czos and
               Aarti Singh},
  title     = {Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1810.02054},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.02054},
  archivePrefix = {arXiv},
  eprint    = {1810.02054},
  timestamp = {Tue, 30 Oct 2018 10:49:09 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-02054},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{li_overparameterized,
  author    = {Yuanzhi Li and
               Yingyu Liang},
  title     = {Learning Overparameterized Neural Networks via Stochastic Gradient
               Descent on Structured Data},
  journal   = {CoRR},
  volume    = {abs/1808.01204},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.01204},
  archivePrefix = {arXiv},
  eprint    = {1808.01204},
  timestamp = {Sun, 02 Sep 2018 15:01:55 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1808-01204},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{brutzkus_overparameterized_linear,
  author    = {Alon Brutzkus and
               Amir Globerson and
               Eran Malach and
               Shai Shalev{-}Shwartz},
  title     = {{SGD} Learns Over-parameterized Networks that Provably Generalize
               on Linearly Separable Data},
  journal   = {CoRR},
  volume    = {abs/1710.10174},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.10174},
  archivePrefix = {arXiv},
  eprint    = {1710.10174},
  timestamp = {Mon, 13 Aug 2018 16:47:33 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-10174},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zhu_rnn,
  author    = {Zeyuan Allen{-}Zhu and
               Yuanzhi Li and
               Zhao Song},
  title     = {On the Convergence Rate of Training Recurrent Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1810.12065},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.12065},
  archivePrefix = {arXiv},
  eprint    = {1810.12065},
  timestamp = {Thu, 01 Nov 2018 18:03:07 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-12065},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{elman_rnn,
title = "Finding structure in time",
journal = "Cognitive Science",
volume = "14",
number = "2",
pages = "179 - 211",
year = "1990",
issn = "0364-0213",
doi = "https://doi.org/10.1016/0364-0213(90)90002-E",
url = "http://www.sciencedirect.com/science/article/pii/036402139090002E",
author = "Jeffrey L. Elman",
abstract = "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
}

@article{cho_rnn,
  author    = {KyungHyun Cho and
               Bart van Merrienboer and
               Dzmitry Bahdanau and
               Yoshua Bengio},
  title     = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
  journal   = {CoRR},
  volume    = {abs/1409.1259},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.1259},
  archivePrefix = {arXiv},
  eprint    = {1409.1259},
  timestamp = {Mon, 13 Aug 2018 16:47:23 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ChoMBB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{chung_rnn,
  author    = {Junyoung Chung and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               KyungHyun Cho and
               Yoshua Bengio},
  title     = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
               Modeling},
  journal   = {CoRR},
  volume    = {abs/1412.3555},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.3555},
  archivePrefix = {arXiv},
  eprint    = {1412.3555},
  timestamp = {Mon, 13 Aug 2018 16:47:38 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ChungGCB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{khrulkov_rnn,
  author    = {Valentin Khrulkov and
               Alexander Novikov and
               Ivan V. Oseledets},
  title     = {Expressive power of recurrent neural networks},
  journal   = {CoRR},
  volume    = {abs/1711.00811},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.00811},
  archivePrefix = {arXiv},
  eprint    = {1711.00811},
  timestamp = {Mon, 13 Aug 2018 16:45:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1711-00811},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{infinite_width,
  author    = {Pedro Savarese and
               Itay Evron and
               Daniel Soudry and
               Nathan Srebro},
  title     = {How do infinite width bounded norm networks look in function space?},
  journal   = {CoRR},
  volume    = {abs/1902.05040},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.05040},
  archivePrefix = {arXiv},
  eprint    = {1902.05040},
  timestamp = {Sat, 02 Mar 2019 16:35:43 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1902-05040},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{neyshabur_reg,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}

@book{boyd_convex,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@ARTICLE{robust_candes, 
author={E. J. {Candes} and J. {Romberg} and T. {Tao}}, 
journal={IEEE Transactions on Information Theory}, 
title={Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information}, 
year={2006}, 
volume={52}, 
number={2}, 
pages={489-509}, 
keywords={image reconstruction;linear programming;convex programming;minimisation;Fourier analysis;image sampling;piecewise constant techniques;probability;sparse matrices;indeterminancy;signal reconstruction;signal sampling;robust uncertainty principle;signal reconstruction;incomplete frequency information;discrete-time signal;Fourier coefficient;minimization problem;convex optimization;image reconstruction;linear programming;sparse random matrix;trigonometric expansion;nonlinear sampling theorem;piecewise constant object;probability value;Robustness;Uncertainty;Signal reconstruction;Frequency;Image reconstruction;Mathematics;Biomedical imaging;Sampling methods;Linear programming;Signal processing;Convex optimization;duality in optimization;free probability;image reconstruction;linear programming;random matrices;sparsity;total-variation minimization;trigonometric expansions;uncertainty principle}, 
doi={10.1109/TIT.2005.862083}, 
ISSN={0018-9448}, 
month={Feb},}

@Article{sparsity_zhao,
author="Zhao, Yun-Bin",
title="Equivalence and Strong Equivalence Between the Sparsest and Least {$\ell_1$}-Norm Nonnegative Solutions of Linear Systems and Their Applications",
journal="Journal of the Operations Research Society of China",
year="2014",
month="Jun",
day="01",
volume="2",
number="2",
pages="171--193",
issn="2194-6698",
doi="10.1007/s40305-014-0043-1",
url="https://doi.org/10.1007/s40305-014-0043-1"
}

@article{quantized_hartmut,
  title={Gradient descent quantizes ReLU network features},
  author={Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
  journal={arXiv preprint arXiv:1803.08367},
  year={2018}
}

@book{semiinfinite_goberna,
author = {Goberna, Miguel Angel and L\'{o}pez-Cerd\'{a}, Marco},
year = {1998},
month = {01},
pages = {},
title = {Linear semi-infinite optimization},
doi = {10.1007/978-1-4899-8044-1_3}
}

@book{TalagrandBook,
  title={Probability in Banach Spaces: isoperimetry and processes},
  author={Ledoux, Michel and Talagrand, Michel},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@inproceedings{earlystopping_caruna,
  title={Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping},
  author={Caruana, Rich and Lawrence, Steve and Giles, C Lee},
  booktitle={Advances in neural information processing systems},
  pages={402--408},
  year={2001}
}

@inproceedings{weightdecay_krogh,
  title={A simple weight decay can improve generalization},
  author={Krogh, Anders and Hertz, John A},
  booktitle={Advances in neural information processing systems},
  pages={950--957},
  year={1992}
}


@misc{cifar10,
  title={The {CIFAR}-10 dataset},
  author={Krizhevsky, Alex and Nair, Vinod and Hinton, Geoffrey},
  howpublished="\url{http://www. cs. toronto. edu/kriz/cifar. html}",
  year={2014}
}

@misc{news20,
  title={20 Newsgroups},
  howpublished="\url{http://qwone.com/~jason/20Newsgroups/}"
}

@misc{rcv,
  title={Text datasets in matlab format},
  howpublished="\url{http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.htm}"
}

@misc{mnist,
  title={The {MNIST} database of handwritten digits},
  author={LeCun, Yann},
  howpublished ="\url{http://yann. lecun. com/exdb/mnist/}"
}

@misc{regression,
author = "L. Torgo",
institution = "University of Porto, Department of Computer Science",
title = "Regression Data Sets",
howpublished = "\url{http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html}"
}

@incollection{kmenas_andrewng,
  title={Learning feature representations with k-means},
  author={Coates, Adam and Ng, Andrew Y},
  booktitle={Neural networks: Tricks of the trade},
  pages={561--580},
  year={2012},
  publisher={Springer}
}

@article{mitchell1997mcgraw,
  title={Mcgraw-hill science},
  author={Mitchell, Tom M and Learning, Machine},
  journal={Engineering/Math},
  volume={1},
  pages={27},
  year={1997}
}

@article{zhao2006model,
  title={On model selection consistency of Lasso},
  author={Zhao, Peng and Yu, Bin},
  journal={Journal of Machine learning research},
  volume={7},
  number={Nov},
  pages={2541--2563},
  year={2006}
}

@article{fung2011equivalence,
  title={Equivalence of minimal  $\ell_0$-and $\ell_p$-norm solutions of linear equalities, inequalities and linear programs for sufficiently small p},
  author={Fung, GM and Mangasarian, OL},
  journal={Journal of optimization theory and applications},
  volume={151},
  number={1},
  pages={1--10},
  year={2011},
  publisher={Springer}
}


@article{frank_wolfe,
author = {Frank, Marguerite and Wolfe, Philip},
title = {An algorithm for quadratic programming},
journal = {Naval Research Logistics Quarterly},
volume = {3},
number = {1‐2},
pages = {95-110},
doi = {10.1002/nav.3800030109},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800030109},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800030109},
year = {1956}
}

@article{sion_minimax,
author = "Sion, Maurice",
fjournal = "Pacific Journal of Mathematics",
journal = "Pacific J. Math.",
number = "1",
pages = "171--176",
publisher = "Pacific Journal of Mathematics, A Non-profit Corporation",
title = "On general minimax theorems.",
url = "https://projecteuclid.org:443/euclid.pjm/1103040253",
volume = "8",
year = "1958"
}


@article{implicit_reg_blanc,
  author    = {Guy Blanc and
               Neha Gupta and
               Gregory Valiant and
               Paul Valiant},
  title     = {Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck
               like process},
  journal   = {CoRR},
  volume    = {abs/1904.09080},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.09080},
  archivePrefix = {arXiv},
  eprint    = {1904.09080},
  timestamp = {Fri, 26 Apr 2019 13:18:53 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1904-09080},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{margin_theory_tengyu,
  title={On the margin theory of feedforward neural networks},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  journal={arXiv preprint arXiv:1810.05369},
  year={2018}
}

@article{lazy_training_bach,
  title={A note on lazy training in supervised differentiable programming},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  year={2018}
}

@inproceedings{rudelson2010non,
  title={Non-asymptotic theory of random matrices: extreme singular values},
  author={Rudelson, Mark and Vershynin, Roman},
  booktitle={Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4 Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II--IV: Invited Lectures},
  pages={1576--1602},
  year={2010},
  organization={World Scientific}
}

@article{understanding_zhang,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}

@article{baksalary2007particular,
  title={Particular formulae for the Moore--Penrose inverse of a columnwise partitioned matrix},
  author={Baksalary, Jerzy K and Baksalary, Oskar Maria},
  journal={Linear algebra and its applications},
  volume={421},
  number={1},
  pages={16--23},
  year={2007},
  publisher={Elsevier}
}

@inproceedings{bengio2006convex,
  title={Convex neural networks},
  author={Bengio, Yoshua and Roux, Nicolas L and Vincent, Pascal and Delalleau, Olivier and Marcotte, Patrice},
  booktitle={Advances in neural information processing systems},
  pages={123--130},
  year={2006}
}

@article{bach2017breaking,
  title={Breaking the curse of dimensionality with convex neural networks},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={629--681},
  year={2017},
  publisher={JMLR. org}
}

@article{guruswami2009hardness,
  title={Hardness of learning halfspaces with noise},
  author={Guruswami, Venkatesan and Raghavendra, Prasad},
  journal={SIAM Journal on Computing},
  volume={39},
  number={2},
  pages={742--765},
  year={2009},
  publisher={SIAM}
}

@article{ha1980minimax,
  title={Minimax and fixed point theorems},
  author={Ha, Chung-Wei},
  journal={Mathematische Annalen},
  volume={248},
  number={1},
  pages={73--77},
  year={1980},
  publisher={Springer}
}

@misc{spgl1,
  author = {E. van den Berg and M. P. Friedlander},
  title = {{SPGL1}: A solver for large-scale sparse reconstruction},
  note = {http://www.cs.ubc.ca/labs/scl/spgl1},
  month = {June},
  year ={2007}
}


@article{superscs,
  title={SuperMann: a superlinearly convergent algorithm for finding fixed points of nonexpansive operators},
  author={Themelis, Andreas and Patrinos, Panagiotis},
  journal={IEEE Transactions on Automatic Control},
  year={2019},
  publisher={IEEE}
}

@inproceedings{ntk_jacot,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}

@article{ntk_1d,
  title={Gradient Dynamics of Shallow Univariate ReLU Networks},
  author={Williams, Francis and Trager, Matthew and Silva, Claudio and Panozzo, Daniele and Zorin, Denis and Bruna, Joan},
  journal={arXiv preprint arXiv:1906.07842},
  year={2019}
}

@incollection{neal_priorinfinite,
  title={Priors for infinite networks},
  author={Neal, Radford M},
  booktitle={Bayesian Learning for Neural Networks},
  pages={29--53},
  year={1996},
  publisher={Springer}
}

@article{gaussian_infinite,
  title={Gaussian process behaviour in wide deep neural networks},
  author={Matthews, Alexander G de G and Rowland, Mark and Hron, Jiri and Turner, Richard E and Ghahramani, Zoubin},
  journal={arXiv preprint arXiv:1804.11271},
  year={2018}
}

@article{gaussian_infinite2,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
}

@article{ntk_relu_1d,
  title={On the inductive bias of neural tangent kernels},
  author={Bietti, Alberto and Mairal, Julien},
  journal={arXiv preprint arXiv:1905.12173},
  year={2019}
}




@InProceedings{du_planted,
  title = 	 {Width Provably Matters in Optimization for Deep Linear Neural Networks},
  author = 	 {Du, Simon and Hu, Wei},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {1655--1664},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/du19a/du19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/du19a.html},
  abstract = 	 {We prove that for an $L$-layer fully-connected linear neural network, if the width of every hidden layer is $\widetilde{\Omega}\left(L \cdot r \cdot d_{out} \cdot \kappa^3 \right)$, where $r$ and $\kappa$ are the rank and the condition number of the input data, and $d_{out}$ is the output dimension, then gradient descent with Gaussian random initialization converges to a global minimum at a linear rate. The number of iterations to find an $\epsilon$-suboptimal solution is $O(\kappa \log(\frac{1}{\epsilon}))$. Our polynomial upper bound on the total running time for wide deep linear networks and the $\exp\left(\Omega\left(L\right)\right)$ lower bound for narrow deep linear neural networks [Shamir, 2018] together demonstrate that wide layers are necessary for optimizing deep models.}
}

@article{arora_cntk,
  author    = {Sanjeev Arora and
               Simon S. Du and
               Wei Hu and
               Zhiyuan Li and
               Ruslan Salakhutdinov and
               Ruosong Wang},
  title     = {On Exact Computation with an Infinitely Wide Neural Net},
  journal   = {CoRR},
  volume    = {abs/1904.11955},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.11955},
  archivePrefix = {arXiv},
  eprint    = {1904.11955},
  timestamp = {Mon, 25 Nov 2019 14:34:49 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1904-11955},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{parhi_minimum,
  title={Minimum ``Norm'' Neural Networks are Splines},
  author={Parhi, Rahul and Nowak, Robert D},
  journal={arXiv preprint arXiv:1910.02333},
  year={2019}
}

@article{shapiro2009semi,
  title={Semi-infinite programming, duality, discretization and optimality conditions},
  author={Shapiro, Alexander},
  journal={Optimization},
  volume={58},
  number={2},
  pages={133--161},
  year={2009},
  publisher={Taylor \& Francis}
}

@article{stanley2004introduction,
  title={An introduction to hyperplane arrangements},
  author={Stanley, Richard P and others},
  journal={Geometric combinatorics},
  volume={13},
  pages={389--496},
  year={2004}
}

@article{ojha2000enumeration,
  title={Enumeration of linear threshold functions from the lattice of hyperplane intersections},
  author={Ojha, Piyush C},
  journal={IEEE Transactions on Neural Networks},
  volume={11},
  number={4},
  pages={839--850},
  year={2000},
  publisher={IEEE}
}

@article{cover1965geometrical,
  title={Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition},
  author={Cover, Thomas M},
  journal={IEEE transactions on electronic computers},
  number={3},
  pages={326--334},
  year={1965},
  publisher={IEEE}
}

@article{winder1966partitions,
  title={Partitions of N-space by hyperplanes},
  author={Winder, RO},
  journal={SIAM Journal on Applied Mathematics},
  volume={14},
  number={4},
  pages={811--818},
  year={1966},
  publisher={SIAM}
}

@article{boob2018complexity,
  title={Complexity of training relu neural network},
  author={Boob, Digvijay and Dey, Santanu S and Lan, Guanghui},
  journal={arXiv preprint arXiv:1809.10787},
  year={2018}
}

@inproceedings{blum1989threenode,
  title={Training a 3-node neural network is NP-complete},
  author={Blum, Avrim and Rivest, Ronald L},
  booktitle={Advances in neural information processing systems},
  pages={494--501},
  year={1989}
}

@article{bienstock2018principled,
  title={Principled deep neural network training through linear programming},
  author={Bienstock, Daniel and Mu{\~n}oz, Gonzalo and Pokutta, Sebastian},
  journal={arXiv preprint arXiv:1810.03218},
  year={2018}
}

@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge University Press}
}

@article{recht2010guaranteed,
  title={Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization},
  author={Recht, Benjamin and Fazel, Maryam and Parrilo, Pablo A},
  journal={SIAM review},
  volume={52},
  number={3},
  pages={471--501},
  year={2010},
  publisher={SIAM}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8139--8148},
  year={2019}
}

@article{edelsbrunner1986constructing,
  title={Constructing arrangements of lines and hyperplanes with applications},
  author={Edelsbrunner, Herbert and O’Rourke, Joseph and Seidel, Raimund},
  journal={SIAM Journal on Computing},
  volume={15},
  number={2},
  pages={341--363},
  year={1986},
  publisher={SIAM}
}

@misc{regression,
author = {L. Torgo},
institution = {University of Porto, Department of Computer Science},
title = {Regression Data Sets},
howpublished = {\url{http://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html}}
}

@misc{cvx,
  author       = {Michael Grant and Stephen Boyd},
  title        = {{CVX}: Matlab Software for Disciplined Convex Programming, version 2.1},
  howpublished = {\url{http://cvxr.com/cvx}},
  month        = mar,
  year         = 2014
}

@article{cvxpy,
  author  = {Steven Diamond and Stephen Boyd},
  title   = {{CVXPY}: A {P}ython-Embedded Modeling Language for Convex Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {83},
  pages   = {1--5},
}
@article{cvxpy_rewriting,
  author  = {Akshay Agrawal and Robin Verschueren and Steven Diamond and Stephen Boyd},
  title   = {A Rewriting System for Convex Optimization Problems},
  journal = {Journal of Control and Decision},
  year    = {2018},
  volume  = {5},
  number  = {1},
  pages   = {42--60},
}

@article{tutuncu2001sdpt3,
  title={SDPT3—a Matlab software package for semidefinite-quadratic-linear programming, version 3.0},
  author={T{\"u}t{\"u}nc{\"u}, RH and Toh, KC and Todd, MJ},
  journal={Web page http://www. math. nus. edu. sg/mattohkc/sdpt3. html},
  year={2001}
}

@article{yuan2006model,
  title={Model selection and estimation in regression with grouped variables},
  author={Yuan, Ming and Lin, Yi},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={68},
  number={1},
  pages={49--67},
  year={2006},
  publisher={Wiley Online Library}
}

@inproceedings{zhang2019multibranch,
  title={Deep neural networks with multi-branch architectures are intrinsically less non-convex},
  author={Zhang, Hongyang and Shao, Junru and Salakhutdinov, Ruslan},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1099--1109},
  year={2019}
}

@inproceedings{chollet2017xception,
  title={Xception: Deep learning with depthwise separable convolutions},
  author={Chollet, Fran{\c{c}}ois},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1251--1258},
  year={2017}
}

@article{iandola2016squeezenet,
  title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and< 0.5 MB model size},
  author={Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1602.07360},
  year={2016}
}

@inproceedings{szegedy2017inception,
  title={Inception-v4, inception-resnet and the impact of residual connections on learning},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
  booktitle={Thirty-first AAAI conference on artificial intelligence},
  year={2017}
}

@inproceedings{veit2016ensembleresidual,
  title={Residual networks behave like ensembles of relatively shallow networks},
  author={Veit, Andreas and Wilber, Michael J and Belongie, Serge},
  booktitle={Advances in neural information processing systems},
  pages={550--558},
  year={2016}
}

@inproceedings{xie2017aggregatedresidual,
  title={Aggregated residual transformations for deep neural networks},
  author={Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1492--1500},
  year={2017}
}

@article{zagoruyko2016wideresidual,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}

@inproceedings{mianjy2019dropout,
  title={On Dropout and Nuclear Norm Regularization},
  author={Mianjy, Poorya and Arora, Raman},
  booktitle={International Conference on Machine Learning},
  pages={4575--4584},
  year={2019}
}

@InProceedings{neyshabur2015normcapacity,
  title = 	 {Norm-Based Capacity Control in Neural Networks},
  author = 	 {Behnam Neyshabur and Ryota Tomioka and Nathan Srebro},
  booktitle = 	 {Proceedings of The 28th Conference on Learning Theory},
  pages = 	 {1376--1401},
  year = 	 {2015},
  editor = 	 {Peter Grünwald and Elad Hazan and Satyen Kale},
  volume = 	 {40},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Paris, France},
  month = 	 {03--06 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v40/Neyshabur15.pdf},
  url = 	 {http://proceedings.mlr.press/v40/Neyshabur15.html},
  abstract = 	 {We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks.}
}

@inproceedings{neyshabur2015path,
  title={Path-sgd: Path-normalized optimization in deep neural networks},
  author={Neyshabur, Behnam and Salakhutdinov, Russ R and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2422--2430},
  year={2015}
}


@article{grussler2018lowrank,
   title={Low-Rank Inducing Norms with Optimality Interpretations},
   volume={28},
   ISSN={1095-7189},
   url={http://dx.doi.org/10.1137/17M1115770},
   DOI={10.1137/17m1115770},
   number={4},
   journal={SIAM Journal on Optimization},
   publisher={Society for Industrial & Applied Mathematics (SIAM)},
   author={Grussler, Christian and Giselsson, Pontus},
   year={2018},
   month={Jan},
   pages={3057–3078}
}

@inproceedings{haeffele2017global,
  title={Global optimality in neural network training},
  author={Haeffele, Benjamin D and Vidal, Ren{\'e}},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7331--7339},
  year={2017}
}


@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  year={2016},
  publisher={MIT press Cambridge}
}


@article{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  journal={arXiv preprint arXiv:1703.07950},
  year={2017}
}


@misc{ge2017onehidden,
      title={Learning One-hidden-layer Neural Networks with Landscape Design}, 
      author={Rong Ge and Jason D. Lee and Tengyu Ma},
      year={2017},
      eprint={1711.00501},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{shamir2018spurious,
  title={Spurious local minima are common in two-layer relu neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={4433--4441},
  year={2018},
  organization={PMLR}
}

@inproceedings{anandkumar2016saddle,
  title={Efficient approaches for escaping higher order saddle points in non-convex optimization},
  author={Anandkumar, Animashree and Ge, Rong},
  booktitle={Conference on learning theory},
  pages={81--102},
  year={2016}
}

@article{dasgupta1995complexity,
  title={On the complexity of training neural networks with continuous activation functions},
  author={DasGupta, Bhaskar and Siegelmann, Hava T and Sontag, Eduardo},
  journal={IEEE Transactions on Neural Networks},
  volume={6},
  number={6},
  pages={1490--1504},
  year={1995},
  publisher={IEEE}
}


@inproceedings{bartlett1999hardness,
  title={Hardness results for neural network approximation problems},
  author={Bartlett, Peter and Ben-David, Shai},
  booktitle={European Conference on Computational Learning Theory},
  pages={50--62},
  year={1999},
  organization={Springer}
}

@article{du2018overparameterized,
  title={On the power of over-parametrization in neural networks with quadratic activation},
  author={Du, Simon S and Lee, Jason D},
  journal={arXiv preprint arXiv:1803.01206},
  year={2018}
}


@inproceedings{arora2018overparameterization,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle={35th International Conference on Machine Learning, ICML 2018},
  pages={372--389},
  year={2018},
  organization={International Machine Learning Society (IMLS)}
}


@article{neyshabur2018overparameterization,
  title={Towards understanding the role of over-parametrization in generalization of neural networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={arXiv preprint arXiv:1805.12076},
  year={2018}
}






@inproceedings{rosset2007,
  title={L1 regularization in infinite dimensional feature spaces},
  author={Rosset, Saharon and Swirszcz, Grzegorz and Srebro, Nathan and Zhu, Ji},
  booktitle={International Conference on Computational Learning Theory},
  pages={544--558},
  year={2007},
  organization={Springer}
}


@misc{uci_repository ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@incollection{acute_dataset,
  title={Application of rough sets in the presumptive diagnosis of urinary system diseases},
  author={Czerniak, Jacek and Zarzycki, Hubert},
  booktitle={Artificial intelligence and security in computing systems},
  pages={41--51},
  year={2003},
  publisher={Springer}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@online{fashionmnist,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}


@article{ergen2020journal,
  title={Convex Geometry and Duality of Over-parameterized Neural Networks},
  author={Ergen, Tolga and Pilanci, Mert},
  journal={arXiv preprint arXiv:2002.11219},
  year={2020}
}

@InProceedings{ergen2020aistats,
  title = 	 {Convex Geometry of Two-Layer ReLU Networks: Implicit Autoencoding and Interpretable Models},
  author = 	 {Ergen, Tolga and Pilanci, Mert},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {4024--4033},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Online},
  month = 	 {26--28 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/ergen20a/ergen20a.pdf},
  url = 	 {http://proceedings.mlr.press/v108/ergen20a.html}
}




@INPROCEEDINGS{ergen2019shallow,
  author={T. {Ergen} and M. {Pilanci}},
  booktitle={2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
  title={Convex Optimization for Shallow Neural Networks}, 
  year={2019},
  volume={},
  number={},
  pages={79-83},}
  


@inproceedings{ergen2020cnn,
title={Implicit Convex Regularizers of CNN Architectures: Convex Optimization of Two- and Three-Layer Networks in Polynomial Time},
author={Tolga Ergen and Mert Pilanci},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=0N8jUH4JMv6}
}

@inproceedings{sahiner2021vectoroutput,
title={Vector-output ReLU Neural Network Problems are Copositive Programs: Convex Analysis of Two Layer Networks and Polynomial-time Algorithms},
author={Arda Sahiner and Tolga Ergen and John M. Pauly and Mert Pilanci},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=fGF8qAqpXXG}
}

@article{sahiner2021gan,
  title={Hidden Convexity of Wasserstein GANs: Interpretable Generative Models with Closed-Form Solutions},
  author={Sahiner, Arda and Ergen, Tolga and Ozturkler, Batu and Bartan, Burak and Pauly, John and Mardani, Morteza and Pilanci, Mert},
  journal={arXiv preprint arXiv:2107.05680},
  year={2021}
}

@article{ergen2021bn,
  author    = {Tolga Ergen and
               Arda Sahiner and
               Batu Ozturkler and
               John M. Pauly and
               Morteza Mardani and
               Mert Pilanci},
  title     = {Demystifying Batch Normalization in ReLU Networks: Equivalent Convex
               Optimization Models and Implicit Regularization},
  journal   = {CoRR},
  volume    = {abs/2103.01499},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.01499},
  archivePrefix = {arXiv},
  eprint    = {2103.01499},
  timestamp = {Thu, 04 Mar 2021 17:00:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2103-01499.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{vikul2021generative,
  author={Gupta, Vikul and Bartan, Burak and Ergen, Tolga and Pilanci, Mert},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Convex Neural Autoregressive Models: Towards Tractable, Expressive, and Theoretically-Backed Models for Sequential Forecasting and Generation}, 
  year={2021},
  volume={},
  number={},
  pages={3890-3894},
  doi={10.1109/ICASSP39728.2021.9413662}}
  
  
  @InProceedings{pilanci2020convex, 
  title = {Neural Networks are Convex Regularizers: Exact Polynomial-time Convex Optimization Formulations for Two-layer Networks}, 
  author = {Pilanci, Mert and Ergen, Tolga}, 
  booktitle = {Proceedings of the 37th International Conference on Machine Learning}, 
  pages = {7695--7705}, 
  year = {2020}, 
  editor = {Hal Daumé III and Aarti Singh}, 
  volume = {119}, 
  series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, 
  publisher = {PMLR}, 
  pdf = {http://proceedings.mlr.press/v119/pilanci20a/pilanci20a.pdf}, 
  url = { http://proceedings.mlr.press/v119/pilanci20a.html } }
  
  
  @inproceedings{ergen2019cutting,
  title={Convex duality and cutting plane methods for over-parameterized neural networks},
  author={Ergen, Tolga and Pilanci, Mert},
  booktitle={OPT-ML workshop},
  year={2019}
}


@inproceedings{ergen2020workshop,
  title={Convex Programs for Global Optimization of Convolutional Neural Networks in Polynomial-Time},
  author={Ergen, Tolga and Pilanci, Mert},
  booktitle={OPT-ML workshop},
  year={2020}
}


@article{ergen2021revealing,
  author    = {Tolga Ergen and
               Mert Pilanci},
  title     = {Convex Duality of Deep Neural Networks},
  journal   = {CoRR},
  volume    = {abs/2002.09773},
  year      = {2020},
  url       = {https://arxiv.org/abs/2002.09773},
  archivePrefix = {arXiv},
  eprint    = {2002.09773},
  timestamp = {Tue, 03 Mar 2020 14:32:13 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2002-09773.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}