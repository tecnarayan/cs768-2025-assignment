 
 
@book{milman1986asymptotic,
 author = {Milman, Vitali D and Schechtman, Gideon},
 title = {Asymptotic Theory of Finite Dimensional Normed Spaces},
 year = {1986},
 isbn = {0-387-16769-2},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 

@inproceedings{de2017automated,
  title={Automated inference with adaptive batches},
  author={De, Soham and Yadav, Abhay and Jacobs, David and Goldstein, Tom},
  booktitle={Artificial Intelligence and Statistics},
  pages={1504--1513},
  year={2017}
}

@article{ghorbani2019investigation,
  title={An investigation into neural net optimization via hessian eigenvalue density},
  author={Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
  journal={arXiv preprint arXiv:1901.10159},
  year={2019}
}

@article{schoenholz2016deep,
  title={Deep information propagation},
  author={Schoenholz, Samuel S and Gilmer, Justin and Ganguli, Surya and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1611.01232},
  year={2016}
}

@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii},
  volume={137},
  year={2018},
  publisher={Springer}
}

@book{martens2016second,
  title={Second-order optimization for neural networks},
  author={Martens, James},
  year={2016},
  publisher={University of Toronto (Canada)}
}

@article{zhang2019stochastic,
  title={Stochastic Approximation of Smooth and Strongly Convex Functions: Beyond the O (1/T) Convergence Rate},
  author={Zhang, Lijun and Zhou, Zhi-Hua},
  journal={Proceedings of Machine Learning Research vol},
  volume={99},
  pages={1--20},
  year={2019}
}

@article{srebro2010optimistic,
  title={Optimistic rates for learning with a smooth loss},
  author={Srebro, Nathan and Sridharan, Karthik and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1009.3896},
  year={2010}
}

@article{yang2019mean,
  title={A mean field theory of batch normalization},
  author={Yang, Greg and Pennington, Jeffrey and Rao, Vinay and Sohl-Dickstein, Jascha and Schoenholz, Samuel S},
  journal={arXiv preprint arXiv:1902.08129},
  year={2019}
}

@article{ge2019step,
  title={The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure},
  author={Ge, Rong and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth},
  journal={arXiv preprint arXiv:1904.12838},
  year={2019}
}

@inproceedings{hanin2018neural,
  title={Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients?},
  author={Hanin, Boris},
  booktitle={Advances in Neural Information Processing Systems},
  pages={582--591},
  year={2018}
}

@inproceedings{hanin2018start,
  title={How to start training: The effect of initialization and architecture},
  author={Hanin, Boris and Rolnick, David},
  booktitle={Advances in Neural Information Processing Systems},
  pages={571--581},
  year={2018}
}

@incollection{lecun2012efficient,
  title={Efficient backprop},
  author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--48},
  year={2012},
  publisher={Springer}
}

 @inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the polyak-{\l}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={795--811},
  year={2016},
  organization={Springer}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013}
}

@book{tao2012topics,
  title={Topics in random matrix theory},
  author={Tao, Terence},
  volume={132},
  year={2012},
  publisher={American Mathematical Soc.}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{wu2017towards,
  title={Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes},
  author={Wu, Lei and Zhu, Zhanxing and others},
  journal={arXiv preprint arXiv:1706.10239},
  year={2017}
}

@article{chaudhari2016entropy,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={arXiv preprint arXiv:1611.01838},
  year={2016}
}

@article{balduzzi2017shattered,
  title={The Shattered Gradients Problem: If resnets are the answer, then what is the question?},
  author={Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
  journal={arXiv preprint arXiv:1702.08591},
  year={2017}
}

@article{lojasiewicz1965ensembles,
  title={Ensembles semi-analytiques},
  author={Lojasiewicz, Stanislaw},
  journal={Lectures Notes IHES (Bures-sur-Yvette)},
  year={1965}
}


@article{sagun2017empirical,
  title={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},
  author={Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
  journal={arXiv preprint arXiv:1706.04454},
  year={2017}
}

@article{cooper2018loss,
  title={The loss landscape of overparameterized neural networks},
  author={Cooper, Y},
  journal={arXiv preprint arXiv:1804.10200},
  year={2018}
}

@article{arora2018optimization,
  title={On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  journal={arXiv preprint arXiv:1802.06509},
  year={2018}
}

@incollection{bengio2012practical,
  title={Practical recommendations for gradient-based training of deep architectures},
  author={Bengio, Yoshua},
  booktitle={Neural networks: Tricks of the trade},
  pages={437--478},
  year={2012},
  publisher={Springer}
}

@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}

@article{zagoruyko2016wide,
  title={Wide residual networks},
  author={Zagoruyko, Sergey and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1605.07146},
  year={2016}
}


@article{bertsekas2011incremental,
  title={Incremental gradient, subgradient, and proximal methods for convex optimization: A survey},
  author={Bertsekas, Dimitri P},
  journal={Optimization for Machine Learning},
  volume={2010},
  number={1-38},
  pages={3},
  year={2011},
  publisher={MIT Press}
}

@article{blum2016foundations,
  title={Foundations of data science},
  author={Blum, Avrim and Hopcroft, John and Kannan, Ravindran}
}

@article{byrd2012sample,
  title={Sample size selection in optimization methods for machine learning},
  author={Byrd, Richard H and Chin, Gillian M and Nocedal, Jorge and Wu, Yuchen},
  journal={Mathematical programming},
  volume={134},
  number={1},
  pages={127--155},
  year={2012},
  publisher={Springer}
}

@inproceedings{de2016efficient,
  title={Efficient distributed SGD with variance reduction},
  author={De, Soham and Goldstein, Tom},
  booktitle={Data Mining (ICDM), 2016 IEEE 16th International Conference on},
  pages={111--120},
  year={2016},
  organization={IEEE}
}

@article{de2016big,
  title={Big Batch SGD: Automated Inference using Adaptive Batch Sizes},
  author={De, Soham and Yadav, Abhay and Jacobs, David and Goldstein, Tom},
  journal={arXiv preprint arXiv:1610.05792},
  year={2016}
}

@inproceedings{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  booktitle={Advances in neural information processing systems},
  pages={1646--1654},
  year={2014}
}

@article{friedlander2012hybrid,
  title={Hybrid deterministic-stochastic methods for data fitting},
  author={Friedlander, Michael P and Schmidt, Mark},
  journal={SIAM Journal on Scientific Computing},
  volume={34},
  number={3},
  pages={A1380--A1405},
  year={2012},
  publisher={SIAM}
}

@article{goldstein2016phasemax,
  title={PhaseMax: Convex phase retrieval via basis pursuit},
  author={Goldstein, Tom and Studer, Christoph},
  journal={arXiv preprint arXiv:1610.07531},
  year={2016}
}

@inproceedings{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in neural information processing systems},
  pages={315--323},
  year={2013}
}

@article{ma2017power,
  title={The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning},
  author={Ma, Siyuan and Bassily, Raef and Belkin, Mikhail},
  journal={arXiv preprint arXiv:1712.06559},
  year={2017}
}

@inproceedings{moulines2011non,
  title={Non-asymptotic analysis of stochastic approximation algorithms for machine learning},
  author={Moulines, Eric and Bach, Francis R},
  booktitle={Advances in Neural Information Processing Systems},
  pages={451--459},
  year={2011}
}

@inproceedings{darken1992towards,
  title={Towards faster stochastic gradient search},
  author={Darken, Christian and Moody, John},
  booktitle={Advances in neural information processing systems},
  pages={1009--1016},
  year={1992}
}

@incollection{nedic2001convergence,
  title={Convergence rate of incremental subgradient algorithms},
  author={Nedi{\'c}, Angelia and Bertsekas, Dimitri},
  booktitle={Stochastic optimization: algorithms and applications},
  pages={223--264},
  year={2001},
  publisher={Springer}
}

@inproceedings{needell2014stochastic,
  title={Stochastic gradient descent, weighted sampling, and the randomized Kaczmarz algorithm},
  author={Needell, Deanna and Ward, Rachel and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1017--1025},
  year={2014}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{sedghi2018singular,
  title={The singular values of convolutional layers},
  author={Sedghi, Hanie and Gupta, Vineet and Long, Philip M},
  journal={arXiv preprint arXiv:1805.10408},
  year={2018}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{toulis2017asymptotic,
  title={Asymptotic and finite-sample properties of estimators based on stochastic gradients},
  author={Toulis, Panos and Airoldi, Edoardo M and others},
  journal={The Annals of Statistics},
  volume={45},
  number={4},
  pages={1694--1727},
  year={2017},
  publisher={Institute of Mathematical Statistics}
}

@article{murata1998statistical,
  title={A statistical study of on-line learning},
  author={Murata, Noboru},
  journal={Online Learning and Neural Networks. Cambridge University Press, Cambridge, UK},
  pages={63--92},
  year={1998},
  publisher={Citeseer}
}

@inproceedings{flammarion2015averaging,
  title={From averaging to acceleration, there is only a step-size},
  author={Flammarion, Nicolas and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={658--695},
  year={2015}
}

@article{dieuleveut2017harder,
  title={Harder, better, faster, stronger convergence rates for least-squares regression},
  author={Dieuleveut, Aymeric and Flammarion, Nicolas and Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3520--3570},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{chee2018convergence,
  title={Convergence diagnostics for stochastic gradient descent with constant learning rate},
  author={Chee, Jerry and Toulis, Panos},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1476--1485},
  year={2018}
}

@inproceedings{nesterov1983method,
  title={A method of solving a convex programming problem with convergence rate O (1/k2)},
  author={Nesterov, Yurii},
  year={1983}
}

@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

@article{keskar2017improving,
  title={Improving Generalization Performance by Switching from Adam to SGD},
  author={Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1712.07628},
  year={2017}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1-2},
  pages={83--112},
  year={2017},
  publisher={Springer}
}

@inproceedings{shamir2013stochastic,
  title={Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes},
  author={Shamir, Ohad and Zhang, Tong},
  booktitle={International Conference on Machine Learning},
  pages={71--79},
  year={2013}
}

@article{smith2017don,
  title={Don't Decay the Learning Rate, Increase the Batch Size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Le, Quoc V},
  journal={arXiv preprint arXiv:1711.00489},
  year={2017}
}

@inproceedings{raghu2017expressive,
  title={On the expressive power of deep neural networks},
  author={Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Dickstein, Jascha Sohl},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2847--2854},
  year={2017},
  organization={JMLR. org}
}

@article{telgarsky2016benefits,
  title={Benefits of depth in neural networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1602.04485},
  year={2016}
}

@inproceedings{eldan2016power,
  title={The power of depth for feedforward neural networks},
  author={Eldan, Ronen and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={907--940},
  year={2016}
}

@book{vershynin2016high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge University Press}
}

@inproceedings{
wen2018flipout,
title={Flipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches},
author={Yeming Wen and Paul Vicol and Jimmy Ba and Dustin Tran and Roger Grosse},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJNpifWAb},
}

@inproceedings{wilson2017marginal,
  title={The marginal value of adaptive gradient methods in machine learning},
  author={Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4151--4161},
  year={2017}
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}

@article{bengio1994learning,
  title={Learning long-term dependencies with gradient descent is difficult},
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={2},
  pages={157--166},
  year={1994},
  publisher={IEEE}
}

@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8580--8589},
  year={2018}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}

@book{boucheron2013concentration,
  title={Concentration inequalities: A nonasymptotic theory of independence},
  author={Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Massart, Pascal},
  year={2013},
  publisher={Oxford university press}
}

@article{dziugaite2017computing,
  title={Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data},
  author={Dziugaite, Gintare Karolina and Roy, Daniel M},
  journal={arXiv preprint arXiv:1703.11008},
  year={2017}
}

@article{nagarajan2019generalization,
  title={Generalization in deep networks: The role of distance from initialization},
  author={Nagarajan, Vaishnavh and Kolter, J Zico},
  journal={arXiv preprint arXiv:1901.01672},
  year={2019}
}

@article{du2018adapting,
  title={Adapting auxiliary losses using gradient similarity},
  author={Du, Yunshu and Czarnecki, Wojciech M and Jayakumar, Siddhant M and Pascanu, Razvan and Lakshminarayanan, Balaji},
  journal={arXiv preprint arXiv:1812.02224},
  year={2018}
}

@article{allen2018learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={arXiv preprint arXiv:1811.04918},
  year={2018}
}

@article{schmidt2013fast,
  title={Fast convergence of stochastic gradient descent under a strong growth condition},
  author={Schmidt, Mark and Roux, Nicolas Le},
  journal={arXiv preprint arXiv:1308.6370},
  year={2013}
}

@article{brutzkus2017sgd,
  title={SGD learns over-parameterized networks that provably generalize on linearly separable data},
  author={Brutzkus, Alon and Globerson, Amir and Malach, Eran and Shalev-Shwartz, Shai},
  journal={arXiv preprint arXiv:1710.10174},
  year={2017}
}

@article{oymak2018overparameterized,
  title={Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  journal={arXiv preprint arXiv:1812.10004},
  year={2018}
}

@article{neyshabur2018towards,
  title={Towards Understanding the Role of Over-Parametrization in Generalization of Neural Networks},
  author={Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  journal={arXiv preprint arXiv:1805.12076},
  year={2018}
}

@article{bassily2018exponential,
  title={On exponential convergence of SGD in non-convex over-parametrized learning},
  author={Bassily, Raef and Belkin, Mikhail and Ma, Siyuan},
  journal={arXiv preprint arXiv:1811.02564},
  year={2018}
}

@article{santurkar2018does,
  title={How Does Batch Normalization Help Optimization?(No, It Is Not About Internal Covariate Shift)},
  author={Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  journal={arXiv preprint arXiv:1805.11604},
  year={2018}
}

@article{vaswani2018fast,
  title={Fast and faster convergence of sgd for over-parameterized models and an accelerated perceptron},
  author={Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
  journal={arXiv preprint arXiv:1810.07288},
  year={2018}
}

@article{yin2017gradient,
  title={Gradient diversity: a key ingredient for scalable distributed learning},
  author={Yin, Dong and Pananjady, Ashwin and Lam, Max and Papailiopoulos, Dimitris and Ramchandran, Kannan and Bartlett, Peter},
  journal={arXiv preprint arXiv:1706.05699},
  year={2017}
}

@article{chen2018effect,
  title={The Effect of Network Width on the Performance of Large-batch Training},
  author={Chen, Lingjiao and Wang, Hongyi and Zhao, Jinman and Papailiopoulos, Dimitris and Koutris, Paraschos},
  journal={arXiv preprint arXiv:1806.03791},
  year={2018}
}

@article{zou2018stochastic,
  title={Stochastic gradient descent optimizes over-parameterized deep ReLU networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1811.08888},
  year={2018}
}

@article{allen2018convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={arXiv preprint arXiv:1811.03962},
  year={2018}
}

@article{du2018gradient,
  title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{arora2018convergence,
  title={A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks},
  author={Arora, Sanjeev and Cohen, Nadav and Golowich, Noah and Hu, Wei},
  journal={arXiv preprint arXiv:1810.02281},
  year={2018}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}

@article{mishkin2015all,
  title={All you need is a good init},
  author={Mishkin, Dmytro and Matas, Jiri},
  journal={arXiv preprint arXiv:1511.06422},
  year={2015}
}

@article{zhang2019fixup,
  title={Fixup Initialization: Residual Learning Without Normalization},
  author={Zhang, Hongyi and Dauphin, Yann N and Ma, Tengyu},
  journal={arXiv preprint arXiv:1901.09321},
  year={2019}
}

@inproceedings{nguyen2017loss,
  title={The loss surface of deep and wide neural networks},
  author={Nguyen, Quynh and Hein, Matthias},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2603--2612},
  year={2017},
  organization={JMLR. org}
}

@article{belkin2018reconciling,
  title={Reconciling modern machine learning and the bias-variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={arXiv preprint arXiv:1812.11118},
  year={2018}
}

@article{neal2018modern,
  title={A Modern Take on the Bias-Variance Tradeoff in Neural Networks},
  author={Neal, Brady and Mittal, Sarthak and Baratin, Aristide and Tantia, Vinayak and Scicluna, Matthew and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  journal={arXiv preprint arXiv:1810.08591},
  year={2018}
}

@article{fort2019stiffness,
  title={Stiffness: A New Perspective on Generalization in Neural Networks},
  author={Fort, Stanislav and Nowak, Pawe{\l} Krzysztof and Narayanan, Srini},
  journal={arXiv preprint arXiv:1901.09491},
  year={2019}
}

@article{lee2019wide,
  title={Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:1902.06720},
  year={2019}
}

@article{chizat2018note,
  title={A note on lazy training in supervised differentiable programming},
  author={Chizat, Lenaic and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  year={2018}
}

@inproceedings{salimans2016weight,
  title={Weight normalization: A simple reparameterization to accelerate training of deep neural networks},
  author={Salimans, Tim and Kingma, Durk P},
  booktitle={Advances in Neural Information Processing Systems},
  pages={901--909},
  year={2016}
}

@article{krahenbuhl2015data,
  title={Data-dependent initializations of convolutional neural networks},
  author={Kr{\"a}henb{\"u}hl, Philipp and Doersch, Carl and Donahue, Jeff and Darrell, Trevor},
  journal={arXiv preprint arXiv:1511.06856},
  year={2015}
}

@article{gray2006toeplitz,
  title={Toeplitz and circulant matrices: A review},
  author={Gray, Robert M},
  journal={Foundations and Trends{\textregistered} in Communications and Information Theory},
  volume={2},
  number={3},
  pages={155--239},
  year={2006},
  publisher={Now Publishers, Inc.}
}

@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@article{xiao2018dynamical,
  title={Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks},
  author={Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel S and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:1806.05393},
  year={2018}
}

@inproceedings{hu2020ICLR,
  title={Provable Benefit Of Orthogonal Initialization In Optimizing Deep Linear Networks},
  author={Hu, Wei and Xiao, Lechao and Pennington, Jeffrey},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{du2019width,
  title={Width Provably Matters in Optimization for Deep Linear Neural Networks},
  author={Du, Simon and Hu, Wei},
  booktitle={International Conference on Machine Learning},
  pages={1655--1664},
  year={2019}
}

@article{de2020batch,
  title={Batch Normalization Biases Residual Blocks Towards the Identity Function in Deep Networks},
  author={De, Soham and Smith, Samuel L},
  journal={arXiv preprint arXiv:2002.10444},
  year={2020}
}

@article{smith2020generalization,
  title={On the Generalization Benefit of Noise in Stochastic Gradient Descent},
  author={Samuel L. Smith and Erich Elsen and Soham De},
  journal={arXiv preprint arXiv:2006.15081},
  year={2020}
}