\begin{thebibliography}{38}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi
\expandafter\ifx\csname url\endcsname\relax
  \def\url#1{\texttt{#1}}\fi
\expandafter\ifx\csname urlprefix\endcsname\relax\def\urlprefix{URL }\fi

\bibitem[{Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l and Szepesv{\'a}ri}]{abbasi2011improved}
\textsc{Abbasi-Yadkori, Y.}, \textsc{P{\'a}l, D.} and \textsc{Szepesv{\'a}ri, C.} (2011).
\newblock Improved algorithms for linear stochastic bandits.
\newblock \textit{Advances in neural information processing systems} \textbf{24} 2312--2320.

\bibitem[{Agarwal et~al.(2022)Agarwal, Jin and Zhang}]{agarwal2022vo}
\textsc{Agarwal, A.}, \textsc{Jin, Y.} and \textsc{Zhang, T.} (2022).
\newblock Vo $ q $ l: Towards optimal regret in model-free rl with nonlinear function approximation.
\newblock \textit{arXiv preprint arXiv:2212.06069} .

\bibitem[{Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang and Yang}]{ayoub2020model}
\textsc{Ayoub, A.}, \textsc{Jia, Z.}, \textsc{Szepesvari, C.}, \textsc{Wang, M.} and \textsc{Yang, L.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Cesa-Bianchi and Lugosi(2006)}]{cesa2006prediction}
\textsc{Cesa-Bianchi, N.} and \textsc{Lugosi, G.} (2006).
\newblock \textit{Prediction, learning, and games}.
\newblock Cambridge university press.

\bibitem[{Chu et~al.(2011)Chu, Li, Reyzin and Schapire}]{chu2011contextual}
\textsc{Chu, W.}, \textsc{Li, L.}, \textsc{Reyzin, L.} and \textsc{Schapire, R.} (2011).
\newblock Contextual bandits with linear payoff functions.
\newblock In \textit{Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics}. JMLR Workshop and Conference Proceedings.

\bibitem[{Dann et~al.(2021)Dann, Marinov, Mohri and Zimmert}]{dann2021beyond}
\textsc{Dann, C.}, \textsc{Marinov, T.~V.}, \textsc{Mohri, M.} and \textsc{Zimmert, J.} (2021).
\newblock Beyond value-function gaps: Improved instance-dependent regret bounds for episodic reinforcement learning.
\newblock \textit{Advances in Neural Information Processing Systems} \textbf{34}.

\bibitem[{Du et~al.(2019)Du, Kakade, Wang and Yang}]{du2019good}
\textsc{Du, S.~S.}, \textsc{Kakade, S.~M.}, \textsc{Wang, R.} and \textsc{Yang, L.~F.} (2019).
\newblock Is a good representation sufficient for sample efficient reinforcement learning?
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Hao et~al.(2020)Hao, Lattimore and Szepesvari}]{hao2020adaptive}
\textsc{Hao, B.}, \textsc{Lattimore, T.} and \textsc{Szepesvari, C.} (2020).
\newblock Adaptive exploration in linear contextual bandit.
\newblock In \textit{International Conference on Artificial Intelligence and Statistics}. PMLR.

\bibitem[{He et~al.(2022{\natexlab{a}})He, Zhao, Zhou and Gu}]{he2022nearly}
\textsc{He, J.}, \textsc{Zhao, H.}, \textsc{Zhou, D.} and \textsc{Gu, Q.} (2022{\natexlab{a}}).
\newblock Nearly minimax optimal reinforcement learning for linear markov decision processes.
\newblock \textit{arXiv preprint arXiv:2212.06132} .

\bibitem[{He et~al.(2021{\natexlab{a}})He, Zhou and Gu}]{he2021logarithmic}
\textsc{He, J.}, \textsc{Zhou, D.} and \textsc{Gu, Q.} (2021{\natexlab{a}}).
\newblock Logarithmic regret for reinforcement learning with linear function approximation.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{He et~al.(2021{\natexlab{b}})He, Zhou and Gu}]{he2021uniformpac}
\textsc{He, J.}, \textsc{Zhou, D.} and \textsc{Gu, Q.} (2021{\natexlab{b}}).
\newblock Uniform-{PAC} bounds for reinforcement learning with linear function approximation.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{He et~al.(2022{\natexlab{b}})He, Zhou, Zhang and Gu}]{he22corruptions}
\textsc{He, J.}, \textsc{Zhou, D.}, \textsc{Zhang, T.} and \textsc{Gu, Q.} (2022{\natexlab{b}}).
\newblock Nearly optimal algorithms for linear contextual bandits with adversarial corruptions.
\newblock In \textit{Advances in Neural Information Processing Systems}.

\bibitem[{Hoeffding(1963)}]{Hoeffding1963ProbabilityIF}
\textsc{Hoeffding, W.} (1963).
\newblock Probability inequalities for sum of bounded random variables.

\bibitem[{Ishfaq et~al.(2021)Ishfaq, Cui, Nguyen, Ayoub, Yang, Wang, Precup and Yang}]{ishfaq2021randomized}
\textsc{Ishfaq, H.}, \textsc{Cui, Q.}, \textsc{Nguyen, V.}, \textsc{Ayoub, A.}, \textsc{Yang, Z.}, \textsc{Wang, Z.}, \textsc{Precup, D.} and \textsc{Yang, L.} (2021).
\newblock Randomized exploration in reinforcement learning with general value function approximation.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Jaksch et~al.(2010)Jaksch, Ortner and Auer}]{jaksch2010near}
\textsc{Jaksch, T.}, \textsc{Ortner, R.} and \textsc{Auer, P.} (2010).
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \textit{Journal of Machine Learning Research} \textbf{11}.

\bibitem[{Jia et~al.(2020)Jia, Yang, Szepesvari and Wang}]{jia2020model}
\textsc{Jia, Z.}, \textsc{Yang, L.}, \textsc{Szepesvari, C.} and \textsc{Wang, M.} (2020).
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \textit{Learning for Dynamics and Control}. PMLR.

\bibitem[{Jin et~al.(2020)Jin, Yang, Wang and Jordan}]{jin2020provably}
\textsc{Jin, C.}, \textsc{Yang, Z.}, \textsc{Wang, Z.} and \textsc{Jordan, M.~I.} (2020).
\newblock Provably efficient reinforcement learning with linear function approximation.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Kober et~al.(2013)Kober, Bagnell and Peters}]{kober2013reinforcement}
\textsc{Kober, J.}, \textsc{Bagnell, J.~A.} and \textsc{Peters, J.} (2013).
\newblock Reinforcement learning in robotics: A survey.
\newblock \textit{The International Journal of Robotics Research} \textbf{32} 1238--1274.

\bibitem[{Lattimore et~al.(2020)Lattimore, Szepesvari and Weisz}]{lattimore2020learning}
\textsc{Lattimore, T.}, \textsc{Szepesvari, C.} and \textsc{Weisz, G.} (2020).
\newblock Learning with good feature representations in bandits and in rl with a generative model.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Lykouris et~al.(2021)Lykouris, Simchowitz, Slivkins and Sun}]{lykouris2021corruption}
\textsc{Lykouris, T.}, \textsc{Simchowitz, M.}, \textsc{Slivkins, A.} and \textsc{Sun, W.} (2021).
\newblock Corruption-robust exploration in episodic reinforcement learning.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou, Wierstra and Riedmiller}]{mnih2013playing}
\textsc{Mnih, V.}, \textsc{Kavukcuoglu, K.}, \textsc{Silver, D.}, \textsc{Graves, A.}, \textsc{Antonoglou, I.}, \textsc{Wierstra, D.} and \textsc{Riedmiller, M.} (2013).
\newblock Playing atari with deep reinforcement learning.
\newblock \textit{arXiv preprint arXiv:1312.5602} .

\bibitem[{Modi et~al.(2020)Modi, Jiang, Tewari and Singh}]{modi2020sample}
\textsc{Modi, A.}, \textsc{Jiang, N.}, \textsc{Tewari, A.} and \textsc{Singh, S.} (2020).
\newblock Sample complexity of reinforcement learning using linearly combined model ensembles.
\newblock In \textit{International Conference on Artificial Intelligence and Statistics}.

\bibitem[{Papini et~al.(2021{\natexlab{a}})Papini, Tirinzoni, Pacchiano, Restelli, Lazaric and Pirotta}]{papini2021reinforcement}
\textsc{Papini, M.}, \textsc{Tirinzoni, A.}, \textsc{Pacchiano, A.}, \textsc{Restelli, M.}, \textsc{Lazaric, A.} and \textsc{Pirotta, M.} (2021{\natexlab{a}}).
\newblock Reinforcement learning in linear mdps: Constant regret and representation selection.
\newblock \textit{Advances in Neural Information Processing Systems} \textbf{34} 16371--16383.

\bibitem[{Papini et~al.(2021{\natexlab{b}})Papini, Tirinzoni, Restelli, Lazaric and Pirotta}]{papini2021leveraging}
\textsc{Papini, M.}, \textsc{Tirinzoni, A.}, \textsc{Restelli, M.}, \textsc{Lazaric, A.} and \textsc{Pirotta, M.} (2021{\natexlab{b}}).
\newblock Leveraging good representations in linear contextual bandits.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Simchowitz and Jamieson(2019)}]{simchowitz2019non}
\textsc{Simchowitz, M.} and \textsc{Jamieson, K.~G.} (2019).
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock \textit{Advances in Neural Information Processing Systems} \textbf{32} 1153--1162.

\bibitem[{Takemura et~al.(2021)Takemura, Ito, Hatano, Sumita, Fukunaga, Kakimura and Kawarabayashi}]{takemura2021parameter}
\textsc{Takemura, K.}, \textsc{Ito, S.}, \textsc{Hatano, D.}, \textsc{Sumita, H.}, \textsc{Fukunaga, T.}, \textsc{Kakimura, N.} and \textsc{Kawarabayashi, K.-i.} (2021).
\newblock A parameter-free algorithm for misspecified linear contextual bandits.
\newblock In \textit{International Conference on Artificial Intelligence and Statistics}. PMLR.

\bibitem[{Vial et~al.(2022)Vial, Parulekar, Shakkottai and Srikant}]{vial2022improved}
\textsc{Vial, D.}, \textsc{Parulekar, A.}, \textsc{Shakkottai, S.} and \textsc{Srikant, R.} (2022).
\newblock Improved algorithms for misspecified linear markov decision processes.
\newblock In \textit{International Conference on Artificial Intelligence and Statistics}. PMLR.

\bibitem[{Wang et~al.(2019)Wang, Wang, Du and Krishnamurthy}]{wang2019optimism}
\textsc{Wang, Y.}, \textsc{Wang, R.}, \textsc{Du, S.~S.} and \textsc{Krishnamurthy, A.} (2019).
\newblock Optimism in reinforcement learning with generalized linear function approximation.
\newblock In \textit{International Conference on Learning Representations}.

\bibitem[{Wei et~al.(2022)Wei, Dann and Zimmert}]{wei2022model}
\textsc{Wei, C.-Y.}, \textsc{Dann, C.} and \textsc{Zimmert, J.} (2022).
\newblock A model selection approach for corruption robust reinforcement learning.
\newblock In \textit{International Conference on Algorithmic Learning Theory}. PMLR.

\bibitem[{Yang et~al.(2021)Yang, Yang and Du}]{yang2021q}
\textsc{Yang, K.}, \textsc{Yang, L.} and \textsc{Du, S.} (2021).
\newblock Q-learning with logarithmic regret.
\newblock In \textit{International Conference on Artificial Intelligence and Statistics}. PMLR.

\bibitem[{Yang and Wang(2020)}]{yang2019reinforcement}
\textsc{Yang, L.} and \textsc{Wang, M.} (2020).
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Zanette et~al.(2020{\natexlab{a}})Zanette, Brandfonbrener, Brunskill, Pirotta and Lazaric}]{zanette2020frequentist}
\textsc{Zanette, A.}, \textsc{Brandfonbrener, D.}, \textsc{Brunskill, E.}, \textsc{Pirotta, M.} and \textsc{Lazaric, A.} (2020{\natexlab{a}}).
\newblock Frequentist regret bounds for randomized least-squares value iteration.
\newblock In \textit{International Conference on Artificial Intelligence and Statistics}.

\bibitem[{Zanette et~al.(2020{\natexlab{b}})Zanette, Lazaric, Kochenderfer and Brunskill}]{zanette2020learning}
\textsc{Zanette, A.}, \textsc{Lazaric, A.}, \textsc{Kochenderfer, M.} and \textsc{Brunskill, E.} (2020{\natexlab{b}}).
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\bibitem[{Zhang et~al.(2023{\natexlab{a}})Zhang, He, Fan and Gu}]{zhang2023on}
\textsc{Zhang, W.}, \textsc{He, J.}, \textsc{Fan, Z.} and \textsc{Gu, Q.} (2023{\natexlab{a}}).
\newblock On the interplay between misspecification and sub-optimality gap: From linear contextual bandits to linear {MDP}s.

\bibitem[{Zhang et~al.(2023{\natexlab{b}})Zhang, He, Fan and Gu}]{zhang2023interplay}
\textsc{Zhang, W.}, \textsc{He, J.}, \textsc{Fan, Z.} and \textsc{Gu, Q.} (2023{\natexlab{b}}).
\newblock On the interplay between misspecification and sub-optimality gap in linear contextual bandits.
\newblock \textit{arXiv preprint arXiv:2303.09390} .

\bibitem[{Zhang et~al.(2021)Zhang, He, Zhou, Zhang and Gu}]{zhang2021provably}
\textsc{Zhang, W.}, \textsc{He, J.}, \textsc{Zhou, D.}, \textsc{Zhang, A.} and \textsc{Gu, Q.} (2021).
\newblock Provably efficient representation learning in low-rank markov decision processes.
\newblock \textit{arXiv preprint arXiv:2106.11935} .

\bibitem[{Zhou et~al.(2021{\natexlab{a}})Zhou, Gu and Szepesvari}]{zhou2021nearly}
\textsc{Zhou, D.}, \textsc{Gu, Q.} and \textsc{Szepesvari, C.} (2021{\natexlab{a}}).
\newblock Nearly minimax optimal reinforcement learning for linear mixture markov decision processes.
\newblock In \textit{Conference on Learning Theory}. PMLR.

\bibitem[{Zhou et~al.(2021{\natexlab{b}})Zhou, He and Gu}]{zhou2020provably}
\textsc{Zhou, D.}, \textsc{He, J.} and \textsc{Gu, Q.} (2021{\natexlab{b}}).
\newblock Provably efficient reinforcement learning for discounted mdps with feature mapping.
\newblock In \textit{International Conference on Machine Learning}. PMLR.

\end{thebibliography}
