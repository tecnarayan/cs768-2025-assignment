\begin{thebibliography}{10}

\bibitem{abdolmaleki2018maximum}
Abbas Abdolmaleki, Jost~Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas
  Heess, and Martin Riedmiller.
\newblock Maximum a posteriori policy optimisation.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{adler2002cooperative}
Jeffrey~L Adler and Victor~J Blue.
\newblock A cooperative multi-agent transportation management and route
  guidance system.
\newblock {\em Transportation Research Part C: Emerging Technologies},
  10(5-6):433--454, 2002.

\bibitem{agarwal2020optimistic}
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  104--114. PMLR, 2020.

\bibitem{ajay2020opal}
Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum.
\newblock {OPAL}: Offline primitive discovery for accelerating offline
  reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{asadi2017alternative}
Kavosh Asadi and Michael~L Littman.
\newblock An alternative softmax operator for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  243--252. PMLR, 2017.

\bibitem{berkenkamp2017safe}
Felix Berkenkamp, Matteo Turchetta, Angela~P Schoellig, and Andreas Krause.
\newblock Safe model-based reinforcement learning with stability guarantees.
\newblock {\em arXiv preprint arXiv:1705.08551}, 2017.

\bibitem{callaway2010achieving}
Duncan~S Callaway and Ian~A Hiskens.
\newblock Achieving controllability of electric loads.
\newblock {\em Proceedings of the IEEE}, 99(1):184--199, 2010.

\bibitem{cao2020adversarial}
Yuanjiang Cao, Xiaocong Chen, Lina Yao, Xianzhi Wang, and Wei~Emma Zhang.
\newblock Adversarial attacks and detection on reinforcement learning-based
  interactive recommender systems.
\newblock In {\em Proceedings of the 43rd International ACM SIGIR Conference on
  Research and Development in Information Retrieval}, pages 1669--1672, 2020.

\bibitem{dreves2011solution}
Axel Dreves, Francisco Facchinei, Christian Kanzow, and Simone Sagratella.
\newblock On the solution of the {KKT} conditions of generalized nash
  equilibrium problems.
\newblock {\em SIAM Journal on Optimization}, 21(3):1082--1108, 2011.

\bibitem{ernst2005tree}
Damien Ernst, Pierre Geurts, and Louis Wehenkel.
\newblock Tree-based batch mode reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 6:503--556, 2005.

\bibitem{eysenbach2017leave}
Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine.
\newblock Leave no trace: Learning to reset for safe and autonomous
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1711.06782}, 2017.

\bibitem{fakoor2020p3o}
Rasool Fakoor, Pratik Chaudhari, and Alexander~J Smola.
\newblock {P3O}: Policy-on policy-off policy optimization.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 1017--1027.
  PMLR, 2020.

\bibitem{farajtabar2018more}
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.
\newblock More robust doubly robust off-policy evaluation.
\newblock In {\em International Conference on Machine Learning}, pages
  1447--1456. PMLR, 2018.

\bibitem{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock {D4RL}: Datasets for deep data-driven reinforcement learning.
\newblock {\em arXiv preprint arXiv:2004.07219}, 2020.

\bibitem{fujimoto2019benchmarking}
Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau.
\newblock Benchmarking batch deep reinforcement learning algorithms.
\newblock {\em arXiv preprint arXiv:1910.01708}, 2019.

\bibitem{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In {\em International Conference on Machine Learning}, pages
  2052--2062. PMLR, 2019.

\bibitem{jaques2019way}
Natasha Jaques, Asma Ghandeharioun, Judy~Hanwen Shen, Craig Ferguson, Agata
  Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock {\em arXiv preprint arXiv:1907.00456}, 2019.

\bibitem{jiang2016doubly}
Nan Jiang and Lihong Li.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  652--661. PMLR, 2016.

\bibitem{kahn2018composable}
Gregory Kahn, Adam Villaflor, Pieter Abbeel, and Sergey Levine.
\newblock Composable action-conditioned predictors: Flexible off-policy
  learning for robot navigation.
\newblock In {\em Conference on Robot Learning}, pages 806--816. PMLR, 2018.

\bibitem{kalashnikov2018scalable}
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog,
  Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent
  Vanhoucke, et~al.
\newblock Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock In {\em Conference on Robot Learning}, pages 651--673. PMLR, 2018.

\bibitem{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11784--11794, 2019.

\bibitem{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2006.04779}, 2020.

\bibitem{laroche2019safe}
Romain Laroche, Paul Trichelair, and Remi~Tachet Des~Combes.
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In {\em International Conference on Machine Learning}, pages
  3652--3661. PMLR, 2019.

\bibitem{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock {\em arXiv preprint arXiv:2005.01643}, 2020.

\bibitem{li2021celebrating}
Chenghao Li, Chengjie Wu, Tonghan Wang, Jun Yang, Qianchuan Zhao, and Chongjie
  Zhang.
\newblock Celebrating diversity in shared multi-agent reinforcement learning.
\newblock {\em arXiv preprint arXiv:2106.02195}, 2021.

\bibitem{ma2021modeling}
Xiaoteng Ma, Yiqin Yang, Chenghao Li, Yiwen Lu, Qianchuan Zhao, and Jun Yang.
\newblock Modeling the interaction between agents in cooperative multi-agent
  reinforcement learning.
\newblock In {\em Proceedings of the 20th International Conference on
  Autonomous Agents and MultiAgent Systems}, pages 853--861, 2021.

\bibitem{munos2016q}
R{\'e}mi Munos.
\newblock Q($\lambda$) with off-policy corrections.
\newblock In {\em Algorithmic Learning Theory: International Conference, ALT
  2016, Bari, Italy, October 19-21, 2016, Proceedings}, volume 9925, page 305.
  Springer, 2016.

\bibitem{nachum2019dualdice}
Ofir Nachum, Yinlam Chow, Bo~Dai, and Lihong Li.
\newblock {DualDICE}: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock {\em Advances in Neural Information Processing Systems},
  32:2318--2328, 2019.

\bibitem{nair2020accelerating}
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock {\em arXiv preprint arXiv:2006.09359}, 2020.

\bibitem{neal2001annealed}
Radford~M Neal.
\newblock Annealed importance sampling.
\newblock {\em Statistics and computing}, 11(2):125--139, 2001.

\bibitem{oliehoek2016concise}
Frans~A Oliehoek, Christopher Amato, et~al.
\newblock {\em A concise introduction to decentralized POMDPs}, volume~1.
\newblock Springer, 2016.

\bibitem{osband2018randomized}
Ian Osband, John Aslanides, and Albin Cassirer.
\newblock Randomized prior functions for deep reinforcement learning.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 8626--8638, 2018.

\bibitem{o2018uncertainty}
Brendan Oâ€™Donoghue, Ian Osband, Remi Munos, and Volodymyr Mnih.
\newblock The uncertainty bellman equation and exploration.
\newblock In {\em International Conference on Machine Learning}, pages
  3836--3845, 2018.

\bibitem{pan2020softmax}
Ling Pan, Qingpeng Cai, and Longbo Huang.
\newblock Softmax deep double deterministic policy gradients.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 11767--11777, 2020.

\bibitem{peng2019advantage}
Xue~Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1910.00177}, 2019.

\bibitem{peters2010relative}
Jan Peters, Katharina Mulling, and Yasemin Altun.
\newblock Relative entropy policy search.
\newblock In {\em AAAI Conference on Artificial Intelligence}, volume~24, 2010.

\bibitem{rabbat2004distributed}
Michael Rabbat and Robert Nowak.
\newblock Distributed optimization in sensor networks.
\newblock In {\em Proceedings of the 3rd international symposium on Information
  processing in sensor networks}, pages 20--27, 2004.

\bibitem{rajeswaran2017learning}
Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John
  Schulman, Emanuel Todorov, and Sergey Levine.
\newblock Learning complex dexterous manipulation with deep reinforcement
  learning and demonstrations.
\newblock {\em arXiv preprint arXiv:1709.10087}, 2017.

\bibitem{rashid2018qmix}
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob
  Foerster, and Shimon Whiteson.
\newblock {QMIX}: Monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4295--4304, 2018.

\bibitem{samvelyan2019starcraft}
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder~de Witt, Gregory
  Farquhar, Nantas Nardelli, Tim~GJ Rudner, Chia-Man Hung, Philip~HS Torr,
  Jakob Foerster, and Shimon Whiteson.
\newblock The {StarCraft} multi-agent challenge.
\newblock In {\em International Conference on Autonomous Agents and Multiagent
  Systems}, pages 2186--2188, 2019.

\bibitem{santamaria1997experiments}
Juan~C Santamaria, Richard~S Sutton, and Ashwin Ram.
\newblock Experiments with reinforcement learning in problems with continuous
  state and action spaces.
\newblock {\em Adaptive behavior}, 6(2):163--217, 1997.

\bibitem{siegel2020keep}
Noah Siegel, Jost~Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki,
  Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin
  Riedmiller.
\newblock Keep doing what worked: Behavior modelling priors for offline
  reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{son2019qtran}
Kyunghwan Son, Daewoo Kim, Wan~Ju Kang, David~Earl Hostallero, and Yung Yi.
\newblock {QTRAN}: Learning to factorize with transformation for cooperative
  multi-agent reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  5887--5896, 2019.

\bibitem{sonabend2020expert}
Aaron Sonabend-W, Junwei Lu, Leo~A Celi, Tianxi Cai, and Peter Szolovits.
\newblock Expert-supervised reinforcement learning for offline policy learning
  and evaluation.
\newblock {\em arXiv preprint arXiv:2006.13189}, 2020.

\bibitem{song2019v}
H~Francis Song, Abbas Abdolmaleki, Jost~Tobias Springenberg, Aidan Clark,
  Hubert Soyer, Jack~W Rae, Seb Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala,
  et~al.
\newblock V-mpo: On-policy maximum a posteriori policy optimization for
  discrete and continuous control.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{song2019revisiting}
Zhao Song, Ron Parr, and Lawrence Carin.
\newblock Revisiting the softmax bellman operator: New benefits and new
  perspective.
\newblock In {\em International Conference on Machine Learning}, pages
  5916--5925. PMLR, 2019.

\bibitem{sriperumbudur2012empirical}
Bharath~K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard
  Sch{\"o}lkopf, Gert~RG Lanckriet, et~al.
\newblock On the empirical estimation of integral probability metrics.
\newblock {\em Electronic Journal of Statistics}, 6:1550--1599, 2012.

\bibitem{sunehag2017value}
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech~Marian Czarnecki, Vinicius
  Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel~Z Leibo, Karl
  Tuyls, et~al.
\newblock Value-decomposition networks for cooperative multi-agent learning
  based on team reward.
\newblock In {\em International Conference on Autonomous Agents and MultiAgent
  Systems}, pages 2085--2087, 2018.

\bibitem{sutton2018reinforcement}
Richard~S. Sutton and Andrew~G. Barto.
\newblock {\em Reinforcement Learning: An Introduction}.
\newblock MIT press, 2018.

\bibitem{thomas2016data}
Philip Thomas and Emma Brunskill.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2139--2148. PMLR, 2016.

\bibitem{todorov2007linearly}
Emanuel Todorov.
\newblock Linearly-solvable markov decision problems.
\newblock In {\em Advances in neural information processing systems}, pages
  1369--1376, 2007.

\bibitem{touati2020randomized}
Ahmed Touati, Harsh Satija, Joshua Romoff, Joelle Pineau, and Pascal Vincent.
\newblock Randomized value functions via multiplicative normalizing flows.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 422--432.
  PMLR, 2020.

\bibitem{vallender1974calculation}
SS~Vallender.
\newblock Calculation of the wasserstein distance between probability
  distributions on the line.
\newblock {\em Theory of Probability \& Its Applications}, 18(4):784--786,
  1974.

\bibitem{vuong2018supervised}
Quan Vuong, Yiming Zhang, and Keith~W Ross.
\newblock Supervised policy update for deep reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{wang2020off}
Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang.
\newblock Off-policy multi-agent decomposed policy gradients.
\newblock {\em arXiv preprint arXiv:2007.12322}, 2020.

\bibitem{wang2017optimal}
Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dud{\i}k.
\newblock Optimal and adaptive off-policy evaluation in contextual bandits.
\newblock In {\em International Conference on Machine Learning}, pages
  3589--3597. PMLR, 2017.

\bibitem{watkins1992q}
Christopher~JCH Watkins and Peter Dayan.
\newblock Q-learning.
\newblock {\em Machine learning}, 8(3-4):279--292, 1992.

\bibitem{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:1911.11361}, 2019.

\bibitem{ye2020mastering}
Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu,
  Shaojie Yang, Xipeng Wu, Qingwei Guo, et~al.
\newblock Mastering complex control in {MOBA} games with deep reinforcement
  learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 6672--6679, 2020.

\end{thebibliography}
