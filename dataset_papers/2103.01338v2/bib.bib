@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}


@article{hazan2014beyond,
  title={Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization},
  author={Hazan, Elad and Kale, Satyen},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2489--2512},
  year={2014},
  publisher={JMLR. org}
}

@book{nesterov2003introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@InProceedings{pmlr-v119-scieur20a, title = {Universal Asymptotic Optimality of Polyak Momentum}, author = {Scieur, Damien and Pedregosa, Fabian}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {8565--8572}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/scieur20a/scieur20a.pdf}, url = { http://proceedings.mlr.press/v119/scieur20a.html }}

@InProceedings{pmlr-v119-pedregosa20a, title = {Acceleration through spectral density estimation}, author = {Pedregosa, Fabian and Scieur, Damien}, booktitle = {Proceedings of the 37th International Conference on Machine Learning}, pages = {7553--7562}, year = {2020}, editor = {Hal Daumé III and Aarti Singh}, volume = {119}, series = {Proceedings of Machine Learning Research}, month = {13--18 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v119/pedregosa20a/pedregosa20a.pdf}, url = { http://proceedings.mlr.press/v119/pedregosa20a.html } }

@article{cutkosky2019momentum,
  title={Momentum-based variance reduction in non-convex sgd},
  author={Cutkosky, Ashok and Orabona, Francesco},
  journal={arXiv preprint arXiv:1905.10018},
  year={2019}
}

@article{dozat2016incorporating,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013},
  organization={PMLR}
}

@inproceedings{barre2020complexity,
  title={Complexity guarantees for polyak steps with momentum},
  author={Barr{\'e}, Mathieu and Taylor, Adrien and d’Aspremont, Alexandre},
  booktitle={Conference on Learning Theory},
  pages={452--478},
  year={2020},
  organization={PMLR}
}

@article{polyak1987introduction,
  title={Introduction to optimization. optimization software},
  author={Polyak, Boris T},
  journal={Inc., Publications Division, New York},
  volume={1},
  year={1987}
}

@article{hazan2019revisiting,
  title={Revisiting the Polyak step size},
  author={Hazan, Elad and Kakade, Sham},
  journal={arXiv preprint arXiv:1905.00313},
  year={2019}
}

@article{li2019towards,
  title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  journal={arXiv preprint arXiv:1907.04595},
  year={2019}
}

@article{li2020reconciling,
  title={Reconciling modern deep learning with traditional optimization analyses: The intrinsic learning rate},
  author={Li, Zhiyuan and Lyu, Kaifeng and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2010.02916},
  year={2020}
}

@article{li2019exponential,
  title={An exponential learning rate schedule for deep learning},
  author={Li, Zhiyuan and Arora, Sanjeev},
  journal={arXiv preprint arXiv:1910.07454},
  year={2019}
}

@article{POLYAK19641,
title = {Some methods of speeding up the convergence of iteration methods},
journal = {USSR Computational Mathematics and Mathematical Physics},
volume = {4},
number = {5},
pages = {1-17},
year = {1964},
issn = {0041-5553},
author = {B.T. Polyak}
}

@inproceedings{li2020fast,
  title={A Fast Anderson-Chebyshev Acceleration for Nonlinear Optimization},
  author={Li, Zhize and Li, Jian},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1047--1057},
  year={2020},
  organization={PMLR}
}

@inproceedings{bubeck2019near,
  title={Near-optimal method for highly smooth convex optimization},
  author={Bubeck, S{\'e}bastien and Jiang, Qijia and Lee, Yin Tat and Li, Yuanzhi and Sidford, Aaron},
  booktitle={Conference on Learning Theory},
  pages={492--507},
  year={2019},
  organization={PMLR}
}

@article{nesterov2008accelerating,
  title={Accelerating the cubic regularization of Newton’s method on convex problems},
  author={Nesterov, Yu},
  journal={Mathematical Programming},
  volume={112},
  number={1},
  pages={159--181},
  year={2008},
  publisher={Springer}
}

@article{monteiro2013accelerated,
  title={An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods},
  author={Monteiro, Renato DC and Svaiter, Benar Fux},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={2},
  pages={1092--1125},
  year={2013},
  publisher={SIAM}
}

@article{lin2018catalyst,
  title={Catalyst acceleration for first-order convex optimization: from theory to practice},
  author={Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={7854--7907},
  year={2018}
}

@misc{BubeckBlogNemirovski,
    author = "S{\'e}bastien Bubeck",
    title = "Nemirovski's acceleration (blog post)",
    year = "2019",
    url = "https://blogs.princeton.edu/imabandit/2019/01/09/nemirovskis-acceleration/"
}

@article{wibisono2015accelerated,
  title={On accelerated methods in optimization},
  author={Wibisono, Andre and Wilson, Ashia C},
  journal={arXiv preprint arXiv:1509.03616},
  year={2015}
}

@inproceedings{su2014differential,
  title={A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights.},
  author={Su, Weijie and Boyd, Stephen P and Candes, Emmanuel J},
  booktitle={NIPS},
  volume={14},
  number={27},
  pages={2510--2518},
  year={2014}
}

@article{bubeck2015geometric,
  title={A geometric alternative to Nesterov's accelerated gradient descent},
  author={Bubeck, S{\'e}bastien and Lee, Yin Tat and Singh, Mohit},
  journal={arXiv preprint arXiv:1506.08187},
  year={2015}
}

@article{allen2014linear,
  title={Linear coupling: An ultimate unification of gradient and mirror descent},
  author={Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
  journal={arXiv preprint arXiv:1407.1537},
  year={2014}
}

@misc{BachBlog,
    author = "Francis Bach",
    title = "Machine Learning Research Blog",
    year = "2020",
    url = "https://francisbach.com/acceleration-without-pain/"
}

@article{sidi1986acceleration,
  title={Acceleration of convergence of vector sequences},
  author={Sidi, Avram and Ford, William F and Smith, David A},
  journal={SIAM Journal on Numerical Analysis},
  volume={23},
  number={1},
  pages={178--196},
  year={1986},
  publisher={SIAM}
}

@article{anderson1965iterative,
  title={Iterative procedures for nonlinear integral equations},
  author={Anderson, Donald G},
  journal={Journal of the ACM (JACM)},
  volume={12},
  number={4},
  pages={547--560},
  year={1965},
  publisher={ACM New York, NY, USA}
}

@article{wynn1956device,
  title={On a device for computing the e m (S n) transformation},
  author={Wynn, Peter},
  journal={Mathematical Tables and Other Aids to Computation},
  pages={91--96},
  year={1956},
  publisher={JSTOR}
}

@article{aitken1927xxv,
  title={{XXV}.—{O}n {B}ernoulli's Numerical Solution of Algebraic Equations},
  author={Aitken, Alexander Craig},
  journal={Proceedings of the Royal Society of Edinburgh},
  volume={46},
  pages={289--305},
  year={1927},
  publisher={Royal Society of Edinburgh Scotland Foundation}
}

@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European conference on computer vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}

@inproceedings{gorbunov2020unified,
  title={A unified theory of SGD: Variance reduction, sampling, quantization and coordinate descent},
  author={Gorbunov, Eduard and Hanzely, Filip and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={680--690},
  year={2020},
  organization={PMLR}
}

@article{lebedev1971order,
  title={The order of choice of the iteration parameters in the cyclic {C}hebyshev iteration method},
  author={Lebedev, Vyacheslav Ivanovich and Finogenov, S.A.},
  journal={Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  volume={11},
  number={2},
  pages={425--438},
  year={1971},
  publisher={Russian Academy of Sciences, Branch of Mathematical Sciences}
}

@inproceedings{bottou2007tradeoffs,
  title={The tradeoffs of large scale learning},
  author={Bottou, Leon and Bousquet, Olivier},
  booktitle={Proceedings of the 20th International Conference on Neural Information Processing Systems},
  pages={161--168},
  year={2007}
}

@inproceedings{nesterov1983method,
  title={A method of solving a convex programming problem with convergence rate O(k\^{}2)},
  author={Nesterov, Yurii Evgen'evich},
  booktitle={Doklady Akademii Nauk},
  volume={269},
  number={3},
  pages={543--547},
  year={1983},
  organization={Russian Academy of Sciences}
}

@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={{USSR} Computational Mathematics and Mathematical Physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}

@article{richardson1911ix,
  title={The approximate arithmetical solution by finite differences of physical problems involving differential equations, with an application to the stresses in a masonry dam},
  author={Richardson, Lewis Fry},
  journal={Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
  volume={210},
  number={459-470},
  pages={307--357},
  year={1911},
  publisher={The Royal Society London}
}

@article{flanders1950numerical,
  title={Numerical determination of fundamental modes},
  author={Flanders, Donald A and Shortley, George},
  journal={Journal of Applied Physics},
  volume={21},
  number={12},
  pages={1326--1332},
  year={1950},
  publisher={American Institute of Physics}
}

@book{chebyshev1853theorie,
  title={Th{\'e}orie des m{\'e}canismes connus sous le nom de parall{\'e}logrammes},
  author={Chebyshev, Pafnuti L'vovich},
  year={1853},
  publisher={Imprimerie de l'Acad{\'e}mie imp{\'e}riale des sciences}
}

@book{mason2002chebyshev,
  title={Chebyshev polynomials},
  author={Mason, John C and Handscomb, David C},
  year={2002},
  publisher={CRC press}
}

@article{gavurin1950use,
  title={The use of polynomials of best approximation for improving the convergence of iterative processes},
  author={Gavurin, Mark Konstantinovich},
  journal={Uspekhi Matematicheskikh Nauk},
  volume={5},
  number={3},
  pages={156--160},
  year={1950},
  publisher={Russian Academy of Sciences, Steklov Mathematical Institute of Russian~…}
}

@article{stiefel1958kernel,
  title={Kernel polynomial in linear algebra and their numerical applications},
  author={Stiefel, Eduard L},
  journal={NBS Applied Math. Ser.},
  volume={49},
  pages={1--22},
  year={1958}
}

@article{hestenes1952methods,
  title={Methods of Conjugate Gradients for Solving Linear Systems},
  author={Hestenes, Magnus R and Stiefel, Eduard},
  journal={Journal cf Research of the National Bureau of Standards},
  volume={49},
  number={6},
  year={1952}
}

@book{kelley1999iterative,
  title={Iterative methods for optimization},
  author={Kelley, Carl T},
  year={1999},
  publisher={SIAM}
}

@inproceedings{lecun1993automatic,
  title={Automatic learning rate maximization by on-line estimation of the {H}essian's eigenvectors},
  author={LeCun, Yann and Simard, Patrice Y and Pearlmutter, Barak A},
  year={1993},
  organization={Neural Information Processing Systems}
}

@article{chen2018stability,
  title={Stability and convergence trade-off of iterative optimization algorithms},
  author={Chen, Yuansi and Jin, Chi and Yu, Bin},
  journal={arXiv preprint arXiv:1804.01619},
  year={2018}
}

@article{sachdeva2013faster,
  title={Faster algorithms via approximation theory},
  author={Sachdeva, Sushant and Vishnoi, Nisheeth K},
  journal={Theoretical Computer Science},
  volume={9},
  number={2},
  pages={125--210},
  year={2013}
}

@article{devolder2014first,
  title={First-order methods of smooth convex optimization with inexact oracle},
  author={Devolder, Olivier and Glineur, Fran{\c{c}}ois and Nesterov, Yurii},
  journal={Mathematical Programming},
  volume={146},
  number={1},
  pages={37--75},
  year={2014},
  publisher={Springer}
}

@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1225--1234},
  year={2016},
  organization={PMLR}
}

@article{bousquet2002stability,
  title={Stability and generalization},
  author={Bousquet, Olivier and Elisseeff, Andr{\'e}},
  journal={The Journal of Machine Learning Research},
  volume={2},
  pages={499--526},
  year={2002},
  publisher={JMLR. org}
}

@article{allen2016optimal,
  title={Optimal black-box reductions between optimization objectives},
  author={Allen-Zhu, Zeyuan and Hazan, Elad},
  journal={arXiv preprint arXiv:1603.05642},
  year={2016}
}

@article{agarwal2020disentangling,
  title={Disentangling adaptive gradient methods from learning rates},
  author={Agarwal, Naman and Anil, Rohan and Hazan, Elad and Koren, Tomer and Zhang, Cyril},
  journal={arXiv preprint arXiv:2002.11803},
  year={2020}
}

@article{jiang2020characterizing,
  title={Characterizing Structural Regularities of Labeled Data in Overparameterized Models},
  author={Jiang, Ziheng and Zhang, Chiyuan and Talwar, Kunal and Mozer, Michael C},
  journal={arXiv e-prints},
  pages={arXiv--2002},
  year={2020}
}

@inproceedings{agarwal2020stochastic,
  title={Stochastic Optimization with Laggard Data Pipelines},
  author={Agarwal, Naman and Anil, Rohan and Koren, Tomer and Talwar, Kunal and Zhang, Cyril},
  booktitle={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{o2015adaptive,
  title={Adaptive restart for accelerated gradient schemes},
  author={O’{D}onoghue, Brendan and Candes, Emmanuel},
  journal={Foundations of computational mathematics},
  volume={15},
  number={3},
  pages={715--732},
  year={2015},
  publisher={Springer}
}

@article{loshchilov2016sgdr,
  title={{SGDR}: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}

@article{d2021acceleration,
  title={Acceleration methods},
  author={d'Aspremont, Alexandre and Scieur, Damien and Taylor, Adrien},
  journal={arXiv preprint arXiv:2101.09545},
  year={2021}
}

@article{bubeck2017convex,
  title={Convex Optimization: Algorithms and Complexity},
  author={Bubeck, S{\'e}bastien},
  journal={Foundations and Trends in Machine Learning},
  volume={8},
  year={2017}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge University Press}
}

@article{lessard2016analysis,
  title={Analysis and design of optimization algorithms via integral quadratic constraints},
  author={Lessard, Laurent and Recht, Benjamin and Packard, Andrew},
  journal={SIAM Journal on Optimization},
  volume={26},
  number={1},
  pages={57--95},
  year={2016},
  publisher={SIAM}
}

@article{cutkosky2019momentum,
  title={Momentum-Based Variance Reduction in Non-Convex {SGD}},
  author={Cutkosky, Ashok and Orabona, Francesco},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{li2017stochastic,
  title={Stochastic modified equations and adaptive stochastic gradient algorithms},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  booktitle={International Conference on Machine Learning},
  pages={2101--2110},
  year={2017},
  organization={PMLR}
}

@article{choi2019faster,
  title={Faster neural network training with data echoing},
  author={Choi, Dami and Passos, Alexandre and Shallue, Christopher J and Dahl, George E},
  journal={arXiv preprint arXiv:1907.05550},
  year={2019}
}

@inproceedings{smith2019super,
  title={Super-convergence: Very fast training of neural networks using large learning rates},
  author={Smith, Leslie N and Topin, Nicholay},
  booktitle={Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications},
  volume={11006},
  pages={1100612},
  year={2019},
  organization={International Society for Optics and Photonics}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{ward2019adagrad,
  title={AdaGrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  booktitle={International Conference on Machine Learning},
  pages={6677--6686},
  year={2019},
  organization={PMLR}
}

@book{aarts1989simulated,
  title={Simulated annealing and Boltzmann machines: a stochastic approach to combinatorial optimization and neural computing},
  author={Aarts, Emile and Korst, Jan},
  year={1989},
  publisher={John Wiley \& Sons, Inc.}
}

@article{li2019exponential,
  title={An exponential learning rate schedule for deep learning},
  author={Li, Zhiyuan and Arora, Sanjeev},
  journal={arXiv preprint arXiv:1910.07454},
  year={2019}
}

@inproceedings{smith2018don,
  title={Don't Decay the Learning Rate, Increase the Batch Size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{cohen2020gd,
  title={{GD} on Neural Networks Typically Occurs at the Edge of Stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, Zico and Talwalkar, Ameet},
  year={2020}
}

@inproceedings{giladi2019stability,
  title={At Stability's Edge: How to Adjust Hyperparameters to Preserve Minima Selection in Asynchronous Training of Neural Networks?},
  author={Giladi, Niv and Nacson, Mor Shpigel and Hoffer, Elad and Soudry, Daniel},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

@article{vishnoi2012laplacian,
  title={Laplacian solvers and their algorithmic applications},
  author={Vishnoi, Nisheeth K},
  journal={Theoretical Computer Science},
  volume={8},
  number={1-2},
  pages={1--141},
  year={2012}
}

@article{zhang2019algorithmic,
  title={Which algorithmic choices matter at which batch sizes? {I}nsights from a noisy quadratic model},
  author={Zhang, Guodong and Li, Lala and Nado, Zachary and Martens, James and Sachdeva, Sushant and Dahl, George E and Shallue, Christopher J and Grosse, Roger},
  journal={arXiv preprint arXiv:1907.04164},
  year={2019}
}

@inproceedings{jin2017escape,
  title={How to escape saddle points efficiently},
  author={Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  booktitle={International Conference on Machine Learning},
  pages={1724--1732},
  year={2017},
  organization={PMLR}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{lebedev2002construction,
  title={On construction of the stable permutations of parameters for the Chebyshev iterative methods. Part I},
  author={Lebedev, VI and Finogenov, SA},
  journal={Russian Journal of Numerical Analysis and Mathematical Modelling},
  volume={17},
  number={5},
  pages={437--456},
  year={2002},
  publisher={De Gruyter}
}

@article{shallue2019measuring,
  title={Measuring the Effects of Data Parallelism on Neural Network Training},
  author={Shallue, Christopher J and Lee, Jaehoon and Antognini, Joseph and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E},
  journal={Journal of Machine Learning Research},
  volume={20},
  pages={1--49},
  year={2019}
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@article{lebedev2004construction,
  title={On construction of the stable permutations of parameters for the Chebyshev iterative methods. Part II},
  author={Lebedev, VI and Finogenov, SA},
  journal={Russian Journal of Numerical Analysis and Mathematical Modelling},
  volume={19},
  number={3},
  pages={251--263},
  year={2004},
  publisher={De Gruyter}
}

@article{lebedev1973solution,
  title={Solution of the parameter ordering problem in Chebyshev iterative methods},
  author={Lebedev, VI and Finogenov, SA},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={13},
  number={1},
  pages={21--41},
  year={1973},
  publisher={Elsevier}
}

@article{lebedev1976utilization,
  title={Utilization of ordered Chebyshev parameters in iterative methods},
  author={Lebedev, VI and Finogenov, SA},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={16},
  number={4},
  pages={70--83},
  year={1976},
  publisher={Elsevier}
}

@inproceedings{schaul2013no,
  title={No more pesky learning rates},
  author={Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  booktitle={International Conference on Machine Learning},
  pages={343--351},
  year={2013},
  organization={PMLR}
}

@inproceedings{bello2017neural,
  title={Neural optimizer search with reinforcement learning},
  author={Bello, Irwan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc V},
  booktitle={International Conference on Machine Learning},
  pages={459--468},
  year={2017},
  organization={PMLR}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@book{higham2002accuracy,
  title={Accuracy and stability of numerical algorithms},
  author={Higham, Nicholas J},
  year={2002},
  publisher={SIAM}
}

@book{gottlieb1977numerical,
  title={Numerical analysis of spectral methods: theory and applications},
  author={Gottlieb, David and Orszag, Steven A},
  year={1977},
  publisher={SIAM}
}

@article{liu1989limited,
  title={On the limited memory BFGS method for large scale optimization},
  author={Liu, Dong C and Nocedal, Jorge},
  journal={Mathematical programming},
  volume={45},
  number={1},
  pages={503--528},
  year={1989},
  publisher={Springer}
}

@article{lecun1989backpropagation,
  title={Backpropagation applied to handwritten zip code recognition},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural computation},
  volume={1},
  number={4},
  pages={541--551},
  year={1989},
  publisher={MIT Press}
}

@article{clenshaw1955note,
  title={A note on the summation of Chebyshev series},
  author={Clenshaw, Charles W},
  journal={Mathematics of Computation},
  volume={9},
  number={51},
  pages={118--120},
  year={1955}
}

@article{lan2012optimal,
  title={An optimal method for stochastic composite optimization},
  author={Lan, Guanghui},
  journal={Mathematical Programming},
  volume={133},
  number={1-2},
  pages={365--397},
  year={2012},
  publisher={Springer}
}

@article{ge2019step,
  title={The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure For Least Squares},
  author={Ge, Rong and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={14977--14988},
  year={2019}
}

@inproceedings{orabona2017training,
  title={Training deep networks without learning rates through coin betting},
  author={Orabona, Francesco and Tommasi, Tatiana},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={2157--2167},
  year={2017}
}

@inproceedings{cutkosky2018black,
  title={Black-box reductions for parameter-free online learning in banach spaces},
  author={Cutkosky, Ashok and Orabona, Francesco},
  booktitle={Conference On Learning Theory},
  pages={1493--1529},
  year={2018},
  organization={PMLR}
}
