\begin{thebibliography}{}

\bibitem[Achille and Soatto, 2017]{achille2017emergence}
Achille, A. and Soatto, S. (2017).
\newblock On the emergence of invariance and disentangling in deep
  representations.
\newblock {\em arXiv:1706.01350}.

\bibitem[Alemi and Fischer, 2018]{alemi2018therml}
Alemi, A.~A. and Fischer, I. (2018).
\newblock Therml: Thermodynamics of machine learning.
\newblock {\em arXiv preprint arXiv:1807.04162}.

\bibitem[Alemi et~al., 2016]{alemi2016deep}
Alemi, A.~A., Fischer, I., Dillon, J.~V., and Murphy, K. (2016).
\newblock Deep variational information bottleneck.
\newblock {\em arXiv:1612.00410}.

\bibitem[Alemi et~al., 2017]{alemi2017fixing}
Alemi, A.~A., Poole, B., Fischer, I., Dillon, J.~V., Saurous, R.~A., and
  Murphy, K. (2017).
\newblock Fixing a broken elbo.
\newblock {\em arXiv preprint arXiv:1711.00464}.

\bibitem[Baxter, 2000]{baxter2000model}
Baxter, J. (2000).
\newblock A model of inductive bias learning.
\newblock {\em Journal of artificial intelligence research}, 12:149--198.

\bibitem[Brekelmans et~al., 2019]{brekelmans2019exact}
Brekelmans, R., Moyer, D., Galstyan, A., and Ver~Steeg, G. (2019).
\newblock Exact rate-distortion in autoencoders via echo noise.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3884--3895.

\bibitem[Chaudhari et~al., 2019]{chaudhari2019entropy}
Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C.,
  Chayes, J., Sagun, L., and Zecchina, R. (2019).
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2019(12):124018.

\bibitem[Chaudhari and Soatto, 2017]{chaudhari2017stochastic}
Chaudhari, P. and Soatto, S. (2017).
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock {\em arXiv preprint arXiv:1710.11029}.

\bibitem[Cuturi, 2013]{cuturi2013sinkhorn}
Cuturi, M. (2013).
\newblock Sinkhorn distances: Lightspeed computation of optimal transport.
\newblock In {\em Advances in neural information processing systems}, pages
  2292--2300.

\bibitem[Doersch and Zisserman, 2017]{doersch2017multi}
Doersch, C. and Zisserman, A. (2017).
\newblock Multi-task self-supervised visual learning.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 2051--2060.

\bibitem[Dziugaite and Roy, 2017]{dziugaite2017computing}
Dziugaite, G.~K. and Roy, D.~M. (2017).
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock {\em arXiv preprint arXiv:1703.11008}.

\bibitem[Girshick et~al., 2014]{girshick2014rich}
Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2014).
\newblock Rich feature hierarchies for accurate object detection and semantic
  segmentation.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 580--587.

\bibitem[He et~al., 2016]{he2016identity}
He, K., Zhang, X., Ren, S., and Sun, J. (2016).
\newblock Identity mappings in deep residual networks.
\newblock {\em arXiv:1603.05027}.

\bibitem[Higgins et~al., 2017]{higgins2016beta}
Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M.,
  Mohamed, S., and A, L. (2017).
\newblock {beta-VAE: Learning Basic Visual Concepts with a Constrained
  Variational Framework }.
\newblock In {\em ICLR}.

\bibitem[Ioffe and Szegedy, 2015]{ioffe2015batch}
Ioffe, S. and Szegedy, C. (2015).
\newblock {Batch normalization: Accelerating deep network training by reducing
  internal covariate shift}.
\newblock {\em arXiv:1502.03167}.

\bibitem[Jordan et~al., 1998]{jordan1998introduction}
Jordan, M.~I., Ghahramani, Z., Jaakkola, T.~S., and Saul, L.~K. (1998).
\newblock An introduction to variational methods for graphical models.
\newblock In {\em Learning in graphical models}, pages 105--161. Springer.

\bibitem[Kingma and Ba, 2014]{kingma2014adam}
Kingma, D. and Ba, J. (2014).
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv:1412.6980}.

\bibitem[Kingma and Welling, 2013]{kingma2013auto}
Kingma, D.~P. and Welling, M. (2013).
\newblock {Auto-encoding variational Bayes}.
\newblock {\em arXiv:1312.6114}.

\bibitem[Kirkpatrick et~al., 2017]{kirkpatrick2017overcoming}
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu,
  A.~A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et~al.
  (2017).
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the national academy of sciences},
  114(13):3521--3526.

\bibitem[Krizhevsky, 2009]{krizhevsky2009learning}
Krizhevsky, A. (2009).
\newblock Learning multiple layers of features from tiny images.
\newblock Master's thesis, Computer Science, University of Toronto.

\bibitem[LeCun et~al., 1998]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998).
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324.

\bibitem[Levin and Peres, 2017]{levin2017markov}
Levin, D.~A. and Peres, Y. (2017).
\newblock {\em Markov chains and mixing times}, volume 107.
\newblock American Mathematical Soc.

\bibitem[Mandt et~al., 2017]{mandt2017stochastic}
Mandt, S., Hoffman, M.~D., and Blei, D.~M. (2017).
\newblock {Stochastic Gradient Descent as Approximate Bayesian Inference}.
\newblock {\em arXiv:1704.04289}.

\bibitem[McAllester, 2013]{mcallester2013pac}
McAllester, D. (2013).
\newblock A pac-bayesian tutorial with a dropout bound.
\newblock {\em arXiv:1307.2118}.

\bibitem[Mezard and Montanari, 2009]{mezard2009information}
Mezard, M. and Montanari, A. (2009).
\newblock {\em Information, physics, and computation}.
\newblock Oxford University Press.

\bibitem[Noh et~al., 2015]{noh2015learning}
Noh, H., Hong, S., and Han, B. (2015).
\newblock Learning deconvolution network for semantic segmentation.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1520--1528.

\bibitem[Pearlmutter, 1994]{pearlmutter1994fast}
Pearlmutter, B.~A. (1994).
\newblock Fast exact multiplication by the hessian.
\newblock {\em Neural computation}, 6(1):147--160.

\bibitem[Robbins and Monro, 1951]{robbins1951stochastic}
Robbins, H. and Monro, S. (1951).
\newblock A stochastic approximation method.
\newblock {\em The annals of mathematical statistics}, pages 400--407.

\bibitem[Sethna, 2006]{sethna2006statistical}
Sethna, J. (2006).
\newblock {\em Statistical mechanics: entropy, order parameters, and
  complexity}, volume~14.
\newblock Oxford University Press.

\bibitem[Sharif~Razavian et~al., 2014]{sharif2014cnn}
Sharif~Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. (2014).
\newblock Cnn features off-the-shelf: an astounding baseline for recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition workshops}, pages 806--813.

\bibitem[Shwartz-Ziv and Tishby, 2017]{shwartz2017opening}
Shwartz-Ziv, R. and Tishby, N. (2017).
\newblock Opening the black box of deep neural networks via information.
\newblock {\em arXiv:1703.00810}.

\bibitem[Tishby et~al., 2000]{tishby2000information}
Tishby, N., Pereira, F.~C., and Bialek, W. (2000).
\newblock The information bottleneck method.
\newblock {\em arXiv preprint physics/0004057}.

\bibitem[Ver~Steeg and Galstyan, 2015]{ver2015maximally}
Ver~Steeg, G. and Galstyan, A. (2015).
\newblock Maximally informative hierarchical representations of
  high-dimensional data.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1004--1012.

\bibitem[Villani, 2008]{villani2008optimal}
Villani, C. (2008).
\newblock {\em Optimal transport: old and new}, volume 338.
\newblock Springer Science \& Business Media.

\bibitem[Zamir et~al., 2018]{zamir2018taskonomy}
Zamir, A.~R., Sax, A., Shen, W., Guibas, L.~J., Malik, J., and Savarese, S.
  (2018).
\newblock Taskonomy: Disentangling task transfer learning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3712--3722.

\end{thebibliography}
