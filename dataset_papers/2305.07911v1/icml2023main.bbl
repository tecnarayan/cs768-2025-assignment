\begin{thebibliography}{100}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2019)Abbasi-Yadkori, Bartlett, Bhatia, Lazic,
  Szepesvari, and Weisz]{abbasi2019politex}
Abbasi-Yadkori, Y., Bartlett, P., Bhatia, K., Lazic, N., Szepesvari, C., and
  Weisz, G.
\newblock Politex: Regret bounds for policy iteration using expert prediction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3692--3702. PMLR, 2019.

\bibitem[Agarwal \& Duchi(2012)Agarwal and Duchi]{agarwal2012distributed}
Agarwal, A. and Duchi, J.~C.
\newblock Distributed delayed stochastic optimization.
\newblock In \emph{2012 IEEE 51st IEEE Conference on Decision and Control
  (CDC)}, pp.\  5451--5452. IEEE, 2012.

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020optimality}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock In \emph{Conference on Learning Theory}, pp.\  64--66. PMLR, 2020.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  463--474. PMLR, 2020.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  263--272. JMLR. org, 2017.

\bibitem[Beygelzimer et~al.(2011)Beygelzimer, Langford, Li, Reyzin, and
  Schapire]{beygelzimer2011contextual}
Beygelzimer, A., Langford, J., Li, L., Reyzin, L., and Schapire, R.
\newblock Contextual bandit algorithms with supervised learning guarantees.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  19--26. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Bistritz et~al.(2019)Bistritz, Zhou, Chen, Bambos, and
  Blanchet]{bistritz2019online}
Bistritz, I., Zhou, Z., Chen, X., Bambos, N., and Blanchet, J.
\newblock Online exp3 learning in adversarial bandits with delayed feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11349--11358, 2019.

\bibitem[Bistritz et~al.(2021)Bistritz, Zhou, Chen, Bambos, and
  Blanchet]{bistritz2021no}
Bistritz, I., Zhou, Z., Chen, X., Bambos, N., and Blanchet, J.
\newblock No discounted-regret learning in adversarial bandits with delays.
\newblock \emph{arXiv preprint arXiv:2103.04550}, 2021.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2020provably}
Cai, Q., Yang, Z., Jin, C., and Wang, Z.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1283--1294. PMLR, 2020.

\bibitem[Cesa-Bianchi et~al.(2016)Cesa-Bianchi, Gentile, Mansour, and
  Minora]{cesa2016delay}
Cesa-Bianchi, N., Gentile, C., Mansour, Y., and Minora, A.
\newblock Delay and cooperation in nonstochastic bandits.
\newblock In \emph{Conference on Learning Theory}, pp.\  605--622, 2016.

\bibitem[Cesa-Bianchi et~al.(2018)Cesa-Bianchi, Gentile, and
  Mansour]{cesa2018nonstochastic}
Cesa-Bianchi, N., Gentile, C., and Mansour, Y.
\newblock Nonstochastic bandits with composite anonymous feedback.
\newblock In \emph{Conference On Learning Theory}, pp.\  750--773, 2018.

\bibitem[Cesa-Bianchi et~al.(2019)Cesa-Bianchi, Gentile, and
  Mansour]{cesa2019delay}
Cesa-Bianchi, N., Gentile, C., and Mansour, Y.
\newblock Delay and cooperation in nonstochastic bandits.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 613--650, 2019.

\bibitem[Changuel et~al.(2012)Changuel, Sayadi, and
  Kieffer]{changuel2012online}
Changuel, N., Sayadi, B., and Kieffer, M.
\newblock Online learning for qoe-based video streaming to mobile receivers.
\newblock In \emph{2012 IEEE Globecom Workshops}, pp.\  1319--1324. IEEE, 2012.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Xu, Liu, Li, and
  Zhao]{chen2020delay}
Chen, B., Xu, M., Liu, Z., Li, L., and Zhao, D.
\newblock Delay-aware multi-agent reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2005.05441}, 2020{\natexlab{a}}.

\bibitem[Chen \& Luo(2021)Chen and Luo]{chen2021finding}
Chen, L. and Luo, H.
\newblock Finding the stochastic shortest path with low regret: The adversarial
  cost and unknown transition case.
\newblock \emph{arXiv preprint arXiv:2102.05284}, 2021.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Luo, and Wei]{chen2020minimax}
Chen, L., Luo, H., and Wei, C.-Y.
\newblock Minimax regret for stochastic shortest path with adversarial costs
  and known transition.
\newblock \emph{arXiv preprint arXiv:2012.04053}, 2020{\natexlab{b}}.

\bibitem[Chen et~al.(2021)Chen, Jafarnia-Jahromi, Jain, and
  Luo]{chen2021implicit}
Chen, L., Jafarnia-Jahromi, M., Jain, R., and Luo, H.
\newblock Implicit finite-horizon approximation and efficient optimal
  algorithms for stochastic shortest path.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Jain, and Luo]{chen2022improved}
Chen, L., Jain, R., and Luo, H.
\newblock Improved no-regret algorithms for stochastic shortest path with
  linear mdp.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3204--3245. PMLR, 2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Luo, and
  Rosenberg]{chen2022policy}
Chen, L., Luo, H., and Rosenberg, A.
\newblock Policy optimization for stochastic shortest path.
\newblock In Loh, P. and Raginsky, M. (eds.), \emph{Conference on Learning
  Theory, 2-5 July 2022, London, {UK}}, volume 178 of \emph{Proceedings of
  Machine Learning Research}, pp.\  982--1046. {PMLR}, 2022{\natexlab{b}}.

\bibitem[Cohen et~al.(2021{\natexlab{a}})Cohen, Daniely, Drori, Koren, and
  Schain]{cohen2021asynchronous}
Cohen, A., Daniely, A., Drori, Y., Koren, T., and Schain, M.
\newblock Asynchronous stochastic optimization robust to arbitrary delays.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y.~N., Liang, P., and
  Vaughan, J.~W. (eds.), \emph{Advances in Neural Information Processing
  Systems 34: Annual Conference on Neural Information Processing Systems 2021,
  NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  9024--9035,
  2021{\natexlab{a}}.

\bibitem[Cohen et~al.(2021{\natexlab{b}})Cohen, Efroni, Mansour, and
  Rosenberg]{cohen2021minimax}
Cohen, A., Efroni, Y., Mansour, Y., and Rosenberg, A.
\newblock Minimax regret for stochastic shortest path.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{b}}.

\bibitem[Dai et~al.(2022)Dai, Luo, and Chen]{dai2022follow}
Dai, Y., Luo, H., and Chen, L.
\newblock Follow-the-perturbed-leader for adversarial markov decision processes
  with bandit feedback.
\newblock \emph{arXiv preprint arXiv:2205.13451}, 2022.

\bibitem[Dai et~al.(2023)Dai, Luo, Wei, and Zimmert]{dai2023refined}
Dai, Y., Luo, H., Wei, C.-Y., and Zimmert, J.
\newblock Refined regret for adversarial mdps with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:2301.12942}, 2023.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Dann, C., Lattimore, T., and Brunskill, E.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5713--5723, 2017.

\bibitem[Derman et~al.(2021)Derman, Dalal, and Mannor]{derman2021acting}
Derman, E., Dalal, G., and Mannor, S.
\newblock Acting in delayed environments with non-stationary markov policies.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}, 2021.

\bibitem[Dudik et~al.(2011)Dudik, Hsu, Kale, Karampatziakis, Langford, Reyzin,
  and Zhang]{dudik2011efficient}
Dudik, M., Hsu, D., Kale, S., Karampatziakis, N., Langford, J., Reyzin, L., and
  Zhang, T.
\newblock Efficient optimal learning for contextual bandits.
\newblock In \emph{Proceedings of the Twenty-Seventh Conference on Uncertainty
  in Artificial Intelligence}, pp.\  169--178, 2011.

\bibitem[Efroni et~al.(2019)Efroni, Merlis, Ghavamzadeh, and
  Mannor]{efroni2019tight}
Efroni, Y., Merlis, N., Ghavamzadeh, M., and Mannor, S.
\newblock Tight regret bounds for model-based reinforcement learning with
  greedy policies.
\newblock In Wallach, H.~M., Larochelle, H., Beygelzimer, A.,
  d'Alch{\'{e}}{-}Buc, F., Fox, E.~B., and Garnett, R. (eds.), \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,
  Vancouver, BC, Canada}, pp.\  12203--12213, 2019.

\bibitem[Even-Dar et~al.(2009)Even-Dar, Kakade, and Mansour]{even2009online}
Even-Dar, E., Kakade, S.~M., and Mansour, Y.
\newblock Online markov decision processes.
\newblock \emph{Mathematics of Operations Research}, 34\penalty0 (3):\penalty0
  726--736, 2009.

\bibitem[Freund \& Schapire(1997)Freund and Schapire]{freund1997decision}
Freund, Y. and Schapire, R.~E.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{Journal of computer and system sciences}, 55\penalty0
  (1):\penalty0 119--139, 1997.

\bibitem[Gael et~al.(2020)Gael, Vernade, Carpentier, and
  Valko]{manegueu2020stochastic}
Gael, M.~A., Vernade, C., Carpentier, A., and Valko, M.
\newblock Stochastic bandits with arm-dependent delays.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3348--3356. PMLR, 2020.

\bibitem[Gu et~al.(2017)Gu, Holly, Lillicrap, and Levine]{gu2017deep}
Gu, S., Holly, E., Lillicrap, T., and Levine, S.
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock In \emph{2017 IEEE international conference on robotics and
  automation (ICRA)}, pp.\  3389--3396. IEEE, 2017.

\bibitem[Gyorgy \& Joulani(2021)Gyorgy and Joulani]{gyorgy2020adapting}
Gyorgy, A. and Joulani, P.
\newblock Adapting to delays and data in adversarial multi-armed bandits.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3988--3997. PMLR, 2021.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1870, 2018.

\bibitem[He et~al.(2022)He, Zhou, and Gu]{pmlr-v151-he22a}
He, J., Zhou, D., and Gu, Q.
\newblock Near-optimal policy optimization algorithms for learning adversarial
  linear mixture mdps.
\newblock In Camps-Valls, G., Ruiz, F. J.~R., and Valera, I. (eds.),
  \emph{Proceedings of The 25th International Conference on Artificial
  Intelligence and Statistics}, volume 151 of \emph{Proceedings of Machine
  Learning Research}, pp.\  4259--4280. PMLR, 28--30 Mar 2022.

\bibitem[Howson et~al.(2021)Howson, Pike-Burke, and Filippi]{howson2021delayed}
Howson, B., Pike-Burke, C., and Filippi, S.
\newblock Delayed feedback in episodic reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2111.07615}, 2021.

\bibitem[Howson et~al.(2022)Howson, Pike-Burke, and Filippi]{howson2022delayed}
Howson, B., Pike-Burke, C., and Filippi, S.
\newblock Delayed feedback in generalised linear bandits revisited.
\newblock \emph{arXiv preprint arXiv:2207.10786}, 2022.

\bibitem[Ito et~al.(2020)Ito, Hatano, Sumita, Takemura, Fukunaga, Kakimura, and
  Kawarabayashi]{ito2020delay}
Ito, S., Hatano, D., Sumita, H., Takemura, K., Fukunaga, T., Kakimura, N., and
  Kawarabayashi, K.-I.
\newblock Delay and cooperation in nonstochastic linear bandits.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4872--4883, 2020.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (4), 2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is q-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4863--4873, 2018.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Jin, Luo, Sra, and
  Yu]{jin2019learning}
Jin, C., Jin, T., Luo, H., Sra, S., and Yu, T.
\newblock Learning adversarial markov decision processes with bandit feedback
  and unknown transition.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4860--4869. PMLR, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, Wang, and
  Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143,
  2020{\natexlab{b}}.

\bibitem[Jin \& Luo(2020)Jin and Luo]{jin2020simultaneously}
Jin, T. and Luo, H.
\newblock Simultaneously learning stochastic and adversarial episodic mdps with
  known transition.
\newblock \emph{Advances in neural information processing systems}, 2020.

\bibitem[Jin et~al.(2021)Jin, Huang, and Luo]{jin2021best}
Jin, T., Huang, L., and Luo, H.
\newblock The best of both worlds: stochastic and adversarial episodic mdps
  with unknown transition.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Jin et~al.(2022)Jin, Lancewicki, Luo, Mansour, and
  Rosenberg]{jin2022near}
Jin, T., Lancewicki, T., Luo, H., Mansour, Y., and Rosenberg, A.
\newblock Near-optimal regret for adversarial mdp with delayed bandit feedback.
\newblock \emph{arXiv preprint arXiv:2201.13172}, 2022.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{In Proc. 19th International Conference on Machine Learning}.
  Citeseer, 2002.

\bibitem[Kakade(2001)]{kakade2001natural}
Kakade, S.~M.
\newblock A natural policy gradient.
\newblock \emph{Advances in neural information processing systems},
  14:\penalty0 1531--1538, 2001.

\bibitem[Katsikopoulos \& Engelbrecht(2003)Katsikopoulos and
  Engelbrecht]{katsikopoulos2003markov}
Katsikopoulos, K.~V. and Engelbrecht, S.~E.
\newblock Markov decision processes with delays and asynchronous cost
  collection.
\newblock \emph{IEEE transactions on automatic control}, 48\penalty0
  (4):\penalty0 568--574, 2003.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lancewicki et~al.(2021)Lancewicki, Segal, Koren, and
  Mansour]{lancewicki2021stochastic}
Lancewicki, T., Segal, S., Koren, T., and Mansour, Y.
\newblock Stochastic multi-armed bandits with unrestricted delay distributions.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning, {ICML} 2021, 18-24 July 2021, Virtual Event}, pp.\  5969--5978.
  {PMLR}, 2021.

\bibitem[Lancewicki et~al.(2022{\natexlab{a}})Lancewicki, Rosenberg, and
  Mansour]{Lancewicki0M22}
Lancewicki, T., Rosenberg, A., and Mansour, Y.
\newblock Cooperative online learning in stochastic and adversarial mdps.
\newblock In Chaudhuri, K., Jegelka, S., Song, L., Szepesv{\'{a}}ri, C., Niu,
  G., and Sabato, S. (eds.), \emph{International Conference on Machine
  Learning, {ICML} 2022, 17-23 July 2022, Baltimore, Maryland, {USA}}, volume
  162 of \emph{Proceedings of Machine Learning Research}, pp.\  11918--11968.
  {PMLR}, 2022{\natexlab{a}}.

\bibitem[Lancewicki et~al.(2022{\natexlab{b}})Lancewicki, Rosenberg, and
  Mansour]{lancewicki2020learning}
Lancewicki, T., Rosenberg, A., and Mansour, Y.
\newblock Learning adversarial markov decision processes with delayed feedback.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pp.\  7281--7289, 2022{\natexlab{b}}.

\bibitem[Levine \& Koltun(2013)Levine and Koltun]{levine2013guided}
Levine, S. and Koltun, V.
\newblock Guided policy search.
\newblock In \emph{International conference on machine learning}, pp.\  1--9.
  PMLR, 2013.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Liu et~al.(2014)Liu, Wang, and Liu]{liu2014impact}
Liu, S., Wang, X., and Liu, P.~X.
\newblock Impact of communication delays on secondary frequency control in an
  islanded microgrid.
\newblock \emph{IEEE Transactions on Industrial Electronics}, 62\penalty0
  (4):\penalty0 2021--2031, 2014.

\bibitem[Luo et~al.(2021)Luo, Wei, and Lee]{luo2021policy}
Luo, H., Wei, C.-Y., and Lee, C.-W.
\newblock Policy optimization in adversarial mdps: Improved exploration via
  dilated bonuses.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Mahmood et~al.(2018)Mahmood, Korenkevych, Komer, and
  Bergstra]{mahmood2018setting}
Mahmood, A.~R., Korenkevych, D., Komer, B.~J., and Bergstra, J.
\newblock Setting up a reinforcement learning task with a real-world robot.
\newblock In \emph{2018 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  4635--4640. IEEE, 2018.

\bibitem[Masoudian et~al.(2022)Masoudian, Zimmert, and
  Seldin]{masoudian2022best}
Masoudian, S., Zimmert, J., and Seldin, Y.
\newblock A best-of-both-worlds algorithm for bandits with delayed feedback.
\newblock \emph{arXiv preprint arXiv:2206.14906}, 2022.

\bibitem[Min et~al.(2022)Min, He, Wang, and Gu]{min2022learning}
Min, Y., He, J., Wang, T., and Gu, Q.
\newblock Learning stochastic shortest path with linear function approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  15584--15629. PMLR, 2022.

\bibitem[Neu(2015)]{neu2015explore}
Neu, G.
\newblock Explore no more: Improved high-probability regret bounds for
  non-stochastic bandits.
\newblock \emph{Advances in Neural Information Processing Systems},
  28:\penalty0 3168--3176, 2015.

\bibitem[Neu \& Olkhovskaya(2021)Neu and Olkhovskaya]{neu2021online}
Neu, G. and Olkhovskaya, J.
\newblock Online learning in mdps with linear function approximation and bandit
  feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 10407--10417, 2021.

\bibitem[Neu et~al.(2010{\natexlab{a}})Neu, Gy{\"{o}}rgy, and
  Szepesv{\'{a}}ri]{neu2010loopfree}
Neu, G., Gy{\"{o}}rgy, A., and Szepesv{\'{a}}ri, C.
\newblock The online loop-free stochastic shortest-path problem.
\newblock In \emph{{COLT} 2010 - The 23rd Conference on Learning Theory, Haifa,
  Israel, June 27-29, 2010}, pp.\  231--243, 2010{\natexlab{a}}.

\bibitem[Neu et~al.(2010{\natexlab{b}})Neu, Gy{\"{o}}rgy, and
  Szepesv{\'{a}}ri]{neu2010ossp}
Neu, G., Gy{\"{o}}rgy, A., and Szepesv{\'{a}}ri, C.
\newblock The online loop-free stochastic shortest-path problem.
\newblock In \emph{Conference on Learning Theory {(COLT)}}, pp.\  231--243,
  2010{\natexlab{b}}.

\bibitem[Neu et~al.(2012)Neu, Gy{\"{o}}rgy, and
  Szepesv{\'{a}}ri]{neu2012unknown}
Neu, G., Gy{\"{o}}rgy, A., and Szepesv{\'{a}}ri, C.
\newblock The adversarial stochastic shortest path problem with unknown
  transition probabilities.
\newblock In \emph{Proceedings of the Fifteenth International Conference on
  Artificial Intelligence and Statistics, {(AISTATS)}}, pp.\  805--813, 2012.

\bibitem[Neu et~al.(2014)Neu, Gy{\"{o}}rgy, Szepesv{\'{a}}ri, and
  Antos]{neu2014bandit}
Neu, G., Gy{\"{o}}rgy, A., Szepesv{\'{a}}ri, C., and Antos, A.
\newblock Online {Markov Decision Processes} under bandit feedback.
\newblock \emph{{IEEE} Trans. Automat. Contr.}, 59\penalty0 (3):\penalty0
  676--691, 2014.

\bibitem[Pike-Burke et~al.(2018)Pike-Burke, Agrawal, Szepesvari, and
  Grunewalder]{pike2018bandits}
Pike-Burke, C., Agrawal, S., Szepesvari, C., and Grunewalder, S.
\newblock Bandits with delayed, aggregated anonymous feedback.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4105--4113. PMLR, 2018.

\bibitem[Quanrud \& Khashabi(2015)Quanrud and Khashabi]{quanrud2015online}
Quanrud, K. and Khashabi, D.
\newblock Online learning with adversarial delays.
\newblock \emph{Advances in neural information processing systems},
  28:\penalty0 1270--1278, 2015.

\bibitem[Raffin et~al.(2021)Raffin, Hill, Gleave, Kanervisto, Ernestus, and
  Dormann]{raffin2021stable}
Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N.
\newblock Stable-baselines3: Reliable reinforcement learning implementations.
\newblock \emph{Journal of Machine Learning Research}, 2021.

\bibitem[Rosenberg \& Mansour(2019{\natexlab{a}})Rosenberg and
  Mansour]{rosenberg2019bandit}
Rosenberg, A. and Mansour, Y.
\newblock Online stochastic shortest path with bandit feedback and unknown
  transition function.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2209--2218, 2019{\natexlab{a}}.

\bibitem[Rosenberg \& Mansour(2019{\natexlab{b}})Rosenberg and
  Mansour]{rosenberg2019online}
Rosenberg, A. and Mansour, Y.
\newblock Online convex optimization in adversarial markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5478--5486. PMLR, 2019{\natexlab{b}}.

\bibitem[Rosenberg \& Mansour(2021{\natexlab{a}})Rosenberg and
  Mansour]{RosenbergM21}
Rosenberg, A. and Mansour, Y.
\newblock Oracle-efficient regret minimization in factored mdps with unknown
  structure.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y.~N., Liang, P., and
  Vaughan, J.~W. (eds.), \emph{Advances in Neural Information Processing
  Systems 34: Annual Conference on Neural Information Processing Systems 2021,
  NeurIPS 2021, December 6-14, 2021, virtual}, pp.\  11148--11159,
  2021{\natexlab{a}}.

\bibitem[Rosenberg \& Mansour(2021{\natexlab{b}})Rosenberg and
  Mansour]{rosenberg2021stochastic}
Rosenberg, A. and Mansour, Y.
\newblock Stochastic shortest path with adversarially changing costs.
\newblock In Zhou, Z. (ed.), \emph{Proceedings of the Thirtieth International
  Joint Conference on Artificial Intelligence, {IJCAI} 2021, Virtual Event /
  Montreal, Canada, 19-27 August 2021}, pp.\  2936--2942. ijcai.org,
  2021{\natexlab{b}}.

\bibitem[Rosenberg et~al.(2020)Rosenberg, Cohen, Mansour, and
  Kaplan]{cohen2020ssp}
Rosenberg, A., Cohen, A., Mansour, Y., and Kaplan, H.
\newblock Near-optimal regret bounds for stochastic shortest path.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8210--8219. PMLR, 2020.

\bibitem[Schuitema et~al.(2010)Schuitema, Bu{\c{s}}oniu, Babu{\v{s}}ka, and
  Jonker]{schuitema2010control}
Schuitema, E., Bu{\c{s}}oniu, L., Babu{\v{s}}ka, R., and Jonker, P.
\newblock Control delay in reinforcement learning for real-time dynamic
  systems: a memoryless approach.
\newblock In \emph{2010 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  3226--3231. IEEE, 2010.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shani et~al.(2020{\natexlab{a}})Shani, Efroni, and Mannor]{ShaniEM20}
Shani, L., Efroni, Y., and Mannor, S.
\newblock Adaptive trust region policy optimization: Global convergence and
  faster rates for regularized mdps.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of
  Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium
  on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York,
  NY, USA, February 7-12, 2020}, pp.\  5668--5675. {AAAI} Press,
  2020{\natexlab{a}}.

\bibitem[Shani et~al.(2020{\natexlab{b}})Shani, Efroni, Rosenberg, and
  Mannor]{shani2020optimistic}
Shani, L., Efroni, Y., Rosenberg, A., and Mannor, S.
\newblock Optimistic policy optimization with bandit feedback.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8604--8613. PMLR, 2020{\natexlab{b}}.

\bibitem[Sherman et~al.(2023)Sherman, Koren, and Mansour]{sherman2023improved}
Sherman, U., Koren, T., and Mansour, Y.
\newblock Improved regret for efficient online reinforcement learning with
  linear function approximation.
\newblock \emph{arXiv preprint arXiv:2301.13087}, 2023.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{sutton1999policy}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Tarbouriech et~al.(2020)Tarbouriech, Garcelon, Valko, Pirotta, and
  Lazaric]{tarbouriech2019noregret}
Tarbouriech, J., Garcelon, E., Valko, M., Pirotta, M., and Lazaric, A.
\newblock No-regret exploration in goal-oriented reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9428--9437. PMLR, 2020.

\bibitem[Tarbouriech et~al.(2021)Tarbouriech, Zhou, Du, Pirotta, Valko, and
  Lazaric]{tarbouriech2021stochastic}
Tarbouriech, J., Zhou, R., Du, S.~S., Pirotta, M., Valko, M., and Lazaric, A.
\newblock Stochastic shortest path: Minimax, parameter-free and towards
  horizon-free regret.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Thune et~al.(2019)Thune, Cesa-Bianchi, and
  Seldin]{thune2019nonstochastic}
Thune, T.~S., Cesa-Bianchi, N., and Seldin, Y.
\newblock Nonstochastic multiarmed bandits with unrestricted delays.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6541--6550, 2019.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ international conference on intelligent robots
  and systems}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Tomar et~al.(2022)Tomar, Shani, Efroni, and
  Ghavamzadeh]{tomar2020mirror}
Tomar, M., Shani, L., Efroni, Y., and Ghavamzadeh, M.
\newblock Mirror descent policy optimization.
\newblock In \emph{The Tenth International Conference on Learning
  Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}, 2022.

\bibitem[Van Der~Hoeven \& Cesa-Bianchi(2022)Van Der~Hoeven and
  Cesa-Bianchi]{van2021nonstochastic}
Van Der~Hoeven, D. and Cesa-Bianchi, N.
\newblock Nonstochastic bandits and experts with arm-dependent delays.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}. PMLR, 2022.

\bibitem[Vernade et~al.(2017)Vernade, Capp{\'e}, and
  Perchet]{vernade2017stochastic}
Vernade, C., Capp{\'e}, O., and Perchet, V.
\newblock Stochastic bandit models for delayed conversions.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, 2017.

\bibitem[Vernade et~al.(2020)Vernade, Carpentier, Lattimore, Zappella, Ermis,
  and Brueckner]{vernade2020linear}
Vernade, C., Carpentier, A., Lattimore, T., Zappella, G., Ermis, B., and
  Brueckner, M.
\newblock Linear bandits with stochastic delayed feedback.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9712--9721. PMLR, 2020.

\bibitem[Vial et~al.(2022)Vial, Parulekar, Shakkottai, and
  Srikant]{vial2022regret}
Vial, D., Parulekar, A., Shakkottai, S., and Srikant, R.
\newblock Regret bounds for stochastic shortest path problems with linear
  function approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  22203--22233. PMLR, 2022.

\bibitem[Walsh et~al.(2009)Walsh, Nouri, Li, and Littman]{walsh2009learning}
Walsh, T.~J., Nouri, A., Li, L., and Littman, M.~L.
\newblock Learning and planning in environments with delayed feedback.
\newblock \emph{Autonomous Agents and Multi-Agent Systems}, 18\penalty0
  (1):\penalty0 83, 2009.

\bibitem[Wei et~al.(2021)Wei, Jahromi, Luo, and Jain]{wei2021learning}
Wei, C.-Y., Jahromi, M.~J., Luo, H., and Jain, R.
\newblock Learning infinite-horizon average-reward mdps with linear function
  approximation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3007--3015. PMLR, 2021.

\bibitem[Yang \& Wang(2019)Yang and Wang]{yang2019sample}
Yang, L. and Wang, M.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6995--7004. PMLR, 2019.

\bibitem[Zanette et~al.(2020{\natexlab{a}})Zanette, Brandfonbrener, Brunskill,
  Pirotta, and Lazaric]{zanette2020frequentist}
Zanette, A., Brandfonbrener, D., Brunskill, E., Pirotta, M., and Lazaric, A.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1954--1964, 2020{\natexlab{a}}.

\bibitem[Zanette et~al.(2020{\natexlab{b}})Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10978--10989. PMLR, 2020{\natexlab{b}}.

\bibitem[Zhou \& Gu(2022)Zhou and Gu]{zhou2022computationally}
Zhou, D. and Gu, Q.
\newblock Computationally efficient horizon-free reinforcement learning for
  linear mixture mdps.
\newblock \emph{arXiv preprint arXiv:2205.11507}, 2022.

\bibitem[Zhou et~al.(2021)Zhou, Gu, and Szepesvari]{zhou2021nearly}
Zhou, D., Gu, Q., and Szepesvari, C.
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock In \emph{Conference on Learning Theory}, pp.\  4532--4576. PMLR,
  2021.

\bibitem[Zhou et~al.(2019)Zhou, Xu, and Blanchet]{zhou2019learning}
Zhou, Z., Xu, R., and Blanchet, J.
\newblock Learning in generalized linear contextual bandits with stochastic
  delays.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5197--5208, 2019.

\bibitem[Zimin \& Neu(2013)Zimin and Neu]{zimin2013online}
Zimin, A. and Neu, G.
\newblock Online learning in episodic markovian decision processes by relative
  entropy policy search.
\newblock In \emph{Advances in Neural Information Processing Systems 26: 27th
  Annual Conference on Neural Information Processing Systems 2013. Proceedings
  of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States},
  pp.\  1583--1591, 2013.

\bibitem[Zimmert \& Seldin(2020)Zimmert and Seldin]{zimmert2020optimal}
Zimmert, J. and Seldin, Y.
\newblock An optimal algorithm for adversarial bandits with arbitrary delays.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3285--3294. PMLR, 2020.

\end{thebibliography}
