\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Affentranger and Wieacker(1991)]{affentranger1991convex}
Fernando Affentranger and John~A Wieacker.
\newblock On the convex hull of uniform random points in a simpled-polytope.
\newblock \emph{Discrete \& Computational Geometry}, 6\penalty0 (3):\penalty0
  291--305, 1991.

\bibitem[Amenta et~al.(2007)Amenta, Attali, and Devillers]{amenta07}
Nina Amenta, Dominique Attali, and Olivier Devillers.
\newblock Complexity of delaunay triangulation for points on lower-dimensional
  polyhedra.
\newblock In \emph{Proceedings of the Eighteenth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, SODA '07, pages 1106--1113, 2007.

\bibitem[Anthony and Bartlett(1995)]{ab-aie-95}
Martin Anthony and Peter~L Bartlett.
\newblock Function learning from interpolation.
\newblock In \emph{Computational Learning Theory: Second European Conference,
  EUROCOLT 95, Barcelona Spain, March 1995, Proceedings}, pages 211--221, 1995.

\bibitem[Anthony and Bartlett(1999)]{ab-nnltf-99}
Martin Anthony and Peter~L Bartlett.
\newblock \emph{Neural Network Learning: Theoretical Foundations}.
\newblock Cambridge University Press, 1999.

\bibitem[Audibert and Tsybakov(2007)]{audibert2007fast}
Jean-Yves Audibert and Alexandre~B Tsybakov.
\newblock Fast learning rates for plug-in classifiers.
\newblock \emph{The Annals of statistics}, 35\penalty0 (2):\penalty0 608--633,
  2007.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Peter Bartlett, Dylan~J Foster, and Matus Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{NIPS}, 2017.

\bibitem[Bartlett et~al.(1996)Bartlett, Long, and Williamson]{bartlett1996fat}
Peter~L Bartlett, Philip~M Long, and Robert~C Williamson.
\newblock Fat-shattering and the learnability of real-valued functions.
\newblock \emph{Journal of Computer and System Sciences}, 52\penalty0
  (3):\penalty0 434--452, 1996.

\bibitem[Bassily et~al.(2016)Bassily, Nissim, Smith, Steinke, Stemmer, and
  Ullman]{bassily2016algorithmic}
Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, Uri Stemmer, and
  Jonathan Ullman.
\newblock Algorithmic stability for adaptive data analysis.
\newblock In \emph{Proceedings of the forty-eighth annual ACM symposium on
  Theory of Computing}, pages 1046--1059. ACM, 2016.

\bibitem[Bauer et~al.(2007)Bauer, Pereverzev, and
  Rosasco]{bauer2007regularization}
Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco.
\newblock On regularization algorithms in learning theory.
\newblock \emph{Journal of complexity}, 23\penalty0 (1):\penalty0 52--72, 2007.

\bibitem[Belkin et~al.(2004)Belkin, Matveeva, and
  Niyogi]{belkin2004regularization}
Mikhail Belkin, Irina Matveeva, and Partha Niyogi.
\newblock Regularization and semi-supervised learning on large graphs.
\newblock In \emph{International Conference on Computational Learning Theory},
  pages 624--638. Springer, 2004.

\bibitem[Belkin et~al.(2018{\natexlab{a}})Belkin, Ma, and
  Mandal]{belkin2018understand}
Mikhail Belkin, Siyuan Ma, and Soumik Mandal.
\newblock To understand deep learning we need to understand kernel learning.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pages 541--549, 2018{\natexlab{a}}.

\bibitem[Belkin et~al.(2018{\natexlab{b}})Belkin, Rakhlin, and
  Tsybakov]{belkin2018does}
Mikhail Belkin, Alexander Rakhlin, and Alexandre~B. Tsybakov.
\newblock Does data interpolation contradict statistical optimality?
\newblock \emph{arXiv preprint arXiv:1806.09471}, 2018{\natexlab{b}}.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock \emph{J. Mach. Learn. Res.}, 2:\penalty0 499--526, March 2002.
\newblock ISSN 1532-4435.

\bibitem[Breiman(2001)]{breiman2001random}
Leo Breiman.
\newblock Random forests.
\newblock \emph{Machine learning}, 45\penalty0 (1):\penalty0 5--32, 2001.

\bibitem[Caponnetto and De~Vito(2007)]{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Foundations of Computational Mathematics}, 7\penalty0
  (3):\penalty0 331--368, 2007.

\bibitem[Chaudhuri and Dasgupta(2014)]{chaudhuri2014rates}
Kamalika Chaudhuri and Sanjoy Dasgupta.
\newblock Rates of convergence for nearest neighbor classification.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3437--3445, 2014.

\bibitem[Cover and Hart(1967)]{cover1967nearest}
Thomas Cover and Peter Hart.
\newblock Nearest neighbor pattern classification.
\newblock \emph{IEEE transactions on information theory}, 13\penalty0
  (1):\penalty0 21--27, 1967.

\bibitem[Cutler and Zhao(2001)]{cutler2001pert}
Adele Cutler and Guohua Zhao.
\newblock Pert-perfect random tree ensembles.
\newblock \emph{Computing Science and Statistics}, 33:\penalty0 490--497, 2001.

\bibitem[Davies(1997)]{davies1997multidimensional}
Scott Davies.
\newblock Multidimensional triangulation and interpolation for reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1005--1011, 1997.

\bibitem[Devroye et~al.(1998)Devroye, Gy{\"o}rfi, and
  Krzy{\.z}ak]{devroye1998hilbert}
Luc Devroye, Laszlo Gy{\"o}rfi, and Adam Krzy{\.z}ak.
\newblock The hilbert kernel regression estimate.
\newblock \emph{Journal of Multivariate Analysis}, 65\penalty0 (2):\penalty0
  209--227, 1998.

\bibitem[Fawzi et~al.(2016)Fawzi, Moosavi-Dezfooli, and
  Frossard]{fawzi2016robustness}
Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard.
\newblock Robustness of classifiers: from adversarial to random noise.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1632--1640, 2016.

\bibitem[Fukuda(2004)]{fukuda2004frequently}
Komei Fukuda.
\newblock Polyhedral computation {FAQ}.
\newblock Technical report, Swiss Federal Institute of Technology, Lausanne and
  Zurich, Switzerland, 2004.
\newblock URL
  \url{https://www.cs.mcgill.ca/~fukuda/download/paper/polyfaq.pdf}.

\bibitem[Golowich et~al.(2018)Golowich, Rakhlin, and Shamir]{golowich2017size}
Noah Golowich, Alexander Rakhlin, and Ohad Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock In \emph{Thirty-First Annual Conference on Learning Theory}, 2018.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{Goodfellow-et-al-2016}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock \emph{Deep Learning}.
\newblock MIT Press, 2016.

\bibitem[Gy{\"{o}}rfi et~al.(2002)Gy{\"{o}}rfi, Kohler, Krzyzak, and
  Walk]{gyorfi02}
L{\'{a}}szl{\'{o}} Gy{\"{o}}rfi, Michael Kohler, Adam Krzyzak, and Harro Walk.
\newblock \emph{A Distribution-Free Theory of Nonparametric Regression}.
\newblock Springer series in statistics. Springer, 2002.

\bibitem[Halton(1991)]{halton1991simplicial}
John~H Halton.
\newblock Simplicial multivariable linear interpolation.
\newblock Technical Report TR91-002, University of North Carolina at Chapel
  Hill, Department of Computer Science, 1991.

\bibitem[Koltchinskii and Panchenko(2002)]{koltchinskii2002empirical}
Vladimir Koltchinskii and Dmitry Panchenko.
\newblock Empirical margin distributions and bounding the generalization error
  of combined classifiers.
\newblock \emph{Annals of Statistics}, pages 1--50, 2002.

\bibitem[Liang et~al.(2017)Liang, Poggio, Rakhlin, and Stokes]{liang2017fisher}
Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes.
\newblock Fisher-rao metric, geometry, and complexity of neural networks.
\newblock \emph{arXiv preprint arXiv:1711.01530}, 2017.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Mammen and Tsybakov(1999)]{mammen1999smooth}
Enno Mammen and Alexandre~B Tsybakov.
\newblock Smooth discrimination analysis.
\newblock \emph{The Annals of Statistics}, 27\penalty0 (6):\penalty0
  1808--1829, 1999.

\bibitem[Massart and N{\'e}d{\'e}lec(2006)]{massart2006risk}
Pascal Massart and {\'E}lodie N{\'e}d{\'e}lec.
\newblock Risk bounds for statistical learning.
\newblock \emph{The Annals of Statistics}, 34\penalty0 (5):\penalty0
  2326--2366, 2006.

\bibitem[Nadaraya(1964)]{nadaraya1964estimating}
Elizbar~A Nadaraya.
\newblock On estimating regression.
\newblock \emph{Theory of Probability \& Its Applications}, 9\penalty0
  (1):\penalty0 141--142, 1964.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2017pac}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
\newblock A {PAC}-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Salakhutdinov(2017)]{russ17}
Ruslan Salakhutdinov.
\newblock Deep learning tutorial at the {S}imons {I}nstitute, {B}erkeley, 2017.
\newblock URL
  \url{https://simons.berkeley.edu/talks/ruslan-salakhutdinov-01-26-2017-1}.

\bibitem[Schapire and Freund(2012)]{schapire2012boosting}
Robert~E Schapire and Yoav Freund.
\newblock \emph{Boosting: Foundations and algorithms}.
\newblock MIT press, 2012.

\bibitem[Schapire et~al.(1998)Schapire, Freund, Bartlett, and
  Lee]{schapire1998}
Robert~E Schapire, Yoav Freund, Peter Bartlett, and Wee~Sun Lee.
\newblock Boosting the margin: a new explanation for the effectiveness of
  voting methods.
\newblock \emph{Ann. Statist.}, 26\penalty0 (5), 1998.

\bibitem[Schneider(2004)]{schneider200412}
Rolf Schneider.
\newblock Discrete aspects of stochastic geometry.
\newblock \emph{Handbook of discrete and computational geometry}, page 255,
  2004.

\bibitem[Scholkopf and Smola(2001)]{scholkopf2001learning}
Bernhard Scholkopf and Alexander~J Smola.
\newblock \emph{Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2001.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Shepard(1968)]{shepard1968two}
Donald Shepard.
\newblock A two-dimensional interpolation function for irregularly-spaced data.
\newblock In \emph{Proceedings of the 1968 23rd ACM national conference}, 1968.

\bibitem[Steinwart and Christmann(2008)]{steinwart2008support}
Ingo Steinwart and Andreas Christmann.
\newblock \emph{Support vector machines}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Su et~al.(2017)Su, Vargas, and Kouichi]{su2017one}
Jiawei Su, Danilo~Vasconcellos Vargas, and Sakurai Kouichi.
\newblock One pixel attack for fooling deep neural networks.
\newblock \emph{arXiv preprint arXiv:1710.08864}, 2017.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013adversarial}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Tsybakov(2004)]{tsybakov2004optimal}
Alexander~B Tsybakov.
\newblock Optimal aggregation of classifiers in statistical learning.
\newblock \emph{The Annals of Statistics}, 32\penalty0 (1):\penalty0 135--166,
  2004.

\bibitem[Tsybakov(2009)]{tsybakov2009introduction}
Alexandre~B Tsybakov.
\newblock \emph{Introduction to nonparametric estimation}.
\newblock Springer Series in Statistics. Springer, New York, 2009.

\bibitem[Wang et~al.(2018)Wang, Jha, and Chaudhuri]{wang2017analyzing}
Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri.
\newblock Analyzing the robustness of nearest neighbors to adversarial
  examples.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pages 5133--5142, 2018.

\bibitem[Wasserman(2004)]{wasserman2004all}
Larry Wasserman.
\newblock \emph{All of statistics}.
\newblock Springer, 2004.

\bibitem[Wasserman(2006)]{wasserman2006all}
Larry Wasserman.
\newblock \emph{All of nonparametric statistics}.
\newblock Springer, 2006.

\bibitem[Watson(1964)]{watson1964smooth}
Geoffrey~S Watson.
\newblock Smooth regression analysis.
\newblock \emph{Sankhy{\=a}: The Indian Journal of Statistics, Series A}, pages
  359--372, 1964.

\bibitem[Wyner et~al.(2017)Wyner, Olson, Bleich, and
  Mease]{wyner2017explaining}
Abraham~J Wyner, Matthew Olson, Justin Bleich, and David Mease.
\newblock Explaining the success of adaboost and random forests as
  interpolating classifiers.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (48):\penalty0 1--33, 2017.

\bibitem[Yao et~al.(2007)Yao, Rosasco, and Caponnetto]{yao2007early}
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.
\newblock On early stopping in gradient descent learning.
\newblock \emph{Constructive Approximation}, 26\penalty0 (2):\penalty0
  289--315, 2007.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Zhu et~al.(2003)Zhu, Ghahramani, and Lafferty]{zhu2003semi}
Xiaojin Zhu, Zoubin Ghahramani, and John~D Lafferty.
\newblock Semi-supervised learning using gaussian fields and harmonic
  functions.
\newblock In \emph{Proceedings of the 20th International conference on Machine
  learning (ICML-03)}, pages 912--919, 2003.

\end{thebibliography}
