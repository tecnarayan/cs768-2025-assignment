\begin{thebibliography}{74}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ma et~al.(2020)Ma, Peng, Zhang, Wei, and Huang]{ma-etal-2020-simplify}
Ruotian Ma, Minlong Peng, Qi~Zhang, Zhongyu Wei, and Xuanjing Huang.
\newblock Simplify the usage of lexicon in {C}hinese {NER}.
\newblock In \emph{Proceedings of the ACL 2020}, pages 5951--5960, Online, July 2020.
\newblock \doi{10.18653/v1/2020.acl-main.528}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.528}.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Fu, Zhang, and Xiao]{liu-etal-2021-lexicon}
Wei Liu, Xiyan Fu, Yue Zhang, and Wenming Xiao.
\newblock Lexicon enhanced {C}hinese sequence labeling using {BERT} adapter.
\newblock In \emph{Proceedings of the ACL 2021 and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 5847--5858, Online, August 2021{\natexlab{a}}.
\newblock \doi{10.18653/v1/2021.acl-long.454}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.454}.

\bibitem[Paolini et~al.(2020)Paolini, Athiwaratkun, Krone, Ma, Achille, ANUBHAI, dos Santos, Xiang, and Soatto]{paolini2020structured}
Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro Achille, RISHITA ANUBHAI, Cicero~Nogueira dos Santos, Bing Xiang, and Stefano Soatto.
\newblock Structured prediction as translation between augmented natural languages.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Das et~al.(2023)Das, Zhang, Shi, Yin, and Zhang]{das-etal-2023-unified}
Sarkar Snigdha~Sarathi Das, Haoran Zhang, Peng Shi, Wenpeng Yin, and Rui Zhang.
\newblock Unified low-resource sequence labeling by sample-aware dynamic sparse finetuning.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 6998--7010, Singapore, December 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.emnlp-main.433}.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.433}.

\bibitem[Lu et~al.(2022{\natexlab{a}})Lu, Liu, Dai, Xiao, Lin, Han, Sun, and Wu]{lu2022unified}
Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le~Sun, and Hua Wu.
\newblock Unified structure generation for universal information extraction.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 5755--5772, 2022{\natexlab{a}}.

\bibitem[Lu et~al.(2023{\natexlab{a}})Lu, Zhao, Mac~Namee, and Tan]{LuZhaoMacNameeTan2023}
Jinghui Lu, Rui Zhao, Brian Mac~Namee, and Fei Tan.
\newblock Punifiedner: A prompting-based unified ner system for diverse datasets.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 37\penalty0 (11):\penalty0 13327--13335, Jun. 2023{\natexlab{a}}.
\newblock \doi{10.1609/aaai.v37i11.26564}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/26564}.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Zhou, Zu, Xia, Chen, Zhang, Zheng, Ye, Zhang, Gui, et~al.]{wang2023instructuie}
Xiao Wang, Weikang Zhou, Can Zu, Han Xia, Tianze Chen, Yuansen Zhang, Rui Zheng, Junjie Ye, Qi~Zhang, Tao Gui, et~al.
\newblock Instructuie: Multi-task instruction tuning for unified information extraction.
\newblock \emph{arXiv preprint arXiv:2304.08085}, 2023{\natexlab{a}}.

\bibitem[Raffel et~al.(2020{\natexlab{a}})Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{2020t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (140):\penalty0 1--67, 2020{\natexlab{a}}.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Muennighoff et~al.(2022)Muennighoff, Wang, Sutawika, Roberts, Biderman, Scao, Bari, Shen, Yong, Schoelkopf, et~al.]{muennighoff2022crosslingual}
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven~Le Scao, M~Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et~al.
\newblock Crosslingual generalization through multitask finetuning.
\newblock \emph{arXiv preprint arXiv:2211.01786}, 2022.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, et~al.]{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al.
\newblock Qwen technical report.
\newblock \emph{arXiv preprint arXiv:2309.16609}, 2023.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Xiao, Wang, Zhang, Bian, Yin, Lv, Pan, Wang, Yan, et~al.]{yang2023baichuan}
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce~Bian, Chao Yin, Chenxu Lv, Da~Pan, Dian Wang, Dong Yan, et~al.
\newblock Baichuan 2: Open large-scale language models.
\newblock \emph{arXiv preprint arXiv:2309.10305}, 2023{\natexlab{a}}.

\bibitem[Lu et~al.(2024)Lu, Yu, Wang, Ye, Tang, Yang, Wu, Liu, Feng, Wang, et~al.]{lu2024bounding}
Jinghui Lu, Haiyang Yu, Yanjie Wang, Yongjie Ye, Jingqun Tang, Ziwei Yang, Binghong Wu, Qi~Liu, Hao Feng, Han Wang, et~al.
\newblock A bounding box is worth one token: Interleaving layout and text in a large language model for document understanding.
\newblock \emph{arXiv preprint arXiv:2407.01976}, 2024.

\bibitem[Lu et~al.(2023{\natexlab{b}})Lu, Zhu, Han, Zhao, Mac~Namee, and Tan]{lu-etal-2023-makes}
Jinghui Lu, Dongsheng Zhu, Weidong Han, Rui Zhao, Brian Mac~Namee, and Fei Tan.
\newblock What makes pre-trained language models better zero-shot learners?
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 2288--2303, Toronto, Canada, July 2023{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.128}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.128}.

\bibitem[Lu et~al.(2022{\natexlab{b}})Lu, Yang, Namee, and Zhang]{lu-etal-2022-rationale}
Jinghui Lu, Linyi Yang, Brian Namee, and Yue Zhang.
\newblock A rationale-centric framework for human-in-the-loop machine learning.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 6986--6996, Dublin, Ireland, May 2022{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.481}.
\newblock URL \url{https://aclanthology.org/2022.acl-long.481}.

\bibitem[Feng et~al.(2023)Feng, Wang, Tang, Lu, Zhou, Li, and Huang]{feng2023unidoc}
Hao Feng, Zijian Wang, Jingqun Tang, Jinghui Lu, Wengang Zhou, Houqiang Li, and Can Huang.
\newblock Unidoc: A universal large multimodal model for simultaneous text detection, recognition, spotting and understanding.
\newblock \emph{arXiv preprint arXiv:2308.11592}, 2023.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Sun, Li, Ouyang, Wu, Zhang, Li, and Wang]{wang2023gpt}
Shuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang, Fei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang.
\newblock Gpt-ner: Named entity recognition via large language models.
\newblock \emph{arXiv preprint arXiv:2304.10428}, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Ye, Wu, Zhuo, Ceze, and Krishnamurthy]{chen2023punica}
Lequn Chen, Zihao Ye, Yongji Wu, Danyang Zhuo, Luis Ceze, and Arvind Krishnamurthy.
\newblock Punica: Multi-tenant lora serving.
\newblock \emph{arXiv preprint arXiv:2310.18547}, 2023{\natexlab{a}}.

\bibitem[Ning et~al.(2023)Ning, Lin, Zhou, Yang, and Wang]{ning2023skeleton}
Xuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, and Yu~Wang.
\newblock Skeleton-of-thought: Large language models can do parallel decoding.
\newblock \emph{arXiv preprint arXiv:2307.15337}, 2023.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and Zettlemoyer]{dettmers2022gptint}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock {GPT}3.int8(): 8-bit matrix multiplication for transformers at scale.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=dXiGWqBoxaD}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Borgeaud, Irving, Lespiau, Sifre, and Jumper]{chen2023accelerating}
Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper.
\newblock Accelerating large language model decoding with speculative sampling.
\newblock \emph{arXiv preprint arXiv:2302.01318}, 2023{\natexlab{b}}.

\bibitem[Leviathan et~al.(2023)Leviathan, Kalman, and Matias]{leviathan2023fast}
Yaniv Leviathan, Matan Kalman, and Yossi Matias.
\newblock Fast inference from transformers via speculative decoding.
\newblock In \emph{International Conference on Machine Learning}, pages 19274--19286. PMLR, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 3045--3059, 2021.

\bibitem[Raffel et~al.(2020{\natexlab{b}})Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 5485--5551, 2020{\natexlab{b}}.

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Xu et~al.(2023)Xu, Chen, Peng, Zhang, Xu, Zhao, Wu, Zheng, and Chen]{xu2023large}
Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, and Enhong Chen.
\newblock Large language models for generative information extraction: A survey.
\newblock \emph{arXiv preprint arXiv:2312.17617}, 2023.

\bibitem[Xie et~al.(2023)Xie, Li, Zhang, Zhang, Liu, and Wang]{xie2023empirical}
Tingyu Xie, Qi~Li, Jian Zhang, Yan Zhang, Zuozhu Liu, and Hongwei Wang.
\newblock Empirical study of zero-shot ner with chatgpt.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 7935--7956, 2023.

\bibitem[Sainz et~al.(2023)Sainz, Garc{\'\i}a-Ferrero, Agerri, de~Lacalle, Rigau, and Agirre]{sainz2023gollie}
Oscar Sainz, Iker Garc{\'\i}a-Ferrero, Rodrigo Agerri, Oier~Lopez de~Lacalle, German Rigau, and Eneko Agirre.
\newblock Gollie: Annotation guidelines improve zero-shot information-extraction.
\newblock \emph{arXiv preprint arXiv:2310.03668}, 2023.

\bibitem[Ashok and Lipton(2023)]{ashok2023promptner}
Dhananjay Ashok and Zachary~C Lipton.
\newblock Promptner: Prompting for named entity recognition.
\newblock \emph{arXiv preprint arXiv:2305.15444}, 2023.

\bibitem[Chen et~al.(2023{\natexlab{c}})Chen, Lu, Lin, Lou, Jia, Dai, Wu, Cao, Han, and Sun]{chen-etal-2023-learning}
Jiawei Chen, Yaojie Lu, Hongyu Lin, Jie Lou, Wei Jia, Dai Dai, Hua Wu, Boxi Cao, Xianpei Han, and Le~Sun.
\newblock Learning in-context learning for named entity recognition.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 13661--13675, Toronto, Canada, July 2023{\natexlab{c}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.764}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.764}.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery, and Zhou]{wang2022self}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc~V Le, Ed~H Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022{\natexlab{a}}.

\bibitem[Friedman et~al.(2023)Friedman, Wettig, and Chen]{friedman2023learning}
Dan Friedman, Alexander Wettig, and Danqi Chen.
\newblock Learning transformer programs.
\newblock \emph{arXiv preprint arXiv:2306.01128}, 2023.

\bibitem[Wang et~al.(2021)Wang, Zhang, and Han]{wang2021spatten}
Hanrui Wang, Zhekai Zhang, and Song Han.
\newblock Spatten: Efficient sparse attention architecture with cascade token and head pruning.
\newblock In \emph{2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)}, pages 97--110. IEEE, 2021.

\bibitem[Frantar et~al.(2023)Frantar, Ashkboos, Hoefler, and Alistarh]{frantar2023optq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock {OPTQ}: Accurate quantization for generative pre-trained transformers.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=tcbBPnfwxS}.

\bibitem[Santilli et~al.(2023)Santilli, Severino, Postolache, Maiorca, Mancusi, Marin, and Rodola]{santilli-etal-2023-accelerating}
Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodola.
\newblock Accelerating transformer inference for translation via parallel decoding.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 12336--12355, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.689}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.689}.

\bibitem[Xiao et~al.(2023)Xiao, Lin, Seznec, Wu, Demouth, and Han]{xiao2023smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models.
\newblock In \emph{International Conference on Machine Learning}, pages 38087--38099. PMLR, 2023.

\bibitem[Elbayad et~al.(2019)Elbayad, Gu, Grave, and Auli]{elbayad2019depth}
Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli.
\newblock Depth-adaptive transformer.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Schuster et~al.(2022)Schuster, Fisch, Gupta, Dehghani, Bahri, Tran, Tay, and Metzler]{schuster2022confident}
Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi~Tay, and Donald Metzler.
\newblock Confident adaptive language modeling.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 17456--17472, 2022.

\bibitem[Lan et~al.(2023)Lan, Cai, Wang, Huang, and Mao]{lan2023copy}
Tian Lan, Deng Cai, Yan Wang, Heyan Huang, and Xian-Ling Mao.
\newblock Copy is all you need.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=CROlOA9Nd8C}.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Ge, Wang, Jiao, Jiang, Yang, Majumder, and Wei]{yang2023inference}
Nan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin Jiang, Linjun Yang, Rangan Majumder, and Furu Wei.
\newblock Inference with reference: Lossless acceleration of large language models.
\newblock \emph{arXiv preprint arXiv:2304.04487}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2023)Zhang, Tan, Zhang, and Zhu]{zhang2023nag}
Xinpeng Zhang, Ming Tan, Jingfan Zhang, and Wei Zhu.
\newblock Nag-ner: a unified non-autoregressive generation framework for various ner tasks.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)}, pages 676--686, 2023.

\bibitem[Ding et~al.(2024)Ding, Li, Wang, Tang, Yan, and Zhang]{ding2024rethinking}
Yuyang Ding, Juntao Li, Pinzheng Wang, Zecheng Tang, Bowen Yan, and Min Zhang.
\newblock Rethinking negative instances for generative named entity recognition.
\newblock \emph{arXiv preprint arXiv:2402.16602}, 2024.

\bibitem[Zhou et~al.(2023{\natexlab{a}})Zhou, Zhang, Gu, Chen, and Poon]{zhou2023universalner}
Wenxuan Zhou, Sheng Zhang, Yu~Gu, Muhao Chen, and Hoifung Poon.
\newblock Universalner: Targeted distillation from large language models for open named entity recognition.
\newblock \emph{ICLR 2024}, 2023{\natexlab{a}}.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Xu, Yu, Dai, Ji, Cahyawijaya, Madotto, and Fung]{liu2021crossner}
Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei Ji, Samuel Cahyawijaya, Andrea Madotto, and Pascale Fung.
\newblock Crossner: Evaluating cross-domain named entity recognition.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pages 13452--13460, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2013)Liu, Pasupat, Cyphers, and Glass]{liu2013asgard}
Jingjing Liu, Panupong Pasupat, Scott Cyphers, and Jim Glass.
\newblock Asgard: A portable architecture for multilingual dialogue systems.
\newblock In \emph{2013 IEEE International Conference on Acoustics, Speech and Signal Processing}, pages 8386--8390. IEEE, 2013.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Liu, Chen, Hong, Tang, and Song]{wang-etal-2022-deepstruct}
Chenguang Wang, Xiao Liu, Zui Chen, Haoyun Hong, Jie Tang, and Dawn Song.
\newblock {D}eep{S}truct: Pretraining of language models for structure prediction.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, \emph{Findings of the Association for Computational Linguistics: ACL 2022}, pages 803--823, Dublin, Ireland, May 2022{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.findings-acl.67}.
\newblock URL \url{https://aclanthology.org/2022.findings-acl.67}.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Cheng, Gao, and Poon]{zhang2022optimizing}
Sheng Zhang, Hao Cheng, Jianfeng Gao, and Hoifung Poon.
\newblock Optimizing bi-encoder for named entity recognition via contrastive learning.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022{\natexlab{a}}.

\bibitem[Tjong Kim~Sang and De~Meulder(2003)]{Tjong_Kim_Sang_De_Meulder_2003}
Erik~F. Tjong Kim~Sang and Fien De~Meulder.
\newblock Introduction to the conll-2003 shared task.
\newblock In \emph{Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003 -}, Jan 2003.
\newblock \doi{10.3115/1119176.1119195}.
\newblock URL \url{http://dx.doi.org/10.3115/1119176.1119195}.

\bibitem[Kirkpatrick(2010)]{kirkpatrick2010researching}
Andy Kirkpatrick.
\newblock Researching english as a lingua franca in asia: The asian corpus of english (ace) project.
\newblock \emph{Asian Englishes}, 13\penalty0 (1):\penalty0 4--18, 2010.

\bibitem[Ohta et~al.(2002)Ohta, Tateisi, Kim, Mima, and Tsujii]{ohta2002genia}
Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim, Hideki Mima, and Junichi Tsujii.
\newblock The genia corpus: An annotated research abstract corpus in molecular biology domain.
\newblock In \emph{Proceedings of the human language technology conference}, pages 73--77. Citeseer, 2002.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Jiang, Wang, Hu, Sun, Xie, and Zhang]{zhang2022domain}
Xin Zhang, Yong Jiang, Xiaobin Wang, Xuming Hu, Yueheng Sun, Pengjun Xie, and Meishan Zhang.
\newblock Domain-specific ner via retrieving correlated samples.
\newblock In \emph{Proceedings of the 29th International Conference on Computational Linguistics}, pages 2398--2404, 2022{\natexlab{b}}.

\bibitem[Zhang and Lu(2023)]{zhang2023local}
Miao Zhang and Ling Lu.
\newblock A local information perception enhancement--based method for chinese ner.
\newblock \emph{Applied Sciences}, 13\penalty0 (17):\penalty0 9948, 2023.

\bibitem[Zhang and Yang(2018)]{zhang-yang-2018-chinese}
Yue Zhang and Jie Yang.
\newblock {C}hinese {NER} using lattice {LSTM}.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1554--1564, Melbourne, Australia, July 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P18-1144}.
\newblock URL \url{https://aclanthology.org/P18-1144}.

\bibitem[Peng and Dredze(2015)]{Peng_Dredze_2015}
Nanyun Peng and Mark Dredze.
\newblock Named entity recognition for chinese social media with jointly trained embeddings.
\newblock In \emph{Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}, Jan 2015.
\newblock \doi{10.18653/v1/d15-1064}.
\newblock URL \url{http://dx.doi.org/10.18653/v1/d15-1064}.

\bibitem[Levow(2006)]{levow-2006-third}
Gina-Anne Levow.
\newblock The third international {C}hinese language processing bakeoff: Word segmentation and named entity recognition.
\newblock In \emph{Proceedings of the Fifth {SIGHAN} Workshop on {C}hinese Language Processing}, pages 108--117, Sydney, Australia, July 2006. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/W06-0115}.

\bibitem[Pradhan et~al.(2013)Pradhan, Moschitti, Xue, Ng, Bj{\"o}rkelund, Uryupina, Zhang, and Zhong]{pradhan-etal-2013-towards}
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee~Tou Ng, Anders Bj{\"o}rkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong.
\newblock Towards robust linguistic analysis using {O}nto{N}otes.
\newblock In \emph{Proceedings of the Seventeenth Conference on Computational Natural Language Learning}, pages 143--152, Sofia, Bulgaria, August 2013. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/W13-3516}.

\bibitem[Jie et~al.(2019)Jie, Xie, Lu, Ding, and Li]{jie2019better}
Zhanming Jie, Pengjun Xie, Wei Lu, Ruixue Ding, and Linlin Li.
\newblock Better modeling of incomplete annotations for named entity recognition.
\newblock In \emph{Proceedings of NAACL}, 2019.

\bibitem[Ding et~al.(2019)Ding, Xie, Zhang, Lu, Li, and Si]{ding2019neural}
Ruixue Ding, Pengjun Xie, Xiaoyan Zhang, Wei Lu, Linlin Li, and Luo Si.
\newblock A neural multi-digraph model for chinese ner with gazetteers.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 1462--1467, 2019.

\bibitem[Zaratiana et~al.(2023)Zaratiana, Tomeh, Holat, and Charnois]{zaratiana2023gliner}
Urchade Zaratiana, Nadi Tomeh, Pierre Holat, and Thierry Charnois.
\newblock Gliner: Generalist model for named entity recognition using bidirectional transformer.
\newblock \emph{arXiv preprint arXiv:2311.08526}, 2023.

\bibitem[Li et~al.(2022)Li, Fei, Liu, Wu, Zhang, Teng, Ji, and Li]{LiFeiLiuWuZhangTengJiLi2022}
Jingye Li, Hao Fei, Jiang Liu, Shengqiong Wu, Meishan Zhang, Chong Teng, Donghong Ji, and Fei Li.
\newblock Unified named entity recognition as word-word relation classification.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 36\penalty0 (10):\penalty0 10965--10973, Jun. 2022.
\newblock \doi{10.1609/aaai.v36i10.21344}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/21344}.

\bibitem[Cai et~al.(2024)Cai, Li, Geng, Peng, Lee, Chen, and Dao]{cai2024medusa}
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason~D Lee, Deming Chen, and Tri Dao.
\newblock Medusa: Simple llm inference acceleration framework with multiple decoding heads.
\newblock \emph{arXiv preprint arXiv:2401.10774}, 2024.

\bibitem[Peng et~al.(2023)Peng, Alcaide, Anthony, Albalak, Arcadinho, Cao, Cheng, Chung, Grella, GV, et~al.]{peng2023rwkv}
Bo~Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi~Kiran GV, et~al.
\newblock Rwkv: Reinventing rnns for the transformer era.
\newblock \emph{arXiv preprint arXiv:2305.13048}, 2023.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Li, Wu, and Lee]{liu2023llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock In \emph{NeurIPS}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Li, Li, and Lee]{liu2023improvedllava}
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong~Jae Lee.
\newblock Improved baselines with visual instruction tuning, 2023{\natexlab{b}}.

\bibitem[Huang et~al.(2019)Huang, Chen, He, Bai, Karatzas, Lu, and Jawahar]{huang2019icdar2019}
Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV~Jawahar.
\newblock Icdar2019 competition on scanned receipt ocr and information extraction.
\newblock In \emph{2019 International Conference on Document Analysis and Recognition (ICDAR)}, pages 1516--1520. IEEE, 2019.

\bibitem[Liu and Low(2023)]{liu2023goat}
Tiedong Liu and Bryan Kian~Hsiang Low.
\newblock Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks.
\newblock \emph{arXiv preprint arXiv:2305.14201}, 2023.

\bibitem[Loshchilov and Hutter(2018)]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Skq89Scxx}.

\bibitem[Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer]{min2022rethinking}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning work?
\newblock In \emph{Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 11048--11064, 2022.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Min, Deng, Shen, Wu, Zettlemoyer, and Sun]{wang-etal-2023-towards}
Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun.
\newblock Towards understanding chain-of-thought prompting: An empirical study of what matters.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 2717--2739, Toronto, Canada, July 2023{\natexlab{c}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.153}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.153}.

\bibitem[Zhou et~al.(2023{\natexlab{b}})Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, YU, Zhang, Ghosh, Lewis, Zettlemoyer, and Levy]{zhou2023lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.
\newblock {LIMA}: Less is more for alignment.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=KBMOKmX2he}.

\end{thebibliography}
