\begin{thebibliography}{10}

\bibitem{ablin_2020}
P.~Ablin, G.~Peyr{\'e}, and T.~Moreau.
\newblock Super-efficiency of automatic differentiation for functions defined
  as a minimum.
\newblock In {\em Proc. of ICML}, pages 32--41. PMLR, 2020.

\bibitem{acharyya_2013}
S.~Acharyya.
\newblock Learning to rank in supervised and unsupervised settings using
  convexity and monotonicity.
\newblock 2013.

\bibitem{amari_2016}
S.~Amari.
\newblock {\em
  \href{https://www.springer.com/us/book/9784431559771}{Information Geometry
  and Its Applications}}.
\newblock Springer, 2016.

\bibitem{icnn_icml}
B.~Amos, L.~Xu, and J.~Z. Kolter.
\newblock Input convex neural networks.
\newblock In {\em ICML}, pages 146--155. PMLR, 2017.

\bibitem{aubin_2022}
P.-C. Aubin-Frankowski and S.~Gaubert.
\newblock The tropical analogues of reproducing kernels.
\newblock {\em arXiv preprint arXiv:2202.11410}, 2022.

\bibitem{bakir_2007}
G.~BakIr, T.~Hofmann, B.~Sch{\"o}lkopf, A.~J. Smola, and B.~Taskar.
\newblock {\em Predicting structured data}.
\newblock MIT press, 2007.

\bibitem{bregman_clustering}
A.~Banerjee, S.~Merugu, I.~S. Dhillon, J.~Ghosh, and J.~Lafferty.
\newblock Clustering with bregman divergences.
\newblock {\em Journal of machine learning research}, 6(10), 2005.

\bibitem{exponential_families}
O.~Barndorff-Nielsen.
\newblock {\em
  \href{https://onlinelibrary.wiley.com/doi/book/10.1002/9781118857281}{Information
  and Exponential Families: In Statistical Theory}}.
\newblock John Wiley \& Sons, 1978.

\bibitem{bartlett_2006}
P.~L. Bartlett, M.~I. Jordan, and J.~D. McAuliffe.
\newblock Convexity, classification, and risk bounds.
\newblock {\em Journal of the American Statistical Association},
  101(473):138--156, 2006.

\bibitem{beck_2017}
A.~Beck.
\newblock {\em First-order methods in optimization}.
\newblock SIAM, 2017.

\bibitem{belanger_2016}
D.~Belanger and A.~McCallum.
\newblock Structured prediction energy networks.
\newblock In {\em International Conference on Machine Learning}, pages
  983--992. PMLR, 2016.

\bibitem{belanger_2013}
D.~Belanger, D.~Sheldon, and A.~McCallum.
\newblock
  \href{http://www.cmap.polytechnique.fr/~jaggi/NeurIPS-workshop-FW-greedy/papers/belanger_sheldon_mccallum_final.pdf}{Marginal
  inference in {MRF}s using {F}rank-{W}olfe}.
\newblock In {\em NeurIPS Workshop on Greedy Opt., FW and Friends}, 2013.

\bibitem{belanger_2017}
D.~Belanger, B.~Yang, and A.~McCallum.
\newblock End-to-end learning for structured prediction energy networks.
\newblock In {\em International Conference on Machine Learning}, pages
  429--439. PMLR, 2017.

\bibitem{bell_2008}
B.~M. Bell and J.~V. Burke.
\newblock Algorithmic differentiation of implicit functions and optimal values.
\newblock In {\em Advances in Automatic Differentiation}, pages 67--77.
  Springer, 2008.

\bibitem{bertsekas_1997}
D.~P. Bertsekas.
\newblock Nonlinear programming.
\newblock {\em Journal of the Operational Research Society}, 48(3):334--334,
  1997.

\bibitem{projection_oracles}
M.~Blondel.
\newblock Structured prediction with projection oracles.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{blondel_implicit_diff}
M.~Blondel, Q.~Berthet, M.~Cuturi, R.~Frostig, S.~Hoyer, F.~Llinares-L{\'o}pez,
  F.~Pedregosa, and J.-P. Vert.
\newblock Efficient and modular implicit differentiation.
\newblock {\em arXiv preprint arXiv:2105.15183}, 2021.

\bibitem{fylosses_jmlr}
M.~Blondel, A.~F. Martins, and V.~Niculae.
\newblock Learning with fenchel-young losses.
\newblock {\em JMLR}, 21(35):1--69, 2020.

\bibitem{bonnans_2013}
J.~F. Bonnans and A.~Shapiro.
\newblock {\em Perturbation analysis of optimization problems}.
\newblock Springer Science \& Business Media, 2013.

\bibitem{borwein_2006}
J.~Borwein and A.~Lewis.
\newblock {\em Convex Analysis}.
\newblock 2006.

\bibitem{bottou_2018}
L.~Bottou, M.~Arjovsky, D.~Lopez-Paz, and M.~Oquab.
\newblock Geometrical insights for implicit generative modeling.
\newblock pages 229--268. 2018.

\bibitem{boyd_book}
S.~Boyd, S.~P. Boyd, and L.~Vandenberghe.
\newblock {\em Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem{openaigym}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock Openai gym, 2016.

\bibitem{calafiore_2019}
G.~C. Calafiore, S.~Gaubert, and C.~Possieri.
\newblock Log-sum-exp neural networks and posynomial models for convex and
  log-log-convex data.
\newblock {\em IEEE transactions on neural networks and learning systems},
  31(3):827--838, 2019.

\bibitem{chancelier_2018}
J.-P. Chancelier and M.~De~Lara.
\newblock Fenchel-moreau conjugation inequalities with three couplings and
  application to stochastic bellman equation.
\newblock {\em arXiv preprint arXiv:1804.03034}, 2018.

\bibitem{ciliberto_2016}
C.~Ciliberto, L.~Rosasco, and A.~Rudi.
\newblock A consistent regularization approach for structured prediction.
\newblock In {\em Proc. of NeurIPS}, pages 4412--4420, 2016.

\bibitem{structured_perceptron}
M.~Collins.
\newblock \href{https://dl.acm.org/citation.cfm?id=1118694}{Discriminative
  training methods for Hidden Markov Models: Theory and experiments with
  perceptron algorithms}.
\newblock In {\em Proc. of EMNLP}, 2002.

\bibitem{dadashi2020primal}
R.~Dadashi, L.~Hussenot, M.~Geist, and O.~Pietquin.
\newblock Primal wasserstein imitation learning.
\newblock {\em International Conference on Learning Representations}, 2021.

\bibitem{danskin_1967}
J.~M. Danskin.
\newblock {\em The theory of max-min and its application to weapons allocation
  problems}.
\newblock Springer Science \& Business Media, 1967.

\bibitem{duchi_2016}
J.~C. Duchi, K.~Khosravi, and F.~Ruan.
\newblock \href{http://arxiv.org/abs/1603.00126}{Multiclass classification,
  information, divergence, and surrogate risk}.
\newblock {\em The Annals of Statistics}, 46(6B):3246--3275, 2018.

\bibitem{ghadimi_2018}
S.~Ghadimi and M.~Wang.
\newblock Approximation methods for bilevel programming.
\newblock {\em arXiv preprint arXiv:1802.02246}, 2018.

\bibitem{globerson_2015}
A.~Globerson, T.~Roughgarden, D.~Sontag, and C.~Yildirim.
\newblock How hard is inference for structured prediction?
\newblock In {\em International Conference on Machine Learning}, pages
  2181--2190. PMLR, 2015.

\bibitem{glorot_2011}
X.~Glorot, A.~Bordes, and Y.~Bengio.
\newblock Deep sparse rectifier neural networks.
\newblock In {\em AISTATS}, pages 315--323. JMLR Workshop and Conference
  Proceedings, 2011.

\bibitem{maxout}
I.~Goodfellow, D.~Warde-Farley, M.~Mirza, A.~Courville, and Y.~Bengio.
\newblock Maxout networks.
\newblock In {\em ICML}, pages 1319--1327. PMLR, 2013.

\bibitem{no_mcmc}
W.~Grathwohl, J.~Kelly, M.~Hashemi, M.~Norouzi, K.~Swersky, and D.~Duvenaud.
\newblock No mcmc for me: Amortized samplers for fast and stable training of
  energy-based models.
\newblock 2021.

\bibitem{griewank_2008}
A.~Griewank and A.~Walther.
\newblock {\em Evaluating derivatives: principles and techniques of algorithmic
  differentiation}.
\newblock SIAM, 2008.

\bibitem{hiriart_1993}
J.-B. Hiriart-Urruty and C.~Lemar{\'e}chal.
\newblock {\em Convex analysis and minimization algorithms II}, volume 305.
\newblock Springer science \& business media, 1993.

\bibitem{ho2016generative}
J.~Ho and S.~Ermon.
\newblock Generative adversarial imitation learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2016.

\bibitem{kakade_2009}
S.~Kakade, S.~Shalev-Shwartz, A.~Tewari, et~al.
\newblock On the duality of strong convexity and strong smoothness: Learning
  applications and matrix regularization.
\newblock {\em Tech report}, 2(1):35, 2009.

\bibitem{kostrikov2018discriminator}
I.~Kostrikov, K.~K. Agrawal, D.~Dwibedi, S.~Levine, and J.~Tompson.
\newblock Discriminator-actor-critic: Addressing sample inefficiency and reward
  bias in adversarial imitation learning.
\newblock {\em International Conference on Learning Representations}, 2019.

\bibitem{krantz_2012}
S.~G. Krantz and H.~R. Parks.
\newblock {\em The implicit function theorem: history, theory, and
  applications}.
\newblock Springer Science \& Business Media, 2012.

\bibitem{barrierfw}
R.~G. Krishnan, S.~Lacoste-Julien, and D.~Sontag.
\newblock \href{https://arxiv.org/abs/1511.02124}{Barrier Frank-Wolfe for
  marginal inference}.
\newblock In {\em Proc. of NeurIPS}, 2015.

\bibitem{Lafferty2001}
J.~D. Lafferty, A.~McCallum, and F.~C. Pereira.
\newblock \href{http://dl.acm.org/citation.cfm?id=645530.655813}{Conditional
  Random Fields: Probabilistic models for segmenting and labeling sequence
  data}.
\newblock In {\em Proc. of ICML}, 2001.

\bibitem{lecun_2006}
Y.~LeCun, S.~Chopra, R.~Hadsell, M.~Ranzato, and F.~Huang.
\newblock A tutorial on energy-based learning.
\newblock {\em Predicting structured data}, 1(0), 2006.

\bibitem{lecun_2005}
Y.~LeCun and F.~J. Huang.
\newblock Loss functions for discriminative training of energy-based models.
\newblock In {\em International Workshop on Artificial Intelligence and
  Statistics}, pages 206--213. PMLR, 2005.

\bibitem{lin_2020}
T.~Lin, C.~Jin, and M.~Jordan.
\newblock On gradient descent ascent for nonconvex-concave minimax problems.
\newblock In {\em International Conference on Machine Learning}, pages
  6083--6093. PMLR, 2020.

\bibitem{sparsemax}
A.~F. Martins and R.~F. Astudillo.
\newblock \href{https://arxiv.org/abs/1602.02068} {From softmax to sparsemax: A
  sparse model of attention and multi-label classification}.
\newblock In {\em Proc. of ICML}, 2016.

\bibitem{mccullagh_1989}
P.~McCullagh and J.~A. Nelder.
\newblock {\em
  \href{https://www.crcpress.com/Generalized-Linear-Models/McCullagh-Nelder/p/book/9780412317606}{Generalized
  Linear Models}}, volume~37.
\newblock CRC press, 1989.

\bibitem{milgrom_2002}
P.~Milgrom and I.~Segal.
\newblock Envelope theorems for arbitrary choice sets.
\newblock {\em Econometrica}, 70(2):583--601, 2002.

\bibitem{moreau_1966}
J.-J. Moreau.
\newblock Fonctionnelles convexes.
\newblock {\em S{\'e}minaire Jean Leray}, (2):1--108, 1966.

\bibitem{nash_2019}
C.~Nash and C.~Durkan.
\newblock Autoregressive energy machines.
\newblock In {\em International Conference on Machine Learning}, pages
  1735--1744. PMLR, 2019.

\bibitem{glm}
J.~A. Nelder and R.~J. Baker.
\newblock {\em
  \href{https://onlinelibrary.wiley.com/doi/full/10.1002/0471667196.ess0866.pub2}{Generalized
  Linear Models}}.
\newblock Wiley Online Library, 1972.

\bibitem{nesterov_2003}
Y.~Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem{ng2000algorithms}
A.~Y. Ng, S.~J. Russell, et~al.
\newblock Algorithms for inverse reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, 2000.

\bibitem{sparsemap}
V.~Niculae, A.~F. Martins, M.~Blondel, and C.~Cardie.
\newblock \href{https://arxiv.org/abs/1802.04223}{SparseMAP: Differentiable
  sparse structured inference}.
\newblock In {\em Proc. of ICML}, 2018.

\bibitem{nowak_2019}
A.~Nowak-Vila, F.~Bach, and A.~Rudi.
\newblock A general theory for structured prediction with smooth convex
  surrogates.
\newblock {\em arXiv preprint arXiv:1902.01958}, 2019.

\bibitem{orabona_2019}
F.~Orabona.
\newblock A modern introduction to online learning.
\newblock {\em arXiv preprint arXiv:1912.13213}, 2019.

\bibitem{orsini2021matters}
M.~Orsini, A.~Raichuk, L.~Hussenot, D.~Vincent, R.~Dadashi, S.~Girgin,
  M.~Geist, O.~Bachem, O.~Pietquin, and M.~Andrychowicz.
\newblock What matters for adversarial imitation learning?
\newblock {\em Advances in Neural Information Processing Systems}, 2021.

\bibitem{osokin_2017}
A.~Osokin, F.~Bach, and S.~Lacoste-Julien.
\newblock On structured prediction theory with calibrated convex surrogate
  losses.
\newblock In {\em Proc. of NIPS}, pages 302--313, 2017.

\bibitem{peyre_2019}
G.~Peyr{\'e}, M.~Cuturi, et~al.
\newblock Computational optimal transport: With applications to data science.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  11(5-6):355--607, 2019.

\bibitem{pomerleau1991efficient}
D.~A. Pomerleau.
\newblock Efficient training of artificial neural networks for autonomous
  navigation.
\newblock {\em Neural computation}, 1991.

\bibitem{rockafellar_2009}
R.~T. Rockafellar and R.~J.-B. Wets.
\newblock {\em Variational analysis}, volume 317.
\newblock Springer Science \& Business Media, 2009.

\bibitem{rubinov_2000}
A.~M. Rubinov.
\newblock {\em Abstract convexity and global optimization}.
\newblock Springer Science \& Business Media, 2000.

\bibitem{russell1998learning}
S.~Russell.
\newblock Learning agents for uncertain environments.
\newblock In {\em Conference on Computational learning theory}, 1998.

\bibitem{santambrogio_2015}
F.~Santambrogio.
\newblock Optimal transport for applied mathematicians.
\newblock {\em Birk{\"a}user, NY}, 55(58-63):94, 2015.

\bibitem{singer_1997}
I.~Singer.
\newblock {\em Abstract convex analysis}, volume~25.
\newblock John Wiley \& Sons, 1997.

\bibitem{song_2021}
Y.~Song and D.~P. Kingma.
\newblock How to train your energy-based models.
\newblock {\em arXiv preprint arXiv:2101.03288}, 2021.

\bibitem{steinwart_2007}
I.~Steinwart.
\newblock How to compare different loss functions and their risks.
\newblock {\em Constructive Approximation}, 26(2):225--287, 2007.

\bibitem{sutton_introduction_2011}
C.~Sutton, A.~McCallum, et~al.
\newblock An introduction to conditional random fields.
\newblock {\em Foundations and Trends in Machine Learning}, 4(4):267--373,
  2012.

\bibitem{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock 2018.

\bibitem{wainwright_2008}
M.~J. Wainwright, M.~I. Jordan, et~al.
\newblock Graphical models, exponential families, and variational inference.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  1(1--2):1--305, 2008.

\bibitem{yuille_2003}
A.~L. Yuille and A.~Rangarajan.
\newblock The concave-convex procedure.
\newblock {\em Neural computation}, 15(4):915--936, 2003.

\bibitem{zhang_2004}
T.~Zhang.
\newblock Statistical analysis of some multi-category large margin
  classification methods.
\newblock {\em Journal of Machine Learning Research}, 5(Oct):1225--1251, 2004.

\bibitem{zhang_2004_2}
T.~Zhang.
\newblock Statistical behavior and consistency of classification methods based
  on convex risk minimization.
\newblock {\em The Annals of Statistics}, 32(1):56--85, 2004.

\bibitem{zhou_2018}
X.~Zhou.
\newblock On the fenchel duality between strong convexity and lipschitz
  continuous gradient.
\newblock {\em arXiv preprint arXiv:1803.06573}, 2018.

\end{thebibliography}
