\begin{thebibliography}{10}

\bibitem{vapnik2015uniform}
Vladimir~N Vapnik and A~Ya Chervonenkis.
\newblock On the uniform convergence of relative frequencies of events to their
  probabilities.
\newblock In {\em Measures of complexity}, pages 11--30. Springer, 2015.

\bibitem{bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock {\em Journal of Machine Learning Research}, 3(Nov):463--482, 2002.

\bibitem{talagrand2006generic}
Michel Talagrand.
\newblock {\em The generic chaining: upper and lower bounds of stochastic
  processes}.
\newblock Springer Science \& Business Media, 2006.

\bibitem{mohri2018foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock {\em Foundations of machine learning}.
\newblock MIT press, 2018.

\bibitem{burer2003nonlinear}
Samuel Burer and Renato~DC Monteiro.
\newblock A nonlinear programming algorithm for solving semidefinite programs
  via low-rank factorization.
\newblock {\em Mathematical Programming}, 95(2):329--357, 2003.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{abadi2016tensorflow}
Mart{\'\i}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et~al.
\newblock Tensorflow: a system for large-scale machine learning.
\newblock In {\em OSDI}, volume~16, pages 265--283, 2016.

\bibitem{soudry2017implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock {\em arXiv preprint arXiv:1710.10345}, 2017.

\bibitem{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6151--6159, 2017.

\bibitem{neyshabur2017geometry}
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro.
\newblock Geometry of optimization and implicit regularization in deep
  learning.
\newblock {\em arXiv preprint arXiv:1705.03071}, 2017.

\bibitem{azizan2018stochastic}
Navid Azizan and Babak Hassibi.
\newblock Stochastic gradient/mirror descent: Minimax optimality and implicit
  regularization.
\newblock {\em arXiv preprint arXiv:1806.00952}, 2018.

\bibitem{nacson2018stochastic}
Mor~Shpigel Nacson, Nathan Srebro, and Daniel Soudry.
\newblock Stochastic gradient descent on separable data: Exact convergence with
  a fixed learning rate.
\newblock {\em arXiv preprint arXiv:1806.01796}, 2018.

\bibitem{wilson2017marginal}
Ashia~C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin
  Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4148--4158, 2017.

\bibitem{neyshabur2014search}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock {\em arXiv preprint arXiv:1412.6614}, 2014.

\bibitem{li2018algorithmic}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In {\em Conference On Learning Theory}, pages 2--47, 2018.

\bibitem{bhojanapalli2016global}
Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
\newblock Global optimality of local search for low rank matrix recovery.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3873--3881, 2016.

\bibitem{boumal2016non}
Nicolas Boumal, Vlad Voroninski, and Afonso Bandeira.
\newblock The non-convex burer-monteiro approach works on smooth semidefinite
  programs.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2757--2765, 2016.

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530}, 2016.

\bibitem{soltanolkotabi2018theoretical}
Mahdi Soltanolkotabi, Adel Javanmard, and Jason~D Lee.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock {\em IEEE Transactions on Information Theory}, 2018.

\bibitem{brutzkus2017sgd}
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz.
\newblock Sgd learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock {\em arXiv preprint arXiv:1710.10174}, 2017.

\bibitem{chizat2018global}
Lenaic Chizat and Francis Bach.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock {\em arXiv preprint arXiv:1805.09545}, 2018.

\bibitem{arora2018optimization}
Sanjeev Arora, Nadav Cohen, and Elad Hazan.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock {\em arXiv preprint arXiv:1802.06509}, 2018.

\bibitem{Ji:2018aa}
Ziwei Ji and Matus Telgarsky.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock 10 2018.

\bibitem{venturi2018spurious}
L.~Venturi, A.~Bandeira, and J.~Bruna.
\newblock Spurious valleys in two-layer neural network optimization landscapes.
\newblock {\em arXiv preprint arXiv:1802.06384}, 2018.

\bibitem{Zhu:2018aa}
Zhihui Zhu, Daniel Soudry, Yonina~C. Eldar, and Michael~B. Wakin.
\newblock The global optimization geometry of shallow linear neural networks.
\newblock 05 2018.

\bibitem{Soudry:2016aa}
Daniel Soudry and Yair Carmon.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock 05 2016.

\bibitem{Brutzkus:2018aa}
Alon Brutzkus and Amir Globerson.
\newblock Over-parameterization improves generalization in the xor detection
  problem.
\newblock 10 2018.

\bibitem{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock {\em NeurIPS}, 2018.

\bibitem{allen2018learning}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock {\em arXiv preprint arXiv:1811.04918}, 2018.

\bibitem{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock {\em arXiv preprint arXiv:1811.03962}, 2018.

\bibitem{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock {\em arXiv preprint arXiv:1810.02054}, 2018.

\bibitem{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock {\em arXiv preprint arXiv:1811.08888}, 2018.

\bibitem{du2018gradient2}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock {\em arXiv preprint arXiv:1811.03804}, 2018.

\bibitem{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em arXiv preprint arXiv:1609.04836}, 2016.

\bibitem{sagun2017empirical}
Levent Sagun, Utku Evci, V~Ugur Guney, Yann Dauphin, and Leon Bottou.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock {\em arXiv preprint arXiv:1706.04454}, 2017.

\bibitem{chaudhari2016entropy}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
  Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock {\em arXiv preprint arXiv:1611.01838}, 2016.

\bibitem{Arora:2018aa}
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock 02 2018.

\bibitem{Bartlett:2017aa}
Peter Bartlett, Dylan~J. Foster, and Matus Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock 06 2017.

\bibitem{Golowich:2017aa}
Noah Golowich, Alexander Rakhlin, and Ohad Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock 12 2017.

\bibitem{song2018mean}
Mei Song, A~Montanari, and P~Nguyen.
\newblock A mean field view of the landscape of two-layers neural networks.
\newblock In {\em Proceedings of the National Academy of Sciences}, volume 115,
  pages E7665--E7671, 2018.

\bibitem{oymak2018learning}
Samet Oymak.
\newblock Learning compact neural networks with regularization.
\newblock {\em International Conference on Machine Learning}, 2018.

\bibitem{Brutzkus:2017aa}
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz.
\newblock Sgd learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock 10 2017.

\bibitem{Belkin:2018aa}
Mikhail Belkin, Daniel Hsu, and Partha Mitra.
\newblock Overfitting or perfect fitting? risk bounds for classification and
  regression rules that interpolate.
\newblock 06 2018.

\bibitem{Liang:2018aa}
Tengyuan Liang and Alexander Rakhlin.
\newblock Just interpolate: Kernel "ridgeless" regression can generalize.
\newblock 08 2018.

\bibitem{Belkin:2018ab}
Mikhail Belkin, Alexander Rakhlin, and Alexandre~B. Tsybakov.
\newblock Does data interpolation contradict statistical optimality?
\newblock 06 2018.

\bibitem{rakhlin2012making}
Alexander Rakhlin, Ohad Shamir, Karthik Sridharan, et~al.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In {\em ICML}, volume~12, pages 1571--1578. Citeseer, 2012.

\bibitem{de2015taming}
Christopher~M De~Sa, Ce~Zhang, Kunle Olukotun, and Christopher R{\'e}.
\newblock Taming the wild: A unified analysis of hogwild-style algorithms.
\newblock In {\em Advances in neural information processing systems}, pages
  2674--2682, 2015.

\bibitem{tan2017phase}
Yan~Shuo Tan and Roman Vershynin.
\newblock Phase retrieval via randomized kaczmarz: theoretical guarantees.
\newblock {\em Information and Inference: A Journal of the IMA}, 2017.

\bibitem{lojasiewicz1963topological}
S~Lojasiewicz.
\newblock A topological property of real analytic subsets.
\newblock {\em Coll. du CNRS, Les {\'e}quations aux d{\'e}riv{\'e}es
  partielles}, 117:87--89, 1963.

\bibitem{polyak1963gradient}
Boris~Teodorovich Polyak.
\newblock Gradient methods for minimizing functionals.
\newblock {\em Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  3(4):643--653, 1963.

\bibitem{candes2015phase}
Emmanuel~J Candes, Xiaodong Li, and Mahdi Soltanolkotabi.
\newblock Phase retrieval via wirtinger flow: Theory and algorithms.
\newblock {\em IEEE Transactions on Information Theory}, 61(4):1985--2007,
  2015.

\bibitem{li2018rapid}
Xiaodong Li, Shuyang Ling, Thomas Strohmer, and Ke~Wei.
\newblock Rapid, robust, and reliable blind deconvolution via nonconvex
  optimization.
\newblock {\em Applied and Computational Harmonic Analysis}, 2018.

\bibitem{Soltanolkotabi:2017aa}
Mahdi Soltanolkotabi.
\newblock Structured signal recovery from quadratic measurements: Breaking
  sample complexity barriers via nonconvex optimization.
\newblock 02 2017.

\bibitem{ma2017power}
Siyuan Ma, Raef Bassily, and Mikhail Belkin.
\newblock The power of interpolation: Understanding the effectiveness of sgd in
  modern over-parametrized learning.
\newblock {\em arXiv preprint arXiv:1712.06559}, 2017.

\bibitem{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In {\em Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 795--811. Springer, 2016.

\bibitem{bassily2018exponential}
Raef Bassily, Mikhail Belkin, and Siyuan Ma.
\newblock On exponential convergence of sgd in non-convex over-parametrized
  learning.
\newblock {\em arXiv preprint arXiv:1811.02564}, 2018.

\bibitem{vaswani2018fast}
Sharan Vaswani, Francis Bach, and Mark Schmidt.
\newblock Fast and faster convergence of sgd for over-parameterized models and
  an accelerated perceptron.
\newblock {\em arXiv preprint arXiv:1810.07288}, 2018.

\bibitem{lei2017non}
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael~I Jordan.
\newblock Non-convex finite-sum optimization via scsg methods.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2348--2358, 2017.

\bibitem{sun2018geometric}
Ju~Sun, Qing Qu, and John Wright.
\newblock A geometric analysis of phase retrieval.
\newblock {\em Foundations of Computational Mathematics}, 18(5):1131--1198,
  2018.

\bibitem{tu2015low}
Stephen Tu, Ross Boczar, Max Simchowitz, Mahdi Soltanolkotabi, and Benjamin
  Recht.
\newblock Low-rank solutions of linear matrix equations via procrustes flow.
\newblock {\em arXiv preprint arXiv:1507.03566}, 2015.

\bibitem{chen2018gradient}
Y.~Chen, Y.~Chi, J.~Fan, and C.~Ma.
\newblock Gradient descent with random initialization: Fast global convergence
  for nonconvex phase retrieval.
\newblock {\em arXiv preprint arXiv:1803.07726}, 2018.

\bibitem{Josz:2018aa}
Cedric Josz, Yi~Ouyang, Richard~Y. Zhang, Javad Lavaei, and Somayeh Sojoudi.
\newblock A theory on the absence of spurious solutions for nonconvex and
  nonsmooth optimization.
\newblock 05 2018.

\bibitem{zhong2017recovery}
Kai Zhong, Zhao Song, Prateek Jain, Peter~L Bartlett, and Inderjit~S Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock {\em arXiv preprint arXiv:1706.03175}, 2017.

\bibitem{ge2017learning}
Rong Ge, Jason~D Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock {\em arXiv preprint arXiv:1711.00501}, 2017.

\bibitem{soltanolkotabi2017learning}
Mahdi Soltanolkotabi.
\newblock Learning {ReLUs} via gradient descent.
\newblock {\em arXiv preprint arXiv:1705.04591}, 2017.

\bibitem{oymak2018stochastic}
Samet Oymak.
\newblock Stochastic gradient descent learns state equations with nonlinear
  activations.
\newblock {\em arXiv preprint arXiv:1809.03019}, 2018.

\bibitem{brutzkus2017globally}
A.~Alon~Brutzkus and Amir Globerson.
\newblock Globally optimal gradient descent for a convnet with {G}aussian
  inputs.
\newblock {\em arXiv preprint arXiv:1702.07966}, 2017.

\bibitem{revuz2013continuous}
Daniel Revuz and Marc Yor.
\newblock {\em Continuous martingales and Brownian motion}, volume 293.
\newblock Springer Science \& Business Media, 2013.

\end{thebibliography}
