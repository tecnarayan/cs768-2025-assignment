\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Advani \& Saxe(2017)Advani and Saxe]{advani2017high}
Advani, M.~S. and Saxe, A.~M.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock \emph{arXiv preprint arXiv:1710.03667}, 2017.

\bibitem[Bai \& Silverstein(2010)Bai and Silverstein]{bai2010spectral}
Bai, Z. and Silverstein, J.~W.
\newblock \emph{Spectral analysis of large dimensional random matrices},
  volume~20.
\newblock Springer, 2010.

\bibitem[Bai \& Silverstein(1998)Bai and Silverstein]{bai1998no}
Bai, Z.-D. and Silverstein, J.~W.
\newblock No eigenvalues outside the support of the limiting spectral
  distribution of large-dimensional sample covariance matrices.
\newblock \emph{Annals of probability}, pp.\  316--345, 1998.

\bibitem[Bai \& Silverstein(2008)Bai and Silverstein]{bai2008clt}
Bai, Z.~D. and Silverstein, J.~W.
\newblock {CLT} for linear spectral statistics of large-dimensional sample
  covariance matrices.
\newblock In \emph{Advances In Statistics}, pp.\  281--333. World Scientific,
  2008.

\bibitem[Baik et~al.(2005)Baik, Arous, P{\'e}ch{\'e}, et~al.]{baik2005phase}
Baik, J., Arous, G.~B., P{\'e}ch{\'e}, S., et~al.
\newblock Phase transition of the largest eigenvalue for nonnull complex sample
  covariance matrices.
\newblock \emph{The Annals of Probability}, 33\penalty0 (5):\penalty0
  1643--1697, 2005.

\bibitem[Bartlett \& Mendelson(2002)Bartlett and
  Mendelson]{bartlett2002rademacher}
Bartlett, P.~L. and Mendelson, S.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Benaych-Georges \& Couillet(2016)Benaych-Georges and
  Couillet]{benaych2016spectral}
Benaych-Georges, F. and Couillet, R.
\newblock Spectral analysis of the gram matrix of mixture models.
\newblock \emph{ESAIM: Probability and Statistics}, 20:\penalty0 217--237,
  2016.

\bibitem[Benaych-Georges \& Nadakuditi(2011)Benaych-Georges and
  Nadakuditi]{benaych2011eigenvalues}
Benaych-Georges, F. and Nadakuditi, R.~R.
\newblock The eigenvalues and eigenvectors of finite, low rank perturbations of
  large random matrices.
\newblock \emph{Advances in Mathematics}, 227\penalty0 (1):\penalty0 494--521,
  2011.

\bibitem[Billingsley(2008)]{billingsley2008probability}
Billingsley, P.
\newblock \emph{Probability and measure}.
\newblock John Wiley \& Sons, 2008.

\bibitem[Bishop(2007)]{bishop2007pattern}
Bishop, C.~M.
\newblock \emph{Pattern Recognition and Machine Learning}.
\newblock Springer, 2007.

\bibitem[Bottou(2010)]{bottou2010large}
Bottou, L.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In \emph{Proceedings of COMPSTAT'2010}, pp.\  177--186. Springer,
  2010.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{boyd2004convex}
Boyd, S. and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Choromanska et~al.(2015)Choromanska, Henaff, Mathieu, Arous, and
  LeCun]{choromanska2015loss}
Choromanska, A., Henaff, M., Mathieu, M., Arous, G.~B., and LeCun, Y.
\newblock The loss surfaces of multilayer networks.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  192--204,
  2015.

\bibitem[Couillet \& Debbah(2011)Couillet and Debbah]{couillet2011random}
Couillet, R. and Debbah, M.
\newblock \emph{Random matrix methods for wireless communications}.
\newblock Cambridge University Press, 2011.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  249--256, 2010.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deeplearning}
Goodfellow, I., Bengio, Y., and Courville, A.
\newblock \emph{Deep Learning}.
\newblock MIT Press, 2016.
\newblock \url{http://www.deeplearningbook.org}.

\bibitem[Gradshteyn \& Ryzhik(2014)Gradshteyn and Ryzhik]{gradshteyn2014table}
Gradshteyn, I.~S. and Ryzhik, I.~M.
\newblock \emph{Table of integrals, series, and products}.
\newblock Academic press, 2014.

\bibitem[Hachem et~al.(2007)Hachem, Loubaton, Najim,
  et~al.]{hachem2007deterministic}
Hachem, W., Loubaton, P., Najim, J., et~al.
\newblock Deterministic equivalents for certain functionals of large random
  matrices.
\newblock \emph{The Annals of Applied Probability}, 17\penalty0 (3):\penalty0
  875--930, 2007.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pp.\
  448--456, 2015.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[LeCun et~al.(1998)LeCun, Cortes, and Burges]{lecun1998mnist}
LeCun, Y., Cortes, C., and Burges, C.~J.
\newblock {The MNIST database of handwritten digits}, 1998.

\bibitem[Mar{\v{c}}enko \& Pastur(1967)Mar{\v{c}}enko and
  Pastur]{marvcenko1967distribution}
Mar{\v{c}}enko, V.~A. and Pastur, L.~A.
\newblock Distribution of eigenvalues for some sets of random matrices.
\newblock \emph{Mathematics of the USSR-Sbornik}, 1\penalty0 (4):\penalty0 457,
  1967.

\bibitem[Poggio et~al.(2004)Poggio, Rifkin, Mukherjee, and
  Niyogi]{poggio2004general}
Poggio, T., Rifkin, R., Mukherjee, S., and Niyogi, P.
\newblock General conditions for predictivity in learning theory.
\newblock \emph{Nature}, 428\penalty0 (6981):\penalty0 419, 2004.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{saxe2013exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6120}, 2013.

\bibitem[Schmidhuber(2015)]{schmidhuber2015deep}
Schmidhuber, J.
\newblock Deep learning in neural networks: An overview.
\newblock \emph{Neural networks}, 61:\penalty0 85--117, 2015.

\bibitem[Silverstein \& Choi(1995)Silverstein and
  Choi]{silverstein1995analysis}
Silverstein, J.~W. and Choi, S.-I.
\newblock Analysis of the limiting spectral distribution of large dimensional
  random matrices.
\newblock \emph{Journal of Multivariate Analysis}, 54\penalty0 (2):\penalty0
  295--309, 1995.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Vapnik(2013)]{vapnik2013nature}
Vapnik, V.
\newblock \emph{The nature of statistical learning theory}.
\newblock Springer science \& business media, 2013.

\bibitem[Yao et~al.(2007)Yao, Rosasco, and Caponnetto]{yao2007early}
Yao, Y., Rosasco, L., and Caponnetto, A.
\newblock On early stopping in gradient descent learning.
\newblock \emph{Constructive Approximation}, 26\penalty0 (2):\penalty0
  289--315, 2007.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\end{thebibliography}
