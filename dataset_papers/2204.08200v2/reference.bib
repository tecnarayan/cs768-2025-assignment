
@inproceedings{raghu2019rapid,
title={Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML},
author={Aniruddh Raghu and Maithra Raghu and Samy Bengio and Oriol Vinyals},
booktitle={International Conference on Learning Representations},
year={2020},
}
@misc{ruder2017overview,
      title={An Overview of Multi-Task Learning in Deep Neural Networks}, 
      author={Sebastian Ruder},
      year={2017},
      eprint={1706.05098},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{finn2017model,
  title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={ICML},
  year={2017}
}

@inproceedings{ji2020directional,
  author={Ziwei Ji and Matus Telgarsky},
  title={Directional convergence and alignment in deep learning},
  year={2020},
  booktitle={NeurIPS},
}

@article{tripuraneni2020theory,
  title={On the Theory of Transfer Learning: The Importance of Task Diversity},
  author={Tripuraneni, Nilesh and Jordan, Michael I and Jin, Chi},
  journal={arXiv preprint arXiv:2006.11650},
  year={2020}
}


@misc{learn2learn2019,
      title={learn2learn: A Library for Meta-Learning Research}, 
      author={Sébastien M. R. Arnold and Praateek Mahajan and Debajyoti Datta and Ian Bunner and Konstantinos Saitas Zarkias},
      year={2020},
      eprint={2008.12284},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{meta-ntk,
      title={Global Convergence and Generalization Bound of Gradient-Based Meta-Learning with Deep Neural Nets}, 
      author={Haoxiang Wang and Ruoyu Sun and Bo Li},
      year={2020},
      eprint={2006.14606},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{ji2020gradient,
  title={Gradient descent follows the regularization path for general losses},
  author={Ji, Ziwei and Dud{\'\i}k, Miroslav and Schapire, Robert E and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={2109--2136},
  year={2020}
}
@inproceedings{NEURIPS2018_66808e32,
 author = {Oreshkin, Boris and Rodr\'{\i}guez L\'{o}pez, Pau and Lacoste, Alexandre},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {721--731},
 publisher = {Curran Associates, Inc.},
 title = {TADAM: Task dependent adaptive metric for improved few-shot learning},
 url = {https://proceedings.neurips.cc/paper/2018/file/66808e327dc79d135ba18e051673d906-Paper.pdf},
 volume = {31},
 year = {2018}
}



@inproceedings{maml,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1126--1135},
  year={2017},
  organization={JMLR. org}
}

@article{reptile,
  title={On first-order meta-learning algorithms},
  author={Nichol, Alex and Achiam, Joshua and Schulman, John},
  journal={arXiv preprint arXiv:1803.02999},
  year={2018}
}
@inproceedings{adam,
  author={Diederik P. Kingma and Jimmy Ba},
  title={Adam: A Method for Stochastic Optimization},
  year={2015},
  booktitle={ICLR},
}
@inproceedings{batchnorm,
  title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International Conference on Machine Learning},
  pages={448--456},
  year={2015}
}
@book{interior,
  title={Interior-point polynomial algorithms in convex programming},
  author={Nesterov, Yurii and Nemirovskii, Arkadii},
  volume={13},
  year={1994},
  publisher={Siam}
}
@incollection{learningtolearn,
  title={Learning to learn: Introduction and overview},
  author={Thrun, Sebastian and Pratt, Lorien},
  booktitle={Learning to learn},
  pages={3--17},
  year={1998},
  publisher={Springer}
}

@article{metalearning,
  title={A perspective view and survey of meta-learning},
  author={Vilalta, Ricardo and Drissi, Youssef},
  journal={Artificial intelligence review},
  volume={18},
  number={2},
  pages={77--95},
  year={2002},
  publisher={Springer}
}
@article{omniglot,
  title={Human-level concept learning through probabilistic program induction},
  author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  journal={Science},
  volume={350},
  number={6266},
  pages={1332--1338},
  year={2015},
  publisher={American Association for the Advancement of Science}
}

@article{mnist,
  author = {LeCun, Yann and Cortes, Corinna},
  title = {{MNIST} handwritten digit database},
  url = {http://yann.lecun.com/exdb/mnist/},
  username = {mhwombat},
  year = 1998
}
@article{lstm,
  title={Deep learning in neural networks: An overview},
  author={Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={61},
  pages={85--117},
  year={2015},
  publisher={Elsevier}
}

@inproceedings{rnn1,
  title={Matching networks for one shot learning},
  author={Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Wierstra, Daan and others},
  booktitle={Advances in neural information processing systems},
  pages={3630--3638},
  year={2016}
}

@article{snail,
  title={A Simple Neural Attentive Meta-Learner},
  author={Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
  year={2018}
}

@inproceedings{siamese,
  title={Siamese neural networks for one-shot image recognition},
  author={Koch, Gregory},
  year={2015}
}

@inproceedings{hochreiter2001learning,
  title={Learning to learn using gradient descent},
  author={Hochreiter, Sepp and Younger, A Steven and Conwell, Peter R},
  booktitle={International Conference on Artificial Neural Networks},
  pages={87--94},
  year={2001},
  organization={Springer}
}

@article{li2016learning,
  title={Learning to optimize},
  author={Li, Ke and Malik, Jitendra},
  journal={arXiv preprint arXiv:1606.01885},
  year={2016}
}

@article{li2017learning,
  title={Learning to optimize neural nets},
  author={Li, Ke and Malik, Jitendra},
  journal={arXiv preprint arXiv:1703.00441},
  year={2017}
}

@article{duan2016rl,
  title={\text{RL}$^{2}$: Fast Reinforcement Learning via Slow Reinforcement Learning},
  author={Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1611.02779},
  year={2016}
}

@article{rusu2018meta,
  title={Meta-learning with latent embedding optimization},
  author={Rusu, Andrei A and Rao, Dushyant and Sygnowski, Jakub and Vinyals, Oriol and Pascanu, Razvan and Osindero, Simon and Hadsell, Raia},
  journal={arXiv preprint arXiv:1807.05960},
  year={2018}
}

@inproceedings{finn2018probabilistic,
  title={Probabilistic model-agnostic meta-learning},
  author={Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9516--9527},
  year={2018}
}

@inproceedings{kimbayesian,
  title={Bayesian model-agnostic meta-learning},
  author={Yoon, Jaesik and Kim, Taesup and Dia, Ousmane and Kim, Sungwoong and Bengio, Yoshua and Ahn, Sungjin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={7332--7342},
  year={2018}
}
@article{Hsu2019UnsupervisedLV,
  title={Unsupervised Learning via Meta-Learning},
  author={Kyle Hsu and Sergey Levine and Chelsea Finn},
  journal={CoRR},
  year={2019},
  volume={abs/1810.02334}
}

@misc{zgner2019adversarial,
    title={Adversarial Attacks on Graph Neural Networks via Meta Learning},
    author={Daniel Zügner and Stephan Günnemann},
    year={2019},
    eprint={1902.08412},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{finn2019online,
  title={Online Meta-Learning},
  author={Finn, Chelsea and Rajeswaran, Aravind and Kakade, Sham and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1920--1930},
  year={2019}
}

@misc{nomrl,
    title={NoRML: No-Reward Meta Learning},
    author={Yuxiang Yang and Ken Caluwaerts and Atil Iscen and Jie Tan and Chelsea Finn},
    year={2019},
    eprint={1903.01063},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@inproceedings{alexnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{densenet,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4700--4708},
  year={2017}
}
@incollection{lossvisual,
title = {Visualizing the Loss Landscape of Neural Nets},
author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {6389--6399},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf}
}


@inproceedings{spurious_minima,
  title={Spurious Local Minima are Common in Two-Layer ReLU Neural Networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={4430--4438},
  year={2018}
}

@inproceedings{Xie2018FewShotGI,
  title={Few-Shot Goal Inference for Visuomotor Learning and Planning},
  author={Annie Xie and Avi Singh and Sergey Levine and Chelsea Finn},
  booktitle={CoRL},
  year={2018}
}

@article{AllenZhu2018ACT,
  title={A Convergence Theory for Deep Learning via Over-Parameterization},
  author={Zeyuan Allen-Zhu and Yuanzhi Li and Zhao Song},
  journal={International Conference on Machine Learning},
  series={ICML'19},
  year={2019},

}

@inproceedings{AllenZhu2019Generalization,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in neural information processing systems},
  pages={6155--6166},
  year={2019}
}
@article{du2019icml,
    title={Gradient Descent Finds Global Minima of Deep Neural Networks},
    author={Simon S. Du and Jason D. Lee and Haochuan Li and Liwei Wang and Xiyu Zhai},
  journal={International Conference on Machine Learning},
  year={2019},

}

@inproceedings{
ji2018gradient,
title={Gradient descent aligns the layers of deep linear networks},
author={Ziwei Ji and Matus Telgarsky},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HJflg30qKX},
}


@incollection{baxter1998theoretical,
  title={Theoretical models of learning to learn},
  author={Baxter, Jonathan},
  booktitle={Learning to learn},
  pages={71--94},
  year={1998},
  publisher={Springer}
}



@InProceedings{lqr,
  title = 	 {Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator},
  author = 	 {Fazel, Maryam and Ge, Rong and Kakade, Sham and Mesbahi, Mehran},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1467--1476},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Stockholmsmässan, Stockholm Sweden},
  month = 	 {10--15 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/fazel18a/fazel18a.pdf},
  url = 	 {http://proceedings.mlr.press/v80/fazel18a.html},
  abstract = 	 {Direct policy gradient methods for reinforcement learning and continuous control problems are a popular approach for a variety of reasons: 1) they are easy to implement without explicit knowledge of the underlying model, 2) they are an “end-to-end” approach, directly optimizing the performance metric of interest, 3) they inherently allow for richly parameterized policies. A notable drawback is that even in the most basic continuous control problem (that of linear quadratic regulators), these methods must solve a non-convex optimization problem, where little is understood about their efficiency from both computational and statistical perspectives. In contrast, system identification and model based planning in optimal control theory have a much more solid theoretical footing, where much is known with regards to their computational and statistical properties. This work bridges this gap showing that (model free) policy gradient methods globally converge to the optimal solution and are efficient (polynomially so in relevant problem dependent quantities) with regards to their sample and computational complexities.}
}



 @article{anderson1990optimal,
  title={Optimal control: linear quadratic methods},
  author={Anderson, Brian and Moore, John B},
  year={1990},
  publisher={Prentice-Hall, Inc.}
}


@inproceedings{maml_nonconvex,
  title={On the convergence theory of gradient-based model-agnostic meta-learning algorithms},
  author={Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1082--1092},
  year={2020}
}


@inproceedings{
karras2018progressive,
title={Progressive Growing of {GAN}s for Improved Quality, Stability, and Variation},
author={Tero Karras and Timo Aila and Samuli Laine and Jaakko Lehtinen},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Hk99zCeAb},
}






@inproceedings{provable-gbml,
  title={Provable guarantees for gradient-based meta-learning},
  author={Balcan, Maria-Florina and Khodak, Mikhail and Talwalkar, Ameet},
  booktitle={International Conference on Machine Learning},
  pages={424--433},
  year={2019}
}

@article{van2017l2,
  title={L2 regularization versus batch and weight normalization},
  author={van Laarhoven, Twan},
  journal={arXiv preprint arXiv:1706.05350},
  year={2017}
}

@inproceedings{parkoptimal,
  title={The Effect of Network Width on Stochastic Gradient Descent and Generalization: an Empirical Study},
  author={Daniel S. Park and Jascha Sohl-Dickstein and Quoc V. Le and Samuel L. Smith},
  booktitle={ICML},
  year={2019}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{vinyals2016matching,
  title={Matching networks for one shot learning},
  author={Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Wierstra, Daan and others},
  booktitle={Advances in neural information processing systems},
  pages={3630--3638},
  year={2016}
}

@inproceedings{snell2017prototypical,
  title={Prototypical networks for few-shot learning},
  author={Snell, Jake and Swersky, Kevin and Zemel, Richard},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4077--4087},
  year={2017}
}



@inproceedings{khodak2019adaptive,
  title={Adaptive Gradient-Based Meta-Learning Methods},
  author={Khodak, Mikhail and Florina-Balcan, Maria and Talwalkar, Ameet},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@misc{alphaMAML,
    title={Alpha MAML: Adaptive Model-Agnostic Meta-Learning},
    author={Harkirat Singh Behl and Atılım Güneş Baydin and Philip H. S. Torr},
    year={2019},
    eprint={1905.07435},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}


@inproceedings{
hessian-ntk,
title={The asymptotic spectrum of the Hessian of DNN throughout training},
author={Arthur Jacot and Franck Gabriel and Clement Hongler},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkgscaNYPS}
}

@article{graphNTK,
  title={Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels},
  author={Simon S. Du and Kangcheng Hou and Barnab{\'a}s P{\'o}czos and Ruslan Salakhutdinov and Ruosong Wang and Keyulu Xu},
  journal={NeurIPS},
  year={2019},
}


@inproceedings{
harnessNTK,
title={Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks},
author={Sanjeev Arora and Simon S. Du and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang and Dingli Yu},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkl8sJBYvH}
}

@inproceedings{ntk,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}


@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={NeurIPS},
  year={2019}
}

@article{cntk,
  title={On Exact Computation with an Infinitely Wide Neural Net},
  author={Sanjeev Arora and Simon S. Du and Wei Hu and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang},
  journal={NeurIPS},
  year={2019},
}


@article{ECNTK,
  title={Enhanced convolutional neural tangent kernels},
  author={Li, Zhiyuan and Wang, Ruosong and Yu, Dingli and Du, Simon S and Hu, Wei and Salakhutdinov, Ruslan and Arora, Sanjeev},
  journal={arXiv preprint arXiv:1911.00809},
  year={2019}
}

@inproceedings{large_lr,
  title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11669--11680},
  year={2019}
}

@incollection{lazy,
title = {Limitations of Lazy Training of Two-layers Neural Network},
author = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {9108--9118},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/9111-limitations-of-lazy-training-of-two-layers-neural-network.pdf}
}

@article{NSA,
  title={Neural Spectrum Alignment},
  author={Kopitkov, Dmitry and Indelman, Vadim},
  journal={arXiv preprint arXiv:1910.08720},
  year={2019}
}

@article{GAP,
  title={Network in network},
  author={Lin, Min and Chen, Qiang and Yan, Shuicheng},
  journal={arXiv preprint arXiv:1312.4400},
  year={2013}
}
@inproceedings{
nagarajan2021understanding,
title={Understanding the failure modes of out-of-distribution generalization},
author={Vaishnavh Nagarajan and Anders Andreassen and Behnam Neyshabur},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=fSTD6NFIW_b}
}
@article{kernel_ridge,
  title={Kernel ridge regression},
  author={Welling, Max}
}

@inproceedings{
wei2021theoretical,
title={Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data},
author={Colin Wei and Kendrick Shen and Yining Chen and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=rC8sJ4i6kaH}
}
@inproceedings{verma2019manifold,
  title={Manifold mixup: Better representations by interpolating hidden states},
  author={Verma, Vikas and Lamb, Alex and Beckham, Christopher and Najafi, Amir and Mitliagkas, Ioannis and Lopez-Paz, David and Bengio, Yoshua},
  booktitle={International Conference on Machine Learning},
  pages={6438--6447},
  year={2019},
  organization={PMLR}
}
@article{wang2019implicit,
  title={Implicit semantic data augmentation for deep networks},
  author={Wang, Yulin and Pan, Xuran and Song, Shiji and Zhang, Hong and Huang, Gao and Wu, Cheng},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={12635--12644},
  year={2019}
}
@inproceedings{
sinha2018certifiable,
title={Certifiable Distributional Robustness with Principled Adversarial Training},
author={Aman Sinha and Hongseok Namkoong and John Duchi},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=Hk6kPgZA-},
}

@inproceedings{kumar2020understanding,
  title={Understanding self-training for gradual domain adaptation},
  author={Kumar, Ananya and Ma, Tengyu and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={5468--5479},
  year={2020},
  organization={PMLR}
}

@misc{peyre2020computational,
      title={Computational Optimal Transport}, 
      author={Gabriel Peyré and Marco Cuturi},
      year={2020},
      eprint={1803.00567},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@inproceedings{fine-grained,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019}
}

@inproceedings{imaml,
  title={Meta-learning with implicit gradients},
  author={Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham M and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={113--124},
  year={2019}
}
@article{few-shot-survey,
  title={Generalizing from a few examples: A survey on few-shot learning},
  author={Wang, Yaqing and Yao, Quanming and Kwok, James T and Ni, Lionel M},
  year={2019},
  journal={ACM Computing Surveys (CSUR)},
  publisher={ACM New York, NY, USA}
}

@article{vanschoren2018meta,
  title={Meta-learning: A survey},
  author={Vanschoren, Joaquin},
  journal={arXiv preprint arXiv:1810.03548},
  year={2018}
}

@article{nichol2018first,
  title={On first-order meta-learning algorithms},
  author={Nichol, Alex and Achiam, Joshua and Schulman, John},
  journal={arXiv preprint arXiv:1803.02999},
  year={2018}
}

@article{evgeniou2007multi,
  title={Multi-task feature learning},
  author={Evgeniou, An and Pontil, Massimiliano},
  journal={Advances in neural information processing systems},
  volume={19},
  pages={41},
  year={2007}
}

@inproceedings{zhang2010convex,
  title={A Convex Formulation for Learning Task Relationships in Multi-task Learning},
  author={Zhang, Yu and Yeung, Dit Yan},
  booktitle={Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, UAI 2010},
  pages={733},
  year={2010}
}

@inproceedings{zhao2020efficient,
  title={Efficient multitask feature and relationship learning},
  author={Zhao, Han and Stretcu, Otilia and Smola, Alexander J and Gordon, Geoffrey J},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={777--787},
  year={2020},
  organization={PMLR}
}

@article{argyriou2008convex,
  title={Convex multi-task feature learning},
  author={Argyriou, Andreas and Evgeniou, Theodoros and Pontil, Massimiliano},
  journal={Machine learning},
  volume={73},
  number={3},
  pages={243--272},
  year={2008},
  publisher={Springer}
}

@phdthesis{Finn:EECS-2018-105,
    Author = {Finn, Chelsea},
    Title = {Learning to Learn with Gradients},
    School = {EECS Department, University of California, Berkeley},
    Year = {2018},
    Month = {Aug},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-105.html},
    Number = {UCB/EECS-2018-105},
    Abstract = {Humans have a remarkable ability to learn new concepts from only a few examples and quickly adapt to unforeseen circumstances. To do so, they build upon their prior experience and prepare for the ability to adapt, allowing the combination of previous observations with small amounts of new evidence for fast learning. In most machine learning systems, however, there are distinct train and test phases: training consists of updating the model using data, and at test time, the model is deployed as a rigid decision-making engine. In this thesis, we discuss gradient-based algorithms for learning to learn, or meta-learning, which aim to endow machines with flexibility akin to that of humans. Instead of deploying a fixed, non-adaptable system, these meta-learning techniques explicitly train for the ability to quickly adapt so that, at test time, they can learn quickly when faced with new scenarios.

To study the problem of learning to learn, we first develop a clear and formal definition of the meta-learning problem, its terminology, and desirable properties of meta-learning algorithms. Building upon these foundations, we present a class of model-agnostic meta-learning methods that embed gradient-based optimization into the learner. Unlike prior approaches to learning to learn, this class of methods focus on acquiring a transferable representation rather than a good learning rule. As a result, these methods inherit a number of desirable properties from using a fixed optimization as the learning rule, while still maintaining full expressivity, since the learned representations can control the update rule.

We show how these methods can be extended for applications in motor control by combining elements of meta-learning with techniques for deep model-based reinforcement learning, imitation learning, and inverse reinforcement learning. By doing so, we build simulated agents that can adapt in dynamic environments, enable real robots to learn to manipulate new objects by watching a video of a human, and allow humans to convey goals to robots with only a few images. Finally, we conclude by discussing open questions and future directions in meta-learning, aiming to identify the key shortcomings and limiting assumptions of our existing approaches.}
}

@inproceedings{prob-maml,
  title={Probabilistic model-agnostic meta-learning},
  author={Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9516--9527},
  year={2018}
}
@inproceedings{adaptive-GBML,
  title={Adaptive gradient-based meta-learning methods},
  author={Khodak, Mikhail and Balcan, Maria-Florina F and Talwalkar, Ameet S},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5915--5926},
  year={2019}
}

@misc{
search-activation,
title={Searching for Activation Functions},
author={Prajit Ramachandran, Barret Zoph, Quoc V. Le},
year={2018},
url={https://openreview.net/forum?id=SkBYYyZRZ},
}

@inproceedings{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10835--10845},
  year={2019}
}

@incollection{meta-curvature,
title = {Meta-Curvature},
author = {Park, Eunbyung and Oliva, Junier B},
booktitle = {Advances in Neural Information Processing Systems},
pages = {3309--3319},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8593-meta-curvature.pdf}
}

@inproceedings{
WrapGrad,
title={Meta-Learning with Warped Gradient Descent},
author={Sebastian Flennerhag and Andrei A. Rusu and Razvan Pascanu and Francesco Visin and Hujun Yin and Raia Hadsell},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkeiQlBFPB}
}

@article{zhou2019metalearning,
  title={Efficient Meta Learning via Minibatch Proximal Update},
  author={Pan Zhou and Xiaotong Yuan and Huan Xu and Shuicheng Yan},
  journal={Neural Information Processing Systems},
  year={2019}
}
@article{Lake_2019,
   title={The Omniglot challenge: a 3-year progress report},
   volume={29},
   ISSN={2352-1546},
   url={http://dx.doi.org/10.1016/j.cobeha.2019.04.007},
   DOI={10.1016/j.cobeha.2019.04.007},
   journal={Current Opinion in Behavioral Sciences},
   publisher={Elsevier BV},
   author={Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
   year={2019},
   month={Oct},
   pages={97–104}
}
@article{sklearn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inproceedings{matching-net,
  title={Matching networks for one shot learning},
  author={Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Wierstra, Daan and others},
  booktitle={Advances in neural information processing systems},
  pages={3630--3638},
  year={2016}
}

@inproceedings{imagenet,
        AUTHOR = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
        TITLE = {{ImageNet: A Large-Scale Hierarchical Image Database}},
        BOOKTITLE = {CVPR},
        YEAR = {2009},
        BIBSOURCE = "http://www.image-net.org/papers/imagenet_cvpr09.bib"}
        

@inproceedings{du2019gradient,
title={Gradient Descent Provably Optimizes Over-parameterized Neural Networks},
author={Simon S. Du and Xiyu Zhai and Barnabas Poczos and Aarti Singh},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=S1eK3i09YQ},
}

@article{Zou_2019,
   title={Gradient descent optimizes over-parameterized deep ReLU networks},
   ISSN={1573-0565},
   url={http://dx.doi.org/10.1007/s10994-019-05839-6},
   DOI={10.1007/s10994-019-05839-6},
   journal={Machine Learning},
   publisher={Springer Science and Business Media LLC},
   author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
   year={2019},
   month={Oct}
}
@misc{oymak2019moderate,
    title={Towards moderate overparameterization: global convergence guarantees for training shallow neural networks},
    author={Samet Oymak and Mahdi Soltanolkotabi},
    year={2019},
    eprint={1902.04674},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{ma2019comparative,
  title={A comparative analysis of the optimization and generalization property of two-layer neural network and random feature models under gradient descent dynamics},
  author={Ma, Chao and Wu, Lei and others},
  journal={arXiv preprint arXiv:1904.04326},
  year={2019}
}

@article{cao2020generalization,
    title={Generalization Error Bounds of Gradient Descent for Learning Over-parameterized Deep ReLU Networks},
    author={Yuan Cao and Quanquan Gu},
    journal={AAAI},
    year={2020},
}


@article{cesa2004generalization,
  title={On the generalization ability of on-line learning algorithms},
  author={Cesa-Bianchi, Nicolo and Conconi, Alex and Gentile, Claudio},
  journal={IEEE Transactions on Information Theory},
  volume={50},
  number={9},
  pages={2050--2057},
  year={2004},
  publisher={IEEE}
}

@inproceedings{SVRG,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  booktitle={Advances in neural information processing systems},
  pages={315--323},
  year={2013}
}

@inproceedings{
Ji2020Polylogarithmic,
title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
author={Ziwei Ji and Matus Telgarsky},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HygegyrYwH}
}

@article{higher,
  title={Generalized Inner Loop Meta-Learning},
  author={Grefenstette, Edward and Amos, Brandon and Yarats, Denis and Htut, Phu Mon and Molchanov, Artem and Meier, Franziska and Kiela, Douwe and Cho, Kyunghyun and Chintala, Soumith},
  journal={arXiv preprint arXiv:1910.01727},
  year={2019}
}

@article{1977bounds,
  title={Bounds and perturbation bounds for the matrix exponential},
  author={K{\aa}gstr{\"o}m, Bo},
  journal={BIT Numerical Mathematics},
  volume={17},
  number={1},
  pages={39--57},
  year={1977},
  publisher={Springer}
}
@article{inverse-perturbation,
  title={The optimal perturbation bounds of the Moore--Penrose inverse under the Frobenius norm},
  author={Meng, Lingsheng and Zheng, Bing},
  journal={Linear algebra and its applications},
  volume={432},
  number={4},
  pages={956--963},
  year={2010},
  publisher={Elsevier}
}

@article{hu2020biased,
  title={Biased Stochastic Gradient Descent for Conditional Stochastic Optimization},
  author={Hu, Yifan and Zhang, Siqi and Chen, Xin and He, Niao},
  journal={arXiv preprint arXiv:2002.10790},
  year={2020}
}

@article{ji2020multistep,
  title={Multi-Step Model-Agnostic Meta-Learning: Convergence and Improved Algorithms},
  author={Ji, Kaiyi and Yang, Junjie and Liang, Yingbin},
  journal={arXiv preprint arXiv:2002.07836},
  year={2020}
}

@inproceedings{
random_net_distillation,
title={Exploration by random network distillation},
author={Yuri Burda and Harrison Edwards and Amos Storkey and Oleg Klimov},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1lJJnR5Ym},
}
@article{neural-kernels,
  title={Neural Kernels Without Tangents},
  author={Shankar, Vaishaal and Fang, Alex and Guo, Wenshuo and Fridovich-Keil, Sara and Schmidt, Ludwig and Ragan-Kelley, Jonathan and Recht, Benjamin},
  journal={ICML},
  year={2020}
}

@article{xiao2020dis,
  title={Disentangling trainability and generalization in deep learning},
  author={Xiao, Lechao and Pennington, Jeffrey and Schoenholz, Samuel S},
  journal={ICML},
  year={2020}
}
@article{caruana1997multitask,
  title={Multitask learning},
  author={Caruana, Rich},
  journal={Machine learning},
  volume={28},
  number={1},
  pages={41--75},
  year={1997},
  publisher={Springer}
}

@article{
hypertorch,
    title={On the Iteration Complexity of Hypergradient Computation},
    author={Riccardo Grazzi and Luca Franceschi and Massimiliano Pontil and Saverio Salzo},
  journal={ICML},
  year={2020}
}

@inproceedings{wang2020global,
 Author = {Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
 Booktitle = {International Conference on Machine Learning},
 Pages = {101--110},
 Title = {On the Global Optimality of Model-Agnostic Meta-Learning},
 Year = {2020}}
 
@article{xu2020meta,
    title={Meta Learning in the Continuous Time Limit},
    author={Ruitu Xu and Lin Chen and Amin Karbasi},
    year={2020},
      journal={arXiv preprint arXiv:2006.10921},
}

@inproceedings{rahimi2008random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1177--1184},
  year={2008}
}

@article{sun2019optimization,
  title={Optimization for deep learning: theory and algorithms},
  author={Sun, Ruoyu},
  journal={arXiv preprint arXiv:1912.08957},
  year={2019}
}

@article{liu2020random,
  title={Random Features for Kernel Approximation: A Survey in Algorithms, Theory, and Beyond},
  author={Liu, Fanghui and Huang, Xiaolin and Chen, Yudong and Suykens, Johan AK},
  journal={arXiv preprint arXiv:2004.11154},
  year={2020}
}




@inproceedings{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8157--8166},
  year={2018}
}

@article{liang2019revisiting,
  title={Revisiting Landscape Analysis in Deep Neural Networks: Eliminating Decreasing Paths to Infinity},
  author={Liang, Shiyu and Sun, Ruoyu and Srikant, R},
  journal={arXiv preprint arXiv:1912.13472},
  year={2019}
}

@misc{sirignano2019mean,
	title={Mean Field Analysis of Deep Neural Networks},
	author={Justin Sirignano and Konstantinos Spiliopoulos},
	year={2019},
	eprint={1903.04440},
	archivePrefix={arXiv},
	primaryClass={math.PR}
}

@article{vidal2017mathematics,
	title={Mathematics of deep learning},
	author={Vidal, Rene and Bruna, Joan and Giryes, Raja and Soatto, Stefano},
	journal={arXiv preprint arXiv:1712.04741},
	year={2017}
}

@inproceedings{liang2018adding,
	title={Adding one neuron can eliminate all bad local minima},
	author={Liang, Shiyu and Sun, Ruoyu and Lee, Jason D and Srikant, R},
	booktitle={Advances in Neural Information Processing Systems},
	pages={4355--4365},
	year={2018}
}

@article{kawaguchi2019elimination,
	title={Elimination of all bad local minima in deep learning},
	author={Kawaguchi, Kenji and Kaelbling, Leslie Pack},
	journal={arXiv preprint arXiv:1901.00279},
	year={2019}
}

@inproceedings{liang2018understanding,
  title={Understanding the Loss Surface of Neural Networks for Binary Classification},
  author={Liang, Shiyu and Sun, Ruoyu and Li, Yixuan and Srikant, Rayadurgam},
  booktitle={International Conference on Machine Learning},
  pages={2835--2843},
  year={2018}
}

@article{li2018over,
  title={On the benefit of width for neural networks: Disappearance of bad basins},
  author={Li, Dawei and Ding, Tian and Sun, Ruoyu},
  journal={arXiv preprint arXiv:1812.11039},
  year={2018}
}



@inproceedings{nguyen2018loss,
  title={On the loss landscape of a class of deep neural networks with no bad local valleys},
  author={Nguyen, Quynh and Mukkamala, Mahesh Chandra and Hein, Matthias},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{luo2019mitigating,
  title={Mitigating Data Scarcity in Protein Binding Prediction Using Meta-Learning},
  author={Luo, Yunan and Ma, Jianzhu and Ideker, Xiaoming Trey and Zhao, Jian and Su, Peng1 YufengB and Liu, Yang},
  booktitle={Research in Computational Molecular Biology: 23rd Annual International Conference, RECOMB 2019, Washington, DC, USA, May 5-8, 2019, Proceedings},
  volume={11467},
  pages={305},
  year={2019},
  organization={Springer}
}

@inproceedings{yu-etal-2018-diverse,
    title = "Diverse Few-Shot Text Classification with Multiple Metrics",
    author = "Yu, Mo  and
      Guo, Xiaoxiao  and
      Yi, Jinfeng  and
      Chang, Shiyu  and
      Potdar, Saloni  and
      Cheng, Yu  and
      Tesauro, Gerald  and
      Wang, Haoyu  and
      Zhou, Bowen",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1109",
    doi = "10.18653/v1/N18-1109",
    pages = "1206--1215",
    abstract = "We study few-shot learning in natural language domains. Compared to many existing works that apply either metric-based or optimization-based meta-learning to image domain with low inter-task variance, we consider a more realistic setting, where tasks are diverse. However, it imposes tremendous difficulties to existing state-of-the-art metric-based algorithms since a single metric is insufficient to capture complex task variations in natural language domain. To alleviate the problem, we propose an adaptive metric learning approach that automatically determines the best weighted combination from a set of metrics obtained from meta-training tasks for a newly seen few-shot task. Extensive quantitative evaluations on real-world sentiment analysis and dialog intent classification datasets demonstrate that the proposed method performs favorably against state-of-the-art few shot learning algorithms in terms of predictive accuracy. We make our code and data available for further study.",
}

@misc{bansal2019learning,
    title={Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks},
    author={Trapit Bansal and Rishikesh Jha and Andrew McCallum},
    year={2019},
    eprint={1911.03863},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{saunshi2020sample,
      title={A Sample Complexity Separation between Non-Convex and Convex Meta-Learning}, 
      author={Nikunj Saunshi and Yi Zhang and Mikhail Khodak and Sanjeev Arora},
      year={2020},
      journal={ICML} 
}

@inproceedings{metaOptNet,
  title={Meta-Learning with Differentiable Convex Optimization},
  author={Kwonjoon Lee and Subhransu Maji and Avinash Ravichandran and Stefano Soatto},
  booktitle={CVPR},
  year={2019}
}

@inproceedings{muandet2013domain,
  title={Domain generalization via invariant feature representation},
  author={Muandet, Krikamol and Balduzzi, David and Sch{\"o}lkopf, Bernhard},
  booktitle={International Conference on Machine Learning},
  pages={10--18},
  year={2013}
}

@article{baxter2000model,
  title={A model of inductive bias learning},
  author={Baxter, Jonathan},
  journal={Journal of artificial intelligence research},
  volume={12},
  pages={149--198},
  year={2000}
}

@article{maurer2016benefit,
  title={The benefit of multitask representation learning},
  author={Maurer, Andreas and Pontil, Massimiliano and Romera-Paredes, Bernardino},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={2853--2884},
  year={2016},
  publisher={JMLR. org}
}

@misc{albuquerque2020generalizing,
      title={Generalizing to unseen domains via distribution matching}, 
      author={Isabela Albuquerque and João Monteiro and Mohammad Darvishi and Tiago H. Falk and Ioannis Mitliagkas},
      year={2020},
      eprint={1911.00804},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{laurent2018deep,
  title={Deep linear networks with arbitrary loss: All local minima are global},
  author={Laurent, Thomas and Brecht, James},
  booktitle={International conference on machine learning},
  pages={2902--2907},
  year={2018},
  organization={PMLR}
}

@inproceedings{franceschi2018bilevel,
  title={Bilevel Programming for Hyperparameter Optimization and Meta-Learning},
  author={Franceschi, Luca and Frasconi, Paolo and Salzo, Saverio and Grazzi, Riccardo and Pontil, Massimiliano},
  booktitle={International Conference on Machine Learning},
  pages={1568--1577},
  year={2018}
}

@misc{hospedales2020metalearning,
      title={Meta-Learning in Neural Networks: A Survey}, 
      author={Timothy Hospedales and Antreas Antoniou and Paul Micaelli and Amos Storkey},
      year={2020},
      eprint={2004.05439},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{
r2d2,
title={Meta-learning with differentiable closed-form solvers},
author={Luca Bertinetto and Joao F. Henriques and Philip Torr and Andrea Vedaldi},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HyxnZh0ct7},
}

@inproceedings{zhou2019efficient,
  title={Efficient meta learning via minibatch proximal update},
  author={Zhou, Pan and Yuan, Xiaotong and Xu, Huan and Yan, Shuicheng and Feng, Jiashi},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1534--1544},
  year={2019}
}

@article{layernorm,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@inproceedings{hui2020evaluation,
      title={Evaluation of Neural Architectures Trained with Square Loss vs Cross-Entropy in Classification Tasks}, 
      author={Like Hui and Mikhail Belkin},
      year={2021},
      booktitle={ICLR}, }

@article{tian2020rethink,
  title={Rethinking few-shot image classification: a good embedding is all you need?},
  author={Tian, Yonglong and Wang, Yue and Krishnan, Dilip and Tenenbaum, Joshua B and Isola, Phillip},
  journal={ECCV},
  year={2020}
}

@misc{
bai2021how,
title={How Important is the Train-Validation Split in Meta-Learning?},
author={Yu Bai and Minshuo Chen and Pan Zhou and Tuo Zhao and Jason D. Lee and Sham M. Kakade and Huan Wang and Caiming Xiong},
year={2021},
url={https://openreview.net/forum?id=qG4ZVCCyCB0}
}

@article{henderson1981deriving,
  title={On deriving the inverse of a sum of matrices},
  author={Henderson, Harold V and Searle, Shayle R},
  journal={Siam Review},
  volume={23},
  number={1},
  pages={53--60},
  year={1981},
  publisher={SIAM}
}

@inproceedings{
du2021fewshot,
title={Few-Shot Learning via Learning the Representation, Provably},
author={Simon Shaolei Du and Wei Hu and Sham M. Kakade and Jason D. Lee and Qi Lei},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=pW2Q2xLwIMD}
}

@misc{lin2021to,
title={To Learn Effective Features: Understanding the Task-Specific Adaptation of {\{}MAML{\}}},
author={Zhijie Lin and Zhou Zhao and Zhu Zhang and Huai Baoxing and Jing Yuan},
year={2021},
url={https://openreview.net/forum?id=FPpZrRfz6Ss}
}

@inproceedings{
grant2018recasting,
title={Recasting Gradient-Based Meta-Learning as Hierarchical Bayes},
author={Erin Grant and Chelsea Finn and Sergey Levine and Trevor Darrell and Thomas Griffiths},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BJ_UL-k0b},
}

@inproceedings{goldblum2020unraveling,
  title={Unraveling meta-learning: Understanding feature representations for few-shot tasks},
  author={Goldblum, Micah and Reich, Steven and Fowl, Liam and Ni, Renkun and Cherepanova, Valeriia and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={3607--3616},
  year={2020},
  organization={PMLR}
}

@inproceedings{he2019bag,
  title={Bag of tricks for image classification with convolutional neural networks},
  author={He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={558--567},
  year={2019}
}
@inproceedings{
hui2021evaluation,
title={Evaluation of Neural Architectures Trained with Square Loss vs Cross-Entropy in Classification Tasks},
author={Like Hui and Mikhail Belkin},
booktitle={International Conference on Learning Representations},
year={2021},
}

@article{predict-then-optimize,
      title={Smart "Predict, then Optimize"}, 
      author={Adam N. Elmachtoub and Paul Grigas},
      year={2017},
      eprint={1710.08005},
      archivePrefix={arXiv},
      primaryClass={math.OC},
      journal={To appear on Management Science (arXiv:1710.08005)}
}

@inproceedings{generalization-predict-then-optimize,
 author = {El Balghiti, Othman and Elmachtoub, Adam N. and Grigas, Paul and Tewari, Ambuj},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {14412--14421},
 publisher = {Curran Associates, Inc.},
 title = {Generalization Bounds in the Predict-then-Optimize Framework},
 url = {https://proceedings.neurips.cc/paper/2019/file/a70145bf8b173e4496b554ce57969e24-Paper.pdf},
 volume = {32},
 year = {2019}
}
@inproceedings{neuraltangents2020,
    title={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},
    author={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://github.com/google/neural-tangents}
}

@article{cifar,
  title={Learning Multiple Layers of Features from Tiny Images},
  author={Krizhevsky, Alex},
  year={2009}
}

@inproceedings{
ren2018metalearning,
title={Meta-Learning for Semi-Supervised Few-Shot Classification},
author={Mengye Ren and Sachin Ravi and Eleni Triantafillou and Jake Snell and Kevin Swersky and Josh B. Tenenbaum and Hugo Larochelle and Richard S. Zemel},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HJcSzz-CZ},
}


@article{pytorch,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={8026--8037},
  year={2019}
}
@inproceedings{zhang2014facial,
  title={Facial landmark detection by deep multi-task learning},
  author={Zhang, Zhanpeng and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
  booktitle={European conference on computer vision},
  pages={94--108},
  year={2014},
  organization={Springer}
}
@inproceedings{kendall2018multi,
  title={Multi-task learning using uncertainty to weigh losses for scene geometry and semantics},
  author={Kendall, Alex and Gal, Yarin and Cipolla, Roberto},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7482--7491},
  year={2018}
}
@inproceedings{dong2015multi,
  title={Multi-task learning for multiple language translation},
  author={Dong, Daxiang and Wu, Hua and He, Wei and Yu, Dianhai and Wang, Haifeng},
  booktitle={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={1723--1732},
  year={2015}
}
@inproceedings{
wang2018glue,
title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJ4km2R5t7},
}
@inproceedings{hsu2020meta,
  title={Meta learning for end-to-end low-resource speech recognition},
  author={Hsu, Jui-Yang and Chen, Yuan-Jui and Lee, Hung-yi},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7844--7848},
  year={2020},
  organization={IEEE}
}
@article{overview-mtl,
    author = {Zhang, Yu and Yang, Qiang},
    title = "{An overview of multi-task learning}",
    journal = {National Science Review},
    volume = {5},
    number = {1},
    pages = {30-43},
    year = {2017},
    month = {09},
    abstract = "{As a promising area in machine learning, multi-task learning (MTL) aims to improve the performance of multiple related learning tasks by leveraging useful information among them. In this paper, we give an overview of MTL by first giving a definition of MTL. Then several different settings of MTL are introduced, including multi-task supervised learning, multi-task unsupervised learning, multi-task semi-supervised learning, multi-task active learning, multi-task reinforcement learning, multi-task online learning and multi-task multi-view learning. For each setting, representative MTL models are presented. In order to speed up the learning process, parallel and distributed MTL models are introduced. Many areas, including computer vision, bioinformatics, health informatics, speech, natural language processing, web applications and ubiquitous computing, use MTL to improve the performance of the applications involved and some representative works are reviewed. Finally, recent theoretical analyses for MTL are presented.}",
    issn = {2095-5138},
    doi = {10.1093/nsr/nwx105},
    url = {https://doi.org/10.1093/nsr/nwx105},
    eprint = {https://academic.oup.com/nsr/article-pdf/5/1/30/31567358/nwx105.pdf},
}



@inproceedings{
howtotrainmaml,
title={How to train your {MAML}},
author={Antreas Antoniou and Harrison Edwards and Amos Storkey},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HJGven05Y7},
}
@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.2.5},
  year = {2018},
}

@inproceedings{
lee2018deep,
title={Deep Neural Networks as Gaussian Processes},
author={Jaehoon Lee and Jascha Sohl-dickstein and Jeffrey Pennington and Roman Novak and Sam Schoenholz and Yasaman Bahri},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=B1EA-M-0Z},
}

@inproceedings{
novak2019bayesian,
title={Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes},
author={Roman Novak and Lechao Xiao and Yasaman Bahri and Jaehoon Lee and Greg Yang and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-dickstein},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1g30j0qF7},
}

@inproceedings{
Hanin2020Finite,
title={Finite Depth and Width Corrections to the Neural Tangent Kernel},
author={Boris Hanin and Mihai Nica},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJgndT4KwB}
}

@inproceedings{liu2019radam,
 author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
 booktitle = {Proceedings of the Eighth International Conference on Learning Representations (ICLR 2020)},
 month = {April},
 title = {On the Variance of the Adaptive Learning Rate and Beyond},
 year = {2020}
}

@article{arjovsky2019invariant,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}

@article{arjovsky2017wgan,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}



@article{papyan2020prevalence,
  title={Prevalence of neural collapse during the terminal phase of deep learning training},
  author={Papyan, Vardan and Han, XY and Donoho, David L},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={40},
  pages={24652--24663},
  year={2020},
  publisher={National Acad Sciences}
}

@article{combes2020domain,
  title={Domain Adaptation with Conditional Distribution Matching and Generalized Label Shift},
  author={Combes, Remi Tachet des and Zhao, Han and Wang, Yu-Xiang and Gordon, Geoff},
  journal={arXiv preprint arXiv:2003.04475},
  year={2020}
}
@inproceedings{zhao2019domain,
  title={On learning invariant representations for domain adaptation},
  author={Zhao, Han and Des Combes, Remi Tachet and Zhang, Kun and Gordon, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={7523--7532},
  year={2019},
  organization={PMLR}
}

@book{wainwright2019high,
  title={High-dimensional statistics: A non-asymptotic viewpoint},
  author={Wainwright, Martin J},
  volume={48},
  year={2019},
  publisher={Cambridge University Press}
}

@InProceedings{wang2020understand,
  title = 	 {Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere},
  author =       {Wang, Tongzhou and Isola, Phillip},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  year = 	 {2020},
}
@inproceedings{wang2021bridging,
  title={Bridging Multi-Task Learning and Meta-Learning: Towards Efficient Training and Effective Adaptation},
  author={Wang, Haoxiang and Zhao, Han and Li, Bo},
  booktitle={International Conference on Machine Learning},
  year={2021},
  organization={PMLR}
}

@inproceedings{kuznetsov2014generalization,
  title={Generalization bounds for time series prediction with non-stationary processes},
  author={Kuznetsov, Vitaly and Mohri, Mehryar},
  booktitle={International conference on algorithmic learning theory},
  pages={260--274},
  year={2014},
  organization={Springer}
}
@inproceedings{kuznetsov2015learning,
  title={Learning Theory and Algorithms for Forecasting Non-stationary Time Series.},
  author={Kuznetsov, Vitaly and Mohri, Mehryar},
  booktitle={NIPS},
  pages={541--549},
  year={2015},
  organization={Citeseer}
}
@article{kuznetsov2017generalization,
  title={Generalization bounds for non-stationary mixing processes},
  author={Kuznetsov, Vitaly and Mohri, Mehryar},
  journal={Machine Learning},
  volume={106},
  number={1},
  pages={93--117},
  year={2017},
  publisher={Springer}
}
@article{kuznetsov2020discrepancy,
  title={Discrepancy-based theory and algorithms for forecasting non-stationary time series},
  author={Kuznetsov, Vitaly and Mohri, Mehryar},
  journal={Annals of Mathematics and Artificial Intelligence},
  volume={88},
  number={4},
  pages={367--399},
  year={2020},
  publisher={Springer}
}

@inproceedings{kuznetsov2016time,
  title={Time series prediction and online learning},
  author={Kuznetsov, Vitaly and Mohri, Mehryar},
  booktitle={Conference on Learning Theory},
  pages={1190--1213},
  year={2016},
  organization={PMLR}
}

@article{rakhlin2015online,
  title={Online learning via sequential complexities},
  author={Rakhlin, Alexander and Sridharan, Karthik and Tewari, Ambuj},
  journal={J. Mach. Learn. Res.},
  volume={16},
  number={1},
  pages={155--186},
  year={2015}
}
@misc{rakhlin2014notes,
  title={Statistical learning and sequential prediction},
  author={Rakhlin, Alexander and Sridharan, Karthik},
  journal={Available at \url{http://www.mit.edu/~rakhlin/courses/stat928/stat928_notes.pdf}},
  year={2014}
}

@article{abnar2021gradual,
  title={Gradual Domain Adaptation in the Wild: When Intermediate Distributions are Absent},
  author={Abnar, Samira and Berg, Rianne van den and Ghiasi, Golnaz and Dehghani, Mostafa and Kalchbrenner, Nal and Sedghi, Hanie},
  journal={arXiv preprint arXiv:2106.06080},
  year={2021}
}

@article{chen2021gradual,
  title={Gradual Domain Adaptation without Indexed Intermediate Domains},
  author={Chen, Hong-You and Chao, Wei-Lun},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{zhang2021gradual,
  title={Gradual Domain Adaptation via Self-Training of Auxiliary Models},
  author={Zhang, Yabin and Deng, Bin and Jia, Kui and Zhang, Lei},
  journal={arXiv preprint arXiv:2106.09890},
  year={2021}
}

@article{zhao2018adversarial,
  title={Adversarial multiple source domain adaptation},
  author={Zhao, Han and Zhang, Shanghang and Wu, Guanhang and Moura, Jos{\'e} MF and Costeira, Joao P and Gordon, Geoffrey J},
  journal={Advances in neural information processing systems},
  volume={31},
  pages={8559--8570},
  year={2018}
}

@inproceedings{ganin2015unsupervised,
  title={Unsupervised domain adaptation by backpropagation},
  author={Ganin, Yaroslav and Lempitsky, Victor},
  booktitle={International conference on machine learning},
  pages={1180--1189},
  year={2015},
  organization={PMLR}
}

@article{ganin2016domain,
  title={Domain-adversarial training of neural networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
  journal={The journal of machine learning research},
  volume={17},
  number={1},
  pages={2096--2030},
  year={2016},
  publisher={JMLR. org}
}

@article{wilson2020survey,
  title={A survey of unsupervised deep domain adaptation},
  author={Wilson, Garrett and Cook, Diane J},
  journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
  volume={11},
  number={5},
  pages={1--46},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@inproceedings{long2015learning,
  title={Learning transferable features with deep adaptation networks},
  author={Long, Mingsheng and Cao, Yue and Wang, Jianmin and Jordan, Michael},
  booktitle={International conference on machine learning},
  pages={97--105},
  year={2015},
  organization={PMLR}
}
@inproceedings{tzeng2017adversarial,
  title={Adversarial discriminative domain adaptation},
  author={Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7167--7176},
  year={2017}
}

@article{ben-david2010theory,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine learning},
  volume={79},
  number={1},
  pages={151--175},
  year={2010},
  publisher={Springer}
}


@inproceedings{koh2021wilds,
  title={Wilds: A benchmark of in-the-wild distribution shifts},
  author={Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and others},
  booktitle={International Conference on Machine Learning},
  pages={5637--5664},
  year={2021},
  organization={PMLR}
}

@inproceedings{
sagawa2021extending,
title={Extending the {WILDS} Benchmark for Unsupervised Adaptation},
author={Shiori Sagawa and Pang Wei Koh and Tony Lee and Irena Gao and Sang Michael Xie and Kendrick Shen and Ananya Kumar and Weihua Hu and Michihiro Yasunaga and Henrik Marklund and Sara Beery and Etienne David and Ian Stavness and Wei Guo and Jure Leskovec and Kate Saenko and Tatsunori Hashimoto and Sergey Levine and Chelsea Finn and Percy Liang},
booktitle={NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications},
year={2021},
url={https://openreview.net/forum?id=2EhHKKXMbG0}
}

@inproceedings{gadermayr2018gradual,
  title={Gradual domain adaptation for segmenting whole slide images showing pathological variability},
  author={Gadermayr, Michael and Eschweiler, Dennis and Klinkhammer, Barbara Mara and Boor, Peter and Merhof, Dorit},
  booktitle={International Conference on Image and Signal Processing},
  pages={461--469},
  year={2018},
  organization={Springer}
}

@misc{
  bobu2018adapting,
  title={Adapting to Continuously Shifting Domains},
  author={Andreea Bobu and Eric Tzeng and Judy Hoffman and Trevor Darrell},
  year={2018},
  url={https://openreview.net/forum?id=BJsBjPJvf}
}

@inproceedings{hoffman2014continuous,
  title={Continuous manifold based adaptation for evolving visual domains},
  author={Hoffman, Judy and Darrell, Trevor and Saenko, Kate},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={867--874},
  year={2014}
}

@inproceedings{wulfmeier2018incremental,
  title={Incremental adversarial domain adaptation for continually changing environments},
  author={Wulfmeier, Markus and Bewley, Alex and Posner, Ingmar},
  booktitle={2018 IEEE International conference on robotics and automation (ICRA)},
  pages={4489--4495},
  year={2018},
  organization={IEEE}
}

@inproceedings{
farshchian2018adversarial,
title={Adversarial Domain Adaptation for Stable Brain-Machine Interfaces},
author={Ali Farshchian and Juan A. Gallego and Joseph P. Cohen and Yoshua Bengio and Lee E. Miller and Sara A. Solla},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Hyx6Bi0qYm},
}

@inproceedings{
malinin2021shifts,
title={Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks},
author={Andrey Malinin and Neil Band and Yarin Gal and Mark Gales and Alexander Ganshin and German Chesnokov and Alexey Noskov and Andrey Ploskonosov and Liudmila Prokhorenkova and Ivan Provilkov and Vatsal Raina and Vyas Raina and Denis Roginskiy and Mariya Shmatova and Panagiotis Tigas and Boris Yangel},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
year={2021},
url={https://openreview.net/forum?id=qM45LHaWM6E}
}

@article{zhao2021pyhealth,
  title={PyHealth: A Python Library for Health Predictive Models},
  author={Zhao, Yue and Qiao, Zhi and Xiao, Cao and Glass, Lucas and Sun, Jimeng},
  journal={arXiv preprint arXiv:2101.04209},
  year={2021}
}

@article{scudder1965probability,
  title={Probability of error of some adaptive pattern-recognition machines},
  author={Scudder, Henry},
  journal={IEEE Transactions on Information Theory},
  volume={11},
  number={3},
  pages={363--371},
  year={1965},
  publisher={IEEE}
}

@inproceedings{yarowsky1995unsupervised,
  title={Unsupervised word sense disambiguation rivaling supervised methods},
  author={Yarowsky, David},
  booktitle={33rd annual meeting of the association for computational linguistics},
  pages={189--196},
  year={1995}
}

@inproceedings{xie2020self,
  title={Self-training with noisy student improves imagenet classification},
  author={Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10687--10698},
  year={2020}
}

@inproceedings{lee2013pseudo,
  title={Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
  author={Lee, Dong-Hyun and others},
  booktitle={Workshop on challenges in representation learning, ICML},
  year={2013}
}

@inproceedings{ginosar2015century,
  title={A century of portraits: A visual historical record of american high school yearbooks},
  author={Ginosar, Shiry and Rakelly, Kate and Sachs, Sarah and Yin, Brian and Efros, Alexei A},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision Workshops},
  pages={1--7},
  year={2015}
}

@inproceedings{rakhlin2010online,
 author = {Rakhlin, Alexander and Sridharan, Karthik and Tewari, Ambuj},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Online Learning: Random Averages, Combinatorial Parameters, and Learnability},
 volume = {23},
 year = {2010}
}

@book{vapnik1999nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={1999},
  publisher={Springer science \& business media}
}

@article{bartlett2002rademacher,
  title={Rademacher and Gaussian complexities: Risk bounds and structural results},
  author={Bartlett, Peter L and Mendelson, Shahar},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={463--482},
  year={2002}
}

@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low Bellman rank are PAC-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={International Conference on Machine Learning},
  pages={1704--1713},
  year={2017},
  organization={PMLR}
}

@article{jin2021bellman,
  title={Bellman Eluder dimension: New rich classes of RL problems, and sample-efficient algorithms},
  author={Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
  journal={arXiv preprint arXiv:2102.00815},
  year={2021}
}

@article{littlestone1988learning,
  title={Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm},
  author={Littlestone, Nick},
  journal={Machine learning},
  volume={2},
  number={4},
  pages={285--318},
  year={1988},
  publisher={Springer}
}

@article{ajakan2014domain,
  title={Domain-adversarial neural networks},
  author={Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario},
  journal={arXiv preprint arXiv:1412.4446},
  year={2014}
}

@misc{deepCORAL,
      title={Deep CORAL: Correlation Alignment for Deep Domain Adaptation}, 
      author={Baochen Sun and Kate Saenko},
      year={2016},
      eprint={1607.01719},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{zou2018unsupervised,
  title={Unsupervised Domain Adaptation for Semantic Segmentation via Class-Balanced Self-Training},
  author={Zou, Yang and Yu, Zhiding and Kumar, BVK Vijaya and Wang, Jinsong},
  booktitle={ECCV},
  pages={289--305},
  year={2018}
}

@InProceedings{zou2019confidence,
author = {Zou, Yang and Yu, Zhiding and Liu, Xiaofeng and Kumar, B.V.K. Vijaya and Wang, Jinsong},
title = {Confidence Regularized Self-Training},
booktitle = {ICCV},
month = {October},
year = {2019}
}

@inproceedings{liang2020we, 
 title={Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation}, 
 author={Liang, Jian and Hu, Dapeng and Feng, Jiashi}, 
 booktitle={ICML},  
 pages={6028--6039},
 year={2020}
}
@inproceedings{liang2019distant,
  title={Distant supervised centroid shift: A simple and efficient approach to visual domain adaptation},
  author={Liang, Jian and He, Ran and Sun, Zhenan and Tan, Tieniu},
  booktitle={CVPR},
  pages={2975--2984},
  year={2019}
}

@inproceedings{FixMatch,
 author = {Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {596--608},
 publisher = {Curran Associates, Inc.},
 title = {FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence},
 volume = {33},
 year = {2020}
}

@inproceedings{arazo2020pseudo,
  title={Pseudo-labeling and confirmation bias in deep semi-supervised learning},
  author={Arazo, Eric and Ortego, Diego and Albert, Paul and O’Connor, Noel E and McGuinness, Kevin},
  booktitle={IJCNN},
  pages={1--8},
  year={2020},
  organization={IEEE}
}

@inproceedings{pham2021meta,
  title={Meta pseudo labels},
  author={Pham, Hieu and Dai, Zihang and Xie, Qizhe and Le, Quoc V},
  booktitle={CVPR},
  pages={11557--11568},
  year={2021}
}

@article{CoverType,
  title={Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables},
  author={Blackard, Jock A and Dean, Denis J},
  journal={Computers and electronics in agriculture},
  volume={24},
  number={3},
  pages={131--151},
  year={1999},
  publisher={Elsevier}
}

@misc{UCI_repository,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }

@article{kantorovich1939,
  title={Mathematical Methods of Organizing and Planning Production},
  author={Kantorovich, Leonid V},
  journal={Management science},
  volume={6},
  number={4},
  pages={366--422},
  year={1939},
  publisher={INFORMS}
}

@article{peyre2019computational,
  title={Computational optimal transport: With applications to data science},
  author={Peyr{\'e}, Gabriel and Cuturi, Marco and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={5-6},
  pages={355--607},
  year={2019},
  publisher={Now Publishers, Inc.}
}

@book{villani2009optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2009},
  publisher={Springer}
}



@InProceedings{WGAN,
  title = 	 {{W}asserstein Generative Adversarial Networks},
  author =       {Martin Arjovsky and Soumith Chintala and L{\'e}on Bottou},
  booktitle = 	 {ICML},
  pages = 	 {214--223},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
}


@article{courty2016optimal,
  title={Optimal transport for domain adaptation},
  author={Courty, Nicolas and Flamary, R{\'e}mi and Tuia, Devis and Rakotomamonjy, Alain},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={9},
  pages={1853--1865},
  year={2016},
  publisher={IEEE}
}

@inproceedings{courty2014domain,
  title={Domain adaptation with regularized optimal transport},
  author={Courty, Nicolas and Flamary, R{\'e}mi and Tuia, Devis},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={274--289},
  year={2014},
  organization={Springer}
}
@inproceedings{courty2017joint,
 author = {Courty, Nicolas and Flamary, R\'{e}mi and Habrard, Amaury and Rakotomamonjy, Alain},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Joint distribution optimal transportation for domain adaptation},
 volume = {30},
 year = {2017}
}

@inproceedings{
WAE,
title={Wasserstein Auto-Encoders},
author={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HkL7n1-0b},
}

@inproceedings{redko2019optimal,
  title={Optimal transport for multi-source domain adaptation under target shift},
  author={Redko, Ievgen and Courty, Nicolas and Flamary, R{\'e}mi and Tuia, Devis},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={849--858},
  year={2019},
  organization={PMLR}
}

@article{van2020survey,
  title={A survey on semi-supervised learning},
  author={Van Engelen, Jesper E and Hoos, Holger H},
  journal={Machine Learning},
  volume={109},
  number={2},
  pages={373--440},
  year={2020},
  publisher={Springer}
}

@article{anderson1972more,
  title={More is different},
  author={Anderson, Philip W},
  journal={Science},
  volume={177},
  number={4047},
  pages={393--396},
  year={1972},
  publisher={JSTOR}
}

@article{huang2014ramp,
  title={Ramp loss linear programming support vector machine},
  author={Huang, Xiaolin and Shi, Lei and Suykens, Johan AK},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2185--2211},
  year={2014},
  publisher={JMLR. org}
}

@misc{liang2016cs229t,
  title={Statistical Learning Theory},
  author={Liang, Percy},
  year={2016},
  url={https://web.stanford.edu/class/cs229t/notes.pdf}
}


@InProceedings{wang2020continuously,
  title = 	 {Continuously Indexed Domain Adaptation},
  author =       {Wang, Hao and He, Hao and Katabi, Dina},
  booktitle = 	 {ICML},
  pages = 	 {9898--9907},
  year = 	 {2020},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
}

@inproceedings{vapnik1992principles,
  title={Principles of risk minimization for learning theory},
  author={Vapnik, Vladimir},
  booktitle={Advances in neural information processing systems},
  pages={831--838},
  year={1992}
}


@misc{keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@inproceedings{tensorflow,
author = {Mart{\'\i}n Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
title = {{TensorFlow}: A System for {Large-Scale} Machine Learning},
booktitle = {12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)},
year = {2016},
isbn = {978-1-931971-33-1},
address = {Savannah, GA},
pages = {265--283},
url = {https://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi},
publisher = {USENIX Association},
month = nov,
}


@article{dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{hendrycks2021natural,
  title={Natural adversarial examples},
  author={Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
  booktitle={CVPR},
  pages={15262--15271},
  year={2021}
}
@inproceedings{
gulrajani2021in,
title={In Search of Lost Domain Generalization},
author={Ishaan Gulrajani and David Lopez-Paz},
booktitle={International Conference on Learning Representations},
year={2021},
}

@article{talagrand1995concentration,
  title={Concentration of measure and isoperimetric inequalities in product spaces},
  author={Talagrand, Michel},
  journal={Publications Math{\'e}matiques de l'Institut des Hautes Etudes Scientifiques},
  volume={81},
  number={1},
  pages={73--205},
  year={1995},
  publisher={Springer}
}

@article{ben2007analysis,
  title={Analysis of representations for domain adaptation},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando and others},
  journal={Advances in neural information processing systems},
  volume={19},
  pages={137},
  year={2007},
  publisher={MIT; 1998}
}

@inproceedings{zhou2022online,
  title={Online Continual Adaptation with Active Self-Training},
  author={Zhou, Shiji and Zhao, Han and Zhang, Shanghang and Wang, Lianzhe and Chang, Heng and Wang, Zhi and Zhu, Wenwu},
  booktitle={AISTATS},
  year={2022},
  organization={PMLR}
}

@article{dong2022algorithms,
  title={Algorithms and Theory for Supervised Gradual Domain Adaptation},
  author={Dong, Jing and Zhou, Shiji and Wang, Baoxiang and Zhao, Han},
  journal={arXiv preprint arXiv:2204.11644},
  year={2022}
}

@inproceedings{
ye2022future,
title={Future Gradient Descent for Adapting the Temporal Shifting Data Distribution in Online Recommendation System},
author={Mao Ye and Ruichen Jiang and Haoxiang Wang and Dhruv Choudhary and Xiaocong Du and Bhargav Bhushanam and Aryan Mokhtari and Arun Kejariwal and qiang liu},
booktitle={The 38th Conference on Uncertainty in Artificial Intelligence},
year={2022},
}