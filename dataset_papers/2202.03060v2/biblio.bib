@inproceedings{hazan2019maxent,
  title={Provably Efficient Maximum Entropy Exploration},
  author={Hazan, Elad and Kakade, Sham and Singh, Karan and Van Soest, Abby},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2019},
}

@inproceedings{tarbouriech2019active,
  title={Active Exploration in {M}arkov Decision Processes},
  author={Tarbouriech, Jean and Lazaric, Alessandro},
  booktitle={Proceedings of the International Conference on Artificial Intelligence and Statistics},
  year={2019}
}

@article{lee2019smm,
  title={Efficient exploration via state marginal matching},
  author={Lee, Lisa and Eysenbach, Benjamin and Parisotto, Emilio and Xing, Eric and Levine, Sergey and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1906.05274},
  year={2019}
}

@inproceedings{mutti2020intrinsically,
  title={An Intrinsically-Motivated Approach for Learning Highly Exploring and Fast Mixing Policies},
  author={Mutti, Mirco and Restelli, Marcello},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2020}
}

@inproceedings{mutti2020policy, 
  title={Task-Agnostic Exploration via Policy Gradient of a Non-Parametric State Entropy Estimate}, 
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence}, 
  author={Mutti, Mirco and Pratissoli, Lorenzo and Restelli, Marcello}, 
  year={2021}, 
}

@inproceedings{seo2021state,
  title={State Entropy Maximization with Random Encoders for Efficient Exploration},
  author={Seo, Younggyo and Chen, Lili and Shin, Jinwoo and Lee, Honglak and Abbeel, Pieter and Lee, Kimin},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2021}
}

@inproceedings{yarats2021reinforcement,
  title={Reinforcement Learning with Prototypical Representations},
  author={Yarats, Denis and Fergus, Rob and Lazaric, Alessandro and Pinto, Lerrel},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2021}
}

@InProceedings{liu2021aps,
  title = {{APS}: Active Pretraining with Successor Features},
  author = {Liu, Hao and Abbeel, Pieter},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  year = {2021},
}

@inproceedings{zhang2020exploration,
  title={Exploration by maximizing {R}{\'e}nyi entropy for reward-free {RL} framework},
  author={Zhang, Chuheng and Cai, Yuanying and Huang, Longbo and Li, Jian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2021}
}

@article{guo2021geometric,
  title={Geometric Entropic Exploration},
  author={Guo, Zhaohan Daniel and Azar, Mohammad Gheshlagi and Saade, Alaa and Thakoor, Shantanu and Piot, Bilal and Pires, Bernardo Avila and Valko, Michal and Mesnard, Thomas and Lattimore, Tor and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:2101.02055},
  year={2021}
}

@inproceedings{liu2021behavior,
  title={Behavior from the void: Unsupervised active pre-training},
  author={Liu, Hao and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{jin2020reward,
  title={Reward-free exploration for reinforcement learning},
  author={Jin, Chi and Krishnamurthy, Akshay and Simchowitz, Max and Yu, Tiancheng},
  booktitle={Proceedings of the International Conference on Machine Learning},
  year={2020},
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  year={1992},
}

@article{peters2008reinforcement,
  title={Reinforcement learning of motor skills with policy gradients},
  author={Peters, Jan and Schaal, Stefan},
  journal={Neural networks},
  year={2008},
}

@inproceedings{zhang2020variational,
  title={Variational Policy Gradient Method for Reinforcement Learning with General Utilities},
  author={Zhang, Junyu and Koppel, Alec and Bedi, Amrit Singh and Szepesvari, Csaba and Wang, Mengdi},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020},
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@article{astrom1965optimal,
  title={Optimal control of {M}arkov decision processes with incomplete state estimation},
  author={Astrom, Karl J},
  journal={Journal Mathematical Analysis and Applications},
  year={1965}
}

@article{kaelbling1998planning,
  title={Planning and acting in partially observable stochastic domains},
  author={Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
  journal={Artificial Intelligence},
  year={1998}
}

@book{bertsekas2002introduction,
  title={Introduction to probability},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  year={2002},
  publisher={Athena Scientific Belmont, MA},
}

@article{papadimitriou1987complexity,
  title={The complexity of {M}arkov decision processes},
  author={Papadimitriou, Christos H and Tsitsiklis, John N},
  journal={Mathematics of Operations Research},
  year={1987},
}

@article{mundhenk2000complexity,
  title={Complexity of finite-horizon {M}arkov decision process problems},
  author={Mundhenk, Martin and Goldsmith, Judy and Lusena, Christopher and Allender, Eric},
  journal={Journal of the ACM (JACM)},
  year={2000}
}

@article{lusena2001complexity,
    title = {Nonapproximability Results for Partially Observable {M}arkov Decision Processes},
    author = {Lusena, Christopher and Goldsmith, Judy and Mundhenk, Martin},
    journal = {Journal of Artificial Intelligence Research},
    year = {2001}
}

@article{strehl2008analysis,
  title={An analysis of model-based interval estimation for {M}arkov decision processes},
  author={Strehl, Alexander L and Littman, Michael L},
  journal={Journal of Computer and System Sciences},
  year={2008},
}

@article{hochreiter1997lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  year={1997},
}

@article{williams1989learning,
  title={A learning algorithm for continually running fully recurrent neural networks},
  author={Williams, Ronald J and Zipser, David},
  journal={Neural computation},
  year={1989},
}

@article{deisenroth2013survey,
  title={A survey on policy search for robotics},
  author={Deisenroth, Marc Peter and Neumann, Gerhard and Peters, Jan and others},
  journal={Foundations and Trends in Robotics},
  year={2013}
}

@inproceedings{kocsis2006bandit,
  title={Bandit based {M}onte-{C}arlo planning},
  author={Kocsis, Levente and Szepesv{\'a}ri, Csaba},
  booktitle={European Conference on Machine Learning},
  year={2006}
}

@article{hallak2015contextual,
  title={Contextual {M}arkov decision processes},
  author={Hallak, Assaf and Di Castro, Dotan and Mannor, Shie},
  journal={arXiv preprint arXiv:1502.02259},
  year={2015}
}

@inproceedings{kwon2021latent,
  title={{RL} for latent {MDP}s: Regret guarantees and a lower bound},
  author={Kwon, Jeongyeol and Efroni, Yonathan and Caramanis, Constantine and Mannor, Shie},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{brown2020gpt,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Language Models are Few-Shot Learners},
 year = {2020}
}

@inproceedings{tarbouriech2020gosprl,
  title={A provably efficient sample collection strategy for reinforcement learning},
  author={Tarbouriech, Jean and Pirotta, Matteo and Valko, Michal and Lazaric, Alessandro},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{erhan2009difficulty,
  title={The difficulty of training deep architectures and the effect of unsupervised pre-training},
  author={Erhan, Dumitru and Manzagol, Pierre-Antoine and Bengio, Yoshua and Bengio, Samy and Vincent, Pascal},
  booktitle={Proceedings of the International Conference on Artificial Intelligence and Statistics},
  year={2009},
}

@inproceedings{erhan2010does,
  title={Why does unsupervised pre-training help deep learning?},
  author={Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal},
  booktitle={Proceedings of the International Conference on Artificial Intelligence and Statistics},
  year={2010},
}

@article{campos2021coverage,
  title={Coverage as a Principle for Discovering Transferable Behavior in Reinforcement Learning},
  author={Campos, V{\'\i}ctor and Sprechmann, Pablo and Hansen, Steven and Barreto, Andre and Kapturowski, Steven and Vitvitskyi, Alex and Badia, Adri{\`a} Puigdom{\`e}nech and Blundell, Charles},
  journal={arXiv preprint arXiv:2102.13515},
  year={2021}
}

@book{arora2009complexity,
  title={Computational complexity: a modern approach},
  author={Arora, Sanjeev and Barak, Boaz},
  year={2009},
  publisher={Cambridge University Press}
}

@inproceedings{cheung2019regret,
  title={Regret minimization for reinforcement learning with vectorial feedback and complex objectives},
  author={Cheung, Wang Chi},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@article{cheung2019exploration,
  title={Exploration-exploitation trade-off in reinforcement learning on online {M}arkov decision processes with global concave rewards},
  author={Cheung, Wang Chi},
  journal={arXiv preprint arXiv:1905.06466},
  year={2019}
}

@inproceedings{laskin2021urlb,
  title={{URLB}: Unsupervised Reinforcement Learning Benchmark},
  author={Laskin, Michael and Yarats, Denis and Liu, Hao and Lee, Kimin and Zhan, Albert and Lu, Kevin and Cang, Catherine and Pinto, Lerrel and Abbeel, Pieter},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2021}
}

@inproceedings{mutti2021unsupervised,
  title={Unsupervised Reinforcement Learning in Multiple Environments},
  author={Mutti, Mirco and Mancassola, Mattia and Restelli, Marcello},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2022}
}

@book{levin2017markov,
  title={Markov chains and mixing times},
  author={Levin, David A and Peres, Yuval},
  volume={107},
  year={2017},
  publisher={American Mathematical Society}
}

@inproceedings{akshay2013steady,
  title={The steady-state control problem for {M}arkov decision processes},
  author={Akshay, Sundararaman and Bertrand, Nathalie and Haddad, Serge and Helouet, Loic},
  booktitle={International Conference on Quantitative Evaluation of Systems},
  year={2013}
}

@article{laroche2022non,
  title={Non-{M}arkovian policies occupancy measures},
  author={Laroche, Romain and Combes, Remi Tachet des and Buckman, Jacob},
  journal={arXiv preprint arXiv:2205.13950},
  year={2022}
}

@article{nedergaard2022k,
  title={k-Means Maximum Entropy Exploration},
  author={Nedergaard, Alexander and Cook, Matthew},
  journal={arXiv preprint arXiv:2205.15623},
  year={2022}
}