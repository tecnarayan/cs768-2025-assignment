\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alizadeh et~al.(2018)Alizadeh, Fern{\'a}ndez-Marqu{\'e}s, Lane, and
  Gal]{alizadeh2018empirical}
Alizadeh, M., Fern{\'a}ndez-Marqu{\'e}s, J., Lane, N.~D., and Gal, Y.
\newblock An empirical study of binary neural networks' optimisation.
\newblock 2018.

\bibitem[Ambrosio \& Dal~Maso(1990)Ambrosio and Dal~Maso]{ambrosio1990general}
Ambrosio, L. and Dal~Maso, G.
\newblock A general chain rule for distributional derivatives.
\newblock \emph{Proceedings of the American Mathematical Society}, 108\penalty0
  (3):\penalty0 691--702, 1990.

\bibitem[Bethge et~al.(2020)Bethge, Bartz, Yang, Chen, and
  Meinel]{bethge2020meliusnet}
Bethge, J., Bartz, C., Yang, H., Chen, Y., and Meinel, C.
\newblock Meliusnet: Can binary neural networks achieve mobilenet-level
  accuracy?
\newblock \emph{arXiv preprint arXiv:2001.05936}, 2020.

\bibitem[Brais~Martinez(2020)]{martinez2020training}
Brais~Martinez, Jing~Yang, A. B. G.~T.
\newblock Training binary neural networks with real-to-binary convolutions.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Bulat et~al.(2019)Bulat, Tzimiropoulos, Kossaifi, and
  Pantic]{bulat2019improved}
Bulat, A., Tzimiropoulos, G., Kossaifi, J., and Pantic, M.
\newblock Improved training of binary networks for human pose estimation and
  image recognition.
\newblock \emph{arXiv preprint arXiv:1904.05868}, 2019.

\bibitem[Courbariaux et~al.(2016)Courbariaux, Hubara, Soudry, El-Yaniv, and
  Bengio]{courbariaux2016binarized}
Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized neural networks: Training deep neural networks with weights
  and activations constrained to+ 1 or-1.
\newblock \emph{arXiv preprint arXiv:1602.02830}, 2016.

\bibitem[Ding et~al.(2019)Ding, Chin, Liu, and
  Marculescu]{ding2019regularizing}
Ding, R., Chin, T.-W., Liu, Z., and Marculescu, D.
\newblock Regularizing activation distribution for training binarized deep
  networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  11408--11417, 2019.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adagrad}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Gu et~al.(2019)Gu, Li, Zhang, Han, Cao, Liu, and
  Doermann]{gu2019projection}
Gu, J., Li, C., Zhang, B., Han, J., Cao, X., Liu, J., and Doermann, D.
\newblock Projection convolutional neural networks for 1-bit cnns via discrete
  back propagation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pp.\  8344--8351, 2019.

\bibitem[Helwegen et~al.(2019)Helwegen, Widdicombe, Geiger, Liu, Cheng, and
  Nusselder]{helwegen2019latent}
Helwegen, K., Widdicombe, J., Geiger, L., Liu, Z., Cheng, K.-T., and Nusselder,
  R.
\newblock Latent weights do not exist: Rethinking binarized neural network
  optimization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  7531--7542, 2019.

\bibitem[Hou et~al.(2016)Hou, Yao, and Kwok]{hou2016loss}
Hou, L., Yao, Q., and Kwok, J.~T.
\newblock Loss-aware binarization of deep networks.
\newblock \emph{arXiv preprint arXiv:1611.01600}, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krogh \& Hertz(1992)Krogh and Hertz]{krogh1992simple}
Krogh, A. and Hertz, J.~A.
\newblock A simple weight decay can improve generalization.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  950--957, 1992.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6389--6399, 2018.

\bibitem[Lin et~al.(2017)Lin, Zhao, and Pan]{lin2017abcnet}
Lin, X., Zhao, C., and Pan, W.
\newblock Towards accurate binary convolutional neural network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  345--353, 2017.

\bibitem[Liu et~al.(2019)Liu, Ding, Xia, Zhang, Gu, Liu, Ji, and
  Doermann]{liu2019circulant}
Liu, C., Ding, W., Xia, X., Zhang, B., Gu, J., Liu, J., Ji, R., and Doermann,
  D.
\newblock Circulant binary convolutional networks: Enhancing the performance of
  1-bit dcnns with circulant back propagation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  2691--2699, 2019.

\bibitem[Liu et~al.(2018{\natexlab{a}})Liu, Luo, Wu, Yang, Liu, and
  Cheng]{liu2018bi_journal}
Liu, Z., Luo, W., Wu, B., Yang, X., Liu, W., and Cheng, K.-T.
\newblock Bi-real net: Binarizing deep network towards real-network
  performance.
\newblock \emph{International Journal of Computer Vision}, pp.\  1--18,
  2018{\natexlab{a}}.

\bibitem[Liu et~al.(2018{\natexlab{b}})Liu, Wu, Luo, Yang, Liu, and
  Cheng]{liu2018bi}
Liu, Z., Wu, B., Luo, W., Yang, X., Liu, W., and Cheng, K.-T.
\newblock Bi-real net: Enhancing the performance of 1-bit cnns with improved
  representational capability and advanced training algorithm.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pp.\  722--737, 2018{\natexlab{b}}.

\bibitem[Liu et~al.(2020)Liu, Shen, Savvides, and Cheng]{liu2020reactnet}
Liu, Z., Shen, Z., Savvides, M., and Cheng, K.-T.
\newblock Reactnet: Towards precise binary neural network with generalized
  activation functions.
\newblock \emph{ECCV}, 2020.

\bibitem[Luo et~al.(2019)Luo, Xiong, Liu, and Sun]{luo2019adabound}
Luo, L., Xiong, Y., Liu, Y., and Sun, X.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock \emph{arXiv preprint arXiv:1902.09843}, 2019.

\bibitem[Mishra et~al.(2017)Mishra, Nurvitadhi, Cook, and Marr]{mishra2017wrpn}
Mishra, A., Nurvitadhi, E., Cook, J.~J., and Marr, D.
\newblock Wrpn: wide reduced-precision networks.
\newblock \emph{arXiv preprint arXiv:1709.01134}, 2017.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8024--8035, 2019.

\bibitem[Phan et~al.(2020{\natexlab{a}})Phan, Huynh, He, Savvides, and
  Shen]{Hai_2020_WACV}
Phan, H., Huynh, D., He, Y., Savvides, M., and Shen, Z.
\newblock Mobinet: A mobile binary network for image classification.
\newblock In \emph{The IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, March 2020{\natexlab{a}}.

\bibitem[Phan et~al.(2020{\natexlab{b}})Phan, Liu, Huynh, Savvides, Cheng, and
  Shen]{phan2020binarizing}
Phan, H., Liu, Z., Huynh, D., Savvides, M., Cheng, K.-T., and Shen, Z.
\newblock Binarizing mobilenet via evolution-based searching.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  13420--13429, 2020{\natexlab{b}}.

\bibitem[Qin et~al.(2020)Qin, Gong, Liu, Shen, Wei, Yu, and
  Song]{qin2020forward}
Qin, H., Gong, R., Liu, X., Shen, M., Wei, Z., Yu, F., and Song, J.
\newblock Forward and backward information retention for accurate binary neural
  networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  2250--2259, 2020.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi]{rastegari2016xnor}
Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In \emph{European conference on computer vision}, pp.\  525--542.
  Springer, 2016.

\bibitem[Reddi et~al.(2019)Reddi, Kale, and Kumar]{reddi2019amsgrad}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of adam and beyond.
\newblock \emph{arXiv preprint arXiv:1904.09237}, 2019.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951stochastic}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pp.\  400--407, 1951.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International Journal of Computer Vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Shen et~al.(2021)Shen, Liu, Qin, Huang, Cheng, and
  Savvides]{shen2021s2}
Shen, Z., Liu, Z., Qin, J., Huang, L., Cheng, K.-T., and Savvides, M.
\newblock S2-bnn: Bridging the gap between self-supervised real and 1-bit
  neural networks via guided distribution calibration.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2021.

\bibitem[Tan \& Le(2019)Tan and Le]{tan2019efficientnet}
Tan, M. and Le, Q.~V.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock \emph{arXiv preprint arXiv:1905.11946}, 2019.

\bibitem[Tang et~al.(2017)Tang, Hua, and Wang]{tang2017train}
Tang, W., Hua, G., and Wang, L.
\newblock How to train a compact binary neural network with high accuracy?
\newblock In \emph{Thirty-First AAAI conference on artificial intelligence},
  2017.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{rmsprop}
Tieleman, T. and Hinton, G.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0
  (2):\penalty0 26--31, 2012.

\bibitem[Wang et~al.(2019)Wang, Lu, Tao, Zhou, and Tian]{wang2019ci-bcnn}
Wang, Z., Lu, J., Tao, C., Zhou, J., and Tian, Q.
\newblock Learning channel-wise interactions for binary convolutional neural
  networks.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2019.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{wilson2017marginal}
Wilson, A.~C., Roelofs, R., Stern, M., Srebro, N., and Recht, B.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4148--4158, 2017.

\bibitem[Wu et~al.(2016)Wu, Schuster, Chen, Le, Norouzi, Macherey, Krikun, Cao,
  Gao, Macherey, et~al.]{wu2016google}
Wu, Y., Schuster, M., Chen, Z., Le, Q.~V., Norouzi, M., Macherey, W., Krikun,
  M., Cao, Y., Gao, Q., Macherey, K., et~al.
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation.
\newblock \emph{arXiv preprint arXiv:1609.08144}, 2016.

\bibitem[Yang et~al.(2019)Yang, Huang, Wu, Zhang, Ma, Gambardella, Blott,
  Lavagno, Vissers, Wawrzynek, et~al.]{yang2019synetgy}
Yang, Y., Huang, Q., Wu, B., Zhang, T., Ma, L., Gambardella, G., Blott, M.,
  Lavagno, L., Vissers, K., Wawrzynek, J., et~al.
\newblock Synetgy: Algorithm-hardware co-design for convnet accelerators on
  embedded fpgas.
\newblock In \emph{Proceedings of the 2019 ACM/SIGDA International Symposium on
  Field-Programmable Gate Arrays}, pp.\  23--32, 2019.

\bibitem[Zeiler(2012)]{zeiler2012adadelta}
Zeiler, M.~D.
\newblock Adadelta: an adaptive learning rate method.
\newblock \emph{arXiv preprint arXiv:1212.5701}, 2012.

\bibitem[Zhang et~al.(2019)Zhang, Pan, Yao, Zhao, and Mei]{zhang2019dabnn}
Zhang, J., Pan, Y., Yao, T., Zhao, H., and Mei, T.
\newblock dabnn: A super fast inference framework for binary neural networks on
  arm devices.
\newblock In \emph{Proceedings of the 27th ACM International Conference on
  Multimedia}, pp.\  2272--2275, 2019.

\bibitem[Zhou et~al.(2016)Zhou, Wu, Ni, Zhou, Wen, and Zou]{zhou2016dorefa}
Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y.
\newblock Dorefa-net: Training low bitwidth convolutional neural networks with
  low bitwidth gradients.
\newblock \emph{arXiv preprint arXiv:1606.06160}, 2016.

\bibitem[Zhu et~al.(2019)Zhu, Dong, and Su]{zhu2019binary}
Zhu, S., Dong, X., and Su, H.
\newblock Binary ensemble neural network: More bits per network or more
  networks per bit?
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  4923--4932, 2019.

\bibitem[Zhuang et~al.(2018)Zhuang, Shen, Tan, Liu, and
  Reid]{zhuang2018towards}
Zhuang, B., Shen, C., Tan, M., Liu, L., and Reid, I.
\newblock Towards effective low-bitwidth convolutional neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  7920--7928, 2018.

\bibitem[Zhuang et~al.(2019)Zhuang, Shen, Tan, Liu, and
  Reid]{zhuang2019structured}
Zhuang, B., Shen, C., Tan, M., Liu, L., and Reid, I.
\newblock Structured binary neural networks for accurate image classification
  and semantic segmentation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  413--422, 2019.

\end{thebibliography}
