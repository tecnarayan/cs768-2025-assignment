\begin{thebibliography}{}

\bibitem[Badia et~al., 2020]{badia2020agent57}
Badia, A.~P., Piot, B., Kapturowski, S., Sprechmann, P., Vitvitskyi, A., Guo,
  D., and Blundell, C. (2020).
\newblock Agent57: Outperforming the atari human benchmark.
\newblock {\em arXiv preprint arXiv:2003.13350}.

\bibitem[Cobbe et~al., 2019]{cobbe2019leveraging}
Cobbe, K., Hesse, C., Hilton, J., and Schulman, J. (2019).
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock {\em arXiv preprint arXiv:1912.01588}.

\bibitem[Cobbe et~al., 2018]{cobbe2018quantifying}
Cobbe, K., Klimov, O., Hesse, C., Kim, T., and Schulman, J. (2018).
\newblock Quantifying generalization in reinforcement learning.
\newblock {\em arXiv preprint arXiv:1812.02341}.

\bibitem[DeVries and Taylor, 2017]{devries2017improved}
DeVries, T. and Taylor, G.~W. (2017).
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock {\em arXiv preprint arXiv:1708.04552}.

\bibitem[Espeholt et~al., 2018]{espeholt2018impala}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., et~al. (2018).
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock {\em arXiv preprint arXiv:1802.01561}.

\bibitem[Farebrother et~al., 2018]{farebrother2018generalization}
Farebrother, J., Machado, M.~C., and Bowling, M. (2018).
\newblock Generalization and regularization in dqn.
\newblock {\em arXiv preprint arXiv:1810.00123}.

\bibitem[Gamrian and Goldberg, 2018]{gamrian2018transfer}
Gamrian, S. and Goldberg, Y. (2018).
\newblock Transfer learning for related reinforcement learning tasks via
  image-to-image translation.
\newblock {\em arXiv preprint arXiv:1806.07377}.

\bibitem[Hessel et~al., 2018]{hessel2018rainbow}
Hessel, M., Modayil, J., Van~Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
  W., Horgan, D., Piot, B., Azar, M., and Silver, D. (2018).
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In {\em Thirty-Second AAAI Conference on Artificial Intelligence}.

\bibitem[Igl et~al., 2019]{igl2019generalization}
Igl, M., Ciosek, K., Li, Y., Tschiatschek, S., Zhang, C., Devlin, S., and
  Hofmann, K. (2019).
\newblock Generalization in reinforcement learning with selective noise
  injection and information bottleneck.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  13956--13968.

\bibitem[Ioffe and Szegedy, 2015]{ioffe2015batch}
Ioffe, S. and Szegedy, C. (2015).
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em arXiv preprint arXiv:1502.03167}.

\bibitem[Kostrikov et~al., 2020]{kostrikov2020image}
Kostrikov, I., Yarats, D., and Fergus, R. (2020).
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock {\em arXiv preprint arXiv:2004.13649}.

\bibitem[Laskin et~al., 2020]{laskin2020reinforcement}
Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A.
  (2020).
\newblock Reinforcement learning with augmented data.
\newblock {\em arXiv preprint arXiv:2004.14990}.

\bibitem[Lee et~al., 2020]{lee2020network}
Lee, K., Lee, K., Shin, J., and Lee, H. (2020).
\newblock Network randomization: A simple technique for generalization in deep
  reinforcement learning.
\newblock In {\em International Conference on Learning Representations.
  https://openreview. net/forum}.

\bibitem[Lillicrap et~al., 2015]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D. (2015).
\newblock Continuous control with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1509.02971}.

\bibitem[Mnih et~al., 2015]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
  (2015).
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529--533.

\bibitem[Packer et~al., 2018]{packer2018assessing}
Packer, C., Gao, K., Kos, J., Kr{\"a}henb{\"u}hl, P., Koltun, V., and Song, D.
  (2018).
\newblock Assessing generalization in deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1810.12282}.

\bibitem[Raileanu et~al., 2020]{raileanu2020automatic}
Raileanu, R., Goldstein, M., Yarats, D., Kostrikov, I., and Fergus, R. (2020).
\newblock Automatic data augmentation for generalization in deep reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2006.12862}.

\bibitem[Schulman et~al., 2017]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017).
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}.

\bibitem[Silver et~al., 2016]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al. (2016).
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484.

\bibitem[Silver et~al., 2017a]{silver2017mastering_b}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al. (2017a).
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock {\em arXiv preprint arXiv:1712.01815}.

\bibitem[Silver et~al., 2017b]{silver2017mastering_a}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al. (2017b).
\newblock Mastering the game of go without human knowledge.
\newblock {\em Nature}, 550(7676):354--359.

\bibitem[Singh et~al., 2019]{singh2019end}
Singh, A., Yang, L., Hartikainen, K., Finn, C., and Levine, S. (2019).
\newblock End-to-end robotic reinforcement learning without reward engineering.
\newblock {\em arXiv preprint arXiv:1904.07854}.

\bibitem[Song et~al., 2019]{song2019observational}
Song, X., Jiang, Y., Du, Y., and Neyshabur, B. (2019).
\newblock Observational overfitting in reinforcement learning.
\newblock {\em arXiv preprint arXiv:1912.02975}.

\bibitem[Srivastava et~al., 2014]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R. (2014).
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958.

\bibitem[Sutton et~al., 2000]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y. (2000).
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in neural information processing systems}, pages
  1057--1063.

\bibitem[Tobin et~al., 2017]{tobin2017domain}
Tobin, J., Fong, R., Ray, A., Schneider, J., Zaremba, W., and Abbeel, P.
  (2017).
\newblock Domain randomization for transferring deep neural networks from
  simulation to the real world.
\newblock In {\em 2017 IEEE/RSJ international conference on intelligent robots
  and systems (IROS)}, pages 23--30. IEEE.

\bibitem[Vinyals et~al., 2017]{vinyals2017starcraft}
Vinyals, O., Ewalds, T., Bartunov, S., Georgiev, P., Vezhnevets, A.~S., Yeo,
  M., Makhzani, A., K{\"u}ttler, H., Agapiou, J., Schrittwieser, J., et~al.
  (2017).
\newblock Starcraft ii: A new challenge for reinforcement learning.
\newblock {\em arXiv preprint arXiv:1708.04782}.

\bibitem[Wang et~al., 2016]{wang2016learning}
Wang, J.~X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J.~Z., Munos,
  R., Blundell, C., Kumaran, D., and Botvinick, M. (2016).
\newblock Learning to reinforcement learn.
\newblock {\em arXiv preprint arXiv:1611.05763}.

\bibitem[Zhang et~al., 2018a]{zhang2018dissection}
Zhang, A., Ballas, N., and Pineau, J. (2018a).
\newblock A dissection of overfitting and generalization in continuous
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1806.07937}.

\bibitem[Zhang et~al., 2018b]{zhang2018natural}
Zhang, A., Wu, Y., and Pineau, J. (2018b).
\newblock Natural environment benchmarks for reinforcement learning.
\newblock {\em arXiv preprint arXiv:1811.06032}.

\bibitem[Zhang et~al., 2018c]{zhang2018study}
Zhang, C., Vinyals, O., Munos, R., and Bengio, S. (2018c).
\newblock A study on overfitting in deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1804.06893}.

\bibitem[Zhang et~al., 2017]{zhang2017mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D. (2017).
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv preprint arXiv:1710.09412}.

\end{thebibliography}
