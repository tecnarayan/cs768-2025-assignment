@article{Kohler2001NonparametricRE,
  title={Nonparametric regression estimation using penalized least squares},
  author={Michael Kohler and Adam Krzyżak},
  journal={IEEE Trans. Inf. Theory},
  year={2001},
  volume={47},
  pages={3054-3059}
}

@article{Cucker2001OnTM,
  title={On the mathematical foundations of learning},
  author={Felipe Cucker and Stephen Smale},
  journal={Bulletin of the American Mathematical Society},
  year={2001},
  volume={39},
  pages={1-49}
}

@article{Caponnetto2007OptimalRF,
  title={Optimal Rates for the Regularized Least-Squares Algorithm},
  author={Andrea Caponnetto and Ernesto de Vito},
  journal={Foundations of Computational Mathematics},
  year={2007},
  volume={7},
  pages={331-368}
}

@inproceedings{Steinwart2009OptimalRF,
  title={Optimal Rates for Regularized Least Squares Regression},
  author={Ingo Steinwart and Don R. Hush and Clint Scovel},
  booktitle={Annual Conference Computational Learning Theory},
  year={2009}
}

@inproceedings{Steinwart2008SupportVM,
  title={Support Vector Machines},
  author={Ingo Steinwart and Andreas Christmann},
  booktitle={Information Science and Statistics},
  year={2008}
}

@article{fischer2020_SobolevNorm,
  title = {Sobolev Norm Learning Rates for Regularized Least-Squares Algorithms},
  author = {Fischer, Simon-Raphael and Steinwart, Ingo},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  pages = {205:1-205:38},
  arxivid = {1702.07254},
  keywords = {KRR}
}

@article{rastogi2017_OptimalRates,
  title = {Optimal Rates for the Regularized Learning Algorithms under General Source Condition},
  author = {Rastogi, Abhishake and Sampath, Sivananthan},
  year = {2017},
  journal = {Frontiers in Applied Mathematics and Statistics},
  volume = {3},
  arxivid = {1611.01900}
}

@article{rosasco2005_SpectralMethods,
  title = {Spectral Methods for Regularization in Learning Theory},
  author = {Rosasco, Lorenzo and De Vito, Ernesto and Verri, Alessandro},
  year = {2005},
  journal = {DISI, Universita degli Studi di Genova, Italy, Technical Report DISI-TR-05-18}
}

@article{bauer2007_RegularizationAlgorithms,
  title = {On Regularization Algorithms in Learning Theory},
  author = {Bauer, F. and Pereverzyev, S. and Rosasco, L.},
  year = {2007},
  journal = {Journal of complexity},
  volume = {23},
  number = {1},
  pages = {52--72},
  publisher = {{Elsevier}},
  keywords = {saturation,Spectral algorithm}
}

@article{gerfo2008_SpectralAlgorithms,
  title = {Spectral Algorithms for Supervised Learning},
  author = {Gerfo, L. Lo and Rosasco, Lorenzo and Odone, Francesca and Vito, E. De and Verri, Alessandro},
  year = {2008},
  journal = {Neural Computation},
  volume = {20},
  number = {7},
  pages = {1873--1897},
  publisher = {{MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info \ldots}},
  keywords = {Spectral algorithm}
}

@article{mendelson2010_RegularizationKernel,
  title = {Regularization in Kernel Learning},
  author = {Mendelson, Shahar and Neeman, Joseph},
  year = {2010},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {38},
  number = {1},
  pages = {526--565},
  publisher = {{Institute of Mathematical Statistics}},
  keywords = {Empirical processes,least-squares,Model selection,regression,Regulation,‎reproducing kernel Hilbert ‎space}
}

@inproceedings{Caponnetto2006OptimalRF,
  title={Optimal Rates for Regularization Operators in Learning Theory},
  author={Andrea Caponnetto},
  year={2006}
}

@article{blanchard2018_OptimalRates,
  title = {Optimal Rates for Regularization of Statistical Inverse Learning Problems},
  author = {Blanchard, G. and M{\"u}cke, Nicole},
  year = {2018},
  journal = {Foundations of Computational Mathematics},
  volume = {18},
  pages = {971--1013},
  arxivid = {1604.04054}
}

@article{dicker2017_KernelRidge,
  title = {Kernel Ridge vs. Principal Component Regression: {{Minimax}} Bounds and the Qualification of Regularization Operators},
  author = {Dicker, L. and Foster, Dean Phillips and Hsu, Daniel J.},
  year = {2017},
  journal = {Electronic Journal of Statistics},
  volume = {11},
  pages = {1022--1047},
  keywords = {KRR,saturation}
}

@article{lin2018_OptimalRates,
  title = {Optimal Rates for Spectral Algorithms with Least-Squares Regression over {{Hilbert}} Spaces},
  author = {Lin, Junhong and Rudi, Alessandro and Rosasco, L. and Cevher, V.},
  year = {2018},
  journal = {Applied and Computational Harmonic Analysis},
  volume = {48},
  pages = {868--890},
  arxivid = {1801.06720},
  keywords = {Spectral algorithm}
}

@article{lin2020_OptimalConvergence,
  title = {Optimal Convergence for Distributed Learning with Stochastic Gradient Methods and Spectral Algorithms.},
  author = {Lin, Junhong and Cevher, Volkan},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  pages = {147--1}
}

@inproceedings{steinwart2009_OptimalRates,
  title = {Optimal Rates for Regularized Least Squares Regression},
  booktitle = {{{COLT}}},
  author = {Steinwart, Ingo and Hush, D. and Scovel, C.},
  year = {2009},
  pages = {79--93},
  keywords = {Empirical processes}
}

@article{PillaudVivien2018StatisticalOO,
  title={Statistical Optimality of Stochastic Gradient Descent on Hard Learning Problems through Multiple Passes},
  author={Loucas Pillaud-Vivien and Alessandro Rudi and Francis R. Bach},
  journal={ArXiv},
  year={2018},
  volume={abs/1805.10074}
}

@article{Celisse2020AnalyzingTD,
  title={Analyzing the discrepancy principle for kernelized spectral filter learning algorithms},
  author={Alain Celisse and Martin Wahl},
  journal={J. Mach. Learn. Res.},
  year={2020},
  volume={22},
  pages={76:1-76:59}
}

@article{Liu2022StatisticalOO,
  title={Statistical Optimality of Divide and Conquer Kernel-based Functional Linear Regression},
  author={Jiading Liu and Lei Shi},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.10968}
}

@article{steinwart2012_MercerTheorem,
  title = {Mercer's Theorem on General Domains: {{On}} the Interaction between Measures, Kernels, and {{RKHSs}}},
  shorttitle = {Mercer's Theorem on General Domains},
  author = {Steinwart, Ingo and Scovel, C.},
  year = {2012},
  journal = {Constructive Approximation},
  volume = {35},
  number = {3},
  pages = {363--417},
  publisher = {{Springer}},
  keywords = {Kernel}
}

@inproceedings{Cui2021GeneralizationER,
  title={Generalization Error Rates in Kernel Regression: The Crossover from the Noiseless to Noisy Regime},
  author={Hugo Cui and Bruno Loureiro and Florent Krzakala and Lenka Zdeborov'a},
  booktitle={NeurIPS},
  year={2021}
}

@inproceedings{Bordelon2020SpectrumDL,
  title={Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks},
  author={Blake Bordelon and Abdulkadir Canatar and Cengiz Pehlevan},
  booktitle={ICML},
  year={2020}
}

@article{Spigler2020AsymptoticLC,
  title={Asymptotic learning curves of kernel methods: empirical data versus teacher–student paradigm},
  author={Stefano Spigler and Mario Geiger and Matthieu Wyart},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  year={2020},
  volume={2020}
}

@article{Jun2019KernelTR,
  title={Kernel Truncated Randomized Ridge Regression: Optimal Rates and Low Noise Acceleration},
  author={Kwang-Sung Jun and Ashok Cutkosky and Francesco Orabona},
  journal={ArXiv},
  year={2019},
  volume={abs/1905.10680}
}

@book{adams2003_SobolevSpaces,
  title = {Sobolev Spaces},
  author = {Adams, Robert A and Fournier, John JF},
  year = {2003},
  publisher = {{Elsevier}}
}

@book{sawano2018theory,
    title = {Theory of Besov spaces},
    author = {Sawano, Yoshihiro},
    volume = {56},
    year = {2018},
    publisher = {Springer}
}

@book{adams1975sobolev,
  title={Sobolev Spaces. Adams},
  author={Adams, R.A.},
  series={Pure and applied mathematics},
  url={https://books.google.co.uk/books?id=JxzpSAAACAAJ},
  year={1975},
  publisher={Academic Press}
}

@inbook{wendland_2004, place={Cambridge}, series={Cambridge Monographs on Applied and Computational Mathematics}, title={Native spaces}, DOI={10.1017/CBO9780511617539.011}, booktitle={Scattered Data Approximation}, publisher={Cambridge University Press}, author={Wendland, Holger}, year={2004}, pages={133–171}, collection={Cambridge Monographs on Applied and Computational Mathematics}}

@article{Smale2007LearningTE,
  title={Learning Theory Estimates via Integral Operators and Their Approximations},
  author={Stephen Smale and Ding-Xuan Zhou},
  journal={Constructive Approximation},
  year={2007},
  volume={26},
  pages={153-172}
}

@book{tsybakov2009_IntroductionNonparametric,
  title = {Introduction to Nonparametric Estimation},
  author = {Tsybakov, Alexandre B.},
  year = {2009},
  series = {Springer Series in Statistics},
  edition = {1st},
  publisher = {{Springer}},
  address = {{New York ; London}},
  langid = {english},
  lccn = {QA278.8 .T79 2009},
  keywords = {Estimation theory,Nonparametric statistics},
  annotation = {OCLC: ocn300399286}
}

@article{Lian2021DistributedLF,
  title={Distributed learning for sketched kernel regression},
  author={Heng Lian and Jiamin Liu and Zengyan Fan},
  journal={Neural networks : the official journal of the International Neural Network Society},
  year={2021},
  volume={143},
  pages={
          368-376
        }
}

@article{Guo2017LearningTO,
  title={Learning theory of distributed spectral algorithms},
  author={Zheng-Chu Guo and Shaobo Lin and Ding-Xuan Zhou},
  journal={Inverse Problems},
  year={2017},
  volume={33}
}

@inproceedings{Talwai2022OptimalLR,
  title={Optimal Learning Rates for Regularized Least-Squares with a Fourier Capacity Condition},
  author={Prem M. Talwai and David Simchi-Levi},
  year={2022}
}

@article{Li2022OptimalRF,
  title={Optimal Rates for Regularized Conditional Mean Embedding Learning},
  author={Zhu Li and Dimitri Meunier and Mattes Mollenhauer and Arthur Gretton},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.01711}
}

@book{wainwright2019_HighdimensionalStatistics,
  title = {High-Dimensional Statistics: {{A}} Non-Asymptotic Viewpoint},
  author = {Wainwright, Martin J.},
  year = {2019},
  series = {Cambridge {{Series}} in {{Statistical}} and {{Probabilistic Mathematics}}},
  publisher = {{Cambridge University Press}}
}

@article{ThomasAgnan1996ComputingAF,
  title={Computing a family of reproducing kernels for statistical applications},
  author={Christine Thomas-Agnan},
  journal={Numerical Algorithms},
  year={1996},
  volume={13},
  pages={21-32}
}

@article{Jin2022MinimaxOK,
  title={Minimax Optimal Kernel Operator Learning via Multilevel Training},
  author={Jikai Jin and Yiping Lu and Jos{\'e} H. Blanchet and Lexing Ying},
  journal={ArXiv},
  year={2022},
  volume={abs/2209.14430}
}

@inproceedings{li2023saturation,
  title={On the Saturation Effect of Kernel Ridge Regression},
  author={Li, Yicheng and Zhang, Haobo and Lin, Qian},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2023}
}

@book{edmunds_triebel_1996, place = {Cambridge}, series = {Cambridge Tracts in Mathematics}, title = {Function Spaces, Entropy Numbers, Differential Operators}, DOI = {10.1017/CBO9780511662201}, publisher = {Cambridge University Press}, author = {Edmunds, D. E. and Triebel, H.}, year = {1996}, collection = {Cambridge Tracts in Mathematics} }

@article{Jin2021LearningCF,
  title={Learning curves for Gaussian process regression with power-law priors and targets},
  author={Hui Jin and Pradeep Kr. Banerjee and Guido Mont{\'u}far},
  journal={ArXiv},
  year={2021},
  volume={abs/2110.12231}
}

@article{kanagawa2018_GaussianProcesses,
  title = {Gaussian Processes and Kernel Methods: {{A}} Review on Connections and Equivalences},
  shorttitle = {Gaussian Processes and Kernel Methods},
  author = {Kanagawa, Motonobu and Hennig, Philipp and Sejdinovic, Dino and Sriperumbudur, Bharath K.},
  year = {2018},
  journal = {arXiv preprint arXiv:1807.02582},
  eprint = {1807.02582},
  archiveprefix = {arxiv}
}

@article{Jacot_NTK_2018,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}