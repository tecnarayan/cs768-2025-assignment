\begin{thebibliography}{}

\bibitem[Ahn et~al., 2024]{ahn2024transformers}
Ahn, K., Cheng, X., Daneshmand, H., and Sra, S. (2024).
\newblock Transformers learn to implement preconditioned gradient descent for in-context learning.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Aky{\"u}rek et~al., 2022]{akyurek2022learning}
Aky{\"u}rek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D. (2022).
\newblock What learning algorithm is in-context learning? investigations with linear models.
\newblock {\em arXiv preprint arXiv:2211.15661}.

\bibitem[Bai et~al., 2023]{bai2023transformers}
Bai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S. (2023).
\newblock Transformers as statisticians: Provable in-context learning with in-context algorithm selection.
\newblock {\em arXiv preprint arXiv:2306.04637}.

\bibitem[Belkin, 2024]{belkin2024necessity}
Belkin, M. (2024).
\newblock The necessity of machine learning theory in mitigating ai risk.
\newblock {\em ACM/JMS Journal of Data Science}.

\bibitem[Bommasani et~al., 2021]{bommasani2021opportunities}
Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al. (2021).
\newblock On the opportunities and risks of foundation models.
\newblock {\em arXiv preprint arXiv:2108.07258}.

\bibitem[Brown et~al., 2020]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al. (2020).
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901.

\bibitem[Chen and Li, 2024]{chen2024provably}
Chen, S. and Li, Y. (2024).
\newblock Provably learning a multi-head attention layer.
\newblock {\em arXiv preprint arXiv:2402.04084}.

\bibitem[Chen et~al., 2024]{chen2024training}
Chen, S., Sheen, H., Wang, T., and Yang, Z. (2024).
\newblock Training dynamics of multi-head softmax attention for in-context learning: Emergence, convergence, and optimality.
\newblock {\em arXiv preprint arXiv:2402.19442}.

\bibitem[Cheng et~al., 2023]{cheng2023transformers}
Cheng, X., Chen, Y., and Sra, S. (2023).
\newblock Transformers implement functional gradient descent to learn non-linear functions in context.
\newblock {\em arXiv preprint arXiv:2312.06528}.

\bibitem[Cui et~al., 2024]{cui2024superiority}
Cui, Y., Ren, J., He, P., Tang, J., and Xing, Y. (2024).
\newblock Superiority of multi-head attention in in-context linear regression.
\newblock {\em arXiv preprint arXiv:2401.17426}.

\bibitem[Deora et~al., 2023]{deora2023optimization}
Deora, P., Ghaderi, R., Taheri, H., and Thrampoulidis, C. (2023).
\newblock On the optimization and generalization of multi-head attention.
\newblock {\em arXiv preprint arXiv:2310.12680}.

\bibitem[Devlin et~al., 2018]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018).
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}.

\bibitem[Dosovitskiy et~al., 2020]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al. (2020).
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock {\em arXiv preprint arXiv:2010.11929}.

\bibitem[Dryer, 1991]{dryer1991svo}
Dryer, M.~S. (1991).
\newblock Svo languages and the ov: Vo typology1.
\newblock {\em Journal of linguistics}, 27(2):443--482.

\bibitem[Edelman et~al., 2022]{edelman2022inductive}
Edelman, B.~L., Goel, S., Kakade, S., and Zhang, C. (2022).
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In {\em International Conference on Machine Learning}, pages 5793--5831. PMLR.

\bibitem[Frei et~al., 2022]{frei2022implicit}
Frei, S., Vardi, G., Bartlett, P.~L., Srebro, N., and Hu, W. (2022).
\newblock Implicit bias in leaky relu networks trained on high-dimensional data.
\newblock {\em arXiv preprint arXiv:2210.07082}.

\bibitem[Fu et~al., 2023]{fu2023transformers}
Fu, D., Chen, T.-Q., Jia, R., and Sharan, V. (2023).
\newblock Transformers learn higher-order optimization methods for in-context learning: A study with linear models.
\newblock {\em arXiv preprint arXiv:2310.17086}.

\bibitem[Giannou et~al., 2023]{giannou2023looped}
Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J.~D., and Papailiopoulos, D. (2023).
\newblock Looped transformers as programmable computers.
\newblock In {\em International Conference on Machine Learning}, pages 11398--11442. PMLR.

\bibitem[Huang et~al., 2023]{huang2023context}
Huang, Y., Cheng, Y., and Liang, Y. (2023).
\newblock In-context convergence of transformers.
\newblock {\em arXiv preprint arXiv:2310.05249}.

\bibitem[Ji et~al., 2021]{ji2021fast}
Ji, Z., Srebro, N., and Telgarsky, M. (2021).
\newblock Fast margin maximization via dual acceleration.
\newblock In {\em International Conference on Machine Learning}, pages 4860--4869. PMLR.

\bibitem[Ji and Telgarsky, 2021]{ji2021characterizing}
Ji, Z. and Telgarsky, M. (2021).
\newblock Characterizing the implicit bias via a primal-dual analysis.
\newblock In {\em Algorithmic Learning Theory}, pages 772--804. PMLR.

\bibitem[Kim and Suzuki, 2024]{kim2024transformers}
Kim, J. and Suzuki, T. (2024).
\newblock Transformers learn nonlinear features in context: Nonconvex mean-field dynamics on the attention landscape.
\newblock {\em arXiv preprint arXiv:2402.01258}.

\bibitem[Kou et~al., 2024]{kou2024implicit}
Kou, Y., Chen, Z., and Gu, Q. (2024).
\newblock Implicit bias of gradient descent for two-layer relu and leaky relu networks on nearly-orthogonal data.
\newblock {\em Advances in Neural Information Processing Systems}, 36.

\bibitem[Li et~al., 2023]{li2023theoretical}
Li, H., Wang, M., Liu, S., and Chen, P.-Y. (2023).
\newblock A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity.
\newblock {\em arXiv preprint arXiv:2302.06015}.

\bibitem[Li et~al., 2024]{li2024mechanics}
Li, Y., Huang, Y., Ildiz, M.~E., Rawat, A.~S., and Oymak, S. (2024).
\newblock Mechanics of next token prediction with self-attention.
\newblock In {\em International Conference on Artificial Intelligence and Statistics}, pages 685--693. PMLR.

\bibitem[Lin et~al., 2023]{lin2023transformers}
Lin, L., Bai, Y., and Mei, S. (2023).
\newblock Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining.
\newblock {\em arXiv preprint arXiv:2310.08566}.

\bibitem[Mahankali et~al., 2023]{mahankali2023one}
Mahankali, A., Hashimoto, T.~B., and Ma, T. (2023).
\newblock One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention.
\newblock {\em arXiv preprint arXiv:2307.03576}.

\bibitem[Nacson et~al., 2019]{nacson2019convergence}
Nacson, M.~S., Lee, J., Gunasekar, S., Savarese, P. H.~P., Srebro, N., and Soudry, D. (2019).
\newblock Convergence of gradient descent on separable data.
\newblock In {\em The 22nd International Conference on Artificial Intelligence and Statistics}, pages 3420--3428. PMLR.

\bibitem[Nichani et~al., 2024]{nichani2024transformers}
Nichani, E., Damian, A., and Lee, J.~D. (2024).
\newblock How transformers learn causal structure with gradient descent.
\newblock {\em arXiv preprint arXiv:2402.14735}.

\bibitem[Phuong and Lampert, 2020]{phuong2020inductive}
Phuong, M. and Lampert, C.~H. (2020).
\newblock The inductive bias of relu networks on orthogonally separable data.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Soudry et~al., 2018]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N. (2018).
\newblock The implicit bias of gradient descent on separable data.
\newblock {\em Journal of Machine Learning Research}, 19(70):1--57.

\bibitem[Tarzanagh et~al., 2023a]{tarzanagh2023transformers}
Tarzanagh, D.~A., Li, Y., Thrampoulidis, C., and Oymak, S. (2023a).
\newblock Transformers as support vector machines.
\newblock {\em arXiv preprint arXiv:2308.16898}.

\bibitem[Tarzanagh et~al., 2023b]{tarzanagh2023max}
Tarzanagh, D.~A., Li, Y., Zhang, X., and Oymak, S. (2023b).
\newblock Max-margin token selection in attention mechanism.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}.

\bibitem[Thrampoulidis, 2024]{thrampoulidis2024implicit}
Thrampoulidis, C. (2024).
\newblock Implicit bias of next-token prediction.

\bibitem[Tian et~al., 2023a]{tian2023scan}
Tian, Y., Wang, Y., Chen, B., and Du, S.~S. (2023a).
\newblock Scan and snap: Understanding training dynamics and token composition in 1-layer transformer.
\newblock {\em Advances in Neural Information Processing Systems}, 36:71911--71947.

\bibitem[Tian et~al., 2023b]{tian2023joma}
Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. (2023b).
\newblock Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention.
\newblock {\em arXiv preprint arXiv:2310.00535}.

\bibitem[Tsai et~al., 2019]{tsai2019multimodal}
Tsai, Y.-H.~H., Bai, S., Liang, P.~P., Kolter, J.~Z., Morency, L.-P., and Salakhutdinov, R. (2019).
\newblock Multimodal transformer for unaligned multimodal language sequences.
\newblock In {\em Proceedings of the conference. Association for computational linguistics. Meeting}, volume 2019, page 6558. NIH Public Access.

\bibitem[Vardi, 2023]{vardi2023implicit}
Vardi, G. (2023).
\newblock On the implicit bias in deep-learning algorithms.
\newblock {\em Communications of the ACM}, 66(6):86--93.

\bibitem[Vasudeva et~al., 2024]{vasudeva2024implicit}
Vasudeva, B., Deora, P., and Thrampoulidis, C. (2024).
\newblock Implicit bias and fast convergence rates for self-attention.
\newblock {\em arXiv preprint arXiv:2402.05738}.

\bibitem[Vaswani et~al., 2017]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30.

\bibitem[Von~Oswald et~al., 2023]{von2023transformers}
Von~Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. (2023).
\newblock Transformers learn in-context by gradient descent.
\newblock In {\em International Conference on Machine Learning}, pages 35151--35174. PMLR.

\bibitem[Yannakakis, 1982]{yannakakis1982complexity}
Yannakakis, M. (1982).
\newblock The complexity of the partial order dimension problem.
\newblock {\em SIAM Journal on Algebraic Discrete Methods}, 3(3):351--358.

\bibitem[Zhang et~al., 2023]{zhang2023trained}
Zhang, R., Frei, S., and Bartlett, P.~L. (2023).
\newblock Trained transformers learn linear models in-context.
\newblock {\em arXiv preprint arXiv:2306.09927}.

\end{thebibliography}
