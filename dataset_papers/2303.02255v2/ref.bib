% ALPHABETICAL ORDER

@misc{abramowitz1988handbook,
  title={Handbook of mathematical functions with formulas, graphs, and mathematical tables},
  author={Abramowitz, Milton and Stegun, Irene A and Romer, Robert H},
  year={1988},
  publisher={American Association of Physics Teachers}
}

@article{albuquerque2019generalizing,
  title={Generalizing to unseen domains via distribution matching},
  author={Albuquerque, Isabela and Monteiro, Jo{\~a}o and Darvishi, Mohammad and Falk, Tiago H and Mitliagkas, Ioannis},
  journal={arXiv preprint arXiv:1911.00804},
  year={2019}
}

@article{arjovsky2019invariant,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}


@article{aybat2019universally,
  title={A universally optimal multistage accelerated stochastic gradient method},
  author={Aybat, Necdet Serhat and Fallah, Alireza and Gurbuzbalaban, Mert and Ozdaglar, Asuman},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={8525--8536},
  year={2019}
}

@ARTICLE{Bach14,
  title =	{Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic regression},
  author =	"Francis R. Bach",
  journal =	"Journal of Machine Learning Research (JMLR)",
  year = 	"2014",
  volume =	{volume 15},
}

@article{bach2013non,
  title={Non-strongly-convex smooth stochastic approximation with convergence rate $o(1/n)$},
  author={Bach, Francis and Moulines, Eric},
  journal={Advances in neural information processing systems},
  volume={26},
  pages={773--781},
  year={2013}
}
@article{bahadur1967rates,
  title={Rates of convergence of estimates and test statistics},
  author={Bahadur, Raghu Raj},
  journal={The Annals of Mathematical Statistics},
  volume={38},
  number={2},
  pages={303--324},
  year={1967},
  publisher={JSTOR}
}
@book{bahadur1971some,
  title={Some limit theorems in statistics},
  author={Bahadur, Raghu Raj},
  year={1971},
  publisher={SIAM}
}
@article{bartlett2020benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  year={2020},
  publisher={National Acad Sciences}
}
@article{belkin2020two,
  title={Two models of double descent for weak features},
  author={Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={4},
  pages={1167--1180},
  year={2020},
  publisher={SIAM}
}
@article{ben2010theory,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine learning},
  volume={79},
  number={1},
  pages={151--175},
  year={2010},
  publisher={Springer}
}
@article{berthier2020tight,
  title={Tight Nonparametric Convergence Rates for Stochastic Gradient Descent under the Noiseless Linear Model},
  author={Berthier, Rapha{\"e}l and Bach, Francis and Gaillard, Pierre},
  journal={arXiv preprint arXiv:2006.08212},
  year={2020}
}
@book{bishop2006pattern,
  title={Pattern recognition and machine learning},
  author={Bishop, Christopher M and Nasrabadi, Nasser M},
  volume={4},
  year={2006},
  publisher={Springer}
}
@inproceedings{brutzkus2017globally,
  title={Globally optimal gradient descent for a convnet with gaussian inputs},
  author={Brutzkus, Alon and Globerson, Amir},
  booktitle={International conference on machine learning},
  pages={605--614},
  year={2017},
  organization={PMLR}
}
@article{bubeck2014theory,
  title={Theory of convex optimization for machine learning},
  author={Bubeck, S{\'e}bastien},
  journal={arXiv preprint arXiv:1405.4980},
  volume={15},
  year={2014},
  publisher={Citeseer}
}
@article{canatar2021out,
  title={Out-of-Distribution Generalization in Kernel Regression},
  author={Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{chatterji2020finite,
  title={Finite-sample analysis of interpolating linear classifiers in the overparameterized regime},
  author={Chatterji, Niladri S and Long, Philip M},
  journal={arXiv preprint arXiv:2004.12019},
  year={2020}
}
@article{chen2020dimension,
  title={Dimension Independent Generalization Error with Regularized Online Optimization},
  author={Chen, Xi and Liu, Qiang and Tong, Xin T},
  journal={arXiv preprint arXiv:2003.11196},
  year={2020}
}
@article{cheng2022dimension,
  title={Dimension free ridge regression},
  author={Cheng, Chen and Montanari, Andrea},
  journal={arXiv preprint arXiv:2210.08571},
  year={2022}
}
@article{cho2009kernel,
  title={Kernel methods for deep learning},
  author={Cho, Youngmin and Saul, Lawrence},
  journal={Advances in neural information processing systems},
  volume={22},
  year={2009}
}
@article{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2103.00065},
  year={2021}
}
@article{cortes2010learning,
  title={Learning bounds for importance weighting},
  author={Cortes, Corinna and Mansour, Yishay and Mohri, Mehryar},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}
@article{cortes2014domain,
  title={Domain adaptation and sample bias correction theory and algorithm for regression},
  author={Cortes, Corinna and Mohri, Mehryar},
  journal={Theoretical Computer Science},
  volume={519},
  pages={103--126},
  year={2014},
  publisher={Elsevier}
}
@article{cortes2019adaptation,
  title={Adaptation based on generalized discrepancy},
  author={Cortes, Corinna and Mohri, Mehryar and Medina, Andr{\'e}s Munoz},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1--30},
  year={2019},
  publisher={JMLR. org}
}
@inproceedings{david2010impossibility,
  title={Impossibility theorems for domain adaptation},
  author={David, Shai Ben and Lu, Tyler and Luu, Teresa and P{\'a}l, D{\'a}vid},
  booktitle={Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages={129--136},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}


@article{davis2019stochastic,
  title={Stochastic algorithms with geometric step decay converge linearly on sharp functions},
  author={Davis, Damek and Drusvyatskiy, Dmitriy and Charisopoulos, Vasileios},
  journal={arXiv preprint arXiv:1907.09547},
  year={2019}
}

@inproceedings{defossez2015averaged,
  title={Averaged least-mean-squares: Bias-variance trade-offs and optimal sampling distributions},
  author={D{\'e}fossez, Alexandre and Bach, Francis},
  booktitle={Artificial Intelligence and Statistics},
  pages={205--213},
  year={2015}
}

@article{dekel2012optimal,
  title={Optimal Distributed Online Prediction Using Mini-Batches.},
  author={Dekel, Ofer and Gilad-Bachrach, Ran and Shamir, Ohad and Xiao, Lin},
  journal={Journal of Machine Learning Research},
  volume={13},
  number={1},
  year={2012}
}

@article{dhillon2013risk,
  title={A risk comparison of ordinary least squares vs ridge regression},
  author={Dhillon, Paramveer S and Foster, Dean P and Kakade, Sham M and Ungar, Lyle H},
  journal={The Journal of Machine Learning Research},
  volume={14},
  number={1},
  pages={1505--1511},
  year={2013},
  publisher={JMLR. org}
}
@inproceedings{diakonikolas2020approximation,
  title={Approximation schemes for relu regression},
  author={Diakonikolas, Ilias and Goel, Surbhi and Karmalkar, Sushrut and Klivans, Adam R and Soltanolkotabi, Mahdi},
  booktitle={Conference on Learning Theory},
  pages={1452--1485},
  year={2020},
  organization={PMLR}
}

@inproceedings{diakonikolas2022learning,
  title={Learning a Single Neuron with Adversarial Label Noise via Gradient Descent},
  author={Diakonikolas, Ilias and Kontonis, Vasilis and Tzamos, Christos and Zarifis, Nikos},
  booktitle={Conference on Learning Theory},
  pages={4313--4361},
  year={2022},
  organization={PMLR}
}

@article{dieuleveut2017harder,
  title={Harder, better, faster, stronger convergence rates for least-squares regression},
  author={Dieuleveut, Aymeric and Flammarion, Nicolas and Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={3520--3570},
  year={2017},
  publisher={JMLR. org}
}

@Article{DieuleveutB15,
  title =	{Non-parametric Stochastic Approximation with Large Step sizes},
  author =	"Aymeric Dieuleveut and Francis R. Bach",
  journal = "The Annals of Statistics",
  year = 	"2015",
}


@article{dobriban2018high,
  title={High-dimensional asymptotics of prediction: Ridge regression and classification},
  author={Dobriban, Edgar and Wager, Stefan and others},
  journal={The Annals of Statistics},
  volume={46},
  number={1},
  pages={247--279},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}

@article{du2017convolutional,
  title={When is a convolutional filter easy to learn?},
  author={Du, Simon S and Lee, Jason D and Tian, Yuandong},
  journal={arXiv preprint arXiv:1709.06129},
  year={2017}
}

@article{foster2018uniform,
  title={Uniform convergence of gradients for non-convex learning and optimization},
  author={Foster, Dylan J and Sekhari, Ayush and Sridharan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{frei2020agnostic,
  title={Agnostic learning of a single neuron with gradient descent},
  author={Frei, Spencer and Cao, Yuan and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={5417--5428},
  year={2020}
}


@InProceedings{FrostigGKS15b,
  title =	{Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization},
  author =	"Roy Frostig and Rong Ge and Sham Kakade and Aaron Sidford",
  booktitle =	"ICML",
  year = 	"2015",
}

@article{ge2019step,
  title={The step decay schedule: A near optimal, geometrically decaying learning rate procedure for least squares},
  author={Ge, Rong and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth},
  journal={arXiv preprint arXiv:1904.12838},
  year={2019}
}
@inproceedings{germain2013pac,
  title={A PAC-Bayesian approach for domain adaptation with specialization to linear classifiers},
  author={Germain, Pascal and Habrard, Amaury and Laviolette, Fran{\c{c}}ois and Morvant, Emilie},
  booktitle={International conference on machine learning},
  pages={738--746},
  year={2013},
  organization={PMLR}
}

@article{ghadimi2012optimal,
  title={Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={4},
  pages={1469--1492},
  year={2012},
  publisher={SIAM}
}

@inproceedings{goel2017reliably,
  title={Reliably learning the relu in polynomial time},
  author={Goel, Surbhi and Kanade, Varun and Klivans, Adam and Thaler, Justin},
  booktitle={Conference on Learning Theory},
  pages={1004--1042},
  year={2017},
  organization={PMLR}
}

@article{goel2019time,
  title={Time/accuracy tradeoffs for learning a relu with respect to gaussian marginals},
  author={Goel, Surbhi and Karmalkar, Sushrut and Klivans, Adam},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{Goel2020TightHR,
  title={Tight Hardness Results for Training Depth-2 ReLU Networks},
  author={Surbhi Goel and Adam R. Klivans and Pasin Manurangsi and Daniel Reichman},
  booktitle={Information Technology Convergence and Services},
  year={2020}
}

@article{hanneke2019value,
  title={On the value of target data in transfer learning},
  author={Hanneke, Steve and Kpotufe, Samory},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{hastie2019surprises,
  title={Surprises in high-dimensional ridgeless least squares interpolation},
  author={Hastie, Trevor and Montanari, Andrea and Rosset, Saharon and Tibshirani, Ryan J},
  journal={arXiv preprint arXiv:1903.08560},
  year={2019}
}

@article{hazan2014beyond,
  title={Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization},
  author={Hazan, Elad and Kale, Satyen},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={2489--2512},
  year={2014},
  publisher={JMLR. org}
}

@misc{he2015deep,
  title={Deep residual learning for image recognition. CoRR abs/1512.03385 (2015)},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year={2015}
}

@Article{HsuKZ14,
  title =       "Random Design Analysis of Ridge Regression",
  author =      "Daniel J. Hsu and Sham M. Kakade and Tong Zhang",
  journal =     "Foundations of Computational Mathematics",
  year =        "2014",
  number =      "3",
  volume =      "14",
  pages =       "569--600",
}

@article{jain2017markov,
  title={A markov chain theory approach to characterizing the minimax optimality of stochastic gradient descent (for least squares)},
  author={Jain, Prateek and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth and Pillutla, Venkata Krishna and Sidford, Aaron},
  journal={arXiv preprint arXiv:1710.09430},
  year={2017}
}

@article{jain2017parallelizing,
  title={Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification},
  author={Jain, Prateek and Netrapalli, Praneeth and Kakade, Sham M and Kidambi, Rahul and Sidford, Aaron},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8258--8299},
  year={2017},
  publisher={JMLR. org}
}

@article{jain2018accelerating,
	title={Accelerating Stochastic Gradient Descent for Least Squares Regression},
	author={Jain, Prateek and Kakade, M. Sham and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
	journal={COLT},
	pages={545--604},
	year={2018}
}

@article{kakade2011efficient,
  title={Efficient learning of generalized linear and single index models with isotonic regression},
  author={Kakade, Sham M and Kanade, Varun and Shamir, Ohad and Kalai, Adam},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  year={2011}
}

@inproceedings{kalai2009isotron,
  title={The Isotron Algorithm: High-Dimensional Isotonic Regression.},
  author={Kalai, Adam Tauman and Sastry, Ravi},
  booktitle={COLT},
  year={2009}
}

@article{kivinen1997exponentiated,
  title={Exponentiated gradient versus gradient descent for linear predictors},
  author={Kivinen, Jyrki and Warmuth, Manfred K},
  journal={information and computation},
  volume={132},
  number={1},
  pages={1--63},
  year={1997},
  publisher={Elsevier}
}

@article{koehler2021uniform,
  title={Uniform convergence of interpolators: Gaussian width, norm bounds and benign overfitting},
  author={Koehler, Frederic and Zhou, Lijia and Sutherland, Danica J and Srebro, Nathan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={20657--20668},
  year={2021}
}

@inproceedings{kpotufe2017lipschitz,
  title={Lipschitz density-ratios, structured data, and data-driven tuning},
  author={Kpotufe, Samory},
  booktitle={Artificial Intelligence and Statistics},
  pages={1320--1328},
  year={2017},
  organization={PMLR}
}

@inproceedings{kpotufe2018marginal,
  title={Marginal singularity, and the benefits of labels in covariate-shift},
  author={Kpotufe, Samory and Martinet, Guillaume},
  booktitle={Conference On Learning Theory},
  pages={1882--1886},
  year={2018},
  organization={PMLR}
}

@inproceedings{kulis2010implicit,
  title={Implicit online learning},
  author={Kulis, Brian and Bartlett, Peter L},
  booktitle={Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  pages={575--582},
  year={2010}
}

@article{kulunchakov2019generic,
  title={A generic acceleration framework for stochastic composite optimization},
  author={Kulunchakov, Andrei and Mairal, Julien},
  journal={arXiv preprint arXiv:1906.01164},
  year={2019}
}

@article{lacoste2012simpler,
  title={A simpler approach to obtaining an O (1/t) convergence rate for the projected stochastic subgradient method},
  author={Lacoste-Julien, Simon and Schmidt, Mark and Bach, Francis},
  journal={arXiv preprint arXiv:1212.2002},
  year={2012}
}
@inproceedings{lakshminarayanan2018linear,
  title={Linear stochastic approximation: How far does constant step-size and iterate averaging go?},
  author={Lakshminarayanan, Chandrashekar and Szepesvari, Csaba},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1347--1355},
  year={2018}
}

@inproceedings{lei2021near,
  title={Near-optimal linear regression under distribution shift},
  author={Lei, Qi and Hu, Wei and Lee, Jason},
  booktitle={International Conference on Machine Learning},
  pages={6164--6174},
  year={2021},
  organization={PMLR}
}

@article{li2020benign,
  title={Benign Overfitting and Noisy Features},
  author={Li, Zhu and Su, Weijie and Sejdinovic, Dino},
  journal={arXiv preprint arXiv:2008.02901},
  year={2020}
}

@article{ma2022optimally,
  title={Optimally tackling covariate shift in RKHS-based nonparametric regression},
  author={Ma, Cong and Pathak, Reese and Wainwright, Martin J},
  journal={arXiv preprint arXiv:2205.02986},
  year={2022}
}
@article{mansour2009domain,
  title={Domain adaptation: Learning bounds and algorithms},
  author={Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
  journal={arXiv preprint arXiv:0902.3430},
  year={2009}
}
@article{mei2018landscape,
  title={The landscape of empirical risk for nonconvex losses},
  author={Mei, Song and Bai, Yu and Montanari, Andrea},
  journal={The Annals of Statistics},
  volume={46},
  number={6A},
  pages={2747--2774},
  year={2018},
  publisher={Institute of Mathematical Statistics}
}
@article{mei2019generalization,
  title={The generalization error of random features regression: Precise asymptotics and double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={arXiv preprint arXiv:1908.05355},
  year={2019}
}
@inproceedings{mohri2012new,
  title={New analysis and algorithm for learning with drifting distributions},
  author={Mohri, Mehryar and Mu{\~n}oz Medina, Andres},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={124--138},
  year={2012},
  organization={Springer}
}

@article{muthukumar2020classification,
  title={Classification vs regression in overparameterized regimes: Does the loss function matter?},
  author={Muthukumar, Vidya and Narang, Adhyyan and Subramanian, Vignesh and Belkin, Mikhail and Hsu, Daniel and Sahai, Anant},
  journal={arXiv preprint arXiv:2005.08054},
  year={2020}
}

@article{nakkiran2019deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1912.02292},
  year={2019}
}

@article{nakkiran2020optimal,
  title={Optimal regularization can mitigate double descent},
  author={Nakkiran, Preetum and Venkat, Prayaag and Kakade, Sham and Ma, Tengyu},
  journal={arXiv preprint arXiv:2003.01897},
  year={2020}
}

@inproceedings{neu2018iterate,
  title={Iterate averaging as regularization for stochastic gradient descent},
  author={Neu, Gergely and Rosasco, Lorenzo},
  booktitle={Conference On Learning Theory},
  pages={3222--3242},
  year={2018},
  organization={PMLR}
}

@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in neural information processing systems},
  pages={5947--5956},
  year={2017}
}

@article{pan2009survey,
  title={A survey on transfer learning},
  author={Pan, Sinno Jialin and Yang, Qiang},
  journal={IEEE Transactions on knowledge and data engineering},
  volume={22},
  number={10},
  pages={1345--1359},
  year={2009},
  publisher={IEEE}
}

@article{pathak2022new,
  title={A new similarity measure for covariate shift with applications to nonparametric regression},
  author={Pathak, Reese and Ma, Cong and Wainwright, Martin J},
  journal={arXiv preprint arXiv:2202.02837},
  year={2022}
}

@InProceedings{pmlr-v75-jain18a,
title = {Accelerating Stochastic Gradient Descent for Least Squares Regression},
author = {Jain, Prateek and Kakade, Sham M. and Kidambi, Rahul and Netrapalli, Praneeth and Sidford, Aaron},
booktitle = {Proceedings of the 31st Conference On Learning Theory},
year = {2018},
volume = {75},
series = {Proceedings of Machine Learning Research},
publisher = {PMLR},
}

@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}

@book{quinonero2008dataset,
  title={Dataset shift in machine learning},
  author={Qui{\~n}onero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D},
  year={2008},
  publisher={Mit Press}
}

@article{rakhlin2011making,
  title={Making gradient descent optimal for strongly convex stochastic optimization},
  author={Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  journal={arXiv preprint arXiv:1109.5647},
  year={2011}
}


@inproceedings{safran2018spurious,
  title={Spurious local minima are common in two-layer relu neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International conference on machine learning},
  pages={4433--4441},
  year={2018},
  organization={PMLR}
}

@book{scholkopf2002learning,
  title={Learning with kernels: support vector machines, regularization, optimization, and beyond},
  author={Sch{\"o}lkopf, Bernhard and Smola, Alexander J and Bach, Francis and others},
  year={2002},
  publisher={MIT press}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}


@article{shimodaira2000improving,
  title={Improving predictive inference under covariate shift by weighting the log-likelihood function},
  author={Shimodaira, Hidetoshi},
  journal={Journal of statistical planning and inference},
  volume={90},
  number={2},
  pages={227--244},
  year={2000},
  publisher={Elsevier}
}

@article{soltanolkotabi2017learning,
  title={Learning relus via gradient descent},
  author={Soltanolkotabi, Mahdi},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@book{sugiyama2012density,
  title={Density ratio estimation in machine learning},
  author={Sugiyama, Masashi and Suzuki, Taiji and Kanamori, Takafumi},
  year={2012},
  publisher={Cambridge University Press}
}

@book{sugiyama2012machine,
  title={Machine learning in non-stationary environments: Introduction to covariate shift adaptation},
  author={Sugiyama, Masashi and Kawanabe, Motoaki},
  year={2012},
  publisher={MIT press}
}

@inproceedings{thrampoulidis2015regularized,
  title={Regularized linear regression: A precise analysis of the estimation error},
  author={Thrampoulidis, Christos and Oymak, Samet and Hassibi, Babak},
  booktitle={Conference on Learning Theory},
  pages={1683--1709},
  year={2015},
  organization={PMLR}
}

@article{tsigler2020benign,
  title={Benign overfitting in ridge regression},
  author={Tsigler, Alexander and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2009.14286},
  year={2020}
}

@article{varre2021last,
  title={Last iterate convergence of SGD for Least-Squares in the Interpolation regime},
  author={Varre, Aditya and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2102.03183},
  year={2021}
}

@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}

@article{wang2020benign,
  title={Benign Overfitting in Binary Classification of Gaussian Mixtures},
  author={Wang, Ke and Thrampoulidis, Christos},
  journal={arXiv preprint arXiv:2011.09148},
  year={2020}
}

@inproceedings{wu2019domain,
  title={Domain adaptation with asymmetrically-relaxed distribution alignment},
  author={Wu, Yifan and Winston, Ezra and Kaushik, Divyansh and Lipton, Zachary},
  booktitle={International Conference on Machine Learning},
  pages={6872--6881},
  year={2019},
  organization={PMLR}
}

@article{wu2022iterate,
  title={Last Iterate Risk Bounds of SGD with Decaying Stepsize for Overparameterized Linear Regression},
  author={Jingfeng Wu and Difan Zou and Vladimir Braverman and Quanquan Gu and Sham M. Kakade},
  journal={The 39th International Conference on Machine Learning},
  year={2022}
}

@article{wu2022power,
  title={The Power and Limitation of Pretraining-Finetuning for Linear Regression under Covariate Shift},
  author={Jingfeng Wu and Difan Zou and Vladimir Braverman and Quanquan Gu and Sham M. Kakade},
  journal={The 36th Conference on Neural Information Processing Systems},
  year={2022}
}
@inproceedings{yehudai2020learning,
  title={Learning a single neuron with gradient methods},
  author={Yehudai, Gilad and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={3756--3786},
  year={2020},
  organization={PMLR}
}
@article{yosinski2014transferable,
  title={How transferable are features in deep neural networks?},
  author={Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{zhao2019learning,
  title={On learning invariant representations for domain adaptation},
  author={Zhao, Han and Des Combes, Remi Tachet and Zhang, Kun and Gordon, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={7523--7532},
  year={2019},
  organization={PMLR}
}

@article{zhou2020uniform,
  title={On uniform convergence and low-norm interpolation learning},
  author={Zhou, Lijia and Sutherland, Danica J and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6867--6877},
  year={2020}
}

@article{zhou2021optimistic,
  title={Optimistic rates: A unifying theory for interpolation learning and regularization in linear regression},
  author={Zhou, Lijia and Koehler, Frederic and Sutherland, Danica J and Srebro, Nathan},
  journal={arXiv preprint arXiv:2112.04470},
  year={2021}
}

@article{zhou2022non,
  title={A Non-Asymptotic Moreau Envelope Theory for High-Dimensional Generalized Linear Models},
  author={Zhou, Lijia and Koehler, Frederic and Sur, Pragya and Sutherland, Danica J and Srebro, Nathan},
  journal={arXiv preprint arXiv:2210.12082},
  year={2022}
}

@article{zou2021benefits,
  title={The benefits of implicit regularization from sgd in least squares problems},
  author={Zou, Difan and Wu, Jingfeng and Braverman, Vladimir and Gu, Quanquan and Foster, Dean P and Kakade, Sham},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5456--5468},
  year={2021}
}


@inproceedings{zou2021benign,
  title={Benign overfitting of constant-stepsize sgd for linear regression},
  author={Zou, Difan and Wu, Jingfeng and Braverman, Vladimir and Gu, Quanquan and Kakade, Sham},
  booktitle={Conference on Learning Theory},
  pages={4633--4635},
  year={2021},
  organization={PMLR}
}