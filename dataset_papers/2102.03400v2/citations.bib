@inproceedings{song2019active,
  title={Active Learning for Streaming Data in A Contextual Bandit Framework},
  author={Song, Linqi and Xu, Jie and Li, Congduan},
  booktitle={Proceedings of the 2019 5th International Conference on Computing and Data Engineering},
  pages={29--35},
  year={2019}
}

@article{slivkins2019introduction,
  title={Introduction to multi-armed bandits},
  author={Slivkins, Aleksandrs},
  journal={arXiv preprint arXiv:1904.07272},
  year={2019}
}

@inproceedings{seldin2014prediction,
  title={Prediction with limited advice and multiarmed bandits with paid observations},
  author={Seldin, Yevgeny and Bartlett, Peter and Crammer, Koby and Abbasi-Yadkori, Yasin},
  booktitle={International Conference on Machine Learning},
  pages={280--287},
  year={2014},
  organization={PMLR}
}

@article{maurer2009empirical,
  title={Empirical Bernstein bounds and sample variance penalization},
  author={Maurer, Andreas and Pontil, Massimiliano},
  journal={arXiv preprint arXiv:0907.3740},
  year={2009}
}

@inproceedings{sinha2021multi,
  title={Multi-Armed Bandits with Cost Subsidy},
  author={Sinha, Deeksha and Sankararaman, Karthik Abinav and Kazerouni, Abbas and Avadhanula, Vashist},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3016--3024},
  year={2021},
  organization={PMLR}
}


@article{cohen2020near,
  title={Near-optimal regret bounds for stochastic shortest path},
  author={Cohen, Alon and Kaplan, Haim and Mansour, Yishay and Rosenberg, Aviv},
  journal={arXiv preprint arXiv:2002.09869},
  year={2020}
}

@inproceedings{tarbouriech2020no,
  title={No-regret exploration in goal-oriented reinforcement learning},
  author={Tarbouriech, Jean and Garcelon, Evrard and Valko, Michal and Pirotta, Matteo and Lazaric, Alessandro},
  booktitle={International Conference on Machine Learning},
  pages={9428--9437},
  year={2020},
  organization={PMLR}
}


@article{garivier2019explore,
  title={Explore first, exploit next: The true shape of regret in bandit problems},
  author={Garivier, Aur{\'e}lien and M{\'e}nard, Pierre and Stoltz, Gilles},
  journal={Mathematics of Operations Research},
  volume={44},
  number={2},
  pages={377--399},
  year={2019},
  publisher={INFORMS}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{silver2017mastering,
  title={Mastering the game of {Go} without human knowledge},
  author={Silver, D. and Schrittwieser, J. and Simonyan, K. and Antonoglou, I. and Huang, A. and Guez, A. and Hubert, T. and Baker, L. and Lai, M. and Bolton, A. and others},
  journal={Nature},
  volume={550},
  number={7676},
  pages={354},
  year={2017}
}

@inproceedings{garivier2011kl,
  title={The KL-UCB algorithm for bounded stochastic bandits and beyond},
  author={Garivier, Aur{\'e}lien and Capp{\'e}, Olivier},
  booktitle={Conference on Learning Theory},
  pages={359--376},
  year={2011}
}

@article{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:1703.05449},
  year={2017}
}

@article{auer2002finite,
  title={Finite-time analysis of the multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  journal={Machine learning},
  volume={47},
  number={2-3},
  pages={235--256},
  year={2002},
  publisher={Springer}
}

@inproceedings{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2312--2320},
  year={2011}
}

@inproceedings{agrawal2013thompson,
  title={Thompson sampling for contextual bandits with linear payoffs},
  author={Agrawal, Shipra and Goyal, Navin},
  booktitle={International Conference on Machine Learning},
  pages={127--135},
  year={2013}
}

@article{rosenberg2019online,
  title={Online convex optimization in adversarial markov decision processes},
  author={Rosenberg, Aviv and Mansour, Yishay},
  journal={arXiv preprint arXiv:1905.07773},
  year={2019}
}


@book{cesa2006prediction,
  title={Prediction, learning, and games},
  author={Cesa-Bianchi, Nicolo and Lugosi, G{\'a}bor},
  year={2006},
  publisher={Cambridge university press}
}

@inproceedings{dann2015sample,
  title={Sample complexity of episodic fixed-horizon reinforcement learning},
  author={Dann, Christoph and Brunskill, Emma},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2818--2826},
  year={2015}
}

@article{abeille2017linear,
  title={Linear thompson sampling revisited},
  author={Abeille, Marc and Lazaric, Alessandro and others},
  journal={Electronic Journal of Statistics},
  volume={11},
  number={2},
  pages={5165--5197},
  year={2017},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}

@inproceedings{jin2018q,
  title={Is Q-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4863--4873},
  year={2018}
}


@article{zanette2019tighter,
  title={Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds},
  author={Zanette, Andrea and Brunskill, Emma},
  journal={arXiv preprint arXiv:1901.00210},
  year={2019}
}


@article{levine2016end,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, S. and Finn, C. and Darrell, T. and Abbeel, P.},
  journal={Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1334--1373},
  year={2016}
}

@inproceedings{efroni2019tight,
  title={Tight regret bounds for model-based reinforcement learning with greedy policies},
  author={Efroni, Yonathan and Merlis, Nadav and Ghavamzadeh, Mohammad and Mannor, Shie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={12224--12234},
  year={2019}
}

@article{jin2019learning,
  title={Learning adversarial mdps with bandit feedback and unknown transition},
  author={Jin, Tiancheng and Luo, Haipeng},
  journal={arXiv preprint arXiv:1912.01192},
  year={2019}
}

@book{altman1999constrained,
  title={Constrained Markov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}

@inproceedings{Zimin2013online,
  author    = {Alexander Zimin and
               Gergely Neu},
  title     = {Online learning in episodic Markovian decision processes by relative
               entropy policy search},
  booktitle = {{NIPS}},
  pages     = {1583--1591},
  year      = {2013}
}

@book{puterman1994markov,
  title={Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  author={Puterman, Martin L},
  year={1994},
  publisher={John Wiley \& Sons, Inc.}
}

@inproceedings{russo2019worst,
  title={Worst-case regret bounds for exploration via randomized value functions},
  author={Russo, Daniel},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14433--14443},
  year={2019}
}

@article{laurent2000adaptive,
  title={Adaptive estimation of a quadratic functional by model selection},
  author={Laurent, Beatrice and Massart, Pascal},
  journal={Annals of Statistics},
  pages={1302--1338},
  year={2000},
  publisher={JSTOR}
}

@article{weissman2003inequalities,
  title={Inequalities for the L1 deviation of the empirical distribution},
  author={Weissman, Tsachy and Ordentlich, Erik and Seroussi, Gadiel and Verdu, Sergio and Weinberger, Marcelo J},
  journal={Hewlett-Packard Labs, Tech. Rep},
  year={2003}
}

@article{borjesson1979simple,
  title={Simple approximations of the error function Q (x) for communications applications},
  author={Borjesson, P and Sundberg, C-E},
  journal={IEEE Transactions on Communications},
  volume={27},
  number={3},
  pages={639--643},
  year={1979},
  publisher={IEEE}
}

@article{atamturk2017maximizing,
  title={Maximizing a class of utility functions over the vertices of a polytope},
  author={Atamt{\"u}rk, Alper and G{\'o}mez, Andr{\'e}s},
  journal={Operations Research},
  volume={65},
  number={2},
  pages={433--445},
  year={2017},
  publisher={INFORMS}
}

@inproceedings{beygelzimer2011contextual,
  title={Contextual bandit algorithms with supervised learning guarantees},
  author={Beygelzimer, Alina and Langford, John and Li, Lihong and Reyzin, Lev and Schapire, Robert},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={19--26},
  year={2011}
}

@article{jaksch2010near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Apr},
  pages={1563--1600},
  year={2010}
}

@article{maillard2014hard,
  title={How hard is my MDP?" The distribution-norm to the rescue"},
  author={Maillard, Odalric-Ambrym and Mann, Timothy A and Mannor, Shie},
  journal={Advances in Neural Information Processing Systems},
  volume={27},
  pages={1835--1843},
  year={2014}
}

@inproceedings{merlis2019batch,
  title={Batch-size independent regret bounds for the combinatorial multi-armed bandit problem},
  author={Merlis, Nadav and Mannor, Shie},
  booktitle={Conference on Learning Theory},
  pages={2465--2489},
  year={2019},
  organization={PMLR}
}

@inproceedings{merlis2020tight,
  title={Tight lower bounds for combinatorial multi-armed bandits},
  author={Merlis, Nadav and Mannor, Shie},
  booktitle={Conference on Learning Theory},
  pages={2830--2857},
  year={2020},
  organization={PMLR}
}

@article{yun2018multi,
  title={Multi-armed bandit with additional observations},
  author={Yun, Donggyu and Proutiere, Alexandre and Ahn, Sumyeong and Shin, Jinwoo and Yi, Yung},
  journal={Proceedings of the ACM on Measurement and Analysis of Computing Systems},
  volume={2},
  number={1},
  pages={1--22},
  year={2018},
  publisher={ACM New York, NY, USA}
}

@inproceedings{simchowitz2019non,
  title={Non-asymptotic gap-dependent regret bounds for tabular mdps},
  author={Simchowitz, Max and Jamieson, Kevin G},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1153--1162},
  year={2019}
}

@inproceedings{dann2019policy,
  title={Policy certificates: Towards accountable reinforcement learning},
  author={Dann, Christoph and Li, Lihong and Wei, Wei and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={1507--1516},
  year={2019}
}

@article{zhang2020reinforcement,
  title={Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon},
  author={Zhang, Zihan and Ji, Xiangyang and Du, Simon S},
  journal={arXiv preprint arXiv:2009.13503},
  year={2020}
}


@inproceedings{osband2013more,
  title={(More) efficient reinforcement learning via posterior sampling},
  author={Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3003--3011},
  year={2013}
}

@inproceedings{dani2008stochastic,
  author    = {Varsha Dani and
               Thomas P. Hayes and
               Sham M. Kakade},
  title     = {Stochastic Linear Optimization under Bandit Feedback},
  booktitle = {Conference on Learning Theory},
  pages     = {355--366},
  year      = {2008},
}


@inproceedings{osband2017posterior,
  title={Why is posterior sampling better than optimism for reinforcement learning?},
  author={Osband, Ian and Van Roy, Benjamin},
  booktitle={International Conference on Machine Learning},
  pages={2701--2710},
  year={2017}
}

@inproceedings{gopalan2015thompson,
  title={Thompson sampling for learning parameterized Markov decision processes},
  author={Gopalan, Aditya and Mannor, Shie},
  booktitle={Conference on Learning Theory},
  pages={861--898},
  year={2015}
}

@inproceedings{chu2011contextual,
  title={Contextual bandits with linear payoff functions},
  author={Chu, Wei and Li, Lihong and Reyzin, Lev and Schapire, Robert},
  booktitle={Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages={208--214},
  year={2011}
}

@article{osband2016lower,
  title={On lower bounds for regret in reinforcement learning},
  author={Osband, Ian and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:1608.02732},
  year={2016}
}

@inproceedings{dann2017unifying,
  title={Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5713--5723},
  year={2017}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@article{bartlett1951inverse,
  title={An inverse matrix adjustment arising in discriminant analysis},
  author={Bartlett, Maurice S},
  journal={The Annals of Mathematical Statistics},
  volume={22},
  number={1},
  year={1951},
  publisher={JSTOR}
}

@inproceedings{filippi2010parametric,
  title={Parametric bandits: The generalized linear case},
  author={Filippi, Sarah and Cappe, Olivier and Garivier, Aur{\'e}lien and Szepesv{\'a}ri, Csaba},
  booktitle={Advances in Neural Information Processing Systems},
  pages={586--594},
  year={2010}
}

@inproceedings{kveton2020randomized,
  title={Randomized exploration in generalized linear bandits},
  author={Kveton, Branislav and Zaheer, Manzil and Szepesvari, Csaba and Li, Lihong and Ghavamzadeh, Mohammad and Boutilier, Craig},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2066--2076},
  year={2020}
}

@inproceedings{kaufmann2012thompson,
  title={Thompson sampling: An asymptotically optimal finite-time analysis},
  author={Kaufmann, Emilie and Korda, Nathaniel and Munos, R{\'e}mi},
  booktitle={International conference on algorithmic learning theory},
  pages={199--213},
  year={2012},
  organization={Springer}
}

@inproceedings{agrawal2012analysis,
  title={Analysis of thompson sampling for the multi-armed bandit problem},
  author={Agrawal, Shipra and Goyal, Navin},
  booktitle={Conference on learning theory},
  pages={39--1},
  year={2012}
}

@inproceedings{badanidiyuru2013bandits,
  title={Bandits with knapsacks},
  author={Badanidiyuru, Ashwinkumar and Kleinberg, Robert and Slivkins, Aleksandrs},
  booktitle={2013 IEEE 54th Annual Symposium on Foundations of Computer Science},
  pages={207--216},
  year={2013},
  organization={IEEE}
}

@article{foster2020instance,
  title={Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective},
  author={Foster, Dylan J and Rakhlin, Alexander and Simchi-Levi, David and Xu, Yunzong},
  journal={arXiv preprint arXiv:2010.03104},
  year={2020}
}

@article{bretagnolle1979estimation,
  title={Estimation des densit{\'e}s: risque minimax},
  author={Bretagnolle, Jean and Huber, Catherine},
  journal={Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und verwandte Gebiete},
  volume={47},
  number={2},
  pages={119--137},
  year={1979},
  publisher={Springer}
}

@article{foster2019model,
  title={Model selection for contextual bandits},
  author={Foster, Dylan J and Krishnamurthy, Akshay and Luo, Haipeng},
  journal={arXiv preprint arXiv:1906.00531},
  year={2019}
}

@article{efroni2020reinforcement,
  title={Reinforcement Learning with Trajectory Feedback},
  author={Efroni, Yonathan and Merlis, Nadav and Mannor, Shie},
  journal={arXiv preprint arXiv:2008.06036},
  year={2020}
}

@inproceedings{degenne2016anytime,
  title={Anytime optimal algorithms in stochastic multi-armed bandits},
  author={Degenne, R{\'e}my and Perchet, Vianney},
  booktitle={International Conference on Machine Learning},
  pages={1587--1595},
  year={2016},
  organization={PMLR}
}