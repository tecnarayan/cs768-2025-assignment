\begin{thebibliography}{10}

\bibitem{avron2017sharper}
Haim Avron, Kenneth~L Clarkson, and David~P Woodruff.
\newblock Sharper bounds for regularized data fitting.
\newblock In {\em Approximation, Randomization, and Combinatorial Optimization.
  Algorithms and Techniques (APPROX/RANDOM 2017)}. Schloss
  Dagstuhl-Leibniz-Zentrum fuer Informatik, 2017.

\bibitem{pilanci2015randomized}
Mert Pilanci and Martin~J Wainwright.
\newblock Randomized sketches of convex programs with sharp guarantees.
\newblock {\em IEEE Transactions on Information Theory}, 61(9):5096--5115,
  2015.

\bibitem{langberg2010universal}
Michael Langberg and Leonard~J Schulman.
\newblock Universal $\varepsilon$-approximators for integrals.
\newblock In {\em Proceedings of the twenty-first annual ACM-SIAM symposium on
  Discrete Algorithms}, pages 598--607. SIAM, 2010.

\bibitem{feldman2011unified}
Dan Feldman and Michael Langberg.
\newblock A unified framework for approximating and clustering data.
\newblock In {\em Proceedings of the forty-third annual ACM symposium on Theory
  of computing}, pages 569--578. ACM, 2011.

\bibitem{drineas2006sampling}
Petros Drineas, Michael~W Mahoney, and Shan Muthukrishnan.
\newblock Sampling algorithms for $\ell_2$ regression and applications.
\newblock In {\em Proceedings of the seventeenth annual ACM-SIAM symposium on
  Discrete algorithm}, pages 1127--1136. Society for Industrial and Applied
  Mathematics, 2006.

\bibitem{dasgupta2009sampling}
Anirban Dasgupta, Petros Drineas, Boulos Harb, Ravi Kumar, and Michael~W
  Mahoney.
\newblock Sampling algorithms and coresets for $\ell_p$ regression.
\newblock {\em SIAM Journal on Computing}, 38(5):2060--2078, 2009.

\bibitem{cohen2015p}
Michael~B Cohen and Richard Peng.
\newblock $\ell_p$ row sampling by lewis weights.
\newblock In {\em Proceedings of the forty-seventh annual ACM symposium on
  Theory of computing}, pages 183--192. ACM, 2015.

\bibitem{sohler2011subspace}
Christian Sohler and David~P Woodruff.
\newblock Subspace embeddings for the $\ell_1$-norm with applications.
\newblock In {\em Proceedings of the forty-third annual ACM symposium on Theory
  of computing}, pages 755--764. ACM, 2011.

\bibitem{woodruff2014sketching}
David~P Woodruff et~al.
\newblock Sketching as a tool for numerical linear algebra.
\newblock {\em Foundations and Trends{\textregistered} in Theoretical Computer
  Science}, 10(1--2):1--157, 2014.

\bibitem{bachem2017practical}
Olivier Bachem, Mario Lucic, and Andreas Krause.
\newblock Practical coreset constructions for machine learning.
\newblock {\em arXiv preprint arXiv:1703.06476}, 2017.

\bibitem{reddi2015communication}
Sashank~J Reddi, Barnab{\'a}s P{\'o}czos, and Alexander~J Smola.
\newblock Communication efficient coresets for empirical loss minimization.
\newblock In {\em UAI}, pages 752--761, 2015.

\bibitem{curtain2019coresets}
Ryan Curtin, Sungjin Im, Ben Moseley, Kirk Pruhs, and Alireza Samadian.
\newblock On coresets for regularized loss minimization.
\newblock {\em arXiv preprint arXiv:1905.10845}, 2019.

\bibitem{clarkson2016fast}
Kenneth~L Clarkson, Petros Drineas, Malik Magdon-Ismail, Michael~W Mahoney,
  Xiangrui Meng, and David~P Woodruff.
\newblock The fast cauchy transform and faster robust linear regression.
\newblock {\em SIAM Journal on Computing}, 45(3):763--810, 2016.

\bibitem{meng2013low}
Xiangrui Meng and Michael~W Mahoney.
\newblock Low-distortion subspace embeddings in input-sparsity time and
  applications to robust linear regression.
\newblock In {\em Proceedings of the forty-fifth annual ACM symposium on Theory
  of computing}, pages 91--100. ACM, 2013.

\bibitem{woodruff2013subspace}
David Woodruff and Qin Zhang.
\newblock Subspace embeddings and $\ell_p$-regression using exponential random
  variables.
\newblock In {\em Conference on Learning Theory}, pages 546--567, 2013.

\bibitem{drineas2011faster}
Petros Drineas, Michael~W Mahoney, Shan Muthukrishnan, and Tam{\'a}s
  Sarl{\'o}s.
\newblock Faster least squares approximation.
\newblock {\em Numerische mathematik}, 117(2):219--249, 2011.

\bibitem{braverman2016new}
Vladimir Braverman, Dan Feldman, and Harry Lang.
\newblock New frameworks for offline and streaming coreset constructions.
\newblock {\em arXiv preprint arXiv:1612.00889}, 2016.

\bibitem{clarkson2017low}
Kenneth~L Clarkson and David~P Woodruff.
\newblock Low-rank approximation and regression in input sparsity time.
\newblock {\em Journal of the ACM (JACM)}, 63(6):54, 2017.

\bibitem{drineas2012fast}
Petros Drineas, Malik Magdon-Ismail, Michael~W Mahoney, and David~P Woodruff.
\newblock Fast approximation of matrix coherence and statistical leverage.
\newblock {\em Journal of Machine Learning Research}, 13(Dec):3475--3506, 2012.

\bibitem{yang2015implementing}
Jiyan Yang, Xiangrui Meng, and Michael~W Mahoney.
\newblock Implementing randomized matrix algorithms in parallel and distributed
  environments.
\newblock {\em Proceedings of the IEEE}, 104(1):58--92, 2015.

\bibitem{agarwal2004approximating}
Pankaj~K Agarwal, Sariel Har-Peled, and Kasturi~R Varadarajan.
\newblock Approximating extent measures of points.
\newblock {\em Journal of the ACM (JACM)}, 51(4):606--635, 2004.

\bibitem{har2004coresets}
Sariel Har-Peled and Soham Mazumdar.
\newblock On coresets for k-means and k-median clustering.
\newblock In {\em Proceedings of the thirty-sixth annual ACM symposium on
  Theory of computing}, pages 291--300. ACM, 2004.

\bibitem{dickens2018leveraging}
Charlie Dickens, Graham Cormode, and David Woodruff.
\newblock Leveraging well-conditioned bases: Streaming and distributed
  summaries in minkowski $ p $-norms.
\newblock In {\em International Conference on Machine Learning}, pages
  1243--1251, 2018.

\bibitem{raj2019importance}
Anant Raj, Cameron Musco, and Lester Mackey.
\newblock Importance sampling via local sensitivity.
\newblock {\em arXiv preprint arXiv:1911.01575}, 2019.

\bibitem{tolochinsky2018coresets}
Elad Tolochinsky and Dan Feldman.
\newblock Coresets for monotonic functions with applications to deep learning.
\newblock {\em CoRR, abs/1802.07382}, 2018.

\bibitem{wang2006regularized}
Li~Wang, Michael~D Gordon, and Ji~Zhu.
\newblock Regularized least absolute deviations regression and an efficient
  algorithm for parameter tuning.
\newblock In {\em Sixth International Conference on Data Mining (ICDM'06)},
  pages 690--700. IEEE, 2006.

\bibitem{dubhashi2009concentration}
Devdatt~P Dubhashi and Alessandro Panconesi.
\newblock {\em Concentration of measure for the analysis of randomized
  algorithms}.
\newblock Cambridge University Press, 2009.

\bibitem{haussler1995sphere}
David Haussler.
\newblock Sphere packing numbers for subsets of the boolean n-cube with bounded
  vapnik-chervonenkis dimension.
\newblock {\em J. Comb. Theory, Ser. A}, 69(2):217--232, 1995.

\bibitem{phillips2016coresets}
Jeff~M Phillips.
\newblock Coresets and sketches.
\newblock {\em arXiv preprint arXiv:1601.00617}, 2016.

\bibitem{kachamoptimal}
Praneeth Kacham and David~P Woodruff.
\newblock Optimal deterministic coresets for ridge regression.
\newblock {\em AISTAT}, 2020.

\bibitem{batson2012twice}
Joshua Batson, Daniel~A Spielman, and Nikhil Srivastava.
\newblock Twice-ramanujan sparsifiers.
\newblock {\em SIAM Journal on Computing}, 41(6):1704--1721, 2012.

\bibitem{tufekci2014prediction}
P{\i}nar T{\"u}fekci.
\newblock Prediction of full load electrical power output of a base load
  operated combined cycle power plant using machine learning methods.
\newblock {\em International Journal of Electrical Power \& Energy Systems},
  60:126--140, 2014.

\bibitem{maalouf2019fast}
Alaa Maalouf, Ibrahim Jubran, and Dan Feldman.
\newblock Fast and accurate least-mean-squares solvers.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8305--8316, 2019.

\end{thebibliography}
