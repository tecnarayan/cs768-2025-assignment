@inproceedings{alweiss2021discrepancy,
  title={Discrepancy minimization via a self-balancing walk},
  author={Alweiss, Ryan and Liu, Yang P and Sawhney, Mehtaab},
  booktitle={Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing},
  pages={14--20},
  year={2021}
}

@article{barp2022targeted,
  title={Targeted separation and convergence with kernel discrepancies},
  author={Barp, Alessandro and Simon-Gabriel, Carl-Johann and Girolami, Mark and Mackey, Lester},
  journal={arXiv preprint arXiv:2209.12835},
  year={2022}
}

@incollection{bertsekas2011incremental,
  author    = {Bertsekas, Dimitri P.},
  title     = {{Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A Survey}},
  booktitle = {{Optimization for Machine Learning}},
  publisher = {The MIT Press},
  year      = {2011},
}

@incollection{bottou2012stochastic,
  title={Stochastic gradient descent tricks},
  author={Bottou, L{\'e}on},
  booktitle={Neural networks: Tricks of the trade},
  pages={421--436},
  year={2012},
  publisher={Springer}
}

@inproceedings{cha2023tighter,
author = {Cha, Jaeyoung and Lee, Jaewook and Yun, Chulhee},
title = {{Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond}},
year = {2023},
publisher = {JMLR.org},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {156},
numpages = {58},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

  



@article{cooper2023variance,
      title={{Is My Prediction Arbitrary? Measuring Self-Consistency in Fair Classification}}, 
      author={A. Feder Cooper and Katherine Lee and Solon Barocas and Christopher De Sa and Siddhartha Sen and Baobao Zhang},
      year={2023},
      journal={arXiv preprint arXiv:2301.11562}
}

@inproceedings{desa2020shuffle,
  author    = {De Sa, Christopher},
  title     = {{Random Reshuffling is Not Always Better}},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2020},
}

@article{dwivedi2021kernel,
  title={Kernel thinning},
  author={Dwivedi, Raaz and Mackey, Lester},
  journal={arXiv preprint arXiv:2105.05842},
  year={2021}
}
@inproceedings{dwivedi2022generalized,
  title={{Generalized Kernel Thinning}},
  author={Dwivedi, Raaz and Mackey, Lester},
  booktitle={Tenth International Conference on Learning Representations},
  year={2022}
}

@inproceedings{graves2017automated,
  title={Automated curriculum learning for neural networks},
  author={Graves, Alex and Bellemare, Marc G and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  booktitle={international conference on machine learning},
  pages={1311--1320},
  year={2017},
  organization={PMLR}
}

@article{gurbuzbalaban2019convergence,
  author    = {Mert G{\"{u}}rb{\"{u}}zbalaban and
               Asuman E. Ozdaglar and
               Pablo A. Parrilo},
  title     = {{Convergence Rate of Incremental Gradient and Incremental {Newton} Methods}},
  journal   = {{SIAM} Journal on Optimization},
  volume    = {29},
  number    = {4},
  pages     = {2542--2565},
  year      = {2019},
}

@article{gurbuzbalaban2021random,
  title={Why random reshuffling beats stochastic gradient descent},
  author={G{\"u}rb{\"u}zbalaban, Mert and Ozdaglar, Asu and Parrilo, Pablo A},
  journal={Mathematical Programming},
  volume={186},
  number={1},
  pages={49--84},
  year={2021},
  publisher={Springer}
}

@inproceedings{haochen2019random,
  author    = {Jeff Z. HaoChen and
               Suvrit Sra},
  title     = {{Random Shuffling Beats {SGD} after Finite Epochs}},
  booktitle = {Proceedings of the International Conference on Machine Learning},
  volume    = {97},
  pages     = {2624--2633},
  year      = {2019}
}

@inproceedings{harvey2014near,
  author    = {Nick Harvey and
               Samira Samadi},
  title     = {{Near-Optimal Herding}},
  booktitle = {Proceedings of The 27th Conference on Learning Theory},
  volume    = {35},
  pages     = {1165--1182},
  year      = {2014}
}

@article{huang2021distributed,
  title={{Distributed Random Reshuffling over Networks}},
  author={Huang, Kun and Li, Xiao and Milzarek, Andre and Pu, Shi and Qiu, Junwen},
  journal={arXiv preprint arXiv:2112.15287},
  year={2021}
}

@inproceedings{li2014ps,
author = {Li, Mu and Andersen, David G. and Park, Jun Woo and Smola, Alexander J. and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J. and Su, Bor-Yiing},
title = {{Scaling Distributed Machine Learning with the Parameter Server}},
year = {2014},
isbn = {9781931971164},
publisher = {USENIX Association},
address = {USA},
abstract = {We propose a parameter server framework for distributed machine learning problems. Both data and workloads are distributed over worker nodes, while the server nodes maintain globally shared parameters, represented as dense or sparse vectors and matrices. The framework manages asynchronous data communication between nodes, and supports flexible consistency models, elastic scalability, and continuous fault tolerance.To demonstrate the scalability of the proposed framework, we show experimental results on petabytes of real data with billions of examples and parameters on problems ranging from Sparse Logistic Regression to Latent Dirichlet Allocation and Distributed Sketching.},
booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
pages = {583â€“598},
numpages = {16},
location = {Broomfield, CO},
series = {OSDI'14}
}

@inproceedings{lu2021general,
  title={{A General Analysis of Example-Selection for Stochastic Gradient Descent}},
  author={Lu, Yucheng and Meng, Si Yi and De Sa, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@inproceedings{lu2021variance,
  title={{Variance Reduced Training with Stratified Sampling for Forecasting Models}},
  author={Lu, Yucheng and Park, Youngsuk and Chen, Lifan and Wang, Yuyang and De Sa, Christopher and Foster, Dean},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={7145--7155},
  year={2021},
  organization={PMLR}
}

@inproceedings{
    lu2022grab,
    title={{GraB: Finding Provably Better Data Permutations than Random Reshuffling}},
    author={Yucheng Lu and Wentao Guo and Christopher De Sa},
    booktitle={Advances in Neural Information Processing Systems},
    editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
    year={2022},
    url={https://openreview.net/forum?id=nDemfqKHTpK}
}

@article{malinovsky2022server,
  title={{Server-Side Stepsizes and Sampling Without Replacement Provably Help in Federated Optimization}},
  author={Malinovsky, Grigory and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2201.11066},
  year={2022}
}

@article{matiisen2019teacher,
  title={Teacher--student curriculum learning},
  author={Matiisen, Tambet and Oliver, Avital and Cohen, Taco and Schulman, John},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={9},
  pages={3732--3740},
  year={2019},
  publisher={IEEE}
}

@inproceedings{mishchenko2020random,
  author    = {Konstantin Mishchenko and
               Ahmed Khaled and
               Peter Richt{\'{a}}rik},
  title     = {{Random Reshuffling: Simple Analysis with Vast Improvements}},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2020}
}


@article{mohtashami2022characterizing,
  title={{Characterizing \& Finding Good Data Orderings for Fast Convergence of Sequential Gradient Methods}},
  author={Mohtashami, Amirkeivan and Stich, Sebastian and Jaggi, Martin},
  journal={arXiv preprint arXiv:2202.01838},
  year={2022}
}

@inproceedings{needell2014stochastic,
  author    = {Deanna Needell and
               Rachel Ward and
               Nathan Srebro},
  title     = {{Stochastic Gradient Descent, Weighted Sampling, and the Randomized
               Kaczmarz algorithm}},
  booktitle = {Advances in Neural Information Processing Systems},
  pages     = {1017--1025},
  year      = {2014},
}

@misc{pytorchshuffle,
author = {{PyTorch Contributors}},
year = {2023},
title={{DataLoader API}},
url = {https://pytorch.org/docs/stable/data.html}
}

@inproceedings{rajput2021permutationbased,
      title={{Permutation-Based SGD: Is Random Optimal?}}, 
      author={Shashank Rajput and Kangwook Lee and Dimitris Papailiopoulos},
      year={2022},
      booktitle={International Conference on Learning Representations}
}

@inproceedings{recht2012toward,
  author    = {Benjamin Recht and
               Christopher R{\'{e}}},
  title     = {{Toward a Noncommutative Arithmetic-geometric Mean Inequality: Conjectures, Case-studies, and Consequences}},
  booktitle = {Conference on Learning Theory},
  volume    = {23},
  pages     = {11.1--11.24},
  year      = {2012},
}

@article{sadiev2022federated,
  title={{Federated Optimization Algorithms with Random Reshuffling and Gradient Compression}},
  author={Sadiev, Abdurakhmon and Malinovsky, Grigory and Gorbunov, Eduard and Sokolov, Igor and Khaled, Ahmed and Burlachenko, Konstantin and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2206.07021},
  year={2022}
}

@article{schmidt2017minimizing,
  author    = {Mark Schmidt and
               Nicolas Le Roux and
               Francis R. Bach},
  title     = {Minimizing finite sums with the stochastic average gradient},
  journal   = {Mathematical Programming},
  volume    = {162},
  number    = {1-2},
  pages     = {83--112},
  year      = {2017},
}

@inproceedings{smith2018don,
  title={{Don't Decay the Learning Rate, Increase the Batch Size}},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{soviany2022curriculum,
  title={{Curriculum learning: A survey}},
  author={Soviany, Petru and Ionescu, Radu Tudor and Rota, Paolo and Sebe, Nicu},
  journal={International Journal of Computer Vision},
  pages={1--40},
  year={2022},
  publisher={Springer}
}

@inproceedings{welling2009herding,
  title={Herding dynamical weights to learn},
  author={Welling, Max},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={1121--1128},
  year={2009}
}

@inproceedings{ying2017performance,
  title={On the performance of random reshuffling in stochastic learning},
  author={Ying, Bicheng and Yuan, Kun and Vlaski, Stefan and Sayed, Ali H.},
  booktitle={2017 Information Theory and Applications Workshop (ITA)},
  pages={1--5},
  year={2017},
  organization={IEEE}
}

@inproceedings{yun2021can,
  author    = {Chulhee Yun and
               Suvrit Sra and
               Ali Jadbabaie},
  title     = {{Open Problem: Can Single-Shuffle {SGD} be Better than Reshuffling {SGD} and {GD}?}},
  booktitle = {Conference on Learning Theory},
  year      = {2021},
}

@inproceedings{yun2021minibatch,
  title={{Minibatch vs Local SGD with Shuffling: Tight Convergence Bounds and Beyond}},
  author={Yun, Chulhee and Rajput, Shashank and Sra, Suvrit},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{yuan2022decentralized,
  title={Decentralized training of foundation models in heterogeneous environments},
  author={Yuan, Binhang and He, Yongjun and Davis, Jared and Zhang, Tianyi and Dao, Tri and Chen, Beidi and Liang, Percy S and Re, Christopher and Zhang, Ce},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={25464--25477},
  year={2022}
}

@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@article{MAKRIDAKIS202054,
title = {The M4 Competition: 100,000 time series and 61 forecasting methods},
journal = {International Journal of Forecasting},
volume = {36},
number = {1},
pages = {54-74},
year = {2020},
note = {M4 Competition},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2019.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0169207019301128},
author = {Spyros Makridakis and Evangelos Spiliotis and Vassilios Assimakopoulos},
keywords = {Forecasting competitions, M competitions, Forecasting accuracy, Prediction intervals, Time series methods, Machine learning methods, Benchmarking methods, Practice of forecasting},
abstract = {The M4 Competition follows on from the three previous M competitions, the purpose of which was to learn from empirical evidence both how to improve the forecasting accuracy and how such learning could be used to advance the theory and practice of forecasting. The aim of M4 was to replicate and extend the three previous competitions by: (a) significantly increasing the number of series, (b) expanding the number of forecasting methods, and (c) including prediction intervals in the evaluation process as well as point forecasts. This paper covers all aspects of M4 in detail, including its organization and running, the presentation of its results, the top-performing methods overall and by categories, its major findings and their implications, and the computational requirements of the various methods. Finally, it summarizes its main conclusions and states the expectation that its series will become a testing ground for the evaluation of new methods and the improvement of the practice of forecasting, while also suggesting some ways forward for the field.}
}
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}
@inproceedings{merity2017regularizing,
  title={Regularizing and Optimizing LSTM Language Models},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@misc{nccl,
  title={{NVIDIA Collective Communication Library}},
  author={NVIDIA},
  url = 	 {https://https://developer.nvidia.com/nccl},
  year= {2023}
}
@misc{gloo,
  title={{Collective communications library with various primitives for multi-machine training}},
  url = 	 {https://github.com/facebookincubator/gloo},
  year= {2023}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}
@article{inan2016tying,
  title={Tying word vectors and word classifiers: A loss framework for language modeling},
  author={Inan, Hakan and Khosravi, Khashayar and Socher, Richard},
  journal={arXiv preprint arXiv:1611.01462},
  year={2016}
}
@article{spiliotis2020forecasting,
  title={Are forecasting competitions data representative of the reality?},
  author={Spiliotis, Evangelos and Kouloumos, Andreas and Assimakopoulos, Vassilios and Makridakis, Spyros},
  journal={International Journal of Forecasting},
  volume={36},
  number={1},
  pages={37--53},
  year={2020},
  publisher={Elsevier}
}
@article{stephen2017pointer,
  title={Pointer sentinel mixture models},
  author={Stephen, Merity and Caiming, Xiong and James, Bradbury and Socher, Richard},
  journal={Proceedings of ICLR},
  year={2017}
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
@inproceedings{devlin2019bert,
  title={{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4171--4186},
  year={2019}
}
@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}
@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}