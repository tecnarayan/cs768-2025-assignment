\begin{thebibliography}{10}

\bibitem{CovThom06}
T.~M. Cover and J.~A. Thomas.
\newblock {\em Elements of Information Theory}.
\newblock Wiley, New-York, 2nd edition, 2006.

\bibitem{ElGamal2011}
A.~{El Gamal} and Y.-H. Kim.
\newblock {\em Network Information Theory}.
\newblock Cambridge University Press, 2011.

\bibitem{haussler1994bounds}
D.~Haussler, M.~Kearns, and R.~E. Schapire.
\newblock Bounds on the sample complexity of {B}ayesian learning using
  information theory and the {VC} dimension.
\newblock {\em Machine learning}, 14(1):83--113, Jan. 1994.

\bibitem{battiti1994using}
R.~Battiti.
\newblock Using mutual information for selecting features in supervised neural
  net learning.
\newblock {\em IEEE Transactions on Neural Networks}, 5(4):537--550, Jul. 1994.

\bibitem{viola1997alignment}
P.~Viola and W.~M.~Wells III.
\newblock Alignment by maximization of mutual information.
\newblock {\em International Journal of Computer Vision}, 24(2):137--154, Sep.
  1997.

\bibitem{chen2016infogan}
Xi~Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter
  Abbeel.
\newblock Info{GAN}: Interpretable representation learning by information
  maximizing generative adversarial nets.
\newblock In {\em Proceedings of the International Conference on Advances in
  Neural Information Processing Systems (NeurIPS-2016)}, 2016.

\bibitem{alemi2017deep}
A.~A. Alemi, I.~Fischer, J.~V. Dillon, and K.~Murphy.
\newblock Deep variational information bottleneck.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR-2017)}, Toulon, France, Apr. 2017.

\bibitem{higgins2017beta}
I.~Higgins, L.~Matthey, A.~Pal, C.~Burgess, X.~Glorot, M.~Botvinick,
  S.~Mohamed, and A.~Lerchner.
\newblock $\beta$-{VAE:} learning basic visual concepts with a constrained
  variational framework.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR-2019)}, New Orleans, Louisiana, USA, May 2017.

\bibitem{DNNs_Tishby2017}
R.~Shwartz-Ziv and N.~Tishby.
\newblock Opening the black box of deep neural networks via information.
\newblock {\em arXiv preprint arXiv:1703.00810}, 2017.

\bibitem{oord2018representation}
A.~van~den Oord, Y.~Li, and O.~Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv preprint arXiv:1807.03748}, 2018.

\bibitem{achille2018information}
A.~Achille and S.~Soatto.
\newblock Information dropout: {L}earning optimal representations through noisy
  computation.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  40(12):2897--2905, Jan. 2018.

\bibitem{gabrie2018entropy}
M.~Gabri{\'e}, A.~Manoel, C.~Luneau, J.~Barbier, N.~Macris, F.~Krzakala, and
  L.~Zdeborov{\'a}.
\newblock Entropy and mutual information in models of deep neural networks.
\newblock {\em arXiv preprint arXiv:1805.09785}, 2018.

\bibitem{ICML_Info_flow2019}
Z.~Goldfeld, E.~van~den Berg, K.~Greenewald, I.~Melnyk, N.~Nguyen,
  B.~Kingsbury, and Y.~Polyanskiy.
\newblock Estimating information flow in neural networks.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning (ICML-2019)}, volume~97, pages 2299--2308, Long Beach, CA, US, Jun.
  2019.

\bibitem{goldfeld2020information}
Z.~Goldfeld and Y.~Polyanskiy.
\newblock The information bottleneck problem and its applications in machine
  learning.
\newblock {\em IEEE Journal on Selected Areas in Information Theory},
  1(1):19--38, Apr. 2020.

\bibitem{Paninski2003}
L.~Paninski.
\newblock Estimation of entropy and mutual information.
\newblock {\em Neural Computation}, 15:1191--1253, June 2003.

\bibitem{mcallester2020formal}
D.~McAllester and K.~Stratos.
\newblock Formal limitations on the measurement of mutual information.
\newblock In {\em Proceedings of the International Conference on Artificial
  Intelligence and Statistics}, pages 875--884. PMLR, 2020.

\bibitem{goldfeld2021sliced}
Z.~Goldfeld and K.~Greenewald.
\newblock Sliced mutual information: A scalable measure of statistical
  dependence.
\newblock In {\em Proceedings of the International Conference on Advances in
  Neural Information Processing Systems (NeurIPS-2021)}, Online, 2021.

\bibitem{rabin2011wasserstein}
J.~Rabin, G.~Peyr{\'e}, J.~Delon, and M.~Bernot.
\newblock Wasserstein barycenter and its application to texture mixing.
\newblock In {\em Proceedings of the International Conference on Scale Space
  and Variational Methods in Computer Vision (SSVM-2011)}, pages 435--446,
  Gedi, Israel, May 2011.

\bibitem{vayer2019sliced}
T.~Vayer, R.~Flamary, R.~Tavenard, L.~Chapel, and N.~Courty.
\newblock Sliced {G}romov-{W}asserstein.
\newblock In {\em Proceedings of the Annual Conference on Advances in Neural
  Information Processing Systems (NeurIPS-2019)}, Vancouver, Canada, Dec. 2019.

\bibitem{lin2021projection}
T.~Lin, Z.~Zheng, E.~Chen, M.~Cuturi, and M.~Jordan.
\newblock On projection robust optimal transport: Sample complexity and model
  misspecification.
\newblock In {\em Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS-2019)}, pages 262--270, Online, 2021.

\bibitem{nadjahi2020statistical}
K.~Nadjahi, A.~Durmus, L.~Chizat, S.~Kolouri, S.~Shahrampour, and U.~Simsekli.
\newblock Statistical and topological properties of sliced probability
  divergences.
\newblock In {\em Proceedings of the International Conference on Advances in
  Neural Information Processing Systems (NeurIPS-2020)}, Online, Dec. 2020.

\bibitem{otto2000generalization}
F.~Otto and C.~Villani.
\newblock Generalization of an inequality by {T}alagrand and links with the
  logarithmic sobolev inequality.
\newblock {\em Journal of Functional Analysis}, 173(2):361--400, 2000.

\bibitem{gentil2020entropic}
I.~Gentil, C.~L{\'e}onard, L.~Ripani, and L.~Tamanini.
\newblock An entropic interpolation proof of the {HWI} inequality.
\newblock {\em Stochastic Processes and their Applications}, 130(2):907--923,
  2020.

\bibitem{PolyWu_Wasserstein2016}
Y.~Polyanskiy and Y.~Wu.
\newblock Wasserstein continuity of entropy and outer bounds for interference
  channels.
\newblock {\em IEEE Transactions on Information Theory}, 62(7):3992--4002, Jul.
  2016.

\bibitem{belghazi2018mine}
M.~I. Belghazi, A.~Baratin, S.~Rajeswar, S.~Ozair, Y.~Bengio, A.~Courville, and
  R.~D. Hjelm.
\newblock Mutual information neural estimation.
\newblock In {\em Proceedings of the International Conference on Machine
  Learning (ICML-2018)}, volume~80, pages 531--540, Jul. 2018.

\bibitem{poole2018variational}
B.~Poole, S.~Ozair, A.~van~den Oord, A.~A. Alemi, and G.~Tucker.
\newblock On variational lower bounds of mutual information.
\newblock In {\em NeurIPS Workshop on Bayesian Deep Learning}, 2018.

\bibitem{song2019understanding}
J.~Song and S.~Ermon.
\newblock Understanding the limitations of variational mutual information
  estimators.
\newblock {\em arXiv preprint arXiv:1910.06222}, 2019.

\bibitem{chan2019neural}
C.~Chan, A.~Al-Bashabsheh, H.~P. Huang, M.~Lim, D.~S.~H. Tam, and C.~Zhao.
\newblock Neural entropic estimation: A faster path to mutual information
  estimation.
\newblock {\em arXiv preprint arXiv:1905.12957}, 2019.

\bibitem{sreekumar2022neural}
S.~Sreekumar and Z.~Goldfeld.
\newblock Neural estimation of statistical divergences.
\newblock {\em Journal of Machine Learning Research}, 2022.

\bibitem{meckes2012approximation}
E.~Meckes.
\newblock Approximation of projections of random vectors.
\newblock {\em Journal of Theoretical Probability}, 25(2):333--352, 2012.

\bibitem{meckes2012projections}
E.~Meckes.
\newblock Projections of probability distributions: A measure-theoretic
  {D}voretzky theorem.
\newblock In {\em Geometric aspects of functional analysis}, pages 317--326.
  Springer, 2012.

\bibitem{reeves2017conditional}
G.~Reeves.
\newblock Conditional central limit theorems for {G}aussian projections.
\newblock In {\em Proceedings of IEEE International Symposium on Information
  Theory (ISIT-2017)}, pages 3045--3049. IEEE, 2017.

\bibitem{chikuse2003statistics}
Y.~Chikuse.
\newblock {\em Statistics on special manifolds}, volume 174.
\newblock Springer Science \& Business Media, 2003.

\bibitem{gander1980algorithms}
Walter Gander.
\newblock Algorithms for the qr decomposition.
\newblock {\em Research Report}, 80(02):1251--1268, 1980.

\bibitem{sreekumar2021non}
S.~Sreekumar, Z.~Zhang, and Z.~Goldfeld.
\newblock Non-asymptotic performance guarantees for neural estimation of
  $f$-divergences.
\newblock In {\em Proceedings of the International Conference on Artificial
  Intelligence and Statistics (AISTATS-2021)}, pages 3322--3330, 2021.

\bibitem{nadjahi2021fast}
K.~Nadjahi, A.~Durmus, P.E Jacob, R.~Badeau, and U.~Simsekli.
\newblock Fast approximation of the sliced-{W}asserstein distance using
  concentration of random projections.
\newblock In {\em Proceedings of the International Conference on Advances in
  Neural Information Processing Systems (NeurIPS-2021)}, Online, 2021.

\bibitem{rakotomamonjy2021statistical}
A.~Rakotomamonjy, M.~Z. Alaya, M.~Berar, and G.~Gasso.
\newblock Statistical and topological properties of gaussian smoothed sliced
  probability divergences.
\newblock {\em arXiv preprint arXiv:2110.10524}, 2021.

\bibitem{kraskov2004estimating}
H.~St{\"o}gbauer A.~Kraskov and P.~Grassberger.
\newblock Estimating mutual information.
\newblock {\em Physical Review E}, 69(6):066138, June 2004.

\bibitem{kolouri2019generalized}
Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde.
\newblock Generalized sliced {W}asserstein distances.
\newblock In {\em Proceedings of the International Conference on Neural
  Information Processing Systems (NeurIPS-2019)}, volume~32, pages 261--272,
  Vancouver, Canada, 2019.

\bibitem{nguyen2020distributional}
Khai Nguyen, Nhat Ho, Tung Pham, and Hung Bui.
\newblock Distributional sliced-wasserstein and applications to generative
  modeling.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations (ICLR-2020)}, Online, 2020.

\bibitem{rioul2010information}
O.~Rioul.
\newblock Information theoretic proofs of entropy power inequalities.
\newblock {\em IEEE Transactions on Information Theory}, 57(1):33--55, 2010.

\bibitem{anderson2010introduction}
G.~W. Anderson, A.~Guionnet, and O.~Zeitouni.
\newblock {\em An introduction to random matrices}.
\newblock Number 118. Cambridge university press, 2010.

\bibitem{raginsky2013concentration}
M.~Raginsky and I.~Sason.
\newblock Concentration of measure inequalities in information theory,
  communications, and coding.
\newblock {\em Foundations and Trends{\textregistered} in Communications and
  Information Theory}, 10(1-2):1--246, 2013.
\newblock 2nd edition.

\bibitem{chikuse:1990}
Y.~Chikuse.
\newblock The matrix angular central {G}aussian distribution.
\newblock {\em Journal of Multivariate Analysis}, 33(2):265--275, 1990.

\bibitem{cai2022non}
T.~T. Cai, R.~Han, and A.~R. Zhang.
\newblock On the non-asymptotic concentration of heteroskedastic {W}ishart-type
  matrix.
\newblock {\em Electronic Journal of Probability}, 27:1--40, 2022.

\bibitem{imori2020mean}
Shinpei Imori and Dietrich Von~Rosen.
\newblock On the mean and dispersion of the {Moore-Penrose} generalized inverse
  of a {W}ishart matrix.
\newblock {\em The Electronic Journal of Linear Algebra}, 36:124--133, 2020.

\end{thebibliography}
