\begin{thebibliography}{10}

\bibitem{brown2020language}
T.~B. {Brown}, B.~{Mann}, N.~{Ryder}, M.~{Subbiah}, J.~{Kaplan}, P.~{Dhariwal},
  A.~{Neelakantan}, P.~{Shyam}, G.~{Sastry}, A.~{Askell}, S.~{Agarwal},
  A.~{Herbert-Voss}, G.~{Krueger}, T.~{Henighan}, R.~{Child}, A.~{Ramesh},
  D.~M. {Ziegler}, J.~{Wu}, C.~{Winter}, C.~{Hesse}, M.~{Chen}, E.~{Sigler},
  M.~{Litwin}, S.~{Gray}, B.~{Chess}, J.~{Clark}, C.~{Berner}, S.~{McCandlish},
  A.~{Radford}, I.~{Sutskever}, and D.~{Amodei}.
\newblock Language models are few-shot learners.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  volume~33, pages 1877--1901, 2020.

\bibitem{Caron2020Unsupervised}
M.~Caron, I.~Misra, J.~Mairal, P.~Goyal, P.~Bojanowski, and A.~Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  33:9912--9924, 2020.

\bibitem{Chen2020Simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  1597--1607, 2020.

\bibitem{chen2020mocov2}
X.~Chen, H.~Fan, R.~Girshick, and K.~He.
\newblock Improved baselines with momentum contrastive learning.
\newblock {\em arXiv:2003.04297}, 2020.

\bibitem{cheng2021perpixel}
B.~Cheng, A.~Schwing, and A.~Kirillov.
\newblock Per-pixel classification is not all you need for semantic
  segmentation.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem{Crawshaw2020MultiTaskLW}
M.~Crawshaw.
\newblock Multi-task learning with deep neural networks: A survey.
\newblock {\em arXiv preprint arXiv:2009.09796}, 2020.

\bibitem{Devlin2019BERTPO}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{Donahue2017AdversarialFL}
J.~Donahue, P.~Kr{\"a}henb{\"u}hl, and T.~Darrell.
\newblock Adversarial feature learning.
\newblock {\em arXiv preprint arXiv:1605.09782}, 2016.

\bibitem{Donahue2019LargeSA}
J.~Donahue and K.~Simonyan.
\newblock Large scale adversarial representation learning.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  volume~32, 2019.

\bibitem{Eigen2015Predicting}
D.~Eigen and R.~Fergus.
\newblock Predicting depth, surface normals and semantic labels with a common
  multi-scale convolutional architecture.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, pages 2650--2658, 2015.

\bibitem{Fifty2021EfficientlyIT}
C.~Fifty, E.~Amid, Z.~Zhao, T.~Yu, R.~Anil, and C.~Finn.
\newblock Efficiently identifying task groupings for multi-task learning.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem{Gao2021CLIPAdapterBV}
P.~Gao, S.~Geng, R.~Zhang, T.~Ma, R.~Fang, Y.~Zhang, H.~Li, and Y.~Qiao.
\newblock Clip-adapter: Better vision-language models with feature adapters.
\newblock {\em arXiv preprint arXiv:2110.04544}, 2021.

\bibitem{Ghiasi2021Multi}
G.~Ghiasi, B.~Zoph, E.~D. Cubuk, Q.~V. Le, and T.-Y. Lin.
\newblock Multi-task self-training for learning general representations.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 8856--8865, 2021.

\bibitem{Girshick2015FastR}
R.~B. Girshick.
\newblock Fast r-cnn.
\newblock pages 1440--1448, 2015.

\bibitem{Grill2020Bootstrap}
J.-B. Grill, F.~Strub, F.~Altch\'{e}, C.~Tallec, P.~Richemond, E.~Buchatskaya,
  C.~Doersch, B.~Avila~Pires, Z.~Guo, M.~Gheshlaghi~Azar, B.~Piot,
  k.~kavukcuoglu, R.~Munos, and M.~Valko.
\newblock Bootstrap your own latent: a new approach to self-supervised
  learning.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  33:21271--21284, 2020.

\bibitem{gu2022openvocabulary}
X.~Gu, T.-Y. Lin, W.~Kuo, and Y.~Cui.
\newblock Open-vocabulary object detection via vision and language knowledge
  distillation.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2022.

\bibitem{He2020Momentum}
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 9729--9738, 2020.

\bibitem{He2019RethinkingIP}
K.~He, R.~B. Girshick, and P.~Doll{\'a}r.
\newblock Rethinking imagenet pre-training.
\newblock {\em Proceedings of the IEEE/CVF International Conference on Computer
  Vision (ICCV)}, pages 4917--4926, 2019.

\bibitem{He2016Deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 770--778, 2016.

\bibitem{2018Panoptic}
A.~Kirillov, K.~He, R.~Girshick, C.~Rother, and P.~Dollár.
\newblock Panoptic segmentation.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 9404--9413, 2019.

\bibitem{2017UberNet}
I.~Kokkinos.
\newblock Ubernet: Training a `universal' convolutional neural network for
  low-, mid-, and high-level vision using diverse datasets and limited memory.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 6129--6138, 2017.

\bibitem{Lester2021ThePO}
B.~Lester, R.~Al-Rfou, and N.~Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In {\em Proceedings of the Conference on Empirical Methods in Natural
  Language Processing (EMNLP)}, pages 3045--3059, 2021.

\bibitem{li2021prefix}
X.~L. Li and P.~Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock {\em arXiv preprint arXiv:2101.00190}, 2021.

\bibitem{Likhosherstov2021PolyViTCV}
V.~Likhosherstov, A.~Arnab, K.~Choromanski, M.~Lucic, Y.~Tay, A.~Weller, and
  M.~Dehghani.
\newblock Polyvit: Co-training vision transformers on images, videos and audio.
\newblock {\em arXiv preprint arXiv:2111.12993}, 2021.

\bibitem{Lin2017FPN}
T.-Y. Lin, P.~Dollár, R.~Girshick, K.~He, B.~Hariharan, and S.~Belongie.
\newblock Feature pyramid networks for object detection.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 936--944, 2017.

\bibitem{Liu2021PretrainPA}
P.~Liu, W.~Yuan, J.~Fu, Z.~Jiang, H.~Hayashi, and G.~Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting
  methods in natural language processing.
\newblock {\em arXiv preprint arXiv:2107.13586}, 2021.

\bibitem{Liu2021PTuningVP}
X.~Liu, K.~Ji, Y.~Fu, Z.~Du, Z.~Yang, and J.~Tang.
\newblock P-tuning v2: Prompt tuning can be comparable to fine-tuning
  universally across scales and tasks.
\newblock {\em arXiv preprint arXiv:2110.07602}, 2021.

\bibitem{liu2021gpt}
X.~Liu, Y.~Zheng, Z.~Du, M.~Ding, Y.~Qian, Z.~Yang, and J.~Tang.
\newblock Gpt understands, too.
\newblock {\em arXiv preprint arXiv:2103.10385}, 2021.

\bibitem{loshchilov2017decoupled}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{Lu202012}
J.~Lu, V.~Goswami, M.~Rohrbach, D.~Parikh, and S.~Lee.
\newblock 12-in-1: Multi-task vision and language representation learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 10437--10446, 2020.

\bibitem{peng2018megdet}
C.~Peng, T.~Xiao, Z.~Li, Y.~Jiang, X.~Zhang, K.~Jia, G.~Yu, and J.~Sun.
\newblock Megdet: A large mini-batch object detector.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 6181--6189, 2018.

\bibitem{Radford2021LearningTV}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, G.~Krueger, and I.~Sutskever.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  8748--8763, 2021.

\bibitem{Raghu2021DoVT}
M.~Raghu, T.~Unterthiner, S.~Kornblith, C.~Zhang, and A.~Dosovitskiy.
\newblock Do vision transformers see like convolutional neural networks?
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  34, 2021.

\bibitem{Rao2021DenseCLIPLD}
Y.~Rao, W.~Zhao, G.~Chen, Y.~Tang, Z.~Zhu, G.~Huang, J.~Zhou, and J.~Lu.
\newblock Denseclip: Language-guided dense prediction with context-aware
  prompting.
\newblock {\em arXiv preprint arXiv:2112.01518}, 2021.

\bibitem{Ren2015Faster}
S.~Ren, K.~He, R.~Girshick, and J.~Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  28, 2015.

\bibitem{Russakovsky2015ImageNetLS}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~S. Bernstein, A.~C. Berg, and L.~Fei-Fei.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International Journal of Computer Vision (IJCV)},
  115(3):211--252, 2015.

\bibitem{Standley2020WhichTS}
T.~S. Standley, A.~R. Zamir, D.~Chen, L.~J. Guibas, J.~Malik, and S.~Savarese.
\newblock Which tasks should be learned together in multi-task learning?
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  9120--9132, 2020.

\bibitem{Sun2021Sparse}
P.~Sun, R.~Zhang, Y.~Jiang, T.~Kong, C.~Xu, W.~Zhan, M.~Tomizuka, L.~Li,
  Z.~Yuan, C.~Wang, and P.~Luo.
\newblock Sparse r-cnn: End-to-end object detection with learnable proposals.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 14454--14463, 2021.

\bibitem{Sun2020AdaShareLW}
X.~Sun, R.~Panda, and R.~S. Feris.
\newblock Adashare: Learning what to share for efficient deep multi-task
  learning.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  33:8728--8740, 2020.

\bibitem{Szegedy2014IntriguingPO}
C.~Szegedy, W.~Zaremba, I.~Sutskever, J.~Bruna, D.~Erhan, I.~J. Goodfellow, and
  R.~Fergus.
\newblock Intriguing properties of neural networks.
\newblock {\em arXiv preprint arXiv:1312.6199}, 2013.

\bibitem{teichmann2016multinet}
M.~Teichmann, M.~Weber, M.~Zoellner, R.~Cipolla, and R.~Urtasun.
\newblock Multinet: Real-time joint semantic reasoning for autonomous driving.
\newblock In {\em IEEE Intelligent Vehicles Symposium (IV)}, pages 1013--1020,
  2018.

\bibitem{oord2019representation}
A.~van~den Oord, Y.~Li, and O.~Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv preprint arXiv:1807.03748}, 2018.

\bibitem{Vaswani2017AttentionIA}
A.~Vaswani, N.~M. Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  L.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in Neural Information Processing Systems (NeurIPS)},
  30, 2017.

\bibitem{Wang2021DenseCL}
X.~Wang, R.~Zhang, C.~Shen, T.~Kong, and L.~Li.
\newblock Dense contrastive learning for self-supervised visual pre-training.
\newblock {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 3023--3032, 2021.

\bibitem{wu2021yolop}
D.~Wu, M.~Liao, W.~Zhang, and X.~Wang.
\newblock Yolop: You only look once for panoptic driving perception.
\newblock {\em arXiv preprint arXiv:2108.11250}, 2021.

\bibitem{Xiao2018UnifiedPP}
T.~Xiao, Y.~Liu, B.~Zhou, Y.~Jiang, and J.~Sun.
\newblock Unified perceptual parsing for scene understanding.
\newblock In {\em European Conference on Computer Vision (ECCV)}, pages
  418--434, 2018.

\bibitem{Xie2021DetCoUC}
E.~Xie, J.~Ding, W.~Wang, X.~Zhan, H.~Xu, Z.~Li, and P.~Luo.
\newblock Detco: Unsupervised contrastive learning for object detection.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, pages 8392--8401, 2021.

\bibitem{Xie2020SelfTrainingWN}
Q.~Xie, E.~H. Hovy, M.-T. Luong, and Q.~V. Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 10684--10695, 2020.

\bibitem{Xie2021PropagateYE}
Z.~Xie, Y.~Lin, Z.~Zhang, Y.~Cao, S.~Lin, and H.~Hu.
\newblock Propagate yourself: Exploring pixel-level consistency for
  unsupervised visual representation learning.
\newblock {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 16679--16688, 2021.

\bibitem{Yang2018EndtoendMM}
Z.~Yang, Y.~Zhang, J.~Yu, J.~Cai, and J.~Luo.
\newblock End-to-end multi-modal multi-task vehicle control for self-driving
  cars with visual perceptions.
\newblock {\em 24th International Conference on Pattern Recognition (ICPR)},
  pages 2289--2294, 2018.

\bibitem{yao2021cpt}
Y.~Yao, A.~Zhang, Z.~Zhang, Z.~Liu, T.-S. Chua, and M.~Sun.
\newblock Cpt: Colorful prompt tuning for pre-trained vision-language models.
\newblock {\em arXiv preprint arXiv:2109.11797}, 2021.

\bibitem{bdd100k}
F.~Yu, H.~Chen, X.~Wang, W.~Xian, Y.~Chen, F.~Liu, V.~Madhavan, and T.~Darrell.
\newblock Bdd100k: A diverse driving dataset for heterogeneous multitask
  learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 2636--2645, 2020.

\bibitem{zhong2021factual}
Z.~Zhong, D.~Friedman, and D.~Chen.
\newblock Factual probing is [mask]: Learning vs. learning to recall.
\newblock {\em arXiv preprint arXiv:2104.05240}, 2021.

\bibitem{Zhou2021DenseCLIPEF}
C.~Zhou, C.~C. Loy, and B.~Dai.
\newblock Denseclip: Extract free dense labels from clip.
\newblock {\em arXiv preprint arXiv:2112.01071}, 2021.

\bibitem{Zhou2021LearningTP}
K.~Zhou, J.~Yang, C.~C. Loy, and Z.~Liu.
\newblock Learning to prompt for vision-language models.
\newblock {\em arXiv preprint arXiv:2109.01134}, 2021.

\bibitem{Zhu2021UniPerceiverPU}
X.~Zhu, J.~Zhu, H.~Li, X.~Wu, X.~Wang, H.~Li, X.~Wang, and J.~Dai.
\newblock Uni-perceiver: Pre-training unified architecture for generic
  perception for zero-shot and few-shot tasks.
\newblock {\em arXiv preprint arXiv:2112.01522}, 2021.

\end{thebibliography}
