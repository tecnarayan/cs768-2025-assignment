@string{JASA = {Journal of the American Statistical Association}}
@string{JC  = {Journal of Classification}}
@string{JSPI  = {Journal of Statistical Planning and Inference}}
@string{JRSSB  = {Journal of the Royal Statistical Society, Series B}}
@string{SAGMB  = {Statistical Applications in Genetics and Molecular Biology}}
@string{NIPS  = {Advances in Neural Information Processing Systems}}
@string{AOS  = {Annals of Statistics}}
@string{AOAS  = {Annals of Applied Statistics}}
@string{JMLR  = {Journal of Machine Learning Research}}
@string{EJS  = {Electronic Journal of Statistics}}
@string{AISTATS  = {International Conference on Artificial Intelligence and Statistics}}
@string{UAI  = {Conference on Uncertainty in Artificial Intelligence}}
@string{ICML  = {International Conference on Machine Learning}}
@string{COLT  = {Conference on Learning Theory}}
@string{TIT = {IEEE Transactions on Information Theory}}



@article{liu2022partially,
  title={When Is Partially Observable Reinforcement Learning Not Scary?},
  author={Liu, Qinghua and Chung, Alan and Szepesv{\'a}ri, Csaba and Jin, Chi},
  journal={arXiv preprint arXiv:2204.08967},
  year={2022}
}
@article{zhong2021can,
  title={Can Reinforcement Learning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopic Followers?},
  author={Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  journal={arXiv preprint arXiv:2112.13521},
  year={2021}
}
@misc{liu2023objective,
      title={One Objective to Rule Them All: A Maximization Objective Fusing Estimation and Planning for Exploration}, 
      author={Zhihan Liu and Miao Lu and Wei Xiong and Han Zhong and Hao Hu and Shenao Zhang and Sirui Zheng and Zhuoran Yang and Zhaoran Wang},
      year={2023},
      eprint={2305.18258},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{xie2020learning,
  title={Learning zero-sum simultaneous-move markov games using function approximation and correlated equilibrium},
  author={Xie, Qiaomin and Chen, Yudong and Wang, Zhaoran and Yang, Zhuoran},
  booktitle={Conference on learning theory},
  pages={3674--3682},
  year={2020},
  organization={PMLR}
}

@article{foster2023complexity,
  title={On the Complexity of Multi-Agent Decision Making: From Learning in Games to Partial Monitoring},
  author={Foster, Dylan J and Foster, Dean P and Golowich, Noah and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2305.00684},
  year={2023}
}

@article{dann2021provably,
  title={A provably efficient model-free posterior sampling method for episodic reinforcement learning},
  author={Dann, Christoph and Mohri, Mehryar and Zhang, Tong and Zimmert, Julian},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12040--12051},
  year={2021}
}

@article{agarwal2022model,
  title={Model-based RL with Optimistic Posterior Sampling: Structural Conditions and Sample Complexity},
  author={Agarwal, Alekh and Zhang, Tong},
  journal={arXiv preprint arXiv:2206.07659},
  year={2022}
}

@article{cui2023breaking,
  title={Breaking the Curse of Multiagents in a Large State Space: RL in Markov Games with Independent Linear Function Approximation},
  author={Cui, Qiwen and Zhang, Kaiqing and Du, Simon S},
  journal={arXiv preprint arXiv:2302.03673},
  year={2023}
}

@inproceedings{zou2020pseudo,
  title={Pseudo Dyna-Q: A reinforcement learning framework for interactive recommendation},
  author={Zou, Lixin and Xia, Long and Du, Pan and Zhang, Zhuo and Bai, Ting and Liu, Weidong and Nie, Jian-Yun and Yin, Dawei},
  booktitle={Proceedings of the 13th International Conference on Web Search and Data Mining},
  pages={816--824},
  year={2020}
}
@inproceedings{chen2019top,
  title={Top-k off-policy correction for a REINFORCE recommender system},
  author={Chen, Minmin and Beutel, Alex and Covington, Paul and Jain, Sagar and Belletti, Francois and Chi, Ed H},
  booktitle={Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
  pages={456--464},
  year={2019}
}
@article{yu2021reinforcement,
  title={Reinforcement learning in healthcare: A survey},
  author={Yu, Chao and Liu, Jiming and Nemati, Shamim and Yin, Guosheng},
  journal={ACM Computing Surveys (CSUR)},
  volume={55},
  number={1},
  pages={1--36},
  year={2021},
  publisher={ACM New York, NY}
}

@inproceedings{mirhoseini2017device,
  title={Device placement optimization with reinforcement learning},
  author={Mirhoseini, Azalia and Pham, Hieu and Le, Quoc V and Steiner, Benoit and Larsen, Rasmus and Zhou, Yuefeng and Kumar, Naveen and Norouzi, Mohammad and Bengio, Samy and Dean, Jeff},
  booktitle={International Conference on Machine Learning},
  pages={2430--2439},
  year={2017},
  organization={PMLR}
}

@article{wang2023breaking,
  title={Breaking the curse of multiagency: Provably efficient decentralized multi-agent rl with function approximation},
  author={Wang, Yuanhao and Liu, Qinghua and Bai, Yu and Jin, Chi},
  journal={arXiv preprint arXiv:2302.06606},
  year={2023}
}

@inproceedings{sun2019model,
  title={Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches},
  author={Sun, Wen and Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
  booktitle={Conference on learning theory},
  pages={2898--2933},
  year={2019},
  organization={PMLR}
}

@article{zhang2006,
  title={From $\varepsilon$-entropy to KL-entropy: Analysis of minimum information complexity density estimation},
  author={Zhang, Tong},
  journal={The Annals of Statistics},
  volume={34},
  number={5},
  pages={2180--2210},
  year={2006},
  publisher={Institute of Mathematical Statistics}
}



@article{gao2019batched,
  title={Batched multi-armed bandits problem},
  author={Gao, Zijun and Han, Yanjun and Ren, Zhimei and Zhou, Zhengqing},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{perchet2016batched,
  title={Batched bandit problems},
  author={Perchet, Vianney and Rigollet, Philippe and Chassang, Sylvain and Snowberg, Erik},
  journal={The Annals of Statistics},
  pages={660--681},
  year={2016},
  publisher={JSTOR}
}

@article{qiao2023logarithmic,
  title={Logarithmic Switching Cost in Reinforcement Learning beyond Linear MDPs},
  author={Qiao, Dan and Yin, Ming and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:2302.12456},
  year={2023}
}

@article{han2020sequential,
  title={Sequential batch learning in finite-action linear contextual bandits},
  author={Han, Yanjun and Zhou, Zhengqing and Zhou, Zhengyuan and Blanchet, Jose and Glynn, Peter W and Ye, Yinyu},
  journal={arXiv preprint arXiv:2004.06321},
  year={2020}
}

@article{agarwal2020flambe,
  title={Flambe: Structural complexity and representation learning of low rank mdps},
  author={Agarwal, Alekh and Kakade, Sham and Krishnamurthy, Akshay and Sun, Wen},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={20095--20107},
  year={2020}
}
@article{quanquangu2021linearlowcost,
  title={Provably efficient reinforcement learning with linear function approximation under adaptivity constraints},
  author={Wang, Tianhao and Zhou, Dongruo and Gu, Quanquan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={13524--13536},
  year={2021}
}

@article{zhang2020lowswitchcosttabular,
  title={Almost optimal model-free reinforcement learningvia reference-advantage decomposition},
  author={Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15198--15207},
  year={2020}
}

@article{baiyu2019lowswitchcosttabular,
  title={Provably efficient q-learning with low switching cost},
  author={Bai, Yu and Xie, Tengyang and Jiang, Nan and Wang, Yu-Xiang},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{li2017deep,
  title={Deep reinforcement learning: An overview},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1701.07274},
  year={2017}
}
@inproceedings{ishfaq2021randomized,
  title={Randomized exploration in reinforcement learning with general value function approximation},
  author={Ishfaq, Haque and Cui, Qiwen and Nguyen, Viet and Ayoub, Alex and Yang, Zhuoran and Wang, Zhaoran and Precup, Doina and Yang, Lin},
  booktitle={International Conference on Machine Learning},
  pages={4607--4616},
  year={2021},
  organization={PMLR}
}
@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group UK London}
}
@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}
@article{akkaya2019solving,
  title={Solving rubik's cube with a robot hand},
  author={Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and others},
  journal={arXiv preprint arXiv:1910.07113},
  year={2019}
}
@article{singh2022reinforcement,
  title={Reinforcement learning in robotic applications: a comprehensive survey},
  author={Singh, Bharat and Kumar, Rajesh and Singh, Vinay Pratap},
  journal={Artificial Intelligence Review},
  pages={1--46},
  year={2022},
  publisher={Springer}
}
@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{zhuoranyang2022lowcosteluder,
  title={Reinforcement Learning with Logarithmic Regret and Policy Switches},
  author={Velegkas, Grigoris and Yang, Zhuoran and Karbasi, Amin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36040--36053},
  year={2022}
}


@article{jin2021bellman,
  title={Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms},
  author={Jin, Chi and Liu, Qinghua and Miryoosefi, Sobhan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{du2021bilinear,
  title={Bilinear classes: A structural framework for provable generalization in rl},
  author={Du, Simon and Kakade, Sham and Lee, Jason and Lovett, Shachar and Mahajan, Gaurav and Sun, Wen and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={2826--2836},
  year={2021},
  organization={PMLR}
}

@article{jaeger2000observable,
  title={Observable operator models for discrete stochastic time series},
  author={Jaeger, Herbert},
  journal={Neural computation},
  volume={12},
  number={6},
  pages={1371--1398},
  year={2000},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{jin2020sample,
  title={Sample-efficient reinforcement learning of undercomplete POMDPs},
  author={Jin, Chi and Kakade, Sham and Krishnamurthy, Akshay and Liu, Qinghua},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18530--18539},
  year={2020}
}

@article{littman2001predictive,
  title={Predictive representations of state},
  author={Littman, Michael and Sutton, Richard S},
  journal={Advances in neural information processing systems},
  volume={14},
  year={2001}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}


@article{osband2014model,
  title={Model-based reinforcement learning and the eluder dimension},
  author={Osband, Ian and Van Roy, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={27},
  year={2014}
}


@article{jin2018q,
  title={Is Q-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}

@inproceedings{yang2019sample,
  title={Sample-optimal parametric q-learning using linearly additive features},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={6995--7004},
  year={2019},
  organization={PMLR}
}

@InProceedings{jiang@2017,
  title = 	 {Contextual Decision Processes with low {B}ellman rank are {PAC}-Learnable},
  author =       {Nan Jiang and Akshay Krishnamurthy and Alekh Agarwal and John Langford and Robert E. Schapire},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1704--1713},
  year = 	 {2017},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR}
}

@inproceedings{cai2020provably,
  title={Provably efficient exploration in policy optimization},
  author={Cai, Qi and Yang, Zhuoran and Jin, Chi and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={1283--1294},
  year={2020},
  organization={PMLR}
}

@article{wang2020reinforcement,
  title={Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension},
  author={Wang, Ruosong and Salakhutdinov, Russ R and Yang, Lin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6123--6135},
  year={2020}
}

@inproceedings{ayoub2020model,
  title={Model-based reinforcement learning with value-targeted regression},
  author={Ayoub, Alex and Jia, Zeyu and Szepesvari, Csaba and Wang, Mengdi and Yang, Lin},
  booktitle={International Conference on Machine Learning},
  pages={463--474},
  year={2020},
  organization={PMLR}
}

@article{wang2019optimism,
  title={Optimism in reinforcement learning with generalized linear function approximation},
  author={Wang, Yining and Wang, Ruosong and Du, Simon S and Krishnamurthy, Akshay},
  journal={arXiv preprint arXiv:1912.04136},
  year={2019}
}

@inproceedings{du2019provably,
  title={Provably efficient RL with rich observations via latent state decoding},
  author={Du, Simon and Krishnamurthy, Akshay and Jiang, Nan and Agarwal, Alekh and Dudik, Miroslav and Langford, John},
  booktitle={International Conference on Machine Learning},
  pages={1665--1674},
  year={2019},
  organization={PMLR}
}

@article{efroni2022provable,
  title={Provable reinforcement learning with a short-term memory},
  author={Efroni, Yonathan and Jin, Chi and Krishnamurthy, Akshay and Miryoosefi, Sobhan},
  journal={arXiv preprint arXiv:2202.03983},
  year={2022}
}



@article{zhan2022pac,
  title={Pac reinforcement learning for predictive state representations},
  author={Zhan, Wenhao and Uehara, Masatoshi and Sun, Wen and Lee, Jason D},
  journal={arXiv preprint arXiv:2207.05738},
  year={2022}
}

@article{uehara2022provably,
  title={Provably efficient reinforcement learning in partially observable dynamical systems},
  author={Uehara, Masatoshi and Sekhari, Ayush and Lee, Jason D and Kallus, Nathan and Sun, Wen},
  journal={arXiv preprint arXiv:2206.12020},
  year={2022}
}

@article{zhang2022feel,
  title={Feel-good thompson sampling for contextual bandits and reinforcement learning},
  author={Zhang, Tong},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={4},
  number={2},
  pages={834--857},
  year={2022},
  publisher={SIAM}
}

@inproceedings{cai2022reinforcement,
  title={Reinforcement Learning from Partial Observation: Linear Function Approximation with Provable Sample Efficiency},
  author={Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={2485--2522},
  year={2022},
  organization={PMLR}
}

@article{wang2022embed,
  title={Embed to Control Partially Observed Systems: Representation Learning with Provable Sample Efficiency},
  author={Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2205.13476},
  year={2022}
}

@article{abbasi2011improved,
  title={Improved algorithms for linear stochastic bandits},
  author={Abbasi-Yadkori, Yasin and P{\'a}l, D{\'a}vid and Szepesv{\'a}ri, Csaba},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}


@article{kwon2021rl,
  title={RL for latent MDPs: Regret guarantees and a lower bound},
  author={Kwon, Jeongyeol and Efroni, Yonathan and Caramanis, Constantine and Mannor, Shie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24523--24534},
  year={2021}
}


@InProceedings{xiong22b,
  title = 	 {A Self-Play Posterior Sampling Algorithm for Zero-Sum {M}arkov Games},
  author =       {Xiong, Wei and Zhong, Han and Shi, Chengshuai and Shen, Cong and Zhang, Tong},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {24496--24523},
  year = 	 {2022},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR}
}


@article{golowich2022learning,
  title={Learning in Observable POMDPs, without Computationally Intractable Oracles},
  author={Golowich, Noah and Moitra, Ankur and Rohatgi, Dhruv},
  journal={arXiv preprint arXiv:2206.03446},
  year={2022}
}

@article{golowich2022planning,
  title={Planning in observable POMDPs in quasipolynomial time},
  author={Golowich, Noah and Moitra, Ankur and Rohatgi, Dhruv},
  journal={arXiv preprint arXiv:2201.04735},
  year={2022}
}

@inproceedings{zhou2021nearly,
  title={Nearly minimax optimal reinforcement learning for linear mixture markov decision processes},
  author={Zhou, Dongruo and Gu, Quanquan and Szepesvari, Csaba},
  booktitle={Conference on Learning Theory},
  pages={4532--4576},
  year={2021},
  organization={PMLR}
}

@article{russo2013eluder,
  title={Eluder dimension and the sample complexity of optimistic exploration},
  author={Russo, Daniel and Van Roy, Benjamin},
  journal={Advances in Neural Information Processing Systems},
  volume={26},
  year={2013}
}

@techreport{van2014probability,
  title={Probability in high dimension},
  author={Van Handel, Ramon},
  year={2014},
  institution={PRINCETON UNIV NJ}
}


@inproceedings{modi2020sample,
  title={Sample complexity of reinforcement learning using linearly combined model ensembles},
  author={Modi, Aditya and Jiang, Nan and Tewari, Ambuj and Singh, Satinder},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2010--2020},
  year={2020},
  organization={PMLR}
}

@article{krishnamurthy2016pac,
  title={PAC reinforcement learning with rich observations},
  author={Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@inproceedings{munos2003error,
  title={Error bounds for approximate policy iteration},
  author={Munos, R{\'e}mi},
  booktitle={ICML},
  volume={3},
  pages={560--567},
  year={2003}
}

@inproceedings{zanette2020learning,
  title={Learning near optimal policies with low inherent bellman error},
  author={Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={10978--10989},
  year={2020},
  organization={PMLR}
}

@article{foster2021statistical,
  title={The statistical complexity of interactive decision making},
  author={Foster, Dylan J and Kakade, Sham M and Qian, Jian and Rakhlin, Alexander},
  journal={arXiv preprint arXiv:2112.13487},
  year={2021}
}


@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018}
}


@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}


@article{auer2002finite,
  title={Finite-time analysis of the multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  journal={Machine learning},
  volume={47},
  number={2},
  pages={235--256},
  year={2002},
  publisher={Springer}
}

@article{jaksch2010near,
  title={Near-optimal Regret Bounds for Reinforcement Learning},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={11},
  pages={1563--1600},
  year={2010}
}


@article{auer2002using,
  title={Using confidence bounds for exploitation-exploration trade-offs},
  author={Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Nov},
  pages={397--422},
  year={2002}
}



@article{russo2014learning,
  title={Learning to optimize via posterior sampling},
  author={Russo, Daniel and Van Roy, Benjamin},
  journal={Mathematics of Operations Research},
  volume={39},
  number={4},
  pages={1221--1243},
  year={2014},
  publisher={INFORMS}
}

@article{thompson1933likelihood,
  title={On the likelihood that one unknown probability exceeds another in view of the evidence of two samples},
  author={Thompson, William R},
  journal={Biometrika},
  volume={25},
  number={3-4},
  pages={285--294},
  year={1933},
  publisher={Oxford University Press}
}
@article{auer2008near,
  title={Near-optimal regret bounds for reinforcement learning},
  author={Auer, Peter and Jaksch, Thomas and Ortner, Ronald},
  journal={Advances in neural information processing systems},
  volume={21},
  year={2008}
}

@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low bellman rank are PAC-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={International Conference on Machine Learning},
  pages={1704--1713},
  year={2017},
  organization={PMLR}
}


@article{agrawal2017optimistic,
  title={Optimistic posterior sampling for reinforcement learning: worst-case regret bounds},
  author={Agrawal, Shipra and Jia, Randy},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{strens2000bayesian,
  title={A Bayesian framework for reinforcement learning},
  author={Strens, Malcolm},
  booktitle=ICML,
  volume={2000},
  pages={943--950},
  year={2000}
}


@article{lale2020regret,
  title={Regret minimization in partially observable linear quadratic control},
  author={Lale, Sahin and Azizzadenesheli, Kamyar and Hassibi, Babak and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2002.00082},
  year={2020}
}

@inproceedings{osband2016generalization,
  title={Generalization and exploration via randomized value functions},
  author={Osband, Ian and Van Roy, Benjamin and Wen, Zheng},
  booktitle={International Conference on Machine Learning},
  pages={2377--2386},
  year={2016},
  organization={PMLR}
}

@article{russo2019worst,
  title={Worst-case regret bounds for exploration via randomized value functions},
  author={Russo, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{zanette2020frequentist,
  title={Frequentist regret bounds for randomized least-squares value iteration},
  author={Zanette, Andrea and Brandfonbrener, David and Brunskill, Emma and Pirotta, Matteo and Lazaric, Alessandro},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1954--1964},
  year={2020},
  organization={PMLR}
}

@article{agarwal2022non,
  title={Non-Linear Reinforcement Learning in Large Action Spaces: Structural Conditions and Sample-efficiency of Posterior Sampling},
  author={Agarwal, Alekh and Zhang, Tong},
  journal={arXiv preprint arXiv:2203.08248},
  year={2022}
}

@article{kakade2020information,
  title={Information theoretic regret bounds for online nonlinear control},
  author={Kakade, Sham and Krishnamurthy, Akshay and Lowrey, Kendall and Ohnishi, Motoya and Sun, Wen},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15312--15325},
  year={2020}
}

@article{singh2012predictive,
  title={Predictive state representations: A new theory for modeling dynamical systems},
  author={Singh, Satinder and James, Michael and Rudary, Matthew},
  journal={arXiv preprint arXiv:1207.4167},
  year={2012}
}

@inproceedings{guo2016pac,
  title={A pac rl algorithm for episodic pomdps},
  author={Guo, Zhaohan Daniel and Doroudi, Shayan and Brunskill, Emma},
  booktitle={Artificial Intelligence and Statistics},
  pages={510--518},
  year={2016},
  organization={PMLR}
}

@article{xiong2021sublinear,
  title={Sublinear regret for learning pomdps},
  author={Xiong, Yi and Chen, Ningyuan and Gao, Xuefeng and Zhou, Xiang},
  journal={arXiv preprint arXiv:2107.03635},
  year={2021}
}

@inproceedings{azizzadenesheli2016reinforcement,
  title={Reinforcement learning of POMDPs using spectral methods},
  author={Azizzadenesheli, Kamyar and Lazaric, Alessandro and Anandkumar, Animashree},
  booktitle={Conference on Learning Theory},
  pages={193--256},
  year={2016},
  organization={PMLR}
}

@article{papadimitriou1987complexity,
  title={The complexity of Markov decision processes},
  author={Papadimitriou, Christos H and Tsitsiklis, John N},
  journal={Mathematics of operations research},
  volume={12},
  number={3},
  pages={441--450},
  year={1987},
  publisher={INFORMS}
}

@article{uehara2022computationally,
  title={Computationally efficient pac rl in pomdps with latent determinism and conditional embeddings},
  author={Uehara, Masatoshi and Sekhari, Ayush and Lee, Jason D and Kallus, Nathan and Sun, Wen},
  journal={arXiv preprint arXiv:2206.12081},
  year={2022}
}

@article{jiang2018completing,
  title={Completing State representations using spectral learning},
  author={Jiang, Nan and Kulesza, Alex and Singh, Satinder},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{hefny2015supervised,
  title={Supervised learning for dynamical system learning},
  author={Hefny, Ahmed and Downey, Carlton and Gordon, Geoffrey J},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}



@inproceedings{zhang2021reinforcement,
  title={Reinforcement Learning under a Multi-agent Predictive State Representation Model: Method and Theory},
  author={Zhang, Zhi and Yang, Zhuoran and Liu, Han and Tokekar, Pratap and Huang, Furong},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{brafman2002r,
  title={R-max-a general polynomial time algorithm for near-optimal reinforcement learning},
  author={Brafman, Ronen I and Tennenholtz, Moshe},
  journal={Journal of Machine Learning Research},
  volume={3},
  number={Oct},
  pages={213--231},
  year={2002}
}

@article{boots2011closing,
  title={Closing the learning-planning loop with predictive state representations},
  author={Boots, Byron and Siddiqi, Sajid M and Gordon, Geoffrey J},
  journal={The International Journal of Robotics Research},
  volume={30},
  number={7},
  pages={954--966},
  year={2011},
  publisher={SAGE Publications Sage UK: London, England}
}

@article{dann2017unifying,
  title={Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning},
  author={Dann, Christoph and Lattimore, Tor and Brunskill, Emma},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{osband2016deep,
  title={Deep exploration via bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{lu2017ensemble,
  title={Ensemble sampling},
  author={Lu, Xiuyuan and Van Roy, Benjamin},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{chua2018deep,
  title={Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
  author={Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{nagabandi2020deep,
  title={Deep dynamics models for learning dexterous manipulation},
  author={Nagabandi, Anusha and Konolige, Kurt and Levine, Sergey and Kumar, Vikash},
  booktitle={Conference on Robot Learning},
  pages={1101--1112},
  year={2020},
  organization={PMLR}
}

@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011}
}


@article{zhang2022mathematical,
    title={Mathematical Analysis of Machine Learning Algorithms},
  author={Zhang, Tong},
  journal={To Appear},
  url={http://tongzhang-ml.org/lt-book.html},
    year={2022},
}

@article{azuma1967weighted,
  title={Weighted sums of certain dependent random variables},
  author={Azuma, Kazuoki},
  journal={Tohoku Mathematical Journal, Second Series},
  volume={19},
  number={3},
  pages={357--367},
  year={1967},
  publisher={Mathematical Institute, Tohoku University}
}
@article{zhan2022decentralized,
  title={Decentralized Optimistic Hyperpolicy Mirror Descent: Provably No-Regret Learning in Markov Games},
  author={Zhan, Wenhao and Lee, Jason D and Yang, Zhuoran},
  journal={arXiv preprint arXiv:2206.01588},
  year={2022}
}
@article{chen2022unified,
  title={Unified Algorithms for RL with Decision-Estimation Coefficients: No-Regret, PAC, and Reward-Free Learning},
  author={Chen, Fan and Mei, Song and Bai, Yu},
  journal={arXiv preprint arXiv:2209.11745},
  year={2022}
}
@article{chen2022abc,
  title={A General Framework for Sample-Efficient Function Approximation in Reinforcement Learning},
  author={Chen, Zixiang and Li, Chris Junchi and Yuan, Angela and Gu, Quanquan and Jordan, Michael I},
  journal={arXiv preprint arXiv:2209.15634},
  year={2022}
}

@article{zhong2022gec,
  title={A posterior sampling framework for interactive decision making},
  author={Zhong, Han and Xiong, Wei and Zheng, Sirui and Wang, Liwei and Wang, Zhaoran and Yang, Zhuoran and Zhang, Tong},
  journal={arXiv preprint arXiv:2211.01962},
  year={2022}
}

@inproceedings{agarwal2014taming,
  title={Taming the monster: A fast and simple algorithm for contextual bandits},
  author={Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Li, Lihong and Schapire, Robert},
  booktitle={International Conference on Machine Learning},
  pages={1638--1646},
  year={2014},
  organization={PMLR}
}

@article{chen2022partially,
  title={Partially Observable RL with B-Stability: Unified Structural Condition and Sharp Sample-Efficient Algorithms},
  author={Chen, Fan and Bai, Yu and Mei, Song},
  journal={arXiv preprint arXiv:2209.14990},
  year={2022}
}

@article{gao2021lowcostlinear,
  title={A provably efficient algorithm for linear markov decision process with low switching cost},
  author={Gao, Minbo and Xie, Tianle and Du, Simon S and Yang, Lin F},
  journal={arXiv preprint arXiv:2101.00494},
  year={2021}
}

@inproceedings{jin2022power,
  title={The power of exploiter: Provable multi-agent rl in large state spaces},
  author={Jin, Chi and Liu, Qinghua and Yu, Tiancheng},
  booktitle={International Conference on Machine Learning},
  pages={10251--10279},
  year={2022},
  organization={PMLR}
}

@article{gu2021batched,
  title={Batched neural bandits},
  author={Gu, Quanquan and Karbasi, Amin and Khosravi, Khashayar and Mirrokni, Vahab and Zhou, Dongruo},
  journal={arXiv preprint arXiv:2102.13028},
  year={2021}
}

@article{zhong2023SNE,
  title={Can Reinforcement Learning Find Stackelberg-Nash Equilibria in General-Sum Markov Games with Myopically Rational Followers?},
  author={Zhong, Han and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={35},
  pages={1--52},
  year={2023}
}
@inproceedings{zhao2022provably,
  title={Provably efficient policy optimization for two-player zero-sum markov games},
  author={Zhao, Yulai and Tian, Yuandong and Lee, Jason and Du, Simon},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2736--2761},
  year={2022},
  organization={PMLR}
}
@inproceedings{ding2022independent,
  title={Independent policy gradient for large-scale markov potential games: Sharper rates, function approximation, and game-agnostic convergence},
  author={Ding, Dongsheng and Wei, Chen-Yu and Zhang, Kaiqing and Jovanovic, Mihailo},
  booktitle={International Conference on Machine Learning},
  pages={5166--5220},
  year={2022},
  organization={PMLR}
}
@inproceedings{qiu2021reward,
  title={On reward-free rl with kernel and neural function approximations: Single-agent mdp and markov game},
  author={Qiu, Shuang and Ye, Jieping and Wang, Zhaoran and Yang, Zhuoran},
  booktitle={International Conference on Machine Learning},
  pages={8737--8747},
  year={2021},
  organization={PMLR}
}
@article{huang2021markovgamegeneral,
  title={Towards general function approximation in zero-sum markov games},
  author={Huang, Baihe and Lee, Jason D and Wang, Zhaoran and Yang, Zhuoran},
  journal={arXiv preprint arXiv:2107.14702},
  year={2021}
}

@article{bradtke1992reinforcement,
  title={Reinforcement learning applied to linear quadratic regulation},
  author={Bradtke, Steven},
  journal={Advances in neural information processing systems},
  volume={5},
  year={1992}
}

@inproceedings{dong2020banditlowcost,
  title={Multinomial logit bandit with low switching cost},
  author={Dong, Kefan and Li, Yingkai and Zhang, Qin and Zhou, Yuan},
  booktitle={International Conference on Machine Learning},
  pages={2607--2615},
  year={2020},
  organization={PMLR}
}

@article{kong2021eluderlowcost,
  title={Online sub-sampling for reinforcement learning with general function approximation},
  author={Kong, Dingwen and Salakhutdinov, Ruslan and Wang, Ruosong and Yang, Lin F},
  journal={arXiv preprint arXiv:2106.07203},
  year={2021}
}


@article{liu2022optimistic,
  title={Optimistic MLE--A Generic Model-based Algorithm for Partially Observable Sequential Decision Making},
  author={Liu, Qinghua and Netrapalli, Praneeth and Szepesvari, Csaba and Jin, Chi},
  journal={arXiv preprint arXiv:2209.14997},
  year={2022}
}

@article{chen2022general,
  title={A General Framework for Sample-Efficient Function Approximation in Reinforcement Learning},
  author={Chen, Zixiang and Li, Chris Junchi and Yuan, Angela and Gu, Quanquan and Jordan, Michael I},
  journal={arXiv preprint arXiv:2209.15634},
  year={2022}
}

@article{dani2008stochastic,
  title={Stochastic linear optimization under bandit feedback},
  author={Dani, Varsha and Hayes, Thomas P and Kakade, Sham M},
  year={2008}
}

@article{rusmevichientong2010linearly,
  title={Linearly parameterized bandits},
  author={Rusmevichientong, Paat and Tsitsiklis, John N},
  journal={Mathematics of Operations Research},
  volume={35},
  number={2},
  pages={395--411},
  year={2010},
  publisher={INFORMS}
}

@inproceedings{kakade2002approximate,
author = {Kakade, Sham and Langford, John},
title = {Approximately Optimal Approximate Reinforcement Learning},
year = {2002},
booktitle = {Proceedings of the Nineteenth International Conference on Machine Learning},
pages = {267–274},
numpages = {8},
series = {ICML '02}
}

@inproceedings{wu2022nearly,
  title={Nearly optimal policy optimization with stable at any time guarantee},
  author={Wu, Tianhao and Yang, Yunchang and Zhong, Han and Wang, Liwei and Du, Simon and Jiao, Jiantao},
  booktitle={International Conference on Machine Learning},
  pages={24243--24265},
  year={2022},
  organization={PMLR}
}

@inproceedings{zanette2019tighter,
  title={Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds},
  author={Zanette, Andrea and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={7304--7312},
  year={2019},
  organization={PMLR}
}

@article{li2021breaking,
  title={Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning},
  author={Li, Gen and Shi, Laixi and Chen, Yuxin and Gu, Yuantao and Chi, Yuejie},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17762--17776},
  year={2021}
}

@inproceedings{zhang2021isreinforcement,
  title={Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon},
  author={Zhang, Zihan and Ji, Xiangyang and Du, Simon},
  booktitle={Conference on Learning Theory},
  pages={4528--4531},
  year={2021},
  organization={PMLR}
}

@inproceedings{zhang2022horizon,
  title={Horizon-free reinforcement learning in polynomial time: the power of stationary policies},
  author={Zhang, Zihan and Ji, Xiangyang and Du, Simon},
  booktitle={Conference on Learning Theory},
  pages={3858--3904},
  year={2022},
  organization={PMLR}
}

@inproceedings{menard2021ucb,
  title={UCB Momentum Q-learning: Correcting the bias without forgetting},
  author={M{\'e}nard, Pierre and Domingues, Omar Darwiche and Shang, Xuedong and Valko, Michal},
  booktitle={International Conference on Machine Learning},
  pages={7609--7618},
  year={2021},
  organization={PMLR}
}