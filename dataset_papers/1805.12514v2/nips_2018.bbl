\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and
  Wagner]{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock \emph{arXiv preprint arXiv:1802.00420}, 2018.

\bibitem[Carlini and Wagner(2017)]{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{Security and Privacy (SP), 2017 IEEE Symposium on}, pages
  39--57. IEEE, 2017.

\bibitem[Cheng et~al.(2017)Cheng, N{\"u}hrenberg, and Ruess]{cheng2017maximum}
Chih-Hong Cheng, Georg N{\"u}hrenberg, and Harald Ruess.
\newblock Maximum resilience of artificial neural networks.
\newblock In \emph{International Symposium on Automated Technology for
  Verification and Analysis}, pages 251--268. Springer, 2017.

\bibitem[Dvijotham et~al.(2018)Dvijotham, Stanforth, Gowal, Mann, and
  Kohli]{dvijotham2018dual}
Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and
  Pushmeet Kohli.
\newblock A dual approach to scalable verification of deep networks.
\newblock \emph{arXiv preprint arXiv:1803.06567}, 2018.

\bibitem[Ehlers(2017)]{ehlers2017formal}
Ruediger Ehlers.
\newblock Formal verification of piece-wise linear feed-forward neural
  networks.
\newblock In \emph{International Symposium on Automated Technology for
  Verification and Analysis}, 2017.

\bibitem[Fenchel(1949)]{fenchel1949conjugate}
Werner Fenchel.
\newblock On conjugate convex functions.
\newblock \emph{Canad. J. Math}, 1\penalty0 (73-77), 1949.

\bibitem[Gehr et~al.(2018)Gehr, Mirman, Drachsler-Cohen, Tsankov, Chaudhuri,
  and Vechev]{gehr2018ai}
Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat
  Chaudhuri, and Martin Vechev.
\newblock $\mbox{AI}^2$: Safety and robustness certification of neural networks
  with abstract interpretation.
\newblock In \emph{IEEE Conference on Security and Privacy}, 2018.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2015explaining}
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{International Conference on Learning Representations}, 2015.
\newblock URL \url{http://arxiv.org/abs/1412.6572}.

\bibitem[Hein and Andriushchenko(2017)]{hein2017formal}
Matthias Hein and Maksym Andriushchenko.
\newblock Formal guarantees on the robustness of a classifier against
  adversarial manipulation.
\newblock In \emph{Advances in Neural Information Processing Systems}. 2017.

\bibitem[Huang et~al.(2017)Huang, Kwiatkowska, Wang, and Wu]{huang2017safety}
Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu.
\newblock Safety verification of deep neural networks.
\newblock In \emph{International Conference on Computer Aided Verification},
  pages 3--29. Springer, 2017.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kurakin et~al.(2017)Kurakin, Goodfellow, and
  Bengio]{kurakin2016adversarial}
Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
\newblock Adversarial machine learning at scale.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Kurakin et~al.(2018)Kurakin, Goodfellow, Bengio, Dong, Liao, Liang,
  Pang, Zhu, Hu, Xie, et~al.]{kurakin2018adversarial}
Alexey Kurakin, Ian Goodfellow, Samy Bengio, Yinpeng Dong, Fangzhou Liao, Ming
  Liang, Tianyu Pang, Jun Zhu, Xiaolin Hu, Cihang Xie, et~al.
\newblock Adversarial attacks and defences competition.
\newblock \emph{arXiv preprint arXiv:1804.00097}, 2018.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2007)Li, Hastie, and Church]{li2007nonlinear}
Ping Li, Trevor~J Hastie, and Kenneth~W Church.
\newblock Nonlinear estimators and tail bounds for dimension reduction in l1
  using cauchy random projections.
\newblock \emph{Journal of Machine Learning Research}, 8\penalty0
  (Oct):\penalty0 2497--2532, 2007.

\bibitem[Lomuscio and Maganti(2017)]{lomuscio2017approach}
Alessio Lomuscio and Lalit Maganti.
\newblock An approach to reachability analysis for feed-forward relu neural
  networks.
\newblock \emph{arXiv preprint arXiv:1706.07351}, 2017.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Metzen et~al.(2017)Metzen, Genewein, Fischer, and
  Bischoff]{metzen2017detecting}
Jan~Hendrik Metzen, Tim Genewein, Volker Fischer, and Bastian Bischoff.
\newblock On detecting adversarial perturbations.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Wu, Jha, and
  Swami]{papernot2016distillation}
Nicolas Papernot, Patrick McDaniel, Xi~Wu, Somesh Jha, and Ananthram Swami.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In \emph{Security and Privacy (SP), 2016 IEEE Symposium on}, pages
  582--597. IEEE, 2016.

\bibitem[Raghunathan et~al.(2018)Raghunathan, Steinhardt, and
  Liang]{raghunathan2018certified}
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.
\newblock Certified defenses against adversarial examples.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Sinha et~al.(2018)Sinha, Namkoong, and Duchi]{sinha2018certifiable}
Aman Sinha, Hongseok Namkoong, and John Duchi.
\newblock Certifiable distributional robustness with principled adversarial
  training.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Tjeng and Tedrake(2017)]{tjeng2017verifying}
Vincent Tjeng and Russ Tedrake.
\newblock Verifying neural networks with mixed integer programming.
\newblock \emph{CoRR}, abs/1711.07356, 2017.
\newblock URL \url{http://arxiv.org/abs/1711.07356}.

\bibitem[Vempala(2005)]{vempala2005random}
Santosh~S Vempala.
\newblock \emph{The random projection method}, volume~65.
\newblock American Mathematical Soc., 2005.

\bibitem[Wong and Kolter(2017)]{wong2017provable}
Eric Wong and J~Zico Kolter.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock \emph{arXiv preprint arXiv:1711.00851}, 2017.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\end{thebibliography}
