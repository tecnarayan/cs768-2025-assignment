\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar,
  and Zhang]{abadi2016deep}
Abadi, M., Chu, A., Goodfellow, I., McMahan, H.~B., Mironov, I., Talwar, K.,
  and Zhang, L.
\newblock Deep learning with differential privacy.
\newblock In \emph{Proceedings of the 2016 ACM SIGSAC conference on computer
  and communications security}, pp.\  308--318, 2016.

\bibitem[Arjevani et~al.(2022)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{arjevani2022lower}
Arjevani, Y., Carmon, Y., Duchi, J.~C., Foster, D.~J., Srebro, N., and
  Woodworth, B.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{Mathematical Programming}, pp.\  1--50, 2022.

\bibitem[Bennett(1962)]{bennett1962probability}
Bennett, G.
\newblock Probability inequalities for the sum of independent random variables.
\newblock \emph{Journal of the American Statistical Association}, 57\penalty0
  (297):\penalty0 33--45, 1962.

\bibitem[Cutkosky \& Mehta(2021)Cutkosky and Mehta]{cutkosky2021high}
Cutkosky, A. and Mehta, H.
\newblock High-probability bounds for non-convex stochastic optimization with
  heavy tails.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Davis et~al.(2021)Davis, Drusvyatskiy, Xiao, and Zhang]{davis2021low}
Davis, D., Drusvyatskiy, D., Xiao, L., and Zhang, J.
\newblock From low probability to high confidence in stochastic convex
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (49):\penalty0 1--38, 2021.

\bibitem[Dvurechenskii et~al.(2018)Dvurechenskii, Dvinskikh, Gasnikov, Uribe,
  and Nedich]{dvurechenskii2018decentralize}
Dvurechenskii, P., Dvinskikh, D., Gasnikov, A., Uribe, C., and Nedich, A.
\newblock Decentralize and randomize: Faster algorithm for wasserstein
  barycenters.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Dzhaparidze \& Van~Zanten(2001)Dzhaparidze and
  Van~Zanten]{dzhaparidze2001bernstein}
Dzhaparidze, K. and Van~Zanten, J.
\newblock On bernstein-type inequalities for martingales.
\newblock \emph{Stochastic processes and their applications}, 93\penalty0
  (1):\penalty0 109--117, 2001.

\bibitem[Freedman et~al.(1975)]{freedman1975tail}
Freedman, D.~A. et~al.
\newblock On tail probabilities for martingales.
\newblock \emph{the Annals of Probability}, 3\penalty0 (1):\penalty0 100--118,
  1975.

\bibitem[Gasnikov \& Nesterov(2016)Gasnikov and
  Nesterov]{gasnikov2016universal}
Gasnikov, A. and Nesterov, Y.
\newblock Universal fast gradient method for stochastic composit optimization
  problems.
\newblock \emph{arXiv preprint arXiv:1604.05275}, 2016.

\bibitem[Ghadimi \& Lan(2012)Ghadimi and Lan]{ghadimi2012optimal}
Ghadimi, S. and Lan, G.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization i: A generic algorithmic framework.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (4):\penalty0
  1469--1492, 2012.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Gidel et~al.(2019)Gidel, Berard, Vignoud, Vincent, and
  Lacoste-Julien]{gidel2019variational}
Gidel, G., Berard, H., Vignoud, G., Vincent, P., and Lacoste-Julien, S.
\newblock A variational inequality perspective on generative adversarial
  networks.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N., and
  Weinberger, K.~Q. (eds.), \emph{Advances in Neural Information Processing
  Systems}, volume~27. Curran Associates, Inc., 2014.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow2016deep}
Goodfellow, I., Bengio, Y., and Courville, A.
\newblock \emph{Deep learning}.
\newblock MIT press, 2016.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Danilova, and
  Gasnikov]{gorbunov2020stochastic}
Gorbunov, E., Danilova, M., and Gasnikov, A.
\newblock Stochastic optimization with heavy-tailed noise via accelerated
  gradient clipping.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15042--15053, 2020.

\bibitem[Gorbunov et~al.(2021)Gorbunov, Danilova, Shibaev, Dvurechensky, and
  Gasnikov]{gorbunov2021near}
Gorbunov, E., Danilova, M., Shibaev, I., Dvurechensky, P., and Gasnikov, A.
\newblock Near-optimal high probability complexity bounds for non-smooth
  stochastic optimization with heavy-tailed noise.
\newblock \emph{arXiv preprint arXiv:2106.05958}, 2021.

\bibitem[Gorbunov et~al.(2022{\natexlab{a}})Gorbunov, Danilova, Dobre,
  Dvurechensky, Gasnikov, and Gidel]{gorbunov2022clipped}
Gorbunov, E., Danilova, M., Dobre, D., Dvurechensky, P., Gasnikov, A., and
  Gidel, G.
\newblock Clipped stochastic methods for variational inequalities with
  heavy-tailed noise.
\newblock \emph{arXiv preprint arXiv:2206.01095}, 2022{\natexlab{a}}.

\bibitem[Gorbunov et~al.(2022{\natexlab{b}})Gorbunov, Loizou, and
  Gidel]{gorbunov2022extragradient}
Gorbunov, E., Loizou, N., and Gidel, G.
\newblock Extragradient method: ${\cO}(\nicefrac{1}{K})$ last-iterate
  convergence for monotone variational inequalities and connections with
  cocoercivity.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  366--402. PMLR, 2022{\natexlab{b}}.

\bibitem[Harker \& Pang(1990)Harker and Pang]{harker1990finite}
Harker, P.~T. and Pang, J.-S.
\newblock Finite-dimensional variational inequality and nonlinear
  complementarity problems: a survey of theory, algorithms and applications.
\newblock \emph{Mathematical programming}, 48\penalty0 (1):\penalty0 161--220,
  1990.

\bibitem[Jakovetic et~al.(2022)Jakovetic, Bajovic, Sahu, Kar, Milosevic, and
  Stamenkovic]{jakovetic2022nonlinear}
Jakovetic, D., Bajovic, D., Sahu, A.~K., Kar, S., Milosevic, N., and
  Stamenkovic, D.
\newblock Nonlinear gradient mappings and stochastic optimization: A general
  framework with applications to heavy-tail noise.
\newblock \emph{arXiv preprint arXiv:2204.02593}, 2022.

\bibitem[Juditsky et~al.(2011)Juditsky, Nemirovski, and
  Tauvel]{juditsky2011solving}
Juditsky, A., Nemirovski, A., and Tauvel, C.
\newblock Solving variational inequalities with stochastic mirror-prox
  algorithm.
\newblock \emph{Stochastic Systems}, 1\penalty0 (1):\penalty0 17--58, 2011.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Karimi, H., Nutini, J., and Schmidt, M.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the {P}olyak-{L}ojasiewicz condition.
\newblock In \emph{Joint European conference on machine learning and knowledge
  discovery in databases}, pp.\  795--811. Springer, 2016.

\bibitem[Karimireddy et~al.(2021)Karimireddy, He, and
  Jaggi]{karimireddy2021learning}
Karimireddy, S.~P., He, L., and Jaggi, M.
\newblock Learning from history for byzantine robust optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5311--5319. PMLR, 2021.

\bibitem[Khaled \& Richt{\'a}rik(2020)Khaled and
  Richt{\'a}rik]{khaled2020better}
Khaled, A. and Richt{\'a}rik, P.
\newblock Better theory for sgd in the nonconvex world.
\newblock \emph{arXiv preprint arXiv:2002.03329}, 2020.

\bibitem[Korpelevich(1976)]{korpelevich1976extragradient}
Korpelevich, G.~M.
\newblock The extragradient method for finding saddle points and other
  problems.
\newblock \emph{Matecon}, 12:\penalty0 747--756, 1976.

\bibitem[Li \& Orabona(2020)Li and Orabona]{li2020high}
Li, X. and Orabona, F.
\newblock A high probability analysis of adaptive sgd with momentum.
\newblock \emph{arXiv preprint arXiv:2007.14294}, 2020.

\bibitem[Liu et~al.(2022)Liu, Zhu, and Belkin]{liu2022loss}
Liu, C., Zhu, L., and Belkin, M.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock \emph{Applied and Computational Harmonic Analysis}, 59:\penalty0
  85--116, 2022.

\bibitem[Loizou et~al.(2021)Loizou, Berard, Gidel, Mitliagkas, and
  Lacoste-Julien]{loizou2021stochastic}
Loizou, N., Berard, H., Gidel, G., Mitliagkas, I., and Lacoste-Julien, S.
\newblock Stochastic gradient descent-ascent and consensus optimization for
  smooth games: Convergence analysis under expected co-coercivity.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Lojasiewicz(1963)]{lojasiewicz1963topological}
Lojasiewicz, S.
\newblock A topological property of real analytic subsets.
\newblock \emph{Coll. du CNRS, Les {\'e}quations aux d{\'e}riv{\'e}es
  partielles}, 117\penalty0 (87-89):\penalty0 2, 1963.

\bibitem[Mertikopoulos \& Zhou(2019)Mertikopoulos and
  Zhou]{mertikopoulos2019learning}
Mertikopoulos, P. and Zhou, Z.
\newblock Learning in games with continuous action sets and unknown payoff
  functions.
\newblock \emph{Mathematical Programming}, 173\penalty0 (1):\penalty0 465--507,
  2019.

\bibitem[Nazin et~al.(2019)Nazin, Nemirovsky, Tsybakov, and
  Juditsky]{nazin2019algorithms}
Nazin, A.~V., Nemirovsky, A., Tsybakov, A.~B., and Juditsky, A.
\newblock Algorithms of robust stochastic optimization based on mirror descent
  method.
\newblock \emph{Automation and Remote Control}, 80\penalty0 (9):\penalty0
  1607--1627, 2019.

\bibitem[Necoara et~al.(2019)Necoara, Nesterov, and Glineur]{necoara2019linear}
Necoara, I., Nesterov, Y., and Glineur, F.
\newblock Linear convergence of first order methods for non-strongly convex
  optimization.
\newblock \emph{Mathematical Programming}, 175\penalty0 (1):\penalty0 69--107,
  2019.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009robust}
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Nemirovskij \& Yudin(1983)Nemirovskij and
  Yudin]{nemirovskij1983problem}
Nemirovskij, A.~S. and Yudin, D.~B.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[Nesterov(2007)]{nesterov2007dual}
Nesterov, Y.
\newblock Dual extrapolation and its applications to solving variational
  inequalities and related problems.
\newblock \emph{Mathematical Programming}, 109\penalty0 (2):\penalty0 319--344,
  2007.

\bibitem[Nesterov et~al.(2018)]{nesterov2018lectures}
Nesterov, Y. et~al.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Ouyang \& Xu(2021)Ouyang and Xu]{ouyang2021lower}
Ouyang, Y. and Xu, Y.
\newblock Lower complexity bounds of first-order methods for convex-concave
  bilinear saddle-point problems.
\newblock \emph{Mathematical Programming}, 185\penalty0 (1):\penalty0 1--35,
  2021.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Pascanu, R., Mikolov, T., and Bengio, Y.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1310--1318, 2013.

\bibitem[Patel \& Berahas(2022)Patel and Berahas]{patel2022gradient}
Patel, V. and Berahas, A.~S.
\newblock Gradient descent in the absence of global lipschitz continuity of the
  gradients: Convergence, divergence and limitations of its continuous
  approximation.
\newblock \emph{arXiv preprint arXiv:2210.02418}, 2022.

\bibitem[Patel et~al.(2022)Patel, Zhang, and Tian]{patel2022global}
Patel, V., Zhang, S., and Tian, B.
\newblock Global convergence and stability of stochastic gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 36014--36025, 2022.

\bibitem[Polyak(1963)]{polyak1963gradient}
Polyak, B.~T.
\newblock Gradient methods for the minimisation of functionals.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  3\penalty0 (4):\penalty0 864--878, 1963.

\bibitem[Polyak \& Tsypkin(1980)Polyak and Tsypkin]{polyak1980optimal}
Polyak, B.~T. and Tsypkin, Y.~Z.
\newblock Optimal pseudogradient adaptation algorithms.
\newblock \emph{Avtomatika i Telemekhanika}, \penalty0 (8):\penalty0 74--84,
  1980.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951stochastic}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pp.\  400--407, 1951.

\bibitem[Ryu \& Yin(2021)Ryu and Yin]{ryu2021large}
Ryu, E.~K. and Yin, W.
\newblock Large-scale convex optimization via monotone operators, 2021.

\bibitem[Shalev-Shwartz \& Ben-David(2014)Shalev-Shwartz and
  Ben-David]{shalev2014understanding}
Shalev-Shwartz, S. and Ben-David, S.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Song et~al.(2020)Song, Zhou, Zhou, Jiang, and Ma]{song2020optimistic}
Song, C., Zhou, Z., Zhou, Y., Jiang, Y., and Ma, Y.
\newblock Optimistic dual extrapolation for coherent non-monotone variational
  inequalities.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 14303--14314, 2020.

\bibitem[Vural et~al.(2022)Vural, Yu, Balasubramanian, Volgushev, and
  Erdogdu]{vural2022mirror}
Vural, N.~M., Yu, L., Balasubramanian, K., Volgushev, S., and Erdogdu, M.~A.
\newblock Mirror descent strikes again: Optimal stochastic convex optimization
  under infinite noise variance.
\newblock In \emph{Conference on Learning Theory}, pp.\  65--102. PMLR, 2022.

\bibitem[Yue et~al.(2022)Yue, Fang, and Lin]{yue2022lower}
Yue, P., Fang, C., and Lin, Z.
\newblock On the lower bound of minimizing {P}olyak-{L}ojasiewicz functions.
\newblock \emph{arXiv preprint arXiv:2212.13551}, 2022.

\bibitem[Zhang \& Cutkosky(2022)Zhang and Cutkosky]{zhang2022parameter}
Zhang, J. and Cutkosky, A.
\newblock Parameter-free regret in high probability with heavy tails.
\newblock \emph{arXiv preprint arXiv:2210.14355}, 2022.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, He, Sra, and
  Jadbabaie]{zhang2020gradient}
Zhang, J., He, T., Sra, S., and Jadbabaie, A.
\newblock Why gradient clipping accelerates training: A theoretical
  justification for adaptivity.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=BJgnXpVYwS}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Karimireddy, Veit, Kim, Reddi,
  Kumar, and Sra]{zhang2020adaptive}
Zhang, J., Karimireddy, S.~P., Veit, A., Kim, S., Reddi, S.~J., Kumar, S., and
  Sra, S.
\newblock Why are adaptive methods good for attention models?
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2022)Zhang, Hong, and Zhang]{zhang2022lower}
Zhang, J., Hong, M., and Zhang, S.
\newblock On lower iteration complexity bounds for the convex concave saddle
  point problems.
\newblock \emph{Mathematical Programming}, 194\penalty0 (1-2):\penalty0
  901--935, 2022.

\end{thebibliography}
