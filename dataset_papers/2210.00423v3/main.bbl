\begin{thebibliography}{10}

\bibitem{2011improved}
Y.~Abbasi-Yadkori, D.~P{\'a}l, and C.~Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2312--2320, 2011.

\bibitem{allen2019convergence}
Z.~Allen-Zhu, Y.~Li, and Z.~Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em International Conference on Machine Learning}, pages
  242--252. PMLR, 2019.

\bibitem{arora2019exact}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, R.~R. Salakhutdinov, and R.~Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8141--8150, 2019.

\bibitem{ash2021gone}
J.~Ash, S.~Goel, A.~Krishnamurthy, and S.~Kakade.
\newblock Gone fishing: Neural active learning with fisher embeddings.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{ash2019deep}
J.~T. Ash, C.~Zhang, A.~Krishnamurthy, J.~Langford, and A.~Agarwal.
\newblock Deep batch active learning by diverse, uncertain gradient lower
  bounds.
\newblock {\em arXiv preprint arXiv:1906.03671}, 2019.

\bibitem{awasthi2014power}
P.~Awasthi, M.~F. Balcan, and P.~M. Long.
\newblock The power of localization for efficiently learning linear separators
  with noise.
\newblock In {\em Proceedings of the forty-sixth annual ACM symposium on Theory
  of computing}, pages 449--458, 2014.

\bibitem{balcan2009agnostic}
M.-F. Balcan, A.~Beygelzimer, and J.~Langford.
\newblock Agnostic active learning.
\newblock {\em Journal of Computer and System Sciences}, 75(1):78--89, 2009.

\bibitem{balcan2007margin}
M.-F. Balcan, A.~Broder, and T.~Zhang.
\newblock Margin based active learning.
\newblock In {\em International Conference on Computational Learning Theory},
  pages 35--50. Springer, 2007.

\bibitem{ban2020generic}
Y.~Ban and J.~He.
\newblock Generic outlier detection in multi-armed bandit.
\newblock In {\em Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 913--923, 2020.

\bibitem{ban2021local}
Y.~Ban and J.~He.
\newblock Local clustering in contextual multi-armed bandits.
\newblock In {\em Proceedings of the Web Conference 2021}, pages 2335--2346,
  2021.

\bibitem{ban2021multi}
Y.~Ban, J.~He, and C.~B. Cook.
\newblock Multi-facet contextual bandits: A neural network perspective.
\newblock In {\em The 27th {ACM} {SIGKDD} Conference on Knowledge Discovery and
  Data Mining, Virtual Event, Singapore, August 14-18, 2021}, pages 35--45,
  2021.

\bibitem{ban2022neural}
Y.~Ban, Y.~Qi, T.~Wei, and J.~He.
\newblock Neural collaborative filtering bandits via meta learning.
\newblock {\em ArXiv abs/2201.13395}, 2022.

\bibitem{ban2021ee}
Y.~Ban, Y.~Yan, A.~Banerjee, and J.~He.
\newblock {EE}-net: Exploitation-exploration neural networks in contextual
  bandits.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{beygelzimer2009importance}
A.~Beygelzimer, S.~Dasgupta, and J.~Langford.
\newblock Importance weighted active learning.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, pages 49--56, 2009.

\bibitem{brinker2003incorporating}
K.~Brinker.
\newblock Incorporating diversity in active learning with support vector
  machines.
\newblock In {\em Proceedings of the 20th international conference on machine
  learning (ICML-03)}, pages 59--66, 2003.

\bibitem{cao2019generalization}
Y.~Cao and Q.~Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock {\em Advances in Neural Information Processing Systems},
  32:10836--10846, 2019.

\bibitem{citovsky2021batch}
G.~Citovsky, G.~DeSalvo, C.~Gentile, L.~Karydas, A.~Rajagopalan,
  A.~Rostamizadeh, and S.~Kumar.
\newblock Batch active learning at scale.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{cohen2017emnist}
G.~Cohen, S.~Afshar, J.~Tapson, and A.~Van~Schaik.
\newblock Emnist: Extending mnist to handwritten letters.
\newblock In {\em 2017 international joint conference on neural networks
  (IJCNN)}, pages 2921--2926. IEEE, 2017.

\bibitem{cohn1996active}
D.~A. Cohn, Z.~Ghahramani, and M.~I. Jordan.
\newblock Active learning with statistical models.
\newblock {\em Journal of artificial intelligence research}, 4:129--145, 1996.

\bibitem{culotta2005reducing}
A.~Culotta and A.~McCallum.
\newblock Reducing labeling effort for structured prediction tasks.
\newblock In {\em AAAI}, volume~5, pages 746--751, 2005.

\bibitem{dasgupta2005analysis}
S.~Dasgupta, A.~T. Kalai, and C.~Monteleoni.
\newblock Analysis of perceptron-based active learning.
\newblock In {\em International conference on computational learning theory},
  pages 249--263. Springer, 2005.

\bibitem{desalvo2021online}
G.~DeSalvo, C.~Gentile, and T.~S. Thune.
\newblock Online active learning with surrogate loss functions.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{goodfellow2016deep}
I.~Goodfellow, Y.~Bengio, and A.~Courville.
\newblock {\em Deep learning}.
\newblock MIT press, 2016.

\bibitem{hanneke2007bound}
S.~Hanneke.
\newblock A bound on the label complexity of agnostic active learning.
\newblock In {\em Proceedings of the 24th international conference on Machine
  learning}, pages 353--360, 2007.

\bibitem{hanneke2014theory}
S.~Hanneke et~al.
\newblock Theory of disagreement-based active learning.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  7(2-3):131--309, 2014.

\bibitem{hanneke2019surrogate}
S.~Hanneke and L.~Yang.
\newblock Surrogate losses in passive and active learning.
\newblock {\em Electronic Journal of Statistics}, 13(2):4646--4708, 2019.

\bibitem{ntk2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem{joshi2009multi}
A.~J. Joshi, F.~Porikli, and N.~Papanikolopoulos.
\newblock Multi-class active learning for image classification.
\newblock In {\em 2009 ieee conference on computer vision and pattern
  recognition}, pages 2372--2379. IEEE, 2009.

\bibitem{kapoor2007active}
A.~Kapoor, K.~Grauman, R.~Urtasun, and T.~Darrell.
\newblock Active learning with gaussian processes for object categorization.
\newblock In {\em 2007 IEEE 11th international conference on computer vision},
  pages 1--8. IEEE, 2007.

\bibitem{kim2021lada}
Y.-Y. Kim, K.~Song, J.~Jang, and I.-c. Moon.
\newblock Lada: Look-ahead data acquisition via augmentation for deep active
  learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lewis1994sequential}
D.~D. Lewis and W.~A. Gale.
\newblock A sequential algorithm for training text classifiers.
\newblock In {\em SIGIR’94}, pages 3--12. Springer, 1994.

\bibitem{locatelli2017adaptivity}
A.~Locatelli, A.~Carpentier, and S.~Kpotufe.
\newblock Adaptivity to noise parameters in nonparametric active learning.
\newblock In {\em Proceedings of the 2017 Conference on Learning Theory, PMLR},
  2017.

\bibitem{minsker2012plug}
S.~Minsker.
\newblock Plug-in approach to active learning.
\newblock {\em Journal of Machine Learning Research}, 13(1), 2012.

\bibitem{moon2020confidence}
J.~Moon, J.~Kim, Y.~Shin, and S.~Hwang.
\newblock Confidence-aware learning for deep neural networks.
\newblock In {\em international conference on machine learning}, pages
  7034--7044. PMLR, 2020.

\bibitem{mussmann2018uncertainty}
S.~Mussmann and P.~S. Liang.
\newblock Uncertainty sampling is preconditioned stochastic gradient descent on
  zero-one loss.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{prokhorov2001ijcnn}
D.~Prokhorov.
\newblock Ijcnn 2001 neural network competition.
\newblock {\em Slide presentation in IJCNN}, 1(97):38, 2001.

\bibitem{yunzheneural}
Y.~Qi, Y.~Ban, and J.~He.
\newblock Neural bandit with arm group graph.
\newblock In {\em Proceedings of the 28th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, KDD '22, page 1379–1389, New York, NY, USA,
  2022. Association for Computing Machinery.

\bibitem{ren2021survey}
P.~Ren, Y.~Xiao, X.~Chang, P.-Y. Huang, Z.~Li, B.~B. Gupta, X.~Chen, and
  X.~Wang.
\newblock A survey of deep active learning.
\newblock {\em ACM Computing Surveys (CSUR)}, 54(9):1--40, 2021.

\bibitem{roy2001toward}
N.~Roy and A.~McCallum.
\newblock Toward optimal active learning through monte carlo estimation of
  error reduction.
\newblock {\em ICML, Williamstown}, 2:441--448, 2001.

\bibitem{schroder2020survey}
C.~Schr{\"o}der and A.~Niekler.
\newblock A survey of active learning for text classification using deep neural
  networks.
\newblock {\em arXiv preprint arXiv:2008.07267}, 2020.

\bibitem{sener2017active}
O.~Sener and S.~Savarese.
\newblock Active learning for convolutional neural networks: A core-set
  approach.
\newblock {\em arXiv preprint arXiv:1708.00489}, 2017.

\bibitem{settles2009active}
B.~Settles.
\newblock Active learning literature survey.
\newblock 2009.

\bibitem{tan2021diversity}
W.~Tan, L.~Du, and W.~Buntine.
\newblock Diversity enhanced active learning with strictly proper scoring
  rules.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{valko2013finite}
M.~Valko, N.~Korda, R.~Munos, I.~Flaounas, and N.~Cristianini.
\newblock Finite-time analysis of kernelised contextual bandits.
\newblock {\em arXiv preprint arXiv:1309.6869}, 2013.

\bibitem{wang2021deep}
H.~Wang, W.~Huang, A.~Margenot, H.~Tong, and J.~He.
\newblock Deep active learning by leveraging training dynamics.
\newblock {\em arXiv preprint arXiv:2110.08611}, 2021.

\bibitem{wang2021neural}
Z.~Wang, P.~Awasthi, C.~Dann, A.~Sekhari, and C.~Gentile.
\newblock Neural active learning with performance guarantees.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{xiao2017/online}
H.~Xiao, K.~Rasul, and R.~Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem{yang2018benchmark}
Y.~Yang and M.~Loog.
\newblock A benchmark and comparison of active learning for logistic
  regression.
\newblock {\em Pattern Recognition}, 83:401--415, 2018.

\bibitem{zhang2018efficient}
C.~Zhang.
\newblock Efficient active learning of sparse halfspaces.
\newblock In {\em Conference on Learning Theory}, pages 1856--1880. PMLR, 2018.

\bibitem{zhang2020efficient}
C.~Zhang, J.~Shen, and P.~Awasthi.
\newblock Efficient active learning of sparse halfspaces with arbitrary bounded
  noise.
\newblock {\em Advances in Neural Information Processing Systems},
  33:7184--7197, 2020.

\bibitem{zhang2020neural}
W.~Zhang, D.~Zhou, L.~Li, and Q.~Gu.
\newblock Neural thompson sampling.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{Zhang_Tong_Xia_Zhu_Chi_Ying_2022}
Y.~Zhang, H.~Tong, Y.~Xia, Y.~Zhu, Y.~Chi, and L.~Ying.
\newblock Batch active learning with graph neural networks via multi-agent deep
  reinforcement learning.
\newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence},
  36:9118--9126, 2022.

\bibitem{zhou2020neural}
D.~Zhou, L.~Li, and Q.~Gu.
\newblock Neural contextual bandits with ucb-based exploration.
\newblock In {\em International Conference on Machine Learning}, pages
  11492--11502. PMLR, 2020.

\end{thebibliography}
