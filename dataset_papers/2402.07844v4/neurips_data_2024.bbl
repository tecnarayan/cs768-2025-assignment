\begin{thebibliography}{10}

\bibitem{aminabadi2022deepspeed}
Reza~Yazdani Aminabadi, Samyam Rajbhandari, Ammar~Ahmad Awan, Cheng Li, Du~Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et~al.
\newblock Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale.
\newblock In {\em SC22: International Conference for High Performance Computing, Networking, Storage and Analysis}, pages 1--15. IEEE, 2022.

\bibitem{athiwaratkun2022multi}
Ben Athiwaratkun, Sanjay~Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi~Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, et~al.
\newblock Multi-lingual evaluation of code generation models.
\newblock {\em arXiv preprint arXiv:2210.14868}, 2022.

\bibitem{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock {\em arXiv preprint arXiv:2108.07732}, 2021.

\bibitem{qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An~Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.
\newblock Qwen technical report.
\newblock {\em arXiv preprint arXiv:2309.16609}, 2023.

\bibitem{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et~al.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock {\em arXiv preprint arXiv:2204.05862}, 2022.

\bibitem{bakker2022fine}
Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John Aslanides, Matt Botvinick, et~al.
\newblock Fine-tuning language models to find agreement among humans with diverse preferences.
\newblock {\em Advances in Neural Information Processing Systems}, 35:38176--38189, 2022.

\bibitem{chen2022learning}
Binghong Chen, Daniel Tarlow, Kevin Swersky, Martin Maas, Pablo Heiber, Ashish Naik, Milad Hashemi, and Parthasarathy Ranganathan.
\newblock Learning to improve code efficiency.
\newblock {\em arXiv preprint arXiv:2208.05297}, 2022.

\bibitem{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock {\em arXiv preprint arXiv:2107.03374}, 2021.

\bibitem{chowdhery2022palm}
A~Chowdhery, S~Narang, J~Devlin, M~Bosma, G~Mishra, A~Roberts, P~Barham, HW~Chung, C~Sutton, S~Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways (no. arxiv: 2204.02311). arxiv, 2022.

\bibitem{cc_by_nc_4}
Creative Commons.
\newblock Cc by-nc 4.0 deed.
\newblock \url{https://creativecommons.org/licenses/by-nc/4.0/}, 2024.
\newblock [Accessed 25-05-2024].

\bibitem{de2008z3}
Leonardo De~Moura and Nikolaj Bj{\o}rner.
\newblock Z3: An efficient smt solver.
\newblock In {\em International conference on Tools and Algorithms for the Construction and Analysis of Systems}, pages 337--340. Springer, 2008.

\bibitem{deepseekcoder}
Deepseek-Ai.
\newblock Deepseek-ai/deepseek-coder: Deepseek coder: Let the code write itself, 2023.

\bibitem{dettmers2022llmint8}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Llm.int8(): 8-bit matrix multiplication for transformers at scale.
\newblock {\em arXiv preprint arXiv:2208.07339}, 2022.

\bibitem{accelerate}
Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, Marc Sun, and Benjamin Bossan.
\newblock Accelerate: Training and inference at scale made simple, efficient and adaptable.
\newblock \url{https://github.com/huggingface/accelerate}, 2022.

\bibitem{hendrycks2021measuring}
Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et~al.
\newblock Measuring coding challenge competence with apps.
\newblock {\em arXiv preprint arXiv:2105.09938}, 2021.

\bibitem{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{jafari2021survey}
Omid Jafari, Preeti Maurya, Parth Nagarkar, Khandker~Mushfiqul Islam, and Chidambaram Crushev.
\newblock A survey on locality sensitive hashing algorithms and their applications.
\newblock {\em arXiv preprint arXiv:2102.08942}, 2021.

\bibitem{jain2024livecodebench}
Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica.
\newblock Livecodebench: Holistic and contamination free evaluation of large language models for code.
\newblock {\em arXiv preprint arXiv:2403.07974}, 2024.

\bibitem{joshi2003formalism}
Aravind Joshi and Owen Rambow.
\newblock A formalism for dependency grammar based on tree adjoining grammar.
\newblock In {\em Proceedings of the Conference on Meaning-text Theory}, pages 207--216. MTT Paris, France, 2003.

\bibitem{kir2017overcome}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei~A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the National Academy of Sciences - PNAS}, 114(13):3521--3526, 2017.

\bibitem{kulal2019spoc}
Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy~S Liang.
\newblock Spoc: Search-based pseudocode to code.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{lai2023ds}
Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Wen-tau Yih, Daniel Fried, Sida Wang, and Tao Yu.
\newblock Ds-1000: A natural and reliable benchmark for data science code generation.
\newblock In {\em International Conference on Machine Learning}, pages 18319--18345. PMLR, 2023.

\bibitem{leetcode}
LeetCode.
\newblock {L}eet{C}ode.
\newblock \url{https://leetcode.com/problemset/algorithms/}, 2024.
\newblock [Accessed 25-05-2024].

\bibitem{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
\newblock Starcoder: may the source be with you!
\newblock {\em arXiv preprint arXiv:2305.06161}, 2023.

\bibitem{li2022competition}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal~Lago, et~al.
\newblock Competition-level code generation with alphacode.
\newblock {\em Science}, 378(6624):1092--1097, 2022.

\bibitem{evalplusliu}
Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang.
\newblock Is your code generated by chatgpt really correct?
\newblock {\em arXiv}, 2023.

\bibitem{liu2024your}
Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang.
\newblock Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{lozhkov2024starcoder}
Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et~al.
\newblock Starcoder 2 and the stack v2: The next generation.
\newblock {\em arXiv preprint arXiv:2402.19173}, 2024.

\bibitem{mastropaolo2021studying}
Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David~Nader Palacio, Denys Poshyvanyk, Rocco Oliveto, and Gabriele Bavota.
\newblock Studying the usage of text-to-text transfer transformer to support code-related tasks.
\newblock In {\em 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)}, pages 336--347. IEEE, 2021.

\bibitem{nguyen2013statistical}
Tung~Thanh Nguyen, Anh~Tuan Nguyen, Hoan~Anh Nguyen, and Tien~N Nguyen.
\newblock A statistical semantic language model for source code.
\newblock In {\em Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering}, pages 532--542, 2013.

\bibitem{nijkamp2022codegen}
Erik Nijkamp, Bo~Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.
\newblock Codegen: An open large language model for code with multi-turn program synthesis.
\newblock {\em arXiv preprint arXiv:2203.13474}, 2022.

\bibitem{fairuse}
U.S.~Copyright Office.
\newblock U.s. copyright office fair use index.
\newblock \url{https://www.copyright.gov/fair-use/}, 2024.
\newblock [Accessed 25-05-2024].

\bibitem{openai2023gpt4}
R~OpenAI.
\newblock Gpt-4 technical report.
\newblock {\em arXiv}, pages 2303--08774, 2023.

\bibitem{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In {\em Proceedings of the 40th annual meeting of the Association for Computational Linguistics}, pages 311--318, 2002.

\bibitem{rafailov2023direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher~D Manning, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock {\em arXiv preprint arXiv:2305.18290}, 2023.

\bibitem{ren2020codebleu}
Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma.
\newblock Codebleu: a method for automatic evaluation of code synthesis.
\newblock {\em arXiv preprint arXiv:2009.10297}, 2020.

\bibitem{roziere2023code}
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J{\'e}r{\'e}my Rapin, et~al.
\newblock Code llama: Open foundation models for code.
\newblock {\em arXiv preprint arXiv:2308.12950}, 2023.

\bibitem{siddiq2022securityeval}
Mohammed~Latif Siddiq and Joanna~CS Santos.
\newblock Securityeval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques.
\newblock In {\em Proceedings of the 1st International Workshop on Mining Software Repositories Applications for Privacy and Security}, pages 29--33, 2022.

\bibitem{stiennon2020learning}
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul~F Christiano.
\newblock Learning to summarize with human feedback.
\newblock {\em Advances in Neural Information Processing Systems}, 33:3008--3021, 2020.

\bibitem{sutskever2008recurrent}
Ilya Sutskever, Geoffrey~E Hinton, and Graham~W Taylor.
\newblock The recurrent temporal restricted boltzmann machine.
\newblock {\em Advances in neural information processing systems}, 21, 2008.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2022execution}
Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig.
\newblock Execution-based evaluation for open-domain code generation.
\newblock {\em arXiv preprint arXiv:2212.10481}, 2022.

\bibitem{wong2023natural}
Man-Fai Wong, Shangxin Guo, Ching-Nam Hang, Siu-Wai Ho, and Chee-Wei Tan.
\newblock Natural language generation and understanding of big code for ai-assisted programming: A review.
\newblock {\em Entropy}, 25(6):888, 2023.

\bibitem{xu2022survey}
Yichen Xu and Yanqiao Zhu.
\newblock A survey on pretrained language models for neural code intelligence.
\newblock {\em arXiv preprint arXiv:2212.10079}, 2022.

\bibitem{zan2022large}
Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang, and Jian-Guang Lou.
\newblock Large language models meet nl2code: A survey.
\newblock {\em arXiv preprint arXiv:2212.09420}, 2022.

\bibitem{zan2023large}
Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, and Jian-Guang Lou.
\newblock Large language models meet nl2code: A survey.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 7443--7464, 2023.

\bibitem{ziegler2019fine}
Daniel~M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences.
\newblock {\em arXiv preprint arXiv:1909.08593}, 2019.

\end{thebibliography}
