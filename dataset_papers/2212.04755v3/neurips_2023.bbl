\begin{thebibliography}{10}

\bibitem{aghajanyan-etal-2021-muppet}
Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and Sonal Gupta.
\newblock Muppet: Massive multi-task representations with pre-finetuning.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 5799--5811, November 2021.

\bibitem{Asai2020Learning}
Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong.
\newblock Learning to retrieve reasoning paths over wikipedia graph for question answering.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{Wikiextractor2015}
Giusepppe Attardi.
\newblock Wikiextractor.
\newblock \url{https://github.com/attardi/wikiextractor}, 2015.

\bibitem{bird2009natural}
Steven Bird, Ewan Klein, and Edward Loper.
\newblock {\em Natural language processing with Python: analyzing text with the natural language toolkit}.
\newblock " O'Reilly Media, Inc.", 2009.

\bibitem{calixto-etal-2021-wikipedia}
Iacer Calixto, Alessandro Raganato, and Tommaso Pasini.
\newblock {W}ikipedia entities as rendezvous across languages: Grounding multilingual language models by predicting {W}ikipedia hyperlinks.
\newblock In {\em Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, June 2021.

\bibitem{chada-natarajan-2021-fewshotqa}
Rakesh Chada and Pradeep Natarajan.
\newblock {F}ewshot{QA}: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pages 6081--6090, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

\bibitem{Chang2020Pre-training}
Wei-Cheng Chang, Felix~X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar.
\newblock Pre-training tasks for embedding-based large-scale retrieval.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{chen-etal-2022-lightner}
Xiang Chen, Lei Li, Shumin Deng, Chuanqi Tan, Changliang Xu, Fei Huang, Luo Si, Huajun Chen, and Ningyu Zhang.
\newblock {L}ight{NER}: A lightweight tuning paradigm for low-resource {NER} via pluggable prompting.
\newblock In {\em Proceedings of the 29th International Conference on Computational Linguistics}, pages 2374--2387, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics.

\bibitem{chiu-nichols-2016-named}
Jason~P.C. Chiu and Eric Nichols.
\newblock Named entity recognition with bidirectional {LSTM}-{CNN}s.
\newblock {\em Transactions of the Association for Computational Linguistics}, 4:357--370, 2016.

\bibitem{cui-etal-2021-template}
Leyang Cui, Yu~Wu, Jian Liu, Sen Yang, and Yue Zhang.
\newblock Template-based named entity recognition using {BART}.
\newblock In {\em Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, August 2021.

\bibitem{derczynski-etal-2017-results}
Leon Derczynski, Eric Nichols, Marieke van Erp, and Nut Limsopatham.
\newblock Results of the {WNUT}2017 shared task on novel and emerging entity recognition.
\newblock In {\em Proceedings of the 3rd Workshop on Noisy User-generated Text}, September 2017.

\bibitem{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, June 2019.

\bibitem{dua-etal-2019-drop}
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.
\newblock {DROP}: A reading comprehension benchmark requiring discrete reasoning over paragraphs.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 2368--2378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{Dunn2017SearchQAAN}
Matthew Dunn, Levent Sagun, Mike Higgins, V.~Ugur G{\"u}ney, Volkan Cirik, and Kyunghyun Cho.
\newblock Searchqa: A new q\&a dataset augmented with context from a search engine.
\newblock {\em ArXiv}, abs/1704.05179, 2017.

\bibitem{fisch-etal-2019-mrqa}
Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen.
\newblock {MRQA} 2019 shared task: Evaluating generalization in reading comprehension.
\newblock In {\em Proceedings of the 2nd Workshop on Machine Reading for Question Answering}, November 2019.

\bibitem{gao-etal-2021-making}
Tianyu Gao, Adam Fisch, and Danqi Chen.
\newblock Making pre-trained language models better few-shot learners.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 3816--3830, Online, August 2021. Association for Computational Linguistics.

\bibitem{gao-etal-2021-simcse}
Tianyu Gao, Xingcheng Yao, and Danqi Chen.
\newblock {S}im{CSE}: Simple contrastive learning of sentence embeddings.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, November 2021.

\bibitem{glass-etal-2020-span}
Michael Glass, Alfio Gliozzo, Rishav Chakravarti, Anthony Ferritto, Lin Pan, G~P~Shrivatsa Bhargav, Dinesh Garg, and Avi Sil.
\newblock Span selection pre-training for question answering.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pages 2773--2782, Online, July 2020. Association for Computational Linguistics.

\bibitem{joshi2020spanbert}
Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel~S Weld, Luke Zettlemoyer, and Omer Levy.
\newblock Spanbert: Improving pre-training by representing and predicting spans.
\newblock {\em Transactions of the Association for Computational Linguistics}, 8, 2020.

\bibitem{joshi-etal-2017-triviaqa}
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.
\newblock {T}rivia{QA}: A large scale distantly supervised challenge dataset for reading comprehension.
\newblock In {\em Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, July 2017.

\bibitem{kembhavi2017you}
Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi.
\newblock Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and Pattern recognition}, pages 4999--5007, 2017.

\bibitem{keskar2020unifying}
Nitish~Shirish Keskar, Bryan McCann, Caiming Xiong, and Richard Socher.
\newblock Unifying question answering, text classification, and regression via span extraction, 2020.

\bibitem{khashabi-etal-2020-unifiedqa}
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi.
\newblock {UNIFIEDQA}: Crossing format boundaries with a single {QA} system.
\newblock In {\em Findings of the Association for Computational Linguistics: EMNLP 2020}, November 2020.

\bibitem{kwiatkowski-etal-2019-natural}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew~M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.
\newblock Natural questions: A benchmark for question answering research.
\newblock {\em Transactions of the Association for Computational Linguistics}, 7, 2019.

\bibitem{lai-etal-2017-race}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.
\newblock {RACE}: Large-scale {R}e{A}ding comprehension dataset from examinations.
\newblock In {\em Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing}, September 2017.

\bibitem{Lan2020ALBERTAL}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language representations.
\newblock {\em ArXiv}, abs/1909.11942, 2020.

\bibitem{levy-etal-2017-zero}
Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer.
\newblock Zero-shot relation extraction via reading comprehension.
\newblock In {\em Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017)}, pages 333--342, Vancouver, Canada, August 2017. Association for Computational Linguistics.

\bibitem{lewis-etal-2020-bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.
\newblock {BART}: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, July 2020.

\bibitem{lewis-etal-2019-unsupervised}
Patrick Lewis, Ludovic Denoyer, and Sebastian Riedel.
\newblock Unsupervised question answering by cloze translation.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, pages 4896--4910, Florence, Italy, July 2019. Association for Computational Linguistics.

\bibitem{li2023evaluating}
Bo~Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, and Shikun Zhang.
\newblock Evaluating chatgpt's information extraction capabilities: An assessment of performance, explainability, calibration, and faithfulness, 2023.

\bibitem{li-etal-2020-unified}
Xiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu, and Jiwei Li.
\newblock A unified {MRC} framework for named entity recognition.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, July 2020.

\bibitem{liu-etal-2020-event}
Jian Liu, Yubo Chen, Kang Liu, Wei Bi, and Xiaojiang Liu.
\newblock Event extraction as machine reading comprehension.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, November 2020.

\bibitem{liu2021pre}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.
\newblock Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing.
\newblock {\em arXiv preprint arXiv:2107.13586}, 2021.

\bibitem{liu2021gpt}
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang.
\newblock Gpt understands, too.
\newblock {\em arXiv:2103.10385}, 2021.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{lu-etal-2022-unified}
Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le~Sun, and Hua Wu.
\newblock Unified structure generation for universal information extraction.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, May 2022.

\bibitem{luong-etal-2015-effective}
Thang Luong, Hieu Pham, and Christopher~D. Manning.
\newblock Effective approaches to attention-based neural machine translation.
\newblock In {\em Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing}, September 2015.

\bibitem{ma-etal-2022-template}
Ruotian Ma, Xin Zhou, Tao Gui, Yiding Tan, Linyang Li, Qi~Zhang, and Xuanjing Huang.
\newblock Template-free prompt tuning for few-shot {NER}.
\newblock In {\em Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, Seattle, United States, July 2022. Association for Computational Linguistics.

\bibitem{ma2023large}
Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun.
\newblock Large language model is not a good few-shot information extractor, but a good reranker for hard samples!, 2023.

\bibitem{ace2004-annotation}
Alexis Mitchell, Stephanie Strassel, Shudong Huang, and Ramez Zakhary.
\newblock Ace 2004 multilingual training corpus, 2005.

\bibitem{qin2023chatgpt}
Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang.
\newblock Is chatgpt a general-purpose natural language processing task solver?, 2023.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em Journal of Machine Learning Research}, 21, 2020.

\bibitem{rajpurkar-etal-2016-squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In {\em Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing}, November 2016.

\bibitem{ram-etal-2021-shot}
Ori Ram, Yuval Kirstain, Jonathan Berant, Amir Globerson, and Omer Levy.
\newblock Few-shot question answering by pretraining span selection.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 3066--3079, Online, August 2021. Association for Computational Linguistics.

\bibitem{richardson-etal-2013-mctest}
Matthew Richardson, Christopher~J.C. Burges, and Erin Renshaw.
\newblock {MCT}est: A challenge dataset for the open-domain machine comprehension of text.
\newblock In {\em Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing}, October 2013.

\bibitem{saha-etal-2018-duorc}
Amrita Saha, Rahul Aralikatte, Mitesh~M. Khapra, and Karthik Sankaranarayanan.
\newblock {D}uo{RC}: Towards complex language understanding with paraphrased reading comprehension.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 1683--1693, Melbourne, Australia, July 2018. Association for Computational Linguistics.

\bibitem{schick-schutze-2021-exploiting}
Timo Schick and Hinrich Sch{\"u}tze.
\newblock Exploiting cloze-questions for few-shot text classification and natural language inference.
\newblock In {\em Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume}, April 2021.

\bibitem{schick-schutze-2021-just}
Timo Schick and Hinrich Sch{\"u}tze.
\newblock It{'}s not just size that matters: Small language models are also few-shot learners.
\newblock In {\em Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, June 2021.

\bibitem{seonwoo-etal-2021-weakly}
Yeon Seonwoo, Sang-Woo Lee, Ji-Hoon Kim, Jung-Woo Ha, and Alice Oh.
\newblock Weakly supervised pre-training for multi-hop retriever.
\newblock In {\em Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021}, August 2021.

\bibitem{socher-etal-2013-recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning, Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment treebank.
\newblock In {\em Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing}, October 2013.

\bibitem{sun-etal-2019-dream}
Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie.
\newblock {DREAM}: A challenge data set and models for dialogue-based reading comprehension.
\newblock {\em Transactions of the Association for Computational Linguistics}, 7, 2019.

\bibitem{tjong-kim-sang-de-meulder-2003-introduction}
Erik~F. Tjong Kim~Sang and Fien De~Meulder.
\newblock Introduction to the {C}o{NLL}-2003 shared task: Language-independent named entity recognition.
\newblock In {\em Proceedings of the Seventh Conference on Natural Language Learning at {HLT}-{NAACL} 2003}, 2003.

\bibitem{trischler-etal-2017-newsqa}
Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman.
\newblock {N}ews{QA}: A machine comprehension dataset.
\newblock In {\em Proceedings of the 2nd Workshop on Representation Learning for {NLP}}, August 2017.

\bibitem{tsatsaronis2015overview}
George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael~R Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos, et~al.
\newblock An overview of the bioasq large-scale biomedical semantic indexing and question answering competition.
\newblock {\em BMC bioinformatics}, 16(1):1--28, 2015.

\bibitem{NIPS2017_3f5ee243}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett, editors, {\em Advances in Neural Information Processing Systems}, volume~30, 2017.

\bibitem{ace2005-annotation}
Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda.
\newblock Ace 2005 multilingual training corpus, 2006.

\bibitem{wang-etal-2021-improving}
Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Zhongqiang Huang, Fei Huang, and Kewei Tu.
\newblock Improving named entity recognition by external context retrieving and cooperative learning.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, August 2021.

\bibitem{wei2023zeroshot}
Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and Wenjuan Han.
\newblock Zero-shot information extraction via chatting with chatgpt, 2023.

\bibitem{williams-etal-2018-broad}
Adina Williams, Nikita Nangia, and Samuel Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through inference.
\newblock In {\em Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)}, June 2018.

\bibitem{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le~Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, October 2020.

\bibitem{wu-etal-2020-corefqa}
Wei Wu, Fei Wang, Arianna Yuan, Fei Wu, and Jiwei Li.
\newblock {C}oref{QA}: Coreference resolution as query-based span prediction.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, July 2020.

\bibitem{xu-etal-2023-peerda}
Weiwen Xu, Xin Li, Yang Deng, Wai Lam, and Lidong Bing.
\newblock {P}eer{DA}: Data augmentation via modeling peer relation for span identification tasks.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 8681--8699, Toronto, Canada, July 2023. Association for Computational Linguistics.

\bibitem{yamada-etal-2020-luke}
Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto.
\newblock {LUKE}: Deep contextualized entity representations with entity-aware self-attention.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, November 2020.

\bibitem{yan-etal-2021-unified-generative}
Hang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng Zhang, and Xipeng Qiu.
\newblock A unified generative framework for various {NER} subtasks.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, August 2021.

\bibitem{yang-etal-2018-hotpotqa}
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher~D. Manning.
\newblock {H}otpot{QA}: A dataset for diverse, explainable multi-hop question answering.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, October-November 2018.

\bibitem{yasunaga-etal-2022-linkbert}
Michihiro Yasunaga, Jure Leskovec, and Percy Liang.
\newblock {L}ink{BERT}: Pretraining language models with document links.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, May 2022.

\bibitem{zhong-etal-2022-proqa}
Wanjun Zhong, Yifan Gao, Ning Ding, Yujia Qin, Zhiyuan Liu, Ming Zhou, Jiahai Wang, Jian Yin, and Nan Duan.
\newblock {P}ro{QA}: Structural prompt-based pre-training for unified question answering.
\newblock In {\em Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 4230--4243, Seattle, United States, July 2022. Association for Computational Linguistics.

\end{thebibliography}
