\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Munos,
  Heess, and Riedmiller]{abbas2018mpo}
A.~Abdolmaleki, J.~T. Springenberg, Y.~Tassa, R.~Munos, N.~Heess, and M.~A.
  Riedmiller.
\newblock Maximum a posteriori policy optimisation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Agarwal et~al.(2020)Agarwal, Schuurmans, and
  Norouzi]{agarwal2019optimistic}
R.~Agarwal, D.~Schuurmans, and M.~Norouzi.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Akkaya et~al.(2019)Akkaya, Andrychowicz, Chociej, Litwin, McGrew,
  Petron, Paino, Plappert, Powell, Ribas, et~al.]{akkaya2019solving}
I.~Akkaya, M.~Andrychowicz, M.~Chociej, M.~Litwin, B.~McGrew, A.~Petron,
  A.~Paino, M.~Plappert, G.~Powell, R.~Ribas, et~al.
\newblock Solving rubik's cube with a robot hand.
\newblock \emph{arXiv preprint arXiv:1910.07113}, 2019.

\bibitem[Barth-Maron et~al.(2018)Barth-Maron, Hoffman, Budden, Dabney, Horgan,
  Tb, Muldal, Heess, and Lillicrap]{barth2018distributed}
G.~Barth-Maron, M.~W. Hoffman, D.~Budden, W.~Dabney, D.~Horgan, D.~Tb,
  A.~Muldal, N.~Heess, and T.~Lillicrap.
\newblock Distributed distributional deterministic policy gradients.
\newblock \emph{arXiv preprint arXiv:1804.08617}, 2018.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
M.~G. Bellemare, Y.~Naddaf, J.~Veness, and M.~Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, D{\k{e}}biak,
  Dennison, Farhi, Fischer, Hashme, Hesse, et~al.]{berner2019dota}
C.~Berner, G.~Brockman, B.~Chan, V.~Cheung, P.~D{\k{e}}biak, C.~Dennison,
  D.~Farhi, Q.~Fischer, S.~Hashme, C.~Hesse, et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Cabi et~al.(2019)Cabi, Colmenarejo, Novikov, Konyushkova, Reed, Jeong,
  {\.Z}o\l{}na, Aytar, Budden, Vecerik, Sushkov, Barker, Scholz, andx Nando~de
  Freitas, and Wang]{cabi2019framework}
S.~Cabi, S.~G. Colmenarejo, A.~Novikov, K.~Konyushkova, S.~Reed, R.~Jeong,
  K.~{\.Z}o\l{}na, Y.~Aytar, D.~Budden, M.~Vecerik, O.~Sushkov, D.~Barker,
  J.~Scholz, M.~D. andx Nando~de Freitas, and Z.~Wang.
\newblock Scaling data-driven robotics with reward sketching and batch
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1909.12200}, 2019.

\bibitem[Dabney et~al.(2018)Dabney, Ostrovski, Silver, and
  Munos]{dabney2018implicit}
W.~Dabney, G.~Ostrovski, D.~Silver, and R.~Munos.
\newblock Implicit quantile networks for distributional reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1806.06923}, 2018.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[Dulac-Arnold et~al.(2019)Dulac-Arnold, Mankowitz, and
  Hester]{dulacarnold2019challenges}
G.~Dulac-Arnold, D.~Mankowitz, and T.~Hester.
\newblock Challenges of real-world reinforcement learning, 2019.

\bibitem[Dulac-Arnold et~al.(2020)Dulac-Arnold, Levine, Mankowitz, Li,
  Paduraru, Gowal, and Hester]{dulacarnold2020realworldrlempirical}
G.~Dulac-Arnold, N.~Levine, D.~J. Mankowitz, J.~Li, C.~Paduraru, S.~Gowal, and
  T.~Hester.
\newblock An empirical investigation of the challenges of real-world
  reinforcement learning.
\newblock 2020.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
J.~Fu, A.~Kumar, O.~Nachum, G.~Tucker, and S.~Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning, 2020.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Meger, and Precup]{fujimoto2018off}
S.~Fujimoto, D.~Meger, and D.~Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock \emph{arXiv preprint arXiv:1812.02900}, 2018.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Conti, Ghavamzadeh, and
  Pineau]{fujimoto2019benchmarking}
S.~Fujimoto, E.~Conti, M.~Ghavamzadeh, and J.~Pineau.
\newblock Benchmarking batch deep reinforcement learning algorithms.
\newblock \emph{arXiv preprint arXiv:1910.01708}, 2019.

\bibitem[Gu et~al.(2017)Gu, Holly, Lillicrap, and Levine]{gu2017deep}
S.~Gu, E.~Holly, T.~Lillicrap, and S.~Levine.
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock In \emph{2017 IEEE international conference on robotics and
  automation (ICRA)}, pages 3389--3396. IEEE, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{arXiv preprint arXiv:1801.01290}, 2018.

\bibitem[Heess et~al.(2017)Heess, TB, Sriram, Lemmon, Merel, Wayne, Tassa,
  Erez, Wang, Eslami, Riedmiller, and Silver]{heess2017emergence}
N.~Heess, D.~TB, S.~Sriram, J.~Lemmon, J.~Merel, G.~Wayne, Y.~Tassa, T.~Erez,
  Z.~Wang, S.~M.~A. Eslami, M.~Riedmiller, and D.~Silver.
\newblock Emergence of locomotion behaviours in rich environments, 2017.

\bibitem[Henderson et~al.(2008)Henderson, Lemon, and
  Georgila]{henderson2008hybrid}
J.~Henderson, O.~Lemon, and K.~Georgila.
\newblock Hybrid reinforcement/supervised learning of dialogue policies from
  fixed data sets.
\newblock \emph{Computational Linguistics}, 34\penalty0 (4):\penalty0 487--511,
  2008.

\bibitem[Henderson et~al.(2018)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{henderson2018deep}
P.~Henderson, R.~Islam, P.~Bachman, J.~Pineau, D.~Precup, and D.~Meger.
\newblock Deep reinforcement learning that matters.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{hessel2018rainbow}
M.~Hessel, J.~Modayil, H.~Van~Hasselt, T.~Schaul, G.~Ostrovski, W.~Dabney,
  D.~Horgan, B.~Piot, M.~Azar, and D.~Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Hoffman et~al.(2020)Hoffman, Shahriari, Aslanides, Barth-Maron,
  Behbahani, Norman, Abdolmaleki, Cassirer, Yang, Baumli, Henderson, Novikov,
  Colmenarejo, Cabi, Gulcehre, Paine, Cowie, Wang, Piot, and
  de~Freitas]{hoffman2020acme}
M.~Hoffman, B.~Shahriari, J.~Aslanides, G.~Barth-Maron, F.~Behbahani,
  T.~Norman, A.~Abdolmaleki, A.~Cassirer, F.~Yang, K.~Baumli, S.~Henderson,
  A.~Novikov, S.~G. Colmenarejo, S.~Cabi, C.~Gulcehre, T.~L. Paine, A.~Cowie,
  Z.~Wang, B.~Piot, and N.~de~Freitas.
\newblock Acme: A research framework for distributed reinforcement learning.
\newblock \emph{Preprint arXiv:2006.00979}, 2020.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2019way}
N.~Jaques, A.~Ghandeharioun, J.~H. Shen, C.~Ferguson, A.~Lapedriza, N.~Jones,
  S.~Gu, and R.~Picard.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \emph{arXiv preprint arXiv:1907.00456}, 2019.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, et~al.]{kalashnikov2018qt}
D.~Kalashnikov, A.~Irpan, P.~Pastor, J.~Ibarz, A.~Herzog, E.~Jang, D.~Quillen,
  E.~Holly, M.~Kalakrishnan, V.~Vanhoucke, et~al.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock \emph{arXiv preprint arXiv:1806.10293}, 2018.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock 2015.

\bibitem[Laroche et~al.(2017)Laroche, Trichelair, and Combes]{laroche2017safe}
R.~Laroche, P.~Trichelair, and R.~T.~d. Combes.
\newblock Safe policy improvement with baseline bootstrapping.
\newblock \emph{arXiv preprint arXiv:1712.06924}, 2017.

\bibitem[Machado et~al.(2018)Machado, Bellemare, Talvitie, Veness, Hausknecht,
  and Bowling]{machado2018revisiting}
M.~C. Machado, M.~G. Bellemare, E.~Talvitie, J.~Veness, M.~Hausknecht, and
  M.~Bowling.
\newblock Revisiting the arcade learning environment: Evaluation protocols and
  open problems for general agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 61:\penalty0
  523--562, 2018.

\bibitem[Merel et~al.(2019{\natexlab{a}})Merel, Ahuja, Pham, Tunyasuvunakool,
  Liu, Tirumala, Heess, and Wayne]{merel2018hierarchical}
J.~Merel, A.~Ahuja, V.~Pham, S.~Tunyasuvunakool, S.~Liu, D.~Tirumala, N.~Heess,
  and G.~Wayne.
\newblock Hierarchical visuomotor control of humanoids.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Merel et~al.(2019{\natexlab{b}})Merel, Hasenclever, Galashov, Ahuja,
  Pham, Wayne, Teh, and Heess]{merel2018neural}
J.~Merel, L.~Hasenclever, A.~Galashov, A.~Ahuja, V.~Pham, G.~Wayne, Y.~W. Teh,
  and N.~Heess.
\newblock Neural probabilistic motor primitives for humanoid control.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Merel et~al.(2020)Merel, Aldarondo, Marshall, Tassa, Wayne, and
  Ölveczky]{merel2020deep}
J.~Merel, D.~Aldarondo, J.~Marshall, Y.~Tassa, G.~Wayne, and B.~Ölveczky.
\newblock Deep neuroethology of a virtual rodent.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~Graves, I.~Antonoglou, D.~Wierstra, and
  M.~Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{peng2019advantage}
X.~B. Peng, A.~Kumar, G.~Zhang, and S.~Levine.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.00177}, 2019.

\bibitem[Pietquin et~al.(2011)Pietquin, Geist, Chandramohan, and
  Frezza-Buet]{pietquin2011sample}
O.~Pietquin, M.~Geist, S.~Chandramohan, and H.~Frezza-Buet.
\newblock Sample-efficient batch reinforcement learning for dialogue management
  optimization.
\newblock \emph{ACM Transactions on Speech and Language Processing (TSLP)},
  7\penalty0 (3):\penalty0 1--21, 2011.

\bibitem[Pomerleau(1989)]{pomerleau1989alvinn}
D.~A. Pomerleau.
\newblock Alvinn: An autonomous land vehicle in a neural network.
\newblock In \emph{Advances in neural information processing systems}, pages
  305--313, 1989.

\bibitem[Siegel et~al.(2020)Siegel, Springenberg, Berkenkamp, Abdolmaleki,
  Neunert, Lampe, Hafner, and Riedmiller]{siegel2020keep}
N.~Y. Siegel, J.~T. Springenberg, F.~Berkenkamp, A.~Abdolmaleki, M.~Neunert,
  T.~Lampe, R.~Hafner, and M.~Riedmiller.
\newblock Keep doing what worked: Behavioral modelling priors for offline
  reinforcement learning.
\newblock 2020.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484, 2016.

\bibitem[Song et~al.(2020)Song, Abdolmaleki, Springenberg, Clark, Soyer, Rae,
  Noury, Ahuja, Liu, Tirumala, et~al.]{song2019v}
H.~F. Song, A.~Abdolmaleki, J.~T. Springenberg, A.~Clark, H.~Soyer, J.~W. Rae,
  S.~Noury, A.~Ahuja, S.~Liu, D.~Tirumala, et~al.
\newblock {V-MPO: On-Policy Maximum a Posteriori Policy Optimization for
  Discrete and Continuous Control}.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Tassa et~al.(2018)Tassa, Doron, Muldal, Erez, Li, de~Las~Casas,
  Budden, Abdolmaleki, Merel, Lefrancq, Lillicrap, and
  Riedmiller]{tassa2018controlsuite}
Y.~Tassa, Y.~Doron, A.~Muldal, T.~Erez, Y.~Li, D.~de~Las~Casas, D.~Budden,
  A.~Abdolmaleki, J.~Merel, A.~Lefrancq, T.~P. Lillicrap, and M.~A. Riedmiller.
\newblock {DeepMind Control Suite}.
\newblock \emph{CoRR}, abs/1801.00690, 2018.
\newblock URL \url{http://arxiv.org/abs/1801.00690}.

\bibitem[Tassa et~al.(2020)Tassa, Tunyasuvunakool, Muldal, Doron, Liu, Bohez,
  Merel, Erez, Lillicrap, and Heess]{tassa2020dm_control}
Y.~Tassa, S.~Tunyasuvunakool, A.~Muldal, Y.~Doron, S.~Liu, S.~Bohez, J.~Merel,
  T.~Erez, T.~P. Lillicrap, and N.~Heess.
\newblock {dm\_control}: Software and tasks for continuous control.
\newblock \emph{arXiv preprint arXiv:2006.12983}, 2020.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
E.~Todorov, T.~Erez, and Y.~Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033. IEEE, 2012.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
H.~Van~Hasselt, A.~Guez, and D.~Silver.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{Thirtieth AAAI conference on artificial intelligence}, 2016.

\bibitem[Veit et~al.(2016)Veit, Matera, Neumann, Matas, and
  Belongie]{veit2016coco}
A.~Veit, T.~Matera, L.~Neumann, J.~Matas, and S.~Belongie.
\newblock Coco-text: Dataset and benchmark for text detection and recognition
  in natural images.
\newblock \emph{arXiv preprint arXiv:1601.07140}, 2016.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
O.~Vinyals, I.~Babuschkin, W.~M. Czarnecki, M.~Mathieu, A.~Dudzik, J.~Chung,
  D.~H. Choi, R.~Powell, T.~Ewalds, P.~Georgiev, et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Voloshin et~al.(2019)Voloshin, Le, Jiang, and
  Yue]{voloshin2019empirical}
C.~Voloshin, H.~M. Le, N.~Jiang, and Y.~Yue.
\newblock Empirical study of off-policy policy evaluation for reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1911.06854}, 2019.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Y.~Wu, G.~Tucker, and O.~Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\end{thebibliography}
