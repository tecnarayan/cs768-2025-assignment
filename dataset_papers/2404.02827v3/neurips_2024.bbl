\begin{thebibliography}{10}

\bibitem{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
  Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
  Shyamal Anadkat, et~al.
\newblock {GPT}-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{belilovsky2019greedy}
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon.
\newblock Greedy layerwise learning can scale to imagenet.
\newblock In {\em International Conference on Machine Learning}, pages
  583--593, 2019.

\bibitem{bengio2006greedy}
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle.
\newblock Greedy layer-wise training of deep networks.
\newblock {\em Advances in Neural Information Processing Systems}, 19, 2006.

\bibitem{bertsekas1989parallel}
Dimitri~P. Bertsekas and John~N. Tsitsiklis.
\newblock Parallel and distributed computation.
\newblock {\em Prentice-Hall}, 1989.

\bibitem{gpt-3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1877--1901, 2020.

\bibitem{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  et~al.
\newblock Sparks of artificial general intelligence: Early experiments with
  {GPT}-4.
\newblock {\em arXiv preprint arXiv:2303.12712}, 2023.

\bibitem{cai2023cyclic}
Xufeng Cai, Chaobing Song, Stephen Wright, and Jelena Diakonikolas.
\newblock Cyclic block coordinate descent with variance reduction for composite
  nonconvex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  3469--3494. PMLR, 2023.

\bibitem{chakrabarti2024block}
Darshan Chakrabarti, Jelena Diakonikolas, and Christian Kroer.
\newblock Block-coordinate methods and restarting for solving extensive-form
  games.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{chang2011libsvm}
Chih-Chung Chang and Chih-Jen Lin.
\newblock {LIBSVM}: a library for support vector machines.
\newblock {\em ACM transactions on Intelligent Systems and Technology (TIST)},
  2(3):1--27, 2011.

\bibitem{chen2016direct}
Caihua Chen, Bingsheng He, Yinyu Ye, and Xiaoming Yuan.
\newblock The direct extension of {ADMM} for multi-block convex minimization
  problems is not necessarily convergent.
\newblock {\em Mathematical Programming}, 155(1):57--79, 2016.

\bibitem{chen2016training}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock {\em arXiv preprint arXiv:1604.06174}, 2016.

\bibitem{gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al.
\newblock Training verifiers to solve math word problems.
\newblock {\em arXiv preprint arXiv:2110.14168}, 2021.

\bibitem{defossez2022a}
Alexandre D{\'e}fossez, Leon Bottou, Francis Bach, and Nicolas Usunier.
\newblock A simple convergence proof of adam and adagrad.
\newblock {\em Transactions on Machine Learning Research}, 2022.

\bibitem{qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock {QLoRA}: Efficient finetuning of quantized {LLM}s.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{llama3}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad
  Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,
  et~al.
\newblock The llama 3 herd of models.
\newblock {\em arXiv preprint arXiv:2407.21783}, 2024.

\bibitem{gurbuzbalaban2017cyclic}
Mert Gurbuzbalaban, Asuman Ozdaglar, Pablo~A Parrilo, and Nuri Vanli.
\newblock When cyclic coordinate descent outperforms randomized coordinate
  descent.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem{he2021towards}
Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham
  Neubig.
\newblock Towards a unified view of parameter-efficient transfer learning.
\newblock {\em International Conference on Learning Representations}, 2021.

\bibitem{mmlu}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock {\em arXiv preprint arXiv:2009.03300}, 2020.

\bibitem{math}
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric
  Tang, Dawn Song, and Jacob Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock {\em arXiv preprint arXiv:2103.03874}, 2021.

\bibitem{hinton2006fast}
Geoffrey~E Hinton, Simon Osindero, and Yee-Whye Teh.
\newblock A fast learning algorithm for deep belief nets.
\newblock {\em Neural Computation}, 18(7):1527--1554, 2006.

\bibitem{adapter}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for {NLP}.
\newblock In {\em International Conference on Machine Learning}, pages
  2790--2799. PMLR, 2019.

\bibitem{lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock {LoRA}: Low-rank adaptation of large language models.
\newblock {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{nola}
Soroush~Abbasi Koohpayegani, KL~Navaneet, Parsa Nooralinejad, Soheil Kolouri,
  and Hamed Pirsiavash.
\newblock {NOLA}: Networks as linear combination of low rank random basis.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{vera}
Dawid~Jan Kopiczko, Tijmen Blankevoort, and Yuki~M Asano.
\newblock {V}e{RA}: Vector-based random matrix adaptation.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{prompting}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 3045--3059, 2021.

\bibitem{li2024convergence}
Haochuan Li, Alexander Rakhlin, and Ali Jadbabaie.
\newblock Convergence of {A}dam under relaxed assumptions.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
  Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
\newblock Starcoder: may the source be with you!
\newblock {\em arXiv preprint arXiv:2305.06161}, 2023.

\bibitem{prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for
  Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing}, pages 4582--4597, 2021.

\bibitem{relora}
Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, and Anna Rumshisky.
\newblock {R}e{L}o{RA}: High-rank training through low-rank updates.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{aqua}
Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
\newblock Program induction by rationale generation: Learning to solve and
  explain algebraic word problems.
\newblock In Regina Barzilay and Min-Yen Kan, editors, {\em Proceedings of the
  55th Annual Meeting of the Association for Computational Linguistics (Volume
  1: Long Papers)}, pages 158--167. Association for Computational Linguistics,
  July 2017.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock {RoBERTa}: A robustly optimized {BERT} pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{liu2024hift}
Yongkang Liu, Yiqun Zhang, Qian Li, Shi Feng, Daling Wang, Yifei Zhang, and
  Hinrich Sch{\"u}tze.
\newblock {HiFT}: A hierarchical full parameter fine-tuning strategy.
\newblock {\em arXiv preprint arXiv:2401.15207}, 2024.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{lu2015complexity}
Zhaosong Lu and Lin Xiao.
\newblock On the complexity analysis of randomized block-coordinate descent
  methods.
\newblock {\em Mathematical Programming}, 152:615--642, 2015.

\bibitem{luo1992convergence}
Zhi-Quan Luo and Paul Tseng.
\newblock On the convergence of the coordinate descent method for convex
  differentiable minimization.
\newblock {\em Journal of Optimization Theory and Applications}, 72(1):7--35,
  1992.

\bibitem{lomo}
Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu.
\newblock Full parameter fine-tuning for large language models with limited
  resources.
\newblock {\em arXiv preprint arXiv:2306.09782}, 2023.

\bibitem{mezo}
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason~D Lee, Danqi
  Chen, and Sanjeev Arora.
\newblock Fine-tuning language models with just forward passes.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{mihic2021managing}
Kre{\v{s}}imir Mihi{\'c}, Mingxi Zhu, and Yinyu Ye.
\newblock Managing randomization in the multi-block alternating direction
  method of multipliers for quadratic optimization.
\newblock {\em Mathematical Programming Computation}, 13(2):339--413, 2021.

\bibitem{numglue}
Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark,
  Chitta Baral, and Ashwin Kalyan.
\newblock {N}um{GLUE}: A suite of fundamental yet challenging mathematical
  reasoning tasks.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, pages 3505--3523, Dublin,
  Ireland, May 2022. Association for Computational Linguistics.

\bibitem{nesterov2012efficiency}
Yu~Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock {\em SIAM Journal on Optimization}, 22(2):341--362, 2012.

\bibitem{nutini2022let}
Julie Nutini, Issam Laradji, and Mark Schmidt.
\newblock Let's make block coordinate descent converge faster: faster greedy
  rules, message-passing, active-set complexity, and superlinear convergence.
\newblock {\em Journal of Machine Learning Research}, 23(131):1--74, 2022.

\bibitem{ortega1970iterative}
J.M. Ortega and W.C. Rheinboldt.
\newblock {\em Iterative Solution of Nonlinear Equations in Several Variables},
  volume~30.
\newblock SIAM, 1970.

\bibitem{instructgpt}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in Neural Information Processing Systems},
  35:27730--27744, 2022.

\bibitem{pan2024lisa}
Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang, Chi Han, and Tong
  Zhang.
\newblock {LISA}: Layerwise importance sampling for memory-efficient large
  language model fine-tuning.
\newblock {\em arXiv preprint arXiv:2403.17919}, 2024.

\bibitem{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock Instruction tuning with {GPT}-4.
\newblock {\em arXiv preprint arXiv:2304.03277}, 2023.

\bibitem{jiant}
Yada Pruksachatkun, Phil Yeres, Haokun Liu, Jason Phang, Phu~Mon Htut, Alex
  Wang, Ian Tenney, and Samuel~R Bowman.
\newblock jiant: A software toolkit for research on general-purpose text
  understanding models.
\newblock In {\em 58th Annual Meeting of the Association for Computational
  Linguistics, ACL 2020}, pages 109--117, 2020.

\bibitem{rajbhandari2021zero}
Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong He.
\newblock Zero-infinity: Breaking the gpu memory wall for extreme scale deep
  learning.
\newblock In {\em Proceedings of the international conference for high
  performance computing, networking, storage and analysis}, pages 1--14, 2021.

\bibitem{ren2021zero}
Jie Ren, Samyam Rajbhandari, Reza~Yazdani Aminabadi, Olatunji Ruwase, Shuangyan
  Yang, Minjia Zhang, Dong Li, and Yuxiong He.
\newblock Zero-offload: Democratizing billion-scale model training.
\newblock {\em arXiv preprint arXiv:2101.06840}, 2021.

\bibitem{richtarik2014iteration}
Peter Richt{\'a}rik and Martin Tak{\'a}{\v{c}}.
\newblock Iteration complexity of randomized block-coordinate descent methods
  for minimizing a composite function.
\newblock {\em Mathematical Programming}, 144(1):1--38, 2014.

\bibitem{sun2015expected}
Ruoyu Sun, Zhi-Quan Luo, and Yinyu Ye.
\newblock On the expected convergence of randomly permuted {ADMM}.
\newblock {\em arXiv preprint arXiv:1503.06387}, 4(6), 2015.

\bibitem{sun2021worst}
Ruoyu Sun and Yinyu Ye.
\newblock Worst-case complexity of cyclic coordinate descent: O (n\^{} 2) gap
  with randomized version.
\newblock {\em Mathematical Programming}, 185:487--520, 2021.

\bibitem{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford {A}lpaca: An instruction-following {LLaMA} model.
\newblock {\em GitHub repository}, 2023.

\bibitem{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock {Llama} 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{tseng2001convergence}
Paul Tseng.
\newblock Convergence of a block coordinate descent method for
  nondifferentiable minimization.
\newblock {\em Journal of Optimization Theory and Applications}, 109:475--494,
  2001.

\bibitem{wang2019superglue}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
  Felix Hill, Omer Levy, and Samuel Bowman.
\newblock Super{GLUE}: A stickier benchmark for general-purpose language
  understanding systems.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{wang2024closing}
Bohan Wang, Jingwen Fu, Huishuai Zhang, Nanning Zheng, and Wei Chen.
\newblock Closing the gap between the upper bound and lower bound of adam's
  iteration complexity.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{wright2015coordinate}
Stephen~J Wright.
\newblock Coordinate descent algorithms.
\newblock {\em Mathematical Programming}, 151(1):3--34, 2015.

\bibitem{chainoflora}
Wenhan Xia, Chengwei Qin, and Elad Hazan.
\newblock Chain of {LoRA}: Efficient fine-tuning of language models via
  residual learning.
\newblock {\em arXiv preprint arXiv:2401.04151}, 2024.

\bibitem{yang2024harnessing}
Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming
  Jiang, Shaochen Zhong, Bing Yin, and Xia Hu.
\newblock Harnessing the power of {LLM}s in practice: A survey on {ChatGPT} and
  beyond.
\newblock {\em ACM Transactions on Knowledge Discovery from Data}, 18(6):1--32,
  2024.

\bibitem{yue2023mammoth}
Xiang Yue, Xingwei Qu, Ge~Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu~Su, and
  Wenhu Chen.
\newblock Mammoth: Building math generalist models through hybrid instruction
  tuning.
\newblock {\em arXiv preprint arXiv:2309.05653}, 2023.

\bibitem{zhang2024scaling}
Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat.
\newblock When scaling meets {LLM} finetuning: The effect of data, model and
  finetuning method.
\newblock {\em The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem{zhang2023instruction}
Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang,
  Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et~al.
\newblock Instruction tuning for large language models: A survey.
\newblock {\em arXiv preprint arXiv:2308.10792}, 2023.

\bibitem{adam-mini}
Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Yinyu Ye,
  Zhi-Quan Luo, and Ruoyu Sun.
\newblock Adam-mini: Use fewer learning rates to gain more.
\newblock {\em arXiv preprint arXiv:2406.16793}, 2024.

\bibitem{zhang2022adam}
Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo.
\newblock Adam can converge without any modification on update rules.
\newblock {\em Advances in Neural Information Processing Systems},
  35:28386--28399, 2022.

\bibitem{galore}
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and
  Yuandong Tian.
\newblock Galore: Memory-efficient llm training by gradient low-rank
  projection.
\newblock {\em arXiv preprint arXiv:2403.03507}, 2024.

\bibitem{zhao2022randomized}
Lei Zhao, Ding Chen, Daoli Zhu, and Xiao Li.
\newblock Randomized coordinate subgradient method for nonsmooth optimization.
\newblock {\em arXiv preprint arXiv:2206.14981}, 2022.

\bibitem{mtbench}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging {LLM}-as-a-judge with {MT}-bench and chatbot arena.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{zheng2024llamafactory}
Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang
  Ma.
\newblock Llama{F}actory: Unified efficient fine-tuning of 100+ language
  models.
\newblock {\em arXiv preprint arXiv:2403.13372}, 2024.

\bibitem{sat-math}
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin
  Saied, Weizhu Chen, and Nan Duan.
\newblock {AGIE}val: A human-centric benchmark for evaluating foundation
  models.
\newblock In {\em Findings of the Association for Computational Linguistics:
  NAACL 2024}, pages 2299--2314, Mexico City, Mexico, June 2024. Association
  for Computational Linguistics.

\bibitem{zhu2024lift}
Ligeng Zhu, Lanxiang Hu, Ji~Lin, and Song Han.
\newblock {LIFT}: Efficient layer-wise fine-tuning for large model models,
  2024.

\bibitem{zhuang2023open}
Shengyao Zhuang, Bing Liu, Bevan Koopman, and Guido Zuccon.
\newblock Open-source large language models are strong zero-shot query
  likelihood models for document ranking.
\newblock In {\em The 2023 Conference on Empirical Methods in Natural Language
  Processing}, 2023.

\bibitem{zhuang2023efficiently}
Yan Zhuang, Qi~Liu, Yuting Ning, Weizhe Huang, Rui Lv, Zhenya Huang, Guanhao
  Zhao, Zheng Zhang, Qingyang Mao, Shijin Wang, et~al.
\newblock Efficiently measuring the cognitive ability of {LLM}s: An adaptive
  testing perspective.
\newblock {\em arXiv preprint arXiv:2306.10512}, 2023.

\end{thebibliography}
