\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2024)Ahn, Verma, Lou, Liu, Zhang, and Yin]{ahn2024large}
Janice Ahn, Rishu Verma, Renze Lou, Di~Liu, Rui Zhang, and Wenpeng Yin.
\newblock Large language models for mathematical reasoning: Progresses and challenges.
\newblock \emph{arXiv preprint arXiv:2402.00157}, 2024.

\bibitem[Albergo and Vanden-Eijnden(2022)]{albergo2022building}
Michael~S Albergo and Eric Vanden-Eijnden.
\newblock Building normalizing flows with stochastic interpolants.
\newblock \emph{arXiv preprint arXiv:2209.15571}, 2022.

\bibitem[Austin et~al.(2021{\natexlab{a}})Austin, Johnson, Ho, Tarlow, and Van Den~Berg]{austin2021structured}
Jacob Austin, Daniel~D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den~Berg.
\newblock Structured denoising diffusion models in discrete state-spaces.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 17981--17993, 2021{\natexlab{a}}.

\bibitem[Austin et~al.(2021{\natexlab{b}})Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{austin2021program}
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021{\natexlab{b}}.

\bibitem[Besnier and Chen(2023)]{besnier2023MaskGit_pytorch}
Victor Besnier and Mickael Chen.
\newblock A pytorch reproduction of masked generative image transformer, 2023.

\bibitem[Blattmann et~al.(2023)Blattmann, Dockhorn, Kulal, Mendelevitch, Kilian, Lorenz, Levi, English, Voleti, Letts, et~al.]{blattmann2023stable}
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et~al.
\newblock Stable video diffusion: Scaling latent video diffusion models to large datasets.
\newblock \emph{arXiv preprint arXiv:2311.15127}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Campbell et~al.(2022)Campbell, Benton, De~Bortoli, Rainforth, Deligiannidis, and Doucet]{campbell2022continuous}
Andrew Campbell, Joe Benton, Valentin De~Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet.
\newblock A continuous time framework for discrete denoising models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 28266--28279, 2022.

\bibitem[Campbell et~al.(2024)Campbell, Yim, Barzilay, Rainforth, and Jaakkola]{campbell2024generative}
Andrew Campbell, Jason Yim, Regina Barzilay, Tom Rainforth, and Tommi Jaakkola.
\newblock Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design.
\newblock \emph{arXiv preprint arXiv:2402.04997}, 2024.

\bibitem[Chang et~al.(2022)Chang, Zhang, Jiang, Liu, and Freeman]{chang2022maskgit}
Huiwen Chang, Han Zhang, Lu~Jiang, Ce~Liu, and William~T Freeman.
\newblock Maskgit: Masked generative image transformer.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11315--11325, 2022.

\bibitem[Chang et~al.(2023)Chang, Zhang, Barber, Maschinot, Lezama, Jiang, Yang, Murphy, Freeman, Rubinstein, et~al.]{chang2023muse}
Huiwen Chang, Han Zhang, Jarred Barber, AJ~Maschinot, Jose Lezama, Lu~Jiang, Ming-Hsuan Yang, Kevin Murphy, William~T Freeman, Michael Rubinstein, et~al.
\newblock Muse: Text-to-image generation via masked generative transformers.
\newblock \emph{arXiv preprint arXiv:2301.00704}, 2023.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Chen et~al.(2022)Chen, Zhang, and Hinton]{Chen2022AnalogBG}
Ting Chen, Ruixiang Zhang, and Geoffrey~E. Hinton.
\newblock Analog bits: Generating discrete data using diffusion models with self-conditioning.
\newblock \emph{ArXiv}, 2022.

\bibitem[Copet et~al.(2024)Copet, Kreuk, Gat, Remez, Kant, Synnaeve, Adi, and D{\'e}fossez]{copet2024simple}
Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre D{\'e}fossez.
\newblock Simple and controllable music generation.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Dhariwal and Nichol(2021)]{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 8780--8794, 2021.

\bibitem[Dieleman et~al.(2022)Dieleman, Sartran, Roshannai, Savinov, Ganin, Richemond, Doucet, Strudel, Dyer, Durkan, et~al.]{dieleman2022continuous}
Sander Dieleman, Laurent Sartran, Arman Roshannai, Nikolay Savinov, Yaroslav Ganin, Pierre~H Richemond, Arnaud Doucet, Robin Strudel, Chris Dyer, Conor Durkan, et~al.
\newblock Continuous diffusion for categorical data.
\newblock \emph{arXiv preprint arXiv:2211.15089}, 2022.

\bibitem[Esser et~al.(2024)Esser, Kulal, Blattmann, Entezari, M{\"u}ller, Saini, Levi, Lorenz, Sauer, Boesel, et~al.]{esser2024scaling}
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M{\"u}ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et~al.
\newblock Scaling rectified flow transformers for high-resolution image synthesis.
\newblock \emph{arXiv preprint arXiv:2403.03206}, 2024.

\bibitem[Ferruz and H{\"o}cker(2022)]{ferruz2022controllable}
Noelia Ferruz and Birte H{\"o}cker.
\newblock Controllable protein design with language models.
\newblock \emph{Nature Machine Intelligence}, 4\penalty0 (6):\penalty0 521--532, 2022.

\bibitem[Ghazvininejad et~al.(2019)Ghazvininejad, Levy, Liu, and Zettlemoyer]{ghazvininejad2019mask}
Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer.
\newblock Mask-predict: Parallel decoding of conditional masked language models.
\newblock \emph{arXiv preprint arXiv:1904.09324}, 2019.

\bibitem[Gokaslan and Cohen(2019)]{Gokaslan2019OpenWeb}
Aaron Gokaslan and Vanya Cohen.
\newblock Openwebtext corpus.
\newblock \url{http://Skylion007.github.io/OpenWebTextCorpus}, 2019.

\bibitem[Han et~al.(2022)Han, Kumar, and Tsvetkov]{han2022ssd}
Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov.
\newblock Ssd-lm: Semi-autoregressive simplex-based diffusion language model for text generation and modular control.
\newblock \emph{arXiv preprint arXiv:2210.17432}, 2022.

\bibitem[Hassid et~al.(2024)Hassid, Remez, Nguyen, Gat, Conneau, Kreuk, Copet, Defossez, Synnaeve, Dupoux, et~al.]{hassid2024textually}
Michael Hassid, Tal Remez, Tu~Anh Nguyen, Itai Gat, Alexis Conneau, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel Dupoux, et~al.
\newblock Textually pretrained speech language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[He et~al.(2022)He, Sun, Wang, Huang, and Qiu]{He2022DiffusionBERTIG}
Zhengfu He, Tianxiang Sun, Kuan Wang, Xuanjing Huang, and Xipeng Qiu.
\newblock Diffusionbert: Improving generative masked language models with diffusion models.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, 2022.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and Hochreiter]{heusel2017GANs}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash equilibrium.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 6840--6851, 2020.

\bibitem[Hoogeboom et~al.(2021)Hoogeboom, Nielsen, Jaini, Forr{\'e}, and Welling]{hoogeboom2021argmax}
Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr{\'e}, and Max Welling.
\newblock Argmax flows and multinomial diffusion: Learning categorical distributions.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 12454--12465, 2021.

\bibitem[Imani et~al.(2023)Imani, Du, and Shrivastava]{imani2023mathprompter}
Shima Imani, Liang Du, and Harsh Shrivastava.
\newblock Mathprompter: Mathematical reasoning using large language models.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)}, pages 37--42, 2023.

\bibitem[Kreuk et~al.(2022)Kreuk, Synnaeve, Polyak, Singer, D{\'e}fossez, Copet, Parikh, Taigman, and Adi]{kreuk2022audiogen}
Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D{\'e}fossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi.
\newblock Audiogen: Textually guided audio generation.
\newblock \emph{arXiv preprint arXiv:2209.15352}, 2022.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{arXiv}, 2009.

\bibitem[Li et~al.(2023)Li, Allal, Zi, Muennighoff, Kocetkov, Mou, Marone, Akiki, Li, Chim, et~al.]{li2023starcoder}
Raymond Li, Loubna~Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et~al.
\newblock Starcoder: may the source be with you!
\newblock \emph{arXiv preprint arXiv:2305.06161}, 2023.

\bibitem[Li et~al.(2022)Li, Thickstun, Gulrajani, Liang, and Hashimoto]{Li2022DiffusionLMIC}
Xiang~Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto.
\newblock Diffusion-lm improves controllable text generation.
\newblock \emph{ArXiv}, 2022.

\bibitem[Lin et~al.(2022)Lin, Gong, Shen, Wu, Fan, Lin, Chen, and Duan]{Lin2022GENIEL}
Zheng-Wen Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Weizhu Chen, and Nan Duan.
\newblock Genie : Large scale pre-training for generation with diffusion model.
\newblock In \emph{ArXiv}, 2022.

\bibitem[Lipman et~al.(2022)Lipman, Chen, Ben-Hamu, Nickel, and Le]{lipman2022flow}
Yaron Lipman, Ricky~TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.
\newblock Flow matching for generative modeling.
\newblock \emph{arXiv preprint arXiv:2210.02747}, 2022.

\bibitem[Liu et~al.(2022)Liu, Gong, and Liu]{liu2022flow}
Xingchao Liu, Chengyue Gong, and Qiang Liu.
\newblock Flow straight and fast: Learning to generate and transfer data with rectified flow.
\newblock \emph{arXiv preprint arXiv:2209.03003}, 2022.

\bibitem[Lou et~al.(2023)Lou, Meng, and Ermon]{lou2023discrete}
Aaron Lou, Chenlin Meng, and Stefano Ermon.
\newblock Discrete diffusion language modeling by estimating the ratios of the data distribution.
\newblock \emph{arXiv preprint arXiv:2310.16834}, 2023.

\bibitem[Lovelace et~al.(2022)Lovelace, Kishore, gang Wan, Shekhtman, and Weinberger]{Lovelace2022LatentDF}
Justin Lovelace, Varsha Kishore, Chao gang Wan, Eliot Shekhtman, and Kilian~Q. Weinberger.
\newblock Latent diffusion for language generation.
\newblock \emph{ArXiv}, 2022.

\bibitem[Madani et~al.(2023)Madani, Krause, Greene, Subramanian, Mohr, Holton, Olmos, Xiong, Sun, Socher, et~al.]{madani2023large}
Ali Madani, Ben Krause, Eric~R Greene, Subu Subramanian, Benjamin~P Mohr, James~M Holton, Jose~Luis Olmos, Caiming Xiong, Zachary~Z Sun, Richard Socher, et~al.
\newblock Large language models generate functional protein sequences across diverse families.
\newblock \emph{Nature Biotechnology}, 41\penalty0 (8):\penalty0 1099--1106, 2023.

\bibitem[Norris(1998)]{norris1998markov}
James~R Norris.
\newblock \emph{Markov chains}.
\newblock Number~2. Cambridge university press, 1998.

\bibitem[Peebles and Xie(2022)]{Peebles2022DiT}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock \emph{arXiv preprint arXiv:2212.09748}, 2022.

\bibitem[Pooladian et~al.(2023)Pooladian, Ben-Hamu, Domingo-Enrich, Amos, Lipman, and Chen]{pooladian2023multisample}
Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky~TQ Chen.
\newblock Multisample flow matching: Straightening flows with minibatch couplings.
\newblock \emph{arXiv preprint arXiv:2304.14772}, 2023.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{Radford2019LanguageMA}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.
\newblock \url{https://api.semanticscholar.org/CorpusID:160025533}.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem[Romera-Paredes et~al.(2024)Romera-Paredes, Barekatain, Novikov, Balog, Kumar, Dupont, Ruiz, Ellenberg, Wang, Fawzi, et~al.]{romera2024mathematical}
Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M~Pawan Kumar, Emilien Dupont, Francisco~JR Ruiz, Jordan~S Ellenberg, Pengming Wang, Omar Fawzi, et~al.
\newblock Mathematical discoveries from program search with large language models.
\newblock \emph{Nature}, 625\penalty0 (7995):\penalty0 468--475, 2024.

\bibitem[Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin, et~al.]{roziere2023code}
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J{\'e}r{\'e}my Rapin, et~al.
\newblock Code llama: Open foundation models for code.
\newblock \emph{arXiv preprint arXiv:2308.12950}, 2023.

\bibitem[Savinov et~al.(2021)Savinov, Chung, Binkowski, Elsen, and Oord]{savinov2021step}
Nikolay Savinov, Junyoung Chung, Mikolaj Binkowski, Erich Elsen, and Aaron van~den Oord.
\newblock Step-unrolled denoising autoencoders for text generation.
\newblock \emph{arXiv preprint arXiv:2112.06749}, 2021.

\bibitem[Singer et~al.(2022)Singer, Polyak, Hayes, Yin, An, Zhang, Hu, Yang, Ashual, Gafni, et~al.]{singer2022make}
Uriel Singer, Adam Polyak, Thomas Hayes, Xi~Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et~al.
\newblock Make-a-video: Text-to-video generation without text-video data.
\newblock \emph{arXiv preprint arXiv:2209.14792}, 2022.

\bibitem[Song and Ermon(2019)]{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Song et~al.(2020)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and Poole]{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock \emph{arXiv preprint arXiv:2011.13456}, 2020.

\bibitem[Stark et~al.(2024)Stark, Jing, Wang, Corso, Berger, Barzilay, and Jaakkola]{stark2024dirichlet}
Hannes Stark, Bowen Jing, Chenyu Wang, Gabriele Corso, Bonnie Berger, Regina Barzilay, and Tommi Jaakkola.
\newblock Dirichlet flow matching with applications to dna sequence design.
\newblock \emph{arXiv preprint arXiv:2402.05841}, 2024.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 568:\penalty0 127063, 2024.

\bibitem[Tong et~al.(2023)Tong, Malkin, Huguet, Zhang, Rector-Brooks, Fatras, Wolf, and Bengio]{tong2023improving}
Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio.
\newblock Improving and generalizing flow-based generative models with minibatch optimal transport.
\newblock \emph{arXiv preprint arXiv:2302.00482}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Zhang et~al.(2024)Zhang, Ding, Lyv, Wang, Yin, Zhang, Yu, Wang, Li, Xiang, et~al.]{zhang2024scientific}
Qiang Zhang, Keyang Ding, Tianwen Lyv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, et~al.
\newblock Scientific large language models: A survey on biological \& chemical domains.
\newblock \emph{arXiv preprint arXiv:2401.14656}, 2024.

\bibitem[Zhao et~al.(2023)Zhao, Zhou, Li, Tang, Wang, Hou, Min, Zhang, Zhang, Dong, et~al.]{zhao2023survey}
Wayne~Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et~al.
\newblock A survey of large language models.
\newblock \emph{arXiv preprint arXiv:2303.18223}, 2023.

\bibitem[Zheng et~al.(2024)Zheng, Chen, Mao, Liu, Zhu, and Zhang]{zheng2024masked}
Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang.
\newblock Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling.
\newblock \emph{arXiv preprint arXiv:2409.02908}, 2024.

\bibitem[Ziv et~al.(2024)Ziv, Gat, Lan, Remez, Kreuk, D{\'e}fossez, Copet, Synnaeve, and Adi]{ziv2024masked}
Alon Ziv, Itai Gat, Gael~Le Lan, Tal Remez, Felix Kreuk, Alexandre D{\'e}fossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi.
\newblock Masked audio generation using a single non-autoregressive transformer.
\newblock \emph{arXiv preprint arXiv:2401.04577}, 2024.

\end{thebibliography}
