\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2010)Agarwal, Dekel, and Xiao]{agarwal2010optimal}
Agarwal, A., Dekel, O., and Xiao, L.
\newblock Optimal algorithms for online convex optimization with multi-point
  bandit feedback.
\newblock In \emph{Conference on Learning Theory}, pp.\  28--40, 2010.

\bibitem[Cesa-Bianchi et~al.(1996)Cesa-Bianchi, Long, and
  Warmuth]{cesa1996worst}
Cesa-Bianchi, N., Long, P.~M., and Warmuth, M.~K.
\newblock Worst-case quadratic loss bounds for prediction using linear
  functions and gradient descent.
\newblock \emph{IEEE Transactions on Neural Networks}, 7\penalty0 (3):\penalty0
  604--619, 1996.

\bibitem[Chen et~al.(2017)Chen, Ling, and Giannakis]{chen2017online}
Chen, T., Ling, Q., and Giannakis, G.~B.
\newblock An online convex optimization approach to proactive network resource
  allocation.
\newblock \emph{IEEE Transactions on Signal Processing}, 65\penalty0
  (24):\penalty0 6350--6364, 2017.

\bibitem[Crammer et~al.(2006)Crammer, Dekel, Keshet, Shalev-Shwartz, and
  Singer]{crammer2006online}
Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S., and Singer, Y.
\newblock Online passive aggressive algorithms.
\newblock \emph{Journal of Machine Learning Research}, 7:\penalty0 551--585,
  2006.

\bibitem[Gentile \& Warmuth(1999)Gentile and Warmuth]{gentile1999linear}
Gentile, C. and Warmuth, M.~K.
\newblock Linear hinge loss and average margin.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  225--231, 1999.

\bibitem[Goldfarb \& Tucker(2011)Goldfarb and Tucker]{goldfarb2011online}
Goldfarb, A. and Tucker, C.
\newblock Online display advertising: Targeting and obtrusiveness.
\newblock \emph{Marketing Science}, 30\penalty0 (3):\penalty0 389--404, 2011.

\bibitem[Gordon(1999)]{gordon1999regret}
Gordon, G.~J.
\newblock Regret bounds for prediction problems.
\newblock In \emph{Conference on Learning Theory}, pp.\  29--40, 1999.

\bibitem[Hazan(2016)]{hazan2016introduction}
Hazan, E.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends in Optimization}, 2\penalty0
  (3-4):\penalty0 157--325, 2016.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{hazan2007logarithmic}
Hazan, E., Agarwal, A., and Kale, S.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 169--192, 2007.

\bibitem[Jadbabaie et~al.(2015)Jadbabaie, Rakhlin, Shahrampour, and
  Sridharan]{jadbabaie2015online}
Jadbabaie, A., Rakhlin, A., Shahrampour, S., and Sridharan, K.
\newblock Online optimization: Competing with dynamic comparators.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  398--406, 2015.

\bibitem[Jenatton et~al.(2016)Jenatton, Huang, and
  Archambeau]{jenatton2016adaptive}
Jenatton, R., Huang, J., and Archambeau, C.
\newblock Adaptive algorithms for online convex optimization with long-term
  constraints.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  402--411, 2016.

\bibitem[Li et~al.(2020)Li, Yi, and Xie]{Li2018distributed}
Li, X., Yi, X., and Xie, L.
\newblock Distributed online optimization for multi-agent networks with coupled
  inequality constraints.
\newblock \emph{IEEE Transactions on Automatic Control}, 2020.

\bibitem[Mahdavi et~al.(2012)Mahdavi, Jin, and Yang]{mahdavi2012trading}
Mahdavi, M., Jin, R., and Yang, T.
\newblock Trading regret for efficiency: Online convex optimization with long
  term constraints.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (81):\penalty0 2503--2528, 2012.

\bibitem[Mokhtari et~al.(2016)Mokhtari, Shahrampour, Jadbabaie, and
  Ribeiro]{mokhtari2016online}
Mokhtari, A., Shahrampour, S., Jadbabaie, A., and Ribeiro, A.
\newblock Online optimization in dynamic environments: Improved regret rates
  for strongly convex problems.
\newblock In \emph{IEEE Conference on Decision and Control}, pp.\  7195--7201,
  2016.

\bibitem[Neely \& Yu(2017)Neely and Yu]{neely2017online}
Neely, M.~J. and Yu, H.
\newblock Online convex optimization with time-varying constraints.
\newblock \emph{arXiv:1702.04783}, 2017.

\bibitem[Shalev-Shwartz(2012)]{shalev2012online}
Shalev-Shwartz, S.
\newblock Online learning and online convex optimization.
\newblock \emph{Foundations and Trends in Machine Learning}, 4\penalty0
  (2):\penalty0 107--194, 2012.

\bibitem[Sun et~al.(2017)Sun, Dey, and Kapoor]{sun2017safety}
Sun, W., Dey, D., and Kapoor, A.
\newblock Safety-aware algorithms for adversarial contextual bandit.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3280--3288, 2017.

\bibitem[van Erven \& Koolen(2016)van Erven and Koolen]{NIPS2016_14cfdb59}
van Erven, T. and Koolen, W.~M.
\newblock Metagrad: Multiple learning rates in online learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3666--3674, 2016.

\bibitem[Yi et~al.(2020{\natexlab{a}})Yi, Li, Xie, and
  Johansson]{yi2020distributed}
Yi, X., Li, X., Xie, L., and Johansson, K.~H.
\newblock Distributed online convex optimization with time-varying coupled
  inequality constraints.
\newblock \emph{IEEE Transactions on Signal Processing}, 68:\penalty0 731--746,
  2020{\natexlab{a}}.

\bibitem[Yi et~al.(2020{\natexlab{b}})Yi, Li, Yang, Xie, Johansson, and
  Chai]{yi2019distributed}
Yi, X., Li, X., Yang, T., Xie, L., Johansson, K.~H., and Chai, T.
\newblock Distributed bandit online convex optimization with time-varying
  coupled inequality constraints.
\newblock \emph{IEEE Transactions on Automatic Control}, 2020{\natexlab{b}}.

\bibitem[Yi et~al.(2021)Yi, Li, Yang, Xie, Chai, and Johansson]{yi2021regret}
Yi, X., Li, X., Yang, T., Xie, L., Chai, T., and Johansson, K.~H.
\newblock Regret and cumulative constraint violation analysis for distributed
  online constrained convex optimization.
\newblock \emph{arXiv preprint arXiv:2105.00321}, 2021.

\bibitem[Yu \& Neely(2020)Yu and Neely]{yu2020lowJMLR}
Yu, H. and Neely, M.~J.
\newblock A low complexity algorithm with $ {O}(\sqrt{T})$ regret and $ {O}(1)$
  constraint violations for online convex optimization with long term
  constraints.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 1--24, 2020.

\bibitem[Yu et~al.(2017)Yu, Neely, and Wei]{yu2017online}
Yu, H., Neely, M., and Wei, X.
\newblock Online convex optimization with stochastic constraints.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1428--1438, 2017.

\bibitem[Yuan et~al.(2021{\natexlab{a}})Yuan, Proutiere, and
  Shi]{yuan2021distributed}
Yuan, D., Proutiere, A., and Shi, G.
\newblock Distributed online linear regression.
\newblock \emph{IEEE Transactions on Information Theory}, 67\penalty0
  (1):\penalty0 616--639, 2021{\natexlab{a}}.

\bibitem[Yuan et~al.(2021{\natexlab{b}})Yuan, Proutiere, and
  Shi]{yuan2021distributedtac}
Yuan, D., Proutiere, A., and Shi, G.
\newblock Distributed online optimization with long-term constraints.
\newblock \emph{IEEE Transactions on Automatic Control}, 2021{\natexlab{b}}.

\bibitem[Yuan \& Lamperski(2018)Yuan and Lamperski]{NIPS2018_7852}
Yuan, J. and Lamperski, A.
\newblock Online convex optimization for cumulative constraints.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6140--6149, 2018.

\bibitem[Zhang(2020)]{ijcai2020-731}
Zhang, L.
\newblock Online learning in changing environments.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  pp.\  5178--5182, 2020.

\bibitem[Zhang et~al.(2017)Zhang, Yang, Yi, Rong, and Zhou]{zhang2017improved}
Zhang, L., Yang, T., Yi, J., Rong, J., and Zhou, Z.-H.
\newblock Improved dynamic regret for non-degenerate functions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  732--741, 2017.

\bibitem[Zhang et~al.(2018{\natexlab{a}})Zhang, Lu, and
  Zhou]{zhang2018adaptive}
Zhang, L., Lu, S., and Zhou, Z.-H.
\newblock Adaptive online learning in dynamic environments.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1323--1333, 2018{\natexlab{a}}.

\bibitem[Zhang et~al.(2018{\natexlab{b}})Zhang, Yang, Jin, and
  Zhou]{zhang2018dynamic}
Zhang, L., Yang, T., Jin, R., and Zhou, Z.-H.
\newblock Dynamic regret of strongly adaptive methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5882--5891, 2018{\natexlab{b}}.

\bibitem[Zhang et~al.(2019)Zhang, Liu, and Zhou]{pmlr-v97-zhang19}
Zhang, L., Liu, T.-Y., and Zhou, Z.-H.
\newblock Adaptive regret of convex and smooth functions.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7414--7423, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Zhao, and Zhou]{pmlr-v124-zhang20a}
Zhang, Y.-J., Zhao, P., and Zhou, Z.-H.
\newblock A simple online algorithm for competing with dynamic comparators.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, pp.\
  390--399, 2020.

\bibitem[Zhao \& Zhang(2020)Zhao and Zhang]{zhao2020improved}
Zhao, P. and Zhang, L.
\newblock Improved analysis for dynamic regret of strongly convex and smooth
  functions.
\newblock \emph{arXiv:2006.05876}, 2020.

\bibitem[Zhao et~al.(2020)Zhao, Zhang, Zhang, and Zhou]{zhao2020dynamic}
Zhao, P., Zhang, Y.-J., Zhang, L., and Zhou, Z.-H.
\newblock Dynamic regret of convex and smooth functions.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
Zinkevich, M.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  928--936, 2003.

\end{thebibliography}
