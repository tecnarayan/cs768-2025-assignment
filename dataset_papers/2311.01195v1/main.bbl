\begin{thebibliography}{10}

\bibitem{berkenkamp2019no}
F.~Berkenkamp, A.~P. Schoellig, and A.~Krause.
\newblock No-regret {Bayesian} optimization with unknown hyperparameters.
\newblock {\em Journal of Machine Learning Research}, 2019.

\bibitem{binois2019replication}
M.~Binois, J.~Huang, R.~B. Gramacy, and M.~Ludkovski.
\newblock Replication or exploration? sequential design for stochastic simulation experiments.
\newblock {\em Technometrics}, 61(1):7--23, 2019.

\bibitem{bogunovic2018adversarially}
I.~Bogunovic, J.~Scarlett, S.~Jegelka, and V.~Cevher.
\newblock Adversarially robust optimization with {Gaussian} processes.
\newblock In {\em Proc. {NeuIPS}}, 2018.

\bibitem{brockman2016openai}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and W.~Zaremba.
\newblock {OpenAI Gym}.
\newblock {arXiv}:1606.01540, 2016.

\bibitem{cai2021lower}
X.~Cai and J.~Scarlett.
\newblock On lower bounds for standard and robust {Gaussian} process bandit optimization.
\newblock In {\em Proc. {ICML}}, pages 1216--1226. PMLR, 2021.

\bibitem{cakmak2020bayesian}
S.~Cakmak, R.~Astudillo, P.~Frazier, and E.~Zhou.
\newblock {Bayesian} optimization of risk measures.
\newblock In {\em Proc. {NeurIPS}}, 2020.

\bibitem{calandriello2022scaling}
D.~Calandriello, L.~Carratino, A.~Lazaric, M.~Valko, and L.~Rosasco.
\newblock Scaling {Gaussian} process optimization by evaluating a few unique candidates multiple times.
\newblock In {\em Proc. {ICML}}, pages 2523--2541. PMLR, 2022.

\bibitem{chowdhury2017kernelized}
S.~R. Chowdhury and A.~Gopalan.
\newblock On kernelized multi-armed bandits.
\newblock In {\em Proc. {ICML}}, pages 844--853, 2017.

\bibitem{chowdhury2019batch}
S.~R. Chowdhury and A.~Gopalan.
\newblock On batch {Bayesian} optimization.
\newblock {arXiv}:1911.01032, 2019.

\bibitem{cohen2017emnist}
G.~Cohen, S.~Afshar, J.~Tapson, and A.~Van~Schaik.
\newblock {EMNIST}: Extending {MNIST} to handwritten letters.
\newblock In {\em Proc. {IJCNN}}, pages 2921--2926. IEEE, 2017.

\bibitem{contal2013parallel}
E.~Contal, D.~Buffoni, A.~Robicquet, and N.~Vayatis.
\newblock Parallel {Gaussian} process optimization with upper confidence bound and pure exploration.
\newblock In {\em Proc. {ECML/PKDD}}, pages 225--240, 2013.

\bibitem{cowen2020empirical}
A.~I. Cowen-Rivers, W.~Lyu, R.~Tutunov, Z.~Wang, A.~Grosnit, R.~R. Griffiths, A.~M. Maraval, H.~Jianye, J.~Wang, J.~Peters, et~al.
\newblock An empirical study of assumptions in {Bayesian} optimisation.
\newblock {arXiv}:2012.03826, 2020.

\bibitem{dai2023quantum}
Z.~Dai, G.~K.~R. Lau, A.~Verma, Y.~Shu, B.~K.~H. Low, and P.~Jaillet.
\newblock Quantum {Bayesian} optimization.
\newblock In {\em Proc. {NeurIPS}}, 2023.

\bibitem{dai2020federated}
Z.~Dai, B.~K.~H. Low, and P.~Jaillet.
\newblock Federated {Bayesian} optimization via {Thompson} sampling.
\newblock In {\em Proc. {NeurIPS}}, 2020.

\bibitem{dai2021differentially}
Z.~Dai, B.~K.~H. Low, and P.~Jaillet.
\newblock Differentially private federated {Bayesian} optimization with distributed exploration.
\newblock In {\em Proc. {NeurIPS}}, volume~34, 2021.

\bibitem{dai2022sample}
Z.~Dai, Y.~Shu, B.~K.~H. Low, and P.~Jaillet.
\newblock Sample-then-optimize batch neural {Thompson} sampling.
\newblock In {\em Proc. {NeurIPS}}, 2022.

\bibitem{dai2022federated}
Z.~Dai, Y.~Shu, A.~Verma, F.~X. Fan, B.~K.~H. Low, and P.~Jaillet.
\newblock Federated neural bandits.
\newblock In {\em Proc. {ICLR}}, 2023.

\bibitem{daxberger2017distributed}
E.~A. Daxberger and B.~K.~H. Low.
\newblock Distributed batch {Gaussian} process optimization.
\newblock In {\em Proc. {ICML}}, pages 951--960, 2017.

\bibitem{desautels2014parallelizing}
T.~Desautels, A.~Krause, and J.~W. Burdick.
\newblock Parallelizing exploration-exploitation tradeoffs in {Gaussian} process bandit optimization.
\newblock {\em Journal of Machine Learning Research}, 15:3873--3923, 2014.

\bibitem{eriksson2019scalable}
D.~Eriksson, M.~Pearce, J.~Gardner, R.~D. Turner, and M.~Poloczek.
\newblock Scalable global optimization via local {Bayesian} optimization.
\newblock In {\em Proc. {NeurIPS}}, 2019.

\bibitem{frazier2018tutorial}
P.~I. Frazier.
\newblock A tutorial on {Bayesian} optimization.
\newblock {arXiv}:1807.02811, 2018.

\bibitem{gonzalez2016batch}
J.~Gonz{\'a}lez, Z.~Dai, P.~Hennig, and N.~Lawrence.
\newblock Batch {Bayesian} optimization via local penalization.
\newblock In {\em Proc. {AISTATS}}, pages 648--657. PMLR, 2016.

\bibitem{griffiths2021achieving}
R.-R. Griffiths, A.~A. Aldrick, M.~Garcia-Ortegon, V.~Lalchand, et~al.
\newblock Achieving robustness to aleatoric uncertainty with heteroscedastic {Bayesian} optimisation.
\newblock {\em Machine Learning: Science and Technology}, 3(1):015004, 2021.

\bibitem{hoffman2013exploiting}
M.~W. Hoffman, B.~Shahriari, and N.~de~Freitas.
\newblock Exploiting correlation and budget constraints in {Bayesian} multi-armed bandit optimization.
\newblock {arXiv}:1303.6746, 2013.

\bibitem{hutter2019automated}
F.~Hutter, L.~Kotthoff, and J.~Vanschoren.
\newblock {\em Automated machine learning: methods, systems, challenges}.
\newblock Springer Nature, 2019.

\bibitem{iwazaki2021mean}
S.~Iwazaki, Y.~Inatsu, and I.~Takeuchi.
\newblock Mean-variance analysis in {Bayesian} optimization under uncertainty.
\newblock In {\em Proc. {AISTATS}}, pages 973--981. PMLR, 2021.

\bibitem{jain2022biological}
M.~Jain, E.~Bengio, A.~Hernandez-Garcia, J.~Rector-Brooks, B.~F. Dossou, C.~A. Ekbote, J.~Fu, T.~Zhang, M.~Kilgour, D.~Zhang, et~al.
\newblock Biological sequence design with gflownets.
\newblock In {\em proc. {ICML}}, pages 9786--9801. PMLR, 2022.

\bibitem{kandasamy2018parallelised}
K.~Kandasamy, A.~Krishnamurthy, J.~Schneider, and B.~P{\'o}czos.
\newblock Parallelised {Bayesian} optimisation via thompson sampling.
\newblock In {\em Proc. {AISTATS}}, pages 133--142. PMLR, 2018.

\bibitem{kersting2007most}
K.~Kersting, C.~Plagemann, P.~Pfaff, and W.~Burgard.
\newblock Most likely heteroscedastic {Gaussian} process regression.
\newblock In {\em Proc. {ICML}}, pages 393--400, 2007.

\bibitem{kirschner2018information}
J.~Kirschner and A.~Krause.
\newblock Information directed sampling and bandits with heteroscedastic noise.
\newblock In {\em Proc. {COLT}}, pages 358--384. PMLR, 2018.

\bibitem{kyveryga2018farm}
P.~M. Kyveryga, T.~A. Mueller, D.~S. Mueller, D.~Shannon, D.~Clay, and N.~Kitchen.
\newblock On-farm replicated strip trials.
\newblock {\em Precis. Agric. Basics}, pages 189--208, 2018.

\bibitem{letham2020re}
B.~Letham, R.~Calandra, A.~Rai, and E.~Bakshy.
\newblock Re-examining linear embeddings for high-dimensional {Bayesian} optimization.
\newblock In {\em Proc. {NeurIPS}}, volume~33, pages 1546--1558, 2020.

\bibitem{makarova2021risk}
A.~Makarova, I.~Usmanova, I.~Bogunovic, and A.~Krause.
\newblock Risk-averse heteroscedastic {Bayesian} optimization.
\newblock {\em Proc. {NeurIPS}}, 34, 2021.

\bibitem{mutny2019efficient}
M.~Mutn{\`y} and A.~Krause.
\newblock Efficient high dimensional {Bayesian} optimization with additivity and quadrature {Fourier} features.
\newblock In {\em Proc. NeurIPS}, pages 9005--9016. Curran, 2019.

\bibitem{nava2022diversified}
E.~Nava, M.~Mutny, and A.~Krause.
\newblock Diversified sampling for batched bayesian optimization with determinantal point processes.
\newblock In {\em Proc. {AISTATS}}, pages 7031--7054. PMLR, 2022.

\bibitem{nguyen2021optimizing}
Q.~P. Nguyen, Z.~Dai, B.~K.~H. Low, and P.~Jaillet.
\newblock Optimizing conditional value-at-risk of black-box functions.
\newblock In {\em Proc. {NeurIPS}}, volume~34, 2021.

\bibitem{nguyen2021value}
Q.~P. Nguyen, Z.~Dai, B.~K.~H. Low, and P.~Jaillet.
\newblock Value-at-risk optimization with {Gaussian} processes.
\newblock In {\em Proc. {ICML}}, 2021.

\bibitem{nguyen2016budgeted}
V.~Nguyen, S.~Rana, S.~K. Gupta, C.~Li, and S.~Venkatesh.
\newblock Budgeted batch {Bayesian} optimization.
\newblock In {\em Proc. {ICDM}}, pages 1107--1112. IEEE, 2016.

\bibitem{rahimi2007random}
A.~Rahimi, B.~Recht, et~al.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Proc. NeurIPS}, volume~3, page~5. Citeseer, 2007.

\bibitem{rasmussen2004gaussian}
C.~E. Rasmussen and C.~K.~I. Williams.
\newblock {\em {Gaussian Processes} for {Machine Learning}}.
\newblock MIT Press, 2006.

\bibitem{srinivas2009gaussian}
N.~Srinivas, A.~Krause, S.~M. Kakade, and M.~Seeger.
\newblock {Gaussian} process optimization in the bandit setting: No regret and experimental design.
\newblock In {\em Proc. {ICML}}, pages 1015--1022, 2010.

\bibitem{tay2022efficient}
S.~S. Tay, C.~S. Foo, U.~Daisuke, R.~Leong, and B.~K.~H. Low.
\newblock Efficient distributionally robust {Bayesian} optimization with worst-case sensitivity.
\newblock In {\em Proc. {ICML}}, pages 21180--21204. PMLR, 2022.

\bibitem{van2021scalable}
A.~van Beek, U.~F. Ghumman, J.~Munshi, S.~Tao, T.~Chien, G.~Balasubramanian, M.~Plumlee, D.~Apley, and W.~Chen.
\newblock Scalable adaptive batch sampling in simulation-based design with heteroscedastic noise.
\newblock {\em Journal of Mechanical Design}, 143(3):031709, 2021.

\bibitem{verma2022bayesian}
A.~Verma, Z.~Dai, and B.~K.~H. Low.
\newblock Bayesian optimization under stochastic delayed feedback.
\newblock In {\em Proc. {ICML}}, pages 22145--22167. PMLR, 2022.

\bibitem{wang2018batched}
Z.~Wang, C.~Gehring, P.~Kohli, and S.~Jegelka.
\newblock Batched large-scale {Bayesian} optimization in high-dimensional spaces.
\newblock In {\em Proc. {AISTATS}}, pages 745--754. PMLR, 2018.

\bibitem{wang2017max}
Z.~Wang and S.~Jegelka.
\newblock Max-value entropy search for efficient {Bayesian} optimization.
\newblock In {\em Proc. {ICML}}, pages 3627--3635. PMLR, 2017.

\bibitem{wu2016parallel}
J.~Wu and P.~Frazier.
\newblock The parallel knowledge gradient method for batch {Bayesian} optimization.
\newblock In {\em Proc. NeurIPS}, volume~29, pages 3126--3134, 2016.

\end{thebibliography}
