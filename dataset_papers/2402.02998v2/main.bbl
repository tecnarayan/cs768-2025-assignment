\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba{\~n}{\'o}n et~al.(2020)Ba{\~n}{\'o}n, Chen, Haddow, Heafield,
  Hoang, Espl{\`a}-Gomis, Forcada, Kamran, Kirefu, Koehn,
  et~al.]{banon2020paracrawl}
Ba{\~n}{\'o}n, M., Chen, P., Haddow, B., Heafield, K., Hoang, H.,
  Espl{\`a}-Gomis, M., Forcada, M., Kamran, A., Kirefu, F., Koehn, P., et~al.
\newblock Paracrawl: Web-scale acquisition of parallel corpora.
\newblock Association for Computational Linguistics (ACL), 2020.

\bibitem[Cao et~al.(2023)Cao, Jiang, Abolfazli, Hamedani, and
  Mokhtari]{cao2023projection}
Cao, J., Jiang, R., Abolfazli, N., Hamedani, E.~Y., and Mokhtari, A.
\newblock Projection-free methods for stochastic simple bilevel optimization
  with convex lower-level problem.
\newblock \emph{arXiv preprint arXiv:2308.07536}, 2023.

\bibitem[Caruana(1997)]{caruana1997multitask}
Caruana, R.
\newblock Multitask learning.
\newblock \emph{Machine learning}, 28:\penalty0 41--75, 1997.

\bibitem[Chaudhry et~al.(2018)Chaudhry, Ranzato, Rohrbach, and
  Elhoseiny]{chaudhry2018efficient}
Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M.
\newblock Efficient lifelong learning with a-gem.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Cisse et~al.(2017)Cisse, Bojanowski, Grave, Dauphin, and
  Usunier]{cisse2017parseval}
Cisse, M., Bojanowski, P., Grave, E., Dauphin, Y., and Usunier, N.
\newblock Parseval networks: Improving robustness to adversarial examples.
\newblock In \emph{International conference on machine learning}, pp.\
  854--863. PMLR, 2017.

\bibitem[Cooper(2018)]{cooper2018loss}
Cooper, Y.
\newblock The loss landscape of overparameterized neural networks.
\newblock \emph{arXiv preprint arXiv:1804.10200}, 2018.

\bibitem[DeepMind et~al.(2020)DeepMind, Babuschkin, Baumli, Bell, Bhupatiraju,
  Bruce, Buchlovsky, Budden, Cai, Clark, Danihelka, Dedieu, Fantacci, Godwin,
  Jones, Hemsley, Hennigan, Hessel, Hou, Kapturowski, Keck, Kemaev, King,
  Kunesch, Martens, Merzic, Mikulik, Norman, Papamakarios, Quan, Ring, Ruiz,
  Sanchez, Sartran, Schneider, Sezener, Spencer, Srinivasan, Stanojevi\'{c},
  Stokowiec, Wang, Zhou, and Viola]{deepmind2020jax}
DeepMind, Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J.,
  Buchlovsky, P., Budden, D., Cai, T., Clark, A., Danihelka, I., Dedieu, A.,
  Fantacci, C., Godwin, J., Jones, C., Hemsley, R., Hennigan, T., Hessel, M.,
  Hou, S., Kapturowski, S., Keck, T., Kemaev, I., King, M., Kunesch, M.,
  Martens, L., Merzic, H., Mikulik, V., Norman, T., Papamakarios, G., Quan, J.,
  Ring, R., Ruiz, F., Sanchez, A., Sartran, L., Schneider, R., Sezener, E.,
  Spencer, S., Srinivasan, S., Stanojevi\'{c}, M., Stokowiec, W., Wang, L.,
  Zhou, G., and Viola, F.
\newblock The {D}eep{M}ind {JAX} {E}cosystem, 2020.
\newblock URL \url{http://github.com/google-deepmind}.

\bibitem[Dempe et~al.(2010)Dempe, Dinh, and Dutta]{dempe2010optimality}
Dempe, S., Dinh, N., and Dutta, J.
\newblock Optimality conditions for a simple convex bilevel programming
  problem.
\newblock \emph{Variational Analysis and Generalized Differentiation in
  Optimization and Control: In Honor of Boris S. Mordukhovich}, pp.\  149--161,
  2010.

\bibitem[Dery et~al.(2021)Dery, Dauphin, and Grangier]{dery2021auxiliary}
Dery, L.~M., Dauphin, Y., and Grangier, D.
\newblock Auxiliary task update decomposition: The good, the bad and the
  neutral.
\newblock \emph{arXiv preprint arXiv:2108.11346}, 2021.

\bibitem[Du et~al.(2018)Du, Czarnecki, Jayakumar, Farajtabar, Pascanu, and
  Lakshminarayanan]{du2018adapting}
Du, Y., Czarnecki, W.~M., Jayakumar, S.~M., Farajtabar, M., Pascanu, R., and
  Lakshminarayanan, B.
\newblock Adapting auxiliary losses using gradient similarity.
\newblock \emph{arXiv preprint arXiv:1812.02224}, 2018.

\bibitem[Farhad et~al.(2021)Farhad, Arkady, Magdalena, Ond{\v{r}}ej, Rajen,
  Vishrav, Costa-jussa, Cristina, Angela, Christian,
  et~al.]{farhad2021findings}
Farhad, A., Arkady, A., Magdalena, B., Ond{\v{r}}ej, B., Rajen, C., Vishrav,
  C., Costa-jussa, M.~R., Cristina, E.-B., Angela, F., Christian, F., et~al.
\newblock Findings of the 2021 conference on machine translation (wmt21).
\newblock In \emph{Proceedings of the Sixth Conference on Machine Translation},
  pp.\  1--88. Association for Computational Linguistics, 2021.

\bibitem[Gong \& Liu(2021)Gong and Liu]{gong2021bi}
Gong, C. and Liu, X.
\newblock Bi-objective trade-off with dynamic barrier gradient descent.
\newblock \emph{NeurIPS 2021}, 2021.

\bibitem[Grangier et~al.(2023)Grangier, Ablin, and
  Hannun]{grangier2023adaptive}
Grangier, D., Ablin, P., and Hannun, A.
\newblock Adaptive training distributions with scalable online bilevel
  optimization.
\newblock \emph{arXiv preprint arXiv:2311.11973}, 2023.

\bibitem[Gunasekar et~al.(2023)Gunasekar, Zhang, Aneja, Mendes, Del~Giorno,
  Gopi, Javaheripi, Kauffmann, de~Rosa, Saarikivi,
  et~al.]{gunasekar2023textbooks}
Gunasekar, S., Zhang, Y., Aneja, J., Mendes, C. C.~T., Del~Giorno, A., Gopi,
  S., Javaheripi, M., Kauffmann, P., de~Rosa, G., Saarikivi, O., et~al.
\newblock Textbooks are all you need.
\newblock \emph{arXiv preprint arXiv:2306.11644}, 2023.

\bibitem[He et~al.(2022)He, Feng, Cheng, Ji, Guo, and
  Caverlee]{he2022metabalance}
He, Y., Feng, X., Cheng, C., Ji, G., Guo, Y., and Caverlee, J.
\newblock Metabalance: improving multi-task recommendations via adapting
  gradient magnitudes of auxiliary tasks.
\newblock In \emph{Proceedings of the ACM Web Conference 2022}, pp.\
  2205--2215, 2022.

\bibitem[Heek et~al.()Heek, Levskaya, Oliver, Ritter, Rondepierre, Steiner, and
  van Zee]{heek1flax}
Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre, B., Steiner, A.,
  and van Zee, M.
\newblock Flax: A neural network library and ecosystem for jax, 2020.
\newblock \emph{URL http://github. com/google/flax}, 1.

\bibitem[Hotegni et~al.(2023)Hotegni, Berkemeier, and Peitz]{hotegni2023multi}
Hotegni, S.~S., Berkemeier, M., and Peitz, S.
\newblock Multi-objective optimization for sparse deep multi-task learning.
\newblock \emph{arXiv preprint arXiv:2308.12243}, 2023.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{karimi2016linear}
Karimi, H., Nutini, J., and Schmidt, M.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases:
  European Conference, ECML PKDD 2016, Riva del Garda, Italy, September 19-23,
  2016, Proceedings, Part I 16}, pp.\  795--811. Springer, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
LeCun, Y., Cortes, C., and Burges, C.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available:
  http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem[Lewis et~al.(2004)Lewis, Yang, Russell-Rose, and Li]{lewis2004rcv1}
Lewis, D.~D., Yang, Y., Russell-Rose, T., and Li, F.
\newblock Rcv1: A new benchmark collection for text categorization research.
\newblock \emph{Journal of machine learning research}, 5\penalty0
  (Apr):\penalty0 361--397, 2004.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Liu et~al.(2022)Liu, Zhu, and Belkin]{liu2022loss}
Liu, C., Zhu, L., and Belkin, M.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock \emph{Applied and Computational Harmonic Analysis}, 59:\penalty0
  85--116, 2022.

\bibitem[Luo \& Tseng(1993)Luo and Tseng]{luo1993error}
Luo, Z.-Q. and Tseng, P.
\newblock Error bounds and convergence analysis of feasible descent methods: a
  general approach.
\newblock \emph{Annals of Operations Research}, 46\penalty0 (1):\penalty0
  157--178, 1993.

\bibitem[Oquab et~al.(2024)Oquab, Darcet, Moutakanni, Vo, Szafraniec, Khalidov,
  Fernandez, HAZIZA, Massa, El-Nouby, Assran, Ballas, Galuba, Howes, Huang, Li,
  Misra, Rabbat, Sharma, Synnaeve, Xu, Jegou, Mairal, Labatut, Joulin, and
  Bojanowski]{oquab2024dinov}
Oquab, M., Darcet, T., Moutakanni, T., Vo, H.~V., Szafraniec, M., Khalidov, V.,
  Fernandez, P., HAZIZA, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N.,
  Galuba, W., Howes, R., Huang, P.-Y., Li, S.-W., Misra, I., Rabbat, M.,
  Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin,
  A., and Bojanowski, P.
\newblock {DINO}v2: Learning robust visual features without supervision.
\newblock \emph{Transactions on Machine Learning Research}, 2024.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=a68SUt6zFt}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 5485--5551, 2020.

\bibitem[Sabach \& Shtern(2017)Sabach and Shtern]{sabach2017first}
Sabach, S. and Shtern, S.
\newblock A first order method for solving convex bilevel optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (2):\penalty0
  640--660, 2017.

\bibitem[Sener \& Koltun(2018)Sener and Koltun]{sener2018multi}
Sener, O. and Koltun, V.
\newblock Multi-task learning as multi-objective optimization.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Sun et~al.(2023)Sun, Cui, Zhang, Zhang, Yu, Luo, Wang, Rao, Liu,
  Huang, et~al.]{sun2023generative}
Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Luo, Z., Wang, Y., Rao, Y.,
  Liu, J., Huang, T., et~al.
\newblock Generative multimodal models are in-context learners.
\newblock \emph{arXiv preprint arXiv:2312.13286}, 2023.

\bibitem[Terj{\'e}k(2019)]{terjek2019adversarial}
Terj{\'e}k, D.
\newblock Adversarial lipschitz regularization.
\newblock \emph{arXiv preprint arXiv:1907.05681}, 2019.

\bibitem[Tsuzuku et~al.(2018)Tsuzuku, Sato, and Sugiyama]{tsuzuku2018lipschitz}
Tsuzuku, Y., Sato, I., and Sugiyama, M.
\newblock Lipschitz-margin training: Scalable certification of perturbation
  invariance for deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Wang \& Tsvetkov(2021)Wang and Tsvetkov]{wang2021gradient}
Wang, Z. and Tsvetkov, Y.
\newblock Gradient vaccine: Investigating and improving multi-task optimization
  in massively multilingual models.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR)}, 2021.

\bibitem[Xu et~al.(2023)Xu, Xie, Tan, Huang, Howes, Sharma, Li, Ghosh,
  Zettlemoyer, and Feichtenhofer]{xu2023demystifying}
Xu, H., Xie, S., Tan, X.~E., Huang, P.-Y., Howes, R., Sharma, V., Li, S.-W.,
  Ghosh, G., Zettlemoyer, L., and Feichtenhofer, C.
\newblock Demystifying clip data.
\newblock \emph{arXiv preprint arXiv:2309.16671}, 2023.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and
  Finn]{yu2020gradient}
Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C.
\newblock Gradient surgery for multi-task learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5824--5836, 2020.

\end{thebibliography}
