\begin{thebibliography}{138}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{CLIP}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Li, Li, He, Huang, Zhao, Zhang, Xu, Liu, Wang, et~al.]{internvideo}
Yi~Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi~Liu, Zun Wang, et~al.
\newblock Internvideo: General video foundation models via generative and discriminative learning.
\newblock \emph{arXiv preprint arXiv:2212.03191}, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Li, Li, Yu, He, Chen, Pei, Zheng, Xu, Wang, et~al.]{internvideo2}
Yi~Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et~al.
\newblock Internvideo2: Scaling video foundation models for multimodal video understanding.
\newblock \emph{arXiv preprint arXiv:2403.15377}, 2024{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, He, Wang, Li, Wang, Luo, Wang, Wang, and Qiao]{videochat}
KunChang Li, Yinan He, Yi~Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu~Qiao.
\newblock Videochat: Chat-centric video understanding.
\newblock \emph{arXiv preprint arXiv:2305.06355}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2024)Li, Wang, He, Li, Wang, Liu, Wang, Xu, Chen, Luo, et~al.]{mvbench}
Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi~Wang, Yi~Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et~al.
\newblock Mvbench: A comprehensive multi-modal video understanding benchmark.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 22195--22206, 2024.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and Duerig]{ALIGN}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with noisy text supervision.
\newblock In \emph{International conference on machine learning}, pages 4904--4916. PMLR, 2021.

\bibitem[Li et~al.(2022)Li, Li, Xiong, and Hoi]{BLIP}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
\newblock In \emph{International Conference on Machine Learning}, pages 12888--12900. PMLR, 2022.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{Flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 23716--23736, 2022.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Yang, Loy, and Liu]{CoOp}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Learning to prompt for vision-language models.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 2022{\natexlab{a}}.

\bibitem[Zhu et~al.(2024{\natexlab{a}})Zhu, Zhang, Xu, Shen, Chen, Wu, and Wang]{zhu2024efficient}
Yuhan Zhu, Guozhen Zhang, Chen Xu, Haocheng Shen, Xiaoxin Chen, Gangshan Wu, and Limin Wang.
\newblock Efficient test-time prompt tuning for vision-language models.
\newblock \emph{arXiv preprint arXiv:2408.05775}, 2024{\natexlab{a}}.

\bibitem[Lu et~al.(2022)Lu, Liu, Zhang, Liu, and Tian]{ProDA}
Yuning Lu, Jianzhuang Liu, Yonggang Zhang, Yajing Liu, and Xinmei Tian.
\newblock Prompt distribution learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 5206--5215, 2022.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Li, Lin, Lv, Schwing, and Ji]{DeFo}
Feng Wang, Manling Li, Xudong Lin, Hairong Lv, Alex Schwing, and Heng Ji.
\newblock Learning to decompose visual features with latent textual prompts.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022{\natexlab{b}}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, Yang, Loy, and Liu]{CoCoOp}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Conditional prompt learning for vision-language models.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022{\natexlab{b}}.

\bibitem[Yao et~al.(2023{\natexlab{a}})Yao, Zhang, and Xu]{KgCoOp}
Hantao Yao, Rui Zhang, and Changsheng Xu.
\newblock Visual-language prompt tuning with knowledge-guided context optimization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 6757--6767, 2023{\natexlab{a}}.

\bibitem[Khattak et~al.(2023{\natexlab{a}})Khattak, Rasheed, Maaz, Khan, and Khan]{MaPLe}
Muhammad~Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad~Shahbaz Khan.
\newblock Maple: Multi-modal prompt learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 19113--19122, 2023{\natexlab{a}}.

\bibitem[Bulat and Tzimiropoulos(2023)]{LASP}
Adrian Bulat and Georgios Tzimiropoulos.
\newblock Lasp: Text-to-text optimization for language-aware soft prompting of vision \& language models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 23232--23241, 2023.

\bibitem[Lee et~al.(2023)Lee, Song, Suh, Choi, Lee, and Kim]{RPO}
Dongjun Lee, Seokwon Song, Jihee Suh, Joonmyeong Choi, Sanghyeok Lee, and Hyunwoo~J Kim.
\newblock Read-only prompt optimization for vision-language few-shot learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 1401--1411, 2023.

\bibitem[Kan et~al.(2023)Kan, Wang, Lu, Zhen, Guan, and Zheng]{KAPT}
Baoshuo Kan, Teng Wang, Wenpeng Lu, Xiantong Zhen, Weili Guan, and Feng Zheng.
\newblock Knowledge-aware prompt tuning for generalizable vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 15670--15680, 2023.

\bibitem[Yao et~al.(2023{\natexlab{b}})Yao, Zhang, and Xu]{TCP}
Hantao Yao, Rui Zhang, and Changsheng Xu.
\newblock Tcp: Textual-based class-aware prompt tuning for visual-language model.
\newblock \emph{arXiv preprint arXiv:2311.18231}, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Wu, Gao, Shen, and Song]{DePT}
Ji~Zhang, Shihan Wu, Lianli Gao, Hengtao Shen, and Jingkuan Song.
\newblock Dept: Decoupled prompt tuning.
\newblock \emph{arXiv preprint arXiv:2309.07439}, 2023{\natexlab{a}}.

\bibitem[Khattak et~al.(2023{\natexlab{b}})Khattak, Wasim, Naseer, Khan, Yang, and Khan]{PromptSRC}
Muhammad~Uzair Khattak, Syed~Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad~Shahbaz Khan.
\newblock Self-regulating prompts: Foundational model adaptation without forgetting.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 15190--15200, 2023{\natexlab{b}}.

\bibitem[Ren et~al.(2024)Ren, Zhang, Zhu, Zhang, Zheng, Li, Smola, and Sun]{POMP}
Shuhuai Ren, Aston Zhang, Yi~Zhu, Shuai Zhang, Shuai Zheng, Mu~Li, Alexander~J Smola, and Xu~Sun.
\newblock Prompt pre-training with twenty-thousand classes for open-vocabulary visual recognition.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Guo et~al.(2023)Guo, Dong, Ji, Bai, Guo, and Zuo]{TaI-DPT}
Zixian Guo, Bowen Dong, Zhilong Ji, Jinfeng Bai, Yiwen Guo, and Wangmeng Zuo.
\newblock Texts as images in prompt tuning for multi-label image recognition.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 2808--2817, 2023.

\bibitem[Khattak et~al.(2024)Khattak, Naeem, Naseer, Van~Gool, and Tombari]{ProText}
Muhammad~Uzair Khattak, Muhammad~Ferjad Naeem, Muzammal Naseer, Luc Van~Gool, and Federico Tombari.
\newblock Learning to prompt with text only supervision for vision-language models.
\newblock \emph{arXiv preprint arXiv:2401.02418}, 2024.

\bibitem[Shi and Yang(2023)]{LoGoPrompt}
Cheng Shi and Sibei Yang.
\newblock Logoprompt: Synthetic text images can be good visual prompts for vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2932--2941, 2023.

\bibitem[Wang et~al.(2023)Wang, Liang, He, Xu, Wang, and Tan]{SHIP}
Zhengbo Wang, Jian Liang, Ran He, Nan Xu, Zilei Wang, and Tieniu Tan.
\newblock Improving zero-shot generalization for clip with synthesized prompts.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 3032--3042, 2023.

\bibitem[Zhu et~al.(2023{\natexlab{a}})Zhu, Niu, Han, Wu, and Zhang]{ProGrad}
Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang.
\newblock Prompt-aligned gradient for prompt tuning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 15659--15669, 2023{\natexlab{a}}.

\bibitem[Huang et~al.(2022)Huang, Chu, and Wei]{UPL}
Tony Huang, Jack Chu, and Fangyun Wei.
\newblock Unsupervised prompt learning for vision-language models.
\newblock \emph{arXiv preprint arXiv:2204.03649}, 2022.

\bibitem[Zang et~al.(2022)Zang, Li, Zhou, Huang, and Loy]{UPT}
Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen~Change Loy.
\newblock Unified vision and language prompt learning.
\newblock \emph{arXiv preprint arXiv:2210.07225}, 2022.

\bibitem[Long et~al.(2023)Long, Zhao, Yuan, Tan, Liu, Zhou, Wang, and Wang]{TFT}
Sifan Long, Zhen Zhao, Junkun Yuan, Zichang Tan, Jiangjiang Liu, Luping Zhou, Shengsheng Wang, and Jingdong Wang.
\newblock Task-oriented multi-modal mutual leaning for vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 21959--21969, 2023.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Gao, Wei, Tang, Zhang, Li, Ji, Tian, Chua, and Zhuang]{GRAM}
Juncheng Li, Minghe Gao, Longhui Wei, Siliang Tang, Wenqiao Zhang, Mengze Li, Wei Ji, Qi~Tian, Tat-Seng Chua, and Yueting Zhuang.
\newblock Gradient-regulated meta-prompt learning for generalizable vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2551--2562, 2023{\natexlab{b}}.

\bibitem[Xu et~al.(2024{\natexlab{a}})Xu, Zhu, Shen, Chen, Liao, Chen, and Wang]{ProVP}
Chen Xu, Yuhan Zhu, Haocheng Shen, Boheng Chen, Yixuan Liao, Xiaoxin Chen, and Limin Wang.
\newblock Progressive visual prompt learning with contrastive feature re-formation.
\newblock \emph{International Journal of Computer Vision}, pages 1--16, 2024{\natexlab{a}}.

\bibitem[Xu et~al.(2023)Xu, Zhu, Zhang, Shen, Liao, Chen, Wu, and Wang]{DPL}
Chen Xu, Yuhan Zhu, Guozhen Zhang, Haocheng Shen, Yixuan Liao, Xiaoxin Chen, Gangshan Wu, and Limin Wang.
\newblock Dpl: Decoupled prompt learning for vision-language models.
\newblock \emph{arXiv preprint arXiv:2308.10061}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{GPT-3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{InstructGPT}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[OpenAI(2023)]{GPT-4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{ArXiv}, abs/2303.08774, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:257532815}.

\bibitem[Wang et~al.(2020)Wang, Shelhamer, Liu, Olshausen, and Darrell]{Tent}
Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell.
\newblock Tent: Fully test-time adaptation by entropy minimization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Monge(1781)]{OT}
Gaspard Monge.
\newblock M{\'e}moire sur la th{\'e}orie des d{\'e}blais et des remblais.
\newblock \emph{Mem. Math. Phys. Acad. Royale Sci.}, pages 666--704, 1781.

\bibitem[Peyr{\'e} et~al.(2019)Peyr{\'e}, Cuturi, et~al.]{ComputationalOT}
Gabriel Peyr{\'e}, Marco Cuturi, et~al.
\newblock Computational optimal transport: With applications to data science.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 11\penalty0 (5-6):\penalty0 355--607, 2019.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{Chain-of-thought}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Feng et~al.(2022)Feng, Zhong, Jie, Chu, Ren, Wei, Xie, and Ma]{feng2022promptdet}
Chengjian Feng, Yujie Zhong, Zequn Jie, Xiangxiang Chu, Haibing Ren, Xiaolin Wei, Weidi Xie, and Lin Ma.
\newblock Promptdet: Towards open-vocabulary detection using uncurated images.
\newblock In \emph{European Conference on Computer Vision}, pages 701--717. Springer, 2022.

\bibitem[Gu et~al.(2021)Gu, Lin, Kuo, and Cui]{gu2021zero}
Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
\newblock Zero-shot detection via vision and language knowledge distillation.
\newblock \emph{arXiv preprint arXiv:2104.13921}, 2\penalty0 (3):\penalty0 4, 2021.

\bibitem[Du et~al.(2022)Du, Wei, Zhang, Shi, Gao, and Li]{du2022learning}
Yu~Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li.
\newblock Learning to prompt for open-vocabulary object detection with vision-language model.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 14084--14093, 2022.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and Chen]{DALLE2}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 1\penalty0 (2):\penalty0 3, 2022.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022high}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem[Wang et~al.(2021)Wang, Xing, and Liu]{ActionCLIP}
Mengmeng Wang, Jiazheng Xing, and Yong Liu.
\newblock Actionclip: A new paradigm for video action recognition.
\newblock \emph{arXiv preprint arXiv:2109.08472}, 2021.

\bibitem[Weng et~al.(2023)Weng, Yang, Li, Wu, and Jiang]{Open-VCLIP}
Zejia Weng, Xitong Yang, Ang Li, Zuxuan Wu, and Yu-Gang Jiang.
\newblock Open-vclip: Transforming clip to an open-vocabulary video model via interpolated weight optimization.
\newblock In \emph{ICML}, 2023.

\bibitem[Huang et~al.(2024)Huang, Zhou, Yao, and Han]{FROSTER}
Xiaohu Huang, Hao Zhou, Kun Yao, and Kai Han.
\newblock Froster: Frozen clip is a strong teacher for open-vocabulary action recognition.
\newblock \emph{arXiv preprint arXiv:2402.03241}, 2024.

\bibitem[Shu et~al.(2022)Shu, Nie, Huang, Yu, Goldstein, Anandkumar, and Xiao]{TPT}
Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao.
\newblock Test-time prompt tuning for zero-shot generalization in vision-language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 14274--14289, 2022.

\bibitem[Feng et~al.(2023)Feng, Yu, Liu, Khan, and Zuo]{DiffTPT}
Chun-Mei Feng, Kai Yu, Yong Liu, Salman Khan, and Wangmeng Zuo.
\newblock Diverse data augmentation with diffusions for effective test-time prompt tuning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2704--2714, 2023.

\bibitem[Ma et~al.(2024)Ma, Zhang, Guo, and Xu]{SwapPrompt}
Xiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu.
\newblock Swapprompt: Test-time prompt adaptation for vision-language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Abdul~Samadh et~al.(2024)Abdul~Samadh, Gani, Hussein, Khattak, Naseer, Shahbaz~Khan, and Khan]{PromptAlign}
Jameel Abdul~Samadh, Mohammad~Hanan Gani, Noor Hussein, Muhammad~Uzair Khattak, Muhammad~Muzammal Naseer, Fahad Shahbaz~Khan, and Salman~H Khan.
\newblock Align your prompts: Test-time prompting with distribution alignment for zero-shot generalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Zhu, Tang, Ma, Zhou, and Zhang]{dmn}
Yabin Zhang, Wenjie Zhu, Hui Tang, Zhiyuan Ma, Kaiyang Zhou, and Lei Zhang.
\newblock Dual memory networks: A versatile adaptation approach for vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 28718--28728, 2024{\natexlab{a}}.

\bibitem[Karmanov et~al.(2024)Karmanov, Guan, Lu, El~Saddik, and Xing]{karmanov2024efficient}
Adilbek Karmanov, Dayan Guan, Shijian Lu, Abdulmotaleb El~Saddik, and Eric Xing.
\newblock Efficient test-time adaptation of vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 14162--14171, 2024.

\bibitem[Gao et~al.(2023)Gao, Geng, Zhang, Ma, Fang, Zhang, Li, and Qiao]{CLIP-Adapter}
Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu~Qiao.
\newblock Clip-adapter: Better vision-language models with feature adapters.
\newblock \emph{International Journal of Computer Vision}, pages 1--15, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Fang, Gao, Li, Dai, Qiao, and Li]{Tip-adapter}
Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu~Qiao, and Hongsheng Li.
\newblock Tip-adapter: Training-free adaption of clip for few-shot classification.
\newblock In \emph{European Conference on Computer Vision}, pages 493--510. Springer, 2022.

\bibitem[Zhu et~al.(2023{\natexlab{b}})Zhu, Zhang, He, Zhou, Wang, Zhao, and Gao]{APE}
Xiangyang Zhu, Renrui Zhang, Bowei He, Aojun Zhou, Dong Wang, Bin Zhao, and Peng Gao.
\newblock Not all features matter: Enhancing few-shot clip with adaptive prior refinement.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2605--2615, 2023{\natexlab{b}}.

\bibitem[Xu et~al.(2024{\natexlab{b}})Xu, Chen, Nie, Wang, Zhuang, and Okumura]{xu2024advancing}
Yicheng Xu, Yuxin Chen, Jiahao Nie, Yusong Wang, Huiping Zhuang, and Manabu Okumura.
\newblock Advancing cross-domain discriminability in continual learning of vison-language models.
\newblock \emph{arXiv preprint arXiv:2406.18868}, 2024{\natexlab{b}}.

\bibitem[Yang et~al.(2024)Yang, Zhang, Wang, and Xie]{mma}
Lingxiao Yang, Ru-Yuan Zhang, Yanchen Wang, and Xiaohua Xie.
\newblock Mma: Multi-modal adapter for vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 23826--23837, 2024.

\bibitem[Ge et~al.(2023)Ge, Ren, Gallagher, Wang, Yang, Adam, Itti, Lakshminarayanan, and Zhao]{Hierarchy-CLIP}
Yunhao Ge, Jie Ren, Andrew Gallagher, Yuxiao Wang, Ming-Hsuan Yang, Hartwig Adam, Laurent Itti, Balaji Lakshminarayanan, and Jiaping Zhao.
\newblock Improving zero-shot generalization and robustness of multi-modal models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11093--11101, 2023.

\bibitem[Menon and Vondrick(2022)]{VisDesc}
Sachit Menon and Carl Vondrick.
\newblock Visual classification via description from large language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Roth et~al.(2023)Roth, Kim, Koepke, Vinyals, Schmid, and Akata]{WaffleCLIP}
Karsten Roth, Jae~Myung Kim, A~Koepke, Oriol Vinyals, Cordelia Schmid, and Zeynep Akata.
\newblock Waffling around for performance: Visual classification with random words and broad concepts.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 15746--15757, 2023.

\bibitem[Pratt et~al.(2023)Pratt, Covert, Liu, and Farhadi]{CuPL}
Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi.
\newblock What does a platypus look like? generating customized prompts for zero-shot image classification.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 15691--15701, 2023.

\bibitem[Novack et~al.(2023)Novack, McAuley, Lipton, and Garg]{novack2023chils}
Zachary Novack, Julian McAuley, Zachary~Chase Lipton, and Saurabh Garg.
\newblock Chils: Zero-shot image classification with hierarchical label sets.
\newblock In \emph{International Conference on Machine Learning}, pages 26342--26362. PMLR, 2023.

\bibitem[Udandarao et~al.(2023)Udandarao, Gupta, and Albanie]{SuS-X}
Vishaal Udandarao, Ankush Gupta, and Samuel Albanie.
\newblock Sus-x: Training-free name-only transfer of vision-language models.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2725--2736, 2023.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Hu, Li, Huang, Deng, Qiao, Gao, and Li]{CaFo}
Renrui Zhang, Xiangfei Hu, Bohao Li, Siyuan Huang, Hanqiu Deng, Yu~Qiao, Peng Gao, and Hongsheng Li.
\newblock Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 15211--15222, 2023{\natexlab{b}}.

\bibitem[Khamis et~al.(2024)Khamis, Tsuchida, Tarek, Rolland, and Petersson]{OT_survey}
Abdelwahed Khamis, Russell Tsuchida, Mohamed Tarek, Vivien Rolland, and Lars Petersson.
\newblock Scalable optimal transport methods in machine learning: A contemporary survey.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2024.

\bibitem[Rubner et~al.(2000)Rubner, Tomasi, and Guibas]{EMD}
Yossi Rubner, Carlo Tomasi, and Leonidas~J Guibas.
\newblock The earth mover's distance as a metric for image retrieval.
\newblock \emph{International journal of computer vision}, 40:\penalty0 99--121, 2000.

\bibitem[Cuturi(2013)]{Sinkhorn}
Marco Cuturi.
\newblock Sinkhorn distances: Lightspeed computation of optimal transport.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Ge et~al.(2021)Ge, Liu, Li, Yoshie, and Sun]{ge2021ota}
Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian Sun.
\newblock Ota: Optimal transport assignment for object detection.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 303--312, 2021.

\bibitem[De~Plaen et~al.(2023)De~Plaen, De~Plaen, Suykens, Proesmans, Tuytelaars, and Van~Gool]{de2023unbalanced}
Henri De~Plaen, Pierre-Fran{\c{c}}ois De~Plaen, Johan~AK Suykens, Marc Proesmans, Tinne Tuytelaars, and Luc Van~Gool.
\newblock Unbalanced optimal transport: A unified framework for object detection.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 3198--3207, 2023.

\bibitem[Courty et~al.(2016)Courty, Flamary, Tuia, and Rakotomamonjy]{courty2016optimal}
Nicolas Courty, R{\'e}mi Flamary, Devis Tuia, and Alain Rakotomamonjy.
\newblock Optimal transport for domain adaptation.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 39\penalty0 (9):\penalty0 1853--1865, 2016.

\bibitem[Yan et~al.(2018)Yan, Li, Wu, Min, Tan, and Wu]{yan2018semi}
Yuguang Yan, Wen Li, Hanrui Wu, Huaqing Min, Mingkui Tan, and Qingyao Wu.
\newblock Semi-supervised optimal transport for heterogeneous domain adaptation.
\newblock In \emph{Proceedings of the 27th International Joint Conference on Artificial Intelligence}, pages 2969--2975, 2018.

\bibitem[Damodaran et~al.(2018)Damodaran, Kellenberger, Flamary, Tuia, and Courty]{damodaran2018deepjdot}
Bharath~Bhushan Damodaran, Benjamin Kellenberger, R{\'e}mi Flamary, Devis Tuia, and Nicolas Courty.
\newblock Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the European conference on computer vision (ECCV)}, pages 447--463, 2018.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and Bottou]{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In \emph{International conference on machine learning}, pages 214--223. PMLR, 2017.

\bibitem[Salimans et~al.(2018)Salimans, Zhang, Radford, and Metaxas]{salimans2018improving}
Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas.
\newblock Improving gans using optimal transport.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and Courville]{gulrajani2017improved}
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron~C Courville.
\newblock Improved training of wasserstein gans.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Liutkus et~al.(2019)Liutkus, Simsekli, Majewski, Durmus, and St{\"o}ter]{liutkus2019sliced}
Antoine Liutkus, Umut Simsekli, Szymon Majewski, Alain Durmus, and Fabian-Robert St{\"o}ter.
\newblock Sliced-wasserstein flows: Nonparametric generative modeling via optimal transport and diffusions.
\newblock In \emph{International Conference on Machine Learning}, pages 4104--4113. PMLR, 2019.

\bibitem[Liu et~al.(2020)Liu, Zhu, Yamada, and Yang]{liu2020semantic}
Yanbin Liu, Linchao Zhu, Makoto Yamada, and Yi~Yang.
\newblock Semantic correspondence as an optimal transport problem.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 4463--4472, 2020.

\bibitem[Puy et~al.(2020)Puy, Boulch, and Marlet]{puy2020flot}
Gilles Puy, Alexandre Boulch, and Renaud Marlet.
\newblock Flot: Scene flow on point clouds guided by optimal transport.
\newblock In \emph{European conference on computer vision}, pages 527--544. Springer, 2020.

\bibitem[Shen et~al.(2021)Shen, Feydy, Liu, Curiale, San Jose~Estepar, San Jose~Estepar, and Niethammer]{shen2021accurate}
Zhengyang Shen, Jean Feydy, Peirong Liu, Ariel~H Curiale, Ruben San Jose~Estepar, Raul San Jose~Estepar, and Marc Niethammer.
\newblock Accurate point cloud registration with robust optimal transport.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 5373--5389, 2021.

\bibitem[Li et~al.(2021)Li, Lin, and Xie]{li2021self}
Ruibo Li, Guosheng Lin, and Lihua Xie.
\newblock Self-point-flow: Self-supervised scene flow estimation from point clouds with optimal transport and random walk.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 15577--15586, 2021.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Yao, Song, Li, Rao, and Zhang]{PLOT}
Guangyi Chen, Weiran Yao, Xiangchen Song, Xinyue Li, Yongming Rao, and Kun Zhang.
\newblock Plot: Prompt learning with optimal transport for vision-language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022{\natexlab{a}}.

\bibitem[Kim et~al.(2023)Kim, Oh, and Ye]{kim2023zegot}
Kwanyoung Kim, Yujin Oh, and Jong~Chul Ye.
\newblock Zegot: Zero-shot segmentation through optimal transport of text prompts.
\newblock \emph{arXiv preprint arXiv:2301.12171}, 2023.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Li, Liu, Xu, Chen, and Zhang]{wang2024tuning}
Dongsheng Wang, Miaoge Li, Xinyang Liu, MingSheng Xu, Bo~Chen, and Hanwang Zhang.
\newblock Tuning multi-mode token-level prompt alignment across modalities.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{b}}.

\bibitem[Chen et~al.(2023)Chen, Yu, Yang, Li, Sun, and Chen]{chen2023ost}
Tongjia Chen, Hongshan Yu, Zhengeng Yang, Zechuan Li, Wei Sun, and Chen Chen.
\newblock Ost: Refining text knowledge with optimal spatio-temporal descriptor for general video recognition.
\newblock \emph{arXiv preprint arXiv:2312.00096}, 2023.

\bibitem[Zhang et~al.(2021)Zhang, Zhao, Liu, Liu, and Liu]{zhang2021deep}
Kaihua Zhang, Zicheng Zhao, Dong Liu, Qingshan Liu, and Bo~Liu.
\newblock Deep transport network for unsupervised video object segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 8781--8790, 2021.

\bibitem[Kantorovich(2006)]{Kantorovich_Relaxation}
L.~Kantorovich.
\newblock On the translocation of masses.
\newblock \emph{Journal of Mathematical Sciences}, 133, 03 2006.
\newblock \doi{10.1007/s10958-006-0049-2}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{Imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern recognition}, pages 248--255. Ieee, 2009.

\bibitem[Nilsback and Zisserman(2008)]{OxfordFlowers}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{2008 Sixth Indian conference on computer vision, graphics \& image processing}, pages 722--729. IEEE, 2008.

\bibitem[Cimpoi et~al.(2014)Cimpoi, Maji, Kokkinos, Mohamed, and Vedaldi]{DTD}
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.
\newblock Describing textures in the wild.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 3606--3613, 2014.

\bibitem[Parkhi et~al.(2012)Parkhi, Vedaldi, Zisserman, and Jawahar]{OxfordPets}
Omkar~M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV~Jawahar.
\newblock Cats and dogs.
\newblock In \emph{2012 IEEE conference on computer vision and pattern recognition}, pages 3498--3505. IEEE, 2012.

\bibitem[Krause et~al.(2013)Krause, Stark, Deng, and Fei-Fei]{StanfordCars}
Jonathan Krause, Michael Stark, Jia Deng, and Li~Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision workshops}, pages 554--561, 2013.

\bibitem[Soomro et~al.(2012)Soomro, Zamir, and Shah]{UCF101}
Khurram Soomro, Amir~Roshan Zamir, and Mubarak Shah.
\newblock Ucf101: A dataset of 101 human actions classes from videos in the wild.
\newblock \emph{arXiv preprint arXiv:1212.0402}, 2012.

\bibitem[Fei-Fei(2004)]{Caltech101}
Li~Fei-Fei.
\newblock Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories.
\newblock In \emph{2004 conference on computer vision and pattern recognition workshop}, pages 178--178. IEEE, 2004.

\bibitem[Bossard et~al.(2014)Bossard, Guillaumin, and Van~Gool]{Food101}
Lukas Bossard, Matthieu Guillaumin, and Luc Van~Gool.
\newblock Food-101--mining discriminative components with random forests.
\newblock In \emph{Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13}, pages 446--461. Springer, 2014.

\bibitem[Xiao et~al.(2010)Xiao, Hays, Ehinger, Oliva, and Torralba]{SUN397}
Jianxiong Xiao, James Hays, Krista~A Ehinger, Aude Oliva, and Antonio Torralba.
\newblock Sun database: Large-scale scene recognition from abbey to zoo.
\newblock In \emph{2010 IEEE computer society conference on computer vision and pattern recognition}, pages 3485--3492. IEEE, 2010.

\bibitem[Maji et~al.(2013)Maji, Rahtu, Kannala, Blaschko, and Vedaldi]{FGVCAircraft}
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.
\newblock Fine-grained visual classification of aircraft.
\newblock \emph{arXiv preprint arXiv:1306.5151}, 2013.

\bibitem[Helber et~al.(2019)Helber, Bischke, Dengel, and Borth]{EuroSAT}
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth.
\newblock Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification.
\newblock \emph{IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 12\penalty0 (7):\penalty0 2217--2226, 2019.

\bibitem[Berg et~al.(2014)Berg, Liu, Lee, Alexander, Jacobs, and Belhumeur]{Birdsnap}
Thomas Berg, Jiongxin Liu, Seung~Woo Lee, Michelle~L. Alexander, David~W. Jacobs, and Peter~N. Belhumeur.
\newblock Birdsnap: Large-scale fine-grained visual categorization of birds.
\newblock In \emph{Proc. Conf. Computer Vision and Pattern Recognition (CVPR)}, June 2014.

\bibitem[Griffin et~al.(2007)Griffin, Holub, and Perona]{Caltech256}
Gregory Griffin, Alex Holub, and Pietro Perona.
\newblock Caltech-256 object category dataset.
\newblock 2007.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and Belongie]{CUB}
Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock 2011.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Zhao, Basart, Steinhardt, and Song]{ImageNet-A}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 15262--15271, 2021{\natexlab{a}}.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and Shankar]{ImageNetV2}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International conference on machine learning}, pages 5389--5400. PMLR, 2019.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo, Desai, Zhu, Parajuli, Guo, et~al.]{ImageNet-R}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of out-of-distribution generalization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 8340--8349, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2019)Wang, Ge, Lipton, and Xing]{ImageNet-Sketch}
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric~P Xing.
\newblock Learning robust global representations by penalizing local predictive power.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Kuehne et~al.(2011)Kuehne, Jhuang, Garrote, Poggio, and Serre]{hmdb51}
Hildegard Kuehne, Hueihan Jhuang, Est{\'\i}baliz Garrote, Tomaso Poggio, and Thomas Serre.
\newblock Hmdb: a large video database for human motion recognition.
\newblock In \emph{2011 International conference on computer vision}, pages 2556--2563. IEEE, 2011.

\bibitem[Carreira et~al.(2018)Carreira, Noland, Banki-Horvath, Hillier, and Zisserman]{k600}
Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman.
\newblock A short note about kinetics-600.
\newblock \emph{arXiv preprint arXiv:1808.01340}, 2018.

\bibitem[Ni et~al.(2022)Ni, Peng, Chen, Zhang, Meng, Fu, Xiang, and Ling]{X-CLIP}
Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling.
\newblock Expanding language-image pretrained models for general video recognition.
\newblock In \emph{European Conference on Computer Vision}, pages 1--18. Springer, 2022.

\bibitem[Ju et~al.(2022)Ju, Han, Zheng, Zhang, and Xie]{VPT(video)}
Chen Ju, Tengda Han, Kunhao Zheng, Ya~Zhang, and Weidi Xie.
\newblock Prompting visual-language models for efficient video understanding.
\newblock In \emph{European Conference on Computer Vision}, pages 105--124. Springer, 2022.

\bibitem[Wu et~al.(2023)Wu, Sun, and Ouyang]{Text4Vis}
Wenhao Wu, Zhun Sun, and Wanli Ouyang.
\newblock Revisiting classifier: Transferring vision-language models for video recognition.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~37, pages 2847--2855, 2023.

\bibitem[Yang et~al.(2022)Yang, Zhu, Xie, Zhang, Chen, and Li]{AIM}
Taojiannan Yang, Yi~Zhu, Yusheng Xie, Aston Zhang, Chen Chen, and Mu~Li.
\newblock Aim: Adapting image models for efficient video action recognition.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2022.

\bibitem[Pan et~al.(2022)Pan, Lin, Zhu, Shao, and Li]{St-adapter}
Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li.
\newblock St-adapter: Parameter-efficient image-to-video transfer learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 26462--26477, 2022.

\bibitem[Wasim et~al.(2023)Wasim, Naseer, Khan, Khan, and Shah]{Vita-CLIP}
Syed~Talal Wasim, Muzammal Naseer, Salman Khan, Fahad~Shahbaz Khan, and Mubarak Shah.
\newblock Vita-clip: Video and text adaptive clip via multimodal prompting.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 23034--23044, 2023.

\bibitem[Rasheed et~al.(2023)Rasheed, Khattak, Maaz, Khan, and Khan]{ViFi-CLIP}
Hanoona Rasheed, Muhammad~Uzair Khattak, Muhammad Maaz, Salman Khan, and Fahad~Shahbaz Khan.
\newblock Fine-tuned clip models are efficient video learners.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 6545--6554, 2023.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Ge, Tong, Wang, Song, Wang, and Luo]{Adaptformer}
Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo.
\newblock Adaptformer: Adapting vision transformers for scalable visual recognition.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 16664--16678, 2022{\natexlab{b}}.

\bibitem[Brattoli et~al.(2020)Brattoli, Tighe, Zhdanov, Perona, and Chalupka]{brattoli2020rethinking}
Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka.
\newblock Rethinking zero-shot video classification: End-to-end training for realistic applications.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 4613--4623, 2020.

\bibitem[Chen and Huang(2021)]{chen2021elaborative}
Shizhe Chen and Dong Huang.
\newblock Elaborative rehearsal for zero-shot action recognition.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 13638--13647, 2021.

\bibitem[Kay et~al.(2017)Kay, Carreira, Simonyan, Zhang, Hillier, Vijayanarasimhan, Viola, Green, Back, Natsev, et~al.]{k400}
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et~al.
\newblock The kinetics human action video dataset.
\newblock \emph{arXiv preprint arXiv:1705.06950}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{ResNet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{ViT}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zhai et~al.(2023)Zhai, Mustafa, Kolesnikov, and Beyer]{SigLIP}
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.
\newblock Sigmoid loss for language image pre-training.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 11975--11986, 2023.

\bibitem[Sun et~al.(2023)Sun, Fang, Wu, Wang, and Cao]{EVA-CLIP}
Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.
\newblock Eva-clip: Improved training techniques for clip at scale.
\newblock \emph{arXiv preprint arXiv:2303.15389}, 2023.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{Cifar}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Ren et~al.(2023)Ren, Li, Ren, Zhao, and Sun]{clip_openness}
Shuhuai Ren, Lei Li, Xuancheng Ren, Guangxiang Zhao, and Xu~Sun.
\newblock Delving into the openness of clip.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pages 9587--9606, 2023.

\bibitem[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, et~al.]{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu~Han, Fei Huang, et~al.
\newblock Qwen technical report.
\newblock \emph{arXiv preprint arXiv:2309.16609}, 2023.

\bibitem[Loshchilov and Hutter(2016)]{cos_decay}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and Hinton]{sgd}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, pages 1139--1147. PMLR, 2013.

\bibitem[Hendrycks and Gimpel(2016)]{GeLU}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Liu, Cao, Zhao, Zhao, Ma, and Wang]{zhang2024dynamic}
Guozhen Zhang, Jingyu Liu, Shengming Cao, Xiaotong Zhao, Kevin Zhao, Kai Ma, and Limin Wang.
\newblock Dynamic and compressive adaptation of transformers from images to videos.
\newblock \emph{arXiv preprint arXiv:2408.06840}, 2024{\natexlab{b}}.

\bibitem[Li and Wang(2023)]{li2023zeroi2v}
Xinhao Li and Limin Wang.
\newblock Zeroi2v: Zero-cost adaptation of pre-trained transformers from image to video.
\newblock \emph{arXiv preprint arXiv:2310.01324}, 2023.

\bibitem[Zhang et~al.(2023{\natexlab{c}})Zhang, Zhu, Wang, Chen, Wu, and Wang]{EMA}
Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen, Gangshan Wu, and Limin Wang.
\newblock Extracting motion and appearance via inter-frame attention for efficient video frame interpolation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 5682--5692, 2023{\natexlab{c}}.

\bibitem[Zhang et~al.(2024{\natexlab{c}})Zhang, Liu, Cui, Zhao, Ma, and Wang]{vfimamba}
Guozhen Zhang, Chunxu Liu, Yutao Cui, Xiaotong Zhao, Kai Ma, and Limin Wang.
\newblock Vfimamba: Video frame interpolation with state space models.
\newblock \emph{arXiv preprint arXiv:2407.02315}, 2024{\natexlab{c}}.

\bibitem[Liu et~al.(2024)Liu, Zhang, Zhao, and Wang]{liu2024sparse}
Chunxu Liu, Guozhen Zhang, Rui Zhao, and Limin Wang.
\newblock Sparse global matching for video frame interpolation with large motion.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 19125--19134, 2024.

\bibitem[Zhu et~al.(2024{\natexlab{b}})Zhu, Zhang, Tan, Wu, and Wang]{DualDETR}
Yuhan Zhu, Guozhen Zhang, Jing Tan, Gangshan Wu, and Limin Wang.
\newblock Dual detrs for multi-label temporal action detection.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 18559--18569, 2024{\natexlab{b}}.

\bibitem[Xu et~al.(2024{\natexlab{c}})Xu, Song, Feng, Li, Ge, Zheng, and Wang]{xu2024accelerating}
Chen Xu, Tianhui Song, Weixin Feng, Xubin Li, Tiezheng Ge, Bo~Zheng, and Limin Wang.
\newblock Accelerating image generation with sub-path linear approximation model.
\newblock \emph{arXiv preprint arXiv:2404.13903}, 2024{\natexlab{c}}.

\bibitem[Zhang et~al.(2023{\natexlab{d}})Zhang, Rao, and Agrawala]{zhang2023adding}
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.
\newblock Adding conditional control to text-to-image diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 3836--3847, 2023{\natexlab{d}}.

\end{thebibliography}
