\begin{thebibliography}{76}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{arora2018stronger}
Arora, S., Ge, R., Neyshabur, B., and Zhang, Y.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock \emph{arXiv preprint arXiv:1802.05296}, 2018.

\bibitem[Baek et~al.(2022)Baek, Jiang, Raghunathan, and
  Kolter]{baek2022agreement}
Baek, C., Jiang, Y., Raghunathan, A., and Kolter, Z.
\newblock Agreement-on-the-line: Predicting the performance of neural networks
  under distribution shift.
\newblock \emph{arXiv preprint arXiv:2206.13089}, 2022.

\bibitem[Baldock et~al.(2021)Baldock, Maennel, and Neyshabur]{baldock2021deep}
Baldock, R., Maennel, H., and Neyshabur, B.
\newblock Deep learning through the lens of example difficulty.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 10876--10889, 2021.

\bibitem[Barbu et~al.(2019)Barbu, Mayo, Alverio, Luo, Wang, Gutfreund,
  Tenenbaum, and Katz]{barbu2019objectnet}
Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum,
  J., and Katz, B.
\newblock Objectnet: A large-scale bias-controlled dataset for pushing the
  limits of object recognition models.
\newblock In \emph{Advances in neural information processing systems}, 2019.

\bibitem[Benesty et~al.(2009)Benesty, Chen, Huang, and
  Cohen]{benesty2009pearson}
Benesty, J., Chen, J., Huang, Y., and Cohen, I.
\newblock Pearson correlation coefficient.
\newblock In \emph{Noise reduction in speech processing}, pp.\  1--4. Springer,
  2009.

\bibitem[Bridle et~al.(1991)Bridle, Heading, and
  MacKay]{bridle1991unsupervised}
Bridle, J., Heading, A., and MacKay, D.
\newblock Unsupervised classifiers, mutual information and'phantom targets.
\newblock \emph{Advances in neural information processing systems}, 4, 1991.

\bibitem[Cao et~al.(2019)Cao, Wei, Gaidon, Arechiga, and Ma]{cao2019learning}
Cao, K., Wei, C., Gaidon, A., Arechiga, N., and Ma, T.
\newblock Learning imbalanced datasets with label-distribution-aware margin
  loss.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Liu, Avci, Wu, Liang, and
  Jha]{chen2021detecting}
Chen, J., Liu, F., Avci, B., Wu, X., Liang, Y., and Jha, S.
\newblock Detecting errors and estimating accuracy on unlabeled data with
  self-training ensembles.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Goel, Sohoni, Poms, Fatahalian,
  and R{\'e}]{chen2021mandoline}
Chen, M., Goel, K., Sohoni, N.~S., Poms, F., Fatahalian, K., and R{\'e}, C.
\newblock Mandoline: Model evaluation under distribution shift.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1617--1629, 2021{\natexlab{b}}.

\bibitem[Chrabaszcz et~al.(2017)Chrabaszcz, Loshchilov, and
  Hutter]{chrabaszcz2017downsampled}
Chrabaszcz, P., Loshchilov, I., and Hutter, F.
\newblock A downsampled variant of imagenet as an alternative to the cifar
  datasets.
\newblock \emph{arXiv preprint arXiv:1707.08819}, 2017.

\bibitem[Chuang et~al.(2020)Chuang, Torralba, and
  Jegelka]{chuang2020estimating}
Chuang, C.-Y., Torralba, A., and Jegelka, S.
\newblock Estimating generalization under distribution shifts via
  domain-invariant representations.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Chuang et~al.(2021)Chuang, Mroueh, Greenewald, Torralba, and
  Jegelka]{chuang2021measuring}
Chuang, C.-Y., Mroueh, Y., Greenewald, K., Torralba, A., and Jegelka, S.
\newblock Measuring generalization with optimal transport.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  8294--8306, 2021.

\bibitem[Corneanu et~al.(2020)Corneanu, Escalera, and
  Martinez]{corneanu2020computing}
Corneanu, C.~A., Escalera, S., and Martinez, A.~M.
\newblock Computing the testing error without a testing set.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  2677--2685, 2020.

\bibitem[Cui et~al.(2020)Cui, Wang, Zhuo, Li, Huang, and Tian]{cui2020towards}
Cui, S., Wang, S., Zhuo, J., Li, L., Huang, Q., and Tian, Q.
\newblock Towards discriminability and diversity: Batch nuclear-norm
  maximization under label insufficient situations.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  3941--3950, 2020.

\bibitem[Cui et~al.(2021)Cui, Wang, Zhuo, Li, Huang, and Tian]{cui2021fast}
Cui, S., Wang, S., Zhuo, J., Li, L., Huang, Q., and Tian, Q.
\newblock Fast batch nuclear-norm maximization and minimization for robust
  domain adaptation.
\newblock \emph{arXiv preprint arXiv:2107.06154}, 2021.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  248--255, 2009.

\bibitem[Deng \& Zheng(2021)Deng and Zheng]{deng2021labels}
Deng, W. and Zheng, L.
\newblock Are labels always necessary for classifier accuracy evaluation?
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  15069--15078, 2021.

\bibitem[Deng et~al.(2021)Deng, Gould, and Zheng]{Deng:ICML2021}
Deng, W., Gould, S., and Zheng, L.
\newblock What does rotation prediction tell us about classifier accuracy under
  varying testing environments?
\newblock In \emph{International conference on machine learning}, 2021.

\bibitem[Deng et~al.(2022)Deng, Gould, and Zheng]{deng2022strong}
Deng, W., Gould, S., and Zheng, L.
\newblock On the strong correlation between model invariance and
  generalization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Ding et~al.(2021)Ding, Zhang, Ma, Han, Ding, and Sun]{ding2021repvgg}
Ding, X., Zhang, X., Ma, N., Han, J., Ding, G., and Sun, J.
\newblock Repvgg: Making vgg-style convnets great again.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  13733--13742, 2021.

\bibitem[Donmez et~al.(2010)Donmez, Lebanon, and
  Balasubramanian]{donmez2010unsupervised}
Donmez, P., Lebanon, G., and Balasubramanian, K.
\newblock Unsupervised supervised learning i: Estimating classification and
  regression errors without labels.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (4), 2010.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Du et~al.(2020)Du, Chang, Bhunia, Xie, Ma, Song, and Guo]{du2020fine}
Du, R., Chang, D., Bhunia, A.~K., Xie, J., Ma, Z., Song, Y.-Z., and Guo, J.
\newblock Fine-grained visual classification via progressive multi-granularity
  training of jigsaw patches.
\newblock In \emph{European Conference on Computer Vision}, pp.\  153--168,
  2020.

\bibitem[Eilertsen et~al.(2020)Eilertsen, J{\"o}nsson, Ropinski, Unger, and
  Ynnerman]{eilertsen2020classifying}
Eilertsen, G., J{\"o}nsson, D., Ropinski, T., Unger, J., and Ynnerman, A.
\newblock Classifying the classifier: dissecting the weight space of neural
  networks.
\newblock \emph{arXiv preprint arXiv:2002.05688}, 2020.

\bibitem[Fazel(2002)]{fazel2002matrix}
Fazel, M.
\newblock \emph{Matrix rank minimization with applications}.
\newblock PhD thesis, PhD thesis, Stanford University, 2002.

\bibitem[Garg et~al.(2020)Garg, Wu, Balakrishnan, and Lipton]{garg2020unified}
Garg, S., Wu, Y., Balakrishnan, S., and Lipton, Z.
\newblock A unified view of label shift estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3290--3300, 2020.

\bibitem[Garg et~al.(2022)Garg, Balakrishnan, Lipton, Neyshabur, and
  Sedghi]{garg2022leveraging}
Garg, S., Balakrishnan, S., Lipton, Z.~C., Neyshabur, B., and Sedghi, H.
\newblock Leveraging unlabeled data to predict out-of-distribution performance.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Gong et~al.(2021)Gong, Lin, Yao, Dietterich, Divakaran, and
  Gervasio]{gong2021confidence}
Gong, Y., Lin, X., Yao, Y., Dietterich, T.~G., Divakaran, A., and Gervasio, M.
\newblock Confidence calibration for domain generalization under covariate
  shift.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  8958--8967, 2021.

\bibitem[Guillory et~al.(2021)Guillory, Shankar, Ebrahimi, Darrell, and
  Schmidt]{guillory2021predicting}
Guillory, D., Shankar, V., Ebrahimi, S., Darrell, T., and Schmidt, L.
\newblock Predicting with confidence on unseen distributions.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  1134--1144, 2021.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock In \emph{Proc. ICML}, pp.\  1321--1330, 2017.

\bibitem[Gupta et~al.(2021)Gupta, Rahimi, Ajanthan, Mensink, Sminchisescu, and
  Hartley]{gupta2020calibration}
Gupta, K., Rahimi, A., Ajanthan, T., Mensink, T., Sminchisescu, C., and
  Hartley, R.
\newblock Calibration of neural networks using splines.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hendrycks \& Dietterich(2019)Hendrycks and
  Dietterich]{hendrycks2019robustness}
Hendrycks, D. and Dietterich, T.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Hendrycks \& Gimpel(2017)Hendrycks and Gimpel]{hendrycks2017baseline}
Hendrycks, D. and Gimpel, K.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo,
  Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2021many}
Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai,
  R., Zhu, T., Parajuli, S., Guo, M., et~al.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  8340--8349, 2021.

\bibitem[Jiang et~al.(2019{\natexlab{a}})Jiang, Krishnan, Mobahi, and
  Bengio]{jiang2018predicting}
Jiang, Y., Krishnan, D., Mobahi, H., and Bengio, S.
\newblock Predicting the generalization gap in deep networks with margin
  distributions.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Jiang et~al.(2019{\natexlab{b}})Jiang, Neyshabur, Mobahi, Krishnan,
  and Bengio]{jiang2019fantastic}
Jiang, Y., Neyshabur, B., Mobahi, H., Krishnan, D., and Bengio, S.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Karandikar et~al.(2021)Karandikar, Cain, Tran, Lakshminarayanan,
  Shlens, Mozer, and Roelofs]{karandikar2021soft}
Karandikar, A., Cain, N., Tran, D., Lakshminarayanan, B., Shlens, J., Mozer,
  M.~C., and Roelofs, B.
\newblock Soft calibration objectives for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  29768--29779, 2021.

\bibitem[Kendall(1948)]{kendall1948rank}
Kendall, M.~G.
\newblock Rank correlation methods.
\newblock 1948.

\bibitem[Kolesnikov et~al.(2020)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{kolesnikov2020big}
Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and
  Houlsby, N.
\newblock Big transfer (bit): General visual representation learning.
\newblock In \emph{European conference on computer vision}, pp.\  491--507,
  2020.

\bibitem[Krause et~al.(2010)Krause, Perona, and
  Gomes]{krause2010discriminative}
Krause, A., Perona, P., and Gomes, R.
\newblock Discriminative clustering by regularized information maximization.
\newblock \emph{Advances in neural information processing systems}, 23, 2010.

\bibitem[Krishnan \& Tickoo(2020)Krishnan and Tickoo]{krishnan2020improving}
Krishnan, R. and Tickoo, O.
\newblock Improving model calibration with accuracy versus uncertainty
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  18237--18248, 2020.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Liang et~al.(2020)Liang, Hu, and Feng]{liang2020we}
Liang, J., Hu, D., and Feng, J.
\newblock Do we really need to access the source data? source hypothesis
  transfer for unsupervised domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6028--6039. PMLR, 2020.

\bibitem[Lipton et~al.(2018)Lipton, Wang, and Smola]{lipton2018detecting}
Lipton, Z., Wang, Y.-X., and Smola, A.
\newblock Detecting and correcting for label shift with black box predictors.
\newblock In \emph{International conference on machine learning}, pp.\
  3122--3130, 2018.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu2021swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  10012--10022, 2021.

\bibitem[Liu et~al.(2022)Liu, Mao, Wu, Feichtenhofer, Darrell, and
  Xie]{liu2022convnet}
Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S.
\newblock A convnet for the 2020s.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2022.

\bibitem[Madani et~al.(2004)Madani, Pennock, and Flake]{madani2004co}
Madani, O., Pennock, D., and Flake, G.
\newblock Co-validation: Using model disagreement on unlabeled data to validate
  classification algorithms.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  873--880, 2004.

\bibitem[Miller et~al.(2021)Miller, Taori, Raghunathan, Sagawa, Koh, Shankar,
  Liang, Carmon, and Schmidt]{miller2021accuracy}
Miller, J.~P., Taori, R., Raghunathan, A., Sagawa, S., Koh, P.~W., Shankar, V.,
  Liang, P., Carmon, Y., and Schmidt, L.
\newblock Accuracy on the line: on the strong correlation between
  out-of-distribution and in-distribution generalization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7721--7735, 2021.

\bibitem[Minderer et~al.(2021)Minderer, Djolonga, Romijnders, Hubis, Zhai,
  Houlsby, Tran, and Lucic]{minderer2021revisiting}
Minderer, M., Djolonga, J., Romijnders, R., Hubis, F., Zhai, X., Houlsby, N.,
  Tran, D., and Lucic, M.
\newblock Revisiting the calibration of modern neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  15682--15694, 2021.

\bibitem[Mintun et~al.(2021)Mintun, Kirillov, and Xie]{mintun2021interaction}
Mintun, E., Kirillov, A., and Xie, S.
\newblock On interaction between augmentations and corruptions in natural
  corruption robustness.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Nagelkerke et~al.(1991)]{nagelkerke1991note}
Nagelkerke, N.~J. et~al.
\newblock A note on a general definition of the coefficient of determination.
\newblock \emph{Biometrika}, 78\penalty0 (3):\penalty0 691--692, 1991.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur2017exploring}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N.
\newblock Exploring generalization in deep learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5947--5956, 2017.

\bibitem[Ovadia et~al.(2019)Ovadia, Fertig, Ren, Nado, Sculley, Nowozin,
  Dillon, Lakshminarayanan, and Snoek]{ovadia2019can}
Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon,
  J., Lakshminarayanan, B., and Snoek, J.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Platanios et~al.(2017)Platanios, Poon, Mitchell, and
  Horvitz]{platanios2017estimating}
Platanios, E., Poon, H., Mitchell, T.~M., and Horvitz, E.~J.
\newblock Estimating accuracy from unlabeled data: A probabilistic logic
  approach.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4361--4370, 2017.

\bibitem[Platanios et~al.(2016)Platanios, Dubey, and
  Mitchell]{platanios2016estimating}
Platanios, E.~A., Dubey, A., and Mitchell, T.
\newblock Estimating accuracy from unlabeled data: A bayesian approach.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1416--1425, 2016.

\bibitem[Recht et~al.(2010)Recht, Fazel, and Parrilo]{recht2010guaranteed}
Recht, B., Fazel, M., and Parrilo, P.~A.
\newblock Guaranteed minimum-rank solutions of linear matrix equations via
  nuclear norm minimization.
\newblock \emph{SIAM review}, 52\penalty0 (3):\penalty0 471--501, 2010.

\bibitem[Recht et~al.(2018)Recht, Roelofs, Schmidt, and
  Shankar]{recht2018cifar}
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V.
\newblock Do cifar-10 classifiers generalize to cifar-10?
\newblock \emph{arXiv preprint arXiv:1806.00451}, 2018.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and
  Shankar]{recht2019imagenet}
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5389--5400. PMLR, 2019.

\bibitem[Schiff et~al.(2021)Schiff, Quanz, Das, and Chen]{schiff2021predicting}
Schiff, Y., Quanz, B., Das, P., and Chen, P.-Y.
\newblock Predicting deep neural network generalization with perturbation
  response curves.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Shi \& Sha(2012)Shi and Sha]{shi2012information}
Shi, Y. and Sha, F.
\newblock Information-theoretical learning of discriminative clusters for
  unsupervised domain adaptation.
\newblock \emph{arXiv preprint arXiv:1206.6438}, 2012.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Sugiyama \& Kawanabe(2012)Sugiyama and Kawanabe]{sugiyama2012machine}
Sugiyama, M. and Kawanabe, M.
\newblock \emph{Machine learning in non-stationary environments: Introduction
  to covariate shift adaptation}.
\newblock MIT press, 2012.

\bibitem[Sun et~al.(2022)Sun, Lu, and Ling]{sun2022prior}
Sun, T., Lu, C., and Ling, H.
\newblock Prior knowledge guided unsupervised domain adaptation.
\newblock In \emph{European Conference on Computer Vision}, 2022.

\bibitem[Tang et~al.(2020)Tang, Chen, and Jia]{tang2020unsupervised}
Tang, H., Chen, K., and Jia, K.
\newblock Unsupervised domain adaptation via structurally regularized deep
  clustering.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  8725--8735, 2020.

\bibitem[Taori et~al.(2020)Taori, Dave, Shankar, Carlini, Recht, and
  Schmidt]{taori2020measuring}
Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L.
\newblock Measuring robustness to natural distribution shifts in image
  classification.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18583--18599, 2020.

\bibitem[Tian et~al.(2020)Tian, Liu, Glaser, Hsu, and Kira]{tian2020posterior}
Tian, J., Liu, Y.-C., Glaser, N., Hsu, Y.-C., and Kira, Z.
\newblock Posterior re-calibration for imbalanced datasets.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 8101--8113, 2020.

\bibitem[Unterthiner et~al.(2020)Unterthiner, Keysers, Gelly, Bousquet, and
  Tolstikhin]{unterthiner2020predicting}
Unterthiner, T., Keysers, D., Gelly, S., Bousquet, O., and Tolstikhin, I.
\newblock Predicting neural network accuracy from weights.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and
  Belongie]{wah2011caltech}
Wah, C., Branson, S., Welinder, P., Perona, P., and Belongie, S.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock 2011.

\bibitem[Wang et~al.(2019)Wang, Ge, Lipton, and Xing]{wang2019learning}
Wang, H., Ge, S., Lipton, Z., and Xing, E.~P.
\newblock Learning robust global representations by penalizing local predictive
  power.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10506--10518, 2019.

\bibitem[Wang et~al.(2020)Wang, Chen, Wang, Long, and
  Wang]{wang2020progressive}
Wang, S., Chen, X., Wang, Y., Long, M., and Wang, J.
\newblock Progressive adversarial networks for fine-grained domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pp.\  9213--9222, 2020.

\bibitem[Wei et~al.(2021)Wei, Song, Mac~Aodha, Wu, Peng, Tang, Yang, and
  Belongie]{wei2021fine}
Wei, X.-S., Song, Y.-Z., Mac~Aodha, O., Wu, J., Peng, Y., Tang, J., Yang, J.,
  and Belongie, S.
\newblock Fine-grained image analysis with deep learning: A survey.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2021.

\bibitem[Wightman(2019)]{rw2019timm}
Wightman, R.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Yang et~al.(2021)Yang, van~de Weijer, Herranz, Jui,
  et~al.]{yang2021exploiting}
Yang, S., van~de Weijer, J., Herranz, L., Jui, S., et~al.
\newblock Exploiting the intrinsic neighborhood structure for source-free
  domain adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  29393--29405, 2021.

\bibitem[Yang et~al.(2022)Yang, Wang, Wang, Jui, and van~de
  Weijer]{yang2022attracting}
Yang, S., Wang, Y., Wang, K., Jui, S., and van~de Weijer, J.
\newblock Attracting and dispersing: A simple approach for source-free domain
  adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Yu et~al.(2022)Yu, Yang, Wei, Ma, and Steinhardt]{yu2022predicting}
Yu, Y., Yang, Z., Wei, A., Ma, Y., and Steinhardt, J.
\newblock Predicting out-of-distribution error with the projection norm.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\end{thebibliography}
