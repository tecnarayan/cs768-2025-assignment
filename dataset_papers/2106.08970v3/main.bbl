\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar,
  and Zhang]{abadi2016deep}
Martin Abadi, Andy Chu, Ian Goodfellow, H~Brendan McMahan, Ilya Mironov, Kunal
  Talwar, and Li~Zhang.
\newblock Deep learning with differential privacy.
\newblock In \emph{Proceedings of the 2016 ACM SIGSAC conference on computer
  and communications security}, pages 308--318, 2016.

\bibitem[Bagdasaryan et~al.(2020)Bagdasaryan, Veit, Hua, Estrin, and
  Shmatikov]{bagdasaryan2020backdoor}
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
  Shmatikov.
\newblock How to backdoor federated learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2938--2948. PMLR, 2020.

\bibitem[Barni et~al.(2019)Barni, Kallas, and Tondi]{barni2019new}
Mauro Barni, Kassem Kallas, and Benedetta Tondi.
\newblock A new backdoor attack in cnns by training set corruption without
  label poisoning.
\newblock In \emph{2019 IEEE International Conference on Image Processing
  (ICIP)}, pages 101--105. IEEE, 2019.

\bibitem[Biggio et~al.(2012)Biggio, Nelson, and Laskov]{biggio2012poisoning}
Battista Biggio, Blaine Nelson, and Pavel Laskov.
\newblock Poisoning attacks against support vector machines.
\newblock In \emph{Proceedings of the 29th International Coference on
  International Conference on Machine Learning}, pages 1467--1474, 2012.

\bibitem[Borgnia et~al.(2021)Borgnia, Geiping, Cherepanova, Fowl, Gupta,
  Ghiasi, Huang, Goldblum, and Goldstein]{borgnia2021dp}
Eitan Borgnia, Jonas Geiping, Valeriia Cherepanova, Liam Fowl, Arjun Gupta,
  Amin Ghiasi, Furong Huang, Micah Goldblum, and Tom Goldstein.
\newblock Dp-instahide: Provably defusing poisoning and backdoor attacks with
  differentially private data augmentations.
\newblock \emph{arXiv preprint arXiv:2103.02079}, 2021.

\bibitem[Chen et~al.(2019)Chen, Carvalho, Baracaldo, Ludwig, Edwards, Lee,
  Molloy, and Srivastava]{chen2018detecting}
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin
  Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava.
\newblock Detecting backdoor attacks on deep neural networks by activation
  clustering.
\newblock In \emph{SafeAI@ AAAI}, 2019.

\bibitem[Chen et~al.(2017)Chen, Liu, Li, Lu, and Song]{chen2017targeted}
Xinyun Chen, Chang Liu, Bo~Li, Kimberly Lu, and Dawn Song.
\newblock Targeted backdoor attacks on deep learning systems using data
  poisoning.
\newblock \emph{arXiv preprint arXiv:1712.05526}, 2017.

\bibitem[Feng et~al.(2019)Feng, Cai, and Zhou]{feng2019learning}
Ji~Feng, Qi-Zhi Cai, and Zhi-Hua Zhou.
\newblock Learning to confuse: generating training time adversarial data with
  auto-encoder.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Fowl et~al.(2021{\natexlab{a}})Fowl, Chiang, Goldblum, Geiping,
  Bansal, Czaja, and Goldstein]{fowl2021preventing}
Liam Fowl, Ping-yeh Chiang, Micah Goldblum, Jonas Geiping, Arpit Bansal, Wojtek
  Czaja, and Tom Goldstein.
\newblock Preventing unauthorized use of proprietary data: Poisoning for secure
  dataset release.
\newblock \emph{arXiv preprint arXiv:2103.02683}, 2021{\natexlab{a}}.

\bibitem[Fowl et~al.(2021{\natexlab{b}})Fowl, Goldblum, Chiang, Geiping, Czaja,
  and Goldstein]{fowl2021adversarial}
Liam Fowl, Micah Goldblum, Ping-yeh Chiang, Jonas Geiping, Wojciech Czaja, and
  Tom Goldstein.
\newblock Adversarial examples make strong poisons.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 30339--30351, 2021{\natexlab{b}}.

\bibitem[Gao et~al.(2019)Gao, Xu, Wang, Chen, Ranasinghe, and
  Nepal]{gao2019strip}
Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith~C Ranasinghe, and
  Surya Nepal.
\newblock Strip: A defence against trojan attacks on deep neural networks.
\newblock In \emph{Proceedings of the 35th Annual Computer Security
  Applications Conference}, pages 113--125, 2019.

\bibitem[Geiping et~al.(2021)Geiping, Fowl, Huang, Czaja, Taylor, Moeller, and
  Goldstein]{geiping2020witches}
Jonas Geiping, Liam Fowl, W~Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael
  Moeller, and Tom Goldstein.
\newblock Witches' brew: Industrial scale data poisoning via gradient matching.
\newblock \emph{ICLR}, 2021.

\bibitem[Goldblum et~al.(2022)Goldblum, Tsipras, Xie, Chen, Schwarzschild,
  Song, Madry, Li, and Goldstein]{goldblum2020data}
Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild,
  Dawn Song, Aleksander Madry, Bo~Li, and Tom Goldstein.
\newblock Dataset security for machine learning: Data poisoning, backdoor
  attacks, and defenses.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2022.

\bibitem[Gu et~al.(2017)Gu, Dolan-Gavitt, and Garg]{gu2017badnets}
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Badnets: Identifying vulnerabilities in the machine learning model
  supply chain.
\newblock \emph{arXiv preprint arXiv:1708.06733}, 2017.

\bibitem[Guo and Liu(2020)]{guo2020practical}
Junfeng Guo and Cong Liu.
\newblock Practical poisoning attacks on neural networks.
\newblock In \emph{European Conference on Computer Vision}, pages 142--158.
  Springer, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hong et~al.(2020)Hong, Chandrasekaran, Kaya, Dumitra{\c{s}}, and
  Papernot]{hong2020effectiveness}
Sanghyun Hong, Varun Chandrasekaran, Yi{\u{g}}itcan Kaya, Tudor Dumitra{\c{s}},
  and Nicolas Papernot.
\newblock On the effectiveness of mitigating data poisoning attacks with
  gradient shaping.
\newblock \emph{arXiv preprint arXiv:2002.11497}, 2020.

\bibitem[Huang et~al.(2020{\natexlab{a}})Huang, Ma, Erfani, Bailey, and
  Wang]{huang2021unlearnable}
Hanxun Huang, Xingjun Ma, Sarah~Monazam Erfani, James Bailey, and Yisen Wang.
\newblock Unlearnable examples: Making personal data unexploitable.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Huang et~al.(2020{\natexlab{b}})Huang, Geiping, Fowl, Taylor, and
  Goldstein]{huang2020metapoison}
W~Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein.
\newblock Metapoison: Practical general-purpose clean-label data poisoning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 12080--12091, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2019)Li, Zhao, Yu, Xue, Kaafar, and Zhu]{li2019invisible}
Shaofeng Li, Benjamin Zi~Hao Zhao, Jiahao Yu, Minhui Xue, Dali Kaafar, and
  Haojin Zhu.
\newblock Invisible backdoor attacks against deep neural networks.
\newblock \emph{arXiv preprint arXiv:1909.02742}, 2019.

\bibitem[Li et~al.(2020)Li, Xue, Zhao, Zhu, and Zhang]{li2020invisible}
Shaofeng Li, Minhui Xue, Benjamin Zhao, Haojin Zhu, and Xinpeng Zhang.
\newblock Invisible backdoor attacks on deep neural networks via steganography
  and regularization.
\newblock \emph{IEEE Transactions on Dependable and Secure Computing}, 2020.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Lyu, Koren, Lyu, Li, and
  Ma]{li2021anti}
Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo~Li, and Xingjun Ma.
\newblock Anti-backdoor learning: Training clean models on poisoned data.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 14900--14912, 2021{\natexlab{a}}.

\bibitem[Li et~al.(2022)Li, Jiang, Li, and Xia]{li2020backdoor}
Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia.
\newblock Backdoor learning: A survey.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2022.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Li, Wu, Li, He, and
  Lyu]{li2021invisible}
Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu.
\newblock Invisible backdoor attack with sample-specific triggers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 16463--16472, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2017)Liu, Ma, Aafer, Lee, Zhai, Wang, and
  Zhang]{liu2017trojaning}
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,
  and Xiangyu Zhang.
\newblock Trojaning attack on neural networks.
\newblock 2017.

\bibitem[Liu et~al.(2020)Liu, Ma, Bailey, and Lu]{liu2020reflection}
Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu.
\newblock Reflection backdoor: A natural backdoor attack on deep neural
  networks.
\newblock In \emph{European Conference on Computer Vision}, pages 182--199.
  Springer, 2020.

\bibitem[{Muñoz-González} et~al.(2017){Muñoz-González}, Biggio, Demontis,
  Paudice, Wongrassamee, Lupu, and Roli]{munoz-gonzalez_towards_2017}
Luis {Muñoz-González}, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin
  Wongrassamee, Emil~C. Lupu, and Fabio Roli.
\newblock Towards {{Poisoning}} of {{Deep Learning Algorithms}} with
  {{Back}}-gradient {{Optimization}}.
\newblock In \emph{Proceedings of the 10th {{ACM Workshop}} on {{Artificial
  Intelligence}} and {{Security}}}, {{AISec}} '17, pages 27--38, {New York, NY,
  USA}, 2017. {ACM}.
\newblock ISBN 978-1-4503-5202-4.
\newblock \doi{10.1145/3128572.3140451}.

\bibitem[Nguyen and Tran(2020)]{nguyen2021wanet}
Tuan~Anh Nguyen and Anh~Tuan Tran.
\newblock Wanet-imperceptible warping-based backdoor attack.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Saha et~al.(2020)Saha, Subramanya, and Pirsiavash]{saha2019hidden}
Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash.
\newblock Hidden trigger backdoor attacks.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pages 11957--11965, 2020.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4510--4520, 2018.

\bibitem[Schwarzschild et~al.(2021)Schwarzschild, Goldblum, Gupta, Dickerson,
  and Goldstein]{schwarzschild2020just}
Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John~P Dickerson, and Tom
  Goldstein.
\newblock Just how toxic is data poisoning? a unified benchmark for backdoor
  and data poisoning attacks.
\newblock In \emph{International Conference on Machine Learning}, pages
  9389--9398. PMLR, 2021.

\bibitem[Shafahi et~al.(2018)Shafahi, Huang, Najibi, Suciu, Studer, Dumitras,
  and Goldstein]{shafahi2018poison}
Ali Shafahi, W~Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer,
  Tudor Dumitras, and Tom Goldstein.
\newblock Poison frogs! targeted clean-label poisoning attacks on neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Shen et~al.(2019)Shen, Zhu, and Ma]{shen2019tensorclog}
Juncheng Shen, Xiaolei Zhu, and De~Ma.
\newblock Tensorclog: An imperceptible poisoning attack on deep neural network
  applications.
\newblock \emph{IEEE Access}, 7:\penalty0 41498--41506, 2019.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Steinhardt et~al.(2017)Steinhardt, Koh, and
  Liang]{steinhardt_certified_2017}
Jacob Steinhardt, Pang Wei~W Koh, and Percy~S Liang.
\newblock Certified {{Defenses}} for {{Data Poisoning Attacks}}.
\newblock In \emph{Advances in {{Neural Information Processing Systems}} 30},
  pages 3517--3529. {Curran Associates, Inc.}, 2017.

\bibitem[Tran et~al.(2018)Tran, Li, and Madry]{tran2018spectral}
Brandon Tran, Jerry Li, and Aleksander Madry.
\newblock Spectral signatures in backdoor attacks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Turner et~al.(2019)Turner, Tsipras, and Madry]{turner2019label}
Alexander Turner, Dimitris Tsipras, and Aleksander Madry.
\newblock Label-consistent backdoor attacks.
\newblock \emph{arXiv preprint arXiv:1912.02771}, 2019.

\bibitem[Wang et~al.(2019)Wang, Yao, Shan, Li, Viswanath, Zheng, and
  Zhao]{wang2019neural}
Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
  Zheng, and Ben~Y Zhao.
\newblock Neural cleanse: Identifying and mitigating backdoor attacks in neural
  networks.
\newblock In \emph{2019 IEEE Symposium on Security and Privacy (SP)}, pages
  707--723. IEEE, 2019.

\bibitem[Wenger et~al.(2021)Wenger, Passananti, Bhagoji, Yao, Zheng, and
  Zhao]{wenger2021backdoor}
Emily Wenger, Josephine Passananti, Arjun~Nitin Bhagoji, Yuanshun Yao, Haitao
  Zheng, and Ben~Y Zhao.
\newblock Backdoor attacks against deep learning systems in the physical world.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 6206--6215, 2021.

\bibitem[Wu and Wang(2021)]{wu2021adversarial}
Dongxian Wu and Yisen Wang.
\newblock Adversarial neuron pruning purifies backdoored deep models.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 16913--16925, 2021.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Zhao et~al.(2021)Zhao, Mopuri, and Bilen]{zhao2020dataset}
Bo~Zhao, Konda~Reddy Mopuri, and Hakan Bilen.
\newblock Dataset condensation with gradient matching.
\newblock In \emph{Ninth International Conference on Learning Representations
  2021}, 2021.

\bibitem[Zhu et~al.(2019)Zhu, Huang, Li, Taylor, Studer, and
  Goldstein]{zhu2019transferable}
Chen Zhu, W~Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom
  Goldstein.
\newblock Transferable clean-label poisoning attacks on deep neural nets.
\newblock In \emph{International Conference on Machine Learning}, pages
  7614--7623. PMLR, 2019.

\end{thebibliography}
