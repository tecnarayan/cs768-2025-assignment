\begin{thebibliography}{10}

\bibitem{Goodfellow:Bengio2016}
I.~Goodfellow, Y.~Bengio, and A.~Courville.
\newblock {\em Deep Learning}.
\newblock MIT Press, Cambridge, MA, 2016.

\bibitem{bp}
P.~Werbos.
\newblock {\em Beyond regression: New tools for prediction and analysis in the
  behavioral sciences}.
\newblock PhD thesis, Harvard University, 1974.

\bibitem{rumelhart1986learning}
D.~E. Rumelhart, G.~E. Hinton, and R.~J. Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em Nature}, 323(6088):533, 1986.

\bibitem{targetprop}
Y.~Bengio.
\newblock How auto-encoders could provide credit assignment in deep networks
  via target propagation.
\newblock {\em arXiv:1407.7906}, 2014.

\bibitem{dtp}
D.-H. Lee, S.~Zhang, A.~Fischer, and Y.~Bengio.
\newblock Difference target propagation.
\newblock In {\em ECML PKDD}, pages 498--515, 2015.

\bibitem{Breiman2001}
L.~Breiman.
\newblock Random forests.
\newblock {\em Machine Learning}, 45(1):5--32, 2001.

\bibitem{gbdt}
J.~H. Friedman.
\newblock Greedy function approximation: A gradient boosting machine.
\newblock {\em Annals of Statistics}, 29:1189--1232, 2000.

\bibitem{gcForest17}
Z.-H. Zhou and J.~Feng.
\newblock Deep forest: Towards an alternative to deep neural networks.
\newblock In {\em IJCAI}, pages 3553--3559, 2017.

\bibitem{Hinton:1986:DR:104279.104287}
G.~E. Hinton, J.~L. McClelland, and D.~E. Rumelhart.
\newblock Distributed representations.
\newblock In {\em Parallel Distributed Processing: Explorations in the
  Microstructure of Cognition, Vol. 1}, pages 77--109. 1986.

\bibitem{bengio2013representation}
Y.~Bengio, A.~Courville, and P.~Vincent.
\newblock Representation learning: A review and new perspectives.
\newblock {\em IEEE Trans. Pattern Analysis and Machine Intelligence},
  35(8):1798--1828, 2013.

\bibitem{bengio2013better}
Y.~Bengio, G.~Mesnil, Y.~Dauphin, and S.~Rifai.
\newblock Better mixing via deep representations.
\newblock In {\em ICML}, pages 552--560, 2013.

\bibitem{bengio2013generalized}
Y.~Bengio, L.~Yao, G.~Alain, and P.~Vincent.
\newblock Generalized denoising auto-encoders as generative models.
\newblock In {\em NIPS}, pages 899--907, 2013.

\bibitem{TishbyZ15}
N.~Tishby and N.~Zaslavsky.
\newblock Deep learning and the information bottleneck principle.
\newblock {\em arXiv:1503.02406}, 2015.

\bibitem{feedback}
T.~P. Lillicrap, D.~Cownden, D.~B. Tweed, and C.~J. Akerman.
\newblock Random synaptic feedback weights support error backpropagation for
  deep learning.
\newblock {\em Nature communications}, 7:13276, 2016.

\bibitem{directFeedback}
A.~N{\o}kland.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock In {\em NIPS}, pages 1037--1045, 2016.

\bibitem{Zhou2012}
Z.-H. Zhou.
\newblock {\em Ensemble Methods: Foundations and Algorithms}.
\newblock CRC, Boca Raton, FL, 2012.

\bibitem{Breiman1996}
L.~Breiman.
\newblock Bagging predictors.
\newblock {\em Machine Learning}, 24(2):123--140, 1996.

\bibitem{Freund:Schapire:1999}
Y.~Freund and R.~E. Schapire.
\newblock A short introduction to boosting.
\newblock {\em Journal of Japanese Society for Artificial Intelligence},
  14(5):771--780, 1999.

\bibitem{chen2016xgboost}
T.-Q. Chen and C.~Guestrin.
\newblock {XGB}oost: A scalable tree boosting system.
\newblock In {\em KDD}, pages 785--794, 2016.

\bibitem{DBLP:conf/kdd/HePJXLXSAHBC14}
X.~He, J.~Pan, O.~Jin, T.~Xu, B.~Liu, T.~Xu, Y.~Shi, A.~Atallah, R.~Herbrich,
  S.~Bowers, and J.~Qui{\~{n}}onero.
\newblock Practical lessons from predicting clicks on ads at facebook.
\newblock In {\em ADKDD}, pages 5:1--5:9, 2014.

\bibitem{chen2015higgs}
T.-Q. Chen and T.~He.
\newblock Higgs boson discovery with boosted trees.
\newblock In {\em NIPS Workshop}, pages 69--80, 2015.

\bibitem{eForest}
J.~Feng and Z.-H. Zhou.
\newblock Autoencoder by forest.
\newblock In {\em AAAI}, 2018.

\bibitem{xgboostGPU}
M.~Rory and F.~Eibe.
\newblock Accelerating the xgboost algorithm using {GPU} computing.
\newblock {\em PeerJ Computer Science}, 3:127, 2017.

\bibitem{xgbDropout}
V.~R. Korlakai and G.~B. Ran.
\newblock {DART:} dropouts meet multiple additive regression trees.
\newblock In {\em AISTATS}, pages 489--497, 2015.

\bibitem{fastxgboost}
S.~Si, H.~Zhang, S.~S. Keerthi, D.~Mahajan, I.~S. Dhillon, and C.{-}J. Hsieh.
\newblock Gradient boosted decision trees for high dimensional sparse output.
\newblock In {\em ICML}, pages 3182--3190, 2017.

\bibitem{foolCNN}
A.~Nguyen, J.~Yosinski, and J.~Clune.
\newblock Deep neural networks are easily fooled: High confidence predictions
  for unrecognizable images.
\newblock pages 427--436, 2015.

\bibitem{adversarial17}
S.~H. Huang, N.~Papernot, I.~Goodfellow, Y.~Duan, and P.~Abbeel.
\newblock Adversarial attacks on neural network policies.
\newblock {\em arXiv:1702.02284}, 2017.

\bibitem{Lichman2013}
M.~Lichman.
\newblock {UCI} machine learning repository, 2013.

\bibitem{t-SNE}
L.J.P van~der Maaten and G.E. Hinton.
\newblock Visualizing high-dimensional data using t-sne.
\newblock {\em Journal of Machine Learning Research}, 9:2579â€“2605, 2008.

\bibitem{kingma2014adam}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv:1412.6980}, 2014.

\end{thebibliography}
