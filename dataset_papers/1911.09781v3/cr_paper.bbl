\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arazo et~al.(2019)Arazo, Ortego, Albert, O'Connor, and
  McGuinness]{arazo2019unsupervised}
Arazo, E., Ortego, D., Albert, P., O'Connor, N.~E., and McGuinness, K.
\newblock Unsupervised label noise modeling and loss correction.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{k{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Arpit, D., Jastrz{k{e}}bski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal,
  M.~S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Azadi et~al.(2016)Azadi, Feng, Jegelka, and
  Darrell]{azadi2016auxiliary}
Azadi, S., Feng, J., Jegelka, S., and Darrell, T.
\newblock Auxiliary image regularization for deep cnns with noisy labels.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Bootkrajang \& Kab{\'a}n(2012)Bootkrajang and
  Kab{\'a}n]{bootkrajang2012label}
Bootkrajang, J. and Kab{\'a}n, A.
\newblock Label-noise robust logistic regression and its applications.
\newblock In \emph{European Conference on Machine Learning and Principles and
  Practice of Knowledge Discovery in Databases (ECML PKDD)}, 2012.

\bibitem[Charoenphakdee et~al.(2019)Charoenphakdee, Lee, and
  Sugiyama]{charoenphakdee2019symmetric}
Charoenphakdee, N., Lee, J., and Sugiyama, M.
\newblock On symmetric losses for learning from corrupted labels.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Chen et~al.(2019)Chen, Liao, Chen, and Zhang]{chen2019understanding}
Chen, P., Liao, B., Chen, G., and Zhang, S.
\newblock Understanding and utilizing deep neural networks trained with noisy
  labels.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Chen \& Gupta(2015)Chen and Gupta]{chen2015webly}
Chen, X. and Gupta, A.
\newblock Webly supervised learning of convolutional networks.
\newblock In \emph{International Conference on Computer Vision (ICCV)}, 2015.

\bibitem[Cheng et~al.(2019)Cheng, Jiang, and Macherey]{cheng2019robust}
Cheng, Y., Jiang, L., and Macherey, W.
\newblock Robust neural machine translation with doubly adversarial inputs.
\newblock In \emph{Annual Conference of the Association for Computational
  Linguistics (ACL)}, 2019.

\bibitem[Cheng et~al.(2020)Cheng, Jiang, Macherey, and
  Eisenstein]{cheng2020advaug}
Cheng, Y., Jiang, L., Macherey, W., and Eisenstein, J.
\newblock Advaug: Robust adversarial augmentation for neural machine
  translation.
\newblock In \emph{Annual Conference of the Association for Computational
  Linguistics (ACL)}, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2009.

\bibitem[Fan et~al.(2017)Fan, He, Liang, and Hu]{fan2017self}
Fan, Y., He, R., Liang, J., and Hu, B.
\newblock Self-paced learning: an implicit regularization perspective.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2017.

\bibitem[Goldberger \& Ben-Reuven(2017)Goldberger and
  Ben-Reuven]{goldberger2017training}
Goldberger, J. and Ben-Reuven, E.
\newblock Training deep neural-networks using a noise adaptation layer.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Guo et~al.(2018)Guo, Huang, Zhang, Zhuang, Dong, Scott, and
  Huang]{guo2018curriculumnet}
Guo, S., Huang, W., Zhang, H., Zhuang, C., Dong, D., Scott, M.~R., and Huang,
  D.
\newblock Curriculumnet: Weakly supervised learning from large-scale web
  images.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2018.

\bibitem[Han et~al.(2018{\natexlab{a}})Han, Yao, Niu, Zhou, Tsang, Zhang, and
  Sugiyama]{han2018masking}
Han, B., Yao, J., Niu, G., Zhou, M., Tsang, I., Zhang, Y., and Sugiyama, M.
\newblock Masking: A new perspective of noisy supervision.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2018{\natexlab{a}}.

\bibitem[Han et~al.(2018{\natexlab{b}})Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han2018co}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2018{\natexlab{b}}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2016.

\bibitem[Hendrycks \& Dietterich(2019)Hendrycks and
  Dietterich]{hendrycks2019benchmarking}
Hendrycks, D. and Dietterich, T.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Hendrycks et~al.(2018)Hendrycks, Mazeika, Wilson, and
  Gimpel]{hendrycks2018using}
Hendrycks, D., Mazeika, M., Wilson, D., and Gimpel, K.
\newblock Using trusted data to train deep networks on labels corrupted by
  severe noise.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Huang et~al.(2019)Huang, Qu, Jia, and Zhao]{huang2019o2u}
Huang, J., Qu, L., Jia, R., and Zhao, B.
\newblock O2u-net: A simple noisy label detection approach for deep neural
  networks.
\newblock In \emph{International Conference on Computer Vision (ICCV)}, 2019.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2015.

\bibitem[Jiang et~al.(2014)Jiang, Meng, Yu, Lan, Shan, and
  Hauptmann]{jiang2014self}
Jiang, L., Meng, D., Yu, S.-I., Lan, Z., Shan, S., and Hauptmann, A.
\newblock Self-paced learning with diversity.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2014.

\bibitem[Jiang et~al.(2015)Jiang, Meng, Zhao, Shan, and
  Hauptmann]{jiang2015self}
Jiang, L., Meng, D., Zhao, Q., Shan, S., and Hauptmann, A.~G.
\newblock Self-paced curriculum learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2015.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and
  Fei-Fei]{jiang2018mentornet}
Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Kornblith et~al.(2019)Kornblith, Shlens, and Le]{kornblith2019better}
Kornblith, S., Shlens, J., and Le, Q.~V.
\newblock Do better imagenet models transfer better?
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2019.

\bibitem[Krause et~al.(2013)Krause, Deng, Stark, and
  Fei-Fei]{krause2013collecting}
Krause, J., Deng, J., Stark, M., and Fei-Fei, L.
\newblock Collecting a large-scale dataset of fine-grained cars.
\newblock In \emph{Second Workshop on Fine-Grained Visual Categorization},
  2013.

\bibitem[Krause et~al.(2016)Krause, Sapp, Howard, Zhou, Toshev, Duerig,
  Philbin, and Fei-Fei]{krause2016unreasonable}
Krause, J., Sapp, B., Howard, A., Zhou, H., Toshev, A., Duerig, T., Philbin,
  J., and Fei-Fei, L.
\newblock The unreasonable effectiveness of noisy data for fine-grained
  recognition.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2016.

\bibitem[Kumar et~al.(2010)Kumar, Packer, and Koller]{kumar2010self}
Kumar, M.~P., Packer, B., and Koller, D.
\newblock Self-paced learning for latent variable models.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2010.

\bibitem[Lee et~al.(2019)Lee, Yun, Lee, Lee, Li, and Shin]{lee2019robust}
Lee, K., Yun, S., Lee, K., Lee, H., Li, B., and Shin, J.
\newblock Robust inference via generative classifiers for handling noisy
  labels.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Lee et~al.(2018)Lee, He, Zhang, and Yang]{lee2017cleannet}
Lee, K.-H., He, X., Zhang, L., and Yang, L.
\newblock Cleannet: Transfer learning for scalable image classifier training
  with label noise.
\newblock \emph{Conference on Computer Vision and Pattern Recognition (CVPR)},
  2018.

\bibitem[Li et~al.(2019)Li, Wong, Zhao, and Kankanhalli]{li2019learning}
Li, J., Wong, Y., Zhao, Q., and Kankanhalli, M.~S.
\newblock Learning to learn from noisy labeled data.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2019.

\bibitem[Li et~al.(2020)Li, Socher, and Hoi]{li2020dividemix}
Li, J., Socher, R., and Hoi, S.~C.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Li et~al.(2017{\natexlab{a}})Li, Wang, Li, Agustsson, and
  Van~Gool]{li2017webvision}
Li, W., Wang, L., Li, W., Agustsson, E., and Van~Gool, L.
\newblock Webvision database: Visual learning and understanding from web data.
\newblock \emph{arXiv preprint arXiv:1708.02862}, 2017{\natexlab{a}}.

\bibitem[Li et~al.(2017{\natexlab{b}})Li, Yang, Song, Cao, Li, and
  Luo]{Li2017ICCV}
Li, Y., Yang, J., Song, Y., Cao, L., Li, J., and Luo, J.
\newblock Learning from noisy labels with distillation.
\newblock In \emph{International Conference on Computer Vision (ICCV)},
  2017{\natexlab{b}}.

\bibitem[Liang et~al.(2016)Liang, Jiang, Meng, and
  Hauptmann]{liang2016learning}
Liang, J., Jiang, L., Meng, D., and Hauptmann, A.~G.
\newblock Learning to detect concepts from webly-labeled video data.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2016.

\bibitem[Liang et~al.(2020)Liang, Jiang, and Hauptmann]{liang2020simaug}
Liang, J., Jiang, L., and Hauptmann, A.
\newblock Simaug: Learning robust representations from simulation for
  trajectory prediction.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2020.

\bibitem[Ma et~al.(2018)Ma, Wang, Houle, Zhou, Erfani, Xia, Wijewickrema, and
  Bailey]{ma2018dimensionality}
Ma, X., Wang, Y., Houle, M.~E., Zhou, S., Erfani, S.~M., Xia, S.-T.,
  Wijewickrema, S., and Bailey, J.
\newblock Dimensionality-driven learning with noisy labels.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Mahajan et~al.(2018)Mahajan, Girshick, Ramanathan, He, Paluri, Li,
  Bharambe, and van~der Maaten]{mahajan2018exploring}
Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y.,
  Bharambe, A., and van~der Maaten, L.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2018.

\bibitem[Mnih \& Hinton(2012)Mnih and Hinton]{mnih2012learning}
Mnih, V. and Hinton, G.~E.
\newblock Learning to label aerial images from noisy data.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2012.

\bibitem[Noh et~al.(2017)Noh, You, Mun, and Han]{noh2017regularizing}
Noh, H., You, T., Mun, J., and Han, B.
\newblock Regularizing deep neural networks by noise: its interpretation and
  optimization.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2017.

\bibitem[Northcutt et~al.(2019)Northcutt, Jiang, and
  Chuang]{northcutt2019confident}
Northcutt, C.~G., Jiang, L., and Chuang, I.~L.
\newblock Confident learning: Estimating uncertainty in dataset labels.
\newblock \emph{arXiv preprint arXiv:1911.00068}, 2019.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and
  Qu]{patrini2017making}
Patrini, G., Rozza, A., Krishna~Menon, A., Nock, R., and Qu, L.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2017.

\bibitem[Reed et~al.(2014)Reed, Lee, Anguelov, Szegedy, Erhan, and
  Rabinovich]{reed2014training}
Reed, S., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and Rabinovich, A.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock \emph{arXiv preprint arXiv:1412.6596}, 2014.

\bibitem[Reeve \& Kab{\'a}n(2019)Reeve and Kab{\'a}n]{reeve2019fast}
Reeve, H.~W. and Kab{\'a}n, A.
\newblock Fast rates for a knn classifier robust to unknown asymmetric label
  noise.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun]{ren2018learning}
Ren, M., Zeng, W., Yang, B., and Urtasun, R.
\newblock Learning to reweight examples for robust deep learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Rolnick et~al.(2017)Rolnick, Veit, Belongie, and
  Shavit]{rolnick2017deep}
Rolnick, D., Veit, A., Belongie, S., and Shavit, N.
\newblock Deep learning is robust to massive label noise.
\newblock \emph{arXiv preprint arXiv:1705.10694}, 2017.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{sandler2018mobilenetv2}
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2018.

\bibitem[Saxena et~al.(2019)Saxena, Tuzel, and DeCoste]{saxena2019data}
Saxena, S., Tuzel, O., and DeCoste, D.
\newblock Data parameters: A new family of parameters for learning a
  differentiable curriculum.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Seo et~al.(2019)Seo, Kim, and Han]{seo2019combinatorial}
Seo, P.~H., Kim, G., and Han, B.
\newblock Combinatorial inference against label noise.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Shu et~al.(2019)Shu, Xie, Yi, Zhao, Zhou, Xu, and Meng]{shu2019meta}
Shu, J., Xie, Q., Yi, L., Zhao, Q., Zhou, S., Xu, Z., and Meng, D.
\newblock Meta-weight-net: Learning an explicit mapping for sample weighting.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Song et~al.(2019)Song, Kim, and Lee]{song2019selfie}
Song, H., Kim, M., and Lee, J.-G.
\newblock Selfie: Refurbishing unclean samples for robust deep learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2016.

\bibitem[Szegedy et~al.(2017)Szegedy, Ioffe, Vanhoucke, and
  Alemi]{szegedy2017inception}
Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A.~A.
\newblock Inception-v4, inception-resnet and the impact of residual connections
  on learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2017.

\bibitem[Tan \& Le(2019)Tan and Le]{tan2019efficientnet}
Tan, M. and Le, Q.~V.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Vahdat(2017)]{vahdat2017toward}
Vahdat, A.
\newblock Toward robustness against label noise in training deep discriminative
  neural networks.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2017.

\bibitem[Van~Rooyen et~al.(2015)Van~Rooyen, Menon, and
  Williamson]{van2015learning}
Van~Rooyen, B., Menon, A., and Williamson, R.~C.
\newblock Learning with symmetric label noise: The importance of being
  unhinged.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2015.

\bibitem[Veit et~al.(2017)Veit, Alldrin, Chechik, Krasin, Gupta, and
  Belongie]{veit2017learning}
Veit, A., Alldrin, N., Chechik, G., Krasin, I., Gupta, A., and Belongie, S.
\newblock Learning from noisy large-scale datasets with minimal supervision.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2017.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra,
  et~al.]{vinyals2016matching}
Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et~al.
\newblock Matching networks for one shot learning.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2016.

\bibitem[Wang et~al.(2018)Wang, Liu, Ma, Bailey, Zha, Song, and
  Xia]{wang2018iterative}
Wang, Y., Liu, W., Ma, X., Bailey, J., Zha, H., Song, L., and Xia, S.-T.
\newblock Iterative learning with open-set noisy labels.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2018.

\bibitem[Xiao et~al.(2015)Xiao, Xia, Yang, Huang, and Wang]{xiao2015learning}
Xiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2015.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2018mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, Xing, Ghaoui, and
  Jordan]{zhang2019theoretically}
Zhang, H., Yu, Y., Jiao, J., Xing, E.~P., Ghaoui, L.~E., and Jordan, M.~I.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Zhang \& Sabuncu(2018)Zhang and Sabuncu]{zhang2018generalized}
Zhang, Z. and Sabuncu, M.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Zhang, Arik, Lee, and
  Pfister]{zhang2020distilling}
Zhang, Z., Zhang, H., Arik, S.~O., Lee, H., and Pfister, T.
\newblock Distilling effective supervision from severe label noise.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2020.

\end{thebibliography}
