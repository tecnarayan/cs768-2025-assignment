\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{arora2016understanding}
R.~Arora, A.~Basu, P.~Mianjy, and A.~Mukherjee.
\newblock Understanding deep neural networks with rectified linear units.
\newblock {\em arXiv preprint arXiv:1611.01491}, 2016.

\bibitem{ben2018multi}
H.~Ben-Hamu, H.~Maron, I.~Kezurer, G.~Avineri, and Y.~Lipman.
\newblock Multi-chart generative surface modeling.
\newblock In {\em SIGGRAPH Asia 2018 Technical Papers}, page 215. ACM, 2018.

\bibitem{ben1966newton}
A.~Ben-Israel.
\newblock A newton-raphson method for the solution of systems of equations.
\newblock {\em Journal of Mathematical analysis and applications},
  15(2):243--252, 1966.

\bibitem{berger2017survey}
M.~Berger, A.~Tagliasacchi, L.~M. Seversky, P.~Alliez, G.~Guennebaud, J.~A.
  Levine, A.~Sharf, and C.~T. Silva.
\newblock A survey of surface reconstruction from point clouds.
\newblock In {\em Computer Graphics Forum}, volume~36, pages 301--329. Wiley
  Online Library, 2017.

\bibitem{Bogo:CVPR:2014}
F.~Bogo, J.~Romero, M.~Loper, and M.~J. Black.
\newblock {FAUST}: Dataset and evaluation for {3D} mesh registration.
\newblock In {\em Proceedings IEEE Conf. on Computer Vision and Pattern
  Recognition (CVPR)}, Piscataway, NJ, USA, June 2014. IEEE.

\bibitem{chen2018learning}
Z.~Chen and H.~Zhang.
\newblock Learning implicit fields for generative shape modeling.
\newblock {\em arXiv preprint arXiv:1812.02822}, 2018.

\bibitem{cortes1995support}
C.~Cortes and V.~Vapnik.
\newblock Support-vector networks.
\newblock {\em Machine learning}, 20(3):273--297, 1995.

\bibitem{cybenko1989approximation}
G.~Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock {\em Mathematics of control, signals and systems}, 2(4):303--314,
  1989.

\bibitem{ding2018max}
G.~W. Ding, Y.~Sharma, K.~Y.~C. Lui, and R.~Huang.
\newblock Max-margin adversarial (mma) training: Direct input space margin
  maximization through adversarial training.
\newblock {\em arXiv preprint arXiv:1812.02637}, 2018.

\bibitem{ding2018advertorch}
G.~W. Ding, L.~Wang, and X.~Jin.
\newblock {AdverTorch} v0.1: An adversarial robustness toolbox based on
  pytorch.
\newblock {\em arXiv preprint arXiv:1902.07623}, 2019.

\bibitem{elsayed2018large}
G.~Elsayed, D.~Krishnan, H.~Mobahi, K.~Regan, and S.~Bengio.
\newblock Large margin deep networks for classification.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  842--852, 2018.

\bibitem{fan2017point}
H.~Fan, H.~Su, and L.~J. Guibas.
\newblock A point set generation network for 3d object reconstruction from a
  single image.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 605--613, 2017.

\bibitem{groueix2018papier}
T.~Groueix, M.~Fisher, V.~G. Kim, B.~C. Russell, and M.~Aubry.
\newblock A papier-m{\^a}ch{\'e} approach to learning 3d surface generation.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 216--224, 2018.

\bibitem{hein2017formal}
M.~Hein and M.~Andriushchenko.
\newblock Formal guarantees on the robustness of a classifier against
  adversarial manipulation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2266--2276, 2017.

\bibitem{hornik1989multilayer}
K.~Hornik, M.~Stinchcombe, and H.~White.
\newblock Multilayer feedforward networks are universal approximators.
\newblock {\em Neural networks}, 2(5):359--366, 1989.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{krantz2012implicit}
S.~G. Krantz and H.~R. Parks.
\newblock {\em The implicit function theorem: history, theory, and
  applications}.
\newblock Springer Science \& Business Media, 2012.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{kurakin2016adversarial}
A.~Kurakin, I.~Goodfellow, and S.~Bengio.
\newblock Adversarial machine learning at scale.
\newblock {\em arXiv preprint arXiv:1611.01236}, 2016.

\bibitem{lecun1998mnist}
Y.~LeCun.
\newblock The mnist database of handwritten digits.
\newblock {\em http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem{lorensen1987marching}
W.~E. Lorensen and H.~E. Cline.
\newblock Marching cubes: A high resolution 3d surface construction algorithm.
\newblock In {\em ACM siggraph computer graphics}, volume~21, pages 163--169.
  ACM, 1987.

\bibitem{madry2017towards}
A.~Madry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock {\em arXiv preprint arXiv:1706.06083}, 2017.

\bibitem{matyasko2017margin}
A.~Matyasko and L.-P. Chau.
\newblock Margin maximization for robust classification using deep learning.
\newblock In {\em 2017 International Joint Conference on Neural Networks
  (IJCNN)}, pages 300--307. IEEE, 2017.

\bibitem{mescheder2018occupancy}
L.~Mescheder, M.~Oechsle, M.~Niemeyer, S.~Nowozin, and A.~Geiger.
\newblock Occupancy networks: Learning 3d reconstruction in function space.
\newblock {\em arXiv preprint arXiv:1812.03828}, 2018.

\bibitem{moosavi2016deepfool}
S.-M. Moosavi-Dezfooli, A.~Fawzi, and P.~Frossard.
\newblock Deepfool: a simple and accurate method to fool deep neural networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2574--2582, 2016.

\bibitem{park2019deepsdf}
J.~J. Park, P.~Florence, J.~Straub, R.~Newcombe, and S.~Lovegrove.
\newblock Deepsdf: Learning continuous signed distance functions for shape
  representation.
\newblock {\em arXiv preprint arXiv:1901.05103}, 2019.

\bibitem{paszke2017automatic}
A.~Paszke, S.~Gross, S.~Chintala, G.~Chanan, E.~Yang, Z.~DeVito, Z.~Lin,
  A.~Desmaison, L.~Antiga, and A.~Lerer.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem{sokolic2017robust}
J.~Sokoli{\'c}, R.~Giryes, G.~Sapiro, and M.~R. Rodrigues.
\newblock Robust large margin deep neural networks.
\newblock {\em IEEE Transactions on Signal Processing}, 65(16):4265--4280,
  2017.

\bibitem{sun2015large}
S.~Sun, W.~Chen, L.~Wang, and T.-Y. Liu.
\newblock Large margin deep neural networks: Theory and algorithms.
\newblock {\em arXiv preprint arXiv:1506.05232}, 148, 2015.

\bibitem{tang2013deep}
Y.~Tang.
\newblock Deep learning using support vector machines.
\newblock {\em CoRR, abs/1306.0239}, 2, 2013.

\bibitem{williams2018deep}
F.~Williams, T.~Schneider, C.~Silva, D.~Zorin, J.~Bruna, and D.~Panozzo.
\newblock Deep geometric prior for surface reconstruction.
\newblock {\em arXiv preprint arXiv:1811.10943}, 2018.

\bibitem{wong2017provable}
E.~Wong and J.~Z. Kolter.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock {\em arXiv preprint arXiv:1711.00851}, 2017.

\bibitem{wong2018scaling}
E.~Wong, F.~Schmidt, J.~H. Metzen, and J.~Z. Kolter.
\newblock Scaling provable adversarial defenses.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8400--8409, 2018.

\bibitem{xiao2017fashion}
H.~Xiao, K.~Rasul, and R.~Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em arXiv preprint arXiv:1708.07747}, 2017.

\bibitem{zhang2019theoretically}
H.~Zhang, Y.~Yu, J.~Jiao, E.~P. Xing, L.~E. Ghaoui, and M.~I. Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock {\em arXiv preprint arXiv:1901.08573}, 2019.

\bibitem{zhao2001fast}
H.-K. Zhao, S.~Osher, and R.~Fedkiw.
\newblock Fast surface reconstruction using the level set method.
\newblock In {\em Proceedings IEEE Workshop on Variational and Level Set
  Methods in Computer Vision}, pages 194--201. IEEE, 2001.

\end{thebibliography}
