\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bai et~al.(2021{\natexlab{a}})Bai, Yan, Jiang, Xia, and
  Wang]{bai2021clustering}
Yang Bai, Xin Yan, Yong Jiang, Shu-Tao Xia, and Yisen Wang.
\newblock Clustering effect of (linearized) adversarial robust models.
\newblock In \emph{NeurIPS}, 2021{\natexlab{a}}.

\bibitem[Bai et~al.(2021{\natexlab{b}})Bai, Zeng, Jiang, Xia, Ma, and
  Wang]{bai2021improving}
Yang Bai, Yuyuan Zeng, Yong Jiang, Shu-Tao Xia, Xingjun Ma, and Yisen Wang.
\newblock Improving adversarial robustness via channel-wise activation
  suppressing.
\newblock In \emph{ICLR}, 2021{\natexlab{b}}.

\bibitem[Bao et~al.(2022)Bao, Dong, Piao, and Wei]{beit}
Hangbo Bao, Li~Dong, Songhao Piao, and Furu Wei.
\newblock {BE}it: {BERT} pre-training of image transformers.
\newblock In \emph{ICLR}, 2022.

\bibitem[Benz et~al.(2021)Benz, Zhang, and Kweon]{benz2021batch}
Philipp Benz, Chaoning Zhang, and In~So Kweon.
\newblock Batch normalization increases adversarial vulnerability and decreases
  adversarial transferability: A non-robust feature perspective.
\newblock In \emph{CVPR}, pages 7818--7827, 2021.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{simclr}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{ICML}, 2020.

\bibitem[Croce and Hein(2020)]{croce2020reliable}
Francesco Croce and Matthias Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In \emph{ICML}, pages 2206--2216. PMLR, 2020.

\bibitem[Cui et~al.(2023)Cui, Huang, Wang, and Wang]{cui2023rethinking}
Jingyi Cui, Weiran Huang, Yifei Wang, and Yisen Wang.
\newblock Rethinking weak supervision in helping contrastive learning.
\newblock In \emph{ICML}, 2023.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL-HLT}, 2019.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Fawzi et~al.(2016)Fawzi, Moosavi-Dezfooli, and
  Frossard]{fawzi2016robustness}
Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard.
\newblock Robustness of classifiers: from adversarial to random noise.
\newblock \emph{NeurIPS}, 29, 2016.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{ICLR}, 2015.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Gaidon, and Ma]{haochen}
Jeff~Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma.
\newblock Provable guarantees for self-supervised deep learning with spectral
  contrastive loss.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{moco}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{CVPR}, 2020.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and Girshick]{mae}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{CVPR}, pages 16000--16009, 2022.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{densenet}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Tsipras, Engstrom, Tran, and
  Madry]{ilyas2019adversarial}
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Joe et~al.(2019)Joe, Hwang, and Shin]{joe2019learning}
Byunggill Joe, Sung~Ju Hwang, and Insik Shin.
\newblock Learning to disentangle robust and vulnerable features for
  adversarial detection.
\newblock \emph{arXiv preprint arXiv:1909.04311}, 2019.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{cifar}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{alexnet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{NeurIPS}, 2012.

\bibitem[Luo et~al.(2023)Luo, Wang, and Wang]{DynACL}
Rundong Luo, Yifei Wang, and Yisen Wang.
\newblock Rethinking the effect of data augmentation in adversarial contrastive
  learning.
\newblock In \emph{ICLR}, 2023.

\bibitem[Ma et~al.(2020)Ma, Niu, Gu, Wang, Zhao, Bailey, and
  Lu]{ma2019understanding}
Xingjun Ma, Yuhao Niu, Lin Gu, Yisen Wang, Yitian Zhao, James Bailey, and Feng
  Lu.
\newblock Understanding adversarial attacks on deep learning based medical
  image analysis systems.
\newblock \emph{Pattern Recognition}, 2020.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Niu et~al.(2021)Niu, Guo, and Wang]{niu2021morie}
Dantong Niu, Ruohao Guo, and Yisen Wang.
\newblock Mori{\'e} attack (ma): A new potential risk of screen photos.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{InfoNCE}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Ren et~al.(2021)Ren, Zhang, Wang, Chen, Zhou, Chen, Cheng, Wang, Zhou,
  Shi, et~al.]{ren2021unified}
Jie Ren, Die Zhang, Yisen Wang, Lu~Chen, Zhanpeng Zhou, Yiting Chen, Xu~Cheng,
  Xin Wang, Meng Zhou, Jie Shi, et~al.
\newblock A unified game-theoretic interpretation of adversarial robustness.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{MICCAI}. Springer, 2015.

\bibitem[Schmidt et~al.(2018)Schmidt, Santurkar, Tsipras, Talwar, and
  Madry]{schmidt2018adversarially}
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and
  Aleksander Madry.
\newblock Adversarially robust generalization requires more data.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Shafahi et~al.(2018)Shafahi, Huang, Studer, Feizi, and
  Goldstein]{shafahi2018adversarial}
Ali Shafahi, W~Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein.
\newblock Are adversarial examples inevitable?
\newblock \emph{arXiv preprint arXiv:1809.02104}, 2018.

\bibitem[Shi et~al.(2020)Shi, Riba, Mishkin, Moreno, and
  Nicolaou]{eriba2019kornia}
Jian Shi, Edgar Riba, Dmytro Mishkin, Francesc Moreno, and Anguelos Nicolaou.
\newblock Differentiable data augmentation with kornia, 2020.

\bibitem[Song et~al.(2020)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and
  Poole]{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock In \emph{ICLR}, 2020.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{ICLR}, 2014.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{inception}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In \emph{CVPR}, 2015.

\bibitem[Tanay and Griffin(2016)]{tanay2016boundary}
Thomas Tanay and Lewis Griffin.
\newblock A boundary tilting persepective on the phenomenon of adversarial
  examples.
\newblock \emph{arXiv preprint arXiv:1608.07690}, 2016.

\bibitem[Tao et~al.(2022)Tao, Feng, Wei, Yi, Huang, and Chen]{tao2022can}
Lue Tao, Lei Feng, Hongxin Wei, Jinfeng Yi, Sheng-Jun Huang, and Songcan Chen.
\newblock Can adversarial training be manipulated by non-robust features?
\newblock \emph{NeurIPS}, 35:\penalty0 26504--26518, 2022.

\bibitem[Tsilivis et~al.(2022)Tsilivis, Su, and Kempe]{tsilivis2022can}
Nikolaos Tsilivis, Jingtong Su, and Julia Kempe.
\newblock Can we achieve robustness from data alone?
\newblock \emph{arXiv preprint arXiv:2207.11727}, 2022.

\bibitem[Tsipras et~al.(2019)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{ICLR}, 2019.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Ren, Lin, Zhu, Wang, and
  Zhang]{wang2021unified}
Xin Wang, Jie Ren, Shuyun Lin, Xiangming Zhu, Yisen Wang, and Quanshi Zhang.
\newblock A unified approach to interpreting and boosting adversarial
  transferability.
\newblock In \emph{ICLR}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Geng, Jiang, Li, Wang, Yang, and
  Lin]{wang2021residual}
Yifei Wang, Zhengyang Geng, Feng Jiang, Chuming Li, Yisen Wang, Jiansheng Yang,
  and Zhouchen Lin.
\newblock Residual relaxation for multi-view representation learning.
\newblock In \emph{NeurIPS}, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2022)Wang, Zhang, Wang, Yang, and Lin]{wang2022chaos}
Yifei Wang, Qi~Zhang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin.
\newblock Chaos is a ladder: A new theoretical understanding of contrastive
  learning via augmentation overlap.
\newblock In \emph{ICLR}, 2022.

\bibitem[Wang et~al.(2023)Wang, Zhang, Du, Yang, Lin, and
  Wang]{wang2023message}
Yifei Wang, Qi~Zhang, Tianqi Du, Jiansheng Yang, Zhouchen Lin, and Yisen Wang.
\newblock A message passing perspective on learning dynamics of contrastive
  learning.
\newblock In \emph{ICLR}, 2023.

\bibitem[Wang et~al.(2019)Wang, Ma, Bailey, Yi, Zhou, and Gu]{wang2019dynamic}
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu.
\newblock On the convergence and robustness of adversarial training.
\newblock In \emph{ICML}, 2019.

\bibitem[Wang et~al.(2020)Wang, Zou, Yi, Bailey, Ma, and Gu]{wang2020improving}
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu.
\newblock Improving adversarial robustness requires revisiting misclassified
  examples.
\newblock In \emph{ICLR}, 2020.

\bibitem[Wei et~al.(2023)Wei, Wang, Guo, and Wang]{wei2023cfa}
Zeming Wei, Yifei Wang, Yiwen Guo, and Yisen Wang.
\newblock Cfa: Class-wise calibrated fair adversarial training.
\newblock In \emph{CVPR}, pages 8193--8201, 2023.

\bibitem[Wu et~al.(2020{\natexlab{a}})Wu, Wang, Xia, Bailey, and
  Ma]{wu2020skip}
Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma.
\newblock Skip connections matter: On the transferability of adversarial
  examples generated with resnets.
\newblock In \emph{ICLR}, 2020{\natexlab{a}}.

\bibitem[Wu et~al.(2020{\natexlab{b}})Wu, Xia, and Wang]{wu2020adversarial}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock In \emph{NeurIPS}, 2020{\natexlab{b}}.

\bibitem[Wu et~al.(2017)Wu, Zhang, and Xu]{tinyImageNet}
Jiayu Wu, Qixiang Zhang, and Guoxi Xu.
\newblock Tiny {ImageNet} challenge, 2017.

\bibitem[Xiang et~al.(2023)Xiang, Yang, Huang, and Wang]{xiang2023denoising}
Weilai Xiang, Hongyu Yang, Di~Huang, and Yunhong Wang.
\newblock Denoising diffusion autoencoders are unified self-supervised
  learners.
\newblock \emph{arXiv preprint arXiv:2303.09769}, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Wang, and Wang]{zhang2022mask}
Qi~Zhang, Yifei Wang, and Yisen Wang.
\newblock How mask matters: Towards theoretical understandings of masked
  autoencoders.
\newblock \emph{NeurIPS}, 35:\penalty0 27127--27139, 2022.

\bibitem[Zhang et~al.(2023)Zhang, Wang, and Wang]{zhang2023on}
Qi~Zhang, Yifei Wang, and Yisen Wang.
\newblock On the generalization of multi-modal contrastive learning.
\newblock In \emph{ICML}, 2023.

\bibitem[Zhuo et~al.(2023)Zhuo, Wang, Ma, and Wang]{zhuo2023towards}
Zhijian Zhuo, Yifei Wang, Jinwen Ma, and Yisen Wang.
\newblock Towards a unified theoretical understanding of non-contrastive
  learning via rank differential mechanism.
\newblock In \emph{ICLR}, 2023.

\end{thebibliography}
