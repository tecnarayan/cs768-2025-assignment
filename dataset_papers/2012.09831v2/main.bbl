\begin{thebibliography}{10}

\bibitem{allen2019infinite}
K.~R. Allen, E.~Shelhamer, H.~Shin, and J.~B. Tenenbaum.
\newblock Infinite mixture prototypes for few-shot learning.
\newblock In {\em International Conference on Machine Learning}, 2019.

\bibitem{altae2017low}
H.~Altae-Tran, B.~Ramsundar, A.~S. Pappu, and V.~Pande.
\newblock Low data drug discovery with one-shot learning.
\newblock {\em ACS central science}, 2017.

\bibitem{bai2021important}
Y.~Bai, M.~Chen, P.~Zhou, T.~Zhao, J.~Lee, S.~Kakade, H.~Wang, and C.~Xiong.
\newblock How important is the train-validation split in meta-learning?
\newblock In {\em International Conference on Machine Learning}, 2021.

\bibitem{bengio1992optimization}
S.~Bengio, Y.~Bengio, J.~Cloutier, and J.~Gecsei.
\newblock On the optimization of a synaptic learning rule.
\newblock In {\em Preprints Conf. Optimality in Artificial and Biological
  Neural Networks}. Univ. of Texas, 1992.

\bibitem{bertinetto2019meta}
L.~Bertinetto, J.~F. Henriques, P.~H. Torr, and A.~Vedaldi.
\newblock Meta-learning with differentiable closed-form solvers.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{bertinetto2016learning}
L.~Bertinetto, J.~F. Henriques, J.~Valmadre, P.~Torr, and A.~Vedaldi.
\newblock Learning feed-forward one-shot learners.
\newblock In {\em Advances in Neural Information Processing Systems}, 2016.

\bibitem{cao2020theoretical}
T.~Cao, M.~Law, and S.~Fidler.
\newblock A theoretical analysis of the number of shots in few-shot learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{chen2020closer}
J.~Chen, X.-M. Wu, Y.~Li, Q.~Li, L.-M. Zhan, and F.-l. Chung.
\newblock A closer look at the training strategy for modern meta-learning.
\newblock {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock {\em arXiv preprint arXiv:2002.05709}, 2020.

\bibitem{chen2019closer}
W.-Y. Chen, Y.-C. Liu, Z.~Kira, Y.-C.~F. Wang, and J.-B. Huang.
\newblock A closer look at few-shot classification.
\newblock {\em International Conference on Learning Representations}, 2019.

\bibitem{chen2020new}
Y.~Chen, X.~Wang, Z.~Liu, H.~Xu, and T.~Darrell.
\newblock A new meta-baseline for few-shot learning.
\newblock {\em arXiv preprint arXiv:2003.04390}, 2020.

\bibitem{dhillon2020baseline}
G.~S. Dhillon, P.~Chaudhari, A.~Ravichandran, and S.~Soatto.
\newblock A baseline for few-shot image classification.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{fei2021melr}
N.~Fei, Z.~Lu, T.~Xiang, and S.~Huang.
\newblock Melr: Meta-learning via modeling episode-level relationships for
  few-shot learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{finn2019lecture}
C.~Finn.
\newblock Stanford cs330: Multi-task and meta-learning, 2019 | lecture 4 -
  non-parametric meta-learners.
\newblock
  \url{https://www.youtube.com/watch?v=bc-6tzTyYcM&list=PLoROMvodv4rMC6zfYmnD7UG3LVvwaITY5&index=4},
  2019.
\newblock Accessed on 26/01/2021.

\bibitem{finn2017model}
C.~Finn, P.~Abbeel, and S.~Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em International Conference on Machine Learning}, 2017.

\bibitem{frosst2019analyzing}
N.~Frosst, N.~Papernot, and G.~Hinton.
\newblock Analyzing and improving representations with the soft nearest
  neighbor loss.
\newblock {\em International Conference on Machine Learning}, 2019.

\bibitem{furlanello2018born}
T.~Furlanello, Z.~Lipton, M.~Tschannen, L.~Itti, and A.~Anandkumar.
\newblock Born again neural networks.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{garcia2021circumpapillary}
G.~Garc{\'\i}a, R.~Del~Amor, A.~Colomer, R.~Verd{\'u}-Monedero,
  J.~Morales-S{\'a}nchez, and V.~Naranjo.
\newblock Circumpapillary oct-focused hybrid learning for glaucoma grading
  using tailored prototypical neural networks.
\newblock {\em Artificial Intelligence in Medicine}, 118:102132, 2021.

\bibitem{ghiasi2018dropblock}
G.~Ghiasi, T.-Y. Lin, and Q.~V. Le.
\newblock Dropblock: A regularization method for convolutional networks.
\newblock In {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem{gidaris2019boosting}
S.~Gidaris, A.~Bursuc, N.~Komodakis, P.~P{\'e}rez, and M.~Cord.
\newblock Boosting few-shot visual learning with self-supervision.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  2019.

\bibitem{goldberger2005neighborhood}
J.~Goldberger, G.~E. Hinton, S.~T. Roweis, and R.~R. Salakhutdinov.
\newblock Neighbourhood components analysis.
\newblock In {\em Advances in Neural Information Processing Systems}, 2005.

\bibitem{goldblum2020unraveling}
M.~Goldblum, S.~Reich, L.~Fowl, R.~Ni, V.~Cherepanova, and T.~Goldstein.
\newblock Unraveling meta-learning: Understanding feature representations for
  few-shot tasks.
\newblock In {\em International Conference on Machine Learning}. PMLR, 2020.

\bibitem{graves2014neural}
A.~Graves, G.~Wayne, and I.~Danihelka.
\newblock Neural turing machines.
\newblock {\em arXiv preprint arXiv:1410.5401}, 2014.

\bibitem{guo2017calibration}
C.~Guo, G.~Pleiss, Y.~Sun, and K.~q. Weinberger.
\newblock On calibration of modern neural networks.
\newblock In {\em International Conference on Machine Learning}, 2017.

\bibitem{hospedales2020meta}
T.~Hospedales, A.~Antoniou, P.~Micaelli, and A.~Storkey.
\newblock Meta-learning in neural networks: A survey.
\newblock {\em arXiv preprint arXiv:2004.05439}, 2020.

\bibitem{khosla2020supervisedcontrastive}
P.~Khosla, P.~Teterwak, C.~Wang, A.~Sarna, Y.~Tian, P.~Isola, A.~Maschinot,
  C.~Liu, and D.~Krishnan.
\newblock Supervised contrastive learning.
\newblock {\em arXiv preprint arXiv:2004.11362}, 2020.

\bibitem{lake2015human}
B.~M. Lake, R.~Salakhutdinov, and J.~B. Tenenbaum.
\newblock Human-level concept learning through probabilistic program induction.
\newblock {\em Science}, 2015.

\bibitem{lee2019meta}
K.~Lee, S.~Maji, A.~Ravichandran, and S.~Soatto.
\newblock Meta-learning with differentiable convex optimization.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  2019.

\bibitem{miller1995wordnet}
G.~A. Miller.
\newblock Wordnet: a lexical database for english.
\newblock {\em Communications of the ACM}, 38(11):39--41, 1995.

\bibitem{munkhdalai2017meta}
T.~Munkhdalai and H.~Yu.
\newblock Meta networks.
\newblock In {\em International Conference on Machine Learning}, 2017.

\bibitem{munkhdalai2018rapid}
T.~Munkhdalai, X.~Yuan, S.~Mehri, and A.~Trischler.
\newblock Rapid adaptation with conditionally shifted neurons.
\newblock In {\em International Conference on Machine Learning}, 2018.

\bibitem{oreshkin2018tadam}
B.~Oreshkin, P.~R. L{\'o}pez, and A.~Lacoste.
\newblock Tadam: Task dependent adaptive metric for improved few-shot learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem{patacchiola2020bayesian}
M.~Patacchiola, J.~Turner, E.~J. Crowley, M.~O'Boyle, and A.~J. Storkey.
\newblock Bayesian meta-learning for the few-shot setting via deep kernels.
\newblock {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{qi2018low}
H.~Qi, M.~Brown, and D.~G. Lowe.
\newblock Low-shot learning with imprinted weights.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  2018.

\bibitem{qiao2019transductive}
L.~Qiao, Y.~Shi, J.~Li, Y.~Wang, T.~Huang, and Y.~Tian.
\newblock Transductive episodic-wise adaptive metric for few-shot learning.
\newblock In {\em IEEE International Conference on Computer Vision}, 2019.

\bibitem{raghu2020rapid}
A.~Raghu, M.~Raghu, S.~Bengio, and O.~Vinyals.
\newblock Rapid learning or feature reuse? towards understanding the
  effectiveness of maml.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{ravi2016optimization}
S.~Ravi and H.~Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{ravichandran2019few}
A.~Ravichandran, R.~Bhotika, and S.~Soatto.
\newblock Few-shot learning with embedded class models and shot-free meta
  training.
\newblock In {\em IEEE International Conference on Computer Vision}, 2019.

\bibitem{ren2018meta}
M.~Ren, E.~Triantafillou, S.~Ravi, J.~Snell, K.~Swersky, J.~B. Tenenbaum,
  H.~Larochelle, and R.~S. Zemel.
\newblock Meta-learning for semi-supervised few-shot classification.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{salakhutdinov2007learning}
R.~Salakhutdinov and G.~Hinton.
\newblock Learning a nonlinear embedding by preserving class neighbourhood
  structure.
\newblock In {\em Artificial Intelligence and Statistics}, 2007.

\bibitem{salekin2021understanding}
A.~Salekin and N.~Russo.
\newblock Understanding autism: the power of eeg harnessed by prototypical
  learning.
\newblock In {\em Proceedings of the Workshop on Medical Cyber Physical Systems
  and Internet of Medical Things}, pages 12--16, 2021.

\bibitem{santoro2016meta}
A.~Santoro, S.~Bartunov, M.~Botvinick, D.~Wierstra, and T.~Lillicrap.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In {\em International Conference on Machine Learning}, 2016.

\bibitem{schmidhuber1987evolutionary}
J.~Schmidhuber.
\newblock {\em Evolutionary principles in self-referential learning, or on
  learning how to learn: the meta-meta-... hook}.
\newblock PhD thesis, Technische Universit{\"a}t M{\"u}nchen, 1987.

\bibitem{schmidhuber1992learning}
J.~Schmidhuber.
\newblock Learning to control fast-weight memories: An alternative to dynamic
  recurrent networks.
\newblock {\em Neural Computation}, 1992.

\bibitem{snell2017prototypical}
J.~Snell, K.~Swersky, and R.~Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{snell2020bayesian}
J.~Snell and R.~Zemel.
\newblock Bayesian few-shot classification with one-vs-each p\'olya-gamma
  augmented gaussian processes.
\newblock {\em International Conference on Learning Representations}, 2021.

\bibitem{sun2019meta}
Q.~Sun, Y.~Liu, T.-S. Chua, and B.~Schiele.
\newblock Meta-transfer learning for few-shot learning.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition},
  2019.

\bibitem{thrun1996learning}
S.~Thrun.
\newblock Is learning the n-th thing any easier than learning the first?
\newblock In {\em Advances in Neural Information Processing Systems}, 1996.

\bibitem{tian2020rethinking}
Y.~Tian, Y.~Wang, D.~Krishnan, J.~B. Tenenbaum, and P.~Isola.
\newblock Rethinking few-shot image classification: a good embedding is all you
  need?
\newblock {\em European Conference on Computer Vision}, 2020.

\bibitem{triantafillou2017few}
E.~Triantafillou, R.~S. Zemel, and R.~Urtasun.
\newblock Few-shot learning through an information retrieval lens.
\newblock In {\em Advances in Neural Information Processing Systems}, 2017.

\bibitem{utgoff1986shift}
P.~E. Utgoff.
\newblock Shift of bias for inductive concept learning.
\newblock {\em Machine learning: An artificial intelligence approach}, 1986.

\bibitem{vilalta2002perspective}
R.~Vilalta and Y.~Drissi.
\newblock A perspective view and survey of meta-learning.
\newblock {\em Artificial Intelligence Review}, 2002.

\bibitem{vinyals2016matching}
O.~Vinyals, C.~Blundell, T.~Lillicrap, D.~Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2016.

\bibitem{wang2019simpleshot}
Y.~Wang, W.-L. Chao, K.~Q. Weinberger, and L.~van~der Maaten.
\newblock Simpleshot: Revisiting nearest-neighbor classification for few-shot
  learning.
\newblock {\em arXiv preprint arXiv:1911.04623}, 2019.

\bibitem{yoon2019tapnet}
S.~W. Yoon, J.~Seo, and J.~Moon.
\newblock Tapnet: Neural network augmented with task-adaptive projection for
  few-shot learning.
\newblock In {\em International Conference on Machine Learning}, 2019.

\bibitem{zhang2019variational}
J.~Zhang, C.~Zhao, B.~Ni, M.~Xu, and X.~Yang.
\newblock Variational few-shot learning.
\newblock In {\em IEEE International Conference on Computer Vision}, 2019.

\end{thebibliography}
