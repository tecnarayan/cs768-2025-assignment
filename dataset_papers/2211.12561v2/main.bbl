\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Ge, Shakeri, and
  Al-Rfou]{Agarwal2021KnowledgeGB}
Agarwal, O., Ge, H., Shakeri, S., and Al-Rfou, R.
\newblock Knowledge graph based synthetic corpus generation for
  knowledge-enhanced language model pre-training.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics (NAACL)}, 2021.

\bibitem[Aghajanyan et~al.(2022)Aghajanyan, Huang, Ross, Karpukhin, Xu, Goyal,
  Okhonko, Joshi, Ghosh, Lewis, and Zettlemoyer]{aghajanyan2022cm3}
Aghajanyan, A., Huang, B., Ross, C., Karpukhin, V., Xu, H., Goyal, N., Okhonko,
  D., Joshi, M., Ghosh, G., Lewis, M., and Zettlemoyer, L.
\newblock {CM3}: A causal masked multimodal model of the internet.
\newblock \emph{arXiv preprint arXiv:2201.07520}, 2022.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc,
  K., Mensch, A., Millican, K., Reynolds, M., et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{arXiv preprint arXiv:2204.14198}, 2022.

\bibitem[Ashual et~al.(2022)Ashual, Sheynin, Polyak, Singer, Gafni, Nachmani,
  and Taigman]{ashual2022knn}
Ashual, O., Sheynin, S., Polyak, A., Singer, U., Gafni, O., Nachmani, E., and
  Taigman, Y.
\newblock Knn-diffusion: Image generation via large-scale retrieval.
\newblock \emph{arXiv preprint arXiv:2204.02849}, 2022.

\bibitem[Birhane et~al.(2021{\natexlab{a}})Birhane, Prabhu, and
  Kahembwe]{birhane2021multimodal}
Birhane, A., Prabhu, V.~U., and Kahembwe, E.
\newblock Multimodal datasets: misogyny, pornography, and malignant
  stereotypes.
\newblock \emph{arXiv preprint arXiv:2110.01963}, 2021{\natexlab{a}}.

\bibitem[Birhane et~al.(2021{\natexlab{b}})Birhane, Prabhu, and
  Kahembwe]{denton2021ethical}
Birhane, A., Prabhu, V.~U., and Kahembwe, E.
\newblock {Ethical considerations of generative AI}.
\newblock \emph{AI for Content Creation Workshop, CVPR}, 2021{\natexlab{b}}.

\bibitem[Blattmann et~al.(2022)Blattmann, Rombach, Oktay, and
  Ommer]{blattmann2022retrieval}
Blattmann, A., Rombach, R., Oktay, K., and Ommer, B.
\newblock Retrieval-augmented diffusion models.
\newblock \emph{arXiv preprint arXiv:2204.11824}, 2022.

\bibitem[Borgeaud et~al.(2022)Borgeaud, Mensch, Hoffmann, Cai, Rutherford,
  Millican, Van Den~Driessche, Lespiau, Damoc, Clark,
  et~al.]{borgeaud2022improving}
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K.,
  Van Den~Driessche, G.~B., Lespiau, J.-B., Damoc, B., Clark, A., et~al.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Hu, Chen, Verga, and
  Cohen]{chen2022murag}
Chen, W., Hu, H., Chen, X., Verga, P., and Cohen, W.~W.
\newblock Murag: Multimodal retrieval-augmented generator for open question
  answering over images and text.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Hu, Saharia, and Cohen]{reimagen}
Chen, W., Hu, H., Saharia, C., and Cohen, W.~W.
\newblock Re-imagen: Retrieval-augmented text-to-image generator.
\newblock \emph{arXiv preprint arXiv:2209.14491}, 2022{\natexlab{b}}.

\bibitem[Cho et~al.(2020)Cho, Lu, Schwenk, Hajishirzi, and Kembhavi]{cho2020x}
Cho, J., Lu, J., Schwenk, D., Hajishirzi, H., and Kembhavi, A.
\newblock X-lxmert: Paint, caption and answer questions with multi-modal
  transformers.
\newblock \emph{arXiv preprint arXiv:2009.11278}, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2009.

\bibitem[Ding et~al.(2021)Ding, Yang, Hong, Zheng, Zhou, Yin, Lin, Zou, Shao,
  Yang, et~al.]{ding2021cogview}
Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X.,
  Shao, Z., Yang, H., et~al.
\newblock Cogview: Mastering text-to-image generation via transformers.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Ding et~al.(2022)Ding, Zheng, Hong, and Tang]{ding2022cogview2}
Ding, M., Zheng, W., Hong, W., and Tang, J.
\newblock Cogview2: Faster and better text-to-image generation via hierarchical
  transformers.
\newblock \emph{arXiv preprint arXiv:2204.14217}, 2022.

\bibitem[Esser et~al.(2021)Esser, Rombach, and Ommer]{esser2021taming}
Esser, P., Rombach, R., and Ommer, B.
\newblock Taming transformers for high-resolution image synthesis.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2021.

\bibitem[Forever(2021)]{ru-dalle}
Forever, A.
\newblock rudall-e.
\newblock \url{https://github.com/ai-forever/ru-dalle}, 2021.

\bibitem[Fried et~al.(2022)Fried, Aghajanyan, Lin, Wang, Wallace, Shi, Zhong,
  Yih, Zettlemoyer, and Lewis]{fried2022incoder}
Fried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R.,
  Yih, W.-t., Zettlemoyer, L., and Lewis, M.
\newblock Incoder: A generative model for code infilling and synthesis.
\newblock \emph{arXiv preprint arXiv:2204.05999}, 2022.

\bibitem[Gafni et~al.(2022)Gafni, Polyak, Ashual, Sheynin, Parikh, and
  Taigman]{gafni2022make}
Gafni, O., Polyak, A., Ashual, O., Sheynin, S., Parikh, D., and Taigman, Y.
\newblock Make-a-scene: Scene-based text-to-image generation with human priors.
\newblock \emph{arXiv preprint arXiv:2203.13131}, 2022.

\bibitem[Gur et~al.(2021)Gur, Neverova, Stauffer, Lim, Kiela, and
  Reiter]{gur2021cross}
Gur, S., Neverova, N., Stauffer, C., Lim, S.-N., Kiela, D., and Reiter, A.
\newblock Cross-modal retrieval augmentation for multi-modal classification.
\newblock \emph{arXiv preprint arXiv:2104.08108}, 2021.

\bibitem[Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang]{guu2020realm}
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.-W.
\newblock Realm: Retrieval-augmented language model pre-training.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Hao et~al.(2022)Hao, Song, Dong, Huang, Chi, Wang, Ma, and
  Wei]{hao2022language}
Hao, Y., Song, H., Dong, L., Huang, S., Chi, Z., Wang, W., Ma, S., and Wei, F.
\newblock Language models are general-purpose interfaces.
\newblock \emph{arXiv preprint arXiv:2206.06336}, 2022.

\bibitem[Hashimoto et~al.(2018)Hashimoto, Guu, Oren, and
  Liang]{hashimoto2018retrieve}
Hashimoto, T.~B., Guu, K., Oren, Y., and Liang, P.~S.
\newblock A retrieve-and-edit framework for predicting structured outputs.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017fid}
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Johnson et~al.(2019)Johnson, Douze, and J{\'e}gou]{johnson2019billion}
Johnson, J., Douze, M., and J{\'e}gou, H.
\newblock Billion-scale similarity search with {GPUs}.
\newblock \emph{IEEE Transactions on Big Data}, 2019.

\bibitem[Karpukhin et~al.(2020)Karpukhin, O{\u{g}}uz, Min, Lewis, Wu, Edunov,
  Chen, and Yih]{karpukhin2020dense}
Karpukhin, V., O{\u{g}}uz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen,
  D., and Yih, W.-t.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  2020.

\bibitem[Keskar et~al.(2019)Keskar, McCann, Varshney, Xiong, and
  Socher]{keskar2019ctrl}
Keskar, N.~S., McCann, B., Varshney, L.~R., Xiong, C., and Socher, R.
\newblock Ctrl: A conditional transformer language model for controllable
  generation.
\newblock \emph{arXiv preprint arXiv:1909.05858}, 2019.

\bibitem[Khandelwal et~al.(2019)Khandelwal, Levy, Jurafsky, Zettlemoyer, and
  Lewis]{khandelwal2019generalization}
Khandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer, L., and Lewis, M.
\newblock Generalization through memorization: Nearest neighbor language
  models.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Kim et~al.(2021)Kim, Cho, Kim, Lee, and Baek]{kakaobrain2021minDALL-E}
Kim, S., Cho, S., Kim, C., Lee, D., and Baek, W.
\newblock mindall-e on conceptual captions.
\newblock \url{https://github.com/kakaobrain/minDALL-E}, 2021.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2015adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Lewis et~al.(2020{\natexlab{a}})Lewis, Ghazvininejad, Ghosh,
  Aghajanyan, Wang, and Zettlemoyer]{lewis2020pre}
Lewis, M., Ghazvininejad, M., Ghosh, G., Aghajanyan, A., Wang, S., and
  Zettlemoyer, L.
\newblock Pre-training via paraphrasing.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)},
  2020{\natexlab{a}}.

\bibitem[Lewis et~al.(2020{\natexlab{b}})Lewis, Perez, Piktus, Petroni,
  Karpukhin, Goyal, K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel,
  et~al.]{lewis2020retrieval}
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N.,
  K{\"u}ttler, H., Lewis, M., Yih, W.-t., Rockt{\"a}schel, T., et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2019)Li, Qi, Lukasiewicz, and Torr]{li2019controllable}
Li, B., Qi, X., Lukasiewicz, T., and Torr, P.
\newblock Controllable text-to-image generation.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Li et~al.(2022)Li, Pan, Yao, and Mei]{li2022comprehending}
Li, Y., Pan, Y., Yao, T., and Mei, T.
\newblock Comprehending and ordering semantics for image captioning.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2022.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{lin2014microsoft}
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
  Doll{\'a}r, P., and Zitnick, C.~L.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{European conference on computer vision}, 2014.

\bibitem[Metzler et~al.(2021)Metzler, Tay, Bahri, and
  Najork]{metzler2021rethinking}
Metzler, D., Tay, Y., Bahri, D., and Najork, M.
\newblock Rethinking search: making domain experts out of dilettantes.
\newblock In \emph{ACM SIGIR Forum}, 2021.

\bibitem[Nichol et~al.(2021)Nichol, Dhariwal, Ramesh, Shyam, Mishkin, McGrew,
  Sutskever, and Chen]{nichol2021glide}
Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,
  Sutskever, I., and Chen, M.
\newblock Glide: Towards photorealistic image generation and editing with
  text-guided diffusion models.
\newblock \emph{arXiv preprint arXiv:2112.10741}, 2021.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and
  Sutskever, I.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and
  Chen]{ramesh2022hierarchical}
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 2022.

\bibitem[Ramos et~al.(2023)Ramos, Martins, Elliott, and
  Kementchedjhieva]{ramos2022smallcap}
Ramos, R., Martins, B., Elliott, D., and Kementchedjhieva, Y.
\newblock Smallcap: Lightweight image captioning prompted with retrieval
  augmentation.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2023.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and
  Ommer]{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2022.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton,
  Ghasemipour, Ayan, Mahdavi, Lopes, et~al.]{saharia2022photorealistic}
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour,
  S. K.~S., Ayan, B.~K., Mahdavi, S.~S., Lopes, R.~G., et~al.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock \emph{arXiv preprint arXiv:2205.11487}, 2022.

\bibitem[Sarto et~al.(2022)Sarto, Cornia, Baraldi, and
  Cucchiara]{sarto2022retrieval}
Sarto, S., Cornia, M., Baraldi, L., and Cucchiara, R.
\newblock Retrieval-augmented transformer for image captioning.
\newblock In \emph{Proceedings of the 19th International Conference on
  Content-based Multimedia Indexing}, 2022.

\bibitem[Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis,
  Katta, Coombes, Jitsev, and Komatsuzaki]{schuhmann2021laion}
Schuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R., Mullis, C., Katta, A.,
  Coombes, T., Jitsev, J., and Komatsuzaki, A.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text
  pairs.
\newblock \emph{arXiv preprint arXiv:2111.02114}, 2021.

\bibitem[Shi et~al.(2023)Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer,
  and Yih]{shi2023replug}
Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L.,
  and Yih, W.-t.
\newblock Replug: Retrieval-augmented black-box language models.
\newblock \emph{arXiv preprint arXiv:2301.12652}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2017.

\bibitem[Vedantam et~al.(2015)Vedantam, Lawrence~Zitnick, and
  Parikh]{vedantam2015cider}
Vedantam, R., Lawrence~Zitnick, C., and Parikh, D.
\newblock Cider: Consensus-based image description evaluation.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2015.

\bibitem[Wang(2021)]{lucidrains-dalle}
Wang, P.
\newblock Dall-e in pytorch.
\newblock \url{https://github.com/lucidrains/DALLE-pytorch}, 2021.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Yasunaga, Ren, Wada, and
  Leskovec]{wang2022vqa}
Wang, Y., Yasunaga, M., Ren, H., Wada, S., and Leskovec, J.
\newblock Vqa-gnn: Reasoning with multimodal semantic graph for visual question
  answering.
\newblock \emph{arXiv preprint arXiv:2205.11501}, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Yu, Yu, Dai, Tsvetkov, and
  Cao]{wang2021simvlm}
Wang, Z., Yu, J., Yu, A.~W., Dai, Z., Tsvetkov, Y., and Cao, Y.
\newblock Simvlm: Simple visual language model pretraining with weak
  supervision.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022{\natexlab{b}}.

\bibitem[Xie et~al.(2022)Xie, Wu, Shi, Zhong, Scholak, Yasunaga, Wu, Zhong,
  Yin, Wang, Zhong, Wang, Li, Boyle, Ni, Yao, Radev, Xiong, Kong, Zhang, Smith,
  Zettlemoyer, and Yu]{xie2022unifiedskg}
Xie, T., Wu, C.~H., Shi, P., Zhong, R., Scholak, T., Yasunaga, M., Wu, C.-S.,
  Zhong, M., Yin, P., Wang, S.~I., Zhong, V., Wang, B., Li, C., Boyle, C., Ni,
  A., Yao, Z., Radev, D., Xiong, C., Kong, L., Zhang, R., Smith, N.~A.,
  Zettlemoyer, L., and Yu, T.
\newblock Unifiedskg: Unifying and multi-tasking structured knowledge grounding
  with text-to-text language models.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  2022.

\bibitem[Yasunaga et~al.(2021)Yasunaga, Ren, Bosselut, Liang, and
  Leskovec]{yasunaga2021qa}
Yasunaga, M., Ren, H., Bosselut, A., Liang, P., and Leskovec, J.
\newblock {QA-GNN}: Reasoning with language models and knowledge graphs for
  question answering.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics (NAACL)}, 2021.

\bibitem[Yasunaga et~al.(2022{\natexlab{a}})Yasunaga, Bosselut, Ren, Zhang,
  Manning, Liang, and Leskovec]{yasunaga2022dragon}
Yasunaga, M., Bosselut, A., Ren, H., Zhang, X., Manning, C.~D., Liang, P., and
  Leskovec, J.
\newblock Deep bidirectional language-knowledge graph pretraining.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)},
  2022{\natexlab{a}}.

\bibitem[Yasunaga et~al.(2022{\natexlab{b}})Yasunaga, Leskovec, and
  Liang]{yasunaga2022linkbert}
Yasunaga, M., Leskovec, J., and Liang, P.
\newblock {LinkBERT}: Pretraining language models with document links.
\newblock In \emph{Association for Computational Linguistics (ACL)},
  2022{\natexlab{b}}.

\bibitem[Yu et~al.(2022)Yu, Xu, Koh, Luong, Baid, Wang, Vasudevan, Ku, Yang,
  Ayan, et~al.]{yu2022scaling}
Yu, J., Xu, Y., Koh, J.~Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku,
  A., Yang, Y., Ayan, B.~K., et~al.
\newblock Scaling autoregressive models for content-rich text-to-image
  generation.
\newblock \emph{arXiv preprint arXiv:2206.10789}, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{zhang2022opt}
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C.,
  Diab, M., Li, X., Lin, X.~V., et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhang et~al.(2019)Zhang, Han, Liu, Jiang, Sun, and
  Liu]{zhang2019ernie}
Zhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., and Liu, Q.
\newblock Ernie: Enhanced language representation with informative entities.
\newblock In \emph{Association for Computational Linguistics (ACL)}, 2019.

\end{thebibliography}
