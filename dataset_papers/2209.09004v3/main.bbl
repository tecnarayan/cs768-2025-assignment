\begin{thebibliography}{10}

\bibitem{andoni2015practical}
A.~Andoni, P.~Indyk, T.~Laarhoven, I.~Razenshteyn, and L.~Schmidt.
\newblock Practical and optimal lsh for angular distance.
\newblock In {\em NeurIPS}, volume~28, pages 1225--1233, 2015.

\bibitem{bai2021binarybert}
H.~Bai, W.~Zhang, L.~Hou, L.~Shang, J.~Jin, X.~Jiang, Q.~Liu, M.~Lyu, and
  I.~King.
\newblock Binarybert: Pushing the limit of bert quantization.
\newblock In {\em ACL}, pages 4334--4348, 2021.

\bibitem{bai2019proxquant}
Y.~Bai, Y.-X. Wang, and E.~Liberty.
\newblock Proxquant: Quantized neural networks via proximal operators.
\newblock In {\em ICLR}, pages 1--19, 2019.

\bibitem{beltagy2020longformer}
I.~Beltagy, M.~E. Peters, and A.~Cohan.
\newblock Longformer: The long-document transformer.
\newblock {\em arXiv preprint arXiv:2004.05150}, 2020.

\bibitem{bengio2013estimating}
Y.~Bengio.
\newblock Estimating or propagating gradients through stochastic neurons.
\newblock {\em arXiv preprint arXiv:1305.2982}, 2013.

\bibitem{bulat2019xnor}
A.~Bulat and G.~Tzimiropoulos.
\newblock Xnor-net++: Improved binary neural networks.
\newblock In {\em BMVC}, page~62, 2019.

\bibitem{charikar2002similarity}
M.~S. Charikar.
\newblock Similarity estimation techniques from rounding algorithms.
\newblock In {\em STOC}, pages 380--388, 2002.

\bibitem{chen2020addernet}
H.~Chen, Y.~Wang, C.~Xu, B.~Shi, C.~Xu, Q.~Tian, and C.~Xu.
\newblock Addernet: Do we really need multiplications in deep learning?
\newblock In {\em CVPR}, pages 1468--1477, 2020.

\bibitem{choromanski2021rethinking}
K.~M. Choromanski, V.~Likhosherstov, D.~Dohan, X.~Song, A.~Gane, T.~Sarlos,
  P.~Hawkins, J.~Q. Davis, A.~Mohiuddin, L.~Kaiser, D.~B. Belanger, L.~J.
  Colwell, and A.~Weller.
\newblock Rethinking attention with performers.
\newblock In {\em ICLR}, pages 1--38, 2021.

\bibitem{twins}
X.~Chu, Z.~Tian, Y.~Wang, B.~Zhang, H.~Ren, X.~Wei, H.~Xia, and C.~Shen.
\newblock Twins: Revisiting the design of spatial attention in vision
  transformers.
\newblock In {\em NeurIPS}, pages 9355--9366, 2021.

\bibitem{courbariaux2015binaryconnect}
M.~Courbariaux, Y.~Bengio, and J.-P. David.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In {\em NeurIPS}, volume~28, pages 3123--3131, 2015.

\bibitem{daras2020smyrf}
G.~Daras, N.~Kitaev, A.~Odena, and A.~G. Dimakis.
\newblock Smyrf-efficient attention using asymmetric clustering.
\newblock In {\em NeurIPS}, volume~33, pages 6476--6489, 2020.

\bibitem{datar2004locality}
M.~Datar, N.~Immorlica, P.~Indyk, and V.~S. Mirrokni.
\newblock Locality-sensitive hashing scheme based on p-stable distributions.
\newblock In {\em SoCG}, pages 253--262, 2004.

\bibitem{dehghani2018universal}
M.~Dehghani, S.~Gouws, O.~Vinyals, J.~Uszkoreit, and L.~Kaiser.
\newblock Universal transformers.
\newblock In {\em ICLR}, pages 1--23, 2019.

\bibitem{devlin2019bert}
J.~Devlin, M.~Chang, K.~Lee, and K.~Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In J.~Burstein, C.~Doran, and T.~Solorio, editors, {\em NAACL-HLT},
  pages 4171--4186, 2019.

\bibitem{ding2019regularizing}
R.~Ding, T.-W. Chin, Z.~Liu, and D.~Marculescu.
\newblock Regularizing activation distribution for training binarized deep
  networks.
\newblock In {\em CVPR}, pages 11408--11417, 2019.

\bibitem{dosovitskiy2021an}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em ICLR}, pages 1--21, 2021.

\bibitem{esser2020learned}
S.~K. Esser, J.~L. McKinstry, D.~Bablani, R.~Appuswamy, and D.~S. Modha.
\newblock Learned step size quantization.
\newblock In {\em ICLR}, pages 1--12, 2020.

\bibitem{gionis1999similarity}
A.~Gionis, P.~Indyk, and R.~Motwani.
\newblock Similarity search in high dimensions via hashing.
\newblock In {\em Vldb}, page 518â€“529, 1999.

\bibitem{gong2012iterative}
Y.~Gong, S.~Lazebnik, A.~Gordo, and F.~Perronnin.
\newblock Iterative quantization: A procrustean approach to learning binary
  codes for large-scale image retrieval.
\newblock {\em TPAMI}, 35(12):2916--2929, 2012.

\bibitem{han2016eie}
S.~Han, X.~Liu, H.~Mao, J.~Pu, A.~Pedram, M.~A. Horowitz, and W.~J. Dally.
\newblock Eie: Efficient inference engine on compressed deep neural network.
\newblock {\em ACM SIGARCH Computer Architecture News}, 44(3):243--254, 2016.

\bibitem{horowitz20141}
M.~Horowitz.
\newblock 1.1 computing's energy problem (and what we can do about it).
\newblock In {\em ISSCC}, pages 10--14, 2014.

\bibitem{hou2018loss}
L.~Hou and J.~T. Kwok.
\newblock Loss-aware weight quantization of deep networks.
\newblock In {\em ICLR}, pages 1--16, 2018.

\bibitem{hubara2016binarized}
I.~Hubara, M.~Courbariaux, D.~Soudry, R.~El-Yaniv, and Y.~Bengio.
\newblock Binarized neural networks.
\newblock In {\em NeurIPS}, pages 4107--4115, 2016.

\bibitem{hubara2017quantized}
I.~Hubara, M.~Courbariaux, D.~Soudry, R.~El-Yaniv, and Y.~Bengio.
\newblock Quantized neural networks: Training neural networks with low
  precision weights and activations.
\newblock {\em JMLR}, 18(1):6869--6898, 2017.

\bibitem{jaegle2021perceiver}
A.~Jaegle, F.~Gimeno, A.~Brock, O.~Vinyals, A.~Zisserman, and J.~Carreira.
\newblock Perceiver: General perception with iterative attention.
\newblock In {\em ICML}, pages 4651--4664, 2021.

\bibitem{jung2019learning}
S.~Jung, C.~Son, S.~Lee, J.~Son, J.-J. Han, Y.~Kwak, S.~J. Hwang, and C.~Choi.
\newblock Learning to quantize deep networks by optimizing quantization
  intervals with task loss.
\newblock In {\em CVPR}, pages 4350--4359, 2019.

\bibitem{katharopoulos2020transformers}
A.~Katharopoulos, A.~Vyas, N.~Pappas, and F.~Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In {\em ICML}, pages 5156--5165, 2020.

\bibitem{reformer}
N.~Kitaev, L.~Kaiser, and A.~Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In {\em ICLR}, pages 1--12, 2020.

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em NeurIPS}, volume~25, pages 1106--1114, 2012.

\bibitem{kulis2011kernelized}
B.~Kulis and K.~Grauman.
\newblock Kernelized locality-sensitive hashing.
\newblock {\em TPAMI}, 34(6):1092--1104, 2011.

\bibitem{lian2019high}
X.~Lian, Z.~Liu, Z.~Song, J.~Dai, W.~Zhou, and X.~Ji.
\newblock High-performance fpga-based cnn accelerator with block-floating-point
  arithmetic.
\newblock {\em TVLSI}, 27(8):1874--1885, 2019.

\bibitem{lin2014fast}
G.~Lin, C.~Shen, Q.~Shi, A.~Van~den Hengel, and D.~Suter.
\newblock Fast supervised hashing with decision trees for high-dimensional
  data.
\newblock In {\em CVPR}, pages 1963--1970, 2014.

\bibitem{lin2017towards}
X.~Lin, C.~Zhao, and W.~Pan.
\newblock Towards accurate binary convolutional neural network.
\newblock In {\em NeurIPS}, pages 344--352, 2017.

\bibitem{liu2014discrete}
W.~Liu, C.~Mu, S.~Kumar, and S.-F. Chang.
\newblock Discrete graph hashing.
\newblock In {\em NeurIPS}, volume~27, pages 3419--3427, 2014.

\bibitem{liu2012supervised}
W.~Liu, J.~Wang, R.~Ji, Y.-G. Jiang, and S.-F. Chang.
\newblock Supervised hashing with kernels.
\newblock In {\em CVPR}, pages 2074--2081, 2012.

\bibitem{liu2011hashing}
W.~Liu, J.~Wang, S.~Kumar, and S.-F. Chang.
\newblock Hashing with graphs.
\newblock In {\em ICML}, pages 1--8, 2011.

\bibitem{liu2021swin}
Z.~Liu, Y.~Lin, Y.~Cao, H.~Hu, Y.~Wei, Z.~Zhang, S.~Lin, and B.~Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em ICCV}, pages 10012--10022, 2021.

\bibitem{Liu_2018_ECCV}
Z.~Liu, B.~Wu, W.~Luo, X.~Yang, W.~Liu, and K.-T. Cheng.
\newblock Bi-real net: Enhancing the performance of 1-bit cnns with improved
  representational capability and advanced training algorithm.
\newblock In {\em ECCV}, pages 722--737, 2018.

\bibitem{adamw}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em ICLR}, pages 1--18, 2019.

\bibitem{lu2021soft}
J.~Lu, J.~Yao, J.~Zhang, X.~Zhu, H.~Xu, W.~Gao, C.~Xu, T.~Xiang, and L.~Zhang.
\newblock Soft: Softmax-free transformer with linear complexity.
\newblock In {\em NeurIPS}, volume~34, pages 21297--21309, 2021.

\bibitem{martinez2020training}
B.~Martinez, J.~Yang, A.~Bulat, and G.~Tzimiropoulos.
\newblock Training binary neural networks with real-to-binary convolutions.
\newblock In {\em ICLR}, pages 1--11, 2020.

\bibitem{mishra2017apprentice}
A.~Mishra and D.~Marr.
\newblock Apprentice: Using knowledge distillation techniques to improve
  low-precision network accuracy.
\newblock In {\em ICLR}, pages 1--17, 2018.

\bibitem{norouzi2011minimal}
M.~Norouzi and D.~J. Fleet.
\newblock Minimal loss hashing for compact binary codes.
\newblock In {\em ICML}, pages 353--360, 2011.

\bibitem{peng2020random}
H.~Peng, N.~Pappas, D.~Yogatama, R.~Schwartz, N.~Smith, and L.~Kong.
\newblock Random feature attention.
\newblock In {\em ICLR}, pages 1--19, 2021.

\bibitem{qin2022bibert}
H.~Qin, Y.~Ding, M.~Zhang, Q.~YAN, A.~Liu, Q.~Dang, Z.~Liu, and X.~Liu.
\newblock Bi{BERT}: Accurate fully binarized {BERT}.
\newblock In {\em ICLR}, pages 1--24, 2022.

\bibitem{zhen2022cosformer}
Z.~Qin, W.~Sun, H.~Deng, D.~Li, Y.~Wei, B.~Lv, J.~Yan, L.~Kong, and Y.~Zhong.
\newblock cosformer: Rethinking softmax in attention.
\newblock In {\em ICLR}, pages 1--15, 2022.

\bibitem{NIPS2007_013a006f}
A.~Rahimi and B.~Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em NeurIPS}, volume~20, pages 1177--1184, 2007.

\bibitem{rastegari2016xnor}
M.~Rastegari, V.~Ordonez, J.~Redmon, and A.~Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In {\em ECCV}, pages 525--542, 2016.

\bibitem{ren2021combiner}
H.~Ren, H.~Dai, Z.~Dai, M.~Yang, J.~Leskovec, D.~Schuurmans, and B.~Dai.
\newblock Combiner: Full attention transformer with sparse computation cost.
\newblock In {\em NeurIPS}, volume~34, pages 22470--22482, 2021.

\bibitem{sharma2018bit}
H.~Sharma, J.~Park, N.~Suda, L.~Lai, B.~Chau, V.~Chandra, and H.~Esmaeilzadeh.
\newblock Bit fusion: Bit-level dynamically composable architecture for
  accelerating deep neural network.
\newblock In {\em ISCA}, pages 764--775, 2018.

\bibitem{shen2020q}
S.~Shen, Z.~Dong, J.~Ye, L.~Ma, Z.~Yao, A.~Gholami, M.~W. Mahoney, and
  K.~Keutzer.
\newblock Q-bert: Hessian based ultra low precision quantization of bert.
\newblock In {\em AAAI}, volume~34, pages 8815--8821, 2020.

\bibitem{shu2021adder}
H.~Shu, J.~Wang, H.~Chen, L.~Li, Y.~Yang, and Y.~Wang.
\newblock Adder attention for vision transformer.
\newblock In {\em NeurIPS}, volume~34, pages 19899--19909, 2021.

\bibitem{sun2021sparse}
Z.~Sun, Y.~Yang, and S.~Yoo.
\newblock Sparse attention with learning to hash.
\newblock In {\em ICLR}, pages 1--20, 2022.

\bibitem{tay2020long}
Y.~Tay, M.~Dehghani, S.~Abnar, Y.~Shen, D.~Bahri, P.~Pham, J.~Rao, L.~Yang,
  S.~Ruder, and D.~Metzler.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock In {\em ICLR}, pages 1--19, 2021.

\bibitem{touvron2021training}
H.~Touvron, M.~Cord, M.~Douze, F.~Massa, A.~Sablayrolles, and H.~J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In {\em ICML}, pages 10347--10357, 2021.

\bibitem{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez,
  {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NeurIPS}, volume~30, pages 5998--6008, 2017.

\bibitem{vyas2020fast}
A.~Vyas, A.~Katharopoulos, and F.~Fleuret.
\newblock Fast transformers with clustered attention.
\newblock In {\em NeurIPS}, volume~33, pages 21665--21674, 2020.

\bibitem{wang2012semi}
J.~Wang, S.~Kumar, and S.-F. Chang.
\newblock Semi-supervised hashing for large-scale search.
\newblock {\em TPAMI}, 34(12):2393--2406, 2012.

\bibitem{linformer}
S.~Wang, B.~Li, M.~Khabsa, H.~Fang, and H.~Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock {\em arXiv preprint arXiv:2006.04768}, 2020.

\bibitem{wang2021pyramid}
W.~Wang, E.~Xie, X.~Li, D.-P. Fan, K.~Song, D.~Liang, T.~Lu, P.~Luo, and
  L.~Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock In {\em ICCV}, pages 568--578, 2021.

\bibitem{pvtv2}
W.~Wang, E.~Xie, X.~Li, D.-P. Fan, K.~Song, D.~Liang, T.~Lu, P.~Luo, and
  L.~Shao.
\newblock Pvtv2: Improved baselines with pyramid vision transformer.
\newblock {\em Computational Visual Media}, 8(3):415--424, 2022.

\bibitem{weiss2008spectral}
Y.~Weiss, A.~Torralba, and R.~Fergus.
\newblock Spectral hashing.
\newblock In {\em NeurIPS}, volume~21, pages 1--8, 2008.

\bibitem{xiong2021nystromformer}
Y.~Xiong, Z.~Zeng, R.~Chakraborty, M.~Tan, G.~Fung, Y.~Li, and V.~Singh.
\newblock Nystr{\"o}mformer: A nyst{\"o}m-based algorithm for approximating
  self-attention.
\newblock In {\em AAAI}, volume~35, page 14138, 2021.

\bibitem{yan2020deep}
C.~Yan, B.~Gong, Y.~Wei, and Y.~Gao.
\newblock Deep multi-view enhancement hashing for image retrieval.
\newblock {\em TPAMI}, 43(4):1445--1451, 2020.

\bibitem{zadeh2020gobo}
A.~H. Zadeh, I.~Edo, O.~M. Awad, and A.~Moshovos.
\newblock Gobo: Quantizing attention-based nlp models for low latency and
  energy efficient inference.
\newblock In {\em MICRO}, pages 811--824, 2020.

\bibitem{zafrir2019q8bert}
O.~Zafrir, G.~Boudoukh, P.~Izsak, and M.~Wasserblat.
\newblock Q8bert: Quantized 8bit bert.
\newblock In {\em EMC2-NIPS}, pages 36--39, 2019.

\bibitem{zhou2016dorefa}
S.~Zhou, Y.~Wu, Z.~Ni, X.~Zhou, H.~Wen, and Y.~Zou.
\newblock Dorefa-net: Training low bitwidth convolutional neural networks with
  low bitwidth gradients.
\newblock {\em arXiv preprint arXiv:1606.06160}, 2016.

\bibitem{zhu1997algorithm}
C.~Zhu, R.~H. Byrd, P.~Lu, and J.~Nocedal.
\newblock Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale
  bound-constrained optimization.
\newblock {\em ACM TOMS}, 23(4):550--560, 1997.

\bibitem{zhu2021long}
C.~Zhu, W.~Ping, C.~Xiao, M.~Shoeybi, T.~Goldstein, A.~Anandkumar, and
  B.~Catanzaro.
\newblock Long-short transformer: Efficient transformers for language and
  vision.
\newblock In {\em NeurIPS}, volume~34, pages 17723--17736, 2021.

\bibitem{zhuang2016fast}
B.~Zhuang, G.~Lin, C.~Shen, and I.~Reid.
\newblock Fast training of triplet-based deep binary embedding networks.
\newblock In {\em CVPR}, pages 5955--5964, 2016.

\bibitem{zhuang2019structured}
B.~Zhuang, C.~Shen, M.~Tan, L.~Liu, and I.~Reid.
\newblock Structured binary neural network for accurate image classification
  and semantic segmentation.
\newblock In {\em CVPR}, pages 413--422, 2019.

\end{thebibliography}
