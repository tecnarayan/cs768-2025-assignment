\begin{thebibliography}{xx}

\harvarditem{Allen-Zhu}{2016}{allen2016katyusha}
Allen-Zhu, Z.  \harvardyearleft 2016\harvardyearright , `Katyusha: The first
  direct acceleration of stochastic gradient methods', {\em arXiv preprint
  arXiv:1603.05953} .

\harvarditem{Anderson}{1965}{anderson1965iterative}
Anderson, D.~G.  \harvardyearleft 1965\harvardyearright , `Iterative procedures
  for nonlinear integral equations', {\em Journal of the ACM (JACM)} {\bf
  12}(4),~547--560.

\harvarditem{Cabay \harvardand\ Jackson}{1976}{Caba76}
Cabay, S. \harvardand\ Jackson, L.  \harvardyearleft 1976\harvardyearright , `A
  polynomial extrapolation method for finding limits and antilimits of vector
  sequences', {\em SIAM Journal on Numerical Analysis} {\bf 13}(5),~734--752.

\harvarditem[Defazio et~al.]{Defazio, Bach \harvardand\
  Lacoste-Julien}{2014}{defazio2014saga}
Defazio, A., Bach, F. \harvardand\ Lacoste-Julien, S.  \harvardyearleft
  2014\harvardyearright , Saga: A fast incremental gradient method with support
  for non-strongly convex composite objectives, {\em in} `Advances in Neural
  Information Processing Systems', pp.~1646--1654.

\harvarditem{D{\'e}fossez \harvardand\ Bach}{2015}{defossez2015averaged}
D{\'e}fossez, A. \harvardand\ Bach, F.  \harvardyearleft 2015\harvardyearright
  , Averaged least-mean-squares: Bias-variance trade-offs and optimal sampling
  distributions, {\em in} `Artificial Intelligence and Statistics',
  pp.~205--213.

\harvarditem{Fercoq \harvardand\ Qu}{2016}{fercoq2016restarting}
Fercoq, O. \harvardand\ Qu, Z.  \harvardyearleft 2016\harvardyearright ,
  `Restarting accelerated gradient methods with a rough strong convexity
  estimate', {\em arXiv preprint arXiv:1609.07358} .

\harvarditem{Flammarion \harvardand\ Bach}{2015}{flammarion2015averaging}
Flammarion, N. \harvardand\ Bach, F.  \harvardyearleft 2015\harvardyearright ,
  From averaging to acceleration, there is only a step-size, {\em in}
  `Conference on Learning Theory', pp.~658--695.

\harvarditem[Howard et~al.]{Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto \harvardand\ Adam}{2017}{howard2017mobilenets}
Howard, A.~G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T.,
  Andreetto, M. \harvardand\ Adam, H.  \harvardyearleft 2017\harvardyearright ,
  `Mobilenets: Efficient convolutional neural networks for mobile vision
  applications', {\em arXiv preprint arXiv:1704.04861} .

\harvarditem[Jain et~al.]{Jain, Kakade, Kidambi, Netrapalli \harvardand\
  Sidford}{2016}{jain2016parallelizing}
Jain, P., Kakade, S.~M., Kidambi, R., Netrapalli, P. \harvardand\ Sidford, A.
  \harvardyearleft 2016\harvardyearright , `Parallelizing stochastic
  approximation through mini-batching and tail-averaging', {\em arXiv preprint
  arXiv:1610.03774} .

\harvarditem{Johnson \harvardand\ Zhang}{2013}{johnson2013accelerating}
Johnson, R. \harvardand\ Zhang, T.  \harvardyearleft 2013\harvardyearright ,
  Accelerating stochastic gradient descent using predictive variance reduction,
  {\em in} `Advances in Neural Information Processing Systems', pp.~315--323.

\harvarditem[LeCun et~al.]{LeCun, Bottou, Bengio \harvardand\
  Haffner}{1998}{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y. \harvardand\ Haffner, P.  \harvardyearleft
  1998\harvardyearright , `Gradient-based learning applied to document
  recognition', {\em Proceedings of the IEEE} {\bf 86}(11),~2278--2324.

\harvarditem[Lin et~al.]{Lin, Mairal \harvardand\
  Harchaoui}{2015}{lin2015universal}
Lin, H., Mairal, J. \harvardand\ Harchaoui, Z.  \harvardyearleft
  2015\harvardyearright , A universal catalyst for first-order optimization,
  {\em in} `Advances in Neural Information Processing Systems', pp.~3384--3392.

\harvarditem{Me{\v{s}}ina}{1977}{Mesi77}
Me{\v{s}}ina, M.  \harvardyearleft 1977\harvardyearright , `Convergence
  acceleration for the iterative solution of the equations x= ax+ f', {\em
  Computer Methods in Applied Mechanics and Engineering} {\bf 10}(2),~165--173.

\harvarditem{Moulines \harvardand\ Bach}{2011}{moulines2011non}
Moulines, E. \harvardand\ Bach, F.~R.  \harvardyearleft 2011\harvardyearright ,
  Non-asymptotic analysis of stochastic approximation algorithms for machine
  learning, {\em in} `Advances in Neural Information Processing Systems',
  pp.~451--459.

\harvarditem{Nedi{\'c} \harvardand\ Bertsekas}{2001}{nedic2001convergence}
Nedi{\'c}, A. \harvardand\ Bertsekas, D.  \harvardyearleft
  2001\harvardyearright , Convergence rate of incremental subgradient
  algorithms, {\em in} `Stochastic optimization: algorithms and applications',
  Springer, pp.~223--264.

\harvarditem{Nesterov}{2013}{Nest03a}
Nesterov, Y.  \harvardyearleft 2013\harvardyearright , {\em Introductory
  lectures on convex optimization: A basic course}, Vol.~87, Springer Science
  \& Business Media.

\harvarditem[Schmidt et~al.]{Schmidt, Le~Roux \harvardand\
  Bach}{2013}{schmidt2013minimizing}
Schmidt, M., Le~Roux, N. \harvardand\ Bach, F.  \harvardyearleft
  2013\harvardyearright , `Minimizing finite sums with the stochastic average
  gradient', {\em Mathematical Programming} pp.~1--30.

\harvarditem[Scieur et~al.]{Scieur, d'Aspremont \harvardand\
  Bach}{2016}{scieur2016regularized}
Scieur, D., d'Aspremont, A. \harvardand\ Bach, F.  \harvardyearleft
  2016\harvardyearright , Regularized nonlinear acceleration, {\em in}
  `Advances In Neural Information Processing Systems', pp.~712--720.

\harvarditem{Shalev-Shwartz \harvardand\ Zhang}{2013}{shalev2013stochastic}
Shalev-Shwartz, S. \harvardand\ Zhang, T.  \harvardyearleft
  2013\harvardyearright , `Stochastic dual coordinate ascent methods for
  regularized loss minimization', {\em Journal of Machine Learning Research}
  {\bf 14}(Feb),~567--599.

\harvarditem{Shalev-Shwartz \harvardand\ Zhang}{2014}{shalev2014accelerated}
Shalev-Shwartz, S. \harvardand\ Zhang, T.  \harvardyearleft
  2014\harvardyearright , Accelerated proximal stochastic dual coordinate
  ascent for regularized loss minimization., {\em in} `ICML', pp.~64--72.

\harvarditem{Zagoruyko \harvardand\ Komodakis}{2016}{zagoruyko2016wide}
Zagoruyko, S. \harvardand\ Komodakis, N.  \harvardyearleft
  2016\harvardyearright , `Wide residual networks', {\em arXiv preprint
  arXiv:1605.07146} .

\end{thebibliography}
