\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Barbu et~al.(2019)Barbu, Mayo, Alverio, Luo, Wang, Gutfreund,
  Tenenbaum, and Katz]{objectnet}
Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum,
  J., and Katz, B.
\newblock Objectnet: A large-scale bias-controlled dataset for pushing the
  limits of object recognition models.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Bossard et~al.(2014)Bossard, Guillaumin, and Van~Gool]{food101}
Bossard, L., Guillaumin, M., and Van~Gool, L.
\newblock Food-101 -- mining discriminative components with random forests.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2014.

\bibitem[Bujwid \& Sullivan(2021)Bujwid and Sullivan]{bujwid2021large}
Bujwid, S. and Sullivan, J.
\newblock Large-scale zero-shot image classification from rich and diverse
  textual descriptions.
\newblock In \emph{Proceedings of the Third Workshop on Beyond Vision and
  LANguage: inTEgrating Real-world kNowledge (LANTERN)}, 2021.

\bibitem[Cao et~al.(2020)Cao, Lu, Cui, and Zhang]{CAO2020107488}
Cao, Z., Lu, J., Cui, S., and Zhang, C.
\newblock Zero-shot handwritten chinese character recognition with hierarchical
  decomposition embedding.
\newblock \emph{Pattern Recognition}, 2020.

\bibitem[Chalkidis et~al.(2020)Chalkidis, Fergadiotis, Kotitsas, Malakasiotis,
  Aletras, and Androutsopoulos]{chalkidis2020empirical}
Chalkidis, I., Fergadiotis, M., Kotitsas, S., Malakasiotis, P., Aletras, N.,
  and Androutsopoulos, I.
\newblock An empirical study on large-scale multi-label text classification
  including few and zero-shot labels.
\newblock \emph{arXiv preprint arXiv:2010.01653}, 2020.

\bibitem[Chen et~al.(2021)Chen, Xie, Liu, Peng, Sun, Li, You, and
  Shao]{chen2021hsva}
Chen, S., Xie, G., Liu, Y., Peng, Q., Sun, B., Li, H., You, X., and Shao, L.
\newblock Hsva: Hierarchical semantic-visual adaptation for zero-shot learning.
\newblock 2021.

\bibitem[Cheng et~al.(2017)Cheng, Han, and Lu]{resisc45}
Cheng, G., Han, J., and Lu, X.
\newblock Remote sensing image scene classification: Benchmark and state of the
  art.
\newblock \emph{Proceedings of the IEEE}, 2017.

\bibitem[Cho et~al.(2022)Cho, Yoon, Kale, Dernoncourt, Bui, and
  Bansal]{Cho2022CLIPReward}
Cho, J., Yoon, S., Kale, A., Dernoncourt, F., Bui, T., and Bansal, M.
\newblock Fine-grained image captioning with clip reward.
\newblock In \emph{Findings of NAACL}, 2022.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR)}, 2009.

\bibitem[Dimitrovski et~al.(2011)Dimitrovski, Kocev, Loskovska, and
  Džeroski]{DIMITROVSKI20112436}
Dimitrovski, I., Kocev, D., Loskovska, S., and Džeroski, S.
\newblock Hierarchical annotation of medical images.
\newblock \emph{Pattern Recognition}, 2011.

\bibitem[Ding et~al.(2022)Ding, Liu, Tian, Yang, and Ding]{ding2022conti}
Ding, Y., Liu, L., Tian, C., Yang, J., and Ding, H.
\newblock Don't stop learning: Towards continual learning for the clip model,
  2022.

\bibitem[Gadre et~al.(2022)Gadre, Wortsman, Ilharco, Schmidt, and
  Song]{gadre2022clip}
Gadre, S.~Y., Wortsman, M., Ilharco, G., Schmidt, L., and Song, S.
\newblock Clip on wheels: Zero-shot object navigation as object localization
  and exploration.
\newblock \emph{arXiv preprint arXiv:2203.10421}, 2022.

\bibitem[Gao et~al.(2021)Gao, Geng, Zhang, Ma, Fang, Zhang, Li, and
  Qiao]{gao2021clip}
Gao, P., Geng, S., Zhang, R., Ma, T., Fang, R., Zhang, Y., Li, H., and Qiao, Y.
\newblock Clip-adapter: Better vision-language models with feature adapters.
\newblock \emph{arXiv preprint arXiv:2110.04544}, 2021.

\bibitem[Gao et~al.(2020)Gao, Fisch, and Chen]{gao2020making}
Gao, T., Fisch, A., and Chen, D.
\newblock Making pre-trained language models better few-shot learners.
\newblock \emph{arXiv preprint arXiv:2012.15723}, 2020.

\bibitem[Helber et~al.(2018)Helber, Bischke, Dengel, and Borth]{eurosat2}
Helber, P., Bischke, B., Dengel, A., and Borth, D.
\newblock Introducing eurosat: A novel dataset and deep learning benchmark for
  land use and land cover classification.
\newblock 2018.

\bibitem[Helber et~al.(2019)Helber, Bischke, Dengel, and Borth]{eurosat1}
Helber, P., Bischke, B., Dengel, A., and Borth, D.
\newblock Eurosat: A novel dataset and deep learning benchmark for land use and
  land cover classification.
\newblock \emph{IEEE Journal of Selected Topics in Applied Earth Observations
  and Remote Sensing}, 2019.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019parameter}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De~Laroussilhe, Q.,
  Gesmundo, A., Attariyan, M., and Gelly, S.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Huang et~al.(2022)Huang, Chu, and Wei]{huang2022upl}
Huang, T., Chu, J., and Wei, F.
\newblock Unsupervised prompt learning for vision-language models, 2022.

\bibitem[Ilharco et~al.(2022)Ilharco, Wortsman, Gadre, Song, Hajishirzi,
  Kornblith, Farhadi, and Schmidt]{ilharco2022patching}
Ilharco, G., Wortsman, M., Gadre, S.~Y., Song, S., Hajishirzi, H., Kornblith,
  S., Farhadi, A., and Schmidt, L.
\newblock Patching open-vocabulary models by interpolating weights.
\newblock \emph{arXiv preprint arXiv:2208.05592}, 2022.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and
  Duerig]{jia2021scaling}
Jia, C., Yang, Y., Xia, Y., Chen, Y.-T., Parekh, Z., Pham, H., Le, Q., Sung,
  Y.-H., Li, Z., and Duerig, T.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Jia et~al.(2022)Jia, Tang, Chen, Cardie, Belongie, Hariharan, and
  Lim]{jia2022visual}
Jia, M., Tang, L., Chen, B.-C., Cardie, C., Belongie, S., Hariharan, B., and
  Lim, S.-N.
\newblock Visual prompt tuning.
\newblock \emph{arXiv preprint arXiv:2203.12119}, 2022.

\bibitem[Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain,
  Perez, Schiefer, Dodds, DasSarma, Tran-Johnson, Johnston, El-Showk, Jones,
  Elhage, Hume, Chen, Bai, Bowman, Fort, Ganguli, Hernandez, Jacobson, Kernion,
  Kravec, Lovitt, Ndousse, Olsson, Ringer, Amodei, Brown, Clark, Joseph, Mann,
  McCandlish, Olah, and Kaplan]{kadavath2022know}
Kadavath, S., Conerly, T., Askell, A., Henighan, T., Drain, D., Perez, E.,
  Schiefer, N., Dodds, Z.~H., DasSarma, N., Tran-Johnson, E., Johnston, S.,
  El-Showk, S., Jones, A., Elhage, N., Hume, T., Chen, A., Bai, Y., Bowman, S.,
  Fort, S., Ganguli, D., Hernandez, D., Jacobson, J., Kernion, J., Kravec, S.,
  Lovitt, L., Ndousse, K., Olsson, C., Ringer, S., Amodei, D., Brown, T.,
  Clark, J., Joseph, N., Mann, B., McCandlish, S., Olah, C., and Kaplan, J.
\newblock Language models (mostly) know what they know, 2022.

\bibitem[Krizhevsky(2009)]{cifar100}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Kuo \& Chen(2022)Kuo and Chen]{kuo2022qa}
Kuo, C.-C. and Chen, K.-Y.
\newblock Toward zero-shot and zero-resource multilingual question answering.
\newblock \emph{IEEE Access}, 2022.

\bibitem[Larochelle et~al.(2008)Larochelle, Erhan, and
  Bengio]{larochelle2008zero}
Larochelle, H., Erhan, D., and Bengio, Y.
\newblock Zero-data learning of new tasks.
\newblock In \emph{Association for the Advancement of Artificial Intelligence
  (AAAI)}, 2008.

\bibitem[Liu et~al.(2021)Liu, Zhang, Yin, and Zhu]{liu2021improving}
Liu, H., Zhang, D., Yin, B., and Zhu, X.
\newblock Improving pretrained models for zero-shot multi-label text
  classification through reinforced label hierarchy reasoning.
\newblock \emph{arXiv preprint arXiv:2104.01666}, 2021.

\bibitem[Mensink et~al.(2014)Mensink, Gavves, and Snoek]{mensink2014costa}
Mensink, T., Gavves, E., and Snoek, C.~G.
\newblock Costa: Co-occurrence statistics for zero-shot classification.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, 2014.

\bibitem[Minderer et~al.(2021)Minderer, Djolonga, Romijnders, Hubis, Zhai,
  Houlsby, Tran, and Lucic]{minderer2021revisiting}
Minderer, M., Djolonga, J., Romijnders, R., Hubis, F., Zhai, X., Houlsby, N.,
  Tran, D., and Lucic, M.
\newblock Revisiting the calibration of modern neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Mureşan \& Oltean(2018)Mureşan and Oltean]{fruits360}
Mureşan, H. and Oltean, M.
\newblock Fruit recognition from images using deep learning.
\newblock \emph{Acta Universitatis Sapientiae, Informatica}, 2018.

\bibitem[Pham et~al.(2021)Pham, Dai, Ghiasi, Kawaguchi, Liu, Yu, Yu, Chen,
  Luong, Wu, et~al.]{pham2021scaling}
Pham, H., Dai, Z., Ghiasi, G., Kawaguchi, K., Liu, H., Yu, A.~W., Yu, J., Chen,
  Y.-T., Luong, M.-T., Wu, Y., et~al.
\newblock Combined scaling for open-vocabulary image classification.
\newblock \emph{arXiv preprint arXiv: 2111.10050}, 2021.

\bibitem[Pratt et~al.(2022)Pratt, Liu, and Farhadi]{rosanne2022cupl}
Pratt, S., Liu, R., and Farhadi, A.
\newblock What does a platypus look like? generating customized prompts for
  zero-shot image classification, 2022.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Ren et~al.(2022)Ren, Li, Ren, Zhao, and Sun]{ren2022open}
Ren, S., Li, L., Ren, X., Zhao, G., and Sun, X.
\newblock Rethinking the openness of clip, 2022.

\bibitem[Saenko et~al.(2010)Saenko, Kulis, Fritz, and Darrell]{office31}
Saenko, K., Kulis, B., Fritz, M., and Darrell, T.
\newblock Adapting visual category models to new domains.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2010.

\bibitem[Santurkar et~al.(2021)Santurkar, Tsipras, and
  Madry]{santurkar2020breeds}
Santurkar, S., Tsipras, D., and Madry, A.
\newblock Breeds: Benchmarks for subpopulation shift.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Santurkar et~al.(2022)Santurkar, Dubois, Taori, Liang, and
  Hashimoto]{santurkar2022clip}
Santurkar, S., Dubois, Y., Taori, R., Liang, P., and Hashimoto, T.
\newblock Is a caption worth a thousand images? a controlled study for
  representation learning, 2022.

\bibitem[Shen et~al.(2021)Shen, Li, Tan, Bansal, Rohrbach, Chang, Yao, and
  Keutzer]{shen2021much}
Shen, S., Li, L.~H., Tan, H., Bansal, M., Rohrbach, A., Chang, K.-W., Yao, Z.,
  and Keutzer, K.
\newblock How much can clip benefit vision-and-language tasks?
\newblock \emph{arXiv preprint arXiv:2107.06383}, 2021.

\bibitem[Shen et~al.(2022)Shen, Li, Hu, Xie, Yang, Zhang, Rohrbach, Gan, Wang,
  Yuan, Liu, Keutzer, Darrell, and Gao]{shen2022klite}
Shen, S., Li, C., Hu, X., Xie, Y., Yang, J., Zhang, P., Rohrbach, A., Gan, Z.,
  Wang, L., Yuan, L., Liu, C., Keutzer, K., Darrell, T., and Gao, J.
\newblock K-lite: Learning transferable visual models with external knowledge,
  2022.

\bibitem[Silla \& Freitas(2010)Silla and Freitas]{Silla2010ASO}
Silla, C.~N. and Freitas, A.~A.
\newblock A survey of hierarchical classification across different application
  domains.
\newblock \emph{Data Mining and Knowledge Discovery}, 2010.

\bibitem[Tewel et~al.(2021)Tewel, Shalev, Schwartz, and Wolf]{tewel2021i2t}
Tewel, Y., Shalev, Y., Schwartz, I., and Wolf, L.
\newblock Zerocap: Zero-shot image-to-text generation for visual-semantic
  arithmetic, 2021.

\bibitem[Venkateswara et~al.(2017)Venkateswara, Eusebio, Chakraborty, and
  Panchanathan]{officehome}
Venkateswara, H., Eusebio, J., Chakraborty, S., and Panchanathan, S.
\newblock Deep hashing network for unsupervised domain adaptation.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2017.

\bibitem[Wortsman et~al.(2021)Wortsman, Ilharco, Kim, Li, Kornblith, Roelofs,
  Gontijo-Lopes, Hajishirzi, Farhadi, Namkoong, and
  Schmidt]{wortsman2021robust}
Wortsman, M., Ilharco, G., Kim, J.~W., Li, M., Kornblith, S., Roelofs, R.,
  Gontijo-Lopes, R., Hajishirzi, H., Farhadi, A., Namkoong, H., and Schmidt, L.
\newblock Robust fine-tuning of zero-shot models.
\newblock \emph{arXiv preprint arXiv:2109.01903}, 2021.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes,
  Morcos, Namkoong, Farhadi, Carmon, Kornblith, et~al.]{modelsoup}
Wortsman, M., Ilharco, G., Gadre, S.~Y., Roelofs, R., Gontijo-Lopes, R.,
  Morcos, A.~S., Namkoong, H., Farhadi, A., Carmon, Y., Kornblith, S., et~al.
\newblock Model soups: averaging weights of multiple fine-tuned models improves
  accuracy without increasing inference time.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  23965--23998. PMLR, 2022.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{fashionmnist}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem[{Xiao} et~al.(2010){Xiao}, {Hays}, {Ehinger}, {Oliva}, and
  {Torralba}]{sun397}
{Xiao}, J., {Hays}, J., {Ehinger}, K.~A., {Oliva}, A., and {Torralba}, A.
\newblock Sun database: Large-scale scene recognition from abbey to zoo.
\newblock In \emph{2010 IEEE Computer Society Conference on Computer Vision and
  Pattern Recognition}, 2010.

\bibitem[Xiao et~al.(2015)Xiao, Xia, Yang, Huang, and Wang]{fashion1M}
Xiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2015.

\bibitem[Yi et~al.(2022)Yi, Shen, Gou, and Elhoseiny]{yi2022exploring}
Yi, K., Shen, X., Gou, Y., and Elhoseiny, M.
\newblock Exploring hierarchical graph representation for large-scale zero-shot
  image classification.
\newblock \emph{arXiv preprint arXiv:2203.01386}, 2022.

\bibitem[Yu et~al.(2015)Yu, Seff, Zhang, Song, Funkhouser, and Xiao]{lsun}
Yu, F., Seff, A., Zhang, Y., Song, S., Funkhouser, T., and Xiao, J.
\newblock Lsun: Construction of a large-scale image dataset using deep learning
  with humans in the loop, 2015.

\bibitem[Yu et~al.(2022)Yu, Chung, Yun, Hessel, Park, Lu, Ammanabrolu, Zellers,
  Bras, Kim, and Choi]{yu2022esper}
Yu, Y., Chung, J., Yun, H., Hessel, J., Park, J., Lu, X., Ammanabrolu, P.,
  Zellers, R., Bras, R.~L., Kim, G., and Choi, Y.
\newblock Multimodal knowledge alignment with reinforcement learning, 2022.

\bibitem[Zeng et~al.(2022)Zeng, Attarian, Ichter, Choromanski, Wong, Welker,
  Tombari, Purohit, Ryoo, Sindhwani, Lee, Vanhoucke, and
  Florence]{zeng2022socraticmodels}
Zeng, A., Attarian, M., Ichter, B., Choromanski, K., Wong, A., Welker, S.,
  Tombari, F., Purohit, A., Ryoo, M., Sindhwani, V., Lee, J., Vanhoucke, V.,
  and Florence, P.
\newblock Socratic models: Composing zero-shot multimodal reasoning with
  language.
\newblock \emph{arXiv}, 2022.

\bibitem[Zhai et~al.(2022)Zhai, Wang, Mustafa, Steiner, Keysers, Kolesnikov,
  and Beyer]{zhai2022lit}
Zhai, X., Wang, X., Mustafa, B., Steiner, A., Keysers, D., Kolesnikov, A., and
  Beyer, L.
\newblock Lit: Zero-shot transfer with locked-image text tuning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2022.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Fang, Gao, Zhang, Li, Dai,
  Qiao, and Li]{zhang2021tip}
Zhang, R., Fang, R., Gao, P., Zhang, W., Li, K., Dai, J., Qiao, Y., and Li, H.
\newblock Tip-adapter: Training-free clip-adapter for better vision-language
  modeling.
\newblock \emph{arXiv preprint arXiv:2111.03930}, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Guo, Zhang, Li, Miao, Cui,
  Qiao, Gao, and Li]{zhang2021pointclip}
Zhang, R., Guo, Z., Zhang, W., Li, K., Miao, X., Cui, B., Qiao, Y., Gao, P.,
  and Li, H.
\newblock Pointclip: Point cloud understanding by clip.
\newblock \emph{arXiv preprint arXiv:2112.02413}, 2021{\natexlab{b}}.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Yang, Loy, and
  Liu]{zhou2022cocoop}
Zhou, K., Yang, J., Loy, C.~C., and Liu, Z.
\newblock Conditional prompt learning for vision-language models.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2022{\natexlab{a}}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, Yang, Loy, and Liu]{zhou2022coop}
Zhou, K., Yang, J., Loy, C.~C., and Liu, Z.
\newblock Learning to prompt for vision-language models.
\newblock \emph{International Journal of Computer Vision (IJCV)},
  2022{\natexlab{b}}.

\end{thebibliography}
