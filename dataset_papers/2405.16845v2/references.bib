@article{DBLP:journals/jmlr/NADE,
  author       = {Benigno Uria and
                  Marc{-}Alexandre C{\^{o}}t{\'{e}} and
                  Karol Gregor and
                  Iain Murray and
                  Hugo Larochelle},
  title        = {Neural Autoregressive Distribution Estimation},
  journal      = {Journal of Machine Learning Research},
  volume       = {17},
  pages        = {205:1--205:37},
  year         = {2016}
}

@inproceedings{DBLP:conf/nips/gpt3,
  author       = {Tom B. Brown and
                  Benjamin Mann and
                  Nick Ryder and
                  Melanie Subbiah and
                  Jared Kaplan and
                  Prafulla Dhariwal and
                  Arvind Neelakantan and
                  Pranav Shyam and
                  Girish Sastry and
                  Amanda Askell and
                  Sandhini Agarwal and
                  Ariel Herbert{-}Voss and
                  Gretchen Krueger and
                  Tom Henighan and
                  Rewon Child and
                  Aditya Ramesh and
                  Daniel M. Ziegler and
                  Jeffrey Wu and
                  Clemens Winter and
                  Christopher Hesse and
                  Mark Chen and
                  Eric Sigler and
                  Mateusz Litwin and
                  Scott Gray and
                  Benjamin Chess and
                  Jack Clark and
                  Christopher Berner and
                  Sam McCandlish and
                  Alec Radford and
                  Ilya Sutskever and
                  Dario Amodei},
  title        = {Language Models are Few-Shot Learners},
  booktitle    = {NeurIPS},
  year         = {2020}
}

@article{DBLP:journals/corr/gpt4,
  author       = {OpenAI},
  title        = {{GPT-4} Technical Report},
  journal      = {CoRR},
  volume       = {abs/2303.08774},
  year         = {2023}
}

@article{DBLP:journals/corr/llama,
  author       = {Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert and
                  Amjad Almahairi and
                  Yasmine Babaei and
                  Nikolay Bashlykov and
                  Soumya Batra and
                  Prajjwal Bhargava and
                  Shruti Bhosale and
                  Dan Bikel and
                  Lukas Blecher and
                  Cristian Canton{-}Ferrer and
                  Moya Chen and
                  Guillem Cucurull and
                  David Esiobu and
                  Jude Fernandes and
                  Jeremy Fu and
                  Wenyin Fu and
                  Brian Fuller and
                  Cynthia Gao and
                  Vedanuj Goswami and
                  Naman Goyal and
                  Anthony Hartshorn and
                  Saghar Hosseini and
                  Rui Hou and
                  Hakan Inan and
                  Marcin Kardas and
                  Viktor Kerkez and
                  Madian Khabsa and
                  Isabel Kloumann and
                  Artem Korenev and
                  Punit Singh Koura and
                  Marie{-}Anne Lachaux and
                  Thibaut Lavril and
                  Jenya Lee and
                  Diana Liskovich and
                  Yinghai Lu and
                  Yuning Mao and
                  Xavier Martinet and
                  Todor Mihaylov and
                  Pushkar Mishra and
                  Igor Molybog and
                  Yixin Nie and
                  Andrew Poulton and
                  Jeremy Reizenstein and
                  Rashi Rungta and
                  Kalyan Saladi and
                  Alan Schelten and
                  Ruan Silva and
                  Eric Michael Smith and
                  Ranjan Subramanian and
                  Xiaoqing Ellen Tan and
                  Binh Tang and
                  Ross Taylor and
                  Adina Williams and
                  Jian Xiang Kuan and
                  Puxin Xu and
                  Zheng Yan and
                  Iliyan Zarov and
                  Yuchen Zhang and
                  Angela Fan and
                  Melanie Kambadur and
                  Sharan Narang and
                  Aur{\'{e}}lien Rodriguez and
                  Robert Stojnic and
                  Sergey Edunov and
                  Thomas Scialom},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023}
}


@article{DBLP:journals/corr/mesa-opt,
  author       = {Johannes von Oswald and
                  Eyvind Niklasson and
                  Maximilian Schlegel and
                  Seijin Kobayashi and
                  Nicolas Zucchet and
                  Nino Scherrer and
                  Nolan Miller and
                  Mark Sandler and
                  Blaise Ag{\"{u}}era y Arcas and
                  Max Vladymyrov and
                  Razvan Pascanu and
                  Jo{\~{a}}o Sacramento},
  title        = {Uncovering mesa-optimization algorithms in Transformers},
  journal      = {CoRR},
  volume       = {abs/2309.05858},
  year         = {2023}
}

@inproceedings{DBLP:journals/corr/taiji-AR,
  author       = {Michael Eli Sander and
                  Raja Giryes and
                  Taiji Suzuki and
                  Mathieu Blondel and
                  Gabriel Peyr{\'{e}}},
  title        = {How do Transformers Perform In-Context Autoregressive Learning ?},
  booktitle    = {{ICML}},
  year         = {2024}
}

@inproceedings{DBLP:conf/nips/transformer,
  author       = {Ashish Vaswani and
                  Noam Shazeer and
                  Niki Parmar and
                  Jakob Uszkoreit and
                  Llion Jones and
                  Aidan N. Gomez and
                  Lukasz Kaiser and
                  Illia Polosukhin},
  title        = {Attention is All you Need},
  booktitle    = {NIPS},
  pages        = {5998--6008},
  year         = {2017}
}

@article{zhang2024ICL,
  title={Trained Transformers Learn Linear Models In-Context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={49},
  pages={1--55},
  year={2024}
}

@inproceedings{DBLP:conf/icml/Osw-ICL,
  author       = {Johannes von Oswald and
                  Eyvind Niklasson and
                  Ettore Randazzo and
                  Jo{\~{a}}o Sacramento and
                  Alexander Mordvintsev and
                  Andrey Zhmoginov and
                  Max Vladymyrov},
  title        = {Transformers Learn In-Context by Gradient Descent},
  booktitle    = {ICML},
  volume       = {202},
  pages        = {35151--35174},
  year         = {2023}
}

@inproceedings{DBLP:conf/nips/Ahn-23-ICL,
  author       = {Kwangjun Ahn and
                  Xiang Cheng and
                  Hadi Daneshmand and
                  Suvrit Sra},
  title        = {Transformers learn to implement preconditioned gradient descent for in-context learning},
  booktitle    = {NeurIPS},
  year         = {2023}
}


@inproceedings{DBLP:journals/corr/Ahn-linearatten,
  author       = {Kwangjun Ahn and
                  Xiang Cheng and
                  Minhak Song and
                  Chulhee Yun and
                  Ali Jadbabaie and
                  Suvrit Sra},
  title        = {Linear attention is (maybe) all you need (to understand Transformer optimization)},
  booktitle    = {{ICLR}},
  year         = {2024},
}


@inproceedings{DBLP:journals/corr/tengyu-ICL,
  author       = {Arvind V. Mahankali and
                  Tatsunori Hashimoto and
                  Tengyu Ma},
  title        = {One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention},
  booktitle    = {{ICLR}},
  year         = {2024}
}

@inproceedings{DBLP:journals/corr/jingfeng-ICL,
  author       = {Jingfeng Wu and
                  Difan Zou and
                  Zixiang Chen and
                  Vladimir Braverman and
                  Quanquan Gu and
                  Peter L. Bartlett},
  title        = {How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?},
  booktitle    = {{ICLR}},
  year         = {2024}
}

@inproceedings{DBLP:journals/corr/huang-ICL,
  author       = {Yu Huang and
                  Yuan Cheng and
                  Yingbin Liang},
  title        = {In-context Convergence of Transformers},
  booktitle    = {{ICML}},
  year         = {2024}
}


@inproceedings{DBLP:conf/nips/AroraCHL19,
  author       = {Sanjeev Arora and
                  Nadav Cohen and
                  Wei Hu and
                  Yuping Luo},
  title        = {Implicit Regularization in Deep Matrix Factorization},
  booktitle    = {NeurIPS},
  pages        = {7411--7422},
  year         = {2019}
}

@article{DBLP:journals/corr/zhang2024transformerblock,
  author       = {Ruiqi Zhang and
                  Jingfeng Wu and
                  Peter L. Bartlett},
  title        = {In-Context Learning of a Linear Transformer Block: Benefits of the
                  {MLP} Component and One-Step {GD} Initialization},
  journal      = {CoRR},
  volume       = {abs/2402.14951},
  year         = {2024}
}

@article{DBLP:journals/corr/DeoraMultiAtten,
  author       = {Puneesh Deora and
                  Rouzbeh Ghaderi and
                  Hossein Taheri and
                  Christos Thrampoulidis},
  title        = {On the Optimization and Generalization of Multi-head Attention},
  journal      = {CoRR},
  volume       = {abs/2310.12680},
  year         = {2023}
}

@inproceedings{DBLP:journals/corr/chenMultiAtten,
  author       = {Siyu Chen and
                  Heejune Sheen and
                  Tianhao Wang and
                  Zhuoran Yang},
  editor       = {Shipra Agrawal and
                  Aaron Roth},
  title        = {Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality (extended abstract)},
  booktitle    = {COLT},
  volume       = {247},
  pages        = {4573},
  year         = {2024}
}

@article{DBLP:journals/corr/CuiMultiAtten,
  author       = {Yingqian Cui and
                  Jie Ren and
                  Pengfei He and
                  Jiliang Tang and
                  Yue Xing},
  title        = {Superiority of Multi-Head Attention in In-Context Linear Regression},
  journal      = {CoRR},
  volume       = {abs/2401.17426},
  year         = {2024}
}

@article{DBLP:journals/corr/Jason2024causal,
  author       = {Eshaan Nichani and
                  Alex Damian and
                  Jason D. Lee},
  title        = {How Transformers Learn Causal Structure with Gradient Descent},
  journal      = {CoRR},
  volume       = {abs/2402.14735},
  year         = {2024}
}

@inproceedings{mahdavi2024revisiting,
  title={Revisiting the Equivalence of In-Context Learning and Gradient Descent: The Impact of Data Distribution},
  author={Mahdavi, Sadegh and Liao, Renjie and Thrampoulidis, Christos},
  booktitle={ICASSP},
  pages={7410--7414},
  year={2024},
  organization={IEEE}
}

@article{shah2023learningDDPM,
  title={Learning mixtures of gaussians using the ddpm objective},
  author={Shah, Kulin and Chen, Sitan and Klivans, Adam},
  journal={NeurIPS},
  volume={36},
  pages={19636--19649},
  year={2023}
}

@article{he2022information,
  title={Information-theoretic characterization of the generalization error for iterative semi-supervised learning},
  author={He, Haiyun and Yan, Hanshu and Tan, Vincent YF},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={287},
  pages={1--52},
  year={2022}
}

@article{schmidt2018adversarially,
  title={Adversarially robust generalization requires more data},
  author={Schmidt, Ludwig and Santurkar, Shibani and Tsipras, Dimitris and Talwar, Kunal and Madry, Aleksander},
  journal={NeurIPS},
  volume={31},
  year={2018}
}

@article{zheng2024toward,
  title={Toward understanding generative data augmentation},
  author={Zheng, Chenyu and Wu, Guoqiang and Li, Chongxuan},
  journal={NeurIPS},
  volume={36},
  year={2024}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@inproceedings{ramesh2021DALLE,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={ICML},
  pages={8821--8831},
  year={2021}
}

@inproceedings{lee2022autoregressive,
  title={Autoregressive image generation using residual quantization},
  author={Lee, Doyup and Kim, Chiheon and Kim, Saehoon and Cho, Minsu and Han, Wook-Shin},
  booktitle={CVPR},
  pages={11523--11532},
  year={2022}
}

@inproceedings{DBLP:conf/nips/visionllm,
  author       = {Wenhai Wang and
                  Zhe Chen and
                  Xiaokang Chen and
                  Jiannan Wu and
                  Xizhou Zhu and
                  Gang Zeng and
                  Ping Luo and
                  Tong Lu and
                  Jie Zhou and
                  Yu Qiao and
                  Jifeng Dai},
  title        = {VisionLLM: Large Language Model is also an Open-Ended Decoder for
                  Vision-Centric Tasks},
  booktitle    = {NeurIPS},
  year         = {2023}
}

@article{tian2024visual,
  title={Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction},
  author={Tian, Keyu and Jiang, Yi and Yuan, Zehuan and Peng, Bingyue and Wang, Liwei},
  journal={arXiv preprint arXiv:2404.02905},
  year={2024}
}

@article{DBLP:journals/corr/gpt4v,
  author       = {Zhengyuan Yang and
                  Linjie Li and
                  Kevin Lin and
                  Jianfeng Wang and
                  Chung{-}Ching Lin and
                  Zicheng Liu and
                  Lijuan Wang},
  title        = {The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)},
  journal      = {CoRR},
  volume       = {abs/2309.17421},
  year         = {2023}
}

@inproceedings{DBLP:conf/nips/LLAVA,
  author       = {Haotian Liu and
                  Chunyuan Li and
                  Qingyang Wu and
                  Yong Jae Lee},
  title        = {Visual Instruction Tuning},
  booktitle    = {NeurIPS},
  year         = {2023}
}

@article{DBLP:journals/corr/anygpt,
  author       = {Jun Zhan and
                  Junqi Dai and
                  Jiasheng Ye and
                  Yunhua Zhou and
                  Dong Zhang and
                  Zhigeng Liu and
                  Xin Zhang and
                  Ruibin Yuan and
                  Ge Zhang and
                  Linyang Li and
                  Hang Yan and
                  Jie Fu and
                  Tao Gui and
                  Tianxiang Sun and
                  Yugang Jiang and
                  Xipeng Qiu},
  title        = {AnyGPT: Unified Multimodal {LLM} with Discrete Sequence Modeling},
  journal      = {CoRR},
  volume       = {abs/2402.12226},
  year         = {2024}
}

@article{DBLP:journals/corr/gemini15,
  author       = {Machel Reid and
                  Nikolay Savinov and
                  Denis Teplyashin and
                  Dmitry Lepikhin and
                  Timothy P. Lillicrap and
                  Jean{-}Baptiste Alayrac and
                  Radu Soricut and
                  Angeliki Lazaridou and
                  Orhan Firat and
                  Julian Schrittwieser and
                  Ioannis Antonoglou and
                  Rohan Anil and
                  Sebastian Borgeaud and
                  Andrew M. Dai and
                  Katie Millican and
                  Ethan Dyer and
                  Mia Glaese and
                  Thibault Sottiaux and
                  Benjamin Lee and
                  Fabio Viola and
                  Malcolm Reynolds and
                  Yuanzhong Xu and
                  James Molloy and
                  Jilin Chen and
                  Michael Isard and
                  Paul Barham and
                  Tom Hennigan and
                  Ross McIlroy and
                  Melvin Johnson and
                  Johan Schalkwyk and
                  Eli Collins and
                  Eliza Rutherford and
                  Erica Moreira and
                  Kareem Ayoub and
                  Megha Goel and
                  Clemens Meyer and
                  Gregory Thornton and
                  Zhen Yang and
                  Henryk Michalewski and
                  Zaheer Abbas and
                  Nathan Schucher and
                  Ankesh Anand and
                  Richard Ives and
                  James Keeling and
                  Karel Lenc and
                  Salem Haykal and
                  Siamak Shakeri and
                  Pranav Shyam and
                  Aakanksha Chowdhery and
                  Roman Ring and
                  Stephen Spencer and
                  Eren Sezener and
                  et al.},
  title        = {Gemini 1.5: Unlocking multimodal understanding across millions of
                  tokens of context},
  journal      = {CoRR},
  volume       = {abs/2403.05530},
  year         = {2024}
}

@inproceedings{DBLP:conf/icml/imagegpt,
  author       = {Mark Chen and
                  Alec Radford and
                  Rewon Child and
                  Jeffrey Wu and
                  Heewoo Jun and
                  David Luan and
                  Ilya Sutskever},
  title        = {Generative Pretraining From Pixels},
  booktitle    = {ICML},
  volume       = {119},
  pages        = {1691--1703},
  year         = {2020}
}

@inproceedings{DBLP:conf/nips/gargICL,
  author       = {Shivam Garg and
                  Dimitris Tsipras and
                  Percy Liang and
                  Gregory Valiant},
  title        = {What Can Transformers Learn In-Context? {A} Case Study of Simple Function
                  Classes},
  booktitle    = {NeurIPS},
  year         = {2022}
}


@inproceedings{DBLP:journals/corr/ICLdiscrete,
  author       = {Satwik Bhattamishra and
                  Arkil Patel and
                  Phil Blunsom and
                  Varun Kanade},
  title        = {Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions},
  booktitle    = {{ICLR}},
  year         = {2024}
}

@inproceedings{DBLP:conf/icml/LiIPO23ICLstability,
  author       = {Yingcong Li and
                  Muhammed Emrullah Ildiz and
                  Dimitris Papailiopoulos and
                  Samet Oymak},
  title        = {Transformers as Algorithms: Generalization and Stability in In-context
                  Learning},
  booktitle    = {{ICML}},
  volume       = {202},
  pages        = {19565--19594},
  year         = {2023}
}

@inproceedings{DBLP:conf/iclr/AkyurekSA0Z23ICL,
  author       = {Ekin Aky{\"{u}}rek and
                  Dale Schuurmans and
                  Jacob Andreas and
                  Tengyu Ma and
                  Denny Zhou},
  title        = {What learning algorithm is in-context learning? Investigations with
                  linear models},
  booktitle    = {{ICLR}},
  year         = {2023}
}

@article{DBLP:journals/corr/Rongge,
  author       = {Max Vladymyrov and
                  Johannes von Oswald and
                  Mark Sandler and
                  Rong Ge},
  title        = {Linear Transformers are Versatile In-Context Learners},
  journal      = {CoRR},
  volume       = {abs/2402.14180},
  year         = {2024}
}

@article{DBLP:journals/corr/Hub-mesa-opt,
  author       = {Evan Hubinger and
                  Chris van Merwijk and
                  Vladimir Mikulik and
                  Joar Skalse and
                  Scott Garrabrant},
  title        = {Risks from Learned Optimization in Advanced Machine Learning Systems},
  journal      = {CoRR},
  volume       = {abs/1906.01820},
  year         = {2019}
}

@inproceedings{DBLP:conf/acl/mesa-ICL-ACL,
  author       = {Damai Dai and
                  Yutao Sun and
                  Li Dong and
                  Yaru Hao and
                  Shuming Ma and
                  Zhifang Sui and
                  Furu Wei},
  title        = {Why Can {GPT} Learn In-Context? Language Models Secretly Perform Gradient
                  Descent as Meta-Optimizers},
  booktitle    = {Findings of {ACL}},
  pages        = {4005--4019},
  publisher    = {Association for Computational Linguistics},
  year         = {2023}
}

@inproceedings{DBLP:conf/iclr/XieRL022,
  author       = {Sang Michael Xie and
                  Aditi Raghunathan and
                  Percy Liang and
                  Tengyu Ma},
  title        = {An Explanation of In-context Learning as Implicit Bayesian Inference},
  booktitle    = {{ICLR}},
  year         = {2022}
}

@article{DBLP:journals/corr/wang-topic,
  author       = {Xinyi Wang and
                  Wanrong Zhu and
                  William Yang Wang},
  title        = {Large Language Models Are Implicitly Topic Models: Explaining and
                  Finding Good Demonstrations for In-Context Learning},
  journal      = {CoRR},
  volume       = {abs/2301.11916},
  year         = {2023}
}

@inproceedings{DBLP:conf/nips/WiesLS23,
  author       = {Noam Wies and
                  Yoav Levine and
                  Amnon Shashua},
  title        = {The Learnability of In-Context Learning},
  booktitle    = {NeurIPS},
  year         = {2023}
}

@inproceedings{DBLP:conf/icml/LiLR23-topic,
  author       = {Yuchen Li and
                  Yuanzhi Li and
                  Andrej Risteski},
  title        = {How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding},
  booktitle    = {{ICML}},
  volume       = {202},
  pages        = {19689--19729},
  year         = {2023}
}

@article{DBLP:journals/corr/han-ICL-kernel,
  author       = {Chi Han and
                  Ziqi Wang and
                  Han Zhao and
                  Heng Ji},
  title        = {In-Context Learning of Large Language Models Explained as Kernel Regression},
  journal      = {CoRR},
  volume       = {abs/2305.12766},
  year         = {2023}
}

@inproceedings{DBLP:journals/corr/deep-linear-net,
  author       = {Andrew M. Saxe and
                  James L. McClelland and
                  Surya Ganguli},
  title        = {Exact solutions to the nonlinear dynamics of learning in deep linear
                  neural networks},
  booktitle    = {{ICLR}},
  year         = {2014}
}

@article{polyak1963gradient,
  title={Gradient methods for minimizing functionals},
  author={Polyak, Boris Teodorovich},
  journal={Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  volume={3},
  number={4},
  pages={643--653},
  year={1963},
  publisher={Russian Academy of Sciences, Branch of Mathematical Sciences}
}

@inproceedings{DBLP:conf/pkdd/KarimiNS16,
  author       = {Hamed Karimi and
                  Julie Nutini and
                  Mark Schmidt},
  title        = {Linear Convergence of Gradient and Proximal-Gradient Methods Under
                  the Polyak-{\L}ojasiewicz Condition},
  booktitle    = {{ECML} {PKDD}},
  volume       = {9851},
  pages        = {795--811},
  year         = {2016}
}

@inproceedings{DBLP:conf/nips/distribution-ICL-22,
  author       = {Stephanie C. Y. Chan and
                  Adam Santoro and
                  Andrew K. Lampinen and
                  Jane X. Wang and
                  Aaditya K. Singh and
                  Pierre H. Richemond and
                  James L. McClelland and
                  Felix Hill},
  title        = {Data Distributional Properties Drive Emergent In-Context Learning
                  in Transformers},
  booktitle    = {NeurIPS},
  year         = {2022}
}

@article{DBLP:journals/corr/ICL-OOD-2023,
  author       = {Kartik Ahuja and
                  David Lopez{-}Paz},
  title        = {A Closer Look at In-Context Learning under Distribution Shifts},
  journal      = {CoRR},
  volume       = {abs/2305.16704},
  year         = {2023}
}

@inproceedings{DBLP:conf/emnlp/MinLHALHZ22-emnlp,
  author       = {Sewon Min and
                  Xinxi Lyu and
                  Ari Holtzman and
                  Mikel Artetxe and
                  Mike Lewis and
                  Hannaneh Hajishirzi and
                  Luke Zettlemoyer},
  title        = {Rethinking the Role of Demonstrations: What Makes In-Context Learning
                  Work?},
  booktitle    = {{EMNLP}},
  pages        = {11048--11064},
  year         = {2022}
}

@article{DBLP:journals/corr/hongkang-ICL-2024,
  author       = {Hongkang Li and
                  Meng Wang and
                  Songtao Lu and
                  Xiaodong Cui and
                  Pin{-}Yu Chen},
  title        = {Training Nonlinear Transformers for Efficient In-Context Learning:
                  {A} Theoretical Learning and Generalization Analysis},
  journal      = {CoRR},
  volume       = {abs/2402.15607},
  year         = {2024}
}

@inproceedings{DBLP:journals/corr/taiji-nonlinear-ICL,
  author       = {Juno Kim and
                  Taiji Suzuki},
  title        = {Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field
                  Dynamics on the Attention Landscape},
  booktitle    = {{ICML}},
  year         = {2024}
}


@article{DBLP:journals/corr/christos-AR,
  author       = {Christos Thrampoulidis},
  title        = {Implicit Bias of Next-Token Prediction},
  journal      = {CoRR},
  volume       = {abs/2402.18551},
  year         = {2024}
}

@inproceedings{li2024AR,
  title={Mechanics of next token prediction with self-attention},
  author={Li, Yingcong and Huang, Yixiao and Ildiz, Muhammed E and Rawat, Ankit Singh and Oymak, Samet},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={685--693},
  year={2024},
}

@article{petersen2008matrix,
  title={The matrix cookbook},
  author={Petersen, Kaare Brandt and Pedersen, Michael Syskind and others},
  journal={Technical University of Denmark},
  volume={7},
  number={15},
  pages={510},
  year={2008}
}

@inproceedings{DBLP:conf/iclr/TransBayesian,
  author       = {Samuel M{\"{u}}ller and
                  Noah Hollmann and
                  Sebastian Pineda{-}Arango and
                  Josif Grabocka and
                  Frank Hutter},
  title        = {Transformers Can Do Bayesian Inference},
  booktitle    = {{ICLR}},
  year         = {2022}
}

@article{DBLP:journals/corr/Fu2023high,
  author       = {Deqing Fu and
                  Tian{-}Qi Chen and
                  Robin Jia and
                  Vatsal Sharan},
  title        = {Transformers Learn Higher-Order Optimization Methods for In-Context Learning: {A} Study with Linear Models},
  journal      = {CoRR},
  volume       = {abs/2310.17086},
  year         = {2023}
}

@article{DBLP:journals/corr/Giannou23newton,
  author       = {Angeliki Giannou and
                  Liu Yang and
                  Tianhao Wang and
                  Dimitris Papailiopoulos and
                  Jason D. Lee},
  title        = {How Well Can Transformers Emulate In-context Newton's Method?},
  journal      = {CoRR},
  volume       = {abs/2403.03183},
  year         = {2024}
}


@inproceedings{DBLP:conf/icml/ZhengWBCLZ23,
  author       = {Chenyu Zheng and
                  Guoqiang Wu and
                  Fan Bao and
                  Yue Cao and
                  Chongxuan Li and
                  Jun Zhu},
  title        = {Revisiting Discriminative vs. Generative Classifiers: Theory and Implications},
  booktitle    = {{ICML}},
  volume       = {202},
  pages        = {42420--42477},
  publisher    = {{PMLR}},
  year         = {2023}
}