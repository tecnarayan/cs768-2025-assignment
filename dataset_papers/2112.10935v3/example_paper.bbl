\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2021theory}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (98):\penalty0 1--76, 2021.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  263--272. PMLR, 2017.

\bibitem[Beck \& Teboulle(2003)Beck and Teboulle]{beck2003mirror}
Beck, A. and Teboulle, M.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock \emph{Operations Research Letters}, 31\penalty0 (3):\penalty0
  167--175, 2003.

\bibitem[Bhandari \& Russo(2019)Bhandari and Russo]{bhandari2019global}
Bhandari, J. and Russo, D.
\newblock Global optimality guarantees for policy gradient methods.
\newblock \emph{arXiv preprint arXiv:1906.01786}, 2019.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2020provably}
Cai, Q., Yang, Z., Jin, C., and Wang, Z.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1283--1294. PMLR, 2020.

\bibitem[Domingues et~al.(2021)Domingues, M{\'e}nard, Kaufmann, and
  Valko]{domingues2021episodic}
Domingues, O.~D., M{\'e}nard, P., Kaufmann, E., and Valko, M.
\newblock Episodic reinforcement learning in finite mdps: Minimax lower bounds
  revisited.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  578--598. PMLR, 2021.

\bibitem[Fazel et~al.(2018)Fazel, Ge, Kakade, and Mesbahi]{fazel2018global}
Fazel, M., Ge, R., Kakade, S., and Mesbahi, M.
\newblock Global convergence of policy gradient methods for the linear
  quadratic regulator.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1467--1476. PMLR, 2018.

\bibitem[Fei et~al.(2020)Fei, Yang, Wang, and Xie]{fei2020dynamic}
Fei, Y., Yang, Z., Wang, Z., and Xie, Q.
\newblock Dynamic regret of policy optimization in non-stationary environments.
\newblock \emph{arXiv preprint arXiv:2007.00148}, 2020.

\bibitem[Gu et~al.(2017)Gu, Holly, Lillicrap, and Levine]{gu2017deep}
Gu, S., Holly, E., Lillicrap, T., and Levine, S.
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock In \emph{2017 IEEE international conference on robotics and
  automation (ICRA)}, pp.\  3389--3396. IEEE, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Hartikainen, Tucker, Ha, Tan,
  Kumar, Zhu, Gupta, Abbeel, et~al.]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar,
  V., Zhu, H., Gupta, A., Abbeel, P., et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018.

\bibitem[He et~al.(2021)He, Zhou, and Gu]{he2021nearly}
He, J., Zhou, D., and Gu, Q.
\newblock Nearly optimal regret for learning adversarial mdps with linear
  function approximation.
\newblock \emph{arXiv preprint arXiv:2102.08940}, 2021.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (4), 2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is q-learning provably efficient?
\newblock \emph{arXiv preprint arXiv:1807.03765}, 2018.

\bibitem[Kakade(2001)]{kakade2001natural}
Kakade, S.~M.
\newblock A natural policy gradient.
\newblock \emph{Advances in neural information processing systems}, 14, 2001.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, et~al.]{kalashnikov2018qt}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E.,
  Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et~al.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock \emph{arXiv preprint arXiv:1806.10293}, 2018.

\bibitem[Lancewicki et~al.(2020)Lancewicki, Rosenberg, and
  Mansour]{lancewicki2020learning}
Lancewicki, T., Rosenberg, A., and Mansour, Y.
\newblock Learning adversarial markov decision processes with delayed feedback.
\newblock \emph{arXiv preprint arXiv:2012.14843}, 2020.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Liu et~al.(2019)Liu, Cai, Yang, and Wang]{liu2019neural}
Liu, B., Cai, Q., Yang, Z., and Wang, Z.
\newblock Neural trust region/proximal policy optimization attains globally
  optimal policy.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10565--10576, 2019.

\bibitem[Luo et~al.(2021)Luo, Wei, and Lee]{luo2021policy}
Luo, H., Wei, C.-Y., and Lee, C.-W.
\newblock Policy optimization in adversarial mdps: Improved exploration via
  dilated bonuses.
\newblock \emph{arXiv preprint arXiv:2107.08346}, 2021.

\bibitem[Maurer \& Pontil(2009)Maurer and Pontil]{maurer2009empirical}
Maurer, A. and Pontil, M.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock \emph{arXiv preprint arXiv:0907.3740}, 2009.

\bibitem[Menard et~al.(2021)Menard, Domingues, Shang, and Valko]{menard2021ucb}
Menard, P., Domingues, O.~D., Shang, X., and Valko, M.
\newblock Ucb momentum q-learning: Correcting the bias without forgetting.
\newblock \emph{arXiv preprint arXiv:2103.01312}, 2021.

\bibitem[Orabona(2019)]{orabona2019modern}
Orabona, F.
\newblock A modern introduction to online learning.
\newblock \emph{arXiv preprint arXiv:1912.13213}, 2019.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897. PMLR, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shani et~al.(2020)Shani, Efroni, Rosenberg, and
  Mannor]{shani2020optimistic}
Shani, L., Efroni, Y., Rosenberg, A., and Mannor, S.
\newblock Optimistic policy optimization with bandit feedback.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8604--8613. PMLR, 2020.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, Mansour,
  et~al.]{sutton1999policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., Mansour, Y., et~al.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{NIPs}, volume~99, pp.\  1057--1063. Citeseer, 1999.

\bibitem[Wang et~al.(2019)Wang, Cai, Yang, and Wang]{wang2019neural}
Wang, L., Cai, Q., Yang, Z., and Wang, Z.
\newblock Neural policy gradient methods: Global optimality and rates of
  convergence.
\newblock \emph{arXiv preprint arXiv:1909.01150}, 2019.

\bibitem[Weissman et~al.(2003)Weissman, Ordentlich, Seroussi, Verdu, and
  Weinberger]{weissman2003inequalities}
Weissman, T., Ordentlich, E., Seroussi, G., Verdu, S., and Weinberger, M.~J.
\newblock Inequalities for the l1 deviation of the empirical distribution.
\newblock \emph{Hewlett-Packard Labs, Tech. Rep}, 2003.

\bibitem[Zanette \& Brunskill(2019)Zanette and Brunskill]{zanette2019tighter}
Zanette, A. and Brunskill, E.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7304--7312. PMLR, 2019.

\bibitem[Zanette et~al.(2020)Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10978--10989. PMLR, 2020.

\bibitem[Zanette et~al.(2021)Zanette, Cheng, and
  Agarwal]{zanette2021cautiously}
Zanette, A., Cheng, C.-A., and Agarwal, A.
\newblock Cautiously optimistic policy optimization and exploration with linear
  function approximation.
\newblock \emph{arXiv preprint arXiv:2103.12923}, 2021.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Ji, and
  Du]{zhang2020reinforcement}
Zhang, Z., Ji, X., and Du, S.~S.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock \emph{arXiv preprint arXiv:2009.13503}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Zhou, and Ji]{zhang2020almost}
Zhang, Z., Zhou, Y., and Ji, X.
\newblock Almost optimal model-free reinforcement learningvia
  reference-advantage decomposition.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\bibitem[Zhong et~al.(2021)Zhong, Yang, Wang, and
  Szepesv{\'a}ri]{zhong2021optimistic}
Zhong, H., Yang, Z., Wang, Z., and Szepesv{\'a}ri, C.
\newblock Optimistic policy optimization is provably efficient in
  non-stationary mdps.
\newblock \emph{arXiv preprint arXiv:2110.08984}, 2021.

\end{thebibliography}
