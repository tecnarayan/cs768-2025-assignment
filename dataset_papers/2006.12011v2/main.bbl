\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{ZYWG19}

\bibitem[ADHV19]{andoni2019attribute}
Alexandr Andoni, Rishabh Dudeja, Daniel Hsu, and Kiran Vodrahalli.
\newblock Attribute-efficient learning of monomials over highly-correlated
  variables.
\newblock In {\em Thirtieth International Conference on Algorithmic Learning
  Theory}, 2019.

\bibitem[APVZ14]{andoni2014learning}
Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li~Zhang.
\newblock Learning sparse polynomial functions.
\newblock In {\em Proceedings of the twenty-fifth annual ACM-SIAM symposium on
  Discrete algorithms}, pages 500--510. SIAM, 2014.

\bibitem[AS20]{Abbe}
Emmanuel Abbe and Colin Sandon.
\newblock Poly-time universality and limitations of deep learning.
\newblock {\em arXiv preprint arXiv:2001.02992}, 2020.

\bibitem[BFJ{\etalchar{+}}94]{blum1994weakly}
Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and
  Steven Rudich.
\newblock Weakly learning dnf and characterizing statistical query learning
  using fourier analysis.
\newblock In {\em Proceedings of the twenty-sixth annual ACM symposium on
  Theory of computing}, pages 253--262, 1994.

\bibitem[BG17]{BG17}
Alon Brutzkus and Amir Globerson.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock {\em CoRR}, abs/1702.07966, 2017.

\bibitem[Boy84]{boyd1984asymptotic}
John~P Boyd.
\newblock Asymptotic coefficients of hermite function series.
\newblock {\em Journal of Computational Physics}, 54(3):382--410, 1984.

\bibitem[BR89]{blum1989training}
Avrim Blum and Ronald~L Rivest.
\newblock Training a 3-node neural network is {NP}-complete.
\newblock In {\em Advances in neural information processing systems}, pages
  494--501, 1989.

\bibitem[DKKZ20]{diakonikolas2020algorithms}
Ilias Diakonikolas, Daniel Kane, Vasilis Kontonis, and Nikos Zarifis.
\newblock {Algorithms and SQ Lower Bounds for PAC Learning One-Hidden-Layer
  ReLU Networks}.
\newblock In {\em Conference on Learning Theory}, 2020.
\newblock To appear.

\bibitem[DV20]{vdaniely}
Amit Daniely and Gal Vardi.
\newblock Hardness of learning neural networks with natural weights.
\newblock {\em arXiv preprint arXiv:2006.03177}, 2020.

\bibitem[Fel12]{feldman2012complete}
Vitaly Feldman.
\newblock A complete characterization of statistical query learning with
  applications to evolvability.
\newblock {\em Journal of Computer and System Sciences}, 78(5):1444--1459,
  2012.

\bibitem[Fel17]{feldman2017general}
Vitaly Feldman.
\newblock A general characterization of the statistical query complexity.
\newblock In {\em Conference on Learning Theory}, pages 785--830, 2017.

\bibitem[FGR{\etalchar{+}}17]{feldman2017statistical}
Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh~S Vempala, and Ying Xiao.
\newblock Statistical algorithms and a lower bound for detecting planted
  cliques.
\newblock {\em Journal of the ACM (JACM)}, 64(2):8, 2017.

\bibitem[GKK19]{goel2019time}
Surbhi Goel, Sushrut Karmalkar, and Adam Klivans.
\newblock Time/accuracy tradeoffs for learning a relu with respect to gaussian
  marginals.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8582--8591, 2019.

\bibitem[GKKT17]{goel2016reliably}
Surbhi Goel, Varun Kanade, Adam~R. Klivans, and Justin Thaler.
\newblock Reliably learning the relu in polynomial time.
\newblock In {\em {COLT}}, pages 1004--1042, 2017.

\bibitem[GLM18]{GLM18}
Rong Ge, Jason~D. Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In {\em ICLR}. OpenReview.net, 2018.

\bibitem[JHG18]{JacotHG18}
Arthur Jacot, Cl{\'e}ment Hongler, and Franck Gabriel.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In Samy Bengio, Hanna~M. Wallach, Hugo Larochelle, Kristen Grauman,
  Nicol{\`o} Cesa-Bianchi, and Roman Garnett, editors, {\em NeurIPS}, pages
  8580--8589, 2018.

\bibitem[JSA15]{janzamin2015beating}
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar.
\newblock Beating the perils of non-convexity: Guaranteed training of neural
  networks using tensor methods.
\newblock {\em arXiv preprint arXiv:1506.08473}, 2015.

\bibitem[Kea98]{kearns1998efficient}
Michael Kearns.
\newblock Efficient noise-tolerant learning from statistical queries.
\newblock {\em Journal of the ACM (JACM)}, 45(6):983--1006, 1998.

\bibitem[KK14]{KlivansK14}
Adam~R. Klivans and Pravesh Kothari.
\newblock Embedding hard learning problems into gaussian space.
\newblock In {\em APPROX-RANDOM}, volume~28 of {\em LIPIcs}, pages 793--809.
  Schloss Dagstuhl - Leibniz-Zentrum fuer Informatik, 2014.

\bibitem[KS94]{kearns1994pconcept}
Michael~J Kearns and Robert~E Schapire.
\newblock Efficient distribution-free learning of probabilistic concepts.
\newblock {\em Journal of Computer and System Sciences}, 48(3):464--497, 1994.

\bibitem[KS09]{klivans2009cryptographic}
Adam~R Klivans and Alexander~A Sherstov.
\newblock Cryptographic hardness for learning intersections of halfspaces.
\newblock {\em Journal of Computer and System Sciences}, 75(1):2--12, 2009.

\bibitem[LSSS14]{livni2014computational}
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir.
\newblock On the computational efficiency of training neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  855--863, 2014.

\bibitem[Sha18]{Shamir18}
Ohad Shamir.
\newblock Distribution-specific hardness of learning neural networks.
\newblock {\em J. Mach. Learn. Res}, 19:32:1--32:29, 2018.

\bibitem[SSSS17]{shalev2017failures}
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah.
\newblock Failures of gradient-based deep learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3067--3075, 2017.

\bibitem[SVWX17]{song2017complexity}
Le~Song, Santosh Vempala, John Wilmes, and Bo~Xie.
\newblock On the complexity of learning neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5514--5522, 2017.

\bibitem[Sz{\"o}09]{szorenyi2009characterizing}
Bal{\'a}zs Sz{\"o}r{\'e}nyi.
\newblock Characterizing statistical query learning: simplified notions and
  proofs.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 186--200. Springer, 2009.

\bibitem[Vu98]{vu1998infeasibility}
Van~H Vu.
\newblock On the infeasibility of training neural networks with small
  mean-squared error.
\newblock {\em IEEE Transactions on Information Theory}, 44(7):2892--2900,
  1998.

\bibitem[VW19]{vempala2018gradient}
Santosh Vempala and John Wilmes.
\newblock Gradient descent for one-hidden-layer neural networks: Polynomial
  convergence and sq lower bounds.
\newblock In {\em Conference on Learning Theory}, pages 3115--3117, 2019.

\bibitem[ZPS17]{ZhangPS17}
Qiuyi Zhang, Rina Panigrahy, and Sushant Sachdeva.
\newblock Electron-proton dynamics in deep learning.
\newblock {\em CoRR}, abs/1702.00458, 2017.

\bibitem[ZSJ{\etalchar{+}}17]{Zhao}
Kai Zhong, Zhao Song, Prateek Jain, Peter~L. Bartlett, and Inderjit~S. Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In {\em ICML}, volume~70, pages 4140--4149. JMLR.org, 2017.

\bibitem[ZYWG19]{ZhangYWG19}
Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu.
\newblock Learning one-hidden-layer relu networks via gradient descent.
\newblock In Kamalika Chaudhuri and Masashi Sugiyama, editors, {\em The 22nd
  International Conference on Artificial Intelligence and Statistics, AISTATS
  2019, 16-18 April 2019, Naha, Okinawa, Japan}, volume~89 of {\em Proceedings
  of Machine Learning Research}, pages 1524--1534. PMLR, 2019.

\end{thebibliography}
