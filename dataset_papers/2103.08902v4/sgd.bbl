\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agrawal et~al.(2019)Agrawal, Amos, Barratt, Boyd, Diamond, and
  Kolter]{agrawal2019differentiable}
Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and
  Zico Kolter.
\newblock Differentiable convex optimization layers.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Amos and Kolter(2017)]{amos2017optnet}
Brandon Amos and J~Zico Kolter.
\newblock Optnet: Differentiable optimization as a layer in neural networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Bamford et~al.(2009)Bamford, Nichol, Baldry, Land, Lintott,
  Schawinski, Slosar, Szalay, Thomas, Torki, et~al.]{bamford2009galaxy}
Steven~P Bamford, Robert~C Nichol, Ivan~K Baldry, Kate Land, Chris~J Lintott,
  Kevin Schawinski, An{\v{z}}e Slosar, Alexander~S Szalay, Daniel Thomas, Mehri
  Torki, et~al.
\newblock Galaxy zoo: the dependence of morphology and colour on environment.
\newblock \emph{Monthly Notices of the Royal Astronomical Society},
  393\penalty0 (4):\penalty0 1324--1352, 2009.

\bibitem[Bansal et~al.(2021)Bansal, Nushi, Kamar, Horvitz, and
  Weld]{bansal2020optimizing}
Gagan Bansal, Besmira Nushi, Ece Kamar, Eric Horvitz, and Daniel~S. Weld.
\newblock {Optimizing AI for Teamwork}.
\newblock In \emph{AAAI}, 2021.

\bibitem[Bartlett and Wegkamp(2008)]{bartlett2008classification}
Peter Bartlett and Marten Wegkamp.
\newblock Classification with a reject option using a hinge loss.
\newblock \emph{JMLR}, 9\penalty0 (Aug):\penalty0 1823--1840, 2008.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
Stephen Boyd, Stephen~P Boyd, and Lieven Vandenberghe.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Chen and Price(2017)]{chen2017active}
Xue Chen and Eric Price.
\newblock Active regression via linear-sample sparsification.
\newblock In \emph{COLT}, 2017.

\bibitem[Cohn et~al.(1995)Cohn, Ghahramani, and Jordan]{cohn1995active}
David Cohn, Zoubin Ghahramani, and Michael Jordan.
\newblock Active learning with statistical models.
\newblock In \emph{NeurIPS}, 1995.

\bibitem[Cortes et~al.(2016)Cortes, DeSalvo, and Mohri]{cortes2016learning}
Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri.
\newblock Learning with rejection.
\newblock In \emph{ALT}, 2016.

\bibitem[Davidson et~al.()Davidson, Warmsley, Macy, and Weber]{hateoffensive}
Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber.
\newblock Automated hate speech detection and the problem of offensive
  language.
\newblock ICWSM, pages 512--515.

\bibitem[De et~al.(2020)De, Koley, Ganguly, and Gomez-Rodriguez]{de2020aaai}
Abir De, Paramita Koley, Niloy Ganguly, and Manuel Gomez-Rodriguez.
\newblock Regression under human assistance.
\newblock In \emph{AAAI}, 2020.

\bibitem[De et~al.(2021)De, Okati, Zarezadeh, and Gomez-Rodriguez]{de2021aaai}
Abir De, Nastaran Okati, Ali Zarezadeh, and Manuel Gomez-Rodriguez.
\newblock Classification under human assistance.
\newblock In \emph{AAAI}, 2021.

\bibitem[Geifman and El-Yaniv(2019)]{geifman2019selectivenet}
Yonatan Geifman and Ran El-Yaniv.
\newblock Selectivenet: A deep neural network with an integrated reject option.
\newblock In \emph{ICML}, 2019.

\bibitem[Geifman et~al.(2019)Geifman, Uziel, and El-Yaniv]{geifman2018bias}
Yonatan Geifman, Guy Uziel, and Ran El-Yaniv.
\newblock Bias-reduced uncertainty estimation for deep neural classifiers.
\newblock 2019.

\bibitem[Guo and Schuurmans(2008)]{guo2008discriminative}
Yuhong Guo and Dale Schuurmans.
\newblock Discriminative batch mode active learning.
\newblock In \emph{NeurIPS}, 2008.

\bibitem[Hashemi et~al.(2019)Hashemi, Ghasemi, Vikalo, and
  Topcu]{hashemi2019submodular}
Abolfazl Hashemi, Mahsa Ghasemi, Haris Vikalo, and Ufuk Topcu.
\newblock Submodular observation selection and information gathering for
  quadratic models.
\newblock In \emph{ICML}, 2019.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition, 2015.

\bibitem[Hoi et~al.(2006)Hoi, Jin, Zhu, and Lyu]{hoi2006batch}
Steven Hoi, Rong Jin, Jianke Zhu, and Michael~R Lyu.
\newblock Batch mode active learning and its application to medical image
  classification.
\newblock In \emph{ICML}, 2006.

\bibitem[Joulin et~al.(2016)Joulin, Grave, Bojanowski, Douze, J{\'e}gou, and
  Mikolov]{ft}
Armand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H{\'e}rve
  J{\'e}gou, and Tomas Mikolov.
\newblock Fasttext. zip: Compressing text classification models.
\newblock \emph{arXiv preprint arXiv:1612.03651}, 2016.

\bibitem[Kim(2014)]{kim2014convolutional}
Yoon Kim.
\newblock Convolutional neural networks for sentence classification, 2014.

\bibitem[Kingma and Ba(2017)]{kingma2017adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization, 2017.

\bibitem[Liu et~al.(2019)Liu, Wang, Liang, Salakhutdinov, Morency, and
  Ueda]{liu2019deep}
Ziyin Liu, Zhikang Wang, Paul~Pu Liang, Russ~R Salakhutdinov, Louis-Philippe
  Morency, and Masahito Ueda.
\newblock Deep gamblers: Learning to abstain with portfolio theory.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Lubars and Tan(2020)]{lubars2020ask}
Brian Lubars and Chenhao Tan.
\newblock Ask not what ai can do, but what ai should do: Towards a framework of
  task delegability.
\newblock 2020.

\bibitem[Madras et~al.(2018)Madras, Pitassi, and Zemel]{Madras2018PredictRI}
David Madras, Toniann Pitassi, and Richard~S. Zemel.
\newblock Predict responsibly: Improving fairness and accuracy by learning to
  defer.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Mozannar and Sontag(2020)]{sontag2020}
Hussein Mozannar and David Sontag.
\newblock Consistent estimators for learning to defer to an expert.
\newblock In \emph{ICML}, 2020.

\bibitem[Pradel and Sen(2018)]{pradel2018deepbugs}
Michael Pradel and Koushik Sen.
\newblock Deepbugs: A learning approach to name-based bug detection.
\newblock In \emph{OOPSLA}, 2018.

\bibitem[Raghu et~al.(2019{\natexlab{a}})Raghu, Blumer, Corrado, Kleinberg,
  Obermeyer, and Mullainathan]{raghu2019algorithmic}
Maithra Raghu, Katy Blumer, Greg Corrado, Jon Kleinberg, Ziad Obermeyer, and
  Sendhil Mullainathan.
\newblock The algorithmic automation problem: Prediction, triage, and human
  effort.
\newblock \emph{arXiv preprint arXiv:1903.12220}, 2019{\natexlab{a}}.

\bibitem[Raghu et~al.(2019{\natexlab{b}})Raghu, Blumer, Sayres, Obermeyer,
  Kleinberg, Mullainathan, and Kleinberg]{raghu2019direct}
Maithra Raghu, Katy Blumer, Rory Sayres, Ziad Obermeyer, Bobby Kleinberg,
  Sendhil Mullainathan, and Jon Kleinberg.
\newblock Direct uncertainty prediction for medical second opinions.
\newblock In \emph{ICML}, 2019{\natexlab{b}}.

\bibitem[Ramaswamy et~al.(2018)Ramaswamy, Tewari, and
  Agarwal]{ramaswamy2018consistent}
Harish Ramaswamy, Ambuj Tewari, and Shivani Agarwal.
\newblock Consistent algorithms for multiclass classification with an abstain
  option.
\newblock \emph{Electronic J. of Statistics}, 12\penalty0 (1):\penalty0
  530--554, 2018.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Sabato and Munos(2014)]{sabato2014active}
Sivan Sabato and Remi Munos.
\newblock Active regression by stratification.
\newblock In \emph{NeurIPS}, 2014.

\bibitem[Singh(2008)]{singh2008nonparametric}
Aarti Singh.
\newblock \emph{Nonparametric Set Estimation Problems in Statistical Inference
  and Learning}.
\newblock PhD thesis, University of Wisconsin--Madison, 2008.

\bibitem[Sugiyama(2006)]{sugiyama2006active}
Masashi Sugiyama.
\newblock Active learning in approximately linear regression based on
  conditional expectation of generalization error.
\newblock \emph{JMLR}, 7\penalty0 (Jan):\penalty0 141--166, 2006.

\bibitem[Thulasidasan et~al.(2019)Thulasidasan, Bhattacharya, Bilmes,
  Chennupati, and Mohd-Yusof]{thulasidasan2019combating}
Sunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, and
  Jamal Mohd-Yusof.
\newblock Combating label noise in deep learning using abstention.
\newblock \emph{arXiv preprint arXiv:1905.10964}, 2019.

\bibitem[Topol(2019)]{topol2019high}
Eric Topol.
\newblock High-performance medicine: the convergence of human and artificial
  intelligence.
\newblock \emph{Nature medicine}, 25\penalty0 (1):\penalty0 44, 2019.

\bibitem[Wilder et~al.(2020)Wilder, Horvitz, and Kamar]{wilder2020learning}
Bryan Wilder, Eric Horvitz, and Ece Kamar.
\newblock Learning to complement humans.
\newblock In \emph{IJCAI}, 2020.

\bibitem[Willett et~al.(2006)Willett, Nowak, and Castro]{willett2006faster}
Rebecca Willett, Robert Nowak, and Rui~M Castro.
\newblock Faster rates in regression via active learning.
\newblock In \emph{NeurIPS}, 2006.

\bibitem[Willett and Nowak(2007)]{willett2007minimax}
Rebecca~M Willett and Robert~D Nowak.
\newblock Minimax optimal level-set estimation.
\newblock \emph{IEEE Transactions on Image Processing}, 16\penalty0
  (12):\penalty0 2965--2979, 2007.

\bibitem[Ziyin et~al.(2020)Ziyin, Chen, Wang, Liang, Salakhutdinov, Morency,
  and Ueda]{ziyin2020learning}
Liu Ziyin, Blair Chen, Ru~Wang, Paul~Pu Liang, Ruslan Salakhutdinov,
  Louis-Philippe Morency, and Masahito Ueda.
\newblock Learning not to learn in the presence of noisy labels.
\newblock \emph{arXiv preprint arXiv:2002.06541}, 2020.

\end{thebibliography}
