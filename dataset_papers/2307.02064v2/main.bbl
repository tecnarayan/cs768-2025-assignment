\begin{thebibliography}{70}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{layernorm}
Jimmy Ba, Jamie~Ryan Kiros, and Geoffrey Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Beattie et~al.(2016)Beattie, Leibo, Teplyashin, Ward, Wainwright, K{\"u}ttler, Lefrancq, Green, Vald{\'e}s, Sadik, et~al.]{beattie2016deepmind}
Charles Beattie, Joel~Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K{\"u}ttler, Andrew Lefrancq, Simon Green, V{\'\i}ctor Vald{\'e}s, Amir Sadik, et~al.
\newblock {DeepMind Lab}.
\newblock \emph{arXiv preprint arXiv:1612.03801}, 2016.

\bibitem[Bengio et~al.(2013)Bengio, L{\'e}onard, and Courville]{bengio2013estimating}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for conditional computation.
\newblock \emph{arXiv preprint arXiv:1308.3432}, 2013.

\bibitem[Blelloch(1990)]{blelloch1990prefix}
Guy Blelloch.
\newblock Prefix sums and their applications.
\newblock Technical report, School of Computer Science, Carnegie Mellon University, 1990.

\bibitem[Chen et~al.(2021)Chen, Wu, Yoon, and Ahn]{transdreamer}
Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn.
\newblock {TransDreamer}: Reinforcement learning with {Transformer} world models.
\newblock In \emph{Deep RL Workshop NeurIPS 2021}, 2021.

\bibitem[Chevalier-Boisvert et~al.(2018)Chevalier-Boisvert, Willems, and Pal]{minigrid}
Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal.
\newblock Minimalistic gridworld environment for {Gymnasium}, 2018.
\newblock URL \url{https://github.com/Farama-Foundation/Minigrid}.

\bibitem[Chung et~al.(2014)Chung, Gulcehre, Cho, and Bengio]{gru}
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio.
\newblock Empirical evaluation of gated recurrent neural networks on sequence modeling.
\newblock \emph{arXiv preprint arXiv:1412.3555}, 2014.

\bibitem[Cobbe et~al.(2020)Cobbe, Hesse, Hilton, and Schulman]{cobbe2020leveraging}
Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and Salakhutdinov]{transformer-xl}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.
\newblock {Transformer-XL}: Attentive language models beyond a fixed-length context.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics}, 2019.

\bibitem[David et~al.(2023)David, Zimerman, Nachmani, and Wolf]{decision_s4}
Shmuel~Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf.
\newblock Decision {S4}: Efficient sequence-based {RL} via state spaces layers.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Deng et~al.(2022)Deng, Jang, and Ahn]{dreamerpro}
Fei Deng, Ingook Jang, and Sungjin Ahn.
\newblock {D}reamer{P}ro: Reconstruction-free model-based reinforcement learning with prototypical representations.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: {Transformers} for image recognition at scale.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Esser et~al.(2021)Esser, Rombach, and Ommer]{vqgan}
Patrick Esser, Robin Rombach, and Bjorn Ommer.
\newblock Taming {Transformers} for high-resolution image synthesis.
\newblock In \emph{CVPR}, 2021.

\bibitem[Fortunato et~al.(2019)Fortunato, Tan, Faulkner, Hansen, Puigdom\`{e}nech~Badia, Buttimore, Deck, Leibo, and Blundell]{fortunato2019generalization}
Meire Fortunato, Melissa Tan, Ryan Faulkner, Steven Hansen, Adri\`{a} Puigdom\`{e}nech~Badia, Gavin Buttimore, Charles Deck, Joel~Z Leibo, and Charles Blundell.
\newblock Generalization of reinforcement learners with working and episodic memory.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Fu et~al.(2023)Fu, Dao, Saab, Thomas, Rudra, and R{\'e}]{h3}
Daniel~Y Fu, Tri Dao, Khaled~Kamal Saab, Armin~W Thomas, Atri Rudra, and Christopher R{\'e}.
\newblock {Hungry Hungry Hippos}: Towards language modeling with state space models.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Fu et~al.(2021)Fu, Yang, Agrawal, and Jaakkola]{tia}
Xiang Fu, Ge~Yang, Pulkit Agrawal, and Tommi Jaakkola.
\newblock Learning task informed abstractions.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Goel et~al.(2022)Goel, Gu, Donahue, and R{\'e}]{sashimi}
Karan Goel, Albert Gu, Chris Donahue, and Christopher R{\'e}.
\newblock It’s raw! {A}udio generation with state-space models.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R\'{e}]{hippo}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\'{e}.
\newblock {HiPPO}: Recurrent memory with optimal polynomial projections.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Gu et~al.(2021)Gu, Johnson, Goel, Saab, Dao, Rudra, and R{\'e}]{lssl}
Albert Gu, Isys Johnson, Karan Goel, Khaled~Kamal Saab, Tri Dao, Atri Rudra, and Christopher R{\'e}.
\newblock Combining recurrent, convolutional, and continuous-time models with linear state space layers.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Gu et~al.(2022{\natexlab{a}})Gu, Goel, Gupta, and R{\'e}]{s4d}
Albert Gu, Karan Goel, Ankit Gupta, and Christopher R{\'e}.
\newblock On the parameterization and initialization of diagonal state space models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{a}}.

\bibitem[Gu et~al.(2022{\natexlab{b}})Gu, Goel, and R{\'e}]{s4}
Albert Gu, Karan Goel, and Christopher R{\'e}.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{International Conference on Learning Representations}, 2022{\natexlab{b}}.

\bibitem[Gu et~al.(2023)Gu, Johnson, Timalsina, Rudra, and R{\'e}]{httyh}
Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R{\'e}.
\newblock How to train your {HiPPO}: State space models with generalized orthogonal basis projections.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Gupta et~al.(2022{\natexlab{a}})Gupta, Gu, and Berant]{dss}
Ankit Gupta, Albert Gu, and Jonathan Berant.
\newblock Diagonal state spaces are as effective as structured state spaces.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{a}}.

\bibitem[Gupta et~al.(2022{\natexlab{b}})Gupta, Mehta, and Berant]{dlr}
Ankit Gupta, Harsh Mehta, and Jonathan Berant.
\newblock Simplifying and understanding state space models with diagonal linear {RNN}s.
\newblock \emph{arXiv preprint arXiv:2212.00768}, 2022{\natexlab{b}}.

\bibitem[Ha and Schmidhuber(2018)]{worldmodels}
David Ha and J\"{u}rgen Schmidhuber.
\newblock Recurrent world models facilitate policy evolution.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Ha et~al.(2023)Ha, Kim, and Kim]{drg}
Jeongsoo Ha, Kyungsoo Kim, and Yusung Kim.
\newblock Dream to generalize: Zero-shot model-based reinforcement learning for unseen visual distractions.
\newblock In \emph{AAAI}, 2023.

\bibitem[Hafner(2022)]{crafter}
Danijar Hafner.
\newblock Benchmarking the spectrum of agent capabilities.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Hafner et~al.(2019)Hafner, Lillicrap, Fischer, Villegas, Ha, Lee, and Davidson]{rssm}
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson.
\newblock Learning latent dynamics for planning from pixels.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Hafner et~al.(2020)Hafner, Lillicrap, Ba, and Norouzi]{dreamer}
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hafner et~al.(2021)Hafner, Lillicrap, Norouzi, and Ba]{dreamerv2}
Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba.
\newblock Mastering {Atari} with discrete world models.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Hafner et~al.(2023)Hafner, Pasukonis, Ba, and Lillicrap]{dreamerv3}
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap.
\newblock Mastering diverse domains through world models.
\newblock \emph{arXiv preprint arXiv:2301.04104}, 2023.

\bibitem[Harvey et~al.(2022)Harvey, Naderiparizi, Masrani, Weilbach, and Wood]{fdm}
William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood.
\newblock Flexible diffusion modeling of long videos.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Hochreiter and Schmidhuber(1997)]{lstm}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Islam and Bertasius(2022)]{vis4mer}
Md~Mohaiminul Islam and Gedas Bertasius.
\newblock Long movie clip classification with state-space video models.
\newblock In \emph{ECCV}, 2022.

\bibitem[Jain et~al.(2022)Jain, Sujit, Joshi, Michalski, Hafner, and Ebrahimi~Kahou]{vsg}
Arnav~Kumar Jain, Shivakanth Sujit, Shruti Joshi, Vincent Michalski, Danijar Hafner, and Samira Ebrahimi~Kahou.
\newblock Learning robust dynamics through variational sparse gating.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Knigge et~al.(2023)Knigge, Romero, Gu, Gavves, Bekkers, Tomczak, Hoogendoorn, and Sonke]{ccnn}
David~M Knigge, David~W Romero, Albert Gu, Efstratios Gavves, Erik~J Bekkers, Jakub~Mikolaj Tomczak, Mark Hoogendoorn, and {Jan-jakob} Sonke.
\newblock Modelling long range dependencies in {$N$D}: From task-specific to a general purpose {CNN}.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Lampinen et~al.(2021)Lampinen, Chan, Banino, and Hill]{lampinen2021towards}
Andrew~Kyle Lampinen, Stephanie~C.Y. Chan, Andrea Banino, and Felix Hill.
\newblock Towards mental time travel: a hierarchical memory for reinforcement learning agents.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Lu et~al.(2023)Lu, Schroecker, Gu, Parisotto, Foerster, Singh, and Behbahani]{lu2023structured}
Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani.
\newblock Structured state space models for in-context reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2303.03982}, 2023.

\bibitem[Mattar and Lengyel(2022)]{mattar22}
Marcelo~G Mattar and Máté Lengyel.
\newblock Planning in the brain.
\newblock \emph{Neuron}, 110\penalty0 (6):\penalty0 914--934, 2022.

\bibitem[Mehta et~al.(2023)Mehta, Gupta, Cutkosky, and Neyshabur]{gss}
Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur.
\newblock Long range language modeling via gated state spaces.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Micheli et~al.(2023)Micheli, Alonso, and Fleuret]{iris}
Vincent Micheli, Eloi Alonso, and Fran{\c{c}}ois Fleuret.
\newblock {Transformers} are sample-efficient world models.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Moerland et~al.(2023)Moerland, Broekens, Plaat, and Jonker]{mbrlsurvey}
Thomas~M Moerland, Joost Broekens, Aske Plaat, and Catholijn~M Jonker.
\newblock Model-based reinforcement learning: A survey.
\newblock \emph{Foundations and Trends® in Machine Learning}, 16\penalty0 (1):\penalty0 1--118, 2023.

\bibitem[Nguyen et~al.(2022)Nguyen, Goel, Gu, Downs, Shah, Dao, Baccus, and R{\'e}]{s4nd}
Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R{\'e}.
\newblock {S4ND}: Modeling images and videos as multidimensional signals with state spaces.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Nguyen et~al.(2021)Nguyen, Shu, Pham, Bui, and Ermon]{tpc}
Tung~D Nguyen, Rui Shu, Tuan Pham, Hung Bui, and Stefano Ermon.
\newblock Temporal predictive coding for model-based planning in latent space.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Okada and Taniguchi(2021)]{dreaming}
Masashi Okada and Tadahiro Taniguchi.
\newblock Dreaming: Model-based reinforcement learning by latent imagination without reconstruction.
\newblock In \emph{ICRA}, 2021.

\bibitem[Okada and Taniguchi(2022)]{dreamingv2}
Masashi Okada and Tadahiro Taniguchi.
\newblock {DreamingV2}: Reinforcement learning with discrete world models without reconstruction.
\newblock In \emph{IROS}, 2022.

\bibitem[Orvieto et~al.(2023)Orvieto, Smith, Gu, Fernando, Gulcehre, Pascanu, and De]{orvieto2023resurrecting}
Antonio Orvieto, Samuel~L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De.
\newblock Resurrecting recurrent neural networks for long sequences.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Parisotto et~al.(2020)Parisotto, Song, Rae, Pascanu, Gulcehre, Jayakumar, Jaderberg, Kaufman, Clark, Noury, Botvinick, Heess, and Hadsell]{parisotto2020stabilizing}
Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar, Max Jaderberg, Rapha{\"e}l~Lopez Kaufman, Aidan Clark, Seb Noury, Matthew Botvinick, Nicolas Heess, and Raia Hadsell.
\newblock Stabilizing {Transformers} for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and Bengio]{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2013.

\bibitem[Pa{\v{s}}ukonis et~al.(2023)Pa{\v{s}}ukonis, Lillicrap, and Hafner]{memory_maze}
Jurgis Pa{\v{s}}ukonis, Timothy Lillicrap, and Danijar Hafner.
\newblock Evaluating long-term memory in {3D} mazes.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Pearson(2019)]{pearson2019human}
Joel Pearson.
\newblock The human imagination: the cognitive neuroscience of visual mental imagery.
\newblock \emph{Nature Reviews Neuroscience}, 20\penalty0 (10):\penalty0 624--634, 2019.

\bibitem[Pleines et~al.(2023)Pleines, Pallasch, Zimmer, and Preuss]{memory_gym}
Marco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss.
\newblock Memory {Gym}: Partially observable challenges to memory-based agents.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Ramachandran et~al.(2017)Ramachandran, Zoph, and Le]{silu}
Prajit Ramachandran, Barret Zoph, and Quoc~V Le.
\newblock Searching for activation functions.
\newblock \emph{arXiv preprint arXiv:1710.05941}, 2017.

\bibitem[Robine et~al.(2023)Robine, H{\"o}ftmann, Uelwer, and Harmeling]{robine2023transformer}
Jan Robine, Marc H{\"o}ftmann, Tobias Uelwer, and Stefan Harmeling.
\newblock {Transformer}-based world models are happy with 100k interactions.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Saxena et~al.(2021)Saxena, Ba, and Hafner]{cwvae}
Vaibhav Saxena, Jimmy Ba, and Danijar Hafner.
\newblock Clockwork variational autoencoders.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Seo et~al.(2023)Seo, Hafner, Liu, Liu, James, Lee, and Abbeel]{seo2023masked}
Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee, and Pieter Abbeel.
\newblock Masked world models for visual control.
\newblock In \emph{CoRL}, 2023.

\bibitem[Smith et~al.(2023)Smith, Warrington, and Linderman]{s5}
Jimmy~T.H. Smith, Andrew Warrington, and Scott Linderman.
\newblock Simplified state space layers for sequence modeling.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Suddendorf et~al.(2009)Suddendorf, Addis, and Corballis]{suddendorf2009mental}
Thomas Suddendorf, Donna~Rose Addis, and Michael~C Corballis.
\newblock Mental time travel and the shaping of the human mind.
\newblock \emph{Philosophical Transactions of the Royal Society B: Biological Sciences}, 364\penalty0 (1521):\penalty0 1317--1324, 2009.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang, Ruder, and Metzler]{longrangearena}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock {Long Range Arena}: A benchmark for efficient {Transformers}.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Tulving(1985)]{tulving1985memory}
Endel Tulving.
\newblock Memory and consciousness.
\newblock \emph{Canadian Psychology/Psychologie canadienne}, 26\penalty0 (1):\penalty0 1, 1985.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Wang et~al.(2023)Wang, Zhu, Wang, Yu, Liu, Omar, and Hamid]{selective_s4}
Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu, Mohamed Omar, and Raffay Hamid.
\newblock Selective structured state-spaces for long-form video understanding.
\newblock In \emph{CVPR}, 2023.

\bibitem[Wang et~al.(2022)Wang, Mu, Li, Zhang, Zhao, Zhuang, Luo, Wang, and Hao]{wang2022prototypical}
Junjie Wang, Yao Mu, Dong Li, Qichao Zhang, Dongbin Zhao, Yuzheng Zhuang, Ping Luo, Bin Wang, and Jianye Hao.
\newblock Prototypical context-aware dynamics generalization for high-dimensional model-based reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2211.12774}, 2022.

\bibitem[Wu et~al.(2023{\natexlab{a}})Wu, Ma, Deng, and Long]{wu2023pre}
Jialong Wu, Haoyu Ma, Chaoyi Deng, and Mingsheng Long.
\newblock Pre-training contextualized world models with in-the-wild videos for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2305.18499}, 2023{\natexlab{a}}.

\bibitem[Wu et~al.(2023{\natexlab{b}})Wu, Escontrela, Hafner, Abbeel, and Goldberg]{daydreamer}
Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg.
\newblock {D}ay{D}reamer: World models for physical robot learning.
\newblock In \emph{CoRL}, 2023{\natexlab{b}}.

\bibitem[Wydmuch et~al.(2018)Wydmuch, Kempka, and Ja{\'s}kowski]{wydmuch2018vizdoom}
Marek Wydmuch, Micha{\l} Kempka, and Wojciech Ja{\'s}kowski.
\newblock {ViZDoom} competitions: Playing {Doom} from pixels.
\newblock \emph{IEEE Transactions on Games}, 11\penalty0 (3):\penalty0 248--259, 2018.

\bibitem[Yan et~al.(2023)Yan, Hafner, James, and Abbeel]{teco}
Wilson Yan, Danijar Hafner, Stephen James, and Pieter Abbeel.
\newblock Temporally consistent {Transformers} for video generation.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Ying et~al.(2023)Ying, Hao, Zhou, Su, Liu, Li, Yan, and Zhu]{ying2023reward}
Chengyang Ying, Zhongkai Hao, Xinning Zhou, Hang Su, Songming Liu, Jialian Li, Dong Yan, and Jun Zhu.
\newblock Reward informed dreamer for task generalization in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2303.05092}, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Poli, Xu, Massaroli, and Ermon]{ls4}
Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon.
\newblock Deep latent state space models for time-series generation.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Zuo et~al.(2022)Zuo, Liu, Jiao, Charles, Manavoglu, Zhao, and Gao]{spade}
Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao.
\newblock Efficient long sequence modeling via state space augmented {Transformer}.
\newblock \emph{arXiv preprint arXiv:2212.08136}, 2022.

\end{thebibliography}
