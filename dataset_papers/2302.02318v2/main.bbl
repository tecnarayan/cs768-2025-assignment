\begin{thebibliography}{94}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Afham et~al.(2022)Afham, Dissanayake, Dissanayake, Dharmasiri,
  Thilakarathna, and Rodrigo]{CrossPoint22}
Afham, M., Dissanayake, I., Dissanayake, D., Dharmasiri, A., Thilakarathna, K.,
  and Rodrigo, R.
\newblock Crosspoint: Self-supervised cross-modal contrastive learning for 3d
  point cloud understanding.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, 2022.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, Ring, Rutherford, Cabi, Han, Gong, Samangooei,
  Monteiro, Menick, Borgeaud, Brock, Nematzadeh, Sharifzadeh, Binkowski,
  Barreira, Vinyals, Zisserman, and Simonyan]{Flamingo22}
Alayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K.,
  Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S.,
  Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S.,
  Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R.,
  Vinyals, O., Zisserman, A., and Simonyan, K.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NeurIPS)}, 2022.

\bibitem[Bachman et~al.(2019)Bachman, Hjelm, and Buchwalter]{AugDeepInfoMax19}
Bachman, P., Hjelm, R.~D., and Buchwalter, W.
\newblock Learning representations by maximizing mutual information across
  views.
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NeurIPS)}, 2019.

\bibitem[Bao et~al.(2022)Bao, Dong, Piao, and Wei]{BEiT}
Bao, H., Dong, L., Piao, S., and Wei, F.
\newblock Beit: {BERT} pre-training of image transformers.
\newblock In \emph{Int. Conf. Learn. Represent. (ICLR)}. OpenReview.net, 2022.

\bibitem[Belghazi et~al.(2018)Belghazi, Baratin, Rajeswar, Ozair, Bengio,
  Hjelm, and Courville]{MINE18}
Belghazi, M.~I., Baratin, A., Rajeswar, S., Ozair, S., Bengio, Y., Hjelm,
  R.~D., and Courville, A.~C.
\newblock Mutual information neural estimation.
\newblock In \emph{Proc. Int. Conf. Mach. Learn. (ICML)}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  530--539. {PMLR},
  2018.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill, Brynjolfsson, Buch, Card,
  Castellon, Chatterji, Chen, Creel, Davis, Demszky, Donahue, Doumbouya,
  Durmus, Ermon, Etchemendy, Ethayarajh, Fei{-}Fei, Finn, Gale, Gillespie,
  Goel, Goodman, Grossman, Guha, Hashimoto, Henderson, Hewitt, Ho, Hong, Hsu,
  Huang, Icard, Jain, Jurafsky, Kalluri, Karamcheti, Keeling, Khani, Khattab,
  Koh, Krass, Krishna, Kuditipudi, and et~al.]{FoundationModel21}
Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S.,
  Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E.,
  Buch, S., Card, D., Castellon, R., Chatterji, N.~S., Chen, A.~S., Creel, K.,
  Davis, J.~Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S.,
  Etchemendy, J., Ethayarajh, K., Fei{-}Fei, L., Finn, C., Gale, T., Gillespie,
  L., Goel, K., Goodman, N.~D., Grossman, S., Guha, N., Hashimoto, T.,
  Henderson, P., Hewitt, J., Ho, D.~E., Hong, J., Hsu, K., Huang, J., Icard,
  T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani,
  F., Khattab, O., Koh, P.~W., Krass, M.~S., Krishna, R., Kuditipudi, R., and
  et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{CoRR}, abs/2108.07258, 2021.

\bibitem[Boser et~al.(1992)Boser, Guyon, and Vapnik]{MarginSVM92}
Boser, B.~E., Guyon, I., and Vapnik, V.
\newblock A training algorithm for optimal margin classifiers.
\newblock In \emph{ACM Conf. Comput. Learn. Theory (COLT)}, pp.\  144--152.
  {ACM}, 1992.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{GPT3_20}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert{-}Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A.,
  Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin,
  M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A.,
  Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NeurIPS)}, 2020.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'{e}}gou, Mairal,
  Bojanowski, and Joulin]{DINO21}
Caron, M., Touvron, H., Misra, I., J{\'{e}}gou, H., Mairal, J., Bojanowski, P.,
  and Joulin, A.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{Int. Conf. Comput. Vis. (ICCV)}, pp.\  9630--9640. {IEEE},
  2021.

\bibitem[Chang et~al.(2015)Chang, Funkhouser, Guibas, Hanrahan, Huang, Li,
  Savarese, Savva, Song, Su, Xiao, Yi, and Yu]{ShapeNet15}
Chang, A.~X., Funkhouser, T.~A., Guibas, L.~J., Hanrahan, P., Huang, Q., Li,
  Z., Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., and Yu, F.
\newblock Shapenet: An information-rich 3d model repository.
\newblock \emph{CoRR}, abs/1512.03012, 2015.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{SimCLR}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.~E.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{Proc. Int. Conf. Mach. Learn. (ICML)}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  1597--1607. {PMLR},
  2020.

\bibitem[Chen \& He(2021)Chen and He]{SimSiam}
Chen, X. and He, K.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, pp.\
  15750--15758, 2021.

\bibitem[Chen et~al.(2021)Chen, Xie, and He]{MoCoThree21}
Chen, X., Xie, S., and He, K.
\newblock An empirical study of training self-supervised vision transformers.
\newblock In \emph{Int. Conf. Comput. Vis. (ICCV)}, pp.\  9620--9629. {IEEE},
  2021.

\bibitem[Chen et~al.(2022)Chen, Nie{\ss}ner, and Dai]{4DContrast22}
Chen, Y., Nie{\ss}ner, M., and Dai, A.
\newblock 4dcontrast: Contrastive learning with dynamic correspondences for 3d
  scene understanding.
\newblock In \emph{Eur. Conf. Comput. Vis. (ECCV)}, 2022.

\bibitem[Dangovski et~al.(2022)Dangovski, Jing, Loh, Han, Srivastava, Cheung,
  Agrawal, and Soljacic]{EquivariantSSL22}
Dangovski, R., Jing, L., Loh, C., Han, S., Srivastava, A., Cheung, B., Agrawal,
  P., and Soljacic, M.
\newblock Equivariant self-supervised learning: Encouraging equivariance in
  representations.
\newblock In \emph{Int. Conf. Learn. Represent. (ICLR)}. OpenReview.net, 2022.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei{-}Fei]{ImageNet09}
Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Fei{-}Fei, L.
\newblock Imagenet: {A} large-scale hierarchical image database.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, 2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{BERT}
Devlin, J., Chang, M., Lee, K., and Toutanova, K.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume
  1 (Long and Short Papers)}, pp.\  4171--4186. Association for Computational
  Linguistics, 2019.

\bibitem[Dong et~al.(2023)Dong, Qi, Zhang, Zhang, Sun, Ge, Yi, and Ma]{ACT23}
Dong, R., Qi, Z., Zhang, L., Zhang, J., Sun, J., Ge, Z., Yi, L., and Ma, K.
\newblock Autoencoders as cross-modal teachers: Can pretrained 2d image
  transformers help 3d representation learning?
\newblock In \emph{Int. Conf. Learn. Represent. (ICLR)}, 2023.

\bibitem[Dong et~al.(2022)Dong, Bao, Zhang, Chen, Gu, Zhang, Yuan, Chen, Wen,
  and Yu]{TuneCLIP22}
Dong, X., Bao, J., Zhang, T., Chen, D., Gu, S., Zhang, W., Yuan, L., Chen, D.,
  Wen, F., and Yu, N.
\newblock {CLIP} itself is a strong fine-tuner: Achieving 85.7{\%} and 88.0{\%}
  top-1 accuracy with vit-b and vit-l on imagenet.
\newblock \emph{CoRR}, abs/2212.06138, 2022.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{ViT}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{Int. Conf. Learn. Represent. (ICLR)}, 2021.

\bibitem[Ermolov et~al.(2021)Ermolov, Siarohin, Sangineto, and
  Sebe]{WhiteContrast21}
Ermolov, A., Siarohin, A., Sangineto, E., and Sebe, N.
\newblock Whitening for self-supervised representation learning.
\newblock In \emph{Proc. Int. Conf. Mach. Learn. (ICML)}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  3015--3024. {PMLR},
  2021.

\bibitem[Faghri et~al.(2018)Faghri, Fleet, Kiros, and Fidler]{VSEPP}
Faghri, F., Fleet, D.~J., Kiros, J.~R., and Fidler, S.
\newblock {VSE++:} improving visual-semantic embeddings with hard negatives.
\newblock In \emph{Brit. Mach. Vis. Conf. (BMVC)}, pp.\ ~12. {BMVA} Press,
  2018.

\bibitem[Fan et~al.(2017)Fan, Su, and Guibas]{ChamferDistance17}
Fan, H., Su, H., and Guibas, L.~J.
\newblock A point set generation network for 3d object reconstruction from a
  single image.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, 2017.

\bibitem[Goyal et~al.(2021)Goyal, Law, Liu, Newell, and Deng]{SimpleView}
Goyal, A., Law, H., Liu, B., Newell, A., and Deng, J.
\newblock Revisiting point cloud shape classification with a simple and
  effective baseline.
\newblock In \emph{Proc. Int. Conf. Mach. Learn. (ICML)}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  3809--3820. {PMLR},
  2021.

\bibitem[Goyal et~al.(2019)Goyal, Mahajan, Gupta, and Misra]{BenchmarkSSL19}
Goyal, P., Mahajan, D., Gupta, A., and Misra, I.
\newblock Scaling and benchmarking self-supervised visual representation
  learning.
\newblock In \emph{Int. Conf. Comput. Vis. (ICCV)}, pp.\  6390--6399. {IEEE},
  2019.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'{e}}, Tallec, Richemond,
  Buchatskaya, Doersch, Pires, Guo, Azar, Piot, Kavukcuoglu, Munos, and
  Valko]{BYOL}
Grill, J., Strub, F., Altch{\'{e}}, F., Tallec, C., Richemond, P.~H.,
  Buchatskaya, E., Doersch, C., Pires, B.~{\'{A}}., Guo, Z., Azar, M.~G., Piot,
  B., Kavukcuoglu, K., Munos, R., and Valko, M.
\newblock Bootstrap your own latent - {A} new approach to self-supervised
  learning.
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NeurIPS)}, 2020.

\bibitem[Guo et~al.(2021)Guo, Cai, Liu, Mu, Martin, and Hu]{PCT}
Guo, M., Cai, J., Liu, Z., Mu, T., Martin, R.~R., and Hu, S.
\newblock {PCT:} point cloud transformer.
\newblock \emph{Comput. Vis. Media}, 7\penalty0 (2):\penalty0 187--199, 2021.

\bibitem[Hadsell et~al.(2006)Hadsell, Chopra, and LeCun]{LeCunContrastive}
Hadsell, R., Chopra, S., and LeCun, Y.
\newblock Dimensionality reduction by learning an invariant mapping.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, pp.\
  1735--1742, 2006.

\bibitem[Hamdi et~al.(2021)Hamdi, Giancola, and Ghanem]{MVTN}
Hamdi, A., Giancola, S., and Ghanem, B.
\newblock {MVTN:} multi-view transformation network for 3d shape recognition.
\newblock In \emph{Int. Conf. Comput. Vis. (ICCV)}, pp.\  1--11. {IEEE}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{ResNet16}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, pp.\
  770--778. {IEEE} Computer Society, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{MoCo}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.~B.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, pp.\
  9726--9735. Computer Vision Foundation / {IEEE}, 2020.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'{a}}r, and Girshick]{MAE}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'{a}}r, P., and Girshick, R.~B.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, 2022.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{HintonKD15}
Hinton, G.~E., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NeurIPS)}, volume
  abs/1503.02531, 2015.

\bibitem[Hjelm et~al.(2019)Hjelm, Fedorov, Lavoie{-}Marchildon, Grewal,
  Bachman, Trischler, and Bengio]{DeepInfoMax19}
Hjelm, R.~D., Fedorov, A., Lavoie{-}Marchildon, S., Grewal, K., Bachman, P.,
  Trischler, A., and Bengio, Y.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock In \emph{Int. Conf. Learn. Represent. (ICLR)}, 2019.

\bibitem[Huang et~al.(2022)Huang, Dong, Yang, Huang, Lau, Ouyang, and
  Zuo]{CLIP2Point22}
Huang, T., Dong, B., Yang, Y., Huang, X., Lau, R. W.~H., Ouyang, W., and Zuo,
  W.
\newblock Clip2point: Transfer {CLIP} to point cloud classification with
  image-depth pre-training.
\newblock \emph{CoRR}, abs/2210.01055, 2022.

\bibitem[Jing et~al.(2021)Jing, Vahdani, Tan, and Tian]{CenterLoss21}
Jing, L., Vahdani, E., Tan, J., and Tian, Y.
\newblock Cross-modal center loss for 3d cross-modal retrieval.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, pp.\
  3142--3151. Computer Vision Foundation / {IEEE}, 2021.

\bibitem[Khosla et~al.(2020)Khosla, Teterwak, Wang, Sarna, Tian, Isola,
  Maschinot, Liu, and Krishnan]{SCL20}
Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot,
  A., Liu, C., and Krishnan, D.
\newblock Supervised contrastive learning.
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NeurIPS)}, 2020.

\bibitem[Kong \& Zhang(2023)Kong and Zhang]{InvariantMIM22}
Kong, X. and Zhang, X.
\newblock Understanding masked image modeling via learning occlusion invariant
  feature.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, 2023.

\bibitem[Kosmann-Schwarzbach(2011)]{InvarianceTheory11}
Kosmann-Schwarzbach, Y.
\newblock \emph{The Noether Theorems}, pp.\  55--64.
\newblock Springer New York, New York, NY, 2011.
\newblock ISBN 978-0-387-87868-3.
\newblock \doi{10.1007/978-0-387-87868-3_3}.
\newblock URL \url{https://doi.org/10.1007/978-0-387-87868-3_3}.

\bibitem[Li et~al.(2021)Li, Selvaraju, Gotmare, Joty, Xiong, and Hoi]{albef21}
Li, J., Selvaraju, R.~R., Gotmare, A., Joty, S.~R., Xiong, C., and Hoi, S.~C.
\newblock Align before fuse: Vision and language representation learning with
  momentum distillation.
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NeurIPS)}, pp.\
  9694--9705, 2021.

\bibitem[Li et~al.(2023)Li, Li, Savarese, and Hoi]{BLIP223}
Li, J., Li, D., Savarese, S., and Hoi, S. C.~H.
\newblock {BLIP-2:} bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock \emph{CoRR}, abs/2301.12597, 2023.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Zhang, Zhang, Yang, Li, Zhong, Wang,
  Yuan, Zhang, Hwang, Chang, and Gao]{GLIP22}
Li, L.~H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan,
  L., Zhang, L., Hwang, J., Chang, K., and Gao, J.
\newblock Grounded language-image pre-training.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, pp.\
  10955--10965. {IEEE}, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2018)Li, Bu, Sun, Wu, Di, and Chen]{PointCNN}
Li, Y., Bu, R., Sun, M., Wu, W., Di, X., and Chen, B.
\newblock Pointcnn: Convolution on x-transformed points.
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NeurIPS)}, pp.\
  828--838, 2018.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Fan, Hu, Feichtenhofer, and
  He]{FLIP22}
Li, Y., Fan, H., Hu, R., Feichtenhofer, C., and He, K.
\newblock Scaling language-image pre-training via masking.
\newblock \emph{CoRR}, abs/2212.00794, 2022{\natexlab{b}}.

\bibitem[Liu et~al.(2022)Liu, Cai, and Lee]{MaskPoint}
Liu, H., Cai, M., and Lee, Y.~J.
\newblock Masked discrimination for self-supervised learning on point clouds.
\newblock In \emph{Eur. Conf. Comput. Vis. (ECCV)}, 2022.

\bibitem[Liu et~al.(2019)Liu, Fan, Xiang, and Pan]{RSCNN}
Liu, Y., Fan, B., Xiang, S., and Pan, C.
\newblock Relation-shape convolutional neural network for point cloud analysis.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, pp.\
  8895--8904. Computer Vision Foundation / {IEEE}, 2019.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Fan, Zhang, Dong, Funkhouser, and
  Yi]{TupleInfoNCE21}
Liu, Y., Fan, Q., Zhang, S., Dong, H., Funkhouser, T.~A., and Yi, L.
\newblock Contrastive multimodal fusion with tupleinfonce.
\newblock In \emph{Int. Conf. Comput. Vis. (ICCV)}, pp.\  734--743. {IEEE},
  2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{SwinT}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{Int. Conf. Comput. Vis. (ICCV)}, pp.\  9992--10002. {IEEE},
  2021{\natexlab{b}}.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{CosineLRSGDR}
Loshchilov, I. and Hutter, F.
\newblock {SGDR:} stochastic gradient descent with warm restarts.
\newblock In \emph{Int. Conf. Learn. Represent. (ICLR)}. OpenReview.net, 2017.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{AdamW19}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{Int. Conf. Learn. Represent. (ICLR)}, 2019.

\bibitem[Ma et~al.(2022)Ma, Qin, You, Ran, and Fu]{PointMLP}
Ma, X., Qin, C., You, H., Ran, H., and Fu, Y.
\newblock Rethinking network design and local geometry in point cloud: {A}
  simple residual {MLP} framework.
\newblock In \emph{Int. Conf. Learn. Represent. (ICLR)}. OpenReview.net, 2022.

\bibitem[OpenAI(2022)]{ChatGPT}
OpenAI.
\newblock Introducing chatgpt.
\newblock 2022.
\newblock URL \url{https://openai.com/blog/chatgpt}.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell,
  Welinder, Christiano, Leike, and Lowe]{InstructGPT22}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.~L., Mishkin, P.,
  Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton,
  F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P.~F.,
  Leike, J., and Lowe, R.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{CoRR}, abs/2203.02155, 2022.

\bibitem[Pang et~al.(2022)Pang, Wang, Tay, Liu, Tian, and Yuan]{PointMAE}
Pang, Y., Wang, W., Tay, F. E.~H., Liu, W., Tian, Y., and Yuan, L.
\newblock Masked autoencoders for point cloud self-supervised learning.
\newblock In \emph{Eur. Conf. Comput. Vis. (ECCV)}, 2022.

\bibitem[Park \& Kwak(2020)Park and Kwak]{Feed20}
Park, S. and Kwak, N.
\newblock Feature-level ensemble knowledge distillation for aggregating
  knowledge from multiple networks.
\newblock In Giacomo, G.~D., Catal{\'{a}}, A., Dilkina, B., Milano, M., Barro,
  S., Bugar{\'{\i}}n, A., and Lang, J. (eds.), \emph{Eur. Conf. Artif. Intell.
  (ECAI)}, volume 325 of \emph{Frontiers in Artificial Intelligence and
  Applications}, pp.\  1411--1418. {IOS} Press, 2020.

\bibitem[Qi et~al.(2017{\natexlab{a}})Qi, Su, Mo, and Guibas]{PointNet}
Qi, C.~R., Su, H., Mo, K., and Guibas, L.~J.
\newblock Pointnet: Deep learning on point sets for 3d classification and
  segmentation.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, pp.\
  77--85, 2017{\natexlab{a}}.

\bibitem[Qi et~al.(2017{\natexlab{b}})Qi, Yi, Su, and Guibas]{PointNet++}
Qi, C.~R., Yi, L., Su, H., and Guibas, L.~J.
\newblock Pointnet++: Deep hierarchical feature learning on point sets in a
  metric space.
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NIPS)}, pp.\
  5099--5108, 2017{\natexlab{b}}.

\bibitem[Qian et~al.(2022)Qian, Li, Peng, Mai, Hammoud, Elhoseiny, and
  Ghanem]{PointNext}
Qian, G., Li, Y., Peng, H., Mai, J., Hammoud, H. A. A.~K., Elhoseiny, M., and
  Ghanem, B.
\newblock Pointnext: Revisiting pointnet++ with improved training and scaling
  strategies.
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NeurIPS)}, 2022.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, Sutskever,
  et~al.]{GPTv1_18}
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et~al.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever]{CLIP}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{Proc. Int. Conf. Mach. Learn. (ICML)}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  8748--8763. {PMLR},
  2021.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{DALL-E}
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and
  Sutskever, I.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{Proc. Int. Conf. Mach. Learn. (ICML)}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  8821--8831. {PMLR},
  2021.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and
  Ommer]{StableDiffusion22}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, pp.\
  10674--10685. {IEEE}, 2022.

\bibitem[Ruder(2017)]{MTLOverview}
Ruder, S.
\newblock An overview of multi-task learning in deep neural networks.
\newblock \emph{CoRR}, abs/1706.05098, 2017.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman,
  Cherti, Coombes, Katta, Mullis, Wortsman, Schramowski, Kundurthy, Crowson,
  Schmidt, Kaczmarczyk, and Jitsev]{LAION5B2022}
Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M.,
  Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy,
  S., Crowson, K., Schmidt, L., Kaczmarczyk, R., and Jitsev, J.
\newblock {LAION-5B:} an open large-scale dataset for training next generation
  image-text models.
\newblock \emph{CoRR}, abs/2210.08402, 2022.

\bibitem[Tao et~al.(2022)Tao, Zhu, Huang, Qiao, Wang, and Dai]{sim22}
Tao, C., Zhu, X., Huang, G., Qiao, Y., Wang, X., and Dai, J.
\newblock Siamese image modeling for self-supervised vision representation
  learning.
\newblock \emph{CoRR}, abs/2206.01204, 2022.

\bibitem[Tian et~al.(2020{\natexlab{a}})Tian, Krishnan, and Isola]{CMC20}
Tian, Y., Krishnan, D., and Isola, P.
\newblock Contrastive multiview coding.
\newblock In Vedaldi, A., Bischof, H., Brox, T., and Frahm, J. (eds.),
  \emph{Eur. Conf. Comput. Vis. (ECCV)}, volume 12356 of \emph{Lecture Notes in
  Computer Science}, pp.\  776--794. Springer, 2020{\natexlab{a}}.

\bibitem[Tian et~al.(2020{\natexlab{b}})Tian, Krishnan, and Isola]{CRD20}
Tian, Y., Krishnan, D., and Isola, P.
\newblock Contrastive representation distillation.
\newblock In \emph{Int. Conf. Learn. Represent. (ICLR)}, 2020{\natexlab{b}}.

\bibitem[Tian et~al.(2020{\natexlab{c}})Tian, Sun, Poole, Krishnan, Schmid, and
  Isola]{GoodViewContrast20}
Tian, Y., Sun, C., Poole, B., Krishnan, D., Schmid, C., and Isola, P.
\newblock What makes for good views for contrastive learning?
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NeurIPS)},
  2020{\natexlab{c}}.

\bibitem[Uy et~al.(2019)Uy, Pham, Hua, Nguyen, and Yeung]{ScanObjectNN19}
Uy, M.~A., Pham, Q.-H., Hua, B.-S., Nguyen, T., and Yeung, S.-K.
\newblock Revisiting point cloud classification: A new benchmark dataset and
  classification model on real-world data.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, pp.\
  1588--1597, 2019.

\bibitem[van~den Oord et~al.(2018)van~den Oord, Li, and Vinyals]{InfoNCE}
van~den Oord, A., Li, Y., and Vinyals, O.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{CoRR}, abs/1807.03748, 2018.

\bibitem[Vapnik(1998)]{SLTheory98}
Vapnik, V.
\newblock \emph{Statistical learning theory}.
\newblock Wiley, 1998.
\newblock ISBN 978-0-471-03003-4.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{AttentionIsAllYouNeed}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NIPS)}, pp.\
  5998--6008, 2017.

\bibitem[Wang et~al.(2021)Wang, Liu, Yue, Lasenby, and Kusner]{OcCo}
Wang, H., Liu, Q., Yue, X., Lasenby, J., and Kusner, M.~J.
\newblock Unsupervised point cloud pre-training via occlusion completion.
\newblock In \emph{Int. Conf. Comput. Vis. (ICCV)}, pp.\  9782--9792, 2021.

\bibitem[Wang \& Yoon(2022)Wang and Yoon]{KDOverview22}
Wang, L. and Yoon, K.
\newblock Knowledge distillation and student-teacher learning for visual
  intelligence: {A} review and new outlooks.
\newblock \emph{IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI)}, 44\penalty0
  (6):\penalty0 3048--3068, 2022.

\bibitem[Wang \& Qi(2021)Wang and Qi]{StrongAugContrast21}
Wang, X. and Qi, G.
\newblock Contrastive learning with stronger augmentations.
\newblock \emph{CoRR}, abs/2104.07713, 2021.

\bibitem[Wang et~al.(2019)Wang, Sun, Liu, Sarma, Bronstein, and Solomon]{DGCNN}
Wang, Y., Sun, Y., Liu, Z., Sarma, S.~E., Bronstein, M.~M., and Solomon, J.~M.
\newblock Dynamic graph {CNN} for learning on point clouds.
\newblock \emph{ACM Trans. Graph.}, 38\penalty0 (5):\penalty0 146:1--146:12,
  2019.

\bibitem[Wang et~al.(2022)Wang, Yu, Rao, Zhou, and Lu]{P2P}
Wang, Z., Yu, X., Rao, Y., Zhou, J., and Lu, J.
\newblock {P2P:} tuning pre-trained image models for point cloud analysis with
  point-to-pixel prompting.
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NeurIPS)}, 2022.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Fan, Xie, Wu, Yuille, and
  Feichtenhofer]{MaskFeat}
Wei, C., Fan, H., Xie, S., Wu, C.-Y., Yuille, A., and Feichtenhofer, C.
\newblock Masked feature prediction for self-supervised visual pre-training.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)},
  2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, brian
  ichter, Xia, Chi, Le, and Zhou]{CoT22}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi,
  E.~H., Le, Q.~V., and Zhou, D.
\newblock Chain of thought prompting elicits reasoning in large language
  models.
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NeurIPS)},
  2022{\natexlab{b}}.

\bibitem[Wu et~al.(2015)Wu, Song, Khosla, Yu, Zhang, Tang, and
  Xiao]{ModelNet15}
Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., and Xiao, J.
\newblock 3d shapenets: A deep representation for volumetric shapes.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, pp.\
  1912--1920, 2015.

\bibitem[Wu et~al.(2018)Wu, Xiong, Yu, and Lin]{InstanceDiscrimination18}
Wu, Z., Xiong, Y., Yu, S.~X., and Lin, D.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)}, pp.\
  3733--3742. Computer Vision Foundation / {IEEE} Computer Society, 2018.

\bibitem[Xiang et~al.(2020)Xiang, Ding, and Han]{MultiExpertKD}
Xiang, L., Ding, G., and Han, J.
\newblock Learning from multiple experts: Self-paced knowledge distillation for
  long-tailed classification.
\newblock In \emph{Eur. Conf. Comput. Vis. (ECCV)}, volume 12350 of
  \emph{Lecture Notes in Computer Science}, pp.\  247--263. Springer, 2020.

\bibitem[Xie et~al.(2020)Xie, Gu, Guo, Qi, Guibas, and Litany]{PointContrast20}
Xie, S., Gu, J., Guo, D., Qi, C.~R., Guibas, L.~J., and Litany, O.
\newblock Pointcontrast: Unsupervised pre-training for 3d point cloud
  understanding.
\newblock In \emph{Eur. Conf. Comput. Vis. (ECCV)}, volume 12348 of
  \emph{Lecture Notes in Computer Science}, pp.\  574--591. Springer, 2020.

\bibitem[Xie et~al.(2022{\natexlab{a}})Xie, Geng, Hu, Zhang, Hu, and
  Cao]{DarkMIM22}
Xie, Z., Geng, Z., Hu, J., Zhang, Z., Hu, H., and Cao, Y.
\newblock Revealing the dark secrets of masked image modeling.
\newblock \emph{CoRR}, abs/2205.13543, 2022{\natexlab{a}}.

\bibitem[Xie et~al.(2022{\natexlab{b}})Xie, Zhang, Cao, Lin, Wei, Dai, and
  Hu]{ScaleMIM22}
Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Wei, Y., Dai, Q., and Hu, H.
\newblock On data scaling in masked image modeling.
\newblock \emph{CoRR}, abs/2206.04664, 2022{\natexlab{b}}.

\bibitem[Yi et~al.(2016)Yi, Kim, Ceylan, Shen, Yan, Su, Lu, Huang, Sheffer, and
  Guibas]{ShapeNetPart16}
Yi, L., Kim, V.~G., Ceylan, D., Shen, I.-C., Yan, M., Su, H., Lu, C., Huang,
  Q., Sheffer, A., and Guibas, L.
\newblock A scalable active framework for region annotation in 3d shape
  collections.
\newblock \emph{ACM Trans. Graph.}, 35\penalty0 (6):\penalty0 1--12, 2016.

\bibitem[Yu et~al.(2022{\natexlab{a}})Yu, Wang, Vasudevan, Yeung,
  Seyedhosseini, and Wu]{CoCa22}
Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock \emph{Trans. Mach. Learn. Res. (TMLR)}, 2022{\natexlab{a}}.
\newblock ISSN 2835-8856.

\bibitem[Yu et~al.(2022{\natexlab{b}})Yu, Tang, Rao, Huang, Zhou, and
  Lu]{PointBERT}
Yu, X., Tang, L., Rao, Y., Huang, T., Zhou, J., and Lu, J.
\newblock Point-bert: Pre-training 3d point cloud transformers with masked
  point modeling.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)},
  2022{\natexlab{b}}.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{BarlowTwins21}
Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In \emph{Proc. Int. Conf. Mach. Learn. (ICML)}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  12310--12320. {PMLR},
  2021.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Chen, Zhang, Dong, and
  Ma]{CDS22}
Zhang, L., Chen, X., Zhang, J., Dong, R., and Ma, K.
\newblock Contrastive deep supervision.
\newblock In \emph{Eur. Conf. Comput. Vis. (ECCV)}, volume 13686 of
  \emph{Lecture Notes in Computer Science}, pp.\  1--19. Springer,
  2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Guo, Gao, Fang, Zhao, Wang,
  Qiao, and Li]{PointM2AE22}
Zhang, R., Guo, Z., Gao, P., Fang, R., Zhao, B., Wang, D., Qiao, Y., and Li, H.
\newblock Point-m2{AE}: Multi-scale masked autoencoders for hierarchical point
  cloud pre-training.
\newblock In \emph{Adv. Neural Inform. Process. Syst. (NeurIPS)},
  2022{\natexlab{b}}.

\bibitem[Zhang et~al.(2022{\natexlab{c}})Zhang, Guo, Zhang, Li, Miao, Cui,
  Qiao, Gao, and Li]{PointCLIP22}
Zhang, R., Guo, Z., Zhang, W., Li, K., Miao, X., Cui, B., Qiao, Y., Gao, P.,
  and Li, H.
\newblock Pointclip: Point cloud understanding by {CLIP}.
\newblock In \emph{IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR)},
  2022{\natexlab{c}}.

\bibitem[Zhang et~al.(2021)Zhang, Girdhar, Joulin, and Misra]{DepthContrast21}
Zhang, Z., Girdhar, R., Joulin, A., and Misra, I.
\newblock Self-supervised pretraining of 3d features on any point-cloud.
\newblock In \emph{Int. Conf. Comput. Vis. (ICCV)}, pp.\  10232--10243. {IEEE},
  2021.

\bibitem[Zhou et~al.(2022)Zhou, Wei, Wang, Shen, Xie, Yuille, and Kong]{iBoT}
Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A.~L., and Kong, T.
\newblock ibot: Image {BERT} pre-training with online tokenizer.
\newblock In \emph{Int. Conf. Learn. Represent. (ICLR)}, 2022.

\end{thebibliography}
