\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aly and Chan(1974)]{aly74boundaryvalue}
G.M Aly and W.C Chan.
\newblock Numerical computation of optimal control problems with unknown final
  time.
\newblock \emph{Journal of Mathematical Analysis and Applications}, 45\penalty0
  (2):\penalty0 274--284, 1974.
\newblock ISSN 0022-247X.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, Mcgrew, Tobin, Abbeel, and Zaremba]{andrychowicz2017her}
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,
  Peter Welinder, Bob Mcgrew, Josh Tobin, Pieter Abbeel, and Wojciech Zaremba.
\newblock Hindsight experience replay.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Attias(2003)]{attias2003planning}
H.~Attias.
\newblock Planning by probabilistic inference.
\newblock In \emph{Proceedings of the 9th International Workshop on Artificial
  Intelligence and Statistics}, 2003.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Chen et~al.(2018)Chen, Georgiou, and Pavon]{chen2018optimal}
Yongxin Chen, Tryphon~T. Georgiou, and Michele Pavon.
\newblock Optimal steering of a linear stochastic system to a final probability
  distributionâ€”part iii.
\newblock \emph{IEEE Transactions on Automatic Control}, 63\penalty0
  (9):\penalty0 3112--3118, 2018.

\bibitem[Choi et~al.(2021)Choi, Sharma, Lee, Levine, and
  Gu]{choi2021variational}
Jongwook Choi, Archit Sharma, Honglak Lee, Sergey Levine, and Shixiang~Shane
  Gu.
\newblock Variational empowerment as representation learning for goal-based
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Chua et~al.(2018)Chua, Calandra, Mcallister, and
  Levine]{chua18probabilisticdynamics}
Kurtland Chua, Roberto Calandra, Rowan Mcallister, and Sergey Levine.
\newblock Deep reinforcement learning in a handful of trials using
  probabilistic dynamics models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Ebert et~al.(2017)Ebert, Finn, Lee, and
  Levine]{ebert2017videoprediction}
Frederik Ebert, Chelsea Finn, Alex~X Lee, and Sergey Levine.
\newblock Self-supervised visual planning with temporal skip connections.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2017.

\bibitem[Eysenbach et~al.(2019)Eysenbach, Salakhutdinov, and
  Levine]{eysenbach2019search}
Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine.
\newblock Search on the replay buffer: Bridging planning and reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1906.05253}, 2019.

\bibitem[Fellows et~al.(2019)Fellows, Mahajan, Rudner, and
  Whiteson]{Fellows2019Virel}
Matthew Fellows, Anuj Mahajan, Tim G.~J. Rudner, and Shimon Whiteson.
\newblock {VIREL}: {A} variational inference framework for reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, 2019.

\bibitem[Fu et~al.(2018)Fu, Singh, Ghosh, Yang, and Levine]{fu2018vice}
Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine.
\newblock Variational inverse control with events: A general framework for
  data-driven reward definition.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Neural Information Processing Systems
  (NeurIPS)}, pages 8538--8547. 2018.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and Meger]{fujimoto2018td3}
Scott Fujimoto, Herke van Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Galashov et~al.(2019)Galashov, Jayakumar, Hasenclever, Tirumala,
  Schwarz, Desjardins, Czarnecki, Teh, Pascanu, and
  Heess]{galashov2019information}
Alexandre Galashov, Siddhant~M. Jayakumar, Leonard Hasenclever, Dhruva
  Tirumala, Jonathan Schwarz, Guillaume Desjardins, Wojciech~M. Czarnecki,
  Yee~Whye Teh, Razvan Pascanu, and Nicolas Heess.
\newblock Information asymmetry in {KL}-regularized {RL}.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}, 2019.

\bibitem[Goebel and Raitums(1990)]{goebel90boundaryvalue}
Manfred Goebel and Uldis Raitums.
\newblock Optimal control of two point boundary value problems.
\newblock In H.~J. Sebastian and K.~Tammer, editors, \emph{System Modelling and
  Optimization}, pages 281--290, Berlin, Heidelberg, 1990. Springer Berlin
  Heidelberg.

\bibitem[Grigoriadis and Skelton(1997)]{grigoriadis1997minimum}
Karolos~M. Grigoriadis and Robert~E. Skelton.
\newblock Minimum-energy covariance controllers.
\newblock \emph{Automatica}, 33\penalty0 (4):\penalty0 569--578, 1997.
\newblock ISSN 0005-1098.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018sac}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning},
  2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Hartikainen,
  Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, and
  Levine]{haarnoja2018applications}
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha,
  Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey
  Levine.
\newblock Soft actor-critic algorithms and applications, 2018{\natexlab{b}}.

\bibitem[Hartikainen et~al.(2020)Hartikainen, Geng, Haarnoja, and
  Levine]{hartikainen2019dynamical}
Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, and Sergey Levine.
\newblock Dynamical distance learning for unsupervised and semi-supervised
  skill discovery.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Hoffman et~al.(2009)Hoffman, Freitas, Doucet, and
  Peters]{hoffman2009expectation}
Matthew Hoffman, Nando Freitas, Arnaud Doucet, and Jan Peters.
\newblock An expectation maximization algorithm for continuous markov decision
  processes with arbitrary reward.
\newblock In \emph{Artificial intelligence and statistics}, pages 232--239,
  2009.

\bibitem[Hotz and Skelton(1987)]{hotz1987covariance}
Anthony~F. Hotz and Robert~E. Skelton.
\newblock A covariance control theory.
\newblock In \emph{System Identification and Adaptive Control, Part 2 of 3},
  volume~26 of \emph{Control and Dynamic Systems}, pages 225--276. Academic
  Press, 1987.

\bibitem[Janner et~al.(2019)Janner, Fu, Zhang, and Levine]{janner2019mbpo}
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.
\newblock When to trust your model: Model-based policy optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Kaelbling(1993)]{kaelbling1993goals}
Leslie~P Kaelbling.
\newblock Learning to achieve goals.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, volume vol.2, pages 1094 -- 8, 1993.

\bibitem[Kappen et~al.(2012)Kappen, G{\'o}mez, and Opper]{kappen2012pgm}
H.~J. Kappen, V.~G{\'o}mez, and M.~Opper.
\newblock Optimal control as a graphical model inference problem.
\newblock \emph{Machine Learning}, 87\penalty0 (2):\penalty0 159--182, 2012.

\bibitem[K{\'a}rn{\`y}(1996)]{karny1996towards}
Miroslav K{\'a}rn{\`y}.
\newblock Towards fully probabilistic control design.
\newblock \emph{Automatica}, 32\penalty0 (12):\penalty0 1719--1722, 1996.

\bibitem[Levine(2018)]{levine2018tutorial}
Sergey Levine.
\newblock Reinforcement learning and control as probabilistic inference:
  Tutorial and review.
\newblock 2018.

\bibitem[Levy et~al.(2017)Levy, Konidaris, Platt, and Saenko]{levy2017learning}
Andrew Levy, George Konidaris, Robert Platt, and Kate Saenko.
\newblock Learning multi-level hierarchies with hindsight.
\newblock \emph{arXiv preprint arXiv:1712.00948}, 2017.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Nachum et~al.(2018)Nachum, Brain, Gu, Lee, and Levine]{nachum2018hiro}
Ofir Nachum, Google Brain, Shixiang Gu, Honglak Lee, and Sergey Levine.
\newblock Data-efficient hierarchical reinforcement learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Nagabandi et~al.(2018)Nagabandi, Kahn, Fearing, and
  Levine]{nagaband2018mbmf}
Anusha Nagabandi, Gregory Kahn, Ronald~S Fearing, and Sergey Levine.
\newblock Neural network dynamics for model-based deep reinforcement learning
  with model-free fine-tuning.
\newblock In \emph{IEEE International Conference on Robotics and Automation
  (ICRA)}, 2018.

\bibitem[Nair et~al.(2018)Nair, Pong, Dalal, Bahl, Lin, and
  Levine]{nair2018rig}
Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey
  Levine.
\newblock Visual reinforcement learning with imagined goals.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem[Peters et~al.(2010)Peters, M{\"{u}}lling, and
  Alt{\"{u}}n]{peters2010reps}
Jan Peters, Katharina M{\"{u}}lling, and Yasemin Alt{\"{u}}n.
\newblock Relative entropy policy search.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, pages
  1607--1612, 2010.

\bibitem[Plappert et~al.(2018)Plappert, Andrychowicz, Ray, Mcgrew, Baker,
  Powell, Schneider, Tobin, Chociej, Welinder, Kumar, and
  Zaremba]{Plappert2018}
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob Mcgrew, Bowen Baker,
  Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder,
  Vikash Kumar, and Wojciech Zaremba.
\newblock Multi-goal reinforcement learning: Challenging robotics environments
  and request for research.
\newblock \emph{arXiv preprint arXiv:1802.09464}, 2018.

\bibitem[Pong et~al.(2018)Pong, Gu, Dalal, and Levine]{pong2018tdm}
Vitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey Levine.
\newblock Temporal difference models: Model-free deep {RL} for model-based
  control.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Pong et~al.(2019)Pong, Dalal, Lin, Nair, Bahl, and
  Levine]{pong2019skewfit}
Vitchyr~H. Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and
  Sergey Levine.
\newblock {Skew-Fit}: State-covering self-supervised reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1903.03698}, abs/1903.03698, 2019.

\bibitem[Rawlik et~al.(2013)Rawlik, Toussaint, and Vijayakumar]{rawlik2013soc}
K.~Rawlik, M.~Toussaint, and S.~Vijayakumar.
\newblock On stochastic optimal control and reinforcement learning by
  approximate inference.
\newblock In \emph{Robotics: Science and Systems (RSS)}, 2013.

\bibitem[Rawlik et~al.(2010)Rawlik, Toussaint, and
  Vijayakumar]{rawlik2010approximate}
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar.
\newblock An approximate inference approach to temporal optimization in optimal
  control.
\newblock In J.~Lafferty, C.~Williams, J.~Shawe-Taylor, R.~Zemel, and
  A.~Culotta, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~23. Curran Associates, Inc., 2010.

\bibitem[Ridderhof et~al.(2019)Ridderhof, Okamoto, and
  Tsiotras]{ridderhof2019nonlinear}
Jack Ridderhof, Kazuhide Okamoto, and Panagiotis Tsiotras.
\newblock Nonlinear uncertainty control with iterative covariance steering,
  2019.

\bibitem[Rudner et~al.(2021{\natexlab{a}})Rudner, Chen, and
  Gal]{rudner2021fsvi}
Tim G.~J. Rudner, Zonghao Chen, and Yarin Gal.
\newblock Rethinking function-space variational inference in {B}ayesian neural
  networks.
\newblock In \emph{Third Symposium on Advances in Approximate {B}ayesian
  Inference}, 2021{\natexlab{a}}.

\bibitem[Rudner et~al.(2021{\natexlab{b}})Rudner, Lu, Osborne, Gal, and
  Teh]{rudner2020pathologies}
Tim G.~J. Rudner, Cong Lu, Michael~A. Osborne, Yarin Gal, and Yee~Whye Teh.
\newblock On pathologies in {KL}-regularized reinforcement learning from expert
  demonstrations.
\newblock In \emph{Advances in Neural Information Processing Systems 34}.
  2021{\natexlab{b}}.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor, and Silver]{schaul2015uva}
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver.
\newblock Universal value function approximators.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  1312--1320, 2015.

\bibitem[Schroecker and Isbell(2020)]{schroecker2020universal}
Yannick Schroecker and Charles Isbell.
\newblock Universal value density estimation for imitation learning and
  goal-conditioned reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.06473}, 2020.

\bibitem[Singh et~al.(2019)Singh, Yang, Hartikainen, Finn, and
  Levine]{singh2019end}
Avi Singh, Larry Yang, Kristian Hartikainen, Chelsea Finn, and Sergey Levine.
\newblock End-to-end robotic reinforcement learning without reward engineering.
\newblock \emph{arXiv preprint arXiv:1904.07854}, 2019.

\bibitem[Sutton and Barto(1998)]{sutton1998rl}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock 1998.

\bibitem[Szepesv{\'{a}}ri(2010)]{szepesvari2010}
Csaba Szepesv{\'{a}}ri.
\newblock \emph{Algorithms for Reinforcement Learning}, volume~4.
\newblock 2010.

\bibitem[Todorov(2006)]{todorov2006lmdp}
E.~Todorov.
\newblock Linearly-solvable {M}arkov decision problems.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2006.

\bibitem[Toussaint(2009)]{toussaint2009soc}
M.~Toussaint.
\newblock Robot trajectory optimization using approximate inference.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2009.

\bibitem[Toussaint and Storkey(2006)]{toussaint2006probabilistic}
Marc Toussaint and Amos Storkey.
\newblock Probabilistic inference for solving discrete and continuous state
  {M}arkov decision processes.
\newblock In \emph{Proceedings of the 23rd international conference on Machine
  learning}, pages 945--952, 2006.

\bibitem[Toussaint et~al.(2006)Toussaint, Harmeling, and
  Storkey]{toussaint06probabilisticinference}
Marc Toussaint, Stefan Harmeling, and Amos Storkey.
\newblock Probabilistic inference for solving {(PO)MDPs}.
\newblock Technical report, 2006.

\bibitem[Veerapaneni et~al.(2020)Veerapaneni, Co-Reyes, Chang, Janner, Finn,
  Wu, Tenenbaum, and Levine]{veerapaneni2020entity}
Rishi Veerapaneni, John~D Co-Reyes, Michael Chang, Michael Janner, Chelsea
  Finn, Jiajun Wu, Joshua Tenenbaum, and Sergey Levine.
\newblock Entity abstraction in visual model-based reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pages 1439--1456. PMLR, 2020.

\bibitem[Venkattaramanujam et~al.(2019)Venkattaramanujam, Crawford, Doan, and
  Precup]{venkattaramanujam2019self}
Srinivas Venkattaramanujam, Eric Crawford, Thang Doan, and Doina Precup.
\newblock Self-supervised learning of distance functions for goal-conditioned
  reinforcement learning.
\newblock \emph{arXiv:1907.02998}, 2019.

\bibitem[Warde-Farley et~al.(2019)Warde-Farley, {Van De Wiele}, Kulkarni,
  Ionescu, Hansen, Volodymyr, and Deepmind]{wardefarley2019discern}
David Warde-Farley, Tom {Van De Wiele}, Tejas Kulkarni, Catalin Ionescu, Steven
  Hansen, {\&}~Volodymyr, and Mnih Deepmind.
\newblock Unsupervised control through non-parametric discriminative rewards.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Watkins and Dayan(1992)]{watkins1992q}
Christopher~JCH Watkins and Peter Dayan.
\newblock {Q}-learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 279--292, 1992.

\bibitem[White(2017)]{white2017unifying}
Martha White.
\newblock Unifying task specification in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3742--3750. PMLR, 2017.

\bibitem[Xu and Skelton(1992)]{xu1992}
J.-H. Xu and R.E. Skelton.
\newblock An improved covariance assignment theory for discrete systems, 1992.

\bibitem[{Yi} et~al.(2020){Yi}, {Cao}, {Theodorou}, and
  {Chen}]{yi2019nonlinear}
Z.~{Yi}, Z.~{Cao}, E.~{Theodorou}, and Y.~{Chen}.
\newblock Nonlinear covariance control via differential dynamic programming.
\newblock In \emph{2020 American Control Conference (ACC)}, pages 3571--3576,
  2020.

\bibitem[Yu et~al.(2020)Yu, Quillen, He, Julian, Hausman, Finn, and
  Levine]{yu2020meta}
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea
  Finn, and Sergey Levine.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pages 1094--1100. PMLR, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Vikram, Smith, Abbeel, Johnson, and
  Levine]{zhang2019solar}
Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew~J. Johnson,
  and Sergey Levine.
\newblock {SOLAR}: Deep structured representations for model-based
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, aug
  2019.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maxent}
Brian~D Ziebart, Andrew Maas, J~Andrew Bagnell, and Anind~K Dey.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, pages
  1433--1438, 2008.

\end{thebibliography}
