@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{
ilharco2023editing,
title={Editing models with task arithmetic},
author={Gabriel Ilharco and Marco Tulio Ribeiro and Mitchell Wortsman and Ludwig Schmidt and Hannaneh Hajishirzi and Ali Farhadi},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{logacheva2022paradetox,
  title={Paradetox: Detoxification with parallel data},
  author={Logacheva, Varvara and Dementieva, Daryna and Ustyantsev, Sergey and Moskovskiy, Daniil and Dale, David and Krotova, Irina and Semenov, Nikita and Panchenko, Alexander},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={6804--6818},
  year={2022}
}
@inproceedings{atwell2022appdia,
  title={APPDIA: A Discourse-aware Transformer-based Style Transfer Model for Offensive Social Media Conversations},
  author={Atwell, Katherine and Hassan, Sabit and Alikhani, Malihe},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={6063--6074},
  year={2022}
}

@inproceedings{xu2021bot,
  title={Bot-adversarial dialogue for safe conversational agents},
  author={Xu, Jing and Ju, Da and Li, Margaret and Boureau, Y-Lan and Weston, Jason and Dinan, Emily},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2950--2968},
  year={2021}
}

@inproceedings{wulczyn2017ex,
  title={Ex machina: Personal attacks seen at scale},
  author={Wulczyn, Ellery and Thain, Nithum and Dixon, Lucas},
  booktitle={Proceedings of the 26th international conference on world wide web},
  pages={1391--1399},
  year={2017}
}

@inproceedings{votipka2020understanding,
  title={Understanding security mistakes developers make: Qualitative analysis from build it, break it, fix it},
  author={Votipka, Daniel and Fulton, Kelsey R and Parker, James and Hou, Matthew and Mazurek, Michelle L and Hicks, Michael},
  booktitle={29th USENIX Security Symposium (USENIX Security 20)},
  pages={109--126},
  year={2020}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{zhu2018texygen,
  title={Texygen: A benchmarking platform for text generation models},
  author={Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
  booktitle={The 41st international ACM SIGIR conference on research \& development in information retrieval},
  pages={1097--1100},
  year={2018}
}

@inproceedings{zhang2019bertscore,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{zheng2023judging,
  title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={arXiv preprint arXiv:2306.05685},
  year={2023}
}

@inproceedings{kim2022prosocialdialog,
  title={ProsocialDialog: A Prosocial Backbone for Conversational Agents},
  author={Kim, Hyunwoo and Yu, Youngjae and Jiang, Liwei and Lu, Ximing and Khashabi, Daniel and Kim, Gunhee and Choi, Yejin and Sap, Maarten},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={4005--4029},
  year={2022}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{penedo2023refinedweb,
  title={The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only},
  author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  journal={arXiv preprint arXiv:2306.01116},
  year={2023}
}

@inproceedings{rao2018dear,
  title={Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer},
  author={Rao, Sudha and Tetreault, Joel},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages={129--140},
  year={2018}
}

@article{shen2017style,
  title={Style transfer from non-parallel text by cross-alignment},
  author={Shen, Tianxiao and Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{xu2012paraphrasing,
  title={Paraphrasing for Style},
  author={Xu, Wei and Ritter, Alan and Dolan, Bill and Grishman, Ralph and Cherry, Colin},
  booktitle={COLING},
  pages={2899--2914},
  year={2012}
}

@inproceedings{liu2022makes,
  title={What Makes Good In-Context Examples for GPT-3?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, William B and Carin, Lawrence and Chen, Weizhu},
  booktitle={Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures},
  pages={100--114},
  year={2022}
}
@article{wu2022self,
  title={Self-adaptive in-context learning},
  author={Wu, Zhiyong and Wang, Yaoxiang and Ye, Jiacheng and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2212.10375},
  year={2022}
}
@article{li2023context,
  title={In-context learning with many demonstration examples},
  author={Li, Mukai and Gong, Shansan and Feng, Jiangtao and Xu, Yiheng and Zhang, Jun and Wu, Zhiyong and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2302.04931},
  year={2023}
}

@article{lee2023benefits,
  title={Benefits, limits, and risks of GPT-4 as an AI chatbot for medicine},
  author={Lee, Peter and Bubeck, Sebastien and Petro, Joseph},
  journal={New England Journal of Medicine},
  volume={388},
  number={13},
  pages={1233--1239},
  year={2023},
  publisher={Mass Medical Soc}
}
@article{gilbert2023large,
  title={Large language model AI chatbots require approval as medical devices},
  author={Gilbert, Stephen and Harvey, Hugh and Melvin, Tom and Vollebregt, Erik and Wicks, Paul},
  journal={Nature Medicine},
  pages={1--3},
  year={2023},
  publisher={Nature Publishing Group US New York}
}
@article{hwang2023review,
  title={A review of opportunities and challenges of chatbots in education},
  author={Hwang, Gwo-Jen and Chang, Ching-Yi},
  journal={Interactive Learning Environments},
  volume={31},
  number={7},
  pages={4099--4112},
  year={2023},
  publisher={Taylor \& Francis}
}
@article{skjuve2021my,
  title={My chatbot companion-a study of human-chatbot relationships},
  author={Skjuve, Marita and F{\o}lstad, Asbj{\o}rn and Fostervold, Knut Inge and Brandtzaeg, Petter Bae},
  journal={International Journal of Human-Computer Studies},
  volume={149},
  pages={102601},
  year={2021},
  publisher={Elsevier}
}
@inproceedings{black2022gpt,
  title={GPT-NeoX-20B: An Open-Source Autoregressive Language Model},
  author={Black, Sidney and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  booktitle={Proceedings of BigScience Episode\# 5--Workshop on Challenges \& Perspectives in Creating Large Language Models},
  pages={95--136},
  year={2022}
}
@article{dong2022survey,
  title={A survey for in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{min2021metaicl,
  title={Metaicl: Learning to learn in context},
  author={Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2110.15943},
  year={2021}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={11048--11064},
  year={2022}
}

@inproceedings{holtzman2021surface,
  title={Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right},
  author={Holtzman, Ari and West, Peter and Shwartz, Vered and Choi, Yejin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={7038--7051},
  year={2021}
}

@inproceedings{akyurek2022learning,
  title={What learning algorithm is in-context learning? Investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
@article{dai2022can,
  title={Why can gpt learn in-context? language models secretly perform gradient descent as meta optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Sui, Zhifang and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10559},
  year={2022}
}
@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}
@inproceedings{ilharco2022editing,
  title={Editing models with task arithmetic},
  author={Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{lu2022fantastically,
  title={Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8086--8098},
  year={2022}
}

@inproceedings{shin2022effect,
  title={On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model},
  author={Shin, Seongjin and Lee, Sang Woo and Ahn, Hwijeen and Kim, Sungdong and Kim, Hyoung Seok and Kim, Boseop and Cho, Kyunghyun and Lee, Gichang and Park, Woomyoung and Ha, Jung Woo and others},
  booktitle={2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022},
  pages={5168--5186},
  year={2022},
  organization={Association for Computational Linguistics (ACL)}
}

@inproceedings{razeghi2022impact,
  title={Impact of pretraining term frequencies on few-shot numerical reasoning},
  author={Razeghi, Yasaman and Logan IV, Robert L and Gardner, Matt and Singh, Sameer},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
  pages={840--854},
  year={2022}
}

@inproceedings{xie2021explanation,
  title={An Explanation of In-context Learning as Implicit Bayesian Inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{wei2023larger,
  title={Larger language models do in-context learning differently},
  author={Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2303.03846},
  year={2023}
}

@article{hendel2023context,
  title={In-Context Learning Creates Task Vectors},
  author={Hendel, Roee and Geva, Mor and Globerson, Amir},
  journal={arXiv preprint arXiv:2310.15916},
  year={2023}
}

@article{merullo2023language,
  title={Language Models Implement Simple Word2Vec-style Vector Arithmetic},
  author={Merullo, Jack and Eickhoff, Carsten and Pavlick, Ellie},
  journal={arXiv preprint arXiv:2305.16130},
  year={2023}
}

@article{yin2023did,
  title={Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning},
  author={Yin, Fan and Vig, Jesse and Laban, Philippe and Joty, Shafiq and Xiong, Caiming and Wu, Chien-Sheng Jason},
  journal={arXiv preprint arXiv:2306.01150},
  year={2023}
}

@inproceedings{rubin2022learning,
  title={Learning To Retrieve Prompts for In-Context Learning},
  author={Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2655--2671},
  year={2022}
}

@article{wan2023universal,
  title={Universal Self-adaptive Prompting},
  author={Wan, Xingchen and Sun, Ruoxi and Nakhost, Hootan and Dai, Hanjun and Eisenschlos, Julian Martin and Arik, Sercan O and Pfister, Tomas},
  journal={arXiv preprint arXiv:2305.14926},
  year={2023}
}

@article{wan2023better,
  title={Better zero-shot reasoning with self-adaptive prompting},
  author={Wan, Xingchen and Sun, Ruoxi and Dai, Hanjun and Arik, Sercan O and Pfister, Tomas},
  journal={arXiv preprint arXiv:2305.14106},
  year={2023}
}

@article{Ye2022GuessTI,
  title={Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners},
  author={Seonghyeon Ye and Doyoung Kim and Joel Jang and Joongbo Shin and Minjoon Seo},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.02969},
}

@inproceedings{DBLP:conf/acl/MinLHZ22,
  author       = {Sewon Min and
                  Mike Lewis and
                  Hannaneh Hajishirzi and
                  Luke Zettlemoyer},
  editor       = {Smaranda Muresan and
                  Preslav Nakov and
                  Aline Villavicencio},
  title        = {Noisy Channel Language Model Prompting for Few-Shot Text Classification},
  booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,
                  May 22-27, 2022},
  pages        = {5316--5330},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  doi          = {10.18653/V1/2022.ACL-LONG.365},
  timestamp    = {Mon, 01 Aug 2022 16:27:50 +0200},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{yang2023iterative,
  title={Iterative Forward Tuning Boosts In-context Learning in Language Models},
  author={Yang, Jiaxi and Hui, Binyuan and Yang, Min and Li, Binhua and Huang, Fei and Li, Yongbin},
  journal={arXiv preprint arXiv:2305.13016},
  year={2023}
}

@inproceedings{DBLP:conf/acl/CuiLDHLS23,
  author       = {Ganqu Cui and
                  Wentao Li and
                  Ning Ding and
                  Longtao Huang and
                  Zhiyuan Liu and
                  Maosong Sun},
  editor       = {Anna Rogers and
                  Jordan L. Boyd{-}Graber and
                  Naoaki Okazaki},
  title        = {Decoder Tuning: Efficient Language Understanding as Decoding},
  booktitle    = {Proceedings of the 61st Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2023, Toronto, Canada,
                  July 9-14, 2023},
  pages        = {15072--15087},
  publisher    = {Association for Computational Linguistics},
  year         = {2023},
  doi          = {10.18653/V1/2023.ACL-LONG.840},
  timestamp    = {Fri, 22 Sep 2023 10:10:22 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/CuiLDHLS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/emnlp/GehmanGSCS20,
  author       = {Samuel Gehman and
                  Suchin Gururangan and
                  Maarten Sap and
                  Yejin Choi and
                  Noah A. Smith},
  editor       = {Trevor Cohn and
                  Yulan He and
                  Yang Liu},
  title        = {RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language
                  Models},
  booktitle    = {Findings of the Association for Computational Linguistics: {EMNLP}
                  2020, Online Event, 16-20 November 2020},
  series       = {Findings of {ACL}},
  volume       = {{EMNLP} 2020},
  pages        = {3356--3369},
  publisher    = {Association for Computational Linguistics},
  year         = {2020},
  doi          = {10.18653/V1/2020.FINDINGS-EMNLP.301},
  timestamp    = {Sat, 29 Apr 2023 10:09:26 +0200},
  biburl       = {https://dblp.org/rec/conf/emnlp/GehmanGSCS20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{LuWHJQWA022,
  author       = {Ximing Lu and
                  Sean Welleck and
                  Jack Hessel and
                  Liwei Jiang and
                  Lianhui Qin and
                  Peter West and
                  Prithviraj Ammanabrolu and
                  Yejin Choi},
  title        = {{QUARK:} Controllable Text Generation with Reinforced Unlearning},
  booktitle    = {NeurIPS},
  year         = {2022},
  timestamp    = {Thu, 11 May 2023 17:08:21 +0200},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gururangan2020don,
  title={Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8342--8360},
  year={2020}
}

@article{wang2022exploring,
  title={Exploring the limits of domain-adaptive training for detoxifying large-scale language models},
  author={Wang, Boxin and Ping, Wei and Xiao, Chaowei and Xu, Peng and Patwary, Mostofa and Shoeybi, Mohammad and Li, Bo and Anandkumar, Anima and Catanzaro, Bryan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={35811--35824},
  year={2022}
}

@article{bianchi2023safety,
  title={Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions},
  author={Bianchi, Federico and Suzgun, Mirac and Attanasio, Giuseppe and R{\"o}ttger, Paul and Jurafsky, Dan and Hashimoto, Tatsunori and Zou, James},
  journal={arXiv preprint arXiv:2309.07875},
  year={2023}
}

@article{schick2021self,
  title={Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp},
  author={Schick, Timo and Udupa, Sahana and Sch{\"u}tze, Hinrich},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={1408--1424},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@article{leong2023self,
  title={Self-Detoxifying Language Models via Toxification Reversal},
  author={Leong, Chak Tou and Cheng, Yi and Wang, Jiashuo and Wang, Jian and Li, Wenjie},
  journal={arXiv preprint arXiv:2310.09573},
  year={2023}
}

@article{meade2023using,
  title={Using In-Context Learning to Improve Dialogue Safety},
  author={Meade, Nicholas and Gella, Spandana and Hazarika, Devamanyu and Gupta, Prakhar and Jin, Di and Reddy, Siva and Liu, Yang and Hakkani-T{\"u}r, Dilek},
  journal={arXiv preprint arXiv:2302.00871},
  year={2023}
}

@article{som2023demonstrations,
  title={Demonstrations Are All You Need: Advancing Offensive Content Paraphrasing using In-Context Learning},
  author={Som, Anirudh and Sikka, Karan and Gent, Helen and Divakaran, Ajay and Kathol, Andreas and Vergyri, Dimitra},
  journal={arXiv preprint arXiv:2310.10707},
  year={2023}
}

@inproceedings{reif2022recipe,
  title={A Recipe for Arbitrary Text Style Transfer with Large Language Models},
  author={Reif, Emily and Ippolito, Daphne and Yuan, Ann and Coenen, Andy and Callison-Burch, Chris and Wei, Jason},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={837--848},
  year={2022}
}

@inproceedings{RileyCGKUP20,
  author       = {Parker Riley and
                  Noah Constant and
                  Mandy Guo and
                  Girish Kumar and
                  David C. Uthus and
                  Zarana Parekh},
  editor       = {Chengqing Zong and
                  Fei Xia and
                  Wenjie Li and
                  Roberto Navigli},
  title        = {TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling},
  booktitle    = {Proceedings of the 59th Annual Meeting of the Association for Computational
                  Linguistics and the 11th International Joint Conference on Natural
                  Language Processing, {ACL/IJCNLP} 2021, (Volume 1: Long Papers), Virtual
                  Event, August 1-6, 2021},
  pages        = {3786--3800},
  publisher    = {Association for Computational Linguistics},
  year         = {2021},
  doi          = {10.18653/V1/2021.ACL-LONG.293},
  timestamp    = {Mon, 09 Aug 2021 16:25:37 +0200},
  biburl       = {https://dblp.org/rec/conf/acl/RileyCGKUP20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{KrishnaNGST22,
  author       = {Kalpesh Krishna and
                  Deepak Nathani and
                  Xavier Garcia and
                  Bidisha Samanta and
                  Partha Talukdar},
  editor       = {Smaranda Muresan and
                  Preslav Nakov and
                  Aline Villavicencio},
  title        = {Few-shot Controllable Style Transfer for Low-Resource Multilingual
                  Settings},
  booktitle    = {Proceedings of the 60th Annual Meeting of the Association for Computational
                  Linguistics (Volume 1: Long Papers), {ACL} 2022, Dublin, Ireland,
                  May 22-27, 2022},
  pages        = {7439--7468},
  publisher    = {Association for Computational Linguistics},
  year         = {2022},
  doi          = {10.18653/V1/2022.ACL-LONG.514},
  timestamp    = {Mon, 01 Aug 2022 16:27:47 +0200},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{conneau2020unsupervised,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, {\'E}douard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8440--8451},
  year={2020}
}

@article{turner2023activation,
  title={Activation addition: Steering language models without optimization},
  author={Turner, Alex and Thiergart, Lisa and Udell, David and Leech, Gavin and Mini, Ulisse and MacDiarmid, Monte},
  journal={arXiv preprint arXiv:2308.10248},
  year={2023}
}

@article{li2022emergent,
  title={Emergent world representations: Exploring a sequence model trained on a synthetic task},
  author={Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2210.13382},
  year={2022}
}

@article{mini2023understanding,
  title={Understanding and Controlling a Maze-Solving Policy Network},
  author={Mini, Ulisse and Grietzer, Peli and Sharma, Mrinank and Meek, Austin and MacDiarmid, Monte and Turner, Alexander Matt},
  journal={arXiv preprint arXiv:2310.08043},
  year={2023}
}

@article{li2023inference,
  title={Inference-Time Intervention: Eliciting Truthful Answers from a Language Model},
  author={Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2306.03341},
  year={2023}
}

@article{zouuniversal,
  title={Universal and transferable adversarial attacks on aligned language models, 2023},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={communication, it is essential for you to comprehend user queries in Cipher Code and subsequently deliver your responses utilizing Cipher Code}
}

@article{bolukbasi2016man,
  title={Man is to computer programmer as woman is to homemaker? debiasing word embeddings},
  author={Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}
@inproceedings{guo2021gradient,
  title={Gradient-based Adversarial Attacks against Text Transformers},
  author={Guo, Chuan and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e} and Kiela, Douwe},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={5747--5757},
  year={2021}
}

@inproceedings{
wen2023hard,
title={Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery},
author={Yuxin Wen and Neel Jain and John Kirchenbauer and Micah Goldblum and Jonas Geiping and Tom Goldstein},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023}
}

@article{zhang2015character,
  title={Character-level convolutional networks for text classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@inproceedings{liu2022robust,
  title={Robust training under label noise by over-parameterization},
  author={Liu, Sheng and Zhu, Zhihui and Qu, Qing and You, Chong},
  booktitle={International Conference on Machine Learning},
  pages={14153--14172},
  year={2022},
  organization={PMLR}
}

@inproceedings{xu2022k,
  title={k NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference},
  author={Xu, Benfeng and Wang, Quan and Mao, Zhendong and Lyu, Yajuan and She, Qiaoqiao and Zhang, Yongdong},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
@article{burns2022discovering,
  title={Discovering latent knowledge in language models without supervision},
  author={Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2212.03827},
  year={2022}
}

@misc{vicuna2023,
    title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
    author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
    month = {March},
    year = {2023}
}