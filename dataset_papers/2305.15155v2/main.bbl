\begin{thebibliography}{85}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock {QSGD}: {C}ommunication-efficient {SGD} via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem[Alistarh et~al.(2018)Alistarh, Hoefler, Johansson, Khirirat,
  Konstantinov, and Renggli]{Alistarh-EF-NIPS2018}
Dan Alistarh, Torsten Hoefler, Mikael Johansson, Sarit Khirirat, Nikola
  Konstantinov, and C\'{e}dric Renggli.
\newblock The convergence of sparsified gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Arjevani et~al.(2019)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{arjevani2019lower}
Yossi Arjevani, Yair Carmon, John~C Duchi, Dylan~J Foster, Nathan Srebro, and
  Blake Woodworth.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1912.02365}, 2019.

\bibitem[Beznosikov et~al.(2020)Beznosikov, Horv{\'a}th, Richt{\'a}rik, and
  Safaryan]{beznosikov2020biased}
Aleksandr Beznosikov, Samuel Horv{\'a}th, Peter Richt{\'a}rik, and Mher
  Safaryan.
\newblock On biased compression for distributed learning.
\newblock \emph{arXiv preprint arXiv:2002.12410}, 2020.

\bibitem[Chang and Lin(2011)]{chang2011libsvm}
Chih-Chung Chang and Chih-Jen Lin.
\newblock {LIBSVM}: a library for support vector machines.
\newblock \emph{{ACM} {T}ransactions on {I}ntelligent {S}ystems and
  {T}echnology (TIST)}, 2\penalty0 (3):\penalty0 1--27, 2011.

\bibitem[Chen et~al.(2022)Chen, He, Hu, and Ye]{Chen_Network_Revenue_2022}
Xin Chen, Niao He, Yifan Hu, and Zikun Ye.
\newblock Efficient algorithms for minimizing compositions of convex functions
  and random functions and its applications in network revenue management.
\newblock \emph{arXiv preprint arXiv:2205.01774}, 2022.

\bibitem[Cutkosky and Mehta(2020)]{Cutkosky_MomNSGD_2020}
Ashok Cutkosky and Harsh Mehta.
\newblock Momentum improves normalized {SGD}.
\newblock In \emph{International Conference on Machine Learning}, page
  2260–2268. PMLR, 2020.

\bibitem[Cutkosky and Mehta(2021)]{Cutkosky_High_Prob_Tails_2021}
Ashok Cutkosky and Harsh Mehta.
\newblock High-probability bounds for non-convex stochastic optimization with
  heavy tails.
\newblock pages 4883--4895, 2021.

\bibitem[Cutkosky and Orabona(2019)]{Cutkosky_STORM_2019}
Ashok Cutkosky and Francesco Orabona.
\newblock Momentum-based variance reduction in non-convex {SGD}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Defazio(2021)]{Defazio_Mom_PrimalAvg_2021}
Aaron Defazio.
\newblock Momentum via primal averaging: Theoretical insights and learning rate
  schedules for non-convex optimization.
\newblock \emph{arXiv preprint arXiv:2010.00406}, 2021.

\bibitem[Doan et~al.(2019)Doan, Maguluri, and Romberg]{Doan_FT_TD_4_MARL_2019}
Thinh Doan, Siva Maguluri, and Justin Romberg.
\newblock Finite-time analysis of distributed {TD}(0) with linear function
  approximation on multi-agent reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1626--1635. PMLR, 2019.

\bibitem[Dorfman et~al.(2023)Dorfman, Vargaftik, Ben-Itzhak, and
  Levy]{Dorfman_DoCoFL_2023}
Ron Dorfman, Shay Vargaftik, Yaniv Ben-Itzhak, and Kfir~Y. Levy.
\newblock $\texttt{DoCoFL}$: Downlink compression for cross-device federated
  learning.
\newblock \emph{arXiv preprint arXiv:2302.00543}, 2023.

\bibitem[Fatkhullin et~al.(2021)Fatkhullin, Sokolov, Gorbunov, Li, and
  Richtárik]{EF21BW_2021}
Ilyas Fatkhullin, Igor Sokolov, Eduard Gorbunov, Zhize Li, and Peter
  Richtárik.
\newblock {EF21} with bells \& whistles: Practical algorithmic extensions of
  modern error feedback.
\newblock \emph{arXiv preprint arXiv:2110.03294}, 2021.

\bibitem[Gao et~al.(2023)Gao, Liu, Dai, Huang, and
  Gu]{Gao_Distr_Stoch_Grad_Tracking_2023}
Juan Gao, Xin-Wei Liu, Yu-Hong Dai, Yakui Huang, and Junhua Gu.
\newblock Distributed stochastic gradient tracking methods with momentum
  acceleration for non-convex optimization.
\newblock \emph{Computational Optimization and Applications}, 84\penalty0
  (2):\penalty0 531–572, 2023.

\bibitem[Ghadimi et~al.(2015)Ghadimi, Feyzmahdavian, and
  Johansson]{Ghadimi_Glob_HB_Convex_2014}
Euhanna Ghadimi, Hamid~Reza Feyzmahdavian, and Mikael Johansson.
\newblock Global convergence of the heavy-ball method for convex optimization.
\newblock In \emph{2015 European control conference (ECC)}, pages 310--315.
  IEEE, 2015.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Kovalev, Makarenko, and
  Richt\'{a}rik]{Lin_EC_SGD}
Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, and Peter Richt\'{a}rik.
\newblock Linearly converging error compensated {SGD}.
\newblock In \emph{34th Conference on Neural Information Processing Systems},
  2020.

\bibitem[Gorbunov et~al.(2021)Gorbunov, Burlachenko, Li, and
  Richt{\'a}rik]{MARINA}
Eduard Gorbunov, Konstantin Burlachenko, Zhize Li, and Peter Richt{\'a}rik.
\newblock {MARINA}: Faster non-convex distributed learning with compression.
\newblock In \emph{International Conference on Machine Learning}, pages
  3788--3798. PMLR, 2021.

\bibitem[Gruntkowska et~al.(2022)Gruntkowska, Tyurin, and
  Richtárik]{Gruntkowska_EF21_P_2022}
Kaja Gruntkowska, Alexander Tyurin, and Peter Richtárik.
\newblock Ef21-p and friends: Improved theoretical communication complexity for
  distributed optimization with bidirectional compression.
\newblock \emph{arXiv preprint arXiv:2209.15218}, Sep 2022.

\bibitem[Gupta et~al.(2015)Gupta, Agrawal, Gopalakrishnan, and
  Narayanan]{Gupta_DL_limited_presition_2015}
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
\newblock Deep learning with limited numerical precision.
\newblock In \emph{International Conference on Machine Learning}, page
  1737–1746, Feb 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 770--778, 2016.

\bibitem[Horv\'{a}th et~al.(2019{\natexlab{a}})Horv\'{a}th, Ho, \v{L}udov\'{i}t
  Horv\'{a}th, Sahu, Canini, and Richt\'{a}rik]{Cnat}
Samuel Horv\'{a}th, Chen-Yu Ho, \v{L}udov\'{i}t Horv\'{a}th, Atal~Narayan Sahu,
  Marco Canini, and Peter Richt\'{a}rik.
\newblock Natural compression for distributed deep learning.
\newblock \emph{arXiv preprint arXiv:1905.10988}, 2019{\natexlab{a}}.

\bibitem[Horv\'{a}th et~al.(2019{\natexlab{b}})Horv\'{a}th, Kovalev,
  Mishchenko, Stich, and Richt\'{a}rik]{DIANA2}
Samuel Horv\'{a}th, Dmitry Kovalev, Konstantin Mishchenko, Sebastian Stich, and
  Peter Richt\'{a}rik.
\newblock Stochastic distributed learning with gradient quantization and
  variance reduction.
\newblock \emph{arXiv preprint arXiv:1904.05115}, 2019{\natexlab{b}}.

\bibitem[Huang et~al.(2022)Huang, Chen, Yin, and Yuan]{huang2022lower}
Xinmeng Huang, Yiming Chen, Wotao Yin, and Kun Yuan.
\newblock Lower bounds and nearly optimal algorithms in distributed learning
  with communication compression.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Islamov et~al.(2022)Islamov, Qian, Hanzely, Safaryan, and
  Richtárik]{Islamov_Newton_3PC_2022}
Rustem Islamov, Xun Qian, Slavomír Hanzely, Mher Safaryan, and Peter
  Richtárik.
\newblock Distributed newton-type methods with communication compression and
  bernoulli aggregation.
\newblock \emph{arXiv preprint arXiv:2206.03588}, 2022.

\bibitem[Jelassi and Li(2022)]{Jelassi_Mom_Improves_Generalization_2022}
Samy Jelassi and Yuanzhi Li.
\newblock Towards understanding how momentum improves generalization in deep
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  9965--10040. PMLR, 2022.

\bibitem[Jin et~al.(2022)Jin, Peng, Yang, Wang, and
  Zhang]{Jin_FedRL_Env_Heterogeneity_2022}
Hao Jin, Yang Peng, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
\newblock Federated reinforcement learning with environment heterogeneity.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, volume 151 of \emph{Proceedings of Machine Learning Research},
  pages 18--37. PMLR, 28--30 Mar 2022.

\bibitem[Kairouz(2019)]{FL-big}
Peter et~al Kairouz.
\newblock Advances and open problems in federated learning.
\newblock \emph{arXiv preprint arXiv:1912.04977}, 2019.

\bibitem[Kale et~al.(2021)Kale, Sekhari, and
  Sridharan]{Kale_SGD_Role_of_Implicit_Reg_2021}
Satyen Kale, Ayush Sekhari, and Karthik Sridharan.
\newblock Sgd: The role of implicit regularization, batch-size and
  multiple-epochs.
\newblock \emph{arXiv preprint arXiv:2107.05074}, 2021.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Rebjock, Stich, and
  Jaggi]{Karimireddy_SignSGD}
Sai~Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi.
\newblock Error feedback fixes {S}ign{SGD} and other gradient compression
  schemes.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Karimireddy et~al.(2021)Karimireddy, Jaggi, Kale, Mohri, Reddi, Stich,
  and Suresh]{Karimireddy_Mime_2021}
Sai~Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank~J.
  Reddi, Sebastian~U. Stich, and Ananda~Theertha Suresh.
\newblock Mime: Mimicking centralized stochastic algorithms in federated
  learning.
\newblock \emph{arXiv preprint arXiv:2008.03606}, 2021.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{Keskar_OnLargeBatch_DL_2017}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Khodadadian et~al.(2022)Khodadadian, Sharma, Joshi, and
  Maguluri]{Khodadadian_FedRL_Linear_speed_2022}
Sajad Khodadadian, Pranay Sharma, Gauri Joshi, and Siva~Theja Maguluri.
\newblock Federated reinforcement learning: Linear speedup under markovian
  sampling.
\newblock In \emph{International Conference on Machine Learning}, page
  10997–11057. PMLR, Jun 2022.

\bibitem[Kingma and Ba(2015)]{ADAM}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR (Poster)}, 2015.

\bibitem[Kleinberg et~al.(2018)Kleinberg, Li, and
  Yuan]{Kleinberg_When_SGD_Escape_Local_2018}
Bobby Kleinberg, Yuanzhi Li, and Yang Yuan.
\newblock An alternative view: When does sgd escape local minima?
\newblock In \emph{International Conference on Machine Learning}, page
  2698–2707. PMLR, 2018.

\bibitem[Koloskova et~al.(2020)Koloskova, Lin, Stich, and
  Jaggi]{Koloskova2019DecentralizedDL}
Anastasia Koloskova, Tao Lin, S.~Stich, and Martin Jaggi.
\newblock Decentralized deep learning with arbitrary communication compression.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Kone\v{c}n\'{y} et~al.(2016)Kone\v{c}n\'{y}, McMahan, Yu,
  Richt\'{a}rik, Suresh, and Bacon]{FEDLEARN}
Jakub Kone\v{c}n\'{y}, H.~Brendan McMahan, Felix Yu, Peter Richt\'{a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: strategies for improving communication
  efficiency.
\newblock In \emph{NIPS Private Multi-Party Machine Learning Workshop}, 2016.

\bibitem[Kovalev et~al.(2021)Kovalev, Koloskova, Jaggi, Richt\'{a}rik, and
  Stich]{D-DIANA}
Dmitry Kovalev, Anastasia Koloskova, Martin Jaggi, Peter Richt\'{a}rik, and
  Sebastian Stich.
\newblock A linearly convergent algorithm for decentralized optimization:
  Sending less bits for free!
\newblock In \emph{The 24th International Conference on Artificial Intelligence
  and Statistics (AISTATS 2021)}, 2021.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, Toronto, 2009.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
Yann LeCun, Corinna Cortes, and CJ~Burges.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available:
  http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem[Li and Orabona(2020)]{Li_Orabona_High_Prob_2020}
Xiaoyu Li and Francesco Orabona.
\newblock A high probability analysis of adaptive {SGD} with momentum.
\newblock \emph{arXiv preprint arXiv:2007.14294}, 2020.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Liu, and
  Orabona]{Li_On_Last_Iterate_Mom_2022}
Xiaoyu Li, Mingrui Liu, and Francesco Orabona.
\newblock On the last iterate convergence of momentum methods.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 699--717. PMLR, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Karimi, and Li]{Li_COMP_AMS_2022}
Xiaoyun Li, Belhal Karimi, and Ping Li.
\newblock On distributed adaptive optimization with gradient compression.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{b}}.

\bibitem[Li et~al.(2020)Li, Kovalev, Qian, and Richt{\'a}rik]{ADIANA}
Zhize Li, Dmitry Kovalev, Xun Qian, and Peter Richt{\'a}rik.
\newblock Acceleration for compressed gradient descent in distributed and
  federated optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  5895--5904. PMLR, 2020.

\bibitem[Li et~al.(2021)Li, Bao, Zhang, and Richt\'{a}rik]{PAGE2021}
Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richt\'{a}rik.
\newblock {PAGE}: A simple and optimal probabilistic gradient estimator for
  nonconvex optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  6286--6295. PMLR, 2021.

\bibitem[Lin et~al.(2018)Lin, Han, Mao, Wang, and Dally]{lin2018deep}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and Bill Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Liu et~al.(2020)Liu, Gao, and Yin]{Liu_Improved_analysis_SGDM_2020}
Yanli Liu, Yuan Gao, and Wotao Yin.
\newblock An improved analysis of stochastic gradient descent with momentum.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Makarenko et~al.(2022)Makarenko, Gasanov, Islamov, Sadiev, and
  Richtarik]{Makarenko_AdaCGD_2022}
Maksim Makarenko, Elnur Gasanov, Rustem Islamov, Abdurakhmon Sadiev, and Peter
  Richtarik.
\newblock Adaptive compression for communication-efficient distributed
  training.
\newblock \emph{arXiv preprint arXiv:2211.00188}, Oct 2022.

\bibitem[Mishchenko et~al.(2019)Mishchenko, Gorbunov, Tak{\'a}{\v{c}}, and
  Richt{\'a}rik]{DIANA}
Konstantin Mishchenko, Eduard Gorbunov, Martin Tak{\'a}{\v{c}}, and Peter
  Richt{\'a}rik.
\newblock Distributed learning with compressed gradient differences.
\newblock \emph{arXiv preprint arXiv:1901.09269}, 2019.

\bibitem[Mishchenko et~al.(2022)Mishchenko, Malinovsky, Stich, and
  Richtarik]{Mishchenko_Proxskip_2022}
Konstantin Mishchenko, Grigory Malinovsky, Sebastian Stich, and Peter
  Richtarik.
\newblock {P}rox{S}kip: Yes! {L}ocal gradient steps provably lead to
  communication acceleration! {F}inally!
\newblock In \emph{International Conference on Machine Learning}, pages
  15750--15769. PMLR, 2022.

\bibitem[Mitra et~al.(2023)Mitra, Pappas, and Hassani]{Mitra_TD_EF_2023}
Aritra Mitra, George~J. Pappas, and Hamed Hassani.
\newblock Temporal difference learning with compressed updates: Error-feedback
  meets reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2301.00944}, 2023.

\bibitem[Philippenko and Dieuleveut(2020)]{Artemis2020}
Constantin Philippenko and Aymeric Dieuleveut.
\newblock Bidirectional compression in heterogeneous settings for distributed
  or federated learning with partial participation: tight convergence
  guarantees.
\newblock \emph{arXiv preprint arXiv:2006.14591}, 2020.

\bibitem[Plattner(2022)]{Plattner_SGDM_Thesis_2022}
Maximilian Plattner.
\newblock On {SGD} with momentum.
\newblock \emph{Master's Thesis}, 2022.

\bibitem[Polyak(1964)]{Polyak_Some_methods_1964}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{Ussr computational mathematics and mathematical physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Qian et~al.(2020)Qian, Richt\'{a}rik, and Zhang]{EC-Katyusha}
Xun Qian, Peter Richt\'{a}rik, and Tong Zhang.
\newblock Error compensated distributed {SGD} can be accelerated.
\newblock \emph{arXiv preprint arXiv:2010.00091}, 2020.

\bibitem[Richt{\'a}rik et~al.(2021)Richt{\'a}rik, Sokolov, and
  Fatkhullin]{EF21}
Peter Richt{\'a}rik, Igor Sokolov, and Ilyas Fatkhullin.
\newblock {EF}21: A new, simpler, theoretically better, and practically faster
  error feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Richtárik et~al.(2022)Richtárik, Sokolov, Fatkhullin, Gasanov, Li,
  and Gorbunov]{3PC}
Peter Richtárik, Igor Sokolov, Ilyas Fatkhullin, Elnur Gasanov, Zhize Li, and
  Eduard Gorbunov.
\newblock {3PC}: Three point compressors for communication-efficient
  distributed training and a better theory for lazy aggregation.
\newblock \emph{arXiv preprint arXiv:2202.00998}, 2022.

\bibitem[Rieke et~al.(2020)Rieke, Hancox, Li, Milletarì, Roth, Albarqouni,
  Bakas, Galtier, Landman, Maier-Hein, Ourselin, Sheller, Summers, Trask, Xu,
  Baust, and Cardoso]{Rieke_Dig_Health_FL_2020}
Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletarì, Holger~R. Roth, Shadi
  Albarqouni, Spyridon Bakas, Mathieu~N. Galtier, Bennett~A. Landman, Klaus
  Maier-Hein, Sébastien Ourselin, Micah Sheller, Ronald~M. Summers, Andrew
  Trask, Daguang Xu, Maximilian Baust, and M.~Jorge Cardoso.
\newblock The future of digital health with federated learning.
\newblock \emph{npj Digital Medicine}, 3\penalty0 (11):\penalty0 1–7, 2020.

\bibitem[Safaryan et~al.(2021)Safaryan, Shulgin, and Richt\'{a}rik]{UP2021}
Mher Safaryan, Egor Shulgin, and Peter Richt\'{a}rik.
\newblock Uncertainty principle for communication compression in distributed
  and federated learning and the search for an optimal compressor.
\newblock \emph{Information and Inference: A Journal of the IMA}, 2021.

\bibitem[Safaryan et~al.(2022)Safaryan, Islamov, Qian, and
  Richt\'{a}rik]{FedNL}
Mher Safaryan, Rustem Islamov, Xun Qian, and Peter Richt\'{a}rik.
\newblock {FedNL}: Making {N}ewton-type methods applicable to federated
  learning.
\newblock In \emph{Internatioanl Conference on Machine Learning}, 2022.

\bibitem[Sahu et~al.(2021)Sahu, Dutta, M.~Abdelmoniem, Banerjee, Canini, and
  Kalnis]{Sahu_abs_comp_2021}
Atal Sahu, Aritra Dutta, Ahmed M.~Abdelmoniem, Trambak Banerjee, Marco Canini,
  and Panos Kalnis.
\newblock Rethinking gradient sparsification as total error minimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, page
  8133–8146, 2021.

\bibitem[Sapio et~al.(2021)Sapio, Canini, Ho, Nelson, Kalnis, Kim,
  Krishnamurthy, and Moshref]{Sapio_Scaling_Dist_ML_2021}
Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis, Changhoon
  Kim, Arvind Krishnamurthy, and Masoud Moshref.
\newblock Scaling distributed machine learning with in-network aggregation.
\newblock In \emph{18th USENIX Symposium on Networked Systems Design and
  Implementation}, page 785–808, 2021.

\bibitem[Sebbouh et~al.(2019)Sebbouh, Dossal, and
  Rondepierre]{sebbouh2019nesterov}
Othmane Sebbouh, Charles Dossal, and Aude Rondepierre.
\newblock {Nesterov}'s acceleration and {Polyak}'s heavy ball method in
  continuous time: convergence rate analysis under geometric conditions and
  perturbations.
\newblock \emph{arXiv preprint arXiv:1907.02710}, 2019.

\bibitem[Sebbouh et~al.(2021)Sebbouh, Gower, and
  Defazio]{Sebbouh_AS_Conv_SHB_2021}
Othmane Sebbouh, Robert~M Gower, and Aaron Defazio.
\newblock Almost sure convergence rates for stochastic gradient descent and
  stochastic heavy ball.
\newblock In \emph{Conference on Learning Theory}, pages 3935--3971. PMLR,
  2021.

\bibitem[Seide et~al.(2014)Seide, Fu, Droppo, Li, and Yu]{Seide2014}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech {DNN}s.
\newblock In \emph{Fifteenth Annual Conference of the International Speech
  Communication Association}, 2014.

\bibitem[Singh et~al.(2021)Singh, Data, George, and
  Diggavi]{Singh_SquarmSGD_2021}
Navjot Singh, Deepesh Data, Jemin George, and Suhas Diggavi.
\newblock Squarm-sgd: Communication-efficient momentum sgd for decentralized
  optimization.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  2\penalty0 (3):\penalty0 954--969, 2021.

\bibitem[Stich and Karimireddy(2019)]{Stich_Delayed_2019}
Sebastian Stich and Sai~Praneeth Karimireddy.
\newblock The error-feedback framework: Better rates for {SGD} with delayed
  gradients and compressed communication.
\newblock \emph{arXiv preprint arXiv:1909.05350}, 2019.

\bibitem[Stich and Karimireddy(2021)]{EF_delay_2021}
Sebastian~U. Stich and Sai~Praneeth Karimireddy.
\newblock The {{Error-Feedback Framework}}: {{Better Rates}} for {{SGD}} with
  {{Delayed Gradients}} and {{Compressed Communication}}.
\newblock \emph{arXiv preprint arXiv:1909.05350}, 2021.

\bibitem[Stich et~al.(2018)Stich, Cordonnier, and Jaggi]{Stich-EF-NIPS2018}
Sebastian~U. Stich, J.-B. Cordonnier, and Martin Jaggi.
\newblock Sparsified {SGD} with memory.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Szlendak et~al.(2021)Szlendak, Tyurin, and
  Richt{\'a}rik]{szlendak2021permutation}
Rafa{\l} Szlendak, Alexander Tyurin, and Peter Richt{\'a}rik.
\newblock Permutation compressors for provably faster distributed nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:2110.03300}, 2021.

\bibitem[Takezawa et~al.(2022)Takezawa, Bao, Niwa, Sato, and
  Yamada]{Takezawa_Mom_Tracking_2022}
Yuki Takezawa, Han Bao, Kenta Niwa, Ryoma Sato, and Makoto Yamada.
\newblock Momentum tracking: Momentum acceleration for decentralized deep
  learning on heterogeneous data.
\newblock \emph{arXiv preprint arXiv:2209.15505}, 2022.

\bibitem[Tang et~al.(2020)Tang, Lian, Yu, Zhang, and Liu]{DoubleSqueeze}
Hanlin Tang, Xiangru Lian, Chen Yu, Tong Zhang, and Ji~Liu.
\newblock {D}ouble{S}queeze: {P}arallel stochastic gradient descent with
  double-pass error-compensated compression.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Tyurin and Richtárik(2022)]{DASHA_2022}
Alexander Tyurin and Peter Richtárik.
\newblock Dasha: Distributed nonconvex optimization with communication
  compression, optimal oracle complexity, and no client synchronization.
\newblock \emph{arXiv preprint arXiv:2202.01268}, 2022.

\bibitem[Vogels et~al.(2019)Vogels, Karimireddy, and Jaggi]{PowerSGD}
Thijs Vogels, Sai~Praneeth Karimireddy, and Martin Jaggi.
\newblock Powersgd: Practical low-rank gradient compression for distributed
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Wang and Abernethy(2021)]{Wang_Quickly_Finding_HB_2021}
Jun-Kun Wang and Jacob Abernethy.
\newblock Quickly finding a benign region via heavy ball momentum in non-convex
  optimization.
\newblock \emph{arXiv preprint arXiv:2010.01449}, 2021.

\bibitem[Wang et~al.(2022)Wang, Lin, and Chen]{Wang_CD_ADAM-AMSGrad_2022}
Yujia Wang, Lu~Lin, and Jinghui Chen.
\newblock Communication-compressed adaptive gradient method for distributed
  nonconvex optimization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 6292--6320. PMLR, 2022.

\bibitem[Xiao and Yang(2022)]{Xiao_Conv_SGD_type_2022}
Tiannan Xiao and Guoguo Yang.
\newblock A convergence study of sgd-type methods for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:2211.06197}, 2022.

\bibitem[Xie et~al.(2020)Xie, Zheng, Koyejo, Gupta, Li, and Lin]{CSER}
Cong Xie, Shuai Zheng, Oluwasanmi Koyejo, Indranil Gupta, Mu~Li, and Haibin
  Lin.
\newblock {CSER:} {C}ommunication-efficient {SGD} with error reset.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  12593--12603, 2020.

\bibitem[Xu and Huang(2022)]{xu2022detached}
An~Xu and Heng Huang.
\newblock Detached error feedback for distributed sgd with random
  sparsification.
\newblock In \emph{International Conference on Machine Learning}, pages
  24550--24575. PMLR, 2022.

\bibitem[Yang et~al.(2023)Yang, Li, Fatkhullin, and He]{Yang_Two_Sides_2023}
Junchi Yang, Xiang Li, Ilyas Fatkhullin, and Niao He.
\newblock Two sides of one coin: the limits of untuned {SGD} and the power of
  adaptive methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Yang et~al.(2016)Yang, Lin, and Li]{Yang_Unified_momentum_2016}
Tianbao Yang, Qihang Lin, and Zhe Li.
\newblock Unified convergence analysis of stochastic momentum methods for
  convex and non-convex optimization.
\newblock \emph{arXiv preprint arXiv:1604.03257}, 2016.

\bibitem[Yau and Wai(2022)]{Yau_DoCoM_SGT_2022}
Chung-Yiu Yau and Hoi-To Wai.
\newblock {DoCoM-SGT}: Doubly compressed momentum-assisted stochastic gradient
  tracking algorithm for communication efficient decentralized learning.
\newblock \emph{arXiv preprint arXiv:2202.00255}, 2022.

\bibitem[Yu et~al.(2019)Yu, Jin, and
  Yang]{Yu_LinearSpeedup_Com_Efficient_SGDM_2019}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  sgd for distributed non-convex optimization.
\newblock In \emph{International Conference on Machine Learning}, page
  7184–7193. PMLR, 2019.

\bibitem[Zavriev and Kostyuk(1993)]{Zavriev_heavy_ball_1993}
SK~Zavriev and FV~Kostyuk.
\newblock Heavy-ball method in nonconvex optimization problems.
\newblock \emph{Computational Mathematics and Modeling}, 4\penalty0
  (4):\penalty0 336--341, 1993.

\bibitem[Zhao et~al.(2022)Zhao, Li, Li, Richt{\'a}rik, and Chi]{Zhao_BEER_2022}
Haoyu Zhao, Boyue Li, Zhize Li, Peter Richt{\'a}rik, and Yuejie Chi.
\newblock {BEER}: Fast {O(1/T)} rate for decentralized nonconvex optimization
  with communication compression.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Zheng et~al.(2019)Zheng, Huang, and Kwok]{Zheng_EF_SGD_Nesterov_2019}
Shuai Zheng, Ziyue Huang, and James Kwok.
\newblock Communication-efficient distributed blockwise momentum {SGD} with
  error-feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\end{thebibliography}
