\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{DKTZ20b}

\bibitem[ABHU15]{AwasthiBHU15}
P.~Awasthi, M.~F. Balcan, N.~Haghtalab, and R.~Urner.
\newblock Efficient learning of linear separators under bounded noise.
\newblock In {\em Proceedings of The 28th Conference on Learning Theory, {COLT}
  2015}, pages 167--190, 2015.

\bibitem[ABHZ16]{AwasthiBHZ16}
P.~Awasthi, M.~F. Balcan, N.~Haghtalab, and H.~Zhang.
\newblock Learning and 1-bit compressed sensing under asymmetric noise.
\newblock In {\em Proceedings of the 29th Conference on Learning Theory, {COLT}
  2016}, pages 152--192, 2016.

\bibitem[AL88]{AL88}
D.~Angluin and P.~Laird.
\newblock Learning from noisy examples.
\newblock {\em Machine Learning}, 2(4):343--370, 1988.

\bibitem[BBH{\etalchar{+}}20]{brennan2020statistical}
M.~Brennan, G.~Bresler, S.~Hopkins, J.~Li, and T.~Schramm.
\newblock Statistical query algorithms and low-degree tests are almost
  equivalent.
\newblock {\em arXiv preprint arXiv:2009.06107}, 2020.

\bibitem[BFKV97]{BFK+:97}
A.~Blum, A.~Frieze, R.~Kannan, and S.~Vempala.
\newblock A polynomial time algorithm for learning noisy linear threshold
  functions.
\newblock {\em Algorithmica}, 22(1/2):35--52, 1997.

\bibitem[Coh97]{Cohen:97}
E.~Cohen.
\newblock Learning noisy perceptrons by a perceptron in polynomial time.
\newblock In {\em Proceedings of the Thirty-Eighth Symposium on Foundations of
  Computer Science}, pages 514--521, 1997.

\bibitem[DDK{\etalchar{+}}23]{DDKWZ23}
I.~Diakonikolas, J.~Diakonikolas, D.~M. Kane, P.~Wang, and N.~Zarifis.
\newblock Information-computation tradeoffs for learning margin halfspaces with
  random classification noise.
\newblock {\em CoRR}, abs/2306.16352, 2023.
\newblock Conference version in COLT'23.

\bibitem[DK22]{DK20-SQ-Massart}
I.~Diakonikolas and D.~Kane.
\newblock Near-optimal statistical query hardness of learning halfspaces with
  massart noise.
\newblock In {\em Conference on Learning Theory}, volume 178 of {\em
  Proceedings of Machine Learning Research}, pages 4258--4282. {PMLR}, 2022.

\bibitem[DKK{\etalchar{+}}20]{DKKTZ20}
I.~Diakonikolas, D.~M. Kane, V.~Kontonis, C.~Tzamos, and N.~Zarifis.
\newblock A polynomial time algorithm for learning halfspaces with {T}sybakov
  noise.
\newblock {\em arXiv}, 2020.

\bibitem[DKK{\etalchar{+}}21]{DKKTZ21b}
I.~Diakonikolas, D.~M. Kane, V.~Kontonis, C.~Tzamos, and N.~Zarifis.
\newblock Efficiently learning halfspaces with {T}sybakov noise.
\newblock {\em STOC}, 2021.

\bibitem[DKK{\etalchar{+}}22]{Diakonikolas2022b}
I.~Diakonikolas, D.~M. Kane, V.~Kontonis, C.~Tzamos, and N.~Zarifis.
\newblock Learning general halfspaces with general massart noise under the
  gaussian distribution.
\newblock In {\em {STOC} '22: 54th Annual {ACM} {SIGACT} Symposium on Theory of
  Computing, Rome, Italy, June 20 - 24, 2022}, pages 874--885. {ACM}, 2022.

\bibitem[DKMR22]{DKMR22}
I.~Diakonikolas, D.~M. Kane, P.~Manurangsi, and L.~Ren.
\newblock Cryptographic hardness of learning halfspaces with massart noise.
\newblock {\em CoRR}, abs/2207.14266, 2022.
\newblock Conference version in NeurIPS'22.

\bibitem[DKS17]{DKS17-sq}
I.~Diakonikolas, D.~M. Kane, and A.~Stewart.
\newblock Statistical query lower bounds for robust estimation of
  high-dimensional gaussians and gaussian mixtures.
\newblock In {\em 2017 IEEE 58th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 73--84, 2017.

\bibitem[DKS18]{DKS18a}
I.~Diakonikolas, D.~M. Kane, and A.~Stewart.
\newblock Learning geometric concepts with nasty noise.
\newblock In {\em Proceedings of the 50th Annual {ACM} {SIGACT} Symposium on
  Theory of Computing, {STOC} 2018}, pages 1061--1073, 2018.

\bibitem[DKT21]{DKT21}
I.~Diakonikolas, D.~Kane, and C.~Tzamos.
\newblock Forster decomposition and learning halfspaces with noise.
\newblock In {\em Advances in Neural Information Processing Systems 34: Annual
  Conference on Neural Information Processing Systems 2021, NeurIPS 2021},
  pages 7732--7744, 2021.

\bibitem[DKTZ20a]{DKTZ20}
I.~Diakonikolas, V.~Kontonis, C.~Tzamos, and N.~Zarifis.
\newblock Learning halfspaces with massart noise under structured
  distributions.
\newblock In {\em Conference on Learning Theory, {COLT}}, 2020.

\bibitem[DKTZ20b]{DKTZ20b}
I.~Diakonikolas, V.~Kontonis, C.~Tzamos, and N.~Zarifis.
\newblock Learning halfspaces with {T}sybakov noise.
\newblock {\em arXiv}, 2020.

\bibitem[DKTZ22]{DKTZ22a}
I.~Diakonikolas, V.~Kontonis, C.~Tzamos, and N.~Zarifis.
\newblock Learning {{General Halfspaces}} with {{Adversarial Label Noise}} via
  {{Online Gradient Descent}}.
\newblock In {\em Proceedings of the 39th {{International Conference}} on
  {{Machine Learning}}}, 2022.

\bibitem[DTK22]{DTK23}
I.~Diakonikolas, C.~Tzamos, and D.~M. Kane.
\newblock A strongly polynomial algorithm for approximate forster transforms
  and its application to halfspace learning.
\newblock {\em CoRR}, abs/2212.03008, 2022.
\newblock To appear in STOC'23.

\bibitem[Fel16]{Feldman16b}
V.~Feldman.
\newblock Statistical query learning.
\newblock In {\em Encyclopedia of Algorithms}, pages 2090--2095. 2016.

\bibitem[FGR{\etalchar{+}}17]{FeldmanGRVX17}
V.~Feldman, E.~Grigorescu, L.~Reyzin, S.~Vempala, and Y.~Xiao.
\newblock Statistical algorithms and a lower bound for detecting planted
  cliques.
\newblock {\em J. {ACM}}, 64(2):8:1--8:37, 2017.

\bibitem[FGV17]{FeldmanGV17}
V.~Feldman, C.~Guzman, and S.~S. Vempala.
\newblock Statistical query algorithms for mean vector estimation and
  stochastic convex optimization.
\newblock In Philip~N. Klein, editor, {\em Proceedings of the Twenty-Eighth
  Annual {ACM-SIAM} Symposium on Discrete Algorithms, {SODA} 2017}, pages
  1265--1277. {SIAM}, 2017.

\bibitem[Foa78]{MehlerForm}
D.~Foata.
\newblock A combinatorial proof of the mehler formula.
\newblock {\em Journal of Combinatorial Theory, Series A}, 1978.

\bibitem[FS97]{FreundSchapire:97}
Y.~Freund and R.~Schapire.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock {\em Journal of Computer and System Sciences}, 55(1):119--139, 1997.

\bibitem[God55]{Godwin1955}
H.~J. Godwin.
\newblock On generalizations of tchebychef's inequality.
\newblock {\em Journal of the American Statistical Association},
  50(271):923--945, 1955.

\bibitem[HY15]{HannekeY15}
S.~Hanneke and L.~Yang.
\newblock Minimax analysis of active learning.
\newblock {\em J. Mach. Learn. Res.}, 16:3487--3602, 2015.

\bibitem[Kea98]{Kearns:98}
M.~J. Kearns.
\newblock Efficient noise-tolerant learning from statistical queries.
\newblock {\em Journal of the ACM}, 45(6):983--1006, 1998.

\bibitem[MN06]{Massart2006}
P.~Massart and E.~Nedelec.
\newblock Risk bounds for statistical learning.
\newblock {\em Ann. Statist.}, 34(5):2326--2366, 10 2006.

\bibitem[Ros58]{Rosenblatt:58}
F.~Rosenblatt.
\newblock The {P}erceptron: a probabilistic model for information storage and
  organization in the brain.
\newblock {\em Psychological Review}, 65:386--407, 1958.

\bibitem[Val84a]{Valiant:84}
L.~Valiant.
\newblock A theory of the learnable.
\newblock {\em Communications of the ACM}, 27(11):1134--1142, 1984.

\bibitem[Val84b]{val84}
L.~G. Valiant.
\newblock A theory of the learnable.
\newblock In {\em Proc.\ 16th Annual ACM Symposium on Theory of Computing
  (STOC)}, pages 436--445. ACM Press, 1984.

\bibitem[Vap98]{Vapnik:98}
V.~Vapnik.
\newblock {\em Statistical Learning Theory}.
\newblock Wiley-Interscience, New York, 1998.

\bibitem[YZ17]{YanZ17}
S.~Yan and C.~Zhang.
\newblock Revisiting perceptron: Efficient and label-optimal learning of
  halfspaces.
\newblock In {\em Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017}, pages 1056--1066,
  2017.

\bibitem[ZL21]{ZL21}
C.~Zhang and Y.~Li.
\newblock Improved algorithms for efficient active learning halfspaces with
  massart and tsybakov noise.
\newblock In {\em Proceedings of The 34th Conference on Learning Theory,
  {COLT}}, 2021.

\bibitem[ZLC17]{ZhangLC17}
Y.~Zhang, P.~Liang, and M.~Charikar.
\newblock A hitting time analysis of stochastic gradient langevin dynamics.
\newblock In {\em Proceedings of the 30th Conference on Learning Theory, {COLT}
  2017}, pages 1980--2022, 2017.

\bibitem[ZSA20]{ZSA20}
C.~Zhang, J.~Shen, and P.~Awasthi.
\newblock Efficient active learning of sparse halfspaces with arbitrary bounded
  noise.
\newblock In {\em Advances in Neural Information Processing Systems,
  {NeurIPS}}, 2020.

\end{thebibliography}
