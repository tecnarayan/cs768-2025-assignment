\begin{thebibliography}{82}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alfeld et~al.(2017)Alfeld, Zhu, and Barford]{alfeld2017explicit}
Alfeld, S., Zhu, X., and Barford, P.
\newblock Explicit defense actions against test-set attacks.
\newblock In \emph{AAAI}, 2017.

\bibitem[Arandjelovi{\'c} \& Zisserman(2021)Arandjelovi{\'c} and
  Zisserman]{arandjelovic2021nerf}
Arandjelovi{\'c}, R. and Zisserman, A.
\newblock Nerf in detail: Learning to sample for view synthesis.
\newblock \emph{arXiv preprint arXiv:2106.05264}, 2021.

\bibitem[Arbel et~al.(2019)Arbel, Korba, Salim, and Gretton]{arbel2019maximum}
Arbel, M., Korba, A., Salim, A., and Gretton, A.
\newblock Maximum mean discrepancy gradient flow.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Arjevani et~al.(2016)Arjevani, Shalev-Shwartz, and
  Shamir]{arjevani2016lower}
Arjevani, Y., Shalev-Shwartz, S., and Shamir, O.
\newblock On lower and upper bounds in smooth and strongly convex optimization.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 4303--4353, 2016.

\bibitem[Atzmon \& Lipman(2020)Atzmon and Lipman]{atzmon2020sal}
Atzmon, M. and Lipman, Y.
\newblock Sal: Sign agnostic learning of shapes from raw data.
\newblock In \emph{CVPR}, 2020.

\bibitem[Bietti \& Mairal(2019)Bietti and Mairal]{bietti2019inductive}
Bietti, A. and Mairal, J.
\newblock On the inductive bias of neural tangent kernels.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Bietti et~al.(2019)Bietti, Mialon, Chen, and Mairal]{bietti2019kernel}
Bietti, A., Mialon, G., Chen, D., and Mairal, J.
\newblock A kernel perspective for regularizing deep neural networks.
\newblock In \emph{ICML}, 2019.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
Boyd, S., Boyd, S.~P., and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Burgess \& Van~Veen(1996)Burgess and Van~Veen]{burgess1996subspace}
Burgess, K.~A. and Van~Veen, B.~D.
\newblock Subspace-based adaptive generalized likelihood ratio detection.
\newblock \emph{IEEE Transactions on Signal Processing}, 44\penalty0
  (4):\penalty0 912--927, 1996.

\bibitem[Chen et~al.(2023)Chen, Yang, Fitzmeyer, and Hao]{chen2023rapid}
Chen, H., Yang, H., Fitzmeyer, S., and Hao, C.
\newblock Rapid-inr: Storage efficient cpu-free dnn training using implicit
  neural representation.
\newblock In \emph{ICCAD}, 2023.

\bibitem[Chen \& Xu(2020)Chen and Xu]{chen2020deep}
Chen, L. and Xu, S.
\newblock Deep neural tangent kernel and laplace kernel have the same rkhs.
\newblock In \emph{ICLR}, 2020.

\bibitem[Coleman(2012)]{coleman2012calculus}
Coleman, R.
\newblock \emph{Calculus on normed vector spaces}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Cormen et~al.(2022)Cormen, Leiserson, Rivest, and
  Stein]{cormen2022introduction}
Cormen, T.~H., Leiserson, C.~E., Rivest, R.~L., and Stein, C.
\newblock \emph{Introduction to algorithms}.
\newblock MIT press, 2022.

\bibitem[Dou \& Liang(2021)Dou and Liang]{dou2021training}
Dou, X. and Liang, T.
\newblock Training neural networks as learning data-adaptive kernels: Provable
  representation and approximation benefits.
\newblock \emph{Journal of the American Statistical Association}, 116\penalty0
  (535):\penalty0 1507--1520, 2021.

\bibitem[Dupont et~al.(2021)Dupont, Goli{\'n}ski, Alizadeh, Teh, and
  Doucet]{dupont2021coin}
Dupont, E., Goli{\'n}ski, A., Alizadeh, M., Teh, Y.~W., and Doucet, A.
\newblock Coin: Compression with implicit neural representations.
\newblock In \emph{ICLR Neural Compression Workshop}, 2021.

\bibitem[Dupont et~al.(2022)Dupont, Kim, Eslami, Rezende, and
  Rosenbaum]{dupont2022data}
Dupont, E., Kim, H., Eslami, S., Rezende, D., and Rosenbaum, D.
\newblock From data to functa: Your data point is a function and you can treat
  it like one.
\newblock In \emph{ICML}, 2022.

\bibitem[{Eastman Kodak Company}(1999)]{kodak}
{Eastman Kodak Company}.
\newblock Kodak lossless true color image suite.
\newblock \url{http://r0k.us/graphics/kodak/}, 1999.
\newblock [Accessed 14-08-2023].

\bibitem[Gao et~al.(2019)Gao, Cai, Li, Hsieh, Wang, and
  Lee]{gao2019convergence}
Gao, R., Cai, T., Li, H., Hsieh, C.-J., Wang, L., and Lee, J.~D.
\newblock Convergence of adversarial training in overparametrized neural
  networks.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Geifman et~al.(2020)Geifman, Yadav, Kasten, Galun, Jacobs, and
  Ronen]{geifman2020similarity}
Geifman, A., Yadav, A., Kasten, Y., Galun, M., Jacobs, D., and Ronen, B.
\newblock On the similarity between the laplace and neural tangent kernels.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Gelfand et~al.(2000)Gelfand, Silverman, et~al.]{gelfand2000calculus}
Gelfand, I.~M., Silverman, R.~A., et~al.
\newblock \emph{Calculus of variations}.
\newblock Courier Corporation, 2000.

\bibitem[Godunov(1997)]{godunov1997ordinary}
Godunov, S.~K.
\newblock \emph{Ordinary differential equations with constant coefficient},
  volume 169.
\newblock American Mathematical Soc., 1997.

\bibitem[Grattarola \& Vandergheynst(2022)Grattarola and
  Vandergheynst]{grattarola2022generalised}
Grattarola, D. and Vandergheynst, P.
\newblock Generalised implicit neural representations.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Graves et~al.(2017)Graves, Bellemare, Menick, Munos, and
  Kavukcuoglu]{graves2017automated}
Graves, A., Bellemare, M.~G., Menick, J., Munos, R., and Kavukcuoglu, K.
\newblock Automated curriculum learning for neural networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Gropp et~al.(2020)Gropp, Yariv, Haim, Atzmon, and
  Lipman]{gropp2020implicit}
Gropp, A., Yariv, L., Haim, N., Atzmon, M., and Lipman, Y.
\newblock Implicit geometric regularization for learning shapes.
\newblock In \emph{ICML}, 2020.

\bibitem[Hall(2013)]{hall2013quantum}
Hall, B.~C.
\newblock \emph{Quantum theory for mathematicians}.
\newblock Springer, 2013.

\bibitem[Hartman(2002)]{hartman2002ordinary}
Hartman, P.
\newblock \emph{Ordinary differential equations}.
\newblock SIAM, 2002.

\bibitem[Henaff(2020)]{henaff2020data}
Henaff, O.
\newblock Data-efficient image recognition with contrastive predictive coding.
\newblock In \emph{ICML}, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Kakade \& Tewari(2008)Kakade and Tewari]{kakade2008generalization}
Kakade, S.~M. and Tewari, A.
\newblock On the generalization ability of online strongly convex programming
  algorithms.
\newblock In \emph{NeurIPS}, 2008.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Kuk(1995)]{kuk1995asymptotically}
Kuk, A.~Y.
\newblock Asymptotically unbiased estimation in generalized linear models with
  random effects.
\newblock \emph{Journal of the Royal Statistical Society Series B: Statistical
  Methodology}, 57\penalty0 (2):\penalty0 395--407, 1995.

\bibitem[Lax(2002)]{lax2002functional}
Lax, P.~D.
\newblock \emph{Functional analysis}, volume~55.
\newblock John Wiley \& Sons, 2002.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Leshno et~al.(1993)Leshno, Lin, Pinkus, and
  Schocken]{leshno1993multilayer}
Leshno, M., Lin, V.~Y., Pinkus, A., and Schocken, S.
\newblock Multilayer feedforward networks with a nonpolynomial activation
  function can approximate any function.
\newblock \emph{Neural networks}, 6\penalty0 (6):\penalty0 861--867, 1993.

\bibitem[Li et~al.(2024{\natexlab{a}})Li, Liu, Huang, and Wong]{li2023scone}
Li, J. C.~L., Liu, C., Huang, B., and Wong, N.
\newblock Learning spatially collaged fourier bases for implicit neural
  representation.
\newblock In \emph{AAAI}, 2024{\natexlab{a}}.

\bibitem[Li et~al.(2024{\natexlab{b}})Li, Luo, Xu, and Wong]{2024asmr}
Li, J. C.~L., Luo, S. T.~S., Xu, L., and Wong, N.
\newblock Asmr: Activation-sharing multi-resolution coordinate networks for
  efficient inference.
\newblock In \emph{ICLR}, 2024{\natexlab{b}}.

\bibitem[Li et~al.(2023)Li, Wang, and Meng]{li2023regularize}
Li, Z., Wang, H., and Meng, D.
\newblock Regularize implicit neural representation by itself.
\newblock In \emph{CVPR}, 2023.

\bibitem[Lindell et~al.(2022)Lindell, Van~Veen, Park, and
  Wetzstein]{lindell2022bacon}
Lindell, D.~B., Van~Veen, D., Park, J.~J., and Wetzstein, G.
\newblock Bacon: Band-limited coordinate networks for multiscale scene
  representation.
\newblock In \emph{CVPR}, 2022.

\bibitem[Liu(2017)]{liu2017stein}
Liu, Q.
\newblock Stein variational gradient descent as gradient flow.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Liu \& Wang(2016)Liu and Wang]{liu2016stein}
Liu, Q. and Wang, D.
\newblock Stein variational gradient descent: A general purpose bayesian
  inference algorithm.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Liu et~al.(2017)Liu, Dai, Humayun, Tay, Yu, Smith, Rehg, and
  Song]{liu2017iterative}
Liu, W., Dai, B., Humayun, A., Tay, C., Yu, C., Smith, L.~B., Rehg, J.~M., and
  Song, L.
\newblock Iterative machine teaching.
\newblock In \emph{ICML}, 2017.

\bibitem[Liu et~al.(2018)Liu, Dai, Li, Liu, Rehg, and Song]{liu2018towards}
Liu, W., Dai, B., Li, X., Liu, Z., Rehg, J., and Song, L.
\newblock Towards black-box iterative machine teaching.
\newblock In \emph{ICML}, 2018.

\bibitem[Loshchilov \& Hutter(2015)Loshchilov and Hutter]{loshchilov2015online}
Loshchilov, I. and Hutter, F.
\newblock Online batch selection for faster training of neural networks.
\newblock In \emph{ICLR Workshop}, 2015.

\bibitem[Ma et~al.(2019)Ma, Zhang, Sun, and Zhu]{ma2019policy}
Ma, Y., Zhang, X., Sun, W., and Zhu, J.
\newblock Policy poisoning in batch reinforcement learning and control.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Martin-Brualla et~al.(2021)Martin-Brualla, Radwan, Sajjadi, Barron,
  Dosovitskiy, and Duckworth]{martin2021nerf}
Martin-Brualla, R., Radwan, N., Sajjadi, M.~S., Barron, J.~T., Dosovitskiy, A.,
  and Duckworth, D.
\newblock Nerf in the wild: Neural radiance fields for unconstrained photo
  collections.
\newblock In \emph{CVPR}, 2021.

\bibitem[Mildenhall et~al.(2021)Mildenhall, Srinivasan, Tancik, Barron,
  Ramamoorthi, and Ng]{mildenhall2021nerf}
Mildenhall, B., Srinivasan, P.~P., Tancik, M., Barron, J.~T., Ramamoorthi, R.,
  and Ng, R.
\newblock Nerf: Representing scenes as neural radiance fields for view
  synthesis.
\newblock In \emph{ECCV}, 2021.

\bibitem[Mindermann et~al.(2022)Mindermann, Brauner, Razzak, Sharma, Kirsch,
  Xu, H{\"o}ltgen, Gomez, Morisot, Farquhar, et~al.]{mindermann2022prioritized}
Mindermann, S., Brauner, J.~M., Razzak, M.~T., Sharma, M., Kirsch, A., Xu, W.,
  H{\"o}ltgen, B., Gomez, A.~N., Morisot, A., Farquhar, S., et~al.
\newblock Prioritized training on points that are learnable, worth learning,
  and not yet learnt.
\newblock In \emph{ICML}, 2022.

\bibitem[Molaei et~al.(2023)Molaei, Aminimehr, Tavakoli, Kazerouni, Azad, Azad,
  and Merhof]{molaei2023implicit}
Molaei, A., Aminimehr, A., Tavakoli, A., Kazerouni, A., Azad, B., Azad, R., and
  Merhof, D.
\newblock Implicit neural representation in medical imaging: A comparative
  survey.
\newblock In \emph{ICCV}, 2023.

\bibitem[M{\"u}ller et~al.(2022)M{\"u}ller, Evans, Schied, and
  Keller]{muller2022instant}
M{\"u}ller, T., Evans, A., Schied, C., and Keller, A.
\newblock Instant neural graphics primitives with a multiresolution hash
  encoding.
\newblock \emph{ACM transactions on graphics (TOG)}, 41\penalty0 (4):\penalty0
  1--15, 2022.

\bibitem[{NASA}(2018)]{pluto}
{NASA}.
\newblock True colors of pluto.
\newblock
  \url{https://solarsystem.nasa.gov/resources/933/true-colors-of-pluto/?category=planets/dwarf-planets_pluto},
  2018.

\bibitem[Panayotov et~al.(2015)Panayotov, Chen, Povey, and
  Khudanpur]{panayotov2015librispeech}
Panayotov, V., Chen, G., Povey, D., and Khudanpur, S.
\newblock Librispeech: an asr corpus based on public domain audio books.
\newblock In \emph{ICASSP}, 2015.

\bibitem[Park et~al.(2019)Park, Florence, Straub, Newcombe, and
  Lovegrove]{park2019deepsdf}
Park, J.~J., Florence, P., Straub, J., Newcombe, R., and Lovegrove, S.
\newblock Deepsdf: Learning continuous signed distance functions for shape
  representation.
\newblock In \emph{CVPR}, 2019.

\bibitem[Pistilli et~al.(2022)Pistilli, Valsesia, Fracastoro, and
  Magli]{pistilli2022signal}
Pistilli, F., Valsesia, D., Fracastoro, G., and Magli, E.
\newblock Signal compression via neural implicit representations.
\newblock In \emph{ICASSP}, 2022.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{rahaman2019spectral}
Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F.,
  Bengio, Y., and Courville, A.
\newblock On the spectral bias of neural networks.
\newblock In \emph{ICML}, 2019.

\bibitem[Rakhsha et~al.(2020)Rakhsha, Radanovic, Devidze, Zhu, and
  Singla]{rakhsha2020policy}
Rakhsha, A., Radanovic, G., Devidze, R., Zhu, X., and Singla, A.
\newblock Policy teaching via environment poisoning: Training-time adversarial
  attacks against reinforcement learning.
\newblock In \emph{ICML}, 2020.

\bibitem[Reddy et~al.(2021)Reddy, Zhang, Wang, Fisher, Jin, and
  Mitra]{reddy2021multi}
Reddy, P., Zhang, Z., Wang, Z., Fisher, M., Jin, H., and Mitra, N.
\newblock A multi-implicit neural representation for fonts.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Ruder(2016)]{ruder2016overview}
Ruder, S.
\newblock An overview of gradient descent optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1609.04747}, 2016.

\bibitem[Sch{\"o}lkopf et~al.(2002)Sch{\"o}lkopf, Smola, Bach,
  et~al.]{scholkopf2002learning}
Sch{\"o}lkopf, B., Smola, A.~J., Bach, F., et~al.
\newblock \emph{Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2002.

\bibitem[Schwarz et~al.(2023)Schwarz, Tack, Teh, Lee, and
  Shin]{schwarz2023modality}
Schwarz, J.~R., Tack, J., Teh, Y.~W., Lee, J., and Shin, J.
\newblock Modality-agnostic variational compression of implicit neural
  representations.
\newblock In \emph{ICML}, 2023.

\bibitem[Shen et~al.(2020)Shen, Wang, Ribeiro, and Hassani]{shen2020sinkhorn}
Shen, Z., Wang, Z., Ribeiro, A., and Hassani, H.
\newblock Sinkhorn barycenter via functional gradient descent.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Singla et~al.(2014)Singla, Bogunovic, Bart{\'o}k, Karbasi, and
  Krause]{singla2014near}
Singla, A., Bogunovic, I., Bart{\'o}k, G., Karbasi, A., and Krause, A.
\newblock Near-optimally teaching the crowd to classify.
\newblock In \emph{ICML}, 2014.

\bibitem[Sitzmann et~al.(2020{\natexlab{a}})Sitzmann, Chan, Tucker, Snavely,
  and Wetzstein]{sitzmann2020metasdf}
Sitzmann, V., Chan, E., Tucker, R., Snavely, N., and Wetzstein, G.
\newblock Metasdf: Meta-learning signed distance functions.
\newblock In \emph{NeurIPS}, 2020{\natexlab{a}}.

\bibitem[Sitzmann et~al.(2020{\natexlab{b}})Sitzmann, Martel, Bergman, Lindell,
  and Wetzstein]{sitzmann2020implicit}
Sitzmann, V., Martel, J., Bergman, A., Lindell, D., and Wetzstein, G.
\newblock Implicit neural representations with periodic activation functions.
\newblock In \emph{NeurIPS}, 2020{\natexlab{b}}.

\bibitem[{Stanford Computer Graphics Laboratory}(2007)]{stanford3d}
{Stanford Computer Graphics Laboratory}.
\newblock The stanford 3d scanning repository.
\newblock \url{https://graphics.stanford.edu/data/3Dscanrep/}, 2007.

\bibitem[Str{\"u}mpler et~al.(2022)Str{\"u}mpler, Postels, Yang, Gool, and
  Tombari]{strumpler2022implicit}
Str{\"u}mpler, Y., Postels, J., Yang, R., Gool, L.~V., and Tombari, F.
\newblock Implicit neural representations for image compression.
\newblock In \emph{ECCV}, 2022.

\bibitem[Tack et~al.(2023)Tack, Kim, Yu, Lee, Shin, and
  Schwarz]{tack2023learning}
Tack, J., Kim, S., Yu, S., Lee, J., Shin, J., and Schwarz, J.~R.
\newblock Learning large-scale neural fields via context pruned meta-learning.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich-Keil,
  Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{tancik2020fourier}
Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N.,
  Singhal, U., Ramamoorthi, R., Barron, J., and Ng, R.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Tancik et~al.(2021)Tancik, Mildenhall, Wang, Schmidt, Srinivasan,
  Barron, and Ng]{tancik2021learned}
Tancik, M., Mildenhall, B., Wang, T., Schmidt, D., Srinivasan, P.~P., Barron,
  J.~T., and Ng, R.
\newblock Learned initializations for optimizing coordinate-based neural
  representations.
\newblock In \emph{CVPR}, 2021.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{touvron2021training}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou,
  H.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{ICML}, 2021.

\bibitem[Van~der Walt et~al.(2014)Van~der Walt, Sch{\"o}nberger,
  Nunez-Iglesias, Boulogne, Warner, Yager, Gouillart, and Yu]{van2014scikit}
Van~der Walt, S., Sch{\"o}nberger, J.~L., Nunez-Iglesias, J., Boulogne, F.,
  Warner, J.~D., Yager, N., Gouillart, E., and Yu, T.
\newblock scikit-image: image processing in python.
\newblock \emph{PeerJ}, 2:\penalty0 e453, 2014.

\bibitem[Wang \& Vasconcelos(2021)Wang and Vasconcelos]{wang2021machine}
Wang, P. and Vasconcelos, N.
\newblock A machine teaching framework for scalable recognition.
\newblock In \emph{ICCV}, 2021.

\bibitem[Wang et~al.(2021)Wang, Nagrecha, and Vasconcelos]{wang2021gradient}
Wang, P., Nagrecha, K., and Vasconcelos, N.
\newblock Gradient-based algorithms for machine teaching.
\newblock In \emph{CVPR}, 2021.

\bibitem[Wang et~al.(2022)Wang, Fan, Chen, and Wang]{wang2022neural}
Wang, P., Fan, Z., Chen, T., and Wang, Z.
\newblock Neural implicit dictionary learning via mixture-of-expert training.
\newblock In \emph{ICML}, 2022.

\bibitem[Watanabe \& Katagiri(1995)Watanabe and
  Katagiri]{watanabe1995discriminative}
Watanabe, H. and Katagiri, S.
\newblock Discriminative subspace method for minimum error pattern recognition.
\newblock In \emph{IEEE Workshop on Neural Networks for Signal Processing},
  1995.

\bibitem[Wright(2015)]{wright2015coordinate}
Wright, S.~J.
\newblock Coordinate descent algorithms.
\newblock \emph{Mathematical programming}, 151\penalty0 (1):\penalty0 3--34,
  2015.

\bibitem[Xie et~al.(2023)Xie, Zhu, Liu, Zhang, Zhou, Cao, and Ma]{xie2023diner}
Xie, S., Zhu, H., Liu, Z., Zhang, Q., Zhou, Y., Cao, X., and Ma, Z.
\newblock Diner: Disorder-invariant implicit neural representation.
\newblock In \emph{CVPR}, 2023.

\bibitem[Y{\"u}ce et~al.(2022)Y{\"u}ce, Ortiz-Jim{\'e}nez, Besbinar, and
  Frossard]{yuce2022structured}
Y{\"u}ce, G., Ortiz-Jim{\'e}nez, G., Besbinar, B., and Frossard, P.
\newblock A structured dictionary perspective on implicit neural
  representations.
\newblock In \emph{CVPR}, 2022.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Cao, Liu, Tsang, and
  Kwok]{zhang2023mint}
Zhang, C., Cao, X., Liu, W., Tsang, I., and Kwok, J.
\newblock Nonparametric teaching for multiple learners.
\newblock In \emph{NeurIPS}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Cao, Liu, Tsang, and
  Kwok]{zhang2023nonparametric}
Zhang, C., Cao, X., Liu, W., Tsang, I., and Kwok, J.
\newblock Nonparametric iterative machine teaching.
\newblock In \emph{ICML}, 2023{\natexlab{b}}.

\bibitem[Zhou et~al.(2018)Zhou, Nelakurthi, and He]{zhou2018unlearn}
Zhou, Y., Nelakurthi, A.~R., and He, J.
\newblock Unlearn what you have learned: Adaptive crowd teaching with
  exponentially decayed memory learners.
\newblock In \emph{SIGKDD}, 2018.

\bibitem[Zhu(2015)]{zhu2015machine}
Zhu, X.
\newblock Machine teaching: An inverse problem to machine learning and an
  approach toward optimal education.
\newblock In \emph{AAAI}, 2015.

\bibitem[Zhu et~al.(2018)Zhu, Singla, Zilles, and Rafferty]{zhu2018overview}
Zhu, X., Singla, A., Zilles, S., and Rafferty, A.~N.
\newblock An overview of machine teaching.
\newblock \emph{arXiv preprint arXiv:1801.05927}, 2018.

\end{thebibliography}
