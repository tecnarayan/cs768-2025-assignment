\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu(2018)]{allen2018make}
Allen-Zhu, Z.
\newblock How to make the gradients small stochastically: Even faster convex
  and nonconvex {SGD}.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Bach \& Perchet(2016)Bach and Perchet]{bach2016highly}
Bach, F. and Perchet, V.
\newblock Highly-smooth zero-th order online optimization.
\newblock In \emph{COLT}. PMLR, 2016.

\bibitem[Bubeck et~al.(2012)Bubeck, Cesa-Bianchi, and
  Kakade]{bubeck2012towards}
Bubeck, S., Cesa-Bianchi, N., and Kakade, S.~M.
\newblock Towards minimax policies for online linear optimization with bandit
  feedback.
\newblock In \emph{COLT}, 2012.

\bibitem[Carlini \& Wagner(2017)Carlini and Wagner]{carlini2017towards}
Carlini, N. and Wagner, D.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{IEEE Symposium on Security and Privacy}, 2017.

\bibitem[Chang \& Lin(2011)Chang and Lin]{chang2011libsvm}
Chang, C.-C. and Lin, C.-J.
\newblock {LIBSVM}: A library for support vector machines.
\newblock \emph{ACM transactions on intelligent systems and technology},
  2\penalty0 (3):\penalty0 1--27, 2011.
\newblock URL \url{https://www.csie.ntu.edu.tw/~cjlin/libsvm/}.

\bibitem[Chen et~al.(2017)Chen, Zhang, Sharma, Yi, and Hsieh]{chen2017zoo}
Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J.
\newblock {ZOO}: Zeroth order optimization based black-box attacks to deep
  neural networks without training substitute models.
\newblock In \emph{Workshop on AISec}, pp.\  15--26, 2017.

\bibitem[Choromanski et~al.(2018)Choromanski, Rowland, Sindhwani, Turner, and
  Weller]{choromanski2018structured}
Choromanski, K., Rowland, M., Sindhwani, V., Turner, R., and Weller, A.
\newblock Structured evolution with compact architectures for scalable policy
  optimization.
\newblock In \emph{ICML}, 2018.

\bibitem[Clarke(1990)]{clarke1990optimization}
Clarke, F.~H.
\newblock \emph{Optimization and nonsmooth analysis}.
\newblock SIAM, 1990.

\bibitem[Cutkosky \& Orabona(2019)Cutkosky and Orabona]{cutkosky2019momentum}
Cutkosky, A. and Orabona, F.
\newblock Momentum-based variance reduction in non-convex {SGD}.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Cutkosky et~al.(2023)Cutkosky, Mehta, and
  Orabona]{cutkosky2023optimal}
Cutkosky, A., Mehta, H., and Orabona, F.
\newblock Optimal stochastic non-smooth non-convex optimization through
  online-to-non-convex conversion.
\newblock \emph{arXiv preprint arXiv:2302.03775}, 2023.

\bibitem[Davis et~al.(2022)Davis, Drusvyatskiy, Lee, Padmanabhan, and
  Ye]{davis2021gradient}
Davis, D., Drusvyatskiy, D., Lee, Y.~T., Padmanabhan, S., and Ye, G.
\newblock A gradient sampling method with complexity guarantees for {Lipschitz}
  functions in high and low dimensions.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Duchi et~al.(2012)Duchi, Bartlett, and
  Wainwright]{duchi2012randomized}
Duchi, J.~C., Bartlett, P.~L., and Wainwright, M.~J.
\newblock Randomized smoothing for stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  674--701, 2012.

\bibitem[Duchi et~al.(2015)Duchi, Jordan, Wainwright, and
  Wibisono]{duchi2015optimal}
Duchi, J.~C., Jordan, M.~I., Wainwright, M.~J., and Wibisono, A.
\newblock Optimal rates for zero-order convex optimization: The power of two
  function evaluations.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (5):\penalty0 2788--2806, 2015.

\bibitem[Duffie(2010)]{duffie2010dynamic}
Duffie, D.
\newblock \emph{Dynamic asset pricing theory}.
\newblock Princeton University Press, 2010.

\bibitem[Fan \& Li(2001)Fan and Li]{fan2001variable}
Fan, J. and Li, R.
\newblock Variable selection via nonconcave penalized likelihood and its oracle
  properties.
\newblock \emph{Journal of the American statistical Association}, 96\penalty0
  (456):\penalty0 1348--1360, 2001.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Fang, C., Li, C.~J., Lin, Z., and Zhang, T.
\newblock {SPIDER}: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Flaxman et~al.(2004)Flaxman, Kalai, and McMahan]{flaxman2004online}
Flaxman, A.~D., Kalai, A.~T., and McMahan, H.~B.
\newblock Online convex optimization in the bandit setting: gradient descent
  without a gradient.
\newblock \emph{arXiv preprint cs/0408007}, 2004.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Glorot et~al.(2011)Glorot, Bordes, and Bengio]{glorot2011deep}
Glorot, X., Bordes, A., and Bengio, Y.
\newblock Deep sparse rectifier neural networks.
\newblock In \emph{AISTATS}, 2011.

\bibitem[Goldstein(1977)]{goldstein1977optimization}
Goldstein, A.
\newblock Optimization of lipschitz continuous functions.
\newblock \emph{Mathematical Programming}, 13\penalty0 (1):\penalty0 14--22,
  1977.

\bibitem[Hong et~al.(2015)Hong, Nelson, and Xu]{hong2015discrete}
Hong, L.~J., Nelson, B.~L., and Xu, J.
\newblock Discrete optimization via simulation.
\newblock \emph{Handbook of simulation optimization}, pp.\  9--44, 2015.

\bibitem[Huang et~al.(2022)Huang, Gao, Pei, and Huang]{huang2022accelerated}
Huang, F., Gao, S., Pei, J., and Huang, H.
\newblock Accelerated zeroth-order and first-order momentum methods from mini
  to minimax optimization.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (36):\penalty0 1--70, 2022.

\bibitem[Ji et~al.(2019)Ji, Wang, Zhou, and Liang]{ji2019improved}
Ji, K., Wang, Z., Zhou, Y., and Liang, Y.
\newblock Improved zeroth-order variance reduced algorithms and analysis for
  nonconvex optimization.
\newblock In \emph{ICML}, 2019.

\bibitem[Jing et~al.(2021)Jing, Bai, George, Chakrabortty, and
  Sharma]{jing2021asynchronous}
Jing, G., Bai, H., George, J., Chakrabortty, A., and Sharma, P.~K.
\newblock Asynchronous distributed reinforcement learning for lqr control via
  zeroth-order block coordinate descent.
\newblock \emph{arXiv preprint arXiv:2107.12416}, 2021.

\bibitem[Jordan et~al.(2022)Jordan, Lin, and Zampetakis]{jordan2022complexity}
Jordan, M.~I., Lin, T., and Zampetakis, M.
\newblock On the complexity of deterministic nonsmooth and nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:2209.12463}, 2022.

\bibitem[Kornowski \& Shamir(2021)Kornowski and Shamir]{kornowski2021oracle}
Kornowski, G. and Shamir, O.
\newblock Oracle complexity in nonsmooth nonconvex optimization.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Kornowski \& Shamir(2022)Kornowski and
  Shamir]{kornowski2022complexity}
Kornowski, G. and Shamir, O.
\newblock On the complexity of finding small subgradients in nonsmooth
  optimization.
\newblock \emph{arXiv preprint arXiv:2209.10346}, 2022.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee et~al.(2021)Lee, Park, and Ryu]{lee2021geometric}
Lee, J., Park, C., and Ryu, E.
\newblock A geometric structure of acceleration and its role in making
  gradients small fast.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Levy et~al.(2021)Levy, Kavis, and Cevher]{levy2021storm+}
Levy, K., Kavis, A., and Cevher, V.
\newblock {STORM${}^+$}: Fully adaptive {SGD} with recursive momentum for
  nonconvex optimization.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Li(2019)]{li2019ssrgd}
Li, Z.
\newblock {SSRGD}: Simple stochastic recursive gradient descent for escaping
  saddle points.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Li \& Li(2022)Li and Li]{li2022simple}
Li, Z. and Li, J.
\newblock Simple and optimal stochastic gradient methods for nonsmooth
  nonconvex optimization.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (239):\penalty0 1--61, 2022.

\bibitem[Li et~al.(2021)Li, Bao, Zhang, and Richt{\'a}rik]{li2021page}
Li, Z., Bao, H., Zhang, X., and Richt{\'a}rik, P.
\newblock {PAGE}: A simple and optimal probabilistic gradient estimator for
  nonconvex optimization.
\newblock In \emph{ICML}, 2021.

\bibitem[Lin et~al.(2022)Lin, Zheng, and Jordan]{lin2022gradient}
Lin, T., Zheng, Z., and Jordan, M.~I.
\newblock Gradient-free methods for deterministic and stochastic nonsmooth
  nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:2209.05045}, 2022.

\bibitem[Liu et~al.(2018)Liu, Kailkhura, Chen, Ting, Chang, and
  Amini]{liu2018zeroth}
Liu, S., Kailkhura, B., Chen, P.-Y., Ting, P., Chang, S., and Amini, L.
\newblock Zeroth-order stochastic variance reduction for nonconvex
  optimization.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Mania et~al.(2018)Mania, Guy, and Recht]{mania2018simple}
Mania, H., Guy, A., and Recht, B.
\newblock Simple random search provides a competitive approach to reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1803.07055}, 2018.

\bibitem[Mazumder et~al.(2011)Mazumder, Friedman, and
  Hastie]{mazumder2011sparsenet}
Mazumder, R., Friedman, J.~H., and Hastie, T.
\newblock Sparsenet: Coordinate descent with nonconvex penalties.
\newblock \emph{Journal of the American Statistical Association}, 106\penalty0
  (495):\penalty0 1125--1138, 2011.

\bibitem[Nair \& Hinton(2010)Nair and Hinton]{nair2010rectified}
Nair, V. and Hinton, G.~E.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In \emph{ICML}, 2010.

\bibitem[Nelson(2010)]{nelson2010optimization}
Nelson, B.~L.
\newblock Optimization via simulation over discrete decision variables.
\newblock \emph{Risk and optimization in an uncertain world}, pp.\  193--207,
  2010.

\bibitem[Nesterov(2012)]{nesterov2012make}
Nesterov, Y.
\newblock How to make the gradients small.
\newblock \emph{Optima. Mathematical Optimization Society Newsletter},
  88:\penalty0 10--11, 2012.

\bibitem[Nesterov \& Spokoiny(2017)Nesterov and Spokoiny]{nesterov2017random}
Nesterov, Y. and Spokoiny, V.
\newblock Random gradient-free minimization of convex functions.
\newblock \emph{Foundations of Computational Mathematics}, 17\penalty0
  (2):\penalty0 527--566, 2017.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2017sarah}
Nguyen, L.~M., Liu, J., Scheinberg, K., and Tak{\'a}{\v{c}}, M.
\newblock {SARAH}: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In \emph{ICML}, 2017.

\bibitem[Pham et~al.(2020)Pham, Nguyen, Phan, and Tran-Dinh]{pham2020proxsarah}
Pham, N.~H., Nguyen, L.~M., Phan, D.~T., and Tran-Dinh, Q.
\newblock {ProxSARAH}: An efficient algorithmic framework for stochastic
  composite nonconvex optimization.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (110):\penalty0 1--48, 2020.

\bibitem[Reddi et~al.(2016)Reddi, Sra, Poczos, and Smola]{j2016proximal}
Reddi, S.~J., Sra, S., Poczos, B., and Smola, A.~J.
\newblock Proximal stochastic methods for nonsmooth nonconvex finite-sum
  optimization.
\newblock In \emph{NIPS}, 2016.

\bibitem[Shamir(2013)]{shamir2013complexity}
Shamir, O.
\newblock On the complexity of bandit and derivative-free stochastic convex
  optimization.
\newblock In \emph{COLT}. PMLR, 2013.

\bibitem[Shamir(2017)]{shamir2017optimal}
Shamir, O.
\newblock An optimal algorithm for bandit and zero-order convex optimization
  with two-point feedback.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 1703--1713, 2017.

\bibitem[Stadtler(2008)]{stadtler2008supply}
Stadtler, H.
\newblock Supply chain managementâ€”an overview.
\newblock \emph{Supply chain management and advanced planning}, pp.\  9--36,
  2008.

\bibitem[Suh et~al.(2022)Suh, Simchowitz, Zhang, and
  Tedrake]{suh2022differentiable}
Suh, H.~J., Simchowitz, M., Zhang, K., and Tedrake, R.
\newblock Do differentiable simulators give better policy gradients?
\newblock In \emph{ICML}, 2022.

\bibitem[Tian \& So(2021)Tian and So]{tian2021hardness}
Tian, L. and So, A. M.-C.
\newblock On the hardness of computing near-approximate stationary points of
  clarke regular nonsmooth nonconvex problems and certain {DC} programs.
\newblock In \emph{ICML Workshop on Beyond First-Order Methods in ML Systems},
  2021.

\bibitem[Tian \& So(2022)Tian and So]{tian2022no}
Tian, L. and So, A. M.-C.
\newblock No dimension-free deterministic algorithm computes approximate
  stationarities of lipschitzians.
\newblock \emph{arXiv preprint arXiv:2210.06907}, 2022.

\bibitem[Tian et~al.(2022)Tian, Zhou, and So]{tian2022finite}
Tian, L., Zhou, K., and So, A. M.-C.
\newblock On the finite-time complexity and practical computation of
  approximate stationarity concepts of lipschitz functions.
\newblock In \emph{ICML}, 2022.

\bibitem[Wang et~al.(2019)Wang, Ji, Zhou, Liang, and
  Tarokh]{wang2019spiderboost}
Wang, Z., Ji, K., Zhou, Y., Liang, Y., and Tarokh, V.
\newblock Spider{B}oost and momentum: Faster variance reduction algorithms.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Xiao \& Zhang(2014)Xiao and Zhang]{xiao2014proximal}
Xiao, L. and Zhang, T.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (4):\penalty0
  2057--2075, 2014.

\bibitem[Ye et~al.(2018)Ye, Huang, Fang, Li, and Zhang]{ye2018hessian}
Ye, H., Huang, Z., Fang, C., Li, C.~J., and Zhang, T.
\newblock Hessian-aware zeroth-order optimization for black-box adversarial
  attack.
\newblock \emph{arXiv preprint arXiv:1812.11377}, 2018.

\bibitem[Zhang(2010{\natexlab{a}})]{zhang2010nearly}
Zhang, C.-H.
\newblock Nearly unbiased variable selection under minimax concave penalty.
\newblock \emph{The Annals of statistics}, 38\penalty0 (2):\penalty0 894--942,
  2010{\natexlab{a}}.

\bibitem[Zhang et~al.(2006)Zhang, Ahn, Lin, and Park]{zhang2006gene}
Zhang, H.~H., Ahn, J., Lin, X., and Park, C.
\newblock Gene selection using support vector machines with non-convex penalty.
\newblock \emph{bioinformatics}, 22\penalty0 (1):\penalty0 88--95, 2006.

\bibitem[Zhang et~al.(2020)Zhang, Lin, Jegelka, Sra, and
  Jadbabaie]{zhang2020complexity}
Zhang, J., Lin, H., Jegelka, S., Sra, S., and Jadbabaie, A.
\newblock Complexity of finding stationary points of nonsmooth nonconvex
  functions.
\newblock In \emph{ICML}, 2020.

\bibitem[Zhang(2010{\natexlab{b}})]{zhang2010analysis}
Zhang, T.
\newblock Analysis of multi-stage convex relaxation for sparse regularization.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (3),
  2010{\natexlab{b}}.

\end{thebibliography}
