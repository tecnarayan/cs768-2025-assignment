@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{chen2020adversarial,
  title={Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning},
  author={Chen, Tianlong and Liu, Sijia and Chang, Shiyu and Cheng, Yu and Amini, Lisa and Wang, Zhangyang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={699--708},
  year={2020}
}

@article{ganesh2020compressing,
  title={Compressing large-scale transformer-based models: A case study on bert},
  author={Ganesh, Prakhar and Chen, Yao and Lou, Xin and Khan, Mohammad Ali and Yang, Yin and Chen, Deming and Winslett, Marianne and Sajjad, Hassan and Nakov, Preslav},
  journal={arXiv preprint arXiv:2002.11985},
  year={2020}
}

@article{guo2020reweighted,
  title={Reweighted proximal pruning for large-scale language representation},
  author={Guo, Fu-Ming and Liu, Sijia and Mungall, Finlay S and Lin, Xue and Wang, Yanzhi},
  journal={arXiv preprint arXiv:1909.12486},
  year={2019}
}

@inproceedings{
Fan2020Reducing,
title={Reducing Transformer Depth on Demand with Structured Dropout},
author={Angela Fan and Edouard Grave and Armand Joulin},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SylO2yStDr}
}

@inproceedings{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  booktitle={Advances in Neural Information Processing Systems},
  pages={14014--14024},
  year={2019}
}

@article{mccarley2019structured,
    title={Structured Pruning of a BERT-based Question Answering Model},
    author={J. S. McCarley and Rishav Chakravarti and Avirup Sil},
    year={2019},
    journal={arXiv preprint arXiv:1910.06360}
}

@inproceedings{
Lan2020ALBERT:,
title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1eA7AEtvS}
}

@article{zafrir2019q8bert,
  title={Q8bert: Quantized 8bit bert},
  author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
  journal={arXiv preprint arXiv:1910.06188},
  year={2019}
}

@inproceedings{shen2019qbert,
  author={Sheng Shen and Zhen Dong and Jiayu Ye and Linjian Ma and Zhewei Yao and Amir Gholami and Michael W. Mahoney and Kurt Keutzer},
  title={Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT},
  year={2020},
  cdate={1577836800000},
  pages={8815-8821},
  url={https://aaai.org/ojs/index.php/AAAI/article/view/6409},
  booktitle={Association for the Advancement of Artificial Intelligence},
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}


@article{jiao2020tinybert,
  title={Tinybert: Distilling bert for natural language understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={arXiv preprint arXiv:1909.10351},
  year={2019}
}

@inproceedings{sun2020mobilebert,
    title = "{M}obile{BERT}: a Compact Task-Agnostic {BERT} for Resource-Limited Devices",
    author = "Sun, Zhiqing  and
      Yu, Hongkun  and
      Song, Xiaodan  and
      Liu, Renjie  and
      Yang, Yiming  and
      Zhou, Denny",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.195",
    doi = "10.18653/v1/2020.acl-main.195",
    pages = "2158--2170",
}


@article{tang2019distilling,
  title={Distilling task-specific knowledge from bert into simple neural networks},
  author={Tang, Raphael and Lu, Yao and Liu, Linqing and Mou, Lili and Vechtomova, Olga and Lin, Jimmy},
  journal={arXiv preprint arXiv:1903.12136},
  year={2019}
}


@article{mukherjee2019distilling,
  title={Distilling transformers into simple neural networks with unlabeled transfer data},
  author={Mukherjee, Subhabrata and Awadallah, Ahmed Hassan},
  journal={arXiv preprint arXiv:1910.01769},
  year={2019}
}

@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={OpenAI Blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{gpt3,
    title={Language Models are Few-Shot Learners},
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year={2020},
    eprint={2005.14165},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{
liu2018rethinking,
title={Rethinking the Value of Network Pruning},
author={Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJlnB3C5Ym},
}

@article{turing,
  title={Turing-NLG: A 17-billion-parameter language model by Microsoft},
  author={Rosset, Corby},
  journal={Microsoft Research Blog},
  volume={2},
  number={13},
  year={2020}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}


@article{blalock2020state,
  title={What is the state of neural network pruning?},
  author={Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
  journal={arXiv preprint arXiv:2003.03033},
  year={2020}
}

@article{Desai_2019,
   title={Evaluating Lottery Tickets Under Distributional Shifts},
   url={http://dx.doi.org/10.18653/v1/D19-6117},
   DOI={10.18653/v1/d19-6117},
   journal={Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)},
   publisher={Association for Computational Linguistics},
   author={Desai, Shrey and Zhan, Hongyuan and Aly, Ahmed},
   year={2019}
}



@inproceedings{strubell-acl,
    title = "Energy and Policy Considerations for Deep Learning in {NLP}",
    author = "Strubell, Emma  and
      Ganesh, Ananya  and
      McCallum, Andrew",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1355",
    doi = "10.18653/v1/P19-1355",
    pages = "3645--3650",
    abstract = "Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.",
}

@inproceedings{
snip,
title={{SNIP}: {SINGLE}-{SHOT} {NETWORK} {PRUNING} {BASED} {ON} {CONNECTION} {SENSITIVITY}},
author={Namhoon Lee and Thalaiyasingam Ajanthan and Philip Torr},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1VZqjAcYX},
}

@article{dettmers,
  title={Sparse networks from scratch: Faster training without losing performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}

@inproceedings{
grasp,
title={Picking Winning Tickets Before Training by Preserving Gradient Flow},
author={Chaoqi Wang and Guodong Zhang and Roger Grosse},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkgsACVKPH}
}


@article{liu2019attentive,
  title={Attentive student meets multi-task teacher: Improved knowledge distillation for pretrained models},
  author={Liu, Linqing and Wang, Huan and Lin, Jimmy and Socher, Richard and Xiong, Caiming},
  journal={arXiv preprint arXiv:1911.03588},
  year={2019}
}

@InProceedings{moco,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
 booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@inproceedings{wu2018deep,
  title={Deep k-Means: Re-Training and Parameter Sharing with Harder Cluster Assignments for Compressing Deep Convolutions},
  author={Wu, Junru and Wang, Yue and Wu, Zhenyu and Wang, Zhangyang and Veeraraghavan, Ashok and Lin, Yingyan},
  booktitle={International Conference on Machine Learning},
  pages={5363--5372},
  year={2018}
}

@inproceedings{chen2019data,
  title={Data-free learning of student networks},
  author={Chen, Hanting and Wang, Yunhe and Xu, Chang and Yang, Zhaohui and Liu, Chuanjian and Shi, Boxin and Xu, Chunjing and Xu, Chao and Tian, Qi},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={3514--3522},
  year={2019}
}


@inproceedings{wang2018adversarial,
  title={Adversarial Learning of Portable Student Networks},
  author={Wang, Yunhe and Xu, Chang and Xu, Chao and Tao, Dacheng},
  booktitle={Association for the Advancement of Artificial Intelligence},
  year={2018}
}

@inproceedings{yang2019legonet,
  title={Legonet: Efficient convolutional neural networks with lego filters},
  author={Yang, Zhaohui and Wang, Yunhe and Liu, Chuanjian and Chen, Hanting and Xu, Chunjing and Shi, Boxin and Xu, Chao and Xu, Chang},
  booktitle={International Conference on Machine Learning},
  pages={7005--7014},
  year={2019}
}


@article{simclr,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2002.05709},
  year={2020}
}

@article{Sun_2019,
   title={Patient Knowledge Distillation for BERT Model Compression},
   url={http://dx.doi.org/10.18653/v1/d19-1441},
   DOI={10.18653/v1/d19-1441},
   journal={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
   publisher={Association for Computational Linguistics},
   author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
   year={2019}
}

@inproceedings{evci2019rigging,
  title={Rigging the Lottery: Making All Tickets Winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

@article{erhan2010does,
  title={Why does unsupervised pre-training help deep learning?},
  author={Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Feb},
  pages={625--660},
  year={2010}
}

@inproceedings{
frankle2018the,
title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
author={Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJl-b3RcF7},
}

@inproceedings{frankle2019lottery,
  title={Linear Mode Connectivity and the Lottery Ticket Hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  year={2020}
}

@inproceedings{elsen2020fast,
  title={Fast sparse convnets},
  author={Elsen, Erich and Dukhan, Marat and Gale, Trevor and Simonyan, Karen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14629--14638},
  year={2020}
}

@inproceedings{
Renda2020Comparing,
title={Comparing Rewinding and Fine-tuning in Neural Network Pruning},
author={Alex Renda and Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1gSj0NKvB}
}

@article{yu2019playing,
  title={Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP},
  author={Yu, Haonan and Edunov, Sergey and Tian, Yuandong and Morcos, Ari S},
  journal={arXiv preprint arXiv:1906.02768},
  year={2019}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}

@article{mehta2019sparse,
  title={Sparse transfer learning via winning lottery tickets},
  author={Mehta, Rahul},
  journal={arXiv preprint arXiv:1905.07785},
  year={2019}
}

@article{jin2019bert,
  title={Is bert really robust? natural language attack on text classification and entailment},
  author={Jin, Di and Jin, Zhijing and Zhou, Joey Tianyi and Szolovits, Peter},
  journal={arXiv preprint arXiv:1907.11932},
  volume={2},
  year={2019}
}

@inproceedings{morcos2019one,
  title={One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
  author={Morcos, Ari and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4933--4943},
  year={2019}
}

@article{you2019drawing,
  title={Drawing early-bird tickets: Towards more efficient training of deep networks},
  author={You, Haoran and Li, Chaojian and Xu, Pengfei and Fu, Yonggan and Wang, Yue and Chen, Xiaohan and Lin, Yingyan and Wang, Zhangyang and Baraniuk, Richard G},
  journal={arXiv preprint arXiv:1909.11957},
  year={2019}
}

@inproceedings{Frankle2020The,
title={The Early Phase of Neural Network Training},
author={Jonathan Frankle and David J. Schwab and Ari S. Morcos},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=Hkl1iRNFwS}
}

@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@inproceedings{
Wang2020Picking,
title={Picking Winning Tickets Before Training by Preserving Gradient Flow},
author={Chaoqi Wang and Guodong Zhang and Roger Grosse},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkgsACVKPH}
}


@inproceedings{
loshchilov2018decoupled,
title={Decoupled Weight Decay Regularization},
author={Ilya Loshchilov and Frank Hutter},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=Bkg6RiCqY7},
}


@inproceedings{
wang2018glue,
title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
author={Alex Wang and Amanpreet Singh and Julian Michael and Felix Hill and Omer Levy and Samuel R. Bowman},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJ4km2R5t7},
}

@inproceedings{RajpurkarZLL16,
  author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  year={2016},
  cdate={1451606400000},
  pages={2383-2392},
  url={http://aclweb.org/anthology/D/D16/D16-1264.pdf},
  booktitle={EMNLP}
}

@article{prasanna2020bert,
  title={When BERT Plays the Lottery, All Tickets Are Winning},
  author={Prasanna, Sai and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2005.00561},
  year={2020}
}