\begin{thebibliography}{89}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[com()]{commoncrawl}
Common crawl.
\newblock \url{https://commoncrawl.org/}.
\newblock Accessed: 2022-05-18.

\bibitem[lai()]{laion5b}
Laion-5b: A new era of open large-scale multi-modal datasets.
\newblock
  \url{https://laion.ai/laion-5b-a-new-era-of-open-large-scale-multi-modal-datasets/}.
\newblock Accessed: 2022-05-18.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds,
  et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{arXiv preprint arXiv:2204.14198}, 2022.

\bibitem[Andreassen et~al.(2021)Andreassen, Bahri, Neyshabur, and
  Roelofs]{andreassen2021evolution}
Anders Andreassen, Yasaman Bahri, Behnam Neyshabur, and Rebecca Roelofs.
\newblock The evolution of out-of-distribution robustness throughout
  fine-tuning, 2021.
\newblock \url{https://arxiv.org/abs/2106.15831}.

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{arjovsky2019invariant}
Martin Arjovsky, L{\'e}on Bottou, Ishaan Gulrajani, and David Lopez-Paz.
\newblock Invariant risk minimization, 2019.
\newblock \url{https://arxiv.org/abs/1907.02893}.

\bibitem[Barbu et~al.(2019)Barbu, Mayo, Alverio, Luo, Wang, Gutfreund,
  Tenenbaum, and Katz]{barbu2019objectnet}
Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan
  Gutfreund, Josh Tenenbaum, and Boris Katz.
\newblock Objectnet: A large-scale bias-controlled dataset for pushing the
  limits of object recognition models.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Bauer and Kohavi(1999)]{bauer1999empirical}
Eric Bauer and Ron Kohavi.
\newblock An empirical comparison of voting classification algorithms: Bagging,
  boosting, and variants.
\newblock \emph{Machine learning}, 1999.
\newblock \url{https://link.springer.com/article/10.1023/A:1007515423169}.

\bibitem[Biggio and Roli(2018)]{biggio2018wild}
Battista Biggio and Fabio Roli.
\newblock Wild patterns: Ten years after the rise of adversarial machine
  learning.
\newblock \emph{Pattern Recognition}, 84:\penalty0 317--331, 2018.

\bibitem[Biggio et~al.(2013)Biggio, Corona, Maiorca, Nelson, {\v{S}}rndi{\'c},
  Laskov, Giacinto, and Roli]{biggio2013evasion}
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim
  {\v{S}}rndi{\'c}, Pavel Laskov, Giorgio Giacinto, and Fabio Roli.
\newblock Evasion attacks against machine learning at test time.
\newblock In \emph{Joint European conference on machine learning and knowledge
  discovery in databases}, pages 387--402. Springer, 2013.

\bibitem[Birhane et~al.(2021)Birhane, Prabhu, and
  Kahembwe]{birhane2021multimodal}
Abeba Birhane, Vinay~Uday Prabhu, and Emmanuel Kahembwe.
\newblock Multimodal datasets: misogyny, pornography, and malignant
  stereotypes.
\newblock \emph{arXiv preprint arXiv:2110.01963}, 2021.

\bibitem[Breiman(1996)]{breiman1996bagging}
Leo Breiman.
\newblock Bagging predictors.
\newblock \emph{Machine learning}, 1996.
\newblock \url{https://link.springer.com/article/10.1007/BF00058655}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Changpinyo et~al.(2021)Changpinyo, Sharma, Ding, and
  Soricut]{changpinyo2021conceptual}
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 3558--3568, 2021.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[D'Amour et~al.(2020)D'Amour, Heller, Moldovan, Adlam, Alipanahi,
  Beutel, Chen, Deaton, Eisenstein, Hoffman, et~al.]{d2020underspecification}
Alexander D'Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi,
  Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew~D
  Hoffman, et~al.
\newblock Underspecification presents challenges for credibility in modern
  machine learning, 2020.
\newblock \url{https://arxiv.org/abs/2011.03395}.

\bibitem[Darlow et~al.(2018)Darlow, Crowley, Antoniou, and
  Storkey]{darlow2018cinic}
Luke~N Darlow, Elliot~J Crowley, Antreas Antoniou, and Amos~J Storkey.
\newblock Cinic-10 is not imagenet or cifar-10.
\newblock \emph{arXiv preprint arXiv:1810.03505}, 2018.

\bibitem[Dasgupta et~al.(2019)Dasgupta, Hsu, Poulis, and
  Zhu]{dasgupta2019teaching}
Sanjoy Dasgupta, Daniel Hsu, Stefanos Poulis, and Xiaojin Zhu.
\newblock Teaching a black-box learner.
\newblock In \emph{International Conference on Machine Learning}, pages
  1547--1555. PMLR, 2019.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition},
  2009.
\newblock \url{https://ieeexplore.ieee.org/document/5206848}.

\bibitem[Desai et~al.(2021)Desai, Kaul, Aysola, and Johnson]{desai2021redcaps}
Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson.
\newblock Redcaps: Web-curated image-text data created by the people, for the
  people.
\newblock \emph{arXiv preprint arXiv:2111.11431}, 2021.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dietterich(2000)]{dietterich2000ensemble}
Thomas~G Dietterich.
\newblock Ensemble methods in machine learning.
\newblock In \emph{International workshop on multiple classifier systems},
  2000.
\newblock \url{https://link.springer.com/chapter/10.1007/3-540-45014-9_1}.

\bibitem[Djolonga et~al.(2021)Djolonga, Yung, Tschannen, Romijnders, Beyer,
  Kolesnikov, Puigcerver, Minderer, D'Amour, Moldovan,
  et~al.]{djolonga2021robustness}
Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer,
  Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D'Amour,
  Dan Moldovan, et~al.
\newblock On robustness and transferability of convolutional neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 16458--16468, 2021.

\bibitem[Fang et~al.(2022)Fang, Ilharco, Wortsman, Wan, Shankar, Dave, and
  Schmidt]{fang2022data}
Alex Fang, Gabriel Ilharco, Mitchell Wortsman, Yuhao Wan, Vaishaal Shankar,
  Achal Dave, and Ludwig Schmidt.
\newblock Data determines distributional robustness in contrastive language
  image pre-training (clip).
\newblock \emph{arXiv preprint arXiv:2205.01397}, 2022.

\bibitem[Fort et~al.(2019)Fort, Hu, and Lakshminarayanan]{fort2019deep}
Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan.
\newblock Deep ensembles: A loss landscape perspective, 2019.
\newblock \url{https://arxiv.org/abs/1912.02757}.

\bibitem[Freund and Schapire(1997)]{FREUND1997119}
Yoav Freund and Robert~E Schapire.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{Journal of Computer and System Sciences}, 1997.
\newblock
  \url{https://www.sciencedirect.com/science/article/pii/S002200009791504X}.

\bibitem[Fu et~al.(2021)Fu, Jia, Gao, Gong, Zhao, Maybank, and Tao]{fu20213d}
Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and
  Dacheng Tao.
\newblock 3d-future: 3d furniture shape with texture.
\newblock \emph{International Journal of Computer Vision}, pages 1--25, 2021.

\bibitem[Gao et~al.(2021)Gao, Geng, Zhang, Ma, Fang, Zhang, Li, and
  Qiao]{gao2021clip}
Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang,
  Hongsheng Li, and Yu~Qiao.
\newblock Clip-adapter: Better vision-language models with feature adapters,
  2021.
\newblock \url{https://arxiv.org/abs/2110.04544}.

\bibitem[Gontijo-Lopes et~al.(2021)Gontijo-Lopes, Dauphin, and
  Cubuk]{gontijo2021no}
Raphael Gontijo-Lopes, Yann Dauphin, and Ekin~D Cubuk.
\newblock No one representation to rule them all: Overlapping features of
  training methods, 2021.
\newblock \url{https://arxiv.org/abs/2007.01434}.

\bibitem[Greff et~al.(2022)Greff, Belletti, Beyer, Doersch, Du, Duckworth,
  Fleet, Gnanapragasam, Golemo, Herrmann, et~al.]{greff2022kubric}
Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel
  Duckworth, David~J Fleet, Dan Gnanapragasam, Florian Golemo, Charles
  Herrmann, et~al.
\newblock Kubric: A scalable dataset generator.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 3749--3761, 2022.

\bibitem[Gulrajani and Lopez-Paz(2020)]{gulrajani2020search}
Ishaan Gulrajani and David Lopez-Paz.
\newblock In search of lost domain generalization.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2020.
\newblock \url{https://arxiv.org/abs/2007.01434}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 9729--9738, 2020.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and
  Girshick]{he2022masked}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross
  Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 16000--16009, 2022.

\bibitem[Hendrycks and Dietterich(2019)]{imagenetc}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.
\newblock \url{https://arxiv.org/abs/1903.12261}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo,
  Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2021many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 8340--8349, 2021.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Ilharco et~al.(2021)Ilharco, Wortsman, Carlini, Taori, Dave, Shankar,
  Namkoong, Miller, Hajishirzi, Farhadi, and
  Schmidt]{ilharco_gabriel_2021_5143773}
Gabriel Ilharco, Mitchell Wortsman, Nicholas Carlini, Rohan Taori, Achal Dave,
  Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali
  Farhadi, and Ludwig Schmidt.
\newblock Openclip, July 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5143773}.
\newblock If you use this software, please cite it as below.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and
  Duerig]{jia2021scaling}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
  Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In \emph{International Conference on Machine Learning}, pages
  4904--4916. PMLR, 2021.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Killamsetty et~al.(2021)Killamsetty, Durga, Ramakrishnan, De, and
  Iyer]{killamsetty2021grad}
Krishnateja Killamsetty, S~Durga, Ganesh Ramakrishnan, Abir De, and Rishabh
  Iyer.
\newblock Grad-match: Gradient matching based data subset selection for
  efficient deep model training.
\newblock In \emph{International Conference on Machine Learning}, pages
  5464--5474. PMLR, 2021.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Gao, et~al.]{koh2021wilds}
Pang~Wei Koh, Shiori Sagawa, Henrik Marklund, Sang~Michael Xie, Marvin Zhang,
  Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard~Lanas Phillips,
  Irena Gao, et~al.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning}, pages
  5637--5664. PMLR, 2021.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{cifar10}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krueger et~al.(2021)Krueger, Caballero, Jacobsen, Zhang, Binas, Zhang,
  Le~Priol, and Courville]{kruegernr021out}
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
  Binas, Dinghuai Zhang, Remi Le~Priol, and Aaron Courville.
\newblock Out-of-distribution generalization via risk extrapolation (rex),
  2021.
\newblock \url{https://arxiv.org/abs/2003.00688}.

\bibitem[Lakshminarayanan et~al.(2017)Lakshminarayanan, Pritzel, and
  Blundell]{deepensembles}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2017.
\newblock \url{https://arxiv.org/abs/1612.01474}.

\bibitem[Li et~al.(2021)Li, Liang, Zhao, Cui, Ouyang, Shao, Yu, and
  Yan]{li2021supervision}
Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao,
  Fengwei Yu, and Junjie Yan.
\newblock Supervision exists everywhere: A data efficient contrastive
  language-image pre-training paradigm.
\newblock \emph{arXiv preprint arXiv:2110.05208}, 2021.

\bibitem[Lieber et~al.(2021)Lieber, Sharir, Lenz, and
  Shoham]{lieber2021jurassic}
Opher Lieber, Or~Sharir, Barak Lenz, and Yoav Shoham.
\newblock Jurassic-1: Technical details and evaluation.
\newblock \emph{White Paper. AI21 Labs}, 2021.

\bibitem[Loshchilov and Hutter(2016)]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[McCormac et~al.(2016)McCormac, Handa, Leutenegger, and
  Davison]{mccormac2016scenenet}
John McCormac, Ankur Handa, Stefan Leutenegger, and Andrew~J Davison.
\newblock Scenenet rgb-d: 5m photorealistic images of synthetic indoor
  trajectories with ground truth.
\newblock \emph{arXiv preprint arXiv:1612.05079}, 2016.

\bibitem[Miller et~al.(2020)Miller, Krauth, Recht, and
  Schmidt]{miller2020effect}
John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt.
\newblock The effect of natural distribution shift on question answering
  models.
\newblock In \emph{International Conference on Machine Learning}, pages
  6905--6916. PMLR, 2020.

\bibitem[Miller et~al.(2021)Miller, Taori, Raghunathan, Sagawa, Koh, Shankar,
  Liang, Carmon, and Schmidt]{miller2021accuracy}
John~P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang~Wei Koh,
  Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt.
\newblock Accuracy on the line: on the strong correlation between
  out-of-distribution and in-distribution generalization.
\newblock In \emph{International Conference on Machine Learning}, pages
  7721--7735. PMLR, 2021.

\bibitem[Mokady et~al.(2021)Mokady, Hertz, and Bermano]{mokady2021clipcap}
Ron Mokady, Amir Hertz, and Amit~H Bermano.
\newblock Clipcap: Clip prefix for image captioning.
\newblock \emph{arXiv preprint arXiv:2111.09734}, 2021.

\bibitem[Mu et~al.(2021)Mu, Kirillov, Wagner, and Xie]{mu2021slip}
Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie.
\newblock Slip: Self-supervision meets language-image pre-training.
\newblock \emph{arXiv preprint arXiv:2112.12750}, 2021.

\bibitem[Nichol et~al.(2021)Nichol, Dhariwal, Ramesh, Shyam, Mishkin, McGrew,
  Sutskever, and Chen]{nichol2021glide}
Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
  Bob McGrew, Ilya Sutskever, and Mark Chen.
\newblock Glide: Towards photorealistic image generation and editing with
  text-guided diffusion models.
\newblock \emph{arXiv preprint arXiv:2112.10741}, 2021.

\bibitem[Nixon et~al.(2020)Nixon, Lakshminarayanan, and Tran]{nixon2020why}
Jeremy Nixon, Balaji Lakshminarayanan, and Dustin Tran.
\newblock Why are bootstrapped deep ensembles not better?
\newblock In \emph{''I Can't Believe It's Not Better!'' NeurIPS 2020 workshop},
  2020.
\newblock URL \url{https://openreview.net/forum?id=dTCir0ceyv0}.

\bibitem[Paul et~al.(2021)Paul, Ganguli, and Dziugaite]{paul2021deep}
Mansheej Paul, Surya Ganguli, and Gintare~Karolina Dziugaite.
\newblock Deep learning on a data diet: Finding important examples early in
  training.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 20596--20607, 2021.

\bibitem[Pham et~al.(2021)Pham, Dai, Ghiasi, Liu, Yu, Luong, Tan, and
  Le]{pham2021combined}
Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams~Wei Yu, Minh-Thang
  Luong, Mingxing Tan, and Quoc~V Le.
\newblock Combined scaling for zero-shot transfer learning.
\newblock \emph{arXiv preprint arXiv:2111.10050}, 2021.

\bibitem[Prato et~al.(2021)Prato, Guiroy, Caballero, Rish, and
  Chandar]{prato2021scaling}
Gabriele Prato, Simon Guiroy, Ethan Caballero, Irina Rish, and Sarath Chandar.
\newblock Scaling laws for the few-shot adaptation of pre-trained image
  classifiers.
\newblock \emph{arXiv preprint arXiv:2110.06990}, 2021.

\bibitem[Qui{\~n}onero-Candela et~al.(2008)Qui{\~n}onero-Candela, Sugiyama,
  Schwaighofer, and Lawrence]{quinonero2008dataset}
Joaquin Qui{\~n}onero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil~D
  Lawrence.
\newblock \emph{Dataset shift in machine learning}.
\newblock Mit Press, 2008.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{International Conference on Machine Learning}, pages
  8821--8831. PMLR, 2021.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and
  Chen]{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 2022.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and
  Shankar]{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International Conference on Machine Learning}, pages
  5389--5400. PMLR, 2019.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization, 2019.
\newblock \url{https://arxiv.org/abs/1911.08731}.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton,
  Ghasemipour, Ayan, Mahdavi, Lopes, et~al.]{saharia2022photorealistic}
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
  Denton, Seyed Kamyar~Seyed Ghasemipour, Burcu~Karagol Ayan, S~Sara Mahdavi,
  Rapha~Gontijo Lopes, et~al.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock \emph{arXiv preprint arXiv:2205.11487}, 2022.

\bibitem[Schuhmann et~al.(2021)Schuhmann, Vencu, Beaumont, Kaczmarczyk, Mullis,
  Katta, Coombes, Jitsev, and Komatsuzaki]{schuhmann2021laion}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk,
  Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran
  Komatsuzaki.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text
  pairs.
\newblock \emph{arXiv preprint arXiv:2111.02114}, 2021.

\bibitem[Shankar et~al.(2020)Shankar, Roelofs, Mania, Fang, Recht, and
  Schmidt]{shankar2020evaluating}
Vaishaal Shankar, Rebecca Roelofs, Horia Mania, Alex Fang, Benjamin Recht, and
  Ludwig Schmidt.
\newblock Evaluating machine accuracy on imagenet.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.
\newblock \url{http://proceedings.mlr.press/v119/shankar20c/shankar20c.pdf}.

\bibitem[Sorscher et~al.(2022)Sorscher, Geirhos, Shekhar, Ganguli, and
  Morcos]{sorscher2022beyond}
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari~S
  Morcos.
\newblock Beyond neural scaling laws: beating power law scaling via data
  pruning.
\newblock \emph{arXiv preprint arXiv:2206.14486}, 2022.

\bibitem[Srinivasan et~al.(2021)Srinivasan, Raman, Chen, Bendersky, and
  Najork]{srinivasan2021wit}
Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc
  Najork.
\newblock Wit: Wikipedia-based image text dataset for multimodal multilingual
  machine learning.
\newblock In \emph{Proceedings of the 44th International ACM SIGIR Conference
  on Research and Development in Information Retrieval}, pages 2443--2449,
  2021.

\bibitem[Stefanini et~al.(2021)Stefanini, Cornia, Baraldi, Cascianelli,
  Fiameni, and Cucchiara]{stefanini2021show}
Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia Cascianelli,
  Giuseppe Fiameni, and Rita Cucchiara.
\newblock From show to tell: A survey on image captioning.
\newblock \emph{arXiv preprint arXiv:2107.06912}, 2021.

\bibitem[Sumbul et~al.(2019)Sumbul, Charfuelan, Demir, and
  Markl]{Sumbul2019BigEarthNetAL}
Gencer Sumbul, Marcela Charfuelan, Beg{"u}m Demir, and Volker Markl.
\newblock Bigearthnet: A large-scale benchmark archive for remote sensing image
  understanding.
\newblock \emph{CoRR}, abs/1902.06148, 2019.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Taori et~al.(2020)Taori, Dave, Shankar, Carlini, Recht, and
  Schmidt]{taori2020measuring}
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht,
  and Ludwig Schmidt.
\newblock Measuring robustness to natural distribution shifts in image
  classification.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18583--18599, 2020.

\bibitem[Thomee et~al.(2016)Thomee, Shamma, Friedland, Elizalde, Ni, Poland,
  Borth, and Li]{thomee2016yfcc100m}
Bart Thomee, David~A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni,
  Douglas Poland, Damian Borth, and Li-Jia Li.
\newblock Yfcc100m: The new data in multimedia research.
\newblock \emph{Communications of the ACM}, 59\penalty0 (2):\penalty0 64--73,
  2016.

\bibitem[Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer,
  Kulshreshtha, Cheng, Jin, Bos, Baker, Du, et~al.]{thoppilan2022lamda}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv
  Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du,
  et~al.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}, 2022.

\bibitem[Torralba and Efros(2011)]{torralba2011unbiased}
Antonio Torralba and Alexei~A Efros.
\newblock Unbiased look at dataset bias.
\newblock In \emph{CVPR 2011}, pages 1521--1528. IEEE, 2011.

\bibitem[Van~Horn et~al.(2018)Van~Horn, Mac~Aodha, Song, Cui, Sun, Shepard,
  Adam, Perona, and Belongie]{van2018inaturalist}
Grant Van~Horn, Oisin Mac~Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard,
  Hartwig Adam, Pietro Perona, and Serge Belongie.
\newblock The inaturalist species classification and detection dataset.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 8769--8778, 2018.

\bibitem[Veeling et~al.(2018)Veeling, Linmans, Winkens, Cohen, and
  Welling]{veeling2018rotation}
Bastiaan~S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling.
\newblock Rotation equivariant cnns for digital pathology.
\newblock In \emph{International Conference on Medical image computing and
  computer-assisted intervention}, pages 210--218. Springer, 2018.

\bibitem[Wang et~al.(2019)Wang, Ge, Lipton, and Xing]{wang2019learning}
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric~P Xing.
\newblock Learning robust global representations by penalizing local predictive
  power.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Wortsman et~al.(2021)Wortsman, Ilharco, Kim, Li, Kornblith, Roelofs,
  Gontijo-Lopes, Hajishirzi, Farhadi, Namkoong, and
  Schmidt]{wortsman2021robust}
Mitchell Wortsman, Gabriel Ilharco, Jong~Wook Kim, Mike Li, Simon Kornblith,
  Rebecca Roelofs, Raphael Gontijo-Lopes, Hannaneh Hajishirzi, Ali Farhadi,
  Hongseok Namkoong, and Ludwig Schmidt.
\newblock Robust fine-tuning of zero-shot models.
\newblock \emph{arXiv preprint arXiv:2109.01903}, 2021.
\newblock \url{https://arxiv.org/abs/2109.01903}.

\bibitem[Wortsman et~al.(2022)Wortsman, Ilharco, Gadre, Roelofs, Gontijo-Lopes,
  Morcos, Namkoong, Farhadi, Carmon, Kornblith, et~al.]{wortsman2022model}
Mitchell Wortsman, Gabriel Ilharco, Samir~Yitzhak Gadre, Rebecca Roelofs,
  Raphael Gontijo-Lopes, Ari~S Morcos, Hongseok Namkoong, Ali Farhadi, Yair
  Carmon, Simon Kornblith, et~al.
\newblock Model soups: averaging weights of multiple fine-tuned models improves
  accuracy without increasing inference time.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2022.
\newblock \url{https://arxiv.org/abs/2203.05482}.

\bibitem[Yao et~al.(2021)Yao, Huang, Hou, Lu, Niu, Xu, Liang, Li, Jiang, and
  Xu]{yao2021filip}
Lewei Yao, Runhui Huang, Lu~Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan
  Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu.
\newblock Filip: Fine-grained interactive language-image pre-training.
\newblock \emph{arXiv preprint arXiv:2111.07783}, 2021.

\bibitem[Yu et~al.(2022)Yu, Xu, Koh, Luong, Baid, Wang, Vasudevan, Ku, Yang,
  Ayan, et~al.]{yu2022scaling}
Jiahui Yu, Yuanzhong Xu, Jing~Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang,
  Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu~Karagol Ayan, et~al.
\newblock Scaling autoregressive models for content-rich text-to-image
  generation.
\newblock \emph{arXiv preprint arXiv:2206.10789}, 2022.

\bibitem[Yuan et~al.(2021)Yuan, Chen, Chen, Codella, Dai, Gao, Hu, Huang, Li,
  Li, et~al.]{yuan2021florence}
Lu~Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao,
  Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et~al.
\newblock Florence: A new foundation model for computer vision.
\newblock \emph{arXiv preprint arXiv:2111.11432}, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Fang, Gao, Zhang, Li, Dai, Qiao, and
  Li]{zhang2021tip}
Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Kunchang Li, Jifeng Dai,
  Yu~Qiao, and Hongsheng Li.
\newblock Tip-adapter: Training-free clip-adapter for better vision-language
  modeling, 2021.
\newblock \url{https://arxiv.org/abs/2111.03930}.

\bibitem[Zhou et~al.(2022)Zhou, Yang, Loy, and Liu]{zhou2022cocoop}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Conditional prompt learning for vision-language models.
\newblock \emph{Conference on Computer Vision and Pattern Recognition (CVPR)},
  2022.
\newblock \url{https://arxiv.org/abs/2109.01134}.

\bibitem[Zhou et~al.(2020)Zhou, Palangi, Zhang, Hu, Corso, and
  Gao]{zhou2020unified}
Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng
  Gao.
\newblock Unified vision-language pre-training for image captioning and vqa.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 13041--13049, 2020.

\end{thebibliography}
