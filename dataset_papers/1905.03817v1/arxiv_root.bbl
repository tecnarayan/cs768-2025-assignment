\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aji \& Heafield(2017)Aji and Heafield]{Aji17EMNLP}
Aji, A.~F. and Heafield, K.
\newblock Sparse communication for distributed gradient descent.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing (EMNLP)}, 2017.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{Alistarh17NIPS}
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M.
\newblock {QSGD}: Communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Assran et~al.(2018)Assran, Loizou, Ballas, and Rabbat]{Assran18ArXiv}
Assran, M., Loizou, N., Ballas, N., and Rabbat, M.
\newblock Stochastic gradient push for distributed deep learning.
\newblock \emph{arXiv:1811.10792}, 2018.

\bibitem[Chen \& Huo(2016)Chen and Huo]{Chen16ICASSP}
Chen, K. and Huo, Q.
\newblock Scalable training of deep learning machines by incremental block
  training with intra-block parallel optimization and blockwise model-update
  filtering.
\newblock In \emph{Proceedings of IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, 2016.

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, Senior,
  Tucker, Yang, Le, et~al.]{Dean12NIPS}
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Senior, A.,
  Tucker, P., Yang, K., Le, Q.~V., et~al.
\newblock Large scale distributed deep networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2012.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{Dekel12JMLR}
Dekel, O., Gilad-Bachrach, R., Shamir, O., and Xiao, L.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0 (165--202),
  2012.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{ImageNet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock {ImageNet}: A large-scale hierarchical image database.
\newblock In \emph{IEEE conference on computer vision and pattern recognition
  (CVPR)}, 2009.

\bibitem[Ghadimi et~al.(2014)Ghadimi, Feyzmahdavian, and
  Johansson]{Ghadimi14ArXiv}
Ghadimi, E., Feyzmahdavian, H.~R., and Johansson, M.
\newblock Global convergence of the heavy-ball method for convex optimization.
\newblock \emph{arXiv:1412.7457}, 2014.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{GhadimiLan13SIOPT}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{Goyal17ArXiv}
Goyal, P., Doll{\'a}r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch {SGD}: training imagenet in 1 hour.
\newblock \emph{arXiv:1706.02677}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He16CVPR}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{IEEE conference on computer vision and pattern recognition
  (CVPR)}, 2016.

\bibitem[Horn \& Johnson(1985)Horn and Johnson]{book_MatrixAnalysis}
Horn, R.~A. and Johnson, C.~R.
\newblock \emph{Matrix Analysis}.
\newblock Cambridge University Press, 1985.

\bibitem[Jiang \& Agrawal(2018)Jiang and Agrawal]{Jiang18NIPS}
Jiang, P. and Agrawal, G.
\newblock A linear speedup analysis of distributed deep learning with sparse
  and quantized communication.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Jiang et~al.(2017)Jiang, Balu, Hegde, and Sarkar]{Jiang17NIPS}
Jiang, Z., Balu, A., Hegde, C., and Sarkar, S.
\newblock Collaborative deep learning in fixed topology networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{Krizhevsky09}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Technical report, University of Toronto}, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{Krizhevsky12NIPS}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2012.

\bibitem[Li et~al.(2014)Li, Andersen, Smola, and Yu]{Li14NIPS}
Li, M., Andersen, D.~G., Smola, A.~J., and Yu, K.
\newblock Communication efficient distributed machine learning with the
  parameter server.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2014.

\bibitem[Lian et~al.(2015)Lian, Huang, Li, and Liu]{Lian15NIPS}
Lian, X., Huang, Y., Li, Y., and Liu, J.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2015.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{Lian17NIPS}
Lian, X., Zhang, C., Zhang, H., Hsieh, C.-J., Zhang, W., and Liu, J.
\newblock Can decentralized algorithms outperform centralized algorithms? {A}
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Lin et~al.(2018)Lin, Stich, and Jaggi]{Lin18ArXiv}
Lin, T., Stich, S.~U., and Jaggi, M.
\newblock Don't use large mini-batches, use local {SGD}.
\newblock \emph{arXiv:1808.07217}, 2018.

\bibitem[Mania et~al.(2017)Mania, Pan, Papailiopoulos, Recht, Ramchandran, and
  Jordan]{Mania17SIOPT}
Mania, H., Pan, X., Papailiopoulos, D., Recht, B., Ramchandran, K., and Jordan,
  M.~I.
\newblock Perturbed iterate analysis for asynchronous stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2202--2229, 2017.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson,
  et~al.]{McMahan17AISTATS}
McMahan, H.~B., Moore, E., Ramage, D., Hampson, S., et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2017.

\bibitem[Nesterov(2004)]{book_ConvexOpt_Nesterov}
Nesterov, Y.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer Science \& Business Media, 2004.

\bibitem[Polyak(1964)]{Polyak64}
Polyak, B.~T.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Seide \& Agarwal(2016)Seide and Agarwal]{CNTK}
Seide, F. and Agarwal, A.
\newblock {CNTK}: {M}icrosoft's open-source deep-learning toolkit.
\newblock In \emph{Proceedings of ACM SIGKDD International Conference on
  Knowledge Discovery and Data Mining}. ACM, 2016.

\bibitem[Seide et~al.(2014)Seide, Fu, Droppo, Li, and Yu]{Seide14Interspeech}
Seide, F., Fu, H., Droppo, J., Li, G., and Yu, D.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech {DNN}s.
\newblock In \emph{Annual Conference of the International Speech Communication
  Association (INTERSPEECH)}, 2014.

\bibitem[Stich(2018)]{Stich18ArXiv}
Stich, S.~U.
\newblock Local {SGD} converges fast and communicates little.
\newblock \emph{arXiv:1805.09767}, 2018.

\bibitem[Strom(2015)]{Strom15Interspeech}
Strom, N.
\newblock Scalable distributed {DNN} training using commodity {GPU} cloud
  computing.
\newblock In \emph{Annual Conference of the International Speech Communication
  Association (INTERSPEECH)}, 2015.

\bibitem[Su et~al.(2018)Su, Chen, and Xu]{Su18ArXiv}
Su, H., Chen, H., and Xu, H.
\newblock Experiments on parallel training of deep neural network using model
  averaging.
\newblock \emph{arXiv:1507.01239v3}, 2018.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{Sutskever13ICML}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning (ICML)}, 2013.

\bibitem[Tang et~al.(2018)Tang, Lian, Yan, Zhang, and Liu]{Tang18ArXiv}
Tang, H., Lian, X., Yan, M., Zhang, C., and Liu, J.
\newblock ${D}^{2}$: Decentralized training over decentralized data.
\newblock \emph{arXiv:1803.07068}, 2018.

\bibitem[Wang \& Joshi(2018{\natexlab{a}})Wang and Joshi]{WangJoshi18ArXiv}
Wang, J. and Joshi, G.
\newblock Cooperative {SGD}: A unified framework for the design and analysis of
  communication-efficient {SGD} algorithms.
\newblock \emph{arXiv:1808.07576}, 2018{\natexlab{a}}.

\bibitem[Wang \& Joshi(2018{\natexlab{b}})Wang and Joshi]{WangJoshi18ArXiv2}
Wang, J. and Joshi, G.
\newblock Adaptive communication strategies to achieve the best error-runtime
  trade-off in local-update {SGD}.
\newblock \emph{arXiv:1810.08313}, 2018{\natexlab{b}}.

\bibitem[Wen et~al.(2017)Wen, Xu, Yan, Wu, Wang, Chen, and Li]{Wen17NIPS}
Wen, W., Xu, C., Yan, F., Wu, C., Wang, Y., Chen, Y., and Li, H.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Yan et~al.(2018)Yan, Yang, Li, Lin, and Yang]{Yan18IJCAI}
Yan, Y., Yang, T., Li, Z., Lin, Q., and Yang, Y.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2018.

\bibitem[Yu et~al.(2018)Yu, Yang, and Zhu]{Yu18ArXivAAAI}
Yu, H., Yang, S., and Zhu, S.
\newblock Parallel restarted {SGD} with faster convergence and less
  communication: Demystifying why model averaging works for deep learning.
\newblock \emph{arXiv:1807.06629}, 2018.

\bibitem[Zavriev \& Kostyuk(1993)Zavriev and Kostyuk]{Zavriev93}
Zavriev, S. and Kostyuk, F.
\newblock Heavy-ball method in nonconvex optimization problems.
\newblock \emph{Computational Mathematics and Modeling}, 4\penalty0
  (4):\penalty0 336--341, 1993.

\bibitem[Zhou \& Cong(2018)Zhou and Cong]{Zhou18IJCAI}
Zhou, F. and Cong, G.
\newblock On the convergence properties of a ${K}$-step averaging stochastic
  gradient descent algorithm for nonconvex optimization.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2018.

\end{thebibliography}
