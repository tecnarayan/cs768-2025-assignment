\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adams(1975)]{adams1975sobolev}
Adams, R.
\newblock \emph{Sobolev Spaces. Adams}.
\newblock Pure and applied mathematics. Academic Press, 1975.
\newblock URL \url{https://books.google.co.uk/books?id=JxzpSAAACAAJ}.

\bibitem[Adams \& Fournier(2003)Adams and Fournier]{adams2003_SobolevSpaces}
Adams, R.~A. and Fournier, J.~J.
\newblock \emph{Sobolev Spaces}.
\newblock {Elsevier}, 2003.

\bibitem[Bauer et~al.(2007)Bauer, Pereverzyev, and
  Rosasco]{bauer2007_RegularizationAlgorithms}
Bauer, F., Pereverzyev, S., and Rosasco, L.
\newblock On regularization algorithms in learning theory.
\newblock \emph{Journal of complexity}, 23\penalty0 (1):\penalty0 52--72, 2007.

\bibitem[Blanchard \& M{\"u}cke(2018)Blanchard and
  M{\"u}cke]{blanchard2018_OptimalRates}
Blanchard, G. and M{\"u}cke, N.
\newblock Optimal rates for regularization of statistical inverse learning
  problems.
\newblock \emph{Foundations of Computational Mathematics}, 18:\penalty0
  971--1013, 2018.

\bibitem[Bordelon et~al.(2020)Bordelon, Canatar, and
  Pehlevan]{Bordelon2020SpectrumDL}
Bordelon, B., Canatar, A., and Pehlevan, C.
\newblock Spectrum dependent learning curves in kernel regression and wide
  neural networks.
\newblock In \emph{ICML}, 2020.

\bibitem[Caponnetto(2006)]{Caponnetto2006OptimalRF}
Caponnetto, A.
\newblock Optimal rates for regularization operators in learning theory.
\newblock 2006.

\bibitem[Caponnetto \& de~Vito(2007)Caponnetto and
  de~Vito]{Caponnetto2007OptimalRF}
Caponnetto, A. and de~Vito, E.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Foundations of Computational Mathematics}, 7:\penalty0
  331--368, 2007.

\bibitem[Celisse \& Wahl(2020)Celisse and Wahl]{Celisse2020AnalyzingTD}
Celisse, A. and Wahl, M.
\newblock Analyzing the discrepancy principle for kernelized spectral filter
  learning algorithms.
\newblock \emph{J. Mach. Learn. Res.}, 22:\penalty0 76:1--76:59, 2020.

\bibitem[Cucker \& Smale(2001)Cucker and Smale]{Cucker2001OnTM}
Cucker, F. and Smale, S.
\newblock On the mathematical foundations of learning.
\newblock \emph{Bulletin of the American Mathematical Society}, 39:\penalty0
  1--49, 2001.

\bibitem[Cui et~al.(2021)Cui, Loureiro, Krzakala, and
  Zdeborov'a]{Cui2021GeneralizationER}
Cui, H., Loureiro, B., Krzakala, F., and Zdeborov'a, L.
\newblock Generalization error rates in kernel regression: The crossover from
  the noiseless to noisy regime.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Dicker et~al.(2017)Dicker, Foster, and Hsu]{dicker2017_KernelRidge}
Dicker, L., Foster, D.~P., and Hsu, D.~J.
\newblock Kernel ridge vs. principal component regression: {{Minimax}} bounds
  and the qualification of regularization operators.
\newblock \emph{Electronic Journal of Statistics}, 11:\penalty0 1022--1047,
  2017.

\bibitem[Edmunds \& Triebel(1996)Edmunds and Triebel]{edmunds_triebel_1996}
Edmunds, D.~E. and Triebel, H.
\newblock \emph{Function Spaces, Entropy Numbers, Differential Operators}.
\newblock Cambridge Tracts in Mathematics. Cambridge University Press, 1996.
\newblock \doi{10.1017/CBO9780511662201}.

\bibitem[Fischer \& Steinwart(2020)Fischer and
  Steinwart]{fischer2020_SobolevNorm}
Fischer, S.-R. and Steinwart, I.
\newblock Sobolev norm learning rates for regularized least-squares algorithms.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0
  205:1--205:38, 2020.

\bibitem[Gerfo et~al.(2008)Gerfo, Rosasco, Odone, Vito, and
  Verri]{gerfo2008_SpectralAlgorithms}
Gerfo, L.~L., Rosasco, L., Odone, F., Vito, E.~D., and Verri, A.
\newblock Spectral algorithms for supervised learning.
\newblock \emph{Neural Computation}, 20\penalty0 (7):\penalty0 1873--1897,
  2008.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{Jacot_NTK_2018}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jin et~al.(2021)Jin, Banerjee, and Mont{\'u}far]{Jin2021LearningCF}
Jin, H., Banerjee, P.~K., and Mont{\'u}far, G.
\newblock Learning curves for gaussian process regression with power-law priors
  and targets.
\newblock \emph{ArXiv}, abs/2110.12231, 2021.

\bibitem[Jun et~al.(2019)Jun, Cutkosky, and Orabona]{Jun2019KernelTR}
Jun, K.-S., Cutkosky, A., and Orabona, F.
\newblock Kernel truncated randomized ridge regression: Optimal rates and low
  noise acceleration.
\newblock \emph{ArXiv}, abs/1905.10680, 2019.

\bibitem[Kanagawa et~al.(2018)Kanagawa, Hennig, Sejdinovic, and
  Sriperumbudur]{kanagawa2018_GaussianProcesses}
Kanagawa, M., Hennig, P., Sejdinovic, D., and Sriperumbudur, B.~K.
\newblock Gaussian processes and kernel methods: {{A}} review on connections
  and equivalences.
\newblock \emph{arXiv preprint arXiv:1807.02582}, 2018.

\bibitem[Kohler \& Krzyżak(2001)Kohler and
  Krzyżak]{Kohler2001NonparametricRE}
Kohler, M. and Krzyżak, A.
\newblock Nonparametric regression estimation using penalized least squares.
\newblock \emph{IEEE Trans. Inf. Theory}, 47:\penalty0 3054--3059, 2001.

\bibitem[Li et~al.(2023)Li, Zhang, and Lin]{li2023saturation}
Li, Y., Zhang, H., and Lin, Q.
\newblock On the saturation effect of kernel ridge regression.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Li et~al.(2022)Li, Meunier, Mollenhauer, and Gretton]{Li2022OptimalRF}
Li, Z., Meunier, D., Mollenhauer, M., and Gretton, A.
\newblock Optimal rates for regularized conditional mean embedding learning.
\newblock \emph{ArXiv}, abs/2208.01711, 2022.

\bibitem[Lin \& Cevher(2020)Lin and Cevher]{lin2020_OptimalConvergence}
Lin, J. and Cevher, V.
\newblock Optimal convergence for distributed learning with stochastic gradient
  methods and spectral algorithms.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 147--1,
  2020.

\bibitem[Lin et~al.(2018)Lin, Rudi, Rosasco, and Cevher]{lin2018_OptimalRates}
Lin, J., Rudi, A., Rosasco, L., and Cevher, V.
\newblock Optimal rates for spectral algorithms with least-squares regression
  over {{Hilbert}} spaces.
\newblock \emph{Applied and Computational Harmonic Analysis}, 48:\penalty0
  868--890, 2018.

\bibitem[Liu \& Shi(2022)Liu and Shi]{Liu2022StatisticalOO}
Liu, J. and Shi, L.
\newblock Statistical optimality of divide and conquer kernel-based functional
  linear regression.
\newblock \emph{ArXiv}, abs/2211.10968, 2022.

\bibitem[Mendelson \& Neeman(2010)Mendelson and
  Neeman]{mendelson2010_RegularizationKernel}
Mendelson, S. and Neeman, J.
\newblock Regularization in kernel learning.
\newblock \emph{The Annals of Statistics}, 38\penalty0 (1):\penalty0 526--565,
  February 2010.

\bibitem[Pillaud-Vivien et~al.(2018)Pillaud-Vivien, Rudi, and
  Bach]{PillaudVivien2018StatisticalOO}
Pillaud-Vivien, L., Rudi, A., and Bach, F.~R.
\newblock Statistical optimality of stochastic gradient descent on hard
  learning problems through multiple passes.
\newblock \emph{ArXiv}, abs/1805.10074, 2018.

\bibitem[Rastogi \& Sampath(2017)Rastogi and Sampath]{rastogi2017_OptimalRates}
Rastogi, A. and Sampath, S.
\newblock Optimal rates for the regularized learning algorithms under general
  source condition.
\newblock \emph{Frontiers in Applied Mathematics and Statistics}, 3, 2017.

\bibitem[Rosasco et~al.(2005)Rosasco, De~Vito, and
  Verri]{rosasco2005_SpectralMethods}
Rosasco, L., De~Vito, E., and Verri, A.
\newblock Spectral methods for regularization in learning theory.
\newblock \emph{DISI, Universita degli Studi di Genova, Italy, Technical Report
  DISI-TR-05-18}, 2005.

\bibitem[Sawano(2018)]{sawano2018theory}
Sawano, Y.
\newblock \emph{Theory of Besov spaces}, volume~56.
\newblock Springer, 2018.

\bibitem[Smale \& Zhou(2007)Smale and Zhou]{Smale2007LearningTE}
Smale, S. and Zhou, D.-X.
\newblock Learning theory estimates via integral operators and their
  approximations.
\newblock \emph{Constructive Approximation}, 26:\penalty0 153--172, 2007.

\bibitem[Spigler et~al.(2020)Spigler, Geiger, and
  Wyart]{Spigler2020AsymptoticLC}
Spigler, S., Geiger, M., and Wyart, M.
\newblock Asymptotic learning curves of kernel methods: empirical data versus
  teacher–student paradigm.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2020,
  2020.

\bibitem[Steinwart \& Christmann(2008)Steinwart and
  Christmann]{Steinwart2008SupportVM}
Steinwart, I. and Christmann, A.
\newblock Support vector machines.
\newblock In \emph{Information Science and Statistics}, 2008.

\bibitem[Steinwart \& Scovel(2012)Steinwart and
  Scovel]{steinwart2012_MercerTheorem}
Steinwart, I. and Scovel, C.
\newblock Mercer's theorem on general domains: {{On}} the interaction between
  measures, kernels, and {{RKHSs}}.
\newblock \emph{Constructive Approximation}, 35\penalty0 (3):\penalty0
  363--417, 2012.

\bibitem[Steinwart et~al.(2009)Steinwart, Hush, and
  Scovel]{steinwart2009_OptimalRates}
Steinwart, I., Hush, D., and Scovel, C.
\newblock Optimal rates for regularized least squares regression.
\newblock In \emph{{{COLT}}}, pp.\  79--93, 2009.

\bibitem[Talwai \& Simchi-Levi(2022)Talwai and
  Simchi-Levi]{Talwai2022OptimalLR}
Talwai, P.~M. and Simchi-Levi, D.
\newblock Optimal learning rates for regularized least-squares with a fourier
  capacity condition.
\newblock 2022.

\bibitem[Thomas-Agnan(1996)]{ThomasAgnan1996ComputingAF}
Thomas-Agnan, C.
\newblock Computing a family of reproducing kernels for statistical
  applications.
\newblock \emph{Numerical Algorithms}, 13:\penalty0 21--32, 1996.

\bibitem[Tsybakov(2009)]{tsybakov2009_IntroductionNonparametric}
Tsybakov, A.~B.
\newblock \emph{Introduction to Nonparametric Estimation}.
\newblock Springer Series in Statistics. {Springer}, {New York ; London}, 1st
  edition, 2009.

\bibitem[Wainwright(2019)]{wainwright2019_HighdimensionalStatistics}
Wainwright, M.~J.
\newblock \emph{High-Dimensional Statistics: {{A}} Non-Asymptotic Viewpoint}.
\newblock Cambridge {{Series}} in {{Statistical}} and {{Probabilistic
  Mathematics}}. {Cambridge University Press}, 2019.

\end{thebibliography}
