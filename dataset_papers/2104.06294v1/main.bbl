\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Schuurmans, and
  Norouzi]{agarwal2020optimistic}
Agarwal, R., Schuurmans, D., and Norouzi, M.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  104--114. PMLR, 2020.

\bibitem[Akkaya et~al.(2019)Akkaya, Andrychowicz, Chociej, Litwin, McGrew,
  Petron, Paino, Plappert, Powell, Ribas, et~al.]{akkaya2019solving}
Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A.,
  Paino, A., Plappert, M., Powell, G., Ribas, R., et~al.
\newblock Solving rubik's cube with a robot hand.
\newblock \emph{arXiv preprint arXiv:1910.07113}, 2019.

\bibitem[Argenson \& Dulac-Arnold(2020)Argenson and
  Dulac-Arnold]{argenson2020modelbased}
Argenson, A. and Dulac-Arnold, G.
\newblock Model-based offline planning, 2020.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization, 2016.

\bibitem[Barth-Maron et~al.(2018)Barth-Maron, Hoffman, Budden, Dabney, Horgan,
  TB, Muldal, Heess, and Lillicrap]{d4pg}
Barth-Maron, G., Hoffman, M.~W., Budden, D., Dabney, W., Horgan, D., TB, D.,
  Muldal, A., Heess, N., and Lillicrap, T.
\newblock {D}istributed {D}istributional {D}eterministic {P}olicy {G}radients,
  2018.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and Bowling]{ALE}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The {A}rcade {L}earning {E}nvironment: An evaluation platform for
  general agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Dabney et~al.(2018)Dabney, Rowland, Bellemare, and Munos]{qr-dqn}
Dabney, W., Rowland, M., Bellemare, M.~G., and Munos, R.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{AAAI}, 2018.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, et~al.]{impala}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., et~al.
\newblock {IMPALA}: Scalable distributed deep-{RL} with importance weighted
  actor-learner architectures.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2018.

\bibitem[Gulcehre et~al.(2020)Gulcehre, Wang, Novikov, Paine, Colmenarejo,
  Zolna, Agarwal, Merel, Mankowitz, Paduraru, Dulac-Arnold, Li, Norouzi,
  Hoffman, Nachum, Tucker, Heess, and de~Freitas]{rl_unplugged}
Gulcehre, C., Wang, Z., Novikov, A., Paine, T.~L., Colmenarejo, S.~G., Zolna,
  K., Agarwal, R., Merel, J., Mankowitz, D., Paduraru, C., Dulac-Arnold, G.,
  Li, J., Norouzi, M., Hoffman, M., Nachum, O., Tucker, G., Heess, N., and
  de~Freitas, N.
\newblock {RL} {U}nplugged: {B}enchmarks for {O}ffline {R}einforcement
  {L}earning.
\newblock 2020.
\newblock URL \url{https://arxiv.org/pdf/2006.13888}.

\bibitem[Hafner et~al.(2018)Hafner, Lillicrap, Fischer, Villegas, Ha, Lee, and
  Davidson]{hafner:planet}
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and
  Davidson, J.
\newblock Learning latent dynamics for planning from pixels.
\newblock \emph{arXiv preprint arXiv:1811.04551}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resv2}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock \emph{CoRR}, abs/1603.05027, 2016.
\newblock URL \url{http://arxiv.org/abs/1603.05027}.

\bibitem[He \& Hou(2021)He and Hou]{he2021popo}
He, Q. and Hou, X.
\newblock {POPO}: {P}essimistic {O}ffline {P}olicy {O}ptimization, 2021.

\bibitem[Hennigan et~al.(2020)Hennigan, Cai, Norman, and
  Babuschkin]{haiku2020github}
Hennigan, T., Cai, T., Norman, T., and Babuschkin, I.
\newblock {H}aiku: {S}onnet for {JAX}, 2020.
\newblock URL \url{http://github.com/deepmind/dm-haiku}.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{rainbow}
Hessel, M., Modayil, J., Van~Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
  W., Horgan, D., Piot, B., Azar, M., and Silver, D.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, Krizhevsky, Sutskever, and
  Salakhutdinov]{hinton2012dropout}
Hinton, G.~E., Srivastava, N., Krizhevsky, A., Sutskever, I., and
  Salakhutdinov, R.~R.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock \emph{arXiv preprint arXiv:1207.0580}, 2012.

\bibitem[Hubert et~al.(2021)Hubert, Schrittwieser, Antonoglou, Barekatain,
  Schmitt, and Silver]{muzero_sampled}
Hubert, T., Schrittwieser, J., Antonoglou, I., Barekatain, M., Schmitt, S., and
  Silver, D.
\newblock {L}earning and {P}lanning in {C}omplex {A}ction {S}paces.
\newblock \emph{arXiv e-prints}, April 2021.

\bibitem[Jaderberg et~al.(2016)Jaderberg, Mnih, Czarnecki, Schaul, Leibo,
  Silver, and Kavukcuoglu]{unreal}
Jaderberg, M., Mnih, V., Czarnecki, W.~M., Schaul, T., Leibo, J.~Z., Silver,
  D., and Kavukcuoglu, K.
\newblock Reinforcement learning with unsupervised auxiliary tasks.
\newblock \emph{arXiv preprint arXiv:1611.05397}, 2016.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models, 2020.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T.
\newblock {MOReL} : {M}odel-{B}ased {O}ffline {R}einforcement {L}earning, 2020.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{CoRR}, abs/1412.6980, 2015.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock {C}onservative {Q}-learning for {O}ffline {R}einforcement {L}earning,
  2020.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems, 2020.

\bibitem[Lin(1992)]{replay}
Lin, L.-J.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 293--321, 1992.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{adam_weight_decay}
Loshchilov, I. and Hutter, F.
\newblock Fixing weight decay regularization in adam.
\newblock \emph{CoRR}, abs/1711.05101, 2017.
\newblock URL \url{http://arxiv.org/abs/1711.05101}.

\bibitem[Machado et~al.(2017)Machado, Bellemare, Talvitie, Veness, Hausknecht,
  and Bowling]{Marlos2017Atari}
Machado, M., Bellemare, M., Talvitie, E., Veness, J., Hausknecht, M., and
  Bowling, M.
\newblock Revisiting the {A}rcade {L}earning {E}nvironment: Evaluation
  protocols and open problems for general agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 61, 09 2017.
\newblock \doi{10.1613/jair.5699}.

\bibitem[Matsushima et~al.(2020)Matsushima, Furuta, Matsuo, Nachum, and
  Gu]{matsushima2020deploymentefficient}
Matsushima, T., Furuta, H., Matsuo, Y., Nachum, O., and Gu, S.
\newblock Deployment-efficient reinforcement learning via model-based offline
  optimization, 2020.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{dqn}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Rafailov et~al.(2020)Rafailov, Yu, Rajeswaran, and
  Finn]{rafailov2020offline}
Rafailov, R., Yu, T., Rajeswaran, A., and Finn, C.
\newblock Offline reinforcement learning from images with latent space models.
\newblock \emph{arXiv preprint arXiv:2012.11547}, 2020.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and Silver]{Schaul2016}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations},
  Puerto Rico, 2016.

\bibitem[Schmitt et~al.(2019)Schmitt, Hessel, and Simonyan]{laser}
Schmitt, S., Hessel, M., and Simonyan, K.
\newblock Off-policy actor-critic with shared experience replay.
\newblock \emph{arXiv preprint arXiv:1909.11583}, 2019.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan,
  Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel, Lillicrap, and
  Silver]{muzero}
Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L.,
  Schmitt, S., Guez, A., Lockhart, E., Hassabis, D., Graepel, T., Lillicrap,
  T.~P., and Silver, D.
\newblock Mastering {A}tari, {G}o, {C}hess and {S}hogi by {P}lanning with a
  {L}earned {M}odel.
\newblock \emph{Nature}, 588\penalty0 (7839):\penalty0 604--609, 2020.

\bibitem[Siegel et~al.(2020)Siegel, Springenberg, Berkenkamp, Abdolmaleki,
  Neunert, Lampe, Hafner, and Riedmiller]{siegel2020keep}
Siegel, N.~Y., Springenberg, J.~T., Berkenkamp, F., Abdolmaleki, A., Neunert,
  M., Lampe, T., Hafner, R., and Riedmiller, M.
\newblock Keep doing what worked: Behavioral modelling priors for offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.08396}, 2020.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, van~den
  Driessche, Graepel, and Hassabis]{Silver17AG0}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, T., Hui,
  F., Sifre, L., van~den Driessche, G., Graepel, T., and Hassabis, D.
\newblock Mastering the game of {Go} without human knowledge.
\newblock \emph{Nature}, 550:\penalty0 354--359, October 2017.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{Silver18AZ}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and {Go} through self-play.
\newblock \emph{Science}, 362\penalty0 (6419):\penalty0 1140--1144, 2018.

\bibitem[Tassa et~al.(2018)Tassa, Doron, Muldal, Erez, Li, de~Las~Casas,
  Budden, Abdolmaleki, Merel, Lefrancq, Lillicrap, and
  Riedmiller]{tassa2018deepmind}
Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., de~Las~Casas, D., Budden,
  D., Abdolmaleki, A., Merel, J., Lefrancq, A., Lillicrap, T., and Riedmiller,
  M.
\newblock Deepmind control suite, 2018.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{alphastar}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in {StarCraft II} using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, pp.\  1--5, 2019.

\bibitem[Wang et~al.(2020)Wang, Novikov, Zolna, Merel, Springenberg, Reed,
  Shahriari, Siegel, Gulcehre, Heess, et~al.]{wang2020critic}
Wang, Z., Novikov, A., Zolna, K., Merel, J.~S., Springenberg, J.~T., Reed,
  S.~E., Shahriari, B., Siegel, N., Gulcehre, C., Heess, N., et~al.
\newblock {C}ritic {R}egularized {R}egression.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019brac}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine, S., Finn, C., and Ma,
  T.
\newblock {MOPO}: {M}odel-based {O}ffline {P}olicy {O}ptimization, 2020.

\end{thebibliography}
