\begin{thebibliography}{101}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andriushchenko and Flammarion(2020)]{andriushchenko2020understanding}
Maksym Andriushchenko and Nicolas Flammarion.
\newblock Understanding and improving fast adversarial training.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Andriushchenko et~al.(2020)Andriushchenko, Croce, Flammarion, and
  Hein]{andriushchenko2020square}
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein.
\newblock Square attack: a query-efficient black-box adversarial attack via
  random search.
\newblock In \emph{European Conference on Computer Vision}, pages 484--501.
  Springer, 2020.

\bibitem[Archambault et~al.(2019)Archambault, Mao, Guo, and
  Zhang]{archambault2019mixup}
Guillaume~P Archambault, Yongyi Mao, Hongyu Guo, and Richong Zhang.
\newblock Mixup as directional adversarial training.
\newblock \emph{arXiv preprint arXiv:1906.06875}, 2019.

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and
  Wagner]{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In \emph{International Conference on Machine Learning}, pages
  274--283. PMLR, 2018.

\bibitem[Awais et~al.(2020)Awais, Shamshad, and Bae]{awais2020towards}
Muhammad Awais, Fahad Shamshad, and Sung-Ho Bae.
\newblock Towards an adversarially robust normalization approach.
\newblock \emph{arXiv preprint arXiv:2006.11007}, 2020.

\bibitem[Awais et~al.(2021)Awais, Zhou, Xu, Hong, Luo, Bae, and
  Li]{awais2021adversarial}
Muhammad Awais, Fengwei Zhou, Hang Xu, Lanqing Hong, Ping Luo, Sung-Ho Bae, and
  Zhenguo Li.
\newblock Adversarial robustness for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 8568--8577, 2021.

\bibitem[Bai et~al.(2021)Bai, Zeng, Jiang, Xia, Ma, and Wang]{bai2021improving}
Yang Bai, Yuyuan Zeng, Yong Jiang, Shu-Tao Xia, Xingjun Ma, and Yisen Wang.
\newblock Improving adversarial robustness via channel-wise activation
  suppressing.
\newblock \emph{arXiv preprint arXiv:2103.08307}, 2021.

\bibitem[Bau et~al.(2020)Bau, Zhu, Strobelt, Lapedriza, Zhou, and
  Torralba]{bau2020units}
David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and
  Antonio Torralba.
\newblock Understanding the role of individual units in a deep neural network.
\newblock \emph{Proceedings of the National Academy of Sciences}, 2020.
\newblock ISSN 0027-8424.
\newblock \doi{10.1073/pnas.1907375117}.
\newblock URL \url{https://www.pnas.org/content/early/2020/08/31/1907375117}.

\bibitem[Biggio et~al.(2013)Biggio, Corona, Maiorca, Nelson, {\v{S}}rndi{\'c},
  Laskov, Giacinto, and Roli]{biggio2013evasion}
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim
  {\v{S}}rndi{\'c}, Pavel Laskov, Giorgio Giacinto, and Fabio Roli.
\newblock Evasion attacks against machine learning at test time.
\newblock In \emph{Joint European conference on machine learning and knowledge
  discovery in databases}, pages 387--402. Springer, 2013.

\bibitem[Brown et~al.(2017)Brown, Man{\'e}, Roy, Abadi, and
  Gilmer]{brown2017adversarial}
Tom~B Brown, Dandelion Man{\'e}, Aurko Roy, Mart{\'\i}n Abadi, and Justin
  Gilmer.
\newblock Adversarial patch.
\newblock \emph{arXiv preprint arXiv:1712.09665}, 2017.

\bibitem[Buciluǎ et~al.(2006)Buciluǎ, Caruana, and
  Niculescu-Mizil]{bucilu2006model}
Cristian Buciluǎ, Rich Caruana, and Alexandru Niculescu-Mizil.
\newblock Model compression.
\newblock In \emph{Proceedings of the 12th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 535--541, 2006.

\bibitem[Buckman et~al.(2018)Buckman, Roy, Raffel, and
  Goodfellow]{buckman2018thermometer}
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow.
\newblock Thermometer encoding: One hot way to resist adversarial examples.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Bunk et~al.(2021)Bunk, Chattopadhyay, Manjunath, and
  Chandrasekaran]{bunk2021adversarially}
Jason Bunk, Srinjoy Chattopadhyay, BS~Manjunath, and Shivkumar Chandrasekaran.
\newblock Adversarially optimized mixup for robust classification.
\newblock \emph{arXiv preprint arXiv:2103.11589}, 2021.

\bibitem[Carlini and Wagner(2017)]{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{2017 ieee symposium on security and privacy (sp)}, pages
  39--57. IEEE, 2017.

\bibitem[Carmon et~al.(2019)Carmon, Raghunathan, Schmidt, Liang, and
  Duchi]{carmon2019unlabeled}
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John~C Duchi.
\newblock Unlabeled data improves adversarial robustness.
\newblock \emph{arXiv preprint arXiv:1905.13736}, 2019.

\bibitem[Chan et~al.(2020)Chan, Tay, and Ong]{chan2020thinks}
Alvin Chan, Yi~Tay, and Yew-Soon Ong.
\newblock What it thinks is important is important: Robustness transfers
  through input gradients.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 332--341, 2020.

\bibitem[Chen et~al.(2021)Chen, Zhang, Xu, Hu, Niu, Chen, and
  Sugiyama]{chen2021guided}
Chen Chen, Jingfeng Zhang, Xilie Xu, Tianlei Hu, Gang Niu, Gang Chen, and
  Masashi Sugiyama.
\newblock Guided interpolation for adversarial training.
\newblock \emph{arXiv e-prints}, pages arXiv--2102, 2021.

\bibitem[Croce and Hein(2020{\natexlab{a}})]{croce2020minimally}
Francesco Croce and Matthias Hein.
\newblock Minimally distorted adversarial examples with a fast adaptive
  boundary attack.
\newblock In \emph{International Conference on Machine Learning}, pages
  2196--2205. PMLR, 2020{\natexlab{a}}.

\bibitem[Croce and Hein(2020{\natexlab{b}})]{croce2020reliable}
Francesco Croce and Matthias Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In \emph{ICML}, 2020{\natexlab{b}}.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops}, pages 702--703, 2020.

\bibitem[Das et~al.(2018)Das, Shanbhogue, Chen, Hohman, Li, Chen, Kounavis, and
  Chau]{das2018compression}
Nilaksh Das, Madhuri Shanbhogue, Shang-Tse Chen, Fred Hohman, Siwei Li,
  Li~Chen, Michael~E Kounavis, and Duen~Horng Chau.
\newblock Compression to the rescue: Defending from adversarial attacks across
  modalities.
\newblock In \emph{ACM SIGKDD Conference on Knowledge Discovery and Data
  Mining}, 2018.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[Dhillon et~al.(2018)Dhillon, Azizzadenesheli, Lipton, Bernstein,
  Kossaifi, Khanna, and Anandkumar]{dhillon2018stochastic}
Guneet~S Dhillon, Kamyar Azizzadenesheli, Zachary~C Lipton, Jeremy~D Bernstein,
  Jean Kossaifi, Aran Khanna, and Animashree Anandkumar.
\newblock Stochastic activation pruning for robust adversarial defense.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Engstrom et~al.(2019)Engstrom, Ilyas, Santurkar, Tsipras, Tran, and
  Madry]{engstrom2019adversarial}
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon
  Tran, and Aleksander Madry.
\newblock Adversarial robustness as a prior for learned representations.
\newblock \emph{arXiv preprint arXiv:1906.00945}, 2019.

\bibitem[Gao and Pavel(2017)]{gao2017properties}
Bolin Gao and Lacra Pavel.
\newblock On the properties of the softmax function with application in game
  theory and reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1704.00805}, 2017.

\bibitem[Goldblum et~al.(2020{\natexlab{a}})Goldblum, Fowl, Feizi, and
  Goldstein]{goldblum2019adversarially}
Micah Goldblum, Liam Fowl, Soheil Feizi, and Tom Goldstein.
\newblock Adversarially robust distillation.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  2020{\natexlab{a}}.
\newblock \doi{10.1609/aaai.v34i04.5816}.

\bibitem[Goldblum et~al.(2020{\natexlab{b}})Goldblum, Fowl, Feizi, and
  Goldstein]{goldblum2020adversarially}
Micah Goldblum, Liam Fowl, Soheil Feizi, and Tom Goldstein.
\newblock Adversarially robust distillation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 3996--4003, 2020{\natexlab{b}}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Gowal et~al.(2020)Gowal, Qin, Uesato, Mann, and
  Kohli]{gowal2020uncovering}
Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli.
\newblock Uncovering the limits of adversarial training against norm-bounded
  adversarial examples.
\newblock \emph{arXiv preprint arXiv:2010.03593}, 2020.
\newblock URL \url{https://arxiv.org/pdf/2010.03593}.

\bibitem[Gu and Rigazio(2014)]{gu2014towards}
Shixiang Gu and Luca Rigazio.
\newblock Towards deep neural network architectures robust to adversarial
  examples.
\newblock \emph{arXiv preprint arXiv:1412.5068}, 2014.

\bibitem[Guo et~al.(2017)Guo, Rana, Cisse, and Van
  Der~Maaten]{guo2017countering}
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der~Maaten.
\newblock Countering adversarial images using input transformations.
\newblock \emph{arXiv preprint arXiv:1711.00117}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Lee, and
  Mazeika]{hendrycks2019pretraining}
Dan Hendrycks, Kimin Lee, and Mantas Mazeika.
\newblock Using pre-training can improve model robustness and uncertainty.
\newblock \emph{Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[Heo et~al.(2019{\natexlab{a}})Heo, Kim, Yun, Park, Kwak, and
  Choi]{heo2019comprehensive}
Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, Nojun Kwak, and Jin~Young
  Choi.
\newblock A comprehensive overhaul of feature distillation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1921--1930, 2019{\natexlab{a}}.

\bibitem[Heo et~al.(2019{\natexlab{b}})Heo, Lee, Yun, and
  Choi]{heo2019knowledge}
Byeongho Heo, Minsik Lee, Sangdoo Yun, and Jin~Young Choi.
\newblock Knowledge transfer via distillation of activation boundaries formed
  by hidden neurons.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 3779--3787, 2019{\natexlab{b}}.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hsu et~al.(2021)Hsu, Ji, Telgarsky, and Wang]{hsu2021generalization}
Daniel Hsu, Ziwei Ji, Matus Telgarsky, and Lan Wang.
\newblock Generalization bounds via distillation.
\newblock \emph{arXiv preprint arXiv:2104.05641}, 2021.

\bibitem[Huang and Wang(2017)]{huang2017like}
Zehao Huang and Naiyan Wang.
\newblock Like what you like: Knowledge distill via neuron selectivity
  transfer.
\newblock \emph{arXiv preprint arXiv:1707.01219}, 2017.

\bibitem[Ilyas et~al.(2019)Ilyas, Santurkar, Engstrom, Tran, and
  Madry]{ilyas2019adversarial}
Andrew Ilyas, Shibani Santurkar, Logan Engstrom, Brandon Tran, and Aleksander
  Madry.
\newblock Adversarial examples are not bugs, they are features.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Kim et~al.(2020)Kim, Tack, and Hwang]{kim2020adversarial}
Minseon Kim, Jihoon Tack, and Sung~Ju Hwang.
\newblock Adversarial self-supervised contrastive learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Koratana et~al.(2019)Koratana, Kang, Bailis, and
  Zaharia]{koratana2019lit}
Animesh Koratana, Daniel Kang, Peter Bailis, and Matei Zaharia.
\newblock Lit: Learned intermediate representation training for model
  compression.
\newblock In \emph{International Conference on Machine Learning}, pages
  3509--3518. PMLR, 2019.

\bibitem[Kurakin et~al.(2016)Kurakin, Goodfellow, Bengio,
  et~al.]{kurakin2016adversarial}
Alexey Kurakin, Ian Goodfellow, Samy Bengio, et~al.
\newblock Adversarial examples in the physical world, 2016.

\bibitem[Lee et~al.(2020)Lee, Lee, and Yoon]{lee2020adversarial}
Saehyung Lee, Hyungyu Lee, and Sungroh Yoon.
\newblock Adversarial vertex mixup: Toward better adversarially robust
  generalization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 272--281, 2020.

\bibitem[Liao et~al.(2018)Liao, Liang, Dong, Pang, Hu, and
  Zhu]{liao2018defense}
Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu.
\newblock Defense against adversarial attacks using high-level representation
  guided denoiser.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1778--1787, 2018.

\bibitem[Lin et~al.(2020)Lin, Pong~Lau, Levine, Chellappa, and
  Feizi]{lin2020dual}
Wei-An Lin, Chun Pong~Lau, Alexander Levine, Rama Chellappa, and Soheil Feizi.
\newblock Dual manifold adversarial robustness: Defense against lp and non-lp
  adversarial attacks.
\newblock \emph{Advances in Neural Information Processing Systems Foundation
  (NeurIPS)}, 2020.

\bibitem[Liu et~al.(2018)Liu, Liu, Liu, Wang, Jin, and Wen]{liu2018security}
Qi~Liu, Tao Liu, Zihao Liu, Yanzhi Wang, Yier Jin, and Wujie Wen.
\newblock Security analysis and enhancement of model compressed deep learning
  systems under adversarial attacks.
\newblock In \emph{2018 23rd Asia and South Pacific Design Automation
  Conference (ASP-DAC)}, pages 721--726. IEEE, 2018.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov10sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{International Conference on Learning Representations},
  10:\penalty0 3, 2017.

\bibitem[Ma et~al.(2021)Ma, Niu, Gu, Wang, Zhao, Bailey, and
  Lu]{ma2021understanding}
Xingjun Ma, Yuhao Niu, Lin Gu, Yisen Wang, Yitian Zhao, James Bailey, and Feng
  Lu.
\newblock Understanding adversarial attacks on deep learning based medical
  image analysis systems.
\newblock \emph{Pattern Recognition}, 110:\penalty0 107332, 2021.

\bibitem[Madaan et~al.(2020)Madaan, Shin, and Hwang]{madaan2019adversarial}
Divyam Madaan, Jinwoo Shin, and Sung~Ju Hwang.
\newblock Adversarial neural pruning with latent vulnerability suppression.
\newblock In \emph{International Conference on Machine Learning}, pages
  6575--6585. PMLR, 2020.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Mao et~al.(2019)Mao, Zhong, Yang, Vondrick, and Ray]{mao2019metric}
Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, and Baishakhi Ray.
\newblock Metric learning for adversarial robustness.
\newblock \emph{arXiv preprint arXiv:1909.00900}, 2019.

\bibitem[Mustafa et~al.(2020)Mustafa, Khan, Hayat, Goecke, Shen, and
  Shao]{mustafa2020deeply}
Aamir Mustafa, Salman~H Khan, Munawar Hayat, Roland Goecke, Jianbing Shen, and
  Ling Shao.
\newblock Deeply supervised discriminative learning for adversarial defense.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 2020.

\bibitem[Najafi et~al.(2019)Najafi, Maeda, Koyama, and
  Miyato]{najafi2019robustness}
Amir Najafi, Shin-ichi Maeda, Masanori Koyama, and Takeru Miyato.
\newblock Robustness to adversarial perturbations in learning from incomplete
  data.
\newblock \emph{arXiv preprint arXiv:1905.13021}, 2019.

\bibitem[Nakkiran(2019)]{nakkiran2019adversarial}
Preetum Nakkiran.
\newblock Adversarial robustness may be at odds with simplicity.
\newblock \emph{arXiv preprint arXiv:1901.00532}, 2019.

\bibitem[Pang et~al.(2019)Pang, Xu, and Zhu]{pang2019mixup}
Tianyu Pang, Kun Xu, and Jun Zhu.
\newblock Mixup inference: Better exploiting mixup to defend adversarial
  attacks.
\newblock \emph{arXiv preprint arXiv:1909.11515}, 2019.

\bibitem[Pang et~al.(2020{\natexlab{a}})Pang, Yang, Dong, Su, and
  Zhu]{pang2020bag}
Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu.
\newblock Bag of tricks for adversarial training.
\newblock \emph{arXiv preprint arXiv:2010.00467}, 2020{\natexlab{a}}.

\bibitem[Pang et~al.(2020{\natexlab{b}})Pang, Yang, Dong, Xu, Zhu, and
  Su]{pang2020boosting}
Tianyu Pang, Xiao Yang, Yinpeng Dong, Kun Xu, Jun Zhu, and Hang Su.
\newblock Boosting adversarial training with hypersphere embedding.
\newblock \emph{arXiv preprint arXiv:2002.08619}, 2020{\natexlab{b}}.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Wu, Jha, and
  Swami]{papernot2016distillation}
Nicolas Papernot, Patrick McDaniel, Xi~Wu, Somesh Jha, and Ananthram Swami.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In \emph{2016 IEEE symposium on security and privacy (SP)}, pages
  582--597. IEEE, 2016.

\bibitem[Papernot et~al.(2017)Papernot, McDaniel, Goodfellow, Jha, Celik, and
  Swami]{papernot2017practical}
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z~Berkay Celik,
  and Ananthram Swami.
\newblock Practical black-box attacks against machine learning.
\newblock In \emph{Proceedings of the 2017 ACM on Asia conference on computer
  and communications security}, pages 506--519, 2017.

\bibitem[Rakin et~al.(2018)Rakin, Yi, Gong, and Fan]{rakin2018defend}
Adnan~Siraj Rakin, Jinfeng Yi, Boqing Gong, and Deliang Fan.
\newblock Defend deep neural networks against adversarial examples via fixed
  and dynamic quantized activation functions.
\newblock \emph{arXiv preprint arXiv:1807.06714}, 2018.

\bibitem[Rice et~al.(2020)Rice, Wong, and Kolter]{rice2020overfitting}
Leslie Rice, Eric Wong, and Zico Kolter.
\newblock Overfitting in adversarially robust deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  8093--8104. PMLR, 2020.

\bibitem[Romero et~al.(2014)Romero, Ballas, Kahou, Chassang, Gatta, and
  Bengio]{romero2014fitnets}
Adriana Romero, Nicolas Ballas, Samira~Ebrahimi Kahou, Antoine Chassang, Carlo
  Gatta, and Yoshua Bengio.
\newblock Fitnets: Hints for thin deep nets.
\newblock \emph{arXiv preprint arXiv:1412.6550}, 2014.

\bibitem[Ross and Doshi-Velez(2018)]{ross2018improving}
Andrew Ross and Finale Doshi-Velez.
\newblock Improving the adversarial robustness and interpretability of deep
  neural networks by regularizing their input gradients.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2018.

\bibitem[Salman et~al.(2020)Salman, Ilyas, Engstrom, Kapoor, and
  Madry]{salman2020adversarially}
Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry.
\newblock Do adversarially robust imagenet models transfer better?
\newblock \emph{arXiv preprint arXiv:2007.08489}, 2020.

\bibitem[Schmidt et~al.(2018)Schmidt, Santurkar, Tsipras, Talwar, and
  M{\k{a}}dry]{schmidt2018adversarially}
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and
  Aleksander M{\k{a}}dry.
\newblock Adversarially robust generalization requires more data.
\newblock \emph{arXiv preprint arXiv:1804.11285}, 2018.

\bibitem[Sehwag et~al.(2020)Sehwag, Wang, Mittal, and Jana]{sehwag2020hydra}
Vikash Sehwag, Shiqi Wang, Prateek Mittal, and Suman Jana.
\newblock Hydra: Pruning adversarially robust neural networks.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  7, 2020.

\bibitem[Shafahi et~al.(2019{\natexlab{a}})Shafahi, Najibi, Ghiasi, Xu,
  Dickerson, Studer, Davis, Taylor, and Goldstein]{shafahi2019adversarial}
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph
  Studer, Larry~S Davis, Gavin Taylor, and Tom Goldstein.
\newblock Adversarial training for free!
\newblock \emph{arXiv preprint arXiv:1904.12843}, 2019{\natexlab{a}}.

\bibitem[Shafahi et~al.(2019{\natexlab{b}})Shafahi, Saadatpanah, Zhu, Ghiasi,
  Studer, Jacobs, and Goldstein]{shafahi2019adversarially}
Ali Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer, David
  Jacobs, and Tom Goldstein.
\newblock Adversarially robust transfer learning.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Shen et~al.(2019)Shen, He, and Xue]{shen2019meal}
Zhiqiang Shen, Zhankui He, and Xiangyang Xue.
\newblock Meal: Multi-model ensemble via adversarial learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 4886--4893, 2019.

\bibitem[Singh et~al.(2019)Singh, Sinha, Kumari, Machiraju, Krishnamurthy, and
  Balasubramanian]{singh2019harnessing}
Mayank Singh, Abhishek Sinha, Nupur Kumari, Harshitha Machiraju, Balaji
  Krishnamurthy, and Vineeth~N Balasubramanian.
\newblock Harnessing the vulnerability of latent layers in adversarially
  trained models.
\newblock \emph{arXiv preprint arXiv:1905.05186}, 2019.

\bibitem[Srinivas and Fleuret(2018)]{srinivas2018knowledge}
Suraj Srinivas and Fran{\c{c}}ois Fleuret.
\newblock Knowledge transfer with jacobian matching.
\newblock In \emph{International Conference on Machine Learning}, pages
  4723--4731. PMLR, 2018.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Tian et~al.(2019)Tian, Krishnan, and Isola]{tian2019contrastive}
Yonglong Tian, Dilip Krishnan, and Phillip Isola.
\newblock Contrastive representation distillation.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Tram{\`e}r et~al.(2017)Tram{\`e}r, Kurakin, Papernot, Goodfellow,
  Boneh, and McDaniel]{tramer2017ensemble}
Florian Tram{\`e}r, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan
  Boneh, and Patrick McDaniel.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock \emph{arXiv preprint arXiv:1705.07204}, 2017.

\bibitem[Tsipras et~al.(2018)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock \emph{arXiv preprint arXiv:1805.12152}, 2018.

\bibitem[Tung and Mori(2019)]{tung2019similarity}
Frederick Tung and Greg Mori.
\newblock Similarity-preserving knowledge distillation.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1365--1374, 2019.

\bibitem[Uesato et~al.(2019)Uesato, Alayrac, Huang, Stanforth, Fawzi, and
  Kohli]{uesato2019labels}
Jonathan Uesato, Jean-Baptiste Alayrac, Po-Sen Huang, Robert Stanforth,
  Alhussein Fawzi, and Pushmeet Kohli.
\newblock Are labels required for improving adversarial robustness?
\newblock \emph{arXiv preprint arXiv:1905.13725}, 2019.

\bibitem[Wang et~al.(2018)Wang, Lin, Zhu, Yin, Bertozzi, and
  Osher]{wang2018adversarial}
Bao Wang, Alex~T Lin, Wei Zhu, Penghang Yin, Andrea~L Bertozzi, and Stanley~J
  Osher.
\newblock Adversarial defense via data dependent activation function and total
  variation minimization.
\newblock \emph{arXiv preprint arXiv:1809.08516}, 2018.

\bibitem[Wang et~al.(2019)Wang, Zou, Yi, Bailey, Ma, and Gu]{wang2019improving}
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu.
\newblock Improving adversarial robustness requires revisiting misclassified
  examples.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Wong et~al.(2020)Wong, Rice, and Kolter]{wong2020fast}
Eric Wong, Leslie Rice, and J~Zico Kolter.
\newblock Fast is better than free: Revisiting adversarial training.
\newblock \emph{arXiv preprint arXiv:2001.03994}, 2020.

\bibitem[Wu et~al.(2020{\natexlab{a}})Wu, Chen, Cai, He, and Gu]{wu2020does}
Boxi Wu, Jinghui Chen, Deng Cai, Xiaofei He, and Quanquan Gu.
\newblock Does network width really help adversarial robustness?
\newblock \emph{arXiv preprint arXiv:2010.01279}, 2020{\natexlab{a}}.

\bibitem[Wu et~al.(2020{\natexlab{b}})Wu, Xia, and Wang]{wu2020adversarial}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\bibitem[Xiao et~al.(2019)Xiao, Zhong, and Zheng]{xiao2019resisting}
Chang Xiao, Peilin Zhong, and Changxi Zheng.
\newblock Resisting adversarial attacks by k-winners-take-all.
\newblock \emph{arXiv preprint arXiv:1905.10510}, 1\penalty0 (2), 2019.

\bibitem[Xie and Yuille(2020)]{Xie2020intriguing}
Cihang Xie and Alan Yuille.
\newblock Intriguing properties of adversarial training at scale.
\newblock In \emph{ICLR}, 2020.

\bibitem[Xie et~al.(2019)Xie, Wu, Maaten, Yuille, and He]{xie2019feature}
Cihang Xie, Yuxin Wu, Laurens van~der Maaten, Alan~L Yuille, and Kaiming He.
\newblock Feature denoising for improving adversarial robustness.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 501--509, 2019.

\bibitem[Xie et~al.(2020)Xie, Tan, Gong, Yuille, and Le]{xie2020smooth}
Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, and Quoc~V Le.
\newblock Smooth adversarial training.
\newblock \emph{arXiv preprint arXiv:2006.14536}, 2020.

\bibitem[Xu et~al.(2019)Xu, Liu, Zhang, Sun, Zhao, Fan, Gan, and
  Lin]{xu2019interpreting}
Kaidi Xu, Sijia Liu, Gaoyuan Zhang, Mengshu Sun, Pu~Zhao, Quanfu Fan, Chuang
  Gan, and Xue Lin.
\newblock Interpreting adversarial examples by activation promotion and
  suppression.
\newblock \emph{arXiv preprint arXiv:1904.02057}, 2019.

\bibitem[Ye et~al.(2019)Ye, Xu, Liu, Cheng, Lambrechts, Zhang, Zhou, Ma, Wang,
  and Lin]{ye2019adversarial}
Shaokai Ye, Kaidi Xu, Sijia Liu, Hao Cheng, Jan-Henrik Lambrechts, Huan Zhang,
  Aojun Zhou, Kaisheng Ma, Yanzhi Wang, and Xue Lin.
\newblock Adversarial robustness vs. model compression, or both?
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 111--120, 2019.

\bibitem[Yim et~al.(2017)Yim, Joo, Bae, and Kim]{yim2017gift}
Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim.
\newblock A gift from knowledge distillation: Fast optimization, network
  minimization and transfer learning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4133--4141, 2017.

\bibitem[Zagoruyko and Komodakis(2016{\natexlab{a}})]{zagoruyko2016paying}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Paying more attention to attention: Improving the performance of
  convolutional neural networks via attention transfer.
\newblock \emph{arXiv preprint arXiv:1612.03928}, 2016{\natexlab{a}}.

\bibitem[Zagoruyko and Komodakis(2016{\natexlab{b}})]{zagoruyko8wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{NIN}, 8:\penalty0 35--67, 2016{\natexlab{b}}.

\bibitem[Zhai et~al.(2019)Zhai, Cai, He, Dan, He, Hopcroft, and
  Wang]{zhai2019adversarially}
Runtian Zhai, Tianle Cai, Di~He, Chen Dan, Kun He, John Hopcroft, and Liwei
  Wang.
\newblock Adversarially robust generalization just requires more unlabeled
  data.
\newblock \emph{arXiv preprint arXiv:1906.00555}, 2019.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Zhang, Lu, Zhu, and
  Dong]{zhang2019you}
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong.
\newblock You only propagate once: Accelerating adversarial training via
  maximal principle.
\newblock \emph{arXiv preprint arXiv:1905.00877}, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Yu, Jiao, Xing, El~Ghaoui, and
  Jordan]{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El~Ghaoui, and
  Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In \emph{International Conference on Machine Learning}, pages
  7472--7482. PMLR, 2019{\natexlab{b}}.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Xu, Han, Niu, Cui, Sugiyama,
  and Kankanhalli]{zhang2020attacks}
Jingfeng Zhang, Xilie Xu, Bo~Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and
  Mohan Kankanhalli.
\newblock Attacks which do not kill training make adversarial learning
  stronger.
\newblock In \emph{International Conference on Machine Learning}, pages
  11278--11287. PMLR, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Zhu, Niu, Han, Sugiyama, and
  Kankanhalli]{zhang2020geometry}
Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo~Han, Masashi Sugiyama, and Mohan
  Kankanhalli.
\newblock Geometry-aware instance-reweighted adversarial training.
\newblock \emph{arXiv preprint arXiv:2010.01736}, 2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2020{\natexlab{c}})Zhang, Deng, Kawaguchi, Ghorbani, and
  Zou]{zhang2020does}
Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou.
\newblock How does mixup help with robustness and generalization?
\newblock \emph{arXiv preprint arXiv:2010.04819}, 2020{\natexlab{c}}.

\bibitem[Zhang and Zhu(2019)]{zhang2019interpreting}
Tianyuan Zhang and Zhanxing Zhu.
\newblock Interpreting adversarially trained convolutional neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  7502--7511. PMLR, 2019.

\bibitem[Zhu et~al.(2021)Zhu, Ma, Sun, Chen, Jiang, Chen, and
  Li]{zhu2021towards}
Yao Zhu, Jiacheng Ma, Jiacheng Sun, Zewei Chen, Rongxin Jiang, Yaowu Chen, and
  Zhenguo Li.
\newblock Towards understanding the generative capability of adversarially
  robust classifiers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 7728--7737, 2021.

\end{thebibliography}
