\begin{thebibliography}{10}

\bibitem{tensorflow2015-whitepaper}
M.~Abadi, A.~Agarwal, P.~Barham, E.~Brevdo, Z.~Chen, C.~Citro, G.~S. Corrado,
  A.~Davis, J.~Dean, M.~Devin, S.~Ghemawat, I.~Goodfellow, A.~Harp, G.~Irving,
  M.~Isard, Y.~Jia, R.~Jozefowicz, L.~Kaiser, M.~Kudlur, J.~Levenberg,
  D.~Man\'{e}, R.~Monga, S.~Moore, D.~Murray, C.~Olah, M.~Schuster, J.~Shlens,
  B.~Steiner, I.~Sutskever, K.~Talwar, P.~Tucker, V.~Vanhoucke, V.~Vasudevan,
  F.~Vi\'{e}gas, O.~Vinyals, P.~Warden, M.~Wattenberg, M.~Wicke, Y.~Yu, and
  X.~Zheng.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock Software available from tensorflow.org.

\bibitem{alquier2016properties}
P.~Alquier, J.~Ridgway, and N.~Chopin.
\newblock On the properties of variational approximations of gibbs posteriors.
\newblock {\em The Journal of Machine Learning Research}, 17(1):8374--8414,
  2016.

\bibitem{amit18a}
R.~Amit and R.~Meir.
\newblock Meta-learning by adjusting priors based on extended {PAC}-{B}ayes
  theory.
\newblock In J.~Dy and A.~Krause, editors, {\em Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of {\em Proceedings
  of Machine Learning Research}, pages 205--214, Stockholmsmässan, Stockholm
  Sweden, 10--15 Jul 2018. PMLR.

\bibitem{bansal2019learning}
T.~Bansal, R.~Jha, and A.~McCallum.
\newblock Learning to few-shot learn across diverse natural language
  classification tasks.
\newblock {\em arXiv preprint arXiv:1911.03863}, 2019.

\bibitem{bansal2020self}
T.~Bansal, R.~Jha, T.~Munkhdalai, and A.~McCallum.
\newblock Self-supervised meta-learning for few-shot natural language
  classification tasks.
\newblock {\em arXiv preprint arXiv:2009.08445}, 2020.

\bibitem{baxter1998theoretical}
J.~Baxter.
\newblock Theoretical models of learning to learn.
\newblock In {\em Learning to learn}, pages 71--94. Springer, 1998.

\bibitem{devlin2019bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, June 2019.

\bibitem{ding2017cold}
N.~Ding and R.~Soricut.
\newblock Cold-start reinforcement learning with softmax policy gradient.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 2814--2823, 2017.

\bibitem{finn2017model}
C.~Finn, P.~Abbeel, and S.~Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock {\em arXiv preprint arXiv:1703.03400}, 2017.

\bibitem{germain2016pac}
P.~Germain, F.~Bach, A.~Lacoste, and S.~Lacoste-Julien.
\newblock {PAC}-bayesian theory meets bayesian inference.
\newblock {\em Advances in Neural Information Processing Systems},
  29:1884--1892, 2016.

\bibitem{germain2016new}
P.~Germain, A.~Habrard, F.~Laviolette, and E.~Morvant.
\newblock A new pac-bayesian perspective on domain adaptation.
\newblock In {\em International conference on machine learning}, pages
  859--868. PMLR, 2016.

\bibitem{germain2009pac}
P.~Germain, A.~Lacasse, F.~Laviolette, and M.~Marchand.
\newblock {PAC}-bayesian learning of linear classifiers.
\newblock In {\em Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 353--360, 2009.

\bibitem{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{liu2016stein}
Q.~Liu and D.~Wang.
\newblock Stein variational gradient descent: A general purpose bayesian
  inference algorithm.
\newblock {\em arXiv preprint arXiv:1608.04471}, 2016.

\bibitem{mcallester1999some}
D.~A. McAllester.
\newblock Some {PAC}-bayesian theorems.
\newblock {\em Machine Learning}, 37(3):355--363, 1999.

\bibitem{nichol1803first}
A.~Nichol, J.~Achiam, and J.~Schulman.
\newblock On first-order meta-learning algorithms.
\newblock {\em arXiv preprint arXiv:1803.02999}, 2018.

\bibitem{NEURIPS2019_9015}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito,
  M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and
  S.~Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems 32}, pages 8024--8035. Curran Associates,
  Inc., 2019.

\bibitem{pentina14}
A.~Pentina and C.~H. Lampert.
\newblock A pac-bayesian bound for lifelong learning.
\newblock In {\em Proceedings of the 31st International Conference on
  International Conference on Machine Learning - Volume 32}, ICML'14, page
  II–991–II–999, 2014.

\bibitem{pentina2015lifelong}
A.~Pentina and C.~H. Lampert.
\newblock Lifelong learning with non-iid tasks.
\newblock {\em Advances in Neural Information Processing Systems},
  28:1540--1548, 2015.

\bibitem{raghu2019rapid}
A.~Raghu, M.~Raghu, S.~Bengio, and O.~Vinyals.
\newblock Rapid learning or feature reuse? towards understanding the
  effectiveness of maml.
\newblock {\em arXiv preprint arXiv:1909.09157}, 2019.

\bibitem{Ravi2017OptimizationAA}
S.~Ravi and H.~Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock In {\em ICLR}, 2017.

\bibitem{rothfuss2020pacoh}
J.~Rothfuss, V.~Fortuin, and A.~Krause.
\newblock {PACOH}: Bayes-optimal meta-learning with pac-guarantees, 2020.

\bibitem{russakovsky2015imagenet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International journal of computer vision}, 115(3):211--252,
  2015.

\bibitem{tripuraneni2020theory}
N.~Tripuraneni, M.~I. Jordan, and C.~Jin.
\newblock On the theory of transfer learning: The importance of task diversity.
\newblock {\em arXiv preprint arXiv:2006.11650}, 2020.

\bibitem{vinyals2016matching}
O.~Vinyals, C.~Blundell, T.~Lillicrap, K.~Kavukcuoglu, and D.~Wierstra.
\newblock Matching networks for one shot learning.
\newblock {\em arXiv preprint arXiv:1606.04080}, 2016.

\bibitem{wang2018glue}
A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock {\em arXiv preprint arXiv:1804.07461}, 2018.

\bibitem{welling2011bayesian}
M.~Welling and Y.~W. Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In {\em Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688. Citeseer, 2011.

\bibitem{Williams92}
R.~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock In {\em Machine Learning}, pages 229--256, 1992.

\bibitem{yoon2018bayesian}
J.~Yoon, T.~Kim, O.~Dia, S.~Kim, Y.~Bengio, and S.~Ahn.
\newblock Bayesian model-agnostic meta-learning.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pages 7343--7353, 2018.

\end{thebibliography}
