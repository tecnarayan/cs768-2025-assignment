\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdulle et~al.(2014)Abdulle, Vilmart, and Zygalakis]{abdulle2014high}
A.~Abdulle, G.~Vilmart, and K.~C. Zygalakis.
\newblock High order numerical approximation of the invariant measure of
  ergodic sdes.
\newblock \emph{SIAM Journal on Numerical Analysis}, 52\penalty0 (4):\penalty0
  1600--1622, 2014.

\bibitem[Abdulle et~al.(2015)Abdulle, Vilmart, and Zygalakis]{abdulle2015long}
A.~Abdulle, G.~Vilmart, and K.~C. Zygalakis.
\newblock Long time accuracy of {L}ie-{T}rotter splitting methods for
  {L}angevin dynamics.
\newblock \emph{SIAM Journal on Numerical Analysis}, 53\penalty0 (1):\penalty0
  1--16, 2015.

\bibitem[Ahn et~al.(2012)Ahn, Korattikara, and Welling]{ahn2012bayesian}
S.~Ahn, A.~Korattikara, and M.~Welling.
\newblock {B}ayesian posterior sampling via stochastic gradient {F}isher
  scoring.
\newblock In \emph{International Conference on Machine Learning}, pages
  1771--1778, 2012.

\bibitem[Betancourt(2015)]{pmlr-v37-betancourt15}
M.~Betancourt.
\newblock The fundamental incompatibility of scalable {H}amiltonian {M}onte
  {C}arlo and naive data subsampling.
\newblock In F.~Bach and D.~Blei, editors, \emph{International Conference on
  Machine Learning}, volume~37 of \emph{Proceedings of Machine Learning
  Research}, pages 533--540, Lille, France, 07--09 Jul 2015.

\bibitem[Betancourt et~al.(2018)Betancourt, Jordan, and
  Wilson]{betancourt2018symplectic}
M.~Betancourt, M.~I. Jordan, and A.~C. Wilson.
\newblock On symplectic optimization.
\newblock \emph{arXiv preprint arXiv:1802.03653}, 2018.

\bibitem[Bishop(2006)]{Bishop06}
C.~M. Bishop.
\newblock \emph{{Pattern recognition and machine learning}}.
\newblock Springer, 1st ed. 2006. corr. 2nd printing 2011 edition, 2006.
\newblock ISBN 0387310738.

\bibitem[Bou-Rabee and Owhadi(2010)]{bou2010long}
N.~Bou-Rabee and H.~Owhadi.
\newblock Long-run accuracy of variational integrators in the stochastic
  context.
\newblock \emph{SIAM Journal on Numerical Analysis}, 48\penalty0 (1):\penalty0
  278--297, 2010.

\bibitem[Chaudhari and Soatto(2018)]{chaudhari2018stochastic}
P.~Chaudhari and S.~Soatto.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock In \emph{2018 Information Theory and Applications Workshop (ITA)},
  pages 1--10. IEEE, 2018.

\bibitem[Chen et~al.(2015)Chen, Ding, and Carin]{chen2015convergence}
C.~Chen, N.~Ding, and L.~Carin.
\newblock {On the convergence of stochastic gradient MCMC algorithms with
  high-order integrators}.
\newblock \emph{Advances in neural information processing systems},
  28:\penalty0 2278--2286, 2015.

\bibitem[Chen et~al.(2014)Chen, Fox, and Guestrin]{chen2014stochastic}
T.~Chen, E.~Fox, and C.~Guestrin.
\newblock Stochastic gradient {H}amiltonian {M}onte {C}arlo.
\newblock In \emph{International conference on machine learning}, pages
  1683--1691, 2014.

\bibitem[Childs and Su(2019)]{childs2019nearly}
A.~M. Childs and Y.~Su.
\newblock Nearly optimal lattice simulation by product formulas.
\newblock \emph{Physical review letters}, 123\penalty0 (5):\penalty0 050503,
  2019.

\bibitem[Childs et~al.(2019)Childs, Ostrander, and Su]{childs2019faster}
A.~M. Childs, A.~Ostrander, and Y.~Su.
\newblock Faster quantum simulation by randomization.
\newblock \emph{Quantum}, 3:\penalty0 182, 2019.

\bibitem[Childs et~al.(2021)Childs, Su, Tran, Wiebe, and Zhu]{childs2019theory}
A.~M. Childs, Y.~Su, M.~C. Tran, N.~Wiebe, and S.~Zhu.
\newblock Theory of {T}rotter error with commutator scaling.
\newblock \emph{Physical Review X}, 11\penalty0 (1):\penalty0 011020, 2021.

\bibitem[Debussche and Faou(2012)]{debussche2012weak}
A.~Debussche and E.~Faou.
\newblock Weak backward error analysis for sdes.
\newblock \emph{SIAM Journal on Numerical Analysis}, 50\penalty0 (3):\penalty0
  1735--1752, 2012.

\bibitem[Dynkin(1947)]{dynkin1947calculation}
E.~B. Dynkin.
\newblock Calculation of the coefficients in the campbell-hausdorff formula.
\newblock In \emph{Dokl. Akad. Nauk. SSSR (NS)}, volume~57, pages 323--326,
  1947.

\bibitem[Fran{\c{c}}a et~al.(2020)Fran{\c{c}}a, Sulam, Robinson, and
  Vidal]{francca2020conformal}
G.~Fran{\c{c}}a, J.~Sulam, D.~P. Robinson, and R.~Vidal.
\newblock Conformal symplectic and relativistic optimization.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2020\penalty0 (12):\penalty0 124008, 2020.

\bibitem[Fran{\c{c}}a et~al.(2021)Fran{\c{c}}a, Jordan, and
  Vidal]{francca2021dissipative}
G.~Fran{\c{c}}a, M.~I. Jordan, and R.~Vidal.
\newblock On dissipative symplectic integration with applications to
  gradient-based optimization.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2021\penalty0 (4):\penalty0 043402, 2021.

\bibitem[Futami et~al.(2020)Futami, Sato, and Sugiyama]{futami2020accelerating}
F.~Futami, I.~Sato, and M.~Sugiyama.
\newblock Accelerating the diffusion-based ensemble sampling by non-reversible
  dynamics.
\newblock In \emph{International Conference on Machine Learning}, pages
  3337--3347, 2020.

\bibitem[Gao et~al.(2018{\natexlab{a}})Gao, Gurbuzbalaban, and
  Zhu]{gao2018breaking}
X.~Gao, M.~Gurbuzbalaban, and L.~Zhu.
\newblock Breaking reversibility accelerates {L}angevin dynamics for global
  non-convex optimization.
\newblock \emph{arXiv preprint arXiv:1812.07725}, 2018{\natexlab{a}}.

\bibitem[Gao et~al.(2018{\natexlab{b}})Gao, G{\"u}rb{\"u}zbalaban, and
  Zhu]{gao2018global}
X.~Gao, M.~G{\"u}rb{\"u}zbalaban, and L.~Zhu.
\newblock Global convergence of stochastic gradient {H}amiltonian {M}onte
  {C}arlo for non-convex stochastic optimization: Non-asymptotic performance
  bounds and momentum-based acceleration.
\newblock \emph{arXiv preprint arXiv:1809.04618}, 2018{\natexlab{b}}.

\bibitem[Gardiner(2004)]{gardiner2004handbook}
C.~W. Gardiner.
\newblock \emph{Handbook of stochastic methods for physics, chemistry and the
  natural sciences}, volume~13 of \emph{Springer Series in Synergetics}.
\newblock Springer-Verlag, third edition, 2004.
\newblock ISBN 3-540-20882-8.

\bibitem[Hatano and Suzuki(2005)]{hatano2005finding}
N.~Hatano and M.~Suzuki.
\newblock Finding exponential product formulas of higher orders.
\newblock In \emph{Quantum annealing and other optimization methods}, pages
  37--68. Springer, 2005.

\bibitem[Hoffman and Gelman(2014)]{Hoffman2014}
M.~D. Hoffman and A.~Gelman.
\newblock The no-u-turn sampler: Adaptively setting path lengths in
  {H}amiltonian {M}onte {C}arlo.
\newblock \emph{J. Mach. Learn. Res.}, 15\penalty0 (1):\penalty0 1593--1623,
  Jan. 2014.

\bibitem[H{\"o}rmander(1967)]{hormander1967hypoelliptic}
L.~H{\"o}rmander.
\newblock Hypoelliptic second order differential equations.
\newblock \emph{Acta Mathematica}, 119\penalty0 (1):\penalty0 147--171, 1967.

\bibitem[Horowitz(1991)]{horowitz1991generalized}
A.~M. Horowitz.
\newblock A generalized guided {M}onte {C}arlo algorithm.
\newblock \emph{Physics Letters B}, 268\penalty0 (2):\penalty0 247--252, 1991.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock {Neural Tangent Kernel: Convergence and Generalization in Neural
  Networks}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, pages 8571--8580. Curran Associates, Inc., 2018.

\bibitem[Kloeden and Platen(2013)]{kloeden2013numerical}
P.~E. Kloeden and E.~Platen.
\newblock \emph{Numerical solution of stochastic differential equations},
  volume~23.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl-Dickstein]{lee2020finite}
J.~Lee, S.~S. Schoenholz, J.~Pennington, B.~Adlam, L.~Xiao, R.~Novak, and
  J.~Sohl-Dickstein.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume
  33 (to appear), 2020.

\bibitem[Leen and Moody(1992)]{leen1992weight}
T.~Leen and J.~Moody.
\newblock Weight space probability densities in stochastic learning: I.
  dynamics and equilibria.
\newblock \emph{Advances in neural information processing systems}, 5:\penalty0
  451--458, 1992.

\bibitem[Low et~al.(2019)Low, Kliuchnikov, and Wiebe]{low2019well}
G.~H. Low, V.~Kliuchnikov, and N.~Wiebe.
\newblock Well-conditioned multiproduct {H}amiltonian simulation.
\newblock \emph{arXiv preprint arXiv:1907.11679}, 2019.

\bibitem[Ma et~al.(2015)Ma, Chen, and Fox]{ma2015complete}
Y.-A. Ma, T.~Chen, and E.~Fox.
\newblock A complete recipe for stochastic gradient mcmc.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2917--2925, 2015.

\bibitem[Mandt et~al.(2017)Mandt, Hoffman, and Blei]{mandt2017stochastic}
S.~Mandt, M.~D. Hoffman, and D.~M. Blei.
\newblock Stochastic gradient descent as approximate {B}ayesian inference.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 4873--4907, 2017.

\bibitem[Mattingly et~al.(2010)Mattingly, Stuart, and
  Tretyakov]{mattingly2010convergence}
J.~C. Mattingly, A.~M. Stuart, and M.~V. Tretyakov.
\newblock Convergence of numerical time-averaging and stationary measures via
  poisson equations.
\newblock \emph{SIAM Journal on Numerical Analysis}, 48\penalty0 (2):\penalty0
  552--577, 2010.

\bibitem[Milstein and Tretyakov(2007)]{MILSTEIN200781}
G.~Milstein and M.~Tretyakov.
\newblock Computing ergodic limits for {L}angevin equations.
\newblock \emph{Physica D: Nonlinear Phenomena}, 229\penalty0 (1):\penalty0
  81--95, 2007.
\newblock ISSN 0167-2789.
\newblock \doi{https://doi.org/10.1016/j.physd.2007.03.011}.

\bibitem[Milstein and Tretyakov(2003)]{milstein2003quasi}
G.~Milstein and M.~V. Tretyakov.
\newblock Quasi-symplectic methods for {L}angevin-type equations.
\newblock \emph{IMA journal of numerical analysis}, 23\penalty0 (4):\penalty0
  593--626, 2003.

\bibitem[Milstein et~al.(2002)Milstein, Repin, and
  Tretyakov]{milstein2002symplectic}
G.~N. Milstein, Y.~M. Repin, and M.~V. Tretyakov.
\newblock Symplectic integration of {H}amiltonian systems with additive noise.
\newblock \emph{SIAM Journal on Numerical Analysis}, 39\penalty0 (6):\penalty0
  2066--2088, 2002.

\bibitem[Mukhoti et~al.(2018)Mukhoti, Stenetorp, and Gal]{gal2018w}
J.~Mukhoti, P.~Stenetorp, and Y.~Gal.
\newblock On the importance of strong baselines in {B}ayesian deep learning.
\newblock In \emph{NeurIPS 2018: Workshop on Bayesian Deep Learning}, 2018.

\bibitem[Neal(1996)]{neal1996bayesian}
R.~M. Neal.
\newblock \emph{{{B}ayesian Learning for Neural Networks (Lecture Notes in
  Statistics)}}.
\newblock Springer, 1996.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
R.~M. Neal et~al.
\newblock Mcmc using {H}amiltonian dynamics.
\newblock \emph{Handbook of {M}arkov chain {M}onte {C}arlo}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Patterson and Teh(2013)]{NIPS2013_4883}
S.~Patterson and Y.~W. Teh.
\newblock Stochastic gradient {R}iemannian {L}angevin dynamics on the
  probability simplex.
\newblock In C.~J.~C. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.~Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, pages 3102--3110. Curran Associates, Inc., 2013.

\bibitem[Shahbaba et~al.(2014)Shahbaba, Lan, Johnson, and
  Neal]{shahbaba2014split}
B.~Shahbaba, S.~Lan, W.~O. Johnson, and R.~M. Neal.
\newblock Split {H}amiltonian {M}onte {C}arlo.
\newblock \emph{Statistics and Computing}, 24\penalty0 (3):\penalty0 339--349,
  2014.

\bibitem[Shi et~al.(2020)Shi, Su, and Jordan]{shi2020learning}
B.~Shi, W.~J. Su, and M.~I. Jordan.
\newblock On learning rates and schr$\backslash$" odinger operators.
\newblock \emph{arXiv preprint arXiv:2004.06977}, 2020.

\bibitem[Springenberg et~al.(2016)Springenberg, Klein, Falkner, and
  Hutter]{springenberg2016bayesian}
J.~T. Springenberg, A.~Klein, S.~Falkner, and F.~Hutter.
\newblock {B}ayesian optimization with robust {B}ayesian neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  4134--4142, 2016.

\bibitem[Suzuki(1977)]{suzuki1977convergence}
M.~Suzuki.
\newblock On the convergence of exponential operatorsâ€”the {Z}assenhaus
  formula, {BCH} formula and systematic approximants.
\newblock \emph{Communications in Mathematical Physics}, 57\penalty0
  (3):\penalty0 193--200, 1977.

\bibitem[Talay(2002)]{talay2002stochastic}
D.~Talay.
\newblock Stochastic {H}amiltonian systems: exponential convergence to the
  invariant measure, and discretization by the implicit {E}uler scheme.
\newblock \emph{Markov Process. Related Fields}, 8\penalty0 (2):\penalty0
  163--198, 2002.

\bibitem[Talay and Tubaro(1990)]{talay1990expansion}
D.~Talay and L.~Tubaro.
\newblock Expansion of the global error for numerical schemes solving
  stochastic differential equations.
\newblock \emph{Stochastic analysis and applications}, 8\penalty0 (4):\penalty0
  483--509, 1990.

\bibitem[Tran et~al.(2020)Tran, Rossi, Milios, and Filippone]{tran2020need}
B.-H. Tran, S.~Rossi, D.~Milios, and M.~Filippone.
\newblock All you need is a good functional prior for {B}ayesian deep learning,
  2020.

\bibitem[Vollmer et~al.(2016)Vollmer, Zygalakis, and
  Teh]{vollmer2016exploration}
S.~J. Vollmer, K.~C. Zygalakis, and Y.~W. Teh.
\newblock Exploration of the (non-) asymptotic bias and variance of stochastic
  gradient {L}angevin dynamics.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 5504--5548, 2016.

\bibitem[Welling and Teh(2011)]{welling2011bayesian}
M.~Welling and Y.~W. Teh.
\newblock {B}ayesian learning via stochastic gradient {L}angevin dynamics.
\newblock In \emph{International Conference on Machine Learning}, pages
  681--688, 2011.

\bibitem[Wenzel et~al.(2020)Wenzel, Roth, Veeling, Swiatkowski, Tran, Mandt,
  Snoek, Salimans, Jenatton, and Nowozin]{wenzel2020good}
F.~Wenzel, K.~Roth, B.~Veeling, J.~Swiatkowski, L.~Tran, S.~Mandt, J.~Snoek,
  T.~Salimans, R.~Jenatton, and S.~Nowozin.
\newblock How good is the {B}ayes posterior in deep neural networks really?
\newblock In \emph{International Conference on Machine Learning}, pages
  10248--10259, 2020.

\bibitem[Xu et~al.(2018)Xu, Chen, Zou, and Gu]{xu2018global}
P.~Xu, J.~Chen, D.~Zou, and Q.~Gu.
\newblock Global convergence of {L}angevin dynamics based algorithms for
  nonconvex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3122--3133, 2018.

\bibitem[Yaida(2019)]{yaida2018fluctuation}
S.~Yaida.
\newblock Fluctuation-dissipation relations for stochastic gradient descent.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zhang(2012)]{zhang2012randomized}
C.~Zhang.
\newblock Randomized algorithms for {H}amiltonian simulation.
\newblock In \emph{{M}onte {C}arlo and Quasi-{M}onte {C}arlo Methods 2010},
  pages 709--719. Springer, 2012.

\bibitem[Zou and Gu(2021)]{zou2021convergence}
D.~Zou and Q.~Gu.
\newblock On the convergence of hamiltonian monte carlo with stochastic
  gradients.
\newblock In \emph{International Conference on Machine Learning}, pages
  13012--13022. PMLR, 2021.

\end{thebibliography}
