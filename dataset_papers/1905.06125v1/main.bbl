\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Artzner et~al.(1999)Artzner, Delbaen, Eber, and
  Heath]{artzner1999coherent}
Artzner, P., Delbaen, F., Eber, J.-M., and Heath, D.
\newblock Coherent measures of risk.
\newblock \emph{Mathematical finance}, 9\penalty0 (3):\penalty0 203--228, 1999.

\bibitem[Auer(2002)]{auer2002using}
Auer, P.
\newblock Using confidence bounds for exploitation-exploration trade-offs.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0 (3):\penalty0
  397--422, 2002.

\bibitem[Barth-Maron et~al.(2018)Barth-Maron, Hoffman, Budden, Dabney, Horgan,
  Muldal, Heess, and Lillicrap]{barth2018distributed}
Barth-Maron, G., Hoffman, M.~W., Budden, D., Dabney, W., Horgan, D., Muldal,
  A., Heess, N., and Lillicrap, T.
\newblock Distributed distributional deterministic policy gradients.
\newblock \emph{arXiv:1804.08617}, 2018.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul,
  Saxton, and Munos]{bellemare2016unifying}
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and
  Munos, R.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock \emph{NIPS}, 2016.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{bellemare2017distributional}
Bellemare, M.~G., Dabney, W., and Munos, R.
\newblock A distributional perspective on reinforcement learning.
\newblock \emph{arXiv:1707.06887}, 2017.

\bibitem[Bellman(2013)]{bellman2013dynamic}
Bellman, R.
\newblock \emph{Dynamic programming}.
\newblock Courier Corporation, 2013.

\bibitem[Chen et~al.(2018)Chen, Qian, Yao, Luo, Zhang, and Liu]{cc_lane}
Chen, C., Qian, J., Yao, H., Luo, J., Zhang, H., and Liu, W.
\newblock Towards comprehensive maneuver decisions for lane change using
  reinforcement learning.
\newblock \emph{NIPS Workshop on Machine Learning for Intelligent
  Transportation Systems (MLITS)}, 2018.

\bibitem[Chen et~al.(2017)Chen, Sidor, Abbeel, and Schulman]{chen2017ucb}
Chen, R.~Y., Sidor, S., Abbeel, P., and Schulman, J.
\newblock Ucb exploration via q-ensembles.
\newblock \emph{arXiv:1706.01502}, 2017.

\bibitem[Dabney et~al.(2017)Dabney, Rowland, Bellemare, and
  Munos]{dabney2017distributional}
Dabney, W., Rowland, M., Bellemare, M.~G., and Munos, R.
\newblock Distributional reinforcement learning with quantile regression.
\newblock \emph{arXiv:1710.10044}, 2017.

\bibitem[Dabney et~al.(2018)Dabney, Ostrovski, Silver, and
  Munos]{dabney2018implicit}
Dabney, W., Ostrovski, G., Silver, D., and Munos, R.
\newblock Implicit quantile networks for distributional reinforcement learning.
\newblock \emph{arXiv:1806.06923}, 2018.

\bibitem[Dosovitskiy et~al.(2017)Dosovitskiy, Ros, Codevilla, Lopez, and
  Koltun]{dosovitskiy2017carla}
Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., and Koltun, V.
\newblock Carla: An open urban driving simulator.
\newblock \emph{arXiv:1711.03938}, 2017.

\bibitem[Fridman et~al.(2018)Fridman, Jenik, and Terwilliger]{deeptraffic_mit}
Fridman, L., Jenik, B., and Terwilliger, J.
\newblock Deeptraffic: Driving fast through dense traffic with deep
  reinforcement learning.
\newblock \emph{arxiv:1801.02805}, 2018.

\bibitem[Hampel et~al.(2011)Hampel, Ronchetti, Rousseeuw, and
  Stahel]{hampel2011robust}
Hampel, F.~R., Ronchetti, E.~M., Rousseeuw, P.~J., and Stahel, W.~A.
\newblock \emph{Robust statistics: the approach based on influence functions},
  volume 196.
\newblock John Wiley \& Sons, 2011.

\bibitem[Hessel et~al.(2017)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{hessel2017rainbow}
Hessel, M., Modayil, J., Van~Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
  W., Horgan, D., Piot, B., Azar, M., and Silver, D.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock \emph{arXiv:1710.02298}, 2017.

\bibitem[Huber(1964)]{huber1964robust}
Huber, P.~J.
\newblock Robust estimation of a location parameter.
\newblock \emph{The Annals of Mathematical Statistics}, 35\penalty0
  (1):\penalty0 73--101, 1964.

\bibitem[Huber(2011)]{huber2011robust}
Huber, P.~J.
\newblock Robust statistics.
\newblock \emph{International Encyclopedia of Statistical Science}, 35\penalty0
  (1):\penalty0 1248--1251, 2011.

\bibitem[Jaquette(1973)]{jaquette1973markov}
Jaquette, S.~C.
\newblock Markov decision processes with a new optimality criterion: Discrete
  time.
\newblock \emph{The Annals of Statistics}, 1\penalty0 (3):\penalty0 496--505,
  1973.

\bibitem[Kaufmann et~al.(2012)Kaufmann, Capp{\'e}, and
  Garivier]{kaufmann2012bayesian}
Kaufmann, E., Capp{\'e}, O., and Garivier, A.
\newblock On bayesian upper confidence bounds for bandit problems.
\newblock \emph{AISTAT}, 2012.

\bibitem[Koenker(2005)]{koenker2005quantile}
Koenker, R.
\newblock \emph{Quantile Regression}.
\newblock Econometric Society Monographs. Cambridge University Press, 2005.

\bibitem[Koenker \& Bassett~Jr(1978)Koenker and
  Bassett~Jr]{koenker1978regression}
Koenker, R. and Bassett~Jr, G.
\newblock Regression quantiles.
\newblock \emph{Econometrica: Journal of the Econometric Society}, 46\penalty0
  (1):\penalty0 33--50, 1978.

\bibitem[Lai \& Robbins(1985)Lai and Robbins]{lai1985asymptotically}
Lai, T.~L. and Robbins, H.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock \emph{Advances in Applied Mathematics}, 6\penalty0 (1):\penalty0
  4--22, 1985.

\bibitem[Machado et~al.(2017)Machado, Bellemare, Talvitie, Veness, Hausknecht,
  and Bowling]{machado2017revisiting}
Machado, M.~C., Bellemare, M.~G., Talvitie, E., Veness, J., Hausknecht, M., and
  Bowling, M.
\newblock Revisiting the arcade learning environment: Evaluation protocols and
  open problems for general agents.
\newblock \emph{arXiv:1709.06009}, 2017.

\bibitem[Mavrin et~al.(2019{\natexlab{a}})Mavrin, Yao, and Kong]{mavrin_decor}
Mavrin, B., Yao, H., and Kong, L.
\newblock Deep reinforcement learning with decorrelation.
\newblock \emph{arxiv:1903.07765}, 2019{\natexlab{a}}.

\bibitem[Mavrin et~al.(2019{\natexlab{b}})Mavrin, Zhang, Yao, and
  Kong]{mavrin_aamas_abstract}
Mavrin, B., Zhang, S., Yao, H., and Kong, L.
\newblock Exploration in the face of parametric and intrinsic uncertainties.
\newblock \emph{AAMAS}, 2019{\natexlab{b}}.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Morimura et~al.(2012)Morimura, Sugiyama, Kashima, Hachiya, and
  Tanaka]{morimura2012parametric}
Morimura, T., Sugiyama, M., Kashima, H., Hachiya, H., and Tanaka, T.
\newblock Parametric return density estimation for reinforcement learning.
\newblock \emph{arXiv:1203.3497}, 2012.

\bibitem[Mullooly(1988)]{mullooly1988variance}
Mullooly, J.~P.
\newblock The variance of left-truncated continuous nonnegative distributions.
\newblock \emph{The American Statistician}, 42\penalty0 (3):\penalty0 208--210,
  1988.

\bibitem[O'Donoghue et~al.(2017)O'Donoghue, Osband, Munos, and
  Mnih]{o2017uncertainty}
O'Donoghue, B., Osband, I., Munos, R., and Mnih, V.
\newblock The uncertainty bellman equation and exploration.
\newblock \emph{arXiv:1709.05380}, 2017.

\bibitem[Ostrovski et~al.(2017)Ostrovski, Bellemare, Oord, and
  Munos]{ostrovski2017count}
Ostrovski, G., Bellemare, M.~G., Oord, A. v.~d., and Munos, R.
\newblock Count-based exploration with neural density models.
\newblock \emph{arXiv:1703.01310}, 2017.

\bibitem[Philippe(2006)]{philippe2001value}
Philippe, J.
\newblock \emph{Value at risk: the new benchmark for managing financial risk,
  3rd Ed.}
\newblock McGraw-Hill Education, 2006.

\bibitem[Rowland et~al.(2018)Rowland, Bellemare, Dabney, Munos, and
  Teh]{rowland2018analysis}
Rowland, M., Bellemare, M.~G., Dabney, W., Munos, R., and Teh, Y.~W.
\newblock An analysis of categorical distributional reinforcement learning.
\newblock \emph{arXiv:1802.08163}, 2018.

\bibitem[Sakib et~al.(2019)Sakib, Yao, and Zhang]{sakib_iros}
Sakib, N., Yao, H., and Zhang, H.
\newblock Reinforcing classical planning for adversary driving scenarios.
\newblock \emph{arxiv:1903.08606}, 2019.

\bibitem[Strehl \& Littman(2005)Strehl and Littman]{strehl2005theoretical}
Strehl, A.~L. and Littman, M.~L.
\newblock A theoretical analysis of model-based interval estimation.
\newblock \emph{ICML}, 2005.

\bibitem[Sutton(1988)]{sutton1988learning}
Sutton, R.~S.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine Learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction (2nd Edition)}.
\newblock MIT press, 2018.

\bibitem[Szepesv{\'a}ri(2010)]{szepesvari2010algorithms}
Szepesv{\'a}ri, C.
\newblock \emph{Algorithms for Reinforcement Learning}.
\newblock Morgan and Claypool, 2010.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, Stooke, Chen, Duan, Schulman,
  DeTurck, and Abbeel]{tang2017exploration}
Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, O.~X., Duan, Y.,
  Schulman, J., DeTurck, F., and Abbeel, P.
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock \emph{NIPS}, 2017.

\bibitem[Tang \& Agrawal(2018)Tang and Agrawal]{tang2018exploration}
Tang, Y. and Agrawal, S.
\newblock Exploration by distributional reinforcement learning.
\newblock \emph{arXiv:1805.01907}, 2018.

\bibitem[Valdez(2005)]{valdez2005tail}
Valdez, E.~A.
\newblock Tail conditional variance for elliptically contoured distributions.
\newblock \emph{Belgian Actuarial Bulletin}, 5\penalty0 (1):\penalty0 26--36,
  2005.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{watkins1992q}
Watkins, C.~J. and Dayan, P.
\newblock Q-learning.
\newblock \emph{Machine Learning}, 8\penalty0 (3-4):\penalty0 279--292, 1992.

\bibitem[Yao et~al.(2017)Yao, Nosrati, and Rezaee]{yao_mcts_uda}
Yao, H., Nosrati, M.~S., and Rezaee, K.
\newblock Monte-carlo tree search vs. model-predictive controller: A
  track-following example.
\newblock \emph{NIPS Workshop on Machine Learning for Intelligent
  Transportation Systems (MLITS)}, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Mavrin, Yao, Kong, and Liu]{zhang2018quota}
Zhang, S., Mavrin, B., Yao, H., Kong, L., and Liu, B.
\newblock {QUOTA}: The quantile option architecture for reinforcement learning.
\newblock \emph{AAAI}, 2019.

\end{thebibliography}
