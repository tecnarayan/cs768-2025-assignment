\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agakov \& Barber(2004)Agakov and Barber]{agakov2004auxiliary}
Agakov, F.~V. and Barber, D.
\newblock An auxiliary variational method.
\newblock In \emph{Neural Information Processing}, 2004.

\bibitem[Bachman \& Precup(2015)Bachman and Precup]{bachman2015training}
Bachman, P. and Precup, D.
\newblock Training deep generative models: Variations on a theme.
\newblock In \emph{NIPS Approximate Inference Workshop}, 2015.

\bibitem[Bornschein \& Bengio(2014)Bornschein and
  Bengio]{bornschein2014reweighted}
Bornschein, J. and Bengio, Y.
\newblock Reweighted wake-sleep.
\newblock \emph{arXiv preprint arXiv:1406.2751}, 2014.

\bibitem[Burda et~al.(2015)Burda, Grosse, and
  Salakhutdinov]{burda2015importance}
Burda, Y., Grosse, R., and Salakhutdinov, R.
\newblock Importance weighted autoencoders.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Clevert et~al.(2016)Clevert, Unterthiner, and
  Hochreiter]{clevert2015fast}
Clevert, D.-A., Unterthiner, T., and Hochreiter, S.
\newblock Fast and accurate deep network learning by exponential linear units
  (elus).
\newblock \emph{International Conference on Learning Representations}, 2016.

\bibitem[Cremer et~al.(2017)Cremer, Morris, and
  Duvenaud]{cremer2017reinterpreting}
Cremer, C., Morris, Q., and Duvenaud, D.
\newblock Reinterpreting importance-weighted autoencoders.
\newblock \emph{arXiv preprint arXiv:1704.02916}, 2017.

\bibitem[Cremer et~al.(2018)Cremer, Li, and Duvenaud]{cremer2018inference}
Cremer, C., Li, X., and Duvenaud, D.
\newblock Inference suboptimality in variational autoencoders.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Dieng et~al.(2017)Dieng, Tran, Ranganath, Paisley, and
  Blei]{dieng2017variational}
Dieng, A.~B., Tran, D., Ranganath, R., Paisley, J., and Blei, D.
\newblock Variational inference via $\chi$ upper bound minimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Domke \& Sheldon(2018)Domke and Sheldon]{domke2018importance}
Domke, J. and Sheldon, D.~R.
\newblock Importance weighting and variational inference.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4475--4484, 2018.

\bibitem[Edwards \& Storkey(2017)Edwards and Storkey]{edwards2016towards}
Edwards, H. and Storkey, A.
\newblock Towards a neural statistician.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Huang \& Courville(2017)Huang and Courville]{huangsequentialized}
Huang, C.-W. and Courville, A.
\newblock Sequentialized sampling importance resampling and scalable iwae.
\newblock \emph{NIPS Bayesian Deep Learning Workshop}, 2017.

\bibitem[Huang et~al.(2018)Huang, Krueger, Lacoste, and
  Courville]{huang2018neural}
Huang, C.-W., Krueger, D., Lacoste, A., and Courville, A.
\newblock Neural autoregressive flows.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, Jozefowicz, Chen, Sutskever, and
  Welling]{kingma2016improved}
Kingma, D.~P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and
  Welling, M.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Klys et~al.(2018)Klys, Bettencourt, and Duvenaud]{klys2018joint}
Klys, J., Bettencourt, J., and Duvenaud, D.
\newblock Joint importance sampling for variational inference.
\newblock 2018.

\bibitem[Lake et~al.(2015)Lake, Salakhutdinov, and Tenenbaum]{lake2015human}
Lake, B.~M., Salakhutdinov, R., and Tenenbaum, J.~B.
\newblock Human-level concept learning through probabilistic program induction.
\newblock \emph{Science}, 350\penalty0 (6266):\penalty0 1332--1338, 2015.

\bibitem[Larochelle \& Murray(2011)Larochelle and Murray]{larochelle2011}
Larochelle, H. and Murray, I.
\newblock The neural autoregressive distribution estimator.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2011.

\bibitem[Le et~al.(2018)Le, Kosiorek, Siddharth, Teh, and
  Wood]{le2018revisiting}
Le, T.~A., Kosiorek, A.~R., Siddharth, N., Teh, Y.~W., and Wood, F.
\newblock Revisiting reweighted wake-sleep.
\newblock \emph{arXiv preprint arXiv:1805.10469}, 2018.

\bibitem[Maal{\o}e et~al.(2016)Maal{\o}e, S{\o}nderby, S{\o}nderby, and
  Winther]{maaloe2016auxiliary}
Maal{\o}e, L., S{\o}nderby, C.~K., S{\o}nderby, S.~K., and Winther, O.
\newblock Auxiliary deep generative models.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Maddison et~al.(2017)Maddison, Lawson, Tucker, Heess, Norouzi, Mnih,
  Doucet, and Teh]{maddison2017filtering}
Maddison, C.~J., Lawson, J., Tucker, G., Heess, N., Norouzi, M., Mnih, A.,
  Doucet, A., and Teh, Y.
\newblock Filtering variational objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6573--6583, 2017.

\bibitem[Marlin et~al.(2010)Marlin, Swersky, Chen, and
  Freitas]{marlin2010inductive}
Marlin, B., Swersky, K., Chen, B., and Freitas, N.
\newblock Inductive principles for restricted boltzmann machine learning.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  509--516, 2010.

\bibitem[Miller et~al.(2016)Miller, Foti, and Adams]{miller2016variational}
Miller, A.~C., Foti, N., and Adams, R.~P.
\newblock Variational boosting: Iteratively refining posterior approximations.
\newblock \emph{arXiv preprint arXiv:1611.06585}, 2016.

\bibitem[Minka et~al.(2005)]{minka2005divergence}
Minka, T. et~al.
\newblock Divergence measures and message passing.
\newblock Technical report, Technical report, Microsoft Research, 2005.

\bibitem[Mnih \& Rezende(2016)Mnih and Rezende]{mnih2016variational}
Mnih, A. and Rezende, D.~J.
\newblock Variational inference for monte carlo objectives.
\newblock \emph{arXiv preprint arXiv:1602.06725}, 2016.

\bibitem[Molchanov et~al.(2018)Molchanov, Kharitonov, Sobolev, and
  Vetrov]{molchanov2018doubly}
Molchanov, D., Kharitonov, V., Sobolev, A., and Vetrov, D.
\newblock Doubly semi-implicit variational inference.
\newblock \emph{arXiv preprint arXiv:1810.02789}, 2018.

\bibitem[M{\"u}ller et~al.(2018)M{\"u}ller, McWilliams, Rousselle, Gross, and
  Nov{\'a}k]{muller2018neural}
M{\"u}ller, T., McWilliams, B., Rousselle, F., Gross, M., and Nov{\'a}k, J.
\newblock Neural importance sampling.
\newblock \emph{arXiv preprint arXiv:1808.03856}, 2018.

\bibitem[Nowozin(2018)]{nowozindebiasing}
Nowozin, S.
\newblock Debiasing evidence approximations: On importance-weighted
  autoencoders and jackknife variational inference.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Owen(2013)]{mcbook}
Owen, A.~B.
\newblock \emph{Monte Carlo theory, methods and examples}.
\newblock 2013.

\bibitem[Rainforth et~al.(2018)Rainforth, Kosiorek, Le, Maddison, Igl, Wood,
  and Teh]{rainforth2018tighter}
Rainforth, T., Kosiorek, A.~R., Le, T.~A., Maddison, C.~J., Igl, M., Wood, F.,
  and Teh, Y.~W.
\newblock Tighter variational bounds are not necessarily better.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Ranganath et~al.(2016)Ranganath, Tran, and
  Blei]{ranganath2016hierarchical}
Ranganath, R., Tran, D., and Blei, D.
\newblock Hierarchical variational models.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Rezende \& Mohamed(2015)Rezende and Mohamed]{rezende2015variational}
Rezende, D.~J. and Mohamed, S.
\newblock Variational inference with normalizing flows.
\newblock In \emph{International Conference on Machine Learning}, 2015.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{International Conference on Machine Learning}, 2014.

\bibitem[Sobolev \& Vetrov(2018)Sobolev and Vetrov]{sobolevimportance}
Sobolev, A. and Vetrov, D.
\newblock Importance weighted hierarchical variational inference.
\newblock 2018.

\bibitem[Tucker et~al.(2018)Tucker, Lawson, Gu, and Maddison]{tucker2018doubly}
Tucker, G., Lawson, D., Gu, S., and Maddison, C.~J.
\newblock Doubly reparameterized gradient estimators for monte carlo
  objectives.
\newblock \emph{arXiv preprint arXiv:1810.04152}, 2018.

\bibitem[Wu et~al.(2018)Wu, Goodman, and Ermon]{wu2018differentiable}
Wu, M., Goodman, N., and Ermon, S.
\newblock Differentiable antithetic sampling for variance reduction in
  stochastic variational inference.
\newblock \emph{arXiv preprint arXiv:1810.02555}, 2018.

\bibitem[Yin \& Zhou(2018)Yin and Zhou]{yin2018semi}
Yin, M. and Zhou, M.
\newblock Semi-implicit variational inference.
\newblock \emph{arXiv preprint arXiv:1805.11183}, 2018.

\end{thebibliography}
