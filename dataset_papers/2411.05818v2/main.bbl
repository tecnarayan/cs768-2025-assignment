\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bar()]{BartLarge}
Bart large xsum.
\newblock URL \url{https://huggingface.co/facebook/bart-large-xsum}.

\bibitem[Ope()]{OpenAI}
Openai, https://openai.com.
\newblock URL \url{https://openai.com/}.

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar, and Zhang]{abadi2016deep}
Martin Abadi, Andy Chu, Ian Goodfellow, H~Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li~Zhang.
\newblock Deep learning with differential privacy.
\newblock In \emph{Proceedings of the 2016 ACM SIGSAC conference on computer and communications security}, pages 308--318, 2016.

\bibitem[Antropic()]{antropicclaude}
Antropic.
\newblock Introducing claude.
\newblock \emph{Antropic Website}.
\newblock URL \url{https://www.anthropic.com/index/introducing-claude}.
\newblock 2023-03-14, \url{https://www.anthropic.com/index/introducing-claude}.

\bibitem[Bansal et~al.(2019)Bansal, Jha, and McCallum]{bansaldisaster2019}
Trapit Bansal, Rishikesh Jha, and Andrew McCallum.
\newblock Learning to few-shot learn across diverse natural language classification tasks.
\newblock \emph{arXiv preprint arXiv:1911.03863}, 2019.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O’Brien, Hallahan, Khan, Purohit, Prashanth, Raff, et~al.]{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, et~al.
\newblock Pythia: A suite for analyzing large language models across training and scaling.
\newblock In \emph{International Conference on Machine Learning}, pages 2397--2430. PMLR, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Carlini et~al.(2021)Carlini, Tramer, Wallace, Jagielski, Herbert-Voss, Lee, Roberts, Brown, Song, Erlingsson, Oprea, and Raffel]{carlini21extracting}
Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel.
\newblock Extracting training data from large language models.
\newblock In \emph{USENIX Security Symposium}, 2021.
\newblock URL \url{https://arxiv.org/abs/2012.07805}.

\bibitem[Carlini et~al.(2024)Carlini, Paleka, Dvijotham, Steinke, Hayase, Cooper, Lee, Jagielski, Nasr, Conmy, Yona, Wallace, Rolnick, and Tramèr]{carlini2024stealingproductionlanguagemodel}
Nicholas Carlini, Daniel Paleka, Krishnamurthy~Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A.~Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, Itay Yona, Eric Wallace, David Rolnick, and Florian Tramèr.
\newblock Stealing part of a production language model.
\newblock In \emph{International Conference on Machine Learning}, 2024.

\bibitem[Chen et~al.(2022)Chen, Bao, Huang, Dong, Jiao, Jiang, Zhou, Li, and Wei]{chen-etal-2022-x}
Tianyu Chen, Hangbo Bao, Shaohan Huang, Li~Dong, Binxing Jiao, Daxin Jiang, Haoyi Zhou, Jianxin Li, and Furu Wei.
\newblock {THE}-{X}: Privacy-preserving transformer inference with homomorphic encryption.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, \emph{Findings of the Association for Computational Linguistics: ACL 2022}, pages 3510--3520, Dublin, Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.findings-acl.277}.
\newblock URL \url{https://aclanthology.org/2022.findings-acl.277}.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Chiang et~al.(2024)Chiang, Zheng, Sheng, Angelopoulos, Li, Li, Zhang, Zhu, Jordan, Gonzalez, and Stoica]{chiang2024chatbot}
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios~Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph~E. Gonzalez, and Ion Stoica.
\newblock Chatbot arena: An open platform for evaluating llms by human preference, 2024.

\bibitem[Choquette-Choo et~al.(2021)Choquette-Choo, Dullerud, Dziedzic, Zhang, Jha, Papernot, and Wang]{choquette2021capc}
Christopher~A Choquette-Choo, Natalie Dullerud, Adam Dziedzic, Yunxiang Zhang, Somesh Jha, Nicolas Papernot, and Xiao Wang.
\newblock Capc learning: Confidential and private collaborative learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Dehghani et~al.(2021)Dehghani, Arnab, Beyer, Vaswani, and Tay]{dehghani2021efficiency}
Mostafa Dehghani, Anurag Arnab, Lucas Beyer, Ashish Vaswani, and Yi~Tay.
\newblock The efficiency misnomer.
\newblock \emph{arXiv preprint arXiv:2110.12894}, 2021.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{arXiv preprint arXiv:2305.14314}, 2023.

\bibitem[Duan et~al.(2023{\natexlab{a}})Duan, Dziedzic, Papernot, and Boenisch]{duan2023flocks}
Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch.
\newblock Flocks of stochastic parrots: Differentially private prompt learning for large language models.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)}, 2023{\natexlab{a}}.

\bibitem[Duan et~al.(2023{\natexlab{b}})Duan, Dziedzic, Yaghini, Papernot, and Boenisch]{duan2023privacy}
Haonan Duan, Adam Dziedzic, Mohammad Yaghini, Nicolas Papernot, and Franziska Boenisch.
\newblock On the privacy risk of in-context learning.
\newblock In \emph{The 61st Annual Meeting Of The Association For Computational Linguistics}, 2023{\natexlab{b}}.

\bibitem[Durfee and Rogers(2019)]{durfee2019practical}
David Durfee and Ryan~M Rogers.
\newblock Practical differentially private top-k selection with pay-what-you-get composition.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Dwork(2006)]{dwork2006differential}
Cynthia Dwork.
\newblock Differential privacy.
\newblock In \emph{Automata, Languages and Programming: 33rd International Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part II 33}, pages 1--12. Springer, 2006.

\bibitem[Dwork et~al.(2014)Dwork, Roth, et~al.]{dwork2014algorithmic}
Cynthia Dwork, Aaron Roth, et~al.
\newblock The algorithmic foundations of differential privacy.
\newblock \emph{Foundations and Trends{\textregistered} in Theoretical Computer Science}, 9\penalty0 (3--4):\penalty0 211--407, 2014.

\bibitem[Geng and Liu(2023)]{openlm2023openllama}
Xinyang Geng and Hao Liu.
\newblock Openllama: An open reproduction of llama, May 2023.
\newblock URL \url{https://github.com/openlm-research/open_llama}.

\bibitem[Gillenwater et~al.(2022)Gillenwater, Joseph, Munoz, and Diaz]{gillenwater2022joint}
Jennifer Gillenwater, Matthew Joseph, Andres Munoz, and Monica~Ribero Diaz.
\newblock A joint exponential mechanism for differentially private top-$ k$.
\newblock In \emph{International Conference on Machine Learning}, pages 7570--7582. PMLR, 2022.

\bibitem[Gliwa et~al.(2019)Gliwa, Mochol, Biesek, and Wawer]{gliwa-etal-2019-samsum}
Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer.
\newblock {SAMS}um corpus: A human-annotated dialogue dataset for abstractive summarization.
\newblock In \emph{Proceedings of the 2nd Workshop on New Frontiers in Summarization}, pages 70--79, Hong Kong, China, November 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-5409}.
\newblock URL \url{https://www.aclweb.org/anthology/D19-5409}.

\bibitem[Hao et~al.(2022)Hao, Li, Chen, Xing, Xu, and Zhang]{hao2022iron}
Meng Hao, Hongwei Li, Hanxiao Chen, Pengzhi Xing, Guowen Xu, and Tianwei Zhang.
\newblock Iron: Private inference on transformers.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=deyqjpcTfsG}.

\bibitem[Hong et~al.(2024)Hong, Wang, Zhang, LI, Li, and Wang]{hong2024dpopt}
Junyuan Hong, Jiachen~T. Wang, Chenhui Zhang, Zhangheng LI, Bo~Li, and Zhangyang Wang.
\newblock {DP}-{OPT}: Make large language model your differentially-private prompt engineer.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=Ifz3IgsEPX}.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, de~las Casas, Hanna, Bressand, Lengyel, Bour, Lample, Lavaud, Saulnier, Lachaux, Stock, Subramanian, Yang, Antoniak, Scao, Gervet, Lavril, Wang, Lacroix, and Sayed]{jiang2024mixtralexperts}
Albert~Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio~Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven~Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mixtral of experts, 2024.
\newblock URL \url{https://arxiv.org/abs/2401.04088}.

\bibitem[Lermen and Rogers-Smith(2024)]{lermen2024lora}
Simon Lermen and Charlie Rogers-Smith.
\newblock Lo{RA} fine-tuning efficiently undoes safety training in llama 2-chat 70b.
\newblock In \emph{ICLR 2024 Workshop on Secure and Trustworthy Large Language Models}, 2024.
\newblock URL \url{https://openreview.net/forum?id=Y52UbVhglu}.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Brian Lester, Rami Al-Rfou, and Noah Constant.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, November 2021.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.243}.

\bibitem[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy, Stoyanov, and Zettlemoyer]{bart_citation}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.
\newblock {BART:} denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.
\newblock \emph{CoRR}, abs/1910.13461, 2019.
\newblock URL \url{http://arxiv.org/abs/1910.13461}.

\bibitem[Li et~al.(2023)Li, Wang, Shao, Guo, Xing, and Zhang]{li2023mpcformer}
Dacheng Li, Hongyi Wang, Rulin Shao, Han Guo, Eric Xing, and Hao Zhang.
\newblock {MPCFORMER}: {FAST}, {PERFORMANT} {AND} {PRIVATE} {TRANSFORMER} {INFERENCE} {WITH} {MPC}.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=CWmvjOEhgH-}.

\bibitem[Li and Liang(2021)]{li2021prefixtuningv1}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 4582--4597, Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.353}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.353}.

\bibitem[Li and Roth(2002)]{li-roth-2002-learning}
Xin Li and Dan Roth.
\newblock Learning question classifiers.
\newblock In \emph{{COLING} 2002: The 19th International Conference on Computational Linguistics}, 2002.
\newblock URL \url{https://www.aclweb.org/anthology/C02-1150}.

\bibitem[Li et~al.(2022)Li, Tramer, Liang, and Hashimoto]{li2022large}
Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto.
\newblock Large language models can be strong differentially private learners.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=bVuP3ltATMz}.

\bibitem[Lin(2004)]{lin-2004-rouge}
Chin-Yew Lin.
\newblock {ROUGE}: A package for automatic evaluation of summaries.
\newblock In \emph{Text Summarization Branches Out}, pages 74--81, Barcelona, Spain, July 2004. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/W04-1013}.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Tam, Mohammed, Mohta, Huang, Bansal, and Raffel]{liu2022fewshot}
Haokun Liu, Derek Tam, Muqeeth Mohammed, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel.
\newblock Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=rBCvMG-JsPd}.

\bibitem[Liu et~al.(2012)Liu, Cyphers, Pasupat, Mcgraw, and Glass]{liu2012MITMovies}
Jingjing Liu, Scott Cyphers, Panupong Pasupat, Ian Mcgraw, and Jim Glass.
\newblock A conversational movie search system based on conditional random fields.
\newblock \emph{13th Annual Conference of the International Speech Communication Association 2012, INTERSPEECH 2012}, 3, 01 2012.

\bibitem[Liu et~al.(2021)Liu, Ji, Fu, Tam, Du, Yang, and Tang]{liu2021p2}
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng~Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.
\newblock P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.
\newblock \emph{arXiv preprint arXiv:2110.07602}, 2021.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Ji, Fu, Tam, Du, Yang, and Tang]{liu2022p}
Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang.
\newblock {P}-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pages 61--68, Dublin, Ireland, May 2022{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-short.8}.
\newblock URL \url{https://aclanthology.org/2022.acl-short.8}.

\bibitem[Liu et~al.(2020)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis, Zettlemoyer, and Stoyanov]{liu2020roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Ro{\{}bert{\}}a: A robustly optimized {\{}bert{\}} pretraining approach, 2020.
\newblock URL \url{https://openreview.net/forum?id=SyxS0T4tvS}.

\bibitem[Lu et~al.(2021)Lu, Bartolo, Moore, Riedel, and Stenetorp]{lumpqa2021}
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.
\newblock Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity.
\newblock \emph{arXiv preprint arXiv:2104.08786}, 2021.

\bibitem[Mattern et~al.(2023)Mattern, Mireshghallah, Jin, Schoelkopf, Sachan, and Berg-Kirkpatrick]{mattern-etal-2023-membership}
Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schoelkopf, Mrinmaya Sachan, and Taylor Berg-Kirkpatrick.
\newblock Membership inference attacks against language models via neighbourhood comparison.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, \emph{Findings of the Association for Computational Linguistics: ACL 2023}, pages 11330--11343, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-acl.719}.
\newblock URL \url{https://aclanthology.org/2023.findings-acl.719}.

\bibitem[McSherry and Talwar(2007)]{mcsherry2007mechanism}
Frank McSherry and Kunal Talwar.
\newblock Mechanism design via differential privacy.
\newblock In \emph{48th Annual IEEE Symposium on Foundations of Computer Science (FOCS'07)}, pages 94--103. IEEE, 2007.

\bibitem[Mironov et~al.(2019)Mironov, Talwar, and Zhang]{mironov2019r}
Ilya Mironov, Kunal Talwar, and Li~Zhang.
\newblock R$\backslash$'enyi differential privacy of the sampled gaussian mechanism.
\newblock \emph{arXiv preprint arXiv:1908.10530}, 2019.

\bibitem[Mitchell et~al.(2023)Mitchell, Lee, Khazatsky, Manning, and Finn]{mitchell2023detectgpt}
Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher~D Manning, and Chelsea Finn.
\newblock Detectgpt: Zero-shot machine-generated text detection using probability curvature.
\newblock In \emph{International Conference on Machine Learning}, pages 24950--24962. PMLR, 2023.

\bibitem[Narayan et~al.(2018)Narayan, Cohen, and Lapata]{narayan2018XSUM}
Shashi Narayan, Shay~B. Cohen, and Mirella Lapata.
\newblock Don{'}t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 1797--1807, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1206}.
\newblock URL \url{https://aclanthology.org/D18-1206}.

\bibitem[Nissim et~al.(2007)Nissim, Raskhodnikova, and Smith]{nissim2007smooth}
Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith.
\newblock Smooth sensitivity and sampling in private data analysis.
\newblock In \emph{Proceedings of the thirty-ninth annual ACM symposium on Theory of computing}, pages 75--84, 2007.

\bibitem[Papernot et~al.(2017)Papernot, Abadi, Erlingsson, Goodfellow, and Talwar]{papernot2017semi}
Nicolas Papernot, Mart{\'{\i}}n Abadi, {\'{U}}lfar Erlingsson, Ian~J. Goodfellow, and Kunal Talwar.
\newblock Semi-supervised knowledge transfer for deep learning from private training data.
\newblock In \emph{5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings}, 2017.

\bibitem[Papernot et~al.(2018)Papernot, Song, Mironov, Raghunathan, Talwar, and Erlingsson]{papernot2018scalable}
Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and {\'{U}}lfar Erlingsson.
\newblock Scalable private learning with {PATE}.
\newblock In \emph{6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings}, 2018.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and jing Zhu]{Papineni02bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock pages 311--318, 2002.

\bibitem[Post(2018)]{post-2018-call-sacrebleu}
Matt Post.
\newblock A call for clarity in reporting {BLEU} scores.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation: Research Papers}, pages 186--191, Belgium, Brussels, October 2018. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/W18-6319}.

\bibitem[Shi et~al.(2024)Shi, Ajith, Xia, Huang, Liu, Blevins, Chen, and Zettlemoyer]{shi2024detecting}
Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer.
\newblock Detecting pretraining data from large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=zWqr3MQuNs}.

\bibitem[Sordoni et~al.(2023)Sordoni, Yuan, C{\^o}t{\'e}, Pereira, Trischler, Xiao, Hosseini, Niedtner, and Roux]{sordoni2023deep}
Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre C{\^o}t{\'e}, Matheus Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, and Nicolas~Le Roux.
\newblock Deep language networks: Joint prompt training of stacked llms using variational inference.
\newblock \emph{arXiv preprint arXiv:2306.12509}, 2023.

\bibitem[Su et~al.(2022)Su, Wang, Qin, Chan, Lin, Wang, Wen, Liu, Li, Li, et~al.]{su2022transferability}
Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et~al.
\newblock On transferability of prompt tuning for natural language processing.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 3949--3969, 2022.

\bibitem[Tang et~al.(2024)Tang, Shin, Inan, Manoel, Mireshghallah, Lin, Gopi, Kulkarni, and Sim]{tang2024privacypreserving}
Xinyu Tang, Richard Shin, Huseyin~A Inan, Andre Manoel, Fatemehsadat Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, and Robert Sim.
\newblock Privacy-preserving in-context learning with differentially private few-shot generation.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=oZtt0pRnOl}.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Tian et~al.(2022)Tian, Zhao, Huang, Wang, Zhang, and He]{tian2022seqpate}
Zhiliang Tian, Yingxiu Zhao, Ziyue Huang, Yu-Xiang Wang, Nevin~L Zhang, and He~He.
\newblock Seqpate: Differentially private text generation via knowledge distillation.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 11117--11130, 2022.

\bibitem[Tito et~al.(2023)Tito, Nguyen, Tobaben, Kerkouche, Souibgui, Jung, Kang, Valveny, Honkela, Fritz, and Karatzas]{tito2023privacy}
Rub{\`e}n Tito, Khanh Nguyen, Marlon Tobaben, Raouf Kerkouche, Mohamed~Ali Souibgui, Kangsoo Jung, Lei Kang, Ernest Valveny, Antti Honkela, Mario Fritz, and Dimosthenis Karatzas.
\newblock Privacy-aware document visual question answering.
\newblock \emph{arXiv preprint arXiv:2312.10108}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and Bowman]{wang-etal-2018-glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural language understanding.
\newblock In \emph{Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}}, pages 353--355, Brussels, Belgium, November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W18-5446}.
\newblock URL \url{https://aclanthology.org/W18-5446}.

\bibitem[Wu et~al.(2024)Wu, Panda, Wang, and Mittal]{wu2024privacypreserving}
Tong Wu, Ashwinee Panda, Jiachen~T. Wang, and Prateek Mittal.
\newblock Privacy-preserving in-context learning for large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=x4OPJ7lHVU}.

\bibitem[Yu et~al.(2022)Yu, Naik, Backurs, Gopi, Inan, Kamath, Kulkarni, Lee, Manoel, Wutschitz, Yekhanin, and Zhang]{yu2022differentially}
Da~Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin~A Inan, Gautam Kamath, Janardhan Kulkarni, Yin~Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and Huishuai Zhang.
\newblock Differentially private fine-tuning of language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Q42f0dfjECO}.

\bibitem[ZHAO et~al.(2024)ZHAO, Deng, Madras, Zou, and Ren]{zhao2024learning}
Jiachen ZHAO, Zhun Deng, David Madras, James Zou, and Mengye Ren.
\newblock Learning and forgetting unsafe examples in large language models, 2024.
\newblock URL \url{https://openreview.net/forum?id=hkQOYyUChL}.

\bibitem[Zheng et~al.(2024)Zheng, Yin, Zhou, Meng, Zhou, Chang, Huang, and Peng]{ICML_ZhengY0M0CHP24}
Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai{-}Wei Chang, Minlie Huang, and Nanyun Peng.
\newblock On prompt-driven safeguarding for large language models.
\newblock In \emph{Forty-first International Conference on Machine Learning, {ICML} 2024, Vienna, Austria, July 21-27, 2024}. OpenReview.net, 2024.
\newblock URL \url{https://openreview.net/forum?id=ugxGpOEkox}.

\bibitem[Zhu and Wang(2022)]{zhu2022adaptive}
Yuqing Zhu and Yu-Xiang Wang.
\newblock Adaptive private-k-selection with adaptive k and application to multi-label pate.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 5622--5635. PMLR, 2022.

\end{thebibliography}
