\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anil et~al.(2019)Anil, Lucas, and Grosse]{anil2019sorting}
Anil, C., Lucas, J., and Grosse, R.~B.
\newblock Sorting out lipschitz function approximation.
\newblock In \emph{International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  291--301, 2019.

\bibitem[Avant \& Morgansen(2021)Avant and Morgansen]{avant2021analytical}
Avant, T. and Morgansen, K.~A.
\newblock Analytical bounds on the local lipschitz constants of relu networks.
\newblock \emph{arXiv preprint arXiv:2104.14672}, 2021.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
  Long Beach, CA, {USA}}, pp.\  6240--6249, 2017.

\bibitem[Bhowmick et~al.(2021)Bhowmick, D’Souza, and
  Raghavan]{bhowmick2021lipbab}
Bhowmick, A., D’Souza, M., and Raghavan, G.~S.
\newblock Lipbab: computing exact lipschitz constant of relu networks.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  151--162, 2021.

\bibitem[Blake(1998)]{blake1998uci}
Blake, C.
\newblock Uci repository of machine learning databases.
\newblock \emph{http://www. ics. uci. edu/\~{} mlearn/MLRepository. html},
  1998.

\bibitem[Bunel et~al.(2018)Bunel, Turkaslan, Torr, Kohli, and
  Mudigonda]{bunel2018unified}
Bunel, R., Turkaslan, I., Torr, P. H.~S., Kohli, P., and Mudigonda, P.~K.
\newblock A unified view of piecewise linear neural network verification.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4795--4804, 2018.

\bibitem[Bunel et~al.(2020)Bunel, Mudigonda, Turkaslan, Torr, Lu, and
  Kohli]{bunel2020branch}
Bunel, R., Mudigonda, P., Turkaslan, I., Torr, P., Lu, J., and Kohli, P.
\newblock Branch and bound for piecewise linear neural network verification.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (2020),
  2020.

\bibitem[Clarke(1975)]{clarke1975generalized}
Clarke, F.~H.
\newblock Generalized gradients and applications.
\newblock \emph{Transactions of the American Mathematical Society},
  205:\penalty0 247--262, 1975.

\bibitem[Daniels \& Velikova(2010)Daniels and Velikova]{daniels2010monotone}
Daniels, H. and Velikova, M.
\newblock Monotone and partially monotone neural networks.
\newblock \emph{IEEE Transactions on Neural Networks}, 21\penalty0
  (6):\penalty0 906--917, 2010.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Li]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Li, F.
\newblock Imagenet: {A} large-scale hierarchical image database.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  248--255, 2009.
\newblock \doi{10.1109/CVPR.2009.5206848}.

\bibitem[Dvijotham et~al.(2018)Dvijotham, Stanforth, Gowal, Mann, and
  Kohli]{dvijotham2018dual}
Dvijotham, K., Stanforth, R., Gowal, S., Mann, T.~A., and Kohli, P.
\newblock A dual approach to scalable verification of deep networks.
\newblock In \emph{Proceedings of the Thirty-Fourth Conference on Uncertainty
  in Artificial Intelligence, {UAI} 2018, Monterey, California, USA, August
  6-10, 2018}, pp.\  550--559, 2018.

\bibitem[Dwork et~al.(2012)Dwork, Hardt, Pitassi, Reingold, and
  Zemel]{dwork2012fairness}
Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel, R.
\newblock Fairness through awareness.
\newblock In \emph{Proceedings of the 3rd innovations in theoretical computer
  science conference}, pp.\  214--226, 2012.

\bibitem[Fazlyab et~al.(2019)Fazlyab, Robey, Hassani, Morari, and
  Pappas]{fazlyab2019efficient}
Fazlyab, M., Robey, A., Hassani, H., Morari, M., and Pappas, G.~J.
\newblock Efficient and accurate estimation of lipschitz constants for deep
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11423--11434, 2019.

\bibitem[Fel et~al.(2022)Fel, Vigouroux, Cad{\`e}ne, and Serre]{fel2022good}
Fel, T., Vigouroux, D., Cad{\`e}ne, R., and Serre, T.
\newblock How good is your explanation? algorithmic stability measures to
  assess the quality of explanations for deep neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, pp.\  720--730, 2022.

\bibitem[G{\'{o}}mez et~al.(2020)G{\'{o}}mez, Rolland, and
  Cevher]{latorre2019lipschitz}
G{\'{o}}mez, F.~L., Rolland, P., and Cevher, V.
\newblock Lipschitz constant estimation of neural networks via sparse
  polynomial optimization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Gouk et~al.(2021)Gouk, Frank, Pfahringer, and
  Cree]{gouk2021regularisation}
Gouk, H., Frank, E., Pfahringer, B., and Cree, M.~J.
\newblock Regularisation of neural networks by enforcing lipschitz continuity.
\newblock \emph{Machine Learning}, 110\penalty0 (2):\penalty0 393--416, 2021.

\bibitem[Gowal et~al.(2018)Gowal, Dvijotham, Stanforth, Bunel, Qin, Uesato,
  Mann, and Kohli]{gowal2018effectiveness}
Gowal, S., Dvijotham, K., Stanforth, R., Bunel, R., Qin, C., Uesato, J., Mann,
  T., and Kohli, P.
\newblock On the effectiveness of interval bound propagation for training
  verifiably robust models.
\newblock \emph{arXiv preprint arXiv:1810.12715}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  770--778, 2016.
\newblock \doi{10.1109/CVPR.2016.90}.

\bibitem[Hein \& Andriushchenko(2017)Hein and Andriushchenko]{hein2017formal}
Hein, M. and Andriushchenko, M.
\newblock Formal guarantees on the robustness of a classifier against
  adversarial manipulation.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
  Long Beach, CA, {USA}}, pp.\  2266--2276, 2017.

\bibitem[Hickey et~al.(2001)Hickey, Ju, and Van~Emden]{hickey2001interval}
Hickey, T., Ju, Q., and Van~Emden, M.~H.
\newblock Interval arithmetic: From principles to implementation.
\newblock \emph{Journal of the ACM (JACM)}, 48\penalty0 (5):\penalty0
  1038--1068, 2001.

\bibitem[Huang et~al.(2021)Huang, Zhang, Shi, Kolter, and
  Anandkumar]{huang2021training}
Huang, Y., Zhang, H., Shi, Y., Kolter, J.~Z., and Anandkumar, A.
\newblock Training certifiably robust neural networks with efficient local
  lipschitz bounds.
\newblock In \emph{Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021.

\bibitem[Imbert(2002)]{imbert2002support}
Imbert, C.
\newblock Support functions of clarke's generalized jacobian and of its plenary
  hull.
\newblock \emph{Nonlinear Analysis: Theory, Methods and Applications},
  29\penalty0 (8):\penalty0 1111--1125, 2002.

\bibitem[Jordan \& Dimakis(2020)Jordan and Dimakis]{jordan2020exactly}
Jordan, M. and Dimakis, A.~G.
\newblock Exactly computing the local lipschitz constant of relu networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Kang et~al.(2020)Kang, He, Maciejewski, and Tong]{kang2020inform}
Kang, J., He, J., Maciejewski, R., and Tong, H.
\newblock Inform: Individual fairness on graph mining.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  379--389, 2020.

\bibitem[Katz et~al.(2017)Katz, Barrett, Dill, Julian, and
  Kochenderfer]{katz2017reluplex}
Katz, G., Barrett, C., Dill, D.~L., Julian, K., and Kochenderfer, M.~J.
\newblock Reluplex: An efficient smt solver for verifying deep neural networks.
\newblock In \emph{International Conference on Computer Aided Verification},
  pp.\  97--117, 2017.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{cifar}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Technical Report TR-2009}, 2009.

\bibitem[Laurel et~al.(2022)Laurel, Yang, Singh, and
  Misailovic]{laurel2022dual}
Laurel, J., Yang, R., Singh, G., and Misailovic, S.
\newblock A dual number abstraction for static analysis of clarke jacobians.
\newblock \emph{Proceedings of the ACM on Programming Languages}, 6\penalty0
  (POPL):\penalty0 1--30, 2022.

\bibitem[Le \& Yang(2015)Le and Yang]{le2015tiny}
Le, Y. and Yang, X.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 2015.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{mnist}
LeCun, Y., Cortes, C., and Burges, C.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]. Available:
  http://yann.lecun.com/exdb/mnist}, 2, 2010.

\bibitem[Leino et~al.(2021)Leino, Wang, and Fredrikson]{leino2021globally}
Leino, K., Wang, Z., and Fredrikson, M.
\newblock Globally-robust neural networks.
\newblock In \emph{International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  6212--6222, 2021.

\bibitem[Li et~al.(2019)Li, Haque, Anil, Lucas, Grosse, and
  Jacobsen]{li2019preventing}
Li, Q., Haque, S., Anil, C., Lucas, J., Grosse, R.~B., and Jacobsen, J.
\newblock Preventing gradient attenuation in lipschitz constrained
  convolutional networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  15364--15376, 2019.

\bibitem[Liu et~al.(2020)Liu, Han, Zhang, and Liu]{liu2020certified}
Liu, X., Han, X., Zhang, N., and Liu, Q.
\newblock Certified monotonic neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Lyu et~al.(2020)Lyu, Ko, Kong, Wong, Lin, and Daniel]{lyu2019fastened}
Lyu, Z., Ko, C., Kong, Z., Wong, N., Lin, D., and Daniel, L.
\newblock Fastened {CROWN:} tightened neural network robustness certificates.
\newblock In \emph{The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence}, pp.\  5037--5044, 2020.

\bibitem[Mirman et~al.(2018)Mirman, Gehr, and Vechev]{mirman2018differentiable}
Mirman, M., Gehr, T., and Vechev, M.~T.
\newblock Differentiable abstract interpretation for provably robust neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  3575--3583, 2018.

\bibitem[Ryou et~al.(2021)Ryou, Chen, Balunovic, Singh, Dan, and
  Vechev]{ryou2021scalable}
Ryou, W., Chen, J., Balunovic, M., Singh, G., Dan, A., and Vechev, M.
\newblock Scalable polyhedral verification of recurrent neural networks.
\newblock In \emph{International Conference on Computer Aided Verification},
  pp.\  225--248, 2021.

\bibitem[Schaible et~al.(1996)]{schaible1996generalized}
Schaible, S. et~al.
\newblock Generalized monotone nonsmooth maps.
\newblock \emph{Journal of Convex Analysis}, 3:\penalty0 195--206, 1996.

\bibitem[Shi et~al.(2019)Shi, Zhang, Chang, Huang, and
  Hsieh]{shi2019robustness}
Shi, Z., Zhang, H., Chang, K.-W., Huang, M., and Hsieh, C.-J.
\newblock Robustness verification for transformers.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Shi et~al.(2021)Shi, Wang, Zhang, Yi, and Hsieh]{shi2021fast}
Shi, Z., Wang, Y., Zhang, H., Yi, J., and Hsieh, C.-J.
\newblock Fast certified robust training with short warmup.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Sill(1997)]{sill1997monotonic}
Sill, J.
\newblock Monotonic networks.
\newblock \emph{Advances in neural information processing systems}, 10, 1997.

\bibitem[Singh et~al.(2019)Singh, Gehr, P{\"u}schel, and
  Vechev]{singh2019abstract}
Singh, G., Gehr, T., P{\"u}schel, M., and Vechev, M.
\newblock An abstract domain for certifying neural networks.
\newblock \emph{Proceedings of the ACM on Programming Languages}, 3\penalty0
  (POPL):\penalty0 41, 2019.

\bibitem[Singla \& Feizi(2021)Singla and Feizi]{singla2021skew}
Singla, S. and Feizi, S.
\newblock Skew orthogonal convolutions.
\newblock In \emph{International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  9756--9766, 2021.

\bibitem[Sivaraman et~al.(2020)Sivaraman, Farnadi, Millstein, and den
  Broeck]{sivaraman2020counterexample}
Sivaraman, A., Farnadi, G., Millstein, T.~D., and den Broeck, G.~V.
\newblock Counterexample-guided learning of monotonic neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow,
  I.~J., and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Tsuzuku et~al.(2018)Tsuzuku, Sato, and Sugiyama]{tsuzuku2018lipschitz}
Tsuzuku, Y., Sato, I., and Sugiyama, M.
\newblock Lipschitz-margin training: Scalable certification of perturbation
  invariance for deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6542--6551, 2018.

\bibitem[Virmaux \& Scaman(2018)Virmaux and Scaman]{virmaux2018lipschitz}
Virmaux, A. and Scaman, K.
\newblock Lipschitz regularity of deep neural networks: analysis and efficient
  estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3839--3848, 2018.

\bibitem[Wang et~al.(2018{\natexlab{a}})Wang, Pei, Whitehouse, Yang, and
  Jana]{wang2018efficient}
Wang, S., Pei, K., Whitehouse, J., Yang, J., and Jana, S.
\newblock Efficient formal safety analysis of neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6369--6379, 2018{\natexlab{a}}.

\bibitem[Wang et~al.(2018{\natexlab{b}})Wang, Pei, Whitehouse, Yang, and
  Jana]{wang2018formal}
Wang, S., Pei, K., Whitehouse, J., Yang, J., and Jana, S.
\newblock Formal security analysis of neural networks using symbolic intervals.
\newblock In \emph{27th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$
  Security 18)}, pp.\  1599--1614, 2018{\natexlab{b}}.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Zhang, Xu, Lin, Jana, Hsieh, and
  Kolter]{wang2021beta}
Wang, S., Zhang, H., Xu, K., Lin, X., Jana, S., Hsieh, C.-J., and Kolter, J.~Z.
\newblock Beta-crown: Efficient bound propagation with per-neuron split
  constraints for neural network robustness verification.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 29909--29921, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Shi, Gu, and
  Hsieh]{wang2021convergence}
Wang, Y., Shi, Z., Gu, Q., and Hsieh, C.-J.
\newblock On the convergence of certified robust training with interval bound
  propagation.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.

\bibitem[Weng et~al.(2018)Weng, Zhang, Chen, Song, Hsieh, Daniel, Boning, and
  Dhillon]{weng2018towards}
Weng, T., Zhang, H., Chen, H., Song, Z., Hsieh, C., Daniel, L., Boning, D.~S.,
  and Dhillon, I.~S.
\newblock Towards fast computation of certified robustness for relu networks.
\newblock In \emph{International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5273--5282, 2018.

\bibitem[Wong \& Kolter(2018)Wong and Kolter]{wong2018provable}
Wong, E. and Kolter, J.~Z.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock In \emph{International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5283--5292, 2018.

\bibitem[Xu et~al.(2020)Xu, Shi, Zhang, Wang, Chang, Huang, Kailkhura, Lin, and
  Hsieh]{xu2020automatic}
Xu, K., Shi, Z., Zhang, H., Wang, Y., Chang, K., Huang, M., Kailkhura, B., Lin,
  X., and Hsieh, C.
\newblock Automatic perturbation analysis for scalable certified robustness and
  beyond.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Xu et~al.(2021)Xu, Zhang, Wang, Wang, Jana, Lin, and
  Hsieh]{xu2020fast}
Xu, K., Zhang, H., Wang, S., Wang, Y., Jana, S., Lin, X., and Hsieh, C.
\newblock Fast and complete: Enabling complete neural network verification with
  rapid and massively parallel incomplete verifiers.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Cai, Lu, He, and
  Wang]{zhang2021towards}
Zhang, B., Cai, T., Lu, Z., He, D., and Wang, L.
\newblock Towards certifying l-infinity robustness using neural networks with
  l-inf-dist neurons.
\newblock In \emph{International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  12368--12379,
  2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Jiang, He, and
  Wang]{zhang2021boosting}
Zhang, B., Jiang, D., He, D., and Wang, L.
\newblock Boosting the certified robustness of l-infinity distance nets.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Jiang, He, and
  Wang]{zhang2022rethinking}
Zhang, B., Jiang, D., He, D., and Wang, L.
\newblock Rethinking lipschitz neural networks and certified robustness: A
  boolean function perspective.
\newblock \emph{Advances in Neural Information Processing Systems}, 35,
  2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2018)Zhang, Weng, Chen, Hsieh, and
  Daniel]{zhang2018efficient}
Zhang, H., Weng, T., Chen, P., Hsieh, C., and Daniel, L.
\newblock Efficient neural network robustness certification with general
  activation functions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4944--4953, 2018.

\bibitem[Zhang et~al.(2019)Zhang, Zhang, and Hsieh]{zhang2019recurjac}
Zhang, H., Zhang, P., and Hsieh, C.
\newblock Recurjac: An efficient recursive algorithm for bounding jacobian
  matrix of neural networks and its applications.
\newblock In \emph{The Thirty-Third {AAAI} Conference on Artificial
  Intelligence}, pp.\  5757--5764, 2019.
\newblock \doi{10.1609/aaai.v33i01.33015757}.

\bibitem[Zhang et~al.(2020)Zhang, Chen, Xiao, Gowal, Stanforth, Li, Boning, and
  Hsieh]{zhang2019towards}
Zhang, H., Chen, H., Xiao, C., Gowal, S., Stanforth, R., Li, B., Boning, D.~S.,
  and Hsieh, C.
\newblock Towards stable and efficient training of verifiably robust neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Wang, Xu, Li, Li, Jana, Hsieh,
  and Kolter]{zhang2022gcpcrown}
Zhang, H., Wang, S., Xu, K., Li, L., Li, B., Jana, S., Hsieh, C.-J., and
  Kolter, J.~Z.
\newblock General cutting planes for bound-propagation-based neural network
  verification.
\newblock \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.

\end{thebibliography}
