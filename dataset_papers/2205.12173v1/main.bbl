\begin{thebibliography}{62}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Mané, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Viégas, Vinyals, Warden, Wattenberg, Wicke, Yu, and Zheng]{Tensorflow2015}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A.,
  Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
  Levenberg, J., Mané, D., Monga, R., Moore, S., Murray, D., Olah, C.,
  Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Viégas, F., Vinyals, O., Warden, P.,
  Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems, 2015.

\bibitem[Alistarh et~al.(2018)Alistarh, Allen-Zhu, and
  Li]{alistarh2018byzantine}
Alistarh, D., Allen-Zhu, Z., and Li, J.
\newblock Byzantine stochastic gradient descent.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  4618--4628, 2018.

\bibitem[Allen-Zhu et~al.(2020)Allen-Zhu, Ebrahimianghazani, Li, and
  Alistarh]{allen2020byzantine}
Allen-Zhu, Z., Ebrahimianghazani, F., Li, J., and Alistarh, D.
\newblock Byzantine-resilient non-convex stochastic gradient descent.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Arnold \& Groeneveld(1979)Arnold and Groeneveld]{arnold1979bounds}
Arnold, B.~C. and Groeneveld, R.~A.
\newblock Bounds on expectations of linear systematic statistics based on
  dependent samples.
\newblock \emph{The Annals of Statistics}, pp.\  220--223, 1979.

\bibitem[Baruch et~al.(2019)Baruch, Baruch, and Goldberg]{little}
Baruch, M., Baruch, G., and Goldberg, Y.
\newblock A little is enough: Circumventing defenses for distributed learning.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, 8-14 December 2019,
  Long Beach, CA, {USA}}, 2019.

\bibitem[Bertsekas \& Tsitsiklis(2015)Bertsekas and
  Tsitsiklis]{bertsekas2015parallel}
Bertsekas, D. and Tsitsiklis, J.
\newblock \emph{Parallel and distributed computation: numerical methods}.
\newblock Athena Scientific, 2015.

\bibitem[Bertsimas et~al.(2006)Bertsimas, Natarajan, and
  Teo]{bertsimas2006tight}
Bertsimas, D., Natarajan, K., and Teo, C.-P.
\newblock Tight bounds on expected order statistics.
\newblock \emph{Probability in the Engineering and Informational Sciences},
  20\penalty0 (4):\penalty0 667--686, 2006.

\bibitem[Blanchard et~al.(2017)Blanchard, El~Mhamdi, Guerraoui, and
  Stainer]{krum}
Blanchard, P., El~Mhamdi, E.~M., Guerraoui, R., and Stainer, J.
\newblock Machine learning with adversaries: Byzantine tolerant gradient
  descent.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30}, pp.\  119--129. Curran Associates, Inc.,
  2017.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{Siam Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
Boyd, S., Boyd, S.~P., and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Cao \& Lai(2019)Cao and Lai]{cao2019distributed}
Cao, X. and Lai, L.
\newblock Distributed gradient descent algorithm robust to an arbitrary number
  of byzantine attackers.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (22):\penalty0 5850--5864, 2019.

\bibitem[Charikar et~al.(2017)Charikar, Steinhardt, and
  Valiant]{charikar2017learning}
Charikar, M., Steinhardt, J., and Valiant, G.
\newblock Learning from untrusted data.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  47--60, 2017.

\bibitem[Chen et~al.(2018)Chen, Wang, Charles, and Papailiopoulos]{redundancy}
Chen, L., Wang, H., Charles, Z.~B., and Papailiopoulos, D.~S.
\newblock {DRACO:} byzantine-resilient distributed training via redundant
  gradients.
\newblock In Dy, J.~G. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  902--911. {PMLR},
  2018.

\bibitem[Chen et~al.(2017)Chen, Su, and Xu]{chen2017distributed}
Chen, Y., Su, L., and Xu, J.
\newblock Distributed statistical machine learning in adversarial settings:
  Byzantine gradient descent.
\newblock \emph{Proceedings of the ACM on Measurement and Analysis of Computing
  Systems}, 1\penalty0 (2):\penalty0 1--25, 2017.

\bibitem[Cohen et~al.(2016)Cohen, Lee, Miller, Pachocki, and
  Sidford]{cohen2016geometric}
Cohen, M.~B., Lee, Y.~T., Miller, G., Pachocki, J., and Sidford, A.
\newblock Geometric median in nearly linear time.
\newblock In \emph{Proceedings of the forty-eighth annual ACM symposium on
  Theory of Computing}, pp.\  9--21, 2016.

\bibitem[Cutkosky \& Orabona(2019)Cutkosky and Orabona]{cutkosky2019momentum}
Cutkosky, A. and Orabona, F.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 15236--15245, 2019.

\bibitem[Data \& Diggavi(2021)Data and Diggavi]{data2021byzantine_icml}
Data, D. and Diggavi, S.
\newblock Byzantine-resilient high-dimensional sgd with local iterations on
  heterogeneous data.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2478--2488. PMLR, 2021.

\bibitem[Data et~al.(2020)Data, Song, and Diggavi]{data2020data}
Data, D., Song, L., and Diggavi, S.~N.
\newblock Data encoding for byzantine-resilient distributed optimization.
\newblock \emph{IEEE Transactions on Information Theory}, 67\penalty0
  (2):\penalty0 1117--1140, 2020.

\bibitem[Diakonikolas et~al.(2017)Diakonikolas, Kane, and
  Stewart]{diakonikolas2017statistical}
Diakonikolas, I., Kane, D.~M., and Stewart, A.
\newblock Statistical query lower bounds for robust estimation of
  high-dimensional gaussians and gaussian mixtures.
\newblock In \emph{2017 IEEE 58th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pp.\  73--84. IEEE, 2017.

\bibitem[Diakonikolas et~al.(2019{\natexlab{a}})Diakonikolas, Kamath, Kane, Li,
  Moitra, and Stewart]{diakonikolas2019robust}
Diakonikolas, I., Kamath, G., Kane, D., Li, J., Moitra, A., and Stewart, A.
\newblock Robust estimators in high-dimensions without the computational
  intractability.
\newblock \emph{SIAM Journal on Computing}, 48\penalty0 (2):\penalty0 742--864,
  2019{\natexlab{a}}.

\bibitem[Diakonikolas et~al.(2019{\natexlab{b}})Diakonikolas, Kamath, Kane, Li,
  Steinhardt, and Stewart]{diakonikolas2019sever}
Diakonikolas, I., Kamath, G., Kane, D., Li, J., Steinhardt, J., and Stewart, A.
\newblock Sever: A robust meta-algorithm for stochastic optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1596--1606. PMLR, 2019{\natexlab{b}}.

\bibitem[El~Mhamdi et~al.(2018)El~Mhamdi, Guerraoui, and Rouault]{brute_bulyan}
El~Mhamdi, E.~M., Guerraoui, R., and Rouault, S.
\newblock The hidden vulnerability of distributed learning in byzantium, 2018.

\bibitem[El~Mhamdi et~al.(2020)El~Mhamdi, Guerraoui, Guirguis, Hoang, and
  Rouault]{el2020genuinely}
El~Mhamdi, E.~M., Guerraoui, R., Guirguis, A., Hoang, L.~N., and Rouault, S.
\newblock Genuinely distributed byzantine machine learning.
\newblock In \emph{Proceedings of the 39th Symposium on Principles of
  Distributed Computing}, pp.\  355--364, 2020.

\bibitem[El~Mhamdi et~al.(2021{\natexlab{a}})El~Mhamdi, Farhadkhani, Guerraoui,
  Guirguis, Hoang, and Rouault]{collaborativeElMhamdi21}
El~Mhamdi, E.~M., Farhadkhani, S., Guerraoui, R., Guirguis, A., Hoang, L.~N.,
  and Rouault, S.
\newblock Collaborative learning in the jungle (decentralized, byzantine,
  heterogeneous, asynchronous and nonconvex learning).
\newblock In \emph{Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021{\natexlab{a}}.

\bibitem[El~Mhamdi et~al.(2021{\natexlab{b}})El~Mhamdi, Guerraoui, and
  Rouault]{distributed-momentum}
El~Mhamdi, E.~M., Guerraoui, R., and Rouault, S.
\newblock Distributed momentum for byzantine-resilient stochastic gradient
  descent.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Vienna, Austria, May 4–8, 2021}. OpenReview.net,
  2021{\natexlab{b}}.

\bibitem[Feng et~al.(2015)Feng, Xu, and Mannor]{feng2015distributed}
Feng, J., Xu, H., and Mannor, S.
\newblock Distributed robust learning, 2015.

\bibitem[Feng et~al.(2017)Feng, Xu, and Mannor]{feng2017onlinelearning}
Feng, J., Xu, H., and Mannor, S.
\newblock Outlier robust online learning.
\newblock \emph{CoRR}, abs/1701.00251, 2017.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Gupta \& Vaidya(2019)Gupta and Vaidya]{gupta2019randomized}
Gupta, N. and Vaidya, N.~H.
\newblock Randomized reactive redundancy for byzantine fault-tolerance in
  parallelized learning.
\newblock \emph{arXiv preprint arXiv:1912.09528}, 2019.

\bibitem[Gupta \& Vaidya(2020)Gupta and Vaidya]{gupta2020fault}
Gupta, N. and Vaidya, N.~H.
\newblock Fault-tolerance in distributed optimization: The case of redundancy.
\newblock In \emph{Proceedings of the 39th Symposium on Principles of
  Distributed Computing}, pp.\  365--374, 2020.

\bibitem[Gupta et~al.(2021)Gupta, Liu, and Vaidya]{gupta2021byzantine}
Gupta, N., Liu, S., and Vaidya, N.
\newblock Byzantine fault-tolerant distributed machine learning with norm-based
  comparative gradient elimination.
\newblock In \emph{2021 51st Annual IEEE/IFIP International Conference on
  Dependable Systems and Networks Workshops (DSN-W)}, pp.\  175--181. IEEE,
  2021.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, Avent, Bellet, Bennis, Bhagoji,
  Bonawitz, Charles, Cormode, Cummings, D’Oliveira, Eichner, Rouayheb, Evans,
  Gardner, Garrett, Gascón, Ghazi, Gibbons, Gruteser, Harchaoui, He, He, Huo,
  Hutchinson, Hsu, Jaggi, Javidi, Joshi, Khodak, Konecný, Korolova,
  Koushanfar, Koyejo, Lepoint, Liu, Mittal, Mohri, Nock, Özgür, Pagh, Qi,
  Ramage, Raskar, Raykova, Song, Song, Stich, Sun, Suresh, Tramèr, Vepakomma,
  Wang, Xiong, Xu, Yang, Yu, Yu, and Zhao]{OpenProblemsinFed2021}
Kairouz, P., McMahan, H.~B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.~N.,
  Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., D’Oliveira, R. G.~L.,
  Eichner, H., Rouayheb, S.~E., Evans, D., Gardner, J., Garrett, Z., Gascón,
  A., Ghazi, B., Gibbons, P.~B., Gruteser, M., Harchaoui, Z., He, C., He, L.,
  Huo, Z., Hutchinson, B., Hsu, J., Jaggi, M., Javidi, T., Joshi, G., Khodak,
  M., Konecný, J., Korolova, A., Koushanfar, F., Koyejo, S., Lepoint, T., Liu,
  Y., Mittal, P., Mohri, M., Nock, R., Özgür, A., Pagh, R., Qi, H., Ramage,
  D., Raskar, R., Raykova, M., Song, D., Song, W., Stich, S.~U., Sun, Z.,
  Suresh, A.~T., Tramèr, F., Vepakomma, P., Wang, J., Xiong, L., Xu, Z., Yang,
  Q., Yu, F.~X., Yu, H., and Zhao, S.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends® in Machine Learning}, 14\penalty0
  (1–2):\penalty0 1--210, 2021.
\newblock ISSN 1935-8237.
\newblock \doi{10.1561/2200000083}.

\bibitem[Karimireddy et~al.(2020)Karimireddy, He, and
  Jaggi]{karimireddy2020byzantine}
Karimireddy, S.~P., He, L., and Jaggi, M.
\newblock Byzantine-robust learning on heterogeneous datasets via bucketing.
\newblock \emph{arXiv preprint arXiv:2006.09365}, 2020.

\bibitem[Karimireddy et~al.(2021)Karimireddy, He, and Jaggi]{Karimireddy2021}
Karimireddy, S.~P., He, L., and Jaggi, M.
\newblock Learning from history for byzantine robust optimization.
\newblock \emph{International Conference On Machine Learning, Vol 139}, 139,
  2021.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Nair, and Hinton]{cifar}
Krizhevsky, A., Nair, V., and Hinton, G.
\newblock Cifar-100 (canadian institute for advanced research).
\newblock 2009.

\bibitem[Lai et~al.(2016)Lai, Rao, and Vempala]{lai2016agnostic}
Lai, K.~A., Rao, A.~B., and Vempala, S.
\newblock Agnostic estimation of mean and covariance.
\newblock In \emph{2016 IEEE 57th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pp.\  665--674. IEEE, 2016.

\bibitem[Lamport et~al.(1982)Lamport, Shostak, and Pease]{ByzProblem}
Lamport, L., Shostak, R., and Pease, M.
\newblock The byzantine generals problem.
\newblock \emph{ACM Trans. Program. Lang. Syst.}, 4\penalty0 (3):\penalty0
  382–401, July 1982.
\newblock ISSN 0164-0925.
\newblock \doi{10.1145/357172.357176}.

\bibitem[LeCun \& Cortes(2010)LeCun and Cortes]{mnist}
LeCun, Y. and Cortes, C.
\newblock {MNIST} handwritten digit database.
\newblock 2010.

\bibitem[Lei et~al.(2019)Lei, Hu, Li, and Tang]{lei2019stochastic}
Lei, Y., Hu, T., Li, G., and Tang, K.
\newblock Stochastic gradient descent for nonconvex learning without bounded
  gradient assumptions.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  31\penalty0 (10):\penalty0 4394--4400, 2019.

\bibitem[Li et~al.(2019)Li, Xu, Chen, Giannakis, and Ling]{li2019rsa}
Li, L., Xu, W., Chen, T., Giannakis, G.~B., and Ling, Q.
\newblock Rsa: Byzantine-robust stochastic aggregation methods for distributed
  learning from heterogeneous datasets.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pp.\  1544--1551, 2019.

\bibitem[Lian et~al.(2015)Lian, Huang, Li, and Liu]{lian2015asynchronous}
Lian, X., Huang, Y., Li, Y., and Liu, J.
\newblock Asynchronous parallel stochastic gradient for nonconvex optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  28:\penalty0 2737--2745, 2015.

\bibitem[Liu et~al.(2021)Liu, Gupta, and Vaidya]{liu2021approximate}
Liu, S., Gupta, N., and Vaidya, N.~H.
\newblock Approximate byzantine fault-tolerance in distributed optimization.
\newblock In \emph{Proceedings of the 2021 ACM Symposium on Principles of
  Distributed Computing}, PODC'21, pp.\  379–389, New York, NY, USA, 2021.
  Association for Computing Machinery.
\newblock ISBN 9781450385480.
\newblock \doi{10.1145/3465084.3467902}.

\bibitem[Lynch(1996)]{lynch1996distributed}
Lynch, N.~A.
\newblock \emph{Distributed algorithms}.
\newblock Elsevier, 1996.

\bibitem[Mhamdi et~al.(2021)Mhamdi, Farhadkhani, Guerraoui, and
  Hoang]{Geomed21}
Mhamdi, E. M.~E., Farhadkhani, S., Guerraoui, R., and Hoang, L.~N.
\newblock On the strategyproofness of the geometric median.
\newblock \emph{CoRR}, abs/2106.02394, 2021.

\bibitem[Pillutla et~al.(2019)Pillutla, Kakade, and
  Harchaoui]{pillutla2019robust}
Pillutla, K., Kakade, S.~M., and Harchaoui, Z.
\newblock Robust aggregation for federated learning, 2019.

\bibitem[Polyak(1964)]{momentum}
Polyak, B.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  4:\penalty0 1--17, 12 1964.
\newblock \doi{10.1016/0041-5553(64)90137-5}.

\bibitem[Prasad et~al.(2020)Prasad, Suggala, Balakrishnan, and
  Ravikumar]{prasad2020robust}
Prasad, A., Suggala, A.~S., Balakrishnan, S., and Ravikumar, P.
\newblock Robust estimation via robust gradient estimation.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 82\penalty0 (3):\penalty0 601--627, 2020.

\bibitem[Rajput et~al.(2019)Rajput, Wang, Charles, and
  Papailiopoulos]{rajput2019detox}
Rajput, S., Wang, H., Charles, Z., and Papailiopoulos, D.
\newblock Detox: A redundancy-based framework for faster and more robust
  gradient aggregation.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Regatti et~al.(2020)Regatti, Chen, and Gupta]{regatti2020bygars}
Regatti, J., Chen, H., and Gupta, A.
\newblock Bygars: Byzantine sgd with arbitrary number of attackers.
\newblock \emph{arXiv preprint arXiv:2006.13421}, 2020.

\bibitem[Rousseeuw(1985)]{rousseeuw1985multivariate}
Rousseeuw, P.~J.
\newblock Multivariate estimation with high breakdown point.
\newblock \emph{Mathematical statistics and applications}, 8\penalty0
  (37):\penalty0 283--297, 1985.

\bibitem[Steinhardt et~al.(2018)Steinhardt, Charikar, and
  Valiant]{steinhardt2018resilience}
Steinhardt, J., Charikar, M., and Valiant, G.
\newblock Resilience: A criterion for learning in the presence of arbitrary
  outliers.
\newblock In \emph{9th Innovations in Theoretical Computer Science Conference
  (ITCS 2018)}. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018.

\bibitem[Su \& Shahrampour(2019)Su and Shahrampour]{su2019finite}
Su, L. and Shahrampour, S.
\newblock Finite-time guarantees for byzantine-resilient distributed state
  estimation with noisy measurements.
\newblock \emph{IEEE Transactions on Automatic Control}, 65\penalty0
  (9):\penalty0 3758--3771, 2019.

\bibitem[Su \& Vaidya(2016)Su and Vaidya]{su2016fault}
Su, L. and Vaidya, N.~H.
\newblock Fault-tolerant multi-agent optimization: optimal iterative
  distributed algorithms.
\newblock In \emph{Proceedings of the 2016 ACM symposium on principles of
  distributed computing}, pp.\  425--434, 2016.

\bibitem[Tsitsiklis et~al.(1986)Tsitsiklis, Bertsekas, and
  Athans]{tsitsiklis1986distributed}
Tsitsiklis, J., Bertsekas, D., and Athans, M.
\newblock Distributed asynchronous deterministic and stochastic gradient
  optimization algorithms.
\newblock \emph{IEEE transactions on automatic control}, 31\penalty0
  (9):\penalty0 803--812, 1986.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{fashion-mnist}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Xie et~al.(2018)Xie, Koyejo, and Gupta]{meamed}
Xie, C., Koyejo, O., and Gupta, I.
\newblock Generalized byzantine-tolerant sgd, 2018.

\bibitem[Xie et~al.(2019{\natexlab{a}})Xie, Koyejo, and Gupta]{empire}
Xie, C., Koyejo, O., and Gupta, I.
\newblock Fall of empires: Breaking byzantine-tolerant {SGD} by inner product
  manipulation.
\newblock In \emph{Proceedings of the Thirty-Fifth Conference on Uncertainty in
  Artificial Intelligence, {UAI} 2019, Tel Aviv, Israel, July 22-25, 2019},
  pp.\ ~83, 2019{\natexlab{a}}.

\bibitem[Xie et~al.(2019{\natexlab{b}})Xie, Koyejo, and Gupta]{suspicion}
Xie, C., Koyejo, S., and Gupta, I.
\newblock Zeno: Distributed stochastic gradient descent with suspicion-based
  fault-tolerance.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June
  2019, Long Beach, California, {USA}}, volume~97 of \emph{Proceedings of
  Machine Learning Research}, pp.\  6893--6901. {PMLR}, 2019{\natexlab{b}}.

\bibitem[Xie et~al.(2020)Xie, Koyejo, and Gupta]{xie2020zeno++}
Xie, C., Koyejo, S., and Gupta, I.
\newblock Zeno++: Robust fully asynchronous sgd.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10495--10503. PMLR, 2020.

\bibitem[Yao et~al.(2019)Yao, Huang, Zhang, Li, and Sun]{yao2019federated}
Yao, X., Huang, T., Zhang, R.-X., Li, R., and Sun, L.
\newblock Federated learning with unbiased gradient aggregation and
  controllable meta updating.
\newblock \emph{arXiv preprint arXiv:1910.08234}, 2019.

\bibitem[Yin et~al.(2018)Yin, Chen, Kannan, and Bartlett]{yin2018byzantine}
Yin, D., Chen, Y., Kannan, R., and Bartlett, P.
\newblock Byzantine-robust distributed learning: Towards optimal statistical
  rates.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5650--5659. PMLR, 2018.

\bibitem[Yin et~al.(2019)Yin, Chen, Kannan, and Bartlett]{yin2019defending}
Yin, D., Chen, Y., Kannan, R., and Bartlett, P.
\newblock Defending against saddle point attack in byzantine-robust distributed
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7074--7084. PMLR, 2019.

\end{thebibliography}
