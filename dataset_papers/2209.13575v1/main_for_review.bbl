\begin{thebibliography}{10}

\bibitem{sr}
Miles Cranmer.
\newblock Pysr: Fast \& parallelized symbolic regression in python/julia,
  september 2020.
\newblock {\em URL https://doi. org/10.5281/zenodo}, 4052869.

\bibitem{nofreelunch}
David~H Wolpert and William~G Macready.
\newblock No free lunch theorems for optimization.
\newblock {\em IEEE transactions on evolutionary computation}, 1(1):67--82,
  1997.

\bibitem{citeseer}
C~Lee Giles, Kurt~D Bollacker, and Steve Lawrence.
\newblock Citeseer: An automatic citation indexing system.
\newblock In {\em Proceedings of the third ACM conference on Digital
  libraries}, pages 89--98, 1998.

\bibitem{cora}
Andrew~Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore.
\newblock Automating the construction of internet portals with machine
  learning.
\newblock {\em Information Retrieval}, 3(2):127--163, 2000.

\bibitem{mrpc}
Bill Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In {\em Third International Workshop on Paraphrasing (IWP2005)},
  2005.

\bibitem{pubmed}
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher,
  and Tina Eliassi-Rad.
\newblock Collective classification in network data.
\newblock {\em AI magazine}, 29(3):93--93, 2008.

\bibitem{rte}
Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo.
\newblock The fifth pascal recognizing textual entailment challenge.
\newblock In {\em TAC}, 2009.

\bibitem{cifar10}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{l2lgd2}
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew~W Hoffman, David Pfau,
  Tom Schaul, Brendan Shillingford, and Nando De~Freitas.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{l2o}
Ke~Li and Jitendra Malik.
\newblock Learning to optimize.
\newblock {\em arXiv preprint arXiv:1606.01885}, 2016.

\bibitem{nsp}
Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong
  Zhou, and Pushmeet Kohli.
\newblock Neuro-symbolic program synthesis.
\newblock {\em arXiv preprint arXiv:1611.01855}, 2016.

\bibitem{wideresnet}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock {\em arXiv preprint arXiv:1605.07146}, 2016.

\bibitem{nos}
Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc~V Le.
\newblock Neural optimizer search with reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  459--468. PMLR, 2017.

\bibitem{sts}
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia.
\newblock Semeval-2017 task 1: Semantic textual similarity-multilingual and
  cross-lingual focused evaluation.
\newblock {\em arXiv preprint arXiv:1708.00055}, 2017.

\bibitem{l2lngd}
Yutian Chen, Matthew~W Hoffman, Sergio~G{\'o}mez Colmenarejo, Misha Denil,
  Timothy~P Lillicrap, Matt Botvinick, and Nando Freitas.
\newblock Learning to learn without gradient descent by gradient descent.
\newblock In {\em International Conference on Machine Learning}, pages
  748--756. PMLR, 2017.

\bibitem{ppi}
Will Hamilton, Zhitao Ying, and Jure Leskovec.
\newblock Inductive representation learning on large graphs.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{pgd}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock {\em arXiv preprint arXiv:1706.06083}, 2017.

\bibitem{gat}
Petar Veli{\v{c}}kovi{\'c}, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
  Pietro Lio, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock {\em arXiv preprint arXiv:1710.10903}, 2017.

\bibitem{nas}
Barret Zoph and Quoc~V. Le.
\newblock Neural architecture search with reinforcement learning.
\newblock In {\em ICLR}, 2017.

\bibitem{houdini}
Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, and Swarat
  Chaudhuri.
\newblock Houdini: Lifelong learning as program synthesis.
\newblock {\em Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem{glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{snas}
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin.
\newblock Snas: stochastic neural architecture search.
\newblock {\em arXiv preprint arXiv:1812.09926}, 2018.

\bibitem{carmon2019}
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John~C Duchi, and Percy~S
  Liang.
\newblock Unlabeled data improves adversarial robustness.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{clustergcn}
Wei-Lin Chiang, Xuanqing Liu, Si~Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh.
\newblock Cluster-gcn: An efficient algorithm for training deep and large graph
  convolutional networks.
\newblock In {\em Proceedings of the 25th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 257--266, 2019.

\bibitem{gdas}
Xuanyi Dong and Yi~Yang.
\newblock Searching for a robust neural architecture in four gpu hours.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1761--1770, 2019.

\bibitem{repl}
Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando
  Solar-Lezama.
\newblock Write, execute, assess: Program synthesis with a repl.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{engstrom2019}
Logan Engstrom, Andrew Ilyas, Hadi Salman, Shibani Santurkar, and Dimitris
  Tsipras.
\newblock Robustness (python library), 2019.

\bibitem{nasdetect}
Golnaz Ghiasi, Tsung-Yi Lin, and Quoc~V Le.
\newblock Nas-fpn: Learning scalable feature pyramid architecture for object
  detection.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 7036--7045, 2019.

\bibitem{nasnlp}
Yufan Jiang, Chi Hu, Tong Xiao, Chunliang Zhang, and Jingbo Zhu.
\newblock Improved differentiable architecture search for language modeling and
  named entity recognition.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 3585--3590, 2019.

\bibitem{bert}
Jacob Devlin Ming-Wei~Chang Kenton and Lee~Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of NAACL-HLT}, pages 4171--4186, 2019.

\bibitem{symbolic-math}
Guillaume Lample and Fran{\c{c}}ois Charton.
\newblock Deep learning for symbolic mathematics.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{autoloss}
Chuming Li, Xin Yuan, Chen Lin, Minghao Guo, Wei Wu, Junjie Yan, and Wanli
  Ouyang.
\newblock Am-lfs: Automl for loss function search.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 8410--8419, 2019.

\bibitem{fastautoaug}
Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim.
\newblock Fast autoaugment.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{autodeeplab}
Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan~L
  Yuille, and Li~Fei-Fei.
\newblock Auto-deeplab: Hierarchical neural architecture search for semantic
  image segmentation.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 82--92, 2019.

\bibitem{autoaug}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, 2019.

\bibitem{darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock {DARTS}: Differentiable architecture search.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{metz2019understanding}
Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha
  Sohl-Dickstein.
\newblock Understanding and correcting pathologies in the training of learned
  optimizers.
\newblock In {\em International Conference on Machine Learning}, pages
  4556--4565. PMLR, 2019.

\bibitem{wang2020}
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu.
\newblock Improving adversarial robustness requires revisiting misclassified
  examples.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{cola}
Alex Warstadt, Amanpreet Singh, and Samuel~R Bowman.
\newblock Neural network acceptability judgments.
\newblock {\em Transactions of the Association for Computational Linguistics},
  7:625--641, 2019.

\bibitem{adabert}
Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo
  Deng, Jun Huang, Wei Lin, and Jingren Zhou.
\newblock Adabert: Task-adaptive bert compression with differentiable neural
  architecture search.
\newblock {\em arXiv preprint arXiv:2001.04246}, 2020.

\bibitem{robustbench}
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti,
  Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein.
\newblock Robustbench: a standardized adversarial robustness benchmark.
\newblock {\em arXiv preprint arXiv:2010.09670}, 2020.

\bibitem{autoattack}
Francesco Croce and Matthias Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In {\em ICML}, 2020.

\bibitem{gowal2020}
Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli.
\newblock Uncovering the limits of adversarial training against norm-bounded
  adversarial examples.
\newblock {\em arXiv preprint arXiv:2010.03593}, 2020.

\bibitem{ogbn}
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
  Michele Catasta, and Jure Leskovec.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock {\em Advances in neural information processing systems},
  33:22118--22133, 2020.

\bibitem{nasspeech}
Abhinav Mehrotra, Alberto Gil~CP Ramos, Sourav Bhattacharya, {\L}ukasz Dudziak,
  Ravichander Vipperla, Thomas Chau, Mohamed~S Abdelfattah, Samin Ishtiaq, and
  Nicholas~Donald Lane.
\newblock Nas-bench-asr: Reproducible neural architecture search for speech
  recognition.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{automl-zero}
Esteban Real, Chen Liang, David So, and Quoc Le.
\newblock Automl-zero: Evolving machine learning algorithms from scratch.
\newblock In {\em International Conference on Machine Learning}, pages
  8007--8019. PMLR, 2020.

\bibitem{sehwag2020}
Vikash Sehwag, Shiqi Wang, Prateek Mittal, and Suman Jana.
\newblock Hydra: Pruning adversarially robust neural networks.
\newblock {\em Advances in Neural Information Processing Systems},
  33:19655--19666, 2020.

\bibitem{near}
Ameesh Shah, Eric Zhan, Jennifer Sun, Abhinav Verma, Yisong Yue, and Swarat
  Chaudhuri.
\newblock Learning differentiable programs with admissible neural heuristics.
\newblock {\em Advances in neural information processing systems},
  33:4940--4952, 2020.

\bibitem{feynman}
Silviu-Marian Udrescu and Max Tegmark.
\newblock Ai feynman: A physics-inspired method for symbolic regression.
\newblock {\em Science Advances}, 6(16):eaay2631, 2020.

\bibitem{dartspt}
Ruochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, and Cho-Jui Hsieh.
\newblock Rethinking architecture selection in differentiable nas.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{huggingface}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online,
  October 2020. Association for Computational Linguistics.

\bibitem{wong2020}
Eric Wong, Leslie Rice, and J~Zico Kolter.
\newblock Fast is better than free: Revisiting adversarial training.
\newblock {\em arXiv preprint arXiv:2001.03994}, 2020.

\bibitem{wu2020}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock In {\em NeurIPS}, 2020.

\bibitem{l2obench}
Tianlong Chen, Xiaohan Chen, Wuyang Chen, Howard Heaton, Jialin Liu, Zhangyang
  Wang, and Wotao Yin.
\newblock Learning to optimize: A primer and a benchmark.
\newblock {\em arXiv preprint arXiv:2103.12828}, 2021.

\bibitem{dpads}
Guofeng Cui and He~Zhu.
\newblock Differentiable synthesis of program architectures.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{gmnas}
Shoukang Hu, Ruochen Wang, HONG Lanqing, Zhenguo Li, Cho-Jui Hsieh, and Jiashi
  Feng.
\newblock Generalizing few-shot nas with gradient matching.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{symbolicRL}
Mikel Landajuela, Brenden~K Petersen, Sookyung Kim, Claudio~P Santiago, Ruben
  Glatt, Nathan Mundhenk, Jacob~F Pettit, and Daniel Faissol.
\newblock Discovering symbolic policies with deep reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  5979--5989. PMLR, 2021.

\bibitem{deepsr}
Brenden~K Petersen, Mikel Landajuela, T~Nathan Mundhenk, Claudio~P Santiago,
  Soo~K Kim, and Joanne~T Kim.
\newblock Deep symbolic regression: Recovering mathematical expressions from
  data via risk-seeking policy gradients.
\newblock In {\em Proc. of the International Conference on Learning
  Representations}, 2021.

\bibitem{xd}
Nicholas Roberts, Mikhail Khodak, Tri Dao, Liam Li, Christopher R{\'e}, and
  Ameet Talwalkar.
\newblock Rethinking neural operations for diverse tasks.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{vicol2021unbiased}
Paul Vicol, Luke Metz, and Jascha Sohl-Dickstein.
\newblock Unbiased gradient estimation in unrolled computation graphs with
  persistent evolution strategies.
\newblock In {\em International Conference on Machine Learning}, pages
  10553--10563. PMLR, 2021.

\bibitem{symbolicl2o}
Wenqing Zheng, Tianlong Chen, Ting-Kuei Hu, and Zhangyang Wang.
\newblock Symbolic learning to optimize: Towards interpretability and
  scalability.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{automl-opt}
Xiangning Chen, Chen Liang, Da~Huang, Esteban Real, Yao Liu, Kaiyuan Wang,
  Cho-Jui Hsieh, Yifeng Lu, and Quoc~V Le.
\newblock Evolved optimizer for visio.
\newblock In {\em AutoML 2022 Workshop}, 2022.

\bibitem{mc}
Yu~A Shreider.
\newblock {\em The Monte Carlo method: the method of statistical trials},
  volume~87.
\newblock Elsevier, 2014.

\end{thebibliography}
