\begin{thebibliography}{10}

\bibitem{ahn2022can}
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et~al.
\newblock Do as i can, not as i say: Grounding language in robotic affordances.
\newblock {\em arXiv preprint arXiv:2204.01691}, 2022.

\bibitem{angelopoulos2021gentle}
Anastasios~N Angelopoulos and Stephen Bates.
\newblock A gentle introduction to conformal prediction and distribution-free uncertainty quantification.
\newblock {\em arXiv preprint arXiv:2107.07511}, 2021.

\bibitem{angelopoulos2022conformal}
Anastasios~N Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, and Tal Schuster.
\newblock Conformal risk control.
\newblock {\em arXiv preprint arXiv:2208.02814}, 2022.

\bibitem{baan2023uncertainty}
Joris Baan, Nico Daheim, Evgenia Ilia, Dennis Ulmer, Haau-Sing Li, Raquel Fern{\'a}ndez, Barbara Plank, Rico Sennrich, Chrysoula Zerva, and Wilker Aziz.
\newblock Uncertainty in natural language generation: From theory to applications.
\newblock {\em arXiv preprint arXiv:2307.15703}, 2023.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{chang2023prompting}
Edward~Y Chang.
\newblock Prompting large language models with the socratic method.
\newblock In {\em 2023 IEEE 13th Annual Computing and Communication Workshop and Conference (CCWC)}, pages 0351--0360. IEEE, 2023.

\bibitem{desai2020calibration}
Shrey Desai and Greg Durrett.
\newblock Calibration of pre-trained transformers.
\newblock {\em arXiv preprint arXiv:2003.07892}, 2020.

\bibitem{fomicheva2020unsupervised}
Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Fr{\'e}d{\'e}ric Blain, Francisco Guzm{\'a}n, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia.
\newblock Unsupervised quality estimation for neural machine translation.
\newblock {\em Transactions of the Association for Computational Linguistics}, 8:539--555, 2020.

\bibitem{gao-2021-prompting}
Tianyu Gao, Adam Fisch, and Danqi Chen.
\newblock Making pre-trained language models better few-shot learners.
\newblock In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 3816--3830, Online, August 2021. Association for Computational Linguistics.

\bibitem{guu2020retrieval}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang.
\newblock Retrieval augmented language model pre-training.
\newblock In {\em International conference on machine learning}, pages 3929--3938. PMLR, 2020.

\bibitem{huang2022language}
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
\newblock Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
\newblock In {\em International Conference on Machine Learning}, pages 9118--9147. PMLR, 2022.

\bibitem{huang2022inner}
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et~al.
\newblock Inner monologue: Embodied reasoning through planning with language models.
\newblock {\em arXiv preprint arXiv:2207.05608}, 2022.

\bibitem{ji2023hallucination_survey}
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye~Jin Bang, Andrea Madotto, and Pascale Fung.
\newblock Survey of hallucination in natural language generation.
\newblock {\em ACM Computing Surveys}, 55(12):1--38, 2023.

\bibitem{ji2023survey}
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye~Jin Bang, Andrea Madotto, and Pascale Fung.
\newblock Survey of hallucination in natural language generation.
\newblock {\em ACM Computing Surveys}, 55(12):1--38, 2023.

\bibitem{jiang2021can}
Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig.
\newblock How can we know when language models know? on the calibration of language models for question answering.
\newblock {\em Transactions of the Association for Computational Linguistics}, 9:962--977, 2021.

\bibitem{jiang2023active}
Zhengbao Jiang, Frank~F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig.
\newblock Active retrieval augmented generation.
\newblock {\em arXiv preprint arXiv:2305.06983}, 2023.

\bibitem{kambhampati2024llms}
Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Kaya Stechly, Mudit Verma, Siddhant Bhambri, Lucas Saldyt, and Anil Murthy.
\newblock Llms can't plan, but can help planning in llm-modulo frameworks.
\newblock {\em arXiv preprint arXiv:2402.01817}, 2024.

\bibitem{kojima2022zerocot}
Takeshi Kojima, Shixiang~Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.
\newblock Large language models are zero-shot reasoners.
\newblock {\em Advances in neural information processing systems}, 35:22199--22213, 2022.

\bibitem{Leake2012introspective}
David~B. Leake.
\newblock {\em Introspective Learning and Reasoning}, pages 1638--1640.
\newblock Springer US, Boston, MA, 2012.

\bibitem{lewis2020retrieval}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K{\"u}ttler, Mike Lewis, Wen-tau Yih, Tim Rockt{\"a}schel, et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock {\em Advances in Neural Information Processing Systems}, 33:9459--9474, 2020.

\bibitem{liang2023code}
Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng.
\newblock Code as policies: Language model programs for embodied control.
\newblock In {\em 2023 IEEE International Conference on Robotics and Automation (ICRA)}, pages 9493--9500. IEEE, 2023.

\bibitem{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock {\em arXiv preprint arXiv:2109.07958}, 2021.

\bibitem{liu2023reflect}
Zeyi Liu, Arpit Bahety, and Shuran Song.
\newblock Reflect: Summarizing robot experiences for failure explanation and correction.
\newblock {\em arXiv preprint arXiv:2306.15724}, 2023.

\bibitem{madaan2023self}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock {\em arXiv preprint arXiv:2303.17651}, 2023.

\bibitem{mallen2022not}
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi.
\newblock When not to trust language models: Investigating effectiveness of parametric and non-parametric memories.
\newblock {\em arXiv preprint arXiv:2212.10511}, 2022.

\bibitem{min2022rethinking}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning work?
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing}, pages 11048--11064, 2022.

\bibitem{ott2018analyzing}
Myle Ott, Michael Auli, David Grangier, and Marcâ€™Aurelio Ranzato.
\newblock Analyzing uncertainty in neural machine translation.
\newblock In {\em International Conference on Machine Learning}, pages 3956--3965. PMLR, 2018.

\bibitem{paul2023refiner}
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings.
\newblock Refiner: Reasoning feedback on intermediate representations.
\newblock {\em arXiv preprint arXiv:2304.01904}, 2023.

\bibitem{ram2023context}
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
\newblock In-context retrieval-augmented language models.
\newblock {\em Transactions of the Association for Computational Linguistics}, 11:1316--1331, 2023.

\bibitem{reimers2019sentence}
Nils Reimers and Iryna Gurevych.
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock {\em arXiv preprint arXiv:1908.10084}, 2019.

\bibitem{knowno2023}
Allen~Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, and Anirudha Majumdar.
\newblock Robots that ask for help: Uncertainty alignment for large language model planners.
\newblock In {\em arXiv preprint arXiv:2307.01928}, 2023.

\bibitem{sadinle2019least}
Mauricio Sadinle, Jing Lei, and Larry Wasserman.
\newblock Least ambiguous set-valued classifiers with bounded error levels.
\newblock {\em Journal of the American Statistical Association}, 114(525):223--234, 2019.

\bibitem{schick2024toolformer}
Timo Schick, Jane Dwivedi-Yu, Roberto Dess{\`\i}, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{shinn2023reflexion}
Noah Shinn, Beck Labash, and Ashwin Gopinath.
\newblock Reflexion: an autonomous agent with dynamic memory and self-reflection.
\newblock {\em arXiv preprint arXiv:2303.11366}, 2023.

\bibitem{singh2023progprompt}
Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg.
\newblock Progprompt: Generating situated robot task plans using large language models.
\newblock In {\em 2023 IEEE International Conference on Robotics and Automation (ICRA)}, pages 11523--11530. IEEE, 2023.

\bibitem{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock {\em arXiv preprint arXiv:2206.04615}, 2022.

\bibitem{tamkin2023task}
Alex Tamkin, Kunal Handa, Avash Shrestha, and Noah Goodman.
\newblock Task ambiguity in humans and language models.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2023.

\bibitem{vovk2012conditional}
Vladimir Vovk.
\newblock Conditional validity of inductive conformal predictors.
\newblock In {\em Asian conference on machine learning}, pages 475--490. PMLR, 2012.

\bibitem{wang2023describe}
Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang.
\newblock Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents.
\newblock {\em arXiv preprint arXiv:2302.01560}, 2023.

\bibitem{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed~H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus.
\newblock Emergent abilities of large language models.
\newblock {\em Transactions on Machine Learning Research}, 2022.
\newblock Survey Certification.

\bibitem{wei2022cot}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:24824--24837, 2022.

\bibitem{xiao2022uncertainty}
Yuxin Xiao, Paul~Pu Liang, Umang Bhatt, Willie Neiswanger, Ruslan Salakhutdinov, and Louis-Philippe Morency.
\newblock Uncertainty quantification with pre-trained language models: A large-scale empirical analysis.
\newblock {\em arXiv preprint arXiv:2210.04714}, 2022.

\bibitem{xu2023recomp}
Fangyuan Xu, Weijia Shi, and Eunsol Choi.
\newblock Recomp: Improving retrieval-augmented lms with compression and selective augmentation.
\newblock {\em arXiv preprint arXiv:2310.04408}, 2023.

\bibitem{yao2023tree}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas~L Griffiths, Yuan Cao, and Karthik Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language models.
\newblock {\em arXiv preprint arXiv:2305.10601}, 2023.

\bibitem{yao2022react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
\newblock React: Synergizing reasoning and acting in language models.
\newblock {\em arXiv preprint arXiv:2210.03629}, 2022.

\bibitem{yin2023large}
Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang.
\newblock Do large language models know what they don't know?
\newblock {\em arXiv preprint arXiv:2305.18153}, 2023.

\bibitem{yoran2023making}
Ori Yoran, Tomer Wolfson, Ori Ram, and Jonathan Berant.
\newblock Making retrieval-augmented language models robust to irrelevant context.
\newblock {\em arXiv preprint arXiv:2310.01558}, 2023.

\bibitem{Zeng2023Demo}
Andy Zeng, Brian Ichter, Fei Xia, Ted Xiao, and Vikas Sindhwani.
\newblock {Demonstrating Large Language Models on Robots}.
\newblock In {\em Proceedings of Robotics: Science and Systems}, Daegu, Republic of Korea, July 2023.

\bibitem{zhang2023automatic}
Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola.
\newblock Automatic chain of thought prompting in large language models.
\newblock In {\em The Eleventh International Conference on Learning Representations (ICLR 2023)}, 2023.

\bibitem{zhao2021calibrate}
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.
\newblock Calibrate before use: Improving few-shot performance of language models.
\newblock In {\em International Conference on Machine Learning}, pages 12697--12706. PMLR, 2021.

\bibitem{zhou2023language}
Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang.
\newblock Language agent tree search unifies reasoning acting and planning in language models.
\newblock {\em arXiv preprint arXiv:2310.04406}, 2023.

\end{thebibliography}
