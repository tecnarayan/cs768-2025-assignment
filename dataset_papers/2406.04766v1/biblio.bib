@article{alsabbaghDistributedChargingManagement2019,
  title = {Distributed Charging Management of Multi-Class Electric Vehicles with Different Charging Priorities},
  author = {Alsabbagh, Amro and Yin, He and Ma, Chengbin},
  year = {2019},
  journal = {IET Generation, Transmission \& Distribution},
  volume = {13},
  number = {22},
  pages = {5257--5264},
  urldate = {2024-02-01},
  abstract = {This study proposes distributed energy management approach for charging multi-class electric vehicles (EVs) in community microgrids. The energy management problem is implemented in real-time and represented by a non-cooperative Stackelberg game for the power distribution inside the microgrid. In this game, a battery energy storage system is chosen as a leader and the EVs are designated as followers. The charging power distribution among EVs is tackled in the two cases of `plenty of power' and `lack of power'. The challenging case of `lack of power' occurs when the total charging power is insufficient to meet the need of each EV, such as when weather conditions are unfavourable. A priority factor is included in the EV utility functions to address charging priorities of different classes of EVs in practical scenarios. A consensus-based distributed algorithm is developed later to iteratively reach the Nash equilibrium, i.e. final charging power distribution, among EVs with different charging priorities. Both simulation and experimental results show that the charging power is properly distributed when the predefined charging priorities are followed, particularly in the case of a `lack of power'.},
  copyright = {{\copyright} 2019 The Authors. IET Generation, Transmission \& Distribution published by John Wiley \& Sons, Ltd. on behalf of The Institution of Engineering and Technology},
  langid = {english},
  keywords = {battery energy storage system,battery powered vehicles,charging power distribution,charging priorities,community microgrids,consensus-based distributed algorithm,distributed charging management,distributed power generation,electric vehicle charging,energy management approach,energy management problem,energy management systems,EV utility,game theory,iterative methods,multiclass electric vehicles,noncooperative Stackelberg game,predefined charging priorities,priority factor,total charging power},
  file = {/Users/luweber/Zotero/storage/WJM8XXGG/Alsabbagh et al. - 2019 - Distributed charging management of multi-class ele.pdf;/Users/luweber/Zotero/storage/KT6J9BRC/iet-gtd.2019.html}
}

@book{andersonLAPACKUsersGuide1999,
  title = {{{LAPACK Users}}' {{Guide}}},
  author = {Anderson, E. and Bai, Z. and Bischof, C. and Blackford, L. S. and Demmel, J. and Dongarra, J. and Du Croz, J. and Greenbaum, A. and Hammarling, S. and McKenney, A. and Sorensen, D.},
  year = {1999},
  month = jan,
  series = {Software, {{Environments}}, and {{Tools}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  urldate = {2023-08-01},
  keywords = {C,computer programming,FORTRAN,LAPACK,subroutines}
}

@article{anselmiReinforcementLearningBirth2022,
  title = {Reinforcement {{Learning}} in a {{Birth}} and {{Death Process}}: {{Breaking}} the {{Dependence}} on the {{State Space}}},
  shorttitle = {Reinforcement {{Learning}} in a {{Birth}} and {{Death Process}}},
  author = {Anselmi, Jonatha and Gaujal, Bruno and Rebuffi, Louis-S{\'e}bastien},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {14464--14474},
  urldate = {2023-08-01},
  langid = {english},
  file = {/Users/luweber/Zotero/storage/IZ8GNYN7/Anselmi et al. - 2022 - Reinforcement Learning in a Birth and Death Proces.pdf}
}

@article{aspvallKhachiyanLinearProgramming1980,
  title = {Khachiyan's Linear Programming Algorithm},
  author = {Aspvall, Bengt and Stone, Richard E},
  year = {1980},
  month = mar,
  journal = {Journal of Algorithms},
  volume = {1},
  number = {1},
  pages = {1--13},
  urldate = {2024-05-21},
  abstract = {L. G. Khachiyan's polynomial time algorithm for determining whether a system of linear inequalities is satisfiable is presented together with a proof of its validity. The algorithm can be used to solve linear programs in polynomial time.},
  file = {/Users/luweber/Zotero/storage/LZYK26ZI/0196677480900024.html}
}

@inproceedings{auerLogarithmicOnlineRegret2006,
  title = {Logarithmic {{Online Regret Bounds}} for {{Undiscounted Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Auer, Peter and Ortner, Ronald},
  year = {2006},
  volume = {19},
  pages = {49--56},
  publisher = {MIT Press},
  urldate = {2023-07-25},
  abstract = {We present a learning algorithm for undiscounted reinforcement learning. Our interest lies in bounds for the algorithm's online performance after some finite number of steps. In the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems, we use upper confidence bounds to show that our UCRL algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy.},
  file = {/Users/luweber/Zotero/storage/7VGAMEDQ/Auer et Ortner - 2006 - Logarithmic Online Regret Bounds for Undiscounted .pdf}
}

@misc{azarMinimaxRegretBounds2017,
  title = {Minimax {{Regret Bounds}} for {{Reinforcement Learning}}},
  author = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  year = {2017},
  month = jul,
  number = {arXiv:1703.05449},
  eprint = {1703.05449},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-16},
  abstract = {We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of \${\textbackslash}tilde\{O\}( {\textbackslash}sqrt\{HSAT\} + H{\textasciicircum}2S{\textasciicircum}2A+H{\textbackslash}sqrt\{T\})\$ where \$H\$ is the time horizon, \$S\$ the number of states, \$A\$ the number of actions and \$T\$ the number of time-steps. This result improves over the best previous known bound \${\textbackslash}tilde\{O\}(HS {\textbackslash}sqrt\{AT\})\$ achieved by the UCRL2 algorithm of Jaksch et al., 2010. The key significance of our new results is that when \$T{\textbackslash}geq H{\textasciicircum}3S{\textasciicircum}3A\$ and \$SA{\textbackslash}geq H\$, it leads to a regret of \${\textbackslash}tilde\{O\}({\textbackslash}sqrt\{HSAT\})\$ that matches the established lower bound of \${\textbackslash}Omega({\textbackslash}sqrt\{HSAT\})\$ up to a logarithmic factor. Our analysis contains two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in \$S\$), and we define Bernstein-based "exploration bonuses" that use the empirical variance of the estimated values at the next states (to improve scaling in \$H\$).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luweber/Zotero/storage/D53BDPQL/Azar et al. - 2017 - Minimax Regret Bounds for Reinforcement Learning.pdf;/Users/luweber/Zotero/storage/DFIN8KYS/1703.html}
}

@article{beutlerUniformizationSemiMarkovDecision1987,
  title = {Uniformization for {{Semi-Markov Decision Processes}} under {{Stationary Policies}}},
  author = {Beutler, Frederick J. and Ross, Keith W.},
  year = {1987},
  journal = {Journal of Applied Probability},
  volume = {24},
  number = {3},
  eprint = {3214096},
  eprinttype = {jstor},
  pages = {644--656},
  publisher = {Applied Probability Trust},
  urldate = {2022-04-25},
  abstract = {Uniformization permits the replacement of a semi-Markov decision process (SMDP) by a Markov chain exhibiting the same average rewards for simple (non-randomized) policies. It is shown that various anomalies may occur, especially for stationary (randomized) policies; uniformization introduces virtual jumps with concomitant action changes not present in the original process. Since these lead to discrepancies in the average rewards for stationary processes, uniformization can be accepted as valid only for simple policies. We generalize uniformization to yield consistent results for stationary policies also. These results are applied to constrained optimization of SMDP, in which stationary (randomized) policies appear naturally. The structure of optimal constrained SMDP policies can then be elucidated by studying the corresponding controlled Markov chains. Moreover, constrained SMDP optimal policy computations can be more easily implemented in discrete time, the generalized uniformization being employed to relate discrete- and continuous-time optimal constrained policies.},
  keywords = {notion}
}

@article{blackwellDiscreteDynamicProgramming1962,
  title = {Discrete {{Dynamic Programming}}},
  author = {Blackwell, David},
  year = {1962},
  journal = {The Annals of Mathematical Statistics},
  volume = {33},
  number = {2},
  eprint = {2237546},
  eprinttype = {jstor},
  pages = {719--726},
  publisher = {Institute of Mathematical Statistics},
  urldate = {2023-06-15},
  abstract = {We consider a system with a finite number S of states s, labeled by the integers 1, 2, {$\cdots$}, S. Periodically, say once a day, we observe the current state of the system, and then choose an action a from a finite set A of possible actions. As a joint result of the current state s and the chosen action a, two things happen: (1) we receive an immediate income i(s, a) and (2) the system moves to a new state s' with the probability of a particular new state s' given by a function q = q(s' {$\mid$} s, a). Finally there is specified a discount factor \${\textbackslash}beta, 0 {\textbackslash}leqq {\textbackslash}beta {$<$} 1\$, so that the value of unit income n days in the future is {$\beta$}n. Our problem is to choose a policy which maximizes our total expected income. This problem, which is an interesting special case of the general dynamic programming problem, has been solved by Howard in his excellent book [3]. The case {$\beta$} = 1, also studied by Howard, is substantially more difficult. We shall obtain in this case results slightly beyond those of Howard, though still not complete. Our method, which treats {$\beta$} = 1 as a limiting case of \${\textbackslash}beta {$<$} 1\$, seems rather simpler than Howard's.},
  file = {/Users/luweber/Zotero/storage/QIH4MFKU/Blackwell - 1962 - Discrete Dynamic Programming.pdf}
}

@inproceedings{bourelTighteningExplorationUpper2020a,
  title = {Tightening {{Exploration}} in {{Upper Confidence Reinforcement Learning}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Bourel, Hippolyte and Maillard, Odalric and Talebi, Mohammad Sadegh},
  year = {2020},
  month = nov,
  pages = {1056--1066},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-04},
  abstract = {The upper confidence reinforcement learning (UCRL2) algorithm introduced in {\textbackslash}citep\{jaksch2010near\} is a popular method to perform regret minimization in unknown discrete Markov Decision Processes under the average-reward criterion. Despite its nice and generic theoretical regret guarantees, this algorithm and its variants have remained until now mostly theoretical as numerical experiments in simple environments exhibit long burn-in phases before the learning takes place. In pursuit of practical efficiency, we present UCRL3, following the lines of UCRL2, but with two key modifications: First, it uses state-of-the-art time-uniform concentration inequalities to compute confidence sets on the reward and (component-wise) transition distributions for each state-action pair. Furthermore, to tighten exploration, it uses an adaptive computation of the support of each transition distribution, which in turn enables us to revisit the extended value iteration procedure of UCRL2 to optimize over distributions with reduced support by disregarding low probability transitions, while still ensuring near-optimism. We demonstrate, through numerical experiments in standard environments, that reducing exploration this way yields a substantial numerical improvement compared to UCRL2 and its variants. On the theoretical side, these key modifications enable us to derive a regret bound for UCRL3 improving on UCRL2, that for the first time makes appear notions of local diameter and local effective support, thanks to variance-aware concentration bounds.},
  langid = {english},
  file = {/Users/luweber/Zotero/storage/UDEB8W6C/Bourel et al. - 2020 - Tightening Exploration in Upper Confidence Reinfor.pdf;/Users/luweber/Zotero/storage/VTFHP2AX/Bourel et al. - 2020 - Tightening Exploration in Upper Confidence Reinfor.pdf}
}

@article{bubeckBanditsHeavyTail2013,
  title = {Bandits {{With Heavy Tail}}},
  author = {Bubeck, S{\'e}bastien and {Cesa-Bianchi}, Nicol{\`o} and Lugosi, G{\'a}bor},
  year = {2013},
  month = nov,
  journal = {IEEE Transactions on Information Theory},
  volume = {59},
  number = {11},
  pages = {7711--7717},
  urldate = {2024-02-01},
  abstract = {The stochastic multiarmed bandit problem is well understood when the reward distributions are sub-Gaussian. In this paper, we examine the bandit problem under the weaker assumption that the distributions have moments of order 1 + {\textbackslash}varepsilon , for some {\textbackslash}varepsilon {\i}n (0,1]. Surprisingly, moments of order 2 (i.e., finite variance) are sufficient to obtain regret bounds of the same order as under sub-Gaussian reward distributions. In order to achieve such regret, we define sampling strategies based on refined estimators of the mean such as the truncated empirical mean, Catoni's M -estimator, and the median-of-means estimator. We also derive matching lower bounds that also show that the best achievable regret deteriorates when {\textbackslash}varepsilon {$<$} 1.},
  keywords = {Electronic mail,Equations,Heavy-tailed distributions,Indexes,Probability distribution,Random variables,regret bounds,robust estimators,Robustness,Standards,stochastic multi-armed bandit},
  file = {/Users/luweber/Zotero/storage/UNYFUA3Q/Bubeck et al. - 2013 - Bandits With Heavy Tail.pdf}
}

@article{carpenterDifferentiatedServicesInternet2002,
  title = {Differentiated Services in the {{Internet}}},
  author = {Carpenter, B.E. and Nichols, K.},
  year = {2002},
  month = sep,
  journal = {Proceedings of the IEEE},
  volume = {90},
  number = {9},
  pages = {1479--1494},
  abstract = {Architectures for Internet quality of service (QoS) have been under discussion for over a decade and, with the commercialization of the Internet, the topic has become increasingly important. This paper gives a background and history of QoS for the Internet, then introduces and motivates the differentiated services (Diffserv) approach. The major advantages of the Diffserv approach are that it is a good match to the Internet architecture and that it can be initially deployed with a minimalist approach, adding complexity as needed. Despite the long history of discussion, the phrase "quality of service" does not have a universally accepted meaning. In this paper QoS is used to describe a set of measurable parameters, such as delay, throughput, and loss rate, that can be attached to some identifiable subset of the traffic of IP packets through a given network domain. The identifiable subset of traffic belongs to a "user" of IP QoS where "user" spans a range of granularities, from a single application program to an entire company. Providing guarantees about the values of network parameters requires the implementation and deployment of physical mechanisms throughout the network and then configuring these mechanisms in such a way that their effect, when viewed from the edges of the network, composes into the desired QoS. Diffserv uses simple mechanisms in a more complex composition, allowing the details of the composition to evolve while the mechanisms, part of the network infrastructure, can remain the same. The paper discusses the specifics of this approach and why it is well-matched to the Internet. Some practical issues for deployment are addressed. Further we address resource allocation and configuration questions, including simple possibilities for early deployment and some of the open questions for a more complex future deployment. This paper takes the position that it is possible to maintain reasonable QoS levels without recourse to any of the class of constrained routing approaches (including MPLS), though Diffserv can be used with these approaches if desired.},
  keywords = {Commercialization,Diffserv networks,History,Loss measurement,Quality of service,Resource management,Routing,Telecommunication traffic,Throughput,Web and internet services},
  file = {/Users/luweber/Zotero/storage/VILZZ5T6/Carpenter et Nichols - 2002 - Differentiated services in the Internet.pdf;/Users/luweber/Zotero/storage/FGT84YJG/1041059.html}
}

@article{cohenLearningBasedOptimalAdmission2024,
  title = {Learning-{{Based Optimal Admission Control}} in a {{Single-Server Queuing System}}},
  author = {Cohen, Asaf and Subramanian, Vijay and Zhang, Yili},
  year = {2024},
  month = jan,
  journal = {Stochastic Systems},
  publisher = {INFORMS},
  urldate = {2024-02-01},
  abstract = {We consider a long-term average profit--maximizing admission control problem in an M/M/1 queuing system with unknown service and arrival rates. With a fixed reward collected upon service completion and a cost per unit of time enforced on customers waiting in the queue, a dispatcher decides upon arrivals whether to admit the arriving customer or not based on the full history of observations of the queue length of the system. Naor [Naor P (1969) The regulation of queue size by levying tolls. Econometrica 37(1):15--24] shows that, if all the parameters of the model are known, then it is optimal to use a static threshold policy: admit if the queue length is less than a predetermined threshold and otherwise not. We propose a learning-based dispatching algorithm and characterize its regret with respect to optimal dispatch policies for the full-information model of Naor [Naor P (1969) The regulation of queue size by levying tolls. Econometrica 37(1):15--24]. We show that the algorithm achieves an O(1) regret when all optimal thresholds with full information are nonzero and achieves an  {$O$}( ln 1+{$\in$} ({$N$})) O(ln1+{$\epsilon$}(N))  regret for any specified  {$\in>$}0 {$\epsilon>$}0  in the case that an optimal threshold with full information is 0 (i.e., an optimal policy is to reject all arrivals), where N is the number of arrivals. Funding: A. Cohen is partially supported by the National Science Foundation [Grant DMS-2006305]. V. Subramanian is supported in part by the NSF [Grants CCF-2008130, ECCS-2038416, CNS-1955777, and CMMI-2240981].},
  keywords = {queueing systems with uncertainty,reinforcement learning},
  file = {/Users/luweber/Zotero/storage/L6KQABMF/Cohen et al. - 2024 - Learning-Based Optimal Admission Control in a Sing.pdf}
}

@article{dellavecchiaIllustratedReviewConvergence2012,
  title = {Illustrated Review of Convergence Conditions of the Value Iteration Algorithm and the Rolling Horizon Procedure for Average-Cost {{MDPs}}},
  author = {Della Vecchia, Eugenio and Di Marco, Silvia and {Jean-Marie}, Alain},
  year = {2012},
  month = oct,
  journal = {Annals of Operations Research},
  volume = {199},
  number = {1},
  pages = {193--214},
  urldate = {2023-07-19},
  abstract = {This paper is concerned with the links between the Value Iteration algorithm and the Rolling Horizon procedure, for solving problems of stochastic optimal control under the long-run average criterion, in Markov Decision Processes with finite state and action spaces. We review conditions of the literature which imply the geometric convergence of Value Iteration to the optimal value. Aperiodicity is an essential prerequisite for convergence. We prove that the convergence of Value Iteration generally implies that of Rolling Horizon. We also present a modified Rolling Horizon procedure that can be applied to models without analyzing periodicity, and discuss the impact of this transformation on convergence. We illustrate with numerous examples the different results. Finally, we discuss rules for stopping Value Iteration or finding the length of a Rolling Horizon. We provide an example which demonstrates the difficulty of the question, disproving in particular a conjectured rule proposed by Puterman.},
  langid = {english},
  keywords = {Heuristic methods,Markov decision problems,Rolling horizon,Value iteration},
  file = {/Users/luweber/Zotero/storage/IIR5TGWA/Della Vecchia et al. - 2012 - Illustrated review of convergence conditions of th.pdf}
}

@inproceedings{efroniTightRegretBounds2019,
  title = {Tight {{Regret Bounds}} for {{Model-Based Reinforcement Learning}} with {{Greedy Policies}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Efroni, Yonathan and Merlis, Nadav and Ghavamzadeh, M. and Mannor, Shie},
  year = {2019},
  month = may,
  urldate = {2023-08-08},
  abstract = {State-of-the-art efficient model-based Reinforcement Learning (RL) algorithms typically act by iteratively solving empirical models, i.e., by performing {\textbackslash}emph\{full-planning\} on Markov Decision Processes (MDPs) built by the gathered experience. In this paper, we focus on model-based RL in the finite-state finite-horizon MDP setting and establish that exploring with {\textbackslash}emph\{greedy policies\} -- act by {\textbackslash}emph\{1-step planning\} -- can achieve tight minimax performance in terms of regret, \${\textbackslash}tilde\{{\textbackslash}mathcal\{O\}\}({\textbackslash}sqrt\{HSAT\})\$. Thus, full-planning in model-based RL can be avoided altogether without any performance degradation, and, by doing so, the computational complexity decreases by a factor of \$S\$. The results are based on a novel analysis of real-time dynamic programming, then extended to model-based RL. Specifically, we generalize existing algorithms that perform full-planning to such that act by 1-step planning. For these generalizations, we prove regret bounds with the same rate as their full-planning counterparts.},
  file = {/Users/luweber/Zotero/storage/H7K3IDT8/Efroni et al. - 2019 - Tight Regret Bounds for Model-Based Reinforcement .pdf}
}

@article{feinbergOptimalityRandomizedTrunk1994a,
  title = {Optimality of {{Randomized Trunk Reservation}}},
  author = {Feinberg, Eugene A. and Reiman, Martin I.},
  year = {1994},
  month = oct,
  journal = {Probability in the Engineering and Informational Sciences},
  volume = {8},
  number = {4},
  pages = {463--489},
  publisher = {Cambridge University Press},
  urldate = {2023-07-11},
  abstract = {We consider a controlled queueing system that is a generalization of the M/M/c/W queue. There are m types of customers that arrive according to independent Poisson processes. Service times are exponential and independent and do not depend on the customer type. There is room in the system for a total of N customers; if there are N customers in the system, new arrivals are lost. Type j customers are more profitable than type (j + 1 ) customers, j = 2,{\dots}, m ---, and type 1 customers are at least as profitable as type 2 customers. The allowed control is to accept or reject customers at arrival. No preemption of customers in service is allowed. The goal is to maximize the average reward per unit of time subject to a constraint that the blocking probability of type 1 customers is no greater than a given level.For an M/M/c/c system without a constraint, Miller [12] proved that an optimal policy has a simple threshold structure. We show that, for the constrained problem described above, an optimal policy has a similar structure, but one of the thresholds might have to be randomized. We also derive an algorithm that constructs an optimal policy and describe other forms of optimal policies.},
  langid = {english},
  file = {/Users/luweber/Zotero/storage/NIW37KEH/Feinberg et Reiman - 1994 - Optimality of Randomized Trunk Reservation.pdf}
}

@article{feinbergOptimalityTrunkReservation2011,
  title = {Optimality of Trunk Reservation for an {{M}}/{{M}}/k/{{N}} Queue with Several Customer Types and Holding Costs},
  author = {Feinberg, Eugene A. and Yang, Fenghsu},
  year = {2011},
  month = oct,
  journal = {Probability in the Engineering and Informational Sciences},
  volume = {25},
  number = {4},
  pages = {537--560},
  urldate = {2022-05-11},
  abstract = {In this article we study optimal admission to an               M               /               M               /               k               /               N               queue with several customer types. The reward structure consists of revenues collected from admitted customers and holding costs, both of which depend on customer types. This article studies average rewards per unit time and describes the structures of stationary optimal, canonical, bias optimal, and Blackwell optimal policies. Similar to the case without holding costs, bias optimal and Blackwell optimal policies are unique, coincide, and have a trunk reservation form with the largest optimal control level for each customer type. Problems with one holding cost rate have been studied previously in the literature.},
  langid = {english},
  keywords = {notion},
  file = {/Users/luweber/Zotero/storage/98XCJPRH/Feinberg et Yang - 2011 - OPTIMALITY OF TRUNK RESERVATION FOR AN MMKN QUE.pdf}
}

@inproceedings{filippiOptimismReinforcementLearning2010a,
  title = {Optimism in Reinforcement Learning and {{Kullback-Leibler}} Divergence},
  booktitle = {2010 48th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}} ({{Allerton}})},
  author = {Filippi, Sarah and Capp{\'e}, Olivier and Garivier, Aur{\'e}lien},
  year = {2010},
  month = sep,
  pages = {115--122},
  abstract = {We consider model-based reinforcement learning in finite Markov Decision Processes (MDPs), focussing on so-called optimistic strategies. In MDPs, optimism can be implemented by carrying out extended value iterations under a constraint of consistency with the estimated model transition probabilities. The UCRL2 algorithm by Auer, Jaksch and Ortner (2009), which follows this strategy, has recently been shown to guarantee near-optimal regret bounds. In this paper, we strongly argue in favor of using the Kullback-Leibler (KL) divergence for this purpose. By studying the linear maximization problem under KL constraints, we provide an efficient algorithm, termed KL-UCRL, for solving KL-optimistic extended value iteration. Using recent deviation bounds on the KL divergence, we prove that KL-UCRL provides the same guarantees as UCRL2 in terms of regret. However, numerical experiments on classical benchmarks show a significantly improved behavior, particularly when the MDP has reduced connectivity. To support this observation, we provide elements of comparison between the two algorithms based on geometric considerations.},
  keywords = {Algorithm design and analysis,Benchmark testing,Context modeling,Equations,Kullback-Leibler divergence,Learning,Markov decision processes,Markov processes,Mathematical model,Model-based approaches,Optimism,Regret bounds,Reinforcement learning},
  file = {/Users/luweber/Zotero/storage/T86948G8/Filippi et al. - 2010 - Optimism in reinforcement learning and Kullback-Le.pdf;/Users/luweber/Zotero/storage/9PUV5QLG/5706896.html}
}

@inproceedings{fruitRegretMinimizationMDPs2017,
  title = {Regret {{Minimization}} in {{MDPs}} with {{Options}} without {{Prior Knowledge}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Brunskill, Emma},
  year = {2017},
  volume = {30},
  pages = {3166--3176},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-05-04},
  abstract = {The option framework integrates temporal abstraction into the reinforcement learning model through the introduction of macro-actions (i.e., options). Recent works leveraged on the mapping of Markov decision processes (MDPs) with options to semi-MDPs (SMDPs) and introduced SMDP-versions of exploration-exploitation algorithms (e.g., RMAX-SMDP and UCRL-SMDP) to analyze the impact of options on the learning performance. Nonetheless, the PAC-SMDP sample complexity of RMAX-SMDP can hardly be translated into equivalent PAC-MDP theoretical guarantees, while UCRL-SMDP requires prior knowledge of the parameters characterizing the distributions of the cumulative reward and duration of each option, which are hardly available in practice. In this paper, we remove this limitation by combining the SMDP view together with the inner Markov structure of options into a novel algorithm whose regret performance matches UCRL-SMDP's up to an additive regret term. We show scenarios where this term is negligible and the advantage of temporal abstraction is preserved. We also report preliminary empirical result supporting the theoretical findings.},
  file = {/Users/luweber/Zotero/storage/RJF8YY4Q/Fruit et al. - 2017 - Regret Minimization in MDPs with Options without P.pdf}
}

@misc{gaoLogarithmicRegretBounds2023,
  title = {Logarithmic Regret Bounds for Continuous-Time Average-Reward {{Markov}} Decision Processes},
  author = {Gao, Xuefeng and Zhou, Xun Yu},
  year = {2023},
  month = jul,
  number = {arXiv:2205.11168},
  eprint = {2205.11168},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-06-04},
  abstract = {We consider reinforcement learning for continuous-time Markov decision processes (MDPs) in the infinite-horizon, average-reward setting. In contrast to discrete-time MDPs, a continuous-time process moves to a state and stays there for a random holding time after an action is taken. With unknown transition probabilities and rates of exponential holding times, we derive instance-dependent regret lower bounds that are logarithmic in the time horizon. Moreover, we design a learning algorithm and establish a finite-time regret bound that achieves the logarithmic growth rate. Our analysis builds upon upper confidence reinforcement learning, a delicate estimation of the mean holding times, and stochastic comparison of point processes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/luweber/Zotero/storage/8QA38TLG/Gao et Zhou - 2023 - Logarithmic regret bounds for continuous-time aver.pdf;/Users/luweber/Zotero/storage/T9B7ZPHH/2205.html}
}

@article{havivBiasOptimalityControlled1998,
  title = {Bias {{Optimality}} in {{Controlled Queueing Systems}}},
  author = {Haviv, Moshe and Puterman, Martin L.},
  year = {1998},
  journal = {Journal of Applied Probability},
  volume = {35},
  number = {1},
  pages = {136--150},
  abstract = {This paper studies an admission control M/M/1 queueing system. It shows that the only gain (average) optimal stationary policies with gain and bias which satisfy the optimality equation are of control limit type, that there are at most two and, if there are two, they occur consecutively. Conditions are provided which ensure the existence of two gain optimal control limit policies and are illustrated with an example. The main result is that bias optimality distinguishes these two gain optimal policies and that the larger of the two control limits is the unique bias optimal stationary policy. Consequently it is also Blackwell optimal. This result is established by appealing to the third optimality equation of the Markov decision process and some observations concerning the structure of solutions of the second optimality equation.}
}

@article{JMLR:v11:jaksch10a,
  title = {Near-Optimal Regret Bounds for Reinforcement Learning},
  author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {51},
  pages = {1563--1600},
  file = {/Users/luweber/Zotero/storage/UYAKTGWT/Jaksch et al. - 2010 - Near-optimal regret bounds for reinforcement learn.pdf}
}

@article{kempStochasticModellingAnalysis1987,
  title = {Stochastic {{Modelling}} and {{Analysis}}: {{A Computational Approach}}},
  shorttitle = {Stochastic {{Modelling}} and {{Analysis}}},
  author = {Kemp, A. W.},
  year = {1987},
  month = oct,
  journal = {Journal of the Royal Statistical Society Series D: The Statistician},
  volume = {36},
  number = {4},
  pages = {417},
  urldate = {2023-07-19},
  file = {/Users/luweber/Zotero/storage/AJ7IIS2S/7121680.html}
}

@book{lattimoreBanditAlgorithms2020,
  title = {Bandit {{Algorithms}}},
  author = {Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year = {2020},
  month = jul,
  publisher = {Cambridge University Press},
  abstract = {Decision-making in the face of uncertainty is a significant challenge in machine learning, and the multi-armed bandit model is a commonly used framework to address it. This comprehensive and rigorous introduction to the multi-armed bandit problem examines all the major settings, including stochastic, adversarial, and Bayesian frameworks. A focus on both mathematical intuition and carefully worked proofs makes this an excellent reference for established researchers and a helpful resource for graduate students in computer science, engineering, statistics, applied mathematics and economics. Linear bandits receive special attention as one of the most useful models in applications, while other chapters are dedicated to combinatorial bandits, ranking, non-stationary problems, Thompson sampling and pure exploration. The book ends with a peek into the world beyond bandits with an introduction to partial monitoring and learning in Markov decision processes.},
  googlebooks = {bbjpDwAAQBAJ},
  langid = {english},
  keywords = {Business & Economics / Economics / Microeconomics,Computers / Artificial Intelligence / Computer Vision & Pattern Recognition,Computers / Artificial Intelligence / General,Computers / Programming / Algorithms,Mathematics / Discrete Mathematics,Mathematics / Game Theory,Mathematics / Optimization}
}

@article{leeDemandSideManagement2011,
  title = {Demand {{Side Management With Air Conditioner Loads Based}} on the {{Queuing System Model}}},
  author = {Lee, S. C. and Kim, S. J. and Kim, S. H.},
  year = {2011},
  month = may,
  journal = {IEEE Transactions on Power Systems},
  volume = {26},
  number = {2},
  pages = {661--668},
  urldate = {2024-04-30},
  abstract = {This paper presents a novel load management technique for the air conditioner loads in large apartment complexes. Through systematic aggregations and load factor controls of the air conditioner loads based on a queuing system model and the Markov birth and death process, the proposed technique can offer effective and convenient load management measures to both the power companies and the customers. The customers' inconveniences or discomforts can be moderated by the aggregations plus the incentives based on the participation levels in the load management. A numerical example using the load data from a large apartment complex shows a good potential for future applications.},
  keywords = {Air conditioner load,Air conditioning,Companies,Computer integrated manufacturing,demand side management,load management,Load management,Load modeling,Markov birth and death process,Markov processes,Power systems,queuing system,token},
  file = {/Users/luweber/Zotero/storage/SLDTNKRC/Lee et al. - 2011 - Demand Side Management With Air Conditioner Loads .pdf;/Users/luweber/Zotero/storage/5KUJUKCB/5587911.html}
}

@book{levinMarkovChainsMixing2008,
  title = {Markov {{Chains}} and {{Mixing Times}}, Second Edition},
  shorttitle = {Markov\_chains\_and\_mixing\_times},
  author = {Levin, David A and Peres, Yuval},
  year = {2008},
  edition = {second},
  publisher = {American Mathematical Society},
  langid = {english},
  file = {/Users/luweber/Zotero/storage/H6FHZCBN/Levin et Peres - Markov Chains and Mixing Times, second edition.pdf}
}

@incollection{lewisBiasOptimality2002,
  title = {Bias {{Optimality}}},
  booktitle = {Handbook of {{Markov Decision Processes}}: {{Methods}} and {{Applications}}},
  author = {Lewis, Mark E. and Puterman, Martin L.},
  editor = {Feinberg, Eugene A. and Shwartz, Adam},
  year = {2002},
  series = {International {{Series}} in {{Operations Research}} \& {{Management Science}}},
  pages = {89--111},
  publisher = {Springer US},
  address = {Boston, MA},
  urldate = {2023-06-14},
  abstract = {The use of the long-run average reward or the gain as an optimally criterion has received considerable attention in the literature. However, for many practical models the gain has the undesirable property of being underselective, that is, there may be several gain optimal policies. After finding the set of policies that achieve the primary objective of maximizing the long-run average reward one might search for that which maximizes the ``short-run'' reward. This reward, called the bias aids in distinguishing among multiple gain optimal policies. This chapter focuses on establishing the usefulness of the bias in distinguishing among multiple gain optimal policies, computing it and demonstrating the implicit discounting captured by bias on recurrent states.},
  langid = {english},
  keywords = {Average Reward,Bias Optimality,Markov Decision Process,Optimal Policy,Total Reward}
}

@article{lewisBiasOptimalityQueue1999,
  title = {Bias {{Optimality In A Queue With Admission Control}}},
  author = {Lewis, Mark E. and Ayhan, Hayriye and Foley, Robert D.},
  year = {1999},
  journal = {Probability in the Engineering and Informational Sciences},
  volume = {13},
  number = {3},
  pages = {309--327},
  abstract = {We consider a finite capacity queueing system in  which each arriving customer offers a reward. A gatekeeper  decides based on the reward offered and the space remaining  whether each arriving customer should be accepted or rejected.  The gatekeeper only receives the offered reward if the  customer is accepted. A traditional objective function  is to maximize the gain, that is, the long-run average  reward. It is quite possible, however, to have several  different gain optimal policies that behave quite differently.  Bias and Blackwell optimality are more refined objective  functions that can distinguish among multiple stationary,  deterministic gain optimal policies. This paper focuses  on describing the structure of stationary, deterministic,  optimal policies and extending this optimality to distinguish  between multiple gain optimal policies. We show that these  policies are of trunk reservation form and must occur consecutively.  We then prove that we can distinguish among these gain  optimal policies using the bias or transient reward and  extend to Blackwell optimality.}
}

@inproceedings{malekLinearProgrammingLargeScale2014,
  title = {Linear {{Programming}} for {{Large-Scale Markov Decision Problems}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Malek, Alan and {Abbasi-Yadkori}, Yasin and Bartlett, Peter},
  year = {2014},
  month = jun,
  pages = {496--504},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2024-05-21},
  abstract = {We consider the problem of controlling a Markov decision  process (MDP) with a large state space, so as to minimize average cost.  Since it is intractable to compete with the optimal policy for large  scale problems, we pursue the more modest goal of competing with a  low-dimensional family of policies. We use the dual linear programming  formulation of the MDP average cost problem, in which the variable is  a stationary distribution over state-action pairs, and we consider a  neighborhood of a low-dimensional subset of the set of stationary  distributions (defined in terms of state-action features) as  the comparison class.  We propose two techniques, one based on stochastic convex optimization,  and one based on constraint sampling. In both cases, we give bounds  that show that the performance of our algorithms approaches the best  achievable by any policy in the comparison class. Most importantly,  these results depend on the size of the comparison class, but not  on the size of the state space.  Preliminary experiments  show the effectiveness of the proposed algorithms in a queuing  application.},
  langid = {english},
  file = {/Users/luweber/Zotero/storage/8BLTR8NT/Malek et al. - 2014 - Linear Programming for Large-Scale Markov Decision.pdf}
}

@article{manneLinearProgrammingSequential1960,
  title = {Linear {{Programming}} and {{Sequential Decisions}}},
  author = {Manne, Alan S.},
  year = {1960},
  journal = {Management Science},
  volume = {6},
  number = {3},
  eprint = {2627340},
  eprinttype = {jstor},
  pages = {259--267},
  publisher = {INFORMS},
  urldate = {2024-05-24},
  abstract = {Using an illustration drawn from the area of inventory control, this paper demonstrates how a typical sequential probabilistic model may be formulated in terms of (a) an initial decision rule and (b) a Markov process, and then optimized by means of linear programming. This linear programming technique may turn out to be an efficient alternative to the functional equation approach in the numerical analysis of such problems. Regardless of computational significance, however, it is of interest that there should be such a close relationship between the two traditionally distinct areas of dynamic programming and linear programming.},
  file = {/Users/luweber/Zotero/storage/UPP7DKFC/Manne - 1960 - Linear Programming and Sequential Decisions.pdf}
}

@inproceedings{massaroOptimalTrunkReservationPolicy2019,
  title = {Optimal {{Trunk-Reservation}} by {{Policy Learning}}},
  booktitle = {{{IEEE INFOCOM}} 2019 - {{IEEE Conference}} on {{Computer Communications}}},
  author = {Massaro, Antonio and De Pellegrini, Francesco and Maggi, Lorenzo},
  year = {2019},
  month = apr,
  pages = {127--135},
  issn = {2641-9874},
  abstract = {In the framework of queuing theory with multiclass jobs, trunk-reservation is an admission control technique to handle job class priority in an online fashion, and serve as many high priority jobs as possible. This is achieved by rejecting jobs with sufficiently low priority when the buffer space becomes a scarce resource, i.e., when the queue may soon overflow. Mathematically, the objective is to maximize the long-term reward of the admitted jobs, where each job is assigned a reward which is monotonic with respect to the priority class it belongs to. In this paper we study online learning of optimal trunk-reservation policies when the system parameters, i.e., class arrival rates and service time, are unknown. Starting from a Markov Decision Process (MDP) formulation, we leverage the stairway structure of the optimal policy to define Integer Gradient Ascent (IGA), a reinforcement learning (RL) algorithm based on policy-gradient methods, specifically tailored to the mathematical properties of the problem at hand. We provide theoretical results on the convergence properties of IGA. Extensive numerical experiments characterize its behavior and confirm that it outperforms standard RL techniques in terms of convergence rate.},
  keywords = {Admission control,Aerospace electronics,Convergence,Markov processes,notion,policy gradient,reinforce,reinforcement learning,Reinforcement learning,Servers,Standards,trunk-reservation},
  file = {/Users/luweber/Zotero/storage/KXX64IZY/Massaro et al. - 2019 - Optimal Trunk-Reservation by Policy Learning.pdf;/Users/luweber/Zotero/storage/C552YF73/8737552.html}
}

@article{millerQueueingRewardSystem1969a,
  title = {A {{Queueing Reward System}} with {{Several Customer Classes}}},
  author = {Miller, Bruce L.},
  year = {1969},
  journal = {Management Science},
  volume = {16},
  number = {3},
  eprint = {2628300},
  eprinttype = {jstor},
  pages = {234--245},
  publisher = {INFORMS},
  urldate = {2023-05-16},
  abstract = {This paper considers an n-server queueing system with m customer classes distinguished by the reward associated with serving customers of that class. Our objective is to accept or reject customers so as to maximize the expected value of the rewards received over an infinite planning horizon. By making the assumptions of Poisson arrivals and a common exponential service time this problem can be formulated as an infinite horizon continuous time Markov decision problem. In {\S}3 we customize the general algorithm for solving continuous time Markov decision problems to our queueing model. In {\S}4 we obtain qualitative results about the form of the optimal policy. {\S}6 reports the results of simulation tests which compare heuristic policies for our model when the service times associated with each customer class have arbitrary distributions. The "winning" policy is based on a rather intricate theorem whose proof comprises {\S}5.},
  file = {/Users/luweber/Zotero/storage/DBDCS49W/Miller - 1969 - A Queueing Reward System with Several Customer Cla.pdf}
}

@book{niThresholdReservationBased2005,
  title = {Threshold and Reservation Based Call Admission Control Policies for Multiservice Resource-Sharing Systems},
  author = {Ni, Jian and Tsang, D.H.K. and Tatikonda, S. and Bensaou, Brahim},
  year = {2005},
  month = apr,
  volume = {2},
  pages = {783 vol. 2},
  doi = {10.1109/INFCOM.2005.1498309},
  abstract = {Many communications and networking systems can be modelled as resource-sharing systems with multiple classes of calls. Call admission control (CAC) is an essential component of such systems. For most practical systems it is prohibitively difficult to compute the optimal CAC policy that optimizes certain performance metrics because of the 'curse of dimensionality'. In this paper we study two families of structured CAC policies: threshold and reservation policies. These policies are easy to implement and have good performance in practice. However, since the number of structured policies grows exponentially with the number of call classes and the capacity of the system, finding the optimal structured policies is a complex unsolved problem. In this paper efficient search algorithms are proposed to find the coordinate optimal structured policies among all structured policies. Through extensive numerical experiments we show that the search algorithms converge quickly and work for systems with large capacity and many call classes. In addition, the returned structured policies have optimal or near-optimal performance, and outperform those structured policies with parameters chosen based on simple heuristics.},
  file = {/Users/luweber/Zotero/storage/AUCRGTZS/Ni et al. - 2005 - Threshold and reservation based call admission con.pdf}
}

@inproceedings{osbandMoreEfficientReinforcement2013,
  title = {({{More}}) {{Efficient Reinforcement Learning}} via {{Posterior Sampling}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Osband, Ian and Russo, Daniel and Van Roy, Benjamin},
  year = {2013},
  volume = {26},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-05-30},
  file = {/Users/luweber/Zotero/storage/XNMWPGRM/Osband et al. - 2013 - (More) Efficient Reinforcement Learning via Poster.pdf}
}

@article{osbandPosteriorSamplingReinforcement2016a,
  title = {Posterior {{Sampling}} for {{Reinforcement Learning Without Episodes}}},
  author = {Osband, Ian and Van Roy, Benjamin},
  year = {2016},
  month = aug,
  journal = {arXiv preprint arXiv:1608.02731},
  eprint = {1608.02731},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-30},
  abstract = {This is a brief technical note to clarify some of the issues with applying the application of the algorithm posterior sampling for reinforcement learning (PSRL) in environments without fixed episodes. In particular, this paper aims to: - Review some of results which have been proven for finite horizon MDPs (Osband et al 2013, 2014a, 2014b, 2016) and also for MDPs with finite ergodic structure (Gopalan et al 2014). - Review similar results for optimistic algorithms in infinite horizon problems (Jaksch et al 2010, Bartlett and Tewari 2009, Abbasi-Yadkori and Szepesvari 2011), with particular attention to the dynamic episode growth. - Highlight the delicate technical issue which has led to a fault in the proof of the lazy-PSRL algorithm (Abbasi-Yadkori and Szepesvari 2015). We present an explicit counterexample to this style of argument. Therefore, we suggest that the Theorem 2 in (Abbasi-Yadkori and Szepesvari 2015) be instead considered a conjecture, as it has no rigorous proof. - Present pragmatic approaches to apply PSRL in infinite horizon problems. We conjecture that, under some additional assumptions, it will be possible to obtain bounds \$O( {\textbackslash}sqrt\{T\} )\$ even without episodic reset. We hope that this note serves to clarify existing results in the field of reinforcement learning and provides interesting motivation for future work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luweber/Zotero/storage/7CNUIKE7/Osband et Van Roy - 2016 - Posterior Sampling for Reinforcement Learning With.pdf;/Users/luweber/Zotero/storage/H79487B4/1608.html}
}

@inproceedings{osbandWhyPosteriorSampling2017,
  title = {Why Is {{Posterior Sampling Better}} than {{Optimism}} for {{Reinforcement Learning}}?},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Osband, Ian and Roy, Benjamin Van},
  year = {2017},
  month = jul,
  pages = {2701--2710},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-07-11},
  abstract = {Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms existing algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an {$O$}{\~}~({$HSAT$}-----{\textsurd})O{\textasciitilde}(HSAT){\textbackslash}tilde\{O\}(H{\textbackslash}sqrt\{SAT\}) Bayesian regret bound for PSRL in finite-horizon episodic Markov decision processes. This improves upon the best previous Bayesian regret bound of {$O$}{\~}~({$HSAT$}---{\textsurd})O{\textasciitilde}(HSAT){\textbackslash}tilde\{O\}(H S {\textbackslash}sqrt\{AT\}) for any reinforcement learning algorithm. Our theoretical results are supported by extensive empirical evaluation.},
  langid = {english},
  file = {/Users/luweber/Zotero/storage/45W67NKE/Osband et Roy - 2017 - Why is Posterior Sampling Better than Optimism for.pdf;/Users/luweber/Zotero/storage/VTX3PAFA/Osband et Roy - 2017 - Why is Posterior Sampling Better than Optimism for.pdf}
}

@article{pesquerelIMEDRLRegretOptimal2022,
  title = {{{IMED-RL}}: {{Regret}} Optimal Learning of Ergodic {{Markov}} Decision Processes},
  shorttitle = {{{IMED-RL}}},
  author = {Pesquerel, Fabien and Maillard, Odalric-Ambrym},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {26363--26374},
  urldate = {2023-07-11},
  langid = {english},
  file = {/Users/luweber/Zotero/storage/5XIJ49DL/Pesquerel et Maillard - 2022 - IMED-RL Regret optimal learning of ergodic Markov.pdf}
}

@book{privaultUnderstandingMarkovChains2018,
  title = {Understanding {{Markov Chains}}: {{Examples}} and {{Applications}}},
  shorttitle = {Understanding {{Markov Chains}}},
  author = {Privault, Nicolas},
  year = {2018},
  series = {Springer {{Undergraduate Mathematics Series}}},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-13-0659-4},
  urldate = {2023-08-08},
  keywords = {Applications of Stochastic Processes,Discrete and continuous-time Markov Chains,First-step analysis in Markov Chains,Gambling Processes and random walks in Markov Chains,Highly accessible textbook on Stochastic Processes,Introduction to Stochastic Processes,Markov Chains self-study,Markov Chains textbook,Markov Chains textbook with examples,Modern textbook on Stochastic Processes,Nicolas Privault Stochastic Processes,Solved problems in Markov Chains},
  file = {/Users/luweber/Zotero/storage/UWLJA7KW/Privault - 2018 - Understanding Markov Chains Examples and Applicat.pdf}
}

@book{putermanMarkovDecisionProcesses2005a,
  title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  shorttitle = {Markov Decision Processes},
  author = {Puterman, Martin L.},
  year = {2005},
  series = {Wiley Series in Probability and Statistics},
  publisher = {Wiley-Interscience},
  address = {Hoboken, NJ},
  langid = {english},
  file = {/Users/luweber/Zotero/storage/E8683J57/Puterman - 2005 - Markov decision processes discrete stochastic dyn.pdf}
}

@misc{raaijmakersReinforcementLearningAdmission2021,
  title = {Reinforcement Learning for {{Admission Control}} in {{5G Wireless Networks}}},
  author = {Raaijmakers, Youri and Mandelli, Silvio and Doll, Mark},
  year = {2021},
  month = apr,
  number = {arXiv:2104.10761},
  eprint = {2104.10761},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2023-07-11},
  abstract = {The key challenge in admission control in wireless networks is to strike an optimal trade-off between the blocking probability for new requests while minimizing the dropping probability of ongoing requests. We consider two approaches for solving the admission control problem: i) the typically adopted threshold policy and ii) our proposed policy relying on reinforcement learning with neural networks. Extensive simulation experiments are conducted to analyze the performance of both policies. The results show that the reinforcement learning policy outperforms the threshold-based policies in the scenario with heterogeneous time-varying arrival rates and multiple user equipment types, proving its applicability in realistic wireless network scenarios.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Networking and Internet Architecture,Electrical Engineering and Systems Science - Signal Processing},
  file = {/Users/luweber/Zotero/storage/VB7GCKW4/Raaijmakers et al. - 2021 - Reinforcement learning for Admission Control in 5G.pdf;/Users/luweber/Zotero/storage/MCTITL5B/2104.html}
}

@misc{rigterOptimalAdmissionControl2022,
  title = {Optimal {{Admission Control}} for {{Multiclass Queues}} with {{Time-Varying Arrival Rates}} via {{State Abstraction}}},
  author = {Rigter, Marc and Dervovic, Danial and Hassanzadeh, Parisa and Long, Jason and Zehtabi, Parisa and Magazzeni, Daniele},
  year = {2022},
  month = mar,
  number = {arXiv:2203.08019},
  eprint = {2203.08019},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2023-07-11},
  abstract = {We consider a novel queuing problem where the decision-maker must choose to accept or reject randomly arriving tasks into a no buffer queue which are processed by \$N\$ identical servers. Each task has a price, which is a positive real number, and a class. Each class of task has a different price distribution and service rate, and arrives according to an inhomogenous Poisson process. The objective is to decide which tasks to accept so that the total price of tasks processed is maximised over a finite horizon. We formulate the problem as a discrete time Markov Decision Process (MDP) with a hybrid state space. We show that the optimal value function has a specific structure, which enables us to solve the hybrid MDP exactly. Moreover, we prove that as the time step is reduced, the discrete time solution approaches the optimal solution to the original continuous time problem. To improve the scalability of our approach to a greater number of task classes, we present an approximation based on state abstraction. We validate our approach on synthetic data, as well as a real financial fraud data set, which is the motivating application for this work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/Users/luweber/Zotero/storage/TQZIJMYD/Rigter et al. - 2022 - Optimal Admission Control for Multiclass Queues wi.pdf;/Users/luweber/Zotero/storage/Y9DJ6ZC2/2203.html}
}

@article{royOnlineReinforcementLearning2022,
  title = {Online {{Reinforcement Learning}} of {{Optimal Threshold Policies}} for {{Markov Decision Processes}}},
  author = {Roy, Arghyadip and Borkar, Vivek and Karandikar, Abhay and Chaporkar, Prasanna},
  year = {2022},
  month = jul,
  journal = {IEEE Transactions on Automatic Control},
  volume = {67},
  number = {7},
  pages = {3722--3729},
  urldate = {2024-02-01},
  abstract = {To overcome the curses of dimensionality and modeling of dynamic programming methods to solve Markov decision process problems, reinforcement learning (RL) methods are adopted in practice. Contrary to traditional RL algorithms, which do not consider the structural properties of the optimal policy, we propose a structure-aware learning algorithm to exploit the ordered multithreshold structure of the optimal policy, if any. We prove the asymptotic convergence of the proposed algorithm to the optimal policy. Due to the reduction in the policy space, the proposed algorithm provides remarkable improvements in storage and computational complexities over classical RL algorithms. Simulation results establish that the proposed algorithm converges faster than other RL algorithms.},
  keywords = {Computational complexity,Computational modeling,Convergence,Markov decision process (MDP),Markov processes,online learning of threshold policies,Process control,Reinforcement learning,reinforcement learning (RL),Simulation,stochastic approximation (SA) algorithms,stochastic control},
  file = {/Users/luweber/Zotero/storage/6CDPD46B/Roy et al. - 2022 - Online Reinforcement Learning of Optimal Threshold.pdf;/Users/luweber/Zotero/storage/PQ66ZF6P/9524527.html}
}

@inproceedings{talebiVarianceAwareRegretBounds2018b,
  title = {Variance-{{Aware Regret Bounds}} for {{Undiscounted Reinforcement Learning}} in {{MDPs}}},
  booktitle = {Proceedings of {{Algorithmic Learning Theory}}},
  author = {Talebi, Mohammad Sadegh and Maillard, Odalric-Ambrym},
  year = {2018},
  month = apr,
  pages = {770--805},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-04},
  abstract = {The problem of reinforcement learning in an unknown and discrete Markov Decision Process (MDP) under the average-reward criterion is considered, when  the learner interacts with the system in a single stream of observations, starting from an initial state without any reset.  We revisit the minimax lower bound for that problem by making appear the local variance of the bias function  in place of the diameter of the MDP.  Furthermore, we provide a novel analysis of the {\textbackslash}texttt\{{\textbackslash}textsc\{KL-Ucrl\}\} algorithm establishing a high-probability regret bound scaling as  {\texttildelow}({$S\sums$},{$a\mathbf{V}\stars$},{$aT$}------------{\textsurd})O{\textasciitilde}(S{$\sum$}s,aVs,a{$\star$}T){\textbackslash}widetilde \{{\textbackslash}mathcal O\}{\textbackslash}Bigl(\{{\textbackslash}textstyle {\textbackslash}sqrt\{S{\textbackslash}sum\_\{s,a\}\{{\textbackslash}bf V\}{\textasciicircum}{\textbackslash}star\_\{s,a\}T\}\}{\textbackslash}Big) for this algorithm for ergodic MDPs, where {$S$}SS denotes the number of states and  where {$\mathbf{V}\stars$},{$a$}Vs,a{$\star$}\{{\textbackslash}bf V\}{\textasciicircum}{\textbackslash}star\_\{s,a\} is the variance of the bias function with respect to the next-state distribution following action {$a$}aa in state {$s$}ss.  The resulting bound improves upon the best previously known regret bound {\textbackslash}Ocal{\texttildelow}({$DSAT$}---{\textsurd}){\textbackslash}Ocal{\textasciitilde}(DSAT){\textbackslash}widetilde \{{\textbackslash}Ocal\}(DS{\textbackslash}sqrt\{AT\}) for that algorithm, where {$A$}AA and {$D$}DD respectively denote  the maximum number of actions (per state) and the diameter of MDP. We finally compare the leading terms of the two bounds in some benchmark MDPs indicating  that the derived bound can provide an order of magnitude improvement in some cases. Our analysis leverages novel variations of the transportation lemma  combined with Kullback-Leibler concentration inequalities, that we believe to be of independent interest.},
  langid = {english},
  file = {/Users/luweber/Zotero/storage/W6M3TJF8/Talebi et Maillard - 2018 - Variance-Aware Regret Bounds for Undiscounted Rein.pdf}
}

@article{tossouNearoptimalOptimisticReinforcement2019,
  title = {Near-Optimal {{Optimistic Reinforcement Learning}} Using {{Empirical Bernstein Inequalities}}},
  author = {Tossou, Aristide and Basu, Debabrota and Dimitrakakis, Christos},
  year = {2019},
  month = dec,
  journal = {arXiv preprint arXiv:1905.12425},
  eprint = {1905.12425},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-16},
  abstract = {We study model-based reinforcement learning in an unknown finite communicating Markov decision process. We propose a simple algorithm that leverages a variance based confidence interval. We show that the proposed algorithm, UCRL-V, achieves the optimal regret \${\textbackslash}tilde\{{\textbackslash}mathcal\{O\}\}({\textbackslash}sqrt\{DSAT\})\$ up to logarithmic factors, and so our work closes a gap with the lower bound without additional assumptions on the MDP. We perform experiments in a variety of environments that validates the theoretical bounds as well as prove UCRL-V to be better than the state-of-the-art algorithms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/luweber/Zotero/storage/XQ5MIDNU/Tossou et al. - 2019 - Near-optimal Optimistic Reinforcement Learning usi.pdf;/Users/luweber/Zotero/storage/Y3MQYGSA/1905.html}
}

@article{weissmanInequalitiesL1Deviation2003,
  title = {Inequalities for the {{L1 Deviation}} of the {{Empirical Distribution}}},
  author = {Weissman, T. and Ordentlich, E. and Seroussi, G. and Verd{\'u}, S. and Weinberger, M.},
  year = {2003},
  journal = {Hewlett-{{Packard Labs}}, {{Tech}}. {{Rep}}},
  urldate = {2024-05-22},
  abstract = {We derive bounds on the probability that the L1 distance between the empirical distribution of a sequence of independent identically distributed random variables and the true distribution is more than a specified value. We also derive a generalization of Pinsker's inequality relating the L1 distance to the divergence.}
}

@inproceedings{wuNearlyMinimaxOptimal2022,
  title = {Nearly {{Minimax Optimal Regret}} for {{Learning Infinite-horizon Average-reward MDPs}} with {{Linear Function Approximation}}},
  booktitle = {Proceedings of {{The}} 25th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Wu, Yue and Zhou, Dongruo and Gu, Quanquan},
  year = {2022},
  month = may,
  pages = {3883--3913},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-05-22},
  abstract = {We study reinforcement learning in an infinite-horizon average-reward setting with linear function approximation for linear mixture Markov decision processes (MDPs), where the transition probability function of the underlying MDP admits a linear form over a feature mapping of the current state, action, and next state. We propose a new algorithm UCRL2-VTR, which can be seen as an extension of the UCRL2 algorithm with linear function approximation. We show that UCRL2-VTR with Bernstein-type bonus can achieve a regret of {$O$}{\~}~({$dDT$}----{\textsurd})O{\textasciitilde}(dDT){\textbackslash}tilde\{O\}(d{\textbackslash}sqrt\{DT\}), where {$d$}dd is the dimension of the feature mapping, {$T$}TT is the horizon, and {$D$}DD is the diameter of the MDP. We also prove a matching lower bound {\~{\textohm}}~({$dDT$}----{\textsurd}){\textohm}{\textasciitilde}(dDT){\textbackslash}tilde\{{\textbackslash}Omega\}(d{\textbackslash}sqrt\{DT\}), which suggests that the proposed UCRL2-VTR is minimax optimal up to logarithmic factors. To the best of our knowledge, our algorithm is the first nearly minimax optimal RL algorithm with function approximation in the infinite-horizon average-reward setting.},
  langid = {english},
  file = {/Users/luweber/Zotero/storage/HIKQULZS/Wu et al. - 2022 - Nearly Minimax Optimal Regret for Learning Infinit.pdf}
}

@article{xiaoOptimizationModelElectric2020,
  title = {An Optimization Model for Electric Vehicle Charging Infrastructure Planning Considering Queuing Behavior with Finite Queue Length},
  author = {Xiao, Dan and An, Shi and Cai, Hua and Wang, Jian and Cai, Haiming},
  year = {2020},
  month = jun,
  journal = {Journal of Energy Storage},
  volume = {29},
  pages = {101317},
  urldate = {2023-08-07},
  abstract = {As clean energy vehicles, electric vehicles (EVs) have been paid unprecedented attention in dealing with serious energy crises and heavy tailpipe emission in recent years. Due to its limited battery range and long charging time, it's significant to reasonably determine the locations and capacities of EV charging infrastructure. There are two research gaps in existing researches: unrealistically assuming the infinite queuing length based on the M/M/1 or M/M/S queuing model and lacking the research on variable quantities of chargers allocated at different charging stations. To fill up these gaps, we propose an optimal location model to determine the optimal locations and capacities of EV charging infrastructure to minimize the comprehensive total cost, which considers the charging queuing behavior with finite queue length and various siting constrains. And the results show that (1) the proposed model has a good performance in determining the optimal locations and capacities of EV charging infrastructure (i.e. the optimal locations of charging stations, the optimal quantities of chargers installed at each charging station, the optimal allowable maximum queue length and maximum capacity of each charging station); (2) the quantity of chargers and allowable maximum queue length at each charging station are consistent with the distribution densities of existing charging stations at these locations; (3) the two parameters of unit value of time and unit distance cost have a more significant impact on the total cost. Therefore, the total cost can be effectively reduced by appropriately increasing the quantity of chargers at each charging station and the distribution density of charging stations.},
  langid = {english},
  keywords = {Charging infrastructure,Electric vehicles,Facility location,Finite queue length,Queuing theory},
  file = {/Users/luweber/Zotero/storage/UDG5UYEA/S2352152X19309053.html}
}

@inproceedings{zanetteTighterProblemDependentRegret2019,
  title = {Tighter {{Problem-Dependent Regret Bounds}} in {{Reinforcement Learning}} without {{Domain Knowledge}} Using {{Value Function Bounds}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Zanette, Andrea and Brunskill, Emma},
  year = {2019},
  month = may,
  pages = {7304--7312},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-08-08},
  abstract = {Strong worst-case performance bounds for episodic reinforcement learning exist but fortunately in practice RL algorithms perform much better than such bounds would predict. Algorithms and theory that provide strong problem-dependent bounds could help illuminate the key features of what makes a RL problem hard and reduce the barrier to using RL algorithms in practice. As a step towards this we derive an algorithm and analysis for finite horizon discrete MDPs with state-of-the-art worst-case regret bounds and substantially tighter bounds if the RL environment has special features but without apriori knowledge of the environment from the algorithm. As a result of our analysis, we also help address an open learning theory question~{\textbackslash}cite\{jiang2018open\} about episodic MDPs with a constant upper-bound on the sum of rewards, providing a regret bound function of the number of episodes with no dependence on the horizon.},
  langid = {english},
  file = {/Users/luweber/Zotero/storage/7TEJCP8V/Zanette et Brunskill - 2019 - Tighter Problem-Dependent Regret Bounds in Reinfor.pdf}
}

@inproceedings{zhangRegretMinimizationReinforcement2019a,
  title = {Regret {{Minimization}} for {{Reinforcement Learning}} by {{Evaluating}} the {{Optimal Bias Function}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Zihan and Ji, Xiangyang},
  year = {2019},
  volume = {32},
  pages = {2827--2836},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-08-01},
  file = {/Users/luweber/Zotero/storage/DUSCGDGW/Zhang et Ji - 2019 - Regret Minimization for Reinforcement Learning by .pdf}
}

@inproceedings{zhouReinforcementLearningbasedAdaptive2017,
  title = {Reinforcement Learning-Based Adaptive Resource Management of Differentiated Services in Geo-Distributed Data Centers},
  booktitle = {2017 {{IEEE}}/{{ACM}} 25th {{International Symposium}} on {{Quality}} of {{Service}} ({{IWQoS}})},
  author = {Zhou, Xiaojie and Wang, Kun and Jia, Weijia and Guo, Minyi},
  year = {2017},
  month = jun,
  pages = {1--6},
  urldate = {2024-02-01},
  abstract = {For better service provision and utilization of renewable energy, Internet service providers have already built their data centers in geographically distributed locations. These companies balance quality of service (QoS) revenue and power consumption by migrating virtual machines (VMs) and allocating the resource of servers adaptively. However, existing approaches model the QoS revenue by service-level agreement (SLA) violation, and ignore the network communication cost and immigration time. In this paper, we propose a reinforcement learning-based adaptive resource management algorithm, which aims to get the balance between QoS revenue and power consumption. Our algorithm does not need to assume prior distribution of resource requirements, and is robust in actual workload. It outperforms other existing approaches in three aspects: (1) The QoS revenue is directly modeled by differentiated revenue of different tasks, instead of using SLA violation. (2) For geodistributed data centers, the time spent on VM migration and network communication cost are taken into consideration. (3) The information storage and random action selection of reinforcement learning algorithms are optimized for rapid decision making. Experiments show that our proposed algorithm is more robust than the existing algorithms. Besides, the power consumption of our algorithm is around 13.3\% and 9.6\% better than the existing algorithms in non-differentiated and differentiated services.},
  keywords = {Differentiated services,Geo-distributed data centers,Learning (artificial intelligence),Power consumption,Power demand,QoS revenue,Quality of service,Reinforcement learning,Resource management,Servers,Silicon,Switches},
  file = {/Users/luweber/Zotero/storage/EUVJP4QU/Zhou et al. - 2017 - Reinforcement learning-based adaptive resource man.pdf;/Users/luweber/Zotero/storage/3LX9567R/7969161.html}
}
