@inproceedings{huang,
	title        = {GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
	author       = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and Chen, zhifeng},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems}
}
@inproceedings{han,
	title        = {Learning both Weights and Connections for Efficient Neural Network},
	author       = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
	year         = 2015,
	booktitle    = {Advances in Neural Information Processing Systems}
}
@article{sze,
	title        = {Efficient Processing of Deep Neural Networks: A Tutorial and Survey},
	author       = {V. {Sze} and Y. {Chen} and T. {Yang} and J. S. {Emer}},
	year         = 2017,
	journal      = {Proceedings of the IEEE}
}
@inproceedings{Yang_2017_CVPR,
	title        = {Designing Energy-Efficient Convolutional Neural Networks Using Energy-Aware Pruning},
	author       = {Yang, Tien-Ju and Chen, Yu-Hsin and Sze, Vivienne},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@article{janowsky,
	title        = {Pruning versus clipping in neural networks},
	author       = {Janowsky, Steven A.},
	year         = 1989,
	journal      = {Phys. Rev. A}
}
@inproceedings{mozer,
	title        = {Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment},
	author       = {Mozer, Michael C and Smolensky, Paul},
	year         = 1989,
	booktitle    = {Advances in Neural Information Processing Systems}
}
@article{karnin,
	title        = {A simple procedure for pruning back-propagation trained neural networks},
	author       = {E. D. {Karnin}},
	year         = 1990,
	journal      = {IEEE Transactions on Neural Networks}
}
@inproceedings{frankle2018the,
	title        = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
	author       = {Jonathan Frankle and Michael Carbin},
	year         = 2019,
	booktitle    = {International Conference on Learning Representations}
}
@article{gale,
	title        = {The state of sparsity in deep neural networks},
	author       = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1902.09574}
}
@inproceedings{lee2018snip,
	title        = {{SNIP}: Single-shot Network Pruning based on Connection Sensitivity},
	author       = {Namhoon Lee and Thalaiyasingam Ajanthan and Philip Torr},
	year         = 2019,
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{pmlr-v70-molchanov17a,
	title        = {Variational Dropout Sparsifies Deep Neural Networks},
	author       = {Dmitry Molchanov and Arsenii Ashukha and Dmitry Vetrov},
	year         = 2017,
	series       = {Proceedings of Machine Learning Research}
}
@inproceedings{louizos2018learning,
	title        = {Learning Sparse Neural Networks through $L_0$ Regularization},
	author       = {Christos Louizos and Max Welling and Diederik P. Kingma},
	year         = 2018,
	booktitle    = {International Conference on Learning Representations}
}
@article{mocanu2018scalable,
	title        = {Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
	author       = {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
	year         = 2018,
	journal      = {Nature communications}
}
@inproceedings{evci2020rigging,
	title        = {Rigging the Lottery: Making All Tickets Winners},
	author       = {Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
	year         = 2020,
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	series       = {Proceedings of Machine Learning Research}
}
@inproceedings{You2020Drawing,
	title        = {Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks},
	author       = {Haoran You and Chaojian Li and Pengfei Xu and Yonggan Fu and Yue Wang and Xiaohan Chen and Richard G. Baraniuk and Zhangyang Wang and Yingyan Lin},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations}
}
@article{verdenius2020pruning,
	title        = {Pruning via Iterative Ranking of Sensitivity Statistics},
	author       = {Verdenius, Stijn and Stol, Maarten and Forr{\'e}, Patrick},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2006.00896}
}
@inproceedings{Li,
	title        = {Pruning Filters for Efficient ConvNets},
	author       = {Hao Li and Asim Kadav and Igor Durdanovic and Hanan Samet and Hans Peter Graf},
	year         = 2017,
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	biburl       = {https://dblp.org/rec/conf/iclr/0022KDSG17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{He_2017_ICCV,
	title        = {Channel Pruning for Accelerating Very Deep Neural Networks},
	author       = {He, Yihui and Zhang, Xiangyu and Sun, Jian},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)}
}
@inproceedings{Wang2020Picking,
	title        = {Picking Winning Tickets Before Training by Preserving Gradient Flow},
	author       = {Chaoqi Wang and Guodong Zhang and Roger Grosse},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations}
}
@article{tanaka2020pruning,
	title        = {Pruning neural networks without any data by iteratively conserving synaptic flow},
	author       = {Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel LK and Ganguli, Surya},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2006.05467}
}
@inproceedings{gohil2020one,
	title        = {One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
	author       = {Morcos, Ari and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems}
}
@inproceedings{liu2018rethinking,
	title        = {Rethinking the Value of Network Pruning},
	author       = {Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},
	year         = 2019,
	booktitle    = {International Conference on Learning Representations}
}
@misc{frankle2020stabilizing,
	title        = {Stabilizing the Lottery Ticket Hypothesis},
	author       = {Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},
	year         = 2020,
	eprint       = {1903.01611},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{Renda2020Comparing,
	title        = {Comparing Rewinding and Fine-tuning in Neural Network Pruning},
	author       = {Alex Renda and Jonathan Frankle and Michael Carbin},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{frankle2020linear,
	title        = {Linear Mode Connectivity and the Lottery Ticket Hypothesis},
	author       = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
	year         = 2020,
	booktitle    = {Proceedings of the 37th International Conference on Machine Learning},
	series       = {Proceedings of Machine Learning Research}
}
@article{dejorge2020progressive,
	title        = {Progressive skeletonization: Trimming more fat from a network at initialization},
	author       = {de Jorge, Pau and Sanyal, Amartya and Behl, Harkirat S and Torr, Philip HS and Rogez, Gregory and Dokania, Puneet K},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2006.09081}
}
@article{blalock2020state,
	title        = {What is the state of neural network pruning?},
	author       = {Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2003.03033}
}
@inproceedings{molchanov,
	title        = {Pruning Convolutional Neural Networks for Resource Efficient Inference},
	author       = {Pavlo Molchanov and Stephen Tyree and Tero Karras and Timo Aila and Jan Kautz},
	year         = 2017,
	booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
	biburl       = {https://dblp.org/rec/conf/iclr/MolchanovTKAK17.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{hassibi,
	title        = {Optimal Brain Surgeon and general network pruning},
	author       = {B. {Hassibi} and D. G. {Stork} and G. J. {Wolff}},
	year         = 1993,
	booktitle    = {IEEE International Conference on Neural Networks}
}
@inproceedings{dong,
	title        = {Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon},
	author       = {Dong, Xin and Chen, Shangyu and Pan, Sinno},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems}
}
@incollection{lecun,
	title        = {Optimal Brain Damage},
	author       = {LeCun, Yann and John S. Denker and Sara A. Solla},
	year         = 1990,
	booktitle    = {Advances in Neural Information Processing Systems 2}
}
@inproceedings{wangchaoqi,
	title        = {EigenDamage: Structured Pruning in the Kronecker-Factored Eigenbasis},
	author       = {Chaoqi Wang and Roger B. Grosse and Sanja Fidler and Guodong Zhang},
	year         = 2019,
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
	series       = {Proceedings of Machine Learning Research},
	biburl       = {https://dblp.org/rec/conf/icml/WangGFZ19.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{srinivas2016generalized,
	title        = {Generalized dropout},
	author       = {Srinivas, Suraj and Babu, R Venkatesh},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1611.06791}
}
@misc{labach2020framework,
	title        = {A Framework for Neural Network Pruning Using Gibbs Distributions},
	author       = {Alex Labach and Shahrokh Valaee},
	year         = 2020,
	eprint       = {2006.04981},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{ijcai2020-358,
	title        = {Beyond Network Pruning: a Joint Search-and-Training Approach},
	author       = {Lu, Xiaotong and Huang, Han and Dong, Weisheng and Li, Xin and Shi, Guangming},
	year         = 2020,
	booktitle    = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, {IJCAI-20}}
}
@article{su2020sanitychecking,
	title        = {Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot},
	author       = {Su, Jingtong and Chen, Yihang and Cai, Tianle and Wu, Tianhao and Gao, Ruiqi and Wang, Liwei and Lee, Jason D},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems}
}
@inproceedings{ding,
	title        = {Centripetal SGD for Pruning Very Deep Convolutional Networks With Complicated Structure},
	author       = {X. {Ding} and G. {Ding} and Y. {Guo} and J. {Han}},
	year         = 2019,
	booktitle    = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@article{dettmers2020sparse,
	title        = {Sparse networks from scratch: Faster training without losing performance},
	author       = {Dettmers, Tim and Zettlemoyer, Luke},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1907.04840}
}
@inproceedings{mostafa2019parameter,
	title        = {Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization},
	author       = {Mostafa, Hesham and Wang, Xin},
	year         = 2019,
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	series       = {Proceedings of Machine Learning Research}
}
@inproceedings{bellec2018deep,
	title        = {Deep Rewiring: Training very sparse deep networks},
	author       = {Guillaume Bellec and David Kappel and Wolfgang Maass and Robert Legenstein},
	year         = 2018,
	booktitle    = {International Conference on Learning Representations}
}
@article{frankle2020pruning,
	title        = {Pruning Neural Networks at Initialization: Why are We Missing the Mark?},
	author       = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2009.08576}
}
@inproceedings{Yu2020Playing,
	title        = {Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP},
	author       = {Haonan Yu and Sergey Edunov and Yuandong Tian and Ari S. Morcos},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{brix2020successfully,
	title        = {Successfully Applying the Stabilized Lottery Ticket Hypothesis to the Transformer Architecture},
	author       = {Brix, Christopher  and Bahar, Parnia  and Ney, Hermann},
	year         = 2020,
	booktitle    = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}
}
@article{chen2020lottery,
	title        = {The lottery ticket hypothesis for pre-trained bert networks},
	author       = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems}
}
@article{brown2020language,
	title        = {Language models are few-shot learners. arXiv 2020},
	author       = {Brown, TB and Mann, B and Ryder, N and Subbiah, M and Kaplan, J and Dhariwal, P and Neelakantan, A and Shyam, P and Sastry, G and Askell, A and others},
	journal      = {arXiv preprint arXiv:2005.14165}
}
@inproceedings{Lee2020A,
	title        = {A Signal Propagation Perspective for Pruning Neural Networks at Initialization},
	author       = {Namhoon Lee and Thalaiyasingam Ajanthan and Stephen Gould and Philip H. S. Torr},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{pmlr-v80-kalchbrenner18a,
	title        = {Efficient Neural Audio Synthesis},
	author       = {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and van den Oord, Aaron and Dieleman, Sander and Kavukcuoglu, Koray},
	year         = 2018,
	booktitle    = {Proceedings of the 35th International Conference on Machine Learning},
	series       = {Proceedings of Machine Learning Research}
}
@article{Gray2017GPUKF,
	title        = {Gpu kernels for block-sparse weights},
	author       = {Gray, Scott and Radford, Alec and Kingma, Diederik P},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1711.09224}
}
@article{sagun2018empirical,
	title        = {Empirical analysis of the hessian of over-parametrized neural networks},
	author       = {Sagun, Levent and Evci, Utku and Guney, V Ugur and Dauphin, Yann and Bottou, Leon},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1706.04454}
}
@article{gur-ari2019gradient,
	title        = {Gradient descent happens in a tiny subspace},
	author       = {Gur-Ari, Guy and Roberts, Daniel A and Dyer, Ethan},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1812.04754}
}
@inproceedings{Yang2020DeepHoyer:,
	title        = {DeepHoyer: Learning Sparser Neural Network with Differentiable Scale-Invariant Sparsity Measures},
	author       = {Huanrui Yang and Wei Wen and Hai Li},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{kamalika,
	title        = {An Investigation into Neural Net Optimization via Hessian Eigenvalue Density},
	author       = {Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
	year         = 2019,
	booktitle    = {Proceedings of the 36th International Conference on Machine Learning},
	series       = {Proceedings of Machine Learning Research}
}
@article{yao2020pyhessian,
	title        = {PyHessian: Neural networks through the lens of the Hessian},
	author       = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1912.07145}
}
@inproceedings{NEURIPS2019_8558cb40,
	title        = {Can you trust your model\textquotesingle s uncertainty?  Evaluating predictive uncertainty under dataset shift},
	author       = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems}
}
@article{szegedy2013intriguing,
	title        = {Intriguing properties of neural networks. arXiv 2013},
	author       = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	year         = 2013,
	journal      = {arXiv preprint arXiv:1312.6199}
}
@misc{thom2016sparse,
	title        = {Sparse Activity and Sparse Connectivity in Supervised Learning},
	author       = {Markus Thom and Günther Palm},
	year         = 2016,
	eprint       = {1603.08367},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{thom2015sparse,
	title        = {Sparse Neural Networks},
	author       = {Markus Thom},
	year         = 2015,
	journal      = {Open Access Repositorium der Universität Ulm und Technischen Hochschule Ulm}
}
@book{kandel1991principle-neural-science,
	title        = {Principles of Neural Science},
	year         = 1991
}
@article{olshausen2004sparse-coding,
	title        = {Sparse coding of sensory inputs},
	author       = {Bruno A Olshausen and David J Field},
	year         = 2004,
	journal      = {Current Opinion in Neurobiology}
}
@article{lennie2003cost-cortical-computation,
	title        = {The Cost of Cortical Computation},
	author       = {Peter Lennie},
	year         = 2003,
	journal      = {Current Biology}
}
@article{chechik1998synaptic-pruning,
	title        = {Synaptic Pruning in Development: A Computational Account},
	author       = {Chechik, Gal and Meilijson, Isaac and Ruppin, Eytan},
	year         = 1998,
	journal      = {Neural computation}
}
@inproceedings{lubana2021gradient-flow-framework,
	title        = {A Gradient Flow Framework For Analyzing Network Pruning},
	author       = {Ekdeep Singh Lubana and Robert Dick},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{woodworth2020regime,
	title        = {Kernel and Rich Regimes in Overparametrized Models},
	author       = {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
	year         = 2020,
	booktitle    = {Proceedings of Thirty Third Conference on Learning Theory}
}
@misc{goldblum2020truth,
	title        = {Truth or Backpropaganda? An Empirical Investigation of Deep Learning Theory},
	author       = {Micah Goldblum and Jonas Geiping and Avi Schwarzschild and Michael Moeller and Tom Goldstein},
	year         = 2020,
	eprint       = {1910.00359},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{you2019gate,
	title        = {Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks},
	author       = {Zhonghui You and Kun Yan and Jinmian Ye and Meng Ma and Ping Wang},
	year         = 2019,
	eprint       = {1909.08174},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@misc{liu2017learning,
	title        = {Learning Efficient Convolutional Networks through Network Slimming},
	author       = {Zhuang Liu and Jianguo Li and Zhiqiang Shen and Gao Huang and Shoumeng Yan and Changshui Zhang},
	year         = 2017,
	eprint       = {1708.06519},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@article{li2016pruning,
	title        = {Pruning Filters for Efficient ConvNets},
	author       = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
	year         = 2016
}
@misc{frankle2021pruning,
	title        = {Pruning Neural Networks at Initialization: Why are We Missing the Mark?},
	author       = {Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},
	year         = 2021,
	eprint       = {2009.08576},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{achille2019critical,
	title        = {Critical Learning Periods in Deep Neural Networks},
	author       = {Alessandro Achille and Matteo Rovere and Stefano Soatto},
	year         = 2019,
	eprint       = {1711.08856},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{luo2020phase,
	title        = {Phase diagram for two-layer ReLU neural networks at infinite-width limit},
	author       = {Tao Luo and Zhi-Qin John Xu and Zheng Ma and Yaoyu Zhang},
	year         = 2020,
	eprint       = {2007.07497},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{sun2019optimization,
	title        = {Optimization for deep learning: theory and algorithms},
	author       = {Ruoyu Sun},
	year         = 2019,
	eprint       = {1912.08957},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{amari2020target,
	title        = {Any Target Function Exists in a Neighborhood of Any Sufficiently Wide Random Network: A Geometrical Perspective},
	author       = {Shun-ichi Amari},
	year         = 2020,
	eprint       = {2001.06931},
	archiveprefix = {arXiv},
	primaryclass = {stat.ML}
}
@misc{ghorbani2020linearized,
	title        = {Linearized two-layers neural networks in high dimension},
	author       = {Behrooz Ghorbani and Song Mei and Theodor Misiakiewicz and Andrea Montanari},
	year         = 2020,
	eprint       = {1904.12191},
	archiveprefix = {arXiv},
	primaryclass = {math.ST}
}
@inproceedings{NEURIPS2019_1113d7a7,
	title        = {Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask},
	author       = {Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems}
}
@inproceedings{jorge2021progressive,
	title        = {Progressive Skeletonization: Trimming more fat from a network at initialization},
	author       = {Pau de Jorge and Amartya Sanyal and Harkirat Behl and Philip Torr and Gr{\'e}gory Rogez and Puneet K. Dokania},
	year         = 2021,
	booktitle    = {International Conference on Learning Representations}
}
@inproceedings{jacot2018,
	title        = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
	author       = {Arthur Jacot and Clément Hongler and Franck Gabriel},
	year         = 2018,
	booktitle    = {NeurIPS},
	cdate        = 1514764800000
}
@article{Krizhevsky09,
	title        = {Learning multiple layers of features from tiny images},
	author       = {Krizhevsky, A. and Hinton, G.},
	year         = 2009,
	journal      = {Master's thesis, Department of Computer Science, University of Toronto}
}
@inproceedings{5206848,
	title        = {ImageNet: A large-scale hierarchical image database},
	author       = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
	year         = 2009,
	booktitle    = {2009 IEEE Conference on Computer Vision and Pattern Recognition}
}
@inproceedings{adamoptimizer,
	title        = {Adam: A Method for Stochastic Optimization},
	author       = {Diederik P. Kingma and Jimmy Ba},
	year         = 2015,
	booktitle    = {ICLR (Poster)},
	cdate        = 1420070400000
}
@article{codecarbon,
	title        = {{CodeCarbon: Estimate and Track Carbon Emissions from Machine Learning Computing}},
	author       = {Victor Schmidt and Kamal Goyal and Aditya Joshi and Boris Feld and Liam Conell and Nikolas Laskaris and Doug Blank and Jonathan Wilson and Sorelle Friedler and Sasha Luccioni},
	year         = 2021
}
@inproceedings{Silberman:ECCV12,
	title        = {Indoor Segmentation and Support Inference from RGBD Images},
	author       = {Nathan Silberman, Derek Hoiem, Pushmeet Kohli and Rob Fergus},
	year         = 2012,
	booktitle    = {ECCV}
}
@inproceedings{laina2016deeper,
	title        = {Deeper depth prediction with fully convolutional residual networks},
	author       = {Laina, Iro and Rupprecht, Christian and Belagiannis, Vasileios and Tombari, Federico and Navab, Nassir},
	year         = 2016,
	booktitle    = {3D Vision (3DV), 2016 Fourth International Conference on},
	organization = {IEEE}
}
@article{Merity2017PointerSM,
	title        = {Pointer Sentinel Mixture Models},
	author       = {Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
	year         = 2017,
	journal      = {ArXiv}
}
@article{PTBDATASET,
	title        = {Building a Large Annotated Corpus of English: The Penn Treebank},
	author       = {Marcus, Mitchell P. and Marcinkiewicz, Mary Ann and Santorini, Beatrice},
	year         = 1993,
	journal      = {Comput. Linguist.},
	issn         = {0891-2017},
	issue_date   = {June 1993}
}
@misc{openaigym,
	title        = {OpenAI Gym},
	author       = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
	year         = 2016,
	eprint       = {arXiv:1606.01540}
}
@incollection{NEURIPS2019_9015,
	title        = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	author       = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems 32}
}
@inproceedings{10.1145/1583991.1584053,
	title        = {Parallel Sparse Matrix-Vector and Matrix-Transpose-Vector Multiplication Using Compressed Sparse Blocks},
	author       = {Bulu\c{c}, Aydin and Fineman, Jeremy T. and Frigo, Matteo and Gilbert, John R. and Leiserson, Charles E.},
	year         = 2009,
	booktitle    = {Proceedings of the Twenty-First Annual Symposium on Parallelism in Algorithms and Architectures},
	series       = {SPAA '09},
}

@misc{dehghani2021efficiency,
      title={The Efficiency Misnomer}, 
      author={Mostafa Dehghani and Anurag Arnab and Lucas Beyer and Ashish Vaswani and Yi Tay},
      year={2021},
      eprint={2110.12894},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{akbik2019flair,
  title={FLAIR: An easy-to-use framework for state-of-the-art NLP},
  author={Akbik, Alan and Bergmann, Tanja and Blythe, Duncan and Rasul, Kashif and Schweter, Stefan and Vollgraf, Roland},
  booktitle={{NAACL} 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)},
  year={2019}
}

@ARTICLE{8816678,
  author={He, Yang and Dong, Xuanyi and Kang, Guoliang and Fu, Yanwei and Yan, Chenggang and Yang, Yi},
  journal={IEEE Transactions on Cybernetics}, 
  title={Asymptotic Soft Filter Pruning for Deep Convolutional Neural Networks}, 
  year={2020},
  volume={50},
  number={8},
  pages={3594-3604},
  doi={10.1109/TCYB.2019.2933477}}
  
@InProceedings{He_2019_CVPR,
author = {He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
title = {Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@inproceedings{10.1145/3295500.3356156,
author = {Lym, Sangkug and Choukse, Esha and Zangeneh, Siavash and Wen, Wei and Sanghavi, Sujay and Erez, Mattan},
title = {PruneTrain: Fast Neural Network Training by Dynamic Sparse Model Reconfiguration},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356156},
doi = {10.1145/3295500.3356156},
abstract = {State-of-the-art convolutional neural networks (CNNs) used in vision applications have large models with numerous weights. Training these models is very compute- and memory-resource intensive. Much research has been done on pruning or compressing these models to reduce the cost of inference, but little work has addressed the costs of training. We focus precisely on accelerating training. We propose PruneTrain, a cost-efficient mechanism that gradually reduces the training cost during training. PruneTrain uses a structured group-lasso regularization approach that drives the training optimization toward both high accuracy and small weight values. Small weights can then be periodically removed by reconfiguring the network model to a smaller one. By using a structured-pruning approach and additional reconfiguration techniques we introduce, the pruned model can still be efficiently processed on a GPU accelerator. Overall, PruneTrain achieves a reduction of 39% in the end-to-end training time of ResNet50 for ImageNet by reducing computation cost by 40% in FLOPs, memory accesses by 37% for memory bandwidth bound layers, and the inter-accelerator communication by 55%.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {36},
numpages = {13},
location = {Denver, Colorado},
series = {SC '19}
}

@article{liu2021sparse,
  title={Sparse Training via Boosting Pruning Plasticity with Neuroregeneration},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Atashgahi, Zahra and Yin, Lu and Kou, Huanyu and Shen, Li and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
  journal={arXiv preprint arXiv:2106.10404},
  year={2021}
}

@article{zhou_learning_2021,
	title = {Learning {N}:{M} {Fine}-grained {Structured} {Sparse} {Neural} {Networks} {From} {Scratch}},
	abstract = {Sparsity in Deep Neural Networks (DNNs) has been widely studied to compress and accelerate the models on resource-constrained environments. It can be generally categorized into unstructured ﬁne-grained sparsity that zeroes out multiple individual weights distributed across the neural network, and structured coarsegrained sparsity which prunes blocks of sub-networks of a neural network. Finegrained sparsity can achieve a high compression ratio but is not hardware friendly and hence receives limited speed gains. On the other hand, coarse-grained sparsity cannot concurrently achieve both apparent acceleration on modern GPUs and decent performance. In this paper, we are the ﬁrst to study training from scratch an N:M ﬁne-grained structured sparse network, which can maintain the advantages of both unstructured ﬁne-grained sparsity and structured coarse-grained sparsity simultaneously on speciﬁcally designed GPUs. Speciﬁcally, a 2 : 4 sparse network could achieve 2× speed-up without performance drop on Nvidia A100 GPUs. Furthermore, we propose a novel and effective ingredient, sparse-reﬁned straightthrough estimator (SR-STE), to alleviate the negative inﬂuence of the approximated gradients computed by vanilla STE during optimization. We also deﬁne a metric, Sparse Architecture Divergence (SAD), to measure the sparse network’s topology change during the training process. Finally, We justify SR-STE’s advantages with SAD and demonstrate the effectiveness of SR-STE by performing comprehensive experiments on various tasks. Source codes and models are available at https://github.com/NM-sparsity/NM-sparsity.},
	language = {en},
	journal = {9th International Conference on Learning Representations, \{ICLR\} 2021},
	author = {Zhou, Aojun and Ma, Yukun and Zhu, Junnan and Liu, Jianbo and Zhang, Zhijie and Yuan, Kun and Sun, Wenxiu and Li, Hongsheng},
	year = {2021},
	pages = {15},
	file = {Zhou et al. - 2021 - LEARNING NM FINE-GRAINED STRUCTURED SPARSE NEURAL.pdf:/Users/simon/Zotero/storage/FM8K9LQZ/Zhou et al. - 2021 - LEARNING NM FINE-GRAINED STRUCTURED SPARSE NEURAL.pdf:application/pdf},
}

@TECHREPORT{Krizhevsky09learningmultiple,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}