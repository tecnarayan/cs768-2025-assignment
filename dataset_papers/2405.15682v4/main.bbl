\begin{thebibliography}{}

\bibitem[Bach and Moulines, 2013]{bachmoulines}
Bach, F. and Moulines, E. (2013).
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate {$O(1/n)$}.
\newblock In Burges, C., Bottou, L., Welling, M., Ghahramani, Z., and
  Weinberger, K., editors, {\em Advances in Neural Information Processing
  Systems}, volume~26. Curran Associates, Inc.

\bibitem[Battaglia et~al., 2018]{battaglia2018relational}
Battaglia, P.~W., Hamrick, J.~B., Bapst, V., Sanchez-Gonzalez, A., Zambaldi,
  V., Malinowski, M., Tacchetti, A., Raposo, D., Santoro, A., Faulkner, R.,
  Gulcehre, C., Song, F., Ballard, A., Gilmer, J., Dahl, G., Vaswani, A.,
  Allen, K., Nash, C., Langston, V., Dyer, C., Heess, N., Wierstra, D., Kohli,
  P., Botvinick, M., Vinyals, O., Li, Y., and Pascanu, R. (2018).
\newblock Relational inductive biases, deep learning, and graph networks.

\bibitem[Bojar et~al., 2017]{wmt17}
Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huang, S.,
  Huck, M., Koehn, P., Liu, Q., Logacheva, V., Monz, C., Negri, M., Post, M.,
  Rubino, R., Specia, L., and Turchi, M. (2017).
\newblock Findings of the 2017 conference on machine translation (wmt17).
\newblock In {\em Proceedings of the 2017 Conference on Machine Translation
  (WMT17)}.

\bibitem[Cesa-Bianchi et~al., 2004]{cesa2004generalization}
Cesa-Bianchi, N., Conconi, A., and Gentile, C. (2004).
\newblock On the generalization ability of on-line learning algorithms.
\newblock {\em IEEE Transactions on Information Theory}, 50(9):2050--2057.

\bibitem[Cettolo et~al., 2014]{cettolo2014report}
Cettolo, M., Niehues, J., St{\"u}ker, S., Bentivogli, L., and Federico, M.
  (2014).
\newblock Report on the 11th {IWSLT} evaluation campaign.
\newblock In {\em IWSLT}.

\bibitem[Chiang et~al., 2012]{chiang2012online}
Chiang, C.-K., Yang, T., Lee, C.-J., Mahdavi, M., Lu, C.-J., Jin, R., and Zhu,
  S. (2012).
\newblock Online optimization with gradual variations.
\newblock In {\em Conference on Learning Theory}, pages 6--1. JMLR Workshop and
  Conference Proceedings.

\bibitem[Criteo, 2022]{criteo1tb}
Criteo (2022).
\newblock Criteo {1TB} click logs dataset.
\newblock
  \url{https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/}.

\bibitem[Cutkosky, 2019]{cutkosky2019anytime}
Cutkosky, A. (2019).
\newblock Anytime online-to-batch, optimism and acceleration.
\newblock In {\em International conference on machine learning}, pages
  1446--1454. PMLR.

\bibitem[Dahl et~al., 2023]{Dahl2023AlgoPerf}
Dahl, G.~E., Schneider, F., Nado, Z., Agarwal, N., Sastry, C.~S., Hennig, P.,
  Medapati, S., Eschenhagen, R., Kasimbeg, P., Suo, D., Bae, J., Gilmer, J.,
  Peirson, A.~L., Khan, B., Anil, R., Rabbat, M., Krishnan, S., Snider, D.,
  Amid, E., Chen, K., Maddison, C.~J., Vasudev, R., Badura, M., Garg, A., and
  Mattson, P. (2023).
\newblock {Benchmarking Neural Network Training Algorithms}.

\bibitem[Defazio, 2020]{defazio2020mom}
Defazio, A. (2020).
\newblock Momentum via primal averaging: Theoretical insights and learning rate
  schedules for non-convex optimization.

\bibitem[Defazio et~al., 2023]{defazio2023when}
Defazio, A., Cutkosky, A., Mehta, H., and Mishchenko, K. (2023).
\newblock When, why and how much? adaptive learning rate scheduling by
  refinement.

\bibitem[Defazio and Gower, 2021]{defazio2021factorial}
Defazio, A. and Gower, R.~M. (2021).
\newblock The power of factorial powers: New parameter settings for
  (stochastic) optimization.
\newblock In Balasubramanian, V.~N. and Tsang, I., editors, {\em Proceedings of
  The 13th Asian Conference on Machine Learning}, volume 157 of {\em
  Proceedings of Machine Learning Research}, pages 49--64. PMLR.

\bibitem[Defazio and Jelassi, 2022]{defazio2022adaptivity}
Defazio, A. and Jelassi, S. (2022).
\newblock Adaptivity without compromise: A momentumized, adaptive, dual
  averaged gradient method for stochastic optimization.
\newblock {\em Journal of Machine Learning Research}, 23:1--34.

\bibitem[Defazio and Mishchenko, 2023]{defazio2023dadapt}
Defazio, A. and Mishchenko, K. (2023).
\newblock Learning-rate-free learning by {D}-adaptation.
\newblock {\em The 40th International Conference on Machine Learning (ICML
  2023)}.

\bibitem[Dehghani et~al., 2023]{pmlr-v202-dehghani23a}
Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J.,
  Steiner, A.~P., Caron, M., Geirhos, R., Alabdulmohsin, I., Jenatton, R.,
  Beyer, L., Tschannen, M., Arnab, A., Wang, X., Riquelme~Ruiz, C., Minderer,
  M., Puigcerver, J., Evci, U., Kumar, M., Steenkiste, S.~V., Elsayed, G.~F.,
  Mahendran, A., Yu, F., Oliver, A., Huot, F., Bastings, J., Collier, M.,
  Gritsenko, A.~A., Birodkar, V., Vasconcelos, C.~N., Tay, Y., Mensink, T.,
  Kolesnikov, A., Pavetic, F., Tran, D., Kipf, T., Lucic, M., Zhai, X.,
  Keysers, D., Harmsen, J.~J., and Houlsby, N. (2023).
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
  and Scarlett, J., editors, {\em Proceedings of the 40th International
  Conference on Machine Learning}, volume 202 of {\em Proceedings of Machine
  Learning Research}, pages 7480--7512. PMLR.

\bibitem[Duchi et~al., 2011]{adagrad}
Duchi, J., Hazan, E., and Singer, Y. (2011).
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of Machine Learning Research}, 12(61).

\bibitem[Gokaslan and Cohen, 2019]{Gokaslan2019OpenWeb}
Gokaslan, A. and Cohen, V. (2019).
\newblock Openwebtext corpus.
\newblock \url{http://Skylion007.github.io/OpenWebTextCorpus}.

\bibitem[Gulati et~al., 2020]{gulati2020conformer}
Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang,
  S., Zhang, Z., Wu, Y., and Pang, R. (2020).
\newblock Conformer: Convolution-augmented transformer for speech recognition.

\bibitem[Hazan, 2022]{hazan2022introduction}
Hazan, E. (2022).
\newblock {\em Introduction to online convex optimization}.
\newblock MIT Press.

\bibitem[Hazan and Kale, 2010]{hazan2010extracting}
Hazan, E. and Kale, S. (2010).
\newblock Extracting certainty from uncertainty: Regret bounded by variation in
  costs.
\newblock {\em Machine learning}, 80:165--188.

\bibitem[He et~al., 2021]{MaskedAutoencoders2021}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'a}r, P., and Girshick, R. (2021).
\newblock Masked autoencoders are scalable vision learners.
\newblock {\em arXiv:2111.06377}.

\bibitem[He et~al., 2016]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J. (2016).
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}.

\bibitem[Hu et~al., 2020]{ogbg}
Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., and
  Leskovec, J. (2020).
\newblock Open graph benchmark: datasets for machine learning on graphs.
\newblock In {\em Proceedings of the 34th International Conference on Neural
  Information Processing Systems}.

\bibitem[Huang et~al., 2017]{densenet}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q. (2017).
\newblock Densely connected convolutional networks.
\newblock In {\em 2017 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 2261--2269.

\bibitem[Izmailov et~al., 2018]{izmailov2019averaging}
Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., and Wilson, A.~G.
  (2018).
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In {\em Conference on Uncertainty in Artificial Intelligence (UAI)}.

\bibitem[Jean-Baptiste~Tien, 2014]{criteo_small}
Jean-Baptiste~Tien, joycenv, O.~C. (2014).
\newblock Display advertising challenge.

\bibitem[Joulani et~al., 2017]{joulani2017modular}
Joulani, P., Gy{\"o}rgy, A., and Szepesv{\'a}ri, C. (2017).
\newblock A modular analysis of adaptive (non-) convex optimization: Optimism,
  composite objectives, and variational bounds.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 681--720. PMLR.

\bibitem[Joulani et~al., 2020]{joulani2020simpler}
Joulani, P., Raj, A., Gyorgy, A., and Szepesv{\'a}ri, C. (2020).
\newblock A simpler approach to accelerated optimization: iterative averaging
  meets optimism.
\newblock In {\em International conference on machine learning}, pages
  4984--4993. PMLR.

\bibitem[Kaddour, 2022]{kaddour2022stop}
Kaddour, J. (2022).
\newblock Stop wasting my time! saving days of {ImageNet} and {BERT} training
  with latest weight averaging.

\bibitem[Kavis et~al., 2019]{kavis2019unixgrad}
Kavis, A., Levy, K.~Y., Bach, F., and Cevher, V. (2019).
\newblock {UniXGrad}: A universal, adaptive algorithm with optimal guarantees
  for constrained optimization.
\newblock {\em Advances in neural information processing systems}, 32.

\bibitem[Kingma and Ba, 2014]{kingma2014adam}
Kingma, D.~P. and Ba, J. (2014).
\newblock Adam: a method for stochastic optimization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Lacoste-Julien et~al., 2012]{lacostejulien2012simpler}
Lacoste-Julien, S., Schmidt, M., and Bach, F. (2012).
\newblock A simpler approach to obtaining an $o(1/t)$ convergence rate for the
  projected stochastic subgradient method.

\bibitem[Lan, 2012]{lanaccel}
Lan, G. (2012).
\newblock An optimal method for stochastic composite optimization.
\newblock {\em Mathematical Programming}, 133(1):365--397.

\bibitem[Loshchilov and Hutter, 2019]{loshchilov2018decoupled}
Loshchilov, I. and Hutter, F. (2019).
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Naumov et~al., 2019]{DLRM19}
Naumov, M., Mudigere, D., Shi, H.~M., Huang, J., Sundaraman, N., Park, J.,
  Wang, X., Gupta, U., Wu, C., Azzolini, A.~G., Dzhulgakov, D., Mallevich, A.,
  Cherniavskii, I., Lu, Y., Krishnamoorthi, R., Yu, A., Kondratenko, V.,
  Pereira, S., Chen, X., Chen, W., Rao, V., Jia, B., Xiong, L., and
  Smelyanskiy, M. (2019).
\newblock Deep learning recommendation model for personalization and
  recommendation systems.
\newblock {\em CoRR}.

\bibitem[Nesterov, 1983]{nes-accel}
Nesterov, Y. (1983).
\newblock A method for solving a convex programming problem with convergence
  rate {$O(1/k^2)$}.
\newblock {\em Soviet Mathematics Doklady}.

\bibitem[Nesterov, 2013]{nesbook}
Nesterov, Y. (2013).
\newblock {\em Lectures on Convex Optimization}.
\newblock Springer Nature.

\bibitem[Nesterov and Shikhman, 2015]{nesterov2015quasi}
Nesterov, Y. and Shikhman, V. (2015).
\newblock Quasi-monotone subgradient methods for nonsmooth convex minimization.
\newblock {\em Journal of Optimization Theory and Applications},
  165(3):917--940.

\bibitem[Orabona, 2019]{orabona2019modern}
Orabona, F. (2019).
\newblock A modern introduction to online learning.
\newblock {\em arXiv preprint arXiv:1912.13213}.

\bibitem[Panayotov et~al., 2015]{librispeech}
Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. (2015).
\newblock Librispeech: An asr corpus based on public domain audio books.
\newblock In {\em 2015 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 5206--5210.

\bibitem[Polyak, 1990]{polyak}
Polyak, B. (1990).
\newblock New stochastic approximation type procedures.
\newblock {\em Avtomatica i Telemekhanika}, 7:98--107.

\bibitem[Portes et~al., 2022]{portes2022fast}
Portes, J., Blalock, D., Stephenson, C., and Frankle, J. (2022).
\newblock Fast benchmarking of accuracy vs. training time with cyclic learning
  rates.

\bibitem[Radford et~al., 2019]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019).
\newblock Language models are unsupervised multitask learners.
\newblock Technical report, OpenAI.

\bibitem[Rakhlin et~al., 2012]{scopt}
Rakhlin, A., Shamir, O., and Sridharan, K. (2012).
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock In {\em Proceedings of the 29th International Coference on
  International Conference on Machine Learning}.

\bibitem[Rakhlin and Sridharan, 2013]{rakhlin2013online}
Rakhlin, A. and Sridharan, K. (2013).
\newblock Online learning with predictable sequences.
\newblock In {\em Conference on Learning Theory}, pages 993--1019. PMLR.

\bibitem[Reddi et~al., 2018]{reddi2018convergence}
Reddi, S.~J., Kale, S., and Kumar, S. (2018).
\newblock On the convergence of {Adam} and beyond.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Ruppert, 1988]{ruppert}
Ruppert, D. (1988).
\newblock Efficient estimations from a slowly convergent {Robbins}-{Monro}
  process.
\newblock {\em Technical Report, Cornell University}.

\bibitem[Russakovsky et~al., 2015]{ILSVRC15}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L. (2015).
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock {\em International Journal of Computer Vision (IJCV)}, 115(3).

\bibitem[Sandler et~al., 2023]{sandler2023trainingtrajectories}
Sandler, M., Zhmoginov, A., Vladymyrov, M., and Miller, N. (2023).
\newblock Training trajectories, mini-batch losses and the curious role of the
  learning rate.

\bibitem[Sanyal et~al., 2023]{sanyal2023early}
Sanyal, S., Neerkaje, A., Kaddour, J., Kumar, A., and Sanghavi, S. (2023).
\newblock Early weight averaging meets high learning rates for {LLM}
  pre-training.

\bibitem[Sebbouh et~al., 2021]{sebbouh2021convergence}
Sebbouh, O., Gower, R.~M., and Defazio, A. (2021).
\newblock On the (asymptotic) convergence of stochastic gradient descent and
  stochastic heavy ball.
\newblock In {\em Conference on Learning Theory, COLT 2021}, Proceedings of
  Machine Learning Research. PMLR.

\bibitem[Shamir and Zhang, 2013]{pmlr-v28-shamir13}
Shamir, O. and Zhang, T. (2013).
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In {\em Proceedings of the 30th International Conference on Machine
  Learning}.

\bibitem[Sriram et~al., 2020]{sriram2020end}
Sriram, A., Zbontar, J., Murrell, T., Defazio, A., Zitnick, C.~L., Yakubova,
  N., Knoll, F., and Johnson, P. (2020).
\newblock End-to-end variational networks for accelerated {MRI} reconstruction.
\newblock In {\em International Conference on Medical Image Computing and
  Computer-Assisted Intervention}. Springer.

\bibitem[Sutskever et~al., 2013]{sut}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.~E. (2013).
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em Proceedings of the 30th International Conference on
  International Conference on Machine Learning - Volume 28}. JMLR.org.

\bibitem[Szegedy et~al., 2016]{inception}
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016).
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em 2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 2818--2826.

\bibitem[Tao et~al., 2018]{tao2018}
Tao, W., Pan, Z., Wu, G., and Tao, Q. (2018).
\newblock Primal averaging: A new gradient evaluation step to attain the
  optimal individual convergence.
\newblock {\em IEEE Transactions on Cybernetics}, PP:1--11.

\bibitem[Vaswani et~al., 2017]{transformer}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L.~u., and Polosukhin, I. (2017).
\newblock Attention is all you need.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc.

\bibitem[Wiseman and Rush, 2016]{wiseman-rush-2016-sequence}
Wiseman, S. and Rush, A.~M. (2016).
\newblock Sequence-to-sequence learning as beam-search optimization.
\newblock In {\em Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}. Association for Computational Linguistics.

\bibitem[Wu and Johnson, 2021]{wu2021rethinking}
Wu, Y. and Johnson, J. (2021).
\newblock Rethinking "batch" in batchnorm.

\bibitem[Zagoruyko and Komodakis, 2016]{BMVC2016_87}
Zagoruyko, S. and Komodakis, N. (2016).
\newblock Wide residual networks.
\newblock In {\em Proceedings of the British Machine Vision Conference (BMVC)}.

\bibitem[Zamani and Glineur, 2023]{zamani2023exact}
Zamani, M. and Glineur, F. (2023).
\newblock Exact convergence rate of the last iterate in subgradient methods.

\bibitem[Zbontar et~al., 2018]{zbontar2018fastmri}
Zbontar, J., Knoll, F., Sriram, A., Muckley, M.~J., Bruno, M., Defazio, A.,
  Parente, M., Geras, K.~J., Katsnelson, J., Chandarana, H., et~al. (2018).
\newblock {fastMRI}: An open dataset and benchmarks for accelerated {MRI}.
\newblock {\em arXiv preprint arXiv:1811.08839}.

\bibitem[Zhang et~al., 2019]{lookahead}
Zhang, M., Lucas, J., Ba, J., and Hinton, G.~E. (2019).
\newblock Lookahead optimizer: $k$ steps forward, 1 step back.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R., editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc.

\bibitem[Zinkevich, 2003]{zinkevich2003online}
Zinkevich, M. (2003).
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In {\em Proceedings of the Twentieth International Conference on
  International Conference on Machine Learning}, pages 928--935.

\end{thebibliography}
