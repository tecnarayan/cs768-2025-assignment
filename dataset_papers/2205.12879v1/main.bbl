\begin{thebibliography}{10}

\bibitem{agarwal2016second}
Naman Agarwal, Brian Bullins, and Elad Hazan.
\newblock Second-order stochastic optimization in linear time.
\newblock {\em stat}, 1050:15, 2016.

\bibitem{bach2019snorkel}
Stephen~H Bach, Daniel Rodriguez, Yintao Liu, Chong Luo, Haidong Shao,
  Cassandra Xia, Souvik Sen, Alex Ratner, Braden Hancock, Houman Alborzi,
  et~al.
\newblock Snorkel drybell: A case study in deploying weak supervision at
  industrial scale.
\newblock In {\em SIGMOD (Industrial)}, pages 362--375, 2019.

\bibitem{barshan2020relatif}
Elnaz Barshan, Marc-Etienne Brunet, and Gintare~Karolina Dziugaite.
\newblock Relatif: Identifying explanatory training samples via relative
  influence.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1899--1909. PMLR, 2020.

\bibitem{basu2020influence}
Samyadeep Basu, Philip Pope, and Soheil Feizi.
\newblock Influence functions in deep learning are fragile.
\newblock {\em arXiv preprint arXiv:2006.14651}, 2020.

\bibitem{boecking2021interactive}
Benedikt Boecking, Willie Neiswanger, Eric Xing, and Artur Dubrawski.
\newblock Interactive weak supervision: Learning useful heuristics for data
  labeling.
\newblock In {\em ICLR}, 2021.

\bibitem{chen2020multi}
Hongge Chen, Si~Si, Yang Li, Ciprian Chelba, Sanjiv Kumar, Duane Boning, and
  Cho-Jui Hsieh.
\newblock Multi-stage influence function.
\newblock {\em Advances in Neural Information Processing Systems},
  33:12732--12742, 2020.

\bibitem{Cohen2020DetectingAS}
Gilad Cohen, Guillermo Sapiro, and Raja Giryes.
\newblock Detecting adversarial samples using influence functions and nearest
  neighbors.
\newblock {\em 2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 14441--14450, 2020.

\bibitem{cook1980characterizations}
R~Dennis Cook and Sanford Weisberg.
\newblock Characterizations of an empirical influence function for detecting
  influential cases in regression.
\newblock {\em Technometrics}, 22(4):495--508, 1980.

\bibitem{DawidSkene}
A.~P. Dawid and A.~M. Skene.
\newblock Maximum likelihood estimation of observer error-rates using the em
  algorithm.
\newblock {\em Journal of the Royal Statistical Society}, 28(1):20--28, 1979.

\bibitem{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL-HLT}, pages 4171--4186, 2019.

\bibitem{doshi2017towards}
Finale Doshi-Velez and Been Kim.
\newblock Towards a rigorous science of interpretable machine learning.
\newblock {\em arXiv preprint arXiv:1702.08608}, 2017.

\bibitem{Dua:2019}
Dheeru Dua and Casey Graff.
\newblock {UCI} machine learning repository, 2017.

\bibitem{fries2017swellshark}
Jason Fries, Sen Wu, Alex Ratner, and Christopher R{\'e}.
\newblock Swellshark: A generative model for biomedical named entity
  recognition without labeled data.
\newblock {\em arXiv preprint arXiv:1704.06360}, 2017.

\bibitem{fu2020fast}
Daniel~Y. Fu, Mayee~F. Chen, Frederic Sala, Sarah~M. Hooper, Kayvon Fatahalian,
  and Christopher R\'e.
\newblock Fast and three-rious: Speeding up weak supervision with triplet
  methods.
\newblock In {\em ICML}, pages 3280--3291, 2020.

\bibitem{hampel2011robust}
Frank~R Hampel, Elvezio~M Ronchetti, Peter~J Rousseeuw, and Werner~A Stahel.
\newblock {\em Robust statistics: the approach based on influence functions},
  volume 196.
\newblock John Wiley \& Sons, 2011.

\bibitem{han2020fortifying}
Xiaochuang Han and Yulia Tsvetkov.
\newblock Fortifying toxic speech detectors against veiled toxicity.
\newblock {\em arXiv preprint arXiv:2010.03154}, 2020.

\bibitem{resnet}
K.~{He}, X.~{Zhang}, S.~{Ren}, and J.~{Sun}.
\newblock Deep residual learning for image recognition.
\newblock In {\em 2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 770--778, 2016.

\bibitem{Hoffmann2011KnowledgeBasedWS}
R.~Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel~S. Weld.
\newblock Knowledge-based weak supervision for information extraction of
  overlapping relations.
\newblock In {\em ACL}, 2011.

\bibitem{hooper2020cut}
Sarah Hooper, Michael Wornow, Ying~Hang Seah, Peter Kellman, Hui Xue, Frederic
  Sala, Curtis Langlotz, and Christopher Re.
\newblock Cut out the annotator, keep the cutout: better segmentation with weak
  supervision.
\newblock In {\em ICLR}, 2020.

\bibitem{hsieh2022nemo}
Cheng-Yu Hsieh, Jieyu Zhang, and Alexander Ratner.
\newblock Nemo: Guiding and contextualizing weak supervision for interactive
  data programming.
\newblock {\em arXiv preprint arXiv:2203.01382}, 2022.

\bibitem{karamanolakis2021self}
Giannis Karamanolakis, Subhabrata Mukherjee, Guoqing Zheng, and Ahmed~Hassan
  Awadallah.
\newblock Self-training with weak supervision.
\newblock In {\em NAACL-HLT}, pages 845--863, 2021.

\bibitem{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In {\em International conference on machine learning}, pages
  1885--1894. PMLR, 2017.

\bibitem{koh2019accuracy}
Pang Wei~W Koh, Kai-Siang Ang, Hubert Teo, and Percy~S Liang.
\newblock On the accuracy of influence functions for measuring group effects.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{kong2022resolving}
Shuming Kong, Yanyan Shen, and Linpeng Huang.
\newblock Resolving training biases via influence-based data relabeling.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{lan2020connet}
Ouyu Lan, Xiao Huang, Bill~Yuchen Lin, He~Jiang, Liyuan Liu, and Xiang Ren.
\newblock Learning to contextually aggregate multi-source supervision for
  sequence labeling.
\newblock In {\em ACL}, pages 2134--2146, 2020.

\bibitem{liang2020bond}
Chen Liang, Yue Yu, Haoming Jiang, Siawpeng Er, Ruijia Wang, Tuo Zhao, and Chao
  Zhang.
\newblock Bond: Bert-assisted open-domain named entity recognition with distant
  supervision.
\newblock In {\em KDD}, pages 1054--1064, 2020.

\bibitem{martens2015optimizing}
James Martens and Roger Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In {\em International conference on machine learning}, pages
  2408--2417. PMLR, 2015.

\bibitem{mazzetto:icml21}
A.~Mazzetto, C.~Cousins, D.~Sam, S.~H. Bach, and E.~Upfal.
\newblock Adversarial multiclass learning under weak supervision with
  performance guarantees.
\newblock In {\em ICML}, 2021.

\bibitem{6470857}
Rami~M. Mohammad, Fadi Thabtah, and Lee McCluskey.
\newblock An assessment of features related to phishing websites using an
  automated technique.
\newblock In {\em 2012 International Conference for Internet Technology and
  Secured Transactions}, pages 492--497, 2012.

\bibitem{peng2019moment}
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo~Wang.
\newblock Moment matching for multi-source domain adaptation.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1406--1415, 2019.

\bibitem{Ratner19}
A.~J. Ratner, B.~Hancock, J.~Dunnmon, F.~Sala, S.~Pandey, and C.~R\'{e}.
\newblock Training complex models with multi-task weak supervision.
\newblock In {\em AAAI}, pages 4763--4771, 2019.

\bibitem{ratner2017snorkel}
Alexander~J Ratner, Stephen~H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and
  Christopher R{\'e}.
\newblock Snorkel: Rapid training data creation with weak supervision.
\newblock In {\em VLDB}, volume~11, page 269, 2017.

\bibitem{Ratner16}
Alexander~J Ratner, Christopher~M De~Sa, Sen Wu, Daniel Selsam, and Christopher
  R{\'e}.
\newblock Data programming: Creating large training sets, quickly.
\newblock In {\em NeurIPS}, volume~29, pages 3567--3575, 2016.

\bibitem{ren2020denoising}
Wendi Ren, Yinghao Li, Hanting Su, David Kartchner, Cassie Mitchell, and Chao
  Zhang.
\newblock Denoising multi-source weak supervision for neural text
  classification.
\newblock In {\em Findings of EMNLP}, pages 3739--3754, 2020.

\bibitem{ribeiro2016should}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock " why should i trust you?" explaining the predictions of any
  classifier.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD international conference
  on knowledge discovery and data mining}, pages 1135--1144, 2016.

\bibitem{safranchik2020weakly}
Esteban Safranchik, Shiying Luo, and Stephen Bach.
\newblock Weakly supervised sequence tagging from noisy rules.
\newblock In {\em AAAI}, volume~34, pages 5570--5578, 2020.

\bibitem{shin2021universalizing}
Changho Shin, Winfred Li, Harit Vishwakarma, Nicholas Roberts, and Frederic
  Sala.
\newblock Universalizing weak supervision.
\newblock In {\em ICLR}, 2022.

\bibitem{optlr}
Daniel Ting and Eric Brochu.
\newblock Optimal subsampling with influence functions.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem{Varma2017InferringGM}
P.~Varma, Bryan~D. He, Payal Bajaj, Nishith Khandwala, I.~Banerjee, D.~Rubin,
  and Christopher R{\'e}.
\newblock Inferring generative model structure with static analysis.
\newblock {\em Advances in neural information processing systems}, 30:239--249,
  2017.

\bibitem{Varma2019multi}
Paroma Varma, Frederic Sala, Shiori Sagawa, Jason Fries, Daniel Fu, Saelig
  Khattar, Ashwini Ramamoorthy, Ke~Xiao, Kayvon Fatahalian, James Priest, and
  Christopher R\'{e}.
\newblock Multi-resolution weak supervision for sequential data.
\newblock In {\em NeurIPS}, volume~32, 2019.

\bibitem{wang2022training}
Haonan Wang, Ziwei Wu, and Jingrui He.
\newblock Training fair deep neural networks by balancing influence.
\newblock {\em CoRR}, abs/2201.05759, 2022.

\bibitem{Wang2020LessIB}
Zifeng Wang, Hong Zhu, Zhenhua Dong, Xiuqiang He, and Shao-Lun Huang.
\newblock Less is better: Unweighted data subsampling via influence function.
\newblock In {\em AAAI}, 2020.

\bibitem{yu2021learning}
Peilin Yu, Tiffany Ding, and Stephen~H Bach.
\newblock Learning from multiple noisy partial labelers.
\newblock {\em AISTATS}, 2022.

\bibitem{yu-etal-2021-fine}
Yue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, and Chao Zhang.
\newblock Fine-tuning pre-trained language model with weak supervision: A
  contrastive-regularized self-training approach.
\newblock In {\em NAACL-HLT}, pages 1063--1077, 2021.

\bibitem{zhang2022survey}
Jieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang, and Alexander Ratner.
\newblock A survey on programmatic weak supervision.
\newblock {\em arXiv preprint arXiv:2202.05433}, 2022.

\bibitem{zhang2021creating}
Jieyu Zhang, Bohan Wang, Xiangchen Song, Yujing Wang, Yaming Yang, Jing Bai,
  and Alexander Ratner.
\newblock Creating training sets via weak indirect supervision.
\newblock In {\em ICLR}, 2022.

\bibitem{zhang2021wrench}
Jieyu Zhang, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and
  Alexander Ratner.
\newblock {WRENCH}: A comprehensive benchmark for weak supervision.
\newblock In {\em Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2021.

\bibitem{zhang2022prboost}
Rongzhi Zhang, Yue Yu, Pranav Shetty, Le~Song, and Chao Zhang.
\newblock Prboost: Prompt-based rule discovery and boosting for interactive
  weakly-supervised learning.
\newblock In {\em ACL}, 2022.

\end{thebibliography}
