\begin{thebibliography}{10}

\bibitem{ribeiro2016should}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock {"Why Should I Trust You?"} explaining the predictions of any
  classifier.
\newblock In {\em KDD}, 2016.

\bibitem{thrun1994extracting}
Sebastian Thrun.
\newblock Extracting rules from artificial neural networks with distributed
  representations.
\newblock In {\em NeurIPS}, 1994.

\bibitem{ribeiro2016nothing}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock Nothing else matters: Model-agnostic explanations by identifying
  prediction invariance.
\newblock {\em stat}, 2016.

\bibitem{lei2016rationalizing}
Tao Lei, Regina Barzilay, and Tommi Jaakkola.
\newblock Rationalizing neural predictions.
\newblock In {\em EMNLP}, 2016.

\bibitem{alikaniotis2016automatic}
Dimitrios Alikaniotis, Helen Yannakoudakis, and Marek Rei.
\newblock Automatic text scoring using neural networks.
\newblock In {\em ACL}, 2016.

\bibitem{strobelt2017lstmvis}
Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, and Alexander~M Rush.
\newblock Lstmvis: A tool for visual analysis of hidden state dynamics in
  recurrent neural networks.
\newblock {\em IEEE TVCG}, 2017.

\bibitem{murdoch2018beyond}
W~James Murdoch, Peter~J Liu, and Bin Yu.
\newblock Beyond word importance: Contextual decomposition to extract
  interactions from lstms.
\newblock In {\em ICLR}, 2018.

\bibitem{peake2018explanation}
Georgina Peake and Jun Wang.
\newblock Explanation mining: Post hoc interpretability of latent factor models
  for recommendation systems.
\newblock In {\em KDD}, 2018.

\bibitem{liang2020adversarial}
Jian Liang, Bing Bai, Yuren Cao, Kun Bai, and Fei Wang.
\newblock Adversarial infidelity learning for model interpretation.
\newblock In {\em KDD}, 2020.

\bibitem{gao2021learning}
Jingyue Gao, Xiting Wang, Yasha Wang, Yulan Yan, and Xing Xie.
\newblock Learning groupwise explanations for black-box models.
\newblock In {\em IJCAI}, 2021.

\bibitem{alvarez2018towards}
David Alvarez~Melis and Tommi Jaakkola.
\newblock Towards robust interpretability with self-explaining neural networks.
\newblock In {\em NeurIPS}, 2018.

\bibitem{rudin2019stop}
Cynthia Rudin.
\newblock Stop explaining black box machine learning models for high stakes
  decisions and use interpretable models instead.
\newblock {\em Nature Machine Intelligence}, 2019.

\bibitem{lundberg2017unified}
Scott~M Lundberg and Su-In Lee.
\newblock A unified approach to interpreting model predictions.
\newblock In {\em NeurIPS}, 2017.

\bibitem{ribeiro2018anchors}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock Anchors: High-precision model-agnostic explanations.
\newblock In {\em AAAI}, 2018.

\bibitem{guan2019towards}
Chaoyu Guan, Xiting Wang, Quanshi Zhang, Runjin Chen, Di~He, and Xing Xie.
\newblock Towards a deep and unified understanding of deep neural models in
  nlp.
\newblock In {\em ICML}, 2019.

\bibitem{hong2020human}
Sungsoo~Ray Hong, Jessica Hullman, and Enrico Bertini.
\newblock Human factors in model interpretability: Industry practices,
  challenges, and needs.
\newblock {\em PACM HCI}, 2020.

\bibitem{letham2015interpretable}
Benjamin Letham, Cynthia Rudin, Tyler~H McCormick, and David Madigan.
\newblock Interpretable classifiers using rules and bayesian analysis: Building
  a better stroke prediction model.
\newblock {\em AOAS}, 2015.

\bibitem{yang2017scalable}
Hongyu Yang, Cynthia Rudin, and Margo Seltzer.
\newblock Scalable bayesian rule lists.
\newblock In {\em ICML}, 2017.

\bibitem{evans2018learning}
Richard Evans and Edward Grefenstette.
\newblock Learning explanatory rules from noisy data.
\newblock {\em Journal of Artificial Intelligence Research}, 2018.

\bibitem{angelino2017learning}
Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia
  Rudin.
\newblock Learning certifiably optimal rule lists.
\newblock In {\em KDD}, 2017.

\bibitem{ming2019interpretable}
Yao Ming, Panpan Xu, Huamin Qu, and Liu Ren.
\newblock Interpretable and steerable sequence learning via prototypes.
\newblock In {\em KDD}, 2019.

\bibitem{chen2020towards}
Zhongxia Chen, Xiting Wang, Xing Xie, Mehul Parsana, Akshay Soni, Xiang Ao, and
  Enhong Chen.
\newblock Towards explainable conversational recommendation.
\newblock In {\em IJCAI}, 2020.

\bibitem{kulesza2015principles}
Todd Kulesza, Margaret Burnett, Weng-Keen Wong, and Simone Stumpf.
\newblock Principles of explanatory debugging to personalize interactive
  machine learning.
\newblock In {\em IUI}, 2015.

\bibitem{schramowski2020making}
Patrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger, Franziska
  Herbert, Xiaoting Shao, Hans-Georg Luigs, Anne-Katrin Mahlein, and Kristian
  Kersting.
\newblock Making deep neural networks right for the right scientific reasons by
  interacting with their explanations.
\newblock {\em Nature Machine Intelligence}, 2020.

\bibitem{lertvittayakumjorn2020find}
Piyawat Lertvittayakumjorn, Lucia Specia, and Francesca Toni.
\newblock Find: Human-in-the-loop debugging deep text classifiers.
\newblock In {\em EMNLP}, 2020.

\bibitem{ciravegna2021human}
Gabriele Ciravegna, Francesco Giannini, Marco Gori, Marco Maggini, and Stefano
  Melacci.
\newblock Human-driven fol explanations of deep learning.
\newblock In {\em IJCAI}, 2021.

\bibitem{stammer2021right}
Wolfgang Stammer, Patrick Schramowski, and Kristian Kersting.
\newblock Right for the right concept: Revising neuro-symbolic concepts by
  interacting with their explanations.
\newblock In {\em CVPR}, 2021.

\bibitem{bontempelli2021toward}
Andrea Bontempelli, Fausto Giunchiglia, Andrea Passerini, and Stefano Teso.
\newblock Toward a unified framework for debugging gray-box models.
\newblock {\em arXiv preprint arXiv:2109.11160}, 2021.

\bibitem{de2020statistical}
Luc De~Raedt, Sebastijan Dumancic, Robin Manhaeve, and Giuseppe Marra.
\newblock From statistical relational to neuro-symbolic artificial
  intelligence.
\newblock In {\em IJCAI}, 2020.

\bibitem{valkov2018houdini}
Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, and Swarat
  Chaudhuri.
\newblock Houdini: lifelong learning as program synthesis.
\newblock In {\em NeurIPS}, 2018.

\bibitem{ellis2018learninglibraries}
Kevin Ellis, Lucas Morales, Mathias Sabl{\'e}-Meyer, Armando Solar-Lezama, and
  Josh Tenenbaum.
\newblock Learning libraries of subroutines for neurally-guided bayesian
  program induction.
\newblock In {\em NeurIPS}, 2018.

\bibitem{kalyan2018neural}
Ashwin Kalyan, Abhishek Mohta, Oleksandr Polozov, Dhruv Batra, Prateek Jain,
  and Sumit Gulwani.
\newblock Neural-guided deductive search for real-time program synthesis from
  examples.
\newblock In {\em ICLR}, 2018.

\bibitem{ellis2018learningto}
Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Josh Tenenbaum.
\newblock Learning to infer graphics programs from hand-drawn images.
\newblock In {\em NeurIPS}, 2018.

\bibitem{mao2019neuro}
Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua~B Tenenbaum, and Jiajun Wu.
\newblock The neuro-symbolic concept learner: Interpreting scenes, words, and
  sentences from natural supervision.
\newblock In {\em ICLR}, 2019.

\bibitem{yu2022probabilistic}
Dongran Yu, Bo~Yang, Qianhao Wei, Anchen Li, and Shirui Pan.
\newblock A probabilistic graphical model based on neural-symbolic reasoning
  for visual relationship detection.
\newblock In {\em CVPR}, 2022.

\bibitem{wang2022multi}
Xiting Wang, Kunpeng Liu, Dongjie Wang, Le~Wu, Yanjie Fu, and Xing Xie.
\newblock Multi-level recommendation reasoning over knowledge graphs with
  reinforcement learning.
\newblock In {\em WebConf}, 2022.

\bibitem{yang2017differentiable}
Fan Yang, Zhilin Yang, and William~W Cohen.
\newblock Differentiable learning of logical rules for knowledge base
  reasoning.
\newblock In {\em NeurIPS}, 2017.

\bibitem{zhao2020leveraging}
Kangzhi Zhao, Xiting Wang, Yuren Zhang, Li~Zhao, Zheng Liu, Chunxiao Xing, and
  Xing Xie.
\newblock Leveraging demonstrations for reinforcement recommendation reasoning
  over knowledge graphs.
\newblock In {\em SIGIR}, 2020.

\bibitem{okajima2019deep}
Yuzuru Okajima and Kunihiko Sadamasa.
\newblock Deep neural networks constrained by decision rules.
\newblock In {\em AAAI}, 2019.

\bibitem{jang2017categorical}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock {\em stat}, 2017.

\bibitem{kendall2018multi}
Alex Kendall, Yarin Gal, and Roberto Cipolla.
\newblock Multi-task learning using uncertainty to weigh losses for scene
  geometry and semantics.
\newblock In {\em CVPR}, 2018.

\bibitem{cho2014properties}
Kyunghyun Cho, Bart van Merri{\"e}nboer, Dzmitry Bahdanau, and Yoshua Bengio.
\newblock On the properties of neural machine translation: Encoder--decoder
  approaches.
\newblock In {\em SSST}, 2014.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NeurIPS}, 2017.

\bibitem{zhang2015character}
Xiang Zhang, Junbo Zhao, and Yann LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock In {\em NeurIPS}, 2015.

\bibitem{clickbait2020kaggle}
Open Data~Science (\url{ODS.ai}).
\newblock {Kaggle} clickbait news detection.
\newblock \url{https://www.kaggle.com/c/clickbait-news-detection}, 2020.

\bibitem{uci_ml_repository}
Dheeru Dua and Casey Graf.
\newblock {UCI} machine learning repository.
\newblock \url{http://archive.ics.uci.edu/ml}, 2017.

\bibitem{saito2015precision}
Takaya Saito and Marc Rehmsmeier.
\newblock The precision-recall plot is more informative than the roc plot when
  evaluating binary classifiers on imbalanced datasets.
\newblock {\em PloS one}, 10(3):e0118432, 2015.

\bibitem{kenton2019bert}
Jacob Devlin Ming-Wei~Chang Kenton and Lee~Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL}, 2019.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{li2019dividemix}
Junnan Li, Richard Socher, and Steven~CH Hoi.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock In {\em ICLR}, 2019.

\bibitem{zhang2018mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock Mixup: Beyond empirical risk minimization.
\newblock In {\em ICLR}, 2018.

\bibitem{speechocean}
Speechocean.
\newblock \url{https://en.speechocean.com/}.

\bibitem{kim2018interpretability}
Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda
  Viegas, et~al.
\newblock Interpretability beyond feature attribution: Quantitative testing
  with concept activation vectors (tcav).
\newblock In {\em ICML}, 2018.

\bibitem{peer2022data}
Eyal Peer, David Rothschild, Andrew Gordon, Zak Evernden, and Ekaterina Damer.
\newblock Data quality of platforms and panels for online behavioral research.
\newblock {\em Behavior Research Methods}, 54(4):1643--1662, 2022.

\bibitem{sagar2018how}
Beth Sagar-Fenton and Lizzy McNeill.
\newblock How many words do you need to speak a language.
\newblock {\em BBC}, 2018.

\end{thebibliography}
