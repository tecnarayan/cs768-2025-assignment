\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aybat et~al.(2019)Aybat, Fallah, Gurbuzbalaban, and
  Ozdaglar]{aybat2019universally}
Aybat, N.~S., Fallah, A., Gurbuzbalaban, M., and Ozdaglar, A.
\newblock A universally optimal multistage accelerated stochastic gradient
  method.
\newblock \emph{Advances in neural information processing systems},
  32:\penalty0 8525--8536, 2019.

\bibitem[Bach \& Moulines(2013)Bach and Moulines]{bach2013non}
Bach, F. and Moulines, E.
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate $o(1/n)$.
\newblock \emph{Advances in neural information processing systems},
  26:\penalty0 773--781, 2013.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2020benign}
Bartlett, P.~L., Long, P.~M., Lugosi, G., and Tsigler, A.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 2020.

\bibitem[Belkin et~al.(2020)Belkin, Hsu, and Xu]{belkin2020two}
Belkin, M., Hsu, D., and Xu, J.
\newblock Two models of double descent for weak features.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (4):\penalty0 1167--1180, 2020.

\bibitem[Berthier et~al.(2020)Berthier, Bach, and Gaillard]{berthier2020tight}
Berthier, R., Bach, F., and Gaillard, P.
\newblock Tight nonparametric convergence rates for stochastic gradient descent
  under the noiseless linear model.
\newblock \emph{arXiv preprint arXiv:2006.08212}, 2020.

\bibitem[Bubeck(2014)]{bubeck2014theory}
Bubeck, S.
\newblock Theory of convex optimization for machine learning.
\newblock \emph{arXiv preprint arXiv:1405.4980}, 15, 2014.

\bibitem[Davis et~al.(2019)Davis, Drusvyatskiy, and
  Charisopoulos]{davis2019stochastic}
Davis, D., Drusvyatskiy, D., and Charisopoulos, V.
\newblock Stochastic algorithms with geometric step decay converge linearly on
  sharp functions.
\newblock \emph{arXiv preprint arXiv:1907.09547}, 2019.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{dekel2012optimal}
Dekel, O., Gilad-Bachrach, R., Shamir, O., and Xiao, L.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0 (1), 2012.

\bibitem[Dhillon et~al.(2013)Dhillon, Foster, Kakade, and
  Ungar]{dhillon2013risk}
Dhillon, P.~S., Foster, D.~P., Kakade, S.~M., and Ungar, L.~H.
\newblock A risk comparison of ordinary least squares vs ridge regression.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 1505--1511, 2013.

\bibitem[Dieuleveut \& Bach(2015)Dieuleveut and Bach]{DieuleveutB15}
Dieuleveut, A. and Bach, F.~R.
\newblock Non-parametric stochastic approximation with large step sizes.
\newblock \emph{The Annals of Statistics}, 2015.

\bibitem[Dieuleveut et~al.(2017)Dieuleveut, Flammarion, and
  Bach]{dieuleveut2017harder}
Dieuleveut, A., Flammarion, N., and Bach, F.
\newblock Harder, better, faster, stronger convergence rates for least-squares
  regression.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 3520--3570, 2017.

\bibitem[Fang et~al.(2018)Fang, Kotz, and Ng]{fang2018symmetric}
Fang, K.-T., Kotz, S., and Ng, K.~W.
\newblock \emph{Symmetric multivariate and related distributions}.
\newblock Chapman and Hall/CRC, 2018.

\bibitem[Ge et~al.(2019)Ge, Kakade, Kidambi, and Netrapalli]{ge2019step}
Ge, R., Kakade, S.~M., Kidambi, R., and Netrapalli, P.
\newblock The step decay schedule: A near optimal, geometrically decaying
  learning rate procedure for least squares.
\newblock \emph{arXiv preprint arXiv:1904.12838}, 2019.

\bibitem[Ghadimi \& Lan(2012)Ghadimi and Lan]{ghadimi2012optimal}
Ghadimi, S. and Lan, G.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization i: A generic algorithmic framework.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (4):\penalty0
  1469--1492, 2012.

\bibitem[Hazan \& Kale(2014)Hazan and Kale]{hazan2014beyond}
Hazan, E. and Kale, S.
\newblock Beyond the regret minimization barrier: optimal algorithms for
  stochastic strongly-convex optimization.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 2489--2512, 2014.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition. corr abs/1512.03385
  (2015), 2015.

\bibitem[Hsu et~al.(2014)Hsu, Kakade, and Zhang]{HsuKZ14}
Hsu, D.~J., Kakade, S.~M., and Zhang, T.
\newblock Random design analysis of ridge regression.
\newblock \emph{Foundations of Computational Mathematics}, 14\penalty0
  (3):\penalty0 569--600, 2014.

\bibitem[Jain et~al.(2017{\natexlab{a}})Jain, Kakade, Kidambi, Netrapalli,
  Pillutla, and Sidford]{jain2017markov}
Jain, P., Kakade, S.~M., Kidambi, R., Netrapalli, P., Pillutla, V.~K., and
  Sidford, A.
\newblock A markov chain theory approach to characterizing the minimax
  optimality of stochastic gradient descent (for least squares).
\newblock \emph{arXiv preprint arXiv:1710.09430}, 2017{\natexlab{a}}.

\bibitem[Jain et~al.(2017{\natexlab{b}})Jain, Netrapalli, Kakade, Kidambi, and
  Sidford]{jain2017parallelizing}
Jain, P., Netrapalli, P., Kakade, S.~M., Kidambi, R., and Sidford, A.
\newblock Parallelizing stochastic gradient descent for least squares
  regression: mini-batching, averaging, and model misspecification.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 8258--8299, 2017{\natexlab{b}}.

\bibitem[Kulunchakov \& Mairal(2019)Kulunchakov and
  Mairal]{kulunchakov2019generic}
Kulunchakov, A. and Mairal, J.
\newblock A generic acceleration framework for stochastic composite
  optimization.
\newblock \emph{arXiv preprint arXiv:1906.01164}, 2019.

\bibitem[Lacoste-Julien et~al.(2012)Lacoste-Julien, Schmidt, and
  Bach]{lacoste2012simpler}
Lacoste-Julien, S., Schmidt, M., and Bach, F.
\newblock A simpler approach to obtaining an o (1/t) convergence rate for the
  projected stochastic subgradient method.
\newblock \emph{arXiv preprint arXiv:1212.2002}, 2012.

\bibitem[Lin \& Rosasco(2017)Lin and Rosasco]{lin2017optimal}
Lin, J. and Rosasco, L.
\newblock Optimal rates for multi-pass stochastic gradient methods.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 3375--3421, 2017.

\bibitem[M{\"u}cke et~al.(2019)M{\"u}cke, Neu, and Rosasco]{mucke2019beating}
M{\"u}cke, N., Neu, G., and Rosasco, L.
\newblock Beating sgd saturation with tail-averaging and minibatching.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Neu \& Rosasco(2018)Neu and Rosasco]{neu2018iterate}
Neu, G. and Rosasco, L.
\newblock Iterate averaging as regularization for stochastic gradient descent.
\newblock In \emph{Conference On Learning Theory}, pp.\  3222--3242. PMLR,
  2018.

\bibitem[Pan et~al.(2021)Pan, Ye, and Zhang]{pan2021eigencurve}
Pan, R., Ye, H., and Zhang, T.
\newblock Eigencurve: Optimal learning rate schedule for sgd on quadratic
  objectives with skewed hessian spectrums.
\newblock \emph{arXiv preprint arXiv:2110.14109}, 2021.

\bibitem[Polyak \& Juditsky(1992)Polyak and Juditsky]{polyak1992acceleration}
Polyak, B.~T. and Juditsky, A.~B.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM journal on control and optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Rakhlin et~al.(2011)Rakhlin, Shamir, and Sridharan]{rakhlin2011making}
Rakhlin, A., Shamir, O., and Sridharan, K.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock \emph{arXiv preprint arXiv:1109.5647}, 2011.

\bibitem[Sch{\"o}lkopf et~al.(2002)Sch{\"o}lkopf, Smola, Bach,
  et~al.]{scholkopf2002learning}
Sch{\"o}lkopf, B., Smola, A.~J., Bach, F., et~al.
\newblock \emph{Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2002.

\bibitem[Tsigler \& Bartlett(2020)Tsigler and Bartlett]{tsigler2020benign}
Tsigler, A. and Bartlett, P.~L.
\newblock Benign overfitting in ridge regression.
\newblock \emph{arXiv preprint arXiv:2009.14286}, 2020.

\bibitem[Varre et~al.(2021)Varre, Pillaud-Vivien, and
  Flammarion]{varre2021last}
Varre, A., Pillaud-Vivien, L., and Flammarion, N.
\newblock Last iterate convergence of sgd for least-squares in the
  interpolation regime.
\newblock \emph{arXiv preprint arXiv:2102.03183}, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021.

\bibitem[Zou et~al.(2021{\natexlab{a}})Zou, Wu, Braverman, Gu, Foster, and
  Kakade]{zou2021benefits}
Zou, D., Wu, J., Braverman, V., Gu, Q., Foster, D.~P., and Kakade, S.~M.
\newblock The benefits of implicit regularization from sgd in least squares
  problems.
\newblock \emph{The 35th Conference on Neural Information Processing Systems},
  2021{\natexlab{a}}.

\bibitem[Zou et~al.(2021{\natexlab{b}})Zou, Wu, Braverman, Gu, and
  Kakade]{zou2021benign}
Zou, D., Wu, J., Braverman, V., Gu, Q., and Kakade, S.~M.
\newblock Benign overfitting of constant-stepsize sgd for linear regression.
\newblock \emph{The 34th Annual Conference on Learning Theory},
  2021{\natexlab{b}}.

\end{thebibliography}
