\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anil et~al.(2018)Anil, Lucas, and Grosse]{Anil2018SortingOL}
Anil, C., Lucas, J., and Grosse, R.~B.
\newblock Sorting out lipschitz function approximation.
\newblock In \emph{ICML}, 2018.

\bibitem[Athalye \& Carlini(2018)Athalye and Carlini]{Athalye2018OnTR}
Athalye, A. and Carlini, N.
\newblock On the robustness of the cvpr 2018 white-box adversarial example
  defenses.
\newblock \emph{ArXiv}, abs/1804.03286, 2018.

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and
  Wagner]{Athalye2018ObfuscatedGG}
Athalye, A., Carlini, N., and Wagner, D.~A.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In \emph{ICML}, 2018.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{Boyd:2004:CO:993483}
Boyd, S. and Vandenberghe, L.
\newblock \emph{Convex Optimization}.
\newblock Cambridge University Press, New York, NY, USA, 2004.
\newblock ISBN 0521833787.

\bibitem[Bunel et~al.(2017)Bunel, Turkaslan, Torr, Kohli, and
  Mudigonda]{Bunel2017AUV}
Bunel, R., Turkaslan, I., Torr, P. H.~S., Kohli, P., and Mudigonda, P.~K.
\newblock A unified view of piecewise linear neural network verification.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Cao \& Gong(2017)Cao and Gong]{Cao2017MitigatingEA}
Cao, X. and Gong, N.~Z.
\newblock Mitigating evasion attacks to deep neural networks via region-based
  classification.
\newblock \emph{ArXiv}, abs/1709.05583, 2017.

\bibitem[Carlini \& Wagner(2017)Carlini and
  Wagner]{Carlini:2017:AEE:3128572.3140444}
Carlini, N. and Wagner, D.
\newblock Adversarial examples are not easily detected: Bypassing ten detection
  methods.
\newblock In \emph{Proceedings of the 10th ACM Workshop on Artificial
  Intelligence and Security}, AISec '17, 2017.

\bibitem[Carlini et~al.(2017)Carlini, Katz, Barrett, and
  Dill]{Carlini2017ProvablyMA}
Carlini, N., Katz, G., Barrett, C.~E., and Dill, D.~L.
\newblock Provably minimally-distorted adversarial examples.
\newblock 2017.

\bibitem[Cheng et~al.(2017)Cheng, N{\"u}hrenberg, and
  Ruess]{Cheng2017MaximumRO}
Cheng, C.-H., N{\"u}hrenberg, G., and Ruess, H.
\newblock Maximum resilience of artificial neural networks.
\newblock In \emph{ATVA}, 2017.

\bibitem[Cohen et~al.(2019)Cohen, Rosenfeld, and Kolter]{Cohen2019CertifiedAR}
Cohen, J.~M., Rosenfeld, E., and Kolter, J.~Z.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock In \emph{ICML}, 2019.

\bibitem[Croce et~al.(2018)Croce, Andriushchenko, and
  Hein]{Croce2018ProvableRO}
Croce, F., Andriushchenko, M., and Hein, M.
\newblock Provable robustness of relu networks via maximization of linear
  regions.
\newblock \emph{ArXiv}, abs/1810.07481, 2018.

\bibitem[Dutta et~al.(2018)Dutta, Jha, Sankaranarayanan, and
  Tiwari]{Dutta2018OutputRA}
Dutta, S., Jha, S., Sankaranarayanan, S., and Tiwari, A.
\newblock Output range analysis for deep feedforward neural networks.
\newblock In \emph{NFM}, 2018.

\bibitem[Dvijotham et~al.(2018{\natexlab{a}})Dvijotham, Gowal, Stanforth,
  Arandjelovic, O'Donoghue, Uesato, and Kohli]{Dvijotham2018TrainingVL}
Dvijotham, K., Gowal, S., Stanforth, R., Arandjelovic, R., O'Donoghue, B.,
  Uesato, J., and Kohli, P.
\newblock Training verified learners with learned verifiers.
\newblock \emph{ArXiv}, abs/1805.10265, 2018{\natexlab{a}}.

\bibitem[Dvijotham et~al.(2018{\natexlab{b}})Dvijotham, Stanforth, Gowal, Mann,
  and Kohli]{Dvijotham2018ADA}
Dvijotham, K., Stanforth, R., Gowal, S., Mann, T.~A., and Kohli, P.
\newblock A dual approach to scalable verification of deep networks.
\newblock In \emph{UAI}, 2018{\natexlab{b}}.

\bibitem[Ehlers(2017)]{Ehlers2017FormalVO}
Ehlers, R.
\newblock Formal verification of piece-wise linear feed-forward neural
  networks.
\newblock \emph{ArXiv}, abs/1705.01320, 2017.

\bibitem[Fazlyab et~al.(2019)Fazlyab, Robey, Hassani, Morari, and
  Pappas]{DBLP:journals/corr/abs-1906-04893}
Fazlyab, M., Robey, A., Hassani, H., Morari, M., and Pappas, G.~J.
\newblock Efficient and accurate estimation of lipschitz constants for deep
  neural networks.
\newblock \emph{CoRR}, abs/1906.04893, 2019.
\newblock URL \url{http://arxiv.org/abs/1906.04893}.

\bibitem[Fischetti \& Jo(2018)Fischetti and Jo]{Fischetti2018DeepNN}
Fischetti, M. and Jo, J.
\newblock Deep neural networks and mixed integer linear optimization.
\newblock \emph{Constraints}, 23:\penalty0 296--309, 2018.

\bibitem[Gehr et~al.(2018)Gehr, Mirman, Drachsler-Cohen, Tsankov, Chaudhuri,
  and Vechev]{Gehr2018AI2SA}
Gehr, T., Mirman, M., Drachsler-Cohen, D., Tsankov, P., Chaudhuri, S., and
  Vechev, M.~T.
\newblock Ai2: Safety and robustness certification of neural networks with
  abstract interpretation.
\newblock \emph{2018 IEEE Symposium on Security and Privacy (SP)}, pp.\  3--18,
  2018.

\bibitem[Gowal et~al.(2018)Gowal, Dvijotham, Stanforth, Bunel, Qin, Uesato,
  Arandjelovic, Mann, and Kohli]{Gowal2018OnTE}
Gowal, S., Dvijotham, K., Stanforth, R., Bunel, R., Qin, C., Uesato, J.,
  Arandjelovic, R., Mann, T.~A., and Kohli, P.
\newblock On the effectiveness of interval bound propagation for training
  verifiably robust models.
\newblock \emph{ArXiv}, abs/1810.12715, 2018.

\bibitem[Hein \& Andriushchenko(2017)Hein and Andriushchenko]{NIPS2017_6821}
Hein, M. and Andriushchenko, M.
\newblock Formal guarantees on the robustness of a classifier against
  adversarial manipulation.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30}, pp.\  2266--2276. 2017.

\bibitem[Huang et~al.(2016)Huang, Kwiatkowska, Wang, and Wu]{Huang2016SafetyVO}
Huang, X., Kwiatkowska, M.~Z., Wang, S., and Wu, M.
\newblock Safety verification of deep neural networks.
\newblock \emph{ArXiv}, abs/1610.06940, 2016.

\bibitem[Katz et~al.(2017)Katz, Barrett, Dill, Julian, and
  Kochenderfer]{Katz2017ReluplexAE}
Katz, G., Barrett, C.~W., Dill, D.~L., Julian, K., and Kochenderfer, M.~J.
\newblock Reluplex: An efficient smt solver for verifying deep neural networks.
\newblock In \emph{CAV}, 2017.

\bibitem[Kurakin et~al.(2016{\natexlab{a}})Kurakin, Goodfellow, and
  Bengio]{Kurakin2016AdversarialEI}
Kurakin, A., Goodfellow, I.~J., and Bengio, S.
\newblock Adversarial examples in the physical world.
\newblock \emph{ArXiv}, abs/1607.02533, 2016{\natexlab{a}}.

\bibitem[Kurakin et~al.(2016{\natexlab{b}})Kurakin, Goodfellow, and
  Bengio]{Kurakin2016AdversarialML}
Kurakin, A., Goodfellow, I.~J., and Bengio, S.
\newblock Adversarial machine learning at scale.
\newblock \emph{ArXiv}, abs/1611.01236, 2016{\natexlab{b}}.

\bibitem[LeCun \& Cortes(2010)LeCun and Cortes]{lecunMNIST}
LeCun, Y. and Cortes, C.
\newblock {MNIST} handwritten digit database.
\newblock 2010.
\newblock URL \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem[L{\'e}cuyer et~al.(2018)L{\'e}cuyer, Atlidakis, Geambasu, Hsu, and
  Jana]{Lcuyer2018CertifiedRT}
L{\'e}cuyer, M., Atlidakis, V., Geambasu, R., Hsu, D., and Jana, S. K.~K.
\newblock Certified robustness to adversarial examples with differential
  privacy.
\newblock In \emph{IEEE S\&P 2019}, 2018.

\bibitem[Li et~al.(2018)Li, Chen, Wang, and Carin]{Li2018CertifiedAR}
Li, B.~H., Chen, C., Wang, W., and Carin, L.
\newblock Certified adversarial robustness with additive gaussian noise.
\newblock 2018.

\bibitem[Liu et~al.(2017)Liu, Cheng, Zhang, and Hsieh]{Liu2017TowardsRN}
Liu, X., Cheng, M., Zhang, H., and Hsieh, C.-J.
\newblock Towards robust neural networks via random self-ensemble.
\newblock \emph{ArXiv}, abs/1712.00673, 2017.

\bibitem[Lomuscio \& Maganti(2017)Lomuscio and Maganti]{Lomuscio2017AnAT}
Lomuscio, A. and Maganti, L.
\newblock An approach to reachability analysis for feed-forward relu neural
  networks.
\newblock \emph{ArXiv}, abs/1706.07351, 2017.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2018towards}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=rJzIBfZAb}.

\bibitem[Mirman et~al.(2018)Mirman, Gehr, and
  Vechev]{Mirman2018DifferentiableAI}
Mirman, M., Gehr, T., and Vechev, M.~T.
\newblock Differentiable abstract interpretation for provably robust neural
  networks.
\newblock In \emph{ICML}, 2018.

\bibitem[Miyato et~al.(2017)Miyato, ichi Maeda, Koyama, and
  Ishii]{Miyato2017VirtualAT}
Miyato, T., ichi Maeda, S., Koyama, M., and Ishii, S.
\newblock Virtual adversarial training: A regularization method for supervised
  and semi-supervised learning.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 41:\penalty0 1979--1993, 2017.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida]{miyato2018spectral}
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1QRgziT-}.

\bibitem[Moosavi~Dezfooli et~al.(2019)Moosavi~Dezfooli, Fawzi, Uesato, and
  Frossard]{MoosaviDezfooliCVPR2019}
Moosavi~Dezfooli, S.~M., Fawzi, A., Uesato, J., and Frossard, P.
\newblock Robustness via curvature regularization, and vice versa.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2019.

\bibitem[{Papernot} et~al.(2016){Papernot}, {McDaniel}, {Wu}, {Jha}, and
  {Swami}]{7546524}
{Papernot}, N., {McDaniel}, P., {Wu}, X., {Jha}, S., and {Swami}, A.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In \emph{2016 IEEE Symposium on Security and Privacy (SP)}, pp.\
  582--597, May 2016.
\newblock \doi{10.1109/SP.2016.41}.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Goodfellow, Jha, Celik, and
  Swami]{DBLP:journals/corr/PapernotMGJCS16}
Papernot, N., McDaniel, P.~D., Goodfellow, I.~J., Jha, S., Celik, Z.~B., and
  Swami, A.
\newblock Practical black-box attacks against deep learning systems using
  adversarial examples.
\newblock \emph{CoRR}, abs/1602.02697, 2016.
\newblock URL \url{http://arxiv.org/abs/1602.02697}.

\bibitem[Peck et~al.(2017)Peck, Roels, Goossens, and Saeys]{Peck2017LowerBO}
Peck, J., Roels, J., Goossens, B., and Saeys, Y.
\newblock Lower bounds on the robustness to adversarial perturbations.
\newblock In \emph{NIPS}, 2017.

\bibitem[Qin et~al.(2019)Qin, Martens, Gowal, Krishnan, Fawzi, De, Stanforth,
  and Kohli]{qin2019adversarial}
Qin, C., Martens, J., Gowal, S., Krishnan, D., Fawzi, A., De, S., Stanforth,
  R., and Kohli, P.
\newblock Adversarial robustness through local linearization.
\newblock \emph{arXiv preprint arXiv:1907.02610}, 2019.

\bibitem[Raghunathan et~al.(2018{\natexlab{a}})Raghunathan, Steinhardt, and
  Liang]{Raghunathan2018CertifiedDA}
Raghunathan, A., Steinhardt, J., and Liang, P.
\newblock Certified defenses against adversarial examples.
\newblock \emph{ArXiv}, abs/1801.09344, 2018{\natexlab{a}}.

\bibitem[Raghunathan et~al.(2018{\natexlab{b}})Raghunathan, Steinhardt, and
  Liang]{Raghunathan2018SemidefiniteRF}
Raghunathan, A., Steinhardt, J., and Liang, P.
\newblock Semidefinite relaxations for certifying robustness to adversarial
  examples.
\newblock In \emph{NeurIPS}, 2018{\natexlab{b}}.

\bibitem[Salman et~al.(2019)Salman, Yang, Li, Zhang, Zhang, Razenshteyn, and
  Bubeck]{Salman2019ProvablyRD}
Salman, H., Yang, G., Li, J., Zhang, P., Zhang, H., Razenshteyn, I.~P., and
  Bubeck, S.
\newblock Provably robust deep learning via adversarially trained smoothed
  classifiers.
\newblock \emph{ArXiv}, abs/1906.04584, 2019.

\bibitem[Samangouei et~al.(2018)Samangouei, Kabkab, and
  Chellappa]{samangouei2018defensegan}
Samangouei, P., Kabkab, M., and Chellappa, R.
\newblock Defense-{GAN}: Protecting classifiers against adversarial attacks
  using generative models.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=BkJ3ibb0-}.

\bibitem[Scaman \& Virmaux(2018)Scaman and Virmaux]{scaman2018lipschitz}
Scaman, K. and Virmaux, A.
\newblock Lipschitz regularity of deep neural networks: analysis and efficient
  estimation, 2018.

\bibitem[Singh et~al.(2018)Singh, Gehr, Mirman, P{\"u}schel, and
  Vechev]{Singh2018FastAE}
Singh, G., Gehr, T., Mirman, M., P{\"u}schel, M., and Vechev, M.~T.
\newblock Fast and effective robustness certification.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Singla et~al.(2019)Singla, Wallace, Feng, and
  Feizi]{Singla2019UnderstandingIO}
Singla, S., Wallace, E., Feng, S., and Feizi, S.
\newblock Understanding impacts of high-order loss approximations and features
  in deep learning interpretation.
\newblock In \emph{ICML}, 2019.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{42503}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2014.
\newblock URL \url{http://arxiv.org/abs/1312.6199}.

\bibitem[Uesato et~al.(2018)Uesato, O'Donoghue, Kohli, and van~den
  Oord]{Uesato2018AdversarialRA}
Uesato, J., O'Donoghue, B., Kohli, P., and van~den Oord, A.
\newblock Adversarial risk and the dangers of evaluating against weak attacks.
\newblock In \emph{ICML}, 2018.

\bibitem[Wang et~al.(2018{\natexlab{a}})Wang, Chen, Abdou, and
  Jana]{Wang2018MixTrainST}
Wang, S., Chen, Y., Abdou, A., and Jana, S. K.~K.
\newblock Mixtrain: Scalable training of verifiably robust neural networks.
\newblock 2018{\natexlab{a}}.

\bibitem[Wang et~al.(2018{\natexlab{b}})Wang, Pei, Whitehouse, Yang, and
  Jana]{Wang2018EfficientFS}
Wang, S., Pei, K., Whitehouse, J., Yang, J., and Jana, S. K.~K.
\newblock Efficient formal safety analysis of neural networks.
\newblock In \emph{NeurIPS}, 2018{\natexlab{b}}.

\bibitem[Weng et~al.(2018)Weng, Zhang, Chen, Song, Hsieh, Boning, Dhillon, and
  Daniel]{Weng2018TowardsFC}
Weng, T.-W., Zhang, H., Chen, H., Song, Z., Hsieh, C.-J., Boning, D.~S.,
  Dhillon, I.~S., and Daniel, L.
\newblock Towards fast computation of certified robustness for relu networks.
\newblock \emph{ArXiv}, abs/1804.09699, 2018.

\bibitem[Wong \& Kolter(2017)Wong and Kolter]{Wong2017ProvableDA}
Wong, E. and Kolter, J.~Z.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock \emph{ArXiv}, abs/1711.00851, 2017.

\bibitem[Wong et~al.(2018)Wong, Schmidt, Metzen, and Kolter]{Wong2018ScalingPA}
Wong, E., Schmidt, F.~R., Metzen, J.~H., and Kolter, J.~Z.
\newblock Scaling provable adversarial defenses.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{fashionMNIST}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{CoRR}, abs/1708.07747, 2017.
\newblock URL \url{http://arxiv.org/abs/1708.07747}.

\bibitem[Zhang et~al.(2018{\natexlab{a}})Zhang, Weng, Chen, Hsieh, and
  Daniel]{NIPS2018_7742}
Zhang, H., Weng, T.-W., Chen, P.-Y., Hsieh, C.-J., and Daniel, L.
\newblock Efficient neural network robustness certification with general
  activation functions.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31}, pp.\  4939--4948. Curran Associates,
  Inc., 2018{\natexlab{a}}.

\bibitem[Zhang et~al.(2018{\natexlab{b}})Zhang, Weng, Chen, Hsieh, and
  Daniel]{Zhang2018EfficientNN}
Zhang, H., Weng, T.-W., Chen, P.-Y., Hsieh, C.-J., and Daniel, L.
\newblock Efficient neural network robustness certification with general
  activation functions.
\newblock \emph{ArXiv}, abs/1811.00866, 2018{\natexlab{b}}.

\bibitem[Zhang et~al.(2018{\natexlab{c}})Zhang, Zhang, and
  Hsieh]{Zhang2018RecurJacAE}
Zhang, H., Zhang, P., and Hsieh, C.-J.
\newblock Recurjac: An efficient recursive algorithm for bounding jacobian
  matrix of neural networks and its applications.
\newblock In \emph{AAAI}, 2018{\natexlab{c}}.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Chen, Xiao, Li, Boning, and
  Hsieh]{ZhangCROWNIBP}
Zhang, H., Chen, H., Xiao, C., Li, B., Boning, D.~S., and Hsieh, C.
\newblock Towards stable and efficient training of verifiably robust neural
  networks.
\newblock \emph{CoRR}, abs/1906.06316, 2019{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1906.06316}.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Yu, Jiao, Xing, Ghaoui, and
  Jordan]{Zhang2019TheoreticallyPT}
Zhang, H., Yu, Y., Jiao, J., Xing, E.~P., Ghaoui, L.~E., and Jordan, M.~I.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In \emph{ICML}, 2019{\natexlab{b}}.

\bibitem[Zheng et~al.(2016)Zheng, Song, Leung, and
  Goodfellow]{Zheng2016ImprovingTR}
Zheng, S., Song, Y., Leung, T., and Goodfellow, I.~J.
\newblock Improving the robustness of deep neural networks via stability
  training.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  4480--4488, 2016.

\end{thebibliography}
