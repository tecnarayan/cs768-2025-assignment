@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}

@inproceedings{hendrycks2019using,
  title={Using pre-training can improve model robustness and uncertainty},
  author={Hendrycks, Dan and Lee, Kimin and Mazeika, Mantas},
  booktitle={International Conference on Machine Learning},
  pages={2712--2721},
  year={2019},
  organization={PMLR}
}

@inproceedings{xuhong2018explicit,
  title={Explicit inductive bias for transfer learning with convolutional networks},
  author={Xuhong, LI and Grandvalet, Yves and Davoine, Franck},
  booktitle={International Conference on Machine Learning},
  pages={2825--2834},
  year={2018},
  organization={PMLR}
}


@article{gouk2020distance,
  title={Distance-based regularisation of deep networks for fine-tuning},
  author={Gouk, Henry and Hospedales, Timothy M and Pontil, Massimiliano},
  journal={ICLR},
  year={2021}
}

@inproceedings{zeiler2014visualizing,
  title={Visualizing and understanding convolutional networks},
  author={Zeiler, Matthew D and Fergus, Rob},
  booktitle={European conference on computer vision},
  pages={818--833},
  year={2014},
  organization={Springer}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@article{yosinski2014transferable,
  title={How transferable are features in deep neural networks?},
  author={Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@inproceedings{guo2019spottune,
  title={Spottune: transfer learning through adaptive fine-tuning},
  author={Guo, Yunhui and Shi, Honghui and Kumar, Abhishek and Grauman, Kristen and Rosing, Tajana and Feris, Rogerio},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4805--4814},
  year={2019}
}

@article{li2019delta,
  title={Delta: Deep learning transfer using feature map with attention for convolutional networks},
  author={Li, Xingjian and Xiong, Haoyi and Wang, Hanchao and Rao, Yuxuan and Liu, Liping and Chen, Zeyu and Huan, Jun},
  journal={arXiv preprint arXiv:1901.09229},
  year={2019}
}

@inproceedings{godwin2021simple,
  title={Simple gnn regularisation for 3d molecular property prediction and beyond},
  author={Godwin, Jonathan and Schaarschmidt, Michael and Gaunt, Alexander L and Sanchez-Gonzalez, Alvaro and Rubanova, Yulia and Veli{\v{c}}kovi{\'c}, Petar and Kirkpatrick, James and Battaglia, Peter},
  booktitle={International conference on learning representations},
  year={2021}
}

@inproceedings{he2019rethinking,
  title={Rethinking imagenet pre-training},
  author={He, Kaiming and Girshick, Ross and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4918--4927},
  year={2019}
}

@article{wen2021rethinking,
  title={Rethinking pre-training on medical imaging},
  author={Wen, Yang and Chen, Leiting and Deng, Yu and Zhou, Chuan},
  journal={Journal of Visual Communication and Image Representation},
  volume={78},
  pages={103145},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}

@article{devries2018learning,
  title={Learning confidence for out-of-distribution detection in neural networks},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1802.04865},
  year={2018}
}

@article{kumar2022fine,
  title={Fine-tuning can distort pretrained features and underperform out-of-distribution},
  author={Kumar, Ananya and others},
  journal={ICLR},
  year={2022}
}


@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{iusem2003convergence,
  title={On the convergence properties of the projected gradient method for convex optimization},
  author={Iusem, Alfredo N},
  journal={Computational \& Applied Mathematics},
  volume={22},
  pages={37--52},
  year={2003},
  publisher={SciELO Brasil}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{chen2010adaptive,
  title={Adaptive total variation denoising based on difference curvature},
  author={Chen, Qiang and Montesinos, Philippe and Sun, Quan Sen and Heng, Peng Ann and others},
  journal={Image and vision computing},
  volume={28},
  number={3},
  pages={298--306},
  year={2010},
  publisher={Elsevier}
}

@article{condat2013direct,
  title={A direct algorithm for 1-D total variation denoising},
  author={Condat, Laurent},
  journal={IEEE Signal Processing Letters},
  volume={20},
  number={11},
  pages={1054--1057},
  year={2013},
  publisher={IEEE}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@article{zaken2021bitfit,
  title={Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models},
  author={Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
  journal={arXiv preprint arXiv:2106.10199},
  year={2021}
}

@inproceedings{kanavati2021partial,
  title={Partial transfusion: on the expressive influence of trainable batch norm parameters for transfer learning},
  author={Kanavati, Fahdi and Tsuneki, Masayuki},
  booktitle={Medical Imaging with Deep Learning},
  pages={338--353},
  year={2021},
  organization={PMLR}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@inproceedings{xie2021composed,
  title={Composed fine-tuning: Freezing pre-trained denoising autoencoders for improved generalization},
  author={Xie, Sang Michael and Ma, Tengyu and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={11424--11435},
  year={2021},
  organization={PMLR}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{utama2021avoiding,
  title={Avoiding inference heuristics in few-shot prompt-based finetuning},
  author={Utama, Prasetya Ajie and Moosavi, Nafise Sadat and Sanh, Victor and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2109.04144},
  year={2021}
}

@article{zhou2022learning,
  title={Learning to prompt for vision-language models},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  journal={International Journal of Computer Vision},
  volume={130},
  number={9},
  pages={2337--2348},
  year={2022},
  publisher={Springer}
}

@article{touvron2022deit,
  title={Deit iii: Revenge of the vit},
  author={Touvron, Hugo and Cord, Matthieu and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:2204.07118},
  year={2022}
}

@article{touvron2020hj,
  title={HJ egou,“Training data-efficient image transformers \& distillation through attention,”},
  author={Touvron, H and Cord, M and Douze, M and Massa, F and Sablay-rolles, A},
  journal={arXiv preprint arXiv:2012.12877},
  year={2020}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16000--16009},
  year={2022}
}

@article{zhang2017mixup,
  title={mixup: Beyond empirical risk minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1710.09412},
  year={2017}
}

@inproceedings{yun2019cutmix,
  title={Cutmix: Regularization strategy to train strong classifiers with localizable features},
  author={Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6023--6032},
  year={2019}
}

@inproceedings{you2019universal,
  title={Universal domain adaptation},
  author={You, Kaichao and Long, Mingsheng and Cao, Zhangjie and Wang, Jianmin and Jordan, Michael I},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2720--2729},
  year={2019}
}

@article{wang2018deep,
  title={Deep visual domain adaptation: A survey},
  author={Wang, Mei and Deng, Weihong},
  journal={Neurocomputing},
  volume={312},
  pages={135--153},
  year={2018},
  publisher={Elsevier}
}

@article{zhou2022domain,
  title={Domain generalization: A survey},
  author={Zhou, Kaiyang and Liu, Ziwei and Qiao, Yu and Xiang, Tao and Loy, Chen Change},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2022},
  publisher={IEEE}
}

@inproceedings{muandet2013domain,
  title={Domain generalization via invariant feature representation},
  author={Muandet, Krikamol and Balduzzi, David and Sch{\"o}lkopf, Bernhard},
  booktitle={International Conference on Machine Learning},
  pages={10--18},
  year={2013},
  organization={PMLR}
}

@article{tian2021geometric,
  title={A geometric perspective towards neural calibration via sensitivity decomposition},
  author={Tian, Junjiao and Yung, Dylan and Hsu, Yen-Chang and Kira, Zsolt},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={26358--26369},
  year={2021}
}

@inproceedings{wortsman2022robust,
  title={Robust fine-tuning of zero-shot models},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Kim, Jong Wook and Li, Mike and Kornblith, Simon and Roelofs, Rebecca and Lopes, Raphael Gontijo and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7959--7971},
  year={2022}
}

@article{raghu2019transfusion,
  title={Transfusion: Understanding transfer learning for medical imaging},
  author={Raghu, Maithra and Zhang, Chiyuan and Kleinberg, Jon and Bengio, Samy},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@inproceedings{recht2019imagenet,
  title={Do imagenet classifiers generalize to imagenet?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle={International Conference on Machine Learning},
  pages={5389--5400},
  year={2019},
  organization={PMLR}
}

@inproceedings{hendrycks2021many,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8340--8349},
  year={2021}
}

@inproceedings{hendrycks2021natural,
  title={Natural adversarial examples},
  author={Hendrycks, Dan and Zhao, Kevin and Basart, Steven and Steinhardt, Jacob and Song, Dawn},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={15262--15271},
  year={2021}
}

@article{wang2019learning,
  title={Learning robust global representations by penalizing local predictive power},
  author={Wang, Haohan and Ge, Songwei and Lipton, Zachary and Xing, Eric P},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{peng2019moment,
  title={Moment matching for multi-source domain adaptation},
  author={Peng, Xingchao and Bai, Qinxun and Xia, Xide and Huang, Zijun and Saenko, Kate and Wang, Bo},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1406--1415},
  year={2019}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@inproceedings{szegedy2016rethinking,
  title={Rethinking the inception architecture for computer vision},
  author={Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jon and Wojna, Zbigniew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2818--2826},
  year={2016}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@inproceedings{miller2021accuracy,
  title={Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization},
  author={Miller, John P and Taori, Rohan and Raghunathan, Aditi and Sagawa, Shiori and Koh, Pang Wei and Shankar, Vaishaal and Liang, Percy and Carmon, Yair and Schmidt, Ludwig},
  booktitle={International Conference on Machine Learning},
  pages={7721--7735},
  year={2021},
  organization={PMLR}
}

@article{mania2020classifier,
  title={Why do classifier accuracies show linear trends under distribution shift?},
  author={Mania, Horia and Sra, Suvrit},
  journal={arXiv preprint arXiv:2012.15483},
  year={2020}
}

@inproceedings{chen2021empirical,
  title={An empirical study of training self-supervised vision transformers},
  author={Chen, Xinlei and Xie, Saining and He, Kaiming},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9640--9649},
  year={2021}
}

@article{wu2020understanding,
  title={Understanding and improving information transfer in multi-task learning},
  author={Wu, Sen and Zhang, Hongyang R and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2005.00944},
  year={2020}
}

@article{tripuraneni2020theory,
  title={On the theory of transfer learning: The importance of task diversity},
  author={Tripuraneni, Nilesh and Jordan, Michael and Jin, Chi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7852--7862},
  year={2020}
}

@article{du2020few,
  title={Few-shot learning via learning the representation, provably},
  author={Du, Simon S and Hu, Wei and Kakade, Sham M and Lee, Jason D and Lei, Qi},
  journal={arXiv preprint arXiv:2002.09434},
  year={2020}
}

@article{xie2020n,
  title={In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness},
  author={Xie, Sang Michael and Kumar, Ananya and Jones, Robbie and Khani, Fereshte and Ma, Tengyu and Liang, Percy},
  journal={arXiv preprint arXiv:2012.04550},
  year={2020}
}

@article{lee2022surgical,
  title={Surgical Fine-Tuning Improves Adaptation to Distribution Shifts},
  author={Lee, Yoonho and Chen, Annie S and Tajwar, Fahim and Kumar, Ananya and Yao, Huaxiu and Liang, Percy and Finn, Chelsea},
  journal={arXiv preprint arXiv:2210.11466},
  year={2022}
}

@article{neyshabur2020being,
  title={What is being transferred in transfer learning?},
  author={Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={512--523},
  year={2020}
}

@inproceedings{nilsback2006visual,
  title={A visual vocabulary for flower classification},
  author={Nilsback, M-E and Zisserman, Andrew},
  booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  volume={2},
  pages={1447--1454},
  year={2006},
  organization={IEEE}
}

@inproceedings{shen2021partial,
  title={Partial is better than all: Revisiting fine-tuning strategy for few-shot learning},
  author={Shen, Zhiqiang and Liu, Zechun and Qin, Jie and Savvides, Marios and Cheng, Kwang-Ting},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={11},
  pages={9594--9602},
  year={2021}
}


@inproceedings{bergstra2013making,
  title={Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures},
  author={Bergstra, James and Yamins, Daniel and Cox, David},
  booktitle={International conference on machine learning},
  pages={115--123},
  year={2013},
  organization={PMLR}
}

@article{pauli2021training,
  title={Training robust neural networks using Lipschitz bounds},
  author={Pauli, Patricia and Koch, Anne and Berberich, Julian and Kohler, Paul and Allg{\"o}wer, Frank},
  journal={IEEE Control Systems Letters},
  volume={6},
  pages={121--126},
  year={2021},
  publisher={IEEE}
}

@article{huang2021training,
  title={Training certifiably robust neural networks with efficient local lipschitz bounds},
  author={Huang, Yujia and Zhang, Huan and Shi, Yuanyuan and Kolter, J Zico and Anandkumar, Anima},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22745--22757},
  year={2021}
}

@article{gouk2021regularisation,
  title={Regularisation of neural networks by enforcing lipschitz continuity},
  author={Gouk, Henry and Frank, Eibe and Pfahringer, Bernhard and Cree, Michael J},
  journal={Machine Learning},
  volume={110},
  pages={393--416},
  year={2021},
  publisher={Springer}
}

@article{tian2023trainable,
  title={Trainable Projected Gradient Method for Robust Fine-tuning},
  author={Tian, Junjiao and Dai, Xiaoliang and Ma, Chih-Yao and He, Zecheng and Liu, Yen-Cheng and Kira, Zsolt},
  journal={arXiv preprint arXiv:2303.10720},
  year={2023}
}

@article{lee2020lipschitz,
  title={Lipschitz-certifiable training with a tight outer bound},
  author={Lee, Sungyoon and Lee, Jaewook and Park, Saerom},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={16891--16902},
  year={2020}
}

@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9650--9660},
  year={2021}
}


@article{everingham2010pascal,
  title={The pascal visual object classes (voc) challenge},
  author={Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal=ijcv,
  year={2010},
}

@article{liu2022polyhistor,
  title={Polyhistor: Parameter-Efficient Multi-Task Adaptation for Dense Vision Tasks},
  author={Liu, Yen-Cheng and Ma, Chih-Yao and Tian, Junjiao and He, Zijian and Kira, Zsolt},
  journal={arXiv preprint arXiv:2210.03265},
  year={2022}
}


@article{hendrycks2019benchmarking,
  title={Benchmarking neural network robustness to common corruptions and perturbations},
  author={Hendrycks, Dan and Dietterich, Thomas},
  journal={arXiv preprint arXiv:1903.12261},
  year={2019}
}

@article{xie2021segformer,
  title={SegFormer: Simple and efficient design for semantic segmentation with transformers},
  author={Xie, Enze and Wang, Wenhai and Yu, Zhiding and Anandkumar, Anima and Alvarez, Jose M and Luo, Ping},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12077--12090},
  year={2021}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@InProceedings{pmlr-v139-touvron21a,
  title =     {Training data-efficient image transformers and distillation through attention},
  author =    {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
  booktitle = {International Conference on Machine Learning},
  pages =     {10347--10357},
  year =      {2021},
  volume =    {139},
  month =     {July}
}

@article{larsson2016fractalnet,
  title={Fractalnet: Ultra-deep neural networks without residuals},
  author={Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
  journal={arXiv preprint arXiv:1605.07648},
  year={2016}
}

@article{goyal2022finetune,
  title={Finetune like you pretrain: Improved finetuning of zero-shot vision models},
  author={Goyal, Sachin and Kumar, Ananya and Garg, Sankalp and Kolter, Zico and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2212.00638},
  year={2022}
}

@article{smith2022closer,
  title={A closer look at rehearsal-free continual learning},
  author={Smith, James Seale and Tian, Junjiao and Hsu, Yen-Chang and Kira, Zsolt},
  journal={arXiv preprint arXiv:2203.17269},
  year={2022}
}

@article{zhang2020revisiting,
  title={Revisiting few-sample BERT fine-tuning},
  author={Zhang, Tianyi and Wu, Felix and Katiyar, Arzoo and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:2006.05987},
  year={2020}
}


@article{li2016learning,
  title={Learning without forgetting},
  author={Li, Zhizhong and Hoiem, Derek},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2935--2947},
  year={2017},
  publisher={IEEE}
}

@article{wang2022dualprompt,
  title={DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning},
  author={Wang, Zifeng and Zhang, Zizhao and Ebrahimi, Sayna and Sun, Ruoxi and Zhang, Han and Lee, Chen-Yu and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and others},
  journal={arXiv preprint arXiv:2204.04799},
  year={2022}
}

@inproceedings{wang2022learning,
  title={Learning to prompt for continual learning},
  author={Wang, Zifeng and Zhang, Zizhao and Lee, Chen-Yu and Zhang, Han and Sun, Ruoxi and Ren, Xiaoqi and Su, Guolong and Perot, Vincent and Dy, Jennifer and Pfister, Tomas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={139--149},
  year={2022}
}

@article{smith2022coda,
  title={CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning},
  author={Smith, James Seale and Karlinsky, Leonid and Gutta, Vyshnavi and Cascante-Bonilla, Paola and Kim, Donghyun and Arbelle, Assaf and Panda, Rameswar and Feris, Rogerio and Kira, Zsolt},
  journal={arXiv preprint arXiv:2211.13218},
  year={2022}
}

@article{kirkpatrick2017overcoming,
    title={Overcoming catastrophic forgetting in neural networks},
    author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
    journal={Proceedings of the national academy of sciences},
    year={2017}
}

@inproceedings{lee2019overcoming,
  title={Overcoming catastrophic forgetting with unlabeled data in the wild},
  author={Lee, Kibok and Lee, Kimin and Shin, Jinwoo and Lee, Honglak},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={312--321},
  year={2019}
}

@article{baydin2017online,
  title={Online learning rate adaptation with hypergradient descent},
  author={Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank},
  journal={arXiv preprint arXiv:1703.04782},
  year={2017}
}

@incollection{almeida1999parameter,
  title={Parameter adaptation in stochastic optimization},
  author={Almeida, Lu{\'\i}s B and Langlois, Thibault and Amaral, Jos{\'e} D and Plakhov, Alexander},
  booktitle={On-line learning in neural networks},
  pages={111--134},
  year={1999}
}

@article{bengio2000gradient,
  title={Gradient-based optimization of hyperparameters},
  author={Bengio, Yoshua},
  journal={Neural computation},
  volume={12},
  number={8},
  pages={1889--1900},
  year={2000},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{domke2012generic,
  title={Generic methods for optimization-based modeling},
  author={Domke, Justin},
  booktitle={Artificial Intelligence and Statistics},
  pages={318--326},
  year={2012},
  organization={PMLR}
}

@article{feurer2019hyperparameter,
  title={Hyperparameter optimization},
  author={Feurer, Matthias and Hutter, Frank},
  journal={Automated machine learning: Methods, systems, challenges},
  pages={3--33},
  year={2019},
  publisher={Springer International Publishing}
}

@inproceedings{pedregosa2016hyperparameter,
  title={Hyperparameter optimization with approximate gradient},
  author={Pedregosa, Fabian},
  booktitle={International conference on machine learning},
  pages={737--746},
  year={2016},
  organization={PMLR}
}

@inproceedings{wang2021guarantees,
  title={Guarantees for tuning the step size using a learning-to-learn approach},
  author={Wang, Xiang and Yuan, Shuai and Wu, Chenwei and Ge, Rong},
  booktitle={International Conference on Machine Learning},
  pages={10981--10990},
  year={2021},
  organization={PMLR}
}

@article{lodha2023surgical,
  title={On Surgical Fine-tuning for Language Encoders},
  author={Lodha, Abhilasha and Belapurkar, Gayatri and Chalkapurkar, Saloni and Tao, Yuanming and Ghosh, Reshmi and Basu, Samyadeep and Petrov, Dmitrii and Srinivasan, Soundararajan},
  journal={arXiv preprint arXiv:2310.17041},
  year={2023}
}

@inproceedings{fu2023effectiveness,
  title={On the effectiveness of parameter-efficient fine-tuning},
  author={Fu, Zihao and Yang, Haoran and So, Anthony Man-Cho and Lam, Wai and Bing, Lidong and Collier, Nigel},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={11},
  pages={12799--12807},
  year={2023}
}

@inproceedings{balles2018dissecting,
  title={Dissecting adam: The sign, magnitude and variance of stochastic gradients},
  author={Balles, Lukas and Hennig, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={404--413},
  year={2018},
  organization={PMLR}
}

@article{zhuang2020adabelief,
  title={Adabelief optimizer: Adapting stepsizes by the belief in observed gradients},
  author={Zhuang, Juntang and Tang, Tommy and Ding, Yifan and Tatikonda, Sekhar C and Dvornek, Nicha and Papademetris, Xenophon and Duncan, James},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={18795--18806},
  year={2020}
}

@article{garrigos2023handbook,
  title={Handbook of convergence theorems for (stochastic) gradient methods},
  author={Garrigos, Guillaume and Gower, Robert M},
  journal={arXiv preprint arXiv:2301.11235},
  year={2023}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={SIAM review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@article{chandra2022gradient,
  title={Gradient descent: The ultimate optimizer},
  author={Chandra, Kartik and Xie, Audrey and Ragan-Kelley, Jonathan and Meijer, Erik},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8214--8225},
  year={2022}
}

@article{tian2024fast,
  title={Fast Trainable Projection for Robust Fine-Tuning},
  author={Tian, Junjiao and Liu, Yen-Cheng and Smith, James S and Kira, Zsolt},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


@article{you2017large,
  title={Large batch training of convolutional networks},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  year={2017}
}

@article{you2019large,
  title={Large batch optimization for deep learning: Training bert in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:1904.00962},
  year={2019}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}


@article{he2022sparseadapter,
  title={Sparseadapter: An easy approach for improving the parameter-efficiency of adapters},
  author={He, Shwai and Ding, Liang and Dong, Daize and Zhang, Miao and Tao, Dacheng},
  journal={arXiv preprint arXiv:2210.04284},
  year={2022}
}

@article{hu2023llm,
  title={Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models},
  author={Hu, Zhiqiang and Wang, Lei and Lan, Yihuai and Xu, Wanyu and Lim, Ee-Peng and Bing, Lidong and Xu, Xing and Poria, Soujanya and Lee, Roy Ka-Wei},
  journal={arXiv preprint arXiv:2304.01933},
  year={2023}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1139--1147},
  year={2013},
  organization={PMLR}
}

@article{ruder2016overview,
  title={An overview of gradient descent optimization algorithms},
  author={Ruder, Sebastian},
  journal={arXiv preprint arXiv:1609.04747},
  year={2016}
}

@misc{himmelblau,
  title = {Himmelblau's function},
  howpublished = {\url{https://en.wikipedia.org/wiki/Himmelblau%27s_function}},
}

% PaliGemma
@misc{huggingface_paligemma_3b_pt_224,
  author = {Google},
  title = {paligemma-3b-pt-224},
  year = {2024},
  howpublished = {\url{https://huggingface.co/google/paligemma-3b-pt-224}}
}

@misc{huggingface_paligemma_3b_ft_vqav2_224,
  author = {Google},
  title = {paligemma-3b-ft-vqav2-224},
  year = {2024},
  howpublished = {\url{https://huggingface.co/google/paligemma-3b-ft-vqav2-224}}
}

@misc{beyer_paligemma_2024,
	title = {{PaliGemma}: {A} versatile {3B} {VLM} for transfer},
	shorttitle = {{PaliGemma}},
	url = {http://arxiv.org/abs/2407.07726},
	doi = {10.48550/arXiv.2407.07726},
	abstract = {PaliGemma is an open Vision-Language Model (VLM) that is based on the SigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to be a versatile and broadly knowledgeable base model that is effective to transfer. It achieves strong performance on a wide variety of open-world tasks. We evaluate PaliGemma on almost 40 diverse tasks including standard VLM benchmarks, but also more specialized tasks such as remote-sensing and segmentation.},
	urldate = {2024-07-14},
	publisher = {arXiv},
	author = {Beyer, Lucas and Steiner, Andreas and Pinto, André Susano and Kolesnikov, Alexander and Wang, Xiao and Salz, Daniel and Neumann, Maxim and Alabdulmohsin, Ibrahim and Tschannen, Michael and Bugliarello, Emanuele and Unterthiner, Thomas and Keysers, Daniel and Koppula, Skanda and Liu, Fangyu and Grycner, Adam and Gritsenko, Alexey and Houlsby, Neil and Kumar, Manoj and Rong, Keran and Eisenschlos, Julian and Kabra, Rishabh and Bauer, Matthias and Bošnjak, Matko and Chen, Xi and Minderer, Matthias and Voigtlaender, Paul and Bica, Ioana and Balazevic, Ivana and Puigcerver, Joan and Papalampidi, Pinelopi and Henaff, Olivier and Xiong, Xi and Soricut, Radu and Harmsen, Jeremiah and Zhai, Xiaohua},
	month = jul,
	year = {2024},
	note = {arXiv:2407.07726 [cs]
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/WN2HTGFV/Beyer et al. - 2024 - PaliGemma A versatile 3B VLM for transfer.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/GCRC78VI/2407.html:text/html},
}

% vqa_datasets

@misc{agrawal_dont_2018,
	title = {Don't {Just} {Assume}; {Look} and {Answer}: {Overcoming} {Priors} for {Visual} {Question} {Answering}},
	shorttitle = {Don't {Just} {Assume}; {Look} and {Answer}},
	url = {http://arxiv.org/abs/1712.00377},
	abstract = {A number of studies have found that today’s Visual Question Answering (VQA) models are heavily driven by superﬁcial correlations in the training data and lack sufﬁcient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Speciﬁcally, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQACP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades signiﬁcantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture speciﬁcally designed to prevent the model from ‘cheating’ by primarily relying on priors in the training data. Speciﬁcally, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identiﬁcation of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model – Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA signiﬁcantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.},
	language = {en},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
	month = jun,
	year = {2018},
	note = {arXiv:1712.00377 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 10 figures. To appear in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018},
	file = {1712.00377.pdf:/Users/chengyuehuang/Zotero/storage/2W9BTGZI/1712.00377.pdf:application/pdf},
}

@misc{fu_blink_2024,
	title = {{BLINK}: {Multimodal} {Large} {Language} {Models} {Can} {See} but {Not} {Perceive}},
	shorttitle = {{BLINK}},
	url = {http://arxiv.org/abs/2404.12390},
	doi = {10.48550/arXiv.2404.12390},
	abstract = {We introduce Blink, a new benchmark for multimodal language models (LLMs) that focuses on core visual perception abilities not found in other evaluations. Most of the Blink tasks can be solved by humans "within a blink" (e.g., relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning). However, we find these perception-demanding tasks cast significant challenges for current multimodal LLMs because they resist mediation through natural language. Blink reformats 14 classic computer vision tasks into 3,807 multiple-choice questions, paired with single or multiple images and visual prompting. While humans get 95.70\% accuracy on average, Blink is surprisingly challenging for existing multimodal LLMs: even the best-performing GPT-4V and Gemini achieve accuracies of 51.26\% and 45.72\%, only 13.17\% and 7.63\% higher than random guessing, indicating that such perception abilities have not "emerged" yet in recent multimodal LLMs. Our analysis also highlights that specialist CV models could solve these problems much better, suggesting potential pathways for future improvements. We believe Blink will stimulate the community to help multimodal LLMs catch up with human-level visual perception.},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Fu, Xingyu and Hu, Yushi and Li, Bangzheng and Feng, Yu and Wang, Haoyu and Lin, Xudong and Roth, Dan and Smith, Noah A. and Ma, Wei-Chiu and Krishna, Ranjay},
	month = may,
	year = {2024},
	note = {arXiv:2404.12390 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	annote = {Comment: Multimodal Benchmark, Project Url: https://zeyofu.github.io/blink/},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/YHA3DQGC/Fu et al. - 2024 - BLINK Multimodal Large Language Models Can See bu.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/4SIG8GBT/2404.html:text/html},
}

@misc{dancette_beyond_2021,
	title = {Beyond {Question}-{Based} {Biases}: {Assessing} {Multimodal} {Shortcut} {Learning} in {Visual} {Question} {Answering}},
	shorttitle = {Beyond {Question}-{Based} {Biases}},
	url = {http://arxiv.org/abs/2104.03149},
	doi = {10.48550/arXiv.2104.03149},
	abstract = {We introduce an evaluation methodology for visual question answering (VQA) to better diagnose cases of shortcut learning. These cases happen when a model exploits spurious statistical regularities to produce correct answers but does not actually deploy the desired behavior. There is a need to identify possible shortcuts in a dataset and assess their use before deploying a model in the real world. The research community in VQA has focused exclusively on question-based shortcuts, where a model might, for example, answer "What is the color of the sky" with "blue" by relying mostly on the question-conditional training prior and give little weight to visual evidence. We go a step further and consider multimodal shortcuts that involve both questions and images. We first identify potential shortcuts in the popular VQA v2 training set by mining trivial predictive rules such as co-occurrences of words and visual elements. We then introduce VQA-CounterExamples (VQA-CE), an evaluation protocol based on our subset of CounterExamples i.e. image-question-answer triplets where our rules lead to incorrect answers. We use this new evaluation in a large-scale study of existing approaches for VQA. We demonstrate that even state-of-the-art models perform poorly and that existing techniques to reduce biases are largely ineffective in this context. Our findings suggest that past work on question-based biases in VQA has only addressed one facet of a complex issue. The code for our method is available at https://github.com/cdancette/detect-shortcuts.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Dancette, Corentin and Cadene, Remi and Teney, Damien and Cord, Matthieu},
	month = sep,
	year = {2021},
	note = {arXiv:2104.03149 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted at ICCV 2021. Code is available at https://github.com/cdancette/detect-shortcuts},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/8RXYXCVI/Dancette et al. - 2021 - Beyond Question-Based Biases Assessing Multimodal.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/7NLVPT9J/2104.html:text/html},
}

@misc{agrawal_reassessing_2023,
	title = {Reassessing {Evaluation} {Practices} in {Visual} {Question} {Answering}: {A} {Case} {Study} on {Out}-of-{Distribution} {Generalization}},
	shorttitle = {Reassessing {Evaluation} {Practices} in {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2205.12191},
	doi = {10.48550/arXiv.2205.12191},
	abstract = {Vision-and-language (V\&L) models pretrained on large-scale multimodal data have demonstrated strong performance on various tasks such as image captioning and visual question answering (VQA). The quality of such models is commonly assessed by measuring their performance on unseen data that typically comes from the same distribution as the training data. However, when evaluated under out-of-distribution (out-of-dataset) settings for VQA, we observe that these models exhibit poor generalization. We comprehensively evaluate two pretrained V\&L models under different settings (i.e. classification and open-ended text generation) by conducting cross-dataset evaluations. We find that these models tend to learn to solve the benchmark, rather than learning the high-level skills required by the VQA task. We also find that in most cases generative models are less susceptible to shifts in data distribution compared to discriminative ones, and that multimodal pretraining is generally helpful for OOD generalization. Finally, we revisit assumptions underlying the use of automatic VQA evaluation metrics, and empirically show that their stringent nature repeatedly penalizes models for correct responses.},
	urldate = {2024-05-20},
	publisher = {arXiv},
	author = {Agrawal, Aishwarya and Kajić, Ivana and Bugliarello, Emanuele and Davoodi, Elnaz and Gergely, Anita and Blunsom, Phil and Nematzadeh, Aida},
	month = apr,
	year = {2023},
	note = {arXiv:2205.12191 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Findings of EACL 2023. Aishwarya, Ivana, Emanuele and Aida had equal first author contributions. Elnaz and Anita had equal contributions. Aida and Aishwarya had equal senior contributions},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/EMDEIT4G/Agrawal et al. - 2023 - Reassessing Evaluation Practices in Visual Questio.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/HA396PSQ/2205.html:text/html},
}

@inproceedings{akula_crossvqa_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {{CrossVQA}: {Scalably} {Generating} {Benchmarks} for {Systematically} {Testing} {VQA} {Generalization}},
	shorttitle = {{CrossVQA}},
	url = {https://aclanthology.org/2021.emnlp-main.164},
	doi = {10.18653/v1/2021.emnlp-main.164},
	abstract = {One challenge in evaluating visual question answering (VQA) models in the cross-dataset adaptation setting is that the distribution shifts are multi-modal, making it difficult to identify if it is the shifts in visual or language features that play a key role. In this paper, we propose a semi-automatic framework for generating disentangled shifts by introducing a controllable visual question-answer generation (VQAG) module that is capable of generating highly-relevant and diverse question-answer pairs with the desired dataset style. We use it to create CrossVQA, a collection of test splits for assessing VQA generalization based on the VQA2, VizWiz, and Open Images datasets. We provide an analysis of our generated datasets and demonstrate its utility by using them to evaluate several state-of-the-art VQA systems. One important finding is that the visual shifts in cross-dataset VQA matter more than the language shifts. More broadly, we present a scalable framework for systematically evaluating the machine with little human intervention.},
	urldate = {2024-05-20},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Akula, Arjun and Changpinyo, Soravit and Gong, Boqing and Sharma, Piyush and Zhu, Song-Chun and Soricut, Radu},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	month = nov,
	year = {2021},
	keywords = {Readlist},
	pages = {2148--2166},
	annote = {One},
	file = {Full Text PDF:/Users/chengyuehuang/Zotero/storage/TLKKNKBJ/Akula et al. - 2021 - CrossVQA Scalably Generating Benchmarks for Syste.pdf:application/pdf},
}

@misc{peng_synthesize_2024,
	title = {Synthesize, {Diagnose}, and {Optimize}: {Towards} {Fine}-{Grained} {Vision}-{Language} {Understanding}},
	shorttitle = {Synthesize, {Diagnose}, and {Optimize}},
	url = {http://arxiv.org/abs/2312.00081},
	doi = {10.48550/arXiv.2312.00081},
	abstract = {Vision language models (VLM) have demonstrated remarkable performance across various downstream tasks. However, understanding fine-grained visual-linguistic concepts, such as attributes and inter-object relationships, remains a significant challenge. While several benchmarks aim to evaluate VLMs in finer granularity, their primary focus remains on the linguistic aspect, neglecting the visual dimension. Here, we highlight the importance of evaluating VLMs from both a textual and visual perspective. We introduce a progressive pipeline to synthesize images that vary in a specific attribute while ensuring consistency in all other aspects. Utilizing this data engine, we carefully design a benchmark, SPEC, to diagnose the comprehension of object size, position, existence, and count. Subsequently, we conduct a thorough evaluation of four leading VLMs on SPEC. Surprisingly, their performance is close to random guess, revealing significant limitations. With this in mind, we propose a simple yet effective approach to optimize VLMs in fine-grained understanding, achieving significant improvements on SPEC without compromising the zero-shot performance. Results on two additional fine-grained benchmarks also show consistent improvements, further validating the transferability of our approach. Code and data are available at https://github.com/wjpoom/SPEC.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Peng, Wujian and Xie, Sicheng and You, Zuyao and Lan, Shiyi and Wu, Zuxuan},
	month = mar,
	year = {2024},
	note = {arXiv:2312.00081 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by CVPR 2024},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/P2VN4IPY/Peng et al. - 2024 - Synthesize, Diagnose, and Optimize Towards Fine-G.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/SUZFZ9KT/2312.html:text/html},
}

@misc{ma_robust_2024,
	title = {Robust {Visual} {Question} {Answering}: {Datasets}, {Methods}, and {Future} {Challenges}},
	shorttitle = {Robust {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2307.11471},
	doi = {10.48550/arXiv.2307.11471},
	abstract = {Visual question answering requires a system to provide an accurate natural language answer given an image and a natural language question. However, it is widely recognized that previous generic VQA methods often exhibit a tendency to memorize biases present in the training data rather than learning proper behaviors, such as grounding images before predicting answers. Therefore, these methods usually achieve high in-distribution but poor out-of-distribution performance. In recent years, various datasets and debiasing methods have been proposed to evaluate and enhance the VQA robustness, respectively. This paper provides the first comprehensive survey focused on this emerging fashion. Specifically, we first provide an overview of the development process of datasets from in-distribution and out-of-distribution perspectives. Then, we examine the evaluation metrics employed by these datasets. Thirdly, we propose a typology that presents the development process, similarities and differences, robustness comparison, and technical features of existing debiasing methods. Furthermore, we analyze and discuss the robustness of representative vision-and-language pre-training models on VQA. Finally, through a thorough review of the available literature and experimental analysis, we discuss the key areas for future research from various viewpoints.},
	urldate = {2024-05-23},
	publisher = {arXiv},
	author = {Ma, Jie and Wang, Pinghui and Kong, Dechen and Wang, Zewei and Liu, Jun and Pei, Hongbin and Zhao, Junzhou},
	month = feb,
	year = {2024},
	note = {arXiv:2307.11471 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, I.2.10},
	annote = {Comment: Accepted by IEEE TPAMI},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/PNVZLEB2/Ma et al. - 2024 - Robust Visual Question Answering Datasets, Method.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/FL42Q55Y/2307.html:text/html},
}

@misc{li_closer_2021,
	title = {A {Closer} {Look} at the {Robustness} of {Vision}-and-{Language} {Pre}-trained {Models}},
	url = {http://arxiv.org/abs/2012.08673},
	doi = {10.48550/arXiv.2012.08673},
	abstract = {Large-scale pre-trained multimodal transformers, such as ViLBERT and UNITER, have propelled the state of the art in vision-and-language (V+L) research to a new level. Although achieving impressive performance on standard tasks, to date, it still remains unclear how robust these pre-trained models are. To investigate, we conduct a host of thorough evaluations on existing pre-trained models over 4 different types of V+L specific model robustness: (i) Linguistic Variation; (ii) Logical Reasoning; (iii) Visual Content Manipulation; and (iv) Answer Distribution Shift. Interestingly, by standard model finetuning, pre-trained V+L models already exhibit better robustness than many task-specific state-of-the-art methods. To further enhance model robustness, we propose Mango, a generic and efficient approach that learns a Multimodal Adversarial Noise GeneratOr in the embedding space to fool pre-trained V+L models. Differing from previous studies focused on one specific type of robustness, Mango is task-agnostic, and enables universal performance lift for pre-trained models over diverse tasks designed to evaluate broad aspects of robustness. Comprehensive experiments demonstrate that Mango achieves new state of the art on 7 out of 9 robustness benchmarks, surpassing existing methods by a significant margin. As the first comprehensive study on V+L robustness, this work puts robustness of pre-trained models into sharper focus, pointing new directions for future study.},
	urldate = {2024-06-26},
	publisher = {arXiv},
	author = {Li, Linjie and Gan, Zhe and Liu, Jingjing},
	month = mar,
	year = {2021},
	note = {arXiv:2012.08673 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/5BSP3X9F/Li et al. - 2021 - A Closer Look at the Robustness of Vision-and-Lang.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/23R2N6W7/2012.html:text/html},
}

@misc{si_language_2022,
	title = {Language {Prior} {Is} {Not} the {Only} {Shortcut}: {A} {Benchmark} for {Shortcut} {Learning} in {VQA}},
	shorttitle = {Language {Prior} {Is} {Not} the {Only} {Shortcut}},
	url = {http://arxiv.org/abs/2210.04692},
	doi = {10.48550/arXiv.2210.04692},
	abstract = {Visual Question Answering (VQA) models are prone to learn the shortcut solution formed by dataset biases rather than the intended solution. To evaluate the VQA models' reasoning ability beyond shortcut learning, the VQA-CP v2 dataset introduces a distribution shift between the training and test set given a question type. In this way, the model cannot use the training set shortcut (from question type to answer) to perform well on the test set. However, VQA-CP v2 only considers one type of shortcut and thus still cannot guarantee that the model relies on the intended solution rather than a solution specific to this shortcut. To overcome this limitation, we propose a new dataset that considers varying types of shortcuts by constructing different distribution shifts in multiple OOD test sets. In addition, we overcome the three troubling practices in the use of VQA-CP v2, e.g., selecting models using OOD test sets, and further standardize OOD evaluation procedure. Our benchmark provides a more rigorous and comprehensive testbed for shortcut learning in VQA. We benchmark recent methods and find that methods specifically designed for particular shortcuts fail to simultaneously generalize to our varying OOD test sets. We also systematically study the varying shortcuts and provide several valuable findings, which may promote the exploration of shortcut learning in VQA.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Si, Qingyi and Meng, Fandong and Zheng, Mingyu and Lin, Zheng and Liu, Yuanxin and Fu, Peng and Cao, Yanan and Wang, Weiping and Zhou, Jie},
	month = oct,
	year = {2022},
	note = {arXiv:2210.04692 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Fingdings of EMNLP-2022},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/BW3QF5YX/Si et al. - 2022 - Language Prior Is Not the Only Shortcut A Benchma.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/A6CU6QH2/2210.html:text/html},
}

@inproceedings{reichman_outside_2023,
	title = {Outside {Knowledge} {Visual} {Question} {Answering} {Version} 2.0},
	url = {https://ieeexplore.ieee.org/abstract/document/10096074},
	doi = {10.1109/ICASSP49357.2023.10096074},
	abstract = {Visual question answering (VQA) lies at the intersection of language and vision research. It functions as a building block for multimodal conversational AI and serves as a testbed for assessing a model’s capability for open-domain scene understanding. While progress in this area was initially accelerated with the 2015 release of the popular and large dataset "VQA", new datasets are required to continue this research momentum. For example, the 2019 Outside Knowledge VQA dataset "OKVQA" extends VQA by adding more challenging questions that require complex, factual, and commonsense knowledge. However, in our analysis, we found that 41.4\% of the dataset needed to be corrected and 10.6\% needed to be removed. This paper describes the analysis, corrections, and removals completed and presents a new dataset: OK-VQA Version 2.0. To gain insights into the impact of the changes on OK-VQA research, the paper presents results on state-of-the-art models retrained with this new dataset. The side-by-side comparisons show that one method in particular, Knowledge Augmented Transformer for Vision-and-Language, extends its relative lead over competing methods. The dataset is available online.1},
	urldate = {2024-06-27},
	booktitle = {{ICASSP} 2023 - 2023 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Reichman, Benjamin Z. and Sundar, Anirudh and Richardson, Christopher and Zubatiy, Tamara and Chowdhury, Prithwijit and Shah, Aaryan and Truxal, Jack and Grimes, Micah and Shah, Dristi and Chee, Woo Ju and Punjwani, Saif and Jain, Atishay and Heck, Larry},
	month = jun,
	year = {2023},
	note = {ISSN: 2379-190X},
	keywords = {Acoustics, Datasets, Lead, Outside Knowledge VQA, Question answering (information retrieval), Signal processing, Task analysis, Transformers, Visual Question Answering, Visualization},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:/Users/chengyuehuang/Zotero/storage/N6MIJ9MI/10096074.html:text/html},
}

@misc{unni_vqa-gen_2023,
	title = {{VQA}-{GEN}: {A} {Visual} {Question} {Answering} {Benchmark} for {Domain} {Generalization}},
	shorttitle = {{VQA}-{GEN}},
	url = {http://arxiv.org/abs/2311.00807},
	doi = {10.48550/arXiv.2311.00807},
	abstract = {Visual question answering (VQA) models are designed to demonstrate visual-textual reasoning capabilities. However, their real-world applicability is hindered by a lack of comprehensive benchmark datasets. Existing domain generalization datasets for VQA exhibit a unilateral focus on textual shifts while VQA being a multi-modal task contains shifts across both visual and textual domains. We propose VQA-GEN, the first ever multi-modal benchmark dataset for distribution shift generated through a shift induced pipeline. Experiments demonstrate VQA-GEN dataset exposes the vulnerability of existing methods to joint multi-modal distribution shifts. validating that comprehensive multi-modal shifts are critical for robust VQA generalization. Models trained on VQA-GEN exhibit improved cross-domain and in-domain performance, confirming the value of VQA-GEN. Further, we analyze the importance of each shift technique of our pipeline contributing to the generalization of the model.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Unni, Suraj Jyothi and Moraffah, Raha and Liu, Huan},
	month = nov,
	year = {2023},
	note = {arXiv:2311.00807 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/WNXNUZ9E/Unni et al. - 2023 - VQA-GEN A Visual Question Answering Benchmark for.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/XKM28E7B/2311.html:text/html},
}

@misc{singh_towards_2019,
	title = {Towards {VQA} {Models} {That} {Can} {Read}},
	url = {http://arxiv.org/abs/1904.08920},
	doi = {10.48550/arXiv.1904.08920},
	abstract = {Studies have shown that a dominant class of questions asked by visually impaired users on images of their surroundings involves reading text in the image. But today's VQA models can not read! Our paper takes a first step towards addressing this problem. First, we introduce a new "TextVQA" dataset to facilitate progress on this important problem. Existing datasets either have a small proportion of questions about text (e.g., the VQA dataset) or are too small (e.g., the VizWiz dataset). TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Second, we introduce a novel model architecture that reads text in the image, reasons about it in the context of the image and the question, and predicts an answer which might be a deduction based on the text and the image or composed of the strings found in the image. Consequently, we call our approach Look, Read, Reason \& Answer (LoRRA). We show that LoRRA outperforms existing state-of-the-art VQA models on our TextVQA dataset. We find that the gap between human performance and machine performance is significantly larger on TextVQA than on VQA 2.0, suggesting that TextVQA is well-suited to benchmark progress along directions complementary to VQA 2.0.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
	month = may,
	year = {2019},
	note = {arXiv:1904.08920 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: CVPR 2019},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/HAX273JA/Singh et al. - 2019 - Towards VQA Models That Can Read.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/49VRV267/1904.html:text/html},
}

@misc{gokhale_vqa-lol_2020,
	title = {{VQA}-{LOL}: {Visual} {Question} {Answering} under the {Lens} of {Logic}},
	shorttitle = {{VQA}-{LOL}},
	url = {http://arxiv.org/abs/2002.08325},
	doi = {10.48550/arXiv.2002.08325},
	abstract = {Logical connectives and their implications on the meaning of a natural language sentence are a fundamental aspect of understanding. In this paper, we investigate whether visual question answering (VQA) systems trained to answer a question about an image, are able to answer the logical composition of multiple such questions. When put under this {\textbackslash}textit\{Lens of Logic\}, state-of-the-art VQA models have difficulty in correctly answering these logically composed questions. We construct an augmentation of the VQA dataset as a benchmark, with questions containing logical compositions and linguistic transformations (negation, disjunction, conjunction, and antonyms). We propose our \{Lens of Logic (LOL)\} model which uses question-attention and logic-attention to understand logical connectives in the question, and a novel Fr{\textbackslash}'echet-Compatibility Loss, which ensures that the answers of the component questions and the composed question are consistent with the inferred logical operation. Our model shows substantial improvement in learning logical compositions while retaining performance on VQA. We suggest this work as a move towards robustness by embedding logical connectives in visual understanding.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Gokhale, Tejas and Banerjee, Pratyay and Baral, Chitta and Yang, Yezhou},
	month = jul,
	year = {2020},
	note = {arXiv:2002.08325 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	annote = {Comment: Accepted to ECCV 2020},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/3S75GHTU/Gokhale et al. - 2020 - VQA-LOL Visual Question Answering under the Lens .pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/GQWU32EE/2002.html:text/html},
}

@misc{wang_few-shot_2023,
	title = {Few-{Shot} {Learning} with {Visual} {Distribution} {Calibration} and {Cross}-{Modal} {Distribution} {Alignment}},
	url = {http://arxiv.org/abs/2305.11439},
	doi = {10.48550/arXiv.2305.11439},
	abstract = {Pre-trained vision-language models have inspired much research on few-shot learning. However, with only a few training images, there exist two crucial problems: (1) the visual feature distributions are easily distracted by class-irrelevant information in images, and (2) the alignment between the visual and language feature distributions is difficult. To deal with the distraction problem, we propose a Selective Attack module, which consists of trainable adapters that generate spatial attention maps of images to guide the attacks on class-irrelevant image areas. By messing up these areas, the critical features are captured and the visual distributions of image features are calibrated. To better align the visual and language feature distributions that describe the same object class, we propose a cross-modal distribution alignment module, in which we introduce a vision-language prototype for each class to align the distributions, and adopt the Earth Mover's Distance (EMD) to optimize the prototypes. For efficient computation, the upper bound of EMD is derived. In addition, we propose an augmentation strategy to increase the diversity of the images and the text prompts, which can reduce overfitting to the few-shot training images. Extensive experiments on 11 datasets demonstrate that our method consistently outperforms prior arts in few-shot learning. The implementation code will be available at https://github.com/bhrqw/SADA.},
	urldate = {2024-06-27},
	publisher = {arXiv},
	author = {Wang, Runqi and Zheng, Hao and Duan, Xiaoyue and Liu, Jianzhuang and Lu, Yuning and Wang, Tian and Xu, Songcen and Zhang, Baochang},
	month = may,
	year = {2023},
	note = {arXiv:2305.11439 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/CDQUFVI6/Wang et al. - 2023 - Few-Shot Learning with Visual Distribution Calibra.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/WIZTIZAM/2305.html:text/html},
}

@misc{agrawal_vqa_2016,
	title = {{VQA}: {Visual} {Question} {Answering}},
	shorttitle = {{VQA}},
	url = {http://arxiv.org/abs/1505.00468},
	doi = {10.48550/arXiv.1505.00468},
	abstract = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing {\textasciitilde}0.25M images, {\textasciitilde}0.76M questions, and {\textasciitilde}10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa).},
	urldate = {2024-06-30},
	publisher = {arXiv},
	author = {Agrawal, Aishwarya and Lu, Jiasen and Antol, Stanislaw and Mitchell, Margaret and Zitnick, C. Lawrence and Batra, Dhruv and Parikh, Devi},
	month = oct,
	year = {2016},
	note = {arXiv:1505.00468 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	annote = {Comment: The first three authors contributed equally. International Conference on Computer Vision (ICCV) 2015},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/MDZN5NE4/Agrawal et al. - 2016 - VQA Visual Question Answering.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/V4AA2MJ7/1505.html:text/html},
}

@inproceedings{agarwal_towards_2020,
	address = {Seattle, WA, USA},
	title = {Towards {Causal} {VQA}: {Revealing} and {Reducing} {Spurious} {Correlations} by {Invariant} and {Covariant} {Semantic} {Editing}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72817-168-5},
	shorttitle = {Towards {Causal} {VQA}},
	url = {https://ieeexplore.ieee.org/document/9156407/},
	doi = {10.1109/CVPR42600.2020.00971},
	abstract = {Despite signiﬁcant success in Visual Question Answering (VQA), VQA models have been shown to be notoriously brittle to linguistic variations in the questions. Due to deﬁciencies in models and datasets, today’s models often rely on correlations rather than predictions that are causal w.r.t. relevant evidence. In this paper, we propose a novel way to analyze and measure the robustness of the state of the art models w.r.t semantic visual variations as well as propose ways to make models more robust against spurious correlations. Our method performs automated semantic image manipulations and tests for consistency in model predictions to quantify the model robustness as well as generate synthetic data to counter these problems. We perform our analysis on three diverse, state of the art VQA models and diverse question types with a particular focus on challenging counting questions. In addition, we show that models can be made signiﬁcantly more robust against inconsistent predictions using our edited data. Finally, we show that results also translate to real-world error cases of state of the art models, which results in improved overall performance.},
	language = {en},
	urldate = {2024-07-09},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Agarwal, Vedika and Shetty, Rakshith and Fritz, Mario},
	month = jun,
	year = {2020},
	pages = {9687--9695},
	file = {Agarwal_Towards_Causal_VQA_Revealing_and_Reducing_Spurious_Correlations_by_Invariant_CVPR_2020_paper.pdf:/Users/chengyuehuang/Zotero/storage/BWZDL9QB/Agarwal_Towards_Causal_VQA_Revealing_and_Reducing_Spurious_Correlations_by_Invariant_CVPR_2020_paper.pdf:application/pdf},
}

@misc{shah_cycle-consistency_2019,
	title = {Cycle-{Consistency} for {Robust} {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/1902.05660},
	doi = {10.48550/arXiv.1902.05660},
	abstract = {Despite significant progress in Visual Question Answering over the years, robustness of today's VQA models leave much to be desired. We introduce a new evaluation protocol and associated dataset (VQA-Rephrasings) and show that state-of-the-art VQA models are notoriously brittle to linguistic variations in questions. VQA-Rephrasings contains 3 human-provided rephrasings for 40k questions spanning 40k images from the VQA v2.0 validation dataset. As a step towards improving robustness of VQA models, we propose a model-agnostic framework that exploits cycle consistency. Specifically, we train a model to not only answer a question, but also generate a question conditioned on the answer, such that the answer predicted for the generated question is the same as the ground truth answer to the original question. Without the use of additional annotations, we show that our approach is significantly more robust to linguistic variations than state-of-the-art VQA models, when evaluated on the VQA-Rephrasings dataset. In addition, our approach outperforms state-of-the-art approaches on the standard VQA and Visual Question Generation tasks on the challenging VQA v2.0 dataset.},
	urldate = {2024-07-09},
	publisher = {arXiv},
	author = {Shah, Meet and Chen, Xinlei and Rohrbach, Marcus and Parikh, Devi},
	month = feb,
	year = {2019},
	note = {arXiv:1902.05660 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Technical Report},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/VY8DTYUU/Shah et al. - 2019 - Cycle-Consistency for Robust Visual Question Answe.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/X985BG4K/1902.html:text/html},
}

@misc{fort_exploring_2021,
	title = {Exploring the {Limits} of {Out}-of-{Distribution} {Detection}},
	url = {http://arxiv.org/abs/2106.03004},
	doi = {10.48550/arXiv.2106.03004},
	abstract = {Near out-of-distribution detection (OOD) is a major challenge for deep neural networks. We demonstrate that large-scale pre-trained transformers can significantly improve the state-of-the-art (SOTA) on a range of near OOD tasks across different data modalities. For instance, on CIFAR-100 vs CIFAR-10 OOD detection, we improve the AUROC from 85\% (current SOTA) to more than 96\% using Vision Transformers pre-trained on ImageNet-21k. On a challenging genomics OOD detection benchmark, we improve the AUROC from 66\% to 77\% using transformers and unsupervised pre-training. To further improve performance, we explore the few-shot outlier exposure setting where a few examples from outlier classes may be available; we show that pre-trained transformers are particularly well-suited for outlier exposure, and that the AUROC of OOD detection on CIFAR-100 vs CIFAR-10 can be improved to 98.7\% with just 1 image per OOD class, and 99.46\% with 10 images per OOD class. For multi-modal image-text pre-trained transformers such as CLIP, we explore a new way of using just the names of outlier classes as a sole source of information without any accompanying images, and show that this outperforms previous SOTA on standard vision OOD benchmark tasks.},
	urldate = {2024-07-11},
	publisher = {arXiv},
	author = {Fort, Stanislav and Ren, Jie and Lakshminarayanan, Balaji},
	month = jul,
	year = {2021},
	note = {arXiv:2106.03004 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: S.F. and J.R. contributed equally},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/RPPQID9F/Fort et al. - 2021 - Exploring the Limits of Out-of-Distribution Detect.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/R4VRGNFY/2106.html:text/html},
}

@misc{goyal_making_2017,
	title = {Making the {V} in {VQA} {Matter}: {Elevating} the {Role} of {Image} {Understanding} in {Visual} {Question} {Answering}},
	shorttitle = {Making the {V} in {VQA} {Matter}},
	url = {http://arxiv.org/abs/1612.00837},
	doi = {10.48550/arXiv.1612.00837},
	abstract = {Problems at the intersection of vision and language are of significant importance both as challenging research questions and for the rich set of applications they enable. However, inherent structure in our world and bias in our language tend to be a simpler signal for learning than visual modalities, resulting in models that ignore visual information, leading to an inflated sense of their capability. We propose to counter these language priors for the task of Visual Question Answering (VQA) and make vision (the V in VQA) matter! Specifically, we balance the popular VQA dataset by collecting complementary images such that every question in our balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. Our dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs. Our complete balanced dataset is publicly available at www.visualqa.org as part of the 2nd iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0). We further benchmark a number of state-of-art VQA models on our balanced dataset. All models perform significantly worse on our balanced dataset, suggesting that these models have indeed learned to exploit language priors. This finding provides the first concrete empirical evidence for what seems to be a qualitative sense among practitioners. Finally, our data collection protocol for identifying complementary images enables us to develop a novel interpretable model, which in addition to providing an answer to the given (image, question) pair, also provides a counter-example based explanation. Specifically, it identifies an image that is similar to the original image, but it believes has a different answer to the same question. This can help in building trust for machines among their users.},
	urldate = {2024-07-29},
	publisher = {arXiv},
	author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
	month = may,
	year = {2017},
	note = {arXiv:1612.00837 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/5XCS5ITT/Goyal et al. - 2017 - Making the V in VQA Matter Elevating the Role of .pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/7A8MG5Y3/1612.html:text/html},
}

@misc{sheng_human-adversarial_2021,
	title = {Human-{Adversarial} {Visual} {Question} {Answering}},
	url = {http://arxiv.org/abs/2106.02280},
	doi = {10.48550/arXiv.2106.02280},
	abstract = {Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved. In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to find a question where the model's predicted answer is incorrect. We find that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the field and advance the state of the art.},
	urldate = {2024-07-29},
	publisher = {arXiv},
	author = {Sheng, Sasha and Singh, Amanpreet and Goswami, Vedanuj and Magana, Jose Alberto Lopez and Galuba, Wojciech and Parikh, Devi and Kiela, Douwe},
	month = jun,
	year = {2021},
	note = {arXiv:2106.02280 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 22 pages, 13 figures. First two authors contributed equally},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/PZ2JMCRX/Sheng et al. - 2021 - Human-Adversarial Visual Question Answering.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/TWG5G55E/2106.html:text/html},
}

@misc{li_adversarial_2021,
	title = {Adversarial {VQA}: {A} {New} {Benchmark} for {Evaluating} the {Robustness} of {VQA} {Models}},
	shorttitle = {Adversarial {VQA}},
	url = {http://arxiv.org/abs/2106.00245},
	doi = {10.48550/arXiv.2106.00245},
	abstract = {Benefiting from large-scale pre-training, we have witnessed significant performance boost on the popular Visual Question Answering (VQA) task. Despite rapid progress, it remains unclear whether these state-of-the-art (SOTA) models are robust when encountering examples in the wild. To study this, we introduce Adversarial VQA, a new large-scale VQA benchmark, collected iteratively via an adversarial human-and-model-in-the-loop procedure. Through this new benchmark, we discover several interesting findings. (i) Surprisingly, we find that during dataset collection, non-expert annotators can easily attack SOTA VQA models successfully. (ii) Both large-scale pre-trained models and adversarial training methods achieve far worse performance on the new benchmark than over standard VQA v2 dataset, revealing the fragility of these models while demonstrating the effectiveness of our adversarial dataset. (iii) When used for data augmentation, our dataset can effectively boost model performance on other robust VQA benchmarks. We hope our Adversarial VQA dataset can shed new light on robustness study in the community and serve as a valuable benchmark for future work.},
	urldate = {2024-07-29},
	publisher = {arXiv},
	author = {Li, Linjie and Lei, Jie and Gan, Zhe and Liu, Jingjing},
	month = aug,
	year = {2021},
	note = {arXiv:2106.00245 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: To appear in ICCV 2021; Website: https://adversarialvqa.github.io/},
	file = {arXiv Fulltext PDF:/Users/chengyuehuang/Zotero/storage/X3Y9H94T/Li et al. - 2021 - Adversarial VQA A New Benchmark for Evaluating th.pdf:application/pdf;arXiv.org Snapshot:/Users/chengyuehuang/Zotero/storage/E84X7XKE/2106.html:text/html},
}

@article{bigham_vizwiz_nodate,
	title = {{VizWiz}: nearly real-time answers to visual questions},
	abstract = {The lack of access to visual information like text labels, icons, and colors can cause frustration and decrease independence for blind people. Current access technology uses automatic approaches to address some problems in this space, but the technology is error-prone, limited in scope, and quite expensive. In this paper, we introduce VizWiz, a talking application for mobile phones that offers a new alternative to answering visual questions in nearly real-time—asking multiple people on the web. To support answering questions quickly, we introduce a general approach for intelligently recruiting human workers in advance called quikTurkit so that workers are available when new questions arrive. A ﬁeld deployment with 11 blind participants illustrates that blind people can effectively use VizWiz to cheaply answer questions in their everyday lives, highlighting issues that automatic approaches will need to address to be useful. Finally, we illustrate the potential of using VizWiz as part of the participatory design of advanced tools by using it to build and evaluate VizWiz::LocateIt, an interactive mobile tool that helps blind people solve general visual search problems.},
	language = {en},
	author = {Bigham, Jeffrey P and Jayant, Chandrika and Ji, Hanjie and Little, Greg and Miller, Andrew and Miller, Robert C and Miller, Robin and Tatarowicz, Aubrey and White, Brandyn and White, Samual and Yeh, Tom},
	file = {vizwiz.pdf:/Users/chengyuehuang/Zotero/storage/JKWL5VUL/vizwiz.pdf:application/pdf},
}

%code base
@misc{li2022lavislibrarylanguagevisionintelligence,
      title={LAVIS: A Library for Language-Vision Intelligence}, 
      author={Dongxu Li and Junnan Li and Hung Le and Guangsen Wang and Silvio Savarese and Steven C. H. Hoi},
      year={2022},
      eprint={2209.09019},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2209.09019}, 
}