\begin{thebibliography}{77}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghajanyan et~al.(2021)Aghajanyan, Zettlemoyer, and
  Gupta]{aghajanyan2020intrinsic}
Aghajanyan, A., Zettlemoyer, L., and Gupta, S.
\newblock Intrinsic dimensionality explains the effectiveness of language model
  fine-tuning.
\newblock \emph{Association for Computational Linguistics}, 2021.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li, and
  Song]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock \emph{International Conference on Machine Learning},
  2019{\natexlab{a}}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li, and
  Song]{allen2019convergence2}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock On the convergence rate of training recurrent neural networks.
\newblock \emph{Neural Information Processing Systems}, 2019{\natexlab{b}}.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R.~R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock \emph{Neural Information Processing Systems}, 2019.

\bibitem[Bach(2023)]{bach2021learning}
Bach, F.
\newblock \emph{Learning Theory from First Principles}.
\newblock Draft, 2023.

\bibitem[Bach et~al.(2008)Bach, Mairal, and Ponce]{bach2008matrix}
Bach, F., Mairal, J., and Ponce, J.
\newblock Convex sparse matrix factorizations.
\newblock \emph{arXiv preprint arXiv:0812.1869}, 2008.

\bibitem[Baevski et~al.(2020)Baevski, Zhou, Mohamed, and
  Auli]{baevski2020wav2vec}
Baevski, A., Zhou, Y., Mohamed, A., and Auli, M.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech
  representations.
\newblock \emph{Neural Information Processing Systems}, 2020.

\bibitem[Barron(1993)]{barron1993universal}
Barron, A.~R.
\newblock Universal approximation bounds for superpositions of a sigmoidal
  function.
\newblock \emph{IEEE Transactions on Information theory}, 39\penalty0
  (3):\penalty0 930--945, 1993.

\bibitem[Bartlett \& Mendelson(2002)Bartlett and
  Mendelson]{bartlett2002Rademacher}
Bartlett, P.~L. and Mendelson, S.
\newblock Rademacher and gaussian complexities: risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3:\penalty0 463--482,
  2002.

\bibitem[Bartlett et~al.(2002)Bartlett, Boucheron, and
  Lugosi]{bartlett2002model}
Bartlett, P.~L., Boucheron, S., and Lugosi, G.
\newblock Model selection and error estimation.
\newblock \emph{Machine Learning}, 48:\penalty0 85--113, 2002.

\bibitem[Bartlett et~al.(2005)Bartlett, Bousquet, and
  Mendelson]{bartlett2005local}
Bartlett, P.~L., Bousquet, O., and Mendelson, S.
\newblock Local rademacher complexities.
\newblock \emph{The Annals of Statistics}, 33\penalty0 (4):\penalty0
  1497--1537, 2005.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M.~J.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock \emph{Neural Information Processing Systems}, 2017.

\bibitem[Barvinok(1995)]{barvinok1995problems}
Barvinok, A.~I.
\newblock Problems of distance geometry and convex properties of quadratic
  maps.
\newblock \emph{Discrete \& Computational Geometry}, 13:\penalty0 189--202,
  1995.

\bibitem[Bengio \& Delalleau(2011)Bengio and Delalleau]{bengio2011expressive}
Bengio, Y. and Delalleau, O.
\newblock On the expressive power of deep architectures.
\newblock \emph{Algorithmic Learning Theory}, 2011.

\bibitem[Bhojanapalli et~al.(2018)Bhojanapalli, Boumal, Jain, and
  Netrapalli]{bhojanapalli2018smoothed}
Bhojanapalli, S., Boumal, N., Jain, P., and Netrapalli, P.
\newblock Smoothed analysis for low-rank solutions to semidefinite programs in
  quadratic penalty form.
\newblock \emph{Conference On Learning Theory}, 2018.

\bibitem[Boix-Adsera et~al.(2023)Boix-Adsera, Littwin, Abbe, Bengio, and
  Susskind]{boix2023transformers}
Boix-Adsera, E., Littwin, E., Abbe, E., Bengio, S., and Susskind, J.
\newblock Transformers learn through gradual rank increase.
\newblock \emph{Neural Information Processing Systems}, 2023.

\bibitem[Boumal et~al.(2016)Boumal, Voroninski, and Bandeira]{boumal2016non}
Boumal, N., Voroninski, V., and Bandeira, A.
\newblock The non-convex {B}urer--{M}onteiro approach works on smooth
  semidefinite programs.
\newblock \emph{Neural Information Processing Systems}, 29, 2016.

\bibitem[Bousquet \& Elisseeff(2002)Bousquet and
  Elisseeff]{bousquet2002stability}
Bousquet, O. and Elisseeff, A.
\newblock Stability and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 2:\penalty0
  499--526, 2002.

\bibitem[Burer \& Monteiro(2003)Burer and Monteiro]{burer2003nonlinear}
Burer, S. and Monteiro, R.~D.
\newblock A nonlinear programming algorithm for solving semidefinite programs
  via low-rank factorization.
\newblock \emph{Mathematical Programming}, 95\penalty0 (2):\penalty0 329--357,
  2003.

\bibitem[Cabral et~al.(2013)Cabral, De~la Torre, Costeira, and
  Bernardino]{cabral2013unifying}
Cabral, R., De~la Torre, F., Costeira, J.~P., and Bernardino, A.
\newblock Unifying nuclear norm and bilinear factorization approaches for
  low-rank matrix decomposition.
\newblock \emph{International Conference on Computer Vision}, 2013.

\bibitem[Chen et~al.(2020)Chen, Cao, Gu, and Zhang]{chen2020generalized}
Chen, Z., Cao, Y., Gu, Q., and Zhang, T.
\newblock A generalized neural tangent kernel analysis for two-layer neural
  networks.
\newblock \emph{Neural Information Processing Systems}, 2020.

\bibitem[Choi et~al.(2023)Choi, Park, Park, Cho, No, and Ryu]{choilora}
Choi, J.~Y., Park, J., Park, I., Cho, J., No, A., and Ryu, E.~K.
\newblock Lo{RA} can replace time and class embeddings in diffusion
  probabilistic models.
\newblock \emph{NeurIPS 2023 Workshop on Diffusion Models}, 2023.

\bibitem[Cybenko(1989)]{cybenko1989approximation}
Cybenko, G.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of Control, Signals and Systems}, 2\penalty0
  (4):\penalty0 303--314, 1989.

\bibitem[Delalleau \& Bengio(2011)Delalleau and Bengio]{delalleau2011shallow}
Delalleau, O. and Bengio, Y.
\newblock Shallow vs. deep sum-product networks.
\newblock \emph{Neural Information Processing Systems}, 2011.

\bibitem[Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and
  Zettlemoyer]{dettmers2023qlora}
Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.
\newblock Q{L}o{RA}: efficient finetuning of quantized llms.
\newblock \emph{Neural Information Processing Systems}, 2023.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
\newblock Sharp minima can generalize for deep nets.
\newblock \emph{International Conference on Machine Learning}, 2017.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Du \& Lee(2018)Du and Lee]{du2018power}
Du, S. and Lee, J.
\newblock On the power of over-parametrization in neural networks with
  quadratic activation.
\newblock \emph{International Conference on Machine Learning}, 2018.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock \emph{International Conference on Machine Learning}, 2019.

\bibitem[Du et~al.(2017)Du, Jin, Lee, Jordan, Singh, and
  Poczos]{du2017gradient}
Du, S.~S., Jin, C., Lee, J.~D., Jordan, M.~I., Singh, A., and Poczos, B.
\newblock Gradient descent can take exponential time to escape saddle points.
\newblock \emph{Neural Information Processing Systems}, 2017.

\bibitem[Duan et~al.(2023)Duan, Ji, Cai, et~al.]{duan2023minimum}
Duan, Y., Ji, G., Cai, Y., et~al.
\newblock Minimum width of leaky-relu neural networks for uniform universal
  approximation.
\newblock \emph{International Conference on Machine Learning}, 2023.

\bibitem[Fu et~al.(2023)Fu, Yang, So, Lam, Bing, and
  Collier]{fu2023effectiveness}
Fu, Z., Yang, H., So, A. M.-C., Lam, W., Bing, L., and Collier, N.
\newblock On the effectiveness of parameter-efficient fine-tuning.
\newblock \emph{AAAI Conference on Artificial Intelligence}, 2023.

\bibitem[Gao et~al.(2021)Gao, Fisch, and Chen]{gao2020making}
Gao, T., Fisch, A., and Chen, D.
\newblock Making pre-trained language models better few-shot learners.
\newblock \emph{Association for Computational Linguistics}, 2021.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{ge2015escaping}
Ge, R., Huang, F., Jin, C., and Yuan, Y.
\newblock Escaping from saddle pointsâ€”online stochastic gradient for tensor
  decomposition.
\newblock \emph{Conference on Learning Theory}, 2015.

\bibitem[Ge et~al.(2016)Ge, Lee, and Ma]{ge2016matrix}
Ge, R., Lee, J.~D., and Ma, T.
\newblock Matrix completion has no spurious local minimum.
\newblock \emph{Neural Information Processing Systems}, 2016.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Giannou et~al.(2023)Giannou, Rajput, Sohn, Lee, Lee, and
  Papailiopoulos]{giannou2023looped}
Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J.~D., and Papailiopoulos,
  D.
\newblock Looped transformers as programmable computers.
\newblock \emph{International Conference on Machine Learning}, 2023.

\bibitem[Haeffele et~al.(2014)Haeffele, Young, and Vidal]{haeffele14lowrank}
Haeffele, B., Young, E., and Vidal, R.
\newblock Structured low-rank matrix factorization: optimality, algorithm, and
  applications to image processing.
\newblock \emph{International Conference on Machine Learning}, 2014.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
Hardt, M., Recht, B., and Singer, Y.
\newblock Train faster, generalize better: stability of stochastic gradient
  descent.
\newblock \emph{International Conference on Machine Learning}, 2016.

\bibitem[Helmke \& Shayman(1995)Helmke and Shayman]{helmke1995critical}
Helmke, U. and Shayman, M.~A.
\newblock Critical points of matrix least squares distance functions.
\newblock \emph{Linear Algebra and its Applications}, 215:\penalty0 1--19,
  1995.

\bibitem[Hornik et~al.(1990)Hornik, Stinchcombe, and
  White]{hornik1990universal}
Hornik, K., Stinchcombe, M., and White, H.
\newblock Universal approximation of an unknown mapping and its derivatives
  using multilayer feedforward networks.
\newblock \emph{Neural Networks}, 3\penalty0 (5):\penalty0 551--560, 1990.

\bibitem[Hu et~al.(2021)Hu, Wallis, Allen-Zhu, Li, Wang, Wang, Chen,
  et~al.]{hu2021lora}
Hu, E.~J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W.,
  et~al.
\newblock Lo{RA}: low-rank adaptation of large language models.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: convergence and generalization in neural
  networks.
\newblock \emph{Neural Information Processing Systems}, 2018.

\bibitem[Jin et~al.(2017)Jin, Ge, Netrapalli, Kakade, and
  Jordan]{jin2017escape}
Jin, C., Ge, R., Netrapalli, P., Kakade, S.~M., and Jordan, M.~I.
\newblock How to escape saddle points efficiently.
\newblock \emph{International Conference on Machine Learning}, 2017.

\bibitem[Jin et~al.(2023)Jin, Li, Lyu, Du, and Lee]{jin2023understanding}
Jin, J., Li, Z., Lyu, K., Du, S.~S., and Lee, J.~D.
\newblock Understanding incremental learning of gradient descent: A
  fine-grained analysis of matrix sensing.
\newblock \emph{International Conference on Machine Learning}, 2023.

\bibitem[Johnson \& Lindenstrauss(1984)Johnson and
  Lindenstrauss]{johnson1984extension}
Johnson, W. and Lindenstrauss, J.
\newblock Extensions of lipschitz maps into a hilbert space.
\newblock \emph{Contemporary Mathematics}, 26:\penalty0 189--206, 1984.

\bibitem[Koltchinskii \& Panchenko(2000)Koltchinskii and
  Panchenko]{koltchinskii2000rademacher}
Koltchinskii, V. and Panchenko, D.
\newblock Rademacher processes and bounding the risk of function learning.
\newblock In Gin{\'e}, E., Mason, D.~M., and Wellner, J.~A. (eds.), \emph{High
  Dimensional Probability II}, pp.\  443--457. Springer, 2000.

\bibitem[Lee et~al.(2016)Lee, Simchowitz, Jordan, and Recht]{lee2016gradient}
Lee, J.~D., Simchowitz, M., Jordan, M.~I., and Recht, B.
\newblock Gradient descent only converges to minimizers.
\newblock \emph{Conference on Learning Theory}, 2016.

\bibitem[Lialin et~al.(2023)Lialin, Muckatira, Shivagunde, and
  Rumshisky]{lialin2023relora}
Lialin, V., Muckatira, S., Shivagunde, N., and Rumshisky, A.
\newblock Re{L}o{RA}: high-rank training through low-rank updates.
\newblock \emph{Workshop on Advancing Neural Network Training (WANT):
  Computational Efficiency, Scalability, and Resource Optimization}, 2023.

\bibitem[Liu et~al.(2020)Liu, Liu, Gao, Chen, and Han]{liu2020understanding}
Liu, L., Liu, X., Gao, J., Chen, W., and Han, J.
\newblock Understanding the difficulty of training transformers.
\newblock \emph{Empirical Methods in Natural Language Processing}, 2020.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Ro{BERT}a: a robustly optimized {BERT} pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Lu et~al.(2017)Lu, Pu, Wang, Hu, and Wang]{lu2017expressive}
Lu, Z., Pu, H., Wang, F., Hu, Z., and Wang, L.
\newblock The expressive power of neural networks: a view from the width.
\newblock \emph{Neural Information Processing Systems}, 2017.

\bibitem[{Makerere AI Lab}(2020)]{beansdata}
{Makerere AI Lab}.
\newblock Bean disease dataset, 2020.
\newblock URL \url{https://github.com/AI-Lab-Makerere/ibean/}.

\bibitem[Malladi et~al.(2023)Malladi, Wettig, Yu, Chen, and
  Arora]{malladi2023kernel}
Malladi, S., Wettig, A., Yu, D., Chen, D., and Arora, S.
\newblock A kernel-based view of language model fine-tuning.
\newblock \emph{International Conference on Machine Learning}, 2023.

\bibitem[Maurer(2016)]{maurer2016vector}
Maurer, A.
\newblock A vector-contraction inequality for rademacher complexities.
\newblock \emph{Algorithmic Learning Theory}, 2016.

\bibitem[McDiarmid et~al.(1989)]{mcdiarmid1989method}
McDiarmid, C. et~al.
\newblock On the method of bounded differences.
\newblock \emph{Surveys in Combinatorics}, 141\penalty0 (1):\penalty0 148--188,
  1989.

\bibitem[Pataki(1998)]{pataki1998}
Pataki, G.
\newblock On the rank of extreme matrices in semidefinite programs and the
  multiplicity of optimal eigenvalues.
\newblock \emph{Mathematics of Operations Research}, 23\penalty0 (2):\penalty0
  339--358, 1998.

\bibitem[Pataki(2000)]{pataki2000geometry}
Pataki, G.
\newblock The geometry of semidefinite programming.
\newblock In Wolkowicz, H., Saigal, R., and Vandenberghe, L. (eds.),
  \emph{Handbook of Semidefinite Programming: Theory, Algorithms, and
  Applications}, pp.\  29--65. Springer, 2000.

\bibitem[Pilanci \& Ergen(2020)Pilanci and Ergen]{pilanci2020neural}
Pilanci, M. and Ergen, T.
\newblock Neural networks are convex regularizers: exact polynomial-time convex
  optimization formulations for two-layer networks.
\newblock \emph{International Conference on Machine Learning}, 2020.

\bibitem[Polyak(1987)]{polyak1987introduction}
Polyak, B.~T.
\newblock \emph{Introduction to Optimization}.
\newblock New York, Optimization Software, 1987.

\bibitem[Recht et~al.(2010)Recht, Fazel, and Parrilo]{recht2010guaranteed}
Recht, B., Fazel, M., and Parrilo, P.~A.
\newblock Guaranteed minimum-rank solutions of linear matrix equations via
  nuclear norm minimization.
\newblock \emph{SIAM review}, 52\penalty0 (3):\penalty0 471--501, 2010.

\bibitem[Ryu(2023)]{ryu2023low}
Ryu, S.
\newblock Low-rank adaptation for fast text-to-image diffusion fine-tuning,
  2023.
\newblock URL \url{https://github.com/cloneofsimo/lora}.

\bibitem[Schick \& Sch{\"u}tze(2021)Schick and
  Sch{\"u}tze]{schick2020exploiting}
Schick, T. and Sch{\"u}tze, H.
\newblock Exploiting cloze questions for few shot text classification and
  natural language inference.
\newblock \emph{Association for Computational Linguistics}, 2021.

\bibitem[Smith et~al.(2023)Smith, Hsu, Zhang, Hua, Kira, Shen, and
  Jin]{smith2023continual}
Smith, J.~S., Hsu, Y.-C., Zhang, L., Hua, T., Kira, Z., Shen, Y., and Jin, H.
\newblock Continual diffusion: continual customization of text-to-image
  diffusion with c-lora.
\newblock \emph{arXiv preprint arXiv:2304.06027}, 2023.

\bibitem[Soltanolkotabi et~al.(2018)Soltanolkotabi, Javanmard, and
  Lee]{soltanolkotabi2018theoretical}
Soltanolkotabi, M., Javanmard, A., and Lee, J.~D.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock \emph{IEEE Transactions on Information Theory}, 65\penalty0
  (2):\penalty0 742--769, 2018.

\bibitem[Sridharan et~al.(2008)Sridharan, Shalev-Shwartz, and
  Srebro]{sridharan2008fast}
Sridharan, K., Shalev-Shwartz, S., and Srebro, N.
\newblock Fast rates for regularized objectives.
\newblock \emph{Neural Information Processing Systems}, 21, 2008.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Neural Information Processing Systems}, 2017.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Hu, and Steinhardt]{wei2022more}
Wei, A., Hu, W., and Steinhardt, J.
\newblock More than a toy: random matrix models predict how real-world neural
  representations generalize.
\newblock \emph{International Conference on Machine Learning},
  2022{\natexlab{a}}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Chen, and Ma]{wei2022statistically}
Wei, C., Chen, Y., and Ma, T.
\newblock Statistically meaningful approximation: a case study on approximating
  turing machines with transformers.
\newblock \emph{Neural Information Processing Systems}, 2022{\natexlab{b}}.

\bibitem[Wu et~al.(2017)Wu, Zhu, et~al.]{wu2017towards}
Wu, L., Zhu, Z., et~al.
\newblock Towards understanding generalization of deep learning: perspective of
  loss landscapes.
\newblock \emph{arXiv preprint arXiv:1706.10239}, 2017.

\bibitem[Yang et~al.(2021)Yang, Chi, Chuang, Lai, Lakhotia, Lin, Liu, Shi,
  Chang, Lin, et~al.]{yang2021superb}
Yang, S.-w., Chi, P.-H., Chuang, Y.-S., Lai, C.-I.~J., Lakhotia, K., Lin,
  Y.~Y., Liu, A.~T., Shi, J., Chang, X., Lin, G.-T., et~al.
\newblock Superb: Speech processing universal performance benchmark.
\newblock \emph{Interspeech}, 2021.

\bibitem[Yeh et~al.(2024)Yeh, Hsieh, Gao, Yang, Oh, and
  Gong]{yeh2023navigating}
Yeh, S.-Y., Hsieh, Y.-G., Gao, Z., Yang, B.~B., Oh, G., and Gong, Y.
\newblock Navigating text-to-image customization: from {L}y{CORIS} fine-tuning
  to model evaluation.
\newblock \emph{International Conference on Learning Representations}, 2024.

\bibitem[Yun et~al.(2019)Yun, Bhojanapalli, Rawat, Reddi, and
  Kumar]{yun2019transformers}
Yun, C., Bhojanapalli, S., Rawat, A.~S., Reddi, S.~J., and Kumar, S.
\newblock Are transformers universal approximators of sequence-to-sequence
  functions?
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zeng \& Lee(2024)Zeng and Lee]{zeng2023expressive}
Zeng, Y. and Lee, K.
\newblock The expressive power of low-rank adaptation.
\newblock \emph{International Conference on Learning Representations}, 2024.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021.

\bibitem[Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and
  Sra]{zhang2020adaptive}
Zhang, J., Karimireddy, S.~P., Veit, A., Kim, S., Reddi, S., Kumar, S., and
  Sra, S.
\newblock Why are adaptive methods good for attention models?
\newblock \emph{Neural Information Processing Systems}, 2020.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
Zou, D., Cao, Y., Zhou, D., and Gu, Q.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock \emph{Machine Learning}, 109\penalty0 (3):\penalty0 467--492, 2020.

\end{thebibliography}
