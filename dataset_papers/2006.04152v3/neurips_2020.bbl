\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{{NAACL-HLT}}, 2019.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime~G. Carbonell, Ruslan Salakhutdinov,
  and Quoc~V. Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{lan2019albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock {ALBERT:} {A} lite {BERT} for self-supervised learning of language
  representations.
\newblock In \emph{{ICLR}}, 2020.

\bibitem[Kaya et~al.(2019)Kaya, Hong, and Dumitras]{kaya2018shallow}
Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras.
\newblock Shallow-deep networks: Understanding and mitigating network
  overthinking.
\newblock In \emph{{ICML}}, 2019.

\bibitem[Jin et~al.(2020)Jin, Jin, Zhou, and Szolovits]{jin2019bert}
Di~Jin, Zhijing Jin, Joey~Tianyi Zhou, and Peter Szolovits.
\newblock Is {BERT} really robust? natural language attack on text
  classification and entailment.
\newblock In \emph{{AAAI}}, 2020.

\bibitem[Morgan and Bourlard(1989)]{morgan1990generalization}
Nelson Morgan and Herv{\'{e}} Bourlard.
\newblock Generalization and parameter estimation in feedforward netws: Some
  experiments.
\newblock In \emph{{NeurIPS}}, 1989.

\bibitem[Prechelt(1998)]{prechelt1998early}
Lutz Prechelt.
\newblock Early stopping-but when?
\newblock In \emph{Neural Networks: Tricks of the trade}, pages 55--69.
  Springer, 1998.

\bibitem[Cai et~al.(2020)Cai, Gan, Wang, Zhang, and Han]{Cai2020Once-for-All}
Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han.
\newblock Once-for-all: Train one network and specialize it for efficient
  deployment.
\newblock In \emph{{ICLR}}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{{CVPR}}, 2016.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{howard2017mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Zhou, Lin, and Sun]{zhang2018shufflenet}
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
\newblock In \emph{{CVPR}}, 2018.

\bibitem[Tan and Le(2019)]{tan2019efficientnet}
Mingxing Tan and Quoc~V. Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{{ICML}}, Proceedings of Machine Learning Research, 2019.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015.

\bibitem[Wu et~al.(2016)Wu, Leng, Wang, Hu, and Cheng]{wu2016quantized}
Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng.
\newblock Quantized convolutional neural networks for mobile devices.
\newblock In \emph{{CVPR}}, 2016.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[Sun et~al.(2019)Sun, Cheng, Gan, and Liu]{sun2019patient}
Siqi Sun, Yu~Cheng, Zhe Gan, and Jingjing Liu.
\newblock Patient knowledge distillation for {BERT} model compression.
\newblock In \emph{{EMNLP-IJCNLP}}, 2019.

\bibitem[Jiao et~al.(2019)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu]{jiao2019tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu.
\newblock Tinybert: Distilling bert for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1909.10351}, 2019.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{michel2019sixteen}
Paul Michel, Omer Levy, and Graham Neubig.
\newblock Are sixteen heads really better than one?
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and
  Titov]{voita2019analyzing}
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy
  lifting, the rest can be pruned.
\newblock In \emph{{ACL}}, 2019.

\bibitem[Fan et~al.(2020)Fan, Grave, and Joulin]{fan2019reducing}
Angela Fan, Edouard Grave, and Armand Joulin.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock In \emph{{ICLR}}, 2020.

\bibitem[Xu et~al.(2020)Xu, Zhou, Ge, Wei, and Zhou]{xu2020bert}
Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou.
\newblock Bert-of-theseus: Compressing bert by progressive module replacing.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Graves(2016)]{graves2016adaptive}
Alex Graves.
\newblock Adaptive computation time for recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1603.08983}, 2016.

\bibitem[Dehghani et~al.(2019)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{ut}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz
  Kaiser.
\newblock Universal transformers.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Teerapittayanon et~al.(2016)Teerapittayanon, McDanel, and
  Kung]{teerapittayanon2016branchynet}
Surat Teerapittayanon, Bradley McDanel, and H.~T. Kung.
\newblock Branchynet: Fast inference via early exiting from deep neural
  networks.
\newblock In \emph{{ICPR}}, 2016.

\bibitem[Hu et~al.(2020)Hu, Chen, Wang, and Wang]{hu2020triple}
Ting{-}Kuei Hu, Tianlong Chen, Haotao Wang, and Zhangyang Wang.
\newblock Triple wins: Boosting accuracy, robustness and efficiency together by
  enabling input-adaptive inference.
\newblock In \emph{{ICLR}}, 2020.

\bibitem[Wang et~al.(2018)Wang, Yu, Dou, Darrell, and
  Gonzalez]{wang2018skipnet}
Xin Wang, Fisher Yu, Zi{-}Yi Dou, Trevor Darrell, and Joseph~E. Gonzalez.
\newblock Skipnet: Learning dynamic routing in convolutional networks.
\newblock In \emph{{ECCV}}, 2018.

\bibitem[Liu et~al.(2020)Liu, Zhou, Zhao, Wang, Deng, and Ju]{liu2020fastbert}
Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Haotang Deng, and Qi~Ju.
\newblock Fastbert: a self-distilling bert with adaptive inference time.
\newblock \emph{arXiv preprint arXiv:2004.02178}, 2020.

\bibitem[Xin et~al.(2020)Xin, Tang, Lee, Yu, and Lin]{xin2020deebert}
Ji~Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin.
\newblock Deebert: Dynamic early exiting for accelerating bert inference.
\newblock \emph{arXiv preprint arXiv:2004.12993}, 2020.

\bibitem[Schwartz et~al.(2020)Schwartz, Stanovsky, Swayamdipta, Dodge, and
  Smith]{Schwartz:2020}
Roy Schwartz, Gabi Stanovsky, Swabha Swayamdipta, Jesse Dodge, and Noah~A.
  Smith.
\newblock The right tool for the job: Matching model and instance complexities.
\newblock In \emph{{ACL}}, 2020.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian~J. Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{{ICLR}}, 2014.

\bibitem[Jiang et~al.(2018)Jiang, Kim, Guan, and Gupta]{jiang2018trust}
Heinrich Jiang, Been Kim, Melody Guan, and Maya Gupta.
\newblock To trust or not to trust a classifier.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Krogh and Vedelsby(1994)]{krogh1994ensemble}
Anders Krogh and Jesper Vedelsby.
\newblock Neural network ensembles, cross validation, and active learning.
\newblock In \emph{{NeurIPS}}, 1994.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and Bowman]{glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman.
\newblock {GLUE:} {A} multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Dolan and Brockett(2005)]{mrpc}
William~B. Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{IWP@IJCNLP}, 2005.

\bibitem[Conneau and Kiela(2018)]{senteval}
Alexis Conneau and Douwe Kiela.
\newblock Senteval: An evaluation toolkit for universal sentence
  representations.
\newblock In \emph{{LREC}}, 2018.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{sst}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning,
  Andrew~Y. Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{{EMNLP}}, 2013.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{mnli}
Adina Williams, Nikita Nangia, and Samuel~R. Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{{NAACL-HLT}}, 2018.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{qnli}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100, 000+ questions for machine comprehension of text.
\newblock In \emph{{EMNLP}}, 2016.

\bibitem[Warstadt et~al.(2019)Warstadt, Singh, and Bowman]{cola}
Alex Warstadt, Amanpreet Singh, and Samuel~R. Bowman.
\newblock Neural network acceptability judgments.
\newblock \emph{{TACL}}, 2019.

\bibitem[Levesque(2011)]{wnli}
Hector~J. Levesque.
\newblock The winograd schema challenge.
\newblock In \emph{{AAAI} Spring Symposium: Logical Formalizations of
  Commonsense Reasoning}, 2011.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf2020huggingfaces}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}, 2020.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  gpu model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Kurakin et~al.(2017)Kurakin, Goodfellow, and
  Bengio]{kurakin2017adversarial}
Alexey Kurakin, Ian~J. Goodfellow, and Samy Bengio.
\newblock Adversarial examples in the physical world.
\newblock In \emph{{ICLR} (Workshop)}, 2017.

\bibitem[Bowman et~al.(2015)Bowman, Angeli, Potts, and Manning]{snli}
Samuel~R. Bowman, Gabor Angeli, Christopher Potts, and Christopher~D. Manning.
\newblock A large annotated corpus for learning natural language inference.
\newblock In \emph{{EMNLP}}, 2015.

\bibitem[Zhang et~al.(2015)Zhang, Zhao, and LeCun]{yelp}
Xiang Zhang, Junbo~Jake Zhao, and Yann LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock In \emph{{NeurIPS}}, 2015.

\bibitem[Strauss et~al.(2017)Strauss, Hanselmann, Junginger, and
  Ulmer]{strauss2017ensemble}
Thilo Strauss, Markus Hanselmann, Andrej Junginger, and Holger Ulmer.
\newblock Ensemble methods as a defense to adversarial perturbations against
  deep neural networks.
\newblock \emph{arXiv preprint arXiv:1709.03423}, 2017.

\bibitem[Tram{\`{e}}r et~al.(2018)Tram{\`{e}}r, Kurakin, Papernot, Goodfellow,
  Boneh, and McDaniel]{tramer2018ensemble}
Florian Tram{\`{e}}r, Alexey Kurakin, Nicolas Papernot, Ian~J. Goodfellow, Dan
  Boneh, and Patrick~D. McDaniel.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock In \emph{{ICLR}}, 2018.

\bibitem[Pang et~al.(2019)Pang, Xu, Du, Chen, and Zhu]{pang2019improving}
Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu.
\newblock Improving adversarial robustness via promoting ensemble diversity.
\newblock In \emph{{ICML}}, 2019.

\bibitem[Zoph et~al.(2018)Zoph, Vasudevan, Shlens, and Le]{zoph2018learning}
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc~V. Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock In \emph{{CVPR}}, 2018.

\bibitem[Schwartz et~al.(2019)Schwartz, Dodge, Smith, and
  Etzioni]{schwartz2019green}
Roy Schwartz, Jesse Dodge, Noah~A. Smith, and Oren Etzioni.
\newblock Green {AI}.
\newblock \emph{arXiv preprint arXiv:1907.10597}, 2019.

\bibitem[Krizhevsky et~al.(2009)]{krizhevsky2009learning}
Alex Krizhevsky et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\end{thebibliography}
