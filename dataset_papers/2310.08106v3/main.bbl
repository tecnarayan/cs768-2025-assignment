% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry, A.~Askell, P.~Mishkin, J.~Clark \emph{et~al.}, ``Learning transferable visual models from natural language supervision,'' in \emph{ICML}, 2021.

\bibitem{brown2020language}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal, A.~Herbert-Voss, G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~M. Ziegler, J.~Wu, C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray, B.~Chess, J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and D.~Amodei, ``Language models are few-shot learners,'' in \emph{NeurIPS}, 2020.

\bibitem{alayrac2022flamingo}
J.-B. Alayrac, J.~Donahue, P.~Luc, A.~Miech, I.~Barr, Y.~Hasson, K.~Lenc, A.~Mensch, K.~Millican, M.~Reynolds \emph{et~al.}, ``Flamingo: a visual language model for few-shot learning,'' in \emph{NeurIPS}, 2022.

\bibitem{yu2022coca}
J.~Yu, Z.~Wang, V.~Vasudevan, L.~Yeung, M.~Seyedhosseini, and Y.~Wu, ``Coca: Contrastive captioners are image-text foundation models,'' \emph{TMLR}, 2022.

\bibitem{COOP}
K.~Zhou, J.~Yang, C.~C. Loy, and Z.~Liu, ``Learning to prompt for vision-language models,'' \emph{IJCV}, 2022.

\bibitem{prograd}
B.~Zhu, Y.~Niu, Y.~Han, Y.~Wu, and H.~Zhang, ``Prompt-aligned gradient for prompt tuning,'' in \emph{ICCV}, 2023.

\bibitem{pham2023combined}
H.~Pham, Z.~Dai, G.~Ghiasi, K.~Kawaguchi, H.~Liu, A.~W. Yu, J.~Yu, Y.-T. Chen, M.-T. Luong, Y.~Wu \emph{et~al.}, ``Combined scaling for zero-shot transfer learning,'' \emph{arXiv preprint arXiv:2111.10050}, 2021.

\bibitem{wortsman2022robust}
M.~Wortsman, G.~Ilharco, J.~W. Kim, M.~Li, S.~Kornblith, R.~Roelofs, R.~G. Lopes, H.~Hajishirzi, A.~Farhadi, H.~Namkoong \emph{et~al.}, ``Robust fine-tuning of zero-shot models,'' in \emph{CVPR}, 2022.

\bibitem{zhu2023debiased}
B.~Zhu, Y.~Niu, S.~Lee, M.~Hur, and H.~Zhang, ``Debiased fine-tuning for vision-language models by prompt regularization,'' \emph{AAAI}, 2023.

\bibitem{reed2001pareto}
W.~J. Reed, ``The pareto, zipf and other power laws,'' \emph{Economics letters}, 2001.

\bibitem{zhu2023generalized}
B.~Zhu, K.~Tang, Q.~Sun, and H.~Zhang, ``Generalized logit adjustment: Calibrating fine-tuned models by removing label bias in foundation models,'' in \emph{NeurIPS}, 2023.

\bibitem{jia2021scaling}
C.~Jia, Y.~Yang, Y.~Xia, Y.-T. Chen, Z.~Parekh, H.~Pham, Q.~Le, Y.-H. Sung, Z.~Li, and T.~Duerig, ``Scaling up visual and vision-language representation learning with noisy text supervision,'' in \emph{ICML}, 2021.

\bibitem{allingham2023simple}
J.~U. Allingham, J.~Ren, M.~W. Dusenberry, X.~Gu, Y.~Cui, D.~Tran, J.~Z. Liu, and B.~Lakshminarayanan, ``A simple zero-shot prompt weighting technique to improve prompt ensembling in text-image models,'' in \emph{ICML}, 2023.

\bibitem{dietterich2000ensemble}
T.~G. Dietterich, ``Ensemble methods in machine learning,'' in \emph{Multiple Classifier Systems: First International Workshop}, 2000.

\bibitem{bauer1999empirical}
E.~Bauer and R.~Kohavi, ``An empirical comparison of voting classification algorithms: Bagging, boosting, and variants,'' \emph{Machine learning}, 1999.

\bibitem{lakshminarayanan2017simple}
B.~Lakshminarayanan, A.~Pritzel, and C.~Blundell, ``Simple and scalable predictive uncertainty estimation using deep ensembles,'' \emph{NeurIPS}, 2017.

\bibitem{freund1997decision}
Y.~Freund and R.~E. Schapire, ``A decision-theoretic generalization of on-line learning and an application to boosting,'' \emph{Journal of computer and system sciences}, 1997.

\bibitem{pmlr-v180-kumar22a}
A.~Kumar, T.~Ma, P.~Liang, and A.~Raghunathan, ``Calibrated ensembles can mitigate accuracy tradeoffs under distribution shift,'' in \emph{UAI}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2022.

\bibitem{yi2023invariant}
X.~Yi, J.~Deng, Q.~Sun, X.-S. Hua, J.-H. Lim, and H.~Zhang, ``Invariant training 2d-3d joint hard samples for few-shot point cloud recognition,'' in \emph{ICCV}, 2023.

\bibitem{izmailov2018averaging}
P.~Izmailov, D.~Podoprikhin, T.~Garipov, D.~Vetrov, and A.~G. Wilson, ``Averaging weights leads to wider optima and better generalization,'' \emph{UAI}, 2018.

\bibitem{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean, ``Distilling the knowledge in a neural network,'' \emph{NeurIPS Workshop}, 2014.

\bibitem{NEURIPS2020_18df51b9}
T.~Lin, L.~Kong, S.~U. Stich, and M.~Jaggi, ``Ensemble distillation for robust model fusion in federated learning,'' in \emph{NeurIPS}, 2020.

\bibitem{menonlong}
A.~K. Menon, S.~Jayasumana, A.~S. Rawat, H.~Jain, A.~Veit, and S.~Kumar, ``Long-tail learning via logit adjustment,'' in \emph{ICLR}, 2021.

\bibitem{tang2020long}
K.~Tang, J.~Huang, and H.~Zhang, ``Long-tailed classification by keeping the good and removing the bad momentum causal effect,'' \emph{NeurIPS}, 2020.

\bibitem{kangdecoupling}
B.~Kang, S.~Xie, M.~Rohrbach, Z.~Yan, A.~Gordo, J.~Feng, and Y.~Kalantidis, ``Decoupling representation and classifier for long-tailed recognition,'' in \emph{ICLR}, 2020.

\bibitem{wu2021adversarial}
T.~Wu, Z.~Liu, Q.~Huang, Y.~Wang, and D.~Lin, ``Adversarial robustness under long-tailed distribution,'' \emph{CVPR}, 2021.

\bibitem{hong2021disentangling}
Y.~Hong, S.~Han, K.~Choi, S.~Seo, B.~Kim, and B.~Chang, ``Disentangling label distribution for long-tailed visual recognition,'' in \emph{CVPR}, 2021.

\bibitem{gallegoPosada2022cooper}
J.~Gallego-Posada and J.~Ramirez, ``{Cooper: a toolkit for Lagrangian-based constrained optimization},'' 2022.

\bibitem{zhang2004statistical}
T.~Zhang, ``Statistical behavior and consistency of classification methods based on convex risk minimization,'' \emph{The Annals of Statistics}, 2004.

\bibitem{taori2020measuring}
R.~Taori, A.~Dave, V.~Shankar, N.~Carlini, B.~Recht, and L.~Schmidt, ``Measuring robustness to natural distribution shifts in image classification,'' \emph{NeurIPS}, 2020.

\bibitem{arjovsky2020out}
M.~Arjovsky, ``Out of distribution generalization in machine learning,'' Ph.D. dissertation, New York University, 2020.

\bibitem{PLOT}
G.~Chen, W.~Yao, X.~Song, X.~Li, Y.~Rao, and K.~Zhang, ``Prompt learning with optimal transport for vision-language models,'' \emph{ICLR}, 2023.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei, ``Imagenet: A large-scale hierarchical image database,'' in \emph{CVPR}, 2009.

\bibitem{fei2004learning}
L.~Fei-Fei, R.~Fergus, and P.~Perona, ``Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories,'' in \emph{CVPRW}, 2004.

\bibitem{parkhi2012cats}
O.~M. Parkhi, A.~Vedaldi, A.~Zisserman, and C.~Jawahar, ``Cats and dogs,'' in \emph{CVPR}, 2012.

\bibitem{krause20133d}
J.~Krause, M.~Stark, J.~Deng, and L.~Fei-Fei, ``3d object representations for fine-grained categorization,'' in \emph{CVPRW}, 2013.

\bibitem{nilsback2008automated}
M.-E. Nilsback and A.~Zisserman, ``Automated flower classification over a large number of classes,'' in \emph{Indian Conference on Computer Vision, Graphics \& Image Processing}, 2008.

\bibitem{bossard2014food}
L.~Bossard, M.~Guillaumin, and L.~Van~Gool, ``Food-101--mining discriminative components with random forests,'' in \emph{ECCV}, 2014.

\bibitem{maji2013fine}
S.~Maji, E.~Rahtu, J.~Kannala, M.~Blaschko, and A.~Vedaldi, ``Fine-grained visual classification of aircraft,'' \emph{arXiv preprint arXiv:1306.5151}, 2013.

\bibitem{helber2019eurosat}
P.~Helber, B.~Bischke, A.~Dengel, and D.~Borth, ``Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification,'' \emph{IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}, 2019.

\bibitem{soomro2012ucf101}
K.~Soomro, A.~R. Zamir, and M.~Shah, ``Ucf101: A dataset of 101 human actions classes from videos in the wild,'' \emph{arXiv preprint arXiv:1212.0402}, 2012.

\bibitem{cimpoi2014describing}
M.~Cimpoi, S.~Maji, I.~Kokkinos, S.~Mohamed, and A.~Vedaldi, ``Describing textures in the wild,'' in \emph{CVPR}, 2014.

\bibitem{xiao2010sun}
J.~Xiao, J.~Hays, K.~A. Ehinger, A.~Oliva, and A.~Torralba, ``Sun database: Large-scale scene recognition from abbey to zoo,'' in \emph{CVPR}, 2010.

\bibitem{recht2019imagenet}
B.~Recht, R.~Roelofs, L.~Schmidt, and V.~Shankar, ``Do imagenet classifiers generalize to imagenet?'' in \emph{ICML}, 2019.

\bibitem{wang2019learning}
H.~Wang, S.~Ge, Z.~Lipton, and E.~P. Xing, ``Learning robust global representations by penalizing local predictive power,'' \emph{NeurIPS}, 2019.

\bibitem{hendrycks2021natural}
D.~Hendrycks, K.~Zhao, S.~Basart, J.~Steinhardt, and D.~Song, ``Natural adversarial examples,'' in \emph{CVPR}, 2021.

\bibitem{hendrycks2021many}
D.~Hendrycks, S.~Basart, N.~Mu, S.~Kadavath, F.~Wang, E.~Dorundo, R.~Desai, T.~Zhu, S.~Parajuli, M.~Guo \emph{et~al.}, ``The many faces of robustness: A critical analysis of out-of-distribution generalization,'' in \emph{CVPR}, 2021.

\bibitem{Krizhevsky09learningmultiple}
A.~Krizhevsky, ``Learning multiple layers of features from tiny images,'' University of Toronto, Tech. Rep., 2009.

\bibitem{loshchilov2018decoupled}
I.~Loshchilov and F.~Hutter, ``Decoupled weight decay regularization,'' in \emph{ICLR}, 2019.

\bibitem{liu2019large}
Z.~Liu, Z.~Miao, X.~Zhan, J.~Wang, B.~Gong, and S.~X. Yu, ``Large-scale long-tailed recognition in an open world,'' in \emph{CVPR}, 2019.

\bibitem{ren2020balanced}
J.~Ren, C.~Yu, X.~Ma, H.~Zhao, S.~Yi \emph{et~al.}, ``Balanced meta-softmax for long-tailed visual recognition,'' \emph{NeurIPS}, 2020.

\bibitem{ma2021simple}
T.~Ma, S.~Geng, M.~Wang, J.~Shao, J.~Lu, H.~Li, P.~Gao, and Y.~Qiao, ``A simple long-tailed recognition baseline via vision-language model,'' \emph{arXiv preprint arXiv:2111.14745}, 2021.

\bibitem{meyer1980condition}
C.~D. Meyer, Jr, ``The condition of a finite markov chain and perturbation bounds for the limiting probabilities,'' \emph{SIAM Journal on Algebraic Discrete Methods}, vol.~1, no.~3, pp. 273--283, 1980.

\bibitem{lipton2018detecting}
Z.~Lipton, Y.-X. Wang, and A.~Smola, ``Detecting and correcting for label shift with black box predictors,'' in \emph{ICML}, 2018.

\bibitem{ilharco_gabriel_2021_5143773}
G.~Ilharco, M.~Wortsman, R.~Wightman, C.~Gordon, N.~Carlini, R.~Taori, A.~Dave, V.~Shankar, H.~Namkoong, J.~Miller, H.~Hajishirzi, A.~Farhadi, and L.~Schmidt, ``Openclip,'' Zenodo, Tech. Rep., jul 2021.

\bibitem{peng2019moment}
X.~Peng, Q.~Bai, X.~Xia, Z.~Huang, K.~Saenko, and B.~Wang, ``Moment matching for multi-source domain adaptation,'' in \emph{ICCV}, 2019.

\end{thebibliography}
