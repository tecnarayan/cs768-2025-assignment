@inproceedings{adaptdiffuser,
  author       = {Zhixuan Liang and
                  Yao Mu and
                  Mingyu Ding and
                  Fei Ni and
                  Masayoshi Tomizuka and
                  Ping Luo},
  title        = {AdaptDiffuser: Diffusion Models as Adaptive Self-evolving Planners},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {20725--20745},
  publisher    = {{PMLR}},
  year         = {2023}
}
@article{zero_shot_pref,
  title={Zero-shot Preference Learning for Offline RL via Optimal Transport},
  author={Liu, Runze and Du, Yali and Bai, Fengshuo and Lyu, Jiafei and Li, Xiu},
  journal={arXiv preprint arXiv:2306.03615},
  year={2023}
}
@inproceedings{MTDiff,
title={Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning},
author={Haoran He and Chenjia Bai and Kang Xu and Zhuoran Yang and Weinan Zhang and Dong Wang and Bin Zhao and Xuelong Li},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=fAdMly4ki5}
}

@inproceedings{distributionestimator,
  author       = {Kazusato Oko and
                  Shunta Akiyama and
                  Taiji Suzuki},
  title        = {Diffusion Models are Minimax Optimal Distribution Estimators},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {26517--26582},
  publisher    = {{PMLR}},
  year         = {2023}
}
@article{zhong2024panacea,
  title={Panacea: Pareto Alignment via Preference Adaptation for LLMs},
  author={Zhong, Yifan and Ma, Chengdong and Zhang, Xiaoyuan and Yang, Ziran and Zhang, Qingfu and Qi, Siyuan and Yang, Yaodong},
  journal={arXiv preprint arXiv:2402.02030},
  year={2024}
}
@article{chakraborty2024maxmin,
  title={MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences},
  author={Chakraborty, Souradip and Qiu, Jiahao and Yuan, Hui and Koppel, Alec and Huang, Furong and Manocha, Dinesh and Bedi, Amrit Singh and Wang, Mengdi},
  journal={arXiv preprint arXiv:2402.08925},
  year={2024}
}
@misc{zeng2024diversified,
      title={On Diversified Preferences of Large Language Model Alignment}, 
      author={Dun Zeng and Yong Dai and Pengyu Cheng and Longyue Wang and Tianhao Hu and Wanshun Chen and Nan Du and Zenglin Xu},
      year={2024},
      eprint={2312.07401},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
@article{yang2024rewards,
  title={Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment},
  author={Yang, Rui and Pan, Xiaoman and Luo, Feng and Qiu, Shuang and Zhong, Han and Yu, Dong and Chen, Jianshu},
  journal={arXiv preprint arXiv:2402.10207},
  year={2024}
}
@article{guo2024controllable,
  title={Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment},
  author={Guo, Yiju and Cui, Ganqu and Yuan, Lifan and Ding, Ning and Wang, Jiexin and Chen, Huimin and Sun, Bowen and Xie, Ruobing and Zhou, Jie and Lin, Yankai and others},
  journal={arXiv preprint arXiv:2402.19085},
  year={2024}
}
@article{yang2023using,
  title={Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model},
  author={Yang, Kai and Tao, Jian and Lyu, Jiafei and Ge, Chunjiang and Chen, Jiaxin and Li, Qimai and Shen, Weihan and Zhu, Xiaolong and Li, Xiu},
  journal={arXiv preprint arXiv:2311.13231},
  year={2023}
}
@inproceedings{DQL,
title={Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning},
author={Zhendong Wang and Jonathan J Hunt and Mingyuan Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=AHvFDPi-FA}
}

@inproceedings{diffusionpolicy,
  author       = {Cheng Chi and
                  Siyuan Feng and
                  Yilun Du and
                  Zhenjia Xu and
                  Eric Cousineau and
                  Benjamin Burchfiel and
                  Shuran Song},
  title        = {Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},
  booktitle    = {Robotics: Science and Systems},
  year         = {2023}
}
@article{IDQL,
  title={Idql: Implicit q-learning as an actor-critic method with diffusion policies},
  author={Hansen-Estruch, Philippe and Kostrikov, Ilya and Janner, Michael and Kuba, Jakub Grudzien and Levine, Sergey},
  journal={arXiv preprint arXiv:2304.10573},
  year={2023}
}

@inproceedings{humanbehavior,
title={Imitating Human Behaviour with Diffusion Models},
author={Tim Pearce and Tabish Rashid and Anssi Kanervisto and Dave Bignell and Mingfei Sun and Raluca Georgescu and Sergio Valcarcel Macua and Shan Zheng Tan and Ida Momennejad and Katja Hofmann and Sam Devlin},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Pv1GPQzRrC8}
}

@inproceedings{decisiondiffuser,
title={Is Conditional Generative Modeling all you need for Decision Making?},
author={Anurag Ajay and Yilun Du and Abhi Gupta and Joshua B. Tenenbaum and Tommi S. Jaakkola and Pulkit Agrawal},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=sP1fo2K9DFG}
}

@inproceedings{metadiffuser,
  author       = {Fei Ni and
                  Jianye Hao and
                  Yao Mu and
                  Yifu Yuan and
                  Yan Zheng and
                  Bin Wang and
                  Zhixuan Liang},
  title        = {MetaDiffuser: Diffusion Model as Conditional Planner for Offline Meta-RL},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {26087--26105},
  publisher    = {{PMLR}},
  year         = {2023}
}
@inproceedings{SfBC,
title={Offline Reinforcement Learning via High-Fidelity Generative Behavior Modeling},
author={Huayu Chen and Cheng Lu and Chengyang Ying and Hang Su and Jun Zhu},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=42zs3qa2kpy}
}
@inproceedings{diffuser,
  author       = {Michael Janner and
                  Yilun Du and
                  Joshua B. Tenenbaum and
                  Sergey Levine},
  title        = {Planning with Diffusion for Flexible Behavior Synthesis},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {162},
  pages        = {9902--9915},
  publisher    = {{PMLR}},
  year         = {2022}
}

@inproceedings{SER,
title={Synthetic Experience Replay},
author={Cong Lu and Philip J. Ball and Jack Parker-Holder},
booktitle={Workshop on Reincarnating Reinforcement Learning at ICLR 2023},
year={2023},
url={https://openreview.net/forum?id=0a9p3Ty2k_}
}
@article{BTmodel,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}
@inproceedings{hejna2023few,
  title={Few-shot preference learning for human-in-the-loop rl},
  author={Hejna III, Donald Joseph and Sadigh, Dorsa},
  booktitle={Conference on Robot Learning},
  pages={2014--2025},
  year={2023},
  organization={PMLR}
}
@article{dataprocessinginequality,
  title={An intuitive proof of the data processing inequality},
  author={Beaudry, Normand J and Renner, Renato},
  journal={Quantum Information \& Computation},
  volume={12},
  number={5-6},
  pages={432--441},
  year={2012},
  publisher={Rinton Press, Incorporated Paramus, NJ}
}
@book{sutton18,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018}
}
@article{lin2019pareto,
  title={Pareto multi-task learning},
  author={Lin, Xi and Zhen, Hui-Ling and Li, Zhenhua and Zhang, Qing-Fu and Kwong, Sam},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@inproceedings{Birlutiu2009MultitaskPL,
  title={Multi-task Preference learning with Gaussian Processes},
  author={Adriana Birlutiu and Perry Groot and Tom M. Heskes},
  booktitle={The European Symposium on Artificial Neural Networks},
  year={2009},
  url={https://api.semanticscholar.org/CorpusID:3892318}
}
@inproceedings{metaworld,
  title={Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning},
  author={Yu, Tianhe and Quillen, Deirdre and He, Zhanpeng and Julian, Ryan and Hausman, Karol and Finn, Chelsea and Levine, Sergey},
  booktitle={Conference on robot learning},
  pages={1094--1100},
  year={2020},
  organization={PMLR}
}
@article{PbRLsurvey,
  title={A survey of preference-based reinforcement learning methods},
  author={Wirth, Christian and Akrour, Riad and Neumann, Gerhard and F{\"u}rnkranz, Johannes and others},
  journal={Journal of Machine Learning Research},
  volume={18},
  number={136},
  pages={1--46},
  year={2017},
  publisher={Journal of Machine Learning Research/Massachusetts Institute of Technology~â€¦}
}
@inproceedings{PbRLreview,
  title={Advances in preference-based reinforcement learning: A review},
  author={Abdelkareem, Youssef and Shehata, Shady and Karray, Fakhri},
  booktitle={2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages={2527--2532},
  year={2022},
  organization={IEEE}
}
@inproceedings{PEBBLE,
  author       = {Kimin Lee and
                  Laura M. Smith and
                  Pieter Abbeel},
  title        = {{PEBBLE:} Feedback-Efficient Interactive Reinforcement Learning via
                  Relabeling Experience and Unsupervised Pre-training},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {139},
  pages        = {6152--6163},
  publisher    = {{PMLR}},
  year         = {2021}
}
@inproceedings{PT,
title={Preference Transformer: Modeling Human Preferences using Transformers for {RL}},
author={Changyeon Kim and Jongjin Park and Jinwoo Shin and Honglak Lee and Pieter Abbeel and Kimin Lee},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Peot1SFDX0}
}
@inproceedings{MRN,
 author = {Liu, Runze and Bai, Fengshuo and Du, Yali and Yang, Yaodong},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {22270--22284},
 title = {Meta-Reward-Net: Implicitly Differentiable Reward Learning for Preference-based Reinforcement Learning},
 volume = {35},
 year = {2022}
}
@inproceedings{OPPO,
  author       = {Yachen Kang and
                  Diyuan Shi and
                  Jinxin Liu and
                  Li He and
                  Donglin Wang},
  title        = {Beyond Reward: Offline Preference-guided Policy Optimization},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {15753--15768},
  publisher    = {{PMLR}},
  year         = {2023}
}
@article{PbRL2017,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{ddpm,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

@inproceedings{classifierfree,
title={Classifier-Free Diffusion Guidance},
author={Jonathan Ho and Tim Salimans},
booktitle={NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications},
year={2021},
url={https://openreview.net/forum?id=qw8AKxfYbI}
}
@inproceedings{classifierguidance,
  title={Improved denoising diffusion probabilistic models},
  author={Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle={International Conference on Machine Learning},
  pages={8162--8171},
  year={2021},
  organization={PMLR}
}
@article{pro,
  title={Preference ranking optimization for human alignment},
  author={Song, Feifan and Yu, Bowen and Li, Minghao and Yu, Haiyang and Huang, Fei and Li, Yongbin and Wang, Houfeng},
  journal={arXiv preprint arXiv:2306.17492},
  year={2023}
}
@inproceedings{phf,
  title={Pretraining language models with human preferences},
  author={Korbak, Tomasz and Shi, Kejian and Chen, Angelica and Bhalerao, Rasika Vinayak and Buckley, Christopher and Phang, Jason and Bowman, Samuel R and Perez, Ethan},
  booktitle={International Conference on Machine Learning},
  pages={17506--17533},
  year={2023},
  organization={PMLR}
}
@article{TT,
  title={Offline reinforcement learning as one big sequence modeling problem},
  author={Janner, Michael and Li, Qiyang and Levine, Sergey},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={1273--1286},
  year={2021}
}
@inproceedings{curl,
  title={Curl: Contrastive unsupervised representations for reinforcement learning},
  author={Laskin, Michael and Srinivas, Aravind and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={5639--5650},
  year={2020},
  organization={PMLR}
}
@inproceedings{contrastiveucb,
  title={Contrastive ucb: Provably efficient contrastive self-supervised learning in online reinforcement learning},
  author={Qiu, Shuang and Wang, Lingxiao and Bai, Chenjia and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={18168--18210},
  year={2022},
  organization={PMLR}
}
@inproceedings{becl,
  author       = {Rushuai Yang and
                  Chenjia Bai and
                  Hongyi Guo and
                  Siyuan Li and
                  Bin Zhao and
                  Zhen Wang and
                  Peng Liu and
                  Xuelong Li},
  title        = {Behavior Contrastive Learning for Unsupervised Skill Discovery},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {39183--39204},
  publisher    = {{PMLR}},
  year         = {2023}
}
@inproceedings{rcrl,
title={Return-Based Contrastive Representation Learning for Reinforcement  Learning},
author={Guoqing Liu and Chuheng Zhang and Li Zhao and Tao Qin and Jinhua Zhu and Li Jian and Nenghai Yu and Tie-Yan Liu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=_TM6rT7tXke}
}
@inproceedings{contrastivedt,
  title={Contrastive decision transformers},
  author={Konan, Sachin G and Seraj, Esmaeil and Gombolay, Matthew},
  booktitle={Conference on Robot Learning},
  pages={2159--2169},
  year={2023},
  organization={PMLR}
}
@inproceedings{contrastiveenergy,
  author       = {Cheng Lu and
                  Huayu Chen and
                  Jianfei Chen and
                  Hang Su and
                  Chongxuan Li and
                  Jun Zhu},
  title        = {Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling
                  in Offline Reinforcement Learning},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {22825--22855},
  publisher    = {{PMLR}},
  year         = {2023}
}
@article{D2C,
  title={D2c: Diffusion-decoding models for few-shot conditional generation},
  author={Sinha, Abhishek and Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={12533--12548},
  year={2021}
}
@article{DT,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}

@inproceedings{DPO,
    title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
    author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Christopher D Manning and Stefano Ermon and Chelsea Finn},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://arxiv.org/abs/2305.18290}
}
@inproceedings{SURF,
  author       = {Jongjin Park and
                  Younggyo Seo and
                  Jinwoo Shin and
                  Honglak Lee and
                  Pieter Abbeel and
                  Kimin Lee},
  title        = {{SURF:} Semi-supervised Reward Learning with Data Augmentation for
                  Feedback-efficient Preference-based Reinforcement Learning},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2022}
}
@article{instructGPT,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@article{Alphago,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}
@article{diagnosis,
  title={A reinforcement learning model for AI-based decision support in skin cancer},
  author={Barata, Catarina and Rotemberg, Veronica and Codella, Noel CF and Tschandl, Philipp and Rinner, Christoph and Akay, Bengu Nisa and Apalla, Zoe and Argenziano, Giuseppe and Halpern, Allan and Lallas, Aimilios and others},
  journal={Nature Medicine},
  pages={1--6},
  year={2023},
  publisher={Nature Publishing Group US New York}
}
@inproceedings{f-DPG,
  author       = {Dongyoung Go and
                  Tomasz Korbak and
                  Germ{\'{a}}n Kruszewski and
                  Jos Rozen and
                  Nahyeon Ryu and
                  Marc Dymetman},
  title        = {Aligning Language Models with Preferences through f-divergence Minimization},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {11546--11583},
  publisher    = {{PMLR}},
  year         = {2023}
}
@article{bench_off_prefer,
  author       = {Daniel Shin and
                  Anca D. Dragan and
                  Daniel S. Brown},
  title        = {Benchmarks and Algorithms for Offline Preference-Based Reward Learning},
  journal      = {Trans. Mach. Learn. Res.},
  volume       = {2023},
  year         = {2023}
}
@inproceedings{OAP,
  author       = {Qisen Yang and
                  Shenzhi Wang and
                  Matthieu Gaetan Lin and
                  Shiji Song and
                  Gao Huang},
  title        = {Boosting Offline Reinforcement Learning with Action Preference Query},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {39509--39523},
  publisher    = {{PMLR}},
  year         = {2023}
}
@article{HuGE,
  title={Breadcrumbs to the Goal: Goal-Conditioned Exploration from Human-in-the-Loop Feedback},
  author={Torne, Marcel and Balsells, Max and Wang, Zihan and Desai, Samedh and Chen, Tao and Agrawal, Pulkit and Gupta, Abhishek},
  journal={arXiv preprint arXiv:2307.11049},
  year={2023}
}
@article{PrefPPO,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{online_model_selection,
  title={Deploying Offline Reinforcement Learning with Human Feedback},
  author={Li, Ziniu and Xu, Ke and Liu, Liu and Li, Lanqing and Ye, Deheng and Zhao, Peilin},
  journal={arXiv preprint arXiv:2303.07046},
  year={2023}
}
@article{ANOLE,
  title={Efficient meta reinforcement learning for preference-based fast adaptation},
  author={Ren, Zhizhou and Liu, Anji and Liang, Yitao and Peng, Jian and Ma, Jianzhu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15502--15515},
  year={2022}
}
@article{IPL,
  title={Inverse Preference Learning: Preference-based RL without a Reward Function},
  author={Hejna, Joey and Sadigh, Dorsa},
  journal={arXiv preprint arXiv:2305.15363},
  year={2023}
}
@article{dexterous_manipulation,
  title={Learning a Universal Human Prior for Dexterous Manipulation from Human Preference},
  author={Ding, Zihan and Chen, Yuanpei and Ren, Allen Z and Gu, Shixiang Shane and Dong, Hao and Jin, Chi},
  journal={arXiv preprint arXiv:2304.04602},
  year={2023}
}
@inproceedings{L2D,
  title={Learning to Discern: Imitating Heterogeneous Human Demonstrations with Preference and Representation Learning},
  author={Kuhar, Sachit and Cheng, Shuo and Chopra, Shivang and Bronars, Matthew and Xu, Danfei},
  booktitle={7th Annual Conference on Robot Learning},
  year={2023}
}
@inproceedings{MTM,
  author       = {Philipp Wu and
                  Arjun Majumdar and
                  Kevin Stone and
                  Yixin Lin and
                  Igor Mordatch and
                  Pieter Abbeel and
                  Aravind Rajeswaran},
  title        = {Masked Trajectory Models for Prediction, Representation, and Control},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {37607--37623},
  publisher    = {{PMLR}},
  year         = {2023}
}
@inproceedings{explain_decision,
  author       = {Shripad Vilasrao Deshmukh and
                  Arpan Dasgupta and
                  Balaji Krishnamurthy and
                  Nan Jiang and
                  Chirag Agarwal and
                  Georgios Theocharous and
                  Jayakumar Subramanian},
  title        = {Explaining {RL} Decisions with Trajectories},
  booktitle    = {{ICLR}},
  publisher    = {OpenReview.net},
  year         = {2023}
}
@article{problem_RLHF,
  title={Open problems and fundamental limitations of reinforcement learning from human feedback},
  author={Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others},
  journal={arXiv preprint arXiv:2307.15217},
  year={2023}
}
@article{pangu-coder2,
  title={Pangu-coder2: Boosting large language models for code with ranking feedback},
  author={Shen, Bo and Zhang, Jiaxin and Chen, Taihong and Zan, Daoguang and Geng, Bing and Fu, An and Zeng, Muhan and Yu, Ailun and Ji, Jichuan and Zhao, Jingyang and others},
  journal={arXiv preprint arXiv:2307.14936},
  year={2023}
}
@inproceedings{PrefRec,
  author       = {Wanqi Xue and
                  Qingpeng Cai and
                  Zhenghai Xue and
                  Shuo Sun and
                  Shuchang Liu and
                  Dong Zheng and
                  Peng Jiang and
                  Kun Gai and
                  Bo An},
  title        = {PrefRec: Recommender Systems with Human Preferences for Reinforcing
                  Long-term User Engagement},
  booktitle    = {{KDD}},
  pages        = {2874--2884},
  publisher    = {{ACM}},
  year         = {2023}
}
@article{prompt-dt,
  title={Prompt-Tuning Decision Transformer with Preference Ranking},
  author={Hu, Shengchao and Shen, Li and Zhang, Ya and Tao, Dacheng},
  journal={arXiv preprint arXiv:2305.09648},
  year={2023}
}
@article{trafficRLHF,
  title={Reinforcement Learning with Human Feedback for Realistic Traffic Simulation},
  author={Cao, Yulong and Ivanovic, Boris and Xiao, Chaowei and Pavone, Marco},
  journal={arXiv preprint arXiv:2309.00709},
  year={2023}
}
@inproceedings{CEP,
  author       = {Cheng Lu and
                  Huayu Chen and
                  Jianfei Chen and
                  Hang Su and
                  Chongxuan Li and
                  Jun Zhu},
  title        = {Contrastive Energy Prediction for Exact Energy-Guided Diffusion Sampling
                  in Offline Reinforcement Learning},
  booktitle    = {{ICML}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {22825--22855},
  publisher    = {{PMLR}},
  year         = {2023}
}
@article{rl4rs,
  title={Rl4rs: A real-world benchmark for reinforcement learning based recommender system},
  author={Wang, Kai and Zou, Zhene and Shang, Yue and Deng, Qilin and Zhao, Minghao and Liang, Yile and Wu, Runze and Tao, Jianrong and Shen, Xudong and Lyu, Tangjie and others},
  journal={arXiv preprint arXiv:2110.11073},
  year={2021}
}
@article{honor_kings,
  title={Supervised learning achieves human-level performance in moba games: A case study of honor of kings},
  author={Ye, Deheng and Chen, Guibin and Zhao, Peilin and Qiu, Fuhao and Yuan, Bo and Zhang, Wen and Chen, Sheng and Sun, Mingfei and Li, Xiaoqian and Li, Siqin and others},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={33},
  number={3},
  pages={908--918},
  year={2020},
  publisher={IEEE}
}
@inproceedings{hide_and_seek,
title={Emergent Tool Use From Multi-Agent Autocurricula},
author={Bowen Baker and Ingmar Kanitscheider and Todor Markov and Yi Wu and Glenn Powell and Bob McGrew and Igor Mordatch},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkxpxJBKwS}
}
@article{smarts,
  title={Smarts: Scalable multi-agent reinforcement learning training school for autonomous driving},
  author={Zhou, Ming and Luo, Jun and Villella, Julian and Yang, Yaodong and Rusu, David and Miao, Jiayu and Zhang, Weinan and Alban, Montgomery and Fadakar, Iman and Chen, Zheng and others},
  journal={arXiv preprint arXiv:2010.09776},
  year={2020}
}
@article{d4rl,
  title={D4rl: Datasets for deep data-driven reinforcement learning},
  author={Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2004.07219},
  year={2020}
}
@article{aligndiff,
  author       = {Zibin Dong and
                  Yifu Yuan and
                  Jianye Hao and
                  Fei Ni and
                  Yao Mu and
                  Yan Zheng and
                  Yujing Hu and
                  Tangjie Lv and
                  Changjie Fan and
                  Zhipeng Hu},
  title        = {AlignDiff: Aligning Diverse Human Preferences via Behavior-Customisable
                  Diffusion Model},
  journal      = {CoRR},
  volume       = {abs/2310.02054},
  year         = {2023}
}
@inproceedings{DPPO,
  title={Direct Preference-based Policy Optimization without Reward Modeling},
  author={An, Gaon and Lee, Junhyeok and Zuo, Xingdong and Kosaka, Norio and Kim, Kyung-Min and Song, Hyun Oh},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}
@article{MADPO,
  title={Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization},
  author={Zhou, Zhanhui and Liu, Jie and Yang, Chao and Shao, Jing and Liu, Yu and Yue, Xiangyu and Ouyang, Wanli and Qiao, Yu},
  journal={arXiv preprint arXiv:2310.03708},
  year={2023}
}
@article{contrastivepref,
  title={Contrastive Prefence Learning: Learning from Human Feedback without RL},
  author={Hejna, Joey and Rafailov, Rafael and Sikchi, Harshit and Finn, Chelsea and Niekum, Scott and Knox, W Bradley and Sadigh, Dorsa},
  journal={arXiv preprint arXiv:2310.13639},
  year={2023}
}
@article{IPO,
  title={A general theoretical paradigm to understand learning from human preferences},
  author={Azar, Mohammad Gheshlaghi and Rowland, Mark and Piot, Bilal and Guo, Daniel and Calandriello, Daniele and Valko, Michal and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:2310.12036},
  year={2023}
}
@article{reward_ensembles,
  title={Reward Model Ensembles Help Mitigate Overoptimization},
  author={Coste, Thomas and Anwar, Usman and Kirk, Robert and Krueger, David},
  journal={arXiv preprint arXiv:2310.02743},
  year={2023}
}
@article{RLHF_generalize,
  title={Understanding the Effects of RLHF on LLM Generalisation and Diversity},
  author={Kirk, Robert and Mediratta, Ishita and Nalmpantis, Christoforos and Luketina, Jelena and Hambro, Eric and Grefenstette, Edward and Raileanu, Roberta},
  journal={arXiv preprint arXiv:2310.06452},
  year={2023}
}
@article{length_bias,
  title={A long way to go: Investigating length correlations in rlhf},
  author={Singhal, Prasann and Goyal, Tanya and Xu, Jiacheng and Durrett, Greg},
  journal={arXiv preprint arXiv:2310.03716},
  year={2023}
}
@inproceedings{multi-task-pref,
  title={Multi-task learning with user preferences: Gradient descent with controlled ascent in pareto optimization},
  author={Mahapatra, Debabrata and Rajan, Vaibhav},
  booktitle={International Conference on Machine Learning},
  pages={6597--6607},
  year={2020},
  organization={PMLR}
}
@inproceedings{flow_to_better,
title={Flow to Better: Offline Preference-based Reinforcement Learning via Preferred Trajectory Generation},
author={Anonymous},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=EG68RSznLT}
}
@article{CPL,
  title={Contrastive Prefence Learning: Learning from Human Feedback without RL},
  author={Hejna, Joey and Rafailov, Rafael and Sikchi, Harshit and Finn, Chelsea and Niekum, Scott and Knox, W Bradley and Sadigh, Dorsa},
  journal={arXiv preprint arXiv:2310.13639},
  year={2023}
}
@inproceedings{rvs,
title={RvS: What is Essential for Offline {RL} via Supervised Learning?},
author={Scott Emmons and Benjamin Eysenbach and Ilya Kostrikov and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=S874XAIpkR-}
}
@article{scalar-reward-not-enough,
  title={Scalar reward is not enough: A response to silver, singh, precup and sutton (2021)},
  author={Vamplew, Peter and Smith, Benjamin J and K{\"a}llstr{\"o}m, Johan and Ramos, Gabriel and R{\u{a}}dulescu, Roxana and Roijers, Diederik M and Hayes, Conor F and Heintz, Fredrik and Mannion, Patrick and Libin, Pieter JK and others},
  journal={Autonomous Agents and Multi-Agent Systems},
  volume={36},
  number={2},
  pages={41},
  year={2022},
  publisher={Springer}
}
@article{infovae,
  title={Infovae: Information maximizing variational autoencoders},
  author={Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  journal={arXiv preprint arXiv:1706.02262},
  year={2017}
}
@article{infogan,
  title={Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
  author={Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}
@InProceedings{infodiffusion,
  title = 	 {{I}nfo{D}iffusion: Representation Learning Using Information Maximizing Diffusion Models},
  author =       {Wang, Yingheng and Schiff, Yair and Gokaslan, Aaron and Pan, Weishen and Wang, Fei and De Sa, Christopher and Kuleshov, Volodymyr},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {36336--36354},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/wang23ah/wang23ah.pdf},
  url = 	 {https://proceedings.mlr.press/v202/wang23ah.html},
}

@article{reward-hacking,
  title={Defining and characterizing reward gaming},
  author={Skalse, Joar and Howe, Nikolaus and Krasheninnikov, Dmitrii and Krueger, David},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9460--9471},
  year={2022}
}
@inproceedings{reward-misspecification,
title={The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models},
author={Alexander Pan and Kush Bhatia and Jacob Steinhardt},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=JYtwGwIL7ye}
}
@inproceedings{stablediffusion,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}
@inproceedings{controlnet,
  title={Adding conditional control to text-to-image diffusion models},
  author={Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={3836--3847},
  year={2023}
}
@inproceedings{IQL,
title={Offline Reinforcement Learning with Implicit Q-Learning},
author={Ilya Kostrikov and Ashvin Nair and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=68n2s9ZJWF8}
}
@article{tsne,
  title={Visualizing data using t-SNE.},
  author={Van der Maaten, Laurens and Hinton, Geoffrey},
  journal={Journal of machine learning research},
  volume={9},
  number={11},
  year={2008}
}
@inproceedings{SAC,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}
@article{kang2024efficient,
  title={Efficient diffusion policies for offline reinforcement learning},
  author={Kang, Bingyi and Ma, Xiao and Du, Chao and Pang, Tianyu and Yan, Shuicheng},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{song2020denoising,
  title={Denoising Diffusion Implicit Models},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@article{lu2022dpm,
  title={Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps},
  author={Lu, Cheng and Zhou, Yuhao and Bao, Fan and Chen, Jianfei and Li, Chongxuan and Zhu, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={5775--5787},
  year={2022}
}
@article{understand_dm,
  title={Understanding diffusion models: A unified perspective},
  author={Luo, Calvin},
  journal={arXiv preprint arXiv:2208.11970},
  year={2022}
}
@inproceedings{tai2023revisiting,
  title={Revisiting denoising diffusion probabilistic models for speech enhancement: Condition collapse, efficiency and refinement},
  author={Tai, Wenxin and Zhou, Fan and Trajcevski, Goce and Zhong, Ting},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={11},
  pages={13627--13635},
  year={2023}
}
@article{yuan2024reward,
  title={Reward-directed conditional diffusion: Provable distribution estimation and reward improvement},
  author={Yuan, Hui and Huang, Kaixuan and Ni, Chengzhuo and Chen, Minshuo and Wang, Mengdi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{dong2022survey,
  title={A survey for in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}


@article{abernethy2023mechanism,
  title={A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks},
  author={Abernethy, Jacob and Agarwal, Alekh and Marinov, Teodor V and Warmuth, Manfred K},
  journal={arXiv preprint arXiv:2305.17040},
  year={2023}
}

@article{diffusion-1,
  title={Task-agnostic Pre-training and Task-guided Fine-tuning for Versatile Diffusion Planner},
  author={Fan, Chenyou and Bai, Chenjia and Shan, Zhao and He, Haoran and Zhang, Yang and Wang, Zhen},
  journal={arXiv preprint arXiv:2409.19949},
  year={2024}
}

@article{diffusion-2,
  title={Forward KL Regularized Preference Optimization for Aligning Diffusion Policies},
  author={Shan, Zhao and Fan, Chenyou and Qiu, Shuang and Shi, Jiyuan and Bai, Chenjia},
  journal={arXiv preprint arXiv:2409.05622},
  year={2024}
}

@inproceedings{diffusion-3,
  title={Large-scale actionless video pre-training via discrete diffusion for efficient policy learning},
  author={He, Haoran and Bai, Chenjia and Pan, Ling and Zhang, Weinan and Zhao, Bin and Li, Xuelong},
  booktitle={Neural Information Processing Systems},
  year={2024}
}