@article{malkov2014approximate,
	title={Approximate nearest neighbor algorithm based on navigable small world graphs},
	author={Malkov, Yury and Ponomarenko, Alexander and Logvinov, Andrey and Krylov, Vladimir},
	journal={Information Systems},
	volume={45},
	pages={61--68},
	year={2014},
	publisher={Elsevier}
}

@inproceedings{gionis1999similarity,
  title={Similarity search in high dimensions via hashing},
  author={Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev and others},
  booktitle={Vldb},
  volume={99},
  number={6},
  pages={518--529},
  year={1999}
}

@inproceedings{lee2015reasoning,
  title={Reasoning in vector space: An exploratory study of question answering},
  author={Lee, Moontae and He, Xiaodong and Yih, Wen-tau and Gao, Jianfeng and Deng, Li and Smolensky, Paul},
  booktitle={ICLR},
  year={2016}
  }

@article{bengio2003neural,
  title={A neural probabilistic language model},
  author={Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  journal={Journal of machine learning research (JMLR)},
  volume={3},
  number={Feb},
  pages={1137--1155},
  year={2003}
}


@article{chang2020pre,
  title={Pre-training Tasks for Embedding-based Large-scale Retrieval},
  author={Chang, Wei-Cheng and Yu, Felix X and Chang, Yin-Wen and Yang, Yiming and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2002.03932},
  year={2020}
}

@inproceedings{seo2019real,
  title={Real-time open-domain question answering with dense-sparse phrase index},
  author={Seo, Minjoon and Lee, Jinhyuk and Kwiatkowski, Tom and Parikh, Ankur P and Farhadi, Ali and Hajishirzi, Hannaneh},
  booktitle={ACL},
  pages={4430--4441},
  year={2019}
}

@inproceedings{xue2017deep,
	title={Deep Matrix Factorization Models for Recommender Systems.},
	author={Xue, Hong-Jian and Dai, Xinyu and Zhang, Jianbing and Huang, Shujian and Chen, Jiajun},
	booktitle={IJCAI},
	pages={3203--3209},
	year={2017}
}


@inproceedings{hall2015fast,
  title={Fast and Accurate Maximum Inner Product Recommendations on Map-Reduce},
  author={Hall, Rob and Attenberg, Josh},
  booktitle={Proceedings of the 24th International Conference on World Wide Web (WWW)},
  pages={1263--1268},
  year={2015}
}

@inproceedings{boytsov2016off,
  title={Off the beaten path: Let's replace term-based retrieval with k-nn search},
  author={Boytsov, Leonid and Novak, David and Malkov, Yury and Nyberg, Eric},
  booktitle={Proceedings of the 25th ACM international on conference on information and knowledge management (CIKM)},
  pages={1099--1108},
  year={2016}
}

@article{liu2022halos,
  title={HALOS: Hashing Large Output Space for Cheap Inference},
  author={Liu, Zichang and Xu, Zhaozhuo and Ji, Alan and Zhang, Junyan and Li, Jonathan and Chen, Beidi and Shrivastava, Anshumali},
  journal={Proceedings of Machine Learning and Systems},
  volume={4},
  pages={110--125},
  year={2022}
}

@article{chen2021scatterbrain,
  title={Scatterbrain: Unifying sparse and low-rank attention},
  author={Chen, Beidi and Dao, Tri and Winsor, Eric and Song, Zhao and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17413--17426},
  year={2021}
}

@inproceedings{chen2021mongoose,
  title={Mongoose: A learnable lsh framework for efficient neural network training},
  author={Chen, Beidi and Liu, Zichang and Peng, Binghui and Xu, Zhaozhuo and Li, Jonathan Lingjie and Dao, Tri and Song, Zhao and Shrivastava, Anshumali and Re, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{chen2019fast,
  title={Fast and accurate stochastic gradient estimation},
  author={Chen, Beidi and Xu, Yingchen and Shrivastava, Anshumali},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{kkl20,
  title={Reformer: The Efficient Transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  booktitle={ICLR},
  year={2020}
}



@article{chen2020slide,
  title={Slide: In defense of smart algorithms over hardware acceleration for large-scale deep learning systems},
  author={Chen, Beidi and Medini, Tharun and Farwell, James and Tai, Charlie and Shrivastava, Anshumali and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={291--306},
  year={2020}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{derpanis2005mean,
  title={Mean shift clustering},
  author={Derpanis, Konstantinos G},
  journal={Lecture Notes},
  volume={32},
  pages={1--4},
  year={2005}
}

@article{zhang2018navigating,
  title={Navigating with graph representations for fast and scalable decoding of neural language models},
  author={Zhang, Minjia and Wang, Wenhan and Liu, Xiaodong and Gao, Jianfeng and He, Yuxiong},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{indyk1998approximate,
  title={Approximate nearest neighbors: towards removing the curse of dimensionality},
  author={Indyk, Piotr and Motwani, Rajeev},
  booktitle={Proceedings of the thirtieth annual ACM symposium on Theory of computing},
  pages={604--613},
  year={1998}
}

@article{viterbi1967error,
  title={Error bounds for convolutional codes and an asymptotically optimum decoding algorithm},
  author={Viterbi, Andrew},
  journal={IEEE transactions on Information Theory},
  volume={13},
  number={2},
  pages={260--269},
  year={1967},
  publisher={IEEE}
}
@article{hooker2021hardware,
  title={The hardware lottery},
  author={Hooker, Sara},
  journal={Communications of the ACM},
  volume={64},
  number={12},
  pages={58--65},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{chan2022data,
  title={Data distributional properties drive emergent in-context learning in transformers},
  author={Chan, Stephanie CY and Santoro, Adam and Lampinen, Andrew Kyle and Wang, Jane X and Singh, Aaditya K and Richemond, Pierre Harvey and McClelland, James and Hill, Felix},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}

@inproceedings{nn13,
  title={OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings},
  author={Nelson, Jelani and Nguy{\^e}n, Huy L},
  booktitle={2013 ieee 54th annual symposium on foundations of computer science},
  pages={117--126},
  year={2013},
  organization={IEEE}
}

@inproceedings{swz17,
  title={Low rank approximation with entrywise l1-norm error},
  author={Song, Zhao and Woodruff, David P and Zhong, Peilin},
  booktitle={Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={688--701},
  year={2017}
}

@article{swy23,
  author       = {Zhao Song and
                  Weixin Wang and
                  Chenbo Yin},
  title        = {Fast and Efficient Matching Algorithm with Deadline Instances},
  journal      = {CoRR},
  volume       = {abs/2305.08353},
  year         = {2023}
}


@article{sy23,
  author       = {Zhao Song and
                  Mingquan Ye},
  title        = {Efficient Asynchronize Stochastic Gradient Algorithm with Structured
                  Data},
  journal      = {CoRR},
  volume       = {abs/2305.08001},
  year         = {2023}
}

@inproceedings{qszz23,
    title={An Online and Unified Algorithm for Projection Matrix Vector Multiplication with Application to Empirical Risk Minimization},
    author={Lianke Qin and Zhao Song and Lichen Zhang and Danyang Zhuo},
    booktitle={AISTATS},
    year={2023}
}

@article{szz21,
  title={Training multi-layer over-parametrized neural network in subquadratic time},
  author={Song, Zhao and Zhang, Lichen and Zhang, Ruizhe},
  journal={arXiv preprint arXiv:2112.07628},
  year={2021}
}

 @mastersthesis{z22,
author = {Zhang, Lichen},
year = {2022},
school = {Carnegie Mellon University},
title = {Speeding Up Optimizations via Data
Structures: Faster Search, Sample and
Maintenance}
}

@article{hswz22,
  title={Training overparametrized neural networks in sublinear time},
  author={Hu, Hang and Song, Zhao and Weinstein, Omri and Zhuo, Danyang},
  journal={arXiv preprint arXiv:2208.04508},
  year={2022}
}


@inproceedings{bpsw21,
  title={Training (Overparametrized) Neural Networks in Near-Linear Time},
  author={Brand, Jan van den and Peng, Binghui and Song, Zhao and Weinstein, Omri},
  journal={arXiv preprint arXiv:2006.11648},
  booktitle={ITCS},
  year={2021}
}

@article{als+22,
  title={Bypass Exponential Time Preprocessing: Fast Neural Network Training via Weight-Data Correlation Preprocessing},
  author={Alman, Josh and Liang, Jiehao and Song, Zhao and Zhang, Ruizhe and Zhuo, Danyang},
  journal={arXiv preprint arXiv:2211.14227},
  year={2022}
}

@inproceedings{syyz23,
    author = {Song, Zhao and Yang, Xin and Yang, Yuanyuan and Zhang, Lichen},
    booktitle = {International Conference on Machine Learning (ICML)},
    title = {Sketching meets differential privacy: fast algorithm for dynamic Kronecker projection maintenance},
    year = {2023}
}  

@inproceedings{dly21,
  title={A nearly-linear time algorithm for linear programs with small treewidth: A multiscale representation of robust central path},
  author={Dong, Sally and Lee, Yin Tat and Ye, Guanghao},
  booktitle={Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1784--1797},
  year={2021}
}

@article{gs22,
  title={A Faster Small Treewidth SDP Solver},
  author={Gu, Yuzhou and Song, Zhao},
  journal={arXiv preprint arXiv:2211.06033},
  year={2022}
}

@article{zhdk23,
  title={Kdeformer: Accelerating transformers via kernel density estimation},
  author={Zandieh, Amir and Han, Insu and Daliri, Majid and Karbasi, Amin},
  journal={arXiv preprint arXiv:2302.02451},
  year={2023}
}


@article{qsw23,
  author       = {Lianke Qin and
                  Zhao Song and
                  Yitan Wang},
  title        = {Fast Submodular Function Maximization},
  journal      = {CoRR},
  volume       = {abs/2305.08367},
  year         = {2023}
}

@article{gsyz23,
  title={Low rank matrix completion via robust alternating minimization in nearly linear time},
  author={Gu, Yuzhou and Song, Zhao and Yin, Junze and Zhang, Lichen},
  journal={arXiv preprint arXiv:2302.11068},
  year={2023}
}

@article{gms23,
  title={An over-parameterized exponential regression},
  author={Gao, Yeqi and Mahadevan, Sridhar and Song, Zhao},
  journal={arXiv preprint arXiv:2303.16504},
  year={2023}
}

@article{gsy23_dp,
  title={Differentially Private Attention Computation},
  author={Gao, Yeqi and Song, Zhao and Yang, Xin},
  journal={arXiv preprint arXiv:2305.04701},
  year={2023}
}

@article{w14,
  title={Sketching as a tool for numerical linear algebra},
  author={Woodruff, David P.},
  journal={Foundations and Trends{\textregistered} in Theoretical Computer Science},
  volume={10},
  number={1--2},
  pages={1--157},
  year={2014},
  publisher={Now Publishers, Inc.}
}


@inproceedings{jswz21,
  title={A faster algorithm for solving general LPs},
  author={Jiang, Shunhua and Song, Zhao and Weinstein, Omri and Zhang, Hengjie},
  booktitle={Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing},
  pages={823--832},
  year={2021}
}


@inproceedings{sy21,
  title={Oblivious Sketching-based Central Path Method for Linear Programming},
  author={Song, Zhao and Yu, Zheng},
  booktitle={International Conference on Machine Learning},
  pages={9835--9847},
  year={2021},
  organization={PMLR}
}

@inproceedings{lsz19,
  title={Solving empirical risk minimization in the current matrix multiplication time},
  author={Lee, Yin Tat and Song, Zhao and Zhang, Qiuyi},
  booktitle={Conference on Learning Theory},
  pages={2140--2157},
  year={2019},
  organization={PMLR}
}


@inproceedings{swz19,
  title={Relative error tensor low rank approximation},
  author={Song, Zhao and Woodruff, David P and Zhong, Peilin},
  booktitle={Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)},
  pages={2772--2789},
  year={2019},
  organization={SIAM}
}

@inproceedings {bwz16,
    AUTHOR = {Boutsidis, Christos and Woodruff, David P. and Zhong, Peilin},
     TITLE = {Optimal principal component analysis in distributed and
              streaming models},
 BOOKTITLE = {S{TOC}'16---{P}roceedings of the 48th {A}nnual {ACM} {SIGACT}
              {S}ymposium on {T}heory of {C}omputing},
  year = {2016}
}

@inproceedings{cw13,
  title={Low-rank approximation and regression in input sparsity time},
  author={Clarkson, Kenneth L and Woodruff, David P},
  booktitle={STOC},
  year={2013}
}

@inproceedings{ldfu13,
  title={Faster ridge regression via the subsampled randomized hadamard transform},
  author={Lu, Yichao and Dhillon, Paramveer and Foster, Dean P and Ungar, Lyle},
  booktitle={Advances in neural information processing systems (NIPS)},
  pages={369--377},
  year={2013}
}

@inproceedings{c16,
  title={Nearly tight oblivious subspace embeddings by trace inequalities},
  author={Cohen, Michael B},
  booktitle={Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms},
  pages={278--287},
  year={2016},
  organization={SIAM}
}

@inproceedings{rsw16,
  title={Weighted low rank approximations with provable guarantees},
  author={Razenshteyn, Ilya and Song, Zhao and Woodruff, David P},
  booktitle={Proceedings of the forty-eighth annual ACM symposium on Theory of Computing},
  pages={250--263},
  year={2016}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{baum1966statistical,
  title={Statistical inference for probabilistic functions of finite state Markov chains},
  author={Baum, Leonard E and Petrie, Ted},
  journal={The annals of mathematical statistics},
  volume={37},
  number={6},
  pages={1554--1563},
  year={1966},
  publisher={JSTOR}
}

@inproceedings{
xie2022an,
title={An Explanation of In-context Learning as Implicit Bayesian Inference},
author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=RdJVFCHjUMI}
}

@article{michel2019sixteen,
  title={Are sixteen heads really better than one?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{child2019generating,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}
@article{johnson2019billion,
  title={Billion-scale similarity search with {GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}
@article{malkov2018efficient,
  title={Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs},
  author={Malkov, Yu A and Yashunin, Dmitry A},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={42},
  number={4},
  pages={824--836},
  year={2018},
  publisher={IEEE}
}



@article{lee2018snip,
  title={Snip: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  journal={arXiv preprint arXiv:1810.02340},
  year={2018}
}

@article{bansal2022rethinking,
  title={Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale},
  author={Bansal, Hritik and Gopalakrishnan, Karthik and Dingliwal, Saket and Bodapati, Sravan and Kirchhoff, Katrin and Roth, Dan},
  journal={arXiv preprint arXiv:2212.09095},
  year={2022}
}

@article{lm00,
  title={Adaptive estimation of a quadratic functional by model selection},
  author={Laurent, Beatrice and Massart, Pascal},
  journal={Annals of Statistics},
  pages={1302--1338},
  year={2000},
  publisher={JSTOR}
}


@article{bsz23,
  title={Algorithm and Hardness for Dynamic Attention Maintenance in Large Language Models},
  author={Brand, Jan van den and Song, Zhao and Zhou, Tianyi},
  journal={arXiv preprint arXiv:2304.02207},
  year={2023}
}


@article{lsx+23,
  title={The Closeness of In-Context Learning and Weight Shifting for Softmax Regression},
  author={Shuai Li and Zhao Song and Yu Xia and Tong Yu and Tianyi Zhou},
  journal={arXiv preprint},
  year={2023}
}

@article{as23,
  title={Fast Attention Requires Bounded Entries},
  author={Alman, Josh and Song, Zhao},
  journal={arXiv preprint arXiv:2302.13214},
  year={2023}
}

@article{dms23,
    title={Randomized and Deterministic Attention Sparsification Algorithms for Over-parameterized Feature Dimension},
    author={Yichuan Deng and Sridhar Mahadevan and Zhao Song},
    journal={arxiv preprint: arxiv 2304.03426},
    year={2023}
}

@article{dls23,
    title={Attention Scheme Inspired Softmax Regression},
    author={Yichuan Deng and Zhihang Li and Zhao Song},
    journal={arXiv preprint arXiv:2304.10411},
    year={2023}
}


@article{lsz23,
  title={Solving Regularized Exp, Cosh and Sinh Regression Problems},
  author={Li, Zhihang and Song, Zhao and Zhou, Tianyi},
  journal={arXiv preprint, 2303.15725 },
  year={2023}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@inproceedings{lwy21,
  title={Exponentially improved dimensionality reduction for l1: Subspace embeddings and independence testing},
  author={Li, Yi and Woodruff, David and Yasuda, Taisuke},
  booktitle={Conference on Learning Theory},
  pages={3111--3195},
  year={2021},
  organization={PMLR}
}
@article{ww18,
  title={Tight Bounds for lp Oblivious Subspace Embeddings},
  author={Wang, Ruosong and Woodruff, David P},
  year={2018}
}
@inproceedings{MLSYS2022_1ff8a7b5,
 author = {Liu, Zichang and Xu, Zhaozhuo and Ji, Alan  and Zhang, Junyan  and Li, Jonathan  and Chen, Beidi and Shrivastava, Anshumali},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {D. Marculescu and Y. Chi and C. Wu},
 pages = {110--125},
 title = {HALOS: Hashing Large Output Space for Cheap Inference},
 volume = {4},
 year = {2022}
}
%url = {https://proceedings.mlsys.org/paper_files/paper/2022/file/1ff8a7b5dc7a7d1f0ed65aaa29c04b1e-Paper.pdf},

@inproceedings{s06,
  title={Improved approximation algorithms for large matrices via random projections},
  author={Sarlos, Tamas},
  booktitle={2006 47th annual IEEE symposium on foundations of computer science (FOCS)},
  pages={143--152},
  year={2006},
  organization={IEEE}
}

@inproceedings{hanpruning,
  author    = {Song Han and
               Huizi Mao and
               William J. Dally},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Deep Compression: Compressing Deep Neural Network with Pruning, Trained
               Quantization and Huffman Coding},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1510.00149},
  timestamp = {Fri, 20 Nov 2020 16:16:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/HanMD15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v119-kurtz20a,
  title = 	 {Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks},
  author =       {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Shavit, Nir and Alistarh, Dan},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {5533--5543},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/kurtz20a.html},
  abstract = 	 {Optimizing convolutional neural networks for fast inference has recently become an extremely active area of research. One of the go-to solutions in this context is weight pruning, which aims to reduce computational and memory footprint by removing large subsets of the connections in a neural network. Surprisingly, much less attention has been given to exploiting sparsity in the activation maps, which tend to be naturally sparse in many settings thanks to the structure of rectified linear (ReLU) activation functions. In this paper, we present an in-depth analysis of methods for maximizing the sparsity of the activations in a trained neural network, and show that, when coupled with an efficient sparse-input convolution algorithm, we can leverage this sparsity for significant performance gains. To induce highly sparse activation maps without accuracy loss, we introduce a new regularization technique, coupled with a new threshold-based sparsification method based on a parameterized activation function called Forced-Activation-Threshold Rectified Linear Unit (FATReLU). We examine the impact of our methods on popular image classification models, showing that most architectures can adapt to significantly sparser activation maps without any accuracy loss. Our second contribution is showing that these these compression gains can be translated into inference speedups: we provide a new algorithm to enable fast convolution operations over networks with sparse activations, and show that it can enable significant speedups for end-to-end inference on a range of popular models on the large-scale ImageNet image classification task on modern Intel CPUs, with little or no retraining cost.}
}

@misc{sanjiv,
  doi = {10.48550/ARXIV.2210.06313},
  
  url = {https://arxiv.org/abs/2210.06313},
  
  author = {Li, Zonglin and You, Chong and Bhojanapalli, Srinadh and Li, Daliang and Rawat, Ankit Singh and Reddi, Sashank J. and Ye, Ke and Chern, Felix and Yu, Felix and Guo, Ruiqi and Kumar, Sanjiv},
  
  keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}
@article{li2022large,
  title={Large Models are Parsimonious Learners: Activation Sparsity in Trained Transformers},
  author={Li, Zonglin and You, Chong and Bhojanapalli, Srinadh and Li, Daliang and Rawat, Ankit Singh and Reddi, Sashank J and Ye, Ke and Chern, Felix and Yu, Felix and Guo, Ruiqi and others},
  journal={arXiv preprint arXiv:2210.06313},
  year={2022}
}

@article{park2022nuqmm,
  title={nuqmm: Quantized matmul for efficient inference of large-scale generative language models},
  author={Park, Gunho and Park, Baeseong and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2206.09557},
  year={2022}
}
@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}
@article{xiao2022smoothquant,
  title={Smoothquant: Accurate and efficient post-training quantization for large language models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Demouth, Julien and Han, Song},
  journal={arXiv preprint arXiv:2211.10438},
  year={2022}
}
@article{dettmers2022llm,
  title={Llm. int8 (): 8-bit matrix multiplication for transformers at scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}

@article{frantar2022gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2022}
}

@inproceedings{dettmersgpt3,
  title={GPT3. int8 (): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  booktitle={Advances in Neural Information Processing Systems}
}

@article{kwon2022alphatuning,
  title={AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models},
  author={Kwon, Se Jung and Kim, Jeonghoon and Bae, Jeongin and Yoo, Kang Min and Kim, Jin-Hwa and Park, Baeseong and Kim, Byeongwook and Ha, Jung-Woo and Sung, Nako and Lee, Dongsoo},
  journal={arXiv preprint arXiv:2210.03858},
  year={2022}
}

@article{dettmers2022case,
  title={The case for 4-bit precision: k-bit Inference Scaling Laws},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2212.09720},
  year={2022}
}

@article{frantar2023massive,
  title={Massive Language Models Can Be Accurately Pruned in One-Shot},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2301.00774},
  year={2023}
}

@article{yao2022zeroquant,
  title={ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
  author={Yao, Zhewei and Aminabadi, Reza Yazdani and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
  journal={arXiv preprint arXiv:2206.01861},
  year={2022}
}

@article{hoefler2021sparsity,
  title={Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks.},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={J. Mach. Learn. Res.},
  volume={22},
  number={241},
  pages={1--124},
  year={2021}
}

@article{gholami2021survey,
  title={A survey of quantization methods for efficient neural network inference},
  author={Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2103.13630},
  year={2021}
}


@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@inproceedings{am93,
  title={Approximate Nearest Neighbor Queries in Fixed Dimensions.},
  author={Arya, Sunil and Mount, David M},
  booktitle={SODA},
  volume={93},
  pages={271--280},
  year={1993},
  organization={Citeseer}
}
@inproceedings{im98,
  title={Approximate nearest neighbors: towards removing the curse of dimensionality},
  author={Indyk, Piotr and Motwani, Rajeev},
  booktitle={Proceedings of the thirtieth annual ACM symposium on Theory of computing (STOC)},
  pages={604--613},
  year={1998}
}

@inproceedings{diim04,
  title={Locality-sensitive hashing scheme based on p-stable distributions},
  author={Datar, Mayur and Immorlica, Nicole and Indyk, Piotr and Mirrokni, Vahab S},
  booktitle={Proceedings of the twentieth annual symposium on Computational geometry (SoCG)},
  pages={253--262},
  year={2004}
}
@inproceedings{ainr14,
  title={Beyond locality-sensitive hashing},
  author={Andoni, Alexandr and Indyk, Piotr and Nguyen, Huy L and Razenshteyn, Ilya},
  booktitle={Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms},
  pages={1018--1028},
  year={2014},
  organization={SIAM}
}
@inproceedings{ailrs15,
  title={Practical and optimal LSH for angular distance},
  author={Andoni, Alexandr and Indyk, Piotr and Laarhoven, TMM and Razenshteyn, Ilya and Schmidt, Ludwig},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  pages={1225--1233},
  year={2015},
  organization={Curran Associates}
}
@inproceedings{ar15,
  title={Optimal data-dependent hashing for approximate near neighbors},
  author={Andoni, Alexandr and Razenshteyn, Ilya},
  booktitle={Proceedings of the forty-seventh annual ACM symposium on Theory of computing (STOC)},
  pages={793--801},
  year={2015}
}
@inproceedings{iw18,
  title={Approximate nearest neighbors in limited space},
  author={Indyk, Piotr and Wagner, Tal},
  booktitle={Conference On Learning Theory},
  pages={2012--2036},
  year={2018},
  organization={PMLR}
}
@inproceedings{alrw17,
  title={Optimal hashing-based time-space trade-offs for approximate near neighbors},
  author={Andoni, Alexandr and Laarhoven, Thijs and Razenshteyn, Ilya and Waingarten, Erik},
  booktitle={Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)},
  pages={47--66},
  year={2017},
  organization={SIAM}
}
@article{air18,
  title={Approximate nearest neighbor search in high dimensions},
  author={Andoni, Alexandr and Indyk, Piotr and Razenshteyn, Ilya},
  journal={arXiv preprint arXiv:1806.09823},
  volume={7},
  year={2018},
  publisher={World Scientific}
}
@inproceedings{dirw19,
  title={Learning Space Partitions for Nearest Neighbor Search},
  author={Dong, Yihe and Indyk, Piotr and Razenshteyn, Ilya and Wagner, Tal},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{ccd+20,
  title={$\{$SANNS$\}$: Scaling up secure approximate k-nearest neighbors search},
  author={Chen, Hao and Chillotti, Ilaria and Dong, Yihe and Poburinnaya, Oxana and Razenshteyn, Ilya and Riazi, M Sadegh},
  booktitle={29th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$ Security 20)},
  pages={2111--2128},
  year={2020}
}

@inproceedings{ns15,
  title={On symmetric and asymmetric lshs for inner product search},
  author={Neyshabur, Behnam and Srebro, Nathan},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={1926--1934},
  year={2015},
  organization={PMLR}
}
@article{ssx21,
  title={Sublinear Least-Squares Value Iteration via Locality Sensitive Hashing},
  author={Shrivastava, Anshumali and Song, Zhao and Xu, Zhaozhuo},
  journal={arXiv preprint arXiv:2105.08285},
  year={2021}
}
@article{jl84,
  title={Extensions of Lipschitz mappings into a Hilbert space},
  author={Johnson, William B and Lindenstrauss, Joram},
  journal={Contemporary mathematics},
  volume={26},
  number={189-206},
  pages={1},
  year={1984}
}
@inproceedings{c18,
  title={On The Hardness of Approximate and Exact (Bichromatic) Maximum Inner Product},
  author={Chen, Lijie},
  booktitle={33rd Computational Complexity Conference (CCC)},
  year={2018}
}

@inproceedings{ams96,
  title={The space complexity of approximating the frequency moments},
  author={Alon, Noga and Matias, Yossi and Szegedy, Mario},
  booktitle={Proceedings of the twenty-eighth annual ACM symposium on Theory of computing},
  pages={20--29},
  year={1996}
}

@inproceedings{ccf02,
  title={Finding frequent items in data streams},
  author={Charikar, Moses and Chen, Kevin and Farach-Colton, Martin},
  booktitle={International Colloquium on Automata, Languages, and Programming},
  pages={693--703},
  year={2002},
  organization={Springer}
}

@article{ln04,
  title={Embedding the diamond graph in L p and dimension reduction in L 1},
  author={Lee, James R and Naor, Assaf},
  journal={Geometric \& Functional Analysis GAFA},
  volume={14},
  number={4},
  pages={745--747},
  year={2004},
  publisher={Springer}
}

@article{bc05,
  title={On the impossibility of dimension reduction in l1},
  author={Brinkman, Bo and Charikar, Moses},
  journal={Journal of the ACM (JACM)},
  volume={52},
  number={5},
  pages={766--788},
  year={2005},
  publisher={ACM New York, NY, USA}
}
@inproceedings{cs02,
  title={Dimension reduction in the/spl lscr//sub 1/norm},
  author={Charikar, Moses and Sahai, Amit},
  booktitle={The 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002. Proceedings.},
  pages={551--560},
  year={2002},
  organization={IEEE}
}
@inproceedings{cp15,
  title={Lp row sampling by lewis weights},
  author={Cohen, Michael B and Peng, Richard},
  booktitle={Proceedings of the forty-seventh annual ACM symposium on Theory of computing},
  pages={183--192},
  year={2015}
}
@inproceedings{sw11,
  title={Subspace embeddings for the l1-norm with applications},
  author={Sohler, Christian and Woodruff, David P},
  booktitle={Proceedings of the forty-third annual ACM symposium on Theory of computing},
  pages={755--764},
  year={2011}
}
@inproceedings{mm13,
  title={Low-distortion subspace embeddings in input-sparsity time and applications to robust linear regression},
  author={Meng, Xiangrui and Mahoney, Michael W},
  booktitle={Proceedings of the forty-fifth annual ACM symposium on Theory of computing},
  pages={91--100},
  year={2013}
}

@inproceedings{yu2022orca,
  title={Orca: A Distributed Serving System for $\{$Transformer-Based$\}$ Generative Models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={521--538},
  year={2022}
}

@misc{huggingfaceAccelerate,
  author={HuggingFace},
  title = {Hugging Face Accelerate},
  howpublished = {\url{https://huggingface.co/docs/accelerate/index}}
}

@misc{nvidiaft,
  author = {NVIDIA},
  title = {FasterTransformer},
  howpublished = {\url{https://github.com/NVIDIA/FasterTransformer}},
}

@inproceedings{wang2021lightseq,
  title={LightSeq: A High Performance Inference Library for Transformers},
  author={Wang, Xiaohui and Xiong, Ying and Wei, Yang and Wang, Mingxuan and Li, Lei},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers},
  pages={113--120},
  year={2021}
}


@inproceedings{fang2021turbotransformers,
  title={TurboTransformers: an efficient GPU serving system for transformer models},
  author={Fang, Jiarui and Yu, Yang and Zhao, Chengduo and Zhou, Jie},
  booktitle={Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={389--402},
  year={2021}
}

@article{chowdhery2022palm,
  title={Pa{LM}: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{pope2022efficiently,
  title={Efficiently Scaling Transformer Inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={arXiv preprint arXiv:2211.05102},
  year={2022}
}


@inproceedings{aminabadi2022deepspeed,
  title={DeepSpeed-Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale},
  author={Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and others},
  booktitle={2022 SC22: International Conference for High Performance Computing, Networking, Storage and Analysis (SC)},
  pages={646--660},
  year={2022},
  organization={IEEE Computer Society}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}

@inproceedings{nagel2019data,
  title={Data-free quantization through weight equalization and bias correction},
  author={Nagel, Markus and Baalen, Mart van and Blankevoort, Tijmen and Welling, Max},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1325--1334},
  year={2019}
}

@inproceedings{zhao2019improving,
  title={Improving neural network quantization without retraining using outlier channel splitting},
  author={Zhao, Ritchie and Hu, Yuwei and Dotzel, Jordan and De Sa, Chris and Zhang, Zhiru},
  booktitle={International conference on machine learning},
  pages={7543--7552},
  year={2019},
  organization={PMLR}
}


@article{molchanov2016pruning,
  title={Pruning convolutional neural networks for resource efficient inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  journal={arXiv preprint arXiv:1611.06440},
  year={2016}
}

@article{liu2018rethinking,
  title={Rethinking the value of network pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  journal={arXiv preprint arXiv:1810.05270},
  year={2018}
}

@inproceedings{he2019filter,
  title={Filter pruning via geometric median for deep convolutional neural networks acceleration},
  author={He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4340--4349},
  year={2019}
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle={International Conference on Machine Learning},
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}

@inproceedings{cho2019efficacy,
  title={On the efficacy of knowledge distillation},
  author={Cho, Jang Hyun and Hariharan, Bharath},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4794--4802},
  year={2019}
}

@article{tang2019distilling,
  title={Distilling task-specific knowledge from bert into simple neural networks},
  author={Tang, Raphael and Lu, Yao and Liu, Linqing and Mou, Lili and Vechtomova, Olga and Lin, Jimmy},
  journal={arXiv preprint arXiv:1903.12136},
  year={2019}
}

@article{veit2016residual,
  title={Residual networks behave like ensembles of relatively shallow networks},
  author={Veit, Andreas and Wilber, Michael J and Belongie, Serge},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@inproceedings{balduzzi2017shattered,
  title={The shattered gradients problem: If resnets are the answer, then what is the question?},
  author={Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
  booktitle={International Conference on Machine Learning},
  pages={342--350},
  year={2017},
  organization={PMLR}
}

@article{bello2021revisiting,
  title={Revisiting resnets: Improved training and scaling strategies},
  author={Bello, Irwan and Fedus, William and Du, Xianzhi and Cubuk, Ekin Dogus and Srinivas, Aravind and Lin, Tsung-Yi and Shlens, Jonathon and Zoph, Barret},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={22614--22627},
  year={2021}
}

@article{allen2019can,
  title={What can resnet learn efficiently, going beyond kernels?},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{frei2019algorithm,
  title={Algorithm-dependent generalization bounds for overparameterized deep residual networks},
  author={Frei, Spencer and Cao, Yuan and Gu, Quanquan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@inproceedings{smith1998study,
  title={A study of branch prediction strategies},
  author={Smith, James E},
  booktitle={25 years of the international symposia on Computer architecture (selected papers)},
  pages={202--215},
  year={1998}
}

@inproceedings{dao2022flashattention,
  title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022}
}

@article{ivanov2021data,
  title={Data movement is all you need: A case study on optimizing transformers},
  author={Ivanov, Andrei and Dryden, Nikoli and Ben-Nun, Tal and Li, Shigang and Hoefler, Torsten},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={711--732},
  year={2021}
}


@misc{nvidia2022nvidia,
  title={GPU Performance Background User's Guide},
  author={NVIDIA},
  year={2022},
  url = {https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html}
}

@article{harris2013access,
  title={How to access global memory efficiently in {CUDA} {C}/{C}++ kernels},
  author={Harris, Mark},
  journal={NVIDIA, Jan},
  year={2013}
}

@book{cook2012cuda,
author = {Cook, Shane},
title = {CUDA Programming: A Developer’s Guide to Parallel Computing with GPUs},
year = {2012},
isbn = {9780124159334},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
edition = {1st}
}

@inproceedings{tillet2019triton,
  title={Triton: an intermediate language and compiler for tiled neural network computations},
  author={Tillet, Philippe and Kung, Hsiang-Tsung and Cox, David},
  booktitle={Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  pages={10--19},
  year={2019}
}

@misc{merity2016pointer,
      title={Pointer Sentinel Mixture Models},
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{2019t5,
    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
    journal = {arXiv e-prints},
    year = {2019},
    archivePrefix = {arXiv},
    eprint = {1910.10683},
}
@inproceedings{Bisk2020,
  author = {Yonatan Bisk and Rowan Zellers and
            Ronan Le Bras and Jianfeng Gao
            and Yejin Choi},
  title = {PIQA: Reasoning about Physical Commonsense in
           Natural Language},
  booktitle = {Thirty-Fourth AAAI Conference on
               Artificial Intelligence},
  year = {2020},
}

@InProceedings{ai2:winogrande,
title = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
authors={Keisuke, Sakaguchi and Ronan, Le Bras and Chandra, Bhagavatula and Yejin, Choi
},
year={2019}
}

@inproceedings{OpenBookQA2018,
 title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
 author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
 booktitle={EMNLP},
 year={2018}
}
@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}
@inproceedings{zellers2019hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    year={2019}
}
@inproceedings{Marneffe2019TheCI,
  title={The CommitmentBank: Investigating projection in naturally occurring discourse},
  author={Marie-Catherine de Marneffe and Mandy Simons and Judith Tonhauser},
  year={2019}
}

@inproceedings{gordon-etal-2012-semeval,
    title = "{S}em{E}val-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning",
    author = "Gordon, Andrew  and
      Kozareva, Zornitsa  and
      Roemmele, Melissa",
    booktitle = "*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",
    month = "7-8 " # jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S12-1052",
    pages = "394--398",
}
@inproceedings{giampiccolo-etal-2007-third,
    title = "The Third {PASCAL} Recognizing Textual Entailment Challenge",
    author = "Giampiccolo, Danilo  and
      Magnini, Bernardo  and
      Dagan, Ido  and
      Dolan, Bill",
    booktitle = "Proceedings of the {ACL}-{PASCAL} Workshop on Textual Entailment and Paraphrasing",
    month = jun,
    year = "2007",
    address = "Prague",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W07-1401",
    pages = "1--9",
}
@software{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@article{cordonnier2019relationship,
  title={On the relationship between self-attention and convolutional layers},
  author={Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  journal={arXiv preprint arXiv:1911.03584},
  year={2019}
  }

@misc{gpt-j,
  author = {Wang, Ben and Komatsuzaki, Aran},
  title = {{GPT-J-6B}: A 6 Billion Parameter Autoregressive Language Model},
  howpublished = {\url{https://github.com/kingoflolz/mesh-transformer-jax}},
  year = 2021,
  month = May
}
@article{li2019re,
  title={Re-randomized densification for one permutation hashing and bin-wise consistent weighted sampling},
  author={Li, Ping and Li, Xiaoyun and Zhang, Cun-Hui},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@InProceedings{pmlr-v162-li22m,
  title =   {C-{M}in{H}ash: Improving Minwise Hashing with Circulant Permutation},
  author =       {Li, Xiaoyun and Li, Ping},
  booktitle =   {Proceedings of the 39th International Conference on Machine Learning},
  pages =   {12857--12887},
  year =   {2022},
  editor =   {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume =   {162},
  series =   {Proceedings of Machine Learning Research},
  month =   {17--23 Jul},
  publisher =    {PMLR},
  pdf =   {https://proceedings.mlr.press/v162/li22m/li22m.pdf},
  url =   {https://proceedings.mlr.press/v162/li22m.html},
  abstract =   {Minwise hashing (MinHash) is an important and practical algorithm for generating random hashes to approximate the Jaccard (resemblance) similarity in massive binary (0/1) data. The basic theory of MinHash requires applying hundreds or even thousands of independent random permutations to each data vector in the dataset, in order to obtain reliable results for (e.g.,) building large-scale learning models or approximate near neighbor search. In this paper, we propose Circulant MinHash (C-MinHash) and provide the surprising theoretical results that using only two independent random permutations in a circulant manner leads to uniformly smaller Jaccard estimation variance than that of the classical MinHash with K independent permutations. Experiments are conducted to show the effectiveness of the proposed method. We also propose a more convenient C-MinHash variant which reduces two permutations to just one, with extensive numerical results to validate that it achieves essentially the same estimation accuracy as using two permutations.}
}
@inproceedings{gpt-neox-20b,
  title={{GPT-NeoX-20B}: An Open-Source Autoregressive Language Model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, USVSN Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
  booktitle={Proceedings of the ACL Workshop on Challenges \& Perspectives in Creating Large Language Models},
  url={https://arxiv.org/abs/2204.06745},
  year={2022}
}

@article{cox2008multidimensional,
  title={Multidimensional scaling, 315--347},
  author={Cox, MA and Cox, TF},
  journal={Handbook of data visualization. Springer, Berlin, Germany},
  year={2008}
}
@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}