\newcommand{\etalchar}[1]{$^{#1}$}
\providecommand{\bysame}{\leavevmode\hbox to3em{\hrulefill}\thinspace}
\providecommand{\MR}{\relax\ifhmode\unskip\space\fi MR }
% \MRhref is called by the amsart/book/proc definition of \MR.
\providecommand{\MRhref}[2]{%
  \href{http://www.ams.org/mathscinet-getitem?mr=#1}{#2}
}
\providecommand{\href}[2]{#2}
\begin{thebibliography}{GPAM{\etalchar{+}}14}

\bibitem[AJH{\etalchar{+}}21]{Austin2021StructuredDD}
Jacob Austin, Daniel~D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van~den
  Berg, \emph{Structured denoising diffusion models in discrete state-spaces},
  NeurIPS, 2021.

\bibitem[AMS22]{alaoui2022sampling}
Ahmed~El Alaoui, Andrea Montanari, and Mark Sellke, \emph{Sampling from the
  sherrington-kirkpatrick gibbs measure via algorithmic stochastic
  localization}, arXiv preprint arXiv:2203.05093 (2022).

\bibitem[And82]{Anderson1982ReversetimeDE}
Brian. D.~O. Anderson, \emph{Reverse-time diffusion equation models},
  Stochastic Processes and their Applications \textbf{12} (1982), 313--326.

\bibitem[BMR20]{Block2020GenerativeMW}
Adam Block, Youssef Mroueh, and Alexander Rakhlin, \emph{Generative modeling
  with denoising auto-encoders and langevin sampling}, 2020, arXiv:2002.00107.

\bibitem[BVE22]{Boffi2022ProbabilityFS}
Nicholas~M. Boffi and Eric Vanden-Eijnden, \emph{Probability flow solution of
  the fokker-planck equation}, 2022, arXiv:2206.04642.

\bibitem[CCL{\etalchar{+}}22]{SamplingEasy}
Sitan Chen, Sinho Chewi, Jungshian Li, Yuanzhi Li, Adil Salim, and Anru~R.
  Zhang, \emph{Sampling is as easy as learning the score: theory for diffusion
  models with minimal data assumptions}, ArXiv \textbf{abs/2209.11215} (2022).

\bibitem[CEL{\etalchar{+}}22]{pmlr-v178-chewi22a}
Sinho Chewi, Murat~A Erdogdu, Mufan Li, Ruoqi Shen, and Shunshi Zhang,
  \emph{Analysis of langevin monte carlo from poincare to log-sobolev},
  Proceedings of Thirty Fifth Conference on Learning Theory (Po-Ling Loh and
  Maxim Raginsky, eds.), Proceedings of Machine Learning Research, vol. 178,
  PMLR, 02--05 Jul 2022, pp.~1--2.

\bibitem[CSY21]{Chung2021ComeCloserDiffuseFasterAC}
Hyungjin Chung, Byeongsu Sim, and Jong-Chul Ye,
  \emph{Come-closer-diffuse-faster: Accelerating conditional diffusion models
  for inverse problems through stochastic contraction}, 2021, arXiv:2112.05146.

\bibitem[DeB22]{Bortoli2022ConvergenceOD}
Valentin DeBortoli, \emph{Convergence of denoising diffusion models under the
  manifold hypothesis}, 2022, arXiv:2208.05314.

\bibitem[DN21]{Dhariwal2021DiffusionMB}
Prafulla Dhariwal and Alex Nichol, \emph{Diffusion models beat gans on image
  synthesis}, Advances in Neural Information Processing Systems (2021).

\bibitem[DTHD21]{Bortoli2021DiffusionSB}
Valentin DeBortoli, James Thornton, Jeremy Heng, and A.~Doucet, \emph{Diffusion
  schr{\"o}dinger bridge with applications to score-based generative modeling},
  NeurIPS, 2021.

\bibitem[DVK21]{dockhorn2021score}
Tim Dockhorn, Arash Vahdat, and Karsten Kreis, \emph{Score-based generative
  modeling with critically-damped langevin diffusion}, arXiv preprint
  arXiv:2112.07068 (2021).

\bibitem[GPAM{\etalchar{+}}14]{Goodfellow2014GenerativeAN}
Ian~J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
  Warde-Farley, Sherjil Ozair, Aaron~C. Courville, and Yoshua Bengio,
  \emph{Generative adversarial nets}, NIPS, 2014.

\bibitem[GRG{\etalchar{+}}22]{Gnaneshwar2022ScoreBasedGM}
Dwaraknath Gnaneshwar, Bharath Ramsundar, Dhairya Gandhi, Rachel~C. Kurchin,
  and Venkatasubramanian Viswanathan, \emph{Score-based generative models for
  molecule generation}, 2022, arXiv:2203.04698.

\bibitem[KHR22]{statistic-efficiency}
Frederic Koehler, Alexander Heckett, and Andrej Risteski, \emph{Statistical
  efficiency of score matching: The view from isoperimetry}, 2022.

\bibitem[KS91]{Karatzas1987BrownianMA}
Ioannis Karatzas and Steven Shreve, \emph{Brownian motion and stochastic
  calculus}, vol. 113, Springer Science \& Business Media, 1991.

\bibitem[KW14]{Kingma2014AutoEncodingVB}
Diederik~P. Kingma and Max Welling, \emph{Auto-encoding variational bayes},
  2014, arXiv:1312.6114.

\bibitem[LLT22a]{convergence-score}
Holden Lee, Jianfeng Lu, and Yixin Tan, \emph{Convergence for score-based
  generative modeling with polynomial complexity}, 2022.

\bibitem[LLT22b]{convergencescore2}
\bysame, \emph{Convergence of score-based generative modeling for general data
  distributions}, 2022.

\bibitem[LM00]{Laurent2000AdaptiveEO}
B{\'e}atrice Laurent and Pascal Massart, \emph{Adaptive estimation of a
  quadratic functional by model selection}, Annals of Statistics \textbf{28}
  (2000), 1302--1338.

\bibitem[LPSR21]{lee2021universal}
Holden Lee, Chirag Pabbaraju, Anish Sevekari, and Andrej Risteski,
  \emph{Universal approximation for log-concave distributions using
  well-conditioned normalizing flows}, arXiv preprint arXiv:2107.02951 (2021).

\bibitem[RBL{\etalchar{+}}21]{Rombach2021HighResolutionIS}
Robin Rombach, A.~Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn
  Ommer, \emph{High-resolution image synthesis with latent diffusion models},
  CVPR (2021).

\bibitem[RM15]{JimenezRezende2015VariationalIW}
Danilo~Jimenez Rezende and Shakir Mohamed, \emph{Variational inference with
  normalizing flows}, ICML, 2015.

\bibitem[SDME21]{Song2021MaximumLT}
Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon, \emph{Maximum
  likelihood training of score-based diffusion models}, NeurIPS, 2021.

\bibitem[SE19]{SGMsong}
Yang Song and Stefano Ermon, \emph{Generative modeling by estimating gradients
  of the data distribution}, Advances in Neural Information Processing Systems,
  vol.~32, 2019.

\bibitem[SGSE19]{Song2019SlicedSM}
Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon, \emph{Sliced score
  matching: A scalable approach to density and score estimation}, UAI, 2019.

\bibitem[SLXT21]{Shi2021LearningGF}
Chence Shi, Shitong Luo, Minkai Xu, and Jian Tang, \emph{Learning gradient
  fields for molecular conformation generation}, ICML, 2021.

\bibitem[SME21]{Song2021DenoisingDI}
Jiaming Song, Chenlin Meng, and Stefano Ermon, \emph{Denoising diffusion
  implicit models}, 2021, arXiv:2010.02502.

\bibitem[SSK{\etalchar{+}}20]{SGMSDEsong}
Yang Song, Jascha Sohl{-}Dickstein, Diederik~P. Kingma, Abhishek Kumar, and Ben
  Poole, \emph{Score-based generative modeling through stochastic differential
  equations}, International Conference on Learning Representations, 2020.

\bibitem[SSXE22]{Song2022SolvingIP}
Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon, \emph{Solving inverse
  problems in medical imaging with score-based generative models}, 2022,
  arXiv:2111.08005.

\bibitem[Vin11]{Vincent2011ACB}
Pascal Vincent, \emph{A connection between score matching and denoising
  autoencoders}, Neural Computation \textbf{23} (2011), 1661--1674.

\bibitem[VW19]{Vempala2019RapidCO}
Santosh~S. Vempala and Andre Wibisono, \emph{Rapid convergence of the
  unadjusted langevin algorithm: Isoperimetry suffices}, NeurIPS, 2019.

\bibitem[WHZ22]{Wang2022DiffusionPA}
Zhendong Wang, Jonathan~J. Hunt, and Mingyuan Zhou, \emph{Diffusion policies as
  an expressive policy class for offline reinforcement learning}, 2022,
  arXiv:2208.06193.

\bibitem[WY22]{inexactLangevin}
Andre Wibisono and Kaylee~Yingxi Yang, \emph{Convergence in kl divergence of
  the inexact langevin algorithm with application to score-based generative
  models}, 2022.

\bibitem[ZC22]{Zhang2022FastSO}
Qinsheng Zhang and Yongxin Chen, \emph{Fast sampling of diffusion models with
  exponential integrator}, 2022, arXiv:2204.13902.

\bibitem[ZML16]{Zhao2016EnergybasedGA}
Junbo~Jake Zhao, Micha{\"e}l Mathieu, and Yann LeCun, \emph{Energy-based
  generative adversarial network}, 2016, arXiv:1609.03126.

\end{thebibliography}
