@TechReport{amin2016towards,
  title={Towards resolving unidentifiability in inverse reinforcement learning},
  author={Amin, Kareem and Singh, Satinder},
  type={{arXiv}:1601.06569},
  year={2016}
} 
@inproceedings{hadfield2016cooperative,
  title={Cooperative inverse reinforcement learning},
  author={Hadfield-Menell, Dylan and Russell, Stuart J and Abbeel, Pieter and Dragan, Anca},
  booktitle={Proc. {NeurIPS}},
  pages={3909--3917},
  year={2016}
}
@article{Kok2020,
  title = {Trust in Robots: Challenges and Opportunities},
	author = {Kok, Bing Cai and Soh, Harold},
  journal = {Current Robotics Reports},
  volume={1},
  number={4},
  pages={1--13},
  year = {2020},
  publisher={Springer}
	}
@article{osa2018algorithmic,
  title={An Algorithmic Perspective on Imitation Learning},
  author={Osa, Takayuki and Pajarinen, Joni and Neumann, Gerhard and Bagnell, J. Andrew and Abbeel, Pieter and Peters, Jan},
  journal={Foundations and Trends in Robotics},
  volume={7},
  number={1-2},
  pages={1--179},
  year={2018},
  publisher={Now Publishers}
} 
@article{soh2015learning,
  title={Learning assistance by demonstration: Smart mobility with shared control and paired haptic controllers},
  author={Soh, Harold and Demiris, Yiannis},
  journal={Journal of Human-Robot Interaction},
  volume={4},
  number={3},
  pages={76--100},
  year={2015},
  publisher={Journal of Human-Robot Interaction Steering Committee}
}
@InProceedings{rana2017,
  title={High Dimensional {B}ayesian Optimization with Elastic {G}aussian Process},
  author={Rana, Santu and Li, Cheng and Gupta, Sunil and Nguyen, Vu and Venkatesh, Svetha},
  booktitle={Proc. {ICML}},
  pages={2883--2891},
  year={2017},
  }
@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={Proc. {ICML}},
  pages={278--287},
  year={1999}
} 
@incollection{mockus1978application,
  title={The Application of {B}ayesian Methods for Seeking the Extremum},
  author={Mockus, Jonas and Tiesis, Vytautas and Zilinskas, Antanas},
  editor={L. C. W. Dixon and G. P. Szeg{\"{o}}},
  booktitle={Towards Global Optimization 2},
  pages={117--129},
  year={1978},
  publisher={North-Holland Publishing Company}
}
@book{williams2006gaussian,
  title="{Gaussian Processes} for {Machine Learning}",
  author={Williams, Christopher KI and Rasmussen, Carl Edward},
  year={2006},
  publisher={MIT Press}
}
@phdthesis{lizotte08,
author = {Lizotte, Daniel James},
title = {Practical {B}ayesian Optimization},
year = {2008},
school={University of Alberta},
}
@inproceedings{brochu2010bayesian,
  title={A {B}ayesian interactive optimization approach to procedural animation design},
  author={Brochu, Eric and Brochu, Tyson and de Freitas, Nando},
  booktitle={Proc. {SCA}},
  pages={103--112},
  year={2010}
}
@inproceedings{osborne2009gaussian,
  title={Gaussian processes for global optimization},
  author={Osborne, Michael A and Garnett, Roman and Roberts, Stephen J},
  booktitle={Proc. {LION3}},
  pages={1--15},
  year={2009}
}
@article{fosgerau2013link,
  title={A link based network route choice model with unrestricted choice set},
  author={Fosgerau, Mogens and Frejinger, Emma and Karlstrom, Anders},
  journal={Transportation Research Part B: Methodological},
  volume={56},
  pages={70--80},
  year={2013},
  publisher={Elsevier}
}
@inproceedings{ziebart2008maximum,
  title={Maximum entropy inverse reinforcement learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={Proc. {AAAI}},
  pages={1433--1438},
  year={2008}
} 
@TechReport{wulfmeier2015maximum,
  title={Maximum entropy deep inverse reinforcement learning},
  author={Wulfmeier, Markus and Ondruska, Peter and Posner, Ingmar},
  type={{arXiv}:1507.04888},
  year={2015}
} 
@inproceedings{finn2016guided,
  title={Guided cost learning: Deep inverse optimal control via policy optimization},
  author={Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  booktitle={Proc. {ICML}},
  pages={49--58},
  year={2016}
}
@TechReport{fu2017learning,
  title={Learning robust rewards with adversarial inverse reinforcement learning},
  author={Fu, Justin and Luo, Katie and Levine, Sergey},
  type={{arXiv}:1710.11248},
  year={2017}
} 
@inproceedings{ramachandran2007bayesian,
  title={Bayesian inverse reinforcement learning},
  author={Ramachandran, Deepak and Amir, Eyal},
  booktitle={Proc. {IJCAI}},
  pages={2586--2591},
  year={2007}
} 
@inproceedings{levine2011nonlinear,
  title={Nonlinear inverse reinforcement learning with {G}aussian processes},
  author={Levine, Sergey and Popovic, Zoran and Koltun, Vladlen},
  booktitle={Proc. {NeurIPS}},
  pages={19--27},
  year={2011}
}
@inproceedings{dewey2014reinforcement,
  title={Reinforcement learning and the reward engineering principle},
  author={Dewey, Daniel},
  booktitle={Proc. {AAAI}},
  year={2014}
}
@TechReport{ratner2018simplifying,
  title={Simplifying reward design through divide-and-conquer},
  author={Ratner, Ellis and Hadfield-Menell, Dylan and Dragan, Anca D},
  type={{arXiv}:1806.02501},
  year={2018}
}
@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proc. {ICML}},
  year={2004}
}
@inproceedings{malkomes2016bayesian,
  title={Bayesian optimization for automated model selection},
  author={Malkomes, Gustavo and Schaff, Charles and Garnett, Roman},
  booktitle={Proc. {NeurIPS}},
  pages={2900--2908},
  year={2016}
}
@article{abbeel2010autonomous,
  title={Autonomous helicopter aerobatics through apprenticeship learning},
  author={Abbeel, Pieter and Coates, Adam and Ng, Andrew Y},
  journal={The International Journal of Robotics Research},
  volume={29},
  number={13},
  pages={1608--1639},
  year={2010},
  publisher={SAGE Publications}
}
@inproceedings{bogert2016expectation,
  title={Expectation-maximization for inverse reinforcement learning with hidden data},
  author={Bogert, Kenneth and Lin, Jonathan Feng-Shun and Doshi, Prashant and Kulic, Dana},
  booktitle={Proc. {AAMAS}},
  pages={1034--1042},
  year={2016}
}
@inproceedings{boularias2011relative,
  title={Relative entropy inverse reinforcement learning},
  author={Boularias, Abdeslam and Kober, Jens and Peters, Jan},
  booktitle={Proc. {AISTATS}},
  pages={182--189},
  year={2011}
}
@inproceedings{boularias2012structured,
  title={Structured apprenticeship learning},
  author={Boularias, Abdeslam and Kr{\"o}mer, Oliver and Peters, Jan},
  booktitle={Proc. {ECML/PKDD}},
  pages={227--242},
  year={2012}
}
@inproceedings{ratliff2006maximum,
  title={Maximum margin planning},
  author={Ratliff, Nathan D and Bagnell, J Andrew and Zinkevich, Martin A},
  booktitle={Proc. {ICML}},
  pages={729--736},
  year={2006}
}
@TechReport{neu2012apprenticeship,
  title={Apprenticeship learning using inverse reinforcement learning and gradient methods},
  author={Neu, Gergely and Szepesv{\'a}ri, Csaba},
  type={{arXiv}:1206.5264},
  year={2012}
}
@inproceedings{nguyen2015inverse,
  title={Inverse reinforcement learning with locally consistent reward functions},
  author={Nguyen, Quoc Phong and Low, Bryan Kian Hsiang and Jaillet, Patrick},
  booktitle={Proc. {NeurIPS}},
  pages={1747--1755},
  year={2015}
}
@inproceedings{amin2017repeated,
  title={Repeated inverse reinforcement learning},
  author={Amin, Kareem and Jiang, Nan and Singh, Satinder},
  booktitle={Proc. {NeurIPS}},
  pages={1815--1824},
  year={2017}
}
@inproceedings{lopes2009active,
  title={Active learning for reward estimation in inverse reinforcement learning},
  author={Lopes, Manuel and Melo, Francisco and Montesano, Luis},
  booktitle={Proc. {ECML/PKDD}},
  pages={31--46},
  year={2009}
}
@inproceedings{NIPS2017_6800,
  title={Compatible Reward Inverse Reinforcement Learning},
  author={Metelli, Alberto Maria and Pirotta, Matteo and Restelli, Marcello},
  booktitle={Proc. {NeurIPS}},
  pages={2050--2059},
  year={2017}
}
@TechReport{roboticsenv-2018,
  title={Multi-goal reinforcement learning: Challenging robotics environments and request for research},
  author={Plappert, Matthias and Andrychowicz, Marcin and Ray, Alex and McGrew, Bob and Baker, Bowen and Powell, Glenn and Schneider, Jonas and Tobin, Josh and Chociej, Maciek and Welinder, Peter and Kumar, Vikash and Zaremba, Wojciech},
  type={{arXiv}:1206.5264},
  year={2018}
}
@inproceedings{brown2019machine,
  title={Machine teaching for inverse reinforcement learning: Algorithms and applications},
  author={Brown, Daniel S and Niekum, Scott},
  booktitle={Proc. {AAAI}},
  pages={7749--7758},
  year={2019}
}
@article{mannion2017policy,
  title={Policy invariance under reward transformations for multi-objective reinforcement learning},
  author={Mannion, Patrick and Devlin, Sam and Mason, Karl and Duggan, Jim and Howley, Enda},
  journal={Neurocomputing},
  volume={263},
  pages={60--73},
  year={2017},
  publisher={Elsevier}
}
@article{jaynes1982rationale,
  title={On the rationale of maximum-entropy methods},
  author={Jaynes, Edwin T},
  journal={Proceedings of the IEEE},
  volume={70},
  number={9},
  pages={939--952},
  year={1982}
}
@article{ziebart2013principle,
  title={The principle of maximum causal entropy for estimating interacting processes},
  author={Ziebart, Brian D and Bagnell, J Andrew and Dey, Anind K},
  journal={{IEEE} Transactions on Information Theory},
  volume={59},
  number={4},
  pages={1966--1980},
  year={2013}
}
@TechReport{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  type={{arXiv}:1707.06347},
  year={2017}
}
@article{shahriari2015taking,
  title={Taking the human out of the loop: A review of {B}ayesian optimization},
  author={Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P and De Freitas, Nando},
  journal={Proceedings of the IEEE},
  volume={104},
  number={1},
  pages={148--175},
  year={2015}
}
@TechReport{brown2019deep,
  title={Deep {Bayesian} Reward Learning from Preferences},
  author={Brown, Daniel S and Niekum, Scott},
  type={{arXiv}:1912.04472},
  year={2019}
}
@inproceedings{ho2016generative,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  booktitle={Proc. {NeurIPS}},
  pages={4565--4573},
  year={2016}
}
@TechReport{openaigym,
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  Title = "{OpenAI Gym}",
  Year = {2016},
  type={{arXiv}:1606.01540}
}
@inproceedings{NghiaAAAI18,
 author = "T. N. Hoang and Q. M. Hoang and B. K. H. Low",
  title ="Decentralized High-Dimensional {Bayesian} Optimization with Factor Graphs",   
     booktitle="Proc. {AAAI}",
     pages = "3231--3238",
  year = 2018       
}
@inproceedings{ling16,
	author="Ling, C. K. and Low, K. H. and Jaillet, P.",
	year="2016",
	title="{Gaussian} Process	Planning with {Lipschitz} Continuous	Reward Functions: Towards Unifying {Bayesian} Optimization, Active Learning, and Beyond",
	booktitle="Proc. {AAAI}",
	pages="1860--1866"
}
@inproceedings{dmitrii20a,
author = "D. Kharkovskii and C. K. Ling and B. K. H. Low",
title = "Nonmyopic {Gaussian} Process Optimization with Macro-Actions",
booktitle = "Proc. AISTATS",
pages="4593--4604",
year = "2020"
}
@inproceedings{daxberger2017distributed,
  title={Distributed batch {Gaussian} process optimization},
  author={Daxberger, Erik A and Low, Bryan Kian Hsiang},
  booktitle={Proc. {ICML}},
  pages={951--960},
  year={2017}
}
@inproceedings{dai20,
  title={Federated {Bayesian} optimization via {Thompson} sampling},
  author={Z. Dai and B. K. H. Low and P. Jaillet},
  booktitle={Proc. {NeurIPS}},
  year={2020}
}