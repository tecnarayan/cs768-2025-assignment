\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baldi and Hornik(1989)]{baldi1989neural}
P.~Baldi and K.~Hornik.
\newblock Neural networks and principal component analysis: Learning from
  examples without local minima.
\newblock \emph{Neural networks}, 2\penalty0 (1):\penalty0 53--58, 1989.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
P.~L. Bartlett, D.~J. Foster, and M.~J. Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6240--6249, 2017.

\bibitem[Bartlett et~al.(2018)Bartlett, Evans, and
  Long]{bartlett2018representing}
P.~L. Bartlett, S.~N. Evans, and P.~M. Long.
\newblock Representing smooth functions as compositions of near-identity
  functions with implications for deep network optimization.
\newblock \emph{arXiv preprint arXiv:1804.05012}, 2018.

\bibitem[Golowich et~al.(2017)Golowich, Rakhlin, and Shamir]{golowich2017size}
N.~Golowich, A.~Rakhlin, and O.~Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock \emph{arXiv preprint arXiv:1712.06541}, 2017.

\bibitem[Hardt and Ma(2017)]{hardt2017identity}
M.~Hardt and T.~Ma.
\newblock Identity matters in deep learning.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 770--778, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{he2016identity}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European Conference on Computer Vision}, pages 630--645.
  Springer, 2016{\natexlab{b}}.

\bibitem[Kawaguchi(2016)]{kawaguchi2016deep}
K.~Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  586--594, 2016.

\bibitem[Kawaguchi and Bengio(2018)]{kawaguchi2018depth}
K.~Kawaguchi and Y.~Bengio.
\newblock Depth with nonlinearity creates no bad local minima in resnets.
\newblock \emph{arXiv preprint arXiv:1810.09038}, 2018.

\bibitem[Laurent and Brecht(2018)]{laurent2018deep}
T.~Laurent and J.~Brecht.
\newblock Deep linear networks with arbitrary loss: All local minima are
  global.
\newblock In \emph{International Conference on Machine Learning}, pages
  2908--2913, 2018.

\bibitem[Laurent and von Brecht(2017)]{laurent2017multilinear}
T.~Laurent and J.~von Brecht.
\newblock The multilinear structure of {ReLU} networks.
\newblock \emph{arXiv preprint arXiv:1712.10132}, 2017.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
H.~Li, Z.~Xu, G.~Taylor, C.~Studer, and T.~Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6389--6399, 2018.

\bibitem[Liang et~al.(2018{\natexlab{a}})Liang, Sun, Lee, and
  Srikant]{liang2018adding}
S.~Liang, R.~Sun, J.~D. Lee, and R.~Srikant.
\newblock Adding one neuron can eliminate all bad local minima.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4355--4365, 2018{\natexlab{a}}.

\bibitem[Liang et~al.(2018{\natexlab{b}})Liang, Sun, Li, and
  Srikant]{liang2018understanding}
S.~Liang, R.~Sun, Y.~Li, and R.~Srikant.
\newblock Understanding the loss surface of neural networks for binary
  classification.
\newblock In \emph{International Conference on Machine Learning}, pages
  2840--2849, 2018{\natexlab{b}}.

\bibitem[Lin and Jegelka(2018)]{lin2018resnet}
H.~Lin and S.~Jegelka.
\newblock {ResNet} with one-neuron hidden layers is a universal approximator.
\newblock \emph{arXiv preprint arXiv:1806.10909}, 2018.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{neyshabur2015norm}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Conference on Learning Theory}, pages 1376--1401, 2015.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur2017exploring}
B.~Neyshabur, S.~Bhojanapalli, D.~McAllester, and N.~Srebro.
\newblock Exploring generalization in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5947--5956, 2017.

\bibitem[Nguyen and Hein(2017)]{nguyen2017loss}
Q.~Nguyen and M.~Hein.
\newblock The loss surface of deep and wide neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, volume~70, pages 2603--2612, 2017.

\bibitem[Nguyen et~al.(2018)Nguyen, Mukkamala, and Hein]{nguyen2018loss}
Q.~Nguyen, M.~C. Mukkamala, and M.~Hein.
\newblock On the loss landscape of a class of deep neural networks with no bad
  local valleys.
\newblock \emph{arXiv preprint arXiv:1809.10749}, 2018.

\bibitem[Safran and Shamir(2017)]{safran2017spurious}
I.~Safran and O.~Shamir.
\newblock Spurious local minima are common in two-layer {ReLU} neural networks.
\newblock \emph{arXiv preprint arXiv:1712.08968}, 2017.

\bibitem[Shamir(2018)]{shamir2018resnets}
O.~Shamir.
\newblock Are {ResNets} provably better than linear predictors?
\newblock \emph{arXiv preprint arXiv:1804.06739}, 2018.

\bibitem[Soudry and Carmon(2016)]{soudry2016no}
D.~Soudry and Y.~Carmon.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock \emph{arXiv preprint arXiv:1605.08361}, 2016.

\bibitem[Swirszcz et~al.(2016)Swirszcz, Czarnecki, and
  Pascanu]{swirszcz2016local}
G.~Swirszcz, W.~M. Czarnecki, and R.~Pascanu.
\newblock Local minima in training of neural networks.
\newblock \emph{arXiv preprint arXiv:1611.06310}, 2016.

\bibitem[Wu et~al.(2018)Wu, Luo, and Lee]{wu2018no}
C.~Wu, J.~Luo, and J.~D. Lee.
\newblock No spurious local minima in a two hidden unit {ReLU} network.
\newblock In \emph{International Conference on Learning Representations
  Workshop}, 2018.

\bibitem[Xie et~al.(2016)Xie, Liang, and Song]{xie2016diverse}
B.~Xie, Y.~Liang, and L.~Song.
\newblock Diverse neural network learns true target functions.
\newblock \emph{arXiv preprint arXiv:1611.03131}, 2016.

\bibitem[Yu and Chen(1995)]{yu1995local}
X.-H. Yu and G.-A. Chen.
\newblock On the local minima free condition of backpropagation learning.
\newblock \emph{IEEE Transactions on Neural Networks}, 6\penalty0 (5):\penalty0
  1300--1303, 1995.

\bibitem[Yun et~al.(2018)Yun, Sra, and Jadbabaie]{yun2018global}
C.~Yun, S.~Sra, and A.~Jadbabaie.
\newblock Global optimality conditions for deep neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Yun et~al.(2019{\natexlab{a}})Yun, Sra, and
  Jadbabaie]{yun2019efficiently}
C.~Yun, S.~Sra, and A.~Jadbabaie.
\newblock Efficiently testing local optimality and escaping saddles for {ReLU}
  networks.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Yun et~al.(2019{\natexlab{b}})Yun, Sra, and Jadbabaie]{yun2019small}
C.~Yun, S.~Sra, and A.~Jadbabaie.
\newblock Small nonlinearities in activation functions create bad local minima
  in neural networks.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Zhang et~al.(2019)Zhang, Dauphin, and Ma]{zhang2019fixup}
H.~Zhang, Y.~N. Dauphin, and T.~Ma.
\newblock Fixup initialization: Residual learning without normalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Zhou and Liang(2018)]{zhou2018critical}
Y.~Zhou and Y.~Liang.
\newblock Critical points of neural networks: Analytical forms and landscape
  properties.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\end{thebibliography}
