\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2019)Abbasi-Yadkori, Bartlett, Bhatia, Lazic,
  Szepesvari, and Weisz]{abbasi2019politex}
Abbasi-Yadkori, Y., Bartlett, P., Bhatia, K., Lazic, N., Szepesvari, C., and
  Weisz, G.
\newblock Politex: Regret bounds for policy iteration using expert prediction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3692--3702, 2019.

\bibitem[Agarwal et~al.(2019)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2019optimality}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock \emph{arXiv preprint arXiv:1908.00261}, 2019.

\bibitem[Altman(1999)]{altman1999constrained}
Altman, E.
\newblock \emph{Constrained Markov decision processes}, volume~7.
\newblock CRC Press, 1999.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, Freund, and
  Schapire]{auer2002nonstochastic}
Auer, P., Cesa-Bianchi, N., Freund, Y., and Schapire, R.~E.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM journal on computing}, 32\penalty0 (1):\penalty0 48--77,
  2002.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning, {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017}, pp.\
  263--272, 2017.

\bibitem[Beck \& Teboulle(2003)Beck and Teboulle]{beck2003mirror}
Beck, A. and Teboulle, M.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock \emph{Operations Research Letters}, 31\penalty0 (3):\penalty0
  167--175, 2003.

\bibitem[Bhandari \& Russo(2019)Bhandari and Russo]{bhandari2019global}
Bhandari, J. and Russo, D.
\newblock Global optimality guarantees for policy gradient methods.
\newblock \emph{arXiv preprint arXiv:1906.01786}, 2019.

\bibitem[Cai et~al.(2019)Cai, Yang, Jin, and Wang]{cai2019provably}
Cai, Q., Yang, Z., Jin, C., and Wang, Z.
\newblock Provably efficient exploration in policy optimization.
\newblock \emph{arXiv preprint arXiv:1912.05830}, 2019.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Dann, C., Lattimore, T., and Brunskill, E.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5713--5723, 2017.

\bibitem[Deisenroth \& Rasmussen(2011)Deisenroth and
  Rasmussen]{deisenroth2011pilco}
Deisenroth, M. and Rasmussen, C.~E.
\newblock Pilco: A model-based and data-efficient approach to policy search.
\newblock In \emph{Proceedings of the 28th International Conference on machine
  learning (ICML-11)}, pp.\  465--472, 2011.

\bibitem[Efroni et~al.(2019)Efroni, Merlis, Ghavamzadeh, and
  Mannor]{efroni2019tight}
Efroni, Y., Merlis, N., Ghavamzadeh, M., and Mannor, S.
\newblock Tight regret bounds for model-based reinforcement learning with
  greedy policies.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  12203--12213, 2019.

\bibitem[Geist et~al.(2019)Geist, Scherrer, and Pietquin]{geist2019theory}
Geist, M., Scherrer, B., and Pietquin, O.
\newblock A theory of regularized markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2160--2169, 2019.

\bibitem[Gu et~al.(2017)Gu, Holly, Lillicrap, and Levine]{gu2017deep}
Gu, S., Holly, E., Lillicrap, T., and Levine, S.
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock In \emph{2017 IEEE international conference on robotics and
  automation (ICRA)}, pp.\  3389--3396. IEEE, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1870, 2018.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1563--1600, 2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is q-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4863--4873, 2018.

\bibitem[Jin et~al.(2019)Jin, Jin, Luo, Sra, and Yu]{jin2019learning}
Jin, C., Jin, T., Luo, H., Sra, S., and Yu, T.
\newblock Learning adversarial markov decision processes with bandit feedback
  and unknown transition.
\newblock \emph{arXiv preprint arXiv:1912.01192}, 2019.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{ICML}, volume~2, pp.\  267--274, 2002.

\bibitem[Kakade(2002)]{kakade2002natural}
Kakade, S.~M.
\newblock A natural policy gradient.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1531--1538, 2002.

\bibitem[Kearns \& Singh(2002)Kearns and Singh]{kearns2002near}
Kearns, M. and Singh, S.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock \emph{Machine learning}, 49\penalty0 (2-3):\penalty0 209--232, 2002.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Liu et~al.(2019)Liu, Cai, Yang, and Wang]{liu2019neural}
Liu, B., Cai, Q., Yang, Z., and Wang, Z.
\newblock Neural proximal/trust region policy optimization attains globally
  optimal policy.
\newblock \emph{arXiv preprint arXiv:1906.10306}, 2019.

\bibitem[Maurer \& Pontil(2009)Maurer and Pontil]{maurer2009empirical}
Maurer, A. and Pontil, M.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock \emph{stat}, 1050:\penalty0 21, 2009.

\bibitem[Munos(2003)]{munos2003error}
Munos, R.
\newblock Error bounds for approximate policy iteration.
\newblock In Fawcett, T. and Mishra, N. (eds.), \emph{Machine Learning,
  Proceedings of the Twentieth International Conference {(ICML} 2003), August
  21-24, 2003, Washington, DC, {USA}}, pp.\  560--567. {AAAI} Press, 2003.

\bibitem[Neu(2015)]{neu2015explore}
Neu, G.
\newblock Explore no more: Improved high-probability regret bounds for
  non-stochastic bandits.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3168--3176, 2015.

\bibitem[Neu et~al.(2010{\natexlab{a}})Neu, Antos, Gy{\"o}rgy, and
  Szepesv{\'a}ri]{neu2010onlineT23}
Neu, G., Antos, A., Gy{\"o}rgy, A., and Szepesv{\'a}ri, C.
\newblock Online markov decision processes under bandit feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1804--1812, 2010{\natexlab{a}}.

\bibitem[Neu et~al.(2010{\natexlab{b}})Neu, Gy{\"o}rgy, and
  Szepesv{\'a}ri]{neu2010online}
Neu, G., Gy{\"o}rgy, A., and Szepesv{\'a}ri, C.
\newblock The online loop-free stochastic shortest-path problem.
\newblock In \emph{COLT}, volume 2010, pp.\  231--243. Citeseer,
  2010{\natexlab{b}}.

\bibitem[Neu et~al.(2017)Neu, Jonsson, and G{\'o}mez]{neu2017unified}
Neu, G., Jonsson, A., and G{\'o}mez, V.
\newblock A unified view of entropy-regularized markov decision processes.
\newblock \emph{arXiv preprint arXiv:1705.07798}, 2017.

\bibitem[Orabona(2019)]{orabona2019modern}
Orabona, F.
\newblock A modern introduction to online learning.
\newblock \emph{arXiv preprint arXiv:1912.13213}, 2019.

\bibitem[Peters \& Schaal(2006)Peters and Schaal]{peters2006policy}
Peters, J. and Schaal, S.
\newblock Policy gradient methods for robotics.
\newblock In \emph{2006 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  2219--2225. IEEE, 2006.

\bibitem[Peters \& Schaal(2008)Peters and Schaal]{peters2008reinforcement}
Peters, J. and Schaal, S.
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock \emph{Neural networks}, 21\penalty0 (4):\penalty0 682--697, 2008.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Rosenberg \& Mansour(2019{\natexlab{a}})Rosenberg and
  Mansour]{rosenberg2019bandit}
Rosenberg, A. and Mansour, Y.
\newblock Online stochastic shortest path with bandit feedback and unknown
  transition function.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2209--2218, 2019{\natexlab{a}}.

\bibitem[Rosenberg \& Mansour(2019{\natexlab{b}})Rosenberg and
  Mansour]{rosenberg2019full}
Rosenberg, A. and Mansour, Y.
\newblock Online convex optimization in adversarial markov decision processes.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}}, pp.\
  5478--5486, 2019{\natexlab{b}}.

\bibitem[Scherrer(2014)]{scherrer2014approximate}
Scherrer, B.
\newblock Approximate policy iteration schemes: a comparison.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1314--1322, 2014.

\bibitem[Scherrer \& Geist(2014)Scherrer and Geist]{scherrer2014local}
Scherrer, B. and Geist, M.
\newblock Local policy search in a convex space and conservative policy
  iteration as boosted policy search.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  35--50. Springer, 2014.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1889--1897, 2015.

\bibitem[Shani et~al.(2019)Shani, Efroni, and Mannor]{shani2019adaptive}
Shani, L., Efroni, Y., and Mannor, S.
\newblock Adaptive trust region policy optimization: Global convergence and
  faster rates for regularized mdps.
\newblock \emph{arXiv preprint arXiv:1909.02769}, 2019.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1057--1063, 2000.

\bibitem[Vieillard et~al.(2019)Vieillard, Pietquin, and
  Geist]{vieillard2019connections}
Vieillard, N., Pietquin, O., and Geist, M.
\newblock On connections between constrained optimization and reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1910.08476}, 2019.

\bibitem[Wei et~al.(2019)Wei, Jafarnia-Jahromi, Luo, Sharma, and
  Jain]{wei2019model}
Wei, C.-Y., Jafarnia-Jahromi, M., Luo, H., Sharma, H., and Jain, R.
\newblock Model-free reinforcement learning in infinite-horizon average-reward
  markov decision processes.
\newblock \emph{arXiv preprint arXiv:1910.07072}, 2019.

\bibitem[Weissman et~al.(2003)Weissman, Ordentlich, Seroussi, Verdu, and
  Weinberger]{weissman2003inequalities}
Weissman, T., Ordentlich, E., Seroussi, G., Verdu, S., and Weinberger, M.~J.
\newblock Inequalities for the l1 deviation of the empirical distribution.
\newblock \emph{Hewlett-Packard Labs, Tech. Rep}, 2003.

\bibitem[Zanette \& Brunskill(2019)Zanette and Brunskill]{zanette2019tighter}
Zanette, A. and Brunskill, E.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7304--7312, 2019.

\bibitem[Zimin \& Neu(2013)Zimin and Neu]{zimin2013online}
Zimin, A. and Neu, G.
\newblock Online learning in episodic markovian decision processes by relative
  entropy policy search.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1583--1591, 2013.

\end{thebibliography}
