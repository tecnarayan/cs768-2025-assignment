\begin{thebibliography}{}

\bibitem[Agarwal et~al., 2017]{Agarwal2017}
Agarwal, N., Allen-Zhu, Z., Bullins, B., Hazan, E., and Ma, T. (2017).
\newblock Finding approximate local minima faster than gradient descent.
\newblock In {\em Proc. 49th Annual ACM SIGACT Symposium on Theory of Computing
  (STOC)}, pages 1195--1199.

\bibitem[Allen-Zhu, 2017]{Zhu2017}
Allen-Zhu, Z. (2017).
\newblock {Natasha 2: Faster Non-Convex Optimization Than SGD}.
\newblock {\em ArXiv:1708.08694v3}.

\bibitem[Attouch and Bolte, 2009]{Attouch2009}
Attouch, H. and Bolte, J. (2009).
\newblock On the convergence of the proximal algorithm for nonsmooth functions
  involving analytic features.
\newblock {\em Mathematical Programming}, 116(1-2):5--16.

\bibitem[Attouch et~al., 2010]{Attouch2010}
Attouch, H., Bolte, J., Redont, P., and Soubeyran, A. (2010).
\newblock Proximal alternating minimization and projection methods for
  nonconvex problems: An approach based on the {K}urdyka-{{\L}}ojasiewicz
  inequality.
\newblock {\em Mathematics of Operations Research}, 35(2):438--457.

\bibitem[Baldi and Hornik, 1989]{Baldi1989}
Baldi, P. and Hornik, K. (1989).
\newblock Neural networks and principal component analysis: Learning from
  examples without local minima.
\newblock {\em Neural Networks}, 2(1):53 -- 58.

\bibitem[Bhojanapalli et~al., 2016]{Bhojanapalli2016}
Bhojanapalli, S., Neyshabur, B., and Srebro, N. (2016).
\newblock Global optimality of local search for low rank matrix recovery.
\newblock In {\em Proc. Advances in Neural Information Processing Systems
  (NIPS)}.

\bibitem[Bolte et~al., 2007]{Bolte2007}
Bolte, J., Daniilidis, A., and Lewis, A. (2007).
\newblock The {{\L}}ojasiewicz inequality for nonsmooth subanalytic functions
  with applications to subgradient dynamical systems.
\newblock {\em SIAM Journal on Optimization}, 17:1205--1223.

\bibitem[Bolte et~al., 2014]{Bolte2014}
Bolte, J., Sabach, S., and Teboulle, M. (2014).
\newblock Proximal alternating linearized minimization for nonconvex and
  nonsmooth problems.
\newblock {\em Mathematical Programming}, 146(1-2):459--494.

\bibitem[Candès et~al., 2015]{Candes2015}
Candès, E.~J., Li, X., and Soltanolkotabi, M. (2015).
\newblock Phase retrieval via wirtinger flow: Theory and algorithms.
\newblock {\em IEEE Transactions on Information Theory}, 61(4):1985--2007.

\bibitem[{Carmon} and {Duchi}, 2016]{Carmon2016}
{Carmon}, Y. and {Duchi}, J.~C. (2016).
\newblock {Gradient descent efficiently finds the cubic-regularized non-convex
  Newton step}.
\newblock {\em ArXiv: 1612.00547}.

\bibitem[Cartis et~al., 2011a]{Cartis2011b}
Cartis, C., Gould, N. I.~M., and Toint, P. (2011a).
\newblock Adaptive cubic regularization methods for unconstrained optimization.
  part ii: worst-case function- and derivative-evaluation complexity.
\newblock {\em Mathematical Programming}, 130(2):295--319.

\bibitem[Cartis et~al., 2011b]{Cartis2011a}
Cartis, C., Gould, N. I.~M., and Toint, P.~L. (2011b).
\newblock Adaptive cubic regularization methods for unconstrained optimization.
  part i : motivation, convergence and numerical results.
\newblock {\em Mathematical Programming}.

\bibitem[Frankel et~al., 2015]{Frankel2015}
Frankel, P., Garrigos, G., and Peypouquet, J. (2015).
\newblock Splitting methods with variable metric for
  {K}urdyka--{{\L}}ojasiewicz functions and general convergence rates.
\newblock {\em Journal of Optimization Theory and Applications},
  165(3):874--900.

\bibitem[Ge et~al., 2015]{Ge2015}
Ge, R., Huang, F., Jin, C., and Yuan, Y. (2015).
\newblock Escaping from saddle points --- online stochastic gradient for tensor
  decomposition.
\newblock In {\em Proc. 28th Conference on Learning Theory (COLT)}, volume~40,
  pages 797--842.

\bibitem[Ge et~al., 2016]{RongGe2016}
Ge, R., Lee, J., and Ma, T. (2016).
\newblock Matrix completion has no spurious local minimum.
\newblock In {\em Proc. Advances in Neural Information Processing Systems
  (NIPS)}, pages 2973--2981.

\bibitem[Ghadimi et~al., 2017]{Saeed2017}
Ghadimi, S., Liu, H., and Zhang, T. (2017).
\newblock Second-order methods with cubic regularization under inexact
  information.
\newblock {\em ArXiv: 1710.05782}.

\bibitem[Goodfellow et~al., 2016]{Goodfellow2016}
Goodfellow, I., Y, B., and Courville, A. (2016).
\newblock {\em Deep Learning}.
\newblock MIT Press.

\bibitem[Hartman, 2002]{Hartman}
Hartman, P. (2002).
\newblock {\em Ordinary Differential Equations}.
\newblock Society for Industrial and Applied Mathematics, second edition.

\bibitem[{Jiang} et~al., 2017]{Jiang2017}
{Jiang}, B., {Lin}, T., and {Zhang}, S. (2017).
\newblock {A unified scheme to accelerate adaptive cubic regularization and
  gradient methods for convex optimization}.
\newblock {\em ArXiv:1710.04788}.

\bibitem[Karimi et~al., 2016]{Karimi2016}
Karimi, H., Nutini, J., and Schmidt, M. (2016).
\newblock {\em Linear Convergence of Gradient and Proximal-Gradient Methods
  Under the Polyak-{\L}ojasiewicz Condition}, pages 795--811.

\bibitem[Kohler and Lucchi, 2017]{kohler2017}
Kohler, J.~M. and Lucchi, A. (2017).
\newblock Sub-sampled cubic regularization for non-convex optimization.
\newblock In {\em Proc. 34th International Conference on Machine Learning
  (ICML)}, volume~70, pages 1895--1904.

\bibitem[Li et~al., 2017]{Li2017}
Li, Q., Zhou, Y., Liang, Y., and Varshney, P.~K. (2017).
\newblock Convergence analysis of proximal gradient with momentum for nonconvex
  optimization.
\newblock In {\em Proc. 34th International Conference on Machine Learning
  (ICML}, volume~70, pages 2111--2119.

\bibitem[{Liu} and {Yang}, 2017]{Liu2017}
{Liu}, M. and {Yang}, T. (2017).
\newblock {On Noisy Negative Curvature Descent: Competing with Gradient Descent
  for Faster Non-convex Optimization}.
\newblock {\em ArXiv:1709.08571v2}.

\bibitem[{\L}ojasiewicz, 1963]{Lojasiewicz1963}
{\L}ojasiewicz, S. (1963).
\newblock A topological property of real analytic subsets.
\newblock {\em Coll. du CNRS, Les equations aux derivees partielles}, page
  87–89.

\bibitem[{\L}ojasiewicz, 1965]{Lojas1965}
{\L}ojasiewicz, S. (1965).
\newblock {\em Ensembles semi-analytiques}.
\newblock Institut des Hautes Etudes Scientifiques.

\bibitem[Nesterov, 2008]{Nesterov2008}
Nesterov, Y. (2008).
\newblock Accelerating the cubic regularization of newton's method on convex
  problems.
\newblock {\em Mathematical Programming}, 112(1):159--181.

\bibitem[Nesterov and Polyak, 2006]{Nesterov2006}
Nesterov, Y. and Polyak, B. (2006).
\newblock Cubic regularization of newton's method and its global performance.
\newblock {\em Mathematical Programming}.

\bibitem[Noll and Rondepierre, 2013]{Noll2013}
Noll, D. and Rondepierre, A. (2013).
\newblock Convergence of linesearch and trust-region methods using the
  {K}urdyka--{{\L}}ojasiewicz inequality.
\newblock In {\em Proc. Computational and Analytical Mathematics}, pages
  593--611.

\bibitem[Sun et~al., 2015]{Sun2015}
Sun, J., Qu, Q., and Wright, J. (2015).
\newblock {When Are Nonconvex Problems Not Scary?}
\newblock {\em ArXiv:1510.06096v2}.

\bibitem[Sun et~al., 2017]{Sun2017}
Sun, J., Qu, Q., and Wright, J. (2017).
\newblock A geometrical analysis of phase retrieval.
\newblock {\em Foundations of Computational Mathematics}, pages 1--68.

\bibitem[{Tripuraneni} et~al., 2017]{Jin2017}
{Tripuraneni}, N., {Stern}, M., {Jin}, C., {Regier}, J., and {Jordan}, M.~I.
  (2017).
\newblock {Stochastic Cubic Regularization for Fast Nonconvex Optimization}.
\newblock {\em ArXiv: 711.02838}.

\bibitem[Wang et~al., 2018]{Wang2018}
Wang, Z., Zhou, Y., Liang, Y., and Lan, G. (2018).
\newblock {Sample Complexity of Stochastic Variance-Reduced Cubic
  Regularization for Nonconvex Optimization}.
\newblock {\em ArXiv:1802.07372v1}.

\bibitem[Xu et~al., 2017]{Xu2017}
Xu, P., Roosta-Khorasani, F., and Mahoney, M.~W. (2017).
\newblock Newton-type methods for non-convex optimization under inexact hessian
  information.
\newblock {\em ArXiv: 1708.07164}.

\bibitem[Yue et~al., 2018]{Yue2018}
Yue, M., Zhou, Z., and So, M. (2018).
\newblock {On the Quadratic Convergence of the Cubic Regularization Method
  under a Local Error Bound Condition}.
\newblock {\em ArXiv:1801.09387v1}.

\bibitem[Zhang et~al., 2017]{Zhang2017}
Zhang, H., Liang, Y., and Chi, Y. (2017).
\newblock A nonconvex approach for phase retrieval: Reshaped wirtinger flow and
  incremental algorithms.
\newblock {\em Journal of Machine Learning Research}, 18(141):1--35.

\bibitem[Zhou et~al., 2018]{Zhou2018}
Zhou, D., Xu, P., and Gu, Q. (2018).
\newblock {Stochastic Variance-Reduced Cubic Regularized Newton Method}.
\newblock {\em ArXiv:1802.04796v1}.

\bibitem[Zhou and Liang, 2017]{Zhou2017}
Zhou, Y. and Liang, Y. (2017).
\newblock {Characterization of Gradient Dominance and Regularity Conditions for
  Neural Networks}.
\newblock {\em ArXiv:1710.06910v2}.

\bibitem[{Zhou} et~al., 2017]{Zhou_2017a}
{Zhou}, Y., {Yu}, Y., {Dai}, W., {Liang}, Y., and {Xing}, E.~P. (2017).
\newblock {Distributed Proximal Gradient Algorithm for Partially Asynchronous
  Computer Clusters}.
\newblock {\em arXiv:1704.03540}.

\bibitem[Zhou et~al., 2016a]{Zhou2016}
Zhou, Y., Yu, Y., Dai, W., Liang, Y., and Xing, P. (2016a).
\newblock On convergence of model parallel proximal gradient algorithm for
  stale synchronous parallel system.
\newblock In {\em Proc 19th International Conference on Artificial Intelligence
  and Statistics (AISTATS}, volume~51, pages 713--722.

\bibitem[Zhou et~al., 2016b]{Zhou2016b}
Zhou, Y., Zhang, H., and Liang, Y. (2016b).
\newblock Geometrical properties and accelerated gradient solvers of non-convex
  phase retrieval.
\newblock {\em In Proc. 54th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pages 331--335.

\end{thebibliography}
