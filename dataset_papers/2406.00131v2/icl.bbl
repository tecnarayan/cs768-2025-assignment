\begin{thebibliography}{68}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbas et~al.(2024)Abbas, Zhou, Ram, Baracaldo, Samulowitz, Salonidis, and Chen]{abbas2024enhancing}
M.~Abbas, Y.~Zhou, P.~Ram, N.~Baracaldo, H.~Samulowitz, T.~Salonidis, and T.~Chen.
\newblock Enhancing in-context learning via linear probe calibration.
\newblock In \emph{Artificial Intelligence and Statistics}, 2024.

\bibitem[Abernethy et~al.(2024)Abernethy, Agarwal, Marinov, and Warmuth]{abernethy2024a}
J.~Abernethy, A.~Agarwal, T.~V. Marinov, and M.~K. Warmuth.
\newblock A mechanism for sample-efficient in-context learning for sparse retrieval tasks.
\newblock In \emph{Algorithmic Learning Theory}, 2024.

\bibitem[Ahn et~al.(2024)Ahn, Cheng, Daneshmand, and Sra]{ahn2024transformers}
K.~Ahn, X.~Cheng, H.~Daneshmand, and S.~Sra.
\newblock Transformers learn to implement preconditioned gradient descent for in-context learning.
\newblock In \emph{Neural Information Processing Systems}, 2024.

\bibitem[Ahuja and Lopez-Paz(2023)]{ahuja2023a}
K.~Ahuja and D.~Lopez-Paz.
\newblock A closer look at in-context learning under distribution shifts.
\newblock In \emph{Workshop on Efficient Systems for Foundation Models at ICML}, 2023.

\bibitem[Aky{\"u}rek et~al.(2022)Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou]{akyurek2022learning}
E.~Aky{\"u}rek, D.~Schuurmans, J.~Andreas, T.~Ma, and D.~Zhou.
\newblock What learning algorithm is in-context learning? {I}nvestigations with linear models.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Aky{\"u}rek et~al.(2024)Aky{\"u}rek, Wang, Kim, and Andreas]{akyurek2024in}
E.~Aky{\"u}rek, B.~Wang, Y.~Kim, and J.~Andreas.
\newblock In-context language learning: {A}rchitectures and algorithms.
\newblock \emph{arXiv preprint arXiv:2401.12973}, 2024.

\bibitem[Bai et~al.(2023)Bai, Chen, Wang, Xiong, and Mei]{bai2023transformers}
Y.~Bai, F.~Chen, H.~Wang, C.~Xiong, and S.~Mei.
\newblock Transformers as statisticians: {P}rovable in-context learning with in-context algorithm selection.
\newblock In \emph{Neural Information Processing Systems}, 2023.

\bibitem[Bhattamishra et~al.(2023)Bhattamishra, Patel, Blunsom, and Kanade]{bhattamishra2023understanding}
S.~Bhattamishra, A.~Patel, P.~Blunsom, and V.~Kanade.
\newblock Understanding in-context learning in transformers and {LLM}s by learning to learn discrete functions.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Bietti et~al.(2023)Bietti, Cabannes, Bouchacourt, Jegou, and Bottou]{bietti2023birth}
A.~Bietti, V.~Cabannes, D.~Bouchacourt, H.~Jegou, and L.~Bottou.
\newblock Birth of a transformer: {A} memory viewpoint.
\newblock In \emph{Neural Information Processing Systems}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal, A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal, A.~Herbert-Voss, G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~Ziegler, J.~Wu, C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray, B.~Chess, J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and D.~Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{Neural Information Processing Systems}, 2020.

\bibitem[Chan et~al.(2022)Chan, Santoro, Lampinen, Wang, Singh, Richemond, McClelland, and Hill]{chan2022data}
S.~C. Chan, A.~Santoro, A.~K. Lampinen, J.~X. Wang, A.~K. Singh, P.~H. Richemond, J.~McClelland, and F.~Hill.
\newblock Data distributional properties drive emergent in-context learning in transformers.
\newblock In \emph{Neural Information Processing Systems}, 2022.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Sheen, Wang, and Yang]{chen2024training}
S.~Chen, H.~Sheen, T.~Wang, and Z.~Yang.
\newblock Training dynamics of multi-head softmax attention for in-context learning: {E}mergence, convergence, and optimality.
\newblock \emph{arXiv preprint arXiv:2402.19442}, 2024{\natexlab{a}}.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Zhao, Yu, McKeown, and He]{chen2024parallel}
Y.~Chen, C.~Zhao, Z.~Yu, K.~McKeown, and H.~He.
\newblock Parallel structures in pre-training data yield in-context learning.
\newblock \emph{arXiv preprint arXiv:2402.12530}, 2024{\natexlab{b}}.

\bibitem[Chiang and Yogatama(2024)]{chiang2024understanding}
T.-R. Chiang and D.~Yogatama.
\newblock Understanding in-context learning with a pelican soup framework.
\newblock \emph{arXiv preprint arXiv:2402.10424}, 2024.

\bibitem[Collins et~al.(2024)Collins, Parulekar, Mokhtari, Sanghavi, and Shakkottai]{collins2024in}
L.~Collins, A.~Parulekar, A.~Mokhtari, S.~Sanghavi, and S.~Shakkottai.
\newblock In-context learning with transformers: {S}oftmax attention adapts to function {L}ipschitzness.
\newblock \emph{arXiv preprint arXiv:2402.11639}, 2024.

\bibitem[Cui et~al.(2024)Cui, Ren, He, Tang, and Xing]{cui2024superiority}
Y.~Cui, J.~Ren, P.~He, J.~Tang, and Y.~Xing.
\newblock Superiority of multi-head attention in in-context linear regression.
\newblock \emph{arXiv preprint arXiv:2401.17426}, 2024.

\bibitem[Dai et~al.(2023)Dai, Sun, Dong, Hao, Sui, and Wei]{dai2023why}
D.~Dai, Y.~Sun, L.~Dong, Y.~Hao, Z.~Sui, and F.~Wei.
\newblock Why can {GPT} learn in-context? {L}anguage models secretly perform gradient descent as meta optimizers.
\newblock In \emph{Association for Computational Linguistics}, 2023.

\bibitem[Dalal and Misra(2024)]{dalal2024the}
S.~Dalal and V.~Misra.
\newblock The matrix: A {B}ayesian learning model for {LLM}s.
\newblock \emph{arXiv preprint arXiv:2402.03175}, 2024.

\bibitem[Ding et~al.(2024)Ding, Levinboim, Wu, Goodman, and Soricut]{ding2023causallm}
N.~Ding, T.~Levinboim, J.~Wu, S.~Goodman, and R.~Soricut.
\newblock Causal{LM} is not optimal for in-context learning.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Fu et~al.(2023)Fu, Chen, Jia, and Sharan]{fu2023transformers}
D.~Fu, T.-Q. Chen, R.~Jia, and V.~Sharan.
\newblock Transformers learn higher-order optimization methods for in-context learning: {A} study with linear models.
\newblock In \emph{Workshop on Mathematics of Modern Machine Learning at NeurIPS}, 2023.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022what}
S.~Garg, D.~Tsipras, P.~S. Liang, and G.~Valiant.
\newblock What can transformers learn in-context? {A} case study of simple function classes.
\newblock In \emph{Neural Information Processing Systems}, 2022.

\bibitem[Guo et~al.(2023)Guo, Hu, Mei, Wang, Xiong, Savarese, and Bai]{guo2023how}
T.~Guo, W.~Hu, S.~Mei, H.~Wang, C.~Xiong, S.~Savarese, and Y.~Bai.
\newblock How do transformers learn in-context beyond simple functions? {A} case study on learning with representations.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Hahn and Goyal(2023)]{hahn202a}
M.~Hahn and N.~Goyal.
\newblock A theory of emergent in-context learning as implicit structure induction.
\newblock \emph{arXiv preprint arXiv:2303.07971}, 2023.

\bibitem[Han et~al.(2023{\natexlab{a}})Han, Wang, Zhao, and Ji]{han2023explaining}
C.~Han, Z.~Wang, H.~Zhao, and H.~Ji.
\newblock Explaining emergent in-context learning as kernel regression.
\newblock \emph{arXiv preprint arXiv:2305.12766}, 2023{\natexlab{a}}.

\bibitem[Han et~al.(2023{\natexlab{b}})Han, Simig, Mihaylov, Tsvetkov, Celikyilmaz, and Wang]{han2023understanding}
X.~Han, D.~Simig, T.~Mihaylov, Y.~Tsvetkov, A.~Celikyilmaz, and T.~Wang.
\newblock Understanding in-context learning via supportive pretraining data.
\newblock In \emph{Association for Computational Linguistics}, 2023{\natexlab{b}}.

\bibitem[Haviv et~al.(2022)Haviv, Ram, Press, Izsak, and Levy]{haviv2022transformer}
A.~Haviv, O.~Ram, O.~Press, P.~Izsak, and O.~Levy.
\newblock Transformer language models without positional encodings still learn positional information.
\newblock In \emph{Empirical Methods in Natural Language Processing}, 2022.

\bibitem[Huang et~al.(2023)Huang, Cheng, and Liang]{huang2023in}
Y.~Huang, Y.~Cheng, and Y.~Liang.
\newblock In-context convergence of transformers.
\newblock In \emph{Workshop on Mathematics of Modern Machine Learning at NeurIPS}, 2023.

\bibitem[Jeon et~al.(2024)Jeon, Lee, Lei, and Van~Roy]{jeon2024an}
H.~J. Jeon, J.~D. Lee, Q.~Lei, and B.~Van~Roy.
\newblock An information-theoretic analysis of in-context learning.
\newblock \emph{arXiv preprint arXiv:2401.15530}, 2024.

\bibitem[Kingma and Ba(2015)]{kingma2015adam}
D.~Kingma and J.~Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kossen et~al.(2024)Kossen, Gal, and Rainforth]{kossen2023in}
J.~Kossen, Y.~Gal, and T.~Rainforth.
\newblock In-context learning learns label relationships but is not conventional learning.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Song, Xia, Yu, and Zhou]{li2023the}
S.~Li, Z.~Song, Y.~Xia, T.~Yu, and T.~Zhou.
\newblock The closeness of in-context learning and weight shifting for softmax regression.
\newblock \emph{arXiv preprint arXiv:2304.13276}, 2023{\natexlab{a}}.

\bibitem[Li and Qiu(2023)]{li2023finding}
X.~Li and X.~Qiu.
\newblock Finding support examples for in-context learning.
\newblock In \emph{Empirical Methods in Natural Language Processing}, 2023.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Ildiz, Papailiopoulos, and Oymak]{li2023transformers}
Y.~Li, M.~E. Ildiz, D.~Papailiopoulos, and S.~Oymak.
\newblock Transformers as algorithms: {G}eneralization and stability in in-context learning.
\newblock In \emph{International Conference on Machine Learning}, 2023{\natexlab{b}}.

\bibitem[Lin and Lee(2024)]{lin2024dual}
Z.~Lin and K.~Lee.
\newblock Dual operating modes of in-context learning.
\newblock \emph{arXiv preprint arXiv:2402.18819}, 2024.

\bibitem[Mahankali et~al.(2023)Mahankali, Hashimoto, and Ma]{mahankali2023one}
A.~V. Mahankali, T.~Hashimoto, and T.~Ma.
\newblock One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Mikolov et~al.(2013)Mikolov, Chen, Corrado, and Dean]{mikolov2013efficient}
T.~Mikolov, K.~Chen, G.~Corrado, and J.~Dean.
\newblock Efficient estimation of word representations in vector space.
\newblock \emph{arXiv preprint arXiv:1301.3781}, 2013.

\bibitem[Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer]{min2022rethinking}
S.~Min, X.~Lyu, A.~Holtzman, M.~Artetxe, M.~Lewis, H.~Hajishirzi, and L.~Zettlemoyer.
\newblock Rethinking the role of demonstrations: {W}hat makes in-context learning work?
\newblock In \emph{Empirical Methods in Natural Language Processing}, 2022.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen, Conerly, Drain, Ganguli, Hatfield-Dodds, Hernandez, Johnston, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan, McCandlish, and Olah]{olsson2022in}
C.~Olsson, N.~Elhage, N.~Nanda, N.~Joseph, N.~DasSarma, T.~Henighan, B.~Mann, A.~Askell, Y.~Bai, A.~Chen, T.~Conerly, D.~Drain, D.~Ganguli, Z.~Hatfield-Dodds, D.~Hernandez, S.~Johnston, A.~Jones, J.~Kernion, L.~Lovitt, K.~Ndousse, D.~Amodei, T.~Brown, J.~Clark, J.~Kaplan, S.~McCandlish, and C.~Olah.
\newblock In-context learning and induction heads.
\newblock \emph{Transformer Circuits Thread}, 2022.

\bibitem[Panwar et~al.(2023)Panwar, Ahuja, and Goyal]{ahuja2023in}
M.~Panwar, K.~Ahuja, and N.~Goyal.
\newblock In-context learning through the {B}ayesian prism.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Peng et~al.(2024)Peng, Ding, Yuan, Liu, Zhang, Ouyang, and Tao]{peng2024revisiting}
K.~Peng, L.~Ding, Y.~Yuan, X.~Liu, M.~Zhang, Y.~Ouyang, and D.~Tao.
\newblock Revisiting demonstration selection strategies in in-context learning.
\newblock \emph{arXiv preprint arXiv:2401.12087}, 2024.

\bibitem[Qin et~al.(2023)Qin, Zhang, Dagar, and Ye]{qin2023in}
C.~Qin, A.~Zhang, A.~Dagar, and W.~Ye.
\newblock In-context learning with iterative demonstration selection.
\newblock \emph{arXiv preprint arXiv:2310.09881}, 2023.

\bibitem[Raventós et~al.(2023)Raventós, Paul, Chen, and Ganguli]{raventos2023pretraining}
A.~Raventós, M.~Paul, F.~Chen, and S.~Ganguli.
\newblock Pretraining task diversity and the emergence of non-{B}ayesian in-context learning for regression.
\newblock In \emph{Neural Information Processing Systems}, 2023.

\bibitem[Ren et~al.(2024)Ren, Guo, Yan, Liu, Qiu, and Lin]{ren2024identifying}
J.~Ren, Q.~Guo, H.~Yan, D.~Liu, X.~Qiu, and D.~Lin.
\newblock Identifying semantic induction heads to understand in-context learning.
\newblock \emph{arXiv preprint arXiv:2402.13055}, 2024.

\bibitem[Ren and Liu(2023)]{ren2023in}
R.~Ren and Y.~Liu.
\newblock In-context learning with transformer is really equivalent to a contrastive learning pattern.
\newblock \emph{arXiv preprint arXiv:2310.13220}, 2023.

\bibitem[Sander et~al.(2024)Sander, Giryes, Suzuki, Blondel, and Peyr{\'e}]{sander2024how}
M.~E. Sander, R.~Giryes, T.~Suzuki, M.~Blondel, and G.~Peyr{\'e}.
\newblock How do transformers perform in-context autoregressive learning?
\newblock \emph{arXiv preprint arXiv:2402.05787}, 2024.

\bibitem[Shen et~al.(2023)Shen, Mishra, and Khashabi]{shen2023do}
L.~Shen, A.~Mishra, and D.~Khashabi.
\newblock Do pretrained transformers really learn in-context by gradient descent?
\newblock \emph{arXiv preprint arXiv:2310.08540}, 2023.

\bibitem[Singh et~al.(2023)Singh, Chan, Moskovitz, Grant, Saxe, and Hill]{singh2023the}
A.~Singh, S.~Chan, T.~Moskovitz, E.~Grant, A.~Saxe, and F.~Hill.
\newblock The transient nature of emergent in-context learning in transformers.
\newblock In \emph{Neural Information Processing Systems}, 2023.

\bibitem[Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu]{su2024roformer}
J.~Su, M.~Ahmed, Y.~Lu, S.~Pan, W.~Bo, and Y.~Liu.
\newblock Roformer: {E}nhanced transformer with rotary position embedding.
\newblock \emph{Neurocomputing}, 2024.

\bibitem[Swaminathan et~al.(2023)Swaminathan, Dedieu, Vasudeva~Raju, Shanahan, Lazaro-Gredilla, and George]{swaminathan2023schema}
S.~Swaminathan, A.~Dedieu, R.~Vasudeva~Raju, M.~Shanahan, M.~Lazaro-Gredilla, and D.~George.
\newblock Schema-learning and rebinding as mechanisms of in-context learning and emergence.
\newblock In \emph{Neural Information Processing Systems}, 2023.

\bibitem[Todd et~al.(2024)Todd, Li, Sharma, Mueller, Wallace, and Bau]{todd2024function}
E.~Todd, M.~Li, A.~Sharma, A.~Mueller, B.~C. Wallace, and D.~Bau.
\newblock Function vectors in large language models.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix, B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, et~al.
\newblock L{L}a{MA}: {O}pen and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Van et~al.(2024)Van, Wu, et~al.]{van2024in}
M.-H. Van, X.~Wu, et~al.
\newblock In-context learning demonstration selection via influence analysis.
\newblock \emph{arXiv preprint arXiv:2402.11750}, 2024.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, {\L}.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Neural Information Processing Systems}, 2017.

\bibitem[Vladymyrov et~al.(2024)Vladymyrov, von Oswald, Sandler, and Ge]{vladymyrov2024linear}
M.~Vladymyrov, J.~von Oswald, M.~Sandler, and R.~Ge.
\newblock Linear transformers are versatile in-context learners.
\newblock \emph{arXiv preprint arXiv:2402.14180}, 2024.

\bibitem[Von~Oswald et~al.(2023)Von~Oswald, Niklasson, Randazzo, Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov]{oswald2023transformers}
J.~Von~Oswald, E.~Niklasson, E.~Randazzo, J.~Sacramento, A.~Mordvintsev, A.~Zhmoginov, and M.~Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Wang et~al.(2023)Wang, Zhu, Saxon, Steyvers, and Wang]{wang2023large}
X.~Wang, W.~Zhu, M.~Saxon, M.~Steyvers, and W.~Y. Wang.
\newblock Large language models are latent variable models: {E}xplaining and finding good demonstrations for in-context learning.
\newblock In \emph{Neural Information Processing Systems}, 2023.

\bibitem[Wibisono and Wang(2023)]{wibisono2023on}
K.~C. Wibisono and Y.~Wang.
\newblock On the role of unstructured training data in transformers' in-context learning capabilities.
\newblock In \emph{Workshop on Mathematics of Modern Machine Learning at NeurIPS}, 2023.

\bibitem[Wies et~al.(2023)Wies, Levine, and Shashua]{wies2023the}
N.~Wies, Y.~Levine, and A.~Shashua.
\newblock The learnability of in-context learning.
\newblock In \emph{Neural Information Processing Systems}, 2023.

\bibitem[Wu et~al.(2023)Wu, Zou, Chen, Braverman, Gu, and Bartlett]{wu2023how}
J.~Wu, D.~Zou, Z.~Chen, V.~Braverman, Q.~Gu, and P.~Bartlett.
\newblock How many pretraining tasks are needed for in-context learning of linear regression?
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Xie et~al.(2021)Xie, Raghunathan, Liang, and Ma]{xie2021an}
S.~M. Xie, A.~Raghunathan, P.~Liang, and T.~Ma.
\newblock An explanation of in-context learning as implicit {B}ayesian inference.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Xing et~al.(2024)Xing, Lin, Suh, Song, and Cheng]{xing2024benefits}
Y.~Xing, X.~Lin, N.~Suh, Q.~Song, and G.~Cheng.
\newblock Benefits of transformer: {I}n-context learning in linear regression tasks with unstructured data.
\newblock \emph{arXiv preprint arXiv:2402.00743}, 2024.

\bibitem[Xu et~al.(2024)Xu, Shi, and Liang]{xu2024large}
Z.~Xu, Z.~Shi, and Y.~Liang.
\newblock Do large language models have compositional ability? {A}n investigation into limitations and scalability.
\newblock In \emph{Workshop on Mathematical and Empirical Understanding of Foundation Models at ICLR}, 2024.

\bibitem[Yadlowsky et~al.(2023)Yadlowsky, Doshi, and Tripuraneni]{yadlowsky2023pretraining}
S.~Yadlowsky, L.~Doshi, and N.~Tripuraneni.
\newblock Pretraining data mixtures enable narrow model selection capabilities in transformer models.
\newblock \emph{arXiv preprint arXiv:2311.00871}, 2023.

\bibitem[Yan et~al.(2023)Yan, Xu, Song, Wu, Li, and Zhang]{yan2023understanding}
J.~Yan, J.~Xu, C.~Song, C.~Wu, Y.~Li, and Y.~Zhang.
\newblock Understanding in-context learning from repetitions.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Yu and Ananiadou(2024)]{yu2024how}
Z.~Yu and S.~Ananiadou.
\newblock How do large language models learn in-context? {Q}uery and key matrices of in-context heads are two towers for metric learning.
\newblock \emph{arXiv preprint arXiv:2402.02872}, 2024.

\bibitem[Zhang et~al.(2024)Zhang, Frei, and Bartlett]{zhang2024trained}
R.~Zhang, S.~Frei, and P.~L. Bartlett.
\newblock Trained transformers learn linear models in-context.
\newblock \emph{Journal of Machine Learning Research}, 2024.

\bibitem[Zhang et~al.(2023)Zhang, Zhang, Yang, and Wang]{zhang2023what}
Y.~Zhang, F.~Zhang, Z.~Yang, and Z.~Wang.
\newblock What and how does in-context learning learn? {B}ayesian model averaging, parameterization, and generalization.
\newblock \emph{arXiv preprint arXiv:2305.19420}, 2023.

\bibitem[Zhao et~al.(2024)Zhao, Sakai, and Inoue]{zhao2024noisyicl}
Y.~Zhao, Y.~Sakai, and N.~Inoue.
\newblock Noisy{ICL}: {A} little noise in model parameters calibrates in-context learning.
\newblock \emph{arXiv preprint arXiv:2402.05515}, 2024.

\end{thebibliography}
