\begin{thebibliography}{10}

\bibitem{andreas2016neural}
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein.
\newblock Neural module networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 39--48, 2016.

\bibitem{bahdanau2018systematic}
Dzmitry Bahdanau, Shikhar Murty, Michael Noukhovitch, Thien~Huu Nguyen, Harm
  de~Vries, and Aaron Courville.
\newblock Systematic generalization: What is required and can it be learned?
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{BenIsrael1966OnIC}
A.~Ben-Israel and D.~Cohen.
\newblock On iterative computation of generalized inverses and associated
  projections.
\newblock {\em SIAM Journal on Numerical Analysis}, 3:410--419, 1966.

\bibitem{biederman1988surface}
Irving Biederman and Ginny Ju.
\newblock Surface versus edge-based determinants of visual recognition.
\newblock {\em Cognitive psychology}, 20(1):38--64, 1988.

\bibitem{Burda2016ImportanceWA}
Yuri Burda, Roger~B. Grosse, and R.~Salakhutdinov.
\newblock Importance weighted autoencoders.
\newblock {\em ICLR}, 2016.

\bibitem{chollet2019measure}
Fran{\c{c}}ois Chollet.
\newblock On the measure of intelligence.
\newblock {\em arXiv preprint arXiv:1911.01547}, 2019.

\bibitem{cohen2016group}
Taco Cohen and Max Welling.
\newblock Group equivariant convolutional networks.
\newblock In {\em International conference on machine learning}, pages
  2990--2999. PMLR, 2016.

\bibitem{csordas2021devil}
R{\'o}bert Csord{\'a}s, Kazuki Irie, and J{\"u}rgen Schmidhuber.
\newblock The devil is in the detail: Simple tricks improve systematic
  generalization of transformers.
\newblock {\em arXiv preprint arXiv:2108.12284}, 2021.

\bibitem{dinh2014nice}
Laurent Dinh, David Krueger, and Yoshua Bengio.
\newblock {NICE: Non-linear independent components estimation}.
\newblock {\em arXiv preprint arXiv:1410.8516}, 2014.

\bibitem{fedus2021switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity, 2021.

\bibitem{geirhos2018generalisation}
Robert Geirhos, Carlos~RM Temme, Jonas Rauber, Heiko~H Sch{\"u}tt, Matthias
  Bethge, and Felix~A Wichmann.
\newblock Generalisation in humans and deep neural networks.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{gentner2001analogical}
Dedre Gentner, Keith~J Holyoak, and Boicho~N Kokinov.
\newblock {\em The analogical mind: Perspectives from cognitive science}.
\newblock MIT press, 2001.

\bibitem{greff2020binding}
Klaus Greff, Sjoerd Van~Steenkiste, and J{\"u}rgen Schmidhuber.
\newblock On the binding problem in artificial neural networks.
\newblock {\em arXiv preprint arXiv:2012.05208}, 2020.

\bibitem{ha2016hypernetworks}
David Ha, Andrew Dai, and Quoc~V Le.
\newblock Hypernetworks.
\newblock {\em ICLR}, 2017.

\bibitem{kaiser2017learning}
{\L}ukasz Kaiser, Ofir Nachum, Aurko Roy, and Samy Bengio.
\newblock Learning to remember rare events.
\newblock {\em ICLR}, 2017.

\bibitem{kriete2013indirection}
Trenton Kriete, David~C Noelle, Jonathan~D Cohen, and Randall~C O'Reilly.
\newblock Indirection and symbol-like processing in the prefrontal cortex and
  basal ganglia.
\newblock {\em Proceedings of the National Academy of Sciences},
  110(41):16390--16395, 2013.

\bibitem{Krizhevsky2009LearningML}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{le2020neural}
Hung Le, Truyen Tran, and Svetha Venkatesh.
\newblock Neural stored-program memory.
\newblock In {\em ICLR 2020: Proceedings of the 8th International Conference on
  Learning Representations}, 2020.

\bibitem{malsburg1994correlation}
Christoph von~der Malsburg.
\newblock The correlation theory of brain function.
\newblock In {\em Models of neural networks}, pages 95--119. Springer, 1994.

\bibitem{marcus2001algebraic}
Gary Marcus.
\newblock The algebraic mind, 2001.

\bibitem{mcinnes2018umap}
Leland McInnes, John Healy, and James Melville.
\newblock Umap: Uniform manifold approximation and projection for dimension
  reduction.
\newblock {\em arXiv preprint arXiv:1802.03426}, 2018.

\bibitem{mitchell2021abstraction}
Melanie Mitchell.
\newblock Abstraction and analogy-making in artificial intelligence.
\newblock {\em Annals of the New York Academy of Sciences}, 1505(1):79--101,
  2021.

\bibitem{Munkhdalai2019MetalearnedNM}
Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler.
\newblock Metalearned neural memory.
\newblock In {\em NeurIPS}, 2019.

\bibitem{parascandolo2018learning}
Giambattista Parascandolo, Niki Kilbertus, Mateo Rojas-Carulla, and Bernhard
  Sch{\"o}lkopf.
\newblock Learning independent causal mechanisms.
\newblock In {\em International Conference on Machine Learning}, pages
  4036--4044. PMLR, 2018.

\bibitem{rahaman2021dynamic}
Nasim Rahaman, Muhammad~Waleed Gondal, Shruti Joshi, Peter Gehler, Yoshua
  Bengio, Francesco Locatello, and Bernhard Sch{\"o}lkopf.
\newblock Dynamic inference with neural interpreters.
\newblock {\em Advances in Neural Information Processing Systems},
  34:10985--10998, 2021.

\bibitem{rowe2012cognitive}
Ellen~W Rowe, Cristin Miller, Lauren~A Ebenstein, and Dawna~F Thompson.
\newblock Cognitive predictors of reading and math achievement among gifted
  referrals.
\newblock {\em School Psychology Quarterly}, 27(3):144, 2012.

\bibitem{santoro2016meta}
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy
  Lillicrap.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In {\em International conference on machine learning}, pages
  1842--1850. PMLR, 2016.

\bibitem{santoro2017simple}
Adam Santoro, David Raposo, David~G Barrett, Mateusz Malinowski, Razvan
  Pascanu, Peter Battaglia, and Timothy Lillicrap.
\newblock A simple neural network module for relational reasoning.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{shanahan2020explicitly}
Murray Shanahan, Kyriacos Nikiforou, Antonia Creswell, Christos Kaplanis, David
  Barrett, and Marta Garnelo.
\newblock An explicitly relational neural network architecture.
\newblock In {\em International Conference on Machine Learning}, pages
  8593--8603. PMLR, 2020.

\bibitem{Vaswani2017AttentionIA}
Ashish Vaswani, Noam~M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em NeurIPS}, abs/1706.03762, 2017.

\bibitem{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{webb2020learning}
Taylor Webb, Zachary Dulberg, Steven Frankland, Alexander Petrov, Randall
  O'Reilly, and Jonathan Cohen.
\newblock Learning representations that support extrapolation.
\newblock In {\em International conference on machine learning}, pages
  10136--10146. PMLR, 2020.

\bibitem{Webb2021EmergentST}
Taylor~Whittington Webb, Ishan Sinha, and Jonathan Cohen.
\newblock Emergent symbols through binding in external memory.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{zhang2019raven}
Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu.
\newblock Raven: A dataset for relational and analogical visual reasoning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2019.

\end{thebibliography}
