\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019)Allen-Zhu, Li, and Song]{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Arora et~al.(2020)Arora, Du, Li, Salakhutdinov, Wang, and
  Yu]{Arora2020Harnessing}
Sanjeev Arora, Simon~S. Du, Zhiyuan Li, Ruslan Salakhutdinov, Ruosong Wang, and
  Dingli Yu.
\newblock Harnessing the power of infinitely wide deep nets on small-data
  tasks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Bolte et~al.(2017)Bolte, Nguyen, Peypouquet, and
  Suter]{bolte2017error}
J{\'e}r{\^o}me Bolte, Trong~Phong Nguyen, Juan Peypouquet, and Bruce~W Suter.
\newblock From error bounds to the complexity of first-order descent methods
  for convex functions.
\newblock \emph{Mathematical Programming}, 165:\penalty0 471--507, 2017.

\bibitem[Brutzkus and Globerson(2017)]{brutzkus2017globally}
Alon Brutzkus and Amir Globerson.
\newblock Globally optimal gradient descent for a {ConvNet} with {Gaussian}
  inputs.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Chen et~al.(2021)Chen, Cao, Zou, and Gu]{chen2019much}
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu.
\newblock How much over-parameterization is sufficient to learn deep {ReLU}
  networks?
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Chi et~al.(2019)Chi, Lu, and Chen]{chi2018nonconvex}
Yuejie Chi, Yue~M Lu, and Yuxin Chen.
\newblock Nonconvex optimization meets low-rank matrix factorization: An
  overview.
\newblock \emph{{IEEE} Transactions on Signal Processing (TSP)}, 67:\penalty0
  5239--5269, 2019.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
L\'{e}na\"{i}c Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2019.

\bibitem[Clevert et~al.(2016)Clevert, Unterthiner, and
  Hochreiter]{clevert2015fast}
Djork-Arn{\'e} Clevert, Thomas Unterthiner, and Sepp Hochreiter.
\newblock Fast and accurate deep network learning by exponential linear units
  ({ELUs}).
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Daniely(2020)]{Daniely2020}
Amit Daniely.
\newblock Neural networks learning and memorization with (almost) no
  over-parameterization.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2020.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805v2}, 2019.

\bibitem[Du and Lee(2018)]{du2018power}
Simon~S. Du and Jason~D. Lee.
\newblock On the power of over-parametrization in neural networks with
  quadratic activation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Du et~al.(2019)Du, Zhai, Poczos, and Singh]{du2018gradient}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Dugas et~al.(2000)Dugas, Bengio, B\'{e}lisle, Nadeau, and
  Garcia]{Dugas2000}
Charles Dugas, Yoshua Bengio, Fran\c{c}ois B\'{e}lisle, Claude Nadeau, and
  Ren\'{e} Garcia.
\newblock Incorporating second-order functional knowledge for better option
  pricing.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2000.

\bibitem[Eftekhari(2020)]{Eftekhari2020linear}
Armin Eftekhari.
\newblock Training linear neural networks: Non-local convergence and complexity
  results.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Mei, Misiakiewicz, and
  Montanari]{ghorbani2019limitations}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Limitations of lazy training of two-layers neural networks.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2019.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani2017improved}
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron~C
  Courville.
\newblock Improved training of {Wasserstein} {GANs}.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2017.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{Heinit}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  {ImageNet} classification.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2015.

\bibitem[Hendrycks and Gimpel(2020)]{hendrycks2016gaussian}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units ({GELU}).
\newblock \emph{arXiv preprint arXiv:1606.08415v4}, 2020.

\bibitem[Hoeffding(1963)]{Hoeffding}
Wassily Hoeffding.
\newblock Probability inequalities for sums of bounded random variables.
\newblock \emph{Journal of American Statistical Association}, 58:\penalty0
  13--30, 1963.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl\'{e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2018.

\bibitem[Ji and Telgarsky(2020)]{ji2019polylogarithmic}
Ziwei Ji and Matus Telgarsky.
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow {ReLU} networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Kawaguchi and Huang(2019)]{kawaguchi2019gradient}
Kenji Kawaguchi and Jiaoyang Huang.
\newblock Gradient descent finds global minima for generalizable deep neural
  networks of practical sizes.
\newblock In \emph{Annual Allerton Conference on Communication, Control, and
  Computing}, 2019.

\bibitem[Kim et~al.(2018)Kim, Kim, and Kim]{kim2018memorization}
Youngjin Kim, Minjung Kim, and Gunhee Kim.
\newblock Memorization precedes generation: Learning unsupervised {GAN}s with
  memory networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Kumar et~al.(2017)Kumar, Sattigeri, and Fletcher]{Kumar2017}
Abhishek Kumar, Prasanna Sattigeri, and Tom Fletcher.
\newblock Semi-supervised learning with {GANs}: Manifold invariance with
  improved inference.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2017.

\bibitem[Latorre et~al.(2019)Latorre, Eftekhari, and Cevher]{FabianADMM}
Fabian Latorre, Armin Eftekhari, and Volkan Cevher.
\newblock Fast and provable {ADMM} for learning with generative priors.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2019.

\bibitem[Lecun et~al.(1998)Lecun, Bottou, Bengio, and Haffner]{MNIST}
Y.~Lecun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86:\penalty0 2278--2324, 1998.

\bibitem[LeCun et~al.(2012)LeCun, Bottou, Orr, and M\"{u}ller]{LeCuninit}
Yann~A. LeCun, L\'{e}on Bottou, Genevieve~B. Orr, and Klaus-Robert M\"{u}ller.
\newblock \emph{Efficient {BackProp.} In Neural networks: Tricks of the Trade}.
\newblock Springer, 2012.

\bibitem[Lee et~al.(2020)Lee, Schoenholz, Pennington, Adlam, Xiao, Novak, and
  Sohl-Dickstein]{lee2020finite}
Jaehoon Lee, Samuel~S. Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao,
  Roman Novak, and Jascha Sohl-Dickstein.
\newblock Finite versus infinite neural networks: an empirical study.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2020.

\bibitem[Li et~al.(2020)Li, Soltanolkotabi, and Oymak]{oymak2020noise}
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak.
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2020.

\bibitem[Li and Liang(2018)]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2018.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Zhu, and Belkin]{liu2020linearity}
Chaoyue Liu, Libin Zhu, and Mikhail Belkin.
\newblock On the linearity of large non-linear models: when and why the tangent
  kernel is constant.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Zhu, and Belkin]{liu2020loss}
Chaoyue Liu, Libin Zhu, and Mikhail Belkin.
\newblock Loss landscapes and optimization in over-parameterized non-linear
  systems and neural networks.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2020{\natexlab{b}}.

\bibitem[Lu et~al.(2020)Lu, Ma, Lu, Lu, and Ying]{MFT2020}
Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying.
\newblock A mean field analysis of deep resnet and beyond: Towards provably
  optimization via overparameterization from depth.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Mei et~al.(2019)Mei, Misiakiewicz, and Montanari]{MFT2019}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Mean-field theory of two-layers neural networks: dimension-free
  bounds and kernel limit.
\newblock In \emph{Conference on Learning Theory}, 2019.

\bibitem[Nguyen and Mondelli(2020)]{Pyr2020}
Quynh Nguyen and Marco Mondelli.
\newblock Global convergence of deep networks with one wide layer followed by
  pyramidal topology.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2020.

\bibitem[Nocedal and Wright(2006)]{nocedal2006numerical}
J.~Nocedal and S.~Wright.
\newblock \emph{Numerical Optimization}.
\newblock Springer New York, 2006.

\bibitem[Olver et~al.(2010)Olver, Lozier, Boisvert, and Clark]{olver2010nist}
Frank W.~J. Olver, Daniel~W. Lozier, Ronald~F. Boisvert, and Charles~W. Clark.
\newblock \emph{NIST Handbook of Mathematical Functions Paperback and
  {CD-ROM}}.
\newblock Cambridge University Press, 2010.

\bibitem[Oymak and Soltanolkotabi(2019)]{oymak2018overparameterized}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Overparameterized nonlinear learning: Gradient descent takes the
  shortest path?
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Oymak and Soltanolkotabi(2020)]{oymak2019towards}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Towards moderate overparameterization: global convergence guarantees
  for training shallow neural networks.
\newblock \emph{{IEEE} Journal on Selected Areas in Information Theory},
  1:\penalty0 84--105, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{torch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito,
  M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and
  S.~Chintala.
\newblock {PyTorch}: An imperative style, high-performance deep learning
  library.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Song and Yang(2019)]{song2019quadratic}
Zhao Song and Xin Yang.
\newblock Quadratic suffices for over-parametrization via matrix {Chernoff}
  bound.
\newblock \emph{arXiv preprint arXiv:1906.03593v2}, 2019.

\bibitem[Su and Yang(2019)]{Su2019approx}
Lili Su and Pengkun Yang.
\newblock On learning over-parameterized neural networks: A functional
  approximation perspective.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2019.

\bibitem[Vershynin(2012)]{vershynin2010introduction}
Roman Vershynin.
\newblock \emph{Introduction to the Non-asymptotic Analysis of Random
  Matrices}.
\newblock Cambridge University Press, 2012.

\bibitem[{Xu} et~al.(2020){Xu}, {Wang}, {Chen}, and {Li}]{Xu2015}
Bing {Xu}, Naiyan {Wang}, Tianqi {Chen}, and Mu~{Li}.
\newblock Empirical evaluation of rectified activations in convolutional
  network.
\newblock \emph{arXiv preprint arXiv:1505.00853v2}, 2020.

\bibitem[Yehudai and Shamir(2019)]{yehudai2019power}
Gilad Yehudai and Ohad Shamir.
\newblock On the power and limitations of random features for understanding
  neural networks.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Zou and Gu(2019)]{zou2019improved}
Difan Zou and Quanquan Gu.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In \emph{Advances in neural information processing systems
  (NeurIPS)}, 2019.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep {ReLU}
  networks.
\newblock \emph{arXiv preprint arXiv:1811.08888v3}, 2018.

\end{thebibliography}
