\begin{thebibliography}{71}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Gu et~al.(2019)Gu, Dolan-Gavitt, and Garg]{gu2017badnets}
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Badnets: Identifying vulnerabilities in the machine learning model
  supply chain.
\newblock \emph{IEEE Access}, 2019.

\bibitem[Jia et~al.(2022)Jia, Liu, and Gong]{jia2021badencoder}
Jinyuan Jia, Yupei Liu, and Neil~Zhenqiang Gong.
\newblock Badencoder: Backdoor attacks to pre-trained encoders in
  self-supervised learning.
\newblock \emph{2022 IEEE Symposium on Security and Privacy (SP)}, 2022.

\bibitem[Carlini and Terzis(2021)]{carlini2021poisoning}
Nicholas Carlini and Andreas Terzis.
\newblock Poisoning and backdooring contrastive learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Wang et~al.(2021)Wang, Javed, Wu, Guo, Xing, and
  Song]{wang2021backdoorl}
Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song.
\newblock Backdoorl: Backdoor attack against competitive reinforcement
  learning.
\newblock \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2021.

\bibitem[Bagdasaryan and
  Shmatikov(2021{\natexlab{a}})]{bagdasaryan2021spinning}
Eugene Bagdasaryan and Vitaly Shmatikov.
\newblock Spinning sequence-to-sequence models with meta-backdoors.
\newblock \emph{arXiv preprint arXiv:2107.10443}, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021)Chen, Salem, Chen, Backes, Ma, Shen, Wu, and
  Zhang]{chen2021badnl}
Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni
  Shen, Zhonghai Wu, and Yang Zhang.
\newblock Badnl: Backdoor attacks against nlp models with semantic-preserving
  improvements.
\newblock In \emph{Annual Computer Security Applications Conference}, pages
  554--569, 2021.

\bibitem[Xie et~al.(2019)Xie, Huang, Chen, and Li]{xie2019dba}
Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo~Li.
\newblock Dba: Distributed backdoor attacks against federated learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Liu et~al.(2019)Liu, Lee, Tao, Ma, Aafer, and Zhang]{liu2019abs}
Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu
  Zhang.
\newblock Abs: Scanning neural networks for back-doors by artificial brain
  stimulation.
\newblock In \emph{Proceedings of the 2019 ACM SIGSAC Conference on Computer
  and Communications Security}, pages 1265--1282, 2019.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Lyu, Koren, Lyu, Li, and
  Ma]{li2021anti}
Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo~Li, and Xingjun Ma.
\newblock Anti-backdoor learning: Training clean models on poisoned data.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Chen et~al.(2017)Chen, Liu, Li, Lu, and Song]{chen2017targeted}
Xinyun Chen, Chang Liu, Bo~Li, Kimberly Lu, and Dawn Song.
\newblock Targeted backdoor attacks on deep learning systems using data
  poisoning.
\newblock \emph{arXiv preprint arXiv:1712.05526}, 2017.

\bibitem[Turner et~al.(2019)Turner, Tsipras, and Madry]{turner2019label}
Alexander Turner, Dimitris Tsipras, and Aleksander Madry.
\newblock Label-consistent backdoor attacks.
\newblock \emph{arXiv preprint arXiv:1912.02771}, 2019.

\bibitem[Salem et~al.(2022)Salem, Wen, Backes, Ma, and Zhang]{salem2022dynamic}
Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang.
\newblock Dynamic backdoor attacks against machine learning models.
\newblock In \emph{2022 IEEE 7th European Symposium on Security and Privacy
  (EuroS\&P)}, pages 703--718. IEEE, 2022.

\bibitem[Nguyen and Tran(2020)]{nguyen2020input}
Anh Nguyen and Anh Tran.
\newblock Input-aware dynamic backdoor attack.
\newblock \emph{Advances in Neural Information Processing Systems 30.
  Pre-proceedings}, 2020.

\bibitem[Tang et~al.(2021)Tang, Wang, Tang, and Zhang]{tang2021demon}
Di~Tang, XiaoFeng Wang, Haixu Tang, and Kehuan Zhang.
\newblock Demon in the variant: Statistical analysis of dnns for robust
  backdoor contamination detection.
\newblock In \emph{30th USENIX Security Symposium (USENIX Security 21)}, 2021.

\bibitem[Saha et~al.(2022)Saha, Tejankar, Koohpayegani, and
  Pirsiavash]{saha2021backdoor}
Aniruddha Saha, Ajinkya Tejankar, Soroush~Abbasi Koohpayegani, and Hamed
  Pirsiavash.
\newblock Backdoor attacks on self-supervised learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 13337--13346, 2022.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Li, Lv, Jiang, and Xia]{li2021hidden}
Yiming Li, Yanjie Li, Yalei Lv, Yong Jiang, and Shu-Tao Xia.
\newblock Hidden backdoor attack against semantic segmentation models.
\newblock \emph{arXiv preprint arXiv:2103.04038}, 2021{\natexlab{b}}.

\bibitem[Cheng et~al.(2021)Cheng, Liu, Ma, and Zhang]{cheng2021deep}
Siyuan Cheng, Yingqi Liu, Shiqing Ma, and Xiangyu Zhang.
\newblock Deep feature space trojan attack of neural networks by controlled
  detoxification.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 1148--1156, 2021.

\bibitem[Wang et~al.(2022)Wang, Zhai, and Ma]{wang2022bppattack}
Zhenting Wang, Juan Zhai, and Shiqing Ma.
\newblock Bppattack: Stealthy and efficient trojan attacks against deep neural
  networks via image quantization and contrastive adversarial learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 15074--15084, 2022.

\bibitem[Lin et~al.(2014)Lin, Chen, and Yan]{lin2013network}
Min Lin, Qiang Chen, and Shuicheng Yan.
\newblock Network in network.
\newblock \emph{International Conference on Learning Representations}, 2014.

\bibitem[Zhao et~al.(2021)Zhao, Ma, Wang, Bailey, Li, and Jiang]{zhao2021deep}
Shihao Zhao, Xingjun Ma, Yisen Wang, James Bailey, Bo~Li, and Yu-Gang Jiang.
\newblock What do deep nets learn? class-wise patterns revealed in the input
  space.
\newblock \emph{arXiv preprint arXiv:2101.06898}, 2021.

\bibitem[Tran et~al.(2018)Tran, Li, and Madry]{tran2018spectral}
Brandon Tran, Jerry Li, and Aleksander Madry.
\newblock Spectral signatures in backdoor attacks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Chen et~al.(2019)Chen, Carvalho, Baracaldo, Ludwig, Edwards, Lee,
  Molloy, and Srivastava]{chen2018detecting}
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin
  Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava.
\newblock Detecting backdoor attacks on deep neural networks by activation
  clustering.
\newblock \emph{SafeAI@AAAI}, 2019.

\bibitem[Du et~al.(2020)Du, Jia, and Song]{du2019robust}
Min Du, Ruoxi Jia, and Dawn Song.
\newblock Robust anomaly detection and backdoor attack detection via
  differential privacy.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hong et~al.(2020)Hong, Chandrasekaran, Kaya, Dumitra{\c{s}}, and
  Papernot]{hong2020effectiveness}
Sanghyun Hong, Varun Chandrasekaran, Yi{\u{g}}itcan Kaya, Tudor Dumitra{\c{s}},
  and Nicolas Papernot.
\newblock On the effectiveness of mitigating data poisoning attacks with
  gradient shaping.
\newblock \emph{arXiv preprint arXiv:2002.11497}, 2020.

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar,
  and Zhang]{abadi2016deep}
Martin Abadi, Andy Chu, Ian Goodfellow, H~Brendan McMahan, Ilya Mironov, Kunal
  Talwar, and Li~Zhang.
\newblock Deep learning with differential privacy.
\newblock In \emph{Proceedings of the 2016 ACM SIGSAC conference on computer
  and communications security}, pages 308--318, 2016.

\bibitem[Wang et~al.(2019)Wang, Yao, Shan, Li, Viswanath, Zheng, and
  Zhao]{wang2019neural}
Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
  Zheng, and Ben~Y Zhao.
\newblock Neural cleanse: Identifying and mitigating backdoor attacks in neural
  networks.
\newblock In \emph{2019 IEEE Symposium on Security and Privacy (SP)}, pages
  707--723. IEEE, 2019.

\bibitem[Xu et~al.(2021)Xu, Wang, Li, Borisov, Gunter, and Li]{xu2019detecting}
Xiaojun Xu, Qi~Wang, Huichen Li, Nikita Borisov, Carl~A Gunter, and Bo~Li.
\newblock Detecting ai trojans using meta neural analysis.
\newblock In \emph{2021 IEEE Symposium on Security and Privacy (SP)}, pages
  103--120. IEEE, 2021.

\bibitem[Kolouri et~al.(2020)Kolouri, Saha, Pirsiavash, and
  Hoffmann]{kolouri2020universal}
Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann.
\newblock Universal litmus patterns: Revealing backdoor attacks in cnns.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 301--310, 2020.

\bibitem[Huang et~al.(2020)Huang, Peng, Jia, and Tu]{huang2020one}
Shanjiaoyang Huang, Weiqi Peng, Zhiwei Jia, and Zhuowen Tu.
\newblock One-pixel signature: Characterizing cnn models for backdoor
  detection.
\newblock In \emph{European Conference on Computer Vision}, pages 326--341.
  Springer, 2020.

\bibitem[Guo et~al.(2020)Guo, Wang, Xing, Du, and Song]{guo2019tabor}
Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, and Dawn Song.
\newblock Tabor: A highly accurate approach to inspecting and restoring trojan
  backdoors in ai systems.
\newblock \emph{IEEE International Conference on Data Mining (ICDM),}, 2020.

\bibitem[Shen et~al.(2021)Shen, Liu, Tao, An, Xu, Cheng, Ma, and
  Zhang]{shen2021backdoor}
Guangyu Shen, Yingqi Liu, Guanhong Tao, Shengwei An, Qiuling Xu, Siyuan Cheng,
  Shiqing Ma, and Xiangyu Zhang.
\newblock Backdoor scanning for deep neural networks through k-arm
  optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  9525--9536. PMLR, 2021.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Shen, Tao, Wang, Ma, and
  Zhang]{liu2022ex}
Yingqi Liu, Guangyu Shen, Guanhong Tao, Zhenting Wang, Shiqing Ma, and Xiangyu
  Zhang.
\newblock Complex backdoor detection by symmetric feature differencing.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 15003--15013, 2022{\natexlab{a}}.

\bibitem[Tao et~al.(2022)Tao, Shen, Liu, An, Xu, Ma, Li, and
  Zhang]{tao2022better}
Guanhong Tao, Guangyu Shen, Yingqi Liu, Shengwei An, Qiuling Xu, Shiqing Ma,
  Pan Li, and Xiangyu Zhang.
\newblock Better trigger inversion optimization in backdoor scanning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 13368--13378, 2022.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Shen, Tao, An, Ma, and
  Zhang]{liu2022piccolo}
Yingqi Liu, Guangyu Shen, Guanhong Tao, Shengwei An, Shiqing Ma, and Xiangyu
  Zhang.
\newblock Piccolo: Exposing complex backdoors in nlp transformer models.
\newblock In \emph{2022 IEEE Symposium on Security and Privacy (SP)}, pages
  1561--1561. IEEE Computer Society, 2022{\natexlab{b}}.

\bibitem[Shen et~al.(2022)Shen, Liu, Tao, Xu, Zhang, An, Ma, and
  Zhang]{shen2022constrained}
Guangyu Shen, Yingqi Liu, Guanhong Tao, Qiuling Xu, Zhuo Zhang, Shengwei An,
  Shiqing Ma, and Xiangyu Zhang.
\newblock Constrained optimization with dynamic bound-scaling for effective
  nlpbackdoor defense.
\newblock 2022.

\bibitem[Liu et~al.(2018)Liu, Dolan-Gavitt, and Garg]{liu2018fine}
Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Fine-pruning: Defending against backdooring attacks on deep neural
  networks.
\newblock In \emph{International Symposium on Research in Attacks, Intrusions,
  and Defenses}, pages 273--294. Springer, 2018.

\bibitem[Zhao et~al.(2020)Zhao, Chen, Das, Ramamurthy, and
  Lin]{zhao2020bridging}
Pu~Zhao, Pin-Yu Chen, Payel Das, Karthikeyan~Natesan Ramamurthy, and Xue Lin.
\newblock Bridging mode connectivity in loss landscapes and adversarial
  robustness.
\newblock \emph{arXiv preprint arXiv:2005.00060}, 2020.

\bibitem[Li et~al.(2021{\natexlab{c}})Li, Koren, Lyu, Lyu, Li, and
  Ma]{li2021neural}
Yige Li, Nodens Koren, Lingjuan Lyu, Xixiang Lyu, Bo~Li, and Xingjun Ma.
\newblock Neural attention distillation: Erasing backdoor triggers from deep
  neural networks.
\newblock \emph{International Conference on Learning Representations},
  2021{\natexlab{c}}.

\bibitem[Wu and Wang(2021)]{wu2021adversarial}
Dongxian Wu and Yisen Wang.
\newblock Adversarial neuron pruning purifies backdoored deep models.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zeng et~al.(2022)Zeng, Chen, Park, Mao, Jin, and
  Jia]{zeng2021adversarial}
Yi~Zeng, Si~Chen, Won Park, Zhuoqing Mao, Ming Jin, and Ruoxi Jia.
\newblock Adversarial unlearning of backdoors via implicit hypergradient.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Gao et~al.(2019)Gao, Xu, Wang, Chen, Ranasinghe, and
  Nepal]{gao2019strip}
Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith~C Ranasinghe, and
  Surya Nepal.
\newblock Strip: A defence against trojan attacks on deep neural networks.
\newblock In \emph{Proceedings of the 35th Annual Computer Security
  Applications Conference}, pages 113--125, 2019.

\bibitem[Chou et~al.(2020)Chou, Tram{\`e}r, and Pellegrino]{chou2020sentinet}
Edward Chou, Florian Tram{\`e}r, and Giancarlo Pellegrino.
\newblock Sentinet: Detecting localized universal attacks against deep learning
  systems.
\newblock In \emph{2020 IEEE Security and Privacy Workshops (SPW)}, pages
  48--54. IEEE, 2020.

\bibitem[Ma and Liu(2019)]{ma2019nic}
Shiqing Ma and Yingqi Liu.
\newblock Nic: Detecting adversarial samples with neural network invariant
  checking.
\newblock In \emph{Proceedings of the 26th network and distributed system
  security symposium (NDSS 2019)}, 2019.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{International Conference on Learning Representations}, 2015.

\bibitem[Tang et~al.(2020)Tang, Du, Liu, Yang, and Hu]{tang2020embarrassingly}
Ruixiang Tang, Mengnan Du, Ninghao Liu, Fan Yang, and Xia Hu.
\newblock An embarrassingly simple approach for trojan attack in deep neural
  networks.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 218--228, 2020.

\bibitem[Otsu(1979)]{otsu1979threshold}
Nobuyuki Otsu.
\newblock A threshold selection method from gray-level histograms.
\newblock \emph{IEEE transactions on systems, man, and cybernetics}, 9\penalty0
  (1):\penalty0 62--66, 1979.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Stallkamp et~al.(2012)Stallkamp, Schlipsing, Salmen, and
  Igel]{stallkamp2012man}
Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel.
\newblock Man vs. computer: Benchmarking machine learning algorithms for
  traffic sign recognition.
\newblock \emph{Neural networks}, 32:\penalty0 323--332, 2012.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems},
  25:\penalty0 1097--1105, 2012.

\bibitem[Karra et~al.(2020)Karra, Ashcraft, and Fendley]{karra2020trojai}
Kiran Karra, Chace Ashcraft, and Neil Fendley.
\newblock The trojai software framework: An opensource tool for embedding
  trojans into deep learning models.
\newblock \emph{arXiv preprint arXiv:2003.07233}, 2020.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Veldanda et~al.(2020)Veldanda, Liu, Tan, Krishnamurthy, Khorrami,
  Karri, Dolan-Gavitt, and Garg]{veldanda2020nnoculation}
Akshaj~Kumar Veldanda, Kang Liu, Benjamin Tan, Prashanth Krishnamurthy, Farshad
  Khorrami, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg.
\newblock Nnoculation: broad spectrum and targeted treatment of backdoored
  dnns.
\newblock \emph{arXiv preprint arXiv:2002.08313}, 2020.

\bibitem[Doan et~al.(2020)Doan, Abbasnejad, and Ranasinghe]{doan2020februus}
Bao~Gia Doan, Ehsan Abbasnejad, and Damith~C Ranasinghe.
\newblock Februus: Input purification defense against trojan attacks on deep
  neural network systems.
\newblock In \emph{Annual Computer Security Applications Conference}, pages
  897--912, 2020.

\bibitem[Saha et~al.(2020)Saha, Subramanya, and Pirsiavash]{saha2020hidden}
Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash.
\newblock Hidden trigger backdoor attacks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 11957--11965, 2020.

\bibitem[Bagdasaryan and Shmatikov(2021{\natexlab{b}})]{bagdasaryan2021blind}
Eugene Bagdasaryan and Vitaly Shmatikov.
\newblock Blind backdoors in deep learning models.
\newblock In \emph{30th USENIX Security Symposium (USENIX Security 21)}, pages
  1505--1521, 2021{\natexlab{b}}.

\bibitem[Huang et~al.(2022)Huang, Li, Wu, Qin, and Ren]{huang2022backdoor}
Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren.
\newblock Backdoor defense via decoupling the training process.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Bai et~al.(2021)Bai, Wu, Zhang, Li, Li, and Xia]{bai2021targeted}
Jiawang Bai, Baoyuan Wu, Yong Zhang, Yiming Li, Zhifeng Li, and Shu-Tao Xia.
\newblock Targeted attack against deep neural networks via flipping limited
  weight bits.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Nair and Hinton(2010)]{nair2010rectified}
Vinod Nair and Geoffrey~E Hinton.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In \emph{Icml}, 2010.

\bibitem[Maas et~al.(2013)Maas, Hannun, and Ng]{maas2013rectifier}
Andrew~L Maas, Awni~Y Hannun, and Andrew~Y Ng.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In \emph{Proc. icml}, volume~30, page~3. Citeseer, 2013.

\bibitem[Clevert et~al.(2015)Clevert, Unterthiner, and
  Hochreiter]{clevert2015fast}
Djork-Arn{\'e} Clevert, Thomas Unterthiner, and Sepp Hochreiter.
\newblock Fast and accurate deep network learning by exponential linear units
  (elus).
\newblock \emph{arXiv preprint arXiv:1511.07289}, 2015.

\bibitem[Tan()]{Tanhshrink}
Tanhshrink.
\newblock
  \url{https://pytorch.org/docs/stable/generated/torch.nn.Tanhshrink.html}.

\bibitem[Zheng et~al.(2015)Zheng, Yang, Liu, Liang, and Li]{zheng2015improving}
Hao Zheng, Zhanlei Yang, Wenju Liu, Jizhong Liang, and Yanpeng Li.
\newblock Improving deep neural networks using softplus units.
\newblock In \emph{2015 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--4. IEEE, 2015.

\bibitem[Tursynbek et~al.(2020)Tursynbek, Petiushko, and
  Oseledets]{tursynbek2020robustness}
Nurislam Tursynbek, Aleksandr Petiushko, and Ivan Oseledets.
\newblock Robustness threats of differential privacy.
\newblock \emph{arXiv preprint arXiv:2012.07828}, 2020.

\bibitem[Moosavi-Dezfooli et~al.(2019)Moosavi-Dezfooli, Fawzi, Uesato, and
  Frossard]{moosavi2019robustness}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan Uesato, and Pascal
  Frossard.
\newblock Robustness via curvature regularization, and vice versa.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 9078--9086, 2019.

\bibitem[Geiger et~al.(2013)Geiger, Lenz, Stiller, and
  Urtasun]{geiger2013vision}
Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun.
\newblock Vision meets robotics: The kitti dataset.
\newblock \emph{The International Journal of Robotics Research}, 32\penalty0
  (11):\penalty0 1231--1237, 2013.

\bibitem[Nguyen and Tran(2021)]{nguyen2021wanet}
Tuan~Anh Nguyen and Anh~Tuan Tran.
\newblock Wanet-imperceptible warping-based backdoor attack.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Barni et~al.(2019)Barni, Kallas, and Tondi]{barni2019new}
Mauro Barni, Kassem Kallas, and Benedetta Tondi.
\newblock A new backdoor attack in cnns by training set corruption without
  label poisoning.
\newblock In \emph{2019 IEEE International Conference on Image Processing
  (ICIP)}, pages 101--105. IEEE, 2019.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In \emph{British Machine Vision Conference 2016}. British Machine
  Vision Association, 2016.

\bibitem[Li et~al.(2021{\natexlab{d}})Li, Li, Wu, Li, He, and
  Lyu]{li2021invisible}
Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu.
\newblock Invisible backdoor attack with sample-specific triggers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 16463--16472, 2021{\natexlab{d}}.

\end{thebibliography}
