\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{WDVR06}

\bibitem[AJK{\etalchar{+}}22]{acharya2022robust}
Jayadev Acharya, Ayush Jain, Gautam Kamath, Ananda~Theertha Suresh, and Huanyu
  Zhang.
\newblock Robust estimation for random graphs.
\newblock In {\em Conference on Learning Theory}, pages 130--166. PMLR, 2022.

\bibitem[BDLS17]{BDLS17}
S.~Balakrishnan, S.~S. Du, J.~Li, and A.~Singh.
\newblock Computationally efficient robust sparse estimation in high
  dimensions.
\newblock In {\em Proceedings of the 30th Conference on Learning Theory, {COLT}
  2017}, pages 169--212, 2017.

\bibitem[BJK15]{bhatia2015robust}
Kush Bhatia, Prateek Jain, and Purushottam Kar.
\newblock Robust regression via hard thresholding.
\newblock {\em Advances in neural information processing systems}, 28, 2015.

\bibitem[BJKK17]{BhatiaJKK17}
K.~Bhatia, P.~Jain, P.~Kamalaruban, and P.~Kar.
\newblock Consistent robust regression.
\newblock In {\em Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017}, pages 2107--2116,
  2017.

\bibitem[CAT{\etalchar{+}}20]{cherapanamjeri2020optimal}
Yeshwanth Cherapanamjeri, Efe Aras, Nilesh Tripuraneni, Michael~I Jordan,
  Nicolas Flammarion, and Peter~L Bartlett.
\newblock Optimal robust linear regression in nearly linear time.
\newblock {\em arXiv preprint arXiv:2007.08137}, 2020.

\bibitem[CL13]{CL13}
Arun~Tejasvi Chaganty and Percy Liang.
\newblock Spectral experts for estimating mixtures of linear regressions.
\newblock In {\em International Conference on Machine Learning (ICML)}, pages
  1040--1048, 2013.

\bibitem[CLM20]{chen2020learning}
Sitan Chen, Jerry Li, and Ankur Moitra.
\newblock Learning structured distributions from untrusted batches: Faster and
  simpler.
\newblock {\em Advances in Neural Information Processing Systems},
  33:4512--4523, 2020.

\bibitem[CLS20]{chen2019learning}
Sitan Chen, Jerry Li, and Zhao Song.
\newblock Learning mixtures of linear regressions in subexponential time via
  {F}ourier moments.
\newblock In {\em STOC}. \url{https://arxiv.org/pdf/1912.07629.pdf}, 2020.

\bibitem[CP22]{chen22t}
Yanxi Chen and H.~Vincent Poor.
\newblock Learning mixtures of linear dynamical systems.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, {\em Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of {\em Proceedings
  of Machine Learning Research}, pages 3507--3557. PMLR, 17--23 Jul 2022.

\bibitem[CSV17]{charikar2017learning}
Moses Charikar, Jacob Steinhardt, and Gregory Valiant.
\newblock Learning from untrusted data.
\newblock In {\em Proceedings of the 49th Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 47--60, 2017.

\bibitem[DJKS22]{das2022efficient}
Abhimanyu Das, Ayush Jain, Weihao Kong, and Rajat Sen.
\newblock Efficient list-decodable regression using batches.
\newblock {\em arXiv preprint arXiv:2211.12743}, 2022.

\bibitem[DK20]{diakonikolas2020small}
Ilias Diakonikolas and Daniel~M Kane.
\newblock Small covers for near-zero sets of polynomials and learning latent
  variable models.
\newblock In {\em 2020 IEEE 61st Annual Symposium on Foundations of Computer
  Science (FOCS)}, pages 184--195. IEEE, 2020.

\bibitem[DKK{\etalchar{+}}19]{DiakonikolasKKLSS19}
Ilias Diakonikolas, Gautam Kamath, Daniel~M. Kane, Jerry Li, Jacob Steinhardt,
  and Alistair Stewart.
\newblock Sever: A robust meta-algorithm for stochastic optimization.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, ICML '19, pages 1596--1606. JMLR, Inc., 2019.

\bibitem[DKP{\etalchar{+}}21]{diakonikolas2021statistical}
Ilias Diakonikolas, Daniel Kane, Ankit Pensia, Thanasis Pittas, and Alistair
  Stewart.
\newblock Statistical query lower bounds for list-decodable linear regression.
\newblock {\em Advances in Neural Information Processing Systems},
  34:3191--3204, 2021.

\bibitem[DKS19]{diakonikolas2019efficient}
Ilias Diakonikolas, Weihao Kong, and Alistair Stewart.
\newblock Efficient algorithms and lower bounds for robust linear regression.
\newblock In {\em Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 2745--2754. SIAM, 2019.

\bibitem[DT19]{dalalyan2019outlier}
Arnak Dalalyan and Philip Thompson.
\newblock Outlier-robust estimation of a sparse linear model using $\ell_1$
  -penalized huber's m-estimator.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem[FAL17]{FAL17}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, pages 1126--1135, 2017.

\bibitem[Gao20]{gao2020robust}
Chao Gao.
\newblock Robust regression via mutivariate regression depth.
\newblock {\em Bernoulli}, 26(2):1139--1170, 2020.

\bibitem[GKG15]{grottke2015distribution}
Michael Grottke, Julian Knoll, and Rainer Gro{\ss}.
\newblock How the distribution of the number of items rated per user influences
  the quality of recommendations.
\newblock In {\em 2015 15th International Conference on Innovations for
  Community Services (I4CS)}, pages 1--8. IEEE, 2015.

\bibitem[JLST21]{jambulapati2021robust}
Arun Jambulapati, Jerry Li, Tselil Schramm, and Kevin Tian.
\newblock Robust regression revisited: Acceleration and improved estimation
  rates.
\newblock {\em Advances in Neural Information Processing Systems},
  34:4475--4488, 2021.

\bibitem[JO20a]{JainO20b}
Ayush Jain and Alon Orlitsky.
\newblock A general method for robust learning from batches.
\newblock {\em arXiv preprint arXiv:2002.11099}, 2020.

\bibitem[JO20b]{JainO20a}
Ayush Jain and Alon Orlitsky.
\newblock Optimal robust learning of discrete distributions from batches.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, ICML '20, pages 4651--4660. JMLR, Inc., 2020.

\bibitem[JO21]{JainO21}
Ayush Jain and Alon Orlitsky.
\newblock Robust density estimation from batches: The best things in life are
  (nearly) free.
\newblock In {\em International Conference on Machine Learning}, pages
  4698--4708. PMLR, 2021.

\bibitem[KFAL20]{konstantinov2020sample}
Nikola Konstantinov, Elias Frantar, Dan Alistarh, and Christoph Lampert.
\newblock On the sample complexity of adversarial multi-source pac learning.
\newblock In {\em International Conference on Machine Learning}, pages
  5416--5425. PMLR, 2020.

\bibitem[KKK19]{karmalkar2019list}
Sushrut Karmalkar, Adam Klivans, and Pravesh Kothari.
\newblock List-decodable linear regression.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem[KKM18]{klivans2018efficient}
Adam Klivans, Pravesh~K Kothari, and Raghu Meka.
\newblock Efficient algorithms for outlier-robust regression.
\newblock In {\em Conference On Learning Theory}, pages 1420--1430. PMLR, 2018.

\bibitem[KP19]{karmalkar2019compressed}
Sushrut Karmalkar and Eric Price.
\newblock Compressed sensing with adversarial sparse noise via l1 regression.
\newblock In {\em 2nd Symposium on Simplicity in Algorithms}, 2019.

\bibitem[KSAD22]{kong2022trimmed}
Weihao Kong, Rajat Sen, Pranjal Awasthi, and Abhimanyu Das.
\newblock Trimmed maximum likelihood estimation for robust learning in
  generalized linear models.
\newblock {\em arXiv preprint arXiv:2206.04777}, 2022.

\bibitem[KSKO20]{kong2020robust}
Weihao Kong, Raghav Somani, Sham Kakade, and Sewoong Oh.
\newblock Robust meta-learning for mixed linear regression with small batches.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[KSS{\etalchar{+}}20]{kong2020meta}
Weihao Kong, Raghav Somani, Zhao Song, Sham Kakade, and Sewoong Oh.
\newblock Meta-learning for mixed linear regression.
\newblock In {\em International Conference on Machine Learning}, pages
  5394--5404. PMLR, 2020.

\bibitem[KZS15]{KZS15}
Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov.
\newblock Siamese neural networks for one-shot image recognition.
\newblock In {\em ICML deep learning workshop}, volume~2, 2015.

\bibitem[LL18]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning mixtures of linear regressions with nearly optimal
  complexity.
\newblock In {\em COLT}. arXiv preprint arXiv:1802.07895, 2018.

\bibitem[LSLC18]{liu2018high}
Liu Liu, Yanyao Shen, Tianyang Li, and Constantine Caramanis.
\newblock High dimensional robust sparse regression.
\newblock {\em arXiv preprint arXiv:1805.11643}, 2018.

\bibitem[MGJK19]{mukhoty2019globally}
Bhaskar Mukhoty, Govind Gopakumar, Prateek Jain, and Purushottam Kar.
\newblock Globally-convergent iteratively reweighted least squares for robust
  regression problems.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 313--322, 2019.

\bibitem[OLL18]{OLL18}
Boris Oreshkin, Pau~Rodr{\'\i}guez L{\'o}pez, and Alexandre Lacoste.
\newblock Tadam: Task dependent adaptive metric for improved few-shot learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  721--731, 2018.

\bibitem[PJL20]{pensia2020robust}
Ankit Pensia, Varun Jog, and Po-Ling Loh.
\newblock Robust regression with covariate filtering: Heavy tails and
  adversarial contamination.
\newblock {\em arXiv preprint arXiv:2009.12976}, 2020.

\bibitem[PMSG22]{pal2022learning}
Soumyabrata Pal, Arya Mazumdar, Rajat Sen, and Avishek Ghosh.
\newblock On learning mixture of linear regressions in the non-realizable
  setting.
\newblock In {\em International Conference on Machine Learning}, pages
  17202--17220. PMLR, 2022.

\bibitem[PSBR18]{prasad2018robust}
Adarsh Prasad, Arun~Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar.
\newblock Robust estimation via robust gradient estimation.
\newblock {\em arXiv preprint arXiv:1802.06485}, 2018.

\bibitem[PT08]{park2008long}
Yoon-Joo Park and Alexander Tuzhilin.
\newblock The long tail of recommender systems and how to leverage it.
\newblock In {\em Proceedings of the 2008 ACM conference on Recommender
  systems}, pages 11--18, 2008.

\bibitem[QV18]{QiaoV18}
Mingda Qiao and Gregory Valiant.
\newblock Learning discrete distributions from untrusted batches.
\newblock In {\em Proceedings of the 9th Conference on Innovations in
  Theoretical Computer Science}, ITCS '18, pages 47:1--47:20, Dagstuhl,
  Germany, 2018. Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik.

\bibitem[RL17]{RL16}
Sachin Ravi and Hugo Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock In {\em International Conference on Representation Learning}, 2017.

\bibitem[RRS{\etalchar{+}}18]{rusu2018meta}
Andrei~A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu,
  Simon Osindero, and Raia Hadsell.
\newblock Meta-learning with latent embedding optimization.
\newblock {\em arXiv preprint arXiv:1807.05960}, 2018.

\bibitem[RY20]{raghavendra2020list}
Prasad Raghavendra and Morris Yau.
\newblock List decodable learning via sum of squares.
\newblock In {\em Proceedings of the Fourteenth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 161--180. SIAM, 2020.

\bibitem[Sch87]{Sch87}
J{\"u}rgen Schmidhuber.
\newblock {\em Evolutionary principles in self-referential learning, or on
  learning how to learn: the meta-meta-... hook}.
\newblock PhD thesis, Technische Universit{\"a}t M{\"u}nchen, 1987.

\bibitem[SJA16]{sedghi2016provable}
Hanie Sedghi, Majid Janzamin, and Anima Anandkumar.
\newblock Provable tensor methods for learning mixtures of generalized linear
  models.
\newblock In {\em Artificial Intelligence and Statistics (AISTATS)}, pages
  1223--1231, 2016.

\bibitem[SVC16]{steinhardt2016avoiding}
Jacob Steinhardt, Gregory Valiant, and Moses Charikar.
\newblock Avoiding imposters and delinquents: Adversarial crowdsourcing and
  peer prediction.
\newblock {\em Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[TLW99]{ting1999learning}
Kai~Ming Ting, Boon~Toh Low, and Ian~H Witten.
\newblock Learning from batched data: Model combination versus data
  combination.
\newblock {\em Knowledge and Information Systems}, 1:83--106, 1999.

\bibitem[TP12]{TP12}
Sebastian Thrun and Lorien Pratt.
\newblock {\em Learning to learn}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[TZD{\etalchar{+}}19]{triantafillou2019meta}
Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Kelvin Xu,
  Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, and
  Hugo Larochelle.
\newblock Meta-dataset: A dataset of datasets for learning to learn from few
  examples.
\newblock {\em arXiv preprint arXiv:1903.03096}, 2019.

\bibitem[WCX{\etalchar{+}}21]{wang2021field}
Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H~Brendan McMahan, Maruan
  Al-Shedivat, Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data,
  et~al.
\newblock A field guide to federated optimization.
\newblock {\em arXiv preprint arXiv:2107.06917}, 2021.

\bibitem[WDVR06]{wang2006unifying}
Jun Wang, Arjen~P De~Vries, and Marcel~JT Reinders.
\newblock Unifying user-based and item-based collaborative filtering approaches
  by similarity fusion.
\newblock In {\em Proceedings of the 29th annual international ACM SIGIR
  conference on Research and development in information retrieval}, pages
  501--508, 2006.

\bibitem[WZ89]{wax1989unique}
Mati Wax and Ilan Ziskind.
\newblock On unique localization of multiple sources by passive sensor arrays.
\newblock {\em IEEE Transactions on Acoustics, Speech, and Signal Processing},
  37(7):996--1000, 1989.

\bibitem[YCS14]{yi2014alternating}
Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi.
\newblock Alternating minimization for mixed linear regression.
\newblock In {\em International Conference on Machine Learning}, pages
  613--621. PMLR, 2014.

\bibitem[YCS16]{yi2016solving}
Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi.
\newblock Solving a mixture of many random linear equations by tensor
  decomposition and alternating minimization.
\newblock {\em arXiv preprint arXiv:1608.05749}, 2016.

\bibitem[ZJD16]{zhong2016mixed}
Kai Zhong, Prateek Jain, and Inderjit~S Dhillon.
\newblock Mixed linear regression with multiple components.
\newblock In {\em Advances in neural information processing systems (NIPS)},
  pages 2190--2198, 2016.

\end{thebibliography}
