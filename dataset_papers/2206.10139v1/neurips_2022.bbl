\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson \& Farrell(2022)Anderson and Farrell]{fractal}
Connor Anderson and Ryan Farrell.
\newblock Improving fractal pre-training.
\newblock In \emph{{IEEE/CVF} Winter Conference on Applications of Computer
  Vision, {WACV} 2022, Waikoloa, HI, USA, January 3-8, 2022}, pp.\  2412--2421.
  {IEEE}, 2022.
\newblock \doi{10.1109/WACV51458.2022.00247}.
\newblock URL \url{https://doi.org/10.1109/WACV51458.2022.00247}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{brown2020gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Carlini et~al.(2020)Carlini, Tram{\`{e}}r, Wallace, Jagielski,
  Herbert{-}Voss, Lee, Roberts, Brown, Song, Erlingsson, Oprea, and
  Raffel]{DBLP:journals/corr/abs-2012-07805}
Nicholas Carlini, Florian Tram{\`{e}}r, Eric Wallace, Matthew Jagielski, Ariel
  Herbert{-}Voss, Katherine Lee, Adam Roberts, Tom~B. Brown, Dawn Song,
  {\'{U}}lfar Erlingsson, Alina Oprea, and Colin Raffel.
\newblock Extracting training data from large language models.
\newblock \emph{CoRR}, abs/2012.07805, 2020.
\newblock URL \url{https://arxiv.org/abs/2012.07805}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan,
  Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry,
  Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet,
  Such, Cummings, Plappert, Chantzis, Barnes, Herbert{-}Voss, Guss, Nichol,
  Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike,
  Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder,
  McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{chen2021codex}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique~Ponde
  de~Oliveira~Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas
  Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,
  Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick
  Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,
  Clemens Winter, Philippe Tillet, Felipe~Petroski Such, Dave Cummings,
  Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert{-}Voss,
  William~Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor
  Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher
  Hesse, Andrew~N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa,
  Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter
  Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
  Wojciech Zaremba.
\newblock Evaluating large language models trained on code.
\newblock \emph{CoRR}, abs/2107.03374, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.03374}.

\bibitem[Chi et~al.(2020)Chi, Hewitt, and Manning]{probe_cross_lang_1}
Ethan~A. Chi, John Hewitt, and Christopher~D. Manning.
\newblock Finding universal grammatical relations in multilingual {BERT}.
\newblock In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel~R. Tetreault
  (eds.), \emph{Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics, {ACL} 2020, Online, July 5-10, 2020}, pp.\
  5564--5577. Association for Computational Linguistics, 2020.
\newblock \doi{10.18653/v1/2020.acl-main.493}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.acl-main.493}.

\bibitem[Chiang \& Lee(2020)Chiang and Lee]{chiang_old}
David~Cheng{-}Han Chiang and Hung{-}yi Lee.
\newblock Pre-training a language model without human language.
\newblock \emph{CoRR}, abs/2012.11995, 2020.
\newblock URL \url{https://arxiv.org/abs/2012.11995}.

\bibitem[Chiang \& Lee(2021)Chiang and Lee]{chiang}
David~Cheng{-}Han Chiang and Hung{-}yi Lee.
\newblock On the transferability of pre-trained language models: {A} study from
  artificial datasets.
\newblock \emph{CoRR}, abs/2109.03537, 2021.
\newblock URL \url{https://arxiv.org/abs/2109.03537}.

\bibitem[Conneau et~al.(2020)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek,
  Guzm{\'{a}}n, Grave, Ott, Zettlemoyer, and
  Stoyanov]{understand_cross_lang_trans_2}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
  Wenzek, Francisco Guzm{\'{a}}n, Edouard Grave, Myle Ott, Luke Zettlemoyer,
  and Veselin Stoyanov.
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel~R. Tetreault
  (eds.), \emph{Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics, {ACL} 2020, Online, July 5-10, 2020}, pp.\
  8440--8451. Association for Computational Linguistics, 2020.
\newblock \doi{10.18653/v1/2020.acl-main.747}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.acl-main.747}.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{Devlin2019bert}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{ACL}, 2019.

\bibitem[Dufter \& Sch{\"{u}}tze(2020)Dufter and
  Sch{\"{u}}tze]{understand_cross_lang_trans_3}
Philipp Dufter and Hinrich Sch{\"{u}}tze.
\newblock Identifying elements essential for bert's multilinguality.
\newblock In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.),
  \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural
  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020}, pp.\
  4423--4437. Association for Computational Linguistics, 2020.
\newblock \doi{10.18653/v1/2020.emnlp-main.358}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.emnlp-main.358}.

\bibitem[Guo et~al.(2020)Guo, Dai, Vrande{\v{c}}i{\'c}, and Al-Rfou]{wikipedia}
Mandy Guo, Zihang Dai, Denny Vrande{\v{c}}i{\'c}, and Rami Al-Rfou.
\newblock {W}iki-40{B}: Multilingual language model dataset.
\newblock In \emph{Proceedings of the 12th Language Resources and Evaluation
  Conference}, pp.\  2440--2452, Marseille, France, May 2020. European Language
  Resources Association.
\newblock ISBN 979-10-95546-34-4.
\newblock URL \url{https://aclanthology.org/2020.lrec-1.297}.

\bibitem[Huang et~al.(2020)Huang, P{\'{e}}rez, Ba, and Volkovs]{init_3}
Xiao~Shi Huang, Felipe P{\'{e}}rez, Jimmy Ba, and Maksims Volkovs.
\newblock Improving transformer optimization through better initialization.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  4475--4483. {PMLR},
  2020.
\newblock URL \url{http://proceedings.mlr.press/v119/huang20f.html}.

\bibitem[K et~al.(2020)K, Wang, Mayhew, and
  Roth]{understand_cross_lang_trans_1}
Karthikeyan K, Zihan Wang, Stephen Mayhew, and Dan Roth.
\newblock Cross-lingual ability of multilingual {BERT:} an empirical study.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.
\newblock URL \url{https://openreview.net/forum?id=HJeT3yrtDr}.

\bibitem[Krishna et~al.(2021)Krishna, Bigham, and Lipton]{nonsense_summary}
Kundan Krishna, Jeffrey~P. Bigham, and Zachary~C. Lipton.
\newblock Does pretraining for summarization require knowledge transfer?
\newblock In \emph{EMNLP (Findings)}, pp.\  3178--3189, 2021.
\newblock URL \url{https://aclanthology.org/2021.findings-emnlp.273}.

\bibitem[Li et~al.(2021)Li, Arora, Chen, Gupta, Gupta, and Mehdad]{mtop}
Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar
  Mehdad.
\newblock {MTOP}: A comprehensive multilingual task-oriented semantic parsing
  benchmark.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter
  of the Association for Computational Linguistics: Main Volume}, pp.\
  2950--2962, Online, April 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.eacl-main.257}.
\newblock URL \url{https://aclanthology.org/2021.eacl-main.257}.

\bibitem[Liu et~al.(2017)Liu, Ramsundar, Kawthekar, Shi, Gomes, Luu~Nguyen, Ho,
  Sloane, Wender, and Pande]{uspto-50K}
Bowen Liu, Bharath Ramsundar, Prasad Kawthekar, Jade Shi, Joseph Gomes, Quang
  Luu~Nguyen, Stephen Ho, Jack Sloane, Paul Wender, and Vijay Pande.
\newblock Retrosynthetic reaction prediction using neural sequence-to-sequence
  models.
\newblock \emph{ACS Central Science}, 3\penalty0 (10):\penalty0 1103--1113,
  2017.
\newblock \doi{10.1021/acscentsci.7b00303}.
\newblock URL \url{https://doi.org/10.1021/acscentsci.7b00303}.
\newblock PMID: 29104927.

\bibitem[Liu et~al.(2020)Liu, Gu, Goyal, Li, Edunov, Ghazvininejad, Lewis, and
  Zettlemoyer]{cross_lang_transfer}
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan
  Ghazvininejad, Mike Lewis, and Luke Zettlemoyer.
\newblock Multilingual denoising pre-training for neural machine translation.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 8:\penalty0 726--742, 2020.
\newblock URL \url{https://transacl.org/ojs/index.php/tacl/article/view/2107}.

\bibitem[Lu \& Zhang(2022)Lu and Zhang]{chemistry_t5}
Jieyu Lu and Yingkai Zhang.
\newblock Unified deep learning model for multitask reaction predictions with
  explanation.
\newblock \emph{Journal of Chemical Information and Modeling}, 62\penalty0
  (6):\penalty0 1376--1387, 2022.
\newblock \doi{10.1021/acs.jcim.1c01467}.
\newblock URL \url{https://doi.org/10.1021/acs.jcim.1c01467}.
\newblock PMID: 35266390.

\bibitem[Lu et~al.(2021)Lu, Guo, Ren, Huang, Svyatkovskiy, Blanco, Clement,
  Drain, Jiang, Tang, Li, Zhou, Shou, Zhou, Tufano, Gong, Zhou, Duan,
  Sundaresan, Deng, Fu, and Liu]{codexglue}
Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio
  Blanco, Colin~B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge~Li, Lidong
  Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan Duan,
  Neel Sundaresan, Shao~Kun Deng, Shengyu Fu, and Shujie Liu.
\newblock Codexglue: {A} machine learning benchmark dataset for code
  understanding and generation.
\newblock In Joaquin Vanschoren and Sai{-}Kit Yeung (eds.), \emph{Proceedings
  of the Neural Information Processing Systems Track on Datasets and Benchmarks
  1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual}, 2021.
\newblock URL
  \url{https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c16a5320fa475530d9583c34fd356ef5-Abstract-round1.html}.

\bibitem[Papadimitriou \& Jurafsky(2020)Papadimitriou and Jurafsky]{isabel}
Isabel Papadimitriou and Dan Jurafsky.
\newblock Learning music helps you read: Using transfer to study linguistic
  structure in language models.
\newblock In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.),
  \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural
  Language Processing, {EMNLP} 2020, Online, November 16-20, 2020}, pp.\
  6829--6839. Association for Computational Linguistics, 2020.
\newblock \doi{10.18653/v1/2020.emnlp-main.554}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.emnlp-main.554}.

\bibitem[Papadimitriou et~al.(2021)Papadimitriou, Chi, Futrell, and
  Mahowald]{probe_cross_lang_2}
Isabel Papadimitriou, Ethan~A. Chi, Richard Futrell, and Kyle Mahowald.
\newblock Deep subjecthood: Higher-order grammatical features in multilingual
  {BERT}.
\newblock In Paola Merlo, J{\"{o}}rg Tiedemann, and Reut Tsarfaty (eds.),
  \emph{Proceedings of the 16th Conference of the European Chapter of the
  Association for Computational Linguistics: Main Volume, {EACL} 2021, Online,
  April 19 - 23, 2021}, pp.\  2522--2532. Association for Computational
  Linguistics, 2021.
\newblock \doi{10.18653/v1/2021.eacl-main.215}.
\newblock URL \url{https://doi.org/10.18653/v1/2021.eacl-main.215}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21:\penalty0 140:1--140:67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100, 000+ questions for machine comprehension of text.
\newblock \emph{CoRR}, abs/1606.05250, 2016.
\newblock URL \url{http://arxiv.org/abs/1606.05250}.

\bibitem[Reid et~al.(2022)Reid, Yamada, and Gu]{wiki_rl}
Machel Reid, Yutaro Yamada, and Shixiang~Shane Gu.
\newblock Can wikipedia help offline reinforcement learning?
\newblock \emph{CoRR}, abs/2201.12122, 2022.
\newblock URL \url{https://arxiv.org/abs/2201.12122}.

\bibitem[Ri \& Tsuruoka(2022)Ri and Tsuruoka]{ri}
Ryokan Ri and Yoshimasa Tsuruoka.
\newblock Pretraining with artificial language: Studying transferable knowledge
  in language models.
\newblock \emph{CoRR}, abs/2203.10326, 2022.
\newblock \doi{10.48550/arXiv.2203.10326}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2203.10326}.

\bibitem[See et~al.(2017)See, Liu, and Manning]{cnndm}
Abigail See, Peter~J. Liu, and Christopher~D. Manning.
\newblock Get to the point: Summarization with pointer-generator networks.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1073--1083,
  Vancouver, Canada, July 2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P17-1099}.
\newblock URL \url{https://aclanthology.org/P17-1099}.

\bibitem[Sinha et~al.(2021)Sinha, Jia, Hupkes, Pineau, Williams, and
  Kiela]{word_order}
Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and
  Douwe Kiela.
\newblock Masked language modeling and the distributional hypothesis: Order
  word matters pre-training for little.
\newblock In Marie{-}Francine Moens, Xuanjing Huang, Lucia Specia, and
  Scott~Wen{-}tau Yih (eds.), \emph{Proceedings of the 2021 Conference on
  Empirical Methods in Natural Language Processing, {EMNLP} 2021, Virtual Event
  / Punta Cana, Dominican Republic, 7-11 November, 2021}, pp.\  2888--2913.
  Association for Computational Linguistics, 2021.
\newblock \doi{10.18653/v1/2021.emnlp-main.230}.
\newblock URL \url{https://doi.org/10.18653/v1/2021.emnlp-main.230}.

\bibitem[Wang et~al.(2022)Wang, Ma, Dong, Huang, Zhang, and Wei]{init_1}
Hongyu Wang, Shuming Ma, Li~Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.
\newblock Deepnet: Scaling transformers to 1, 000 layers.
\newblock \emph{CoRR}, abs/2203.00555, 2022.
\newblock \doi{10.48550/arXiv.2203.00555}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2203.00555}.

\bibitem[Wu et~al.(2021)Wu, Rabe, Li, Ba, Grosse, and Szegedy]{lime}
Yuhuai Wu, Markus~N Rabe, Wenda Li, Jimmy Ba, Roger~B Grosse, and Christian
  Szegedy.
\newblock Lime: Learning inductive bias for primitives of mathematical
  reasoning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11251--11262. PMLR, 2021.

\bibitem[Xie et~al.(2022)Xie, Wu, Shi, Zhong, Scholak, Yasunaga, Wu, Zhong,
  Yin, Wang, Zhong, Wang, Li, Boyle, Ni, Yao, Radev, Xiong, Kong, Zhang, Smith,
  Zettlemoyer, and Yu]{unifiedskg}
Tianbao Xie, Chen~Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro
  Yasunaga, Chien{-}Sheng Wu, Ming Zhong, Pengcheng Yin, Sida~I. Wang, Victor
  Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao,
  Dragomir~R. Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah~A. Smith,
  Luke Zettlemoyer, and Tao Yu.
\newblock Unifiedskg: Unifying and multi-tasking structured knowledge grounding
  with text-to-text language models.
\newblock \emph{CoRR}, abs/2201.05966, 2022.
\newblock URL \url{https://arxiv.org/abs/2201.05966}.

\bibitem[Yih et~al.(2016)Yih, Richardson, Meek, Chang, and Suh]{webqsp}
Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang, and Jina Suh.
\newblock The value of semantic parse labeling for knowledge base question
  answering.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 2: Short Papers)}, pp.\  201--206,
  Berlin, Germany, August 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P16-2033}.
\newblock URL \url{https://aclanthology.org/P16-2033}.

\bibitem[Zhang et~al.(2019)Zhang, Titov, and Sennrich]{init_4}
Biao Zhang, Ivan Titov, and Rico Sennrich.
\newblock Improving deep transformer with depth-scaled initialization and
  merged attention.
\newblock In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.),
  \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural
  Language Processing and the 9th International Joint Conference on Natural
  Language Processing, {EMNLP-IJCNLP} 2019, Hong Kong, China, November 3-7,
  2019}, pp.\  898--909. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/v1/D19-1083}.
\newblock URL \url{https://doi.org/10.18653/v1/D19-1083}.

\bibitem[Zhang et~al.(2020)Zhang, Williams, Titov, and Sennrich]{init_2}
Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich.
\newblock Improving massively multilingual neural machine translation and
  zero-shot translation.
\newblock In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel~R. Tetreault
  (eds.), \emph{Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics, {ACL} 2020, Online, July 5-10, 2020}, pp.\
  1628--1639. Association for Computational Linguistics, 2020.
\newblock \doi{10.18653/v1/2020.acl-main.148}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.acl-main.148}.

\end{thebibliography}
