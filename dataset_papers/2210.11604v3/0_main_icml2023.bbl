\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Auer et~al.(2008)Auer, Jaksch, and Ortner]{paper:ucrl2}
Auer, P., Jaksch, T., and Ortner, R.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 21, 2008.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{paper:ucbvi}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{ICML}, 2017.

\bibitem[Bartlett \& Tewari(2012)Bartlett and Tewari]{paper:regal}
Bartlett, P.~L. and Tewari, A.
\newblock Regal: A regularization based algorithm for reinforcement learning in
  weakly communicating mdps.
\newblock \emph{arXiv preprint arXiv:1205.2661}, 2012.

\bibitem[Brunskill \& Li(2013)Brunskill and Li]{paper:multitask_rl}
Brunskill, E. and Li, L.
\newblock Sample complexity of multi-task reinforcement learning.
\newblock \emph{ArXiv}, abs/1309.6821, 2013.

\bibitem[Chades et~al.(2012)Chades, Carwardine, Martin, Nicol, Sabbadin, and
  Buffet]{paper:momdp_plan}
Chades, I., Carwardine, J., Martin, T.~G., Nicol, S., Sabbadin, R., and Buffet,
  O.
\newblock Momdps: A solution for modelling adaptive management problems.
\newblock In \emph{AAAI}, 2012.

\bibitem[Chen et~al.(2021)Chen, Jafarnia-Jahromi, Jain, and Luo]{paper:lcb_ssp}
Chen, L., Jafarnia-Jahromi, M., Jain, R., and Luo, H.
\newblock Implicit finite-horizon approximation and efficient optimal
  algorithms for stochastic shortest path.
\newblock volume~34, 2021.

\bibitem[Cohen et~al.(2020)Cohen, Kaplan, Mansour, and
  Rosenberg]{paper:bernstein_ssp}
Cohen, A., Kaplan, H., Mansour, Y., and Rosenberg, A.
\newblock Near-optimal regret bounds for stochastic shortest path.
\newblock In \emph{ICML}, 2020.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{paper:ubev}
Dann, C., Lattimore, T., and Brunskill, E.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{NIPS}, 2017.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{paper:orlc}
Dann, C., Li, L., Wei, W., and Brunskill, E.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1507--1516. PMLR, 2019.

\bibitem[Domingues et~al.(2021)Domingues, M{\'e}nard, Kaufmann, and
  Valko]{paper:episodic_lower_bound}
Domingues, O.~D., M{\'e}nard, P., Kaufmann, E., and Valko, M.
\newblock Episodic reinforcement learning in finite mdps: Minimax lower bounds
  revisited.
\newblock In Feldman, V., Ligett, K., and Sabato, S. (eds.), \emph{Proceedings
  of the 32nd International Conference on Algorithmic Learning Theory}, volume
  132 of \emph{Proceedings of Machine Learning Research}, pp.\  578--598. PMLR,
  16--19 Mar 2021.
\newblock URL \url{https://proceedings.mlr.press/v132/domingues21a.html}.

\bibitem[Doshi-Velez \& Konidaris(2016)Doshi-Velez and
  Konidaris]{paper:hidden_parameter_mdp}
Doshi-Velez, F. and Konidaris, G.
\newblock Hidden parameter markov decision processes: A semiparametric
  regression approach for discovering latent task parametrizations.
\newblock In \emph{IJCAI: proceedings of the conference}, volume 2016, pp.\
  1432. NIH Public Access, 2016.

\bibitem[Filippi et~al.(2010)Filippi, Capp{\'e}, and Garivier]{paper:kl_ucrl}
Filippi, S., Capp{\'e}, O., and Garivier, A.
\newblock Optimism in reinforcement learning and kullback-leibler divergence.
\newblock In \emph{2010 48th Annual Allerton Conference on Communication,
  Control, and Computing (Allerton)}, pp.\  115--122. IEEE, 2010.

\bibitem[Finn et~al.(2018)Finn, Xu, and Levine]{paper:prob_meta_learning}
Finn, C., Xu, K., and Levine, S.
\newblock Probabilistic model-agnostic meta-learning.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Fischer et~al.(2017)Fischer, Gershtein, and
  Oshman]{paper:communication_complexity_triangle}
Fischer, O., Gershtein, S., and Oshman, R.
\newblock On the multiparty communication complexity of testing
  triangle-freeness.
\newblock In \emph{Proceedings of the ACM Symposium on Principles of
  Distributed Computing}, pp.\  111--120, 2017.

\bibitem[Fruit et~al.(2018)Fruit, Pirotta, Lazaric, and Ortner]{paper:scal}
Fruit, R., Pirotta, M., Lazaric, A., and Ortner, R.
\newblock Efficient bias-span-constrained exploration-exploitation in
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1578--1586. PMLR, 2018.

\bibitem[Fruit et~al.(2020)Fruit, Pirotta, and Lazaric]{paper:ucrl2b}
Fruit, R., Pirotta, M., and Lazaric, A.
\newblock Improved analysis of ucrl2 with empirical bernstein inequality.
\newblock \emph{arXiv preprint arXiv:2007.05456}, 2020.

\bibitem[Hallak et~al.(2015)Hallak, Castro, and Mannor]{paper:contextual_mdp}
Hallak, A., Castro, D.~D., and Mannor, S.
\newblock Contextual markov decision processes.
\newblock \emph{ArXiv}, abs/1502.02259, 2015.

\bibitem[Iakovleva et~al.(2020)Iakovleva, Verbeek, and
  Alahari]{paper:meta_learning_vi}
Iakovleva, E., Verbeek, J., and Alahari, K.
\newblock Meta-learning with shared amortized variational inference.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4572--4582. PMLR, 2020.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{paper:cdp_low_rank}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1704--1713. PMLR, 2017.

\bibitem[Jin et~al.(2020)Jin, Krishnamurthy, Simchowitz, and
  Yu]{paper:reward_free_exploration}
Jin, C., Krishnamurthy, A., Simchowitz, M., and Yu, T.
\newblock Reward-free exploration for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4870--4879. PMLR, 2020.

\bibitem[Kim et~al.(2021)Kim, Yang, and Jun]{kim2021improved}
Kim, Y., Yang, I., and Jun, K.-S.
\newblock Improved regret analysis for variance-adaptive linear bandits and
  horizon-free linear mixture mdps.
\newblock \emph{arXiv preprint arXiv:2111.03289}, 2021.

\bibitem[Kwon et~al.(2021)Kwon, Efroni, Caramanis, and
  Mannor]{paper:latent_mdp}
Kwon, J., Efroni, Y., Caramanis, C., and Mannor, S.
\newblock Rl for latent mdps: Regret guarantees and a lower bound, 2021.

\bibitem[Lattimore \& Hutter(2012)Lattimore and Hutter]{paper:two_degree_mdp}
Lattimore, T. and Hutter, M.
\newblock Pac bounds for discounted mdps.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pp.\  320--334. Springer, 2012.

\bibitem[Lazaric(2012)]{paper:transfer}
Lazaric, A.
\newblock Transfer in reinforcement learning: A framework and a survey.
\newblock In \emph{Reinforcement Learning}, 2012.

\bibitem[Li \& Sun(2023)Li and Sun]{li2023variance}
Li, X. and Sun, Q.
\newblock Variance-aware robust reinforcement learning with linear function
  approximation with heavy-tailed rewards.
\newblock \emph{arXiv preprint arXiv:2303.05606}, 2023.

\bibitem[Littman(1994)]{paper:memless}
Littman, M.~L.
\newblock Memoryless policies: Theoretical limitations and practical results.
\newblock In \emph{From Animals to Animats 3: Proceedings of the third
  international conference on simulation of adaptive behavior}, volume~3, pp.\
  238. Cambridge, MA, 1994.

\bibitem[Liu et~al.(2016)Liu, Guo, and
  Brunskill]{paper:cont_state_multitask_rl}
Liu, Y., Guo, Z.~D., and Brunskill, E.
\newblock Pac continuous state online multitask reinforcement learning with
  identification.
\newblock In \emph{AAMAS}, 2016.

\bibitem[Maillard et~al.(2014)Maillard, Mann, and
  Mannor]{paper:distribution_norm}
Maillard, O.-A., Mann, T.~A., and Mannor, S.
\newblock How hard is my mdp?" the distribution-norm to the rescue".
\newblock \emph{Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[Maurer \& Pontil(2009)Maurer and Pontil]{paper:bennett}
Maurer, A. and Pontil, M.
\newblock Empirical bernstein bounds and sample-variance penalization.
\newblock In \emph{COLT}, 2009.

\bibitem[Neu \& Pike-Burke(2020)Neu and Pike-Burke]{paper:mdp_optimism}
Neu, G. and Pike-Burke, C.
\newblock A unifying view of optimism in episodic reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1392--1403, 2020.

\bibitem[Phillips et~al.(2012)Phillips, Verbin, and
  Zhang]{paper:lb_communication_complexity_symm}
Phillips, J.~M., Verbin, E., and Zhang, Q.
\newblock Lower bounds for number-in-hand multiparty communication complexity,
  made easy.
\newblock In \emph{Proceedings of the twenty-third annual ACM-SIAM symposium on
  Discrete Algorithms}, pp.\  486--501. SIAM, 2012.

\bibitem[Ramamoorthy et~al.(2013)Ramamoorthy, Mahmud, Rosman, and
  Kohli]{paper:latent_variable_mdp}
Ramamoorthy, S., Mahmud, M.~H., Rosman, B., and Kohli, P.
\newblock Latent-variable mdp models for adapting the interaction environment
  of diverse users, 2013.

\bibitem[Ren et~al.(2021)Ren, Li, Dai, Du, and Sanghavi]{ren2021nearly}
Ren, T., Li, J., Dai, B., Du, S.~S., and Sanghavi, S.
\newblock Nearly horizon-free offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 15621--15634, 2021.

\bibitem[Simchowitz \& Jamieson(2019)Simchowitz and
  Jamieson]{paper:non_asymptotic_gap}
Simchowitz, M. and Jamieson, K.~G.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Steimle et~al.(2021)Steimle, Kaufman, and
  Denton]{paper:multimodel_mdp}
Steimle, L.~N., Kaufman, D.~L., and Denton, B.~T.
\newblock Multi-model markov decision processes.
\newblock \emph{IISE Transactions}, 53\penalty0 (10):\penalty0 1124--1139,
  2021.

\bibitem[Talebi \& Maillard(2018)Talebi and
  Maillard]{paper:variance_aware_kl_ucrl}
Talebi, M.~S. and Maillard, O.-A.
\newblock Variance-aware regret bounds for undiscounted reinforcement learning
  in mdps.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  770--805. PMLR, 2018.

\bibitem[Tarbouriech et~al.(2021)Tarbouriech, Zhou, Du, Pirotta, Valko, and
  Lazaric]{paper:eb_ssp}
Tarbouriech, J., Zhou, R., Du, S.~S., Pirotta, M., Valko, M., and Lazaric, A.
\newblock Stochastic shortest path: Minimax, parameter-free and towards
  horizon-free regret.
\newblock volume~34, 2021.

\bibitem[Taylor \& Stone(2009)Taylor and Stone]{paper:transfer_learning_survey}
Taylor, M.~E. and Stone, P.
\newblock Transfer learning for reinforcement learning domains: A survey.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0 (7), 2009.

\bibitem[Vempala et~al.(2020)Vempala, Wang, and
  Woodruff]{paper:communication_complexity_opt}
Vempala, S.~S., Wang, R., and Woodruff, D.~P.
\newblock The communication complexity of optimization.
\newblock In \emph{Proceedings of the Fourteenth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pp.\  1733--1752. SIAM, 2020.

\bibitem[Wagenmaker et~al.(2022)Wagenmaker, Chen, Simchowitz, Du, and
  Jamieson]{paper:first_order_lfa}
Wagenmaker, A.~J., Chen, Y., Simchowitz, M., Du, S., and Jamieson, K.
\newblock First-order regret in reinforcement learning with linear function
  approximation: A robust estimation approach.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  22384--22429. PMLR, 2022.

\bibitem[Wang et~al.(2020)Wang, Du, Yang, and Kakade]{wang2020long}
Wang, R., Du, S.~S., Yang, L.~F., and Kakade, S.~M.
\newblock Is long horizon reinforcement learning more difficult than short
  horizon reinforcement learning?
\newblock \emph{arXiv preprint arXiv:2005.00527}, 2020.

\bibitem[Woodruff \& Zhang(2014)Woodruff and Zhang]{paper:lb_message_passing}
Woodruff, D.~P. and Zhang, Q.
\newblock An optimal lower bound for distinct elements in the message passing
  model.
\newblock In \emph{Proceedings of the twenty-fifth annual ACM-SIAM symposium on
  Discrete algorithms}, pp.\  718--733. SIAM, 2014.

\bibitem[Yao et~al.(2018)Yao, Killian, Konidaris, and
  Doshi-Velez]{paper:policy_transfer_hidden_parameter_mdp}
Yao, J., Killian, T., Konidaris, G., and Doshi-Velez, F.
\newblock Direct policy transfer via hidden parameter markov decision
  processes.
\newblock In \emph{LLARLA Workshop, FAIM}, volume 2018, 2018.

\bibitem[Yu et~al.(2020)Yu, Quillen, He, Julian, Hausman, Finn, and
  Levine]{paper:meta_world}
Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In \emph{Conference on robot learning}, pp.\  1094--1100. PMLR, 2020.

\bibitem[Zanette \& Brunskill(2019)Zanette and Brunskill]{paper:euler}
Zanette, A. and Brunskill, E.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{ICML}, 2019.

\bibitem[Zhang \& Wang(2021)Zhang and Wang]{paper:multitask_transfer}
Zhang, C. and Wang, Z.
\newblock Provably efficient multi-task reinforcement learning with model
  transfer.
\newblock \emph{ArXiv}, abs/2107.08622, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Du, and Ji]{zhang2020nearly}
Zhang, Z., Du, S.~S., and Ji, X.
\newblock Nearly minimax optimal reward-free reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.05901}, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Ji, and Du]{paper:mvp}
Zhang, Z., Ji, X., and Du, S.~S.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock In \emph{COLT}, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Yang, Ji, and
  Du]{zhang2021improved}
Zhang, Z., Yang, J., Ji, X., and Du, S.~S.
\newblock Improved variance-aware confidence sets for linear bandits and linear
  mixture mdp.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 4342--4355, 2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2022)Zhang, Ji, and Du]{paper:horizon_free}
Zhang, Z., Ji, X., and Du, S.~S.
\newblock Horizon-free reinforcement learning in polynomial time: the power of
  stationary policies.
\newblock In \emph{Annual Conference Computational Learning Theory}, 2022.

\bibitem[Zhao et~al.(2022)Zhao, Zhou, He, and Gu]{zhao2022bandit}
Zhao, H., Zhou, D., He, J., and Gu, Q.
\newblock Bandit learning with general function classes: Heteroscedastic noise
  and variance-dependent regret bounds.
\newblock \emph{arXiv preprint arXiv:2202.13603}, 2022.

\bibitem[Zhao et~al.(2023)Zhao, He, Zhou, Zhang, and Gu]{zhao2023variance}
Zhao, H., He, J., Zhou, D., Zhang, T., and Gu, Q.
\newblock Variance-dependent regret bounds for linear bandits and reinforcement
  learning: Adaptivity and computational efficiency.
\newblock \emph{arXiv preprint arXiv:2302.10371}, 2023.

\bibitem[Zhou \& Gu(2022)Zhou and Gu]{zhou2022computationally}
Zhou, D. and Gu, Q.
\newblock Computationally efficient horizon-free reinforcement learning for
  linear mixture mdps.
\newblock \emph{arXiv preprint arXiv:2205.11507}, 2022.

\bibitem[Zhou et~al.(2021)Zhou, Gu, and Szepesvari]{zhou2021nearly}
Zhou, D., Gu, Q., and Szepesvari, C.
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock In \emph{Conference on Learning Theory}, pp.\  4532--4576. PMLR,
  2021.

\bibitem[Zhou et~al.(2022)Zhou, Tian, Wu, and Du]{paper:curl_co}
Zhou, R., Tian, Y., Wu, Y., and Du, S.~S.
\newblock Understanding curriculum learning in policy optimization for solving
  combinatorial optimization problems.
\newblock \emph{arXiv preprint arXiv:2202.05423}, 2022.

\end{thebibliography}
