\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint \href{https://arxiv.org/abs/1607.06450}{arXiv:1607.06450}}, 2016.

\bibitem[Bernstein et~al.(2020)Bernstein, Vahdat, Yue, and Liu]{bernstein2020distance}
Jeremy Bernstein, Arash Vahdat, Yisong Yue, and Ming-Yu Liu.
\newblock On the distance between two neural networks and the stability of learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 21370--21381, 2020.
\newblock \href{https://arxiv.org/abs/2002.03432}{arXiv:2002.03432}.

\bibitem[Chen et~al.(2023)Chen, Liang, Huang, Real, Wang, Pham, Dong, Luong, Hsieh, Lu, and Le]{chen2023symbolic}
Xiangning Chen, Chen Liang, Da~Huang, Esteban Real, Kaiyuan Wang, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc~V Le.
\newblock Symbolic discovery of optimization algorithms.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=ne6zeqLFCZ}.
\newblock \href{https://arxiv.org/abs/2302.06675}{arXiv:2302.06675}.

\bibitem[Chiley et~al.(2019)Chiley, Sharapov, Kosson, Koster, Reece, Samaniego de~la Fuente, Subbiah, and James]{chiley2019online}
Vitaliy Chiley, Ilya Sharapov, Atli Kosson, Urs Koster, Ryan Reece, Sofia Samaniego de~la Fuente, Vishal Subbiah, and Michael James.
\newblock Online normalization for training neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.
\newblock \href{https://arxiv.org/abs/1905.05894}{arXiv:1905.05894}.

\bibitem[Everett et~al.(2024)Everett, Xiao, Wortsman, Alemi, Novak, Liu, Gur, Sohl-Dickstein, Kaelbling, Lee, and Pennington]{everett2024scaling}
Katie~E Everett, Lechao Xiao, Mitchell Wortsman, Alexander~A Alemi, Roman Novak, Peter~J Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie~Pack Kaelbling, Jaehoon Lee, and Jeffrey Pennington.
\newblock Scaling exponents across parameterizations and optimizers.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.
\newblock URL \url{https://openreview.net/forum?id=0ksNeD1SJT}.
\newblock \href{https://arxiv.org/abs/2407.05872}{arXiv:2407.05872}.

\bibitem[Fu et~al.(2023)Fu, Wang, Zhang, Zhang, Chen, and Zheng]{fu2023momentum}
Jingwen Fu, Bohan Wang, Huishuai Zhang, Zhizheng Zhang, Wei Chen, and Nanning Zheng.
\newblock When and why momentum accelerates sgd: An empirical study.
\newblock \emph{arXiv preprint \href{https://arxiv.org/abs/2306.09000}{arXiv:2306.09000}}, 2023.

\bibitem[Gilmer et~al.(2022)Gilmer, Ghorbani, Garg, Kudugunta, Neyshabur, Cardoze, Dahl, Nado, and Firat]{gilmer2022a}
Justin Gilmer, Behrooz Ghorbani, Ankush Garg, Sneha Kudugunta, Behnam Neyshabur, David Cardoze, George~Edward Dahl, Zachary Nado, and Orhan Firat.
\newblock A loss curvature perspective on training instabilities of deep learning models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=OcKMT-36vUs}.
\newblock \href{https://arxiv.org/abs/2110.04369}{arXiv:2110.04369}.

\bibitem[Gokaslan and Cohen(2019)]{Gokaslan2019OpenWeb}
Aaron Gokaslan and Vanya Cohen.
\newblock Openwebtext corpus.
\newblock \url{http://Skylion007.github.io/OpenWebTextCorpus}, 2019.

\bibitem[Gotmare et~al.(2019)Gotmare, Keskar, Xiong, and Socher]{gotmare2018heuristics}
Akhilesh Gotmare, Nitish~Shirish Keskar, Caiming Xiong, and Richard Socher.
\newblock A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=r14EOsCqKX}.
\newblock \href{https://arxiv.org/abs/1810.13243}{arXiv:1810.13243}.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski, Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint \href{https://arxiv.org/abs/1706.02677}{arXiv:1706.02677}}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 770--778, 2016.
\newblock \href{https://arxiv.org/abs/1512.03385}{arXiv:1512.03385}.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.
\newblock URL \url{https://arxiv.org/abs/2203.15556}.

\bibitem[Huang et~al.(2020)Huang, Perez, Ba, and Volkovs]{huang2020improving}
Xiao~Shi Huang, Felipe Perez, Jimmy Ba, and Maksims Volkovs.
\newblock Improving transformer optimization through better initialization.
\newblock In \emph{International Conference on Machine Learning}, pages 4475--4483. PMLR, 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/huang20f.html}.

\bibitem[Huang and Belongie(2017)]{huang2017instancenorm}
Xun Huang and Serge Belongie.
\newblock Arbitrary style transfer in real-time with adaptive instance normalization.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pages 1501--1510, 2017.
\newblock \href{https://arxiv.org/abs/1703.06868}{arXiv:1703.06868}.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing internal covariate shift.
\newblock In \emph{International conference on machine learning}, pages 448--456. pmlr, 2015.
\newblock \href{https://arxiv.org/abs/1502.03167}{arXiv:1502.03167}.

\bibitem[Karpathy(2023)]{nanoGPT}
Andrej Karpathy.
\newblock nanogpt.
\newblock \url{https://github.com/karpathy/nanoGPT/}, 2023.

\bibitem[Karras et~al.(2024)Karras, Aittala, Lehtinen, Hellsten, Aila, and Laine]{karras2023analyzing}
Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine.
\newblock Analyzing and improving the training dynamics of diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 24174--24184, 2024.
\newblock \href{https://arxiv.org/abs/2312.02696}{arXiv:2312.02696}.

\bibitem[Kingma and Ba(2015)]{kingma15adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, San Diega, CA, USA, 2015.
\newblock \href{https://arxiv.org/abs/1412.6980}{arXiv:1412.6980}.

\bibitem[Kosson et~al.(2024)Kosson, Messmer, and Jaggi]{kosson2023rotational}
Atli Kosson, Bettina Messmer, and Martin Jaggi.
\newblock Rotational equilibrium: How weight decay balances learning across neural networks.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.
\newblock URL \url{https://openreview.net/forum?id=MQirNNU2pC}.
\newblock \href{https://arxiv.org/abs/2305.17212}{arXiv:2305.17212}.

\bibitem[Krizhevsky(2009)]{Krizhevsky2009LearningML}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{self-published}, 2009.
\newblock URL \url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}.

\bibitem[Li et~al.(2020)Li, Lyu, and Arora]{li2020reconciling}
Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora.
\newblock Reconciling modern deep learning with traditional optimization analyses: The intrinsic learning rate.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 14544--14555, 2020.
\newblock \href{https://arxiv.org/abs/2010.02916}{arXiv:2010.02916}.

\bibitem[Li et~al.(2021)Li, Malladi, and Arora]{li2021validity}
Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora.
\newblock On the validity of modeling {SGD} with stochastic differential equations ({SDE}s).
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan, editors, \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=goEdyJ_nVQI}.
\newblock \href{https://arxiv.org/abs/2102.12470}{arXiv:2102.12470}.

\bibitem[Liu et~al.(2020)Liu, Jiang, He, Chen, Liu, Gao, and Han]{Liu2020Variance}
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgz2aEKDr}.
\newblock \href{https://arxiv.org/abs/1908.03265}{arXiv:1908.03265}.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.
\newblock \href{https://arxiv.org/abs/1711.05101}{arXiv:1711.05101}.

\bibitem[Lyu et~al.(2022)Lyu, Li, and Arora]{lyu2022understanding}
Kaifeng Lyu, Zhiyuan Li, and Sanjeev Arora.
\newblock Understanding the generalization benefit of normalization layers: Sharpness reduction.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=xp5VOBxTxZ}.
\newblock \href{https://arxiv.org/abs/2206.07085}{arXiv:2206.07085}.

\bibitem[Ma and Yarats(2021)]{ma2021adequacy}
Jerry Ma and Denis Yarats.
\newblock On the adequacy of untuned warmup for adaptive optimization.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~35, pages 8828--8836, 2021.
\newblock \href{https://arxiv.org/abs/1910.04209}{arXiv:1910.04209}.

\bibitem[Malladi et~al.(2022)Malladi, Lyu, Panigrahi, and Arora]{malladi2022on}
Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora.
\newblock On the {SDE}s and scaling rules for adaptive gradient algorithms.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=F2mhzjHkQP}.
\newblock \href{https://arxiv.org/abs/2205.10287}{arXiv:2205.10287}.

\bibitem[McCandlish et~al.(2018)McCandlish, Kaplan, Amodei, and Team]{mccandlish2018empirical}
Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI~Dota Team.
\newblock An empirical model of large-batch training.
\newblock \emph{arXiv preprint \href{https://arxiv.org/abs/1812.06162}{arXiv:1812.06162}}, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever]{radford2019gpt2}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{self-published}, 2019.
\newblock URL \url{https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}.

\bibitem[Salimans and Kingma(2016)]{salimans2016weight}
Tim Salimans and Durk~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate training of deep neural networks.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.
\newblock \href{https://arxiv.org/abs/1602.07868}{arXiv:1602.07868}.

\bibitem[Shallue et~al.(2019)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig, and Dahl]{shallue2019measuring}
Christopher~J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein, Roy Frostig, and George~E Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0 (112):\penalty0 1--49, 2019.
\newblock \href{https://arxiv.org/abs/1811.03600}{arXiv:1811.03600}.

\bibitem[Smith et~al.(2020)Smith, Elsen, and De]{smith2020generalization}
Samuel Smith, Erich Elsen, and Soham De.
\newblock On the generalization benefit of noise in stochastic gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages 9058--9067. PMLR, 2020.
\newblock \href{https://arxiv.org/abs/2006.15081}{arXiv:2006.15081}.

\bibitem[Soboleva et~al.(2023)Soboleva, Al-Khateeb, Myers, Steeves, Hestness, and Dey]{soboleva2023slimpajama}
Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob~R Steeves, Joel Hestness, and Nolan Dey.
\newblock Slimpajama: A 627b token cleaned and deduplicated version of redpajama.
\newblock \url{https://cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama}, 2023.

\bibitem[Stich et~al.(2021)Stich, Mohtashami, and Jaggi]{stich2021critical}
Sebastian Stich, Amirkeivan Mohtashami, and Martin Jaggi.
\newblock Critical parameters for scalable distributed learning with large batches and asynchronous updates.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 4042--4050. PMLR, 2021.
\newblock \href{https://arxiv.org/abs/2103.02351}{arXiv:2103.02351}.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and Hinton]{sutskever2013importance}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, pages 1139--1147. PMLR, 2013.
\newblock URL \url{https://proceedings.mlr.press/v28/sutskever13.html}.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.
\newblock \href{https://arxiv.org/abs/1706.03762}{arXiv:1706.03762}.

\bibitem[Wan et~al.(2021)Wan, Zhu, Zhang, and Sun]{wan2021spherical}
Ruosi Wan, Zhanxing Zhu, Xiangyu Zhang, and Jian Sun.
\newblock Spherical motion dynamics: Learning dynamics of normalized neural network using sgd and weight decay.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman Vaughan, editors, \emph{Advances in Neural Information Processing Systems}, volume~34, pages 6380--6391. Curran Associates, Inc., 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper/2021/file/326a8c055c0d04f5b06544665d8bb3ea-Paper.pdf}.
\newblock \href{https://arxiv.org/abs/2006.08419}{arXiv:2006.08419}.

\bibitem[Wang et~al.(2024)Wang, Malladi, Wang, Lyu, and Li]{wang2024marginal}
Runzhe Wang, Sadhika Malladi, Tianhao Wang, Kaifeng Lyu, and Zhiyuan Li.
\newblock The marginal value of momentum for small learning rate {SGD}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=3JjJezzVkT}.
\newblock \href{https://arxiv.org/abs/2307.15196}{arXiv:2307.15196}.

\bibitem[Wightman(2019)]{rw2019timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Wortsman et~al.(2023)Wortsman, Liu, Xiao, Everett, Alemi, Adlam, Co-Reyes, Gur, Kumar, Novak, et~al.]{wortsman2023small}
Mitchell Wortsman, Peter~J Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John~D Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, et~al.
\newblock Small-scale proxies for large-scale transformer training instabilities.
\newblock \emph{arXiv preprint arXiv:2309.14322}, 2023.
\newblock URL \url{https://arxiv.org/abs/2309.14322}.

\bibitem[Wu and He(2018)]{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European conference on computer vision (ECCV)}, pages 3--19, 2018.
\newblock \href{https://arxiv.org/abs/1803.08494}{arXiv:1803.08494}.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan, Wang, and Liu]{xiong2020layer}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{International Conference on Machine Learning}, pages 10524--10533. PMLR, 2020.
\newblock \href{https://arxiv.org/abs/2002.04745}{arXiv:2002.04745}.

\bibitem[Yang et~al.(2021)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder, Pachocki, Chen, and Gao]{yang2021tuning}
Greg Yang, Edward~J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao.
\newblock Tuning large neural networks via zero-shot hyperparameter transfer.
\newblock In A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~Wortman Vaughan, editors, \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Bx6qKuBM2AD}.
\newblock \href{https://arxiv.org/abs/2203.03466}{arXiv:2203.03466}.

\bibitem[Yang et~al.(2023)Yang, Simon, and Bernstein]{yang2023spectral}
Greg Yang, James~B Simon, and Jeremy Bernstein.
\newblock A spectral condition for feature learning.
\newblock \emph{arXiv preprint \href{https://arxiv.org/abs/2310.17813}{arXiv:2310.17813}}, 2023.

\bibitem[Yin et~al.(2018)Yin, Pananjady, Lam, Papailiopoulos, Ramchandran, and Bartlett]{yin2018gradient}
Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan Ramchandran, and Peter Bartlett.
\newblock Gradient diversity: a key ingredient for scalable distributed learning.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 1998--2007. PMLR, 2018.
\newblock \href{https://arxiv.org/abs/1706.05699}{arXiv:1706.05699}.

\bibitem[You et~al.(2017)You, Gitman, and Ginsburg]{you2017lars}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint \href{https://arxiv.org/abs/1708.03888}{arXiv:1708.03888}}, 2017.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song, Demmel, Keutzer, and Hsieh]{you2019lamb}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training bert in 76 minutes.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=Syx4wnEtvH}.
\newblock \href{https://arxiv.org/abs/1904.00962}{arXiv:1904.00962}.

\bibitem[Zhang et~al.(2019)Zhang, Li, Nado, Martens, Sachdeva, Dahl, Shallue, and Grosse]{zhang2019algorithmic}
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl, Chris Shallue, and Roger~B Grosse.
\newblock Which algorithmic choices matter at which batch sizes? {I}nsights from a noisy quadratic model.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.
\newblock \href{https://arxiv.org/abs/1907.04164}{arXiv:1907.04164}.

\end{thebibliography}
