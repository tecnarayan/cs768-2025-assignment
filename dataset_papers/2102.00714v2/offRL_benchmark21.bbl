\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Dem\v{s}ar}{2006}]{demvar.jmlr06}
J.~Dem\v{s}ar.
\newblock Statistical comparisons of classifiers over multi- ple data sets.
\newblock (7):1–--30, 2006.

\bibitem[\protect\citeauthoryear{Dosovitskiy \bgroup \em et al.\egroup
  }{2017}]{CARLA}
Alexey Dosovitskiy, Germ{\'{a}}n Ros, Felipe Codevilla, and et~al.
\newblock {CARLA:} {A}n open urban driving simulator.
\newblock In {\em Proceedings of the 1st Annual Conference on Robot Learning},
  Mountain View, California, 2017.

\bibitem[\protect\citeauthoryear{Dulac{-}Arnold \bgroup \em et al.\egroup
  }{2020}]{RWRL}
Gabriel Dulac{-}Arnold, Nir Levine, Daniel~J. Mankowitz, Jerry Li, Cosmin
  Paduraru, Sven Gowal, and Todd Hester.
\newblock {A}n empirical investigation of the challenges of real-world
  reinforcement learning.
\newblock {\em CoRR}, abs/2003.11881, 2020.

\bibitem[\protect\citeauthoryear{Fu \bgroup \em et al.\egroup }{2020}]{D4RL}
Justin Fu, Aviral Kumar, Ofir Nachum, and et~al.
\newblock {D4RL:} {D}atasets for deep data-driven reinforcement learning.
\newblock {\em CoRR}, abs/2004.07219, 2020.

\bibitem[\protect\citeauthoryear{Fujimoto \bgroup \em et al.\egroup
  }{2019a}]{BenchBatchDRL}
Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and et~al.
\newblock Benchmarking batch deep reinforcement learning algorithms.
\newblock {\em CoRR}, abs/1910.01708, 2019.

\bibitem[\protect\citeauthoryear{Fujimoto \bgroup \em et al.\egroup
  }{2019b}]{BCQ}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, {\em
  Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of {\em Proceedings of Machine Learning Research}, pages
  2052--2062, Long Beach, California, 2019.

\bibitem[\protect\citeauthoryear{G{\"{u}}l{\c{c}}ehre \bgroup \em et al.\egroup
  }{}]{RLunplugged}
{\c{C}}aglar G{\"{u}}l{\c{c}}ehre, Ziyu Wang, Alexander Novikov, and et~al.
\newblock {RL} unplugged: {A} collection of benchmarks for offline
  reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems 33},
  Virtual Conference.

\bibitem[\protect\citeauthoryear{Gupta \bgroup \em et al.\egroup
  }{2019}]{Replay_Learning}
Abhishek Gupta, Vikash Kumar, Corey Lynch, and et~al.
\newblock Relay policy learning: {S}olving long-horizon tasks via imitation and
  reinforcement learning.
\newblock In {\em Proceedings of 3rd Annual Conference on Robot Learning},
  pages 1025--1037, Osaka, Japan, 2019.

\bibitem[\protect\citeauthoryear{Haarnoja \bgroup \em et al.\egroup
  }{2018}]{SAC}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: {O}ff-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, pages 1856--1865, Stockholm, Sweden, 2018.

\bibitem[\protect\citeauthoryear{{Hein} \bgroup \em et al.\egroup
  }{2017}]{hein2017a}
Daniel {Hein}, Stefan {Depeweg}, Michel {Tokic}, Steffen {Udluft}, Alexander
  {Hentschel}, Thomas~A. {Runkler}, and Volkmar {Sterzing}.
\newblock A benchmark environment motivated by industrial control problems.
\newblock In {\em 2017 IEEE Symposium Series on Computational Intelligence
  (SSCI)}, pages 1--8, 2017.

\bibitem[\protect\citeauthoryear{Kidambi \bgroup \em et al.\egroup }{}]{MORel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock {MOReL}: {M}odel-based offline reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems 33},
  Virtual Conference.

\bibitem[\protect\citeauthoryear{Kumar \bgroup \em et al.\egroup }{2019}]{BEAR}
Aviral Kumar, Justin Fu, Matthew Soh, and et~al.
\newblock Stabilizing off-policy {Q}-learning via bootstrapping error
  reduction.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  11761--11771, Vancouver, BC, Canada, 2019.

\bibitem[\protect\citeauthoryear{Kumar \bgroup \em et al.\egroup }{2020}]{CQL}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems 33},
  Virtual Conference, 2020.

\bibitem[\protect\citeauthoryear{Lange \bgroup \em et al.\egroup
  }{2012}]{batchRL}
Sascha Lange, Thomas Gabel, and Martin~A. Riedmiller.
\newblock Batch reinforcement learning.
\newblock In {\em Reinforcement Learning}, pages 45--73. 2012.

\bibitem[\protect\citeauthoryear{Le \bgroup \em et al.\egroup }{2019}]{FQE}
Hoang~Minh Le, Cameron Voloshin, and Yisong Yue.
\newblock Batch policy learning under constraints.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, pages 3703--3712, Long Beach, California, 2019.

\bibitem[\protect\citeauthoryear{{Levine} \bgroup \em et al.\egroup
  }{2020}]{levine2020offline}
Sergey {Levine}, Aviral {Kumar}, George {Tucker}, and Justin {Fu}.
\newblock Offline reinforcement learning: {T}utorial, review, and perspectives
  on open problems.
\newblock {\em arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[\protect\citeauthoryear{{Liu} \bgroup \em et al.\egroup
  }{2020}]{liu2020finrl}
Xiao-Yang {Liu}, Hongyang {Yang}, Qian {Chen}, Runjia {Zhang}, Liuqing {Yang},
  Bowen {Xiao}, and Christina~Dan {Wang}.
\newblock {FinRL}: {A} deep reinforcement learning library for automated stock
  trading in quantitative finance.
\newblock {\em arXiv preprint arXiv:2011.09607}, 2020.

\bibitem[\protect\citeauthoryear{Mnih \bgroup \em et al.\egroup }{2013}]{DQN13}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, and et~al.
\newblock Playing atari with deep reinforcement learning.
\newblock {\em CoRR}, abs/1312.5602, 2013.

\bibitem[\protect\citeauthoryear{Mnih \bgroup \em et al.\egroup }{2015}]{DQN15}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, and et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529--533, 2015.

\bibitem[\protect\citeauthoryear{Paine \bgroup \em et al.\egroup
  }{2020}]{useFQE}
Tom~Le Paine, Cosmin Paduraru, Andrea Michi, and et~al.
\newblock Hyperparameter selection for offline reinforcement learning.
\newblock {\em CoRR}, abs/2007.09055, 2020.

\bibitem[\protect\citeauthoryear{Precup \bgroup \em et al.\egroup
  }{2000}]{OffPolicyEval}
Doina Precup, Richard~S. Sutton, and Satinder~P. Singh.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock In {\em Proceedings of the Seventeenth International Conference on
  Machine Learning, Stanford University, Stanford, CA}, pages 759--766, 2000.

\bibitem[\protect\citeauthoryear{{Schulman} \bgroup \em et al.\egroup
  }{2017}]{schulman2017proximal}
John {Schulman}, Filip {Wolski}, Prafulla {Dhariwal}, Alec {Radford}, and Oleg
  {Klimov}.
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[\protect\citeauthoryear{Shang \bgroup \em et al.\egroup
  }{2019}]{vdidi.kdd19}
Wenjie Shang, Yang Yu, Qingyang Li, Zhiwei Qin, Yiping Meng, and Jieping Ye.
\newblock Environment reconstruction with hidden confounders for reinforcement
  learning based recommendation.
\newblock In {\em Proceedings of the 25th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining (KDD'19)}, Anchorage, AL, 2019.

\bibitem[\protect\citeauthoryear{Shi \bgroup \em et al.\egroup
  }{2019}]{vtaobao.aaai19}
Jing-Cheng Shi, Yang Yu, Qing Da, Shi-Yong Chen, and An-Xiang Zeng.
\newblock {Virtual-Taobao}: {V}irtualizing real-world online retail environment
  for reinforcement learning.
\newblock In {\em Proceedings of the 33rd AAAI Conference on Artificial
  Intelligence}, Honolulu, HI, 2019.

\bibitem[\protect\citeauthoryear{Silver \bgroup \em et al.\egroup
  }{2017}]{alphaGoZero}
David Silver, Julian Schrittwieser, Karen Simonyan, and et~al.
\newblock Mastering the game of go without human knowledge.
\newblock {\em Nature}, 550(7676):354--359, 2017.

\bibitem[\protect\citeauthoryear{Sutton and Barto}{1998}]{SuttonRL}
Richard~S. Sutton and Andrew~G. Barto.
\newblock {\em Reinforcement Learning - An Introduction}.
\newblock {MIT} Press, 1998.

\bibitem[\protect\citeauthoryear{Tassa \bgroup \em et al.\egroup
  }{2018}]{DMControlSuite}
Yuval Tassa, Yotam Doron, Alistair Muldal, and et~al.
\newblock {DeepMind} control suite.
\newblock {\em CoRR}, abs/1801.00690, 2018.

\bibitem[\protect\citeauthoryear{Todorov \bgroup \em et al.\egroup
  }{2012}]{MuJoCo}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock {MuJoCo}: {A} physics engine for model-based control.
\newblock In {\em 2012 {IEEE/RSJ} International Conference on Intelligent
  Robots and Systems}, pages 5026--5033, Vilamoura, Algarve, Portugal, 2012.

\bibitem[\protect\citeauthoryear{Voloshin \bgroup \em et al.\egroup
  }{2019}]{studyOPE}
Cameron Voloshin, Hoang~Minh Le, Nan Jiang, and Yisong Yue.
\newblock Empirical study of off-policy policy evaluation for reinforcement
  learning.
\newblock {\em CoRR}, abs/1911.06854, 2019.

\bibitem[\protect\citeauthoryear{{Vázquez-Canteli} \bgroup \em et al.\egroup
  }{2019}]{canteli2019citylearn}
José~R. {Vázquez-Canteli}, Jérôme {Kämpf}, Gregor {Henze}, and Zoltan
  {Nagy}.
\newblock {CityLearn} v1.0: {A}n {OpenAI} {G}ym environment for demand response
  with deep reinforcement learning.
\newblock In {\em Proceedings of the 6th ACM International Conference on
  Systems for Energy-Efficient Buildings, Cities, and Transportation}, pages
  356--357, 2019.

\bibitem[\protect\citeauthoryear{Wu \bgroup \em et al.\egroup
  }{2019}]{behavior_reg}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock {\em CoRR}, abs/1911.11361, 2019.

\bibitem[\protect\citeauthoryear{Xu \bgroup \em et al.\egroup
  }{2020}]{xu.li.yu.nips2020}
Tian Xu, Ziniu Li, and Yang Yu.
\newblock Error bounds of imitating policies and environments.
\newblock In {\em Advances in Neural Information Processing Systems 33},
  Virtual Conference, 2020.

\bibitem[\protect\citeauthoryear{Yu \bgroup \em et al.\egroup }{2020}]{MOPO}
Tianhe Yu, Garrett Thomas, Lantao Yu, and et~al.
\newblock {MOPO:} {M}odel-based offline policy optimization.
\newblock In {\em Advances in Neural Information Processing Systems 33},
  Virtual Conference, 2020.

\bibitem[\protect\citeauthoryear{Zhou \bgroup \em et al.\egroup }{2020}]{PLAS}
Wenxuan Zhou, Sujay Bajracharya, and David Held.
\newblock {PLAS:} {L}atent action space for offline reinforcement learning.
\newblock {\em CoRR}, abs/2011.07213, 2020.

\end{thebibliography}
