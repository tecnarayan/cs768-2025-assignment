\begin{thebibliography}{61}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson et~al.(2016)Anderson, Fernando, Johnson, and Gould]{spice}
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould.
\newblock Spice: Semantic propositional image caption evaluation.
\newblock In \emph{Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14}, pages 382--398. Springer, 2016.

\bibitem[Antol et~al.(2015)Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, and Parikh]{evqa}
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C~Lawrence Zitnick, and Devi Parikh.
\newblock Vqa: Visual question answering.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pages 2425--2433, 2015.

\bibitem[Banerjee and Lavie(2005)]{meteor}
Satanjeev Banerjee and Alon Lavie.
\newblock Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.
\newblock In \emph{Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization}, pages 65--72, 2005.

\bibitem[Barbu et~al.(2019)Barbu, Mayo, Alverio, Luo, Wang, Gutfreund, Tenenbaum, and Katz]{barbu2019objectnet}
Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz.
\newblock Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Ding et~al.(2023)Ding, Wang, and Tu]{ding2023open}
Zheng Ding, Jieke Wang, and Zhuowen Tu.
\newblock Open-vocabulary universal image segmentation with maskclip.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning}, pages 8090--8102, 2023.

\bibitem[Fan et~al.(2019)Fan, Zhang, Zhang, Wang, Zhang, and Huang]{hme}
Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng Wang, Chi Zhang, and Heng Huang.
\newblock Heterogeneous memory enhanced multimodal attention model for video question answering.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 1999--2007, 2019.

\bibitem[Gao et~al.(2023)Gao, Zhou, Ji, Zhu, Yang, and Shou]{mist}
Difei Gao, Luowei Zhou, Lei Ji, Linchao Zhu, Yi Yang, and Mike~Zheng Shou.
\newblock Mist: Multi-modal iterative spatial-temporal transformer for long-form video question answering.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 14773--14783, 2023.

\bibitem[Gao et~al.(2018)Gao, Ge, Chen, and Nevatia]{comem}
Jiyang Gao, Runzhou Ge, Kan Chen, and Ram Nevatia.
\newblock Motion-appearance co-memory networks for video question answering.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages 6576--6585, 2018.

\bibitem[Grunde-McLaughlin et~al.(2021)Grunde-McLaughlin, Krishna, and Agrawala]{agqa}
Madeleine Grunde-McLaughlin, Ranjay Krishna, and Maneesh Agrawala.
\newblock Agqa: A benchmark for compositional spatio-temporal reasoning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11287--11297, 2021.

\bibitem[Guadarrama et~al.(2013)Guadarrama, Krishnamoorthy, Malkarnenkar, Venugopalan, Mooney, Darrell, and Saenko]{youtube2text}
Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkarnenkar, Subhashini Venugopalan, Raymond Mooney, Trevor Darrell, and Kate Saenko.
\newblock Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pages 2712--2719, 2013.

\bibitem[Han et~al.(2023)Han, Sun, and Firestone]{han2023caricaturing}
Subin Han, Zekun Sun, and Chaz Firestone.
\newblock Caricaturing shapes in visual memory.
\newblock \emph{Journal of Vision}, 23\penalty0 (9):\penalty0 4756--4756, 2023.

\bibitem[He et~al.(2016)He, Shirakabe, Satoh, and Kataoka]{he2016human}
Yun He, Soma Shirakabe, Yutaka Satoh, and Hirokatsu Kataoka.
\newblock Human action recognition without human.
\newblock In \emph{Computer Vision--ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14}, pages 11--17. Springer, 2016.

\bibitem[Jang et~al.(2017)Jang, Song, Yu, Kim, and Kim]{stvqa}
Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim.
\newblock Tgif-qa: Toward spatio-temporal reasoning in visual question answering.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 2758--2766, 2017.

\bibitem[Jang et~al.(2019)Jang, Song, Kim, Yu, Kim, and Kim]{stvqa2}
Yunseok Jang, Yale Song, Chris~Dongjoo Kim, Youngjae Yu, Youngjin Kim, and Gunhee Kim.
\newblock Video question answering with spatio-temporal reasoning.
\newblock \emph{International Journal of Computer Vision}, 127:\penalty0 1385--1412, 2019.

\bibitem[Jiang and Han(2020)]{hga}
Pin Jiang and Yahong Han.
\newblock Reasoning with heterogeneous graph alignment for video question answering.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, pages 11109--11116, 2020.

\bibitem[Le et~al.(2020)Le, Le, Venkatesh, and Tran]{hcrn}
Thao~Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran.
\newblock Hierarchical conditional relation networks for video question answering.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 9972--9981, 2020.

\bibitem[Li et~al.(2022)Li, Niu, and Zhang]{causalvidqa}
Jiangtong Li, Li Niu, and Liqing Zhang.
\newblock From representation to reasoning: Towards both evidence and commonsense reasoning for video question-answering.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 2022.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Li, Savarese, and Hoi]{blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock {BLIP-2:} bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In \emph{ICML}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Wei, Han, and Fan]{intentqa}
Jiapeng Li, Ping Wei, Wenjuan Han, and Lifeng Fan.
\newblock Intentqa: Context-aware video intent reasoning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pages 11963--11974, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2024{\natexlab{a}})Li, Chen, Ren, Cheng, Zhao, Nie, and Wen]{li2024dawn}
Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne~Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.
\newblock The dawn after the dark: An empirical study on factuality hallucination in large language models.
\newblock \emph{arXiv preprint arXiv:2401.03205}, 2024{\natexlab{a}}.

\bibitem[Li et~al.(2024{\natexlab{b}})Li, Wang, He, Li, Wang, Liu, Wang, Xu, Chen, Luo, et~al.]{videochat2}
Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et~al.
\newblock Mvbench: A comprehensive multi-modal video understanding benchmark.
\newblock 2024{\natexlab{b}}.

\bibitem[Li et~al.(2024{\natexlab{c}})Li, Fan, Zhang, Jin, Wang, Guo, Zhao, and Li]{li2024image}
Xiaochuan Li, Baoyu Fan, Runze Zhang, Liang Jin, Di Wang, Zhenhua Guo, Yaqian Zhao, and Rengang Li.
\newblock Image content generation with causal reasoning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, pages 13646--13654, 2024{\natexlab{c}}.

\bibitem[Lin(2004)]{rouge}
Chin-Yew Lin.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In \emph{Text summarization branches out}, pages 74--81, 2004.

\bibitem[Liu et~al.(2024{\natexlab{a}})Liu, Xue, Chen, Chen, Zhao, Wang, Hou, Li, and Peng]{liu2024survey}
Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng.
\newblock A survey on hallucination in large vision-language models.
\newblock \emph{arXiv preprint arXiv:2402.00253}, 2024{\natexlab{a}}.

\bibitem[Liu et~al.(2024{\natexlab{b}})Liu, Xu, Wu, Yuan, Yang, Zhou, Liu, Guan, Wang, Yu, et~al.]{liu2024large}
Xiaoyu Liu, Paiheng Xu, Junda Wu, Jiaxin Yuan, Yifan Yang, Yuhang Zhou, Fuxiao Liu, Tianrui Guan, Haoliang Wang, Tong Yu, et~al.
\newblock Large language models and causal inference in collaboration: A comprehensive survey.
\newblock \emph{arXiv preprint arXiv:2403.09606}, 2024{\natexlab{b}}.

\bibitem[Maharaj et~al.(2017)Maharaj, Ballas, Rohrbach, Courville, and Pal]{moviefib}
Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, and Christopher Pal.
\newblock A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pages 6884--6893, 2017.

\bibitem[Mao et~al.(2022)Mao, Yang, Zhang, Goodman, and Wu]{clevrerhumans}
Jiayuan Mao, Xuelin Yang, Xikun Zhang, Noah Goodman, and Jiajun Wu.
\newblock Clevrer-humans: Describing physical and causal events the human way.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 7755--7768, 2022.

\bibitem[Mauro and Kubovy(1992)]{mauro1992caricature}
Robert Mauro and Michael Kubovy.
\newblock Caricature and face recognition.
\newblock \emph{Memory \& Cognition}, 20:\penalty0 433--440, 1992.

\bibitem[Meringoff et~al.(1983)Meringoff, Vibbert, Char, Fernie, Banker, and Gardner]{meringoff1983children}
Laurene~K Meringoff, Martha~M Vibbert, Cynthia~A Char, David~E Fernie, Gail~S Banker, and Howard Gardner.
\newblock How is children's learning from television distinctive? exploiting the medium methodologically.
\newblock \emph{Children's understanding of television: Research on attention and comprehension}, pages 151--180, 1983.

\bibitem[Mokady et~al.(2021)Mokady, Hertz, and Bermano]{clipcap}
Ron Mokady, Amir Hertz, and Amit~H Bermano.
\newblock Clipcap: Clip prefix for image captioning.
\newblock \emph{arXiv preprint arXiv:2111.09734}, 2021.

\bibitem[{OpenAI}(2024)]{gpt4o}
{OpenAI}.
\newblock Hello gpt-4o.
\newblock \url{https://openai.com/index/hello-gpt-4o/}, 2024.
\newblock [Online; accessed 31-May-2024].

\bibitem[OpenAI(2023)]{gpt4}
team OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu]{bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting of the Association for Computational Linguistics}, pages 311--318, 2002.

\bibitem[Pavlakos et~al.(2022)Pavlakos, Malik, and Kanazawa]{pavlakos2022human}
Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa.
\newblock Human mesh recovery from multiple shots.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 1485--1495, 2022.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever, et~al.]{gpt2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Reimers and Gurevych(2019)]{sentencebert}
Nils Reimers and Iryna Gurevych.
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock \emph{arXiv preprint arXiv:1908.10084}, 2019.

\bibitem[Rhodes et~al.(1987)Rhodes, Brennan, and Carey]{rhodes1987}
Gillian Rhodes, Susan Brennan, and Susan Carey.
\newblock Identification and ratings of caricatures: Implications for mental representations of faces.
\newblock \emph{Cognitive Psychology}, 19\penalty0 (4):\penalty0 473--497, 1987.

\bibitem[Schnotz and Rasch(2008)]{schnotz2008functions}
Wolfgang Schnotz and Thorsten Rasch.
\newblock Functions of animation in comprehension and learning.
\newblock \emph{Learning with animation: Research implications for design}, pages 93--113, 2008.

\bibitem[Shreesha and Tyagi(2016)]{shreesha2016does}
Mairaru Shreesha and Sanjay~Kumar Tyagi.
\newblock Does animation facilitate better learning in primary education? a comparative study of three different subjects.
\newblock \emph{Creative Education}, 7\penalty0 (13):\penalty0 1800--1809, 2016.

\bibitem[Siong et~al.(2023)Siong, Ong, Phang, and Pusppanathan]{siong2023use}
La{\i}~Chin Siong, Yunn Ong, Fatin~Aliah Phang, and Jaysuman Pusppanathan.
\newblock The use of concept cartoons in overcoming the misconception in electricity concepts.
\newblock \emph{Participatory Educational Research}, 10\penalty0 (1):\penalty0 310--329, 2023.

\bibitem[Sun et~al.(2023)Sun, Fang, Wu, Zhang, Zang, Kong, Xiong, Lin, and Wang]{sun2023alpha}
Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang.
\newblock Alpha-clip: A clip model focusing on wherever you want.
\newblock \emph{arXiv preprint arXiv:2312.03818}, 2023.

\bibitem[Thomas and Johnston(1981)]{principles_animations}
Frank Thomas and Ollie Johnston.
\newblock \emph{The illusion of life : Disney animation}.
\newblock Disney Editions, 1981.

\bibitem[Tom and Fandom()]{tom_and_jerry_wiki}
Tom and Jerry~Wiki Fandom.
\newblock Tom and jerry wiki fandom.

\bibitem[Vedantam et~al.(2015)Vedantam, Lawrence~Zitnick, and Parikh]{cider}
Ramakrishna Vedantam, C Lawrence~Zitnick, and Devi Parikh.
\newblock Cider: Consensus-based image description evaluation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 4566--4575, 2015.

\bibitem[Wong et~al.(2015)Wong, Kaelbling, and Lozano-P{\'e}rez]{wong2015data}
Lawson~LS Wong, Leslie~Pack Kaelbling, and Tom{\'a}s Lozano-P{\'e}rez.
\newblock Data association for semantic world modeling from partial views.
\newblock \emph{International Journal of Robotics Research}, 34\penalty0 (7):\penalty0 1064--1082, 2015.

\bibitem[Wu et~al.(2021)Wu, Yu, Chen, Tenenbaum, and Gan]{wu_star}
Bo Wu, Shoubin Yu, Zhenfang Chen, Joshua~B Tenenbaum, and Chuang Gan.
\newblock {STAR}: A benchmark for situated reasoning in real-world videos.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Xiao et~al.(2021)Xiao, Shang, Yao, and Chua]{nextqa}
Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua.
\newblock Next-qa: Next phase of question-answering to explaining temporal actions.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 9777--9786, 2021.

\bibitem[Xu et~al.(2017)Xu, Zhao, Xiao, Wu, Zhang, He, and Zhuang]{msvd}
Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang.
\newblock Video question answering via gradually refined attention over appearance and motion.
\newblock In \emph{Proceedings of the 25th ACM international conference on Multimedia}, pages 1645--1653, 2017.

\bibitem[Xu et~al.(2023{\natexlab{a}})Xu, Liu, Vahdat, Byeon, Wang, and De~Mello]{xu2023open}
Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De~Mello.
\newblock Open-vocabulary panoptic segmentation with text-to-image diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 2955--2966, 2023{\natexlab{a}}.

\bibitem[Xu et~al.(2023{\natexlab{b}})Xu, Zhang, Wei, Hu, and Bai]{xu2023side}
Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai.
\newblock Side adapter network for open-vocabulary semantic segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 2945--2954, 2023{\natexlab{b}}.

\bibitem[Xue et~al.(2017)Xue, Zhao, and Cai]{uatt}
Hongyang Xue, Zhou Zhao, and Deng Cai.
\newblock Unifying the video and question attentions for open-ended video question answering.
\newblock \emph{IEEE Transactions on Image Processing}, 26\penalty0 (12):\penalty0 5656--5666, 2017.

\bibitem[Yi et~al.(2019)Yi, Gan, Li, Kohli, Wu, Torralba, and Tenenbaum]{clevrer}
Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua~B Tenenbaum.
\newblock Clevrer: Collision events for video representation and reasoning.
\newblock \emph{arXiv preprint arXiv:1910.01442}, 2019.

\bibitem[Yin and Fitzgerald(2017)]{yin2017peer}
Khoo~Yin Yin and Robert Fitzgerald.
\newblock Peer learning with concept cartoons enhance critical thinking and performance in secondary school economics.
\newblock \emph{Journal of economics and economic education research}, 18\penalty0 (1):\penalty0 1--13, 2017.

\bibitem[Yu et~al.(2019)Yu, Xu, Yu, Yu, Zhao, Zhuang, and Tao]{activitynetqa}
Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao.
\newblock Activitynet-qa: A dataset for understanding complex web videos via question answering.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, pages 9127--9134, 2019.

\bibitem[Zadeh et~al.(2019)Zadeh, Chan, Liang, Tong, and Morency]{socialiq}
Amir Zadeh, Michael Chan, Paul~Pu Liang, Edmund Tong, and Louis-Philippe Morency.
\newblock Social-iq: A question answering benchmark for artificial social intelligence.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 8807--8817, 2019.

\bibitem[Zeng et~al.(2017)Zeng, Chen, Chuang, Liao, Niebles, and Sun]{zeng2017leveraging}
Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang, Yuan-Hong Liao, Juan~Carlos Niebles, and Min Sun.
\newblock Leveraging video descriptions to learn video question answering.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 2017.

\bibitem[Zhang et~al.(2023)Zhang, Li, and Bing]{videollama}
Hang Zhang, Xin Li, and Lidong Bing.
\newblock Video-{LL}a{MA}: An instruction-tuned audio-visual language model for video understanding.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 543--553, Singapore, 2023. Association for Computational Linguistics.

\bibitem[Zhao et~al.(2018)Zhao, Zhang, Xiao, Yu, Yu, Cai, Wu, and Zhuang]{openendedqa_zhou}
Zhou Zhao, Zhu Zhang, Shuwen Xiao, Zhou Yu, Jun Yu, Deng Cai, Fei Wu, and Yueting Zhuang.
\newblock Open-ended long-form video question answering via adaptive hierarchical reinforced networks.
\newblock In \emph{Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, {IJCAI-18}}, pages 3683--3689. International Joint Conferences on Artificial Intelligence Organization, 2018.

\bibitem[Zhong et~al.(2022)Zhong, Xiao, Ji, Li, Deng, and Chua]{zhong2022video}
Yaoyao Zhong, Junbin Xiao, Wei Ji, Yicong Li, Weihong Deng, and Tat-Seng Chua.
\newblock Video question answering: Datasets, algorithms and challenges.
\newblock \emph{arXiv preprint arXiv:2203.01225}, 2022.

\bibitem[Zhou et~al.(2023)Zhou, Xu, Liu, An, Ai, and Huang]{zhou2023explore}
Yuhang Zhou, Paiheng Xu, Xiaoyu Liu, Bang An, Wei Ai, and Furong Huang.
\newblock Explore spurious correlations at the concept level in language models for text classification.
\newblock \emph{arXiv preprint arXiv:2311.08648}, 2023.

\bibitem[Zhu et~al.(2017)Zhu, Xu, Yang, and Hauptmann]{Videocontextqa}
Linchao Zhu, Zhongwen Xu, Yi Yang, and Alexander~G Hauptmann.
\newblock Uncovering the temporal context for video question answering.
\newblock \emph{International Journal of Computer Vision}, 124:\penalty0 409--421, 2017.

\end{thebibliography}
