\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abraham et~al.(2020)Abraham, Handa, Ratliff, Lowrey, Murphey, and
  Fox]{abraham2020model}
Abraham, I., Handa, A., Ratliff, N., Lowrey, K., Murphey, T.~D., and Fox, D.
\newblock Model-based generalization under parameter uncertainty using path
  integral control.
\newblock \emph{IEEE Robotics and Automation Letters}, 5\penalty0 (2):\penalty0
  2864--2871, 2020.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Brunskill(2012)]{brunskill2012bayes}
Brunskill, E.
\newblock Bayes-optimal reinforcement learning for discrete uncertainty
  domains.
\newblock In \emph{Proceedings of the 11th International Conference on
  Autonomous Agents and Multiagent Systems-Volume 3}, pp.\  1385--1386, 2012.

\bibitem[Chen et~al.(2021)Chen, Wang, Zhou, and Ross]{chen2021randomized}
Chen, X., Wang, C., Zhou, Z., and Ross, K.
\newblock Randomized ensembled double q-learning: Learning fast without a
  model.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021.

\bibitem[Dorfman \& Tamar(2021)Dorfman and Tamar]{dorfman2020offline}
Dorfman, R. and Tamar, A.
\newblock Offline meta reinforcement learning -- identifiability challenges and
  effective data collection strategies.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem[Duan et~al.(2016)Duan, Schulman, Chen, Bartlett, Sutskever, and
  Abbeel]{duan2016rl}
Duan, Y., Schulman, J., Chen, X., Bartlett, P.~L., Sutskever, I., and Abbeel,
  P.
\newblock {RL2}: Fast reinforcement learning via slow reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1611.02779}, 2016.

\bibitem[Duff(2002)]{duff2002optimal}
Duff, M.~O.
\newblock \emph{Optimal Learning: Computational procedures for Bayes-adaptive
  Markov decision processes}.
\newblock University of Massachusetts Amherst, 2002.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Ghavamzadeh et~al.(2016)Ghavamzadeh, Mannor, Pineau, and
  Tamar]{ghavamzadeh2016bayesian}
Ghavamzadeh, M., Mannor, S., Pineau, J., and Tamar, A.
\newblock Bayesian reinforcement learning: A survey.
\newblock \emph{arXiv preprint arXiv:1609.04436}, 2016.

\bibitem[Guez et~al.(2012)Guez, Silver, and Dayan]{guez2012efficient}
Guez, A., Silver, D., and Dayan, P.
\newblock Efficient bayes-adaptive reinforcement learning using sample-based
  search.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2012.

\bibitem[Guez et~al.(2013)Guez, Silver, and Dayan]{guez2013scalable}
Guez, A., Silver, D., and Dayan, P.
\newblock Scalable and efficient bayes-adaptive reinforcement learning based on
  monte-carlo tree search.
\newblock \emph{Journal of Artificial Intelligence Research}, 48:\penalty0
  841--883, 2013.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Hallak et~al.(2015)Hallak, Di~Castro, and
  Mannor]{hallak2015contextual}
Hallak, A., Di~Castro, D., and Mannor, S.
\newblock Contextual markov decision processes.
\newblock \emph{arXiv preprint arXiv:1502.02259}, 2015.

\bibitem[Hausman et~al.(2018)Hausman, Springenberg, Wang, Heess, and
  Riedmiller]{hausman2018learning}
Hausman, K., Springenberg, J.~T., Wang, Z., Heess, N., and Riedmiller, M.
\newblock Learning an embedding space for transferable robot skills.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Humplik et~al.(2019)Humplik, Galashov, Hasenclever, Ortega, Teh, and
  Heess]{humplik2019meta}
Humplik, J., Galashov, A., Hasenclever, L., Ortega, P.~A., Teh, Y.~W., and
  Heess, N.
\newblock Meta reinforcement learning as task inference.
\newblock \emph{arXiv preprint arXiv:1905.06424}, 2019.

\bibitem[Iyengar(2005)]{iyengar2005robust}
Iyengar, G.~N.
\newblock Robust dynamic programming.
\newblock \emph{Mathematics of Operations Research}, 30\penalty0 (2):\penalty0
  257--280, 2005.

\bibitem[Kumar et~al.(2021)Kumar, Fu, Pathak, and Malik]{kumar2021rma}
Kumar, A., Fu, Z., Pathak, D., and Malik, J.
\newblock Rma: Rapid motor adaptation for legged robots.
\newblock \emph{Robotics: Science and Systems (RSS)}, 2021.

\bibitem[Kumar et~al.(2020)Kumar, Kumar, Levine, and Finn]{kumar2020one}
Kumar, S., Kumar, A., Levine, S., and Finn, C.
\newblock One solution is not all you need: Few-shot extrapolation via
  structured maxent rl.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 33, 2020.

\bibitem[Lee et~al.(2020)Lee, Nagabandi, Abbeel, and Levine]{lee2019stochastic}
Lee, A.~X., Nagabandi, A., Abbeel, P., and Levine, S.
\newblock Stochastic latent actor-critic: Deep reinforcement learning with a
  latent variable model.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Lee et~al.(2019)Lee, Hou, Mandalika, Lee, Choudhury, and
  Srinivasa]{lee2018bayesian}
Lee, G., Hou, B., Mandalika, A., Lee, J., Choudhury, S., and Srinivasa, S.~S.
\newblock Bayesian policy optimization for model uncertainty.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Lim et~al.(2013)Lim, Xu, and Mannor]{lim2013reinforcement}
Lim, S.~H., Xu, H., and Mannor, S.
\newblock Reinforcement learning in robust markov decision processes.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 26:\penalty0
  701--709, 2013.

\bibitem[Lin et~al.(2020)Lin, Thomas, Yang, and Ma]{lin2020model}
Lin, Z., Thomas, G., Yang, G., and Ma, T.
\newblock Model-based adversarial meta-reinforcement learning.
\newblock In \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Mankowitz et~al.(2019)Mankowitz, Levine, Jeong, Abdolmaleki,
  Springenberg, Shi, Kay, Hester, Mann, and Riedmiller]{mankowitz2019robust}
Mankowitz, D.~J., Levine, N., Jeong, R., Abdolmaleki, A., Springenberg, J.~T.,
  Shi, Y., Kay, J., Hester, T., Mann, T., and Riedmiller, M.
\newblock Robust reinforcement learning for continuous control with model
  misspecification.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Mehta et~al.(2020)Mehta, Diaz, Golemo, Pal, and
  Paull]{mehta2020active}
Mehta, B., Diaz, M., Golemo, F., Pal, C.~J., and Paull, L.
\newblock Active domain randomization.
\newblock In \emph{Conference on Robot Learning (CoRL)}, pp.\  1162--1176.
  PMLR, 2020.

\bibitem[Mordatch et~al.(2015)Mordatch, Lowrey, and
  Todorov]{mordatch2015ensemble}
Mordatch, I., Lowrey, K., and Todorov, E.
\newblock Ensemble-cio: Full-body dynamic motion planning that transfers to
  physical humanoids.
\newblock In \emph{2015 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  5307--5314. IEEE, 2015.

\bibitem[Morimoto \& Doya(2000)Morimoto and Doya]{morimoto2000robust}
Morimoto, J. and Doya, K.
\newblock Robust reinforcement learning.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, pp.\
  1061--1067, 2000.

\bibitem[Mozian et~al.(2020)Mozian, Higuera, Meger, and
  Dudek]{mozian2020learning}
Mozian, M., Higuera, J. C.~G., Meger, D., and Dudek, G.
\newblock Learning domain randomization distributions for training robust
  locomotion policies.
\newblock In \emph{2020 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  6112--6117. IEEE, 2020.

\bibitem[Nagabandi et~al.(2019)Nagabandi, Clavera, Liu, Fearing, Abbeel,
  Levine, and Finn]{nagabandi2018learning}
Nagabandi, A., Clavera, I., Liu, S., Fearing, R.~S., Abbeel, P., Levine, S.,
  and Finn, C.
\newblock Learning to adapt in dynamic, real-world environments through
  meta-reinforcement learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Nilim \& El~Ghaoui(2005)Nilim and El~Ghaoui]{nilim2005robust}
Nilim, A. and El~Ghaoui, L.
\newblock Robust control of markov decision processes with uncertain transition
  matrices.
\newblock \emph{Operations Research}, 53\penalty0 (5):\penalty0 780--798, 2005.

\bibitem[Parisotto et~al.(2016)Parisotto, Ba, and
  Salakhutdinov]{parisotto2016actor}
Parisotto, E., Ba, L.~J., and Salakhutdinov, R.
\newblock Actor-mimic: Deep multitask and transfer reinforcement learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2016.

\bibitem[Perez et~al.(2018)Perez, Such, and Karaletsos]{perez2018efficient}
Perez, C.~F., Such, F.~P., and Karaletsos, T.
\newblock Efficient transfer learning and online adaptation with latent
  variable models for continuous control.
\newblock \emph{arXiv preprint arXiv:1812.03399}, 2018.

\bibitem[Pinto et~al.(2017)Pinto, Davidson, Sukthankar, and
  Gupta]{pinto2017robust}
Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A.
\newblock Robust adversarial reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2817--2826. PMLR, 2017.

\bibitem[Rajeswaran et~al.(2016)Rajeswaran, Ghotra, Ravindran, and
  Levine]{rajeswaran2016epopt}
Rajeswaran, A., Ghotra, S., Ravindran, B., and Levine, S.
\newblock Epopt: Learning robust neural network policies using model ensembles.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Rakelly et~al.(2019)Rakelly, Zhou, Quillen, Finn, and
  Levine]{rakelly2019efficient}
Rakelly, K., Zhou, A., Quillen, D., Finn, C., and Levine, S.
\newblock Efficient off-policy meta-reinforcement learning via probabilistic
  context variables.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Rockafellar et~al.(2000)Rockafellar, Uryasev,
  et~al.]{rockafellar2000optimization}
Rockafellar, R.~T., Uryasev, S., et~al.
\newblock Optimization of conditional value-at-risk.
\newblock \emph{Journal of risk}, 2:\penalty0 21--42, 2000.

\bibitem[Ross et~al.(2007)Ross, Chaib-draa, and Pineau]{ross2007bayes}
Ross, S., Chaib-draa, B., and Pineau, J.
\newblock Bayes-adaptive pomdps.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, pp.\
  1225--1232, 2007.

\bibitem[Rothfuss et~al.(2019)Rothfuss, Lee, Clavera, Asfour, and
  Abbeel]{rothfuss2018promp}
Rothfuss, J., Lee, D., Clavera, I., Asfour, T., and Abbeel, P.
\newblock Promp: Proximal meta-policy search.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Schoettler et~al.(2020)Schoettler, Nair, Ojea, Levine, and
  Solowjow]{schoettler2020meta}
Schoettler, G., Nair, A., Ojea, J.~A., Levine, S., and Solowjow, E.
\newblock Meta-reinforcement learning for robotic industrial insertion tasks.
\newblock In \emph{2020 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  9728--9735. IEEE, 2020.

\bibitem[Sharma et~al.(2019)Sharma, Harrison, Tsao, and
  Pavone]{sharma2019robust}
Sharma, A., Harrison, J., Tsao, M., and Pavone, M.
\newblock Robust and adaptive planning under model uncertainty.
\newblock In \emph{Proceedings of the International Conference on Automated
  Planning and Scheduling}, volume~29, pp.\  410--418, 2019.

\bibitem[Sodhani et~al.(2021)Sodhani, Zhang, and Pineau]{sodhani2021multi}
Sodhani, S., Zhang, A., and Pineau, J.
\newblock Multi-task reinforcement learning with context-based representations.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Song et~al.(2020)Song, Yang, Choromanski, Caluwaerts, Gao, Finn, and
  Tan]{song2020rapidly}
Song, X., Yang, Y., Choromanski, K., Caluwaerts, K., Gao, W., Finn, C., and
  Tan, J.
\newblock Rapidly adaptable legged robots via evolutionary meta-learning.
\newblock In \emph{2020 IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)}, pp.\  3769--3776. IEEE, 2020.

\bibitem[Tamar et~al.(2015)Tamar, Glassner, and Mannor]{tamar2015optimizing}
Tamar, A., Glassner, Y., and Mannor, S.
\newblock Optimizing the cvar via sampling.
\newblock In \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Tan et~al.(2018)Tan, Zhang, Coumans, Iscen, Bai, Hafner, Bohez, and
  Vanhoucke]{tan2018sim}
Tan, J., Zhang, T., Coumans, E., Iscen, A., Bai, Y., Hafner, D., Bohez, S., and
  Vanhoucke, V.
\newblock Sim-to-real: Learning agile locomotion for quadruped robots.
\newblock \emph{Robotics: Science and Systems (RSS)}, 2018.

\bibitem[Tang et~al.(2019)Tang, Zhang, and Salakhutdinov]{tang2019worst}
Tang, Y.~C., Zhang, J., and Salakhutdinov, R.
\newblock Worst cases policy gradients.
\newblock \emph{Conference on Robot Learning (CoRL)}, 2019.

\bibitem[Teh et~al.(2017)Teh, Bapst, Czarnecki, Quan, Kirkpatrick, Hadsell,
  Heess, and Pascanu]{teh2017distral}
Teh, Y.~W., Bapst, V., Czarnecki, W.~M., Quan, J., Kirkpatrick, J., Hadsell,
  R., Heess, N., and Pascanu, R.
\newblock Distral: Robust multitask reinforcement learning.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem[Tessler et~al.(2019)Tessler, Efroni, and Mannor]{tessler2019action}
Tessler, C., Efroni, Y., and Mannor, S.
\newblock Action robust reinforcement learning and applications in continuous
  control.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  6215--6224. PMLR, 2019.

\bibitem[Vinitsky et~al.(2020)Vinitsky, Du, Parvate, Jang, Abbeel, and
  Bayen]{vinitsky2020robust}
Vinitsky, E., Du, Y., Parvate, K., Jang, K., Abbeel, P., and Bayen, A.
\newblock Robust reinforcement learning using adversarial populations.
\newblock \emph{arXiv preprint arXiv:2008.01825}, 2020.

\bibitem[Wang et~al.(2016)Wang, Kurth-Nelson, Tirumala, Soyer, Leibo, Munos,
  Blundell, Kumaran, and Botvinick]{wang2016learning}
Wang, J.~X., Kurth-Nelson, Z., Tirumala, D., Soyer, H., Leibo, J.~Z., Munos,
  R., Blundell, C., Kumaran, D., and Botvinick, M.
\newblock Learning to reinforcement learn.
\newblock \emph{arXiv preprint arXiv:1611.05763}, 2016.

\bibitem[Wang \& Cunningham(2020)Wang and Cunningham]{wang2020posterior}
Wang, Y. and Cunningham, J.~P.
\newblock Posterior collapse and latent variable non-identifiability.
\newblock In \emph{Third Symposium on Advances in Approximate Bayesian
  Inference}, 2020.

\bibitem[Yang et~al.(2020)Yang, Xu, Wu, and Wang]{yang2020multi}
Yang, R., Xu, H., Wu, Y., and Wang, X.
\newblock Multi-task reinforcement learning with soft modularization.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and
  Finn]{yu2020gradient}
Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C.
\newblock Gradient surgery for multi-task learning.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Yu et~al.(2017)Yu, Tan, Liu, and Turk]{yu2017preparing}
Yu, W., Tan, J., Liu, C.~K., and Turk, G.
\newblock Preparing for the unknown: Learning a universal policy with online
  system identification.
\newblock \emph{Robotics: Science and Systems (RSS)}, 2017.

\bibitem[Yu et~al.(2019)Yu, Liu, and Turk]{yu2018policy}
Yu, W., Liu, C.~K., and Turk, G.
\newblock Policy transfer with strategy optimization.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Zahavy et~al.(2020)Zahavy, Barreto, Mankowitz, Hou, O'Donoghue,
  Kemaev, and Singh]{zahavy2020discovering}
Zahavy, T., Barreto, A., Mankowitz, D.~J., Hou, S., O'Donoghue, B., Kemaev, I.,
  and Singh, S.
\newblock Discovering a set of policies for the worst case reward.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Zhang et~al.(2020)Zhang, Cheung, Finn, Levine, and
  Jayaraman]{zhang2020cautious}
Zhang, J., Cheung, B., Finn, C., Levine, S., and Jayaraman, D.
\newblock Cautious adaptation for reinforcement learning in safety-critical
  settings.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11055--11065. PMLR, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Wang, Hu, Chen, Chen, Fan, and
  Zhang]{zhang2021metacure}
Zhang, J., Wang, J., Hu, H., Chen, T., Chen, Y., Fan, C., and Zhang, C.
\newblock Metacure: Meta reinforcement learning with empowerment-driven
  exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  12600--12610. PMLR, 2021.

\bibitem[Zhao et~al.(2020)Zhao, Nagabandi, Rakelly, Finn, and
  Levine]{zhao2020meld}
Zhao, T.~Z., Nagabandi, A., Rakelly, K., Finn, C., and Levine, S.
\newblock Meld: Meta-reinforcement learning from images via latent state
  models.
\newblock \emph{Conference on Robot Learning (CoRL)}, 2020.

\bibitem[Zintgraf et~al.(2020)Zintgraf, Shiarlis, Igl, Schulze, Gal, Hofmann,
  and Whiteson]{zintgraf2019varibad}
Zintgraf, L., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K., and
  Whiteson, S.
\newblock Varibad: A very good method for bayes-adaptive deep rl via
  meta-learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2020.

\bibitem[Zintgraf et~al.(2019)Zintgraf, Shiarlis, Kurin, Hofmann, and
  Whiteson]{zintgraf2018fast}
Zintgraf, L.~M., Shiarlis, K., Kurin, V., Hofmann, K., and Whiteson, S.
\newblock Fast context adaptation via meta-learning.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2019.

\end{thebibliography}
