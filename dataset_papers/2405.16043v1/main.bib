@article{dinkelbach1967nonlinear,
  title={On nonlinear fractional programming},
  author={Dinkelbach, Werner},
  journal={Management science},
  volume={13},
  number={7},
  pages={492--498},
  year={1967},
  publisher={INFORMS}
}


@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{guha2023embroid,
  title={Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification},
  author={Guha, Neel and Chen, Mayee F and Bhatia, Kush and Mirhoseini, Azalia and Sala, Frederic and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2307.11031},
  year={2023}
}
@inproceedings{ding2022gpt,
    title = "Is {GPT}-3 a Good Data Annotator?",
    author = "Ding, Bosheng  and
      Qin, Chengwei  and
      Liu, Linlin  and
      Chia, Yew Ken  and
      Li, Boyang  and
      Joty, Shafiq  and
      Bing, Lidong",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.626",
    doi = "10.18653/v1/2023.acl-long.626",
    pages = "11173--11195",
    abstract = "Data annotation is the process of labeling data that could be used to train machine learning models. Having high quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output. GPT-3, a large-scale language model developed by OpenAI, has demonstrated im- impressive zero- and few-shot performance on a wide range of NLP tasks. It is therefore natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks. Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP.",
}

@article{he2023annollm,
  title={Annollm: Making large language models to be better crowdsourced annotators},
  author={He, Xingwei and Lin, Zhenghao and Gong, Yeyun and Jin, Alex and Zhang, Hang and Lin, Chen and Jiao, Jian and Yiu, Siu Ming and Duan, Nan and Chen, Weizhu and others},
  journal={arXiv preprint arXiv:2303.16854},
  year={2023}
}
@article{kuzman2023chatgpt,
  title={Chatgpt: Beginning of an end of manual linguistic data annotation? use case of automatic genre identification},
  author={Kuzman, Taja and Mozetic, Igor and Ljube{\v{s}}ic, Nikola},
  journal={arXiv preprint arXiv:2303.03953},
  year={2023}
}

@inproceedings{wang2021want,
  title={Want To Reduce Labeling Cost? GPT-3 Can Help},
  author={Wang, Shuohang and Liu, Yang and Xu, Yichong and Zhu, Chenguang and Zeng, Michael},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={4195--4205},
  year={2021}
}
@inproceedings{agrawal2022large,
  title={Large language models are few-shot clinical information extractors},
  author={Agrawal, Monica and Hegselmann, Stefan and Lang, Hunter and Kim, Yoon and Sontag, David},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={1998--2022},
  year={2022}
}
@inproceedings{lang2022co,
  title={Co-training improves prompt-based learning for large language models},
  author={Lang, Hunter and Agrawal, Monica N and Kim, Yoon and Sontag, David},
  booktitle={International Conference on Machine Learning},
  pages={11985--12003},
  year={2022},
  organization={PMLR}
}
@inproceedings{eisenstein2022honest,
  title={Honest Students from Untrusted Teachers: Learning an Interpretable Question-Answering Pipeline from a Pretrained Language Model},
  author={Eisenstein, Jacob and Andor, Daniel and Bohnet, Bernd and Collins, Michael and Mimno, David},
  booktitle={Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS 2022},
  year={2022}
}


@article{zhang2022understanding,
  title={Understanding programmatic weak supervision via source-aware influence function},
  author={Zhang, Jieyu and Wang, Haonan and Hsieh, Cheng-Yu and Ratner, Alexander J},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2862--2875},
  year={2022}
}
@inproceedings{wang-etal-2023-self-instruct,
    title = "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
    author = "Wang, Yizhong  and
      Kordi, Yeganeh  and
      Mishra, Swaroop  and
      Liu, Alisa  and
      Smith, Noah A.  and
      Khashabi, Daniel  and
      Hajishirzi, Hannaneh",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.754",
    doi = "10.18653/v1/2023.acl-long.754",
    pages = "13484--13508",
    abstract = "Large {``}instruction-tuned{''} language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33{\%} absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5{\%} absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.",
}
@inproceedings{bucilua2006model,
  title={Model compression},
  author={Bucilu«é, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={535--541},
  year={2006}
}
@inproceedings{loshchilov2018decoupled,
  title={Decoupled Weight Decay Regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{wang2019clinical,
  title={A clinical text classification paradigm using weak supervision and deep representation},
  author={Wang, Yanshan and Sohn, Sunghwan and Liu, Sijia and Shen, Feichen and Wang, Liwei and Atkinson, Elizabeth J and Amin, Shreyasee and Liu, Hongfang},
  journal={BMC medical informatics and decision making},
  volume={19},
  pages={1--13},
  year={2019},
  publisher={Springer}
}
@inproceedings{robinson2020strength,
  title={Strength from weakness: Fast learning using weak supervision},
  author={Robinson, Joshua and Jegelka, Stefanie and Sra, Suvrit},
  booktitle={International Conference on Machine Learning},
  pages={8127--8136},
  year={2020},
  organization={PMLR}
}
@article{natarajan2013learning,
  title={Learning with noisy labels},
  author={Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@article{ben2006analysis,
  title={Analysis of representations for domain adaptation},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
  journal={Advances in neural information processing systems},
  volume={19},
  year={2006}
}
@inproceedings{kifer2004detecting,
  title={Detecting change in data streams},
  author={Kifer, Daniel and Ben-David, Shai and Gehrke, Johannes},
  booktitle={VLDB},
  volume={4},
  pages={180--191},
  year={2004},
  organization={Toronto, Canada}
}

@inproceedings{
zhang2021wrench,
title={{WRENCH}: A Comprehensive Benchmark for Weak Supervision},
author={Jieyu Zhang and Yue Yu and Yinghao Li and Yujing Wang and Yaming Yang and Mao Yang and Alexander Ratner},
booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
year={2021},
url={https://openreview.net/forum?id=Q9SKS5k8io}
}

@article{blum1981complexity,
  title={The complexity of testing whether a graph is a superconcentrator},
  author={Blum, Manuel and Karp, Richard M. and Vornberger, Oliver and Papadimitriu, Christos H and Yannakakis, Mihalis},
  journal={Information Processing Letters},
  volume={13},
  number={4-5},
  pages={164--167},
  year={1981},
  publisher={Elsevier}
}
@inproceedings{frei2022self,
  title={Self-training converts weak learners to strong learners in mixture models},
  author={Frei, Spencer and Zou, Difan and Chen, Zixiang and Gu, Quanquan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={8003--8021},
  year={2022},
  organization={PMLR}
}
@inproceedings{kumar2020understanding,
  title={Understanding self-training for gradual domain adaptation},
  author={Kumar, Ananya and Ma, Tengyu and Liang, Percy},
  booktitle={International conference on machine learning},
  pages={5468--5479},
  year={2020},
  organization={PMLR}
}
@article{chen2020self,
  title={Self-training avoids using spurious features under domain shift},
  author={Chen, Yining and Wei, Colin and Kumar, Ananya and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21061--21071},
  year={2020}
}
@article{haochen2022beyond,
  title={Beyond separability: Analyzing the linear transferability of contrastive representations to related subpopulations},
  author={HaoChen, Jeff Z and Wei, Colin and Kumar, Ananya and Ma, Tengyu},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={26889--26902},
  year={2022}
}

@InProceedings{oymak2021theoretical,
  title = 	 { A Theoretical Characterization of Semi-supervised Learning with Self-training for Gaussian Mixture Models },
  author =       {Oymak, Samet and Cihad Gulcu, Talha},
  booktitle = 	 {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3601--3609},
  year = 	 {2021},
  editor = 	 {Banerjee, Arindam and Fukumizu, Kenji},
  volume = 	 {130},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v130/oymak21a/oymak21a.pdf},
  url = 	 {https://proceedings.mlr.press/v130/oymak21a.html},
  abstract = 	 { Self-training is a classical approach in semi-supervised learning which is successfully applied to a variety of machine learning problems. Self-training algorithms generate pseudo-labels for the unlabeled examples and progressively refine these pseudo-labels which hopefully coincides with the actual labels. This work provides theoretical insights into self-training algorithms with a focus on linear classifiers. First, we provide a sample complexity analysis for Gaussian mixture models with two components. This is established by sharp non-asymptotic characterization of the self-training iterations which captures the evolution of the model accuracy in terms of a fixed-point iteration. Our analysis reveals the provable benefits of rejecting samples with low confidence and demonstrates how self-training iterations can gracefully improve the model accuracy. Secondly, we study a generalized GMM where the component means follow a distribution. We demonstrate that ridge regularization and class margin (i.e. separation between the component means) is crucial for the success and lack of regularization may prevent self-training from identifying the core features in the data. }
}

@article{ruhling2021end,
  title={End-to-end weak supervision},
  author={R{\"u}hling Cachay, Salva and Boecking, Benedikt and Dubrawski, Artur},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1845--1857},
  year={2021}
}
@inproceedings{sam2023losses,
  title={Losses over labels: Weakly supervised learning via direct loss construction},
  author={Sam, Dylan and Kolter, J Zico},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={8},
  pages={9695--9703},
  year={2023}
}

@article{li2023self,
  title={Self-alignment with instruction backtranslation},
  author={Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo and Zettlemoyer, Luke and Levy, Omer and Weston, Jason and Lewis, Mike},
  journal={arXiv preprint arXiv:2308.06259},
  year={2023}
}
@misc{veselovsky2023prevalence,
      title={Prevalence and prevention of large language model use in crowd work}, 
      author={Veniamin Veselovsky and Manoel Horta Ribeiro and Philip Cozzolino and Andrew Gordon and David Rothschild and Robert West},
      year={2023},
      eprint={2310.15683},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{irvin2019chexpert,
  title={Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison},
  author={Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and Ciurea-Ilcus, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={33},
  number={01},
  pages={590--597},
  year={2019}
}
@article{johnson2019mimic,
  title={MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports},
  author={Johnson, Alistair EW and Pollard, Tom J and Berkowitz, Seth J and Greenbaum, Nathaniel R and Lungren, Matthew P and Deng, Chih-ying and Mark, Roger G and Horng, Steven},
  journal={Scientific data},
  volume={6},
  number={1},
  pages={317},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{chen2022shoring,
  title={Shoring up the foundations: Fusing model embeddings and weak supervision},
  author={Chen, Mayee F and Fu, Daniel Y and Adila, Dyah and Zhang, Michael and Sala, Frederic and Fatahalian, Kayvon and R{\'e}, Christopher},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={357--367},
  year={2022},
  organization={PMLR}
}
@inproceedings{zhu-etal-2023-weaker,
    title = "Weaker Than You Think: A Critical Look at Weakly Supervised Learning",
    author = "Zhu, Dawei  and
      Shen, Xiaoyu  and
      Mosbach, Marius  and
      Stephan, Andreas  and
      Klakow, Dietrich",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.796",
    doi = "10.18653/v1/2023.acl-long.796",
    pages = "14229--14253",
    abstract = "Weakly supervised learning is a popular approach for training machine learning models in low-resource settings. Instead of requesting high-quality yet costly human annotations, it allows training models with noisy annotations obtained from various weak sources. Recently, many sophisticated approaches have been proposed for robust training under label noise, reporting impressive results. In this paper, we revisit the setup of these approaches and find that the benefits brought by these approaches are significantly overestimated. Specifically, we find that the success of existing weakly supervised learning approaches heavily relies on the availability of clean validation samples which, as we show, can be leveraged much more efficiently by simply training on them. After using these clean labels in training, the advantages of using these sophisticated approaches are mostly wiped out. This remains true even when reducing the size of the available clean data to just five samples per class, making these approaches impractical. To understand the true value of weakly supervised learning, we thoroughly analyze diverse NLP datasets and tasks to ascertain when and why weakly supervised approaches work. Based on our findings, we provide recommendations for future research.",
}
@inproceedings{fu2020fast,
  title={Fast and three-rious: Speeding up weak supervision with triplet methods},
  author={Fu, Daniel and Chen, Mayee and Sala, Frederic and Hooper, Sarah and Fatahalian, Kayvon and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={3280--3291},
  year={2020},
  organization={PMLR}
}
@inproceedings{wei2020theoretical,
  title={Theoretical Analysis of Self-Training with Deep Networks on Unlabeled Data},
  author={Wei, Colin and Shen, Kendrick and Chen, Yining and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@inproceedings{cai2021theory,
  title={A theory of label propagation for subpopulation shift},
  author={Cai, Tianle and Gao, Ruiqi and Lee, Jason and Lei, Qi},
  booktitle={International Conference on Machine Learning},
  pages={1170--1182},
  year={2021},
  organization={PMLR}
}
@inproceedings{pukdee2022label,
  title={Label Propagation with Weak Supervision},
  author={Pukdee, Rattana and Sam, Dylan and Ravikumar, Pradeep Kumar and Balcan, Nina},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{balcan2004co,
  title={Co-training and expansion: Towards bridging theory and practice},
  author={Balcan, Maria-Florina and Blum, Avrim and Yang, Ke},
  journal={Advances in neural information processing systems},
  volume={17},
  year={2004}
}
@article{lang2022training,
  title={Training subset selection for weak supervision},
  author={Lang, Hunter and Vijayaraghavan, Aravindan and Sontag, David},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16023--16036},
  year={2022}
}

@article{tsybakov,
  title={Optimal aggregation of classifiers in statistical learning},
  author={Tsybakov, Alexander B},
  journal={The Annals of Statistics},
  volume={32},
  number={1},
  pages={135--166},
  year={2004},
  publisher={Institute of Mathematical Statistics}
}
@inproceedings{cabannnes2021disambiguation,
  title={Disambiguation of weak supervision leading to exponential convergence rates},
  author={Cabannnes, Vivien A and Bach, Francis and Rudi, Alessandro},
  booktitle={International Conference on Machine Learning},
  pages={1147--1157},
  year={2021},
  organization={PMLR}
}
@article{haochen2021provable,
  title={Provable guarantees for self-supervised deep learning with spectral contrastive loss},
  author={HaoChen, Jeff Z and Wei, Colin and Gaidon, Adrien and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={5000--5011},
  year={2021}
}
@inproceedings{warstadt2020learning,
  title={Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually)},
  author={Warstadt, Alex and Zhang, Yian and Li, Xiaocheng and Liu, Haokun and Bowman, Samuel},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={217--235},
  year={2020}
}

@article{wei2021pretrained,
  title={Why do pretrained language models help in downstream tasks? an analysis of head and prompt tuning},
  author={Wei, Colin and Xie, Sang Michael and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={16158--16170},
  year={2021}
}
@inproceedings{yao2022improving,
  title={Improving out-of-distribution robustness via selective augmentation},
  author={Yao, Huaxiu and Wang, Yu and Li, Sai and Zhang, Linjun and Liang, Weixin and Zou, James and Finn, Chelsea},
  booktitle={International Conference on Machine Learning},
  pages={25407--25437},
  year={2022},
  organization={PMLR}
}
@inproceedings{xu2020adversarial,
  title={Adversarial domain adaptation with domain mixup},
  author={Xu, Minghao and Zhang, Jian and Ni, Bingbing and Li, Teng and Wang, Chengjie and Tian, Qi and Zhang, Wenjun},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={04},
  pages={6502--6509},
  year={2020}
}
@inproceedings{abbe2023generalization,
  title={Generalization on the unseen, logic reasoning and degree curriculum},
  author={Abbe, Emmanuel and Bengio, Samy and Lotfi, Aryo and Rizk, Kevin},
  booktitle={International Conference on Machine Learning},
  pages={31--60},
  year={2023},
  organization={PMLR}
}
@article{teney2023selective,
  title={Selective Mixup Helps with Distribution Shifts, But Not (Only) because of Mixup},
  author={Teney, Damien and Wang, Jindong and Abbasnejad, Ehsan},
  journal={arXiv preprint arXiv:2305.16817},
  year={2023}
}
@article{blitzer2007learning,
  title={Learning bounds for domain adaptation},
  author={Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Wortman, Jennifer},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}
@inproceedings{wei2019improved,
  title={Improved sample complexities for deep neural networks and robust classification via an all-layer margin},
  author={Wei, Colin and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{ratner2016data,
  title={Data programming: Creating large training sets, quickly},
  author={Ratner, Alexander J and De Sa, Christopher M and Wu, Sen and Selsam, Daniel and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}
@inproceedings{ratner2017snorkel,
  title={Snorkel: Rapid training data creation with weak supervision},
  author={Ratner, Alexander and Bach, Stephen H and Ehrenberg, Henry and Fries, Jason and Wu, Sen and R{\'e}, Christopher},
  booktitle={Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases},
  volume={11},
  number={3},
  pages={269},
  year={2017},
  organization={NIH Public Access}
}
@article{zhang2022survey,
  title={A survey on programmatic weak supervision},
  author={Zhang, Jieyu and Hsieh, Cheng-Yu and Yu, Yue and Zhang, Chao and Ratner, Alexander},
  journal={arXiv preprint arXiv:2202.05433},
  year={2022}
}
@inproceedings{varma2019learning,
  title={Learning dependency structures for weak supervision models},
  author={Varma, Paroma and Sala, Frederic and He, Ann and Ratner, Alexander and R{\'e}, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={6418--6427},
  year={2019},
  organization={PMLR}
}
@article{rigollet2007generalization,
  title={Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption.},
  author={Rigollet, Philippe},
  journal={Journal of Machine Learning Research},
  volume={8},
  number={7},
  year={2007}
}
@inproceedings{zhang2018w2f,
  title={W2F: A weakly-supervised to fully-supervised framework for object detection},
  author={Zhang, Yongqiang and Bai, Yancheng and Ding, Mingli and Li, Yongqiang and Ghanem, Bernard},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={928--936},
  year={2018}
}
@article{fries2019weakly,
  title={Weakly supervised classification of aortic valve malformations using unlabeled cardiac MRI sequences},
  author={Fries, Jason A and Varma, Paroma and Chen, Vincent S and Xiao, Ke and Tejeda, Heliodoro and Saha, Priyanka and Dunnmon, Jared and Chubb, Henry and Maskatia, Shiraz and Fiterau, Madalina and others},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={3111},
  year={2019},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{varma2018snuba,
  title={Snuba: Automating weak supervision to label training data},
  author={Varma, Paroma and R{\'e}, Christopher},
  booktitle={Proceedings of the VLDB Endowment. International Conference on Very Large Data Bases},
  volume={12},
  number={3},
  pages={223},
  year={2018},
  organization={NIH Public Access}
}
@inproceedings{muennighoff2022mteb,
    title = "{MTEB}: Massive Text Embedding Benchmark",
    author = "Muennighoff, Niklas  and
      Tazi, Nouamane  and
      Magne, Loic  and
      Reimers, Nils",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.148",
    doi = "10.18653/v1/2023.eacl-main.148",
    pages = "2014--2037",
    abstract = "Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings todate. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-theart results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at \url{https://github.com/embeddings-benchmark/mteb}.",
}

@article{northcutt2021confident,
  title={Confident learning: Estimating uncertainty in dataset labels},
  author={Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={1373--1411},
  year={2021}
}
@article{chiang2023unified,
  title={Unified Risk Analysis for Weakly Supervised Learning},
  author={Chiang, Chao-Kai and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2309.08216},
  year={2023}
}
@book{sugiyama2022machine,
  title={Machine Learning from Weak Supervision: An Empirical Risk Minimization Approach},
  author={Sugiyama, Masashi and Bao, Han and Ishida, Takashi and Lu, Nan and Sakai, Tomoya and Niu, Gang},
  year={2022},
  publisher={MIT Press}
}
@inproceedings{meng2018weakly,
  title={Weakly-supervised neural text classification},
  author={Meng, Yu and Shen, Jiaming and Zhang, Chao and Han, Jiawei},
  booktitle={proceedings of the 27th ACM International Conference on information and knowledge management},
  pages={983--992},
  year={2018}
}
@inproceedings{karamanolakis2021self,
  title={Self-Training with Weak Supervision},
  author={Karamanolakis, Giannis and Mukherjee, Subhabrata and Zheng, Guoqing and Hassan, Ahmed},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={845--863},
  year={2021}
}
@inproceedings{lang2021self,
  title={Self-supervised self-supervision by combining deep learning and probabilistic logic},
  author={Lang, Hunter and Poon, Hoifung},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={6},
  pages={4978--4986},
  year={2021}
}
@inproceedings{yu2021fine,
  title={Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach},
  author={Yu, Yue and Zuo, Simiao and Jiang, Haoming and Ren, Wendi and Zhao, Tuo and Zhang, Chao},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1063--1077},
  year={2021}
}
@inproceedings{ratner2019training,
  title={Training complex models with multi-task weak supervision},
  author={Ratner, Alexander and Hancock, Braden and Dunnmon, Jared and Sala, Frederic and Pandey, Shreyash and R{\'e}, Christopher},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={4763--4771},
  year={2019}
}

@article{dawid1979maximum,
  title={Maximum likelihood estimation of observer error-rates using the EM algorithm},
  author={Dawid, Alexander Philip and Skene, Allan M},
  journal={Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume={28},
  number={1},
  pages={20--28},
  year={1979},
  publisher={Wiley Online Library}
}
@inproceedings{khetan2018learning,
  title={Learning From Noisy Singly-labeled Data},
  author={Khetan, Ashish and Lipton, Zachary C and Anandkumar, Animashree},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@article{karger2011iterative,
  title={Iterative learning for reliable crowdsourcing systems},
  author={Karger, David and Oh, Sewoong and Shah, Devavrat},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}
@article{kannan2006blocking,
  title={Blocking conductance and mixing in random walks},
  author={Kannan, Ravi and Lov{\'a}sz, L{\'a}szl{\'o} and Montenegro, Ravi},
  journal={Combinatorics, Probability and Computing},
  volume={15},
  number={4},
  pages={541--570},
  year={2006},
  publisher={Cambridge University Press}
}
@inproceedings{kwok2016improved,
  title={Improved Cheeger's inequality and analysis of local graph partitioning using vertex expansion and expansion profile},
  author={Kwok, Tsz Chiu and Lau, Lap Chi and Lee, Yin Tat},
  booktitle={Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={1848--1861},
  year={2016},
  organization={SIAM}
}
@article{makarychev2023higher,
  title={Higher-Order Cheeger Inequality for Partitioning with Buffers},
  author={Makarychev, Konstantin and Makarychev, Yury and Shan, Liren and Vijayaraghavan, Aravindan},
  journal={arXiv preprint arXiv:2308.10160},
  year={2023}
}
@article{krivelevich2006pseudo,
  title={Pseudo-random graphs},
  author={Krivelevich, Michael and Sudakov, Benny},
  journal={More sets, graphs and numbers: A Salute to Vera Sos and Andr{\'a}s Hajnal},
  pages={199--262},
  year={2006},
  publisher={Springer}
}
@inproceedings{blum1998combining,
  title={Combining labeled and unlabeled data with co-training},
  author={Blum, Avrim and Mitchell, Tom},
  booktitle={Proceedings of the eleventh annual conference on Computational learning theory},
  pages={92--100},
  year={1998}
}
@article{burns2023weak,
  title={Weak-to-strong generalization: Eliciting strong capabilities with weak supervision},
  author={Burns, Collin and Izmailov, Pavel and Kirchner, Jan Hendrik and Baker, Bowen and Gao, Leo and Aschenbrenner, Leopold and Chen, Yining and Ecoffet, Adrien and Joglekar, Manas and Leike, Jan and others},
  journal={arXiv preprint arXiv:2312.09390},
  year={2023}
}
@article{stanton2021does,
  title={Does knowledge distillation really work?},
  author={Stanton, Samuel and Izmailov, Pavel and Kirichenko, Polina and Alemi, Alexander A and Wilson, Andrew G},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6906--6919},
  year={2021}
}
@inproceedings{li2023characterizing,
  title={Characterizing the Impacts of Semi-supervised Learning for Weak Supervision},
  author={Li, Jeffrey and Zhang, Jieyu and Schmidt, Ludwig and Ratner, Alexander},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023}
}
@inproceedings{bousquet2003introduction,
  title={Introduction to statistical learning theory},
  author={Bousquet, Olivier and Boucheron, St{\'e}phane and Lugosi, G{\'a}bor},
  booktitle={Summer school on machine learning},
  pages={169--207},
  year={2003},
  organization={Springer}
}
@article{vapnik1971uniform,
  title={On the uniform convergence of relative frequencies of events to their probabilities},
  author={Vapnik, VN},
  journal={Theory of Probability and its Applications},
  volume={16},
  number={2},
  pages={264--281},
  year={1971}
}
@article{sauer1972density,
  title={On the density of families of sets},
  author={Sauer, Norbert},
  journal={Journal of Combinatorial Theory, Series A},
  volume={13},
  number={1},
  pages={145--147},
  year={1972},
  publisher={Elsevier}
}
@article{shelah1972combinatorial,
  title={A combinatorial problem; stability and order for models and theories in infinitary languages},
  author={Shelah, Saharon},
  journal={Pacific Journal of Mathematics},
  volume={41},
  number={1},
  pages={247--261},
  year={1972},
  publisher={Mathematical Sciences Publishers}
}
@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{sentencebert,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}
@article{roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@InProceedings{imdbdata,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2011},
  address   = {Portland, Oregon, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {142--150},
  url       = {http://www.aclweb.org/anthology/P11-1015}
}
@article{wang2023improving,
  title={Improving Text Embeddings with Large Language Models},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv preprint arXiv:2401.00368},
  year={2023}
}