\begin{thebibliography}{10}

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models, 2023.

\bibitem{openai2024gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2024.

\bibitem{geminiteam2023gemini}
Gemini Team.
\newblock Gemini: A family of highly capable multimodal models, 2023.

\bibitem{jiang2023mistral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed.
\newblock Mistral 7b, 2023.

\bibitem{bubeck2023agisparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, et~al.
\newblock Sparks of artificial general intelligence: Early experiments with gpt-4.
\newblock {\em arXiv preprint arXiv:2303.12712}, 2023.

\bibitem{bommasani2022opportunities}
Rishi Bommasani, Drew~A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael~S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et~al.
\newblock On the opportunities and risks of foundation models, 2022.

\bibitem{bengio2024managing}
Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Trevor Darrell, Yuval~Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, et~al.
\newblock Managing extreme ai risks amid rapid progress.
\newblock {\em Science}, page eadn0117, 2024.

\bibitem{nadeem2021stereoset}
Moin Nadeem, Anna Bethke, and Siva Reddy.
\newblock Stereoset: Measuring stereotypical bias in pretrained language models.
\newblock In {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 5356--5371, 2021.

\bibitem{bai2024measuring}
Xuechunzi Bai, Angelina Wang, Ilia Sucholutsky, and Thomas~L Griffiths.
\newblock Measuring implicit bias in explicitly unbiased large language models.
\newblock {\em arXiv preprint arXiv:2402.04105}, 2024.

\bibitem{gehman2020realtoxicityprompts}
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah~A Smith.
\newblock Realtoxicityprompts: Evaluating neural toxic degeneration in language models.
\newblock In {\em Findings of the Association for Computational Linguistics: EMNLP 2020}, pages 3356--3369, 2020.

\bibitem{shaikh-etal-2023-second}
Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, and Diyi Yang.
\newblock On second thought, let{'}s not think step by step! bias and toxicity in zero-shot reasoning.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 4454--4470, Toronto, Canada, July 2023. Association for Computational Linguistics.

\bibitem{bhardwaj2023redteaming}
Rishabh Bhardwaj and Soujanya Poria.
\newblock Red-teaming large language models using chain of utterances for safety-alignment, 2023.

\bibitem{wang-etal-2024-answer}
Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin.
\newblock Do-not-answer: Evaluating safeguards in {LLM}s.
\newblock In Yvette Graham and Matthew Purver, editors, {\em Findings of the Association for Computational Linguistics: EACL 2024}, pages 896--911, St. Julian{'}s, Malta, March 2024. Association for Computational Linguistics.

\bibitem{wang2023decodingtrust}
Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et~al.
\newblock Decodingtrust: A comprehensive assessment of trustworthiness in gpt models.
\newblock {\em arXiv preprint arXiv:2306.11698}, 2023.

\bibitem{bommasani2023holistic}
Rishi Bommasani, Percy Liang, and Tony Lee.
\newblock Holistic evaluation of language models.
\newblock {\em Annals of the New York Academy of Sciences}, 1525(1):140--146, 2023.

\bibitem{mcintosh2024inadequacies}
Timothy~R McIntosh, Teo Susnjak, Tong Liu, Paul Watters, and Malka~N Halgamuge.
\newblock Inadequacies of large language model benchmarks in the era of generative artificial intelligence.
\newblock {\em arXiv preprint arXiv:2402.09880}, 2024.

\bibitem{mckenzie2023inverse}
Ian~R McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, et~al.
\newblock Inverse scaling: When bigger isn't better.
\newblock {\em arXiv preprint arXiv:2306.09479}, 2023.

\bibitem{goldstein2023generative}
Josh~A Goldstein, Girish Sastry, Micah Musser, Renee DiResta, Matthew Gentzel, and Katerina Sedova.
\newblock Generative language models and automated influence operations: Emerging threats and potential mitigations.
\newblock {\em arXiv preprint arXiv:2301.04246}, 2023.

\bibitem{yao2023value}
Jing Yao, Xiaoyuan Yi, Xiting Wang, Yifan Gong, and Xing Xie.
\newblock Value fulcra: Mapping large language models to the multidimensional spectrum of basic human values.
\newblock {\em arXiv preprint arXiv:2311.10766}, 2023.

\bibitem{cao2023assessing}
Yong Cao, Li~Zhou, Seolhwa Lee, Laura Cabello, Min Chen, and Daniel Hershcovich.
\newblock Assessing cross-cultural alignment between chatgpt and human societies: An empirical study.
\newblock In {\em Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP)}, pages 53--67, 2023.

\bibitem{scherrer2024evaluating}
Nino Scherrer, Claudia Shi, Amir Feder, and David Blei.
\newblock Evaluating the moral beliefs encoded in llms.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{jiang2021can}
Liwei Jiang, Jena~D Hwang, Chandra Bhagavatula, Ronan Le~Bras, Jenny Liang, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jon Borchardt, Saadia Gabriel, et~al.
\newblock Can machines learn morality? the delphi experiment.
\newblock {\em arXiv e-prints}, pages arXiv--2110, 2021.

\bibitem{hendrycks2020ethics}
Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt.
\newblock Aligning ai with shared human values.
\newblock {\em arXiv preprint arXiv:2008.02275}, 2020.

\bibitem{emelin2020moral_stories}
Denis Emelin, Ronan~Le Bras, Jena~D Hwang, Maxwell Forbes, and Yejin Choi.
\newblock Moral stories: Situated reasoning about norms, intents, actions, and their consequences.
\newblock {\em arXiv preprint arXiv:2012.15738}, 2020.

\bibitem{zhang2023safetybench}
Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, and Minlie Huang.
\newblock Safetybench: Evaluating the safety of large language models with multiple choice questions.
\newblock {\em arXiv preprint arXiv:2309.07045}, 2023.

\bibitem{hu2020cross}
Xiaomeng Hu, Yijie Zhu, Feng Yu, David~A Wilder, Li~Zhang, Sylvia~Xiaohua Chen, and Kaiping Peng.
\newblock A cross-cultural examination on global orientations and moral foundations.
\newblock {\em PsyCh Journal}, 9(1):108--117, 2020.

\bibitem{abdulhai2022moral}
Marwa Abdulhai, Cl{\'e}ment Crepy, Daria Valter, John Canny, and Natasha Jaques.
\newblock Moral foundations of large language models.
\newblock In {\em AAAI 2023 Workshop on Representation Learning for Responsible Human-Centric AI}, 2022.

\bibitem{kang2023values}
Dongjun Kang, Joonsuk Park, Yohan Jo, and JinYeong Bak.
\newblock From values to opinions: Predicting human behaviors and stances using value-injected large language models.
\newblock In {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 15539--15559, 2023.

\bibitem{xu2023cvalues}
Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, et~al.
\newblock Cvalues: Measuring the values of chinese large language models from safety to responsibility.
\newblock {\em arXiv preprint arXiv:2307.09705}, 2023.

\bibitem{zhang2023heterogeneous}
Zhaowei Zhang, Nian Liu, Siyuan Qi, Ceyao Zhang, Ziqi Rong, Yaodong Yang, and Shuguang Cui.
\newblock Heterogeneous value evaluation for large language models.
\newblock {\em arXiv preprint arXiv:2305.17147}, 2023.

\bibitem{duan2023denevil}
Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, and Ning Gu.
\newblock Denevil: Towards deciphering and navigating the ethical values of large language models via instruction learning.
\newblock {\em arXiv preprint arXiv:2310.11053}, 2023.

\bibitem{wang2023do_not_answer}
Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin.
\newblock Do-not-answer: A dataset for evaluating safeguards in llms.
\newblock {\em arXiv preprint arXiv:2308.13387}, 2023.

\bibitem{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock {\em Advances in Neural Information Processing Systems}, 35:27730--27744, 2022.

\bibitem{chang2024evaluation_survey}
Yupeng Chang, Xu~Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et~al.
\newblock A survey on evaluation of large language models.
\newblock {\em ACM Transactions on Intelligent Systems and Technology}, 15(3):1--45, 2024.

\bibitem{gilardi2023outperform_crowd}
Fabrizio Gilardi, Meysam Alizadeh, and Ma{\"e}l Kubli.
\newblock Chatgpt outperforms crowd workers for text-annotation tasks.
\newblock {\em Proceedings of the National Academy of Sciences}, 120(30):e2305016120, 2023.

\bibitem{liu2023prompt_evaluation_summarization}
Yixin Liu, Alexander~R Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, and Arman Cohan.
\newblock Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization.
\newblock {\em arXiv preprint arXiv:2311.09184}, 2023.

\bibitem{liu2303geval}
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu.
\newblock G-eval: Nlg evaluation using gpt-4 with better human alignment (2023).
\newblock {\em URL http://arxiv. org/abs/2303.16634}, 2023.

\bibitem{liu2023calibrating}
Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi~Zhang.
\newblock Calibrating llm-based evaluator.
\newblock {\em arXiv preprint arXiv:2309.13308}, 2023.

\bibitem{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock {\em arXiv preprint arXiv:2206.07682}, 2022.

\bibitem{li2023auto_j}
Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu.
\newblock Generative judge for evaluating alignment.
\newblock {\em arXiv preprint arXiv:2310.05470}, 2023.

\bibitem{kim2023prometheus}
Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et~al.
\newblock Prometheus: Inducing fine-grained evaluation capability in language models.
\newblock {\em arXiv preprint arXiv:2310.08491}, 2023.

\bibitem{schwartz2013culture}
Shalom~H Schwartz.
\newblock Culture matters: National value cultures, sources, and consequences.
\newblock In {\em Understanding culture}, pages 127--150. Psychology Press, 2013.

\bibitem{sagiv2017personal}
Lilach Sagiv, Sonia Roccas, Jan Cieciuch, and Shalom~H Schwartz.
\newblock Personal values in human life.
\newblock {\em Nature human behaviour}, 1(9):630--639, 2017.

\bibitem{huang2024only_classifier}
Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Tiejun Zhao.
\newblock An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers.
\newblock {\em arXiv preprint arXiv:2403.02839}, 2024.

\bibitem{schwartz2012basic_value}
Shalom~H Schwartz.
\newblock An overview of the schwartz theory of basic values.
\newblock {\em Online readings in Psychology and Culture}, 2(1):11, 2012.

\bibitem{ji2023beavertails}
Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce~Bian, Ruiyang Sun, Yizhou Wang, and Yaodong Yang.
\newblock Beavertails: Towards improved safety alignment of llm via a human-preference dataset.
\newblock {\em arXiv preprint arXiv:2307.04657}, 2023.

\bibitem{graham2013moral_foundation}
Jesse Graham, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer, Sean~P Wojcik, and Peter~H Ditto.
\newblock Moral foundations theory: The pragmatic validity of moral pluralism.
\newblock In {\em Advances in experimental social psychology}, volume~47, pages 55--130. Elsevier, 2013.

\bibitem{rudinger2018gender}
Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van~Durme.
\newblock Gender bias in coreference resolution.
\newblock {\em arXiv preprint arXiv:1804.09301}, 2018.

\bibitem{dhamala2021bold}
Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta.
\newblock Bold: Dataset and metrics for measuring biases in open-ended language generation.
\newblock In {\em Proceedings of the 2021 ACM conference on fairness, accountability, and transparency}, pages 862--872, 2021.

\bibitem{parrish2021bbq}
Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu~Mon Htut, and Samuel~R Bowman.
\newblock Bbq: A hand-built bias benchmark for question answering.
\newblock {\em arXiv preprint arXiv:2110.08193}, 2021.

\bibitem{cui2023fft}
Shiyao Cui, Zhenyu Zhang, Yilong Chen, Wenyuan Zhang, Tianyun Liu, Siqi Wang, and Tingwen Liu.
\newblock Fft: Towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity.
\newblock {\em arXiv preprint arXiv:2311.18580}, 2023.

\bibitem{sun2024trustllm}
Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et~al.
\newblock Trustllm: Trustworthiness in large language models.
\newblock {\em arXiv preprint arXiv:2401.05561}, 2024.

\bibitem{yao2023value_fulcra}
Jing Yao, Xiaoyuan Yi, Xiting Wang, Yifan Gong, and Xing Xie.
\newblock Value fulcra: Mapping large language models to the multidimensional spectrum of basic human values.
\newblock {\em arXiv preprint arXiv:2311.10766}, 2023.

\bibitem{scherrer2024emoral_belief}
Nino Scherrer, Claudia Shi, Amir Feder, and David Blei.
\newblock Evaluating the moral beliefs encoded in llms.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{shen2023prompt_evaluation_summarization}
Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing.
\newblock Large language models are not yet human-level evaluators for abstractive summarization.
\newblock In {\em Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 4215--4233, 2023.

\bibitem{zhang2024prompt_evaluation_dialogue}
Chen Zhang, Luis~Fernando D'Haro, Yiming Chen, Malu Zhang, and Haizhou Li.
\newblock A comprehensive analysis of the effectiveness of large language models as automatic dialogue evaluators.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 19515--19524, 2024.

\bibitem{zheng2024judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{lin2023llm_eval}
Yen-Ting Lin and Yun-Nung Chen.
\newblock Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models.
\newblock {\em arXiv preprint arXiv:2305.13711}, 2023.

\bibitem{wang2023position_bias}
Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi~Liu, Tianyu Liu, and Zhifang Sui.
\newblock Large language models are not fair evaluators.
\newblock {\em arXiv preprint arXiv:2305.17926}, 2023.

\bibitem{brown2020gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{zhou2023ethics_cot}
Jingyan Zhou, Minda Hu, Junan Li, Xiaoying Zhang, Xixin Wu, Irwin King, and Helen Meng.
\newblock Rethinking machine ethics--can llms perform moral reasoning through the lens of moral theories?
\newblock {\em arXiv preprint arXiv:2308.15399}, 2023.

\bibitem{wei2022CoT}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:24824--24837, 2022.

\bibitem{wu2023diverse_roles}
Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin Jiang.
\newblock Large language models are diverse role-players for summarization evaluation.
\newblock In {\em CCF International Conference on Natural Language Processing and Chinese Computing}, pages 695--707. Springer, 2023.

\bibitem{chan2023chateval}
Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu.
\newblock Chateval: Towards better llm-based evaluators through multi-agent debate.
\newblock {\em arXiv preprint arXiv:2308.07201}, 2023.

\bibitem{li2023prd}
Ruosen Li, Teerth Patel, and Xinya Du.
\newblock Prd: Peer rank and discussion improve large language model based evaluations.
\newblock {\em arXiv preprint arXiv:2307.02762}, 2023.

\bibitem{hasanbeig2023allure}
Hosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Felipe~Vieira Frujeri, and Ida Momennejad.
\newblock Allure: A systematic protocol for auditing and improving llm-based evaluation of text using iterative in-context-learning.
\newblock {\em arXiv preprint arXiv:2309.13701}, 2023.

\bibitem{kopf2023openassistant}
Andreas K{\"o}pf, Yannic Kilcher, Dimitri von R{\"u}tte, Sotiris Anagnostidis, Zhi~Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Rich{\'a}rd Nagyfi, et~al.
\newblock Openassistant conversations-democratizing large language model alignment.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem{lambert2024rewardbench}
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ~Miranda, Bill~Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et~al.
\newblock Rewardbench: Evaluating reward models for language modeling.
\newblock {\em arXiv preprint arXiv:2403.13787}, 2024.

\bibitem{ke2023critiquellm}
Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, et~al.
\newblock Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation.
\newblock {\em arXiv preprint arXiv:2311.18702}, 2023.

\bibitem{liu2023xeval}
Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav Kumar, Reza Ghanadan, and Lifu Huang.
\newblock X-eval: Generalizable multi-aspect text evaluation via augmented instruction tuning with auxiliary evaluation aspects.
\newblock {\em arXiv preprint arXiv:2311.08788}, 2023.

\bibitem{wang2023shepherd}
Tianlu Wang, Ping Yu, Xiaoqing~Ellen Tan, Sean O'Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz.
\newblock Shepherd: A critic for language model generation.
\newblock {\em arXiv preprint arXiv:2308.04592}, 2023.

\bibitem{gekhman2023distill}
Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, and Idan Szpektor.
\newblock Trueteacher: Learning factual consistency evaluation with large language models.
\newblock {\em arXiv preprint arXiv:2305.11171}, 2023.

\bibitem{ramirez2024optimising}
Guillem Ram{\'\i}rez, Alexandra Birch, and Ivan Titov.
\newblock Optimising calls to large language models with uncertainty-based two-tier selection.
\newblock {\em arXiv preprint arXiv:2405.02134}, 2024.

\bibitem{chen2023hybrid}
Lingjiao Chen, Matei Zaharia, and James Zou.
\newblock Frugalgpt: How to use large language models while reducing cost and improving performance.
\newblock {\em arXiv preprint arXiv:2305.05176}, 2023.

\bibitem{ding2024hybrid}
Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Ruhle, Laks~VS Lakshmanan, and Ahmed~Hassan Awadallah.
\newblock Hybrid llm: Cost-efficient and quality-aware query routing.
\newblock {\em arXiv preprint arXiv:2404.14618}, 2024.

\bibitem{mullner2011agglomerative_clustering}
Daniel M{\"u}llner.
\newblock Modern hierarchical, agglomerative clustering algorithms.
\newblock {\em arXiv preprint arXiv:1109.2378}, 2011.

\bibitem{zhang2023wider}
Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li.
\newblock Wider and deeper llm networks are fairer llm evaluators.
\newblock {\em arXiv preprint arXiv:2308.01862}, 2023.

\bibitem{radford2019gpt2}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{abdin2024phi3}
Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et~al.
\newblock Phi-3 technical report: A highly capable language model locally on your phone.
\newblock {\em arXiv preprint arXiv:2404.14219}, 2024.

\bibitem{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{qiu2022valuenet}
Liang Qiu, Yizhou Zhao, Jinchao Li, Pan Lu, Baolin Peng, Jianfeng Gao, and Song-Chun Zhu.
\newblock Valuenet: A new dataset for human value driven dialogue system.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~36, pages 11183--11191, 2022.

\end{thebibliography}
