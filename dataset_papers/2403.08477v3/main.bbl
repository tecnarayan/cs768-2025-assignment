\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Basu et~al.(2023)Basu, Massiceti, Hu, and Feizi]{basustrong2023}
Basu, S., Massiceti, D., Hu, S.~X., and Feizi, S.
\newblock Strong {Baselines} for {Parameter} {Efficient} {Few}-{Shot} {Fine}-tuning, April 2023.
\newblock arXiv:2304.01917 [cs].

\bibitem[Bateni et~al.(2019)Bateni, Goyal, Masrani, Wood, and Sigal]{Bateni2019ImprovedFV}
Bateni, P., Goyal, R., Masrani, V., Wood, F.~D., and Sigal, L.
\newblock Improved few-shot visual classification.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  14481--14490, 2019.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von Arx, Bernstein, Bohg, Bosselut, Brunskill, et~al.]{bommasani2021opportunities}
Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J\'egou, Mairal, Bojanowski, and Joulin]{caron2021emerging}
Caron, M., Touvron, H., Misra, I., J\'egou, H., Mairal, J., Bojanowski, P., and Joulin, A.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{IEEE International Conference on Computer Vision}, 2021.

\bibitem[Chen et~al.(2022)Chen, Lu, and Chen]{chen2022understanding}
Chen, L., Lu, S., and Chen, T.
\newblock Understanding benign overfitting in gradient-based meta learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 19887--19899, 2022.

\bibitem[Chen et~al.(2023)Chen, Huang, Schwarz, Du, and Wei]{chen2023secure}
Chen, S., Huang, L.-K., Schwarz, J.~R., Du, Y., and Wei, Y.
\newblock Secure out-of-distribution task generalization with energy-based models.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem[Chen et~al.(2021)Chen, Zhong, Zha, Karypis, and He]{Chen2021MetalearningVL}
Chen, Y., Zhong, R., Zha, S., Karypis, G., and He, H.
\newblock Meta-learning via language model in-context tuning.
\newblock \emph{ArXiv}, abs/2110.07814, 2021.

\bibitem[Dai et~al.(2022)Dai, Dong, Ma, Zheng, Sui, Chang, and Wei]{dai-etal-2022-stablemoe}
Dai, D., Dong, L., Ma, S., Zheng, B., Sui, Z., Chang, B., and Wei, F.
\newblock {S}table{M}o{E}: Stable routing strategy for mixture of experts.
\newblock In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  7085--7095, Dublin, Ireland, May 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.acl-long.489}.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby]{Dosovitskiy2020AnII}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{ArXiv}, abs/2010.11929, 2020.

\bibitem[Eustratiadis et~al.(2024)Eustratiadis, Dudziak, Li, and Hospedales]{eustratiadis2024neural}
Eustratiadis, P., Dudziak, {\L}., Li, D., and Hospedales, T.
\newblock Neural fine-tuning search for few-shot learning.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.

\bibitem[Fedus et~al.(2021)Fedus, Zoph, and Shazeer]{Fedus2021SwitchTS}
Fedus, W., Zoph, B., and Shazeer, N.~M.
\newblock Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.
\newblock \emph{J. Mach. Learn. Res.}, 23:\penalty0 120:1--120:39, 2021.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Flennerhag et~al.(2022)Flennerhag, Schroecker, Zahavy, van Hasselt, Silver, and Singh]{flennerhag2022bootstrapped}
Flennerhag, S., Schroecker, Y., Zahavy, T., van Hasselt, H., Silver, D., and Singh, S.
\newblock Bootstrapped meta-learning.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Gallego-Posada et~al.(2022)Gallego-Posada, Ramirez, Erraqabi, Bengio, and Lacoste-Julien]{gallego-posada2022controlled}
Gallego-Posada, J., Ramirez, J., Erraqabi, A., Bengio, Y., and Lacoste-Julien, S.
\newblock Controlled sparsity via constrained optimization or: How i learned to stop tuning penalties and love constraints.
\newblock In \emph{Thirty-Sixth Conference on Neural Information Processing Systems}, 2022.

\bibitem[Gao et~al.(2021)Gao, Fisch, and Chen]{Gao2021MakingPL}
Gao, T., Fisch, A., and Chen, D.
\newblock Making pre-trained language models better few-shot learners.
\newblock In \emph{Annual Meeting of the Association for Computational Linguistics}, 2021.

\bibitem[Grant et~al.(2018)Grant, Finn, Levine, Darrell, and Griffiths]{grant2018recasting}
Grant, E., Finn, C., Levine, S., Darrell, T., and Griffiths, T.
\newblock Recasting gradient-based meta-learning as hierarchical bayes.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hong et~al.(2023)Hong, Zhang, Li, Li, Li, Yao, Yokoya, Li, Jia, Plaza, et~al.]{hong2023spectralgpt}
Hong, D., Zhang, B., Li, X., Li, Y., Li, C., Yao, J., Yokoya, N., Li, H., Jia, X., Plaza, A., et~al.
\newblock Spectralgpt: Spectral foundation model.
\newblock \emph{arXiv preprint arXiv:2311.07113}, 2023.

\bibitem[Hospedales et~al.(2020)Hospedales, Antoniou, Micaelli, and Storkey]{Hospedales2020MetaLearningIN}
Hospedales, T.~M., Antoniou, A., Micaelli, P., and Storkey, A.~J.
\newblock Meta-learning in neural networks: A survey.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence}, 44:\penalty0 5149--5169, 2020.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models.
\newblock \emph{arXiv preprint arXiv:2106.09685}, 2021.

\bibitem[Hu et~al.(2022)Hu, Li, St\"uhmer, Kim, and Hospedales]{hu2022pmf}
Hu, S.~X., Li, D., St\"uhmer, J., Kim, M., and Hospedales, T.~M.
\newblock Pushing the limits of simple pipelines for few-shot learning: External data and fine-tuning make a difference.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition}, 2022.

\bibitem[Ilharco et~al.(2022)Ilharco, Wortsman, Gadre, Song, Hajishirzi, Kornblith, Farhadi, and Schmidt]{Ilharco2022PatchingOM}
Ilharco, G., Wortsman, M., Gadre, S.~Y., Song, S., Hajishirzi, H., Kornblith, S., Farhadi, A., and Schmidt, L.
\newblock Patching open-vocabulary models by interpolating weights.
\newblock \emph{ArXiv}, abs/2208.05592, 2022.

\bibitem[Ilharco et~al.(2023)Ilharco, Ribeiro, Wortsman, Schmidt, Hajishirzi, and Farhadi]{ilharco2023editing}
Ilharco, G., Ribeiro, M.~T., Wortsman, M., Schmidt, L., Hajishirzi, H., and Farhadi, A.
\newblock Editing models with task arithmetic.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Jang et~al.(2017)Jang, Gu, and Poole]{jang2017categorical}
Jang, E., Gu, S., and Poole, B.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Lee et~al.(2021)Lee, Tack, Lee, and Shin]{lee2021meta}
Lee, J., Tack, J., Lee, N., and Shin, J.
\newblock Meta-learning sparse implicit neural representations.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 11769--11780, 2021.

\bibitem[Lee et~al.(2022)Lee, Chen, Tajwar, Kumar, Yao, Liang, and Finn]{Lee2022SurgicalFI}
Lee, Y., Chen, A.~S., Tajwar, F., Kumar, A., Yao, H., Liang, P., and Finn, C.
\newblock Surgical fine-tuning improves adaptation to distribution shifts.
\newblock \emph{ArXiv}, abs/2210.11466, 2022.

\bibitem[Lepikhin et~al.(2021)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun, Shazeer, and Chen]{lepikhin2021gshard}
Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z.
\newblock {\{}GS{\}}hard: Scaling giant models with conditional computation and automatic sharding.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Lewis et~al.(2021)Lewis, Bhosale, Dettmers, Goyal, and Zettlemoyer]{Lewis2021BASELS}
Lewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettlemoyer, L.
\newblock Base layers: Simplifying training of large, sparse models.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Li et~al.(2022)Li, Liu, and Bilen]{li2022TaskSpecificAdapter}
Li, W.-H., Liu, X., and Bilen, H.
\newblock Cross-domain few-shot learning with task-specific adapters.
\newblock In \emph{IEEE/CVF International Conference on Computer Vision and Pattern Recognition (CVPR)}, June 2022.

\bibitem[Li et~al.(2017)Li, Zhou, Chen, and Li]{Li2017MetaSGDLT}
Li, Z., Zhou, F., Chen, F., and Li, H.
\newblock Meta-sgd: Learning to learn quickly for few shot learning.
\newblock \emph{ArXiv}, abs/1707.09835, 2017.

\bibitem[Lin et~al.(2023)Lin, Akin, Rao, Hie, Zhu, Lu, Smetanin, Verkuil, Kabeli, Shmueli, et~al.]{lin2023evolutionary}
Lin, Z., Akin, H., Rao, R., Hie, B., Zhu, Z., Lu, W., Smetanin, N., Verkuil, R., Kabeli, O., Shmueli, Y., et~al.
\newblock Evolutionary-scale prediction of atomic-level protein structure with a language model.
\newblock \emph{Science}, 379\penalty0 (6637):\penalty0 1123--1130, 2023.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Yang, Lu, Chen, Li, Zhang, Bai, Fang, Sun, Yu, et~al.]{liu2023towards}
Liu, J., Yang, C., Lu, Z., Chen, J., Li, Y., Zhang, M., Bai, T., Fang, Y., Sun, L., Yu, P.~S., et~al.
\newblock Towards graph foundation models: A survey and beyond.
\newblock \emph{arXiv preprint arXiv:2310.11829}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Puigcerver, and Blondel]{liu2023sparsityconstrained}
Liu, T., Puigcerver, J., and Blondel, M.
\newblock Sparsity-constrained optimal transport.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2021)Liu, Lee, Zhu, Chen, Shi, and Yang]{triM}
Liu, Y., Lee, J., Zhu, L., Chen, L., Shi, H., and Yang, Y.
\newblock A multi-mode modulator for multi-domain few-shot classification.
\newblock In \emph{2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, pp.\  8433--8442, 2021.
\newblock \doi{10.1109/ICCV48922.2021.00834}.

\bibitem[Louizos et~al.(2018)Louizos, Welling, and Kingma]{louizos2018learning}
Louizos, C., Welling, M., and Kingma, D.~P.
\newblock Learning sparse neural networks through $l_0$ regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Matena \& Raffel(2021)Matena and Raffel]{Matena2021MergingMW}
Matena, M. and Raffel, C.
\newblock Merging models with fisher-weighted averaging.
\newblock \emph{ArXiv}, abs/2111.09832, 2021.

\bibitem[Mikolov et~al.(2013)Mikolov, Sutskever, Chen, Corrado, and Dean]{mikolov2013distributed}
Mikolov, T., Sutskever, I., Chen, K., Corrado, G.~S., and Dean, J.
\newblock Distributed representations of words and phrases and their compositionality.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Min et~al.(2021)Min, Lewis, Zettlemoyer, and Hajishirzi]{Min2021MetaICLLT}
Min, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H.
\newblock Metaicl: Learning to learn in context.
\newblock \emph{ArXiv}, abs/2110.15943, 2021.

\bibitem[Muqeeth et~al.(2023)Muqeeth, Liu, and Raffel]{Muqeeth2023SoftMO}
Muqeeth, M., Liu, H., and Raffel, C.
\newblock Soft merging of experts with adaptive routing.
\newblock \emph{ArXiv}, abs/2306.03745, 2023.

\bibitem[Mustafa et~al.(2022)Mustafa, Ruiz, Puigcerver, Jenatton, and Houlsby]{mustafa2022multimodal}
Mustafa, B., Ruiz, C.~R., Puigcerver, J., Jenatton, R., and Houlsby, N.
\newblock Multimodal contrastive learning with {LIM}oe: the language-image mixture of experts.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Oh et~al.(2020)Oh, Yoo, Kim, and Yun]{oh2020boil}
Oh, J., Yoo, H., Kim, C., and Yun, S.-Y.
\newblock Boil: Towards representation change for few-shot learning.
\newblock \emph{arXiv preprint arXiv:2008.08882}, 2020.

\bibitem[Panigrahi et~al.(2023{\natexlab{a}})Panigrahi, Saunshi, Zhao, and Arora]{Panigrahi2023TaskSpecificSL}
Panigrahi, A., Saunshi, N., Zhao, H., and Arora, S.
\newblock Task-specific skill localization in fine-tuned language models.
\newblock In \emph{International Conference on Machine Learning}, 2023{\natexlab{a}}.

\bibitem[Panigrahi et~al.(2023{\natexlab{b}})Panigrahi, Saunshi, Zhao, and Arora]{panigrahi2023task}
Panigrahi, A., Saunshi, N., Zhao, H., and Arora, S.
\newblock Task-specific skill localization in fine-tuned language models.
\newblock \emph{arXiv preprint arXiv:2302.06600}, 2023{\natexlab{b}}.

\bibitem[Perez et~al.(2018)Perez, Strub, De~Vries, Dumoulin, and Courville]{perez2018film}
Perez, E., Strub, F., De~Vries, H., Dumoulin, V., and Courville, A.
\newblock Film: Visual reasoning with a general conditioning layer.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, 2018.

\bibitem[Puigcerver et~al.(2023)Puigcerver, Riquelme, Mustafa, and Houlsby]{Puigcerver2023FromST}
Puigcerver, J., Riquelme, C., Mustafa, B., and Houlsby, N.
\newblock From sparse to soft mixtures of experts.
\newblock \emph{ArXiv}, abs/2308.00951, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pp.\  8748--8763. PMLR, 2021.

\bibitem[Raghu et~al.(2019)Raghu, Raghu, Bengio, and Vinyals]{raghu2019rapid}
Raghu, A., Raghu, M., Bengio, S., and Vinyals, O.
\newblock Rapid learning or feature reuse? towards understanding the effectiveness of maml.
\newblock \emph{arXiv preprint arXiv:1909.09157}, 2019.

\bibitem[Requeima et~al.(2019)Requeima, Gordon, Bronskill, Nowozin, and Turner]{requeima2019fast}
Requeima, J., Gordon, J., Bronskill, J., Nowozin, S., and Turner, R.~E.
\newblock Fast and flexible multi-task classification using conditional neural adaptive processes.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Riquelme et~al.(2021)Riquelme, Puigcerver, Mustafa, Neumann, Jenatton, Pinto, Keysers, and Houlsby]{Riquelme2021ScalingVW}
Riquelme, C., Puigcerver, J., Mustafa, B., Neumann, M., Jenatton, R., Pinto, A.~S., Keysers, D., and Houlsby, N.
\newblock Scaling vision with sparse mixture of experts.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Roller et~al.(2021)Roller, Sukhbaatar, Szlam, and Weston]{Roller2021HashLF}
Roller, S., Sukhbaatar, S., Szlam, A., and Weston, J.
\newblock Hash layers for large sparse models.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Schwarz \& Teh(2022)Schwarz and Teh]{schwarz2022meta}
Schwarz, J.~R. and Teh, Y.~W.
\newblock Meta-learning sparse compression networks.
\newblock \emph{arXiv preprint arXiv:2205.08957}, 2022.

\bibitem[Schwarz et~al.(2023)Schwarz, Tack, Teh, Lee, and Shin]{schwarz2023modality}
Schwarz, J.~R., Tack, J., Teh, Y.~W., Lee, J., and Shin, J.
\newblock Modality-agnostic variational compression of implicit neural representations.
\newblock \emph{arXiv preprint arXiv:2301.09479}, 2023.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton, and Dean]{shazeer2017}
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J.
\newblock Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Shysheya et~al.(2022)Shysheya, Bronskill, Patacchiola, Nowozin, and Turner]{shysheya2022fit}
Shysheya, A., Bronskill, J., Patacchiola, M., Nowozin, S., and Turner, R.~E.
\newblock Fit: Parameter efficient few-shot transfer learning for personalized and federated image classification.
\newblock \emph{arXiv preprint arXiv:2206.08671}, 2022.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell_prototypical_2017}
Snell, J., Swersky, K., and Zemel, R.
\newblock Prototypical {Networks} for {Few}-shot {Learning}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Tack et~al.(2024)Tack, Kim, Yu, Lee, Shin, and Schwarz]{tack2024learning}
Tack, J., Kim, S., Yu, S., Lee, J., Shin, J., and Schwarz, J.~R.
\newblock Learning large-scale neural fields via context pruned meta-learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Thrun \& Pratt(2012)Thrun and Pratt]{thrun2012learning}
Thrun, S. and Pratt, L.
\newblock \emph{Learning to learn}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Triantafillou et~al.(2020)Triantafillou, Zhu, Dumoulin, Lamblin, Evci, Xu, Goroshin, Gelada, Swersky, Manzagol, and Larochelle]{Triantafillou2020Meta-Dataset}
Triantafillou, E., Zhu, T., Dumoulin, V., Lamblin, P., Evci, U., Xu, K., Goroshin, R., Gelada, C., Swersky, K., Manzagol, P.-A., and Larochelle, H.
\newblock Meta-dataset: A dataset of datasets for learning to learn from few examples.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Triantafillou et~al.(2021{\natexlab{a}})Triantafillou, Larochelle, Zemel, and Dumoulin]{triantafillou2021learning}
Triantafillou, E., Larochelle, H., Zemel, R., and Dumoulin, V.
\newblock Learning a universal template for few-shot dataset generalization.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10424--10433. PMLR, 2021{\natexlab{a}}.

\bibitem[Triantafillou et~al.(2021{\natexlab{b}})Triantafillou, Larochelle, Zemel, and Dumoulin]{Triantafillou2021LearningAU}
Triantafillou, E., Larochelle, H., Zemel, R.~S., and Dumoulin, V.
\newblock Learning a universal template for few-shot dataset generalization.
\newblock \emph{ArXiv}, abs/2105.07029, 2021{\natexlab{b}}.

\bibitem[Von~Oswald et~al.(2021)Von~Oswald, Zhao, Kobayashi, Schug, Caccia, Zucchet, and Sacramento]{von2021learning}
Von~Oswald, J., Zhao, D., Kobayashi, S., Schug, S., Caccia, M., Zucchet, N., and Sacramento, J.
\newblock Learning where to learn: Gradient sparsity in meta and continual learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Wang et~al.(2020)Wang, Tsvetkov, Firat, and Cao]{Wang2020GradientVI}
Wang, Z., Tsvetkov, Y., Firat, O., and Cao, Y.
\newblock Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models.
\newblock \emph{ArXiv}, abs/2010.05874, 2020.

\bibitem[Wortsman et~al.(2021)Wortsman, Ilharco, Li, Kim, Hajishirzi, Farhadi, Namkoong, and Schmidt]{Wortsman2021RobustFO}
Wortsman, M., Ilharco, G., Li, M., Kim, J.~W., Hajishirzi, H., Farhadi, A., Namkoong, H., and Schmidt, L.
\newblock Robust fine-tuning of zero-shot models.
\newblock \emph{2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  7949--7961, 2021.

\bibitem[Yao et~al.(2021)Yao, Wang, Wei, Zhao, Mahdavi, Lian, and Finn]{yao2021meta}
Yao, H., Wang, Y., Wei, Y., Zhao, P., Mahdavi, M., Lian, D., and Finn, C.
\newblock Meta-learning with an adaptive task scheduler.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 7497--7509, 2021.

\bibitem[Yeh et~al.(2023)Yeh, Dai, Chen, Zheng, Fan, Der, Lai, Zhuang, Wang, Wang, et~al.]{yeh2023toward}
Yeh, C.-C.~M., Dai, X., Chen, H., Zheng, Y., Fan, Y., Der, A., Lai, V., Zhuang, Z., Wang, J., Wang, L., et~al.
\newblock Toward a foundation model for time series data.
\newblock In \emph{Proceedings of the 32nd ACM International Conference on Information and Knowledge Management}, pp.\  4400--4404, 2023.

\bibitem[Yu et~al.(2023)Yu, Shi, Pasunuru, Muller, Golovneva, Wang, Babu, Tang, Karrer, Sheynin, et~al.]{yu2023scaling}
Yu, L., Shi, B., Pasunuru, R., Muller, B., Golovneva, O., Wang, T., Babu, A., Tang, B., Karrer, B., Sheynin, S., et~al.
\newblock Scaling autoregressive multi-modal models: Pretraining and instruction tuning.
\newblock \emph{arXiv preprint arXiv:2309.02591}, 2023.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and Finn]{Yu2020GradientSF}
Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C.
\newblock Gradient surgery for multi-task learning.
\newblock \emph{ArXiv}, abs/2001.06782, 2020.

\bibitem[Zhang et~al.(2023)Zhang, Dong, Li, Zhang, Sun, Wang, Li, Hu, Zhang, Wu, et~al.]{zhang2023instruction}
Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., et~al.
\newblock Instruction tuning for large language models: A survey.
\newblock \emph{arXiv preprint arXiv:2308.10792}, 2023.

\bibitem[Zhou et~al.(2022)Zhou, Lei, Liu, Du, Huang, Zhao, Dai, Chen, Le, and Laudon]{zhou2022mixtureofexperts}
Zhou, Y., Lei, T., Liu, H., Du, N., Huang, Y., Zhao, V.~Y., Dai, A.~M., Chen, Z., Le, Q.~V., and Laudon, J.
\newblock Mixture-of-experts with expert choice routing.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Zhuang et~al.(2020)Zhuang, Qi, Duan, Xi, Zhu, Zhu, Xiong, and He]{zhuang2020comprehensive}
Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H., and He, Q.
\newblock A comprehensive survey on transfer learning.
\newblock \emph{Proceedings of the IEEE}, 109\penalty0 (1):\penalty0 43--76, 2020.

\bibitem[Zintgraf et~al.(2019)Zintgraf, Shiarlis, Kurin, Hofmann, and Whiteson]{zintgraf2019fast}
Zintgraf, L.~M., Shiarlis, K., Kurin, V., Hofmann, K., and Whiteson, S.
\newblock Caml: Fast context adaptation via meta-learning.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Zuo et~al.(2022)Zuo, Liu, Jiao, Kim, Hassan, Zhang, Gao, and Zhao]{zuo2022taming}
Zuo, S., Liu, X., Jiao, J., Kim, Y.~J., Hassan, H., Zhang, R., Gao, J., and Zhao, T.
\newblock Taming sparsely activated transformer with stochastic experts.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\end{thebibliography}
