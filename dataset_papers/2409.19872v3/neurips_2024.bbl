\begin{thebibliography}{10}

\bibitem{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{chen2023improving}
Dong Chen, Kaihang Pan, Guoming Wang, Yueting Zhuang, and Siliang Tang.
\newblock Improving vision anomaly detection with the guidance of language modality.
\newblock {\em arXiv preprint arXiv:2310.02821}, 2023.

\bibitem{chen2023decoupling}
Xiang Chen, Lei Li, Ningyu Zhang, Xiaozhuan Liang, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, and Huajun Chen.
\newblock Decoupling knowledge from memorization: Retrieval-augmented prompt learning, 2023.

\bibitem{cheng-etal-2023-edit}
Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi~Chen, Yongheng Wang, Huajun Chen, and Ningyu Zhang.
\newblock Can we edit multimodal large language models?
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 13877--13888, Singapore, December 2023. Association for Computational Linguistics.

\bibitem{dai2021knowledge}
Damai Dai, Li~Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei.
\newblock Knowledge neurons in pretrained transformers.
\newblock {\em arXiv preprint arXiv:2104.08696}, 2021.

\bibitem{de2021editing}
Nicola De~Cao, Wilker Aziz, and Ivan Titov.
\newblock Editing factual knowledge in language models.
\newblock {\em arXiv preprint arXiv:2104.08164}, 2021.

\bibitem{fei2024video}
Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Meishan Zhang, Mong-Li Lee, and Wynne Hsu.
\newblock Video-of-thought: Step-by-step video reasoning from perception to cognition.
\newblock In {\em Proceedings of the International Conference on Machine Learning}, 2024.

\bibitem{fei2024vitron}
Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan.
\newblock Vitron: A unified pixel-level vision llm for understanding, generating, segmenting, editing.
\newblock 2024.

\bibitem{fei2024enhancing}
Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, and Shuicheng Yan.
\newblock Enhancing video-language representations with structural spatio-temporal alignment.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 2024.

\bibitem{gao2023llamaadapter}
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu~Qiao.
\newblock Llama-adapter v2: Parameter-efficient visual instruction model, 2023.

\bibitem{huang2023transformer}
Zeyu Huang, Yikang Shen, Xiaofeng Zhang, Jie Zhou, Wenge Rong, and Zhang Xiong.
\newblock Transformer-patcher: One mistake worth one neuron.
\newblock {\em arXiv preprint arXiv:2301.09785}, 2023.

\bibitem{jin2024rethinking}
Tao Jin, Wang Lin, Ye~Wang, Linjun Li, Xize Cheng, and Zhou Zhao.
\newblock Rethinking the multimodal correlation of multimodal sequential learning via generalizable attentional results alignment.
\newblock In {\em Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 5247--5265, 2024.

\bibitem{karpukhin2020dense}
Vladimir Karpukhin, Barlas O{\u{g}}uz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.
\newblock Dense passage retrieval for open-domain question answering.
\newblock {\em arXiv preprint arXiv:2004.04906}, 2020.

\bibitem{kawaguchi2017generalization}
Kenji Kawaguchi, Leslie~Pack Kaelbling, and Yoshua Bengio.
\newblock Generalization in deep learning.
\newblock {\em arXiv preprint arXiv:1710.05468}, 1(8), 2017.

\bibitem{li2022fine}
Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao Zhu, Lingxi Xie, Yueting Zhuang, Qi~Tian, and Siliang Tang.
\newblock Fine-grained semantically aligned vision-language pre-training.
\newblock {\em Advances in neural information processing systems}, 35:7290--7303, 2022.

\bibitem{li2023fine}
Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, Hanwang Zhang, and Yueting Zhuang.
\newblock Fine-tuning multimodal llms to follow zero-shot demonstrative instructions.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{li2023variational}
Juncheng Li, Siliang Tang, Linchao Zhu, Wenqiao Zhang, Yi~Yang, Tat-Seng Chua, Fei Wu, and Yueting Zhuang.
\newblock Variational cross-graph reasoning and adaptive structured semantics learning for compositional temporal grounding.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}, 45(10):12601--12617, 2023.

\bibitem{li2022compositional}
Juncheng Li, Junlin Xie, Long Qian, Linchao Zhu, Siliang Tang, Fei Wu, Yi~Yang, Yueting Zhuang, and Xin~Eric Wang.
\newblock Compositional temporal grounding with structured variational cross-graph correspondence learning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 3032--3041, 2022.

\bibitem{li2023blip}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In {\em International conference on machine learning}, pages 19730--19742. PMLR, 2023.

\bibitem{lin2024non}
Wang Lin, Jingyuan Chen, Jiaxin Shi, Yichen Zhu, Chen Liang, Junzhong Miao, Tao Jin, Zhou Zhao, Fei Wu, Shuicheng Yan, et~al.
\newblock Non-confusing generation of customized concepts in diffusion models.
\newblock {\em arXiv preprint arXiv:2405.06914}, 2024.

\bibitem{lin2023tavt}
Wang Lin, Tao Jin, Wenwen Pan, Linjun Li, Xize Cheng, Ye~Wang, and Zhou Zhao.
\newblock Tavt: Towards transferable audio-visual text generation.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 14983--14999, 2023.

\bibitem{lin2023exploring}
Wang Lin, Tao Jin, Ye~Wang, Wenwen Pan, Linjun Li, Xize Cheng, and Zhou Zhao.
\newblock Exploring group video captioning with efficient relational approximation.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 15281--15290, 2023.

\bibitem{icl2}
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.
\newblock What makes good in-context examples for gpt-$3$?, 2021.

\bibitem{liu2024democratizing}
Mingxuan Liu, Subhankar Roy, Wenjing Li, Zhun Zhong, Nicu Sebe, and Elisa Ricci.
\newblock Democratizing fine-grained visual recognition with large language models, 2024.

\bibitem{meng2022locating}
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
\newblock Locating and editing factual associations in gpt.
\newblock {\em Advances in Neural Information Processing Systems}, 35:17359--17372, 2022.

\bibitem{meng2022mass}
Kevin Meng, Arnab~Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau.
\newblock Mass-editing memory in a transformer.
\newblock {\em arXiv preprint arXiv:2210.07229}, 2022.

\bibitem{icl1}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning work?, 2022.

\bibitem{mitchell2021fast}
Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher~D Manning.
\newblock Fast model editing at scale.
\newblock {\em arXiv preprint arXiv:2110.11309}, 2021.

\bibitem{mitchell2022memory}
Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher~D Manning, and Chelsea Finn.
\newblock Memory-based model editing at scale.
\newblock In {\em International Conference on Machine Learning}, pages 15817--15831. PMLR, 2022.

\bibitem{pan2023finding}
Haowen Pan, Yixin Cao, Xiaozhi Wang, and Xun Yang.
\newblock Finding and editing multi-modal neurons in pre-trained transformer.
\newblock {\em arXiv preprint arXiv:2311.07470}, 2023.

\bibitem{pan2023controlretriever}
Kaihang Pan, Juncheng Li, Hongye Song, Hao Fei, Wei Ji, Shuo Zhang, Jun Lin, Xiaozhong Liu, and Siliang Tang.
\newblock Controlretriever: Harnessing the power of instructions for controllable retrieval.
\newblock {\em arXiv preprint arXiv:2308.10025}, 2023.

\bibitem{pan2023self}
Kaihang Pan, Juncheng Li, Hongye Song, Jun Lin, Xiaozhong Liu, and Siliang Tang.
\newblock Self-supervised meta-prompt learning with meta-gradient regularization for few-shot generalization.
\newblock {\em arXiv preprint arXiv:2303.12314}, 2023.

\bibitem{pan2024auto}
Kaihang Pan, Siliang Tang, Juncheng Li, Zhaoyu Fan, Wei Chow, Shuicheng Yan, Tat-Seng Chua, Yueting Zhuang, and Hanwang Zhang.
\newblock Auto-encoding morph-tokens for multimodal llm.
\newblock {\em arXiv preprint arXiv:2405.01926}, 2024.

\bibitem{piaget1976piaget}
Jean Piaget.
\newblock Piagetâ€™s theory, 1976.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{tsne}
Laurens van~der Maaten and Geoffrey Hinton.
\newblock Visualizing data using t-sne.
\newblock {\em Journal of Machine Learning Research}, 9(86):2579--2605, 2008.

\bibitem{wang2023weakly}
Ye~Wang, Wang Lin, Shengyu Zhang, Tao Jin, Linjun Li, Xize Cheng, and Zhou Zhao.
\newblock Weakly-supervised spoken video grounding via semantic interaction learning.
\newblock In {\em Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 10914--10932, 2023.

\bibitem{wu2024towards}
Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan.
\newblock Towards semantic equivalence of tokenization in multimodal llm.
\newblock {\em arXiv preprint arXiv:2406.05127}, 2024.

\bibitem{wu24next}
Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua.
\newblock Next-gpt: Any-to-any multimodal llm.
\newblock In {\em Proceedings of the International Conference on Machine Learning}, 2024.

\bibitem{yao2023editing}
Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu Zhang.
\newblock Editing large language models: Problems, methods, and opportunities.
\newblock {\em arXiv preprint arXiv:2305.13172}, 2023.

\bibitem{yu2024hallucidoctor}
Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi~Tian, and Yueting Zhuang.
\newblock Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 2024.

\bibitem{yu2023visually}
Qifan Yu, Juncheng Li, Yu~Wu, Siliang Tang, Wei Ji, and Yueting Zhuang.
\newblock Visually-prompted language model for fine-grained scene graph generation in an open world.
\newblock {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, 2023.

\bibitem{yu2023interactive}
Qifan Yu, Juncheng Li, Wentao Ye, Siliang Tang, and Yueting Zhuang.
\newblock Interactive data synthesis for systematic vision adaptation via llms-aigcs collaboration.
\newblock {\em arXiv preprint arXiv:2305.12799}, 2023.

\bibitem{zhang2024comprehensive}
Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, et~al.
\newblock A comprehensive study of knowledge editing for large language models.
\newblock {\em arXiv preprint arXiv:2401.01286}, 2024.

\bibitem{zheng-etal-2023-edit}
Ce~Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang.
\newblock Can we edit factual knowledge by in-context learning?
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali, editors, {\em Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing}, pages 4862--4876, Singapore, December 2023. Association for Computational Linguistics.

\bibitem{zhong2023mquake}
Zexuan Zhong, Zhengxuan Wu, Christopher~D Manning, Christopher Potts, and Danqi Chen.
\newblock Mquake: Assessing knowledge editing in language models via multi-hop questions.
\newblock {\em arXiv preprint arXiv:2305.14795}, 2023.

\bibitem{zhu2023minigpt}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock {\em arXiv preprint arXiv:2304.10592}, 2023.

\bibitem{zhu2023sgl}
Yun Zhu, Jianhao Guo, and Siliang Tang.
\newblock Sgl-pt: A strong graph learner with graph prompt tuning.
\newblock {\em arXiv preprint arXiv:2302.12449}, 2023.

\bibitem{zhu2022rosa}
Yun Zhu, Jianhao Guo, Fei Wu, and Siliang Tang.
\newblock Rosa: A robust self-aligned framework for node-node graph contrastive learning.
\newblock {\em arXiv preprint arXiv:2204.13846}, 2022.

\bibitem{zhu2024graphcontrol}
Yun Zhu, Yaoke Wang, Haizhou Shi, Zhenshuo Zhang, Dian Jiao, and Siliang Tang.
\newblock Graphcontrol: Adding conditional control to universal graph pre-trained models for graph domain transfer learning.
\newblock In {\em Proceedings of the ACM on Web Conference 2024}, pages 539--550, 2024.

\end{thebibliography}
