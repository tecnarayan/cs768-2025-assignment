\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arazo et~al.(2019)Arazo, Ortego, Albert, O’Connor, and
  McGuinness]{arazo2019unsupervised}
Arazo, E., Ortego, D., Albert, P., O’Connor, N., and McGuinness, K.
\newblock Unsupervised label noise modeling and loss correction.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{Bahdanau:15}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Belinkov \& Bisk(2018)Belinkov and Bisk]{Belinkov:17}
Belinkov, Y. and Bisk, Y.
\newblock Synthetic and natural noise both break neural machine translation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Bengio et~al.(2015)Bengio, Vinyals, Jaitly, and
  Shazeer]{bengio2015scheduled}
Bengio, S., Vinyals, O., Jaitly, N., and Shazeer, N.
\newblock Scheduled sampling for sequence prediction with recurrent neural
  networks.
\newblock \emph{arXiv preprint arXiv:1506.03099}, 2015.

\bibitem[Caswell et~al.(2019)Caswell, Chelba, and Grangier]{caswell2019tagged}
Caswell, I., Chelba, C., and Grangier, D.
\newblock Tagged back-translation.
\newblock \emph{arXiv preprint arXiv:1906.06442}, 2019.

\bibitem[Chen et~al.(2020)Chen, Yang, and Yang]{chen2020mixtext}
Chen, J., Yang, Z., and Yang, D.
\newblock Mixtext: Linguistically-informed interpolation of hidden space for
  semi-supervised text classification.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics (ACL)}, 2020.

\bibitem[Cheng et~al.(2016)Cheng, Xu, He, He, Wu, Sun, and Liu]{Cheng:16}
Cheng, Y., Xu, W., He, Z., He, W., Wu, H., Sun, M., and Liu, Y.
\newblock Semi-supervised learning for neural machine translation.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics (ACL)}, 2016.

\bibitem[Cheng et~al.(2019)Cheng, Jiang, and Macherey]{Cheng:19}
Cheng, Y., Jiang, L., and Macherey, W.
\newblock Robust neural machine translation with doubly adversarial inputs.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics (ACL)}, 2019.

\bibitem[Cheng et~al.(2020)Cheng, Jiang, Macherey, and
  Eisenstein]{cheng2020advaug}
Cheng, Y., Jiang, L., Macherey, W., and Eisenstein, J.
\newblock Advaug: Robust adversarial augmentation for neural machine
  translation.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics (ACL)}, 2020.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics (NAACL)}, 2019.

\bibitem[Edunov et~al.(2018)Edunov, Ott, Auli, and Grangier]{Edunov:18}
Edunov, S., Ott, M., Auli, M., and Grangier, D.
\newblock Understanding back-translation at scale.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  2018.

\bibitem[Edunov et~al.(2019)Edunov, Baevski, and Auli]{edunov2019pre}
Edunov, S., Baevski, A., and Auli, M.
\newblock Pre-trained language model representations for language generation.
\newblock \emph{arXiv preprint arXiv:1903.09722}, 2019.

\bibitem[French(1999)]{french1999catastrophic}
French, R.~M.
\newblock Catastrophic forgetting in connectionist networks.
\newblock \emph{Trends in cognitive sciences}, 1999.

\bibitem[Garg et~al.(2019)Garg, Peitz, Nallasamy, and Paulik]{garg2019jointly}
Garg, S., Peitz, S., Nallasamy, U., and Paulik, M.
\newblock Jointly learning to align and translate with transformer models.
\newblock \emph{arXiv preprint arXiv:1909.02074}, 2019.

\bibitem[Gehring et~al.(2017)Gehring, Auli, Grangier, Yarats, and
  Dauphin]{Gehring:17}
Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y.~N.
\newblock Convolutional sequence to sequence learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Guo et~al.(2020)Guo, Kim, and Rush]{guo:sequencemixed}
Guo, D., Kim, Y., and Rush, A.
\newblock Sequence-level mixed sample data augmentation.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)},
  2020.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Mu, Cubuk, Zoph, Gilmer, and
  Lakshminarayanan]{hendrycks2019augmix}
Hendrycks, D., Mu, N., Cubuk, E.~D., Zoph, B., Gilmer, J., and
  Lakshminarayanan, B.
\newblock Augmix: A simple data processing method to improve robustness and
  uncertainty.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Jiang et~al.(2020)Jiang, Huang, Liu, and Yang]{jiang2020beyond}
Jiang, L., Huang, D., Liu, M., and Yang, W.
\newblock Beyond synthetic noise: Deep learning on controlled noisy labels.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lample \& Conneau(2019)Lample and Conneau]{lample2019cross}
Lample, G. and Conneau, A.
\newblock Cross-lingual language model pretraining.
\newblock \emph{arXiv}, pp.\  arXiv--1901, 2019.

\bibitem[Lample et~al.(2017)Lample, Conneau, Denoyer, and
  Ranzato]{lample2017unsupervised}
Lample, G., Conneau, A., Denoyer, L., and Ranzato, M.
\newblock Unsupervised machine translation using monolingual corpora only.
\newblock \emph{arXiv preprint arXiv:1711.00043}, 2017.

\bibitem[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{lewis2019bart}
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O.,
  Stoyanov, V., and Zettlemoyer, L.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}, 2019.

\bibitem[Liu et~al.(2020{\natexlab{a}})Liu, Duh, Liu, and Gao]{liu2020very}
Liu, X., Duh, K., Liu, L., and Gao, J.
\newblock Very deep transformers for neural machine translation.
\newblock \emph{arXiv preprint arXiv:2008.07772}, 2020{\natexlab{a}}.

\bibitem[Liu et~al.(2020{\natexlab{b}})Liu, Gu, Goyal, Li, Edunov,
  Ghazvininejad, Lewis, and Zettlemoyer]{liu2020multilingual}
Liu, Y., Gu, J., Goyal, N., Li, X., Edunov, S., Ghazvininejad, M., Lewis, M.,
  and Zettlemoyer, L.
\newblock Multilingual denoising pre-training for neural machine translation.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  2020{\natexlab{b}}.

\bibitem[Lo(2019)]{lo2019yisi}
Lo, C.-k.
\newblock Yisi-a unified semantic mt quality evaluation and estimation metric
  for languages with different levels of available resources.
\newblock In \emph{Proceedings of the Fourth Conference on Machine
  Translation}, 2019.

\bibitem[Nguyen et~al.(2019)Nguyen, Joty, Kui, and Aw]{nguyen2019data}
Nguyen, X.-P., Joty, S., Kui, W., and Aw, A.~T.
\newblock Data diversification: An elegant strategy for neural machine
  translation.
\newblock \emph{arXiv preprint arXiv:1911.01986}, 2019.

\bibitem[Northcutt et~al.(2021)Northcutt, Jiang, and
  Chuang]{northcutt2021confident}
Northcutt, C.~G., Jiang, L., and Chuang, I.~L.
\newblock Confident learning: Estimating uncertainty in dataset labels.
\newblock \emph{Journal of Artificial Intelligence Research}, 2021.

\bibitem[Och \& Ney(2004)Och and Ney]{och2004alignment}
Och, F.~J. and Ney, H.
\newblock The alignment template approach to statistical machine translation.
\newblock \emph{Computational linguistics}, 2004.

\bibitem[Ott et~al.(2018)Ott, Edunov, Grangier, and Auli]{ott2018scaling}
Ott, M., Edunov, S., Grangier, D., and Auli, M.
\newblock Scaling neural machine translation.
\newblock \emph{arXiv preprint arXiv:1806.00187}, 2018.

\bibitem[Peters et~al.(2018)Peters, Neumann, Iyyer, Gardner, Clark, Lee, and
  Zettlemoyer]{peters2018deep}
Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and
  Zettlemoyer, L.
\newblock Deep contextualized word representations.
\newblock In \emph{North American Chapter of the Association for Computational
  Linguistics (NAACL)}, 2018.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I.
\newblock Improving language understanding by generative pre-training, 2018.

\bibitem[Ramachandran et~al.(2016)Ramachandran, Liu, and
  Le]{ramachandran2016unsupervised}
Ramachandran, P., Liu, P.~J., and Le, Q.~V.
\newblock Unsupervised pretraining for sequence to sequence learning.
\newblock \emph{arXiv preprint arXiv:1611.02683}, 2016.

\bibitem[Rieger et~al.(2012)Rieger, Michaelis, and Green]{rieger2012glossary}
Rieger, R., Michaelis, A., and Green, M.~M.
\newblock \emph{Glossary of genetics and cytogenetics: classical and
  molecular}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Schuster \& Nakajima(2012)Schuster and Nakajima]{schuster2012japanese}
Schuster, M. and Nakajima, K.
\newblock Japanese and korean voice search.
\newblock In \emph{International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}, 2012.

\bibitem[Sellam et~al.(2020)Sellam, Das, and Parikh]{sellam2020bleurt}
Sellam, T., Das, D., and Parikh, A.
\newblock Bleurt: Learning robust metrics for text generation.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics (ACL)}, 2020.

\bibitem[Sennrich et~al.(2016{\natexlab{a}})Sennrich, Haddow, and
  Birch]{Sennrich:16a}
Sennrich, R., Haddow, B., and Birch, A.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics (ACL)}, 2016{\natexlab{a}}.

\bibitem[Sennrich et~al.(2016{\natexlab{b}})Sennrich, Haddow, and
  Birch]{Sennrich:16b}
Sennrich, R., Haddow, B., and Birch, A.
\newblock Improving nerual machine translation models with monolingual data.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics (ACL)}, 2016{\natexlab{b}}.

\bibitem[Shen et~al.(2019)Shen, Nguyen, Wu, Chen, Chen, Jia, Kannan, Sainath,
  Cao, Chiu, et~al.]{shen2019lingvo}
Shen, J., Nguyen, P., Wu, Y., Chen, Z., Chen, M.~X., Jia, Y., Kannan, A.,
  Sainath, T., Cao, Y., Chiu, C.-C., et~al.
\newblock Lingvo: a modular and scalable framework for sequence-to-sequence
  modeling.
\newblock \emph{arXiv preprint arXiv:1902.08295}, 2019.

\bibitem[Song et~al.(2019)Song, Tan, Qin, Lu, and Liu]{song2019mass}
Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y.
\newblock Mass: Masked sequence to sequence pre-training for language
  generation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Vaswani:17}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2017.

\bibitem[Vincent et~al.(2008)Vincent, Larochelle, Bengio, and
  Manzagol]{vincent2008extracting}
Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A.
\newblock Extracting and composing robust features with denoising autoencoders.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2008.

\bibitem[Xu et~al.(2021)Xu, Zhu, Jiang, and Yang]{xu2021faster}
Xu, Y., Zhu, L., Jiang, L., and Yang, Y.
\newblock Faster meta update strategy for noise-robust deep learning.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2021.

\bibitem[Yang et~al.(2019)Yang, Wang, Zhou, Zhao, Yu, Zhang, and
  Li]{yang2019towards}
Yang, J., Wang, M., Zhou, H., Zhao, C., Yu, Y., Zhang, W., and Li, L.
\newblock Towards making the most of bert in neural machine translation.
\newblock \emph{arXiv preprint arXiv:1908.05672}, 2019.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{yun2019cutmix}
Yun, S., Han, D., Oh, S.~J., Chun, S., Choe, J., and Yoo, Y.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{International Conference on Computer Vision (ICCV)}, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and Lopez-Paz]{Zhang:18}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Zhu et~al.(2020)Zhu, Xia, Wu, He, Qin, Zhou, Li, and
  Liu]{zhu2020bertnmt}
Zhu, J., Xia, Y., Wu, L., He, D., Qin, T., Zhou, W., Li, H., and Liu, T.-Y.
\newblock Incorporating bert into neural machine translation.
\newblock \emph{arXiv preprint arXiv:2002.06823}, 2020.

\end{thebibliography}
