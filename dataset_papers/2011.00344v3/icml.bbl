\begin{thebibliography}{22}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baxter(1998)]{baxter1998theoretical}
Baxter, J.
\newblock Theoretical models of learning to learn.
\newblock In S.~Thrun, L.~P. (ed.), \emph{Learning to learn}, pp.\  71--94.
  Springer, 1998.

\bibitem[Baxter(2000)]{baxter2000model}
Baxter, J.
\newblock A model of inductive bias learning.
\newblock \emph{Journal of Artificial Intelligence Research}, 12:\penalty0
  149--198, 2000.

\bibitem[Bretagnolle \& Huber(1979)Bretagnolle and
  Huber]{bretagnolle1979estimation}
Bretagnolle, J. and Huber, C.
\newblock Estimation des densit{\'e}s: risque minimax.
\newblock \emph{Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und verwandte
  Gebiete}, 47\penalty0 (2):\penalty0 119--137, 1979.

\bibitem[Dempster et~al.(1977)Dempster, Laird, and Rubin]{dempster1977maximum}
Dempster, A.~P., Laird, N.~M., and Rubin, D.~B.
\newblock Maximum likelihood from incomplete data via the em algorithm.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 39\penalty0 (1):\penalty0 1--22, 1977.

\bibitem[Denevi et~al.(2018)Denevi, Ciliberto, Stamos, and
  Pontil]{denevi2018learning}
Denevi, G., Ciliberto, C., Stamos, D., and Pontil, M.
\newblock Learning to learn around a common mean.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, pp.\  10169--10179, 2018.

\bibitem[Denevi et~al.(2019)Denevi, Ciliberto, Grazzi, and
  Pontil]{denevi2019learning}
Denevi, G., Ciliberto, C., Grazzi, R., and Pontil, M.
\newblock Learning-to-learn stochastic gradient descent with biased
  regularization.
\newblock In \emph{International Conference on Machine Learing (ICML)}, 2019.

\bibitem[Du et~al.(2020)Du, Hu, Kakade, Lee, and Lei]{du2020few}
Du, S.~S., Hu, W., Kakade, S.~M., Lee, J.~D., and Lei, Q.
\newblock Few-shot learning via learning the representation, provably.
\newblock arXiv:2002.09434, 2020.

\bibitem[Dua \& Graff(2017)Dua and Graff]{dua2017openml}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International Conference on Machine Learing (ICML)}, 2017.

\bibitem[Finn et~al.(2019)Finn, Rajeswaran, Kakade, and Levine]{finn2019online}
Finn, C., Rajeswaran, A., Kakade, S., and Levine, S.
\newblock Online meta-learning.
\newblock \emph{International Conference on Machine Learing (ICML)}, 2019.

\bibitem[Khodak et~al.(2019{\natexlab{a}})Khodak, Balcan, and
  Talwalkar]{balcan2019provable}
Khodak, M., Balcan, M.-F., and Talwalkar, A.
\newblock Provable guarantees for gradient-based meta-learning.
\newblock In \emph{International Conference on Machine Learing (ICML)}, pp.\
  424--433, 2019{\natexlab{a}}.

\bibitem[Khodak et~al.(2019{\natexlab{b}})Khodak, Balcan, and
  Talwalkar]{khodak2019adaptive}
Khodak, M., Balcan, M.-F.~F., and Talwalkar, A.~S.
\newblock Adaptive gradient-based meta-learning methods.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 5917--5928, 2019{\natexlab{b}}.

\bibitem[Kuzborskij \& Orabona(2013)Kuzborskij and
  Orabona]{kuzborskij2013stability}
Kuzborskij, I. and Orabona, F.
\newblock Stability and {H}ypothesis {T}ransfer {L}earning.
\newblock In \emph{International Conference on Machine Learing (ICML)}, pp.\
  942--950, 2013.

\bibitem[Kuzborskij \& Orabona(2016)Kuzborskij and Orabona]{kuzborskij2016fast}
Kuzborskij, I. and Orabona, F.
\newblock Fast {R}ates by {T}ransferring from {A}uxiliary {H}ypotheses.
\newblock \emph{Machine Learning}, pp.\  1--25, 2016.
\newblock ISSN 1573-0565.
\newblock \doi{10.1007/s10994-016-5594-4}.

\bibitem[Lattimore \& Szepesv{\'a}ri(2018)Lattimore and
  Szepesv{\'a}ri]{lattimore2018bandit}
Lattimore, T. and Szepesv{\'a}ri, C.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2018.

\bibitem[Lucas et~al.(2021)Lucas, Ren, Kameni, Pitassi, and
  Zemel]{lucas2020theoretical}
Lucas, J., Ren, M., Kameni, I., Pitassi, T., and Zemel, R.
\newblock Theoretical bounds on estimation error for meta-learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Maurer(2005)]{maurer2005algorithmic}
Maurer, A.
\newblock Algorithmic stability and meta-learning.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Jun):\penalty0 967--994, 2005.

\bibitem[Maurer(2009)]{maurer2009transfer}
Maurer, A.
\newblock Transfer bounds for linear feature learning.
\newblock \emph{Machine Learning}, 75\penalty0 (3):\penalty0 327--350, 2009.

\bibitem[Maurer et~al.(2016)Maurer, Pontil, and
  Romera-Paredes]{maurer2016benefit}
Maurer, A., Pontil, M., and Romera-Paredes, B.
\newblock The benefit of multitask representation learning.
\newblock \emph{Journal of Machine Learning Research}, 2016.

\bibitem[Pentina \& Lampert(2014)Pentina and Lampert]{pentina2014pac}
Pentina, A. and Lampert, C.
\newblock A pac-bayesian bound for lifelong learning.
\newblock In \emph{International Conference on Machine Learing (ICML)}, pp.\
  991--999, 2014.

\bibitem[Tripuraneni et~al.(2020)Tripuraneni, Jin, and
  Jordan]{tripuraneni2020provable}
Tripuraneni, N., Jin, C., and Jordan, M.~I.
\newblock Provable meta-learning of linear representations.
\newblock arXiv:2002.11684, 2020.

\bibitem[Yang et~al.(2007)Yang, Yan, and Hauptmann]{yang2007cross}
Yang, J., Yan, R., and Hauptmann, A.~G.
\newblock Cross-domain video concept detection using adaptive svms.
\newblock In \emph{Proceedings of the 15th ACM international conference on
  Multimedia}, pp.\  188--197, 2007.

\end{thebibliography}
