\begin{thebibliography}{10}

\bibitem{allen2018convergencetheory}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock {\em arXiv preprint arXiv:1811.03962}, 2018.

\bibitem{arora2019fine}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock {\em arXiv preprint arXiv:1901.08584}, 2019.

\bibitem{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In {\em ICML}, 2018.

\bibitem{bach2017breaking}
Francis Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock {\em The Journal of Machine Learning Research}, 18(1):629--681, 2017.

\bibitem{bach2017equivalence}
Francis Bach.
\newblock On the equivalence between kernel quadrature rules and random feature
  expansions.
\newblock {\em The Journal of Machine Learning Research}, 18(1):714--751, 2017.

\bibitem{bartlett2019nearly}
Peter~L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian.
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock {\em Journal of Machine Learning Research}, 20(63):1--17, 2019.

\bibitem{bietti2019inductive}
Alberto Bietti and Julien Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock {\em arXiv preprint arXiv:1905.12173}, 2019.

\bibitem{brendel2017decision}
Wieland Brendel, Jonas Rauber, and Matthias Bethge.
\newblock Decision-based adversarial attacks: Reliable attacks against
  black-box machine learning models.
\newblock {\em arXiv preprint arXiv:1712.04248}, 2017.

\bibitem{bubeck2018adversarial}
S{\'e}bastien Bubeck, Eric Price, and Ilya Razenshteyn.
\newblock Adversarial examples from computational constraints.
\newblock {\em arXiv preprint arXiv:1805.10204}, 2018.

\bibitem{cai2019gram}
Tianle Cai, Ruiqi Gao, Jikai Hou, Siyu Chen, Dong Wang, Di~He, Zhihua Zhang,
  and Liwei Wang.
\newblock A gram-gauss-newton method learning overparameterized deep neural
  networks for regression problems.
\newblock {\em arXiv preprint arXiv:1905.11675}, 2019.

\bibitem{cao2019generalization}
Yuan Cao and Quanquan Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock {\em arXiv preprint arXiv:1905.13210}, 2019.

\bibitem{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In {\em 2017 IEEE Symposium on Security and Privacy (SP)}, pages
  39--57. IEEE, 2017.

\bibitem{chen2017zoo}
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh.
\newblock Zoo: Zeroth order optimization based black-box attacks to deep neural
  networks without training substitute models.
\newblock In {\em Proceedings of the 10th ACM Workshop on Artificial
  Intelligence and Security}, pages 15--26. ACM, 2017.

\bibitem{cohen2019certified}
Jeremy~M Cohen, Elan Rosenfeld, and J~Zico Kolter.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock {\em arXiv preprint arXiv:1902.02918}, 2019.

\bibitem{daniely2017sgd}
Amit Daniely.
\newblock {SGD} learns the conjugate kernel class of the network.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2422--2430, 2017.

\bibitem{daniely2016toward}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In {\em Advances In Neural Information Processing Systems}, pages
  2253--2261, 2016.

\bibitem{du2018power}
Simon~S Du and Jason~D Lee.
\newblock On the power of over-parametrization in neural networks with
  quadratic activation.
\newblock {\em arXiv preprint arXiv:1803.01206}, 2018.

\bibitem{du2018gradient_deep}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock {\em arXiv preprint arXiv:1811.03804}, 2018.

\bibitem{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock {\em arXiv preprint arXiv:1810.02054}, 2018.

\bibitem{eykholt2017robust}
Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo~Li, Amir Rahmati, Chaowei
  Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song.
\newblock Robust physical-world attacks on deep learning models.
\newblock {\em arXiv preprint arXiv:1707.08945}, 2017.

\bibitem{gonen2018learning}
Alon Gonen and Elad Hazan.
\newblock Learning in non-convex games with an optimization oracle.
\newblock {\em arXiv preprint arXiv:1810.07362}, 2018.

\bibitem{goodfellow2015explaining}
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{guo2017countering}
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van~der Maaten.
\newblock Countering adversarial images using input transformations.
\newblock {\em arXiv preprint arXiv:1711.00117}, 2017.

\bibitem{ilyas2018black}
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin.
\newblock Black-box adversarial attacks with limited queries and information.
\newblock In {\em International Conference on Machine Learning}, pages
  2142--2151, 2018.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock {\em arXiv preprint arXiv:1806.07572}, 2018.

\bibitem{kurakin2016adversarial}
Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
\newblock Adversarial machine learning at scale.
\newblock {\em arXiv preprint arXiv:1611.01236}, 2016.

\bibitem{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock {\em arXiv preprint arXiv:1808.01204}, 2018.

\bibitem{liu2018towards}
Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh.
\newblock Towards robust neural networks via random self-ensemble.
\newblock In {\em European Conference on Computer Vision}, pages 381--397.
  Springer, 2018.

\bibitem{liu2018adversarial}
Xuanqing Liu and Cho-Jui Hsieh.
\newblock Rob-gan: Generator, discriminator, and adversarial attacker.
\newblock In {\em CVPR}, 2019.

\bibitem{ma2018characterizing}
Xingjun Ma, Bo~Li, Yisen Wang, Sarah~M Erfani, Sudanthi Wijewickrema, Michael~E
  Houle, Grant Schoenebeck, Dawn Song, and James Bailey.
\newblock Characterizing adversarial subspaces using local intrinsic
  dimensionality.
\newblock {\em arXiv preprint arXiv:1801.02613}, 2018.

\bibitem{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock {\em arXiv preprint arXiv:1706.06083}, 2017.

\bibitem{micchelli2006universal}
Charles~A Micchelli, Yuesheng Xu, and Haizhang Zhang.
\newblock Universal kernels.
\newblock {\em Journal of Machine Learning Research}, 7(Dec):2651--2667, 2006.

\bibitem{mohri2012new}
Mehryar Mohri and Andres~Munoz Medina.
\newblock New analysis and algorithm for learning with drifting distributions.
\newblock In {\em Algorithmic Learning Theory}, pages 124--138. Springer, 2012.

\bibitem{mohri2018foundations}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock {\em Foundations of Machine Learning}.
\newblock MIT Press, 2018.

\bibitem{nakkiran2019adversarial}
Preetum Nakkiran.
\newblock Adversarial robustness may be at odds with simplicity.
\newblock {\em arXiv preprint arXiv:1901.00532}, 2019.

\bibitem{paulsen2016introduction}
Vern~I Paulsen and Mrinal Raghupathi.
\newblock {\em An introduction to the theory of reproducing kernel Hilbert
  spaces}, volume 152.
\newblock Cambridge University Press, 2016.

\bibitem{rahimi2008uniform}
Ali Rahimi and Benjamin Recht.
\newblock Uniform approximation of functions with random bases.
\newblock In {\em 2008 46th Annual Allerton Conference on Communication,
  Control, and Computing}, pages 555--561. IEEE, 2008.

\bibitem{salman2019convex}
Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang.
\newblock A convex relaxation barrier to tight robust verification of neural
  networks.
\newblock {\em arXiv preprint arXiv:1902.08722}, 2019.

\bibitem{samangouei2018defense}
Pouya Samangouei, Maya Kabkab, and Rama Chellappa.
\newblock Defense-{GAN}: Protecting classifiers against adversarial attacks
  using generative models.
\newblock {\em arXiv preprint arXiv:1805.06605}, 2018.

\bibitem{schmidt2018adversarially}
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and
  Aleksander Madry.
\newblock Adversarially robust generalization requires more data.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5014--5026, 2018.

\bibitem{shafahi2019adversarial}
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph
  Studer, Larry~S Davis, Gavin Taylor, and Tom Goldstein.
\newblock Adversarial training for free!
\newblock {\em arXiv preprint arXiv:1904.12843}, 2019.

\bibitem{singh2018fast}
Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P{\"u}schel, and Martin
  Vechev.
\newblock Fast and effective robustness certification.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10802--10813, 2018.

\bibitem{song2017pixeldefend}
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman.
\newblock Pixeldefend: Leveraging generative models to understand and defend
  against adversarial examples.
\newblock {\em arXiv preprint arXiv:1710.10766}, 2017.

\bibitem{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock {\em arXiv preprint arXiv:1312.6199}, 2013.

\bibitem{vershynin2010introduction}
Roman Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock {\em arXiv preprint arXiv:1011.3027}, 2010.

\bibitem{wang2019convergence}
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu.
\newblock On the convergence and robustness of adversarial training.
\newblock In {\em International Conference on Machine Learning}, pages
  6586--6595, 2019.

\bibitem{weng2018towards}
Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel,
  Duane Boning, and Inderjit Dhillon.
\newblock Towards fast computation of certified robustness for relu networks.
\newblock In {\em International Conference on Machine Learning}, pages
  5273--5282, 2018.

\bibitem{wong2018provable}
Eric Wong and Zico Kolter.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock In {\em International Conference on Machine Learning}, pages
  5283--5292, 2018.

\bibitem{xie2019intriguing}
Cihang Xie and Alan Yuille.
\newblock Intriguing properties of adversarial training.
\newblock {\em arXiv preprint arXiv:1906.03787}, 2019.

\bibitem{yarotsky2018optimal}
Dmitry Yarotsky.
\newblock Optimal approximation of continuous functions by very deep relu
  networks.
\newblock {\em arXiv preprint arXiv:1802.03620}, 2018.

\bibitem{yin2018rademacher}
Dong Yin, Kannan Ramchandran, and Peter Bartlett.
\newblock Rademacher complexity for adversarially robust generalization.
\newblock {\em arXiv preprint arXiv:1810.11914}, 2018.

\bibitem{yun2018finite}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock Finite sample expressive power of small-width relu networks.
\newblock {\em arXiv preprint arXiv:1810.07770}, 2018.

\bibitem{zhang2019you}
Dinghuai Zhang, Tianyuan Zhang, Yiping Lu, Zhanxing Zhu, and Bin Dong.
\newblock You only propagate once: Painless adversarial training using maximal
  principle.
\newblock {\em arXiv preprint arXiv:1905.00877}, 2019.

\bibitem{zhang2018efficient}
Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel.
\newblock Efficient neural network robustness certification with general
  activation functions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4939--4948, 2018.

\bibitem{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep {ReLU}
  networks.
\newblock {\em arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
