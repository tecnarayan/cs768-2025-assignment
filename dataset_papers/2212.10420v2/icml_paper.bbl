\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abel et~al.(2021)Abel, Dabney, Harutyunyan, Ho, Littman, Precup, and
  Singh]{abel2021reward}
Abel, D., Dabney, W., Harutyunyan, A., Ho, M.~K., Littman, M.~L., Precup, D.,
  and Singh, S.
\newblock On the expressivity of {M}arkov reward.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Abel et~al.(2022)Abel, Barreto, Bowling, Dabney, Hansen, Harutyunyan,
  Ho, Kumar, Littman, Precup, and Satinder]{abelexpressing}
Abel, D., Barreto, A., Bowling, M., Dabney, W., Hansen, S., Harutyunyan, A.,
  Ho, M.~K., Kumar, R., Littman, M.~L., Precup, D., and Satinder, S.
\newblock Expressing non-{M}arkov reward to a {M}arkov agent.
\newblock \emph{Multidisciplinary Conference on Reinforcement Learning and
  Decision Making}, 2022.

\bibitem[Aumann(1962)]{aumann1962utility}
Aumann, R.~J.
\newblock Utility theory without the completeness axiom.
\newblock \emph{Econometrica: Journal of the Econometric Society}, pp.\
  445--462, 1962.

\bibitem[Bellemare et~al.(2023)Bellemare, Dabney, and Rowland]{bdr2023}
Bellemare, M.~G., Dabney, W., and Rowland, M.
\newblock \emph{Distributional Reinforcement Learning}.
\newblock MIT Press, 2023.
\newblock \url{http://www.distributional-rl.org}.

\bibitem[Bernoulli(1738)]{bernoulli1738exposition}
Bernoulli, D.
\newblock Specimen theoriae novae de mensura sortis (trans. in 1954 as
  exposition of a new theory on the measurement of risk).
\newblock \emph{Econometrica}, 22\penalty0 (1):\penalty0 23--36, 1738.

\bibitem[Cassandra et~al.(1994)Cassandra, Kaelbling, and
  Littman]{cassandra1994acting}
Cassandra, A.~R., Kaelbling, L.~P., and Littman, M.~L.
\newblock Acting optimally in partially observable stochastic domains.
\newblock In \emph{Proceedings of the AAAI Conference on Artificiall
  Intelligence}, 1994.

\bibitem[Chang(2015)]{chang2015value}
Chang, R.
\newblock Value incomparability and incommensurability.
\newblock \emph{The {O}xford handbook of value theory}, pp.\  205--224, 2015.

\bibitem[Dong et~al.(2021)Dong, Van~Roy, and Zhou]{dong2021simple}
Dong, S., Van~Roy, B., and Zhou, Z.
\newblock Simple agent, complex environment: Efficient reinforcement learning
  with agent states.
\newblock \emph{arXiv preprint arXiv:2102.05261}, 2021.

\bibitem[Fedus et~al.(2019)Fedus, Gelada, Bengio, Bellemare, and
  Larochelle]{fedus2019hyperbolic}
Fedus, W., Gelada, C., Bengio, Y., Bellemare, M.~G., and Larochelle, H.
\newblock Hyperbolic discounting and learning over multiple horizons.
\newblock \emph{arXiv preprint arXiv:1902.06865}, 2019.

\bibitem[G{\'a}bor et~al.(1998)G{\'a}bor, Kalm{\'a}r, and
  Szepesv{\'a}ri]{gabor1998multi}
G{\'a}bor, Z., Kalm{\'a}r, Z., and Szepesv{\'a}ri, C.
\newblock Multi-criteria reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, volume~98, 1998.

\bibitem[Hausner(1953)]{hausner1953multidimensional}
Hausner, M.
\newblock Multidimensional utilities (rev).
\newblock Technical report, RAND CORP SANTA MONICA CA, 1953.

\bibitem[Johnson-Laird(1983)]{johnson1983mental}
Johnson-Laird, P.~N.
\newblock \emph{Mental models: Towards a cognitive science of language,
  inference, and consciousness}.
\newblock Harvard University Press, 1983.

\bibitem[Kahneman \& Tversky(1982)Kahneman and Tversky]{kahneman1982psychology}
Kahneman, D. and Tversky, A.
\newblock The psychology of preferences.
\newblock \emph{Scientific American}, 246\penalty0 (1):\penalty0 160--173,
  1982.

\bibitem[Kahneman et~al.(1982)Kahneman, Slovic, Slovic, and
  Tversky]{kahneman1982judgment}
Kahneman, D., Slovic, S.~P., Slovic, P., and Tversky, A.
\newblock \emph{Judgment under uncertainty: Heuristics and biases}.
\newblock Cambridge university press, 1982.

\bibitem[Keeney et~al.(1993)Keeney, Raiffa, and Meyer]{keeney1993decisions}
Keeney, R.~L., Raiffa, H., and Meyer, R.~F.
\newblock \emph{Decisions with multiple objectives: preferences and value
  trade-offs}.
\newblock Cambridge university press, 1993.

\bibitem[Koopmans(1960)]{koopmans1960stationary}
Koopmans, T.~C.
\newblock Stationary ordinal utility and impatience.
\newblock \emph{Econometrica: Journal of the Econometric Society}, pp.\
  287--309, 1960.

\bibitem[Koopmans et~al.(1964)Koopmans, Diamond, and
  Williamson]{koopmans1964stationary}
Koopmans, T.~C., Diamond, P.~A., and Williamson, R.~E.
\newblock Stationary utility and time perspective.
\newblock \emph{Econometrica: Journal of the Econometric Society}, pp.\
  82--100, 1964.

\bibitem[Kreps \& Porteus(1978)Kreps and Porteus]{kreps1978temporal}
Kreps, D.~M. and Porteus, E.~L.
\newblock Temporal resolution of uncertainty and dynamic choice theory.
\newblock \emph{Econometrica: journal of the Econometric Society}, pp.\
  185--200, 1978.

\bibitem[Lattimore(2014)]{lattimore2014theory}
Lattimore, T.
\newblock \emph{Theory of General Reinforcement Learning}.
\newblock PhD thesis, The Australian National University, 2014.

\bibitem[Lattimore et~al.(2013)Lattimore, Hutter, and
  Sunehag]{lattimore2013sample}
Lattimore, T., Hutter, M., and Sunehag, P.
\newblock The sample-complexity of general reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2013.

\bibitem[Leike(2016)]{leike2016nonparametric}
Leike, J.
\newblock \emph{Nonparametric general reinforcement learning}.
\newblock PhD thesis, The Australian National University, 2016.

\bibitem[Leike et~al.(2018)Leike, Krueger, Everitt, Martic, Maini, and
  Legg]{leike2018scalable}
Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S.
\newblock Scalable agent alignment via reward modeling: a research direction.
\newblock \emph{arXiv preprint arXiv:1811.07871}, 2018.

\bibitem[Lewis(1985)]{lewis1985effectively}
Lewis, A.~A.
\newblock On effectively computable realizations of choice functions: Dedicated
  to professors kenneth j. arrow and anil nerode.
\newblock \emph{Mathematical Social Sciences}, 10\penalty0 (1):\penalty0
  43--80, 1985.

\bibitem[Littman(2017)]{littmanwebRLhypothesis}
Littman, M.
\newblock The reward hypothesis.
\newblock \url{https://tinyurl.com/4z52r3fe}, 2017.

\bibitem[Lu et~al.(2021)Lu, Van~Roy, Dwaracherla, Ibrahimi, Osband, and
  Wen]{lu2021reinforcement}
Lu, X., Van~Roy, B., Dwaracherla, V., Ibrahimi, M., Osband, I., and Wen, Z.
\newblock Reinforcement learning, bit by bit.
\newblock \emph{arXiv preprint arXiv:2103.04047}, 2021.

\bibitem[Machina(1990)]{machina1990expected}
Machina, M.~J.
\newblock Expected utility hypothesis.
\newblock In \emph{Utility and probability}, pp.\  79--95. Springer, 1990.

\bibitem[Mahadevan(1996)]{mahadevan1996average}
Mahadevan, S.
\newblock Average reward reinforcement learning: Foundations, algorithms, and
  empirical results.
\newblock \emph{Machine learning}, 22\penalty0 (1):\penalty0 159--195, 1996.

\bibitem[Majeed(2021)]{majeed2021abstractions}
Majeed, S.~J.
\newblock \emph{Abstractions of General Reinforcement Learning}.
\newblock PhD thesis, The Australian National University, 2021.

\bibitem[Martin(2011)]{martin2011st}
Martin, R.
\newblock The st. petersburg paradox.
\newblock \emph{Stanford Encyclopedia of Philosophy}, 2011.

\bibitem[McCarthy(1998)]{mccarthy1998artificial}
McCarthy, J.
\newblock What is artificial intelligence.
\newblock \emph{URL: http://www-formal. stanford. edu/jmc/whatisai. html},
  1998.

\bibitem[Miura(2022)]{miura2022oteomdr}
Miura, S.
\newblock On the expressivity of multidimensional {M}arkov reward.
\newblock In \emph{In RLDM Workshop on Reinforcement Learning as a Model of
  Agency}, 2022.

\bibitem[Ng et~al.(2000)Ng, Russell, et~al.]{ng2000algorithms}
Ng, A.~Y., Russell, S., et~al.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, pp.\  663--670, 2000.

\bibitem[Pitis(2019)]{pitis2019rethinking}
Pitis, S.
\newblock Rethinking the discount factor in reinforcement learning: A decision
  theoretic approach.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2019.

\bibitem[Pitis et~al.(2022)Pitis, Bailey, and Ba]{pitisrational2022}
Pitis, S., Bailey, D., and Ba, J.
\newblock Rational multi-objective agents must admit non-markov reward
  representations.
\newblock \emph{NeurIPS Workshop on Machine Learning Safety}, 2022.

\bibitem[Ramsey(1926)]{ramsey2016truth}
Ramsey, F.~P.
\newblock Truth and probability.
\newblock In \emph{Readings in formal epistemology}, pp.\  21--45. Springer,
  1926.

\bibitem[Richter \& Wong(1999)Richter and Wong]{richter1999computable}
Richter, M.~K. and Wong, K.-C.
\newblock Computable preference and utility.
\newblock \emph{Journal of Mathematical Economics}, 32\penalty0 (3):\penalty0
  339--354, 1999.

\bibitem[Rustem \& Velupillai(1990)Rustem and
  Velupillai]{rustem1990rationality}
Rustem, B. and Velupillai, K.
\newblock Rationality, computability, and complexity.
\newblock \emph{Journal of Economic Dynamics and Control}, 14\penalty0
  (2):\penalty0 419--432, 1990.

\bibitem[Shakerinava \& Ravanbakhsh(2022)Shakerinava and
  Ravanbakhsh]{shakerinava2022utility}
Shakerinava, M. and Ravanbakhsh, S.
\newblock Utility theory for sequential decision making.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2022.

\bibitem[Silver et~al.(2021)Silver, Singh, Precup, and
  Sutton]{silver2021reward}
Silver, D., Singh, S., Precup, D., and Sutton, R.~S.
\newblock Reward is enough.
\newblock \emph{Artificial Intelligence}, 299:\penalty0 103535, 2021.

\bibitem[Singh et~al.(2009)Singh, Lewis, and Barto]{singh2009rewards}
Singh, S., Lewis, R.~L., and Barto, A.~G.
\newblock Where do rewards come from?
\newblock In \emph{Proceedings of the Annual Conference of the Cognitive
  Science Society}, 2009.

\bibitem[Sobel(1975)]{sobel1975ordinal}
Sobel, M.~J.
\newblock Ordinal dynamic programming.
\newblock \emph{Management science}, 21\penalty0 (9):\penalty0 967--975, 1975.

\bibitem[Sunehag \& Hutter(2011)Sunehag and Hutter]{sunehag2011axioms}
Sunehag, P. and Hutter, M.
\newblock Axioms for rational reinforcement learning.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pp.\  338--352. Springer, 2011.

\bibitem[Sunehag \& Hutter(2015)Sunehag and Hutter]{sunehag2015rationality}
Sunehag, P. and Hutter, M.
\newblock Rationality, optimism and guarantees in general reinforcement
  learning.
\newblock \emph{The Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 1345--1390, 2015.

\bibitem[Sutton(2004)]{suttonwebRLhypothesis}
Sutton, R.~S.
\newblock The reward hypothesis.
\newblock
  \url{http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html},
  2004.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Szepesv{\' a}ri(2020)]{szepesvariwebRLhypothesis}
Szepesv{\' a}ri, C.
\newblock Constrained {MDP}s and the reward hypothesis.
\newblock
  \url{https://readingsml.blogspot.com/2020/03/constrained-mdps-and-reward-hypothesis.html},
  2020.

\bibitem[Tversky \& Kahneman(1983)Tversky and Kahneman]{tversky1983extensional}
Tversky, A. and Kahneman, D.
\newblock Extensional versus intuitive reasoning: The conjunction fallacy in
  probability judgment.
\newblock \emph{Psychological review}, 90\penalty0 (4):\penalty0 293, 1983.

\bibitem[von Neumann \& Morgenstern(1953)von Neumann and
  Morgenstern]{vonneumann1953theory}
von Neumann, J. and Morgenstern, O.
\newblock \emph{Theory of Games and Economic Behavior}.
\newblock Princeton University Press, third edition, 1953.

\bibitem[White(2017)]{white17}
White, M.
\newblock Unifying task specification in reinforcement learning.
\newblock In \emph{Proceedings of the Thirty-fourth International Conference on
  Machine Learning}, 2017.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, F{\"u}rnkranz,
  et~al.]{wirth2017survey}
Wirth, C., Akrour, R., Neumann, G., F{\"u}rnkranz, J., et~al.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (136):\penalty0 1--46, 2017.

\end{thebibliography}
