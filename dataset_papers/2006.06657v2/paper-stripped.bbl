\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adebayo et~al.(2018)Adebayo, Gilmer, Muelly, Goodfellow, Hardt, and
  Kim]{saliency_maps_sanity_checks}
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt,
  and Been Kim.
\newblock Sanity checks for saliency maps.
\newblock In \emph{NIPS}, 2018.
\newblock {\tt arXiv:1810.03292 [cs.CV]}.

\bibitem[Allen-Zhu et~al.(2018)Allen-Zhu, Li, and Song]{allen_deep_opt}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock \emph{arXiv preprint arXiv:1811.03962}, 2018.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and Telgarsky]{spec}
Peter~L Bartlett, Dylan~J Foster, and Matus~J Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6240--6249, 2017.

\bibitem[Bolte et~al.(2007)Bolte, Daniilidis, Lewis, and Shiota]{bolte_clarke}
J{\'e}r{\^o}me Bolte, Aris Daniilidis, Adrian Lewis, and Masahiro Shiota.
\newblock Clarke subgradients of stratifiable functions.
\newblock \emph{SIAM Journal on Optimization}, 18\penalty0 (2):\penalty0
  556--572, 2007.

\bibitem[Borwein and Lewis(2000)]{borwein_lewis}
Jonathan Borwein and Adrian Lewis.
\newblock \emph{Convex Analysis and Nonlinear Optimization}.
\newblock Springer Publishing Company, Incorporated, 2000.

\bibitem[{Chizat} and {Bach}(2018)]{chizat_bach_meanfield}
L{\' e}na{\" i}c {Chizat} and Francis {Bach}.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In \emph{NIPS}, 2018.
\newblock {\tt arXiv:1805.09545 [math.OC]}.

\bibitem[Chizat and Bach(2020)]{chizat_bach_imp}
Lenaic Chizat and Francis Bach.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock \emph{arXiv preprint arXiv:2002.04486}, 2020.

\bibitem[Clarke(1975)]{clarke}
Frank~H Clarke.
\newblock Generalized gradients and applications.
\newblock \emph{Transactions of the American Mathematical Society},
  205:\penalty0 247--262, 1975.

\bibitem[Clarke(1983)]{clarke_opt}
Frank~H. Clarke.
\newblock \emph{Optimization and Nonsmooth Analysis}.
\newblock Siam Classics in Applied Mathematics, 1983.

\bibitem[Coste(2000)]{coste_o}
Michel Coste.
\newblock \emph{An introduction to o-minimal geometry}.
\newblock Istituti editoriali e poligrafici internazionali Pisa, 2000.

\bibitem[Davis et~al.(2020)Davis, Drusvyatskiy, Kakade, and Lee]{davis_tame}
Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason~D Lee.
\newblock Stochastic subgradient method converges on tame functions.
\newblock \emph{Foundations of computational mathematics}, 20\penalty0
  (1):\penalty0 119--154, 2020.

\bibitem[Du et~al.(2018)Du, Lee, Li, Wang, and Zhai]{du_deep_opt}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1811.03804}, 2018.

\bibitem[Freund and Schapire(1997)]{freund_schapire_adaboost}
Yoav Freund and Robert~E. Schapire.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{J. Comput. Syst. Sci.}, 55\penalty0 (1):\penalty0 119--139,
  1997.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{rong_saddle_point_escape}
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.
\newblock Escaping from saddle points --- online stochastic gradient for tensor
  decomposition.
\newblock In \emph{COLT}, 2015.
\newblock {\tt arXiv:1503.02101 [cs.LG]}.

\bibitem[Grandjean(2007)]{grandjean_limit}
V~Grandjean.
\newblock On the limit set at infinity of a gradient trajectory of a
  semialgebraic function.
\newblock \emph{Journal of Differential Equations}, 233\penalty0 (1):\penalty0
  22--41, 2007.

\bibitem[Gunasekar et~al.(2018{\natexlab{a}})Gunasekar, Lee, Soudry, and
  Srebro]{GLSS18}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock \emph{arXiv preprint arXiv:1802.08246}, 2018{\natexlab{a}}.

\bibitem[Gunasekar et~al.(2018{\natexlab{b}})Gunasekar, Lee, Soudry, and
  Srebro]{nati_lnn}
Suriya Gunasekar, Jason~D Lee, Daniel Soudry, and Nati Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9461--9471, 2018{\natexlab{b}}.

\bibitem[Huang et~al.(2017)Huang, Liu, van~der Maaten, and
  Weinberger]{densenet}
Gao Huang, Zhuang Liu, Laurens van~der Maaten, and Kilian~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{CVPR}, 2017.
\newblock {\tt arXiv:1608.06993v5 [cs.CV]}.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot_ntk}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[Ji and Telgarsky(2018{\natexlab{a}})]{align}
Ziwei Ji and Matus Telgarsky.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock \emph{arXiv preprint arXiv:1810.02032}, 2018{\natexlab{a}}.

\bibitem[Ji and Telgarsky(2018{\natexlab{b}})]{min_norm}
Ziwei Ji and Matus Telgarsky.
\newblock Risk and parameter convergence of logistic regression.
\newblock \emph{arXiv preprint arXiv:1803.07300v2}, 2018{\natexlab{b}}.

\bibitem[Ji and Telgarsky(2019)]{dual_conv}
Ziwei Ji and Matus Telgarsky.
\newblock A refined primal-dual analysis of the implicit bias.
\newblock \emph{arXiv preprint arXiv:1906.04540}, 2019.

\bibitem[Jiang et~al.(2019)Jiang, Krishnan, Mobahi, and
  Bengio]{margindist_predict}
Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio.
\newblock Predicting the generalization gap in deep networks with margin
  distributions.
\newblock In \emph{ICLR}, 2019.
\newblock {\tt arXiv:1810.00113 [stat.ML]}.

\bibitem[Jiang et~al.(2020)Jiang, Neyshabur, Mobahi, Krishnan, and
  Bengio]{fantastic}
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy
  Bengio.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{ICLR}, 2020.
\newblock {\tt arXiv:1912.02178 [cs.LG]}.

\bibitem[Krizhevsky(2009)]{cifar}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock \url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf},
  2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{imagenet_sutskever}
Alex Krizhevsky, Ilya Sutskever, and Geoffery Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{NIPS}, 2012.

\bibitem[Kurdyka(1998)]{kurdyka_grad}
Krzysztof Kurdyka.
\newblock On gradients of functions definable in o-minimal structures.
\newblock In \emph{Annales de l'institut Fourier}, volume~48, pages 769--783,
  1998.

\bibitem[Kurdyka et~al.(2000{\natexlab{a}})Kurdyka, Mostowski, and
  Parusinski]{kurdyka_thom}
Krzysztof Kurdyka, Tadeusz Mostowski, and Adam Parusinski.
\newblock Proof of the gradient conjecture of r. thom.
\newblock \emph{Annals of Mathematics}, 152\penalty0 (3):\penalty0 763--792,
  2000{\natexlab{a}}.

\bibitem[Kurdyka et~al.(2000{\natexlab{b}})Kurdyka, Orro, and
  Simon]{kurdyka_sard}
Krzysztof Kurdyka, Patrice Orro, and St{\'e}phane Simon.
\newblock Semialgebraic sard theorem for generalized critical values.
\newblock \emph{Journal of differential geometry}, 56\penalty0 (1):\penalty0
  67--92, 2000{\natexlab{b}}.

\bibitem[Kurdyka et~al.(2006)Kurdyka, Parusi{\'n}ski, et~al.]{kurdyka_quasi}
Krzysztof Kurdyka, Adam Parusi{\'n}ski, et~al.
\newblock Quasi-convex decomposition in o-minimal structures. application to
  the gradient conjecture.
\newblock In \emph{Singularity theory and its applications}, pages 137--177.
  Mathematical Society of Japan, 2006.

\bibitem[L{\^e}~Loi(2010)]{le_lec}
Ta~L{\^e}~Loi.
\newblock Lecture 1: O-minimal structures.
\newblock In \emph{The Japanese-Australian Workshop on Real and Complex
  Singularities: JARCS III}, pages 19--30. Centre for Mathematics and its
  Applications, Mathematical Sciences Institute, The Australian National
  University, 2010.

\bibitem[Lyu and Li(2019)]{kaifeng_jian_margin}
Kaifeng Lyu and Jian Li.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock \emph{arXiv preprint arXiv:1906.05890}, 2019.

\bibitem[Mei et~al.(2019)Mei, Misiakiewicz, and
  Montanari]{montanari_mean_field}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Mean-field theory of two-layers neural networks: dimension-free
  bounds and kernel limit.
\newblock 2019.
\newblock {\tt arXiv:1902.06015 [stat.ML]}.

\bibitem[N{\'e}methi and Zaharia(1992)]{nemethi_milnor}
Andr{\'a}s N{\'e}methi and Alexandru Zaharia.
\newblock Milnor fibration at infinity.
\newblock \emph{Indagationes Mathematicae}, 3\penalty0 (3):\penalty0 323--335,
  1992.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and
  Chintala]{pytorch_official_citation}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{NeuRIPS}. 2019.

\bibitem[Shallue et~al.(2018)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{jascha_step_sizes}
Christopher~J. Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein,
  Roy Frostig, and George~E. Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock 2018.
\newblock {\tt arXiv:1811.03600 [cs.LG]}.

\bibitem[Soudry et~al.(2017)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{nati_logistic}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{arXiv preprint arXiv:1710.10345}, 2017.

\bibitem[Van~den Dries and Miller(1996)]{van_geo}
Lou Van~den Dries and Chris Miller.
\newblock Geometric categories and o-minimal structures.
\newblock \emph{Duke Math. J}, 84\penalty0 (2):\penalty0 497--540, 1996.

\bibitem[Wei et~al.(2018)Wei, Lee, Liu, and Ma]{wei_reg}
Colin Wei, Jason~D Lee, Qiang Liu, and Tengyu Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock \emph{arXiv preprint arXiv:1810.05369}, 2018.

\bibitem[Wilkie(1996)]{wilkie_exp}
Alex~J Wilkie.
\newblock Model completeness results for expansions of the ordered field of
  real numbers by restricted pfaffian functions and the exponential function.
\newblock \emph{Journal of the American Mathematical Society}, 9\penalty0
  (4):\penalty0 1051--1094, 1996.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou_deep_opt}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock \emph{arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
