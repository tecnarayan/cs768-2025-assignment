\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alber et~al.(1998)Alber, Iusem, and Solodov]{alber1998projected}
Ya~I. Alber, Alfredo~N. Iusem, and Mikhail~V. Solodov.
\newblock On the projected subgradient method for nonsmooth convex optimization
  in a {Hilbert} space.
\newblock \emph{Mathematical Programming}, 81\penalty0 (1):\penalty0 23--35,
  1998.

\bibitem[Balles and Hennig(2018)]{balles2018dissecting}
Lukas Balles and Philipp Hennig.
\newblock Dissecting {Adam}: The sign, magnitude and variance of stochastic
  gradients.
\newblock In \emph{International Conference on Machine Learning}, pages
  404--413. PMLR, 2018.

\bibitem[Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and
  Anandkumar]{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
  Anandkumar.
\newblock {signSGD}: Compressed optimisation for non-convex problems.
\newblock In \emph{International Conference on Machine Learning}, pages
  560--569. PMLR, 2018.

\bibitem[Boyd and Vandenberghe(2004)]{boyd2004convex}
Stephen Boyd and Lieven Vandenberghe.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Chen et~al.(2019{\natexlab{a}})Chen, Liu, Sun, and
  Hong]{chen2018convergence}
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong.
\newblock On the convergence of a class of {Adam}-type algorithms for
  non-convex optimization.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Chen et~al.(2019{\natexlab{b}})Chen, Yuan, Yi, Zhou, Chen, and
  Yang]{chen2018universal}
Zaiyi Chen, Zhuoning Yuan, Jinfeng Yi, Bowen Zhou, Enhong Chen, and Tianbao
  Yang.
\newblock Universal stagewise learning for non-convex problems with convergence
  on averaged solutions.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=Syx5V2CcFm}.

\bibitem[Cutkosky and Mehta(2020)]{cutkosky2020momentum}
Ashok Cutkosky and Harsh Mehta.
\newblock Momentum improves normalized {SGD}.
\newblock In \emph{International Conference on Machine Learning}, pages
  2260--2268. PMLR, 2020.

\bibitem[Cutkosky and Mehta(2021)]{CutkoskyM21}
Ashok Cutkosky and Harsh Mehta.
\newblock High-probability bounds for non-convex stochastic optimization with
  heavy tails.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Cutkosky and Orabona(2019)]{CutkoskyO19}
Ashok Cutkosky and Francesco Orabona.
\newblock Momentum-based variance reduction in non-convex {SGD}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  15236--15245, 2019.

\bibitem[D{\'e}fossez et~al.(2020)D{\'e}fossez, Bottou, Bach, and
  Usunier]{defossez2020simple}
Alexandre D{\'e}fossez, L{\'e}on Bottou, Francis Bach, and Nicolas Usunier.
\newblock A simple convergence proof of {Adam} and {Adagrad}.
\newblock \emph{arXiv preprint arXiv:2003.02395}, 2020.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Ermoliev(1988)]{ermoliev1988stochastic}
Yuri Ermoliev.
\newblock Stochastic quasigradient methods.
\newblock In \emph{Numerical techniques for stochastic optimization}, number~10
  in Springer Series in Computational Mathematics, pages 141--185. Springer,
  1988.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Gorbunov et~al.(2020)Gorbunov, Danilova, and
  Gasnikov]{gorbunov2020stochastic}
Eduard Gorbunov, Marina Danilova, and Alexander Gasnikov.
\newblock Stochastic optimization with heavy-tailed noise via accelerated
  gradient clipping.
\newblock \emph{arXiv preprint arXiv:2005.10785}, 2020.

\bibitem[Harvey et~al.(2019)Harvey, Liaw, Plan, and Randhawa]{harvey2019tight}
Nicholas~JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa.
\newblock Tight analyses for non-smooth stochastic gradient descent.
\newblock In \emph{Conference on Learning Theory}, pages 1579--1613. PMLR,
  2019.

\bibitem[Hazan et~al.(2015)Hazan, Levy, and Shalev-Shwartz]{hazan2015beyond}
Elad Hazan, Kfir~Y Levy, and Shai Shalev-Shwartz.
\newblock Beyond convexity: Stochastic quasi-convex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hinton et~al.(2012)Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior,
  Vanhoucke, Nguyen, and Kingsbury]{hinton2012deep}
Geoffrey Hinton, Li~Deng, Dong Yu, George Dahl, Abdel-rahman Mohamed, Navdeep
  Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, and Brian
  Kingsbury.
\newblock Deep neural networks for acoustic modeling in speech recognition.
\newblock \emph{IEEE Signal processing magazine}, 29, 2012.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Jain and Kar(2017)]{Jain2017NonconvexOF}
Prateek Jain and Purushottam Kar.
\newblock Non-convex optimization for machine learning.
\newblock \emph{Found. Trends Mach. Learn.}, 10:\penalty0 142--336, 2017.

\bibitem[Jin et~al.(2021)Jin, Zhang, Wang, and Wang]{Robustness21Jin}
Jikai Jin, Bohang Zhang, Haiyang Wang, and Liwei Wang.
\newblock Non-convex distributionally robust optimization: Non-asymptotic
  analysis.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 2771--2782. Curran Associates, Inc., 2021.

\bibitem[Kamp et~al.(2018)Kamp, Adilova, Sicking, H{\"u}ger, Schlicht, Wirtz,
  and Wrobel]{kamp2018efficient}
Michael Kamp, Linara Adilova, Joachim Sicking, Fabian H{\"u}ger, Peter
  Schlicht, Tim Wirtz, and Stefan Wrobel.
\newblock Efficient decentralized deep learning by dynamic model averaging.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 393--409. Springer, 2018.

\bibitem[Khaled and Richt{\'a}rik(2020)]{khaled2020better}
Ahmed Khaled and Peter Richt{\'a}rik.
\newblock Better theory for {SGD} in the nonconvex world.
\newblock \emph{arXiv preprint arXiv:2002.03329}, 2020.

\bibitem[Kingma and Ba(2015)]{KingmaB14}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Lee et~al.(2015)Lee, Xie, Gallagher, Zhang, and Tu]{lee2015deeply}
Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu.
\newblock Deeply-supervised nets.
\newblock In \emph{Artificial intelligence and statistics}, pages 562--570.
  PMLR, 2015.

\bibitem[Li et~al.(2014)Li, Andersen, Park, Smola, Ahmed, Josifovski, Long,
  Shekita, and Su]{li2014scaling}
Mu~Li, David~G Andersen, Jun~Woo Park, Alexander~J Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{OSDI}, volume~14, pages 583--598, 2014.

\bibitem[Li and Orabona(2019)]{LiO19}
Xiaoyu Li and Francesco Orabona.
\newblock On the convergence of stochastic gradient descent with adaptive
  stepsizes.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 983--992. PMLR, 2019.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Mai and Johansson(2021)]{mai2021stability}
Vien~V Mai and Mikael Johansson.
\newblock Stability and convergence of stochastic gradient clipping: Beyond
  lipschitz continuity and smoothness.
\newblock In \emph{International Conference on Machine Learning}, pages
  7325--7335. PMLR, 2021.

\bibitem[Marcus et~al.(1993)Marcus, Marcinkiewicz, and
  Santorini]{marcus1993building}
Mitchell~P. Marcus, Mary~Ann Marcinkiewicz, and Beatrice Santorini.
\newblock Building a large annotated corpus of {English}: The {Penn}
  {Treebank}.
\newblock \emph{Comput. Linguist.}, 19\penalty0 (2):\penalty0 313â€“330, June
  1993.
\newblock ISSN 0891-2017.

\bibitem[McMahan and Streeter(2010)]{mcmahan2010adaptive}
H~Brendan McMahan and Matthew Streeter.
\newblock Adaptive bound optimization for online convex optimization.
\newblock \emph{arXiv preprint arXiv:1002.4908}, 2010.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and Socher]{wikitext103}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{ICLR}, 2017.
\newblock URL \url{http://arxiv.org/abs/1609.07843}.

\bibitem[Merity et~al.(2018)Merity, Keskar, and Socher]{merity2018regularizing}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock Regularizing and optimizing {LSTM} language models.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=SyyGPP0TZ}.

\bibitem[Orabona and P{\'a}l(2015)]{OrabonaP15}
Francesco Orabona and D\'{a}vid P{\'a}l.
\newblock Scale-free algorithms for online linear optimization.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 287--301. Springer, 2015.

\bibitem[Orabona and P\'{a}l(2018)]{OrabonaP18}
Francesco Orabona and D\'{a}vid P\'{a}l.
\newblock Scale-free online learning.
\newblock \emph{Theoretical Computer Science}, 716:\penalty0 50--69, 2018.
\newblock Special Issue on {ALT} 2015.

\bibitem[Pascanu et~al.(2012)Pascanu, Mikolov, and
  Bengio]{pascanu2012understanding}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock Understanding the exploding gradient problem. corr abs/1211.5063
  (2012).
\newblock \emph{arXiv preprint arXiv:1211.5063}, 2012.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International conference on machine learning}, pages
  1310--1318. PMLR, 2013.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8024--8035, 2019.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{gpt2}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{reddi2019convergence}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of {Adam} and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Reddi et~al.(2021)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\'y}, Kumar, and McMahan]{reddi2021adaptive}
Sashank~J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Kone{\v{c}}n{\'y}, Sanjiv Kumar, and Hugh~Brendan McMahan.
\newblock Adaptive federated optimization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=LkFG3lB13U5}.

\bibitem[Richt{\'a}rik and Tak{\'a}c(2014)]{RichtrikT14}
Peter Richt{\'a}rik and Martin Tak{\'a}c.
\newblock Iteration complexity of randomized block-coordinate descent methods
  for minimizing a composite function.
\newblock \emph{Mathematical Programming}, 144:\penalty0 1--38, 2014.

\bibitem[Riedmiller and Braun(1993)]{riedmiller1993direct}
Martin Riedmiller and Heinrich Braun.
\newblock A direct adaptive method for faster backpropagation learning: The
  {RPROP} algorithm.
\newblock In \emph{IEEE international conference on neural networks}, pages
  586--591. IEEE, 1993.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Shor(2012)]{shor2012minimization}
Naum~Zuselevich Shor.
\newblock \emph{Minimization methods for non-differentiable functions},
  volume~3.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Tieleman et~al.(2012)Tieleman, Hinton, et~al.]{tieleman2012lecture}
Tijmen Tieleman, Geoffrey Hinton, et~al.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0
  (2):\penalty0 26--31, 2012.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{VaswaniSPUJGKP17}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2021)Wang, Kang, Qin, Wang, Xu, Zhang, and
  Fu]{WangKQWXZF21}
Yizhou Wang, Yue Kang, Can Qin, Huan Wang, Yilun Xu, Yulun Zhang, and
  Yun~Raymond Fu.
\newblock Rethinking adam: A twofold exponential moving average approach.
\newblock \emph{arXiv preprint arXiv:2106.11514}, 2021.

\bibitem[Ward et~al.(2019)Ward, Wu, and Bottou]{WardWB19}
Rachel Ward, Xiaoxia Wu, and Leon Bottou.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes.
\newblock In \emph{International Conference on Machine Learning}, pages
  6677--6686. PMLR, 2019.

\bibitem[Wolf.(2019)]{gpt2_naacl}
Thomas Wolf.
\newblock \emph{Transfer Learning in Natural Language Processing}, 2019.
\newblock URL
  \url{https://github.com/huggingface/naacl_transfer_learning_tutorial}.
\newblock Available at
  \url{https://github.com/huggingface/naacl_transfer_learning_tutorial}.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and
  Kumar]{zaheer2018adaptive}
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Jin, Fang, and
  Wang]{zhang2020improved}
Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang.
\newblock Improved analysis of clipping algorithms for non-convex optimization.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 15511--15521. Curran Associates, Inc., 2020{\natexlab{a}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/b282d1735283e8eea45bce393cefe265-Paper.pdf}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, He, Sra, and
  Jadbabaie]{zhang2019gradient}
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie.
\newblock Why gradient clipping accelerates training: A theoretical
  justification for adaptivity.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2020{\natexlab{c}})Zhang, Karimireddy, Veit, Kim, Reddi,
  Kumar, and Sra]{zhang2020adaptive}
Jingzhao Zhang, Sai~Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank
  Reddi, Sanjiv Kumar, and Suvrit Sra.
\newblock Why are adaptive methods good for attention models?
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15383--15393, 2020{\natexlab{c}}.

\bibitem[Zhuang et~al.(2022)Zhuang, Liu, Cutkosky, and
  Orabona]{zhuang2022understanding}
Zhenxun Zhuang, Mingrui Liu, Ashok Cutkosky, and Francesco Orabona.
\newblock Understanding {AdamW} through proximal methods and scale-freeness.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock URL \url{https://openreview.net/forum?id=IKhEPWGdwK}.

\bibitem[Zou et~al.(2021)Zou, Cao, Li, and Gu]{ZouCLG21}
Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu.
\newblock Understanding the generalization of {Adam} in learning neural
  networks with proper regularization.
\newblock \emph{arXiv preprint arXiv:2108.11371}, 2021.

\bibitem[Zou et~al.(2019)Zou, Shen, Jie, Zhang, and Liu]{zou2019sufficient}
Fangyu Zou, Li~Shen, Zequn Jie, Weizhong Zhang, and Wei Liu.
\newblock A sufficient condition for convergences of {Adam} and {RMSProp}.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 11127--11135, 2019.

\end{thebibliography}
