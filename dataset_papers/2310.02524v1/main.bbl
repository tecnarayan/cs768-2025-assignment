\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bao et~al.(2022{\natexlab{a}})Bao, Gu, and Huang]{bao2022accelerated}
Runxue Bao, Bin Gu, and Heng Huang.
\newblock An accelerated doubly stochastic gradient method with faster explicit
  model identification.
\newblock In \emph{Proceedings of the 31st ACM International Conference on
  Information \& Knowledge Management}, pages 57--66, 2022{\natexlab{a}}.

\bibitem[Bao et~al.(2022{\natexlab{b}})Bao, Wu, Xian, and Huang]{bao2022doubly}
Runxue Bao, Xidong Wu, Wenhan Xian, and Heng Huang.
\newblock Doubly sparse asynchronous learning for stochastic composite
  optimization.
\newblock In \emph{Thirty-First International Joint Conference on Artificial
  Intelligence (IJCAI)}, pages 1916--1922, 2022{\natexlab{b}}.

\bibitem[Chen et~al.(2018)Chen, Luo, Dong, Li, and He]{chen2018federated}
Fei Chen, Mi~Luo, Zhenhua Dong, Zhenguo Li, and Xiuqiang He.
\newblock Federated meta-learning with fast convergence and efficient
  communication.
\newblock \emph{arXiv preprint arXiv:1802.07876}, 2018.

\bibitem[Chen(2021)]{chen2021representation}
Jiangce Chen.
\newblock \emph{Representation-agnostic Shape Matching of 3D Models with Graph
  Machine Learning Methods}.
\newblock PhD thesis, University of Connecticut, 2021.

\bibitem[Chen et~al.(2022)Chen, Ilies, and Ding]{chen2022graph}
Jiangce Chen, Horea~T Ilies, and Caiwen Ding.
\newblock Graph-based shape analysis for heterogeneous geometric datasets:
  Similarity, retrieval and substructure matching.
\newblock \emph{Computer-Aided Design}, 143:\penalty0 103125, 2022.

\bibitem[Cutkosky and Orabona(2019)]{cutkosky2019momentum}
Ashok Cutkosky and Francesco Orabona.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Dai et~al.(2017)Dai, He, Pan, Boots, and Song]{dai2017learning}
Bo~Dai, Niao He, Yunpeng Pan, Byron Boots, and Le~Song.
\newblock Learning from conditional distributions via dual embeddings.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1458--1467.
  PMLR, 2017.

\bibitem[Das et~al.(2022)Das, Acharya, Hashemi, Sanghavi, Dhillon, and
  Topcu]{das2022faster}
Rudrajit Das, Anish Acharya, Abolfazl Hashemi, Sujay Sanghavi, Inderjit~S
  Dhillon, and Ufuk Topcu.
\newblock Faster non-convex federated learning via global and local momentum.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 496--506.
  PMLR, 2022.

\bibitem[Deleu et~al.(2019)Deleu, W\"urfl, Samiei, Cohen, and
  Bengio]{deleu2019torchmeta}
Tristan Deleu, Tobias W\"urfl, Mandana Samiei, Joseph~Paul Cohen, and Yoshua
  Bengio.
\newblock {Torchmeta: A Meta-Learning library for PyTorch}, 2019.
\newblock URL \url{https://arxiv.org/abs/1909.06576}.
\newblock Available at: https://github.com/tristandeleu/pytorch-meta.

\bibitem[Dou et~al.(2022)Dou, Jia, Zaslavsky, Bao, Zhang, Ni, Liang, Mao, and
  Mao]{dou2022learning}
Jason~Xiaotian Dou, Minxue Jia, Nika Zaslavsky, Runxue Bao, Shiyi Zhang, Ke~Ni,
  Paul~Pu Liang, Haiyi Mao, and Zhihong Mao.
\newblock Learning more effective cell representations efficiently.
\newblock In \emph{NeurIPS 2022 Workshop on Learning Meaningful Representations
  of Life}, 2022.
\newblock URL \url{https://openreview.net/forum?id=oRULd-eaNZH}.

\bibitem[Fallah et~al.(2020)Fallah, Mokhtari, and
  Ozdaglar]{fallah2020personalized}
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
\newblock Personalized federated learning: A meta-learning approach.
\newblock \emph{arXiv preprint arXiv:2002.07948}, 2020.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic
  path-integrated differential estimator.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International conference on machine learning}, pages
  1126--1135. PMLR, 2017.

\bibitem[Gao et~al.(2022)Gao, Li, and Huang]{gao2022convergence}
Hongchang Gao, Junyi Li, and Heng Huang.
\newblock On the convergence of local stochastic compositional gradient descent
  with momentum.
\newblock In \emph{International Conference on Machine Learning}, pages
  7017--7035. PMLR, 2022.

\bibitem[Gu et~al.(2023)Gu, Bao, Zhang, and Huang]{gu2023new}
Bin Gu, Runxue Bao, Chenkang Zhang, and Heng Huang.
\newblock New scalable and efficient online pairwise learning algorithm.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  2023.

\bibitem[Guo et~al.(2020)Guo, Liu, Yuan, Shen, Liu, and
  Yang]{guo2020communication}
Zhishuai Guo, Mingrui Liu, Zhuoning Yuan, Li~Shen, Wei Liu, and Tianbao Yang.
\newblock Communication-efficient distributed stochastic auc maximization with
  deep neural networks.
\newblock In \emph{International conference on machine learning}, pages
  3864--3874. PMLR, 2020.

\bibitem[Guo et~al.(2022)Guo, Jin, Luo, and Yang]{guo2022fedxl}
Zhishuai Guo, Rong Jin, Jiebo Luo, and Tianbao Yang.
\newblock Fedxl: Provable federated learning for deep x-risk optimization.
\newblock 2022.

\bibitem[Hu et~al.(2019)Hu, Li, Lian, Liu, and Yuan]{hu2019efficient}
Wenqing Hu, Chris~Junchi Li, Xiangru Lian, Ji~Liu, and Huizhuo Yuan.
\newblock Efficient smooth non-convex stochastic compositional optimization via
  stochastic recursive gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Hu et~al.(2020{\natexlab{a}})Hu, Chen, and He]{hu2020sample}
Yifan Hu, Xin Chen, and Niao He.
\newblock Sample complexity of sample average approximation for conditional
  stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (3):\penalty0
  2103--2133, 2020{\natexlab{a}}.

\bibitem[Hu et~al.(2020{\natexlab{b}})Hu, Zhang, Chen, and He]{hu2020biased}
Yifan Hu, Siqi Zhang, Xin Chen, and Niao He.
\newblock Biased stochastic first-order methods for conditional stochastic
  optimization and applications in meta learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2759--2770, 2020{\natexlab{b}}.

\bibitem[Hu et~al.(2021)Hu, Chen, and He]{hu2021bias}
Yifan Hu, Xin Chen, and Niao He.
\newblock On the bias-variance-cost tradeoff of stochastic optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 22119--22131, 2021.

\bibitem[Hu et~al.(2023)Hu, Wu, and Huang]{hu2023beyond}
Zhengmian Hu, Xidong Wu, and Heng Huang.
\newblock Beyond lipschitz smoothness: A tighter analysis for nonconvex
  optimization.
\newblock 2023.

\bibitem[Huang et~al.(2021)Huang, Li, and Huang]{huang2021compositional}
Feihu Huang, Junyi Li, and Heng Huang.
\newblock Compositional federated learning: Applications in distributionally
  robust averaging and meta learning.
\newblock \emph{arXiv preprint arXiv:2106.11264}, 2021.

\bibitem[Huo et~al.(2018)Huo, Gu, Liu, and Huang]{huo2018accelerated}
Zhouyuan Huo, Bin Gu, Ji~Liu, and Heng Huang.
\newblock Accelerated method for stochastic composition optimization with
  nonsmooth regularization.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Karimireddy et~al.(2020{\natexlab{a}})Karimireddy, Jaggi, Kale, Mohri,
  Reddi, Stich, and Suresh]{karimireddy2020mime}
Sai~Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank~J
  Reddi, Sebastian~U Stich, and Ananda~Theertha Suresh.
\newblock Mime: Mimicking centralized stochastic algorithms in federated
  learning.
\newblock \emph{arXiv preprint arXiv:2008.03606}, 2020{\natexlab{a}}.

\bibitem[Karimireddy et~al.(2020{\natexlab{b}})Karimireddy, Kale, Mohri, Reddi,
  Stich, and Suresh]{karimireddy2020scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
  Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International conference on machine learning}, pages
  5132--5143. PMLR, 2020{\natexlab{b}}.

\bibitem[Khanduri et~al.(2021)Khanduri, Sharma, Yang, Hong, Liu, Rajawat, and
  Varshney]{khanduri2021stem}
Prashant Khanduri, Pranay Sharma, Haibo Yang, Mingyi Hong, Jia Liu, Ketan
  Rajawat, and Pramod Varshney.
\newblock Stem: A stochastic two-sided momentum algorithm achieving
  near-optimal sample and communication complexities for federated learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 6050--6061, 2021.

\bibitem[Lake et~al.(2011)Lake, Salakhutdinov, Gross, and
  Tenenbaum]{lake2011one}
Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum.
\newblock One shot learning of simple visual concepts.
\newblock In \emph{Proceedings of the annual meeting of the cognitive science
  society}, volume~33, 2011.

\bibitem[Li et~al.(2022)Li, Zhang, Maybank, and Tao]{li2022bridging}
Jizhizi Li, Jing Zhang, Stephen~J Maybank, and Dacheng Tao.
\newblock Bridging composite and real: towards end-to-end deep image matting.
\newblock \emph{International Journal of Computer Vision}, 130\penalty0
  (2):\penalty0 246--266, 2022.

\bibitem[Li et~al.(2020)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2020federated}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{Proceedings of Machine learning and systems}, 2:\penalty0
  429--450, 2020.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial intelligence and statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem[Mei et~al.(2023)Mei, Zhou, Lan, Venkataramani, and Wei]{mei2023mac}
Yongsheng Mei, Hanhan Zhou, Tian Lan, Guru Venkataramani, and Peng Wei.
\newblock Mac-po: Multi-agent experience replay via collective priority
  optimization.
\newblock In \emph{Proceedings of the 2023 International Conference on
  Autonomous Agents and Multiagent Systems}, pages 466--475, 2023.

\bibitem[Mroueh et~al.(2015)Mroueh, Voinea, and Poggio]{mroueh2015learning}
Youssef Mroueh, Stephen Voinea, and Tomaso~A Poggio.
\newblock Learning with group invariant features: A kernel perspective.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.

\bibitem[Qi et~al.(2021)Qi, Luo, Xu, Ji, and Yang]{qi2021stochastic}
Qi~Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, and Tianbao Yang.
\newblock Stochastic optimization of areas under precision-recall curves with
  provable convergence.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 1752--1765, 2021.

\bibitem[Reddi et~al.(2020)Reddi, Charles, Zaheer, Garrett, Rush,
  Kone{\v{c}}n{\`y}, Kumar, and McMahan]{reddi2020adaptive}
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
  Jakub Kone{\v{c}}n{\`y}, Sanjiv Kumar, and H~Brendan McMahan.
\newblock Adaptive federated optimization.
\newblock \emph{arXiv preprint arXiv:2003.00295}, 2020.

\bibitem[Singh et~al.(2019)Singh, Sahani, and Gretton]{singh2019kernel}
Rahul Singh, Maneesh Sahani, and Arthur Gretton.
\newblock Kernel instrumental variable regression.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra,
  et~al.]{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Wang and Yang(2022)]{why2022finite}
Bokun Wang and Tianbao Yang.
\newblock Finite-sum coupled compositional stochastic optimization: Theory and
  applications.
\newblock In \emph{International Conference on Machine Learning, {ICML} 2022,
  17-23 July 2022, Baltimore, Maryland, {USA}}, volume 162 of \emph{Proceedings
  of Machine Learning Research}, pages 23292--23317. PMLR, 2022.

\bibitem[Wang et~al.(2021)Wang, Yuan, Ying, and Yang]{wang2021memory}
Bokun Wang, Zhuoning Yuan, Yiming Ying, and Tianbao Yang.
\newblock Memory-based optimization methods for model-agnostic meta-learning.
\newblock \emph{arXiv preprint arXiv:2106.04911}, 2021.

\bibitem[Wang et~al.(2022)Wang, Yang, Zhang, and Yang]{wang2022momentum}
Guanghui Wang, Ming Yang, Lijun Zhang, and Tianbao Yang.
\newblock Momentum accelerates the convergence of stochastic auprc
  maximization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3753--3771. PMLR, 2022.

\bibitem[Wang et~al.(2023)Wang, Li, and Hu]{wang2023b}
Jialu Wang, Ping Li, and Feifang Hu.
\newblock A/b testing in network data with covariate-adaptive randomization.
\newblock 2023.

\bibitem[Wang et~al.(2016)Wang, Liu, and Fang]{wang2016accelerating}
Mengdi Wang, Ji~Liu, and Ethan Fang.
\newblock Accelerating stochastic composition optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Wang et~al.(2017)Wang, Fang, and Liu]{wang2017stochastic}
Mengdi Wang, Ethan~X Fang, and Han Liu.
\newblock Stochastic compositional gradient descent: algorithms for minimizing
  compositions of expected-value functions.
\newblock \emph{Mathematical Programming}, 161\penalty0 (1):\penalty0 419--449,
  2017.

\bibitem[Wu et~al.(2022)Wu, Huang, and Huang]{wu2022fast}
Xidong Wu, Feihu Huang, and Heng Huang.
\newblock Fast stochastic recursive momentum methods for imbalanced data
  mining.
\newblock In \emph{2022 IEEE International Conference on Data Mining (ICDM)},
  pages 578--587. IEEE, 2022.

\bibitem[Wu et~al.(2023{\natexlab{a}})Wu, Hu, Pei, and Huang]{wu2023serverless}
Xidong Wu, Zhengmian Hu, Jian Pei, and Heng Huang.
\newblock Serverless federated auprc optimization for multi-party collaborative
  imbalanced data mining.
\newblock In \emph{Proceedings of the 29th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, pages 2648--2659, 2023{\natexlab{a}}.

\bibitem[Wu et~al.(2023{\natexlab{b}})Wu, Huang, Hu, and Huang]{wu2023faster}
Xidong Wu, Feihu Huang, Zhengmian Hu, and Heng Huang.
\newblock Faster adaptive federated learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~37, pages 10379--10387, 2023{\natexlab{b}}.

\bibitem[Wu et~al.(2023{\natexlab{c}})Wu, Sun, Hu, Zhang, and
  Huang]{wu2023solving}
Xidong Wu, Jianhui Sun, Zhengmian Hu, Aidong Zhang, and Heng Huang.
\newblock Solving a class of non-convex minimax optimization in federated
  learning.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2023{\natexlab{c}}.

\bibitem[Yuan and Hu(2020)]{yuan2020stochastic}
Huizhuo Yuan and Wenqing Hu.
\newblock Stochastic recursive momentum method for non-convex compositional
  optimization.
\newblock \emph{arXiv preprint arXiv:2006.01688}, 2020.

\bibitem[Yuan et~al.(2021)Yuan, Guo, Xu, Ying, and Yang]{yuan2021federated}
Zhuoning Yuan, Zhishuai Guo, Yi~Xu, Yiming Ying, and Tianbao Yang.
\newblock Federated deep auc maximization for hetergeneous data with a constant
  communication complexity.
\newblock In \emph{International Conference on Machine Learning}, pages
  12219--12229. PMLR, 2021.

\bibitem[Zhang and Xiao(2019)]{zhang2019composite}
Junyu Zhang and Lin Xiao.
\newblock A composite randomized incremental gradient method.
\newblock In \emph{International Conference on Machine Learning}, pages
  7454--7462. PMLR, 2019.

\end{thebibliography}
