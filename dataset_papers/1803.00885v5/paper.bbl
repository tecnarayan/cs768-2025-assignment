\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio]{bahdanau15neural}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock In \emph{ICLR}, 2015.

\bibitem[Ballard et~al.(2016)Ballard, Stevenson, Das, and
  Wales]{ballard16energy}
Ballard, A.~J., Stevenson, J.~D., Das, R., and Wales, D.~J.
\newblock Energy landscapes for a machine learning application to series data.
\newblock \emph{J. Chem. Phys.}, 144\penalty0 (12):\penalty0 124119, Mar 2016.
\newblock ISSN 1089-7690.
\newblock \doi{10.1063/1.4944672}.
\newblock URL \url{http://dx.doi.org/10.1063/1.4944672}.

\bibitem[{Ballard} et~al.(2017){Ballard}, {Das}, {Martiniani}, {Mehta},
  {Sagun}, {Stevenson}, and {Wales}]{ballard17perspective}
{Ballard}, A.~J., {Das}, R., {Martiniani}, S., {Mehta}, D., {Sagun}, L.,
  {Stevenson}, J.~D., and {Wales}, D.~J.
\newblock {Energy landscapes for machine learning}.
\newblock \emph{Physical Chemistry Chemical Physics (Incorporating Faraday
  Transactions)}, 19:\penalty0 12585--12603, 2017.
\newblock \doi{10.1039/C7CP01108C}.

\bibitem[Choromanska et~al.(2014)Choromanska, Henaff, Mathieu, Arous, and
  LeCun]{choromanska14loss}
Choromanska, A., Henaff, M., Mathieu, M., Arous, G.~B., and LeCun, Y.
\newblock The loss surface of multilayer networks.
\newblock \emph{CoRR}, abs/1412.0233, 2014.
\newblock URL \url{http://arxiv.org/abs/1412.0233}.

\bibitem[Ciresan et~al.(2011)Ciresan, Meier, Masci, Maria~Gambardella, and
  Schmidhuber]{ciresan11flexible}
Ciresan, D.~C., Meier, U., Masci, J., Maria~Gambardella, L., and Schmidhuber,
  J.
\newblock Flexible, high performance convolutional neural networks for image
  classification.
\newblock In \emph{IJCAI Proceedings-International Joint Conference on
  Artificial Intelligence}, volume~22, pp.\  1237. Barcelona, Spain, 2011.

\bibitem[Dauphin et~al.(2014)Dauphin, Pascanu, G{\"{u}}l{\c{c}}ehre, Cho,
  Ganguli, and Bengio]{dauphin14identifying}
Dauphin, Y., Pascanu, R., G{\"{u}}l{\c{c}}ehre, {\c{C}}., Cho, K., Ganguli, S.,
  and Bengio, Y.
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock \emph{CoRR}, abs/1406.2572, June 2014.
\newblock URL \url{http://arxiv.org/abs/1406.2572}.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh17sharp}
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y.
\newblock Sharp minima can generalize for deep nets.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1019--1028, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v70/dinh17b.html}.

\bibitem[{Freeman} \& {Bruna}(2016){Freeman} and {Bruna}]{freeman16topology}
{Freeman}, C.~D. and {Bruna}, J.
\newblock {Topology and Geometry of Half-Rectified Network Optimization}.
\newblock \emph{ArXiv e-prints}, November 2016.
\newblock URL \url{http://arxiv.org/abs/1611.01540}.

\bibitem[{Garipov} et~al.(2018){Garipov}, {Izmailov}, {Podoprikhin}, {Vetrov},
  and {Wilson}]{garipov18loss}
{Garipov}, T., {Izmailov}, P., {Podoprikhin}, D., {Vetrov}, D.~P., and
  {Wilson}, A.~G.
\newblock {Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs}.
\newblock \emph{ArXiv e-prints}, February 2018.
\newblock URL \url{http://arxiv.org/abs/1802.10026}.

\bibitem[Gower \& Ross(1969)Gower and Ross]{gower69minimum}
Gower, J.~C. and Ross, G. J.~S.
\newblock Minimum spanning trees and single linkage cluster analysis.
\newblock \emph{Journal of the Royal Statistical Society. Series C (Applied
  Statistics)}, 18\penalty0 (1):\penalty0 54--64, 1969.
\newblock ISSN 00359254, 14679876.
\newblock URL \url{http://www.jstor.org/stable/2346439}.

\bibitem[Graves et~al.(2013)Graves, Mohamed, and Hinton]{graves13speech}
Graves, A., Mohamed, A.-r., and Hinton, G.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In \emph{Acoustics, speech and signal processing (icassp), 2013 ieee
  international conference on}, pp.\  6645--6649. IEEE, 2013.

\bibitem[Hansen \& Salamon(1990)Hansen and Salamon]{hansen90neural}
Hansen, L.~K. and Salamon, P.
\newblock Neural network ensembles.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 12\penalty0 (10):\penalty0 993--1001, 1990.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he16deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Henkelman \& J{\'o}nsson(2000)Henkelman and
  J{\'o}nsson]{henkelman00improved}
Henkelman, G. and J{\'o}nsson, H.
\newblock Improved tangent estimate in the nudged elastic band method for
  finding minimum energy paths and saddle points.
\newblock \emph{The Journal of chemical physics}, 113\penalty0 (22):\penalty0
  9978--9985, 2000.

\bibitem[Hinton et~al.(2012)Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior,
  Vanhoucke, Nguyen, Sainath, et~al.]{hinton12deep}
Hinton, G., Deng, L., Yu, D., Dahl, G.~E., Mohamed, A.-r., Jaitly, N., Senior,
  A., Vanhoucke, V., Nguyen, P., Sainath, T.~N., et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock \emph{IEEE Signal Processing Magazine}, 29\penalty0 (6):\penalty0
  82--97, 2012.

\bibitem[Huang et~al.(2017)Huang, Liu, Weinberger, and van~der
  Maaten]{huang17densely}
Huang, G., Liu, Z., Weinberger, K.~Q., and van~der Maaten, L.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, volume~1, pp.\ ~3, 2017.

\bibitem[{Ioffe} \& {Szegedy}(2015){Ioffe} and {Szegedy}]{ioffe15batch}
{Ioffe}, S. and {Szegedy}, C.
\newblock {Batch Normalization: Accelerating Deep Network Training by Reducing
  Internal Covariate Shift}.
\newblock \emph{ArXiv e-prints}, February 2015.

\bibitem[J{\'o}nsson et~al.(1998)J{\'o}nsson, Mills, and
  Jacobsen]{jonsson1998nudged}
J{\'o}nsson, H., Mills, G., and Jacobsen, K.~W.
\newblock Nudged elastic band method for finding minimum energy paths of
  transitions.
\newblock In \emph{Classical and quantum dynamics in condensed phase
  simulations}, pp.\  385--404. World Scientific, 1998.

\bibitem[Kawaguchi(2016)]{kawaguchi16deep}
Kawaguchi, K.
\newblock Deep learning without poor local minima.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  586--594, 2016.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar16large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, September 2016.
\newblock URL \url{http://arxiv.org/abs/1609.04836}.

\bibitem[Kolsbjerg et~al.(2016)Kolsbjerg, Groves, and
  Hammer]{kolsbjerg16automated}
Kolsbjerg, E.~L., Groves, M.~N., and Hammer, B.
\newblock An automated nudged elastic band method.
\newblock \emph{The Journal of chemical physics}, 145\penalty0 (9):\penalty0
  094107, 2016.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{krizhevsky09learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun98gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2017)Li, Xu, Taylor, and Goldstein]{li17visualizing}
Li, H., Xu, Z., Taylor, G., and Goldstein, T.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{arXiv preprint arXiv:1712.09913}, 2017.

\bibitem[Liu et~al.(2017)Liu, Li, Shen, Huang, Yan, and Zhang]{liu17learning}
Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., and Zhang, C.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  2736--2744, 2017.

\bibitem[Mont{\'u}far et~al.(2014)Mont{\'u}far, Pascanu, Cho, and
  Bengio]{montufar14number}
Mont{\'u}far, G., Pascanu, R., Cho, K., and Bengio, Y.
\newblock On the number of linear regions of deep neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2924--2932, 2014.

\bibitem[Nguyen \& Hein(2017)Nguyen and Hein]{nguyen17loss}
Nguyen, Q. and Hein, M.
\newblock The loss surface of deep and wide neural networks.
\newblock In \emph{ICML}, 2017.

\bibitem[{Sagun} et~al.(2017){Sagun}, {Evci}, {Ugur Guney}, {Dauphin}, and
  {Bottou}]{sagun17empirical}
{Sagun}, L., {Evci}, U., {Ugur Guney}, V., {Dauphin}, Y., and {Bottou}, L.
\newblock {Empirical Analysis of the Hessian of Over-Parametrized Neural
  Networks}.
\newblock \emph{ArXiv e-prints}, June 2017.
\newblock URL \url{http://arxiv.org/abs/1706.04454}.

\bibitem[Sheppard et~al.(2008)Sheppard, Terrell, and
  Henkelman]{sheppard08optimization}
Sheppard, D., Terrell, R., and Henkelman, G.
\newblock Optimization methods for finding minimum energy paths.
\newblock \emph{The Journal of Chemical Physics}, 128\penalty0 (13):\penalty0
  134106, Apr 2008.
\newblock ISSN 1089-7690.
\newblock \doi{10.1063/1.2841941}.
\newblock URL \url{http://dx.doi.org/10.1063/1.2841941}.

\bibitem[{Soudry} \& {Carmon}(2016){Soudry} and {Carmon}]{soudry16bad}
{Soudry}, D. and {Carmon}, Y.
\newblock {No bad local minima: Data independent training error guarantees for
  multilayer neural networks}.
\newblock \emph{ArXiv e-prints}, May 2016.
\newblock URL \url{http://arxiv.org/abs/1605.08361}.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava14dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Vinyals \& Le(2015)Vinyals and Le]{vinyals15neural}
Vinyals, O. and Le, Q.~V.
\newblock A neural conversational model.
\newblock \emph{CoRR}, abs/1506.05869, 2015.
\newblock URL \url{http://arxiv.org/abs/1506.05869}.

\bibitem[Wales et~al.(1998)Wales, Miller, and Walsh]{wales98archetypal}
Wales, D.~J., Miller, M.~A., and Walsh, T.~R.
\newblock Archetypal energy landscapes.
\newblock \emph{Nature}, 394\penalty0 (6695):\penalty0 758, 1998.

\bibitem[Xiong et~al.(2017)Xiong, Droppo, Huang, Seide, Seltzer, Stolcke, Yu,
  and Zweig]{xiong17toward}
Xiong, W., Droppo, J., Huang, X., Seide, F., Seltzer, M.~L., Stolcke, A., Yu,
  D., and Zweig, G.
\newblock Toward human parity in conversational speech recognition.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 25\penalty0 (12):\penalty0 2410--2423, Dec 2017.
\newblock ISSN 2329-9290.
\newblock \doi{10.1109/TASLP.2017.2756440}.

\end{thebibliography}
